[
  {
    "id": "1",
    "text": "Cette plateforme a pour objectif de permettre aux citoyens d\u0027analyser par eux-mêmes les tweets politiques lors d\u0027événements spécifiques en France. Pour le cas de l\u0027élection présidentielle de 2017, #Idéo2017 analysait en quasi temps réel les messages des candidats, et fournissait leurs principales caractéris-tiques, l\u0027usage du lexique politique et des comparaisons entre les candidats."
  },
  {
    "id": "3",
    "text": "La recommandation de points d\u0027intérêts est devenue une ca-ractéristique essentielle des réseaux sociaux géo-localisés qui a accompa-gné l\u0027émergence des échanges massifs de données digitales. Cependant les faibles densités de points d\u0027intérêts visités par les utilisateurs rendent le problème difficile à traiter, d\u0027autant plus que les espaces de mobilité des utilisateurs sont très hétérogènes, allant de la ville au monde entier. Dans ce papier nous explorons l\u0027impact d\u0027une approche de clustering spatial sur la qualité de la recommandation. Notre approche est basée sur un modèle de factorisation de matrices de Poisson et un réseau social inféré des différents comportements de mobilité. Nous avons conduit une évaluation comparative des performances de notre approche sur un jeu de données réaliste. Les résultats expérimentaux montrent que notre approche permet une précision supérieure aux techniques de recomman-dation alternatives."
  },
  {
    "id": "4",
    "text": "Analyse des sentiments à partir des commentaires Facebook publiés en Arabe standard ou dialectal marocain par une approche d\u0027apprentissage automatique Résumé. L\u0027analyse des sentiments est un processus pendant lequel la polarité (positive, négative ou neutre) d\u0027un texte donné est déterminée. Nous nous in-téressons dans ce travail à l\u0027analyse des sentiments à partir des commentaires Facebook, réels, partagés en arabe standard ou dialectal marocain par une ap-proche basée sur l\u0027apprentissage automatique. Ce processus commence par la collecte des commentaires et leur annotation à l\u0027aide du crowdsourcing suivi d\u0027une phase de prétraitement du texte afin d\u0027extraire des mots arabes réduits à leur racine. Ces mots vont être utilisés pour la construction des variables d\u0027entrée en utilisant plusieurs combinaisons de schémas d\u0027extraction et de pondération. Pour réduire la dimensionnalité, une méthode de sélection de variables est ap-pliquée. Les résultats obtenus des expérimentations sont très prometteurs."
  },
  {
    "id": "5",
    "text": "Cet article présente une approche visant à extraire les informations ex-primées dans un corpus de textes et en produire un résumé. Plusieurs variantes de méthodes extractives de résumé de texte ont été implémentées et évaluées. Leur principale originalité réside dans l\u0027exploitation de structures appelées CDS (pour Clause Description Structure) issues d\u0027un composant d\u0027annotation en rôles sé-mantiques et non directement des phrases composant les textes. Le résumé ob-tenu est un sous-ensemble des CDS issus du corpus d\u0027origine ; ce format permet-tra dans la suite la détection d\u0027incohérences textuelles. Dans ce travail, nous re-transformons les CDS résumés en texte pour permettre la comparaison de notre approche avec celles de la littérature. Les premiers résultats sont très encoura-geants : les variantes que nous proposons obtiennent généralement de meilleurs scores que des implémentations de méthodes de référence."
  },
  {
    "id": "6",
    "text": "Notre analyse du domaine prédictif a établi deux constats : il n\u0027existe pas de définition univer-selle de scénario et l\u0027analyse de scénarios ne prend pas en compte suffisamment de contraintes pour établir des prédictions de qualité prouvée. Nous proposons donc une définition universelle des scénarios pour répondre au premier point. Pour répondre au second nous nous sommes inté-ressés au fait que les prédictions sont plus précises si le volume et l\u0027origine des données étaient conséquent et hétérogène. Ce constat nous a porté dans le domaine du Big Data auxquels doivent être ajoutés la sémantique, la Valeur et la Véracité des données pour une prédiction fiable et sensée. Nous montrons que les ontologies permettent de répondre à l\u0027ensemble de ces critères. Par ailleurs, nous avons identifié que les machines d\u0027états répondent aux besoins (en terme d\u0027outil et de formalisation) de l\u0027analyse de scénario. Nous proposons donc à partir de la définition uniformisée des scénarios, de les analyser par le biais des machines d\u0027états basées sur des ontologies. Cet article ébauche les prérequis de formalisation par la combinaison de 3 domaines qui apportent chacun les réponses aux différents éléments de la définition : l\u0027analyse de scénario, les ontologies et le Big Data."
  },
  {
    "id": "7",
    "text": "Avec plus de 800 000 décès par an dans le monde, le suicide est la troisième cause de décès évitable. Il y a 20 fois plus de tentatives, impliquant de nombreuses hospitalisations, des coûts humains et sociétaux énormes. Ces der-nières années, les modalités de collecte de données, sociologiques et cliniques, concernant les patients reçus en consultation après une tentative, ont connu de profonds changements liés aux outils numériques. Nous présentons les princi-paux résultats d\u0027un processus complet de fouille de données sur un échantillon de suicidants de deux hôpitaux européens. Le premier objectif est d\u0027identifier des groupes de patients similaires et le second d\u0027identifier des facteurs de risque associés au nombre de tentatives. Des méthodes non supervisées (ACM et clustering) et supervisées (arbres de régression) sont appliquées pour y répondre. Les résultats mettent en lumière l\u0027apport de la fouille de données à des fins des-criptives ou explicatives."
  },
  {
    "id": "8",
    "text": "Dans le cadre du clustering prédictif, pour attribuer la classe aux grou-pes formés à la fin de la phase d\u0027apprentissage, le vote majoritaire est la méthode communément utilisée. Cependant, cette approche comporte certaines limitations qui influent directement sur la qualité des résultats obtenus en termes de prédiction. Pour surmonter ce problème, nous proposons d\u0027incorporer des mo-dèles prédictifs localement dans les clusters formés afin d\u0027améliorer la qualité prédictive du modèle global. Les résultats expérimentaux montrent que cette incorporation permet d\u0027obtenir des résultats (en termes de prédiction) significati-vement meilleurs par rapport à ceux obtenus en utilisant le vote majoritaire ainsi que des résultats très compétitifs avec ceux obtenus par des algorithmes perfor-mants d\u0027apprentissage supervisé \"similaires\". Ceci est effectué sans dégrader le pouvoir descriptif (explicatif) du modèle global."
  },
  {
    "id": "9",
    "text": "En classification multi-labels, chaque instance est associée à un ou plusieurs labels. Par exemple, un morceau de musique peut être associé aux labels \u0027heureux\u0027 et \u0027relaxant\u0027. Des relations de co-occurrence peuvent exister entre les labels : par exemple, les labels \u0027heureux\u0027 et \u0027tris-te\u0027 ne peuvent pas être associés au même morceau de musique. Les labels peuvent aussi avoir des relations de préférence : par exemple, pour un mor-ceau de musique contenant plusieurs piques, le label \u0027heureux\u0027 est préféré par rapport au label \u0027relaxant\u0027. Les relations entre les labels peuvent aider à mieux prédire les labels associés aux instances. Les approches existantes peuvent apprendre soit les relations de co-occurrence, soit les relations de préférence. Ce travail introduit une approche permettant de combiner l\u0027apprentissage des deux types de relations. Les expérimentations menées montrent que la nouvelle approche introduite offre les meilleurs résultats de prédiction par rapports à cinq approches de l\u0027état de l\u0027art."
  },
  {
    "id": "10",
    "text": "Les tests A/B sont des procédures utilisées par les entreprises du web et de la santé entre autres, pour mesurer l\u0027impact d\u0027un changement de version d\u0027une variable par rapport à un objectif. Bien qu\u0027un nombre de plus en plus important de données soit disponible, la mise en place concrète d\u0027un tel test peut impliquer un coût important relatif à l\u0027observation et à l\u0027évaluation d\u0027une variation lorsque celle-ci n\u0027est pas optimale. Dans ce papier, nous présentons une nouvelle approche intégrant le principe d\u0027un bandit contextuel prenant en compte ces variables via une procédure de stratification."
  },
  {
    "id": "11",
    "text": "Ce papier propose une méthode basée sur la théorie des ensembles ap-proximatifs et dédiée à l\u0027apprentissage supervisé incrémental dans un contexte de données déséquilibrées. Cette méthode consiste en trois phases : la construction d\u0027une table de décision, l\u0027inférence d\u0027un ensemble de règles de décision et la classification de chaque action potentielle dans l\u0027une des classes de déci-sion prédéfinies. La méthode MAI2P est validée dans le contexte des MOOCs (Massive Open Online Courses)."
  },
  {
    "id": "12",
    "text": "1 Introduction"
  },
  {
    "id": "13",
    "text": "Introduction. La recherche bibliographique est une étape cruciale pour tout chercheur. En effet, la connaissance des travaux existant peut faire gagner un temps précieux tant pour le choix de la méthode à adopter que pour être à jour des dernières avancées. Néanmoins, trouver des articles similaires reste une tâche compliquée et pénible autant pour les domaines étendus que réduits. Les chercheurs passent un temps considérable à chercher des travaux proches de leurs intérêts de recherche, disséminés dans 47\u0027000 revues scientifiques appartenant à quelques 6000 éditeurs différents. Cette étape est cependant incontournable dans tout projet de recherche afin de confronter de nouvelles idées à des solutions existantes, ansi que pour l\u0027acquisition de connaissance à propos d\u0027un domaine spécifique. Dans cet article, une nouvelle méthode d\u0027ex-traction de connexions entre les catégories des mots-clés d\u0027articles scientifiques est proposée. Les limites de notre approche naïve héritée de la recherche exacte ont été soulignées dans La-tard et al. (2017), et cet article fournit une amélioration qui s\u0027attaque à ce problème. Notre recherche a pour but d\u0027intégrer les relations sémantiques dans les moteurs de recherche scien-tifiques afin de les rendre plus intelligents. Effectivement, en fonction du nombre de résultats renvoyés, une requête plus raffinée / étendue pourrait alors être proposée à l\u0027utilisateur."
  },
  {
    "id": "16",
    "text": "https://www-complexnetworks.lip6.fr/tarissan/ Résumé. Définir l\u0027importance des noeuds dans les réseaux statiques est une question de recherche très étudiée depuis de nombreuses années. Dernièrement, des adaptations des métriques classiques ont été proposées pour les réseaux dy-namiques. Ces méthodes reposent sur des approches très différentes dans leur façon d\u0027évaluer l\u0027importance des noeuds à un instant donné. Il est donc néces-saire de pouvoir les évaluer et les comparer. Dans cet article, nous comparons trois approches existes pour mieux comprendre ce qui les différencie. Nous mon-trons que la nature des jeux de données influe grandement sur le comportement des méthodes, et que pour certains d\u0027entre eux, la notion d\u0027importance n\u0027est pas toujours pertinente. Depuis de nombreuses années, les chercheurs étudiant les réseaux complexes se sont in-téressés à la question de l\u0027importance des noeuds. Cela a conduit à l\u0027introduction de plusieurs notions d\u0027importance : centralité de degré, centralité de proximité ou centralité d\u0027intermédia-rité. Les principales approches s\u0027appuient toutes sur la notion de chemin. En d\u0027autres termes un noeud est important s\u0027il est proche des autres noeuds ou si les chemins les plus courts passent par ce noeud. Récemment, des adaptations ont été introduites pour prendre en compte l\u0027as-pect temporel des réseaux complexes. Une première approche (Tang et al., 2010; Uddin et al., 2013) consiste à représenter un réseau dynamique comme une séquence de réseaux statiques. Une autre approche proposée par (Nicosia et al., 2013; Magnien et Tarissan, 2015) consiste à définir des chemins temporels comme une séquence de liens qui respecte l\u0027ordre chronolo-gique. Une autre approche encore consiste à construire un réseau statique à partir du réseau dynamique (Takaguchi et al., 2016) en dupliquant chaque noeud à l\u0027instant auquel il interagit. Enfin d\u0027autres propositions introduites notamment dans (Scholtes et al., 2016; Pan et Saramäki, 2011) prennent en compte les aspects temporels dans ces jeux de données mais ne permettent d\u0027obtenir qu\u0027une seule valeur globale d\u0027importance. Dans cet article nous étudions les trois approches qui considèrent l\u0027importance de noeud par plusieurs valeurs, et nous les comparons pour mieux comprendre leurs différences.-391"
  },
  {
    "id": "17",
    "text": "La tâche de similarité sémantique textuelle consiste à exprimer au-tomatiquement un nombre reflétant la similarité sémantique de deux fragments de texte. Chaque année depuis 2012, les campagnes de SemEval déroulent cette tâche de similarité sémantique textuelle. Cet article présente une méthode asso-ciant différentes représentations vectorielles de phrases dans l\u0027objectif d\u0027amélio-rer les résultats obtenus en similarité sémantique. Notre hypothèse est que dif-férentes représentations permettraient de représenter différents aspects séman-tiques, et par extension, d\u0027améliorer les similarités calculées, la principale dif-ficulté étant de sélectionner les représentations les plus complémentaires pour cette tâche. Notre système se base sur le système vainqueur de la campagne de 2015 ainsi que sur notre méthode de sélection par complémentarité. Les résultats obtenus viennent confirmer l\u0027intérêt de cette méthode lorsqu\u0027ils sont comparés aux résultats de la campagne de 2016."
  },
  {
    "id": "18",
    "text": "L\u0027émergence de l\u0027IoT et du traitement en temps-réel oblige les en-treprises à considérer la détection d\u0027anomalies comme un élément clé de leur activité. Afin de garantir une haute précision dans le processus de détection, des métadonnées fournissant un contexte spatio-temporel sur les mesures des cap-teurs sont nécessaires. Dans cet article, nous présentons un système générique qui aide à capturer, analyser, qualifier et stocker les informations contextuelles d\u0027un domaine d\u0027application donné. L\u0027approche proposée est basée sur des mé-thodes sémantiques qui exploitent des ontologies pour évaluer la pertinence de l\u0027information contextuelle. Après une description des composants principaux de l\u0027architecture, la performance et la pertinence du système sont démontrées par une évaluation sur des ensembles de données du monde réel."
  },
  {
    "id": "19",
    "text": "L\u0027article définit les contraintes prescriptives comme des règles permettant aux moteurs d\u0027inférence de vérifier que certains objets formels sont réellement utilisés-pas seulement inférés-ou non, dans certaines conditions. Il montre que ces contraintes nécessitent de ne pas exploiter de mécanisme d\u0027héritage (ou autres mécanismes ajoutant des relations à des objets) durant les tests des conclusions des règles. Il donne une méthode générale pour effectuer cela et des commandes SPARQL pour implémenter cette méthode lorsque les règles sont représentées via des relations sous-classe-de entre conditions et conclusions. L\u0027article illustre ces commandes avec la vérification de patrons de conception d\u0027ontologies. Plus généralement, l\u0027approche peut être utilisée pour vérifier la complétude d\u0027une ontologie, ou représenter dans une ontologie (plutôt que par des requêtes ou des procédures ad hoc) des contraintes permettant de calculer un degré de complétude d\u0027ontologie. L\u0027approche peut ainsi aider l\u0027élicitation, la modélisation ou la validation de connaissances."
  },
  {
    "id": "20",
    "text": "1 Contexte et motivations"
  },
  {
    "id": "21",
    "text": "Les données séquentielles sont aujourd\u0027hui omniprésentes et concernent divers domaines d\u0027application. La fouille de données de séquences permet d\u0027extraire des informations et des connaissances pouvant être à forte va-leur ajoutée. Cependant, lorsque les données de séquences sont riches en don-nées numériques, des méthodes de fouille de données plus fines sont nécessaires pour extraire des connaissances plus expressives représentant la variabilité des valeurs numériques ainsi que leur éventuelle interdépendance. Dans cet article, nous présentons une nouvelle méthode de découverte de séquences graduelles fréquentes représentées par des graphes à partir d\u0027une source de données de sé-quences en RDF (Resource Description Framework 1). Ces dernières sont trans-formées en graphes graduels partiellement ordonnés, gpo. Nous proposons un algorithme permettant de découvrir les sous-graphes gpo fréquents. Une expéri-mentation sur deux jeux de données réelles ont montré la faisabilité et la perti-nence de notre approche."
  },
  {
    "id": "22",
    "text": "DBpédia, qui encode les connaissances de Wikipédia, est devenue une base de référence pour le web des données. Les ressources peuvent y être réper-toriées par des catégories définies manuellement, dont la sémantique n\u0027est pas directement accessible par des machines. Dans cet article, nous proposons de remédier à cette lacune au moyen de méthodes de fouille de données, à savoir la recherche de règles d\u0027associations et de motifs apparentés. Nous présentons une étude comparative de ces variantes sur une partie de DBpédia et discutons le potentiel des différentes approches."
  },
  {
    "id": "23",
    "text": "Détection de Singularités en temps-réel par combinaison d\u0027apprentissage automatique et web sémantique basés sur Spark"
  },
  {
    "id": "24",
    "text": "L\u0027échantillonnage de motifs est une méthode non-exhaustive pour dé-couvrir des motifs pertinents qui assure une bonne interactivité tout en offrant des garanties statistiques fortes grâce à sa nature aléatoire. Curieusement, une telle approche explorée pour les motifs ensemblistes et les sous-graphes ne l\u0027a pas encore été pour les données séquentielles. Dans cet article, nous proposons la première méthode d\u0027échantillonnage de motifs séquentiels. Outre le passage aux séquences, l\u0027originalité de notre approche est d\u0027introduire une contrainte sur la norme pour maîtriser la longueur des motifs tirés et éviter l\u0027écueil de la « longue traîne ». Nous démontrons que notre méthode fondée sur une procédure aléatoire en deux étapes effectue un tirage exact. Malgré le recours à un échantillonnage avec rejet, les expérimentations montrent qu\u0027elle reste performante."
  },
  {
    "id": "25",
    "text": "FIG. 1-eDOI : création d\u0027une série de sous-graphes basés sur une mesure d\u0027intérêt. Les som-mets bleus sont sélectionnés par l\u0027utilisateur. Chaque sélection utilisateur permet la création d\u0027un nouveau sous-graphe d\u0027intérêt supérieur. Ce dernier est issu du graphe initial et prend en compte les informations sélectionnées dans les sous-graphes précédents. Bien souvent les représentations noeuds-liens d\u0027un réseau sont difficilement lisibles à cause d\u0027une grande taille ou d\u0027une topologie dense ou complexe. Il s\u0027avère dès lors important de s\u0027écarter de la stratégie classique proposant en première intention une vue d\u0027ensemble du graphe. Dans cette optique, nous proposons une stratégie d\u0027exploration appelée eDOI basée sur un calcul de sous-ensembles de noeuds intéressants pour l\u0027utilisateur intégrant à la fois un aspect sémantique et prenant en compte la structure multi-couches potentielle de ces réseaux (i.e. des noeuds d\u0027un type donné pouvant être regroupés en sous-réseaux (Kivelä et al., 2014)). Dans le scénario que nous considérons, l\u0027utilisateur mène une activité d\u0027exploration en ayant une vue partielle des données d\u0027une manière comparable aux moteurs de recherche qui déterminent les éléments les plus pertinents au vu d\u0027une requête utilisateur. Notre méthode-379"
  },
  {
    "id": "26",
    "text": "Ce poster rend compte d\u0027une entreprise d\u0027élaboration d\u0027un système de représentation des connaissances pour le domaine géotechnique. 1. Caractérisation du domaine Ce domaine se caractérise en effet par une grande disparité des données techniques (données largement inconnues ou approchées, liées aux caractéristiques physiques des sols) et une forte propension pour la documentation du domaine à être d\u0027ordre descriptif, le lecteur devant raisonner par analogie pour savoir si l\u0027article sera pertinent pour son problème. 2. Objectif poursuivi Trouver dans la base de données, les articles qui contiennent des phrases qui aideront l\u0027utilisateur à maîtriser son thème de recherche. 3. Source des données Les données d\u0027origine sont extraites de corpus documentaires rassemblé par les ingénieurs du domaine. Les corpus ainsi constitués représentent plusieurs années des travaux d\u0027une communauté transcrits dans des congrès annuels (AFTES ou AITES en l\u0027occurrence). 4. Originalité de la démarche Pour répondre à notre problématique, nous avons d\u0027abord envisagé une ontologie de domaine formalisée en OWL, mais nous nous sommes heurtés à plusieurs difficultés :-La multiplicité des spécialités géotechniques induit une multiplicité d\u0027emplois lexicaux spécialisés et un coût de normalisation assez élevé.-Le raisonnement par catégories aristotéliciennes correspond au final assez peu au raisonnement usuellement employé par les experts du domaine, qui raisonnent plutôt par prototypes et proximité sémantique.-Il existe aussi dans la profession une résistance diffuse à l\u0027utilisation d\u0027un formalisme de représentation étranger au mode de pensée des experts, et plus encore à toute tentative de normaliser ces modes de pensée. Donc, le projet est construit sur une base purement lexicale et informelle, sans formalisme de représentation nécessitant spécification ou formalisation des données de départ.-367"
  },
  {
    "id": "27",
    "text": "Un lien inter-langue dans Wikipédia est un lien qui mène d\u0027un article appartenant à une édition linguistique à un autre article décrivant le même concept dans une autre langue. Ces liens sont ajoutés manuellement par les utili-sateurs de Wikipédia et ainsi ils sont susceptibles d\u0027être erronés. Dans ce papier, nous proposons une approche pour l\u0027élimination automatique des liens inter-langues. Le principe de base est que la présence d\u0027un lien erroné est révélée par l\u0027existence d\u0027un chemin de liens inter-langues reliant deux articles appartenant à une même édition linguistique. Notre approche élimine des liens inter-langues, à partir de ceux qui ont un faible score de correction, jusqu\u0027à ce qu\u0027il n\u0027y ait plus de chemins entre deux articles d\u0027une même édition linguistique. Les résul-tats de notre évaluation sur un sous-graphe de Wikipédia consistant en 8 langues montre que l\u0027approche est prometteuse."
  },
  {
    "id": "28",
    "text": "Dans cet article, nous présentons une méthode d\u0027analyse de corpus afin de générer deux interfaces originales de visualisation dans le domaine de l\u0027e-recrutement. Notre approche s\u0027appuie sur des millions de profils issus de plusieurs réseaux sociaux et sur des milliers d\u0027offres d\u0027emploi collectées sur Internet. Nous décrivons dans ces travaux les étapes nécessaires pour leur réali-sation. La première visualisation est une carte dynamique indiquant les métiers qui recrutent, dans quel domaine, dans quelle région tandis que la seconde met en avant les parcours professionnels et permet d\u0027observer les perspectives ainsi que les antécédents à plus ou moins long terme pour chaque métier considéré."
  },
  {
    "id": "29",
    "text": "Dans les corpus de textes scientifiques, certains articles issus de com-munautés de chercheurs différentes peuvent ne pas être décrits par les mêmes mots-clés alors qu\u0027ils partagent la même thématique. Ce phénomène cause des problèmes dans la recherche d\u0027information, ces articles étant mal indexés, et limite les échanges potentiellement fructueux entre disciplines scientifiques. Notre modèle permet d\u0027attribuer automatiquement une étiquette thématique aux articles au moyen d\u0027un apprentissage des représentations sémantiques d\u0027articles du corpus déjà étiquetés. Passant bien à l\u0027échelle, cette méthode a pu être testée sur une bibliothèque numérique d\u0027articles scientifiques comportant des millions de documents. Nous utilisons un réseau sémantique de synonymes pour extraire davantage d\u0027articles sémantiquement similaires et nous les fusionnons avec ceux obtenus par un modèle de classement thématique. Cette méthode combinée pré-sente de meilleurs taux de rappel que les versions utilisant soit le réseau séman-tique seul, soit la seule représentation sémantique des textes."
  },
  {
    "id": "30",
    "text": "L\u0027influence, d\u0027un point de vue social, peut être définie comme le pouvoir d\u0027un individu qui mobilise des individus cibles pour des actions concrètes ou une opinion donnée. Détecter au-tomatiquement les influenceurs dans les réseaux sociaux fournit des points d\u0027entrée efficaces afin d\u0027avoir un impact dans le cadre d\u0027applications comme la diffusion ciblée d\u0027une information de santé publique, la promotion d\u0027un produit ou encore la réputation en ligne. Nos travaux s\u0027intègrent au projet européen SOMA 1 qui a pour but d\u0027enrichir les connaissances client de systèmes CRM 2 par de l\u0027information issue de médias sociaux ; l\u0027influence en fait partie afin d\u0027évaluer l\u0027impact potentiel d\u0027un client par exemple sur un produit donné. Il est possible d\u0027analyser un réseau social selon sa structure ou le contenu diffusé, ces deux aspects étant les marqueurs de relations interpersonnelles essentielles à l\u0027influence. Les approches qui analysent le contenu s\u0027intéressent au texte en tenant compte de marqueurs dis-cursifs, comme l\u0027argumentation, qui tendent à influencer (Rosenthal et al., 2012). La structure du réseau est utilisée par les mesures de centralité, qui identifient les noeuds dominants d\u0027un réseau d\u0027après leurs liens. Elles sont beaucoup utilisées parce qu\u0027elles ne requièrent princi-palement qu\u0027un graphe d\u0027interactions et sont suffisamment variées pour modéliser différentes modalités d\u0027influence (Benyahia et Largeron, 2015). Cette variété requiert de pouvoir appré-hender les meilleurs cas d\u0027usage des différentes mesures en les évaluant et en les comparant notamment sur des données aux applications diverses (Ghazzali et Ouellet, 2017). Nous voulons déterminer les mesures de centralité les plus aptes à identifier les influenceurs en les confrontant à des données réelles portant différentes catégories d\u0027information sur un ensemble d\u0027utilisateurs. Nous utilisons un corpus constitué dans le cadre de la compétition RepLab 2014 (Amigó et al., 2014) qui présente plus de 7000 comptes Twitter 3 manuellement annotés selon qu\u0027ils sont influenceurs ou non. Nous avons sélectionné six algorithmes qui sont le Degré entrant comme Baseline, l\u0027Intermédiarité qui mesure si un individu fait office de"
  },
  {
    "id": "31",
    "text": "1 Introduction"
  },
  {
    "id": "32",
    "text": "Sur Internet, l\u0027information se propage en particulier au travers des documents textuels. Cette propagation soulève de nombreux défis : identifier une information, suivre son évolution dans le temps, comprendre les mécanismes qui régissent sa propagation, etc. Étant donné un document parmi un grand corpus dans lequel de nombreuses informations circulent, pouvons-nous retrouver les chemins empruntés par l\u0027information pour arriver à ce document ? Nous propo-sons de définir la notion de trajectoire comme l\u0027ensemble des chemins le long desquels de l\u0027information s\u0027est propagée et nous proposons une méthode pour l\u0027estimer. Nous avons mis en oeuvre une évaluation humaine pour juger de la qualité des chemins calculés. Nous montrons que les évaluations concordent la plupart du temps et que notre algorithme est efficace pour retrouver les bons chemins."
  },
  {
    "id": "33",
    "text": "Cet article propose une méthode d\u0027analyse pour des enregistrements opérationnels d\u0027un ensemble de compteurs d\u0027essieux, qui constituent un élément central à l\u0027infrastructure ferroviaire. Notre objectif est de fournir une façon ef-ficace d\u0027extraire automatiquement des éléments de connaissance concernant les défaillances de ces systèmes. Puisque les données fournies ne contiennent pas de vérité de terrain sur les causes de défaillances, les informations et leurs causes doivent être extraites des relations sous-tendant les événements enregistrés. Après une phase de prétraite-ment, les événements sont groupés en fonction des relations qui ont été mises en lumière entre eux. Ces regroupements peuvent ensuite être utilisés pour créer des classes d\u0027événements en utilisant un système de classification adapté. Au delà de cette application spécifique, cette approche est une façon nouvelle d\u0027aborder les problèmes d\u0027analyse de fiabilité."
  },
  {
    "id": "35",
    "text": "Dans cet article nous étudions le problème de l\u0027extraction de motifs fréquents contenant des événements positifs, des événements négatifs spécifiant l\u0027absence d\u0027événement ainsi que des informations temporelles sur le délai entre ces événements. Nous définissons la sémantique de tels motifs et proposons la méthode NTGSP basée sur des approches de l\u0027état de l\u0027art. Les performances de la méthode sont évaluées sur des données commerciales fournies par EDF (Électricité de France)."
  },
  {
    "id": "36",
    "text": "Les systèmes orientés documents permettent de stocker tout document , quel que soit leur schéma. Cette flexibilité génère une potentielle hété-rogénéité des documents qui complexifie leur interrogation car une même entité peut être décrite selon des schémas différents. Cet article présente une approche d\u0027interrogation transparente des systèmes orientés documents. Pour cela, nous proposons de générer un dictionnaire de façon automatique lors de l\u0027insertion des documents, et qui associe à chaque attribut tous les chemins permettant d\u0027y accéder. Ce dictionnaire permet de réécrire la requête utilisateur à partir de dis-jonctions de chemins afin de retrouver tous les documents quelles que soient leurs structures. Nos expérimentations montrent des coûts d\u0027exécution de la re-quête réécrite largement acceptables comparés au coût d\u0027une requête sur sché-mas homogènes."
  },
  {
    "id": "37",
    "text": "Afin d\u0027aider les apprenants à tirer profit du MOOC (Massive Open Online Course) qu\u0027ils suivent, nous proposons un outil pour recommander à chacun d\u0027entre eux une liste ordonnée des \"Apprenants leaders\" capables de le soutenir durant son processus d\u0027apprentissage. La phase de recommandation est basée sur une approche d\u0027aide à la décision multicritère pour la prédiction périodique des \"Apprenants leaders\". Etant donnée l\u0027hétérogénéité des profils des apprenants, nous recommandons à chacun d\u0027entre eux les leaders appropriés à son profil en utilisant la distance euclidienne et le filtrage démographique."
  },
  {
    "id": "38",
    "text": "Les systèmes de recommandation ont pour rôle d\u0027aider les utilisateurs submergés par la quantité d\u0027information à faire de bons choix à partir de vastes catalogues de produits. Le déploiement de ces systèmes dans l\u0027industrie hôte-lière est confronté à des contraintes spécifiques, limitant la performance des ap-proches traditionnelles. Les systèmes de recommandation d\u0027hôtels souffrent en particulier d\u0027un problème de démarrage à froid continu à cause de la volatilité des préférences des voyageurs et du changement de comportements en fonction du contexte. Dans cet article, nous présentons le problème de recommandation d\u0027hôtels ainsi que ses caractéristiques distinctives. Nous proposons de nouvelles méthodes contextuelles qui prennent en compte les dimensions géographique et temporelle ainsi que la raison du voyage, afin de générer les listes de recom-mandation. Nos expérimentations sur des jeux de données réels soulignent la contribution des données contextuelles à l\u0027amélioration de la qualité de recom-mandation."
  },
  {
    "id": "39",
    "text": "L\u0027intégration des données hétérogènes en Sciences de la Vie est un sujet de recherche majeur. L\u0027importance et le volume considérable des informa-tions sur les milieux de vie des microorganismes dans tous les domaines tels que la santé, l\u0027agriculture ou l\u0027environnement justifie le développement de trai-tements automatisés. Nous proposons ici l\u0027ontologie OntoBiotope dont nous dé-crivons les principes de construction ainsi que des exemples d\u0027utilisation pour l\u0027annotation et l\u0027indexation sémantique des habitats microbiens décrits en langue naturelle dans les documents scientifiques."
  },
  {
    "id": "41",
    "text": "Ce court article présente le design et l\u0027utilisation d\u0027un tableau de bord visuel permettant d\u0027explorer, questionner et comprendre l\u0027évolution des com-munautés d\u0027un graphe dynamique. L\u0027exemple ayant motivé la conception et la réalisation de ce tableau de bord est celui d\u0027un réseau d\u0027affiliation des personna-lités présentes dans les médias français. Le suivi de communautés s\u0027avère utile pour cerner le biais potentiel induit de la co-présence répétée des mêmes person-nalités dans les émissions de radio et de télévision au cours du temps."
  },
  {
    "id": "42",
    "text": "Nous présentons dans ce papier un nouvel algorithme Mean-Shift uti-lisant les K-plus proches voisins pour la montée du gradient (NNMS : Nearest Neighbours Mean Shift). Le coût computationnel intensif de ce dernier a long-temps limité son utilisation sur des jeux de données complexes où un partition-nement en clusters non ellipsoïdaux serait bénéfique. Or, une implémentation scalable de l\u0027algorithme ne compense pas l\u0027augmentation du temps d\u0027exécution en fonction de la taille du jeu de données en raison de sa complexité quadra-tique. Afin de pallier, ce problème nous avons introduit le \"Locality Sensitive Hashing\" (LSH) qui est une approximation de la recherche des K-plus proches voisins ainsi qu\u0027une règle empirique pour le choix du K. La combinaison de ces améliorations au sein du NNMS offre l\u0027opportunité d\u0027un traitement pertinent aux problématiques du clustering appliquée aux données massives."
  },
  {
    "id": "43",
    "text": "La multiplicité des enquêtes d\u0027opinion sur un même sujet nécessite la construction de synthèses qui agrègent les résultats obtenus dans des conditions indépendantes. Dans cet article, nous proposons une nouvelle approche ordinale de méta-analyse qui consiste à rechercher un ordre consensus qui rend compte « au mieux » des ordres partiels entre les modalités issus des résultats des diffé-rentes enquêtes. Nous modélisons ce problème par une variante d\u0027une recherche d\u0027un ordre médian sur les sommets d\u0027un graphe orienté pondéré et nous dévelop-pons un algorithme de séparation-évaluation pour le résoudre. Notre approche est appliquée sur un ensemble d\u0027enquêtes internationales portant sur les motivations et les freins à l\u0027intégration de l\u0027Internet des Objets dans les entreprises."
  },
  {
    "id": "44",
    "text": "Les portails d\u0027actualités en ligne produisent un flux d\u0027information ayant un volume et une vélocité importants. Dans ce contexte, il devient plus difficile de proposer en temps réel des recommandations dynamiques adaptées aux intérêts de chaque utilisateur. Dans cet article, nous présentons une approche hybride pour la recommandation des articles d\u0027actualité reposant sur l\u0027analyse sémantique du contenu disponible. L\u0027approche est basée sur l\u0027hybridation de plusieurs approches personnalisées et non personnalisées pour remédier au pro-blème de démarrage à froid. L\u0027expérimentation de notre approche dans un en-vironnement à large échelle et à fortes contraintes temps réel dans le cadre du challenge NEWSREEL a permis d\u0027évaluer la qualité de ses recommandations et de confirmer l\u0027apport de la sémantique dans le processus de recommandation."
  },
  {
    "id": "45",
    "text": "De nombreux travaux actuels s\u0027intéressent aux microblogs et à leur exploitation. Par exemple, SanJuan et al. (2012) ont introduit une tâche d\u0027évaluation à CLEF concernant la contextualisation de tweets pour aider à leur compréhension, en particulier dans le cadre d\u0027évè-nements comme les festivals (Goeuriot et al., 2016; Ermakova et al., 2017). Un évènement possède trois composants essentiels : une localisation, une temporalité, une information sur l\u0027entité concernée. Cet article est centré sur la dimension de localisation qui est vitale pour les applications géo-spatiales (Munro, 2011). Au cours des dernières années, plusieurs systèmes de reconnaissance d\u0027entités nommées (EN) traitent du problème de l\u0027ex-traction de localisations spécifiées dans les documents ; mais ces systèmes ne fonctionnent pas bien sur des textes informels. Plusieurs méthodes se sont intéressées à l\u0027extraction de localisation dans des textes comme Ritter tool (Ritter et al., 2011), Gate NLP (Bontcheva et al., 2013) et Stanford NER (Finkel et al., 2005). Nous avons étudié la combinaison de ces trois méthodes : nous avons extrait les localisations identifiées par chacun des trois outils et les avons fusionnés. Nous avons égale-ment considéré leur filtrage après extraction en nous appuyant sur la base DBpedia. Pour les évaluations, nous avons utilisé deux collections standards : la collection Ritter (Ritter et al., 2011) et la collection MSM2013 (Cano Basave et al., 2013). La collection Ritter contient 2 394 tweets dont 213 (soit 8, 8%) avec localisation et 2 181 sans. MSM2013 contient 2 815 tweets dont 496 (soit 17, 6%) avec localisation et 2 319 sans. Les résultats sont présentés dans la table 1 (avec le test statistique). Données Ritter Données MSM2013 R(%) P(%) F(%) R(%) P(%) F(%) Ritter (témoin)"
  },
  {
    "id": "46",
    "text": "Avec l\u0027avènement des mégadonnées, l\u0027informatique décisionnelle a dû trouver des solutions pour gérer des données de très grands volume et va-riété. Les lacs de données (data lakes) répondent à ces besoins du point du vue du stockage, mais nécessitent la gestion de métadonnées adéquates pour garan-tir un accès efficace aux données. Sur la base d\u0027un modèle multidimensionnel de métadonnées conçu pour un lac de données présentant un défaut d\u0027évolu-tivité de schéma, nous proposons l\u0027utilisation d\u0027un data vault pour traiter ce problème. Pour montrer la faisabilité de cette approche, nous instancions notre modèle conceptuel de métadonnées en modèles logiques et physiques relation-nel et orienté document. Nous comparons également les modèles physiques en termes de stockage et de temps de réponse aux requêtes sur les métadonnées."
  },
  {
    "id": "49",
    "text": "1 Introduction"
  },
  {
    "id": "50",
    "text": "1 Introduction"
  },
  {
    "id": "52",
    "text": "1 Introduction"
  },
  {
    "id": "55",
    "text": "Le financement participatif est un mode de financement d\u0027un projet faisant appel à un grand nombre de personnes qui a connu une forte croissance avec l\u0027émergence d\u0027Internet et des réseaux sociaux. Cependant plus de 60 % des projets ne sont pas financés, il est donc important de bien préparer sa campagne de financement. De plus, en cours de cam-pagne, il est crucial d\u0027avoir une estimation rapide de son succès afin de pouvoir réagir rapidement (restructuration, communication) : des outils de prédiction sont alors indispensables. Nous proposons dans cet article plu-sieurs pistes d\u0027amélioration pour la prédiction du montant levé lors d\u0027une campagne de financement participatif en utilisant l\u0027algorithme k-NN. La première proposition consiste à utiliser un algorithme de clustering afin de segmenter l\u0027ensemble d\u0027apprentissage et faciliter le passage à l\u0027échelle. La seconde proposition consiste à extraire des caractéristiques pertinentes depuis les séries temporelles et les informations sur les campagnes pour avoir une représentation vectorielle."
  },
  {
    "id": "56",
    "text": "L\u0027apprentissage automatique, pardon le « machine learning », a envahi la sphère médiatique grâce à des succès impressionnants comme la victoire d\u0027une machine au Go, ou la promesse de véhicules autonomes arrivant très prochainement sur nos routes. De fait, tant l\u0027exploitation des données massives que la production de code machine à partir de l\u0027expérience de la machine plutôt que par des humains, met l\u0027apprentissage automatique au coeur de l\u0027intelligence artificielle. Très certainement cela signifie que nous savons répondre à la question « qu\u0027est-ce qu\u0027un bon système d\u0027apprentissage ? » et qu\u0027il ne nous reste plus qu\u0027à en décliner la réponse pour obtenir des systèmes adaptés à chaque domaine applicatif. Pourtant, la réponse à cette question a profondément évolué au cours des 60 dernières années, au point que les publications sur l\u0027apprentissage automatique d\u0027il y a quelques décennies semblent venir d\u0027une autre planète et ne sont d\u0027ailleurs plus enseignés aux étudiants. Et ceci pas seulement parce que les connaissances passées seraient jugées obsolètes, mais parce qu\u0027elles ne semblent pas per-tinentes. Avons-nous donc raison ? Nos précurseurs avaient-ils tort ? Et nos successeurs nous citeront-ils dans leurs manuels ? Dans cette présentation, nous examinerons quelques moments clés de l\u0027histoire de l\u0027apprentissage automatique correspondant à des tournants dans la manière de considérer ce qu\u0027est un bon système d\u0027apprentissage. Et nous nous demanderons si nous vivons un autre moment charnière dans lequel changent notre perspective, la question que nous cherchons à résoudre dans nos recherches, les concepts manipulés et la manière d\u0027écrire nos papiers."
  },
  {
    "id": "60",
    "text": "La date de pose est souvent un facteur principal d\u0027explication de la dé-gradation des conduites d\u0027assainissement. Pour les gestionnaires de ces réseaux, connaître cette information permet ainsi (par l\u0027utilisation de modèles de dété-rioration) de prédire l\u0027état de santé actuel des conduites non encore inspectées. Cette connaissance est primordiale pour prendre des décisions dans un contexte de forte contrainte budgétaire. L\u0027objectif est ainsi de reconstituer ces dates de pose à partir des caractéristiques du patrimoine et de son environnement. Les données à manipuler présentent plusieurs niveaux de complexité importants. Leurs sources sont hétérogènes, leur volume est important et les informations sur leur étiquetage (dates) sont limitées : seulement 24 % du linéaire est connu pour les réseaux d\u0027assainissement de la métropole de Lyon. La base de données sous-jacente contient les caractéristiques connues des conduites (profil géomé-trique, matériau utilisé, etc.). Dans ce papier, nous proposons de mesurer l\u0027effet et l\u0027impact de quelques méthodes d\u0027apprentissage statistique semi-supervisé, et de proposer ainsi une approche alternative adaptée à la reconstitution de ce type de données."
  },
  {
    "id": "61",
    "text": "L\u0027évaluation périodique du risque de chute des personnes âgées requiert des informations fiables et nombreuses. Comme il n\u0027est pas possible de recueillir régulièrement toutes ces informations, les observations sont faites au fil du temps et conservées, ce qui entraîne une probléma-tique liée au vieillissement des informations. Cet article traite de la dé-tection des informations obsolètes dans une base d\u0027informations sur une personne âgée. Nous proposons une solution comportant un modèle de connaissances sur les personnes âgées sous forme d\u0027un réseau bayésien et un module de raisonnement chargé de la détection et de la gestion des contradictions et des doutes sur les informations."
  },
  {
    "id": "62",
    "text": "Il existe de nombreux travaux sur l\u0027analyse numérique d\u0027une image ou vidéo. Ces travaux concernent soit l\u0027amélioration du signal pour améliorer la qualité de l\u0027image, soit la reconnaissance d\u0027éléments contenue dans l\u0027image. Au delà des travaux d\u0027extraction d\u0027information à partir du signal numérique construit par les caméras, nous nous sommes intéressés, à la déduction de nouvelles connaissances par agré-gation d\u0027informations provenant de sources hétérogènes multiples. Notre ambition est d\u0027ex-traire des connaissances en couplant les informations issues des algorithmes habituels de trai-tement d\u0027images avec des connaissances contextuels et des savoir-faire liés à l\u0027usage d\u0027un bâtiment. Cette interopérabilité est réalisée par l\u0027intermédiaire d\u0027une agrégation d\u0027ontologies. L\u0027ontologie utilise les axiomes définis dans le langage OWL-2. Elle fournit un vocabulaire pour intégrer, re-organiser et analyser sémantiquement des sources de données hétérogènes. Notre ontologie est constituée d\u0027un ensemble de termes dérivés de plusieurs ontologies :-L\u0027ontologie DUL, fournit un ensemble de concepts utilisés pour permettre l\u0027interopéra-bilité entre différentes ontologies.-L\u0027ontologie event, traite de la notion d\u0027événement et les propiétés associées telles que la localisation le temps, les agents, les facteurs et les produits.-L\u0027ontologie ifcowl, est une représentation sémantique du schéma IFC (standard pour la représentation des données du bâtiment).-L\u0027ontologie person fournit la classe pour décrire une personne physique.-L\u0027ontologie ssn permet la description des capteurs, des observations, des traitements de détection, les capacités de mesures et tout autre concept relatif.-L\u0027ontologie du temps time fournit les concepts pour décrire les propriétés temporelles des ressources. Toutes les sources de données hétérogènes sont intégrées dans l\u0027ontologie du système Wi-seNET (Marroquin et al., 2016) en utilisant des techniques issues du linked data : les URIs (Uniform Resource Identifiers) et RDF (Resource Description Framework). Le traitement des connaissances issues des images est organisé en deux étapes. La première étape concerne la partie extraction et la seconde concerne la partie gestion (voir figure 1). L\u0027extraction des connaissances consiste à extraire une partie des données d\u0027un ensemble, ensuite filtrer les données pour ne conserver que les données pertinentes et finalement, à en-richir les données avec de la connaissance. Dans notre cas, les caméras intelligentes ajoutent de la connaissance aux parties d\u0027images sélectionnées a l\u0027aide d\u0027algorithmes de traitement d\u0027images. Après avoir réalisé l\u0027extraction des connaissances à partir de ce que voit le réseau-385"
  },
  {
    "id": "63",
    "text": "L\u0027objectif de ce travail est de décrire avec une approche réaliste la signification des données d\u0027observation en neuro-imagerie sous un format for-mel pour faciliter leur interprétation par les cliniciens et leur réutilisation dans d\u0027autres contextes."
  },
  {
    "id": "65",
    "text": "Nous proposons un modèle de co-clustering de données mixtes et un critère Bayésien de sélection du meilleur modèle. Le modèle infère automati-quement les discrétisations optimales de toutes les variables et effectue un co-clustering en minimisant un critère Bayésien de sélection de modèle. Un avan-tage de cette approche est qu\u0027elle ne nécessite aucun paramètre utilisateur. De plus, le critère proposé mesure de façon exacte la qualité d\u0027un modèle tout en étant régularisé. L\u0027optimisation de ce critère permet donc d\u0027améliorer continuel-lement les modèles trouvés sans pour autant sur-apprendre les données. Les ex-périences réalisées sur des données réelles montrent l\u0027intérêt de cette approche pour l\u0027analyse exploratoire des grandes bases de données."
  },
  {
    "id": "66",
    "text": "Les portails d\u0027actualités en ligne produisent un flux d\u0027information ayant un volume et une vélocité importants. Dans ce contexte, il devient plus difficile de proposer en temps réel des recommandations dynamiques adaptées aux intérêts de chaque utilisateur. Dans cet article, nous présentons une approche hybride pour la recommandation des articles d\u0027actualité reposant sur l\u0027analyse sémantique du contenu disponible. L\u0027approche est basée sur l\u0027hybridation de plusieurs approches personnalisées et non personnalisées pour remédier au pro-blème de démarrage à froid. L\u0027expérimentation de notre approche dans un en-vironnement à large échelle et à fortes contraintes temps réel dans le cadre du challenge NEWSREEL a permis d\u0027évaluer la qualité de ses recommandations et de confirmer l\u0027apport de la sémantique dans le processus de recommandation."
  },
  {
    "id": "67",
    "text": "1 Introduction"
  },
  {
    "id": "68",
    "text": "L\u0027objectif de notre recherche est de répondre aux besoins croissants et divers d\u0027extraction d\u0027information pertinente exprimés par de nombreuses disciplines. Nous utilisons pour cela l\u0027analyseur multilingue de corpus Unitex/Gram-Lab développé à l\u0027Université Paris-Est Marne-la-Vallée. Il fait appel à une ap-proche symbolique et utilise des ressources linguistiques, dictionnaires électro-niques et grammaires locales. Cette présentation ne constitue qu\u0027une prise en main d\u0027Unitex/GramLab et ne reflète que très partiellement les possibilités du logiciel et son champ d\u0027utilisation, notamment pour l\u0027extraction d\u0027information, qui s\u0027étend du monde de la recherche à celui de l\u0027industrie."
  },
  {
    "id": "69",
    "text": "Universal-endpoint.com est une plateforme web permettant un accès simple au Web des Données par trois aspects : (i) une plateforme de correspon-dance, pour l\u0027accès aux bases du Web des Données depuis un seul point d\u0027accès centralisé, (ii) le langage SimplePARQL, pour une écriture intuitive de requêtes sous forme de triplets à la manière de SPARQL mais ne nécessitant pas une connaissance préalable des bases du Web des Données, et (iii) une aide à la rédaction de requêtes SPARQL."
  },
  {
    "id": "70",
    "text": "Avec l\u0027avènement des réseaux sociaux et la multiplication des messages produits au sujet des entreprises, mieux comprendre les retours clients est devenu un enjeu primordial. Des techniques de classification automatique et de modélisation thématique permettent d\u0027ors déjà d\u0027observer les principales ten-dances observées dans ces données. Il est intéressant, dans une optique d\u0027antici-pation, d\u0027observer les thématiques émergentes et de les identifier avant qu\u0027elles ne prennent de l\u0027ampleur. Afin de résoudre cette problématique, nous avons étu-dié la piste de l\u0027utilisation de modèles LDA pour détecter les documents relatifs à ces thématiques émergentes. Nous avons testé trois systèmes sur plusieurs scé-narios d\u0027arrivées de la nouveauté dans le flux de données. Nous montrons que les modèles thématiques permettent de détecter cette nouveauté mais que cela dépend du scénario envisagé."
  },
  {
    "id": "71",
    "text": "Les maladies cardiovasculaires (MCV) sont la première cause de mortalité dans le monde, on estime à 17,7 millions le nombre de décès imputables aux MCV, soit 40% de la mortalité mondiale totale (WHO, 2017). Si les principaux facteurs de risques cardiovasculaires sont au-jourd\u0027hui bien connus, leur évaluation à tendance à être faite sans considérer les interactions qui les lient. Evaluer ces facteurs séparément conduit souvent à des erreurs ou des interventions contradictoires dans le suivi du patient expliquant en partie les échecs répétés de certains stratégies de prévention. Dans ce travail, nous proposons une visualisation dynamique des interactions entre les fac-teurs de risques afin d\u0027aider à la compréhension du déclenchement des effets en cascade pro-duits par l\u0027intervention sur un facteur (par exemple, agir sur le facteur fumeur pour un patient dépressif peut provoquer une dégradation de son état). Cette approche a pour objectif d\u0027utiliser la visualisation pour faciliter la tâche de définition de stratégie de prévention. La plupart du temps, ces interactions sont représentées par des modèles statistiques prenant la forme de ta-bleaux et résumés en utilisant des graphes. L\u0027objectif que nous poursuivons est la visualisation dynamique de cette représentation sous forme de graphe permettant un parcours des différents facteurs guidé par les interactions existant entre ces mêmes facteurs. Le parcours du graphe permet un accès aux connaissances relatives aux interactions entre facteurs, une réorganisation des connaissances en fonction des cas à traiter, des retours arrières sur les décisions prises ayant conduit à une impasse pour la stratégie de prévention. Les traces de la navigation dans le graphe révèlent des processus de raisonnement la plupart du temps implicites. La démarche adoptée a consisté à effectuer une analyse détaillée du modèle statistique re-présentant les interactions entre facteurs de risque (Meneton et al., 2016). A l\u0027issue de cette analyse nous avons construit un modèle conceptuel représenté sous la forme d\u0027un graphe RDF 1. Les facteurs de risque sont représentés par les noeuds du graphe et les arcs traduisent deux types de relations orientées entre ces noeuds (Fi prédit Fj) et (Fi prédit par Fk) comme par exemple (fumeur, prédit, inactivité physique) ou (fumeur, prédit par, dépression). Différents travaux en psychologie cognitive (Fortin et Rousseau, 1989) ont mis en évi-dence la capacité du cerveau à analyser rapidement des composantes graphiques et à pouvoir raisonner sur des représentations visuelles. La mise en évidence des interactions entre facteurs 1. https ://www.w3.org/TR/REC-rdf-syntax/-373"
  },
  {
    "id": "146",
    "text": "Dans ce travail 1 nous avons réalisé une enquête sur l\u0027usage des médias sociaux pour dé-terminer les sujets sensibles et détecter des vulnérabilités de vie privée. Nous avons collecté 232 réponses complètes et valides d\u0027utilisateurs de médias sociaux. La corrélation par rapport à la variable \"âge\" entre notre échantillon et la population des internautes français 2 est 0,8 et s\u0027élève à 0,95 pour les internautes de plus de 18 ans. Nous avons analysé le compor-tement des internautes sur les médias sociaux suivant quatre critères et défini les sujets sen-sibles comme étant ceux qui appartiennent à au moins deux ensembles parmi les suivants : L\u0027ensemble E discussion des sujets dont la fréquence de discussion globale est inférieure à la fréquence moyenne moins l\u0027écart type. Dans notre étude E discussion est {\"Argent\", \"Achats\", \"Religion\", \"Rencontre\"}. L\u0027ensemble E activite des forums et sites internet dont le taux d\u0027ac-tivité globale est inférieur au taux moyen moins l\u0027écart type est {\"Sortie, Rencontre, Chat\", \" Philosophie, Religion, Libre pensée\"}. L\u0027ensemble E anonyme des sites et forums dont le taux de publication anonyme (sans identification ou avec des profils anonymes) dépasse la moyenne de 8.7 % est {\""
  },
  {
    "id": "147",
    "text": "En analyse exploratoire, l\u0027identification et la visualisation des interactions entre variables dans les grandes bases de données est un défi (Dhillon et al., 2003; Kolda et Sun, 2008). Nous présentons Khiops CoViz, un outil qui permet d\u0027explorer par visualisation les relations importantes entre deux (ou plusieurs) variables, qu\u0027elles soient catégorielles et/ou numériques. La visualisation d\u0027un résultat de coclustering de variables prend la forme d\u0027une grille (ou matrice) dont les dimensions sont partitionnées: les variables catégorielles sont partitionnées en clusters et les variables numériques en intervalles. L\u0027outil permet plusieurs va-riantes de visualisations à différentes échelles de la grille au moyen de plusieurs critères d\u0027intérêt révélant diverses facettes des relations entre les variables."
  },
  {
    "id": "148",
    "text": "Dans cet article, nous présentons une méthodologie originale permet-tant de faire des analyses scientométriques basées sur trois dimensions (spatiale, temporelle et thématique) à partir d\u0027un corpus de publications. Cette méthodo-logie comporte 3 étapes : (1) la préparation et la validation des données pour compléter les critères usuels tels que les noms d\u0027auteurs, affiliation, ... par des critères spatiaux, temporels et thématiques ; (2) l\u0027indexation des contenus des publications et métadonnées associées ; (3) l\u0027analyse et/ou la recherche d\u0027infor-mation multidimentionnelle. Les expérimentations sont menées sur la série de publications des conférences EGC de 2004 à 2015."
  },
  {
    "id": "149",
    "text": "Les articles scientifiques publiés dans les actes des conférences EGC, qui se déroulent chaque année depuis 2001, constituent la richesse de ces évè-nements mettant en avant le fer de lance de la recherche francophone portant sur la gestion et l\u0027extraction de connaissances. Nous nous sommes penchés sur l\u0027analyse de ces publications scientifiques afin d\u0027en extraire l\u0027essence en termes de thématiques de recherches abordées. Premièrement, nous avons analysé les points communs et les spécificités des publications dans les différentes éditions de la conférence ainsi que les principales différences entre les éditions consécu-tives. Puis nous nous sommes intéressés à la façon dont les publications s\u0027arti-culent autour des thématiques extraites et sur lesquelles nous avons essayé de visualiser une approximation sémantique. Enfin nous nous sommes intéressé à l\u0027évolution des thématiques depuis les débuts de cette conférence et jusqu\u0027à l\u0027édition 2015."
  },
  {
    "id": "150",
    "text": "1 Données et méthode d\u0027apprentissage Le signal à prédire est le prix spot de l\u0027électricité en e/MWh. Les données se présentent sous la forme d\u0027une série temporelle échantillonnée au pas de temps horaire mesurée sur une année complète, soit un total d\u0027environ 100 000 points relevés. Afin de normaliser les données, le prix d\u0027une heure de consommation est exprimé par rapport au prix moyen de l\u0027année étudiée. FIG. 1-Échantillon de l\u0027ensemble d\u0027apprentissage. Une moyenne pondérée des prédictions de chacun des arbres de régression appris par une méthode de Random Forests est employée pour déterminer la variable d\u0027intérêt y. 2 Résultats expérimentaux La performance des modèles appris est mesurée grâce à la variance expliquée, similaire à un écart quadratique moyen normalisé : V ar.exp. \u003d 1 − V ariance(y observéeobserv´ observée −y modèle) V ariance(y observéeobserv´observée). Une variance expliquée de ≈ 0, 6 indique un bon degré de prédictivité. Ens. app Ens. test Nb arbres Nb attr. Min. ex./"
  },
  {
    "id": "151",
    "text": "Dans plusieurs domaines les données sont générées d\u0027une façon continue et souvent à une fréquence très rapide. Ce type de données est connu sous le nom de flux de données. Les flux de données sont caractérisés principalement par l\u0027aspect temporel et par leur grande taille, ce qui rend le processus de clustering des éléments du flux une tâche laborieuse. Traiter les éléments d\u0027une manière séparée, au fur et à mesure de leur apparition, conduit souvent à des erreurs dans leur affectation aux nouveaux clusters. La principale idée de notre approche consiste à traiter un groupe de nouveaux éléments arrivant presque simultanément au lieu de traiter chaque élément séparément. Cela permet de prendre en compte les caracté-ristiques d\u0027un groupe de données arrivant dans la même période temporelle. Nous supposons que deux éléments générés successivement sont probablement causés par les mêmes facteurs, ce qui implique qu\u0027il y a de fortes chances qu\u0027ils se ressemblent. Le but de notre approche est de construire incrémentalement un graphe de voisinage permettant de traiter et de visualiser le flux de données. En premier lieu, nous attendons l\u0027arrivée du premier groupe d\u0027éléments (les groupes ont une taille fixe définie par l\u0027utilisateur). Nous appliquons un clustering basé sur le voisinage sur les éléments du premier groupe : nous calculons la distance entre chaque couple d\u0027éléments et nous considérons que deux éléments sont voisins si leur distance est inférieure à un seuil (qui est fixé également par l\u0027utilisateur). Nous considérons que chaque ensemble de voisins constitue un cluster. Nous déterminons ensuite le centroid de chaque cluster (l\u0027élément le plus proche du reste des éléments du cluster). Les clusters obtenus sont représentés dans un graphe de voisinage : pour chaque cluster, chaque élément est représenté par un noeud, les arêtes représentent la distance entre chaque élément et le centroid de son cluster. Les éléments du groupe suivant sont traités, indépendamment dans un premier temps, avec le même processus que les éléments du premier groupe. De la même manière nous obtenons de nouveaux clusters et nous identifions également leurs centroids. Les nouveaux clusters sont utilisés pour mettre à jour le graphe de voisinage : nous calculons la distance entre chaque centroid des nouveaux clusters et les centroids des anciens clusters, si la distance entre deux centroids de clusters est inférieure au seuil, les deux clusters sont reliés. Cela se traduit par la création d\u0027une arête entre les noeuds représentant les deux centroids. Dans le cas ou un nouveau cluster n\u0027est similaire à aucun des anciens clusters, il est rajouté au graphe sans qu\u0027il ne soit relié avec un autre cluster (ce qui représente l\u0027apparition d\u0027un nouveau cluster dans le-533"
  },
  {
    "id": "153",
    "text": "Prendre une décision impliquant plusieurs acteurs aux objectifs diver-gents nécessite de considérer des informations tant qualitatives-les préférences des acteurs sur les décisions possibles-que quantitatives-les paramètres servant d\u0027indicateurs pour les acteurs. Dans cet article nous nous intéressons à l\u0027as-sociation de ces deux types d\u0027approches. Le modèle qualitatif considéré est l\u0027ar-gumentation. Le modèle quantitatif simulant les scénarios découlant de chaque décision est la dynamique des systèmes. Cet article s\u0027intéresse aux éléments per-mettant de connecter les deux formalismes. Un exemple en agroalimentaire vient en appui à cette réflexion."
  },
  {
    "id": "155",
    "text": "Cet article présente une approche pour la catégorisation et la désam-biguïsation des intérêts que les individus renseignent sur les réseaux sociaux en utilisant Wikipédia."
  },
  {
    "id": "156",
    "text": "Ces dernières années de nombreuses méthodes semi-supervisées de clustering ont intégré des contraintes entre paires d\u0027objets ou d\u0027étiquettes de classe, afin que le partitionnement final soit en accord avec les besoins de l\u0027uti-lisateur. Pourtant dans certains cas où les dimensions d\u0027études sont clairement définies, il semble opportun de pouvoir directement exprimer des contraintes sur les attributs pour explorer des données. De plus, une telle formulation per-mettrait d\u0027éviter les écueils classiques de la malédiction de la dimensionnalité et de l\u0027interprétation des clusters. Cet article propose de prendre en compte les préférences de l\u0027utilisateur sur les attributs afin de guider l\u0027apprentissage de la distance pendant le clustering. Plus précisément, nous montrons comment pa-ramétrer la distance euclidienne par une matrice diagonale dont les coefficients doivent être au plus proche des poids fixés par l\u0027utilisateur. Cette approche per-met d\u0027ajuster le clustering pour obtenir un compromis entre les approches gui-dées par les données et par l\u0027utilisateur. Nous observons que l\u0027ajout des préfé-rences est parfois essentiel pour atteindre un clustering de meilleure qualité."
  },
  {
    "id": "157",
    "text": "Nous proposons dans cet article une approche de clustering visuel semi-interactif. L\u0027approche proposée utilise la perception visuelle pour guider l\u0027utilisateur dans le processus interactif. Les clusters sont extraits de manière successive et itérative, puis évalués selon leur ordre d\u0027extraction. Pour l\u0027utilisa-teur, l\u0027approche semi-interactive permet non seulement d\u0027évaluer les classes en fonction d\u0027un critère déterminé mais aussi d\u0027évaluer l\u0027influence de l\u0027extraction d\u0027un cluster sur ceux précédemment extraits. Un protocole de test est présenté afin de comparer cette approche avec les approches purement automatiques et purement interactives. Cet article est un résumé d\u0027un papier accepté 1 pour un journal international."
  },
  {
    "id": "159",
    "text": "Le suicide devient d\u0027année en année une problématique plus préoc-cupante. Les organismes de santé tels que l\u0027OMS se sont engagés à réduire le nombre de suicides de 10% dans l\u0027ensemble des pays membres d\u0027ici 2020. Si le suicide est généralement un geste impulsif, il existe souvent des actes et des paroles qui peuvent révéler un mal être et représenter des signes précurseurs de prédispositions au suicide. L\u0027objectif de cette étude est de mettre en place un système pour détecter semi-automatiquement ces comportements et ces paroles au travers des réseaux sociaux. Des travaux précédents ont proposé la classification de messages issus de Twitter suivant des thèmes liés au suicide : tristesse, blessures psychologiques, état mental, etc. Dans cette étude, nous ajoutons la dimension temporelle pour prendre en compte l\u0027évolution de l\u0027état des personnes monitorées. Nous avons implémenté pour cela différentes méthodes d\u0027apprentis-sage dont une méthode originale de concept drift. Nous avons expérimenté avec succès cette méthode sur des données réelles issues du réseau social Facebook."
  },
  {
    "id": "160",
    "text": "1 Introduction"
  },
  {
    "id": "162",
    "text": "1 Introduction"
  },
  {
    "id": "163",
    "text": "De nombreuses industries manufacturières s\u0027intéressent aujourd\u0027hui à l\u0027exploitation des grandes collections de traces unitaires. Les applications sont multiples et vont du simple \"reporting\" à la détection de fraudes en passant par la gestion de retours ou encore la mise en évidence d\u0027incohérences dans les circuits de distribution. Une étape importante consiste à détecter des anomalies dans des collections de traces. Si les travaux concernant la détection d\u0027anomalies sont assez nombreux, peu permettent de caractériser les anomalies détectées par une description intelligible. Étant donné un ensemble de traces unitaires, nous développons une méthode d\u0027extraction de motifs pour détecter et contextualiser des comportements non conformes à un modèle expert (fourni ou construit à partir des données). Le degré d\u0027anomalie est alors quantifié grâce à la proportion du nombre de mouvements des objets qui ne sont pas prévus dans le modèle expert. Cette recherche est financée partiellement par un programme industriel qui ne permet ni de dévoiler le contexte concret ni de parler des données réelles. Ainsi, nous validons empiriquement la valeur ajoutée de la méthode proposée par l\u0027étude de traces de mobilité dans un jeu vidéo : nous pouvons alors discuter d\u0027un motif qui explicite les raisons de l\u0027inexpérience de certains joueurs."
  },
  {
    "id": "164",
    "text": "1 Introduction"
  },
  {
    "id": "165",
    "text": "Dans ce travail, nous analysons les données concernant les articles publiés à la conférence EGC. Notre objectif est d\u0027identifier et de comprendre les tendances en matière de collaborations. Pour ce faire, nous adoptons une modélisation descriptive, à travers une approche réseau qui consiste à générer tout d\u0027abord le réseau de collaborations des auteurs à partir des données. Nous enrichissons ensuite les noeuds de ce réseau d\u0027une dizaine d\u0027attributs individuels extraits à partir des données. Enfin, nous recherchons des vues conceptuelles, une approche récente de clustering de liens, qui permet de synthétiser des réseaux en mettant en évidence les ensembles d\u0027attributs retrouvés fréquemment liés dans le réseau. Les résultats obtenus montrent les tendances existantes dans les comportements de collaborations. Dans ce papier, nous présentons ces tendances et montrons comment elles évoluent selon différents seuils d\u0027extraction."
  },
  {
    "id": "166",
    "text": "La détection de données aberrantes (outliers) consiste à détecter des observations anormales au sein des données. Durant la dernière décennie, des méthodes de détection d\u0027outliers utilisant les motifs fréquents ont été proposées. Elles extraient dans une première phase tous les motifs fréquents, puis assignent à chaque transaction un score mesurant son degré d\u0027aberration (en fonction du nombre de motifs fréquents qui la couvre). Dans cet article, nous proposons deux nouvelles méthodes pour calculer le score d\u0027aberration fondé sur les motifs fré-quents (FPOF). La première méthode retourne le FPOF exact de chaque transaction sans extraire le moindre motif. Cette méthode s\u0027avère en temps polynomial par rapport à la taille du jeu de données. La seconde méthode est une méthode approchée où l\u0027utilisateur final peut contrôler l\u0027erreur maximale sur l\u0027estimation du FPOF. Une étude expérimentale montre l\u0027intérêt des deux méthodes pour les jeux de données volumineux où une approche exhaustive échoue à calculer une solution exacte. Pour un même nombre de motifs, la précision de notre méthode approchée est meilleure que celle de la méthode classique."
  },
  {
    "id": "168",
    "text": "Les hiérarchies sont des structures cruciales dans un entrepôt de don-nées puisqu\u0027elles permettent l\u0027agrégation de mesures dans le but de proposer une vue analytique plus ou moins globale sur les données entreposées, selon le niveau hiérarchique auquel on se place. Cependant, peu de travaux s\u0027intéressent à la construction de hiérarchies, via un algorithme de fouille de données, pre-nant en compte le contexte multidimensionnel de la dimension concernée. Dans cet article, nous proposons donc un algorithme, implémenté sur une architecture ROLAP, permettant d\u0027enrichir une dimension avec des données factuelles."
  },
  {
    "id": "169",
    "text": "De nos jours, il y a un fort intérêt pour de nouvelles méthodes d\u0027éva-luation des groupes de recherche afin de quantifier l\u0027impact de leur travail sur toute la communauté scientifique et de tenter de prédire leurs performances dans le futur. Dans ce contexte, nous proposons une nouvelle approche hybride qui mesure la centralité d\u0027un groupe de chercheurs publiants. Cette mesure profite de l\u0027expressivité et de la capacité d\u0027inférence apportées par une modélisation ontologique des groupes et des thématiques inférées, et d\u0027une modélisation en graphe qui permet d\u0027explorer les interactions entre ces différents groupes au fil du temps. Ce modèle permet également de détecter les groupes capables de collaborer avec d\u0027autres tout en maintenant un haut niveau de production, et d\u0027identifier ceux qui sont plus déterminants sur les thématiques déduites, afin de développer des collaborations de recherche plus fructueuses."
  },
  {
    "id": "170",
    "text": "Nous présentons dans cet article les méthodes employées et les résul-tats obtenus en réponse au Défi EGC 2016. Notre approche repose d\u0027une part sur des chaînes automatiques de traitements linguistiques en français et en an-glais utilisant le plus possible des ressources et outils publics et d\u0027autre part sur un environnement d\u0027exploration des données basé sur les systèmes d\u0027informa-tion logiques ; ces systèmes exploitent une généralisation des treillis de concepts formels appliquée aux données attribut-valeur ou au web sémantique."
  },
  {
    "id": "171",
    "text": "Les technologies du web sémantique sont de plus en plus utilisées pour la gestion de flux de données. Plusieurs systèmes de traitement de flux RDF ont été proposés : C-SPARQL, CQELS, SPARQL stream , EP-SPARQL, SPARKWAVE, etc. Ces derniers étendent tous à la base, le langage d\u0027interroga-tion sémantique SPARQL. Les données à l\u0027entrée du système sont volumineuses et générées en continu à un rythme rapide et variable. De ce fait, le stockage et le traitement de la totalité du flux deviennent coûteux et le raisonnement presque impossible. Par conséquent, le recours à des techniques permettant de réduire la charge tout en conservant la sémantique des données, permet d\u0027optimiser les trai-tements voire le raisonnement. Cependant, aucune des extensions de SPARQL n\u0027inclut cette fonctionnalité. Ainsi, dans cet article, nous proposons d\u0027étendre le système C-SPARQL pour générer des échantillons à la volée sur flux de graphes RDF. Nous ajoutons trois opérateurs d\u0027échantillonnage (UNIFORM, RESERVOIR et CHAIN) à la syntaxe de C-SPARQL. Les expérimentations montrent la performance de notre extension en terme de temps d\u0027exécution, et de la préser-vation de la sémantique des données."
  },
  {
    "id": "172",
    "text": "Nous détaillerons ici une approche permettant de détecter des affixes à partir de dictionnaires en se basant sur l\u0027algorithme de la plus longue sous-chaîne commune, dans le cadre de la reconnaissance d\u0027entités nommées chimiques sur CHEMDNER. Nous verrons ensuite des méthodes de sélection et de tri afin de les intégrer au mieux dans un système d\u0027apprentissage automatique."
  },
  {
    "id": "173",
    "text": "De grandes quantités de données sont publiées sur le web des don-nées. Les lier consiste à identifier les mêmes ressources dans deux jeux de don-nées permettant l\u0027exploitation conjointe des données publiées. Mais l\u0027extraction de liens n\u0027est pas une tâche facile. Nous avons développé une approche qui ex-trait des clés de liage (link keys). Les clés de liage étendent la notion de clé de l\u0027algèbre relationnelle à plusieurs sources de données. Elles sont fondées sur des ensembles de couples de propriétés identifiant les objets lorsqu\u0027ils ont les mêmes valeurs, ou des valeurs communes, pour ces propriétés. On présentera une manière d\u0027extraire automatiquement les clés de liage candidates à partir de données. Cette opération peut être exprimée dans l\u0027analyse formelle de concepts. La qualité des clés candidates peut-être évaluée en fonction de la disponibilité (cas supervisé) ou non (cas non supervisé) d\u0027un échantillon de liens. La perti-nence et de la robustesse de telles clés seront illustrées sur un exemple réel."
  },
  {
    "id": "174",
    "text": "1 Introduction"
  },
  {
    "id": "175",
    "text": "1 Introduction"
  },
  {
    "id": "178",
    "text": "La plateforme FODOMUST 1 est une implantation concrète des mé-thodes, librairies et interfaces proposées au sein d\u0027ICube. Elle intègre une version multisource de la méthode de classification collaborative multistratégie SA-MARAH. Elle propose aussi un ensemble d\u0027algorithmes de segmentation soit propres à ICUBE soit faisant appel à l\u0027OTB. Enfin, trois interfaces dédiées cha-cune à un type de données différent permettent une interaction avec l\u0027utilisateur. Sa principale originalité est qu\u0027elle permet la classification, basée sur DTW (Dy-namic Time Warping) de données temporelles symboliques ou numériques et de séries temporelles d\u0027images"
  },
  {
    "id": "179",
    "text": "1 Introduction"
  },
  {
    "id": "180",
    "text": "Nous nous intéressons, dans le cadre du projet ANR Qualinca au trai-tement des données redondantes. Nous supposons dans cet article que cette re-dondance a déjà été établie par une étape préalable de liage de données. La question abordée est la suivante : comment proposer une représentation unique en fusionnant les \"duplicats\" identifiés ? Plus spécifiquement, comment décider, pour chaque propriété de la donnée considérée, quelle valeur choisir parmi celles figurant dans les \"duplicats\" à fusionner ? Quelle méthode adopter dans le but de pouvoir, par la suite, retracer et expliquer le résultat obtenu de façon trans-parente et compréhensible par l\u0027utilisateur ? Nous nous appuyons pour cela sur une approche de décision multicritère et d\u0027argumentation."
  },
  {
    "id": "181",
    "text": "L\u0027utilisation des connaissances a priori peut fortement améliorer la classification non-supervisée. L\u0027injection de ces connaissances sous forme de contraintes sur les données figure parmi les techniques les plus efficaces de la littérature. Cependant, la génération des contraintes est très coûteuse et demande l\u0027intervention de l\u0027expert ; la sémantique apportée par l\u0027étiquetage de l\u0027expert est aussi perdue dans ce type de techniques, seuls les contraintes sont retenues par le clustering. Dans cet article, nous proposons une nouvelle approche hy-bride exploitant le raisonnement à base d\u0027ontologie pour générer automatique-ment des contraintes permettant de guider et améliorer le clustering. L\u0027utilisation d\u0027une ontologie comme connaissance a priori a plusieurs avantages. Elle permet l\u0027interprétation automatisée des connaissances, ajoute de la modularité dans la chaîne de traitement et améliore la qualité du clustering en prenant en compte la vision de l\u0027utilisateur. Pour évaluer notre approche, nous l\u0027avons appliquée à la classification d\u0027images satellites et les résultats obtenus démontrent des amé-liorations notables à la fois au niveau de la qualité du clustering et au niveau de l\u0027étiquetage sémantique des clusters sans intervention de l\u0027expert."
  },
  {
    "id": "183",
    "text": "Contexte. Pendant la dernière décennie, Internet a connu une plus vaste portée grâce à l\u0027émergence du Web social 2.0. Ceci a conduit au développement de nouveaux médias tels que les réseaux sociaux et à des travaux associés sur l\u0027analyse des sentiments. L\u0027objectif de cet article est de proposer une méthode de détection automatique de sentiments à partir du corpus 88milSMS (http://88milsms.huma-num.fr) en prenant en considération les spécificités de l\u0027écriture SMS (Panckhurst et al., 2013). Dans ce cadre, nous nous intéressons à l\u0027intégration de connaissances lexicales et sémantiques pour l\u0027analyse de sentiments dans les SMS. Méthode. Dans un premier temps, nous avons identifié les SMS possédant des mots avec al-longements à partir de trois caractères. Puis, de tels mots sont cherchés afin de construire un dictionnaire des mots et des mots allongés associés (exemple, merci / merciiii, merciiiii, mer-ciiiiiii). Dans un deuxième temps, nous avons constitué un échantillon représentatif de 304 SMS possédant des mots allongés et 182 SMS sans allongement. Ce corpus a été annoté ma-nuellement suivant l\u0027opinion véhiculée : \"Très positif\", \"Positif \", \"Négatif\", \"Très négatif\", \"Neutre\", \"Je ne sais pas\". Nous présentons ensuite une méthode fondée sur l\u0027apprentissage supervisé qui s\u0027appuie sur la représentation vectorielle des SMS sous forme de \"sacs de mots\" (Salton et al., 1975). Une représentation booléenne peut alors être effectuée sur la base des vecteurs relatifs à chaque SMS. Enfin, nous avons utilisé un lexique de sentiments et d\u0027émo-tions FEEL (Abdaoui et al., 2014) pour pondérer certains mots porteurs de sentiments. Nous avons considéré que si un mot est présent dans ce dictionnaire, l\u0027attribut correspondant est ins-tancié à 2 dans la représentation vectorielle. Par exemple, si le mot \"besoin\" est présent dans le dictionnaire d\u0027opinion, l\u0027attribut est alors instancié à 2 dans les SMS. Si un attribut est présent dans un SMS, mais absent du dictionnaire, la valeur est instanciée à 1. En absence du mot dans le SMS, la valeur 0 est introduite. Et si un mot allongé est présent dans le dictionnaire d\u0027opinion sous sa forme désallongée, l\u0027attribut correspondant est instancié à 4 dans les SMS (vecteurs) comportant ce mot. Par exemple, si le mot allongé \"besoinnnnn\" est présent dans le dictionnaire d\u0027opinion sous sa forme désallongée comme \"besoin\" l\u0027attribut est instancié à 4. Résultats. Nous avons procédé à une série d\u0027expérimentations sur les différents jeux de don-nées présents dans le Tableau 1 : (1) les corpus \"SMS allongés\" et (2) \"SMS non allongés\", (3) le corpus \"SMS allongés Dico\" issu de l\u0027intégration du dictionnaire d\u0027opinion et des SMS allongés, (4) le corpus \"SMS non allongés Dico\" issu de l\u0027intégration du dictionnaire d\u0027opinion-553"
  },
  {
    "id": "184",
    "text": "La recommandation de points d\u0027intérêts (ou POI), est devenue un pro-blème majeur avec l\u0027émergence des réseaux sociaux (ou LBSN). À la différence des approches de recommandation traditionnelles, les données des LBSN pré-sentent des caractéristiques géographique et temporelle importantes qui limitent les performances des algorithmes traditionnels existant. L\u0027intégration de ces ca-ractéristiques dans un unique modèle de factorisation pour augmenter la qualité de la recommandation n\u0027a pas été un problème très étudié jusqu\u0027à présent. Dans ce papier nous présentons GeoMF-TD, une extension d\u0027un modèle de factori-sation géographique avec des dépendances temporelles. Nos expérimentations sur un jeu de données réel montre jusqu\u0027à 20% de gain sur la précision de la recommandation."
  },
  {
    "id": "185",
    "text": "1 Introduction"
  },
  {
    "id": "186",
    "text": "Cet article présente une méthode d\u0027exploration de données tempo-relles, fondée sur l\u0027analyse relationnelle de concepts (ARC) et appliquée à des données séquentielles construites à partir d\u0027échantillons physico-chimiques et biologiques prélevés dans des cours d\u0027eau. Notre but est de mettre au jour des sous-séquences pertinentes et hiérarchisées, associant les deux types de para-mètres. Pour faciliter la lecture, ces sous-séquences sont représentées sous la forme de motifs partiellement ordonnés (po-motifs). Le processus de fouille de données se décompose en plusieurs étapes : construction d\u0027un modèle temporel ad hoc et mise en oeuvre de l\u0027ARC ; extraction des sous-séquences synthétisées sous la forme de po-motifs ; sélection des po-motifs intéressants grâce à une mesure exploitant la distribution des extensions de concepts. Le processus a été testé sur un jeu de données réelles et évalué quantitativement et qualitativement."
  },
  {
    "id": "187",
    "text": "La géovisualisation est considérée comme un domaine de recherche en pleine expansion, elle utilise des outils intelligents qui peuvent aider à trouver \"une aiguille dans une botte de foin\", en s\u0027appuyant sur des techniques pour filtrer les données pertinentes (Kraak, 2003). En outre, en ce qui concerne les données en temps réel provenant de capteurs, la complexité aug-mente davantage. Plusieurs approches ont été proposées selon lesquelles l\u0027objectif principal est de ne pas renvoyer exactement une carte avec un haut niveau de détails, mais plutôt montrer les aspects les plus pertinents des phénomènes. Ces méthodes de visualisation sont connues sous le vocable de \"Résumés Visuels\" (De Chiara et al., 2011) d\u0027un ensemble de données qui aideront les utilisateurs à trouver ce qui est le plus important ou le plus intéressant à visualiser dans la masse des informations disponibles. La méthode proposée dans le présent article est de plus en plus connue dans le monde des géographes (Brunet, 1986) et plus récemment dans le monde des Systèmes d\u0027Information Géo-graphique sous le nom de \"Chorèmes\". Ceux-ci sont caractérisés comme une sorte de schéma puissant que Roger Brunet a mis au point pour parvenir à transmettre un message essentiel d\u0027une argumentation complexe avec des symboles et des icones significatives. En s\u0027inspirant de cette méthode, notre approche est basée sur l\u0027analyse spatiale par l\u0027interpo-lation des valeurs mesurées qui proviennent de manière régulière d\u0027un ensemble de capteurs météorologiques répartis sur un territoire. Des méthodes géostatistiques à la volée permettent d\u0027extraire des patterns spatio-temporels importants et de détecter parmi ces données des ten-dances au fil du temps sous forme d\u0027un ensemble de règles. Ensuite ces patterns sont visualisés au moyen de chorèmes pour obtenir un meilleur aperçu visuel de ces flux de données à un instant donné. Dans le cas du domaine météorologique (Mudelsee, 2010), une généralisation sémantique est nécessaire afin de sélectionner les aspects météorologiques les plus marquants. Pour cela, nous devons définir clairement les critères de sélection des chorèmes. Selon le cas, certains para-mètres météorologiques ne sont pas importants. Dans la Figure 1, nous montrons le résulat de notre approche. Dans cette carte, un ensemble de chorèmes est représenté, lié à l\u0027Algérie, où certains aspects sont mis en évidence, comme la forme géométrique simplifiée, les villes les plus importantes ; seuls les phénomènes météorologiques pertinents sont représentés par des polygones et des icones. Dans l\u0027animation, de toute évidence les points importants ne peuvent non seulement se déplacer, mais aussi changer de nature.-527"
  },
  {
    "id": "189",
    "text": "La quantité de données dans notre monde a explosé et l\u0027analyse de grands ensembles de données -aussi connu dans l\u0027industrie sous le nom « Big Data » -deviendra un atout majeur de compétitivité, principalement dû à une croissance de productivité et surtout à grâce à plus d\u0027innovation. La croissance exponentielle de données est alimentée par la facilité de la captation et par la multiplication de canaux numériques d\u0027acquisition. On pense non seulement à tous les processus qui sont informatisés aujourd\u0027hui, mais aussi aux médias sociaux et aux objets connectés. L\u0027assurance vie une révolution tout particulière. L\u0027assureur, traditionnellement ges- tionnaire du risque en s\u0027appuyant sur une longue expérience, qu\u0027on traduirait aujour- d\u0027hui par une captation systématique de données, est après la révolution numérique partiellement exclus de canaux digitaux. Ceci est en même temps une menace et une opportunité. Il s\u0027agit d\u0027un défi puisque l\u0027industrie doit réaliser une forte mutation pour se positionner la où la donnée se trouve aujourd\u0027hui, i.e. dans le digital. Il s\u0027agit d\u0027une opportunité puisque ces nou- velles données permettront de mieux appréhender les risques, et plus particulièrement, permettront d\u0027estimer au plus près les risques à la source, plutôt que passer par de variables intermédiaires, comme peut l\u0027être l\u0027âge pour le risque d\u0027accident en conduite. L\u0027opportunité est d\u0027autant plus grande qu\u0027en accédant aux données au plus près des utilisateurs il est possible de faire de la prévention évitant ainsi des accidents coûteux pour l\u0027assureur, mais surtout désastreux pour les victimes. Une fois la révolution engagée, ceci implique, un certain nombre de transformations dans les processus d\u0027extraction et gestion de connaissances. Les défis scientifiques sont nombreux, allant de la captation non-intrusive de la donnée, à la visualisation et gestion de connaissances extraites, en passant par de l\u0027apprentissage artificiel pour pouvoir servir à de millions d\u0027utilisateurs simultanément. Dans cette présentation nous allons couvrir rapidement chacune de ces thématiques avec une attention particulière aux défis scientifiques sous-jacents. Nous allons illustrer notre propos par un exemple phare de cette révolution : la famille d\u0027offres d\u0027assurance dite « pay as you drive » où généralement on obtient une décote ou réduction en fonction de sa façon de conduire. Nous allons ce que ceci implique en termes d\u0027extraction et de gestion de connaissances. Pour conclure, il est important de mentionner que cette révolution implique d\u0027autres challenges cruciaux qui dépassent ce qui est abordé ici. En particulier, pour ne men- tionner que deux grands axes : la protection de la vie privée, aussi bien du point de vue technique que juridique ; et la transformation de métiers accompagné d\u0027une pénurie de talents déjà entamé."
  },
  {
    "id": "191",
    "text": "Nous présentons dans ce papier un protocole de gestion de la cohé-rence appelé LibRe adapté aux systèmes de stockage orientés Cloud (telles que les bases de données NoSQL). Ce protocole garantit l\u0027accès à la donnée la plus récente tout en ne consultant qu\u0027une seule réplique. Cet algorithme est évalué par simulation et est également implémenté au sein du système de stockage Cas-sandra. Les résultats de ces expérimentations ont démontré l\u0027efficacité de notre approche."
  },
  {
    "id": "192",
    "text": "Dans cette démonstration, nous proposons une application de visuali-sation des résultats de la fouille de données séquentielles. Pour illustrer le fonc-tionnement de cette application, nous avons utilisé des données PMSI hospita-lières, plus précisément dans le cas de l\u0027infarctus du myocarde (IM). Les résul-tats obtenus ont été soumis à un spécialiste pour discussion et validation."
  },
  {
    "id": "193",
    "text": "De nos jours, il est intéressant de développer de nouveaux outils d\u0027intégration et de manipulation de données (ETL) afin d\u0027aider à mieux comprendre la sémantique et la structure des données manipulées Boufarès et al. (2013), Ben Salem (2015). Nous réalisons ce travail en collaboration avec la société Talend (éditeur d\u0027un ETL). La première partie du projet a traité des anomalies inter-lignes une fois la sémantique de la colonne est connue et ses anomalies corrigées. La deuxième phase du projet consiste à découvrir d\u0027éventuels liens sémantiques inter-colonnes afin de corriger d\u0027autres types d\u0027anomalies. La vérification des contraintes de dépendances permettra de corriger les anomalies telles que les valeurs nulles et certaines dé-pendances fonctionnelles. La reconnaissance sémantique des données est présentée dans le premier paragraphe. La section deux aborde l\u0027étape de nettoyage de données intra et inter-colonnes. (1) La Catégorisation sémantique des données consiste à déterminer le sens de chaque colonne d\u0027une source de données S. En effet, pour pouvoir qualifier une donnée syntaxique-ment incorrecte, il faudrait l\u0027évaluer dans son contexte. Plusieurs exemples peuvent illustrer nos propos : (i) La chaîne de caractères \"Pari\" ne peut être considérée incorrecte syntaxique-ment que s\u0027il s\u0027agit du nom en français de la ville \"Paris\" ; (ii) Les mots \"Pékin\" et \"Beijing\" désignent la même chose dans deux langues différentes, s\u0027il l\u0027on sait qu\u0027il s\u0027agit de noms de villes. \"Beijing\" pourrait être considérée sémantiquement incorrecte si la langue dominante est le français ; (iii) Les deux chaînes de caractères \"\" représentent la même information de type date définie par une expression régulière. Le format n\u0027est pas le même. Pour ce faire nous utilisons des connaissances stockées dans un réferentiel appelé dictionnaire de données (DD), Zaidi et al. (2015), identifiées (i) par extension, c\u0027est une liste donnée à priori tels que des noms de villes ou des mots clés ; (ii) par intention qui sont des connaissances qui vérifient des propriétés telles que des expressions régulières (un Email ou une date). Chaque catégorie correspond à un seul type de données (String, Nombre ou Date). La reconnaissance de la structure sémantique de données (le processus de catégorisation) renvoie un nom sémantique (une catégorie) à chaque colonne, une sous-catégorie (la langue), un type de données (domaine syntaxique), des contraintes (intra et inter-colonnes) et des com-mentaires. La reconnaissance sémantique consiste à trouver des similarités entre les données de S et celles de DD afin d\u0027inférer la catégorie de chaque colonne en utilisant des mesures de distance de similarité avec les méthodes \"s\u0027écrit comme\" et \"se prononce comme\" telles que Jaro-Winkler et Soundex. La reconnaissance de dépendances sémantiques inter-colonnes-549"
  },
  {
    "id": "194",
    "text": "Clowdflows est un logiciel open source qui permet à un utilisateur de réaliser des processus entiers de fouille de données à partir d\u0027un navigateur et d\u0027une connexion internet. Les calculs sont réalisés dans le \"nuage\", c\u0027est-à-dire de façon transparente sur plusieurs serveurs exécutant les calculs ou hébergeant les données. Dans cet article, nous rappelons les points forts de clowdflows et nous présentons trois familles d\u0027algorithmes de fouille de données relationnelles que nous venons d\u0027y intégrer. En effet clowdflows est la seule plateforme web permettant d\u0027exécuter, voire comparer, plusieurs techniques de fouille de don-nées relationnelles, souvent appelée programmation logique inductive."
  },
  {
    "id": "195",
    "text": "De plus en plus de forums, tels que Slashdot ou Stack Exchange, pro-posent des systèmes de réputations qui se basent sur le vote collaboratif. Les utilisateurs peuvent ainsi donner un score à chaque message posté selon sa per-tinence ou son utilité. Cependant, ces fonctionnalités de vote sont rarement uti-lisées dans de nombreuses communautés en ligne tels que les forums de santé. Dans ces forums, les utilisateurs préfèrent poster un nouveau message expri-mant de l\u0027accord ou du remerciement vis à vis des messages pertinents plutôt que de cliquer sur un bouton de vote. Dans ce travail, nous proposons d\u0027utiliser ces formes implicites d\u0027expression de la confiance pour estimer la réputation des utilisateurs dans les forums de santé."
  },
  {
    "id": "196",
    "text": "L\u0027algorithme de clustering spectral permet en principe d\u0027extraire des clusters de formes arbitraires à partir de données numériques. Cette propriété a contribué à sa popularité, et même si ses bases théoriques sont établies depuis plus d\u0027une décennie, des variantes en ont été proposées jusqu\u0027à récemment. Son fonctionnement repose sur une transformation vers un espace latent dans lequel des formes de clusters arbitraires sont converties en structures faciles à traiter par un algorithme tel que k-means. Toutefois, les distributions dans cet espace latent n\u0027ont été que peu discutées, beaucoup d\u0027auteurs supposant que les pro-priétés prédites par la théorie sont vérifiées. Cet article propose alternativement une approche qualitative pour vérifier si cette structure idéale est effectivement obtenue en pratique. Le travail consiste également à identifier les paramètres de variabilité commandant à la transformation vers l\u0027espace latent, via un état de l\u0027art synthétique de la théorie sous-jacente au clustering spectral. Les observations tirées de nos expériences permettent d\u0027identifier les combinaisons de paramètres efficaces, et les conditions de cette efficacité."
  },
  {
    "id": "197",
    "text": "Nous proposons un nouveau système appelé PersoRec afin de person-naliser les recommandations (d\u0027amis, de tags ou de ressources) faites aux uti-lisateurs dans les folksonomies. La personnalisation des recommandations est réalisée en prenant en compte le profil des utilisateurs. Cette nouvelle donnée permet de proposer aux utilisateurs des tags ou/et ressources plus adaptées à leurs besoins. En plus du profil des utilisateurs, nous avons recours à leur histo-rique de partage de tags et de ressources dans le but de regrouper les utilisateurs ayant partagé des tags et des ressources en commun tout en ayant des profils équivalents (i.e., des structures appelées concepts quadratiques). Ces deux don-nées prises en compte au moment du processus de recommandation a permis d\u0027améliorer la qualité des recommandations faites aux utilisateurs. PersoRec est donc capable de générer une recommandation personnalisée pour chaque utili-sateur selon le mode de recommandation qu\u0027il désire (recommandation d\u0027amis, de tags ou de ressources) et selon le profil qu\u0027il possède."
  },
  {
    "id": "198",
    "text": "Nous explorons le plongement de la métrique de plus court chemin dans l\u0027hypercube de Hamming, dans l\u0027objectif d\u0027améliorer les performances de similarité sémantique dans Wordnet (Subercaze et al. (2015)). Nous montrons que bien qu\u0027un plongement isométrique est impossible en pratique, nous obte-nons de très bons plongements non isométriques. Nous obtenons une améliora-tion des performances de trois ordres de grandeur pour le calcul de la similarité de Leacock et Chodorow (LCH)."
  },
  {
    "id": "199",
    "text": "La qualité des contenus sur les plateformes collaboratives est très hé-térogène. Dans la littérature scientifique, les algorithmes d\u0027analyse structurelle appliqués à la tâche de détection de contenu de qualité reposent généralement sur des graphes définis à partir d\u0027un seul type de noeuds et de relations. Pourtant les graphes sur lesquels reposent ces récentes plateformes présentent de nombreuses sémantiques de noeuds et relations différentes, e.g., producteurs/consommateurs, questions/réponses, etc. Ces solutions souffrent d\u0027un manque de généricité et ne peuvent s\u0027adapter facilement à l\u0027évolution des plateformes. Nous proposons une modélisation générique de ces platformes par les graphes hétérogènes pouvant intégrer automatiquement de nouvelles sémantiques de noeuds et de relations. Un algorithme de prédiction de qualité des contenus reposant sur ce modèle est pro-posé. Nous montrons qu\u0027il généralise plusieurs travaux de la littérature. Enfin, en intégrant certaines relations inter-utilisateurs, nous montrons que notre solution , évaluée sur Wikipedia et Stack Exchange, améliore la tâche de détection de contenu de qualité."
  },
  {
    "id": "200",
    "text": "Dans cet article, nous nous intéressons à une situation de classification non supervisée dans laquelle nous souhaitons imposer une \"forme\" commune à tous les clusters. Dans cette approche, la \"forme\" commune sera caractérisée par un hyperplan qui sera le même pour tous les groupes, à une translation près. Les points sont donc supposés être distribués autour d\u0027hyperplans parallèles. La fonction objectif utilisée peut naturellement s\u0027exprimer comme la minimisation de la somme des distances de chaque point à son hyperplan. Comme pour le cas de k-means, la résolution est effectuée par l\u0027alternance de phases d\u0027affectation de chaque point à l\u0027hyperplan le plus proche et de phases de calcul de l\u0027hyper-plan qui ajuste au mieux l\u0027ensemble des points qui lui sont affectés. L\u0027objectif étant d\u0027obtenir des hyperplans parallèles, cette phase de calcul est menée simul-tanément pour tous les hyperplans, par une méthode de régression."
  },
  {
    "id": "201",
    "text": "Nous présentons un nouvel algorithme parallèle de régression logis-tique (PAR-MC-LR) pour la classification d\u0027images à grande échelle. Nous pro-posons plusieurs extensions de l\u0027algorithme original de régression logistique à deux classes pour en développer une version efficace pour les grands ensembles de données d\u0027images avec plusieurs centaines de classes. Nous présentons un nouvel algorithme LR-BBatch-SGD de descente de gradient stochastique de ré-gression logistique en batch équilibré avec un apprentissage parallèle (approche un contre le reste) multi-classes sur de multiples coeurs. Les résultats expérimen-taux sur des ensembles de données d\u0027ImageNet montrent que notre algorithme est efficace comparés aux algorithmes de classification linéaires de l\u0027état de l\u0027art."
  },
  {
    "id": "202",
    "text": "Les requêtes skyline constituent un outil puissant pour l\u0027analyse de données multidimensionnelles et la décision multicritère. En pratique, le calcul du skyline peut conduire à deux scénarios : soit (i) un nombre important d\u0027objets sont retournés, soit (ii) un nombre réduit d\u0027objets sont retournés, ce qui peut être insuffisant pour la prise de décisions. Dans cet article, nous abordons le second problème et proposons une approche permettant de le traiter. L\u0027idée consiste à rendre le skyline plus permissive en lui ajoutant les objets, non skyline, les plus préférés. L\u0027approche s\u0027appuie sur une nouvelle relation de dominance floue ap-pelée «Much Preferred». Un algorithme efficace pour calculer le skyline relaxé est proposé. Une série d\u0027expériences sont menées pour démontrer la pertinence de l\u0027approche et la performance de l\u0027algorithme proposé."
  },
  {
    "id": "203",
    "text": "À l\u0027ère du Big Data, les profils d\u0027utilisateurs deviennent de plus en plus diversifiés et les données de plus en plus complexes, rendant souvent très difficile l\u0027exploration des données. Dans cet article, nous proposons une technique de réécriture de requêtes pour aider les analystes à formuler leurs interrogations , pour explorer rapidement et intuitivement les données. Nous intro-duisons les requêtes discriminantes, une restriction syntaxique de SQL, avec une condition de sélection qui dissocie des exemples positifs et négatifs. Nous construisons un ensemble de données d\u0027apprentissage dont les exemples positifs correspondent aux résultats souhaités par l\u0027analyste, et les exemples négatifs à ceux qu\u0027il ne veut pas. En utilisant des techniques d\u0027apprentissage automatique, la requête initiale est reformulée en une nouvelle requête, qui amorce un proces-sus itératif d\u0027exploration des données. Nous avons implémenté cette idée dans un prototype (iSQL) et nous avons mené des expérimentations dans le domaine de l\u0027astrophysique."
  },
  {
    "id": "206",
    "text": "La mise en place d\u0027actions marketing efficaces passe par la segmenta-tion de la clientèle. C\u0027est-à-dire que les clients sont regroupés en ensembles ho-mogènes en fonction de leurs habitudes de consommation, ce qui rend possible les actions ciblées. Ces dernières, en personnalisant l\u0027offre permettent d\u0027obtenir des taux de transformation plus importants et de meilleures ventes. Dans cet article, une méthode originale de segmentation comportementale de la clientèle est présentée. Elle permet de visualiser les segments de clients à travers des réseaux de communautés et de déceler aisément des mutations soudaines ou graduelles dans les comportements de quelques individus ou d\u0027un ensemble plus important. L\u0027analyste bénéficie alors d\u0027une meilleure visibilité et peut adapter l\u0027offre à tout moment."
  },
  {
    "id": "207",
    "text": "En apprentissage automatique, la présence d\u0027un grand nombre de variables explicatives conduit à une plus grande complexité des algorithmes et à une forte dégradation des performances des modèles de prédiction. Pour cela, une sélection d\u0027un sous-ensemble optimal discriminant de ces variables s\u0027avère nécessaire. Dans cet article, une approche topologique est proposée pour la sé-lection de ce sous-ensemble optimal. Elle utilise la notion de graphe de voisinage pour classer les variables par ordre de pertinence, ensuite, une méthode pas à pas de type ascendante \"forward\" est appliquée pour construire une suite de modèles dont le meilleur sous-ensemble est choisi selon son degré d\u0027équivalence topolo-gique de discrimination. Pour chaque sous-ensemble, le degré d\u0027équivalence est mesuré en comparant la matrice d\u0027adjacence induite par la mesure de proximité choisie à celle induite par la \"meilleure\" mesure de proximité discriminante dite de référence. Les performances de cette approche sont évaluées à l\u0027aide de don-nées simulées et réelles. Des comparaisons de sélection de variables en discrimination avec une approche métrique montrent une bien meilleure sélection à partir de l\u0027approche topologique proposée."
  },
  {
    "id": "208",
    "text": "Les solutions existantes pour le raisonnement incrémental souffrent principalement de leur incapacité à prendre en charge des ontologies complexes et ne sont pas conçues pour gérer de grandes quantités de connaissances. Dans cet article, nous présentons Slider (Chevalier et al. (2015)), un raisonneur incrémental évolutif par chaînage avant, permettant de raisonner sur des flux de données sémantiques. Les principales caractéristiques de Slider sont les suivantes : Exécution parallèle et passage à l\u0027échelle. Le processus est parallélisé plutôt que distribué, malgré le coût de l\u0027accès concurrent au triplestore. Limitation des doublons. Les connaissances reçues ou inférées par le raisonneur sont acces-sibles de manière concurrente par tous les modules du système. Support de flux de données. L\u0027achitecture parallèle de Slider lui permet de recevoir des don-nées de sources statiques et dynamiques. Indépendance au fragment. Slider supporte nativement les fragments RDFS et ρdf et peut être utilisé pour des fragments plus complexes. Notre système multi-processus est constitué de modules autonomes, chacun associé à une règle d\u0027inférence, comme le présente la figure 1. Un triplestore unique est partagé par ces modules de manière synchronisée. Le partitionnement vertical, introduit par Abadi et al. (2007), permet un accès rapide aux données. Les triples envoyés au système sont récupérés par le distributeur général. Cet élément re-çoit les nouveaux triples, les stocke dans le triplestore, et les envoie aux règles pouvant les utiliser. Chaque règle associée à un buffer récupère les triples envoyés par le distributeur gé-néral. Lorsque qu\u0027un buffer est plein, ou n\u0027a pas reçu de nouveau triples depuis un temps prédéfini (timeout), un exécuteur de règle est instancié afin d\u0027appliquer la règle d\u0027inférence sur les triples présents dans le buffer. Ces buffers permettent d\u0027améliorer la répartition de charge en appliquant les règles sur des ensembles de triples et non pas pour chaque triple, réduisant ainsi la quantité de processus légers instanciés. Ils assurent également que chaque triple est traité par le système en garantissant, même lors de l\u0027exécution d\u0027une règle, qu\u0027ils sont recueillis par le buffer. Les triples inférés par les exécuteurs de règle sont ensuite récoltés par le distribu-teur de la règle correspondante. Ce distributeur les redistribue aux buffers des règles pouvant utiliser ces triples. Afin de déterminer si une règle peut utiliser un triple inféré par une autre-537"
  },
  {
    "id": "209",
    "text": "Dans cet article nous présentons une approche couplant une courbe remplissant l\u0027espace et une chaîne de Markov pour analyser des données spa-tiales concernant la localisation de haies. Du fait de l\u0027hétérogénéité spatiale des données, nous utilisons une courbe adaptative de Hilbert qui permet de linéariser l\u0027espace en s\u0027ajustant localement à la densité des données. Pour ensuite exploiter la séquence produite, il est nécessaire de caractériser la distance entre un point et son prédecesseur sur la courbe ainsi que la densité locale. Nous proposons de calculer un temps d\u0027accès à un point à partir du point précédent en utilisant la notion de profondeur de découpe. Cette variable, couplée avec les variables caractérisant les haies est ensuite analysée avec un modèle de Markov. Nous présentons et interprétons les résultats obtenus sur un jeu de données d\u0027environ 10000 segments de haies d\u0027une zone de la Basse vallée de la Durance."
  },
  {
    "id": "214",
    "text": "1 Introduction"
  },
  {
    "id": "215",
    "text": "Dans cet article nous présentons CoSC, un cadre collaboratif pour la segmentation et la classification d\u0027images de télédétection permettant d\u0027extraire les objets d\u0027une classe thématique donnée. Le processus de collaboration est guidé par la qualité des données évaluée par des critères d\u0027homogénéité ainsi que des critères implicitement liés à la sémantique des objets afin d\u0027extraire une classe thématique donnée. Nos expériences montrent que CoSC atteint des bons résultats en termes de classification, et améliore notablement la segmentation de l\u0027image de manière globale."
  },
  {
    "id": "216",
    "text": "1 Introduction"
  },
  {
    "id": "217",
    "text": "1 Nous présentons une recherche sur la distribution et la classification non-supervisée des graphèmes. Nous visons à réduire l\u0027écart entre les résultats de recherches récentes qui montrent la capacité des algorithmes d\u0027apprentissage et de classification non-supervisée pour détecter les propriétés de phonèmes, et les possibilités actuelles de la représentation textuelle d\u0027Unicode. Nos procé-dures doivent assurer la reproductibilité des expériences et garantir que l\u0027infor-mation recherchée n\u0027est pas implicitement présente dans le pré-traitement des données. Notre approche est capable de catégoriser correctement de potentiels graphèmes, ce qui montre que les propriétés phonologiques sont présentes dans les données textuelles, et peuvent être automatiquement extraites à partir des données textuelles brutes en Unicode, sans avoir besoin de les traduire en repré-sentations phonologiques."
  },
  {
    "id": "218",
    "text": "Depuis 2001, les conférences EGC ont rassemblé 1 782 chercheurs autour de l\u0027extraction et la gestion de connaissances. En 2016, l\u0027association EGC réfléchit à son histoire et se projette en lançant un défi à sa communauté. Que peut-on révéler sur la communauté EGC via des approches développées en EGC ? Notre étude lexico-scientométrique apporte un éclairage sur les théma-tiques du congrès, les lieux de publication investis par ses auteurs, ou encore les auteurs sollicitables comme évaluateurs. Les résultats sont intégrés à un site web sous-tendu par un système d\u0027information décisionnel."
  },
  {
    "id": "219",
    "text": "Mettre en place un dispositif de détection de pannes représente de nos jours l\u0027un des défis majeurs pour les constructeurs des systèmes robotisés. Le processus de détection nécessite l\u0027utilisation d\u0027un certain nombre de cap-teurs afin de surveiller le fonctionnement de ces systèmes. Or, le coût ainsi que les contraintes liées à la mise en place de ces capteurs conduisent souvent les concepteurs à optimiser leurs nombres, ce qui mène à un manque de mesures nécessaires pour la détection de défaillances. L\u0027une des méthodes pour combler ce manque est d\u0027estimer les paramètres non mesurables à partir d\u0027un modèle mathématique décrivant la dynamique du système réel. Cet article présente une approche basée sur des données mixtes (données mesurées et données estimées) pour la détection de défaillances dans les systèmes robotisés. Cette détection est effectuée en utilisant un classifieur de type arbre de décision. Les données utili-sées pour son apprentissage proviennent des mesures prises sur le système réel. Ces données sont ensuite enrichies par des données estimées en provenance d\u0027un observateur basé sur un modèle analytique. Cet enrichissement sous forme d\u0027at-tributs supplémentaires a pour but d\u0027augmenter la connaissance du classifieur sur le fonctionnement du système et par conséquent améliorer le taux de bonne détection de défaillances. Une expérience sur un système d\u0027actionnement d\u0027un siège robotisé, montrant l\u0027intérêt de notre approche, sera présentée à la fin de l\u0027article."
  },
  {
    "id": "220",
    "text": "Cet article porte sur l\u0027étiquetage automatique de documents décrivant des produits, avec des concepts très spécifiques traduisant des besoins précis d\u0027utilisateurs. La particularité du contexte est qu\u0027il se confronte à une triple dif-ficulté : 1) les concepts utilisés pour l\u0027étiquetage n\u0027ont pas de réalisations ter-minologiques directes dans les documents, 2) leurs définitions formelles ne sont pas connues au départ, 3) toutes les informations nécessaires ne sont pas for-cément présentes dans les documents mêmes. Pour résoudre ce problème, nous proposons un processus d\u0027annotation en deux étapes, guidé par une ontologie. La première consiste à peupler l\u0027ontologie avec les données extraites des documents , complétées par d\u0027autres issues de ressources externes. La deuxième est une étape de raisonnement sur les données extraites qui recouvre soit une phase d\u0027apprentissage de définitions de concepts, soit une phase d\u0027application des définitions apprises. L\u0027approche SAUPODOC est ainsi une approche originale d\u0027enrichissement d\u0027ontologie qui exploite les fondements du Web sémantique, en combinant les apports du LOD et d\u0027outils d\u0027analyse de texte, d\u0027apprentissage automatique et de raisonnement. L\u0027évaluation, sur deux domaines d\u0027application, donne des résultats de qualité et démontre l\u0027intérêt de l\u0027approche."
  },
  {
    "id": "222",
    "text": "Nous présentons une méthode de réduction de dimensionnalité pour des données de préférences multicritères lorsque l\u0027espace des évaluations est un treillis distributif borné. Cette méthode vise à réduire la complexité des procédures d\u0027apprentissage d\u0027un modèle d\u0027agrégation sur des données qualita-tives. Ainsi nous considérons comme modèle d\u0027agrégation l\u0027intégrale de Su-geno. L\u0027apprentissage d\u0027un tel modèle à partir de données empiriques est un problème d\u0027optimisation à 2 n paramètres (où n est le nombre de critères consi-dérés). La méthode de réduction que nous proposons s\u0027appuie sur l\u0027observation de certaines relations entre les éléments de ces données, et nous donnons des premiers résultats d\u0027applications."
  },
  {
    "id": "223",
    "text": "Nous proposons une nouvelle approche pour le calcul de similarité sé-mantique entre phrases en utilisant les noyaux sémantiques qui les composent. Ces noyaux, sous la forme de triplets (sujet, verbe et objet) sont supposés por-teurs de l\u0027information des phrases dont ils sont extraits. Sur la base de la compa-raison sémantique de noyaux, on extrait un ensemble d\u0027indicateurs descriptifs. Nous utilisons ensuite un apprentissage automatique, sur un benchmark conte-nant des phrases dont la similarité sémantique a été évaluée par des experts hu-mains, afin de déterminer l\u0027importance de chaque indicateur et de construire ainsi un modèle capable de fournir une mesure de similarité sémantique entre phrases. Les expérimentations et les études comparatives, effectuées avec d\u0027au-tres approches permettant l\u0027estimation des similarités sémantiques entre phrases, montrent les bonnes performances de notre approche. En se basant sur cette der-nière, un outil de navigation sémantique est en cours de développement."
  },
  {
    "id": "224",
    "text": "Les traces de mobilité générées par les divers capteurs qui nous en-tourent peuvent être analysées à des fins prédictives et explicatives pour répondre à divers problèmes du quotidien. Si de nombreuses méthodes ont été propo-sées pour décrire le comportement d\u0027un individu de manière globale à partir des transitions entre ses différents points d\u0027intérêts (par exemple via un modèle de Markov), peu de travaux cherchent à l\u0027expliquer de manière locale. Nous pro-posons dans cet article une méthode qui permet d\u0027extraire pour un individu dont on a une trace de mobilité conséquente des motifs de mobilité dits contextuali-sés. Chaque motif est composé d\u0027une description sur l\u0027ensemble des visites aux différents points d\u0027intérêt de l\u0027individu qui maximise une ou plusieurs mesures avec une sémantique particulière (le motif décrit une phase sédentaire ou excep-tionnel de la mobilité de l\u0027individu). Une expérimentation a été menée à partir de traces de mobilité de véhicules et donne des résultats encourageants."
  },
  {
    "id": "225",
    "text": "Au cours des dernières années, la classification à base de clustering s\u0027est imposée comme un sujet de recherche important. Cette approche vise à décrire et à prédire un concept cible d\u0027une manière simultanée. Partant du fait que le choix des centres pour l\u0027algorithme des K-moyennes standard a un impact direct sur la qualité des résultats obtenus, cet article vise alors à tester à quel point une méthode d\u0027initialisation supervisée pourrait aider l\u0027algorithme des K-moyennes standard à remplir la tâche de la classification à base des K-moyennes."
  },
  {
    "id": "226",
    "text": "Notre objectif, à terme, est de proposer un outil de visualisation analytique (Visual Analy-tics) permettant d\u0027explorer les différents points de vue ou les variantes des sujets traités dans un corpus tel que des articles de presse. Appliqués sur le modèle de sac de mots (matrice termes x documents), les méthodes probabilistes d\u0027extraction de sujets du type Latent Dirichlet Allocation calculent une distribution des termes dans un nombre prédéfini de sujets, pour les regrouper par proximité sémantique. Il en résulte ensuite une distribution des documents dans ces mêmes sujets. D\u0027autres méthodes du type co-clustering (Govaert et Nadif, 2013) consi-dèrent simultanément les vecteurs des termes et des documents pour produire des bi-clusters regroupant les termes sémantiquement proches et les documents qui les partagent. Les visualisations habituelles des sujets avec des nuages de mots permettent d\u0027interpréter les sujets à travers les N termes les plus représentatifs. Dans le contexte d\u0027un corpus d\u0027articles de presse, cette approche met en exergue ce qui est majoritaire et déjà connu. Un analyste cherche, au contraire, à identifier des points de vue alternatifs et inédits. Dans ce but, nous pro-posons de structurer le corpus en regroupant les documents par points de vue partagés, selon différentes combinaisons de mots-clés colocalisés dans les documents. Nous nous appuyons sur Bimax (Prelic et al., 2006), une méthode de bi-clustering non-disjoint qui extrait, à par-tir d\u0027une matrice binaire, tous les bi-clusters (blocs constitués uniquement de 1) vérifiant une contrainte d\u0027inclusion maximale. Cette contrainte impose qu\u0027aucun bi-cluster ne soit entière-ment inclus dans un autre. Bimax est adapté aux matrices sparses (c\u0027est le cas pour le texte) et permet d\u0027extraire tous les bi-clusters optimaux. Dans une matrice termes x documents, discré-tisée avec un seuil sur des poids de type TF-IDF, les bi-clusters regroupent des documents de manière unique selon les multiples co-occurences possibles de mots-clés. Nous faisons l\u0027hy-pothèse que les bi-clusters ainsi obtenus constituent l\u0027ensemble des points de vue concernant les sujets d\u0027un corpus. Cependant, Bimax produit une grande quantité de bi-clusters contenant beaucoup de redondances au niveau des termes et des documents, mais aussi quelques spéci-ficités (termes ou documents exclusifs à un bi-cluster). De plus, la représentation visuelle et l\u0027interprétation des bi-clusters non-disjoints restent des tâches difficiles (Sun et al., 2014). Pour faciliter l\u0027exploration des bi-clusters, nous cherchons à hiérarchiser les éléments des deux dimensions selon leur degré de redondance dans les bi-clusters, en agrégeant ces derniers par points communs. Nous proposons de décomposer la matrice termes x documents en un ensemble de blocs disjoints regroupant les cellules appartenant à une intersection unique de-539"
  },
  {
    "id": "227",
    "text": "1 Introduction"
  },
  {
    "id": "1270",
    "text": "Le choix d\u0027un quartier est primordial lors d\u0027un achat ou d\u0027une location immobilière. Or, il est fréquent de ne pas connaître la ville où l\u0027on arrive (e.g., mutation professionnelle) et la sélection d\u0027un quartier pertinent devient alors un véritable défi. Dans cet article, nous présentons un outil qui facilite la comparai-son entre quartiers. Nous exploitons plusieurs indicateurs pour différencier les quartiers et plusieurs algorithmes permettent soit de recommander un quartier, soit de regrouper des quartiers similaires."
  },
  {
    "id": "1271",
    "text": "De nombreuses applications s\u0027appuient sur des bases de données dis-tribuées. Pourtant, peu de méthodes de découverte de motifs ont été proposées pour les extraire sans centraliser les données. Il faut dire que cette centralisa-tion est souvent moins coûteuse que la communication des motifs extraits. Pour contourner cette difficulté, cet article adopte une approche parcimonieuse en coûts de communication en fournissant à l\u0027utilisateur des motifs à la demande. Plus précisément, nous proposons l\u0027algorithme DDSAMPLING qui tire un motif dans une base de données distribuée proportionnellement à son intérêt. Nous démontrons son exactitude et analysons sa complexité en temps et en communication soulignant son efficacité. Enfin, une étude expérimentale montre sur plusieurs jeux de données la robustesse de DDSAMPLING face aux défaillances d\u0027un site ou du réseau."
  },
  {
    "id": "1272",
    "text": "-292 -"
  },
  {
    "id": "1274",
    "text": "Software Heritage est une initiative à but non lucratif dont l\u0027objectif ambitieux est de col-lecter, préserver et partager le code source de tous les logiciels jamais écrits, avec leur histo-rique de développement complet, en construisant une base de connaissances logicielle univer-selle. Software Heritage répond à une variété de besoins : préserver nos connaissances scienti-fiques et technologiques, améliorer le développement et la réutilisation des logiciels pour la so-ciété et l\u0027industrie, favoriser la science ouverte et construire une infrastructure essentielle pour des études logicielles reproductibles à grande échelle. Nous avons déjà collecté plus de 4 mil-liards de fichiers sources uniques provenant de plus de 80 millions d\u0027origines. Manipuler ce gi-gantesque ensemble de données est une mission complexe et nécessite de nouvelles approches pour stocker et requêter l\u0027information d\u0027une manière compatible avec la croissance explosive du développement logiciel collaboratif. Dans cette conférence, nous explorons quelques uns des nouveaux défis et opportunités que présente Software Heritage.-5"
  },
  {
    "id": "1275",
    "text": "Le contenu de ce papier prend en compte la nature linguistique in-formelle et mixte des langues de médias sociaux qui sont associées au dialecte algérien et utilisées comme moyen d\u0027exprimer des opinions ou des sentiments. Après avoir identifié les défis de ce type de recherche et mis en avant les spécifi-cités du multilinguisme, une plateforme collaborative appelée TWIFIL (TWIter proFIL) pour l\u0027annotation de données multilingues est proposée. Le résultat est un corpus de tweets annotés. Les premières informations recueillies ont permis d\u0027enrichir les informations de chaque tweet. Des tests ont été realisés sur le corpus généré en utilisant les techniques d\u0027apprentissage automatique."
  },
  {
    "id": "1276",
    "text": "Dans cet article, nous proposons une méthode pour l\u0027identification de zones fonctionnelles, utilisant la détection de communautés dans un graphe de mobilité. Les sommets du graphe correspondent à des unités spatiales, issues du découpage d\u0027une ville suivant le réseau routier. Les arêtes relient des sommets entres lesquels des déplacements sont observés et sont pondérées en fonction du nombre de déplacements et de la distance entre sommets. Notre approche optimise la modularité sur ce réseau pour assurer que les zones fonctionnelles obtenues maximisent les interactions spatiales en leur sein. De plus, nous uti-lisons les points d\u0027intérêts pour maintenir une hétérogénéité suffisante dans les zones détectées. Nous avons mené des expérimentations avec des trajectoires de taxi et des points d\u0027intérêts de la ville de Porto, afin de montrer la capacité de notre approche à identifier les zones fonctionnelles."
  },
  {
    "id": "1278",
    "text": "La finalité de notre travail est la détection des anomalies dans les traces de fonctionnement de l\u0027infrastructure de communication du Système d\u0027In-formation (SI) de la SNCF. Deux techniques récentes et indépendantes semblent particulièrement appropriées dans notre cas. Il s\u0027agit d\u0027une part du stockage et de l\u0027indexation de séries temporelles dans un arbre appelé arbre iSAX, et d\u0027autre part d\u0027un score de détection d\u0027anomalie nommé CFOF dont la robustesse au phénomène de concentration en haute dimension a été établie de façon formelle. Dans cet article nous montrons qu\u0027il est possible d\u0027utiliser la structuration des informations dans l\u0027arbre iSAX pour déterminer rapidement une approximation du score CFOF. La valeur obtenue est proche du score exact sur des données synthétiques et réelles. Les premiers retours d\u0027expertises indiquent que la mé-thode semble pertinente pour le déclenchement d\u0027alarmes sur les données issues de trace d\u0027activité du SI de la SNCF."
  },
  {
    "id": "1279",
    "text": "Dans ce travail, nous présentons une nouvelle méthode permettant le calcul de similarités entre objets basée sur les forêts d\u0027arbres extrêmement aléa-toires. L\u0027idée principale de notre méthode est de séparer les données de manière itérative jusqu\u0027à ce qu\u0027une condition d\u0027arrêt soit respectée, et de calculer une similarité basée sur la co-occurrence des instances dans les feuilles de chaque arbre obtenu. Nous évaluons la méthode sur un ensemble de jeux de données synthétiques et réels. Cette évaluation est basée sur la comparaison des similari-tés moyennes entre instances ayant la même étiquette aux similarités moyennes entre instances d\u0027étiquette différente. Ces mesures sont comparables aux notions de similarités intracluster et intercluster, mais ont pour intérêt d\u0027être agnostiques aux choix d\u0027une méthode de clustering en particulier. L\u0027étude empirique montre que la méthode permet effectivement de distinguer les individus n\u0027appartenant pas aux même clusters. Les forêts d\u0027arbres extrêmement aléatoires non supervi-sées ont des propriétés intéressantes, telles que : (i) l\u0027invariance aux transformations monotones de variables, (ii) la robustesse aux variables corrélées, et (iii), la robustesse au bruit. Enfin, nous présentons les résulats obtenus par l\u0027appli-caton d\u0027un algorithme de clustering hiérarchique agglomératif, en utilisant les matrices de similarité obtenues par notre méthode. Les résultats obtenus sur des jeux de données homogènes et hétérogènes sont prometteurs."
  },
  {
    "id": "1280",
    "text": "La recherche de motifs de graphe est l\u0027une des opérations principales de la recherche des correspondances d\u0027une requête dans un graphe donné. Dans ce contexte, trouver des solutions garantissant l\u0027optimalité en termes de préci-sion et de temps de calcul est un problème de recherche difficile et d\u0027actua-lité. Différents modèles ainsi que leurs algorithmes appropriés ont été proposés pour la recherche de motifs dans les graphes de données. Cependant, l\u0027inconvé-nient majeur est leur limitation à trouver des réponses significatives entraînant le problème des réponses vides. Dans cet article nous introduisons un nouveau modèle pour la recherche de motifs de graphe permettant un certain type d\u0027as-souplissement de requêtes afin d\u0027éviter ce problème. Ensuite nous développons un algorithme efficace basé sur des techniques d\u0027optimisation pour trouver les k-meilleurs réponses selon notre modèle. Nos expérimentations sur quatre ensembles de données réelles démontrent l\u0027efficacité de notre approche."
  },
  {
    "id": "1281",
    "text": "1 Introduction"
  },
  {
    "id": "1283",
    "text": "a.garel@octopusmind.info, http ://www.octopusmind.info Résumé. Nous réexaminons dans cet article les méthodes de vectorisation de textes dans le cadre d\u0027une étude de classification de documents. Nous étudions les méthodes basées sur des plongements de mots (word2vec) ou de documents (analyse sémantique latente, ou sac de mots associées à diverses pondérations) ainsi que certaines combinaisons de ces méthodes. A cette fin, nous évaluons ces méthodes de vectorisation en utilisant trois modèles de classification (un percep-tron multicouches, une machine linéaire à vecteurs supports optimisée par des-cente de gradient stochastique et un classifieur multinomial naïf de Bayes). Nos résultats montrent que le modèle proposé pour associer les méthodes word2vec et LSA, qui conjugue les deux caractérisations complémentaires du contexte d\u0027occurrence des mots (local pour word2vec et global pour LSA), permet de produire une vectorisation robuste, en général plus discriminante que les autres approches testées."
  },
  {
    "id": "1284",
    "text": "La fouille d\u0027opinion ciblée est une tâche complexe, susceptible de bé-néficier de l\u0027apport d\u0027approches variées. Nos expérimentations testent des com-binaisons de méthodes sur un corpus d\u0027avis d\u0027internautes concernant les livres. Sur ces données et pour ce qui concerne la polarité de l\u0027opinion, des résultats prometteurs ont été obtenus par une approche basée sur une analyse linguistique de surface et un lexique enrichi par les informations discriminantes basées sur des méthodes de classification statistiques supervisées."
  },
  {
    "id": "1286",
    "text": "1 Introduction"
  },
  {
    "id": "1287",
    "text": "En parallèle à l\u0027essor des travaux en fouille d\u0027opinions, les méthodes de classement connaissent un regain d\u0027intérêt. Ces méthodes qui consistent à faire classer à un échantillon restreint d\u0027individus un ensemble d\u0027items dans des caté-gories pré-fixées ou non tentent d\u0027appréhender plus finement les similarités per-çues et la subjectivité mise en oeuvre dans les décisions. Mais leurs analyses ne s\u0027appuient que sur les classements finaux et ne prennent pas en compte les infor-mations associées à la dynamique de construction du classement. Nous présen-tons ici une démarche originale qui explore l\u0027apport de l\u0027analyse des trajectoires de la souris et des évènements associés. Une expérimentation est menée dans le cadre de la Q-méthodologie, qui est une méthode de classement initialement issue de la psychologie mais utilisée aujourd\u0027hui dans de nombreux domaines. Nos premiers résultats permettent d\u0027identifier différentes stratégies de classe-ment ainsi que des comportements atypiques dont nous évaluons le rôle dans la construction du résultat final qui rend compte des classements individuels par une approche de type factorielle."
  },
  {
    "id": "1288",
    "text": "Les modèles de diffusion proposés dans les médias sociaux reposent pour la plupart sur des hypothèses épidémiologiques et non sur l\u0027observation de données réelles pour décrire les caractéristiques de la diffusion. De tels mo-dèles ne peuvent pas reproduire fidèlement le phénomène de diffusion dans la mesure où ils ne considèrent pas les facteurs observés qui peuvent influencer ce phénomène. Notre approche innove dans le sens où elle se place au niveau des populations des pays et qu\u0027elle consiste à identifier en plus du nombre de populations atteintes, le rayon géographique d\u0027influence autour de ces populations , l\u0027instant de diffusion de l\u0027information, la durée de la diffusion et le pays auquel appartiennent ces populations en connaissant la population à l\u0027origine de l\u0027information et sa thématique."
  },
  {
    "id": "1290",
    "text": "La fouille de règles d\u0027association est un problème qui a donné lieu à une littérature foisonnante, notamment dans les données binaires bidimen-sionnelles classiques. En particulier, la relation entre les ensembles fermés et les règles d\u0027association est bien connue. Tel n\u0027est pas le cas dans les données multidimensionnelles. Dans ce papier, nous montrons que la connaissance des n-ensembles fermés d\u0027un tenseur booléen multidimensionnel est suffisante pour inférer la confiance de toutes les règles d\u0027association multidimensionnelles."
  },
  {
    "id": "1291",
    "text": "Seulement 10% des données disponibles en entreprise sont réellement utilisées tandis que les 90% restants (\"dark data\") restent inexploitées (Veritas, 2016). Dans une économie où la donnée est le nouveau pétrole, des outils faci-litant la découverte et la compréhension de gisements de données représentent une opportunité importante. Cette démonstration présente Dataforum, une pla-teforme d\u0027échange ciblant les organisations désireuses de partager de grands gisements de données d\u0027une manière simple, rapide et fiable."
  },
  {
    "id": "1293",
    "text": "1 Introduction"
  },
  {
    "id": "1294",
    "text": "Les approches basées sur les prototypes sont très populaires en ap-prentissage non supervisé, en raison de la compacité du modèle résultant (les prototypes), de la puissance descriptive de ces prototypes et de la faible com-plexité de calcul du modèle. Habituellement, le meilleur choix de prototype est le barycentre du cluster. Le prototype est alors défini comme l\u0027objet minimisant la somme des distances carrées avec tous les objets du cluster. Cependant, dans de nombreux cas, les objets ne peuvent pas être facilement définis dans un espace euclidien sans perte d\u0027information et/ou un pré-traitement coûteux, ce qui limite la construction des prototypes. Dans cet article, nous proposons une approche de K-moyennes relationnelle utilisant un ensemble unique de points de support basé sur le formalisme des coordonnées barycentriques, afin d\u0027unifier la repré-sentation des objets et des prototypes et permettant un processus d\u0027apprentissage incrémental simple pour le clustering relationnel."
  },
  {
    "id": "1296",
    "text": "La détection d\u0027anomalie est une tâche d\u0027apprentissage dans laquelle les anomalies sont beaucoup plus rares que les comportements normaux. Notre objectif est de détecter une anomalie, en l\u0027occurrence une fuite de fluide, le plus tôt possible, avant l\u0027arrêt préventif de la machine. Dans cet article, nous étu-dions la résistance au bruit et à la rareté des anomalies d\u0027une technique d\u0027ap-prentissage supervisée, les arbres de décision. Nous considérons des données artificielles représentatives d\u0027anomalies de systèmes physiques comme la cre-vaison d\u0027un pneumatique ou la fuite de fluide réfrigérant d\u0027une pompe à chaleur. Nos tests montrent qu\u0027un arbre de décision est capable d\u0027apprendre un seuil sur la pression observée, en présence de bruit, qui s\u0027adapte à des fréquences très faibles d\u0027anomalies, jusqu\u0027à 1 pour 100 000."
  },
  {
    "id": "1297",
    "text": "1 Introduction"
  },
  {
    "id": "1298",
    "text": "L\u0027algorithme de classification non supervisé \u0027k-means\u0027 nécessite un accès itératif et répétitif aux données allant jusqu\u0027à effectuer plusieurs fois le même calcul sur les mêmes données. Ces calculs répétés peuvent s\u0027avérer coû-teux lorsqu\u0027il s\u0027agit de classifier des données massives. Nous proposons d\u0027étendre l\u0027algorithme de k-means en introduisant une approche d\u0027optimisation basée sur le pré-calcul dynamique d\u0027agrégats pouvant ensuite être réutilisés afin d\u0027éviter des calculs redondants."
  },
  {
    "id": "1299",
    "text": "La nature ouverte des graphes de connaissances implique souvent qu\u0027ils soient incomplets. La prédiction de liens consiste à inférer de nouveaux liens entre entités sur la base des liens existants. La plupart des approches exis-tantes s\u0027appuient sur l\u0027apprentissage de vecteurs de traits latents pour l\u0027encodage des entités et des relations. En général cependant, les traits latents ne sont pas facilement interprétables. Les approches à base de règles sont interprétables mais un ensemble de règles différent doit être appris pour chaque relation. Nous proposons une nouvelle approche qui n\u0027a pas besoin de phase d\u0027apprentissage et qui peut fournir des explications intelligibles pour chaque inférence. Elle repose sur le calcul de Concepts de plus proches voisins (Concepts of Nearest Neighbours, CNN) pour identifier des entités similaires fondées sur des motifs de graphe communs. La théorie de Dempster-Shafer est ensuite utilisée pour tirer des inférences à partir des CNN. Nous évaluons notre approche sur FB15k-237, un benchmark classique en prédiction de liens, où elle obtient de meilleures performances que les approches existantes."
  },
  {
    "id": "1300",
    "text": "Une carte cognitive (Tolman, 1948) est un graphe dont les noeuds sont des concepts et les arcs représentent des influences. Elle permet de modéliser des stratégies ou des systèmes d\u0027in-fluence. Les cartes cognitives ne permettent pas de prendre en compte les aspects temporels. Ce manque d\u0027aspects temporel a été relevé dans le cadre du projet KIFANLO. Ce projet vise à valoriser le savoir des pêcheurs de la région Pays de la Loire, pour comparer les espaces des pêches maritimes entre 1970 et aujourd\u0027hui. Pour cela, les stratégies de pêche ont été modé-lisées avec des cartes cognitives construites avec les pécheurs. Cet article propose de prendre en compte les aspects temporels en associant une carte cognitive à une ontologie temporelle. Cette ontologie, composée d\u0027une taxonomie et d\u0027un graphe temporel, permet de représenter différentes entités temporelles et de les mettre en relation. Les concepts de la carte peuvent alors être associés à une caractérisation temporelle. Une extension, appelée CMQLT, du lan-gage de requête de cartes cognitives CMQL, est proposée de sorte à pouvoir accéder aux ca-ractérisations temporelles des concepts et les comparer. Le langage CMQL(Cognitive Map Query Language) (Robert et al., 2019) est le langage qui a été proposé pour pouvoir interroger les cartes cognitives afin de les analyser. CMQL a une syntaxe proche de celle de SQL avec une forme générale de requête de type « SELECT vars FROM maps WHERE { formula } ». Les différentes compositions de formules sont similaires au calcul relationnel de domaine (et donc à la logique du premier ordre). Les formules ato-miques sont soit des expressions (par exemple x \u003e 2), soit des primitives. Les primitives sont des relations qui permettent d\u0027accéder aux différentes caractéristiques du modèle des cartes cognitives. Les attributs de ces primitives sont soit des constantes soit des variables, cela per-met de contraindre plus ou moins la relation. Il existe plusieurs primitives, l\u0027ensemble de ces primitives est extensible. Voici un exemple de requête interrogeant ce qui influence indirecte-ment le plaisir de l\u0027équipage : SELECT ?carte, ?concept FROM ST WHERE{ Path(?carte, ?concept, PlaisirEquipage, ?path) AND Length(?path, ?longueur) AND ?longueur\u003e1} L\u0027ontologie temporelle permet la représentation du temps, elle est composée d\u0027une taxono-mie temporelle et d\u0027un graphe temporel. La taxonomie temporelle ordonne les entités tempo-relles par une relation « est une sorte de » et permet de typer ces entités. Le graphe temporel permet de mettre en relation ces entités en les comparant grâce à des prédicats de comparaison temporels.-369"
  },
  {
    "id": "1301",
    "text": "Les graphes représentent un outil efficace pour la modélisation des relations structurelles entre les objets. Cependant, l\u0027exploitation de ces graphes de données est très coÃ»teuse en raison de la taille. En effet, dans la plupart des applications réelles, la taille des graphes est largement grande, ce qui rend difficile à comprendre l\u0027information et la structure codée dans ces graphes par une simple visualisation. La représentation compacte des grands graphes, ap-pelée aussi compression des graphes, est une opération qui permet la diminu-tion du nombre d\u0027arêtes ou de noeuds du graphe pour faciliter leurs traitements. Dans cet article, nous proposons une nouvelle approche basée sur l\u0027utilisation des contraintes pseudo booléennes pour une représentation condensée de larges graphes. L\u0027avantage d\u0027une telle représentation est qu\u0027au lieu de représenter un graphe (e.g., clique) par un nombre quadratique d\u0027arêtes, on peut l\u0027exprimer sous forme d\u0027une inéquation linéaire dont les modèles correspondent exactement aux arêtes du graphe initial. Notre approche permet le passage à l\u0027échelle tout en garantissant la décompression de graphes par une simple résolution des inéqua-tions linéaires correspondantes. Les expérimentation sur plusieurs graphes réels montrent que notre approche offre de meilleures performances comparée à plu-sieurs approches de l\u0027état de l\u0027art."
  },
  {
    "id": "1302",
    "text": "Les guides de bonnes pratiques sont des documents de référence enévolutionenévolution qui contiennent des recommandations et des r` egles visantàvisantà aider les professionnelsàprofessionnelsà maˆıtrisermaˆıtriser un domaine médical. Dans notre tra-vail, nous sommes intéressés par l\u0027utilisation de ces documents afin d\u0027aider les médecins cardiologuesàcardiologuesà prendre des décisions sur les soins de santé appropriés pour les patientsàpatientsà risque de maladie cardiovasculaire. Plus précisément, notre papier propose une approche automatique qui analyse et transforme le texte (recommandations médicales) en OWL DL (Web Ontology Language-Description Logic) et des r` egles SWRL (Semantic Web Rule Language). Pour analyser le texte, nous avons utilisé une onto-logie du domaine cardiovasculaire et des outils de traitement du langage naturel (TAL). Notre travail est original en ce qu\u0027il propose une transformation automatique des textes en r` egles SWRL alors que les travaux associés portent uniquement sur la transformation de texte en des axiomes OWL légères."
  },
  {
    "id": "1303",
    "text": "La reconnaissance des entités nommées (REN) consiste à repérer des éléments textuels et à les classer dans des catégories prédéfinies (noms de per-sonnes, d\u0027organisations, de marques, d\u0027équipes sportives, etc.). La REN est sou-vent considérée comme l\u0027une des briques de fondation des systèmes visant à structurer un texte tout-venant. Dans cet article, nous décrivons notre système symbolique de REN qui se caractérise par 1) l\u0027utilisation de ressources diction-nairiques limitées et 2) la prise en compte de résultats provenant d\u0027autres briques telles que la résolution de coréférences et l\u0027extraction de relations. Le système est basé sur la sortie d\u0027un analyseur syntaxique en dépendances qui adopte un flot d\u0027exécution itératif intégrant des résultats d\u0027autres briques d\u0027analyse. À chaque itération, des catégories candidates sont générées et sont toutes prises en compte dans les itérations suivantes. L\u0027intérêt d\u0027un tel système est de sélectionner défi-nitivement le meilleur candidat uniquement à la fin du traitement afin de tenir compte de l\u0027ensemble des éléments fournis par les différentes briques. Le sys-tème est comparé à des systèmes académiques et industriels."
  },
  {
    "id": "1304",
    "text": "Au sein du groupe SNCF, le programme PRISME d\u0027excellence sécu-rité intègre une démarche de simplification de l\u0027accès à l\u0027information et de la production de contenus dans la documentation métier. Dans ce contexte, nous avons mis en oeuvre des traitements sur un corpus de référentiels métiers SNCF afin de guider l\u0027utilisateur dans sa recherche documentaire. Les travaux présen-tés visent à évaluer l\u0027usage des plongements lexicaux pour générer des repré-sentations sémantiques denses sur lesquelles se baseront des méthodes de deep learning pour structurer le corpus SNCF. Le protocole mis en place consiste en l\u0027évaluation empirique des voisinages de mots par des experts. Dans cette étude, nous montrons les difficultés d\u0027apprentissage et d\u0027évaluation inhérentes à ce type de corpus avec de nombreux mots soit très spécifiques, soit polysémiques, ren-dant la construction d\u0027un espace de représentations robuste difficile."
  },
  {
    "id": "1305",
    "text": "1 Introduction"
  },
  {
    "id": "1307",
    "text": "La modélisation des préférences utilisateur et de leur dynamique est au coeur de la construction des systèmes de recommandation séquentielle. Les défis résident dans la combinaison réussie de l\u0027historique des utilisateurs et de leurs actions récentes pour fournir des recommandations personnalisées. Les méthodes existantes s\u0027appuient sur des chaînes de Markov d\u0027ordre fixe, limi-tant la personnalisation. Nous proposons d\u0027utiliser des séquences fréquentes de longueur variable, pour mieux identifier la dynamique séquentielle, et projetons les items dans un espace euclidien en fonction de la préférence utilisateur et de leur historique récent. Une étude empirique sur 13 jeux de données montre que notre méthode surpasse les performances des différentes méthodes de l\u0027état de l\u0027art. De plus, nous pouvons fournir des éclairages sur la recommandation."
  },
  {
    "id": "1308",
    "text": "Nous introduisons la notion de similarité par recouvrement de sé-quences pour estimer la similarité entre une séquence et un ensemble de sé-quences. Nous en dérivons une pseudo-distance qui s\u0027apparente aux distances d\u0027édition de type Levenshtein pour comparer des paires de séquences. La com-plexité algorithmique associée à cette semi-métrique peut-être ramenée à O(n · log(n)) en utilisant des arbres de suffixes. Nous introduisons un nouveau mo-dèle discriminant dédié à la classification de données textuelles dont la com-plexité algorithmique ne dépend pas de la taille de l\u0027ensemble d\u0027apprentissage, mais uniquement du nombre de classes et de la longueur des séquences. L\u0027étude expérimentale préliminaire présentée s\u0027appuie sur deux benchmaks : le premier concerne des séquences de nucléotides, le second une tâche de classification de textes. Les résultats obtenus positionnent l\u0027approche proposée au niveau de l\u0027état de l\u0027art (incluant les approches \"deep learning\") sur les tâches considérées., avec des temps de calcul et un nombre de méta-paramètres avantageux."
  },
  {
    "id": "1309",
    "text": "L\u0027algorithme de fouille DEBuNk permet d\u0027identifier des groupes et des contextes montrant un comportement relatif exceptionnellement différent par rapport à celui généralement observé pour ces mêmes groupes. Pour rendre la compréhension et l\u0027interprétation des résultats plus aisées, l\u0027interface graphique inclut un module de génération de texte qui transcrit les motifs trouvés en lan-gage naturel. Nous illustrons notre approche avec la plate-forme web ANCORE, sur les données de vote du parlement européen durant les deux derniers mandats."
  },
  {
    "id": "1310",
    "text": "Le partitionnement horizontal est l\u0027une des techniques les plus per-formantes pour améliorer l\u0027exploitation de données sur les plateformes de trai-tements parallèles comme Hadoop et Spark. Dans les entrepôts de données dis-tribués (EDD), l\u0027opération la plus coûteuse est la jointure en étoile qui nécessite plusieurs cycles MapReduce lors de son exécution. Dans ce papier, nous propo-sons une nouvelle stratégie de placement des données d\u0027un entrepôt volumineux dans Hadoop, en se basant sur l\u0027algorithme K-means équilibré (K-means balanced). Ce schéma de placement permet d\u0027exécuter des opérations de certaines re-quêtes OLAP, dont la jointure en étoile, en une seule étape de Spark. Dans notre approche, nous prenons en compte les caractéristiques physiques du cluster et le volume des données. Pour évaluer notre proposition, nous avons effectué des expérimentations sur un cluster de 5 noeuds avec un entrepôt de données issu du banc d\u0027essai TPC-DS. Les résultats obtenus montrent un gain de temps d\u0027exé-cution, de certaines requêtes OLAP, allant jusqu\u0027à 60% par rapport à d\u0027autres approches existantes."
  },
  {
    "id": "1311",
    "text": "Nous décrivons une approche originale pour extraire efficacement les symboles graphiques d\u0027un fichier vectoriel (de type PDF par exemple). Après passage d\u0027un espace d\u0027objets graphiques 2D à une chaîne de codes (1D), l\u0027iden-tification des symboles consiste à rechercher une sous-séquence de codes qui se répète dans le fichier d\u0027entrée. Les travaux de la littérature utilisent l\u0027arbre ou le tableau des suffixes, notre algorithme s\u0027appuie sur le principe du tri par pa-quets pour identifier les répétitions. La taille et la fréquence sont spécifiées par l\u0027utilisateur."
  },
  {
    "id": "1313",
    "text": "1 Présentation de l\u0027approche Avec l\u0027émergence du numérique de la dernière décennie, nous faisons face à de grands volumes de données issus de plusieurs secteurs d\u0027activité tels que Commerce, Biologie, Mé-decine, Télécommunication, etc. Ces différents jeux de données véhiculent une quantité d\u0027in-formations prodigieuses et pertinentes. Cependant, l\u0027exploitation optimale de ces masses de données reste encore difficile. Ainsi, la mise en place de nouvelles solutions d\u0027analyse de don-nées est devenue un véritable défi pour la communauté scientifique. Dans ce contexte, la batterie de résultats fournie par l\u0027Analyse Formelle des Concepts s\u0027avère d\u0027une grande importance dans le processus d\u0027extraction de connaissances à travers les treillis de Galois. Cependant, leur vraie exploitation a été toujours freinée par le nombre exor-bitants des concepts formels extraits. Dans le but de filtrer ces derniers, plusieurs approches ont été définies. Parmi ces approches, nous nous intéressons à celles qui ont utilisé les métriques de qualité pour garder les concepts les plus intéressants. Plusieurs mesures de qualité ont été proposées dans la littérature telles que la stabilité définie par (Kuznetsov (1990)) 1 , le couplage et la cohésion proposés par (Paul et Scott (2008)), la séparation introduite par (Klimushkin et al. (2010)), la distance définie par (Eklund et al. (2012)), etc. Ce nombre important engendre de nouveaux problèmes, entre autres, le choix de mesures de qualité à utiliser. Cet article propose une nouvelle approche multi-critères permettant de sélectionner les top-k meilleurs concepts d\u0027un ensemble de concepts formels, ayant en entré un ensemble de mesures de qualité. En outre, il importe de souligner que leur importance peut être éventuelle-ment pondérée par un utilisateur et/ou expert. Parmi les méthodes multi-critères, nous proposons d\u0027utiliser l\u0027approche TOPSIS (Tech-nique for Order of Preference by Similarity to Ideal Solution) introduite par (Hwang et Yoon (1981)) pour générer les top-k concepts formels. L\u0027idée fondamentale de cette méthode consiste à choisir une solution, qui se rapproche le plus de la solution idéale (meilleure sur tous les cri-tères) et de s\u0027éloigner le plus possible de la pire solution (qui dégrade tous les critères). 1. Dans cette approche, nous appliquons l\u0027algorithme DFSP proposé par (Mouakher et Ben Yahia (2019)) pour le calcul de la stabilité. Ce dernier ne nécessite aucune relation d\u0027ordre entre les concepts.-373"
  },
  {
    "id": "1314",
    "text": "Les modèles structure-activité (QSAR) cherchent à extraire de l\u0027infor-mation utile dans des observations relatives à des structures, dans le but d\u0027asso-cier des éléments structurels à une activité d\u0027ordre macroscopique. Un exemple typique est celui de la chimie organique, où certaines propriétés physiques et chimiques d\u0027une molécule sont fonction de son agencement interne (conforma-tion). On retrouve en particulier des sous-structures caractéristiques, nommées groupements fonctionnels ou fragments qui s\u0027apparentent à des sous-graphes, ainsi que des structures de liaison. Nous proposons une analyse lexicographique de ces fragments et montrons que ceux-ci suivent approximativement des lois de puissance, proches des lois de Zipf observées dans le cadre des langues na-turelles. En poursuivant cette analogie, nous développons la notion de \"plon-gement\" de fragment (fragment-embedding). Nous montrons l\u0027intérêt de cette notion et en déduisons quelques perspectives."
  },
  {
    "id": "1315",
    "text": "CK-Cartography (Crucial Know-How and Knowing-That Cartogra-phy) est construit autour de deux scénarios d\u0027usage: (i) donner une visibilité générale et détaillée des savoir-faire et des savoirs factuels de l\u0027organisation et (ii) aider à la prise de décision. CK-Cartography permet d\u0027obtenir des réponses à des requêtes en rapport avec ces deux scénarios. Ces requêtes sont traduites à des cartes chacune ayant une spécification particulière. La première finalité de la cartographie des savoirs est l\u0027identification des savoir-faire et des savoirs fac-tuels dans un but de capitalisation. La deuxième finalité est la caractérisation et l\u0027 évaluation multictitére des savoir-faire et des savoirs factuels."
  },
  {
    "id": "1316",
    "text": "La découverte de règles caractéristiques d\u0027une classe reste un pro-blème difficile, particulièrement dans le cadre des données séquentielles (sé-quences d\u0027ensembles). La découverte de sous-groupes est une bonne formalisa-tion de cette tâche et de nombreux algorithmes dédiés ont été proposés ces 20 dernières années. Une exploration dite exhaustive est souvent inapplicable au vu de la taille de l\u0027espace de recherche, et les méthodes heuristiques de référence, principalement les recherches en faisceau, posent des problèmes de paramétrage. Nous proposons une méthode d\u0027échantillonnage depuis l\u0027espace des motifs pour la découverte de sous-groupes dans des données séquentielles étiquetées. Celle-ci permet, entre autres, de trouver des optima locaux, ne nécessite pas de para-métrage, est indépendante de la mesure de qualité utilisée, et est simple à mettre en oeuvre. La validation empirique sur divers jeux de données nous permet de valider les qualités de cette approche."
  },
  {
    "id": "1317",
    "text": "Les flux données web récoltés par les robots doivent avoir un haut niveau de véracité pour pouvoir déterminer des connaissances précises. Aussi, analyser leur qualité est primordial à la vue notamment des imperfections intrin-sèques aux données. Dans cet article, nous présentons un outil de visualisation interactive permettant d\u0027analyser la qualité de ces flux temporels."
  },
  {
    "id": "1318",
    "text": "Le calcul de la géométrie de l\u0027état fondamental d\u0027une molécule est le point de départ de l\u0027immense majorité des travaux en chimie quantique molécu-laire. La base de données ouverte PubChemQC met à disposition les résultats de calculs des états fondamentaux pour plus de trois millions de molécules. Nous avons extrait les géométries convergées afin d\u0027entraîner des modèles d\u0027appren-tissage automatique. Prédire la géométrie complète serait une avancée remar-quable. Nos premiers résultats suggèrent qu\u0027il est difficile d\u0027entraîner un réseau de neurones sur cette tâche complexe. Par contre, nous démontrons qu\u0027un ré-seau de neurones est capable de prédire précisément une distance entre deux atomes. L\u0027objet d\u0027étude de ce travail est la distance la plus complexe en chimie organique, la distance carbone-carbone. Les meilleurs résultats sont obtenus en limitant la quantité d\u0027information grâce à une distance seuil autour de chaque carbone."
  },
  {
    "id": "1319",
    "text": "Nous présentons ici l\u0027application WIB (pour Wikipedia Integrated Browser) qui permet de naviguer dans les documents Wikipédia en même temps que dans les termes contenus dans ces documents et les catégories auxquelles ils appartiennent. Selon le(s) type(s) d\u0027item(s) considéré(s) en entrée et en sortie (termes, documents ou catégories), la tâche résolue par l\u0027application varie (re-cherche d\u0027information, extension de requête, extraction de mots clefs, classification automatique,...) mais il s\u0027agit toujours de sélectionner les items pertinents vis-à-vis de la requête en s\u0027appuyant sur un modèle de pertinence. Cette application est un moyen d\u0027expérimenter en ligne différents modèles de pertinence. Toutes les actions des utilisateurs sont enregistrées et stockées dans une base de données en vue d\u0027analyses comparatives ultérieures. Une première version de l\u0027application est déjà en ligne (http://echo.imag.fr/apps/echopedia/)."
  },
  {
    "id": "1320",
    "text": "L\u0027apprentissage multi-label extrême (noté XML pour \"eXtreme Multi-label Learning\") considère de grands volumes de données où chaque observation est annotée avec quelques labels parmi des centaines de milliers de possibilités. Les méthodes basées sur les arbres, qui divisent hiérarchiquement l\u0027apprentis-sage en sous-problèmes à petite échelle, sont particulièrement prometteuses dans ce contexte pour réduire les complexités d\u0027apprentissage et de prédiction et pour ouvrir la voie à la parallélisation. Cependant, les meilleures approches actuelles n\u0027exploitent pas la diversification des arbres qui a pourtant montré son efficacité dans les forêts aléatoires et elles ont recours à des stratégies de partitionnement complexes. Pour surmonter ces limites, nous introduisons ici un nouvel algo-rithme de forêt avec des arbres diversifiés et une stratégie de partitionnement adaptée à l\u0027XML appelé CRAFTML. Des comparaisons expérimentales sur huit jeux de données tirés de la littérature extrême montrent qu\u0027il est plus performant que les autres approches arborescentes de l\u0027état de l\u0027art."
  },
  {
    "id": "1322",
    "text": "L\u0027objectif de cet article est de rechercher les composés phénoliques des plantes qui pourraient avoir une action sur les microbes du rumen et limiter la production de méthane. Comme il y a une très grande diversité de structures chimiques, nous avons eu recours à la fouille de données pour faire émerger des composés susceptibles d\u0027avoir un effet significatif. La pertinence des règles d\u0027association a été améliorée grâce à une nouvelle mesure d\u0027intensité qui a per-mis de sélectionner quelques composés qui seront à identifier. Ainsi, parmi les 1075 composés inconnus présents dans le jeu de plantes, 26 ont émergé desquels 7 ont un effet seuil."
  },
  {
    "id": "1323",
    "text": "Le \"biclustering\" joue un rôle majeur dans beaucoup d\u0027applications du monde réel. Il est lié au \"clustering\" qui regroupe des lignes similaires dans une matrice de données numériques, tandis que le biclustering cherche à re-grouper simultanément des lignes et colonnes similaires, c\u0027est-à-dire trouver des sous-matrices où émerge une corrélation entre les entrées. Le biclustering s\u0027ap-puie sur un critère de similarité, et dans cet article, nous nous intéressons au biclustering \"à colonnes constantes\" (CC), où les valeurs numériques dans les colonnes des sous-matrices sont constantes pour chaque ligne. L\u0027étude est en-suite étendue au biclustering à \"changements de signes cohérents\" (CSC), où la différence entre les valeurs de deux colonnes consécutives de la sous-matrice est du même signe pour chaque ligne."
  },
  {
    "id": "1324",
    "text": "La découverte de sous-groupes permet d\u0027identifier des ensembles d\u0027ob-jets définis en intention qui sont intéressants vis-à-vis d\u0027une mesure de qualité impliquant un ou plusieurs attributs cibles (par exemple motifs discriminants pour une variable de classe). Dans cet article nous proposons une approche pour un nombre quelconque (≥ 2) d\u0027attributs cibles numériques. Pour cela, nous nous appuyons sur l\u0027exploration conjointe de motifs graduels identifiant des corréla-tions de rang et de sous-groupes afin d\u0027identifier des contextes pour lesquels les corrélations décrites par les motifs graduels sont exceptionnellement fortes par rapport au reste des données. Nous présentons un algorithme d\u0027énumération s\u0027appuyant sur des propriétés d\u0027élagage avec des bornes supérieures. Une étude empirique sur plusieurs jeux de données démontre la pertinence et l\u0027efficacité de notre méthode."
  },
  {
    "id": "1325",
    "text": "Dans ce papier, nous étudions le nombre maximum d\u0027ensembles fer-més dans un cube de données de taille n × n × n. Nous montrons qu\u0027il se situe entre 3.36 n et 3.38 n ."
  },
  {
    "id": "1326",
    "text": "Nous proposons une méthode d\u0027extraction de communautés ego-cen-trées reposant sur l\u0027apprentissage d\u0027un modèle de propagation prétopologique. Là où les méthodes classiques ne considèrent souvent qu\u0027un aspect de la structu-ration du réseau pour en extraire ses communautés, la prétopologie permet une analyse multi-critères du réseau. Notre démarche consiste à apprendre de fa-çon supervisée un espace prétopologique défini par une combinaison logique de descripteurs du réseau. Une communauté locale à chaque noeud peut alors être extraite par une opération définie sur l\u0027espace prétopologique appris.La qualité de chaque communauté locale est ensuite évaluée selon une communauté de ré-férence. Nous avons comparé notre approche aux approches existantes sur des réseaux synthétiques et du réel et montrons ainsi sa pertinence."
  },
  {
    "id": "1327",
    "text": "L\u0027analyse des données dynamique est difficile. En effet, la structure de telles données évolue dans le temps, potentiellement à une vitesse très rapide. De plus, les objets dans ces ensembles de données sont souvent complexes. Dans cet article, notre motivation pratique est d\u0027analyser l\u0027évolution des profils en ligne d\u0027utilisateurs, c\u0027est-à-dire de suivre la localisation géographique des utilisateurs ainsi que leurs traces de navigation en ligne afin de détecter des changements dans leurs habitudes et leurs intérêts. Nous proposons un nouveau cadre dans le-quel nous créons d\u0027abord, pour chaque utilisateur, des signaux de l\u0027évolution de ses intérêts et des localisations physiques enregistrées au cours de sa navigation. Ensuite, nous détectons automatiquement les changements d\u0027intérêt ou de lieu grâce à un nouvel algorithme de détection de sauts dans les signaux."
  }
]