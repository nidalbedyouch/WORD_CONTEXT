Introduction
Les réseaux sociaux font partie du quotidien, notamment en ce qui concerne la « consommation » de l'information (Mercier, 2014). Le service de microblogging Twitter a permis aux réseaux sociaux de prendre une nouvelle dimension car il permet d'évaluer les réactions de ses utilisateurs sur des sujets sociaux (Longhi et al., 2016), politiques (Conover et al., 2011), etc.
L'analyse des tweets politiques lors des campagnes électorales ou d'événements spéci-fiques peut être considérée comme un genre de discours politique spécifique (Longhi, 2013). Dans ce contexte, Roginsky et Cock (2015) proposent une analyse qualitative d'interactions sur Twitter, mais se limitent à une analyse discursive et communicationnelle. De plus, des nombreux travaux ont étudié la prédiction du résultat des élections présidentielles (ou pourquoi cela n'est pas possible) (Tumasjan et al. (2010), Gayo-Avello et al. (2011)).
Ces travaux, relevant des sciences informatiques ou des sciences humaines et sociales, se font parfois difficilement écho. Les résultats sont difficilement accessibles pour les citoyens, et les analyses accessibles sont déjà agrégées par les médias ou elles intègrent des traitements simples (comme le compte @TwitterFrance). Plus ciblés, le projet Semiotweet 1 propose des analyses très limitées linguistiquement par candidat (occurrence des mots), et la plateforme Politoscope 2 est surtout axée sur l'analyse des communautés autours des candidats à l'élection.
Cet article présente la plateforme #Idéo2017 qui, innovante par l'accès à l'information qu'elle permet aux citoyens, propose un outil d'analyse des tweets d'un ensemble de comptes Twitter emblématiques lors d'événements politiques spécifiques (pour l'élection présidentielle en 2017, les comptes officiels des 11 candidats). Dans le cadre des élections, #Idéo2017 traitait les tweets des candidats en constituant un corpus en quasi temps réel (mis à jour chaque 24 heures). Par l'utilisation d'outils et de fonctionnalités issues de la linguistique outillée, ce traitement souligne les principales caractéristiques du corpus et permet des comparaisons entre les différents candidats. De plus, #Idéo2017 permet également une navigation à facettes dans les tweets et une visualisation graphique intuitive, ainsi que l'extraction des corpus.
2 Description de la plateforme #Idéo2017 2.1 Description des fonctionnalités #Idéo2017 propose cinq fonctionnalités qui regroupent plusieurs traitements. La plateforme est adaptable à divers événements politiques et elle a été testée dans le cadre de l'élec-tion présidentielle 2017 avec les comptes des 11 candidats officiels (http://ideo2017. ensea.fr/plateforme/), des législatives 2017 avec les comptes des 9 partis politiques, et du quinquennat actuel avec les comptes du Président, des ministres et des représentants de l'opposition (liens disponibles en ligne).
Analyse par mot employé. Cette fonctionnalité permet à l'utilisateur de choisir un mot parmi les mots souvent employés dans la politique (Alduy (2017)). Dans le cas de l'éléction présidentielle, 13 mots sont étudiés : France, état, république, peuple, loi, travail, liberté, dé-mocratie, sécurité, immigration, terrorisme, islam et laïcité. Cet ensemble de mots est mis à jour en fonction de l'événement politique couvert par la plateforme avec des mots spécifiques.
Le choix du mot donne accès à quatre analyses possibles sur les tweets (exemple pour l'élection présidentielle) : (1) l'usage fait de ce mot par les différents candidats sous forme de sur-/sous-emploi du mot, (2) les mots associés à ce mot extraits à base d'une analyse de cooccurrences, (3) l'emploi du mot et de ses dérivés par les différents candidats, et (4) le nuage de mots pour afficher le lexique complet des tweets.
Analyse par compte. Cette analyse permet à l'utilisateur de choisir un compte Twitter et d'analyser ses tweets. En fonction de l'événement, ce compte peut être celui d'un candidat à l'élection présidentielle, d'un parti pour les législatives ou d'un ministre pour le quinquennat.
Les analyses linguistiques disponibles à réaliser sur le compte Twitter choisi sont les suivantes : (1) la détection des mots les plus utilisés, (2) l'émergence de thématiques en regroupant les mots proches sémantiquement ; cette analyse produit un graphe qui fait émerger les grands domaines lexicaux et dégage les principaux thèmes, (3) la détection des relations entre les mots qui permet d'analyser la similitude des mots sous forme graphique, (4) la mise en place d'un nuage de mots qui affiche le lexique des mots, et, (5) l'indentification des spécificités des différents comptes : les mots et catégories spécifiques, et de les comparer.
Moteur de recherche à base de facettes. Le moteur de recherche autonome et en temps réel, disponible dans la plateforme, permet à l'utilisateur de chercher librement sur les tweets de l'événement à l'aide de plusieurs facettes : les comptes, les hashtags et les mentions, et de trier les résultats selon la date ou l'engagement, mais aussi de connaître la répartition exacte du nombre de tweets par compte ou par grande thématique. Cette connaissance de la distribution de tweets nous offre une contextualisation globale pour chaque requête car notre objectif est autant de réaliser un moteur de recherche que d'offrir à nos utilisateurs une plateforme de veille concurrentielle entre les différentes stratégies de communication des comptes.
C. Marinica et al.
Visualisation graphique. En plus des analyses linguistiques, nous avons ajoutés des visualisations graphiques de données pour les comptes choisis à l'aide de l'outil de visualisation de données Kibana 3 . Pour cela, nous avons proposé, pour le compte choisi, quatre graphes représentant : les 20 hashtags les plus utilisés, les 20 mentions les plus utilisées, et l'évolution du nombre de tweets et du nombre de retweets par jour.
Extraction d'un corpus. Cette dernière fonctionnalité permet à l'utilisateur de télécharger un sous-corpus de tweets de l'ensemble global de tweets. Cette fonctionnalité est très importante pour les chercheurs en sciences humaines et sociales car ils n'ont pas accès à des outils leur permettant d'extraire et de structurer leur corpus avant utilisation.
Cas d'utilisation
#Idéo2017 est une plateforme destinée au plus grand nombre et facile d'utilisation. Un cas d'utilisation est décrit dans la suite. Considérons que l'utilisateur choisit de s'intéresser à l'élection présidentielle et au mot islam. Dans la suite nous allons décrire les étapes que l'utilisateur suit afin de comprendre l'utilisation de ce mot par les candidats à l'élection.
Etape 1. Etant donné l'intérêt pour un mot précis, la fonctionnalité "analyse par mot employé" est utilisée. Parmi les analyses proposées, l'utilisateur choisit de comparer le suremploi ou sous-emploi de ce mot par les différents candidats. L'utilisateur remarque que le mot islam est sous-employé par Marine Le Pen, et modestement employé par François Fillon.
Etape 2. Supposons maintenant que l'utilisateur soit surpris de ce résultat. Ainsi, la plateforme lui permet, toujours dans la même fonctionnalité, de chercher l'emploi de ce mot et de ses dérivés, et d'observer leur fréquence. Il verra ainsi que le nombre de tweets dans lesquels se trouve cette forme est très important chez ces deux candidats.
Etape 3. Ceci se confirme d'ailleurs en regardant les mots associés au mot islam, proposés sous forme graphique. Ce mot est en effet très lié à différents réseaux ou noeuds : (1) islamisme / immigration / communautarisme, (2) islamiste / fondamentaliste / idéologie / attentat / terrorisme, et (3) islamique / totalitarisme / Syrie.
Etape 4. Devant ces résultats, un retour au corpus s'impose pour l'utilisateur, et il est rendu possible grâce au moteur de recherche ; il permet à l'utilisateur de chercher le mot qui l'intéresse, et les propositions de tweets s'affichent dynamiquement avec un accès direct aux tweets. Ceci confirme le résultat graphique obtenu dans les premières requêtes, puisqu'il est question, dans le tweet de Marine Le Pen, de terrorisme islamiste.
Développement
Pour la mise en place de la plateforme, nous avons suivi plusieurs étapes : (1) l'extraction de l'ensemble de tweets, (2) la mise en place d'un sauvegarde des tweets, (3) l'indexation des tweets pour faciliter la recherche dans l'ensemble de tweets, (4) l'application d'un ensemble d'analyses linguistiques sur les tweets (avec l'utilisation et la modification des scripts d'IRaMuTeQ 4 ), (5) la mise en place d'un moteur de recherche sur l'ensemble de tweets, et (6) l'affichage des résultats sur une page web. Afin de traiter un grand nombre de tweets, nous avons utilisé Elasticsearch 5 pour la mise en place d'un index sur les tweets. Celui ci est exploité dans l'implémentation des analyses linguistiques et la mise en place du moteur de recherche à facettes. Elasticsearch permet la sélection rapide et en temps réel des tweets contenant un mot spécifique ou résultant du croisement entre les recherches simple et par facettes.
Conclusion
Cet article a présenté la plateforme #Idéo2017 qui, adaptable aux différents événements politiques, permet aux citoyens d'analyser par eux-mêmes les tweets politiques en lien avec l'événement. Elles permettent aux citoyens d'appréhender les données sociales relatives à des événements, et de leur donner du sens. Plusieurs versions de la plateforme ont été testées : pour l'élection présidentielle, les législatives, et le quinquennat.

Introduction
La recommandation de points d'intérêts (ou POI pour Point Of Interest) consiste à proposer à un utilisateur une liste de POI qu'il pourrait être intéressé de visiter. Ce problème est devenu une composante majeure des réseaux sociaux géo-localisés (ou LBSN pour Location-Based Social Networks) permettant aux utilisateurs de découvrir de nouveaux POI mais également aux POI d'augmenter leur attractivité. Il faut toutefois tenir compte de différentes caractéristiques des LBSN, à savoir : (i) sparsité : la densité de la matrice utilisateur-POI est très faible, (ii) fréquences de visite : nous savons seulement combien de fois un utilisateur a été localisé dans un lieu, et (iii) feedback implicite : nous ne disposons pas d'une évaluation explicite du POI par l'utilisateur.
Nous considérons ici le cas où les visites des utilisateurs couvrent une large éten-due géographique pouvant aller jusqu'au monde entier. Quand la surface couverte par les données augmente les données deviennent de plus en plus creuses car le nombre de POI augmente tandis que le nombre moyen de POI visités par utilisateur reste constant. Il résulte que la densité (i.e. , la proportion de POI visités par utilisateur) diminue. Pour résoudre ce problème de faible densité nous avons étudié des solutions pour augmenter la densité des données sans réduire la zone géographique couverte. Notre idée est de définir un ensemble de superPOI chacun représentant un groupe de POI. L'ensemble des superPOI constitue une structure hiérarchique incluant d'autres superPOI. Le second problème à résoudre est de tenir compte des fluctuations des comportements de mobilité des utilisateurs, pouvant aller d'une échelle géographique locale (une ville) à globale (le monde). Notre principale motivation est de nous assurer que notre modèle de recommandation disposera de suffisamment d'informations pour chaque utilisateur. Notre approche ne se limite pas à 2 classes et peut s'adapter à un nombre de classes quelconque en fonction des données.
Ainsi les contraintes de densité voudraient que nous agrégions les POI dans des superPOI, et en même temps les comportements de mobilité exigent de conserver suffisamment de POI locaux. Pour trouver le bon compromis nous proposons une solution unifiée qui s'appuie sur notre définition de superPOI de façon à agréger les POI de manière flexible (c.f. section 5) et également qui classifie les utilisateurs de façon à trouver le meilleur niveau d'agrégation (c.f. section 5). Nous pouvons résumer les contributions auxquelles nous avons abouti ainsi :
-Une approche probabiliste de factorisation qui passe à l'échelle.
-Une structure hiérarchique pour définir plusieurs niveaux de superPOI.
-La définition de différents comportements de mobilité des utilisateurs.
-Des résultats expérimentaux qui confirment l'efficacité de notre approche sur un jeu de données de grandes dimensions. Ye et al. (2011) ont proposé une méthode de filtrage collaboratif (CF) memory-based qui intègre les influences sociale et géographique. En particulier ils ont mis en évi-dence que le facteur géographique jouait un rôle essentiel dans la qualité du modèle. Malheureusement cette approche n'est pas capable de traiter des jeux de données à grande échelle. De nombreuses méthodes CF model-based ont aussi été proposées notamment par Zhang et Wang (2015); Hu et al. (2008). Parmi ceux-ci la classe de mé-thodes la plus exploitée est la classe des modèles de factorisation de matrice tels que présentés par Zhang et Wang (2015); Griesner et al. (2015). La factorisation de matrice vise à exprimer l'interaction utilisateur-objet en grâce à des vecteurs d'attributs latents. La factorisation de matrice probabiliste de Salakhutdinov et Mnih (2007) (PMF) est une approche qui vise à minimiser une fonction objectif des moindres carré avec des termes de régularisation quadratiques. Plus récemment Griesner et al. (2015) ont proposé un modèle pour traiter le problème de l'évaluation implicite. Leurs résultats étaient prometteurs mais la complexité de leur approche empêchait cette méthode de passer à l'échelle.
État de l'art
L'un des principaux problèmes est l'absence d'évaluation explicite de l'expérience utilisateur. Pour traiter les jeux de données implicites un modèle a été proposé il y a quelques années par Hu et al. (2008). Ce modèle distingue les préférences de l'utilisateur et la confiance que nous pouvons avoir dans cette préférence. Les auteurs ont démontré que leur approche était plus efficace que d'autres méthodes existantes sur de petits jeux de données. Cette méthode a été exploitée et enrichie par Lian et al. (2014) pour intégrer l'influence géographique des POI en modélisant le phénomène de clustering spatial directement dans le processus de factorisation. Cependant la complexité est trop élevée pour l'appliquer sur des jeux de données réels. La factorisation de Poisson a été proposée récemment par Gopalan et al. (2013)  
Modèle des données
La plupart des LBSN disposent d'un ensemble d'utilisateurs U , d'un ensemble de POI P, d'informations temporelles T et sociales S. Dans ce contexte un utilisateur u peut faire des check-ins (i.e. des visites) à un POI donné p à l'instant t.
Definition 1 (POI) Un point d'intérêt correspond à un site unique associé à une activité spécifique (e.g. musée, restaurant, université...). Nous notons P l'ensemble des POI. Ici nous supposons que nous n'avons accès qu'aux localisations, i.e. , aux paires (latitude,longitude) pour chaque POI.
Definition 2 (Check-in) Le check-in de l'utilisateur u visitant un POI p à l'instant t est associé au tuple <u,p,t>. Ainsi pour calculer la fréquence de visite de u à p il suffit d'agréger le nombre de check-ins correspondant. Etant donné que chaque POI est associé au moins à un superPOI, chaque check-in incrémente également la fréquence de visite de ce superPOI.
Definition 3 (SuperPOI) L'agrégation de plusieurs POI ou superPOI sur une zone géogra-phique unique constitue un superPOI. L'ensemble des superPOI est un ensemble de POI ou superPOI. Les zones de chaque superPOI sont disjointes deux à deux (c.f. section 5).
Definition 4 (Profil utilisateur)
Chaque utilisateur dispose d'un profil qui correspond à l'ensemble de tous ses check-ins : P u ={<u,p i ,t j >∈D}. L'agrégation de tous les profils utilisateur constitue le jeu de donnée complet D={P u /u∈U }. Definition 5 (AGRA) Notre graphe d'accessibilité AGRA, noté G=(V,E,ρ) est un graphe orienté où chaque noeud v∈V représente un POI associé à ses coordonnées géographiques, chaque arête e=(p i ,p j )∈E n'existe que si la transition p i →p j est observée dans au moins un itinéraire utilisateur et ρ est une fonction qui associe à chaque arête e=(p i ,p j ) son accessibilité correspondante A i,j (définie à l'équation 3).
Problem 1 Recommandation de POI à large échelle : Étant donnée une collection de check-ins D de faible densité distribuée sur le monde entier, l'objectif est de fournir à un utilisateur u une top-k liste de POI qu'il pourrait avoir envie de visiter.
Modèle des influences géographique et sociale
Cette section présente GeoSPF (i.e. , Geographical Social Poisson Factorization), notre méthode pour extraire les influences sociales implicites à partir des comportements de mobilité géographique. À cet effet nous introduisons notre graphe d'accessibilité (AGRA) que nous utilisons pour notre modèle géographique.
Idée générale
GeoSPF est basé sur l'hypothèse que le choix de l'utilisateur dépend d'une combinaison de préférences géographique, sociale et personnelle. Si nous notons α(u,p) le degré d'intérêt qu'un utilisateur u a pour un POI p, S(u,p) l'influence sociale que u a obtenu pour p, et G(u,p) la préférence géographique de l'utilisateur u concernant p, la probabilité d'observer la paire (u,p) dans les données devrait être directement proportionnelle à l'intérêt de u pour p, et diminuer de façon monotone, tel que :
où F[·] est une fonction qui combine les intérêts personnels, l'influence sociale et l'influence géographique. Les approches existantes proposées par Lian et al. (2014); Griesner et al. (2015) ont vérifié que l'influence géographique a un impact significatif sur la qualité de la recommandation. Cependant ils utilisent en général un espace isotrope uniforme et exploitent seulement les distances entre les check-ins. Ainsi de telles approches ne tiennent pas compte des contraintes naturelles qui pourraient rendre la mobilité entre deux POI difficile même s'ils sont proches les uns des autres. Au-delà des distances, nous introduisons ici le concept d'accessibilité, comme nous le verrons dans la section 4.2, pour mieux intégrer l'influence géographique dans les choix des utilisateurs. Les étapes principales de GeoSPF sont les suivantes : (i) Nous construisons un graphe d'accessibilité AGRA basé sur les transitions observées (d'un POI à l'autre) et leurs probabilités, puis (ii) nous construisons un réseau social implicite ISN à partir de AGRA et des similitudes entre les historiques de transitions des utilisateurs, et enfin (iii) nous intégrons l'ISN dans un modèle de recommandation de factorisation sociale de Poisson pour obtenir notre modèle GeoSPF.
Accessibilité géographique
L'idée de l'accessibilité consiste à modéliser la probabilité qu'un utilisateur se dé-place vers un POI p j+1 après avoir visité le POI p j . Pour ce faire nous appliquons un modèle de Markov de premier ordre. Une transition est observée dans l'itiné-raire d'un utilisateur u s'il existe dans le jeu de données deux check-ins consécu-tifs <u,p i ,t 1 > et <u,p j ,t 2 > effectués dans deux POI différents p i et p j à deux timestamps t 1 et t 2 , tels que t 1 <t 2 et si aucun autre enregistrement intermédiaire <u,p k ,t > (t 1 <t <t 2 ) ne se trouve dans le jeu de données. Nous notons cette transition comme suit : p i →p j dans le reste de l'article. Ainsi pour un utilisateur donné la probabilité de visiter p j+1 sera déduite de sa dernière visite. Plus formellement nous avons P(p j+1 |p j ,p j−1 ,...,p 1 )=P(p j+1 |p j ) où nous définissons P(p j+1 |p j ) comme probabilité de transition T j,j+1 de p j à p j+1 . Nous pouvons calculer cette probabilité en utilisant l'estimation empirique du maximum de vraisemblance comme suit : 
où T j,j+1 fait référence à l'équation 2 et d(p j ,p j+1 ) est la distance euclidienne entre les POI p j et p j+1 . Si p j+1 est loin de p j l'accessibilité sera faible. Mais lorsque beaucoup de transitions sont observées de p j à p j+1 l'accessibilité augmente. L'équation 3 est inspirée des poids géographiques utilisés par Liu et Xiong (2013). La valeur de 0,5 au dénominateur est déterminée empiriquement en fonction des spécificités des données. Cela signifie que nous accordons plus d'importance aux distances inférieures à 500 mètres. Nous utiliserons ensuite cette accessibilité pour définir notre Jaccard symé-trique pondéré d'accessibilité dans la sous-section 4.3.
AGRA : Graphe d'accessibilité
Des travaux précédents tels que Ma et al. (2011); ont montré que les influences sociales jouaient un rôle important dans la qualité finale de la recommandation de POI. Cependant en général dans les LBSN nous n'avons pas accès à un réseau social explicite : nous avons seulement accès à l'historique des check-ins. Ainsi notre approche vise à construire un réseau social implicite (implicit social network ou ISN) basé sur la similarité entre les profils des utilisateurs et leurs transitions dans le graphe AGRA. Nous définissons ci-dessous quatre mesures de similarité possibles choisies pour leur flexibilité et la qualité de leurs résultats. Adamic/Adar : Cette mesure accorde une grande importance aux transitions rares (c'est-à-dire avec une faible accessibilité). Intuitivement plus les deux utilisateurs partagent des POI impliqués dans des transitions rares, plus on s'attend à ce qu'ils soient proches. Ainsi avec D(·) la fonction donnant le degré d'un noeud, nous définissons :
Jaccard standard : Il s'agit de la mesure de Jaccard standard. Ainsi nous définissons la similarité de Jaccard S J (u 1 ,u 2 ) de deux utilisateurs u 1 et u 2 comme suit :
Jaccard pondéré symétrique : Avec cette mesure nous étendons la mesure Jaccard standard en considérant l'accessibilité entre les points d'intérêt visités. Pour ce faire nous ajoutons à l'ensemble des POI visités ceux (Γ(P u )) qui sont accessibles en un seul bond en parcourant AGRA. Soit G=Γ(P u 1 )∪Γ(P u 2 ) l'ensemble des POI visités par u 1 ou u 2 . Soit N=|G|.
De même nous construisons le vecteur ρ u 2 . Il s'agit d'une métrique symétrique. Ainsi nous définissons la similarité de Jaccard symétrique pondérée par l'accessibilité S AWS (u 1 ,u 2 ) comme suit :
Jaccard pondéré antisymétrique : Dans cette métrique nous essayons de prendre en compte l'asymétrie d'influence qui pourrait exister entre deux utilisateurs. Pour ce faire nous changeons la définition de G comme suit : G=Γ(P u 1 )∪P u 2 . Au lieu d'étendre les deux ensembles P u 1 et P u 2 nous étendons seulement l'ensemble des POI visités par l'utilisateur u 1 . Ensuite nous calculons le Jaccard pondéré antisymétrique S AWA (u 1 ,u 2 ) en utilisant l'équation 6. Notons que S AWA (u 1 ,u 2 ) =S AWA (u 2 ,u 1 ).
GeoSPF : Modèle de factorisation
La factorisation de Poisson a été proposée par Gopalan et al. (2013); Chaney et al. (2015). Le but des approches de factorisation est d'approximer la matrice utilisateur-POI X représentant les fréquences de visite, par le produit scalaire de facteurs latents tels que : X≈UV T , où U∈R m×k et V∈R n×k avec kmin(m,n). La factorisation de Poisson (PF) est une approche générative probabiliste basée sur une loi de Poisson pour modéliser les observations. Il s'agit d'un modèle rapide et adapté aux données creuses. Récemment Chaney et al. (2015) en ont proposé une extension appelé SPF (Social Poisson Factorization). L'extension SPF aboutit à de bons résultats pour la recommandation de POI car SPF possède des propriétés intéressantes qui correspondent à nos besoins en termes de qualité et de passage à l'échelle. De plus SPF permet d'inté-grer l'information sociale qui est importante dans notre contexte. Enfin SPF sépare les questions : qui est membre du cercle ? et quelle influence ce membre transmet-il réellement ? SPF suppose que l'appartenance au cercle est connue à l'avance alors que le degré d'influence est appris. Cette séparation est essentielle ici car le degré d'influence d'un utilisateur ne dépend pas des POI qu'il partage avec les autres utilisateurs mais plutôt des interactions cachées (non divulguées) que les utilisateurs peuvent avoir. GeoSPF s'appuie sur la distribution suivante :
où V(i) fait référence à l'ensemble des voisins de l'utilisateur i dans l'ISN, et s i,k fait ré-férence au facteur latent d'influence sociale. Cette variable modélise l'influence que le voisin k a sur l'utilisateur i. Le choix du voisinage V(i) est important car V(i) contiendra tous les voisins les plus proches de l'utilisateur i. Ainsi contrairement à SPF nous n'utilisons aucun réseau social explicite. De plus nous pouvons ajuster la qualité de la recommandation en fonction des métriques de similarité. Enfin nous pouvons éga-lement appliquer des filtres de seuillage soit sur le graphe d'accessibilité, soit sur le graphe social implicite.
Modèle des influences locale et globale
Notre modèle Augmented Local-Global GeoSPF (noté ALGeoSPF) consiste à définir des strates locales et globales de superPOI afin d'augmenter la densité des données. Le principal avantage d'ALGeoSPF est sa capacité à détecter différents comportements de mobilité, des échelles locale jusqu'à mondiale. Un autre avantage est qu'elle permet d'isoler toutes les étapes du processus de recommandation (i.e. , inférence, apprentissage, prédiction du réseau social) au sein de chaque classe d'utilisateurs. On évite ainsi toute propagation de bruit à travers les classes. 
Strates hiérarchiques de SuperPOI
Notre agrégation de SuperPOI permet de traiter des jeux de données de faible densité à grande échelle en agrégeant des portions des jeux de données d'origine. On note P l'ensemble de toutes les strates de superPOI. Nous définissons une structure multi-strates pour agréger progressivement chaque POI en superPOI visité par un nombre croissant d'utilisateurs. La figure 1 représente une illustration de cette structure. Cela permet de répondre aux exigences de densité de notre problème pour diffé-rentes classes d'utilisateurs. Soit k le nombre de strates, P k l'ensemble des superPOI définis au niveau k et N k max le nombre maximum d'utilisateurs visitant un superPOI de P k . La condition suivante définit la zone maximale qu'un superPOI représente, en notant p un superPOI : ∀p∈P k ,N(p)<N k max avec N(p) étant le nombre total d'utilisateurs distincts visitant le POI p. De plus chaque niveau vise à agréger les POI autant que possible pour s'assurer que chaque p n'est pas "trop petit", c'est-à-dire qu'il n'y a pas p dans la strate supérieure P k+1 tel que p agrège p et p satisfait N(p )<N k max .
Algorithme de clustering géographique
Nous adoptons une approche de clustering qui consiste à diviser l'espace géogra-phique initial (par exemple le monde entier pour le jeu de données à grande échelle YFCC) en plusieurs cellules rectangulaires identiques. Les approches traditionnelles, comme proposé par Al-Ghossein et Abdessalem (2016), sont basées sur les quad-tree : il s'agit de construire un arbre où la racine est la carte du monde entier et chaque noeud est le quart de sa région parente. Nous divisons récursivement une cellule c jusqu'à ce qu'elle satisfasse la condition concernant le nombre d'utilisateurs différents qui ont fait des check-ins dans cette cellule : N(c)<N max . Les cellules satisfaisant cette condition sont choisies pour être un superPOI. Le résultat de l'algorithme de clustering est un ensemble de cellules superPOI notées S. Le paramètre N max permet de contrôler le niveau d'agrégation. Nous spécifions l'état d'une cellule superPOI basée sur N(·) au lieu du nombre de POI car ainsi on détecte mieux les comportements de mobilité des utilisateurs lorsque de nombreux POI populaires sont proches les uns des autres dans une zone comptant un petit nombre de POI, ce qui arrive relativement souvent. L'algorithme est présenté dans l'algorithme 1.
Sélection personnalisée de classe
Au moment où un utilisateur attend une recommandation, nous nous appuyons sur l'algorithme de clustering pour ajuster (c'est-à-dire pour optimiser) les deux paramètres qui affectent la densité du jeu de données : la cellule initiale et la taille du cluster. La cellule initiale est la cellule sur laquelle appliquer le clustering. La taille du cluster est définie par N max . Lorsque N max augmente les superPOI seront plus vastes mais il y en aura moins, ce qui augmentera la densité du jeu de données. Cependant N max est borné : pour chaque utilisateur il existe un maximum N max (noté N user max ) audelà duquel la recommandation n'est plus possible car l'utilisateur n'aura pas visité suffisamment de superPOI distincts. else Mettre C i dans S En réglant ces paramètres nous avons constaté que leur impact sur la densité varie beaucoup selon les utilisateurs, ce qui justifie notre approche personnalisée. Pour certains utilisateurs une cellule initiale plutôt petite (e.g. Paris) donne une densité éle-vée. Pour d'autres, bien que Paris soit la cellule initiale (car la cellule Paris contient tous les check-ins de l'utilisateur), la cellule initiale France permet d'avoir N max et permet une densité plus élevée. Plus généralement nous avons observé qu'une telle méthode d'optimisation permet de détecter plusieurs classes d'utilisateurs partageant le même couple quasi-optimal de paramètres (cellule initiale, N max ). Nous observons que chaque utilisateur appartient à l'une ou l'autre de ces classes. Ainsi nous associons chaque utilisateur à son optimal personnalisé N user max qui est le N max qui maximise la densité des check-ins distincts dans les clusters.
Évaluation expérimentale
Nous avons conduit des expériences sur trois jeux de données issus du monde réel contenant des check-ins provenant de LBSN largement utilisés, à savoir : YFCC, Gowalla et Foursquare. Afin d'évaluer la qualité à différentes échelles géogra-phiques nous avons filtré les jeux de données de façon à ce qu'ils couvrent respectivement une petite, une moyenne et une grande superficie. Ainsi Gowalla@Paris couvre une ville, Foursquare couvre une région, Gowalla couvre un pays et YFCC couvre l'Europe. Le jeu de données YFCC a été récemment proposé par Thomee et al. (2016 La figure 3 présente les performances globales obtenues par chaque méthode comparatives listées ci-dessus. Sur la figure 3a le Rappel@5 et le Rappel@10 sont présentés pour le jeu de données Foursquare. La figure 3b (resp. 3c) concerne Gowalla@Paris (resp. Gowalla). Enfin la figure 3d présente la métrique NDCG@5 sur les trois jeux de données. Conformément à nos prévisions, NMF et PMF donnent une qualité relativement mauvaise étant donné qu'ils ont été conçus pour opérer sur des jeux de données explicites. Cette observation est donc cohérente avec les résultats de Liu et Xiong (2013). De même SLIM ne parvient pas à fournir une qualité satisfaisante car il n'est opérant, lui aussi, que sur les jeux de données contenant une évaluation explicite de la part de l'utilisateur. Malheureusement la complexité de WRMF le rend pratiquement inutilisable sur de grands jeux de données : le temps de calcul de WRMF rend sont usage en situation réelle prohibitif. Comme résultat majeur de nos expériences nous observons que l'avantage relatif de GeoSPF sur tous les jeux de données est d'environ 200%. Ce gain notable rend GeoSPF approprié pour la recommandation de POI sur de vastes zones géographiques. Il confirme que l'exploitation d'informations contextuelles restreintes (uniquement le GPS et la date d'enregistrement) par le biais d'une solution géographique et sociale combinée donne une qualité finale élevée.
La figure 2 présente la qualité d'ALGeoSPF appliqué sur le jeu de données YFCC en considérant les utilisateurs urbains isolés des globetrotters. Plus précisément la figure 2a présente le rappel@10 de GeoSPF et d'ALGeoSPF pour différentes tailles moyennes du réseau social implicite. La figure 2b présente le rappel@5 et le rappel@10 des autres méthodes ainsi qu'ALGeoSPF sur le jeu de données YFCC (en utilisant une taille de réseau social fixe de 80). Nous observons qu'ALGeoSPF surpasse les autres méthodes bien que BPR soit très proche. Nous observons également que les mesures de rappel des modèles testés pour le jeu de données YFCC sont bien plus faibles que d'autres jeux de données, ce qui est dû à la faible densité des données, elle-même directement liée à l'étendue de la zone géographique couverte. 
Conclusions
Dans cet article nous avons proposé une nouvelle approche nommée ALGeoSPF qui passe à l'échelle pour la recommandation de POI dans les LBSN. L'objectif principal d'ALGeoSPF est de construire un modèle qui ne souffre pas de la faible densité des données des LBSN et qui prenne en compte les comportements de mobilité spé-cifiques des utilisateurs. Basés sur les concepts de superPOI et d'accessibilité que nous avons introduits, notre approche (i) construit efficacement un modèle de factorisation implicite large-échelle et (ii) intègre les préférences de mobilité de l'utilisateur dans une structure hiérarchique et enfin (iii) présente de meilleurs résultats que la plupart des approches existantes sur des jeux de données de grandes dimensions. Notamment nous avons démontré par des résultats expérimentaux qu'ALGeoSPF surpasse de façon significative toutes les approches alternatives testées en termes de rappel et de NDCG. Nous observons en particulier que nous sommes parmi les premiers à tester une approche de recommandation de POI sur le jeu de données YFCC.

Introduction
L'analyse des sentiments (AS) devient un domaine d'étude très ouvert à la recherche. L'objectif étant d'analyser, à partir des textes partagés sur les réseaux sociaux, les opinions, les sentiments, les attitudes et les émotions des communautés sur différents sujets. En général, on distingue deux catégories d'approches pour l'AS des textes publiés sur les réseaux sociaux : la première est basée sur le lexique, consiste à utiliser une collection prédéfinie de mots et d'annoter chacun avec une valeur traduisant sa polarité (sentiment positif, négatif ou neutre). La deuxième est basée sur des techniques d'apprentissage automatique. L'AS dans ce cas peut être vu comme étant un problème de classification supervisée de texte. Sur les réseaux sociaux, comme Facebook, les commentaires partagés en arabe prend généralement la forme de l'Arabe Standard Moderne (ASM) ou l'Arabe dialectal (Duwairi et Qarqaz, 2014). Nous nous intéres-sons dans ce travail à l'AS à partir des commentaires Facebook écrits en ASM ou en Arabe Dialectal Marocain (ADM) en utilisant une approche basée sur l'apprentissage automatique.
Nos principales contributions dans ce travail consistent à : décrire les propriétés de la langue ASM et surtout l'ADM et leurs défis pour l'AS ; présenter un ensemble de techniques de prétraitement des commentaires Facebook écrits en ASM et en ADM pour l'AS et finalement construire et sélectionner des entités (mots ou séquence mots) des commentaires permettant d'obtenir le meilleur modèle de classification des sentiments.
Le reste de cet article est organisé comme suit : dans la section 2, nous décrivons le processus d'apprentissage automatique proposé et son application aux commentaires Facebook écrits en ASM et en ADM. Nous présentons également les méthodes de sélection et d'extraction des variables (mots ou séquences de mots) utilisées dans la phase de classification. Les résultats des expérimentations sont donnés dans la section 3. Une conclusion et quelques perspectives de ce travail sont présentées dans la section 4. 
Collecte des commentaires Facebook
Le processus d'apprentissage automatique proposé est testé sur des commentaires Facebook écrits en ASM ou en ADM, sur les élections législatives marocaines ayant eu lieu le 7 octobre 2016. Nous avons ciblé des journaux marocains qui publient en ligne des commentaires en langue arabe (moderne ou dialectal). La collecte de ces commentaires a été effectuée utilisant l'API : "Facebook Graph API" sur une période de 70 jours et nous a permis de sélec-tionner 10254 commentaires. Pour une libre exploitation, nous avons publié la base de données dans (ElecMorocco, 2017).
Traitement des commentaires Facebook
Le traitement du texte des commentaires est une phase critique dans le processus d'analyse des sentiments étudié. L'objectif étant d'extraire des variables (sous forme de mots ou de séquences de mots) pour les utiliser dans la classification. La qualité du traitement a donc clairement un impact majeur sur les performances des modèles de classification et les résultats obtenus à la fin du processus.
On commence cette étape par un nettoyage et une normalisation du texte : suppression des signes, des symboles, des lettres répétées, des mots vides ou des mots qui ne fournissent aucune information sur le sujet étudié. La tâche suivante est l'opération de tokénisation par laquelle le texte du commentaire est divisé en unités lexicales (tokens). Dans un texte en Arabe moderne ou dialectal, ces unités sont plus complexes puisqu'elles sont composées souvent de plus d'un mot, d'où l'importance de la tâche de désuffixation ou racinisation. Pour développer et appliquer un racinisateur sur les commentaires collectés, nous nous sommes inspirés du racinisateur light10 de (Larkey et al., 2007) qui propose l'élimination des préfixes et suffixes les plus courants d'un token dans le langage arabe moderne, sur lequel nous ajoutons une extension pour l'arabe dialectal marocain .
Le tableau suivant (Table. 1) présente un exemple de prétraitement d'un commentaire.
TAB. 1 -Exemple de prétraitement d'un commentaire.
Extraction et sélection de variables
L'AS par apprentissage supervisé nécessite la détermination au préalable de la polarité des opinions exprimées dans le texte pour construire la variable objectif. L'annotation des commentaires collectés a été effectuée en utilisant le crowdsourcing. Cette tâche a été confiée à un groupe de volontaires pour définir la polarité des commentaires, positifs ou négatifs. Au final, 6581 commentaires ont été annotés négatifs et 3673 positifs. Les variables d'entrée sont automatiquement extraites depuis le corpus formé à partir des commentaires prétraités utilisant les schémas d'extraction n-gram et de pondération TF / TF-IDF.
Des études telle que celle de (Pang et al., 2008) ont montré que la qualité des modèles de classification dépend des spécificités des données utilisées. Pour cela, nous avons testé six combinaisons de schémas d'extraction et de pondération pour garantir la meilleure qualité des modèles développés.
Afin de réduire la dimensionnalité et améliorer la qualité des modèles de classification, une méthode de sélection de variables a été utilisée. Il s'agit du score « somme des carrés intergroupe à intra-groupe » (BSS / WSS) utilisé dans (Dudoit et al., 2002;Sehgal et al., 2006), pour sélectionner les mots ou les séquences de mots les plus discriminants. Le score permet de classer les variables par ordre de pertinence. Une fois l'ordre établi, on choisit le sous ensemble optimal de mots par la méthode pas à pas de type forward.
Classification des commentaires
Pour classer les commentaires Facebook, nous avons appliqué trois algorithmes de classification supervisée(implémentés sur le logiciel R) : Naïve Bayes(NB), les Forêts Aléatoires (FA) et les Machines à Vecteurs Support (SVM).
Résultats et discussion
La combinaison des schémas d'extraction et de pondération nous a permis de tester six configurations différentes (six jeux de données), pour lesquelles nous avons appliqué le score de sélection de variables. Chaque jeu de données était devisé en trois sous-ensembles : 50% pour l'apprentissage, 25% pour la validation et 25% pour le test. Le tableau ( Pour chaque configuration, nous avons présenté le meilleur taux de bon classement (TBC), obtenu sur la base de l'échantillon de validation. Le graphique (Fig. 2) présente l'évolution des TBC de la configuration [Unigram+Bigram/TF], pour les trois algorithmes utilisés, en fonction du nombre de variables insérées dans l'ordre décroissant de pertinence. Le tableau (Table. 2) présente aussi les TBC obtenus sur l'échantillon test ainsi que ceux calculés en présence de toutes les variables (sans sélection préalable de variable). 
Conclusion
Ce travail a porté sur l'AS en utilisant les commentaires Facebook écrits et partagés en ASM ou ADM. Le processus proposé est appliqué à des données relatives aux élections lé-gislatives marocaines de 2016. Plusieurs combinaisons de schémas d'extraction (n-gram) et de pondération (TF / TF-IDF) pour la construction des variables ont été testées pour garantir les meilleures performances des modèles de classification développés. Les résultats ont montré que la qualité des modèles dépend des sous-ensembles de variables constitués à partir de la combinaison des schémas d'extraction et de pondération. L'application d'une méthode de sélection de variables nous a permis de réduire les dimensions tout en gardant un niveau de performance similaire ou meilleur.
La taille de l'ensemble de données utilisé dans ce travail est relativement réduite. Pour avoir des conclusions plus solides, nous prévoyons de construire une base de commentaires plus importante et d'implémenter notre approche dans un environnement distribué (en utilisant le Framework Hadoop (Nodarakis et al., 2016b)  
Summary
Sentiment analysis is a process during which the semantic orientation or polarity (i.e. positive, negative or neutral) of a given text is determined. This work deals with the sentiment analysis for Facebook's comments written in Arabic Modern Standard or Moroccan Dialectal from a Machine Learning perspective. The process starts by collecting and preparing the Arabic Facebook comments that we have annotated using crowdsourcing. Then, several combinations of extraction and weighting schemes for features construction was conducted to ensure the highest performance of the developed classification models. In addition, to reduce the dimensionality and improve the classification performance, a features selection method is applied. Our Machine Learning approach was implemented with the purpose of analysing the Facebook comments, written in Modern Standard Arabic or in Moroccan Dialectal Arabic, on the real data.

Introduction
Sur le Web, l'utilisateur est démuni face à de très grands volumes de documents de qualité assez inégale et d'une fiabilité parfois douteuse. Le résumé automatique peut permettre aux humains de mieux appréhender ces données surabondantes Notre objectif est d'utiliser le résumer automatique de corpus pour construire une base de connaissances fiable permettant l'identification d'incohérences dans un nouveau texte. Cette approche est illustrée dans la figure 1 : elle consiste à extraire d'une grande quantité de textes les faits redondants en se basant sur l'idée qu'ils représentent des connaissances consensuelles et donc aptes à permettre de façon fiable l'identification d'incohérences dans un nouveau texte. En plus d'un début de modèle de détection d'incohérence, ce travail peut être vu comme une contribution à la tâche de résumé automatique de corpus de textes ; dans cet article, nous le présentons et l'évaluons comme tel.
La principale originalité de l'approche réside dans l'exploitation de structures appelées CDS (pour Clause Description Structure) issues d'un composant propriétaire produisant des  Mani (2001). Cette approche peut être divisée en deux tâches : la représentation sémantique de la phrase et la génération de textes. Les travaux liés à cette approche sont prometteurs mais plus exploratoires et ne sont que rarement mis en oeuvre en contexte industriel.
Assez peu de travaux ont à notre connaissance été menés pour exploiter des représenta-tions du texte de plus haut niveau, comme une analyse en rôles sémantiques Saggion et Poibeau (2013). Quelques études préliminaires ont pourtant montré des résultats encourageants : Trandab˘ a¸ta¸t (2011) exploite les rôles sémantiques, la résolution d'anaphores et les relations de discours, Salim et al. (2010) combine l'analyse en rôles sémantiques à des méthodes statistiques pour déterminer les phrases à sélectionner dans une approche extractive, Khan et al. (2015) étend ce travail à une approche abstractive.
Machine Reading
Dans l'approche que nous présentons, les textes considérés sont traités à l'aide de la technologie de Machine Reading (MR), conçue par Synapse. Les sorties produites par le MR sont proches de celles d'un composant d'annotation en rôles sémantiques Gildea et Jurafsky (2002) (Semantic Role Labeling) et sont décrites dans (Laurent et al., 2015). En résumé, ce traitement appliqué sur un texte produit un ensemble de structures appelées CDS (pour Clause Description Structure) décrivant chacune une clause (une unité lexicale portant sur une formule actancielle) identifiée dans le texte analysé sous forme de prédicat appliqué à des arguments (sujet, objet, compléments). Le composant de Machine Reading a été développé pour le français et l'anglais. Les expérimentations décrites plus bas portent exclusivement sur la langue française.
Méthodes mises en place
Nous avons développé trois méthodes différentes capables d'extraire un sous-ensemble d'un groupe de CDS dans le but de produire un résumé de leur contenu : une méthode très simple constituera une baseline, une autre, SumBasic, est une adaptation au format des CDS d'une méthode de référence en résumé automatique de texte, et la troisième exploite plus directement la structure des CDS et des représentations vectorielles latentes construites à partir de représentations des termes qui les composent.
Baseline
Cette première méthode est une application directe de l'hypothèse selon laquelle les informations à retenir sont les informations redondantes. Dans cette approche, nous sélectionnons simplement les CDS qui sont générées au moins n fois à partir du corpus pour construire la base de connaissances résumée de ce corpus. Dans les expérimentations menées dont les ré-sultats sont décrits en section 5, nous avons considéré que deux CDS sont équivalentes si elles ont les mêmes sujet, verbe, objet, lieu et temps. Nous avons également fixé la valeur de n à 2 ; cette valeur devrait être plus grande pour traiter des corpus plus importants.
SumBasic
La méthode SumBasic introduite dans (Nenkova et Vanderwende, 2005) est une approche extractive pour le résumé d'un corpus de documents exploitant la fréquence relative des mots non nuls. Dans cette méthode, chaque phrase S se voit assigné un score représentant à quel point la phrase est constituée de mots fréquents : Weight(S) = w∈S p(w) |S| où p(w) est le poids de chaque mot ; à l'étape initiale, p(w) reflète la fréquence relative du mot w (rapport entre le nombre d'apparition de w dans le corpus et le nombre total de mots dans le corpus).
Le résumé est construit progressivement en extrayant à chaque étape la phrase présentant le plus grand score jusqu'à obtenir un texte de la taille souhaitée. Pour limiter la redondance du résumé généré, après chaque sélection d'une phrase, le poids de chaque mot de cette phrase est mis à jour :
L'adaptation de la méthode SumBasic au format des CDS est directe : la CDS est l'équi-valent d'une phrase dans la méthode originale, et les mots composant chacun des éléments qui constituent cette CDS sont traités comme les mots d'une phrase.
Embeddings et K-means
Cette méthode consiste à projeter sur un espace vectoriel les CDS d'un corpus de textes puis à appliquer l'algorithme K-means sur l'ensemble des vecteurs de ces CDS.
Chaque CDS est transformée en un vecteur qui est le résultat de la concaténation de chacun des vecteurs des éléments qui la constituent. Un élément de CDS (sujet, action, objet ou complément) peut être composé de un ou plusieurs mots. Le vecteur d'un élément est obtenu en sommant les vecteurs de chaque mot qui le compose (la prise en compte ou non des mots vides n'a exposé aucune différence sur les résultats des expérimentations). Les vecteurs de chaque mot (word embeddings) ont été pré-entraînés 1 avec FastText Bojanowski et al. (2016). Dans les expérimentation, nous avons considéré dans chaque CDS les éléments suivants : sujet, verbe, objet, lieu et temps. Les embeddings de mots étant représentés sur 300 dimensions, le vecteur représentant une CDS obtenu après concaténation a pour dimension 1500.
Nous appliquons ensuite l'algorithme de clustering K-means dans cet espace, l'hypothèse étant que les clusters obtenus réuniront des CDS de sens proches ou identiques. Pour chaque cluster, la CDS la plus proche du centroïde est sélectionnée pour construire le résumé.
Le paramètre k définit donc le nombre de CDS que comportera le résumé généré. On peut ainsi contrôler la taille du résumé et s'assurer que les faits peu appuyés (non redondants) du corpus n'apparaîtront pas dans ce résumé ; en effet, les CDS correspondant à ces faits seront rattachées à des clusters mais elles seront suffisamment éloignées du centroïde pour ne pas être extraites. Pour les expérimentations, nous avons déterminé la valeur de k pour que les résumés générés soient du même ordre de longueur que les résumés manuels.
Evaluation
Nous rappelons que l'objectif de ce travail est d'extraire les informations consensuelles des textes d'un corpus afin d'identifier d'éventuelles contradictions dans un nouveau texte. Des évaluations extrinsèques seront effectuées au fil des développements futurs. Nous présentons dans la suite une évaluation intrinsèque. Les CDS extraites par le composant de résumé sont utilisées pour regénérer un résumé textuel qui peut ensuite être comparé à des résumés effectués manuellement via la métrique ROUGE (Lin, 2004) (Recall-Oriented Understudy for Gisting Evaluation), traditionnellement utilisée pour évaluer la tâche de résumé automatique.
Pour évaluer notre approche, nous avons utilisé le corpus de résumé multi-documents 2 issu du projet RPM2 (Résumé Plurimédia Multi-documents et Multi-opinions) (De Loupy et al., 2010). Ce corpus contient 400 article de journaux (datant de 2009) répartis dans 20 théma-tiques. Chaque thématique est constituée de 2 clusters, chaque cluster contenant 10 documents. Pour chaque catégorie, les articles du second cluster ont été publiés 1 mois après les articles du premier. Le corpus comporte également des résumés produits à la main. Chaque cluster de 10 documents a été résumé manuellement par 4 annotateurs différents. Il y a donc au total 160 résumés manuels, 4 pour chacun des 40 clusters du corpus.
Afin de pouvoir comparer les performances des méthodes présentées plus haut avec celles d'implémentations de méthodes de références, nous avons utilisé la librairie sumy 3 qui fournit des implémentations pour les méthodes de résumé automatique les plus populaires. Le tableau ci-dessous montre les scores des trois variantes présentées plus haut (préfixées de la mention 'CDS_') ainsi que les méthodes LSA, Lex-Rank, KL-Sum et Text-Rank. Les approches exploitant les CDS surpassent en terme de précision et de F-score les mé-thodes de référence pour le jeu de données considéré. La méthode Baseline obtient la meilleure précision, ce qui n'est pas étonnant car celle-ci ne prend pas de risques en ne sélectionnant que du contenu redondant. La méthode K-means obtient le meilleur F-score, ce qui est encourageant car c'est selon nous la méthode qui exploite le mieux les informations portées par les CDS. Il est cependant important de garder à l'esprit que cette méthode d'évaluation présente l'avantage de permettre une comparaison directe aux travaux de la littérature mais n'est pas complètement représentative de l'usage final qui sera fait du composant de résumé automatique. Elle introduit en effet un biais important en traduisant les CDS en texte.
Nous prévoyons pour la suite d'exploiter ces résumés de corpus sous forme de CDS pour mettre en oeuvre des mécanismes de détection d'incohérences sémantiques.


Le suicide : un fléau de santé publique
On estime qu'il y a plus de 800 000 décès par suicide, par an dans le monde (WHO, 2014). Avec environ 10 000 décès en France par an, le suicide est la troisième cause de décès évi-table. Le coût humain est donc colossal et bien souvent sous-estimé à cause des suicides non repérés. Ainsi, en France métropolitaine on considère une sous-estimation des décès de 9, 4%. Le chiffre officiel de 9 715 décès pour l'année 2012 (première année où le nombre de dé-cès est inférieur à 10 000) peut ainsi être ré-évalué à 10 690 (ONS, 2016). Par ailleurs, il est estimé qu'il y a environ vingt tentatives de suicide pour un décès par suicide, ce qui repré-sente environ 200 000 tentatives en France par an. Les proches subissent souvent des consé-quences sévères (Fauré, 2008) augmentant ainsi le coût humain. Le suicide est donc un problème de santé majeur dans toutes les sociétés avec des conséquences également financières importantes (Smith, 2011). Malgré cela, comparativement à d'autres maladies, les efforts gouvernementaux, financiers et humains pour prévenir les maladies psychiatriques et le suicide en particulier sont assez récents et bien souvent jugés insuffisants (Montaigne, 2014;Lytle et al., 2016). L'Observatoire national du suicide, par exemple, n'a été créé en France qu'en 2013.
Si une première tentative représente un très grand facteur de risque (Finkelstein et al., 2015), d'autres critères doivent être pris en compte dans le processus de prise de décision thérapeutique et de prévention. Les facteurs de risque suicidaire ont déjà été étudiés parmi des populations de suicidants, qui sont une cible privilégiée pour l'élaboration de stratégies de prévention et d'intervention. Nous présentons un travail de fouille de données, réalisé avec des psychiatres, mené sur une population de suicidants. La section 2 recense quelques travaux, orientés données, menés pour analyser ce fléau et développer la prévention. Nous présentons le protocole clinique, les données, les méthodes déployées et les principaux résultats de notre étude dans la section 3. Enfin nous concluons dans la section 4.
Données et prévention du suicide
L'analyse de données sur le suicide n'est pas récente notamment grâce à l'attention portée par des sociologues. Durkheim (1897) a dégagé les causes du suicide et proposé une typologie des suicides, selon leurs causes, puis à l'aide d'une analyse statistique précise, l'auteur a montré que le suicide est un phénomène social normal. La statistique fait apparaître des régularités, déjà observées par Durkheim (1897), dans la fréquence des suicides, certaines évoluant avec les changements de rythmes de la vie sociale (Aveline et al., 1984).
Ces dernières années, les modalités de collecte de données, sociologiques et cliniques, concernant les patients reçus en consultation après une tentative, ont connu de profonds changements liés aux outils numériques. Cette collecte de données est augmentée par le dévelop-pement et l'utilisation d'applications web et/ou mobile pour le suivi de patients (Berrouiguet et al., 2016aBarrigón et al., 2017) ou encore par le suivi des médias sociaux, tels Facebook (Moreno et al., 2011) ou Twitter (Abboute et al., 2014), permettant également de considérer l'évolution de l'état des personnes suivies (Maigrot et al., 2016). Le nombre de variables décrivant les patients explose et dans une moindre mesure le nombre de patients suivis. Ainsi, de nouvelles opportunités apparaissent pour des analyses prenant en compte des dizaines ou encore des centaines de variables sur des échantillons de plus en plus grands pour étudier ce phénomène éminemment complexe. Un fort espoir est placé dans la fouille de données (Berrouiguet et al., 2016b;Ribeiro et al., 2016;Rakesh, 2017). Nous présentons ci-après quelques travaux intéressants et récents sans aucune exhaustivité. L'identification de groupes de patients similaires a été l'objet de nombreux travaux avec bien souvent pour but d'identifier des facteurs de risque. Wolodzko et Kokoszka (2014a) identifient 7 groupes avec des risques accrus de conduite suicidaire dans un échantillon de 5 977 américains âgés de 15 à 54 ans. Dans une revue de la littérature, les mêmes auteurs distinguent 5 groupes et concluent à la nécessité de développer l'analyse de groupes sur des populations plus larges, représentatives et homogènes (Wolodzko et Kokoszka, 2014b). D'autres auteurs s'intéressent à la caractérisation de 418 patients asiatiques en distinguant les récidivistes des patients ayant réalisé une unique tentative (Choo et al., 2014). Enfin, notons l'étude de LopezCastroman et al. (2016) sur 1 009 patients qui identifie 3 groupes selon une échelle d'impulsivité et de fréquence de récidive. Les deux groupes sont classifiés ensuite par arbre de décision.
L'apprentissage supervisé, notamment pour l'évaluation du risque de passage à l'acte ou de récidive, est de plus en plus mis à contribution. Les procédures recommandent que chaque patient reçu dans les services hospitaliers après une tentative non fatale doit passer une évaluation psychologique avant sa sortie afin d'évaluer le risque de récidive (Chan et al., 2016). Il existe différents tests psychologiques. Delgado-Gómez et al. (2011) présentent une comparaison de l'échelle d'impulsivité BIS11 de Barratt version 11 (Patton et al., 1995) et le questionnaire IPDE-SQ de dépistage et d'évaluation du trouble de la personnalité (Loranger et al., 1994) afin d'évaluer leur capacité, une fois combinés avec quatre classifieurs, à distinguer des suicidants (345 individus) de non suicidants (534 individus). Les quatre classifieurs obtiennent de meilleurs résultats que la méthode traditionnelle de classification psychométrique, le SVM étant le meilleur. IPDE-SQ est jugée plus discriminante que BIS11. D'autres études, récentes, démontrent l'intérêt des méthodes supervisées pour l'évaluation du risque suicidaire parmi lesquelles celles de Tran et al. (2014), Glenn et Nock (2014), Kessler et al. (2015), Combes et al. (2016), Karmakar et al. (2016) et Walsh et al. (2017).
La plupart des études pointent qu'il est nécessaire de développer les analyses en augmentant la taille des échantillons, en intégrant de nouvelles variables dont des informations écolo-giques obtenues le plus objectivement possible dans le milieu naturel du patient, par exemple la qualité du sommeil. Elles mettent également l'accent sur la nécessité d'améliorer la qualité des données. Enfin notons qu'il est difficile d'extrapoler, donc de comparer, sauf sur quelques variables invariantes comme le sexe et l'âge, les résultats d'une population à une autre. Les études menées sont effectivement très dépendantes des populations de patients et des protocoles de collecte de données suivis, donc des données collectées (Lopez-Castroman et al., 2015).
Fouille de données pour la prévention du risque suicidaire
Nous présentons dans cette section les principaux résultats d'un processus complet de fouille de données sur un échantillon de suicidants reçus dans deux hôpitaux européens. Deux objectifs principaux discutés en concertation avec les psychiatres sont poursuivis. Le premier objectif est d'identifier des groupes de patients similaires et le second d'identifier des facteurs de risque associés au nombre de tentatives de suicide par patient. Le nombre de TS constitue un fort enjeu de santé publique car de nombreux patients sont des multi-récidivistes et les coûts humains et financiers sont colossaux.
Recrutement des patients et évaluation clinique
Recrutement des patients
La population est composée de patients âgés de 18 ans ou plus, reçus pour des tentatives de suicide dans deux hôpitaux universitaires, à Madrid (hôpital Ramon y Cajal), Espagne, et Montpellier (hôpital Lapeyronie), France, entre 1994 et 2006.
Les équipes médicales se sont accordées pour employer les mêmes méthodes cliniques et des procédures d'évaluation comparables. Les données sociodémographiques et cliniques des patients ont été fusionnées dans une base de données commune qui intègre également les résultats des questionnaires d'évaluation. La définition d'une tentative de suicide retenue est la suivante : "a potentially self-injurious behavior with a nonfatal outcome, for which there is evidence (either explicit or implicit) that the person intended at some (nonzero) level to kill himself/herself." Les études ont été approuvées par les différents comités d'éthique et conduites selon les principes de la déclaration d'Helsinki, à propos des principes éthiques de la recherche médicale. Tous les participants ont signé un document de consentement après une explication précise des objectifs de l'étude et des procédures. Le jeu de données complet inclut des patients ne présentant pas d'historiques de tentatives de suicide, des donneurs de sang, et autres patients de "contrôle". Nous nous sommes concentrés sur la seule population des suicidants.
Procédure clinique
L'évaluation clinique des patients a été conduite aux urgences. Elle s'appuie sur des entretiens structurés avec notamment l'échelle Columbia (Columbia Suicide History Form). Cette échelle consiste en une série de questions permettant d'évaluer l'idéation suicidaire et les antécédents suicidaires chez les patients à risque. Les données ont été collectées grâce à l'application MEmind (Berrouiguet et al., 2015). L'application collecte des données sociodémogra-phiques, de diagnostic, et pharmacologiques au sein du protocole d'évaluation. Les variables sociodémographiques incluent notamment l'âge au moment de l'épisode suicidaire, le genre, la profession, le statut marital, la situation professionnelle, le nombre d'enfants et le niveau d'éducation. Des informations sur les historiques familiaux quant à la question du suicide, l'âge de la première tentative, le caractère violent de l'acte, sont également récoltées. Les versions française et espagnole du questionnaire neuropsychiatrique international MINI (Lecrubier et al., 1997) ont été utilisées afin d'obtenir des diagnostics psychiatriques : troubles de l'humeur (par exemple troubles bipolaires, dépression), anxiété, troubles obsessionnels compulsifs, drogues et alcool, troubles psychotiques, alimentaires, somatiques, etc. Le psychiatre en charge du diagnostic complète à chaque fois les informations à l'aide du dossier médical et potentiellement à l'aide d'informations venant des proches. Le risque suicidaire a été évalué à l'aide de l'échelle SIS (Suicide Intent Scale, Beck et al. (1974)), une échelle de risque semi structurée à 15 items qui produit un score global de sévérité quant à l'intention suicidaire. La tentative en tant que telle est évaluée à l'aide de l'échelle RRRS (Risk-Rescue Rating Scale) qui est un questionnaire de 10 items qui mesure la sévérité de l'intention et du geste suicidaire au regard de la létalité et de la vraisemblance d'une intervention de sauvetage au moment du geste (Weisman et Worden, 1972). L'impulsivité du patient a été mesurée à l'aide de l'échelle BIS10 (Barratt Impulsiveness Scale) une série de 34 questions qui mesurent la propension du sujet à prévoir ses gestes, son comportement, dans diverses situations (Patton et al., 1995).
Construction de la base de données analysée
Bien que les données sont censées être collectées selon des processus identiques, une variabilité de la qualité est inévitable de par la diversité des équipes de recueil. De plus, le contexte particulier de l'accueil aux urgences de personnes très fragiles ne facilite pas une collecte exhaustive. Ainsi un processus robuste de qualification des données a été mené afin d'assurer une consistance et une complétude les plus grandes possibles de la base de données pour garantir des résultats aussi fiables que possible. La base de données originale contenait 2 802 patients et 263 variables. De nombreuses variables, notamment liées à la duplication de certains questionnaires (à un questionnaire correspond plusieurs dizaines de variables) codés sous différentes formes, sont redondantes et donc éliminées.
Puis, en accord avec les psychiatres, désireux d'avoir rapidement de premiers résultats, il a été décidé de ne garder que les variables qui satisfaisaient un taux de complétion minimum de 70%. La modélisation de la structure et de la typologie des données manquantes et l'utilisation de méthodes d'imputation seront réalisées ultérieurement. Ensuite, les 34 questions du questionnaire d'impulsivité de Barratt (BIS10, Patton et al. (1995) 
Méthodes de fouille de données
Nous rappelons que, en concertation avec les psychiatres, nous cherchons à d'une part identifier des groupes de patients similaires et d'autre part identifier des facteurs de risque associés au nombre de tentatives de suicide (TS par la suite) par patient. Nous ne présentons pas les résultats de l'analyse statistique descriptive par manque de place.
Résultats 3.4.1 Clustering des patients
Afin de mettre en lumière des groupes de patients partageant des caractéristiques similaires, nous avons procédé en deux étapes. Tout d'abord, une analyse des correspondances multiples (ACM) a été effectuée sur les seules variables qualitatives. Cette méthode factorielle, bien adaptée à l'analyse de questionnaires, représente les individus dans un nouvel espace où chaque dimension est une combinaison des variables de départ. Le nombre de dimensions retenues pour l'ACM est de 5. Les variables quantitatives comme l'âge ne sont pas utilisées pour le calcul des composantes principales, mais projetées ensuite sur le plan factoriel et utilisées pour l'interprétation. Cette méthode, habituellement utilisée en tant que technique de réduc-tion de dimension, sert ici d'étape préalable à l'obtention de clusters robustes. Elle permet également, en transformant les variables qualitatives en variables continues, de représenter les individus dans un nouvel espace auquel on peut associer une métrique. Nous appliquons alors une Classification Hiérarchique sur Composantes Principales sur les individus dans le nouvel espace factoriel. Une classification ascendante hiérarchique permet de regrouper itérativement les paires de clusters les plus proches, en partant de singletons (chaque individu est seul dans son groupe) jusqu'à réunion en un unique cluster de tous les individus. La méthode construit un arbre binaire hiérarchique nommé dendrogramme, qui permet une interprétation visuelle des données et de la proximité entre individus via la hauteur de la branche les reliant. Elle permet d'identifier ainsi des groupes de patients partageant des facteurs de risque similaires et facilite les interactions avec les experts du domaine, mais non experts en fouille de données, afin de décider du nombre de clusters. Chaque cluster peut ensuite être interprété à travers la significativité de son association avec les modalités des différentes variables de départ (v-test).
La figure 1 montre (a) une projection en deux dimensions des individus sur le premier plan factoriel, (b) le dendrogramme. La structure de l'arbre (en terme de gain d'inertie dans la hiérarchie) et une discussion avec les experts en santé mentale ont mené à la sélection de trois clusters, également projetés sur le plan factoriel (a posteriori). Nous avons utilisé pour cela le package R FactoMineR. Une analyse approfondie des trois groupes a été réalisée pour la phase d'interprétation. Des tests d'association mettent en exergue les modalités sur ou sous représentées au sein des trois groupes. Les principales conclusions sont les suivantes : le groupe 1 correspond à un profil moyen plutôt féminin (p < 0, 001), sans troubles bipolaires ni prises de drogues, substance, ou consommation d'alcool (p < 0, 001). En revanche, cette population a en moyenne déjà subi des épisodes de dépression (p < 0, 001) ou des troubles mentaux divers (p < 0, 05). Le troisième groupe s'oppose au premier en exhibant un profil de patients plutôt masculin (p < 0, 001), consommateur d'alcool, substances, ou drogues (p < 0, 001). Les patients de ce groupe sont souvent célibataires et sans enfants (p < 0, 05), et ne sont pas associés à un historique d'épisode dépressif connu (p < 0, 05). Entre ces deux groupes, on trouve le groupe 2, neutre en termes de genre, mais marqué par des personnes présentant une incapacité au travail (p < 0, 05), un faible niveau d'éducation (p < 0, 05) avec de possibles troubles bipolaires (p < 0, 001) mais aucune consommation des produits évoqués précédemment (p < 0, 001). Enfin, ces personnes ne présentent pas d'autres troubles de la santé mentale ou d'épisodes dépressifs connus (p < 0, 001). Ces premiers résultats montrent la capacité de la méthode à identifier des facteurs de risques déjà connus des praticiens comme le sexe, l'impulsivité, la prise de toxique. La méthode fait émerger également des facteurs de risque qui sont moins fréquemment explorés alors qu'ils semblent peser, d'après nos résultats, sur le risque de récidive. Au total, cette méthode couplée à des entretiens cliniques structurés permettrait d'obtenir pour chaque patient un niveau de risque précis sur lequel s'appuierait la décision de prise en charge.
Identification de facteurs de risques pour le nombre de TS
La deuxième phase de l'analyse a consisté à identifier des facteurs associés la multirécidives. La variable d'intérêt est donc le nombre de TS que nous allons estimer par arbres binaires de régression, afin de faciliter l'interprétation des résultats par les psychiatres.
La construction d'un arbre binaire est fondée sur une séquence récursive de divisions des individus en sous-populations à l'aide de tests binaires (test d'égalité pour les variables qualitatives, test d'infériorité pour les variables quantitatives) sur les variables dites prédictives (les facteurs de risque). L'ensemble des individus est regroupé à la racine de l'arbre puis chaque division sépare chaque noeud en deux noeuds plus homogènes que le noeud parent. La variable retenue pour chaque division est celle minimisant l'écart quadratique à la moyenne de TS des individus dans les noeuds créés. L'arbre, de sa racine aux noeuds terminaux, hiérarchise ainsi les variables selon leur capacité à créer des groupes homogènes selon le nombre de TS .
Nous avons utilisé la méthode CART (Breiman et al., 1984)   L'arbre de régression des patients masculins fait apparaître des facteurs de risque propres aux hommes comme le statut professionnel ou un historique de comportement suicidaire dans la famille (nombre de TS moyen 2,8 vs. 1,7 sinon, p < 0, 05). D'autre part, le fait de ne pas avoir d'enfants entraîne un sur-risque, qui n'apparaît pas chez les femmes. Pour les deux souspopulations, il est notable de voir que les scores d'impulsivité sont des facteurs explicatifs du nombre de TS . Pour les femmes, la notion de troubles alimentaires émerge, ce qui constitue un résultat important. En effet, pour les femmes salariées présentant une impulsivité motrice supérieure au score moyen, le nombre moyen de TS est de 2,9 pour les 68 femmes présentant des troubles alimentaires contre 2,3 pour les 202 autres (p < 0, 05). Ce facteur de risque est peu questionné par les praticiens alors que nos résultats indiquent qu'il semble être important.  4 Bilan, conclusion et perspectives Une évaluation systématique de suicidants a permis de constituer une base de données de taille suffisante pour rendre significatif l'emploi de techniques de fouille de données. Nous avons évité d'employer le mot "prédiction". Il ne faut pas confondre évaluation du risque et prédiction du passage à l'acte. C'est sur le premier point que nous tentons modestement de contribuer, la prédiction d'un acte aussi complexe étant actuellement illusoire. Nous avons identifié des clusters de patients puis estimé le nombre de TS en fonction de facteurs de risque. Certains, comme les troubles alimentaires pour les femmes, sont aujourd'hui peu recherchés par les psychiatres participant à cette étude. Des investigations poussées sont nécessaires sur ce point pour revoir éventuellement les entretiens. Nos résultats, couplés à des entretiens structurés, permettraient d'obtenir pour chaque patient un niveau de risque précis sur lequel s'appuierait la décision de prise en charge. Le volume de données assure d'ores et déjà une représenta-tivité statistique, mais ce travail met en évidence un problème de qualité des données dont les psychiatres ont pris conscience ainsi que des mesures à mettre en place. De nouvelles données, de meilleure qualité et contenant la temporalité des événements, sont en cours de constitution, permettront d'étudier la temporalité des TS et d'ouvrir de nouveaux champs d'investigations.
Remerciements. Nous tenons à remercier les relecteurs pour leurs remarques constructives et suggestions ayant permis d'améliorer la version finale de cet article.

Introduction
FIG. 1 -Objectif du clustering prédictif
L'algorithme des K-moyennes prédictives (Eick et al., 2004;Al-Harbi et Rayward-Smith, 2006;Alaoui Ismaili, 2016) est une version modifiée de l'algorithme des Kmoyennes standard. Il vise à décrire et à prédire d'une manière simultanée.
L'idée est de générer dans la phase d'apprentissage un nombre minimal de clusters compacts dont les instances doivent appartenir à la même classe. Ces clusters vont servir par la suite à décrire les données et à prédire la classe des nouvelles instances (voir la figure 1).
La méthode communément utilisée dans la littérature permettant d'attribuer la classe aux clusters formés par l'algorithme des K-moyennes prédictives est le vote majoritaire. Bien que cette approche parvienne à obtenir de bons résultats, celle-ci a également certaines limites. On citera par example :
-pour le taux de bonne classification (ACC) : si un cluster contient Q% d'instances de la classe C1 et 100-Q% d'instances de la classe C2, alors l'utilisation du vote majoritaire va produire un taux de mauvaise classification très important (Q). La présence d'un modèle local à ce cluster devrait permettre de mieux discriminer les exemples selon leur classe d'appartenance. Ceci est très visible pour les clusters E, H, G de la figure 1.
-pour l'aire sous la courbe de ROC (AUC) : le fait de se baser sur la classe majoritairement présente dans un cluster produit une courbe de ROC, ou de lift, ayant l'allure de la courbe rouge dans la figure 2. Le classement des exemples se fait par cluster et la courbe comporte des segments. L'aire sous la courbe de ROC est sous-optimale (voir la figure 2).
FIG. 2 -Illustration courbes de ROC
Pour surmonter ces problèmes, on propose dans cet article d'entrainer un modèle d'apprentissage localement à chaque cluster formé dans la phase d'apprentissage par l'algorithme des K-moyennes prédictives. Le modèle global appris dans cette phase va être par la suite utilisé pour prédire la classe des nouvelles instances.
Le reste de ce papier est organisé comme suit : la section 2 présente les différentes étapes de l'algorithme des K-moyennes prédictives utilisé dans cette étude ainsi que la méthode proposée pour surmonter les problèmes cités ci-dessus. La section 3 présente une brève description des classifieurs qui seront comparés à l'approche proposée, le protocole expérimental et les résultats obtenus. Puis la section 4 présente la partie analyse descriptive du modèle global obtenu. Finalement la section 5 conclue cet article et présente des pistes d'améliorations pour le futur.
2 Les K-moyennes prédictives et choix des modèles locaux 2.1 K-moyennes prédictives L'algorithme des K-moyennes utilisé dans cette étude est l'algorithme proposé dans (Alaoui . Cet algorithme fourni de meilleurs résultats (en termes de prédiction) par rapport aux algorithmes de clustering prédictif les plus répandus dans la littérature tels que l'algorithme de Eick (Eick et al., 2004) et l'algorithme de Al-Harbi (Al-Harbi et Rayward-Smith, 2006. Il est présenté succinctement ci-dessous (voir Algorithme 1). Il correspond à l'algorithme des k-moyennes classique mais où de la supervision a été incorporée dans certaines des étapes. La section 3.1 donnera plus détails sur chacune des étapes supervisées de l'algorithme 1. L'objectif de l'étude qui sera menée au cours du présent article sera de mesurer le fait de changer le vote majoritaire (étape 5) en incorporant des modèles locaux au sein des clusters obtenus à la fin de la convergence de l'algorithme des k-moyennes prédictives.
Choix pour les modèles locaux
Afin de surmonter les problèmes rencontrés par le vote majoritaire lors de l'attribution des classes aux clusters formés par l'algorithme des K-moyennes prédictives, nous pensons que l'ajout de modèles locaux (ajout d'un classifieur localement à chaque cluster) est une solution. Cependant, il est nécessaire que les modèles locaux :
-puissent être entrainés éventuellement avec peu de données -soient robustes (ratio performances train/test) -idéalement puissent ne comporter aucun paramètre utilisateur afin de ne pas avoir à réaliser de cross-validation localement à un cluster. -aient une complexité algorithmique linéaire en apprentissage (en O(N )) -ne soient pas créés si localement au cluster il n'y pas d'information suffisante pour la création d'un modèle local (dans ce cas et pour ce cluster le vote majoritaire serait conservé) -conservent (voire améliorent) les qualités d'interprétation initiales du modèle global -individualisent les prédictions dans un cluster donné afin d'améliorer l'AUC (courbe bleue vis-à-vis de la courbe rouge dans la figure 2). Une large étude a été réalisée dans (Salperwyck et Lemaire, 2011) afin d'étudier la vitesse d'apprentissage des classifieurs les plus couramment utilisés. Vitesse au sens du nombre d'exemples utilisés versus les performances en classification. Cette étude montre la capacité d'un classifieur naïf de Bayes à apprendre avec peu de données (confirmant (Bouchard et Triggs, 2004)) ; que ce soit dans sa version standard ou dans la version où les variables reçoivent des poids (on parle alors de ANB (Averaging Naive Bayes) ou SNB (Selective Naive Bayes)) (Langley et Sage, 1994). On trouve de plus dans les travaux décrits dans (Boullé, 2007a) un critère analytique issu de la famille MODL (Bondu et al., 2013) (muni de son algorithme d'optimisation) permettant d'apprendre un Selective Naive Bayes (SNB) sans paramètre utilisateur et qui est de plus régularisé. Cette régularisation permet d'exhiber de bonnes performances tout en assurant une très bonne robustesse (ratio des performances entre l'apprentissage et le test proche de 1).
Pour ce SNB les prétraitements et le calcul des poids des variables sont basés sur l'approche MODL (Boullé, 2007b). On y trouve ici un point intéressant : si l'approche MODL ne découvre pas assez d'information dans une base de données alors elle sait se taire au sens dire "qu'aucune variable n'est informative". Dans ce cas le vote majoritaire sera alors consi-déré comme meilleur à un autre choix. Dans notre cas applicatif l'intérêt est donc qu'on ne retournera un SNB localement à un cluster si et seulement si il est plus "informatif" que le vote majoritaire. Dans le cas contraire le vote majoritaire sera conservé. L'autre intérêt de l'approche MODL est qu'elle est auto-régularisée : il n'y a pas besoin de faire une cross validation pour trouver les bons paramètres d'un modèle. En termes d'interprétation un SNB s'interprète aisément (Lemaire et al., 2009). Par exemple, le poids des variables peut être utilisé pour dé-crire leur importance respective.
Nous décidons donc qu'un SNB de la famille MODL sera donc entrainé localement dans chacun des K clusters formés (fin de l'étape 4 de l'algorithme 1) : Pour chaque cluster cluster l (l ∈ {1, . . . , K}), un classifieur SNB est entrainé, notons SN B l le modèle obtenu.
Expérimentation
Classifieurs de comparaison
Afin d'étudier l'impact de l'utilisation des modèles locaux sur la qualité des résultats issus de l'algorithme des K-moyennes prédictives, nous allons comparer ses performances prédic-tives avec celles obtenues par la méthode utilisant le vote majoritaire et avec celles obtenues par trois autres algorithmes performants dans le cadre de la classification supervisée.
Le premier modèle combine les arbres de décision avec la régression logistique (Logistic Model Tree (LM T )) (Landwehr et al., 2005), le deuxième modèle combine les arbres de décision avec des classifieurs naïfs de Bayes (Naives Bayes Tree (N BT )) (Kohavi, 1996). Ces modèles ont été choisis pour leurs bonnes performances mais aussi pour leur proximité avec l'approche proposée qui consiste à avoir des modèles locaux entrainés sur une partie des données. Enfin comme notre modèle hybride utilise des SN Bs localement aux clusters nous ajoutons en juge paix un SNB 'global' qui lui est entrainé sur l'ensemble des données.
Note : Nous avons décidé de centrer ici l'analyse sur l'algorithme des K-moyennes en termes de classification et de ne pas comparer la méthode proposée avec des algorithmes de clustering concurrents avec lesquels il est parfaitement possible de faire de la prédiction. On concentre la comparaison dans cet article avec d'autres méthodes hybrides telles que LM T et N BT contenant des modèles locaux. La comparaison avec d'autres méthodes de clustering a néanmoins été partiellement déjà réalisée dans (Alaoui Ismaili, 2016) et l'algorithme des K-moyennes prédictives muni du vote majoritaire était déjà très bien positionné. On donne ci-dessous une brève description de LMT, NBT et du SNB.
Naive Bayes Tree (N BT )
L'arbre de "Naive Bayes" (N BT ) est un algorithme d'induction d'arbres de décision avec des classifieurs naïfs Bayésiens dans ses feuilles (Kohavi, 1996). L'arbre est induit d'une manière descendante (top-down) avec des segmentations univariées en se basant sur le gain d'information. Un pré-élagage est utilisé lors de la phase d'entrainement de l'arbre pour décider si le noeud sera partitionné ou bien il est terminal. Dans ce cas, un modèle naïf Bayésien est entrainé localement sur les instances de la feuille. N BT est un classifieur qui a de bonnes performances par comparaison soit avec les arbres de décision soit avec le classifieur naïf Bayes seul.
Logistic Model Tree (LM T )
L'arbre de régression logistique (LMT) (Landwehr et al., 2005) est un modèle de classification basé sur un algorithme d'apprentissage supervisé qui combine la régression logistique et les arbres de décision. L'objectif est d'améliorer les performances de la classification obtenues par les arbres de décision. A cette fin, au lieu d'associer à chaque feuille un seul label et un seul vecteur de probabilité (piecewise constant model), un modèle de régression logistique est entrainé sur les feuilles afin d'estimer, pour chaque exemple de test, un vecteur de probabilités plus adapté (piecewise linear regression model). L'algorithme LogitBoost est employé pour ajuster un modèle de régression logistique à chaque noeud , ensuite le noeud est partitionné en utilisant le gain d'information comme fonction d'impureté. L'appel de l'algorithme LogitBoost dans chaque noeud utilise comme point initial le modèle obtenu dans le noeud parent. Finalement, l'arbre est élagué au moyen de l'algorithme d'élagage de CART. Le nombre d'ité-rations de LogitBoost dans chaque noeud est déterminé par une validation croisée pour éviter le sur-apprentissage.
Selective Naive Bayes (SN B)
Le classifieur naïf Bayésien est un outil largement utilisé dans les problèmes de classification supervisée. Il a pour avantage de se montrer efficace pour de nombreux jeux de données réels (Hand et Yu, 2001). Cependant, l'hypothèse naïve d'indépendance des variables peut, dans certains cas, dégrader les performances du classifieur. Aussi, des méthodes proposant de réaliser de la sélection de variables ont vu le jour (Langley et Sage, 1994). Elles consistent en la mise en place d'heuristiques d'ajout et de suppression de variables afin de sélectionner le meilleur sous-ensemble de variables maximisant un critère et donc les performances du classifieur, selon une approche wrapper (Guyon et Elisseeff, 2003). Il a été montré par Boullé (Boullé, 2007a) que moyenner un grand nombre de classifieurs Bayésiens naïfs sélectifs, réalisés avec différents sous-ensembles de variables, revenait à ne considérer qu'un seul modèle avec une pondération sur les variables. La formule de Bayes sous l'hypothèse d'indépendance des variables conditionnellement aux classes devient :
où W i représente le poids de la variable i. La classe prédite est celle qui maximise la probabilité conditionnelle P (C k |X). Les probabilités P (X i |C i ) peuvent être estimées par intervalle à l'aide d'une discrétisation pour les variables numériques. Pour les variables catégorielles, cette estimation peut se faire directement si la variable prend peu de modalités différentes ou après un groupage dans le cas contraire.
K-moyennes prédictives (KM
Cet algorithme est présenté brièvement en section 2.1. Il correspond à l'algorithme des k-moyennes classique mais où 3 étapes ont été modifiées pour incorporer de la supervision :
• Prétraitement des données : Généralement, la tâche de clustering nécessite une étape de prétraitement non supervisé afin de fournir des clusters intéressants (pour l'algorithme des K-moyennes voir par exemple (Milligan et Cooper, 1988) et (Celebi et al., 2013)). Cette étape de prétraitement peut empêcher certaines variables de dominer lors du calcul des distances. En s'inspirant de ce résultat, Alaoui et al. ont montré dans ) que l'utilisation d'un prétraitement supervisé peut aider l'algorithme des K-moyennes standard à atteindre une bonne performance prédictive. Nous utilisons ici la méthode qu'ils ont proposée et nommée Conditional-Info. L'avantage de ce prétraitement supervisé est d'introduire une distance Bayé-sienne qui donne des garanties en termes de proximité des instances dans un cluster connaissant leur classe d'appartenance ((Alaoui Ismaili, 2016) chapitre 3).
• Initialisation des centres : Nous utilisons la méthode supervisée d'initialisation des k-moyennes décrite dans (Lemaire et al., 2015). Cette méthode est basée sur l'idée de la dé-composition des classes (Vilalta et al., 2003;Basu et al., 2002). Dans le cas ou K = C chaque centre initial correspond au centre de gravité d'une classe, si K > C les centre suivants sont initialisés à l'aide de la méthode kmeans++ (Arthur et Vassilvitskii, 2007). Dans le cas où K = C, la méthode étant déterministe, l'algorithme de convergence des K-moyennes n'est donc réalisé qu'une seule fois.
• Prédiction de la classe : Une nouvelle instance en test X, est tout d'abord affectée au cluster, l, dont elle est le plus proche puis on distingue le cas où : (i) la classe prédite est la classe majoritaire présente dans ce cluster (KM V M ), (ii) la prédiction de la classe s'effectue selon la règle de décision du modèle SN B l : P (C|X) = argmax 1≤j≤J (P SN B l (C j |X)) s'il existe un modèle local sinon le vote majoritaire est conservé (KM SN B ). Ces deux approches seront comparées plus loin dans l'article.
• Nombre de clusters : Cet article présente une première expérimentation des K-moyennes prédictives munies de modèles locaux. Nous avons arbitrairement décidé de fixer K = C dans ce cadre. Il est évident que l'intérêt de travailler dans un contexte ou K = C (i.e. nombre de clusters = nombre de classes à prédire) limite sérieusement l'apport du clustering prédictif relativement à une méthode concurrente de classification appliquée sur les mêmes données. L'exploitation directe d'un bon classifieur sera sûrement meilleure dans ce contexte. Cela étant dit, nous pourrons néanmoins évaluer si l'utilisation de modèles locaux est meilleure que celle du vote majoritaire. Si de plus avec K = C les résultats sont proches des méthodes concurrentes de l'état de l'art alors des travaux futurs réglant la valeur de K seront surement intéressant à mener.
Protocole expérimental 3.2.1 Base de données utilisées
Pour évaluer et comparer les différents algorithmes, nous allons effectuer des tests sur différents jeux de données de l'UCI (Lichman, 2013). Ces jeux de données ont été choisis afin d'avoir des bases de données diverses en termes de nombre de classes C, de variables (continues V n et/ou catégorielles V c ) et d'instances N (voir Tableau 1). Elles ont été choisies (sauf Adult) dans la liste de comparaison de l'article comparant LMT et NBT (Landwehr et al., 2005). Le lecteur pourra noter que ce sont de "petites" bases (sauf Adult).
Entrainement des modèles -éléments de reproductibilité
Les codes utilisés pour l'apprentissage : -de LM T et de N BT sont ceux contenus dans le logiciel R (Hornik, 2017) (qui utilise des wrappers sur Weka (Hall et al., 2009) -des SN B : nous avons obtenu une licence provisoire du logiciel Khiops (Boullé, 2016) qui nous a permis de produire les SNB globaux (Boullé, 2007a) -des KM V M et KM SN B : pour pouvoir réaliser les éléments de supervision de l'apprentissage de l'algorithme des K-moyennes décrits dans la section 3.1.4 nous avons aussi utilisé le logiciel Khiops afin de produire les SN B locaux aux clusters et les méthodes de prétraitement contenues dans , l'algorithme des KMoyennes classique étant lui réalisé en Matlab (MATLAB, 2010). 
Evaluation des performances
De manière à pouvoir comparer les résultats des 4 modèles KM V M , KM SN B , LM T et SN B les même folds en train / test ont été utilisés. Les performances prédictives présentées dans cet article (ci-dessous) sont données en test sur 10 × 10 folds cross validation "stratifié". Les 100 résultats en test ainsi obtenus permettent le calcul d'un résultat moyen muni de son écart type.
Le logiciel R et le logiciel Khiops n'ayant pas les mêmes façons de calculer les AUCs nous avons recodé ce calcul de manière à produire des valeurs comparables. Nous donnons ci-dessous dans les résultats proposés pour les valeurs d'AUC (aires sous les courbes de ROC (AUC)) l'espérance de l'AUC : AU C = C i P (C i )AU C(C i ), où AU C(i) désigne la valeur d'AUC de classe i contre toutes les autres et P (C i ) désigne le prior sur la classe i (fréquence des éléments de la classe i). Le calcul AU C(i) est réalisé à l'aide du vecteur des probabilités P (C i |X)∀i (et non uniquement de la classe prédite). Ceci n'est en aucune façon un biais en faveur de l'une ou l'autre des méthodes.
Résultats
Le tableau 2 présente les performances prédictives en termes d'accuracy et en termes d'AUC (présenté en %) obtenues par les 4 algorithmes de l'état de l'art (LM T , N BT , SN B, KM V M ) et la variante proposée dans cet article (KM SN B ). 
TAB. 2 -Performances en tests sur 10x10 folds cross-validation
Les résultats montrent que l'algorithme des K-moyennes prédictives suivi par l'insertion de modèles locaux (KM SN B ) exhibe des résultats significativement meilleurs que ceux obtenus par le même algorithme utilisant le vote majoritaire (KM V M ). Même s'il est difficile de comparer des résultats moyens nous notons néanmoins que pour les 8 bases de données testées le gain moyen est de 20% tant en accuracy qu'en AUC.
Les comparaisons entre modèles ayant des classifieurs naïf de Bayes en modèles locaux montrent que les résultats de KM SN B sont légèrement meilleurs que ceux de N BT . On notera que KM SN B contient (K = C) un nombre de modèles locaux très inférieurs à ceux de N BT (voir (Landwehr et al., 2005) pour plus de détails sur la taille des arbres produits par N BT et LM T ). Par contre nous notons un avantage de LM T pour les bases testées 1 sauf pour la base Adult qui est la plus peuplée.
Enfin la comparaison entre KM SN B et le modèle global (juge de paix) SN B indique que la méthode proposée donne des résultats très légèrement supérieures notamment pour les bases de données où l'on sait que les variables explicatives sont très corrélées (où l'hypothèse d'indépendance s'affaiblie) comme pour 'PenDigits'.
Pour compléter la comparaison nous donnons ci-dessous dans le tableau 3 quelques élé-ments supplémentaires de comparaison.  (Landwehr et al., 2005;Kohavi, 1996;Boullé, 2007a;Alaoui Ismaili, 2016) Nous y observons que la méthode proposée est assez bien placée car elle ne nécessite pas de cross validation, gère les valeurs manquantes nativement, opère une sélection de variables tant dans l'étape de clustering que lors de la construction des modèles locaux. Enfin, elle réalise un groupage supervisé des valeurs de variables catégorielles évitant ainsi de passer par un codage disjonctif complet qui entraine la création d'un vecteur d'entrée souvent grand compliquant ensuite l'interprétation du modèle obtenu.
D'autres axes de comparaison existent comme la complexité algorithmique, la robustesse, le nombre de modèles locaux produits... Mais la place nous manquant nous ne pouvons les aborder en détails. Nous mentionnerons juste le fait que le modèle KM SN B est très compétitif (notamment en présence de variables catégorielles ayant beaucoup de modalités) sur ces points.
Synthèse : l'introduction des modèles locaux dans KM V M pour obtenir KM SN B répond aux objectifs que nous nous étions fixés tant en termes de performance que dans la facilité de mise en oeuvre de la méthode. Nous estimons de plus que ce gain en performance n'a pas détruit le caractère interprétable des K-Moyennes prédictives. Même si le but principal de cet article n'est pas de discuter du pouvoir interprétable des K-moyennes prédictives (mais du pouvoir prédictif) nous essayons d'illustrer ce point au cours de la section suivante (dans la limite des considérations de place).
Méthodologie d'analyse des résultats
Pour avoir une idée sur la capacité de notre algorithme des K-moyennes prédictives à fournir à la fois des résultats performants (en termes de prédiction) et faciles à interpréter, la base de donnée Vehicle est utilisée comme un exemple illustratif. Cette base de données est constituée de 846 exemples, 18 variables descriptives et une variable à prédire contenant 4 classes (bus, opel, saab, van). Dans cette étude illustrative, nous utilisons l'ensemble des données pour apprendre le modèle avec K égal au nombre de classes C = 4. Nous ne détaillons pas ici la signification des variables A1 à A18 mais le lecteur pourra les trouver sur le site des bases de l'UCI (Lichman, 2013). Pour des raisons de place nous nous limitons ici à n'utiliser que les 6 variables les plus informatives dans le clustering initial sans que cela ne change la méthodolo-gie d'analyse. Enfin en deuxième niveau on fournit un histogramme par cluster (voir Figure 5) qui donne le poids des variables des SNB locaux permettant de connaitre le pourquoi de la classification locale. Par exemple, on s'aperçoit que le cluster 3 n'a que des poids à 0 indiquant que le classifieur majoritaire a été conservé. Que pour le cluster 1 les variables ont des poids très proches tandis que pour le cluster 2 la variable A12 est la plus importante... Ce deuxième niveau d'analyse contient des éléments d'interprétation qui ne sont pas disponibles dans KM V M .
L'un des avantages d'incorporer les modèles locaux dans notre algorithme des K-moyennes prédictives est d'améliorer leur pouvoir descriptif (interprétation des résultats). En effet, grâce aux modèles locaux, on est capable non seulement de connaître les variables les plus discriminantes dans le modèle global mais également de connaître celles qui contribuent le plus à la construction de chaque groupe dans la phase d'apprentissage. Par conséquent, à l'arrivée d'une nouvelle instance, on est capable de connaître facilement les différentes raisons qui déterminent la pré-diction de sa classe.
Conclusion et perspectives
L'introduction des modèles locaux dans KM V M pour obtenir KM SN B répond aux objectifs que nous nous étions fixés tant en termes de performances, que dans la facilité de mise en oeuvre de la méthode tout en conservant l'aspect interprétable du modèle global hybride obtenu. Les résultats expérimentaux se placent très honorablement dans l'état de l'art sur ces deux aspects, et ce malgré le fait de fixer le nombre de clusters comme étant égal au nombre de classes. Dans de futurs travaux nous étudierons la possibilité de trouver automatiquement le bon nombre de clusters par exemple à l'aide d'un clustering hiérarchique descendant plaçant alors la méthode à la manière de LMT mais dans le champ du clustering prédictif. Nous espé-rons alors un nouveau gain en termes de performances. Enfin nous pensons développer un outil de visualisation des résultats permettant de naviguer dans les clusters afin d'observer aisément les profils moyens et les importances des variables localement aux clusters.

Introduction
La reconnaissance de formes (pattern recognition) constitue une compétence d'intelligence fondamentale. Par exemple, en considérant un ensemble de labels disponibles : {'heureux', 'relaxant', 'triste', 'nerveux'}, l'intelligence humaine est capable d'associer ces labels à des morceaux de musique selon les émotions qu'ils expriment.
Le défi de transférer la compétence de reconnaissance de formes aux ordinateurs fait partie du domaine de l'apprentissage artificiel. L'apprentissage est dit supervisé lorsqu'un ensemble d'instances dont les labels associés sont connus est disponible. L'apprentissage consiste à établir des liens entres les attributs descriptifs des instances et les labels associés. L'objectif de l'apprentissage est de pouvoir prédire les labels associés aux instances en se basant sur les attributs descriptifs. L'ensemble d'instances est dit multi-labels lorsque chaque instance est associée à un ou plusieurs labels parmi un ensemble de labels disponibles.
La classification multi-labels (Herrera et al. (2016)) est une tâche d'apprentissage supervisé sur des instances multi-labels. La classification floue (Bouchon-Meunier et al. (1997)) où l'association entre chaque instance et les labels disponibles possède un degré dans l'intervalle [0, 1] est une généralisation de la classification multi-labels où le degré d'association est binaire (0 ou 1). Dans cet article on s'intéresse à la classification multilabels, et plus particulièrement au défi d'apprendre des relations entre les labels et les exploiter pour améliorer les prédictions (Loza Mencía et Janssen (2014) ;Loza Mencía et Janssen (2016)). Il existe deux types principaux de relations entre les labels :
-les relations de co-occurrence : par exemple, les émotions 'heureux' et 'triste' sont rarement associées au même morceau de musique, et les émotions 'heureux' et 'relaxant' peuvent être associées au même morceau de musique. -les relations de préférence : par exemple, pour un morceau de musique contenant plusieurs piques, le label à préférer entre 'relaxant' et 'heureux' pour l'associer à ce morceau de musique est le label 'heureux'. Les approches de classification multi-labels existantes peuvent apprendre soit uniquement des relations de co-occurrence, soit uniquement des relations de préférence (Gibaja et Ventura (2015)). Dans cet article, nous faisons l'hypothèse qu'une approche qui permet d'apprendre les deux types de relations entre les labels peut avoir une meilleure performance en prédiction que les approches existantes.
La suite de cet article est organisée comme suit : l'état de l'art des approches de classification multi-labels est discuté dans la Section 2 ; la nouvelle approche de classification multi-labels permettant l'apprentissage des relations de co-occurrence et de préférence entre les labels est présentée dans la Section 3 ; l'étude expérimentale comparant la nouvelle approche avec les approches existantes est présentée dans la Section 4.
Classification multi-labels
Description formelle de la classification multi-labels
Chaque instance x i est associée à un sous-ensemble de labels y i ⊆ C. L'ensemble de sous-ensembles de labels est noté P(C). Soit λ : X → P(C) la fonction qui associe chaque instance x i ∈ X au sous-ensemble de labels correspondant λ(x i ) = y i . La fonction λ est dite fonction de supervision de l'ensemble d'apprentissage X. Soit E : P(C) × P(C) → [0, 1] une fonction objectif à optimiser. La classification multi-labels consiste à apprendre à partir de l'ensemble d'apprentissage supervisé (X, λ) un classifieur H : a 1 × . . . × a p → P(C) qui prédit pour chaque instance x ∈ a 1 × . . . × a p l'ensemble de labels correspondants H(x). L'objectif du classifieur multi-labels est d'optimiser la fonction objectif E évaluant la prédiction H(x) par rapport au véritable sous-ensemble de labels y ⊆ C associé à x.
Approches de classification multi-labels
La classification multi-labels est une généralisation de la classification mono-label où chaque instance x i ∈ X ne peut être associée qu'à un seul label à la fois : |y i | = 1. La classification multi-labels peut être effectuée en adaptant les classifieurs mono-label au cas des instances multi-labels (Sun et al. (2016) ; Agrawal et al. (2016) ;Wang et al. (2015)). L'inconvénient de cette catégorie d'approches est qu'il faut modifier l'algorithme de classification pour modifier la stratégie d'apprentissage de relations entre les labels. Une autre stratégie pour construire un classifieur multi-labels consiste à appliquer des transformations sur les instances multi-labels pour se ramener au cas mono-label (Tsoumakas et Katakis (2007)). Cette catégorie d'approches possède deux principaux avantages :
-les classifieurs mono-label existants peuvent être utilisés pour gérer les sous problèmes de classification mono-label générés par la transformation des instances. -il est possible de modifier la stratégie d'apprentissage de relations entre les labels en modifiant juste la méthode de transformation sans modifier l'algorithme de classification de base. Les trois principales catégories d'approches de transformation du cas multi-labels au cas mono-label sont discutées dans la suite.
Approches basées sur la prédiction directe de sous-ensembles de labels
La classification multi-labels peut être transformée en classification mono-label en considérant chaque sous-ensemble de labels en tant qu'un nouveau label (Read (2008)). Soit C l'ensemble de nouveaux labels, et soit L : X → C la fonction qui associe à chaque instance de l'ensemble d'apprentissage
Le nouveau label L(x i ) correspond au sous-ensemble de labels y i initialement associé à x i . L'ensemble de nouveaux labels C peut contenir au plus n labels différents C = {c l } 1≤l≤n dans le cas où les sous-ensembles de labels
être construit à partir de X muni de la fonction de supervision L. Le classifieur multi-labels H fournit une prédiction pour une instance donnée x ∈ a 1 × . . . × a p en faisant la conversion du nouveau label prédit h(x) en un sous-ensemble de labels dans C. Le classifieur H est donné par :
). L'inconvénient de cette catégorie d'approches de transformation est que deux sousensembles de labels ayant des labels en commun sont considérés comme totalement différents dans le problème transformé. Par conséquent, les relations entre les labels appartenant à deux sous-ensembles de labels différents ne peuvent pas être apprises.
Approches basées sur la prédiction intermédiaire de la présence de chaque label
Soit λ c l : X → {0, 1} la fonction qui associe à chaque instance x i ∈ X la valeur 1 si le label c l est associée à l'instance x i (c l ∈ λ(x i )), et la valeur 0 sinon. L'approche 'Binary Relevance' (BR) consiste à construire un classifieur multi-labels
. L'inconvénient de l'approche BR est que les classifieurs {H c l } 1≤l≤k sont tous indépendants. L'approche BR ne permet donc pas d'apprendre des relations de co-occurrence entre les labels. L'approche 'Classifier Chains' (CC) est une extension de l'approche BR permettant l'apprentissage des relations entre les labels en introduisant un ensemble d'attributs descriptifs supplémentaires B = {b l } 1≤l≤k . Chaque instance x i est étendue telle que
, 1} ne peut pas fournir directement une prédiction pour une instance x ∈ a 1 × . . . × a p . En effet, la valeur de l'attribut b 1 est inconnue pour les instances qui ne font pas partie de l'ensemble d'apprentissage. L'instance x est donc d'abord étendue par la prédiction du classifieur H c1 : x = (x a1 , . . . , x ap , H c1 (x i )) avant d'être reçue par le classifieur H c2 . Chaque classifieur H c l a donc la possibilité de fournir des prédictions en se basant sur les prédictions des autres classifieurs qui le précèdent : {H c l } 1≤l <l≤k . L'inconvénient de l'approche CC est que les relations de co-occurrence qui peuvent être apprises dépendent de l'ordre initial des labels. Par conséquent, la prédiction d'un label c l ne peut pas dépendre d'une relation de cooccurrence avec les labels c l , l > l.
L'approche 'Aggregating Independent and Dependent classifiers' (AID) permet d'apprendre les relations entre les labels sans dépendre de l'ordre initial des labels en se basant sur deux ensembles de classifieurs (Montañés et al. (2011)). Le premier ensemble de classifieurs {h c l } 1≤l≤k est construit par l'approche BR (chaque classifieur h l est indépendant des autres classifieurs). Le deuxième ensemble de classifieurs {H c l } 1≤l≤k est construit de façon similaire à l'approche CC. La différence est que l'ensemble d'apprentissage X c l pour le classifieur H c l est construit en projetant l'ensemble étendu X e sur tous les attributs initiaux et supplémentaires sauf l'attribut b l à prédire : A ∪ B − {b l }. Ceci permet au classifieur H c l d'établir sa prédiction en se basant sur la présence ou l'absence des autres labels c l , l = l. Chaque instance donnée x ∈ a 1 × . . . × a p est étendue par les prédictions du premier ensemble de classifieurs
Chaque classifieur dépendant H c l fournit sa prédiction en se basant sur les prédictions initiales {h c l (x)} l =l et en ignorant les prédictions finales des autres classifieurs {H c l (x)} l =l . La prédiction du classifieur multi-labels H est donnée par :
L'approche AID possède deux inconvénients remarquables :
-elle nécessite l'apprentissage de 2k classifieurs -l'ensemble de labels prédit finalement n'est pas nécessairement en accord avec les relations apprises contrairement à l'approche CC. En effet, chaque label prédit finalement est en accord avec les relations apprises uniquement par rapport aux prédictions initiales {h c l (x)} l =l qui sont remplacées par les prédictions finales
L'approche 'Pre-selection, Selection, and Interest of chaining based classifier' (PSI) permet de combiner les avantages des approches CC et AID (Laghmari et al. (2016)). Un ensemble de classifieurs initiaux {H c l } 1≤l≤k est construit de la même façon que les classifieurs dépendants dans l'approche AID. Ceci permet d'apprendre initialement les relations entre les labels sans restriction. Un ordre de prédiction est établi ensuite afin de fournir des prédictions cohérentes avec les relations apprises comme dans l'approche CC. Le fait qu'un classifieur H c l soit appris en considérant l'ensemble étendu d'attributs A ∪ C − {c l } n'implique pas nécessairement que la prédiction de H c l dépend de tous les attributs A ∪ C − {c l }. L'ordre des classifieurs {H c l } est établi tel que chaque classifieur H c l soit précédé par les classifieurs dont il dépend. Le défi relevé par l'approche PSI est le cas d'une dépendance cyclique : par exemple, le cas où un classifieur H c l dépend de l'attribut b l , et le classifieur H c l dépend de l'attribut b l . L'ordre de prédiction entre H c l et H c l ne peut pas être donné dans ce cas. L'approche PSI est basée sur trois mesures appelées pré-selection, sélection, et intérêt de chaînage qui permettent d'éliminer les dépendances cycliques. La mesure de pré-selection fournit l'ensemble de classifieurs impliqués dans une dépendance cyclique. La mesure de sélection sélectionne un classifieur à remplacer par un nouveau classifieur. La mesure d'intérêt de chaînage fournit l'ensemble d'attributs à considérer par le nouveau classifieur de façon à apprendre les relations entre les labels sans retomber dans une dé-pendance cyclique. Lorsqu'un classifieur est remplacé certaines dépendances cycliques disparaissent mais pas nécessairement toutes. Les trois mesures PSI sont appliquées itérativement jusqu'à l'élimination de toutes les dépendances cycliques. L'approche PSI possède deux principaux avantages :
-même si certains classifieurs peuvent être appris deux fois pour éliminer une dé-pendance cyclique, seulement les k classifieurs finaux sont gardés par l'approche PSI. -l'ordre de prédiction est établi après l'apprentissage des classifieurs. Ceci permet de ne pas empêcher l'apprentissage de certaines relations au préalable comme dans l'approche CC.
L'inconvénient de toutes les approches basées sur la prédiction intermédiaire de la présence de chaque label avant de fournir une prédiction multi-labels (BR, CC, AID, et PSI) est que les relations exprimant une préférence entre les labels ne sont pas apprises.  
] la fonction qui fournit pour chaque label c l ∈ C le nombre de fois où il a été préféré pour une instance x (le nombre de votes pour le label c l ) :
L'approche RPC ne fixe pas une méthode de prédiction et permet juste d'ordonner les labels selon le nombre de fois qu'ils ont été préférés par les classifieurs {H c l ,c l } 1≤l<l ≤k . Il est possible par exemple de prédire les labels dont le nombre de votes est supérieur à un seuil fixé v. Le classifieur multi-labels H dans ce cas est donné par :
L'approche 'Calibrated Label Ranking' (CLR) est une extension de l'approche RPC qui permet de sélectionner les labels à prédire en utilisant un label virtuel au lieu d'un paramètre seuil ). L'approche CLR introduit un label virtuel c 0 et apprend k classifieurs de plus {H c l ,c0 } 1≤l≤k par rapport à l'approche RPC. La fonction de supervision correspondant au classifieur H c l ,c0 est donnée par :
Le classifieur multi-labels H prédit tous les labels qui reçoivent plus de votes que le label virtuel :
L'avantage des approches basées sur l'apprentissage de préférences (RPC, et CLR) est que l'ensemble d'apprentissage est réduit pour chaque classifieur permettant de prédire une préférence. Ceci est aussi un inconvénient car cela empêche d'apprendre les relations de co-occurrence puisqu'il n'y a pas nécessairement assez d'instances en commun entres les ensembles d'apprentissage de deux classifieurs.
Nouvelle approche de classification multi-labels
Soit X = {x 1 , x 2 , x 3 , x 4 , x 5 , x 6 } un ensemble d'apprentissage, et C = {c 1 , c 2 , c 3 , c 4 } un ensemble de labels (|C| = k = 4). La Table 1 représente les fonctions de supervision liant les instances aux labels correspondants. Par exemple, le classifieur H c1,c3 permettant de prédire la préférence entre c 1 et c 3 dans l'approche RPC (Section 2.2.3) est construit à partir du sous-ensemble d'apprentissage X c1,c3 = {x3, x6} muni de la fonction de supervision λ c1,c3 . Les instances associées au symbole ∅ dans la Table 1 sont ignorées à l'étape d'apprentissage du classifieur H c1,c3 . 
Expérimentation
Mesures de description des instances multi-labels
La performance des classifieurs multi-labels peut dépendre de la répartition des labels par rapport aux instances. Trois mesures sont souvent utilisées pour décrire la distribution des labels dans un jeu de données multi-labels (Tsoumakas et Katakis (2007)) :
-la cardinalité (label cardinality) qui évalue la moyenne du nombre de label associés à une instance : Tab. 2: Données multi-labels.
-la densité (label density) qui évalue la moyenne du nombre de labels associés à une instance par rapport au nombre total de labels :
-le nombre de combinaisons distinctes de labels (distinct label combinations) qui évalue le nombre de sous-ensembles de labels différents qui sont associés aux instances : DLC = |{y i } 1≤i≤n |.
Données et procédure d'expérimentation
Trois jeux de données multi-labels provenant de domaines différents sont utilisés pour comparer la performance de prédiction des classifieurs multi-labels (Table. 2). Le jeu de données des émotions (Trohidis et al. (2008)) contient un ensemble de 594 instances. Chaque instance représente un morceau de musique décrit par 72 attributs et associé à une ou plusieurs émotions parmi l'ensemble {amazed-suprised, happypleased, relaxing-calm, quiet-still, sad-lonely, angry-aggresive}. Le jeu de données des scènes (Boutell et al. (2004)) contient 2407 instances. Chaque instance représente une image décrite par 294 attributs et associée à un sous-ensemble de labels dans {Beach, Sunset, FallFoliage, Field, Mountain, Urban}. Le jeu de données des protéines contient 2417 instances décrites par 103 attributs (Elisseeff et Weston (2001)). Chaque protéine est associée à une localisation dite composant cellulaire. L'objectif est de prédire les localisations des protéines dans les cellules de levure.
La nouvelle approche introduite Stacked_RPC_PSI est comparée avec cinq autres approches existantes (AID, BR, CC, CLR, et PSI). Les arbres de décision (Quinlan (1993)) sont utilisés pour construire les classifieurs mono-labels de base. La méthode de validation croisée en 10 groupes est appliquée sur chaque jeu de données. La moyenne par rapport aux 10 plis pour les mesures d'évaluation de la prédiction est calculée pour chacun des trois jeux de données 'emotions', 'scenes', et 'yeast'. Tab. 5: Évaluation de la prédiction sur les données 'yeast'
Mesures d'évaluation de la qualité de prédiction
La qualité de prédiction peut être évaluée en se basant sur plusieurs mesures (Herrera et al. (2016)) :
La mesure de l'erreur de hamming (Hamming-loss) (Destercke (2014)) donnée par
, avec y i H(x i ) étant la différence symétrique entre l'ensemble correct de labels associés et l'ensemble de labels prédit donnée par :
L'erreur de Hamming évalue la proportion des erreurs entre les labels effectivement associés à l'instance et les labels prédits par rapport au nombre total de labels disponibles. Le nombre de labels disponibles représente le nombre maximal des erreurs possibles entre les labels effectivement associés à l'instance et les labels prédits. L'erreur de Hamming est une mesure très optimiste pour les données ayant une cardinalité et une densité faibles. Le classifieur multi-labels dans ce cas apprend à prédire des sousensembles de labels avec une cardinalité faible. Ainsi, même si aucun des labels prédits est effectivement associé à l'instance (y i ∩ H(x i ) = ∅) le nombre d'erreurs (|y i H(x i )|) reste petit par rapport au nombre maximal d'erreurs k parce que la cardinalité de l'ensemble y i et de l'ensemble H(x i ) est faible. Le score de Hamming (closely related Hamming score) (Godbole et Sarawagi (2004)) n'est pas sensible à la cardinalité et à la densité des labels. Il mesure le nombre de labels prédits correctement par rapport au nombre de l'union des labels prédits et des labels effectivement associés à l'instance :
La précision (precision) mesure la probabilité qu'un label prédit soit effectivement associé à l'instance :
Le rappel (recall) mesure la probabilité qu'un label associé à l'instance soit prédit :
La mesure F β (van Rijsbergen (1974)) combinant la précision et le rappel est donnée pour chaque β > 0 par :
importance est donnée à la précision pour les valeurs β < 1, et plus d'importance est donnée pour le rappel pour les valeurs β > 1. La même importance est donnée pour la précision et le rappel pour la valeur β = 1.
La moyenne géométrique (GMEAN) (Kubat et al. (1997)) est une mesure d'évalua-tion de la qualité de prédiction adaptée aux données avec un déséquilibre de labels (pré-sence de labels associés à presque toutes les instances, et des labels associés à très peu d'instances). En effet, toutes les mesures précédentes favorisent un classifieur qui prédit le label majoritaire en cas de déséquilibre de labels. La moyenne géométrique combine la précision positive donnée par
en une seule mesure donnée par : GMEAN = √ acc + × acc − . La correspondance exacte (exact match) est la mesure d'évaluation la plus stricte considérant la prédiction d'un ensemble de labels correcte seulement si l'ensemble pré-dit correspond exactement à l'ensemble de labels effectivement associés à l'instance :
Résultats et discussion
Toutes les approches exploitant les relations entre les labels fournissent générale-ment des résultats meilleurs que l'approche BR qui ne permet pas l'apprentissage des relations entre les labels (Tables 3 à 5).
La nouvelle approche Stacked_RPC_PSI fournit les meilleurs résultats pour les données 'emotions' et 'scenes', mais pas pour les données 'yeast'. En effet les données 'yeast' présentent un déséquilibre dans la distribution de labels : certains labels sont très rares ou sont prédominants. L'approche CLR fournit les meilleurs résultats pour les données 'yeast' parce qu'elle est basée sur l'apprentissage de préférence utilisant juste un sous-ensemble réduit de l'ensemble d'apprentissage. L'effet du déséquilibre de la distribution de labels est donc réduit pour l'approche CLR. Les approches CC, AID, et PSI sont basées sur les dépendances entre les labels qui peuvent propager les erreurs de prédiction. Dans le cas où la prédiction d'un label dépend de la prédiction d'un label rare qui ne sera presque jamais prédit, l'erreur de prédiction pour le label rare peut être propagée pour les labels dépendants. La nouvelle approche Stacked_RPC_PSI fournit des résultats meilleurs que les approches CC, AID, et PSI parce qu'elle est basée aussi sur l'apprentissage de préférences. Ceci confirme l'hypothèse que la combinaison des relations de co-occurrence et des relations de préférence peut améliorer les prédictions.
Conclusion
Apprendre les relations entre les labels et les exploiter pour améliorer les prédic-tions est un défi intéressant dans la classification multi-labels. L'approche RPC permet l'apprentissage des relations de préférences entre les labels, et l'approche PSI permet l'apprentissage des relations de co-occurrence entre les labels sans restriction au préalable. Ce travail introduit l'approche Stacked_RPC_PSI qui combine les deux approches RPC et PSI afin de bénéficier à la fois des relations de préférence et de cooccurrence pour améliorer la prédiction. L'expérimentation sur trois jeux de données montrent que l'approche Stacked_RPC_PSI est très compétitive avec les approches de l'état de l'art. L'inconvénient de l'approche Stacked_RPC_PSI est qu'elle est sensible au problème du déséquilibre de la distribution de labels (présence de labels rares ou de labels prédominants). Une idée qui pourrait réduire l'impact du déséquilibre de la distribution de labels dans l'approche Stacked_RPC_PSI consiste à éviter d'apprendre des dépendances par rapport aux classifieurs de préférence triviaux qui prédisent le label majoritaire.

Introduction
Dans de nombreux domaines économiques, industriels voire sociaux, il peut être intéres-sant d'évaluer l'impact d'un changement sur un gain attendu.
Il devient alors nécessaire d'établir un processus permettant d'évaluer l'impact de diffé-rentes alternatives d'une entité (médicament, page web,. . .) sur les gains et ainsi de choisir la plus optimale en fonction de ces derniers. Un Test A/B consiste à évaluer concrètement ces différentes alternatives par rapport à un objectif défini a priori. Pour cela, dans une première phase, bornée dans le temps, appelée phase d'exploration, les items (objets, personnes visées par test,. . .) sont soumis à l'une des alternatives de façon irrévocable. Ainsi, dans le cadre d'un test A/B sur une page web, un certain nombre fixé de visiteurs verront exclusivement la version A de la page, et l'autre partie des visiteurs la version B, jusqu'à la fin du test, et ce même en cas de revisite. À la fin de cette période, les gains cumulés des différentes alternatives (par exemple la somme des conversions . . .) sont comparés. L'estimation du gain moyen associé à chaque alternative se compare en général avec le regret cumulé, défini comme la différence entre l'alternative optimale (celle qui maximise le gain total) et les alternatives proposées. La meilleure alternative est alors choisie pour la phase d'exploitation, c'est-à-dire celle mise en production pour les nouveaux visiteurs.
Une première approche dite fréquentiste nécessite d'anticiper correctement le temps né-cessaire pour conclure sur la différence entre les variations et donc déterminer l'alternative optimale. Elle impose aussi de choisir un ratio fixe d'affectation des items à chaque alternative. Or il peut être intéressant de changer ce ratio au cours du test. En effet, il se peut qu'une alternative réalise très rapidement un gain cumulé important : il convient alors de terminer la phase de test (si cette alternative est optimale) ou de favoriser l'affectation des items à cette alternative. A contrario, une alternative présentant des résultats très médiocres, c'est-à-dire s'avérant sous-optimale, devra être désavantagée afin de limiter les pertes ou le manque à gagner.
Pour contourner ce problème, de nombreuses méthodes adoptent un mécanisme d'allocation dynamique. Ce mécanisme consiste à adapter le ratio d'affectations afin de basculer automatiquement l'affectation des items vers l'alternative optimale lorsqu'elle est identifiée.
Dans le domaine du test A/B, les modèles de bandits sont des stratégies d'allocation dynamique très utilisées.
Cependant, la quantité d'informations disponibles, autrement dit la taille du vecteur contextuel, influe fortement sur la performance des modèles de bandits contextuels (Chu et al., 2011). Par ailleurs, certaines caractéristiques peuvent être bruitées voire inutiles ou au contraire avoir un fort impact sur le gain généré par un item. Enfin, l'information peut être quantitative ou bien qualitative et dans ce dernier cas augmenter fortement la complexité.
Dans cet article, nous proposons un algorithme sélectionnant les informations les plus pertinentes pour identifier des sous-populations homogènes sur chacune desquelles un modèle de bandits indépendants est alors appliqué.
La section 2 de cet article introduit le principe du modèle de bandits contextuels et présente les principales méthodes existantes sur lesquelles nous nous appuyons. La section 3 donne notre proposition d'algorithme CTREE-UCB. La section 4 présente les résultats expérimen-taux. Enfin, la section 5 conclue et donne les perspectives de nos travaux de recherche.
État de l'art sur l'allocation dynamique
Une première stratégie, -greedy, (Auer et al., 2002a) consiste à allouer aléatoirement une proportion du trafic à la meilleure alternative et le reste aux autres alternatives existantes.
D'autres approches comme EXP3 (Auer et al., 2002b) ou UCB (Auer et al.a, 2002) utilisent la borne supérieure du gain moyen estimé de chaque bras pour affecter les items à la meilleure alternative. Après plusieurs itérations, l'intervalle de confiance converge pour chaque bras vers le gain moyen.
Les bandits contextuels estiment la moyenne du gain de chaque bras selon des informations caractérisant un item. Le plus utilisé LinUCB construit une régression linéaire à partir du contexte pour estimer les moyennes des gains obtenus par chaque bras.
Cependant, l'utilisation de ces bandits reste difficile pour plusieurs raisons. En effet, la complexité des algorithmes de bandits contextuels augmente avec la dimension du vecteur contextuel. De plus, la fonction de gain réel n'est pas nécessairement une fonction linéaire du vecteur contextuel. Enfin, identifier les données contextuelles qui ont un réel impact sur la récompense est une problème en soi.
En pratique, ces méthodes nécessitent donc une analyse préalable de la pertinence des données contextuelles qui s'avère, dans la plupart des cas, difficile à réaliser.
L'amélioration apportée par des modèles de bandits lorsqu'ils sont appliqués indépendam-ment sur différents sous-groupes a déjà été prouvée par Maillard et Mannor (2014).
Soient A un ensemble fini de N A bras et B un ensemble fini de N B sous-groupes de N items avec N B N . Chaque item n est défini par un vecteur contextuel f n .
Nous définissons {ν a,b } a∈A,b∈B comme une distribution de probabilités réelles dans l'intervalle [0; 1] de moyenne µ a,b ∈ R telle que ν a,b soit R-sous-gaussienne :
où R est une constante positive et X le gain produit par un item. La première idée de diviser les visiteurs en différents sous-ensembles provient de Agrawal et al. (1989).
Intuitivement, le bras choisi pour l'exploitation n'est pas nécessairement optimal pour un sous-ensemble de la population testée. Cela conduit à un regret linéairement croissant dépen-dant du gap ∆ 2 a,b . Pour identifier et construire des groupes homogènes, de nombreuses techniques existent : le lecteur trouvera une liste détaillée dans Kotsiantis (2007). L'une des méthodes qui nous a semblé la plus prometteuse et la plus adaptée à notre problème est basée sur l'apprentissage d'un arbre construit par partitionnements récursifs. Cette technique permet d'estimer différentes moyennes pour des variables explicatives spécifiques et de les identifier à travers une analyse de régression (Strasser et Weber, 1999). Des algorithmes comme CART (Breiman et al., 1999) et C4.5 (Salzberg, 1994) utilisent cette technique.
Enfin, CTree (conditional inference Tree), dérivé de C.A.R.T (Hothorn et al., 2006), est un algorithme non paramétrique intégrant des modèles de régression arborescents à travers des procédures d'inférences conditionnelles.
Contribution
Maillard et Mannor (2014) ont proposé l'algorithme Single-K-UCB qui affecte les items à différents sous-groupes et associe des modèles de bandits indépendants pour chacun de ses sous-ensembles. Ils supposent ainsi défini a priori l'ensemble B des sous groupes. De plus, ils supposent que les sous-groupes sont homogènes et que les distributions {ν a,b } a∈A,b∈B sont connues. Dans ce cas, ils ont montré que le regret cumulé est borné par le regret d'une souspopulation pour laquelle le bras choisi n'est pas optimal.
Les auteurs ont aussi prouvé que si des sous-groupes pouvaient être associés à des bras optimaux différents, l'utilisation de modèles de bandits indépendants pour chaque sous-groupe permet de réduire le regret cumulé.
Néanmoins, si ces méthodes ont prouvé leur efficacité dans certains cas, elles restent fortement dépendantes de données contextuelles utilisées. Un mauvais choix de celles-ci peut amener à des sous-groupes non pertinents et donc à conserver un regret linéaire continu dû à un ou plusieurs sous-groupes non identifiés.
Dans leurs approches la distribution des sous-groupes doit être connue à priori. Une autre hypothèse est que les sous-groupes aient des probabilités de gain différentes. Cependant, un utilisateur peut proposer des sous groupes ne respectant pas ces propriétés. Un tel algorithme ne sera alors pas plus performant qu'un algorithme non contextuel.
Ainsi, nous nous proposons de baser la construction de ces groupes sur différentes estimations de la récompense : l'idée est que des items produisant un même gain, indépendamment des alternatives possibles, doivent faire partie d'un même sous-groupe. Pour réaliser cette opé-ration, nous nous proposons d'utiliser une méthode d'apprentissage supervisée.
Nous proposons donc d'identifier automatiquement les variables les plus discriminantes pour ainsi identifier des sous-groupes homogènes en utilisant uniquement une distribution de gain issue d'un bras (qui peut par exemple être obtenue à partir de l'alternative originale au préalable du test).
Nous appliquerons ensuite un modèle de bandit à chaque sous-groupe lors de la phase d'exploration.
Pour cela, nous proposons d'identifier toutes les covariables pertinentes des items pour créer automatiquement ces sous-groupes optimaux sous un critère α de confiance.
Dans la suite, nous examinons comment la performance d'un algorithme de bandit peut être améliorée si elle s'applique à un sous-groupe où la probabilité de distribution du gain (indépendamment d'un bras) est très similaire entre chaque item de ces sous-groupes.
Soit une fonction de gain suivant une distribution de Bernoulli pour différents sous-groupes b ∈ B avec un bras optimal b existant, qui peut-être possiblement différent, pour chaque sous groupes.
Dans notre approche, nous identifions des sous-groupes qui maximisent la divergence de Kullback Leibler entre deux distributions ν a,b et ν a,b : KL(ν a,b , ν a,b ). Ces groupes sont basés sur les covariables f observables pour des items soumis à au moins un seul bras.
En pratique, cette approche, contrairement à d'autres algorithmes tels que LinUCB -permet d'identifier les covariables qui influencent la distribution, et n'utilise pas les variables non pertinentes, -présente une complexité moindre dans la mesure où elle utilise un nombre réduit de covariables et où elle estime l'intervalle de confiance du gain de chaque bras 1 , -est parallélisable du fait de l'utilisation de modèles de bandits indépendants. Dans certains domaines comme le marketing, un léger changement sur une page web peut avoir en pratique un très faible impact sur le désir d'achat d'un visiteur. Si une grande partie des visiteurs ne n'est pas affectée par le test, dans le meilleur des cas, les visiteurs seront tous classés dans un sous-groupe, dans le pire des cas, notre algorithme ne sera pas moins performant que les algorithmes de bandits existants, soumis à un regret linéaire. En revanche, nous supposons que l'environnement est stationnaire, ce qui signifie qu'un bras optimal ne peut devenir sous-optimal au cours du temps.
Expérimentations
Dans cette section, nous comparons CTREE_UCB à Lin-UCB et UCB. Nous associons à l'acceptabilité de la voiture la récompense à maximiser. L'accessibilité des voitures présente 4 niveaux associés à la valeur différente du gain possible : unacc, acc,good, vgood : ( -1,0,1,2).
Nous cherchons ici à proposer la taille du coffre à bagages (lug_boot) qui maximise l'acceptabilité de la voiture. Il y a 3 bras possibles, représentant 3 tailles de coffre différentes (small, med, big).
Nous sélectionnons uniquement les items ayant lug_boot = small issus du jeu d'entraî-nement. Les items des autres bras sur le jeu d'entraînement sont alors ignorés pour l'apprentissage.
Le tableau 1 renvoie les résultats selon la taille du jeu d'apprentissage. Le regret est calculé à partir de la moyenne des récompenses réalisées par les items ayant les mêmes caractéris-tiques.
Des expérimentations complémentaires sont actuellement réalisées pour déterminer la performance de l'algorithme CTree_UCB en faisant varier la taille du jeu de d'apprentissage, le nombre de bras, le type de variables explicatives, l'intervalle de confiance (I.C.) utilisé pour la génération de l'arbre, le type de gain et la différence entre les bras sur la fonction de gain.
Dans de nombreuses expériences, nous voyons que CTree_UCB concurrence Lin-UCB et UCB. De plus, il est possible de savoir quel bras est le plus adapté à chaque sous-groupe et de poursuivre l'exploration si le meilleur bras n'a pas encore été identifié.
Conclusion
Nous avons proposé un nouvel algorithme de bandits et montré comment il peut être utilisé pour résoudre certains problèmes concernant les tests personnalisés. Notre méthode propose d'explorer un nouveau type de compromis pour l'exploration contextuelle lorsque le gain n'est pas forcement un fonction linéaire du contexte.
Pour T itérations, K bras, et des vecteurs contextuels à d dimensions, le regret de notre algorithme est borné par O(log (KT )) contrairement au O( T * N f * ln 3 (K * T * ln T /δ) ) de LinUCB avec une constante δ. En pratique, la récompense n'est pas toujours une fonction linéaire du contexte. Dans certains cas, le regret sera linéairement croissant. Cependant, en utilisant un modèle pour chaque sous-groupe, nous obtenons un regret cumulé global inférieur à LinUCB et UCB.

Introduction
Lorsque les exemples d'apprentissage sont fournis de manière séquentielle, l'apprentissage incrémental pour une prise de décision s'avère une obligation (Greco et al., 2004). Générale-ment, la phase d'apprentissage est traitée par les techniques conventionnelles de l'apprentissage machine. Cependant, ces techniques demeurent sensibles au problème des données dés-équilibrées qui résulte de la répartition inégale entre les instances des classes de décision. Cette inégalité affecte considérablement la qualité de la décision en particulier quand il s'agit de données massives. Ce problème peut, toutefois, être surmonté par l'approche DRSA (Dominancebased Rough Set Approach) (Greco et al., 2001) qui repose sur les préférences et l'expertise des décideurs humains pour la construction d'un ensemble d'apprentissage afin de garantir la répartition égale des instances sur l'ensemble de classes de décision.
Ce travail propose une méthode MAI2P (Multicriteria Approach for the Incremental Periodic Prediction) basée sur l'approche DRSA pour la classification multicritère incrémentale et périodique. La méthode MAI2P se compose de trois phases. La première vise à construire une table de décision et repose sur trois étapes : l'identification d'un ensemble d'apprentissage ; la construction d'une famille cohérente de critères pour la caractérisation des actions ; et la classification de chaque action d'apprentissage dans l'une des classes de décision. La deuxième phase est basée sur notre algorithme DRSA-Incremental (Bouzayane et Saad, 2017) pour l'inférence et la mise à jour de l'ensemble de règles de décision. La troisième consiste à la classification des "Actions potentielles", en utilisant les règles précédemment inférées. L'approche MAI2P est validée sur le contexte des MOOCs (Massive Open Online Courses).
Le papier est structuré comme suit : La section 2 définit les notions de base de l'approche DRSA. La section 3 présente un état de l'art. La section 4 détaille la méthode MAI2P. La section 5 discute les résultats de l'expérimentation. La section 6 conclut le papier.
2 Préliminaires : Dominance-based Rough Set Approach L'approche DRSA développée par Greco et al. (2001) est dédiée au problème de tri en aide multicritère à la décision et inspirée de la théorie des ensembles approximatifs. Elle permet de comparer des actions à travers une relation de dominance, rendant compte des préférences d'un décideur, afin d'inférer les règles de décision. Cette approche définit une table d'information par un 4-uplets S= A, F, V, f tels que : A est un ensemble fini des actions de référence ; F est une famille cohérente de critères ; V est un ensemble des valeurs possibles des critères ; et
Chaque action de référence est affectée à une seule classe Cl t ; t ∈ {1, , N }.
Relation de dominance : La relation de dominance D P est définie comme suit :
Cl n signifie que "x appartient au maximum (minimum) à la classe Cl n ou bien à une classe au mieux (moins) aussi bonne que Cl n ".
Approximation inférieure P (Cl ≥ n ) (ou P (Cl ≤ n )) : regroupe toutes les actions dont l'ensemble P-dominant (P-dominé) est affecté avec certitude à des classes au moins (mieux) aussi bonnes que Cl n . En revanche, l'approximation supérieure regroupe toutes les actions dont l'affectation est réalisée d'une manière possible.
Règles de décision : L'ensemble de règles de décision est appelé modèle de préférences. Ces règles sont générées à partir de l'approximation inférieure et se présentent sous la forme :
Travaux antérieurs
Quelques approches dynamiques ont été proposées dans la littérature pour la mise à jour incrémentale des règles de décision suite à la variation de l'ensemble d'apprentissage.
Les auteurs dans (Greco et al., 2004) ont proposé un algorithme appelé Glance. Cet algorithme est basé sur des actions négatives. En effet, chaque règle d'une union donnée doit impérativement ne pas satisfaire x, si x n'appartient pas à cette union, mais elle peut aussi ne satisfaire aucune action x et dans ce cas elle demeure sans supports. Ces règles sans supports sont dites non-robustes et donc l'algorithme est aussi dit non robuste. L'algorithme Glance stocke dans la mémoire uniquement les règles de décision et pas les exemples d'apprentissage et donc il est économe par rapport à l'utilisation de l'espace mémoire. La complexité de l'algorithme est linéaire si on considère le nombre d'actions et elle est exponentielle en considérant le nombre de critères. Les auteurs dans (Li et al., 2013) ont proposé un algorithme de mise à jour incrémentale des approximations inférieures et supérieures de l'approche DRSA lors de l'ajout (ou la suppression) d'une seule action dans le système d'information. La méthode nécessite : premièrement, la mise à jour des unions inférieures et supérieures des classes de dé-cision, deuxièmement, la mise à jour des ensembles P-dominant et P-dominé de chaque action dans le système d'information et enfin la mise à jour des approximations inférieures et supé-rieures des unions des classes de décision. L'algorithme proposé minimise le temps de calcul lorsqu'une action entre ou quitte le système d'information sans affecter la qualité des règles de décision inférées.
Afin d'inférer des règles de décision robustes, nous choisissons de généraliser l'algorithme présenté dans Li et al. (2013)  Cette étape consiste à définir un ensemble d'apprentissage contenant un nombre suffisant d'exemples représentatifs pour chacune des classes de décision prédéfinies. Afin de respecter la terminologie utilisée dans l'approche DRSA, nous appelons les exemples d'apprentissage, "Actions de référence". La construction de cet ensemble s'effectue par un ou plusieurs déci-deurs en fonction de leur expertise et leur expérience. D'un point de vue psychologique (Miller, 1956), un décideur humain se caractérise par une capacité cognitive représentant la limite supérieure à laquelle il peut associer ses réponses aux stimuli qui lui sont accordés. Ainsi, pour la construction de l'ensemble d'apprentissage, il est suffisant que les actions sélectionnées soient représentatives et de qualité, quel que soit leur effectif. L'intervention des experts pour la construction de l'ensemble d'apprentissage permet d'obtenir des sous-ensembles équitables des "Actions de référence" et de surmonter le problème de données déséquilibrées.
La méthode MAI2P doit être appliquée sur les systèmes d'informations qui évoluent dans le temps, où l'ensemble des "Actions de référence" varie d'une période à une autre. Ainsi, chaque période P i , le décideur doit définir un nouvel ensemble A i des "Actions de référence" qui se rajoute à l'ensemble des "Actions de référence", A i−1 , de toutes les périodes précédentes.
Etape 2 : Construction d'une famille cohérente de critères
Comparé à un attribut, un critère doit permettre de mesurer les préférences des décideurs selon un point de vue personnel (Mousseau et al., 1996). Dans ce travail, l'approche que nous adoptons est ascendante qui consiste à construire une famille de critères à partir d'une liste d'indicateurs susceptibles d'influencer l'opinion des décideurs concernant la caractérisation des actions. Ensuite, des réunions directes doivent être menées avec le décideur afin d'obtenir ses informations préférentielles sur chaque critère. Afin d'appliquer les points de vue préféren-tielles, nous adoptons une échelle qualitative.
Etape 3 : Classification de l'ensemble des "Actions de référence"
Cette étape consiste à la construction d'une table de décision D i de la période P i . C'est une matrice dont les colonnes représentent les "p" critères d'évaluation contenus dans F i et dont les lignes forment un ensemble de "m" "Actions de référence" contenues dans A i . Le contenu de la matrice est la fonction d'évaluation f i (A j,i ,g k ) de chaque action A j,i ∈ A i sur chaque critère g k ∈ F i tel que i ∈ {1..T }, j ∈ {1..m} et k ∈ {1..p}. Les variables T, m et p sont respectivement le nombre de périodes à considérer pendant le processus de prédiction, la taille |A i | de l'ensemble des "Actions de référence" définit à la i ` eme période et la taille de l'ensemble |F i | de la famille de critères. La dernière colonne de la table contient la décision d'affectation de chaque "Action de référence" dans l'une des N classes de décision.
Phase 2 : Mise à jour incrémentale des approximations de DRSA
Cette phase applique notre algorithme DRSA-Incremental (Bouzayane et Saad, 2017) sur la table de décision D i construite pendant la période P i afin d'en inférer un modèle de préférence, M P i , susceptible de classer chaque action dans l'une des classes de décision prédéfinies.
Cette phase est appliquée dès que la table de décision est complète. Elle considère l'ensemble des "Actions de référence", A i−1 , de toutes les périodes précédentes et l'ensemble des "Actions de référence", A i , de la période P i . L'algorithme DRSA-Incremental est déclenché dès l'insertion de l'ensemble A i dans la table de décision. Il est composé de quatre étapes : 1. Calculer les unions supérieures et inférieures de chacune des classes de décision Cl i−1 .
2. Calculer les ensembles dominants et dominés pour chaque action insérée x + ∈ A i . 3. Mettre à jour les ensembles dominants et dominés pour chaque action A j,i−1 ∈ A i−1 .
4. Mettre à jour les approximations de chacune des unions de classes de décision.
La sortie de la phase 2 est un modèle de préférence permettant de classer les "Actions potentielles" pendant la période P i+1 .
4.3 Phase 3 : Classification des "Actions potentielles" de la période P i+1
La troisième phase exploite les règles de décision précédemment inférées afin d'attribuer chacune des "Actions potentielles" dans l'une des N classes de décision prédéfinies. Une "action potentielle" est une action susceptible d'être classée dans l'une des classes de décision.
Cette phase s'exécute pendant la période P i+1 tout au long du processus de prédiction tel que i ∈ {2, . . . , T }. Elle commence par l'évaluation de toutes les "Actions potentielles" sur l'ensemble de critères construits. Ensuite, il s'agit d'appliquer les règles de décision inférées pendant la période P i afin de les affecter dans les classes de décision prédéfinies. La méthode MAI2P s'exécute périodiquement tout au long du processus de prédiction : la première et la deuxième phases se déroulent pendant toutes les périodes P i tel que i ∈ {1, . . . , T − 1} alors que la troisième se déroule pendant la période P i ; tel que i ∈ {2, . . . , T }.
Expérimentation et évaluation de la méthode MAI2P
Nous avons traité le cas d'un MOOC (formation en ligne et gratuite) Français qui a duré 5 semaines et accédé par 2360 apprenants. L'objectif est la prédiction hebdomadaire de la classe de décision à laquelle appartiendra un apprenant : Cl 1 des "Apprenants en risque d'abandon" ; Cl 2 des "Apprenants en difficulté" mais qui sont actifs ; et Cl 3 des "Apprenants leaders".
-P hase1. Nous avons construit, avec l'aide de l'équipe pédagogique, quatre ensembles des "Apprenants de référence" A i tel que i ∈ {1, 2, 3, 4} et |A i | = 30. Ensuite, une famille cohérente de 11 critères a été définie dont 8 sont statiques (exp. niveau d'études) et 3 sont dynamiques (exp. le nombre hebdomadaire de messages). Enfin, à la fin de chaque semaine une table de décision est construite. -P hase2. Cette phase a été appliquée à la fin de chaque semaine S i une fois que la table de décision D i est complète tel que i ∈ {1, 2, 3, 4} en appliquant l'algorithme DRSA-Incremental pour la mise à jour incrémentale des règles de décision. -P hase3. Cette phase était appliquée au début de chaque semaine S i du MOOC en appliquant le modèle de préférence inféré à la fin de la semaine S i−1 pour la classification de l'ensemble d'apprenants potentiels tel que i ∈ {2, 3, 4, 5}.
FIG. 2 -Qualité de la prédiction (F-mesure) durant les semaines du MOOC
La Figure 2 met l'accent sur la variation de la F-mesure des trois classes de décision Cl 1 , Cl 2 et Cl 3 d'une semaine à une autre.
-La F-mesure de la classe Cl 1 , des "Apprenants de risque", augmente au cours du temps. En effet, le MOOC est connu par les lurkers. Ces apprenants restent actifs au bout de la première semaine mais en ayant une intention préalable d'abandonner la formation.

Introduction
Dans cet article, nous étudions le clustering conjoint de plusieurs bases de données distribuées (nommées vues), aussi appelé Clustering Collaboratif (CC). Une méthode de CC applicable à des données arrivant en continu permet de résoudre des problèmes en temps réel avec une contrainte de confidentialité sur les données. Cet article constitue un résumé de l'article publié dans la conférence ICONIP 2017 (Maurel et al. (2017)).
L'objectif du CC (Cornuéjols et al. (2018)), est de trouver une manière de partitionner un même ensemble d'individus décrits par différents ensembles de caractéristiques. Pour ce faire, les vues vont échanger des informations, sans pour autant échanger les valeurs qu'elles contiennent (dans un soucis de confidentialité). Cet objectif est atteint par l'intermédiaire de vecteurs synthétisant l'information contenue dans chaque base sous forme d'individus repré-sentatifs de la distribution des données. Ces individus sont appelés les prototypes de la vue.
Le CC peut être décomposé en deux phases : la phase locale durant laquelle chaque algorithme de clustering est appliqué localement afin d'obtenir les prototypes de la vue, et la phase collaborative, durant laquelle chaque vue fournit à ses pairs les informations sur ses prototypes afin de partager ce qui a été appris localement.
La création d'une méthode de CC incrémental présente plusieurs défis. Le premier est que, dans notre cas, le CC se base sur des algorithmes de clustering à base de prototypes (Ghassany et al. (2013)), réduisant de fait le nombre de méthodes utilisables. Le deuxième est que les méthodes de clustering incrémentales ne sont pas forcément compatibles avec le CC, même si elles se basent sur des prototypes. Enfin, le troisième défi se trouve dans l'adaptation nécessairement ad hoc des règles de mise à jour collaboratives suivant l'algorithme employé.
Ce papier présente une méthode d'apprentissage de SOM incrémentales robuste aux éven-tuelles évolutions de la distribution des données et compatible avec le paradigme du CC. La composante principale de cette approche se trouve dans la modification de la fonction de température des SOM, qui devient indépendante du temps. À notre connaissance, ce type de mé-thode n'a encore jamais été proposé dans la littérature.
Cet article est organisé comme suit : un bref bilan des méthodes de clusterings incrémen-tales et collaboratives est présentée dans la Section 2. Notre approche sur les SOM incrémen-tales et sur leurs applications au CC est présentée dans la Section 3, suivie par les résultats expérimentaux présentés en Section 4. Une conclusion ainsi qu'une ouverture sur les futures pistes à suivre sont présentées en Section 5.
Recherches associées
Un état de l'art sur le CC peut être trouvé dans Cornuéjols et al. (2018). Cet article présente les principales spécificités du domaine ainsi que ses principaux défis.
Le CC basé sur les SOM a été étudié dans Grozavu et al. (2014); Rastin et al. (2015) de même que sa version basée sur les Generative Topographic Mapping (GTM) ; Ghassany et al. (2013)) et sur Fuzzy C-Means (Pedrycz et Rai (2008); Mitra et al. (2006)). Néanmoins, les méthodes proposées dans ces articles ne fonctionnent pas dans le contexte incrémental qui est étudié ici. Cette contrainte a déjà été étudiée pour les SOM noncollaboratives dans des travaux uniquement dédiés au clustering incrémental : les méthodes proposées par Deng et Kasabov ou encore Papli´nskiPapli´nski (2012) reposent ainsi sur l'évolution topologique de la SOM au cours du temps. Ces références mettent en lumière que même si des méthodes existent pour chaque sous-partie du problème, il n'existe à notre connaissance aucune méthode permettant de faire du CC basé sur les SOM sans modification topologique de ces dernières.
SOM incrémentale
Dans notre version incrémentale des SOM, nous considérons que les données arrivent en continu. Ainsi, nous supposons qu'à chaque instant, le modèle n'a connaissance que du batch B des N batch derniers individus. La contrainte incrémentale ne permettant pas de définir de temps d'arrêt, notre méthode présente une variation de la fonction de température afin d'éviter toute dépendance temporelle. La nouvelle fonction de température λ est définie par :
Où x i et χ(x i ) correspondent respectivement à un individu du batch et à son plus proche prototype dans la vue étudiée. Cette fonction λ est ensuite bornée entre λ min et λ max afin d'éviter qu'elle ne prenne des valeurs extrêmes, ce qui entraînerait des modifications trop importantes de la topologie de la carte.
Cette définition permet à la carte d'être réactive à la nouveauté. Si les éléments d'un batch sont éloignés des neurones courants, l'ensemble de la carte aura besoin d'être ajusté (hautes valeurs de λ). À l'inverse, si les individus sont proches des prototypes courants, la carte n'aura besoin que de d'ajustements locaux (faibles valeurs de λ). La fonction de voisinage définie par λ sera désignée par K. Dans la suite de cet article, K m i,j désignera la valeur de la fonction de voisinage entre les neurones i et j de la m-ème SOM (et donc de la m-ème vue).
Adaptation au CC
On considère les bases de données {X[i]| i ∈ 1..P } contenant le même ensemble d'individus décrits par différents ensembles de caractéristiques, avec P modèles (ici des SOM) entraînés à représenter chacune des vues. Afin de clarifier les notations, W m∈{1..P } désignera le m-ème modèle créé en utilisant la m-ème base de donnée. On impose de plus le critère d'apprentissage suivant : des neurones correspondants ou leurs voisinages proches devront capturer les même individus indépendamment de la vue considérée.
Afin d'adapter le critère original à la version incrémentale du CC, nous l'approximons en utilisant la nouvelle fonction de voisinage K et en sommant les distances sur le batch courant plutôt que sur l'intégralité des individus :
Avec α et β les coefficients de collaboration fixés et qui définissent les pondérations des termes locaux et collaboratifs dans le critère d'apprentissage. Une synthèse du CC horizontal incrémental peut être trouvée dans l'Alg. 1. Dans un soucis de concision, la formule de Durant ces expérimentations, chaque base a été normalisée puis divisée en 3 vues contenant chacune un tiers des variables originales. Nous supposons ici que l'on dispose de suffisamment d'information sur chaque variable pour permettre sa normalisation au moment où elle apparaît, par exemple en connaissant ses bornes. Les mesures de qualité ici utilisées sont l'erreur de quantification et l'index de pureté communément utilisés pour l'analyse de SOM.
Expérimentations
Dans un soucis de concision, seuls les résultats obtenus sur Isolet sont présentés, ces derniers pouvant être généralisés pour l'ensemble des bases étudiées. Les SOM utilisées sont composées de 10 × 10 neurones, avec λ min = 0.3, λ max = 3, = 0.5 (pas d'apprentissage fixé durant nos expérimentations), N batch = 10. Ces paramètres ont été obtenus empiriquement après plusieurs apprentissages.
Les puretés respectives des cartes peuvent être trouvées sur les Fig. 1a, Fig. 1b et Fig. 1c. Il apparaît que le CC améliore la pureté au détriment de la stabilité par rapport aux SOM incré-mentales. La stabilité réfère ici à l'écart type de la pureté au cours du temps. Cette instabilité peut être causée par l'apprentissage par batchs qui, par définition, ne sont pas représentatifs de la population globale. Néanmoins, le fait que la méthode aboutisse quand même à un résultat laisse penser que les biais successifs se compensent sur le long terme. Dans un second temps, l'impact de la phase collaborative de notre méthode sur le clustering final a été étudié. Sur chaque base de données, deux apprentissages ont été effectué : le premier intitulé SOM Incrémentale (SOMI) ne comprenait que la phase locale de l'Alg. 1 tandis que le second, intitulé CC Incrémental (CCI), comprenait l'ensemble de la méthode. Les résultats de cette expériences sont présentés dans Tab. 1. Les résultats présentés nous permettent de conclure que dans la plupart des cas, la phase collaborative n'améliore pas significativement les résultats des SOM incrémentales utilisées seules. Cependant dans le cas de la base Isolet, la phase collaborative améliore nettement les résultats obtenus. Il est apparu que la base Isolet était une base clairsemé (ou sparse en anglais). Cette caractéristique couplée à la division de la base initiale en 3 vue a réduire l'information déjà limitée disponible pour chaque clustering local. Avec l'ajout de la phase collaborative, l'information contenue dans chaque vue a pu être partagée, ce qui a permis de nettement amé-liorer les résultats par rapport aux clustering locaux. C'est ce résultat qui nous permet de justifier l'intérêt de l'ajout de la phase collaborative de notre méthode.
Spam
Conclusion et Futures Recherches
Dans cette étude, nous avons présenté une méthode permettant de générer des SOM incré-mentales sans modifications topologiques ainsi que leurs application au CC horizontal. Cette méthode se base sur une nouvelle fonction de température λ qui ne dépend plus que des données arrivantes, ce qui permet l'adaptation de la carte à un flux de données continu. Les mé-thodes présentées ont été testées sur 4 bases de données différentes. L'influence du paramètre N batch sur la stabilité de l'apprentissage a aussi été analysée.
Pour poursuivre ces travaux, nous prévoyons d'adapter notre méthode aux GTM, par nature proche des SOM et qui sont susceptibles d'améliorer la qualité de l'apprentissage.



Introduction
De nombreux travaux récents s'intéressent à la similarité sémantique, soit entre mots, soit entre groupes de mots, depuis les syntagmes jusqu'à des documents complets, en passant par la similarité entre phrases. Rapprocher des mots ou des phrases par leur sens permet d'utiliser des traits sémantiques dans des modèles en évitant la dispersion inhérente liée à la taille du vocabulaire, ou à l'espace des phrases possibles. La plupart des travaux dans ce sens calculent des similarités entre des représentations construites sur des bases distributionnelles, c'est à dire où la similarité de sens dérive d'une similarité des contextes d'apparition des mots, une hypothèse énoncée par Harris (1954).
Les représentations prennent la forme de vecteurs, matrices et tenseurs distributionnels (Turney et Pantel, 2010), où les dimensions correspondent à des co-occurrences lexicales (Curran, 2004), syntaxiques (Baroni et Lenci, 2010), ou bien des transformations de ces contextes, par réduction de dimension (Pennington et al., 2014) ou par l'intermédiaire d'apprentissage avec des réseaux de neurones (Mikolov et al., 2013). L'évaluation de ces modèles vectoriels repose soit sur des tâches externes où ils sont mis en jeu, soit sur des mesures intrinsèques, fondées sur des échantillons de mots similaires, ou des groupes de mots similaires.
Plus récemment, la notion de similarité sémantique textuelle motive des représentations vectorielles au delà de mots seuls. La représentation phrastique peut se construire par composition des représentations lexicales (Mitchell et Lapata, 2008;Van de Cruys et al., 2013) ou là encore être construite par l'intermédiaire d'un apprentissage par réseau de neurones (Le et Mikolov, 2014).
Les méthodes appliquées à la reconnaissance de paraphrases ou de similarités s'appuient donc sur des représentations vectorielles et sur différentes façons de les combiner, et utilisent aussi des appariements des éléments de phrase (Sultan et al., 2015). Un problème crucial pour ces représentations vectorielles est l'existence de nombreux hyper-paramètres dans leur construction et dans leur combinaison pour la définition de la similarité textuelle. De même, il existe peu de travaux qui tentent de combiner différentes représentations pour tirer parti d'éventuelles complémentarités des choix effectués en amont.
Nous présentons ici un travail sur la combinaison des représentations vectorielles pour explorer le potentiel de l'association de ces représentations à différentes échelles. Nous revenons dans la section suivante plus en détail sur la tâche de similarité sémantique textuelle avec un bref état de l'art de la campagne SemEval. La section 3 présentera nos motivations et hypothèses qui justifieront cette recherche de complémentarité des représentations vectorielles. Nous détaillerons ensuite notre méthode de recherche de complémentarité à travers deux algorithmes optimisant deux critères différents en section 4. Enfin, nous discuterons des résultats obtenus en les comparant aux résultats de la campagne de l'année 2016.
Similarité sémantique textuelle
Nous revenons brièvement sur les modèles vectoriels de mots et de phrases, pour discuter de leur place dans la tâche spécifique de mesure de similarité textuelle.
Modèles vectoriels
La représentation du texte dans un espace vectoriel est une technique de plus en plus utilisée ces dernières années dans plusieurs disciplines du TAL, comme en analyse de sentiment et en traduction automatique (Le et Mikolov, 2014). Cette représentation permet de « rapprocher » des mots, des phrases, et de manière générale, du texte, sans passer par une représentation précise de tous les éléments du sens. La représentation vectorielle permet ainsi de construire une forme pouvant positionner les mots les uns par rapport aux autres.
Chaque mot étant réduit à un vecteur de nombres, il est possible de calculer une similarité avec des mesures simples telle que la similarité cosinus. La construction des représentations vectorielles est possible par apprentissage non-supervisé sur de larges corpus. Les différentes techniques de construction de ces vecteurs tiennent compte, pour chaque mot, de leurs voisins dans le texte, c'est à dire des mots qui sont proches dans une même phrase : le « contexte ».
Les vecteurs se prêtent également bien à différentes méthodes de composition. Il est possible de composer plusieurs mots afin de construire le vecteur d'une phrase en faisant la moyenne des éléments des vecteurs de tous les mots. D'autres méthodes s'attaquent directement à la représentation de phrases sans passer par la composition de vecteurs de mots (Le et Mikolov, 2014). Des approches supervisées permettent également la représentation de phrases grâce notamment aux réseaux de neurones profonds tels que les recursive networks, les recurrent networks, ou encore les convolutional networks.
La tâche SemEval STS
La tâche STS propose de mesurer quantitativement la similarité sémantique de deux phrases sur une échelle de 0 à 5, la valeur 5 signifiant que les phrases sont strictement identiques sur le plan sémantique. Chaque paire de phrases mise à disposition depuis 2012 a été évaluée par plusieurs annotateurs humains rémunérés via la plate-forme de microworking Amazon Mechanical Turk, en éliminant les annotateurs peu fiables et les phrases dont la mesure montre trop de variance. À chaque paire de phrases correspond une note moyenne, qui peut donner des valeurs réelles dans tout l'intervalle [0,5].
Ce sont au total plus de 14 000 paires de phrases qui ont été mises à disposition dans le cadre de cette tâche depuis 2012. Le score d'un système est calculé, par la corrélation de Pearson entre leurs résultats et les données de test annotées pour l'année courante. Chaque équipe peut proposer trois systèmes et les entraîner sur les données des années antérieures.
De manière générale, les participants à la tâche l'abordent comme un problème de régres-sion supervisée, et utilisent des descripteurs classiques comme le nombre de mots en commun, de séquences de différentes longueurs en commun, etc. Des mesures de similarité basées sur un alignement de mots et des mesures provenant du domaine de la traduction automatique sont aussi utilisées. En 2016, les trois participants vainqueurs ont exploité de nouvelles techniques de représentations vectorielles de phrases, toutes basées sur des architectures de réseaux de neurones profonds comme Rychalska et al. (2016) qui ont utilisé un Recursive Neural Network.
Motivations
Hypothèse
Une comparaison basique entre deux phrases consisterait à calculer leur similarité topicale et lexicale. D'autres méthodes provenant de la sémantique distributionnelle permettent, quant à elles, de représenter l'ensemble de la phrase dans un espace sémantique commun à toutes les phrases d'un corpus donné. Ces méthodes demandent le prétraitement d'un corpus et de paramétrer l'algorithme qui apprend à représenter chaque phrase. Dans cet article, nous proposons une méthode qui tente de représenter plus finement les phrases sans se restreindre à un paramétrage précis. En l'occurrence, nous pensons que l'humain est capable de cibler différents aspects sémantiques dans l'objectif de comparer des morceaux de texte sur plusieurs plans.
Ces différents aspects peuvent, par exemple, être le sujet traité (topic), l'action dans la phrase, le mouvement, les entités impliquées, les informations spatio-temporelles, etc.
En TAL, on cherche en général à trouver le meilleur algorithme qui génère les représen-tations vectorielles, ou on essaye d'optimiser les paramètres de certains algorithmes. À notre connaissance, aucun travail ne tente de détecter automatiquement les aspects sémantiques qui permettraient une comparaison textuelle optimale de niveau humain.
C'est ce que nous proposons de faire grâce à la variation d'un certain nombre de paramètres de prétraitement de phrases et de construction de leur vecteur représentatif. Nous évaluons notre méthode sur la tâche de Semantic Textual Similarity (STS) des campagnes SemEval. Et afin de capturer automatiquement des aspects sémantiques permettant une comparaison optimale, nous optimisons la sélection de différentes représentations vectorielles sur le critère de la complémentarité dans la tâche de STS. Cette variation peut permettre de cibler différents aspects de la phrase, et ainsi faire un jugement de pertinence au plus proche de l'humain.
Dans notre article, nous définissons une suite de représentations vectorielles les plus complémentaires possibles comme une suite de représentations, qui, associées, permettent d'obtenir les meilleurs résultats sur la tâche en exploitant une diversité dans l'affectation des paramètres de prétraitement du corpus et de construction des vecteurs.
Nous proposons d'utiliser une extension de Word2Vec (Mikolov et al., 2013), communé-ment appelée Doc2Vec (Le et Mikolov, 2014) permettant de représenter un document ou un ensemble de phrases dans un espace sémantique. Nous avons utilisé l'implémentation Doc2Vec de Gensim ( ˇ Rehůřek et Sojka, 2010).
Variation de paramètres
Notre contribution se fonde sur l'hypothèse que la combinaison de différentes représen-tations vectorielles peut améliorer la qualité du score de similarité calculé, si ces représenta-tions sont suffisamment complémentaires. Plus concrètement, nous pensons qu'une variation des paramètres de prétraitement des corpus et des paramètres de construction des vecteurs peut diversifier la représentation sémantique, et que l'obtention des représentations les plus complé-mentaires est en mesure d'orienter les calculs de similarité sur différents aspects sémantiques. Les paramètres pris en compte dans notre système sont les suivants : size (entre 2 et 10000) indique la taille des vecteurs générés. Une même phrase représen-tée sur 50 dimensions ou 3000 dimensions portera des indices sémantiques différents, dans la granularité de ces indices (aspects topicals ou aspects sémantiques plus fins).
removePunct (vrai ou faux) indique la suppression ou non de la ponctuation de chaque phrase. La structure de la phrase variera selon la valeur de ce paramètre. Une virgule indiquera une séparation entre deux parties de phrase. Un point d'interrogation indiquera que la phrase est une interrogation ou une demande, et que les informations énoncées ne sont probablement pas factuelles. En revanche, supprimer toute ponctuation produira des vecteurs ne se focalisant que sur les mots et leurs voisins.
window (entre 1 et 20) définit la taille de la fenêtre de contexte de chaque mot. Plus la fenêtre est grande, plus le contexte d'un mot sera constitué de voisins éloignés dans la phrase, à gauche comme à droite. Par exemple, dans la phrase "Bob joue du piano", une window de 1 (qui correspondra à une fenêtre de 3 mots, sauf au bord des phrases) centré sur Bob prendra en compte l'action (le verbe jouer) alors qu'une window de 4 considérera aussi sur quoi s'applique l'action (le piano).
toLowerCase (vrai ou faux) indique la conservation ou non des majuscules présentes dans la phrase. Les majuscules peuvent, par exemple, permettre de différencier le nom commun d'un nom personnel. Leur suppression permet de réduire les noms personnels à leur forme commune si celle-ci existe et permet également de ne pas différencier un même mot en début de phrase ou non.
removeStopWords (vrai ou faux) indique si les mots vides sont supprimés de chaque phrase ou non. Les mots vides sont des mots peu informatifs mais qui permettent de lier les mots informatifs et structurer la phrase. Leur suppression permettra de focaliser l'analyse sur les mots sémantiquement riches. Au contraire, les conserver permettra de mieux représenter l'enchaînement des mots sémantiquement riches.
lemma (vrai ou faux) indique si la phrase est lemmatisée ou non. Une phrase lemmatisée perd en information sémantique puisque, par exemple, en supprimant la conjugaison, le lien entre un verbe et son sujet est affaibli. Une lemmatisation rendra la taille du vocabulaire beaucoup moins grande et les phrases seront plus proches, ce qui accentuera leur similarité « thématique ».
D'autres paramètres, directement liés à la génération des vecteurs par l'algorithme utilisé, ont aussi subi des variations : alpha, iter, sample, negative et min_count. Le lien entre certains hyper-paramètres liés à la constitution des vecteurs et le résultat final n'est pas simple à dé-terminer. Nous proposons donc une méthode d'optimisation qui cherche les combinaisons de paramètres les plus complémentaires.
La figure 1 schématise notre méthode en trois étapes fondamentales. Dans un premier temps l'optimisation des paramètres qui permet d'obtenir un ensemble de modèles classés selon leur performance avec la variation de paramètres décrite en sous-section 3.2. Conjointement à cette optimisation, nous générons des modèles avec des paramètres choisis au hasard. Ensuite, l'algorithme de sélection topdesc associe des modèles suffisamment différents en parcourant le classement issu de l'étape 1. Enfin, l'algorithme de sélection topdelta analyse les séries obtenues à l'étape 2 et associe les modèles les plus complémentaires, qui présentent une forte plus-value dans l'ensemble des séries. Nous mettons à disposition le code source python via la plateforme GitHub 1 .
Combinaison de modèles complémentaires 4.1 Optimisation des paramètres
Nous commençons par générer des modèles 2 en cherchant les paramètres optimaux par une méthode d'optimisation de type recherche locale. Les onze paramètres cités en sous-section 3.2 ont été utilisés pour chaque modèle. Un modèle est généré grâce à l'outil Doc2Vec. Doc2Vec 1. Le premier correspond à un score d'alignement entre deux phrases. Ce score est obtenu en fonction du nombre de mots que l'aligneur a réussi à relier entre les deux phrases selon différentes métriques (dictionnaire de synonymes, distance de Levenshtein, etc).
2. Le second descripteur correspond à une similarité cosinus reprenant les vecteurs de Baroni et al. (2014).
Lorsque nous intégrons un modèle, cela signifie que nous prenons chaque vecteur de chaque phrase (i.e. les représentations vectorielles issues du modèle entraîné avec une certaine
FIG. 2 -Graphique rassemblant les trois types de séries
combinaison de paramètres), puis nous calculons les similarités cosinus entre chaque paire de phrases. Ces similarités sont ajoutées en descripteur dans le système DLS 2015. Par la suite, l'utilisation de plusieurs modèles produira donc plusieurs descripteurs.
Pour notre première phase d'optimisation, nous n'utilisons qu'un seul modèle en descripteur additionnel. Nous évaluons un modèle (défini par son affectation de paramètres) par la performance du système DLS 2015 avec, en plus, le descripteur de ce modèle. Enfin, chaque modèle est classé selon ses performances par évaluation croisée sur toutes les données anté-rieures à celles de l'année 2016.
Algorithme de sélection topdesc
Une fois ce classement obtenu, nous utilisons l'algorithme topdesc pour sélectionner des « séries » de modèles 3 . L'algorithme topdesc prend en entrée une « description », parcourt tous -le modèle sélectionné doit être suffisamment différent des autres ; -celui-ci doit être performant lorsqu'il est utilisé seul. La description de la série se compose de plusieurs éléments :
1. les paramètres à différencier ; 2. la différence minimum avec les modèles déjà sélectionnés pour chaque paramètre à différencier ;
3. le nombre de modèles à sélectionner.
La figure 3 montre un exemple de description. La première ligne signifie que l'algorithme doit sélectionner 20 modèles avec des size et window variant respectivement d'au moins 100 et 2, ensuite 6 modèles variant sur quelques paramètres de prétraitement, et ainsi de suite. Tous les autres paramètres qui n'entrent pas dans la « différenciation » seront les paramètres optimaux trouvés lors de la phase d'optimisation.
La figure 2 montre en bleu l'ensemble des scores pour différentes descriptions et en gris les séries générées aléatoirement. Chaque point correspond au score de la série en fonction d'un certain nombre de modèles utilisés dans la série. Les points situés à droite associent donc plus de modèles que les points plus à gauche, ce qui permet d'observer l'évolution des performances de la série.
L'abscisse correspond donc au nombre de modèles sélectionnés et l'ordonnée correspond au score du système DLS 2015 comprenant, en descripteurs additionnels, les similarités cosinus de tous les modèles de la série courante U i . Chaque point du graphique appartient à une courbe U i (x), x étant un nombre de similarités cosinus (descripteurs) provenant des modèles de la série. Le score en ordonnée correspond donc au système DLS 2015 avec un nombre x de descripteurs additionnels appartenant à la série U i . Les séries se prolongent en x + 1, le score correspondra alors au même système, i.e. aux mêmes descripteurs, composé de la similarité d'un modèle en plus sélectionné pour la série U i . L'objectif de cette seconde étape était d'observer les modèles et leur pouvoir d'améliora-tion dans différentes séries. Une dizaine de séries ont été testées en faisant varier des descriptions semblables à la figure 3. Pour des raisons de lisibilité, seulement 5 d'entres elles ont été représentées sur la figure 2.
La sélection topdesc permet un gain d'environ 1.8% sur la base du système état de l'art 2015 (DLS 2015) et du meilleur modèle toujours présent en début de chaque série. Nous remarquerons que les séries générées aléatoirement permettent de surpasser les séries topdesc si suffisamment de modèles sont utilisés. Pour contrebalancer cet effet, nous introduisons l'algorithme topdelta.
Algorithme de sélection topdelta
L'algorithme de sélection topdesc permet de créer des séries de modèles performants. Mais comme nous avons pu le constater, il ne permet pas un gain important en performance. Un algorithme cherchant les modèles les plus complémentaires doit être capable de discriminer automatiquement les paramètres qui influent le moins sur la diversité des représentations. Par exemple, nous pouvons supposer que certains paramètres comme le nombre d'itérations lors de la construction des vecteurs sont directement liés aux performances du modèle et qu'une variation de ces paramètres ne permettra pas nécessairement d'orienter la représentation sur des aspects sémantiques variés. La sélection ne se faisait que sur le critère de performance et ne prenait pas en compte l'apport concret du calcul de similarité issu du modèle parmi l'ensemble des descripteurs. L'algorithme topdelta permet quant à lui de combiner des modèles par leur pouvoir de complémentarité. L'hypothèse sous-jacente est que les modèles les plus « complémentaires » sont ceux améliorant le plus une série. Cet algorithme consiste en l'attribution d'un score de « potentiel de complémentarité » à chaque modèle utilisé dans les séries topdesc et aléatoires. Une série topdelta correspondra donc à la suite de modèles ayant les meilleurs scores. Plusieurs séries différentes peuvent être générées en fonction des affectations de paramètres de l'équation calculant le score de « potentiel » que nous allons détailler.
Plus concrètement, nous pouvons intuitivement considérer que les modèles les plus complémentaires pouvant faire partie d'une série tiennent compte de deux facteurs :
1. d'une part la performance du modèle seul, i.e. son score en dehors d'une série ; 2. d'autre part son « pouvoir de complémentarité », i.e. la différence moyenne d'améliora-tion des performances entre ce modèle et l'ensemble de tous les modèles prédécesseurs possibles (i.e. qui apparaissent avant dans une série).
Le facteur score seul correspond aux scores obtenus lors de notre première étape. Le deuxième facteur est plus compliqué à mesurer. En effet, le pouvoir de complémentarité moyen d'un modèle doit tenir compte, dans l'idéal, de toutes les associations possibles de celui-ci et de prédécesseurs puisqu'un modèle peut améliorer une série uniquement grâce aux mauvaises performances de ses prédécesseurs. Chaque modèle est utilisé en moyenne trois fois et correspond donc en moyenne à trois points sur la figure 2. Il a donc été possible de faire une moyenne des différences entre le score de la série et le score de la série combiné au modèle courant.
Le score topdelta est défini ainsi :
Le paramètre α permet de régler la balance entre l'influence du score seul S et celle de la complémentarité ∆. La complémentarité ∆ est la différence moyenne entre le modèle x courant auquel nous voulons attribuer un score et l'ensemble de tous les prédécesseurs existants dans l'ensemble des séries topdesc déjà générées. Le modèle x peut également être un modèle présent dans les séries générées aléatoirement. Le paramètre ∆ est normalisé entre sa valeur minimale et maximale parmi toutes les séries. Pour plus de lisibilité, cette normalisation n'est pas incluse dans les équations.
Plus un modèle sera loin dans une série, plus faible sera la probabilité qu'il améliore cette série. Ce constat est général dans la plupart des tâches en apprentissage automatique : tout descripteur peut être indépendamment très performant, mais les performances associées de plusieurs descripteurs ne correspondront pas à la somme de leurs performances individuelles. Il est donc pertinent d'introduire un « bonus » qui permet d'augmenter le delta en fonction de la position du modèle dans la série. Le bonus τ a donc été introduit dans l'équation 1 et normalisé par sa borne supérieure β. Ce bonus peut être défini selon deux informations :
1. le nombre de prédécesseurs, puisque plus le modèle a de prédécesseurs, moins il a de chance d'améliorer la série ;
2. le score de la série au niveau de ce modèle, puisque plus le score de la série est élevé par rapport aux autres séries, moins ce modèle aura de chance d'améliorer globalement la série. Afin de simplifier les équations, cette information sera considérée comme normalisée au même titre que ∆ et S.
Le bonus τ du modèle x pour la série courante i correspond donc à l'équation 2. Avec nbAncestors le nombre de prédécesseurs, σ réglant l'influence des deux informations et β réglant l'importance du bonus τ (i.e. plus β sera grand, plus le bonus pourra être grand) :
La figure 2 montre les séries topdelta générées et optimisées (par une méthode de type recherche locale) sur les paramètres α, β, σ. Les meilleures affectations, représentées par la courbe topdelta la plus haute sur le graphique, sont α = 0.9, β = 1.0, σ = 0.5. Nous remarquerons que le facteur de complémentarité est plus important que le score du modèle seul comme le montre l'affectation de α. L'affectation de β montre que le bonus est aussi important que delta lui-même. Enfin, l'affectation de σ ne priorise pas l'information du nombre de prédécesseurs par rapport au score de la série.
Sur la figure 2, pour des raisons de complexité en espace mémoire, nous avons prolongé uniquement une série topdelta et une série aléatoire, en choisissant la meilleure parmi celles déjà générées. Si l'on considère le score de référence comme étant le système DLS 2015 avec un modèle généré aléatoirement, alors la série topdelta permet un gain de 4%. La prochaine partie est destinée à tester expérimentalement la meilleure série topdelta en évaluant notre système sur les données de test mises à disposition en 2016.
Expérimentations
Lors de la campagne SemEval de 2016 (Agirre et al., 2016), ce sont 43 équipes qui ont participé à la tâche pour un total de 119 essais. Les scores globaux allaient de 0.4 à 0.77 avec une médiane à 0.69. La baseline de la campagne 2016 correspond à une similarité basée sur la similarité cosinus des représentations « sac-de-mots » des paires de phrases et a obtenu un score de 0.51.
Le tableau 1 montre que la meilleure série topdelta a effectivement amélioré significativement les performances du système DLS 2015. Grâce à la sélection automatique de modèles complémentaires, notre système a pu obtenir un score au dessus de la médiane des scores de la campagne de 2016. Par la suite, nous pensons améliorer notre méthodologie pour qu'elle puisse s'appliquer à de plus larges corpus, ce qui permettrait de prendre en compte d'autres paramètres à faire varier comme le ciblage des entités nommées, que l'on peut conserver ou non dans les phrases. Les entités nommées forment un vocabulaire très large et nous pensons qu'elles jouent un rôle particulier en similarité sémantique textuelle.
Les méthodes supervisées qui ont montré obtenir de meilleurs résultats (Rychalska et al., 2016) sur la tâche STS pourront aussi être utilisées dans notre recherche de complémentarité. De plus, d'autres méthodes de représentation vectorielle pourront être combinées à Doc2Vec dans l'objectif de capturer des aspects sémantiques différents et potentiellement complémen-taires.

Introduction
Les capteurs de l'Internet des Objects (IoT) génèrent en continu de grandes quantités de données accumulées et traitées par des plates-formes spécialisées. L'analyse de ces données se fait par le biais de processus avancés basés sur de l'apprentissage automatique (i.e., calcul numérique) ou des approches plus sémantiques (basées sur la représentation des connaissances et l'inférence). Parmi les problématiques phares, l'identification de singularités conduisant à la détection d'anomalies est un domaine de recherche d'actualité. En effet, ce sujet touche à des domaines aussi variés que la médecine (e.g., identification de tumeurs malignes via imagerie IRM), la finance (e.g., découverte de cas de fraudes lors de transactions financières), les technologies de l'information (e.g., détection de piratage de réseaux informatiques).
Dans le cadre du projet Waves 1 , nous nous sommes intéressés à la détection d'anomalies dans les grands réseaux d'eau potable gérés par un leader national expert dans le domaine de l'eau. La détection automatique de telles anomalies est une question importante à la fois sur le plan environnemental et économique. On notera que le volume de pertes d'eau potable enregistré dans le monde dépasse les 32 milliards de m3 / an (soit 14 milliards d'euros par an) dont 90 % reste difficilement identifiable en raison de la nature souterraine du réseau. Théoriquement, ces fuites d'eau peuvent être détectées en fonction de la pression et des mesures d'écoulement extraites des capteurs installés à des points stratégiques du réseau. Dans cet article, nous nous intéressons à un réseau national Français qui est constitué d'environ 100 000 km de canaux équipés de plus de 3 000 capteurs et distribuant de l'eau potable à plus de 12 millions de clients. Selon les experts, il est possible de garantir une grande précision lors du processus de détection si une contextualisation des mesures est effectuée lorsqu'une singularité apparaît. Par exemple, des signaux anormaux de haute pression ou de flux importants pourraient indiquer une fuite d'eau.
Cependant, dans de nombreux cas d'événements particuliers tels que les compétitions sportives, les rencontres culturelles, ou les catastrophes naturelles, ces singularités pourraient aisément s'expliquer rendant les réactions de l'exploitant du réseau plus efficaces. De plus, les conditions météorologiques telles que la canicule, un arrosage important ou un incendie d'origine criminelle impliquent l'utilisation de quantités importantes d'eau et ne sont donc pas de véritables anomalies. Par conséquent, une approche efficace de détection d'anomalies ne peut faire l'économie d'une contextualisation précise intégrant à la fois une dimension spatiale, une dimension temporelle et une dimension sémantique. Conçu pour être un système générique, Scouter vise à simplifier toutes ces tâches en proposant une implémentation efficace et en facilitant considérablement la configuration des composants.
Architecture
Scouter a été développé pour être un système complet qui peut traiter à la fois des évé-nements statiques et dynamiques ainsi que les analyser à l'aide d'un puissant ensemble de fonctions du traitement du langage naturel (TLN) et de méthodes sémantiques avancées. Entièrement configurable, l'objectif principal de Scouter est d'extraire des données efficacement à partir de différentes sources dans le Web, les traiter rapidement afin de quantifier le potentiel de chaque événement à expliquer les anomalies détectées par la plate-forme. Les principaux composants de notre système sont les suivants : un ensemble de connecteurs de données Web, une unité d'analyse multimédia, une unité de géolocalisation, un centre de stockage, un gestionnaire de messages et un fournisseur de services Web.
Les connecteurs Web consomment les données provenant de différentes sources à une certaine fréquence et en fonction de configurations prédéfinies dans une interface Web. Ces sources incluent (a) des réseaux sociaux tels que Twitter et Facebook (e.g., les citoyens commentant les fuites d'eau à proximité),(b) des sources médiatiques via des flux RSS de divers journaux (e.g., un article du Monde mentionnant un incendie), (c)   Berger et Pietra (1996). Simultanément, l'unité de géo-profilage fournit des caractéristiques géographiques pour la zone analysée. Elle déter-mine le type de zone autour de l'emplacement de l'anomalie en générant un profil donné (i.e., résidentiel, touristique, industriel ou agricole).
Suite aux étapes d'annotation et de scoring, les événements sont enregistrés dans une base de données distribuée orientée documents (MongoDB). Le résultat final obtenu est une contextualisation spatio-temporelle en temps réel pouvant expliquer une anomalie détectée dans le réseau d'eau potable. Scouter fournit également un outil de suivi des performances du système grâce à une panoplie de métriques telles que le temps d'exécution des requêtes ou la durée d'extraction des résumés. Ces métriques sont stockées dans une base de données orientée séries temporelles (InfluxDB) permettant un accès en lecture/écriture très rapide. Enfin, le composant de services Web est utilisé pour configurer le système de manière conviviale via une interface Rest.
Media Analytics & Traitement du Langage Naturel
Dans cette section, nous détaillons la méthodologie de collecte des données issues de différentes sources disponibles sur le Web.
Ontologie d'Extraction
Les systèmes de scrapping reposent généralement sur un fichier de configuration qui réper-torie les propriétés des mots, des concepts ou des événements qu'il tentera d'extraire de S Sirisuriya (2015). Dans Scouter, l'extraction est optimisée et améliorée grâce à une ontologie pré-construite qui énumère les principaux concepts que l'utilisateur recherche, elle permet d'organiser les différentes relations en deux dimensions :
Hiérarchie verticale : Un concept donné (e.g., Feu) peut avoir plusieurs sous-concepts (e.g., incendie, brasier, explosion) ou des alias et erreurs d'orthographe (e.g., brazier, pheu).
Dépendance horizontale : Un concept peut avoir plusieurs propriétés qui décrivent un état spécifique durant une période données. Par exemple, l'eau peut être potable, mais peut également être en train de fuir ou avoir une couleur/odeur spécifique.
FIG. 2 -Aperçu de l'ontologie d'extraction
En combinant les concepts et les propriétés avec les prédicats, nous pouvons créer une ontologie expressive telle que celle de la figure 2 utilisée pour le cas d'utilisation des fuites d'eau. Ce type de structure est plus expressif qu'une liste classique de mots-clés de par sa modularité et son extensibilité.
Extraction de Résumés
Après avoir récupéré les événements pertinents des différentes sources de données en se basant sur l'ontologie de concepts et de propriétés, l'étape suivante consiste à extraire des résumés significatifs des événements en suivant le processus décrit dans la Figure 3.
FIG. 3 -Processus d'extraction de résumés
Le prétraitement initial concerne le nettoyage du texte en entrée, l'identification des candidats potentiels et, enfin le tri ainsi que l'harmonisation des majuscules/minuscules. Les fichiers en entrée sont filtrés pour régulariser le texte et déterminer les limites des phrases, puis interviennent le fractionnement en tokens ainsi que quelques opérations de nettoyage (i.e., suppression des apostrophes ou séparation de certaines expression en plusieurs mots). Ensuite, nous considérons toutes les sous-séquences générées afin de pouvoir déterminer celles qui conviennent en tant que phrases complètes et compréhensibles. Pour augmenter la précision, nous utilisons une liste de mots français contenant plus de 500 entrées dans différentes classes syntaxiques (e.g., conjonctions, articles, particules, etc.). Nous trions et harmonisons tous les mots via une méthode itérée très utilisée dans le domaine du TLN Lovins (1968), le processus est répété jusqu'à ce qu'il n'y ait plus d'amélioration possible.
Quant au traitement principal, il est relatif au calcul de deux valeurs distinctes pour chaque phrase candidate : la fréquence des phrases dans le texte d'entrée par rapport à sa rareté dans l'utilisation générale et la première occurrence, qui correspond à une distance dans le texte d'entrée indiquant la première apparition de la phrase. Ces deux valeurs sont converties en données nominales pour faciliter le processus de d'apprentissage automatique et une table de discrétisation pour chacune des valeurs est dérivée des données d'entraînement. Enfin, nous générons un modèle qui donne les scores pour chaque candidat et le classons en utilisant des techniques bayésiennes naïves Domingos et Pazzani (1997).
Pertinence des Résumés
Plusieurs travaux de recherche abordent la question du résumé automatique Ellouze et al. (2017). Dans notre cas, nous avons choisi une approche basée sur une similarité distribuée qui compare le contenu d'entrée et le résumé. Nous considérons qu'un bon résumé devrait être caractérisé par une faible divergence entre les distributions de probabilité des mots en entrée et le résumé généré, et par une forte similitude avec l'entrée. À cette fin, nous avons utilisé deux mesures complémentaires : la divergence de Kullback Leibler et la divergence de Jensen Shannon. Tout d'abord, les mots en entrée et dans le résumé sont triés et segmentés avant tout calcul. Ensuite, nous calculons les deux mesures :
Divergence de Kullback Leibler (KL) : Elle correspond au nombre moyen de bits utilisés pour le codage d'échantillons appartenant à P en utilisant une autre distribution Q, approximative de P. Elle est donnée par :
Dans notre cas, les deux distributions de probabilités sont estimées à partir du texte en entrée et du résumé. Étant donné que la divergence de KL n'est pas symétrique, les divergences du texte en entrée et de son résumé sont introduites en tant que mesures. En outre, nous effectuons un lissage simple via une fonction d'approximation qui capture des comportements spécifiques tout en excluant le bruit et d'autres nuisances à faible échelle.
Divergence de Jensen Shannon (JS) : Elle s'appuie sur le fait que la distance entre deux distributions ne peut pas être trop éloignée de la moyenne des distances de leur distribution moyenne. Elle est donnée par la formule suivante :
Contrairement à la divergence de KL, la divergence de JS est symétrique et toujours définie. Nous calculons à la fois les versions lissées et non lissées de ces divergences en tant que résultats du résumé. La dernière étape est d'utiliser la sortie de ces deux fonctions pour classer les résumés extraits et ne conserver que ceux ayant le meilleur résultat de résumé (i.e., les divergences les plus faibles).
FIG. 4 -Processus d'estimation de la pertinence du résumé
Analyse Sentimentale
Au cours de la dernière décennie, l'analyse du sentiment a connu un développement exponentiel, de nombreuses solutions ont été proposées sur diverses technologies A. Collomb et Brunie (2014). Nous proposons dans cette section une approche simple et efficace mêlant des outils variés dont le toolkit fourni par Stanford CoreNLP Manning et al. (2014).
FIG. 5 -Processus d'Analyse sentimentale
Avant d'appliquer le modèle, nous devons effectuer plusieurs étapes de prétraitement qui améliorent la précision du score final en sortie. Les trois étapes réalisées ici sont (a) la Tokenisation (séparation du texte en une séquence de tokens et division chaque séquence en phrases significatives, (b) la Reconnaissance d'entités (inférence d'informations sur le genre puis annotation en tant que personnes, emplacements, organisations, nombres, etc) et (c) la Ré-solution syntaxique (recherche de dépendances grammaticales et utilisation d'un dictionnaire français).
Après la phase de prétraitement, nous appliquons le modèle de composition sur arbres basé sur une approche d'apprentissage profond. Il s'appuie sur les noeuds d'un arbre binarisé pour chaque phrase, y compris en particulier le noeud racine, chaque noeud étant annoté d'un score de sentiment. Afin de saisir le sentiment d'un texte en entrée, un modèle de réseau de neurones récursifs (i.e., Recursive Neural Tensor Network ou RNTN) est construit en fonction des caractéristiques des phrases en entrée. Cette approche est inspirée des modèles récursifs et profonds développés par l'équipe Stanford Richard (2013).
Correspondance des Résumés
L'objectif des différentes étapes du module analytique est d'extraire les événements uniques les plus pertinents en les annotant avec un résumé expressif. Le système permet d'éviter de stocker des événements en doublons faisant référence à la même occurrence. Pour chaque évé-nement extrait, le système proposera une liste de résumés potentiels basés sur une approche bayésienne. Ensuite, ces résumés seront classés en utilisant les divergences les plus faibles (i.e., divergence KL et divergence JS) afin d'évaluer leur précision. Parmi les mieux classés, nous vérifierons s'ils disposent du même sentiment (i.e., positif, neutre ou négatif). Si deux résumés sélectionnés au cours de ce processus ont le même score de pertinence et le même sentiment, nous supposons alors qu'ils se réfèrent au même événement. Par conséquent, nous concluons que ces événements sont des doublons et nous ne conserverons que le contenu d'un seul. Le processus global de notre module analytique est détaillé dans la figure6.
FIG. 6 -Processus de correspondance des résumés
Même si ce module fournit des fonctions puissantes pour filtrer les événements uniques et pertinents, une dimension spatiale est nécessaire afin de parfaire la contextualisation de l'anomalie détectée. Cette partie sera expliquée dans la section suivante.
Profilage Géographique
Afin de pouvoir établir la pertinence des événements détectés comme origine potentielle d'une anomalie, et pour pouvoir ajuster le score de probabilité qui leur est attribué, Scouter se base sur un système de profilage géographique Lhez et Curé (2016) ; L'objectif est de pouvoir déterminer la composition des secteurs de consommation étudiés en termes de terrain.
Le profilage est réalisé à partir de données cartographiques provenant d'OpenStreetMap Haklay et Weber (2008), un projet international sous licence libre. Le programme établit à partir des informations sur les secteurs de consommation les données à extraire, en construisant FIG. 7 -Architecture du système de profilage une bounding box adaptée. Un fichier de configuration est également fourni au programme, afin de pouvoir établir quelles sont les grandes catégories de terrain à considérer, et quels tags définis par OSM appartiennent à ces catégories. A chaque tag est attribué une note, qui correspond à sa pertinence pour décrire la catégorie de terrain à laquelle il appartient. Le fichier, au format JSON, est donc organisé en une hiérarchie de catégories, incluant les tags en tant que feuille, mais aussi les différentes classes définies par OSM ; cela permet à l'utilisateur de créer ou modifier la configuration du profilage si nécessaire. Un exemple issu du fichier de scores pour le cas d'usage de Waves est fourni en figure 8.
"amenity": { "tourism": { "entertainment": { "arts_centre": 0.8, "casino": 1.0, "community_centre": 0.8, "fountain": 0.3, "gambling": 1.0, "planetarium": 0.8, "theatre": 1.0} }
FIG. 8 -Extrait de l'arborescence du fichier de configuration du profilage de Waves
A partir de ces informations, deux profilages distincts sont réalisés : à partir des points d'intérêt (POI) et des polygones. La première méthode se base sur les nodes récupérés d'OSM, ainsi que sur les notes attribuées aux tags dans le fichier de configuration. Pour chaque mot clef identifié à la fois au sein des données cartographiques et au sein de la classification du fichier, on ajoute la note qui lui est attribuée au score total de la catégorie de terrain qu'il représente. Les catégories les plus représentées dans le secteur auront ainsi un score plus important, et il suffit ensuite d'un simple calcul de proportions finales pour obtenir la répartition finale. Cette méthode est très adaptée pour identifiées les zones denses en POI. La deuxième méthode se base sur l'utilisation des polygones pour établir la répartition. Le procédé est le même que pré-cédemment pour l'identification des tags pertinents ; toutefois, à la place des notes attribuées arbitrairement dans le fichier de configuration, on calcule à la place la surface des polygones.
La répartition finale se calcule de la même manière, avec des scores pour chaque catégorie obtenus différemment, parfaits pour les secteurs riches en polygones.
Chaque méthode de profilage convient donc mieux à un cas précis, et il s'avère qu'elles sont également complémentaires. En effet, les zones géographiques comportant beaucoup de POI sont souvent dépourvues de polygones, car il s'agit de secteurs très divers, sans parcelles de terrain uniforme. A l'inverses, les secteurs riches en polygones seront également la plupart du temps très pauvres en POI, car il s'agit de terrains sans éléments remarquables. Ainsi, dans certains cas, les méthodes peuvent être sélectionnées, voire adaptées pour obtenir un résultat plus précis. Pour cela, il suffit de calculer la densité (proportion) de POI au kilomètre carré à partir des données d'OSM : de la sorte, l'utilisateur peut choisir quel profilage est le plus adapté à ses besoins. En cas de densité moyenne, il peut être pertinent de calculer la moyenne des résultats des profilages pour ajuster les proportions.
L'ensemble du système de profilage est donc configurable à partir du fichier de score, ce qui permet d'effectuer des ajustements. Par ailleurs, il est parfaitement possible de réaliser sa propre méthode de combinaison des profilage si l'approche générique ne convient pas.
Évaluation
Dans cette section, nous évaluons la performance du système sur plusieurs dimension tant quantitatives que qualitatives.
Media Analytics
Lors de cette expérimentation, nous avons collecté des flux durant 9 heures depuis des sources telles que Facebook, Twitter, des feeds RSS, Open Agenda, DBPedia et Open Weather Map. Notre cible géographique était l'agglomération de Versailles. Les paramètres utilisés pour chaque source sont présentés dans la Table 1. Par exemple, en utilisant l'API de streaming de Twitter, nous récupérons les flux de la zone géographique de Versailles mais aussi depuis les comptes e.g., @Versailles et @monversailles. Les mots clés utilisés pour requêter ces tweets sont associés à 12 concepts de l'ontologie pour lesquels un score de pertinence est associé.  (2017). Notre système a tourné durant 9 heures pour collecter les événements de diverses sources du web, en utilisant l'ontologie explicitée dans la section et les scores assignés dans le tableau . L'exploitant du réseau d'eau potable a fourni l'horodatage et l'emplacement de toutes les anomalies rapportées en 2017 dénombrées à 15 au total. À partir de la base de données, nous avons récupéré tous les événements stockés correspondant à l'horodatage et l'emplacement de chaque anomalie et les avons présentés à cinq experts du domaine. Pour chaque événement, on leur a demandé d'estimer si cet événement pouvait fournir une explication pertinente à l'anomalie signalée. Une contrainte a été imposée, la réponse devait être se borner à "oui" ou "non," afin de simplifier l'interprétation des résultats.
Evaluateur Evénements 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
TAB. 3 -Évaluation des Experts de Domaine
Pour évaluer la fiabilité de l'annotation, nous avons utilisé la mesure kappa de Fleiss Fleiss et al. (1971). Il s'agit d'une mesure statistique visant à évaluer la fiabilité de l'accord entre un certain nombre d'évaluateurs lors de l'attribution d'étiquettes à des sujets catégoriels. Cette mesure est exprimée par l'équation ci-dessous dont les résultats sont calculés pour un scénario avec 5 évaluateurs :
D'après le tableau d'interprétation des valeurs kappa Landis et Koch (1977), les évaluateurs ont un accord substantiel sur les événements annotés comme pouvant fournir une explication pertinente pour une anomalie de fuite d'eau. Par conséquent, Scouter a été assez efficace pour sélectionner les événements les plus pertinents.
Profilage Géographique
L'évaluation de la précision du profilage géographique a été réalisée à partir de divers échantillons de données fournis par notre client. Nous présentons ici un exemple basé sur la zone de Versailles en France. Nous allons présenter les résultats des deux méthodes de profilage pour chaque secteur de consommation, et détailler les performances de chaque méthode du programme.
FIG. 9 -Secteurs de consommations de Versailles, superposés aux données d'OpenStreetMap
La figure 9 donne un aperçu de l'organisation du système de profilage. Le profilage utilise 5 types de terrains différents : résidentiel, agricole, naturel, industriel et touristique. Certaines zones sont plus facilement détectables en utilisant des POI, d'autres seront majoritairement représentées par des polygones. Les résultats finaux sont présentés dans le tableau 4. Nous avons utilisé la technique décrite en section 4, et nous réalisons la moyenne des deux méthodes si la densité est jugée moyenne. Une fois que nos ajustements finalisés, nous avons présenté nos résultats à un groupe d'experts du domaine pour recueillir leur avis (4ème colonne du tableau). Leurs évaluations sont majoritairement satisfaisantes, avec quelques remarques pour certains secteurs ne comportant pas de type de terrain majoritaire. 
TAB. 4 -Évaluation des résultats finaux
Les dernières colonnes du tableau 4 détaillent les performances pour chaque méthode de profilage. La taille des données à télécharger dépend de la surface du secteur. Les données des polygones sont généralement plus volumineuses que celles des POI, même quand ces dernières sont plus nombreuses, leur représentation étant plus complexe. Ainsi, la méthode de profilage par polygone est généralement bien plus longue que celle des POI. Le temps d'exécution peut donc être potentiellement long pour des zones vastes, mais le profilage des secteurs ne dépend


Contexte et motivations
Les structures discrètes ont toujours su démontrer leur utilité pour découvrir de l'information. Parmi elles, les arbres ou leurs extensions (Bertrand et Janowitz (2002); Diatta (2005); Bertrand et Diatta (2017)) sont très souvent utilisés, notamment en biologie où les arbres phylogénétiques permettent de capturer les filiations inter-espèces. Le problème est alors de retrouver les évolutions qui ont amené la diversité du vivant actuel, généralement à partir de fragments d'ADN d'espèces contemporaines.
Lorsque plusieurs arbres possibles aboutissent au même résultat, on privilégie les arbres les plus parcimonieux, c'est à dire ceux qui nécessitent le moins de modifications (mutations) pour les espèces considérées. Cependant, même ainsi, plusieurs arbres restent possibles. C'est le cas par exemple si l'on observe des mutations dites inverses (un gène retrouve un état antérieur suite à une nouvelle mutation) ou parallèles (la même mutation se produit chez des espèces sans liens direct). Si on ne souhaite pas privilégier un de ces arbres, la question de la représentation de cette famille se pose. Peut-on trouver une structure encodant l'ensemble des arbres phylogénétiques parcimonieux ?
Pour répondre à cette question, Bandelt a proposé l'utilisation de graphes médians (Bandelt et Hedlíková (1983); Bandelt et al. (1999)). En quelques mots, ces derniers sont des graphes qui ont la particularité que pour tout triplet de sommets, les plus courts chemins entre paires de sommets s'intersectent en un sommet unique, appelé sommet médian. On retrouve l'idée de parcimonie (le croisement des chemins se faisant en un point unique). Un graphe médian est un moyen de représenter l'ensemble des arbres parcominieux.
Toujours parmi les structures discrètes, les treillis des concepts sont au coeur de l'analyse de concepts formels (FCA pour "Formal Concept Analysis", (Ganter et Wille (1999))). Ils sont utilisés dans de nombreux champs applicatifs (Carpineto et Romano (2004)) par une communauté de recherche qui va en s'étoffant, notamment pour des problématiques de classification. Les méthodes de FCA utilisent généralement en entrée une table binaire (appelée contexte), ce qui ne pose pas de véritable problème pour s'intéresser à la filiation inter-espèce et aux problèmes de phylogénie. Il suffira en effet de pré-traiter les données de façon à se ramener à ce cadre. C'est par exemple possible en utilisant un contexte mettant en relation les espèces avec un génome de référence, en indiquant 1 s'il n'y a pas eu mutation sur un gène par rapport à ce génome, et 0 sinon. Il y a des liens forts entre les graphes médians d'un coté et certains treillis de l'autre, ceux ayant la propriété d'être distributifs comme discuté ci-après.
Uta Priss (Priss (2012(Priss ( , 2013) propose de tirer profit de ces liens pour ré-utiliser l'ensemble des outils à disposition dans la communauté FCA pour traiter les problèmes de phylogénétique utilisant un graphe médian. Cependant, un treillis des concepts n'est pas forcément distributif, et n'est donc pas en correspondance directe avec un graphe médian. Il faut par suite modifier le treillis pour qu'il vérifie la propriété de distributivité. Si un exemple de modification est donné dans les articles d'Uta Priss, il n'y a pas dans ses travaux d'algorithme général permettant de transformer un treillis quelconque en treillis distributif de façon systématique et le travail semble avoir été fait manuellement.
L'apport principal de cet article est de proposer un tel algorithme, qui s'appuie sur le théorème de représentation de Birkhoff (Birkhoff (1933(Birkhoff ( , 1967). Notre algorithme produit le contexte d'un treillis distributif à partir du contexte d'un treillis quelconque, ce qui évite de devoir construire le treillis de départ pour le modifier par la suite. De plus, notre algorithme assure que le treillis initial peut se plonger (plongement d'ordre) dans le treillis distributif obtenu.
Le reste de l'article est structuré comme suit. Dans la section 2, nous rappelons les défi-nitions et notations nécessaires sur les ensembles ordonnés et treillis. La section 3 est un état de l'art sur les treillis distributifs. Nous y rappelons les résultats sur lesquels nous construirons notre algorithme. L'algorithme est présenté en section 4 et l'article se termine par une discussion en section 5.
Définitions et notations
Dans l'ensemble de cette section, on ne considère que le cas fini. Pour la plupart des défi-nitions et résultats de ce chapitre, on pourra se référer aux trois ouvrages de références Ganter et Wille (1999); Davey et Priestley (2002);Caspard et al. (2007).
Un ensemble ordonné (P, ≤) est un ensemble d'éléments P muni de la relation d'ordre ≤. Dans cet article, les relations d'ordre seront représentées par leur diagramme de Hasse. Il s'agit du graphe ayant pour sommets les éléments de P et pour arêtes les éléments en relation de couverture (par d'arcs de transitivité ni de réflexivité). Le diagramme de Hasse est orienté du bas vers le haut. Définition 1. Soit un ensemble ordonné (P, ≤) et X ⊆ P , X est un idéal d'ordre (resp. filtre d'ordre) si x ∈ X et y ≤ x alors y ∈ X (resp. ssi x ∈ X et x ≤ y alors y ∈ X).
Pour un ensemble X ⊆ P quelconque, on notera ↓ X l'idéal (resp. ↑ X le filtre) construit à partir de X. Pour un élément x ∈ P , ↓ x est appelé idéal principal (resp. ↑ x filtre principal).
Sur l'exemple de la figure 2 l'idéal principal de Puisque nous considérons le cas fini, un treillis possède un plus petit élément noté ⊥ et un plus grand élément noté . Dans la suite, lorsqu'il n'y a pas d'ambiguité, pour simplifier, on utilisera parfois T pour désigner le treillis (T, ≤, ∨, ∧) et P pour désigner l'ensemble ordonné (P, ≤).
Les éléments ∨-irréductibles (resp. ∧-irréductible) sont donc les éléments de T qui ne peuvent être retrouvés comme étant le suprémum (resp. infimum) d'autres éléments. On notera J(T ) (resp. M (T )) l'ensemble des éléments ∨-irréductibles (resp. ∧-irréductibles) de T . Sans ambiguïté sur T , on notera seulement J et M .
Ces deux ensembles ne sont pas forcément disjoints, un même élément pouvant être à la fois ∨-irréductible et ∧-irréductible (on parle de doublement irréductible). Par convention, dans cet article, les éléments de J seront étiquetés par des lettres, et les éléments de M par des chiffres. Un même élément pourra donc être désigné par deux étiquettes s'il est doublement irréductible. C'est le cas en figure 2, par exemple pour l'élément doublement irréductible qui a pour label b lorsqu'il est considéré comme un élément ∨-irréductible et 1 lorsqu'il est considéré comme élément ∧-irréductible. Cela nous permettra de clarifier le rôle joué par un élément dans le cadre des relations flèches définies ci-après.
Des éléments peuvent bien sûr ne pas être irréductibles. C'est le cas de l'élément au centre du treillis, qui peut être décrit comme a ∨ c ou encore 2 ∧ 4.
Chaque élément x d'un treillis peut en effet être représenté par l'ensemble des éléments ∧-irréductibles qui lui sont supérieurs (représentation ∧-dense, x = { M ∩ ↑ x}) ou bien, de façon équivalente, par l'ensemble des éléments ∨-irréductibles qui lui sont inférieurs (repré-sentation ∨-dense, x = { J∩ ↓ x}).
Une fois la table définie, on peut introduire les connections de Galois qui permettent de reconstruire le treillis à partir de cette table.
Définition 5. Soit (J, M, ≤) la table d'un treillis T , on peut définir les connections de Galois entre J et M comme suit :
La composition est un opérateur de fermeture et permet de définir deux systèmes de fermeture sur J et M ainsi que le treillis de Galois (treillis des concepts) à partir de la table de T .
La table d'un treillis correspond au contexte réduit d'un treillis des concepts en FCA (Ganter et Wille (1999)). Un contexte réduit est un contexte (O, A, I) (une relation binaire I entre objets O et attributs A) pour lequel il n'y a pas deux lignes (resp. colonnes) identiques, et tel que chaque ligne (resp. colonne) ne soit pas l'intersection d'autres lignes (resp. colonnes). Autrement dit, un contexte réduit est réduit aux éléments irréductibles.
On notera C(J, M, ≤) la table (le contexte réduit) d'un treillis. La table est souvent repré-senté par un tableau à double entrée avec J en colonnes, M en lignes et une croix (×) dans la cellule (j, m) ssi j ≤ m. Cette table peut d'autre part être complétée pour servir à présenter d'autres informations, comme les relations flèches définies ci-après.
Ainsi, on a j ↑ m si m est maximal dans l'ordre induit par le treillis, moins les éléments du filtre de j. De même, on a j ↓ m si j est minimal dans l'ordre induit par le treillis, moins les éléments de l'idéal de m. Une illustration d'éléments en relation double flèche est donnée figure 2
On notera C(J, M, ≤, ↓ , ↑ ) la table fléchée (le contexte réduit avec l'information supplémentaire des relations flèches). Par définition, si deux éléments j, m sont en relation flèche (flèche haut, flèche bas ou double flèche), alors j ≤ m. Par suite, la table fléchée est un tableau à deux dimensions où la valeur d'une cellule (j, m) indique la relation entre j et m, celle ci pouvant être (de façon exclusive) × (si j ≤ m), ↓ , ↑ , ↔ ou aucune d'entres elles (la cellule est alors laissée vide).
La figure 2 montre à droite la table fléchée du treillis de gauche. Remarquons que l'en-
Autrement dit, un sous-treillis est fermé pour les opérations ∨ et ∧ du treillis dont il est issu. Certaines classes de treillis, dont les treillis distributif définis ci-après, peuvent être caractérisés par des sous-treillis interdits. La prochaine section donne plusieurs résultats sur les treillis distributifs.
Treillis distributifs
Un treillis est distributif ssi il vérifie la distributivité des opérations ∨ et ∧, c'est à dire, ∀x, y, z ∈ T : x ∨ (y ∧ z) = (x ∨ y) ∧ (x ∨ z). Birkhoff s'est énormément intéressé aux treillis distributifs dès les années 30 avec un article dont est issu un des résultats utilisé ici (Birkhoff (1933)). Cette classe particulière de treillis dispose d'un chapitre à part entière dans son ouvrage de référence sur les treillis (Birkhoff (1967)). Les deux théorèmes repris ici sont aussi détaillés dans l'ouvrage Caspard et al. (2007) qui apporte de plus les caractérisations par relations flèches.
Caractérisations d'un treillis distributif
Caractérisations. Un treillis distributif est un treillis pour lequel les opérations ∨ et ∧ sont distributives entre elles. Il découle de cette définition plusieurs caractérisations équivalentes que nous allons réutiliser par la suite : Théorème 1. Un treillis (T, ≤, ∨, ∧) est distributif ssi l'une (toutes) des conditions équiva-lentes suivantes est vérifiée :
4. T ne contient ni N 5 ni M 3 comme sous-treillis 5. la table fléchée de T contient exactement une double-flèche ↔ par ligne et par colonne, à l'exclusion de toute autre relation flèche.
En particulier, la troisième caractérisation fait un lien direct entre les treillis distributifs d'un coté, et les graphes médians de l'autre. En effet, on peut définir une opération de médiane sur un ensemble M comme une fonction : 
Treillis distributif et treillis des idéaux
Soit l'ensemble ordonné (P, ≤), on note O(P ) l'ensemble des idéaux d'ordre de (P, ≤). O(P ) muni de la relation d'inclusion est lui-même un ensemble ordonné (O(P ), ⊆). C'est de plus un treillis, appelé treillis des idéaux. L'opération de borne inférieure est l'intersection. L'opération de borne supérieure est l'union.
Le théorème 2 du à Birkhoff (Birkhoff (1933)) permet de mettre en évidence que les élé-ments ∨-irréductibles du treillis des idéaux (O(P ), ⊆, ∩, ∪) sont les idéaux principaux de l'ensemble ordonné (P, ≤).
Théorème 2. Soit (P, ≤) un ensemble ordonné et x ∈ P , la fonction : x →↓ x est un isomorphisme d'ordre de P vers J(O(P )).
Une illustration de ce théorème est proposée figure 4 ou l'on voit, à gauche, un ensemble ordonné et à droite le treillis des idéaux correspondant, avec les idéaux principaux en noir, et les autres idéaux en blancs. Le théorème 2 indique qu'il y a isomorphisme entre les ensembles ordonnés d'un coté, et les éléments sup-irréductibles des treillis des idéaux de l'autre. Ce résultat ne donne aucune information sur les propriétés de ces derniers. Cependant, le résultat du théorème 3, toujours du à Birkhoff, permet de remarquer qu'il s'agit de la famille des treillis distributifs.
Théorème 3. Théorème de représentation de Birkhoff.(Birkhoff (1933)) Soit T un treillis distributif, alors la fonction η :
est un isomorphisme de T dans O(J(T )).
On déduit de ce théorème que pour tout treillis (T, ≤, ∨, ∧), on peut trouver un treillis
Autrement dit, on peut toujours, à partir d'un treillis, obtenir un treillis distributif ayant le même ordre induit sur J. C'est ce résultat que nous allons utiliser pour proposer un algorithme construisant la table d'un treillis distributif à partir de la table d'un treillis quelconque.
Résultats
Le problème consiste à transformer un treillis quelconque en un treillis distributif. A moins que le treillis initial ne soit déjà lui-même distributif, il y aura donc forcément modification des données, et par suite de la table fléchée. L'idée est de calculer la table correspondant au treillis distributif à partir de la table du treillis initial quelconque, sans avoir à construire ce dernier, et dans un principe d'économie.
D'autre part, on peut se poser la question de savoir dans quelle mesure le treillis distributif obtenu est "proche" du treillis initial et quelles modifications sont autorisées sur la table de départ. Ici, en considérant (T,
nous avons considéré les hypothèses suivantes :
-
, ≤) (un des deux ordres induits par les éléments irréductibles est isomorphe entre le treillis initial et le treillis distributif).
les tables des deux treillis, alors la relation ≤ est incluse dans la relation ≤ d . Autrement dit, on ne s'autorise qu'à ajouter des croix dans la table, et jamais à en retirer. L'intuition de cette deuxième hypothèse se retrouve dans la maxime "l'absence de preuve n'est pas la preuve de l'absence 1 ". S'il y a une croix, 1. ce qu'on pourrait interpréter comme une variante de l'hypothèse de monde ouvert c'est que l'existence de la mutation a été montrée pour une espèce, et on ne peut le remettre en question. S'il n'y a pas de croix, cette mutation n'a pas été repérée, mais ce n'est pas pour autant qu'elle n'existe pas. Comme nous travaillons avec le théorème de représentation de Birkhoff, la première condition nous assure que le treillis construit a bien une structure commune avec le treillis initial. Comme un passage au treillis dual inverse les éléments ∨-irréductibles et ∧-irréductible, nous pouvons nous appuyer sur l'une comme sur l'autre.
Dans un treillis distributif, les ordres
Dans le treillis initial, trois cas peuvent se produire :
Si on utilise systématiquement l'ensemble des éléments ∨-irréductibles (lignes de la table), alors la table modifiée aura autant (cas 1), plus (cas 2) ou moins (cas 3) de colonnes que la table initiale.
Conserver la relation d'incidence du contexte (conserver les croix existantes) n'est possible que dans les deux premiers cas. En effet, dans le troisième, certains éléments ∧-irréductibles sont amenés à disparaître. On pourra cependant toujours se ramener à l'un des deux premiers cas en raisonnant sur le dual, c'est à dire en échangeant le rôle de
Nous montrerons dans la suite que pour les deux premiers cas, la relation d'incidence initiale est bien conservée (plongée dans la relation d'incidence du treillis distributif.
Il serait aussi possible de raisonner sur le troisième cas en conservant J(T ). Dans ce cas, la table du treillis distributif ayant moins de colonnes, il y aura disparition de croix de la relation d'incidence.
Notons que dans tous les cas, de par l'utilisation du théorème de représentation de Birkhoff, y il aura un plongement du treillis de départ dans le treillis distributif, ce qui nous assure que les ensembles fermés (concepts) initiaux apparaissent toujours dans les données transformées. L'algorithme proposé suit les deux grandes étapes suivantes :
La méthode proposée n'a techniquement pas besoin de la table fléchée C(J, M, ≤, ↑ , ↓ ) pour fonctionner. Cependant la connaissance de ces relations permet de savoir plus facilement quelles modifications sont faites sur la table initiale.
Si besoin, le calcul des relations flèches se fait en temps polynomial en la taille du contexte : (J(T ), ≤) et (M (T ), ≤) peuvent être calculés en temps polynomial (algorithme 1). Pour trouver les éléments en relation ↑ avec un élément de j ∈ J, on retire le filtre de j (c'est à dire j ) de (M (T ), ≤) et on cherche les éléments maximaux de (M (T )\j , ≤). Dualement, pour trouver les éléments en relation ↓ avec un élément m ∈ M , on retire l'idéal de m (c'est à dire m ) de (J(T ), ≤) et on cherche les éléments minimaux de (J(T )\m , ≤).
Dans la suite, nous utiliserons la transformation du treillis N 5 en treillis distributif pour illustrer les différentes parties de l'algorithme, y compris le calcul des relations flèches.
Algorithme 1 : ordreInduitJdepuisTable.
contexte. L'algorithme 1 est basé sur le fait que tout élément d'un treillis peut être identifié par les éléments ∧-irréductibles qui sont dans son filtre. Ainsi un élément j 1 est inférieur à j 2 si les éléments ∧-irréductibles du filtre de j 2 (c'est à dire j 2 ) sont inclus dans ceux du filtre de j 1 (c'est à dire j 1 ). Notons que l'algorithme peut être adapté sans mal pour obtenir (M (T ), ≤)
La complexité de cet algorithme est de O(|J| 2 * |M |) puisque pour chaque couple j 1 , j 2 ∈ J, on va faire des comparaisons ensemblistes sur 
4.2 Construction de la table d'un treillis distributif isomorphe au treillis des idéaux
. Ainsi, le treillis de départ et le treillis distributif obtenus auront le même ordre induit par les éléments ∨-irréductibles. L'algorithme 2 décrit la façon de faire.
Algorithme 2 : Production du contexte d'un treillis distributif.
Propriété 1. L'algorithme 2 produit la table du treillis des idéaux associé à l'ordre J(T ).
Démonstration. Le treillis des idéaux d'un ordre est un treillis distributif ayant pour élément ∨-irréductible les idéaux principaux de cet ordre. Par construction, l'algorithme produit un unique élément m pour chaque j tel que j ↔ m. Puisque j ↔ m, alors ↓ m ⊆ T \ ↑ j. Comme il n'y a pas d'autres relations flèches pour m (théorème 1, caractérisation 5), alors ↓ m = T \ ↑ j. L'ordre induit par J n'est pas modifié par l'algorithme et la table produite est alors par construction celle d'un treillis distributif ayant le même ordre induit par J, c'est à dire le treillis des idéaux de l'ordre (J, ≤).
Illustration. A l'issue de l'étape précédente, nous disposons de l'ordre (J, ≤) et des relations flèches. Puisque nous connaissons les éléments en relation double flèche, et qu'il n'y en a qu'une par ligne et par colonne (malgré des flèches simples), nous allons les utiliser de façon à ce que la table produite conserve ses doubles flèches. Autrement dit :  Dans le cas ou |J(T )| > |M (T )|, cette propriété implique que la relation d'incidence initiale est toujours présente (rajout de croix seulement). De plus, cette preuve nous permet de constater que pour un élément, peut importe la double-flèche considérée dans la ligne ou la colonne. Il suffit alors de choisir les doubles flèches de façon à en avoir une seule par ligne et colonne dans la table du treillis distributif.
Discussion et Conclusion
En nous appuyant sur le théorème de représentation de Birkhoff, nous avons proposé un algorithme polynomial produisant la table d'un treillis distributif à partir de la table d'un treillis des concepts. La relation d'ordre entre éléments ∨-irréductibles est identique dans les deux treillis et le treillis initial peut être plongé dans le treillis distributif obtenu. On pourra aussi s'intéresser aux contextes non binaires, par exemple issus d'extension de l'analyse de concepts formel, comme les treillis de patrons (pattern stucture lattices) Une expérimentation sur données réelles devra permettre de vérifier que la sémantique des données est bien préservée par les transformations proposées par cet algorithme. Enfin, l'algorithme proposé, actuellement en O(|J| 2 * |M |) a été mis en place pour montrer la possibilité d'application en temps polynomial en la taille du contexte. Une étude plus approfondie pouvant abaisser la complexité théorique de cet algorithme devra être menée.

Introduction
En raison du développement du marché des objets connectés et des techniques de géoloca-lisation, les données séquentielles sont omniprésentes et produites en quantité de plus en plus importante. Elles concernent une multitude de domaines d'application, allant de la médecine jusqu'aux télécommunications en passant par l'éducation (Kumar et al. (2011)). Les données séquentielles peuvent être produites sous la forme de suites d'informations ordonnées (e.g., parcours de patients, séquences de génomes, expériences scientifiques) ou sous la forme de séries temporelles (e.g., analyse du signal, données météorologiques, économétrie).
La fouille de motifs séquentiels, qui consiste à découvrir des sous-séquences fréquentes à partir d'une base de données séquentielles, a suscité un grand intérêt dans une multitude de domaines. Par exemple, en télécommunication, des motifs séquentiels sur des mouvements de groupes d'utilisateurs d'appareils mobiles peuvent être utilisés pour prédire la position future d'un utilisateur. Dans le domaine bioinformatique on peut rechercher des motifs dans des sé-quences d'ADN ou des motifs séquentiels permettant la prédiction de la fonction de certaines protéines. Dans le domaine des sciences expérimentales, les scientifiques recherchent dans les comptes-rendus d'expériences des sous-séquences d'étapes fréquentes permettant la prédic-tion de valeurs d'observations manquantes ou encore des relations de cause-à-effet. Dans cet article, nous avons choisi ce dernier domaine d'application pour définir, illustrer et évaluer l'efficacité de notre approche de découverte de séquences graduelles partiellement ordonnées.
Les expériences scientifiques réalisées sont souvent répétées plusieurs fois dans des conditions différentes. Le volume et les proportions des produits utilisés, ainsi que la configuration du milieu de l'expérimentation varient. Ainsi, trouver des observations similaires entre plusieurs expériences n'est pas envisageable. Par contre, dans ce contexte il est pertinent de rechercher des évolutions similaires entre deux observations. Ces augmentations ou diminutions des valeurs des observations sont appelées gradualité. Aussi, une expérience ne se déroule pas de manière linéaire : certaines manipulations sont réalisées en parallèle avec d'autres ce qui constitue une information importante sur l'ordre de déroulement des étapes de l'expérience qui doit être prise en compte. En effet, il faut conserver la coexistence des observations, i.e., il n'existe pas d'ordre total sur celles-ci, mais uniquement un ordre partiel. Comme étape préalable à la découverte de règles causales nous proposons une méthode automatique qui extrait des motifs séquentiels fréquents. Ces motifs sont graduels, puisque qu'ils expriment la diminution et l'augmentation des valeurs des attributs ; ils sont partiellement ordonnés car ils représentent l'ordre partiel pouvant être présent dans les données initiales. L'approche que nous présentons dans cet article prend en entrée une base de séquences valuées partiellement ordonnées sous la forme d'une base de graphes orientés étiquetés aux sommets et fournit en sortie un ensemble de motifs séquentiels graduels partiellement ordonnés représentés par des graphes orientés étiquetés aux arcs. La méthode développée a été testée et évaluée sur deux jeux de données réelles en biologie. Le premier concerne des données issues du projet Qualiment CellExtraDry sur les processus de transformation et de stabilisation de la levure ; et le second concerne des données issues du projet Carrédas sur des expériences sur la dégustation et la digestion de gels laitiers.
Cet article est organisé de la façon suivante. Nous présentons tout d'abord un rapide panorama des méthodes qui s'intéressent à la découverte de séquences fréquences et de motifs graduels. Nous posons ensuite quelques définitions avant de détailler notre proposition ainsi que les algorithmes associés. Nous présentons et discutons les expérimentations réalisées sur des données réelles. Nous concluons par un bilan et des perspectives à court et moyen terme.
État de l'art
Les recherches sur l'extraction de connaissances à partir de données séquentielles restent peu nombreuses au sein de la communauté fouille de données même si un regain se fait ressentir du fait de la mise à disposition de nombreuses séries temporelles (comme par exemple dans le domaine de l'imagerie satellitaire). Nous pouvons citer Pei et al. (2004) qui traite le problème de l'extraction de motifs séquentiels à partir d'une base de données de séquences à l'aide de l'algorithme PrefixSpan, en utilisant une technique de croissance de motif. Afin de lever la contrainte forte de précédence, Pei et al. (2006) propose l'algorithme Frecpo pour extraire des motifs partiellement ordonnés de séquences simples sans items répétés et sans itemsets. Malgré les bonnes performances de Frecpo, son champ d'application est très limité étant donné que la même information peut apparaître plusieurs fois ou plusieurs informations peuvent être datées en même temps dans une base de données.
Dans Casas-Garriga (2005), un algorithme est présenté pour extraire des motifs partiellement ordonnés (po) clos (c'est-à-dire que pour chaque motif extrait, il n'existe pas de sousséquence ayant le même support) d'une base de séquences. Il extrait d'abord des motifs sé-quentiels, puis transforme les motifs séquentiels en motifs po en post-traitement. Il n'extrait donc pas directement les motifs po. D'autre part, d'après Fabrègue et al. (2015), il n'extrait qu'un sous-ensemble des motifs po.
Dans Fabrègue et al. (2015), l'algorithme OrderSpan est décrit et permet d'extraire des motifs po clos d'une base de séquences directement, en utilisant comme dans Pei et al. (2004)   (Pei et al., 2004) pourrait le remplacer. Néanmoins, la contrainte de temporalité est appliquée en post-traitement en filtrant les motifs produits ce qui alourdit le processus. Ajoutons aussi que la base de données d'entrée est très simple et peut être vue comme une seule séquence valuée non partiellement ordonnée.
Nous pouvons constater qu'il n'existe pas d'approche permettant d'extraire directement des motifs graduels po d'une base de données de séquences. C'est ce que nous proposons dans cet article en adoptant l'algorithme Fabrègue et al. (2015) en utilisant également une technique de croissance de motif.
Motifs graduels partiellement ordonnés
Nous allons tout d'abord présenter quelques définitions en les illustrant à l'aide de notre domaine d'application : les expériences scientifiques. Ensuite, nous décrirons notre algorithme.
Concepts et définitions
Une expérience est composée de plusieurs étapes, chaque étape ayant des observations. Une observation possède une qualité et une valeur (par exemple la qualité température et la valeur 42 degrés) et peut être vue comme un item valué. Une étape peut être modélisée par un itemset valué. 
Une évolution entre deux items valués (augmentation, diminution, stagnation) peut être représentée par un item graduel. On dit alors que les deux items valués supportent l'item graduel.
Définition 2 (Item graduel) Un item graduel I g est un item I ∈ I muni d'un symbole • ∈ {<, >, =}, noté I • . On note I g l'ensemble des items graduels :
Les étapes d'une expérience ne se déroulent pas nécessairement l'une après l'autre, par exemple deux étapes peuvent se dérouler en parallèle. On peut donc représenter une expérience par un graphe dont les sommets sont les étapes. Un exemple d'un tel graphe est illustré figure 1. Ce graphe est appelé une séquence valuée partiellement ordonnée.
Définition 4 (Séquence valuée partiellement ordonnée) Une séquence valuée partiellement ordonnée (séquence valuée po, ou vpo) est un graphe orienté acyclique étiqueté aux
-V est l'ensemble des sommets et A est l'ensemble des arcs où (V, ) est un ordre partiel sur les sommets et Les deux sommets étiquetés par et sont utilisés pour marquer le début et la fin de la séquence po.
Pour u, v deux sommets du graphe, u v si et seulement s'il y a un chemin de u à v. S'il n'y a pas de chemin de u à v, alors ces deux éléments ne sont pas comparables.
Si une étape se déroule avant une autre, on peut lister les items graduels représentant les évolutions des observations entre ces deux étapes. On dit alors que la séquence supporte les items graduels.
Définition 5 (Item graduel supporté par une séquence valuée po) Une séquence valuée po
Dans le graphe d'une expérience, les évolutions des observations entre les étapes sont représentées par des arcs étiquetés par l'item graduel associé à cette évolution. Par exemple, d'un sommet la température vaut 36 degrés à un sommet la température vaut 42 degrés, on peut définir un arc la température augmente. Le graphe obtenu est appelé séquence graduelle partiellement ordonnée. La figure 2 illustre la séquence graduelle po correspondant à la sé-quence valuée po de la figure 1.
Définition 6 (Séquence graduelle partiellement ordonnée) Une séquence graduelle partiellement ordonnée (séquence gpo) est un graphe orienté acyclique étiqueté aux arcs
-V est l'ensemble des sommets et A est l'ensemble des arcs -Σ A = I g ∪ {∅} est l'alphabet des étiquettes des arcs, soit l'ensemble des items graduels (ou ∅ pour signifier une absence d'étiquette) -l A : A → Σ A donne la correspondance entre les arcs et leur étiquette 
Support
Notre but est d'extraire un ensemble d'évolutions communes à plusieurs expériences. Plus précisément, nous souhaitons extraire les évolutions qui sont suffisamment fréquentes, que nous appellerons motifs graduels po. Pour cela, nous allons définir la notion de support.
Il s'agit de compter le nombre de séquences dans la base de données qui supportent un motif donné. La notion de M-inclusion est adoptée pour définir formellement l'occurrence du motif dans une séquence. Deux graphes sont m-inclus s'il est possible de passer de l'un à l'autre en supprimant des arêtes ou sommets et en fusionnant des sommets.
Définition 7 (M-inclusion) Soit S, S p des séquences po. On dit que S p est m-inclus dans S si et seulement s'il existe un mineur (Lovász, 2005) de S isomorphe à S p . On note S p m S.
La figure 3 illustre cette définition.
Définition 8 (Support en nombre de séquences) Soit DB une base de données de séquences po et S p une séquence po. Le support de la séquence S p par la base DB est défini par :
Algorithmes
Étant donnée une base de données de séquences valuées po, GradualSpan va extraire l'ensemble des séquences graduelles po ayant un support supérieur à un support minimum θ donné. Ces séquences extraites sont appelées des motifs graduels po. Notre algorithme fonctionne en trois étapes (la dernière n'est pas développée dans cet article) :
-Transformation des items valués en items graduels : chaque séquence valuée po de la base de données est convertie en séquence graduelle po. -Découverte des motifs graduels po : l'algorithme (Fabrègue et al., 2015) OrderSpan est modifié pour accepter une base de données de séquences graduelles po en entrée à la place de séquences po. Il produit des motifs graduels po en sortie.
Transformation des items valués en items graduels
La conversion d'une séquence valuée po en une séquence graduelle po est réalisée à l'aide de l'algorithme 1 par un parcours en profondeur (DFS). L'algorithme est appliqué récursivement à chaque sommet de la séquence valuée po d'entrée. Tout d'abord, comme il y a autant de sommets dans la séquence vpo que dans la séquence gpo associée, un nouveau sommet est créé (lignes 1-5). Le dictionnaire visited permet de ne créer qu'un sommet pour la séquence graduelle par sommet de la séquence valuée.
Ensuite, on va tracer les arcs entre les sommets existants et le nouveau sommet (lignes 6-11). Pour chaque item valué I v , on vérifie si on a déjà rencontré son item I v .item. Si oui, on crée l'item graduel I g à partir de l'item valué déjà rencontré et de l'item valué actuel, puis on dessine l'arc correspondant. Dans tous les cas, on retient dans le dictionnaire last que l'on vient de rencontrer l'item.
Enfin, on rappelle l'algorithme récursivement sur tous les successeurs du sommet actuel (ligne 12). Afin de conserver les arcs de la séquence vpo d'entrée, on dessine des arcs non étiquetés du sommet actuel aux sommets nouvellement créés.
Pendant la traversée du graphe, le dictionnaire last contient tous les items déjà rencontrés lors du parcours du chemin de la racine jusqu'au sommet actuel. Lorsqu'on parcourt les successeurs, chaque appel récursif va déclencher de nouvelles rencontres d'items, or on ne souhaite pas qu'une rencontre d'item dans une branche soit visible dans une autre branche. Pour cela, l'opération d'ajout PUT dans le dictionnaire last ne doit pas le modifier in-place. Pour représenter ce dictionnaire, on préférera donc utiliser un arbre de recherche (Knuth (1998)) plutôt qu'une table de hachage (cela permet de réaliser les opérations en temps logarithmique tout en partageant la mémoire).
L'algorithme VALUEDTOGRADUAL consiste en un parcours en profondeur de chaque sé-quence valuée de la base d'entrée. Néanmoins, afin d'extraire tous les items graduels, il est nécessaire de visiter plusieurs fois les mêmes noeuds, plus précisément de visiter tous les chemins du graphe orienté acyclique. La complexité dans le pire des cas de VALUEDTOGRA-DUAL est O(|DB v |.v max .2 vmax ), avec |DB v | le nombre de séquences dans la base et v max le nombre de sommets de la séquence la plus grande. Cette complexité est linéaire en nombre de séquences à traiter et exponentielle en la taille des séquences.
Découverte de motifs graduels
La découverte de motifs graduels est réalisée avec l'algorithme 2, une variante de OR-DERSPAN (Fabrègue et al., 2015).
Algorithme 2 FORWARDTREEMINING
Entrée : DBg une base de données de séquences graduelles po, θ un support minimum et mined l'ensemble des sous-ensembles de la base de données déjà explorés Sortie : pattern un ensemble de motifs graduels po 1 pattern ← NEWDAG() 2 pattern.begin.database ← NEWPROJECTEDDATABASE(DBg ) 3 nodeQueue ← NEWQUEUE({pattern.begin}) 4 while nodeQueue is not empty do
n .database ← n.database.PROJECTON(occ.item) 10 draw an arrow from n to n labelled by occ.item 11 nodeQueue.PUSH(n ) 12 for all occ in occList such that occ.support < |DBg| do 13 MERGINGSUFFIXTREE(pattern.end) 18 OUTPUT(pattern) Chaque appel à FORWARDTREEMINING extrait un motif ayant un support égal au support maximal de la base de données. Tout d'abord, un graphe acyclique orienté est créé pour contenir le motif qui sera extrait (ligne 1). On va attacher à chaque sommet une base de données projetée contenant les suffixes des séquences de DB g par rapport au préfixe (i.e. items rencontrés sur le chemin de la racine jusqu'au sommet actuel). Une base projetée est attachée après tous les items sur lesquels elle a été projetée. Pour le premier sommet, la base de données projetée est exactement la base de données d'entrée (ligne 2). Une file nodeQueue est initialisée avec le premier sommet (ligne 3).
La boucle ligne 4 étend le motif au maximum, i.e. la file est vide lorsque le motif ne peut plus être étendu. La liste des items ayant un support au moins égal à θ est construite par un parcours en profondeur de chaque séquence de la base projetée (ligne 6).
Dans un premier temps, on ne considère que les items qui ont un support égal au cardinal de la base d'entrée (lignes 7-11). Comme on est en train de construire un motif ayant un support égal au support maximal, seuls ces derniers peuvent être ajoutés au motif sans faire diminuer son support. Pour chacun de ces items, on crée un nouveau sommet n en lui attachant la base de données projetée sur l'item.
Ensuite, on considère les items qui ont un support strictement inférieur au cardinal de la base (lignes 12-16). Ces items vont pouvoir être étendus avec des items du même support pour produire des motifs. Pour cela, on extrait le sous-ensemble DB g de la base de données qui supporte l'item, et on se rappelle récursivement sur ce sous-ensemble de la base d'entrée. Pour éviter de parcourir plusieurs fois les mêmes sous-ensembles de la base d'entrée, on utilise l'ensemble mined.
Une fois que le motif ne peut plus être étendu, on supprime les redondances du motif en appliquant MERGINGSUFFIXTREE (ligne 17, cet algorithme n'est pas présenté dans ce papier).
Notons que cet algorithme a la propriété d'extraire des motifs graduels po clos, c'est-à-dire que pour chaque motif extrait, il n'existe pas de sous-séquence ayant le même support.
La procédure FORWARDTREEMINING a la même complexité qu'ORDERSPAN (Fabrègue et al., 2015) étant donné que le parcours des séquences est strictement identique.
Expérimentations
Nous présentons dans cette section les premiers résultats de l'évaluation de notre approche que nous avons effectuée sur des données scientifiques.
Description du jeu de données. Nous avons évalué notre approche de découverte de motifs graduels partiellement ordonnés sur des données scientifiques représentées en RDF et relevant du domaine de l'agro-alimentaire. Ces données sont décrites suivant les classes et les propriétés de l'ontologie P O 2 (Ibanescu et al. (2016) Résultats de l'algorithme GradualSpan. En exécutant GradualSpan sur la base de données avec un support minimal égal à 15, on obtient 5 motifs graduels po. Le premier a un support de 20 (il est supporté par toute la base). Les suivants ont un support de 16 et sont tous des variations du premier (ils contiennent des items graduels en plus ce qui fait diminuer le support du motif). Dans le tableau 1 nous présentons la liste des attributs représentant la composition des mixtures, la liste des attributs observés ainsi que la liste des étapes impliqués dans des variations exprimées dans les cinq motifs découverts dans la source de données. Les motifs produits par GradualSpan sont représentés par un graphe graduel dont les arcs expriment les variations des valeurs des attributs des produits et des attributs observés ; et les noeuds représentent une étape ou une succession d'étapes. Afin de reconstituer les étapes concernées par les variations, nous avons mis en correspondance les motifs obtenus avec les graphes graduels représentant les 20 itinéraires considérés. En figure 4, nous montrons deux exemples de motifs graduels extraits et traduits en considérant les étapes des graphes graduels de départ (les arcs rouges expriment une diminution et les arcs bleus une augmentation). Nous pouvons remarquer que m 1 et un sous-graphe de m 3 et que dans m 3 nous avons deux infor-mations additionnelles qui expriment le fait qu'on a observé une augmentation de la valeur de l'observation o 7 lorsque l'étape e 6 et suivie de l'étape e 8 ainsi que lorsque e 6 est suivie de e 8 .
L'un des objectifs visés par les experts biologistes ayant produit cette source de données est de trouver des relations de causes-à-effets entre les attributs composants les mixtures et les attributs observés. En effet, on pourrait traduire les différents chemins exprimés dans les motifs graduels partiellement ordonnés en règles logiques, dont les prémisses sont variations sur les valeurs des attributs-produits et des observations intermédiaires et la conclusion représenterait les variations détectées pour les observations concernant les dernières étapes dans le graphe. A partir du motifs m 1 on pourrait générer 12 règles comme par exemple, celles qui expriment : R1 : Si (p < 1 p < 2 p < 3 p < 4 ) entre les étapes e 3 et e 4 alors (o > 4 ) entre les étapes e 4 et e 7 . R2 : Si (p < 1 p < 2 p < 3 p < 4 ) entre les étapes e 3 et e 4 , et (o < 3 o < 6 ) entre les étapes e 4 et e 6 alors (o < 2 o < 3 o < 6 o < 5 ) entre les étapes e 6 et e 8 . En résumé, les motifs graduels obtenus sont très expressifs et permettent de représenter des relations de dépendances entre des variations subies par les attributs des produits composant les mixture et ceux des observations. Il est possible de générer des règles logiques exprimant ces dépendances. Dans la suite nous souhaitons, tout d'abord, faire évaluer les motifs obtenus par les experts du domaine afin de rendre compte de leur qualité et pertinence pour le domaine d'application. Nous souhaitons également générer les règles logiques à partir des motifs de façon complètement automatique.
Conclusion
La méthode que nous avons développée est inspirée de l'approche OrderSpan Fabrègue et al. (2015) qui prend en entrée une base de séquences et qui produit des séquences partiellement ordonnées. Nous avons proposé un premier algorithme pour transformer les séquences partiellement ordonnées en des séquences graduelles partiellement ordonnées exprimant les variations (augmentation et diminution) des valeurs des attributs. Puis un second algorithme a été développé pour rechercher les sous-séquences graduelles partiellement ordonnées fré-quentes, i.e. respectant une valeur de support minimum. Enfin, un post-traitement est ensuite appliqué afin d'éliminer les arcs vides (non porteurs d'informations) mais aussi des arcs redondants ou pouvant être retrouvés par transitivité. Les perspectives associées sont nombreuses. Nous citerons tout d'abord la nécessité d'offrir aux experts une interface aisée de navigation entre les données sources et les motifs po afin de valider ceux-ci. Enfin la prise en compte des itemsets graduels doit être également proposée. 

Introduction
Le foisonnement des bases de connaissances sur le web pose de nouveaux enjeux quant à leur construction, leur enrichissement et leur interrogation. Nous prenons ici l'exemple de DBpédia. Dans cette base de connaissances, les ressources peuvent appartenir à une ou plusieurs catégories, générées manuellement. Cependant, les catégories sont définies en extension : on sait comment les ressources sont groupées, mais pas pourquoi elles sont groupées ainsi. Dans cet article, nous souhaitons définir les catégories en intension. C'est-à-dire que nous nous intéressons à des méthodes permettant d'expliciter les critères de regroupement.
Savoir caractériser ces catégories permettra non seulement d'enrichir DBpédia par cette nouvelle connaissance, mais aussi de corriger la base : les ressources assignées à tort à une catégorie ou inversement, les ressources non assignées à une catégorie à laquelle elles sont sensées appartenir pourront être détectées automatiquement.
La base de connaissances de DBpédia contient un ensemble de triplets RDF, dont les sujets correspondent à des articles de Wikipédia, associés à des paires (predicat, objet) qui représentent les catégories auxquelles ces articles sont rattachés et d'autres informations.
Afin de caractériser ces catégories en terme des autres informations, nous recherchons des définitions en exploitant des techniques de fouille de données. Chaque paire distincte (predicat, objet) est représentée par un attribut Booléen, tandis que chaque article est représenté par une entité, associée à un sous ensemble d'attributs représentant les catégories 
Comme nous travaillons sur des données incomplètes, nous avons besoin d'une notion qui soit moins restrictive qu'une définition exacte. Nous introduisons donc la notion de quasidéfinition, qui est à la définition ce que la règle d'association est à l'implication, et que nous dénotons donc A ↔ B. Pour évaluer la qualité de ces règles nous considérons la confiance minimum, c -(A ↔ B) = min(conf(A → B), conf(B → A)), qui garantie une bonne confiance dans les deux directions et est plus adaptée à une recherche de quasi-définitions. Au contraire, la confiance maximum c + (A ↔ B) prend une valeur de 1 dès que l'une des deux règles est une implication, même si sa réciproque a une confiance très faible.
Algorithmes
Pour extraire des quasi-définitions, nous avons recours à des techniques existantes, déve-loppées pour la fouille de règles d'associations et de redescriptions.
Règles d'association. Depuis l'introduction par Agrawal et al. (1993) des règles d'association, de nombreux algorithmes ont été développés, dont Eclat (Zaki, 2000) que nous utilisons ici. L'objectif est d'énumérer exhaustivement de manière efficace les règles ayant un support et une confiance au-dessus de seuils choisis. Dans la formulation originale du problème, on considère un unique jeu de données et tous les attributs peuvent se trouver aussi bien d'un côté que de l'autre de la règle. Dans notre formulation, au contraire, on considère deux ensembles d'attributs distincts. Nous avons donc ajouté une étape de post-traitement pour ne conserver que les règles qui satisfont la séparation des attributs.
Règles de traduction. Considérant un jeu de données Booléen à deux vues, l'approche proposée par van Leeuwen et Galbrun (2015), avec l'algorithme Translator, vise à obtenir un ensemble compact de règles permettant de "traduire" les données d'une vue vers l'autre et vice-versa. On peut effectivement reconstruire une vue en utilisant l'autre vue et un ensemble de règles d'associations. En partant d'un jeu de données vide et en considérant chaque règle tour à tour, on insère les attributs qui constituent la conséquence de la règle dans les lignes correspondant aux entités qui possèdent son antécédent. Une fois toutes les règles parcourues, on applique un masque pour corriger les erreurs restantes et reconstruire ainsi fidèlement la vue originale. Les règles de traduction peuvent être uni-directionnelles, s'appliquant uniquement dans une direction ou dans l'autre, ou bien bi-directionnelles. La sélection des règles s'inspire du principe de longueur de description minimum (MDL). C'est à dire que le critère de sélection est la compression : on cherche un ensemble de règles qui permette de représenter une vue étant donnée l'autre et vice-versa de la manière la plus succincte possible. Cela permet d'obtenir un nombre restreint de règles très informatives, en évitant la redondance.
Redescriptions. La fouille de redescriptions, introduite par Ramakrishnan et al. (2004), a pour but de trouver des descriptions alternatives pour un même ensemble d'entités. La simila-ERT Android_(OS)_devices ↔ (dbo:operatingSysem dbr:Android_OS) (R1) ET Nokia_mobile_phones ↔ (dbo:manufacturer dbr:Nokia), (dbp:manufacturer Nokia) (R2) R Nokia_mobile_phones ↔ (dbp:manufacturer Nokia) (R3) ER Nokia_mobile_phones ↔ (dbo:manufacturer Nokia) (R4) ERT Samsung_Galaxy ↔ (dbo:manufacturer Samsung_Electronics), (dbo:operatingSysem Android_OS) (R5) ERT Sony ↔ (dbo:manufacturer dbSony), (dbo:operatingSysem Android_OS) (R6) ERT Mobile_operating_systems ↔ (rdf:type dbo:Software), (rdf:type dbo:Work) (R7)
FIG. 1: Exemples de règles obtenues par les algorithmes Eclat (E), Translator (T) et
ReReMi (R). Lorsque le préfixe n'est pas précisé, il s'agit de dbr.
rité des descriptions est mesurée par le coefficient de Jaccard des ensembles d'entités décrites :
L'objectif est donc de construire des paires de descriptions ayant un support et une similarité au-dessus de seuils choisis. Nous utilisons l'algorithme ReReMi (Galbrun et Miettinen, 2012). Considérant un jeu de données à deux vues, incluant potentiellement des variables numériques, l'algorithme ReReMi utilise des heuristiques pour construire des descriptions qui peuvent impliquer à la fois conjonctions et disjonctions. Cependant, nous ne considérons ici que des variables Booléennes, et pour permettre la comparaison avec les autres approches, nous avons restreint l'algorithme aux seules conjonctions.
Résultats expérimentaux
Afin de mener une comparaison qualitative fine et détaillée, nous restreignons notre étude à un sous-ensemble de DBpédia en sélectionnant tous les triplets dont les sujets appartiennent la catégorie Smartphones. Le jeu de données ainsi obtenu contient 566 entités, 330 attributs représentant des catégories et 475 attributs représentant d'autres informations.
Nous avons appliqué les trois algorithmes, Eclat, Translator et ReReMi sur le jeu de données ainsi obtenu. Les règles candidates obtenues sont de la forme R C ↔ R I où R C et R I sont des ensembles d'attributs, représentant des catégories et d'autres informations respectivement, interprétés comme des conjonctions.
Chaque algorithme retourne une liste ordonnée de règles candidates. Pour Translator, les règles sont retournées dans l'ordre dans lequel elles sont intégrées au modèle de compression. Avec Eclat et ReReMi elles sont triées par ordre décroissant de confiance maximum (c + ) pour le premier, et de Jaccard (J) pour le second. Pour un algorithme X, on dénote R k X les k premières règles retournées et R X l'ensemble des règles retournées. Des exemples de règles obtenues sont présentés dans la Figure 1.
Nous avons évalué manuellement toutes les règles retournées par les algorithmes afin de vérifier si elles correspondent effectivement à une définition. Par exemple, la règle (R7) ne constitue pas une définition, puisque la catégorie Mobile_operating_systems est plus spéci-fique que le type Smartphone. La règle (R1) fournit en revanche une définition correcte de la catégorie Android_(OS)_devices. L'ensemble des définitions correctes obtenues avec les trois algorithmes forme une base de 20 définitions, dénotée D, qui nous fournit une référence de vérité pour la comparaison des algorithmes.
Étant donné une règle candidate R C ↔ R I et une définition D C ≡ D I de la base D, on dit que la règle couvre la définition si et seulement si D C ⊆ R C et D I ⊆ R I . On souligne qu'une règle candidate peut couvrir plusieurs définitions de D. Par exemple, la règle (R6) couvre les deux définitions Sony ≡ (dbp:manufacturer dbr:Sony) et Sony ≡ (dbp:manufacturer dbr:Sony), (dbo:operatingSystem dbr:Android_(OS)). Étant donné la base D et un ensemble de règles candidates R on peut déterminer la couverture de D par R comme couv(D, R) = {D ∈ D | ∃ R ∈ R, R couvre D}. Pour chaque algorithme on peut ainsi calculer le pourcentage de définitions couvertes par les règles retournées, ce que nous appelons le rappel de l'algorithme et on peut calculer de la même manière la précision de chaque algorithme, c'est à dire le pourcentage de règles retournées qui constituent une définition correcte. On a donc
Les statistiques des résultats obtenus avec chaque algorithme sont indiquées dans la Table 1a.
On s'intéresse également à l'évolution du nombre de définitions couvertes par les règles candidates à mesure que l'on augmente le nombre de règles considérées. La Figure 1b  Les résultats obtenus avec Eclat sont proches des résultats obtenus avec ReReMi. Cela n'est pas surprenant car ReReMi est limité ici aux conjonctions et les mesures de confiance et de coefficient de Jaccard sont similaires. Par contre, ReReMi ne fait pas une énumération exhaustive et retourne donc moins de règles. L'algorithme Translator retourne des résultats nettement différents des autres algorithmes, ce qui s'explique par un critère de sélection des règles reposant sur la compression.
On observe de la redondance dans les ensembles de règles : certaines règles, bien que n'étant pas identiques, n'apportent pas d'information supplémentaire. Cela se manifeste par des règles prises en compte qui ne permettent pas de couvrir de nouvelle définition, c'est à dire que la valeur de k est incrémentée mais celle de la couverture n'augmente pas (cf. Figure 1b).
Les différents algorithmes utilisent différentes mesures de qualité pour sélectionner et trier les résultats : c + pour l'algorithme Eclat, J pour l'algorithme ReReMi et la compression Translator. On observe qu'utiliser c -pour ordonner les règles candidates est particulière-ment utile avec les résultats d'Eclat, les redondances se retrouvant en fin de liste. Par contre, comme Translator équilibre la qualité et la diversité des résultats, réordonner les résultats n'apporte pas de gain notable.
Discussion
Sélectionner et interpréter les règles. Les approches considérées ici recherchent toutes des associations entre deux ensembles d'attributs, mais elles ne favorisent pas les mêmes motifs, car elles ne partagent pas le même objectif. En particulier, ReReMi favorise les règles courtes dans le but de faciliter l'interprétation tandis que Translator favorise des règles plus longues qui contiennent un maximum d'information afin d'obtenir une représentation 
TAB. 1: Statistiques des ensembles de règles obtenus par les différents algorithmes (1a) et nombre de définitions couvertes en fonction du nombre de règles prises retournées (1b).
compacte. Par exemple, Translator, retourne la règle (R2) tandis que ReReMi retourne les deux règles (R3) et (R4). Ici, le prédicat dbp :manufacturer résulte de l'extraction automatique des données et le prédicat dbo :manufacturer -créé manuellement pour compléter le schéma de l'ontologie -forment un doublon. Les triplets contenant le prédicat dbp :manufacturer que nous avions extraits ont d'ailleurs été récemment supprimés de DBpédia. Cette observation soulève la question de l'interprétation de la conjonction. Celle-ci ne joue pas le même rôle dans (R2) et dans la règle (R5) : alors que les attributs associés par la conjonction dans (R2) sont redondants et qu'il est possible d'en éliminer un sans altérer la validité de la définition, ce n'est pas le cas dans (R4) où la présence des deux attributs est nécessaire. L'interprétation des règles n'est valable que dans le domaine dans lequel elles ont été obtenues. Par exemple, (R3) n'est pas vraie en général, puisqu'il existe des appareils manufacturés par Nokia qui ne sont pas des téléphones mobiles. Elle a été obtenue et s'applique dans le monde clos des smartphones. Ce point ouvre des pistes de réflexion intéressantes, sur l'interprétation en monde ouvert, mais ce n'est pas notre priorité pour le moment : nous considérons les définitions obtenues uniquement dans un monde clos -celui qui est défini lors de l'extraction du jeu de données.
Utiliser des motifs plus expressifs. Nous nous sommes intéressés ici uniquement aux ressources identifiées par une URI, représentées sous la forme de jeux de données Booléens. Il serait intéressant de prendre en compte un vocabulaire plus riche, tant du côté des données que du schéma. Les données contiennent en effet par exemple des valeurs numériques (poids, distances, etc.) ainsi que des dates qu'il serait utile de prendre en compte. L'algorithme ReReMi permet déjà de traiter des valeurs numériques, mais ce n'est pas le cas de Translator. Nous n'avons pas tenu compte du schéma de DBpédia, à savoir les hiérarchies de classes et de prédi-cats. Intégrer ces connaissances dans le processus de fouille permettrait d'affiner les définitions ou d'en trouver de nouvelles. En utilisant les outils de la FCA, cela pourrait se traduire par un scaling, c'est-à-dire l'ajout d'attributs liés à la hiérarchie. Mais nous pourrions également tirer profit des structures de patrons, qui sont une généralisation de la FCA permettant d'intégrer une hiérarchie sur les attributs. Les algorithmes de fouille pourrait aussi être adaptés pour manipuler la hiérarchie directement. La formalisation de ces différentes approches et l'étude de leurs relations fournissent des pistes pour nos travaux à venir.
Dans cette étude, nous avons limité l'algorithme ReReMi aux conjonctions, mais il permet aussi d'utiliser des disjonctions, et ainsi d'obtenir par exemple la redescription Samsung_mo-bile_phones ↔ (dbo:manufacturer dbr:Samsung) ∨ (dbo:manufacturer dbr:Samsung_Elec-tronics). Nous pensons que les disjonctions peuvent contribuer à identifier les doublons, en les regroupant dans une même règle. Tirer parti de l'expressivité des redescriptions permise par ReReMi offrirait des résultats plus fins. Cependant, l'utilisation de disjonctions apporte son lot de difficultés, en terme de complexité de la fouille comme de subtilités d'interprétation.
Conclusion
Dans cet article, nous avons comparé trois algorithmes pour caractériser les catégories de DBpédia : Eclat, Translator et ReReMi. Translator atteint le meilleur rappel, mais les règles sont difficilement interprétables, limitant leur incorporation à la base de connaissances. Les algorithmes Eclat et ReReMi atteignent un rappel similaire. Cependant Eclat retourne beaucoup plus de règles redondantes. L'algorithme ReReMi est prometteur car il permet d'utiliser des disjonctions. Plusieurs pistes sont envisageables pour la suite, dont notamment l'intégration de données plus riches et la prise en compte du schéma de l'ontologie.
Références
Agrawal, R., T. Imieli´nskiImieli´nski, et A. Swami (1993). Mining association rules between sets of items in large databases. In ACM SIGMOD Rec., Volume 22, pp. 207-216. ACM.
Galbrun, E. 
Summary
DBpedia, which encode the knowledge of Wikipedia, has become a reference for the web of data. Resources can be indexed by manually-defined categories which are not accessible by a machine. In this article, we take a step towards addressing this accessibility issue by means of data mining techniques for mining association rules and redescriptions. We compare these approaches on a dataset from DBpedia and present our results.

Introduction
L'apprentissage automatique contient un ensemble puissant d'approches qui peut aider à détecter des anomalies de manière efficace. Cependant, il représente un processus lourd avec des règles strictes et une multitude de tâches telles que l'analyse et le nettoyage des données, la réduction de dimension, l'échantillonnage, la sélection d'algorithmes appropriés, le réglage précis des hyper-paramètres, etc. Notre système a été spécifiquement conçu pour simplifier ce processus lourd et accélérer le déploiement d'une solution en peu de temps. Notre système vise à identifier les anomalies dans un grand réseau d'eau potable géré par un leader national expert dans le domaine de l'eau. En fait, la découverte de telles irrégularités dans le réseau d'eau est une préoccupation critique tant sur le plan écologique que financier. Le volume réel d'eau perdue dans le monde a généré une perte de 32 milliards de m3 / an (soit 14 milliards d'euros par an) dont 90 % reste difficilement identifiable en raison de la nature souterraine du réseau. Sur la base de recherches approfondies menées par les experts, les anomalies peuvent être identifiées en utilisant des mesures de pression et de débit envoyées par des capteurs spécifiques dispersés sur tout le réseau de canalisations.
Architecture
Le système a été conçu pour traiter à la fois des données massives dynamiques et statiques à l'aide d'une architecture distribuée tolérante aux pannes. L'objectif principal est de pouvoir traiter des flux massifs de données en temps réel et de lancer des modèles intensifs d'apprentissage automatique. Pour répondre aux besoins d'un système distribué robuste, scalable et à faible latence, nous avons basé notre conception sur une architecture Lambda. Ce type d'architecture Big Data résout le problème des fonctions de calcul lourdes sur des données en temps réel en décomposant le problème en trois couches : une couche batch, une couche vitesse et couche service. Un scénario général de bout en bout commence par le stockage des données Détection de Singularités en Temps-Réel historiques horodatées sous forme de séries-temporelles à des fins de pré-analyse. La mise en cache de données massives nécessite un système de fichiers distribué robuste pour récupérer les données très rapidement. Le système utilise un cluster Hadoop lors de la première phase de traitement en batch. Cependant, dans la plupart des cas, les données brutes doivent être nettoyés pour augmenter la précision de l'identification des singularités. Deux étapes se succèdent ici, le système infère les données manquantes via des techniques d'interpolation et de maximisation de l'espérance, ensuite des techniques de réduction de dimensions permettent de réduire la taille du dataset initial. Une unité de modélisation distribuée appliquera plusieurs modèles de séries temporelles pour trouver des valeurs aberrantes (e.g., saisonnalité, chronologie, profil, etc). Les valeurs aberrantes seront utilisées pour classer les attributs selon la probabilité d'occurrence d'anomalie, les modèles n'étant appliqués que sur les attributs les mieux classés réduisant ainsi considérablement le temps de traitement. Cette méthode permet une allocation de données dynamique en optimisant la taille des paquets de données transférés entre le HDFS et le moteur Spark. Cette allocation de données est gérée par une unité sémantique profitant des atouts des ontologies. Après avoir converti les données réduites en RDF, un générateur de requêtes continues en SPARQL sélectionnera le graphique de taille minimale en utilisant une ontologie conçue pour le cas d'utilisation actuel. Afin de sélectionner l'algorithme correspondant au profil des flux ingérés, nous utilisons un ensemble complexe de règles telles que l'interdépendance des variables, le profil de distribution des données ou l'estimation de la complexité du traitement. Les résultats trouvés seront envoyés vers un système de messagerie, Apache Kafka, qui mettra en file d'attente les messages de manière ordonnée pour être exposés par un outil de visualisation. Enfin, le système s'appuie sur les annotations de l'utilisateur final pour lancer une nouvelle boucle d'itération qui stockera les signatures de chaque anomalie validée.
Contributions
Le système proposé est évolutif, permettant la détection d'anomalies sur des flux en temps réel à l'aide d'un mélange de techniques d'apprentissage automatique et d'une approche web sémantique. En s'appuyant sur le profil des données historiques, le système utilise un ensemble de règles hiérarchiques pour sélectionner le meilleur algorithme adapté au cas d'usage. Nous estimons qu'il s'agit du premier système visant à automatiser le processus complet d'apprentissage automatique depuis le nettoyage des données au lancement des modèles appropriés. Les tâches effectuées au cours de processus sont généralement effectuées manuellement par des experts qui ont besoin d'une connaissance suffisante du domaine de d'application et de l'apprentissage automatique en général. Utilisant les capacités d'un environnement distribué pour traiter des données massives et véloces, notre système propose de trouver automatiquement l'algorithme pouvant les meilleurs résultats en terme de précision et de temps d'exécution.
Summary
Using machine learning to solve complex use cases is generally a cumbersome, costly, and error-prone process. With our system, we remove the burden of this process and demonstrate that many machine learning tasks can be automated.

Introduction
Les motifs séquentiels ont été introduits par Agrawal et Srikant (1995) il y a plus de 20 ans et leur utilité a été prouvée dans différents domaines de recherche et d'applications comme la fouille d'usage du Web, la fouille de textes, la bioinformatique, la détection de fraudes, etc. Depuis la première publication, de nombreuses méthodes ont optimisé l'extraction des motifs séquentiels (Zaki, 2001;Pei et al., 2001) et ont introduit des variantes (Lo et al., 2008;Gomariz et al., 2013). Malgré toutes ces avancées, l'extraction des motifs séquentiels reste une tâche coûteuse qui génère souvent trop de motifs. Cette limite aussi atteinte par l'extraction des motifs ensemblistes a été contournée par l'échantillonnage de motifs. Une telle approche tire un nombre limité de motifs où la probabilité de tirer un motif est proportionnelle à sa fréquence. Cette approche a l'avantage de contrôler la taille de la sortie et d'apporter une collection de motifs qui reflète l'intégralité de l'espace de recherche. A notre connaissance, une telle approche n'a encore pas été envisagée pour les motifs séquentiels.
Adapter la procédure d'échantillonnage de motifs en deux étapes (Boley et al., 2011) aux données séquentielles n'est pas trivial. D'une part, une limite importante de l'échantillonnage de motifs est d'avoir tendance à retourner des motifs rares correspondant à la longue traîne. En effet, la longue traîne signifie que la très grande majorité des motifs ont une fréquence très faible et elle occulte les motifs les plus fréquents. Ce problème est exacerbé dans le cas des séquences où le nombre de motifs séquentiels de fréquence 1 explose dans les jeux de données réels. Malgré un tirage proportionnel à la fréquence, l'échantillonnage se concentrerait uniquement sur des séquences très longues et de fréquence 1. Pour éviter cet écueil de la longue traîne, nous choisissons d'introduire une contrainte sur la norme (i.e., sur le nombre d'items) pour contrôler la taille des motifs tirés. D'autre part, le coeur de cette approche requiert de dénombrer pour chaque séquence le nombre de sous-séquences distinctes. Cette tâche n'est pas aisée car une même séquence peut contenir plusieurs occurrences d'une même sous-séquence. A cette fin, nous généraliserons le travail de Egho et al. (2015) afin de dénombrer les sousséquences en tenant compte de la norme.
Dans cet article, notre objectif est d'échantillonner les motifs séquentiels proportionnellement à la fréquence avec une contrainte sur la norme. Premièrement, dans la section 4, nous proposons une méthode en deux étapes grâce à la généralisation de la formule de dé-nombrement des sous-séquences de Egho et al. (2015). Nous démontrons que cette méthode effectue un échantillonnage exact. Deuxièmement, dans la section 5, nous expérimentons cette approche sur plusieurs jeux de données réels. Nous montrons que notre approche est suffisamment performante pour retourner des centaines de motifs séquentiels par seconde. Nous montrons également l'apport de la contrainte sur la norme pour mieux maîtriser la qualité des motifs retournés et éviter la malédiction de la longue traîne.
Travaux relatifs
Cet état de l'art distingue les méthodes d'échantillonnage de motifs en entrée et en sortie. L'échantillonnage en entrée (Toivonen et al., 1996) consiste à régénérer depuis un échantillon de données tous les motifs qui auraient été extraits depuis le jeu de données complet. L'échan-tillonnage en sortie (Al Hasan et Zaki, 2009) consiste à générer un échantillon de motifs parmi les motifs qui auraient été extraits depuis le jeu de données complet. Plusieurs approches ont été proposées pour l'échantillonnage en entrée des motifs séquentiels (Raissi et Poncelet, 2007), mais à notre connaissance, cet article propose la première approche d'échantillonnage de motifs séquentiels en sortie. Comme la complexité de l'échantillonnage de motifs est indépen-dante de la taille du langage, elle est propice aux langages structurés dont la combinatoire est forte. D'ailleurs, des méthodes ont été proposées pour les sous-graphes.
Plusieurs procédures ont été proposées pour l'échantillonnage de motifs. La première famille (Al Hasan et Zaki, 2009) repose sur les méthodes de Monte-Carlo par chaînes de Markov. L'idée est que la loi stationnaire de la marche aléatoire corresponde à la distribution à échan-tillonner. La limite de telles approches stochastiques est la vitesse de convergence qui peut être lente. La seconde famille (Boley et al., 2011) consiste à tirer une instance du jeu de données, puis à tirer un motif contenu dans cette instance. En choisissant judicieusement les deux distributions de tirage, il est alors possible d'obtenir un tirage exact selon la distribution désirée. Nous avons opté pour une telle approche en deux étapes pour sa rapidité et sa précision. Outre la difficulté de traiter des séquences plutôt que des itemsets, nous avons également ajouté une contrainte sur la norme des motifs. A notre connaissance, seule une approche (Dzyuba et al., 2017) permettrait de traiter à la fois des langages complexes et des contraintes. Fondée sur la satisfaction de contraintes, elle requière de disposer d'un solveur intégrant efficacement des contraintes XOR et elle n'a été utilisée que pour des motifs ensemblistes. Par ailleurs, Dzyuba et al. (2017) soulignent que leur approche générique rivalisera difficilement avec des approches dédiées à un seul langage et/ou classe de contraintes.
Préliminaires
Après avoir rappelé quelques définitions, cette section formalise le problème de l'échan-tillonnage de motifs séquentiels sous-contraintes sur la norme.
Définitions
Soit I un ensemble fini de littéraux nommés items. Un itemset ou motif X est un sousensemble non vide de I. Une séquence s définie sur I est une liste ordonnée s = X 1 , . . . , X n d'itemsets non-vides X i ⊆ I (1 ≤ i ≤ n, n ∈ N). n est la taille de la séquence s noté |s|. La norme d'une séquence s, notée s, est la somme des cardinalités de ses itemsets, i.e. s = n i=1 |s i |. Par la suite, on note s l le préfixe
. Enfin, on note S l'ensemble universel de toutes les séquences définies sur I, et une base de données séquentielles S sur I est un multi-ensemble de séquences définies sur I.
Nous rappelons maintenant les définitions de sous-séquences et d'occurrences d'une sousséquence dans une séquence donnée.
Définition 1 (Sous-séquence) Une séquence s = X 1 , . . . , X m est une sous-séquence d'une séquence s = X 1 , . . . , X n , noté s s, s'il existe une séquence d'indices 1 ≤ i 1 < i 2 < · · · < i m ≤ n telle que pour tout j ∈ [1..m], on ait X j ⊆ X ij . Etant donnée une séquence s, on note φ(s) l'ensemble des sous-séquences de s, i.e. φ(s) = {s ∈ S | s s}, et Φ(s) la cardinalité de cet ensemble, i.e. Φ(s) = |φ(s)|.
Etant donnée une séquence s = X 1 , . . . , X n , une sous-séquence s = X 1 , . . . , X m de s peut apparaître plusieurs fois au sein de s s'il existe plusieurs séquences d'indices 1 ≤ i 1 < i 2 < · · · < i m ≤ n telles que pour tout j ∈ [1..m], on ait X j ⊆ X ij . Dans ce cas, on parle d'occurrences multiples de la sous-séquence s au sein de s. La définition suivante précise comment ces différentes occurrences peuvent être représentées.
Définition 2 (Occurrence) Etant donnée une séquence s = X 1 , . . . , X n , une liste ordonnée d'itemsets o = Z 1 , . . . , Z n de même taille que s est une occurrence d'une sous-séquence
Formalisation du problème
Une méthode d'extraction de motifs par échantillonnage a généralement pour objectif de tirer aléatoirement un motif par rapport à une mesure d'intérêt donnée. Dans notre cas, la mesure considérée est la fréquence du motif dans une base de données séquentielles.
Définition 3 (Fréquence) Etant données une base de données séquentielles S définie sur I et une sous-séquence s ∈ S. La fréquence de s dans S, noté f req(s, S) ou plus simplement f req(s), est définie par : f req(s, S) = |{s ∈ S | s s }|.
Notre objectif est de tirer aléatoirement des motifs séquentiels par rapport à la fréquence et sous une contrainte de norme. Etant donnés deux entiers m et M tels que m ≤ M , on notera
Le problème posé peut finalement s'énoncer comme suit :
Etant données une base de données séquentielles S, des normes minimale m et maximale M , notre problème consiste à tirer aléatoirement une sous-séquence s ∈ S [m,M ] telle que la probabilité de tirage p(s) de s soit égale à la fréquence de s dans S normalisé par la somme des fréquences des sous-séquences de S dans S [m,M ] , i.e.
f req(s , S) .
Méthode d'échantillonnage en deux étapes sous contrainte 4.1 Aperçu de l'approche
Dans l'approche proposée par Boley et al. (2011), les auteurs montrent comment échan-tillonner des itemsets proportionnellement à leur support dans une base de données transactionnelles. Nous proposons d'utiliser une solution comparable en deux étapes, mais en ajoutant une contrainte sur la norme des motifs extraits. Tirage d'un motif séquentiel Dans la deuxième étape, nous commençons par tirer aléatoi-rement (ligne 3 de l'algorithme 1), la norme k de la sous-séquence de s qui sera finalement retournée. Ce nombre k est tiré proportionnellement au nombre de sous-séquences de s de norme exactement égale à k, i.e. selon la distribution de probabilité
. Finalement, l'algorithme 1 retourne à la ligne 4 une sous-séquence s de s de norme k selon une distribution uniforme, ce qui signifie que toute sous-séquence s de s de norme k sera tirée avec la même probabilité
. Nous montrons dans la section 4.3 comment effectuer un tel tirage uniforme grâce à une méthode par rejet. Le problème principal posé est qu'une sous-séquence s de s peut avoir plusieurs occurrences dans s et qu'il ne faut donc pas tirer avec une probabilité plus élevée des sous-séquences de s ayant un nombre d'occurrences plus important. 
return Une sous-séquence s ∼ u({s s | |s = k) de s où u est la distribution uniforme
Poids et tirage d'une séquence
Dans cette section, nous montrons comment calculer le nombre de sous-séquences d'une séquence s sous contrainte de norme en généralisant la proposition de Egho et al. (2015). La principale difficulté est de ne pas compter plusieurs fois une même sous-séquence même si elle possède plusieurs occurrences dans s. Sans contrainte sur la norme Soient une séquence s = X 1 , . . . , X n et un itemset Y . Par la suite, nous notons s
Intuitivement, si Y est disjoint de tous les itemsets de la séquence s, il est aisé de vérifier que le nombre de sous-séquences distinctes de s • Y est égal au nombre de sous-séquences de s multiplié par le nombre de sous-ensembles de    
où R ≤j (s, Y ) est un terme correcteur défini par :
Ce théorème est une généralisation du théorème 1. Notons par exemple que le premier terme
. Intuitivement, pour construire une sous-séquence de norme inférieure à j de s • Y , on peut concaténer tout sous-ensemble de taille k de Y à une sous-séquence de norme inférieure à j − k de s. Ainsi, on est certain d'obtenir une sous-séquence de s • Y de norme inférieure à k + (j − k) = j, et il faut répéter ce principe pour toute taille possible d'un sousensemble de Y . La même intuition explique la généralisation du terme correcteur R(s, Y ). En poursuivant l'exemple 3, l'exemple suivant illustre le principe de fonctionnement de la formule du théorème 2.
Exemple 4 L'ensemble φ ≤2 (s 1 ) des sous-séquences de s 1 = (ab) de norme inférieure à 2 est défini par φ ≤2 (s 1 ) = {{ {, a, b, (ab)}. Nous avons donc Φ ≤2 (s 1 ) = 4, et on voit aussi aisément que Φ ≤1 (s 1 ) = 3 (la sous-séquence (ab) étant de norme strictement supérieure à
Le premier terme de la somme correspond aux 4 sous-séquences de s 3 obtenues par concaténation du sous-ensemble vide aux sous-séquences de s 2 , alors que le deuxième terme correspond aux 3 sous-séquences de s 3 obtenues par concaténation de l'itemset (c) aux sous-séquences de s 2 de norme inférieure à 1. s[3]). Le second terme de Φ ≤2 (s 3 ), égal à 2 × 4, correspond par exemple au nombre de sous-séquences de s 3 pouvant être obtenues par concaténation d'un sous ensemble de taille 1 de (ab) (au nombre de 2) avec une sous-séquence de s 2 de norme inférieure à 1. Pour finir, le calcul du terme correcteur R ≤2 (s 2 , s[3]) se présente comme suit :
La formule présentée au théorème 2 est récursive. Néanmoins, étant données une séquence s et une borne M ≤ ≤s, cette récursivité peut facilement être supprimée en calculant ligne par ligne les matrices T et R définies par :
-
représente le nombre de sous-séquences de norme inférieure ou égal à j de la séquence s i .  
Tirage par rejet d'une sous-séquence
Après avoir tiré aléatoirement une séquence s ∈ S proportionnellement à son poids w(s) (ligne 2 de l'algorithme 1) et un entier k entre m et M selon la distribution P [m,M ] (k) (ligne 3), l'objectif est maintenant de montrer comment retourner une sous-séquence de norme k tirée uniformément depuis la séquence s (ligne 4). La difficulté est de ne pas favoriser les séquences qui disposent de plusieurs occurrences au sein de la séquence.
Afin de contourner cette difficulté, nous proposons d'utiliser une méthode par rejet, en tirant uniformément une occurrence de la séquence s et en la rejetant si cette occurrence n'est pas la première. Comme chaque séquence dispose d'une unique première occurrence, cette approche garantit un tirage uniforme des motifs séquentiels. Pour commencer, nous formalisons la notion de première occurrence :
Enfin, on appelle première occurrence de s dans s sa plus petite occurrence (selon l'ordre défini précédemment).
Exemple 5 Dans la continuité de l'exemple 1, comme 1, 2 et 1, 3 sont les signatures respectives des deux occurrences o 1 = (a)(c)∅∅ et o 2 = (a)∅(c) de la sous-séquence s = (a)(c) de s = (ab)(cd)(ce), et que 1, 2 précède 1, 3, nous avons o 1 < o 2 . Enfin, il est aisé de vérifier que o 1 est la première occurrence de s dans s, o 1 et o 2 étant les deux seules occurrences de s dans s.
En pratique, nous devons surtout vérifier si une occurrence de la sous-séquence s s est la première occurrence de s au sein de la séquence s :
Propriété 1 Etant donnée une occurrence o d'une sous-séquence s s de signature σ = i 1 , i 2 , . . . , i m , o est la première occurrence de s si et seulement si pour i j ∈ σ, il n'existe pas l ∈ [i j−1 + 1.
Exemple 6 Toujours dans la continuité de l'exemple 1, supposons qu'après avoir tiré k = 2 items de la séquence s = (ab)(cd)(ce), à savoir les items aux positions d'index 1 et 5, nous ayons généré l'occurrence o = (a)∅(c) de signature 1, 3 de la sous-séquence s = (a)(c) de s. Dans ce cas, comme il existe l = 2 appartenant à [1 + 1.
, o n'est pas une première occurrence de s et cette occurrence sera rejetée.
Grâce à la propriété 1, il est finalement aisé de tirer uniformément une sous-séquence de norme k d'une séquence s. En tirant aléatoirement k positions distinctes entre 1 et s de s, on commence par tirer uniformément une occurrence de norme k d'une sous-séquence de s. Si cette occurrence est une première occurrence, on l'accepte et on la retourne. Sinon on la rejette et on effectue un tirage aléatoire d'une nouvelle occurrence de s. Même si cet algorithme repose sur une technique d'échantillonnage avec rejet, nous montrons dans la section suivante que le nombre moyen de tirages avant acceptation est calculable.
Analyse de la méthode
La propriété suivante indique que l'algorithme 1 retourne un échantillon exact des motifs séquentiels avec une contrainte sur la norme :
Propriété 2 (Correction) Soient une base de données séquentielles S, des normes minimale m et maximale M , l'algorithme 1 effectue le tirage d'une sous-séquence de S de norme comprise entre m et M et proportionnellement à sa fréquence.
Concernant la complexité, nous pouvons distinguer deux grandes phases dans notre approche : le pré-traitement (où la distribution des motifs séquentiels en fonction de la norme est calculée pour chaque séquence) et le tirage de sous-séquences. Complexité du pré-traitement Le pré-traitement s'avère coûteux avec une complexité tem- 
Lorsque le nombre moyen de tirages est proche de 1, cela signifie que le tirage d'un motif séquentiel ne donnera pas lieu à un rejet. Pour une séquence donnée, il n'y a pas de rejet si chaque occurrence est la première occurrence i.e., il n'y a pas de répétition au sein de la sé-quence. Dans la pratique, le nombre moyen de tirages mesuré sur des jeux de données réels est souvent très faible (voir la section expérimentale suivante). Finalement, la complexité temporelle du tirage d'une occurrence de norme égale à k ∈ [m..M ] d'une séquence s étant dans le pire des cas en O(M 2 ), la complexité en moyenne du tirage de N sous-séquences d'une base de données S (après la phase de pré-traitement) est en
Expérimentations
L'objectif de cette section expérimentale est d'évaluer la rapidité de notre méthode et d'observer l'impact de la contrainte sur les motifs extraits. Pour cela, nous avons utilisé six jeux de données. bms et sign sont des jeux de données réels disponibles avec SPMF 1 . Les quatre autres ont été construits avec le générateur de données de IBM également disponible sur le site Impact de la contrainte La figure 2 montre la répartition de 10 000 motifs séquentiels échantillonnés selon la fréquence avec une contrainte de norme inférieure à 4 ou 7 (en gris) et sans contrainte (en noir) pour les différents jeux de données. Dans tous les cas, la méthode sans contrainte retourne uniquement des motifs de fréquence très faible (en particulier de fré-quence unitaire sur les jeux de données réels). A l'inverse, la méthode d'échantillonnage avec contrainte sur la norme retourne des sous-séquences de fréquence significativement plus élevée FIG. 2: Répartition de 10 000 motifs séquentiels selon leur fréquence (de 100 à 1000 fois plus élevées), ce qui démontre l'importance d'introduire des contraintes sur la norme. Notons que pour sign, l'effet est peu significatif avec M = 7 mais la diminution de la norme maximale parvient à juguler l'explosion des motifs de fréquence peu élevée.
Conclusion
Cet article propose la première méthode pour échantillonner en sortie des motifs séquen-tiels. Elle permet en outre de spécifier un intervalle sur la norme des motifs séquentiels afin de mieux contrôler les motifs retournés. Nous avons démontré que notre approche est exacte et nous avons estimé son efficacité en fonction du nombre de rejets moyen qui se dégrade avec le nombre de répétitions au sein d'une séquence. Néanmoins, la partie expérimentale a montré que l'approche s'avère très performante sur des jeux de données réels où le taux de répéti-tion est très faible. De plus, les expérimentations montrent que l'ajout d'une contrainte sur la norme évite de retourner trop de motifs trop rares. Dans l'immédiat, nous voudrions appliquer l'échantillonnage de motifs séquentiels à la détection de données aberrantes (Giacometti et Soulet, 2016) ou au sein de systèmes interactifs (Giacometti et Soulet, 2017) pour démon-


Caractérisation du domaine
Ce domaine se caractérise en effet par une grande disparité des données techniques (données largement inconnues ou approchées, liées aux caractéristiques physiques des sols) et une forte propension pour la documentation du domaine à être d'ordre descriptif, le lecteur devant raisonner par analogie pour savoir si l'article sera pertinent pour son problème.
Objectif poursuivi
Trouver dans la base de données, les articles qui contiennent des phrases qui aideront l'utilisateur à maîtriser son thème de recherche.
Source des données
Les données d'origine sont extraites de corpus documentaires rassemblé par les ingénieurs du domaine. Les corpus ainsi constitués représentent plusieurs années des travaux d'une communauté transcrits dans des congrès annuels (AFTES ou AITES en l'occurrence).
Originalité de la démarche
Pour répondre à notre problématique, nous avons d'abord envisagé une ontologie de domaine formalisée en OWL, mais nous nous sommes heurtés à plusieurs difficultés :
-La multiplicité des spécialités géotechniques induit une multiplicité d'emplois lexicaux spécialisés et un coût de normalisation assez élevé. -Le raisonnement par catégories aristotéliciennes correspond au final assez peu au raisonnement usuellement employé par les experts du domaine, qui raisonnent plutôt par prototypes et proximité sémantique. -Il existe aussi dans la profession une résistance diffuse à l'utilisation d'un formalisme de représentation étranger au mode de pensée des experts, et plus encore à toute tentative de normaliser ces modes de pensée. Donc, le projet est construit sur une base purement lexicale et informelle, sans formalisme de représentation nécessitant spécification ou formalisation des données de départ.
Traitement des données
Les données lexicales sont extraites par OCR et traitement morphosyntaxique, permettant de lister les syntagmes signifiants du corpus. Ces syntagmes sont ensuite manuellement classés par les utilisateurs selon des relations simples (hyponymie/hypéronymie, voisinage) au sein de cartes. Ces cartes représentent généralement une thématique ou une spécialité. En général, elles n'excèdent pas 800 syntagmes et respectent quelques contraintes simples. Ces cartes ainsi constituées sont stockées dans une base de données, aident à l'interrogation et sont surtout un mode s'appropriation du système par les utilisateurs. Une analyse des textes permet de détecter les ensembles de phrases déductives de type ; si…alors, qui traduisent en général la connaissance des phénomènes.
Interrogation de la base de connaissances
L'utilisateur peut ensuite interroger la base selon plusieurs modes ; via les noms d'auteur ou via un ensemble de syntagmes constituant une. La recherche des phrases d'articles permet des comparaisons par ressemblance et précise l'origine des informations. De même, les articles peuvent être comparés entre eux et classés par degré de ressemblance.
L'avenir
Les essais ont été menés auprès d'experts du domaine en situation réelle fournissent des résultats complets et surtout moins coûteux en temps.
Il est également prévu d'intégrer une connaissance plus complexe dans le système, sous la forme du granule de connaissances, représentation semi-formelle de connaissances axiomatiques. L'objectif est d'en arriver à une forme de RTO informelle qui permette d'articuler les connaissances entre elles et de fournirdes réponse plus pertinentes.
Plusieurs familles de congrès (JNGG, ITA, AFTES) représentant plusieurs milliers d'articles ont été mis en base. Cette application test, ainsi qu'une bibliographie complète, est en ligne à :
www.pentes-tunnels.eu/MKD/MKD_en_ligne

Introduction
Les liens inter-langues (LILs) de Wikipédia permettent de naviguer facilement entre ses différentes éditions linguistiques et sont également exploités dans des applications de recherche d'information multilingue ; Sorg et Cimiano (2012)]. Cependant, les LILs sont ajoutés manuellement par les utilisateurs de Wikipédia et ainsi ils sont susceptibles d'être erronés (c.a.d. ils relient des articles qui ne décrivent pas un même concept).
Dans ce papier, nous proposons une approche pour l'élimination automatique des LILs. Les travaux existants s'attaquent principalement au problème de la détection des LILs manquants [Bennacer et al. (2015); Moreira et Moreira;Penta et al. (2012); Sorg et Cimiano (2008)]. Comme de Melo et Weikum l'ont fait remarquer, l'existence d'un chemin de LILs entre deux articles appartenant à une même édition linguistique (et, donc, décrivant deux concepts différents) révèle la présence d'un LIL erroné ]. Autrement dit, si deux articles provenant d'une même édition linguistique appartiennent à une même composante connexe, au moins un LIL de cette composante est erroné et la composante est dite incohérente.
Notre approche attribue un score de correction aux liens d'une composante incohérente et élimine des liens de façon itérative, à partir de ceux qui ont un score faible (c.a.d., susceptibles d'être erronés), jusqu'à diviser la composante en deux ou plusieurs composantes cohérentes. La contribution principale de ce papier est l'exploration de métriques obtenues de la topologie du graphe Wikipédia afin de calculer la probabilité qu'un LIL soit erroné.
La présentation du papier est organisée comme suit. Dans la section 2 nous présentons un aperçu de l'état de l'art. Nous introduisons, ensuite, dans la section 3, les notations et la terminologie permettant de décrire notre approche que nous présentons dans la section 4. Nous poursuivons, dans la section 5, par les expérimentations et les évaluations menées sur un sousgraphe de Wikipédia consistant en 8 éditions linguistiques. Enfin, nous concluons et présentons nos perspectives.
2 Aperçu de l'état de l'art Contrairement à l'identification des LILs manquants, qui a fait l'objet de nombreuses recherches [Bennacer et al. (2015); Moreira et Moreira;Penta et al. (2012); Sorg et Cimiano (2008)], peu de travaux ont porté sur l'élimination des LILs erronés.
De Melo et Weikum définissent un ensemble de critères (appelés assertions) pour identifier les LILs qui sont susceptibles d'être erronés dans une composante incohérente ]. Ces critères, qui, contrairement à notre approche, ne prennent pas en compte la topologie du graphe Wikipédia, sont utilisés par un programme linéaire qui divise la composante incohérente en deux ou plusieurs composantes cohérentes tout en minimisant le nombre de liens éliminés.
L'approche proposée par Rinser et ses collègues divise des composantes incohérentes faiblement connexes en plusieurs composantes cohérentes fortement connexes [Rinser et al. (2013)]. Leur évaluation ne montre pas si les liens éliminés sont effectivement ceux erronés.
Enfin, Bolikowski présente une étude intéressante qui montre que le graphe induit par les LILs de Wikipédia consiste en sous-graphes presque complets et la présence de liens entre ces sous-graphes est souvent un signe d'incohérence [Bolikowski (2009)]. Ce papier ne propose pas d'approche automatisée pour l'élimination des LILs erronés.
Terminologie
Nous modélisons Wikipédia comme un graphe orienté W = (P A, IL ∪ CL) : chaque noeud p α ∈ P A correspond à un article Wikipédia (identifié par un titre) dans une langue α ; un arc est soit un lien interne (p α , q α ) ∈ IL entre deux articles de la même édition linguistique, soit un LIL (p α , p β ) ∈ CL. Nous notons que les termes noeud et article sont synonymes dans ce contexte. Le graphe des liens inter-langues C = (P A, CL), obtenu de W en éliminant tous les liens internes, consiste en plusieurs composantes connexes ; une composante est dite incohérente si elle contient deux articles d'une même édition linguistique.
Notre approche
Notre approche identifie d'abord l'ensemble des composantes incohérentes en faisant une visite DFS du graphe C. Ensuite, chaque composante incohérente est divisée en deux ou plusieurs composantes cohérentes en éliminant des LILs de façon itérative. Pour ce faire, chaque lien d'une composante reçoit un score de correction γ qui mesure la probabilité qu'il soit correct ; l'élimination commence par les liens qui ont les scores plus faibles (susceptibles d'être erronés).
Pour le calcul du score de correction γ, nous utilisons la topologie du graphe C. Dès lors que les LILs sont ajoutés manuellement par des utilisateurs différents, la probabilité que deux articles appartenant à deux éditions linguistiques différentes aient un LIL erroné vers un même article est faible. Dans l'exemple de la figure 1, le lien entre es (article de l'édition espagnole) et en 1 (article de l'édition anglaise) est erroné (le lien correct porte vers l'article en 2 ) ; il est fort improbable que it (l'article correspondant à es dans l'édition italienne) ait lui aussi un lien incorrect vers en 1 . En d'autres termes, les LILs erronés sont souvent incidents à des noeuds qui sont périphériques dans leurs composantes, comme c'est le cas du noeud en 1 .  
où cl(v, w) est le nombre de chaînes de liens entre v et w.
Le score de correction. Le score de correction γ(l) d'un LIL l est obtenu en calculant une moyenne pondérée des scores présentés ci-dessus γ(l) = w 1 · β(l) + w 2 · α(l) + w 3 · ζ(l) + w 4 · ξ(l). Les valeurs des poids w i ( w i = 1) sont discutés dans la Section 5.
Expérimentations et évaluations des résultats
Nous avons évalué notre approche sur un sous-graphe de Wikipédia (stocké dans une base de données Neo4j) consistant en huit éditions linguistiques -anglaise, allemande, française, italienne, espagnole, grecque, néerlandaise, chinoise -qui datent de Décembre 2016. Le graphe a 28 539 306 noeuds, 346 165 183 liens internes et 24 033 912 LILs. Nous avons calculé le graphe des liens inter-langues C et sélectionné 400 composantes incohérentes où les LILs erronés ont été identifiés par les auteurs de ce papier. Les expérimentations ont été effectuées sur un ordinateur équipé de Windows 8, d'un processeur Intel Core i7, 8GB de mémoire et un disque SSD de 512 GB.
Résultats. Afin de régler les quatre poids du score de correction, nous avons appliqué notre approche sur un ensemble d'entraînement (240 composantes, respectivement 683 et 7 653 LILs erronés et corrects) et nous avons mesuré sa capacité d'éliminer des LILs erronés en calculant la précision (P = |V P |/(|V P |+|F P |)), le rappel (R = |V P |/(|V P |+|F N |)) et la F-mesure (F , moyenne harmonique de P et R). V P est l'ensemble des liens qui sont correctement considérés erronés par notre approche (vrais positifs) ; F P (liens incorrectement considérés erronés) et F N (liens incorrectement considérés corrects) sont respectivement les faux positifs et négatifs. A l'issue de la phase d'entraînement, les valeurs des poids qui donnent les meilleurs résultats sont les suivantes : w 1 = 0.4, w 2 = 0.6, w 3 = 0 et w 4 = 0.1. Nous remarquons que la métrique "Élimination minimale" entraîne une augmentation du rappel mais a un impact fortement négatif sur la précision, d'où la décision de mettre w 3 = 0. Nous avons appliqué notre méthode avec ces valeurs sur un ensemble de test (160 composantes, respectivement 399 et 4 207 LILs erronés et corrects) et nous avons obtenu P = 0.78, R = 0.83 et F = 0.80. La figure 2 montre que l'approche est plus efficace sur des composantes de petite taille qui constituent la majorité dans C.
En ce qui concerne les performances de l'approche, le calcul du graphe des LILs nécessite de 10 heures (visite DFS du graphe Wikipédia) ; le temps nécessaire pour compléter l'élimina-tion des LILs varie entre 10 et 15 secondes par composante quand l'approche n'utilise pas les chaînes de liens (sinon, il faut compter un temps variable de 1 à 2 minutes). 
Conclusions et perspectives
Dans ce papier nous avons présenté une approche pour l'élimination des liens inter-langues (LILs) erronés dans Wikipédia. La contribution principale de cette approche est l'exploration de métriques basées sur la topologie du graphe Wikipédia. Les résultats de notre évaluation sur un sous-graphe de Wikipédia consistant en 8 langues montre que l'approche est prometteuse. Nos travaux actuels portent sur l'étude de la topologie des composantes cohérentes qui peuvent contenir des LILs erronés et que, à notre connaissance, aucune approche considère. Nous souhaitons également intégrer des heuristiques qui exploitent d'autres éléments de Wikipédia tels

Introduction
Les développements rapides des réseaux sociaux durant la dernière décennie ont considé-rablement modifié la dynamique de recherche d'emploi, comme le décrivent Sivabalan et al. (2014). Dans le cadre du projet de recherche Butterfly Predictive Project 1 (BPP), nous faisons l'hypothèse que l'acquisition et l'exploitation des traces laissées par les individus sur les réseaux sociaux (LinkedIn, Viadeo, etc.) sont une voie intéressante afin de construire des ressources permettant d'aider au positionnement professionnel des candidats et ainsi faciliter leur mise en correspondance avec des emplois à pourvoir. Chaque jour, des milliers d'offres d'emploi sont diffusées sur Internet en même temps que des milliers de profils candidats sont créés ou modifiés sur les sites d'emploi ou les réseaux sociaux, aussitôt indexés par des méta-moteurs spécialisés ou des moteurs de recherche généralistes. Plusieurs organismes privés ou publics publient régulièrement des statistiques permettant de mesurer les tendances. On citera par exemple Keljob 2 , l'United States Department of Labor 3 ou Statistique Canada 4 . Il ne s'agit cependant que d'instantanés reflétant l'état du marché. Nous présentons dans cet article deux visualisations particulières du domaine, ainsi que les méthodes qui ont permis de les gé-nérer. La première visualisation est une carte dynamique permettant de savoir quels sont les métiers qui recrutent, dans quel domaine, quelle région ainsi que les compétences les plus fré-quemment demandées. Construite à partir de la collection de réseaux sociaux professionnels, la seconde visualisation met en avant les parcours professionnels et permet d'observer les perspectives ainsi que les antécédents à plus ou moins long terme pour chaque métier considéré. Nous souhaitons au travers de ces visualisations aider l'étudiant en recherche d'orientation, la personne en recherche d'emploi ou en réflexion sur un changement de carrière, en lui fournissant des éléments d'information sur les perspectives d'évolution dans le métier qu'il envisage. Il pourra aussi vérifier s'il est normal qu'il n'ait pas eu de promotion dans son emploi actuel. Dans la section suivante, nous présentons des travaux liés à notre étude puis, à la section 3, les visualisations obtenues, avant de donner quelques statistiques sur les données exploitées à la section 4. Nous décrivons les différents traitements ayant permis de réaliser ces deux visualisations à la section 5.1 et à la section 5.2.
Travaux connexes
La plupart des travaux dans le domaine de l'e-recrutement se sont principalement intéres-sés à la génération de ressources linguistiques au travers de représentations, généralement dans le but d'effectuer de l'appariement de candidatures par la suite. Les travaux de le Vrang et al. La seconde visualisation décrit les parcours professionnels. Développée également à l'aide de D3, celle-ci permet d'afficher les parcours professionnels sous forme d'arbres de transitions. Interfaçant avec une base de données de type NoSQL du côté serveur, l'application web permet à l'utilisateur de sélectionner le métier qui l'intéresse ou d'effectuer une recherche en saisissant un métier. La figure 2 présente le résultat obtenu pour le métier comptable en choisissant de présenter les résultats sous forme de chronologie. Chaque transition vers un autre poste est représentée par un arc dont l'épaisseur du trait est fonction de la fréquence de la transition. Les 8 transitions les plus fréquentes sont affichées de la plus fréquente à la moins fréquente, de haut en bas. En sélectionnant un métier, l'utilisateur affiche l'ensemble des transitions possibles depuis ce noeud avec la moyenne de temps nécessaire avant de passer au métier suivant. En survolant un métier avec la souris, l'infobulle indique différentes informations comme le pourcentage de profils sur l'ensemble des profils de la collection qui ont suivi cette transition, ainsi que la durée moyenne pour passer d'un poste à l'autre. L'utilisateur peut ainsi sélectionner le métier qui l'intéresse et consulter les carrières possibles pour ce métier, les étapes nécessaires pour y arriver ainsi que les possibilités d'évolution.
7. Les univers sont une modélisation des secteurs d'activité intégré au projet BPP et qui regroupe des familles de métiers assez proches (par exemple la banque et la finance sont regroupées dans un même univers).
FIG. 2 -Exemple de visualisation de parcours professionnel pour le métier de comptable.
Données et statistiques
Nous présentons dans cette section les données qui ont permis de réaliser les visualisations. Dans le cadre du projet BPP plus de dix millions de profils issus de plusieurs réseaux sociaux (LinkedIn, Viadeo, Indeed et d'autres) ont été récoltés à l'aide d'un processus de collecte automatique. Ces données issues de profils publics professionnels ont été préalablement anonymisées puis agrégées. L'origine géographique étant le Canada (2,7 M de profils) et la France (7,5 M de profils), les profils sont soit en français, soit en anglais ou encore bilingues. Chaque profil résume différentes informations sur le parcours du candidat telles que ses diplômes et ses formations et une section du profil rend compte de ses expériences. La collection contient en moyenne 2,75 expériences par profil. Chacune contient plusieurs éléments tels que les dates de début et de fin, le nom de la société employeur, la fonction occupée par le candidat au cours de cette expérience, le lieu et un éventuel descriptif de sa mission au sein de cette société. Chaque profil regroupe un grand nombre de champs, mais il existe cependant environ 12 % de profils vides et 33 % de profils contenant peu d'information. En complément de ces données, 300 000 offres d'emploi ont été collectées sur Internet. Ces offres d'emploi couvrent un grand nombre de métiers et sont issues, elles aussi, du Canada ou de la France. Chaque offre d'emploi contient un titre, une description contenant le détail de l'offre d'emploi, la date de mise en ligne, le lieu de l'emploi proposé ainsi que le nom de la compagnie qui recrute.
Méthodologie
Baromètre de l'emploi
Nous souhaitons au travers de cette visualisation offrir une vue globale des tendances du marché de l'emploi. Pour ce faire, nous avons considéré chacune des composantes d'une offre à savoir le nom de la compagnie, sa localisation géographique, la date de publication de l'offre, la fonction et les compétences requises pour cette fonction ainsi que le salaire. Compte tenu de l'importance de l'aspect géographique pour cette visualisation, nous nous sommes concentrés uniquement sur les offres d'emploi canadiennes écrites en anglais, issues de la collection pré-sentée en section 4. Identifier le nom de la compagnie et la date de publication d'une offre est relativement facile. Géolocaliser l'offre représente un niveau de difficulté accrue. L'extraction de la fonction ou encore des compétences dans le contenu textuel d'une offre d'emploi est une tâche complexe, comme le soulignent Kessler et al. (2008). Afin d'extraire la fonction depuis le titre des offres d'emploi, nous avons ainsi constitué une liste de noms de métiers normalisés regroupant sous un terme unique les différentes écritures pour chaque métier (féminin, pluriel, erreurs typographiques, etc.) classées selon leur fréquence d'apparition. À l'aide de règles, un processus normalise et compare les titres avec cette liste de métiers. Ce processus est décrit plus en détail dans Kessler et Lapalme (2017). L'extraction automatique de compétences ayant eu de faibles résultats (F-score de 0,42), nous avons opté pour une liste de 6000 compétences, extraite des travaux de Bastian et al. (2014). Nous identifions à l'aide de cette liste un grand nombre de compétences. Plusieurs prototypes ont aussi été développés afin d'extraire le salaire, cependant les premières observations ont montré que celui-ci n'était présent que dans 14% des offres d'emploi et qu'il pouvait être formulé de nombreuses façons (taux horaire, mensuel, bonus, etc.). Afin d'offrir une vision des salaires dans le baromètre, nous avons plutôt collecté les salaires annuels moyens sur le site Emploi Québec 8 .
Parcours professionnels
Divers choix ont été faits afin de pouvoir représenter les parcours professionnels. Sur les 10 millions de profils issus de cette collection, nous recensons 26 millions d'expériences dont 23 millions contiennent une date de début et une date de fin. Pour les besoins de ce travail, nous considérons qu'il y a une transition entre deux expériences lorsque la période entre la première expérience et la seconde est inférieure à 3 mois sans chevauchement entre les deux expériences de plus de 3 mois. Nous identifions par la suite les fonctions à l'aide du processus décrit en section 5.1, ainsi que les sociétés où se sont déroulées les expériences. Nous observons une grande variation d'écriture dans les noms de compagnies tels qu'on les trouve écrits dans les réseaux sociaux, ce qui a rendu la tâche plus complexe (ex. : Mac Donalds, Mac Do, etc.). Lorsqu'un utilisateur cherche un poste, nous trouvons tous les profils contenant ce poste, puis agrégeons les parcours de ces profils. Nous recensons ainsi pour ce poste les différentes expé-riences précédentes et suivantes. La dernière étape consiste à transformer les résultats obtenus en arbre de transitions tel que montré à la section 3. Le tout prend moins d'une seconde.
Conclusion et travaux futurs
Nous avons présenté deux interfaces originales de visualisation dans le domaine de l'erecrutement ainsi que les méthodes d'analyse de corpus pour les générer. Nous souhaitons au travers de ces outils proposer aux personnes en recherche d'emploi, en réflexion sur leur carrière, un tableau de bord synthétique et clair du marché ainsi que des perspectives professionnelles en fonction de chaque métier. Construite à partir d'offres d'emploi récoltées sur Internet, la première visualisation est une carte dynamique permettant de connaitre les tendances du marché de l'emploi. Nous prévoyons d'adapter la visualisation pour traiter les offres émanant de la France et pour agréger d'autres sites d'emploi afin d'affiner les statistiques. La seconde visualisation a été constituée à partir de profils issus de réseaux sociaux, et permet 8. http://imt.emploiquebec.gouv.qc.ca/
Summary
In this work, we describe the analysis methods used in the creation of two original visualization interfaces in the field of e-recruitment. These interfaces were designed using millions of user profiles gathered on social networks as well as thousands of job offers collected over the internet. This work also describes the necessary steps for the implementation of the interfaces. The first visualization is a dynamic map indicating which jobs are in-demand, in which field and in which region. The second visualization tool highlights career paths and allows the user to observe the perspectives as well as the antecedents for each job considered.

Introduction
L'activité des chercheurs a été bouleversée par un accès toujours plus important aux bibliothèques numériques en ligne. La recherche d'information dans ces bibliothèques numériques se fait le plus souvent au moyen de mots-clés entrés dans des moteurs de recherche. Néan-moins, l'appariement entre les mots-clés entrés et ceux utilisés pour décrire les documents scientifiques pertinents présents dans ces bibliothèques numériques peut s'avérer limité si la terminologie employée n'est pas la même dans les deux cas. Tout chercheur appartient à une communauté avec laquelle il partage des connaissances et un vocabulaire communs. Cependant, lorsque celui-ci souhaite étendre l'exploration bibliographique au-delà de sa communauté d'appartenance afin de recueillir des éléments d'information qui le conduisent à de nouvelles connaissances, il convient de lever plusieurs verrous scientifiques et techniques induits par la grande taille des bibliothèques numériques, l'hétérogénéité des données et la complexité du langage naturel. Les chercheurs qui travaillent dans un contexte pluri-et trans-disciplinaire doivent pouvoir accéder aux documents qui les intéressent sans pour autant être bloqués par la barrière d'un cloisonnement disciplinaire induit par une méconnaissance du vocabulaire employé par d'autres disciplines scientifiques. Le plus souvent, les réseaux sémantiques sont une bonne réponse aux problèmes de variations linguistiques en retrouvant des synonymes ou des champs lexicaux communs. Dans le domaine scientifique, toutefois, cette approche n'est pas suffisante car elle se heurte à la terminologie propre au jargon scientifique et technique qui, par nature, est très spécifique, et qui a la particularité d'évoluer très rapidement. Une autre solution pourrait être apportée par le plongement lexical (ou "word embedding"). Cette technique permet de retrouver des termes liés par une proximité au sein d'un même document et, de là, de déduire une proximité sémantique. Cette approche présente malgré tout les problèmes de ne pas donner d'information sur le nombre de termes dont il faut tenir compte pour être encore considéré comme sémantiquement proche du terme initial et de ne pas trop bien fonctionner quand il s'agit d'un concept composé de plusieurs termes plutôt que d'un seul et unique terme.
Dans cet article, nous proposons une solution combinant deux sources d'information sé-mantique : la première est issue de l'ensemble de synonymes déduits d'un réseau sémantique, la seconde provient de la représentation sémantique d'une projection vectorielle des articles.
État de l'art
La recherche de documents sémantiquement similaires n'est pas un problème nouveau en fouille de textes. Dans les bibliothèques numériques, les documents peuvent être enrichis par des méta-données qui permettent de les qualifier, les étiqueter et les classer. Ces enrichissements (tags, mots-clés ou catégories de sujets) manquent cependant d'une taxonomie standardisée et sont pénalisés par la subjectivité du jugement des personnes impliquées dans le processus d'annotation manuel (Abrizah et al., 2013).
Dans ces bibliothèques numériques, pour parvenir à atteindre des documents sémantique-ment liés à des documents ou des mots-clés fournis en entrée, l'emploi de sources d'extension sémantique nous semble être une piste incontournable. Une première solution consiste à utiliser des bases de données lexicales comme WordNet (Miller, 1995) ou des bases de connaissances telles que BabelNet (Navigli et Ponzetto, 2012), DBpedia (Lehmann et al., 2015) ou YAGO (Mahdisoltani et al., 2015).
Une autre solution consiste à utiliser des techniques de plongement lexical (Bojanowski et al., 2017) pour trouver des terminologies sémantiquement similaires. Malgré l'avantage de ces techniques, celles-ci ne fournissent pas de critère permettant de définir précisément une proximité et ainsi de concevoir qu'un terme proche dans la projection puisse être encore considéré comme étant sémantiquement proche du terme initial. Les modèles thématiques, tels que l'allocation de Dirichlet latente, ou LDA (Blei et al., 2003), ainsi qu'une version supervisée de LDA (Ramage et al., 2009), sont parmi ceux qui semblent être les plus appropriés pour résoudre le problème qui nous intéresse. Cependant, dans le cas d'une application réelle à des millions de documents, telle qu'une bibliothèque numérique comportant des collections d'articles scientifiques touchant de nombreuses disciplines, et cela sur un grand nombre d'années, même les approches évolutives récentes demandent l'utilisation de puissances de calcul vraiment conséquentes, comme l'emploi d'une ferme de calcul (computer cluster) (Liang et al., 2015).  (Halko et al., 2011), sur la matrice de sac de bi-grammes et uni-grammes de mots). Seuls les mots, non vides, ayant une fréquences d'au moins 20 apparitions sont considérés. Nous construisons ensuite un modèle de classement pour chaque sujet avec les forêts aléatoires. Cette méthode s'appuie sur un ensemble d'entraînement sur des ensembles d'exemples positifs et négatifs de même taille. Les exemples positifs sont extraits du corpus ISTEX avec Elasticsearch et les exemples néga-tifs sont retournés de façon aléatoire. Tous les articles du corpus sont ainsi ordonnés suivant leur probabilité d'appartenir au sujet recherché en une liste qui est ensuite tronquée (100 000 premiers) pour donner la sortie de RSPV. Méthode d'ensemble de synonymes : « Synset ». La méthode d'attribution d'étiquette Synset s'appuie sur une banque de synonymes, telle que BabelNet, pouvant être utilisée à la fois comme un dictionnaire encyclopédique, un réseau sémantique ou une base de connaissances. À partir d'une étiquette issue de termes de référence de la base Web of Science, nous composons un groupe de mots synonymes « synset », ou « synonym set », ayant une équivalence 1. http://www.istex.fr/ 2. https://images.webofknowledge.com/images/help/WOS/hp_subject_category_terms_tasca.html sémantique. Une requête recherchant l'étiquette et tous ses synonymes est ensuite lancée dans le moteur de recherche d'ISTEX sur les méta-données des articles. Cette requête exécutée, nous obtenons une liste d'articles ordonnés par pertinence que nous appellons « liste synset ». Méthode de Auto-Etiquetage Thématique de Texte basé sur la Sémantique : « AE2TS ». Notre méthode AE2TS est une combinaison des deux méthodes décrites précédemment, soit une fusion des résultats des listes synset et RSPV. Nous appliquons la moyenne des rangs attribués à un article donné dans RSPV et synset, puis nous ré-ordonnons les articles avec cette nouvelle valeur de rang moyen, ce qui peut s'exprimer comme suit : soit s A le rang attribué à l'article A par la méthode synset et r A le rang attribué à A par la méthode RSPV, la valeur t A utilisée pour réaliser l'ordre des listes fusionnées sera t A = s A +r A 2 . Lorsque la méthode synset n'attribue aucun rang à un article A, nous appliquons la formule suivante : t A = r A × |S|, où S est l'ensemble des résultats donnés par la méthode synset, et |S| le nombre de résultats de cette liste. Notons que nous restreignons volontairement la liste des résultats de la méthode AE2TS à au plus le double de la taille des résultats obtenus avec la méthode synset, la liste synset ayant un nombre de résultats plus petit que celui de la liste RSPV. Les résultats obtenus par chacune des trois méthodes selon le protocole décrit précédem-ment sont présentés dans le Tableau 1. La qualité des résultats de chaque méthode est évaluée au moyen du rappel. Notons que pour savoir si une réponse est correcte ou non pour un article donné, il faudrait avoir une évaluation humaine experte dans tous les domaines, ce qui n'est pas envisageable. Nous avons ainsi utilisé pour nos expérimentations un petit jeu de test déjà étiqueté (au minimum 100 articles par sujet). En raison du petit nombre d'articles présents dans cet ensemble de test, les valeurs de rappel sont globalement faibles pour les trois méthodes.
Résultats et discussion
La combinaison des deux approches dans la méthode AE2TS est celle qui fournit les meilleurs taux de rappel (15,82%) pour le plus grand nombre de sujets testés (24/33). Les quelques cas où la seule méthode d'utilisation du réseau sémantique (Synset) dépasse les deux autres ne concerne que des sujets pour lesquels le concept est constitué d'un seul terme (comme « psychologie »). Les concepts issus de plusieurs termes (comme « intelligence artificielle ») semblent mieux retrouvés pour les deux autres méthodes que pour la méthode classique Synset, et tout particulièrement pour la méthode AE2TS. Les résultats présentés ici, bien que devant être confortés par d'autres expériences, sont déjà encourageants et confirment l'intérêt de l'apport d'une représentation sémantique issue d'une projection vectorielle pour pouvoir auto-étiqueter des documents scientifiques avec des étiquettes composées d'un ou de plusieurs termes.
Conclusion et perspectives
Dans ce travail, nous avons étudié trois méthodes permettant d'attribuer sémantiquement des étiquettes de sujets scientifiques aux articles d'un corpus. Ces étiquettes sont issues d'une taxonomie de la collection Web of Science. Or les bibliothèques numériques multidisciplinaires combinent des corpus provenant de nombreux éditeurs scientifiques utilisant chacun leur propre taxonomie. Ce phénomène freine l'accès de certains articles à des chercheurs d'autres disciplines par leur emploi d'une terminologie et d'une taxonomie différentes. En enrichissant la bibliothèque numérique avec plus de balises obtenues à travers la méthode d'auto-étiquetage thématique de textes scientifiques AE2TS que nous proposons, la taxonomie et les balises éten-dront l'exploration de la recherche à davantage d'articles sémantiquement pertinents. L'approche AE2TS combine deux sources d'information sémantique (synonymes issus d'un réseau sémantique et résultats de la représentation sémantique d'une projection vectorielle). Notre étude expérimentale montre une amélioration significative en terme de rappel par rapport aux résultats obtenus en utilisant seulement les synonymes de sujets extraits des réseaux sémantiques. Ajoutons que lorsqu'une requête est menée sur un mode exploratoire dans une bibliothèque numérique scientifique, il est difficile de connaître directement les termes exacts de la thématique des documents recherchés. La requête sera donc le plus souvent une périphrase composée de plusieurs termes, situation où la méthode AE2TS retourne les meilleurs résultats.



Introduction
L'information se propage. Lorsqu'elle est reçue, une information est ingérée, nuancée, et reformulée pour être à nouveau transmise. Cette propagation se déroule à tous les niveaux de communication : lors d'une conversation, à la radio, à la télévision, mais aussi lorsque nous publions du contenu, par exemple sur Internet. Les documents que nous partageons, contiennent de multiples informations provenant d'autres documents, et qui seront, en partie, reprises dans le futur. Ainsi les informations présentes dans un document ont une histoire. Ce sont des suites d'événements de propagations qui les ont conduites à être présentes dans ce document. Nous appelons l'ensemble de ces lignées, pour chaque document, la Trajectoire de l'information.
Lorsqu'une information se propage, elle est sujette à des modifications, dans sa forme ou dans son fond. Certains travaux se sont intéressés à la traque de ces changements, comme Leskovec et al. (2009). Cependant, après de nombreuses mutations, il peut être difficile de trouver le lien entre l'information de départ et l'information actuelle, comme le soulignent des travaux cherchant à retrouver les sources d'une information, par exemple Farajtabar et al. (2015).
Nous proposons d'estimer la Trajectoire de l'information en calculant des chaînes de documents textuels le long desquelles il est plausible que de l'information se soit propagée. Pour ce faire nous n'explicitons pas l'information qui circule le long de la chaîne, mais nous inté-ressons à la manière dont se comportent les documents entre eux au sein de la chaîne. Aborder 
Extraction des chaînes de propagation
Le contexte du problème est le suivant : nous analysons un ensemble de documents textuels (un corpus) dont nous connaissons certaines méta-données, comme la date de publication, les auteurs, etc. Notre première hypothèse est l'existence d'un phénomène de propagation de l'information : durant le processus de création des documents, les auteurs récupèrent, interprètent et reformulent différentes informations issues de documents antérieurs du corpus (ou d'ailleurs). Notre seconde hypothèse est qu'une information qui a muté garde un lien séman-tique fort avec l'information dont elle dérive.
On appelle chaîne de propagation une chaîne de documents le long de laquelle au moins une information s'est propagée au sens évoqué ci-dessus. Nous appelons trajectoire un ensemble de chaînes de documents. On dit qu'une chaîne de documents est une chaîne de propagation plausible si des évaluateurs humains s'accordent pour dire qu'il a pu y avoir une propagation d'information le long de cette chaîne. Un exemple de chaîne de propagation plausible est donné en Fig. 1. Une trajectoire n'est un graphe sur les documents. La Fig 2 montre deux trajectoires différentes utilisant les mêmes arêtes. Notre objectif est le suivant : calculer une trajectoire contenant le plus de chaînes de propagation plausibles, c'est-à-dire cohérentes, et le moins de chaînes non plausibles.
Notre approche consiste à parcourir toutes les chaînes possibles et à sélectionner celles qui satisfont un certain critère de cohérence. Toutes les chaînes ne sont pas possibles, en particulier elles doivent satisfaire deux propriétés. La première est une propriété de croissance : une chaîne ABCD est une chaîne de propagation à condition que CD le soit aussi. Sinon, cela veut dire qu'une information circule le long de ABCD sans qu'aucune ne circule le long de CD. Ainsi, si CD ne satisfait pas notre critère de cohérence, nous n'explorons pas les chaînes qui passent par CD. La seconde exploite la date de publication des documents. Une information se propage toujours du document le plus ancien vers le document le plus récent.
Nous procédons de la manière suivante : nous calculons pour chaque document D les chaînes qui finissent en D, que nous notons F inishIn(D). Pour cela, nous calculons l'ensemble des chaînes candidates pour D, que nous notons Candidates(D). Les chaînes candidates pour D sont toutes les chaînes formées de documents publiés avant D. Étant donnée notre propriété de croissance, nous parcourons les chaînes qui finissent en C à la condition que la chaîne CD satisfasse notre critère de cohérence. Une fois tous les candidats accumulés, les chaînes qui finissent en D sont le résultat de notre stratégie de sélection select. La trajectoire calculée T est l'union de toutes les chaînes calculées. Le pseudo-code de l'algorithme est donné en Algorithme 1.
Data : un corpus de document Corpus, une stratégie de sélection select Result : T l'ensemble des chaînes calculées
Notre stratégie de sélection est la suivante : nous définissons la mesure d'attachement d'un document à une chaîne comme une mesure de la vraisemblance de l'ajout du document à la fin de la chaîne. Dans nos expériences, nous avons donné à l'attachement du document D à la chaîne ABC la forme suivante : attach(D, ABC) = F (sim(A, D), sim(B, D), sim(C, D)) où sim est une fonction de similarité sémantique entre documents. F peut être une fonction simple comme le minimum ou une moyenne. Nous sélectionnons les k chaînes maximales selon l'attachement pour le document actuel avec comme contrainte que l'attachement doit être supérieur à un seuil de cohérence.
Expérimentations
L'estimation de la Trajectoire comme nous le proposons étant un problème neuf à notre connaissance, nous nous sommes tournés vers l'évaluation humaine de manière à construire des jeux de données annotés. Nous avons pris deux jeux de données anglophones. Le premier est le Citation Network Dataset V1 d'AMINER 1 construit par Tang et al. (2008). Il est composé de résumés de papiers scientifiques extraits de collections comme ACM et DBLP. Notre second jeu de données correspond à l'ensemble des articles du Huffington Post US sur la pé-riode du 1 er juillet au 30 novembre 2016. Les jeux contiennent respectivement 629 814 et 49 648 documents.
Nous avons créé deux jeux de données dérivés contenant moins de documents pour avoir un nombre de chaînes à évaluer raisonnable. Nous avons choisi de sélectionner 150 résumés au hasard pour les deux corpus. Cependant pour le Huffington Post, nous avons été plus pré-cis. Nous avons enlevé les articles contenant le mot "Trump" très représenté dans le jeu (> 11 000 documents). Nous n'avons gardé que les articles entre 100 et 3000 signes pour ne pas perdre l'attention de l'évaluateur dans des articles trop longs à lire ou qui présentent trop peu de contexte. Nous avons créé nos trajectoires à partir d'une similarité cosinus sur les vecteurs TFIDF des documents. Nous avons construit six trajectoires en faisant varier la mesure d'attachement F d'une part (la moyenne arithmétique ou le minimum) et le seuil d'admissibilité d'autre part (parmi les valeurs 0,1 ou 0,2 ou 0,5). Nous avons réuni ces trajectoires pour chaque jeu de données. Cela nous donne deux ensembles de chaînes à évaluer.
Dans le cas d'une évaluation humaine, l'expertise des évaluateurs entre en jeu. Nous avons demandé à quatre chercheurs en informatique d'annoter les chaînes que nous avons calculées. Ils sont habitués à lire des documents tels que ceux d'AMINER. Les articles du Huffington Post sont destinés à un lectorat étendu et nous n'avons pas remis en cause la capacité de nos participants à les comprendre et à les mettre en contexte. La démarche de l'évaluateur est la suivante : d'abord, l'évaluateur doit prendre connaissance du contexte de la chaîne. Ensuite, il lit le premier document de la chaîne (le plus ancien). Puis, chacun des documents suivant lui est proposé en succession. À partir de là, il doit pour chaque déterminer s'il y a un lien sémantique fort ou faible avec le document précédent et s'il est fortement/faiblement/non plausible que de l'information se soit propagée du premier document jusqu'à celui-ci.
Nous donnons dans la Tab. 1a le ratio d'accord des participants pour l'évaluation des liens directs et celle de l'attachement pour les chaînes d'au moins trois documents. Nous séparons les résultats en deux, selon qu'on considère l'intensité du lien ou juste son existence. Pour les liens directs, les évaluateurs sont d'accord dans au moins 70 % des cas sur les deux jeux de données et dans au moins 80 % des cas (sauf pour l'intensité sur AMINER) pour l'attachement. Cela renforce l'intuition que l'évaluation est plus facile quand le contexte est plus riche. Ces deux résultats montrent que les humains arrivent à évaluer la cohérence des chaînes de documents avec consistance. Ceci nous conforte dans l'idée que le problème que nous traitons est bien posé.
(a) Accord inter-évaluateurs Nous choisissons de répartir nos évaluations en cinq catégories selon l'accord des évalua-teurs. La majorité a jugé qu'il y avait : un lien fort (Catégorie 1), un lien faible (Catégorie 2), un lien sans trancher sur son intensité (Catégorie 3), une absence de lien (Catégorie 4). Il y a une catégorie 5 qui est le cas ou la majorité n'est pas atteinte. La répartition est donnée en Tab. 1b. Nous remarquons que les résultats sont très bons pour AMINER avec seulement 9 % de non-attachement. A contrario, les chaînes sur le HuffPost sont globalement mauvaises à la fois pour le lien direct (64 %) et pour les attachements (75 %). Pour comprendre ce résultat, nous devons nous rappeler comment a été créé l'ensemble de chaînes que nous évaluons. Il s'agit de l'union de plusieurs trajectoires, parmi lesquelles deux trajectoires calculées avec un seuil d'admissibilité de 0,1. Nous montrons plus loin que les mauvaises chaînes proviennent . Nous considérons aussi une similarité calculée par marche aléatoire avec retour, dévelop-pée par Shahaf et Guestrin (2010), que nous nommons RWR, paramétrée avec une probabilité de retour de 99 %. Toutes les similarités sont entraînées sur l'intégralité des documents des corpus créés. Nous définissons une mesure d'attachement par moyenne arithmétique pour chacune de ces mesures. Pour chaque catégorie de chaînes annotées, nous calculons la moyenne et l'écart-type de l'attachement présenté sous forme d'intervalles en Fig. 3. Nous remarquons que les trois mesures attribuent un meilleur score aux chaînes jugées liées qu'aux chaînes jugées non liées. En particulier Doc2Vec semble être la mesure qui dissocie le mieux les chaînes fortement liées des chaînes non liées. Ceci montre qu'il est possible de capturer au moins en partie le jugement humain sur les chaînes avec des mesures bien connues. Si l'évaluation humaine montrait que la tâche est réalisable par des experts, celle-ci renforce notre intuition que la tâche est également réalisable par une machine.
Conclusion
Calculer des approximations de la Trajectoire est un problème encore ouvert. Nous avons proposé un cadre pour le formuler ainsi qu'une approche gloutonne qui calcule des chaînes de proche en proche. Dans le but de qualifier ces chaînes, nous avons mené une campagne d'évaluation humaine. Le bénéfice a été double : d'une part, nous avons vu que les évaluations humaines étaient consistantes entre elles, ce qui nous conforte dans l'idée que le problème est bien posé puisque la tâche est réalisable par l'humain. D'autre part, nous nous sommes servi de ces évaluations comme d'une vérité terrain pour tester différents critères de cohérence. Nous avons vu que ces critères réussissent à capturer les jugements humains. Nous interprétons ce résultat comme une première preuve que la tâche est aussi réalisable de manière automatique.
Plusieurs axes d'améliorations sont envisagés. En particulier, Nous comptons former un critère de cohérence plus performant encore. Pour cela, nous prévoyons une nouvelle campagne d'évaluation avec un nombre plus élevé de participants, ce qui aura aussi pour effet de consolider ou nuancer nos premiers résultats. Nous souhaitons également chercher de nouvelles façons de créer nos chaînes, par exemple en utilisant des méthodes probabilistes qui tireraient un ensemble de chaînes dont la cohérence serait élevée.
Une fois des trajectoires fiables calculées automatiquement, nous pouvons explorer leur utilisation dans plusieurs cas d'exploitation. Le but de la trajectoire est d'isoler les chaînes de propagation, aussi une volonté naturelle serait d'extraire les informations qui se propagent le long de chaque chaîne, mais aussi étudier la manière dont ses informations interagissent entre elles le long des chaînes. On peut aussi plonger les chaînes dans l'espace des auteurs afin d'étudier la manière dont ces derniers relaient l'information. Enfin, nous nous posons la question de la synthèse et de la visualisation des chaînes elles-mêmes. Cela peut être la constitution d'un résumé de la propagation de l'information, une piste prometteuse en ce sens réside dans les travaux menés par Shahaf et al. (2013).

Introduction
Pour la sûreté et l'opérabilité des trains, l'infrastructure des réseaux ferroviaires est massivement surveillée, en temps réel, par des opérateurs de contrôle de trafic. Les données sont enregistrées et analysées pour améliorer leur fiabilité (Rosenberger et Pointner, 2015). Les sorties principales du système de surveillance sont les détections de pannes et leur diagnostic. En outre, les informations sur les contrôles, réparations et révisions sont parfois enregistrées, ce qui facilite l'extraction d'une information utile à partir des données.
Cet article propose une méthode d'extraction de connaissances pour une telle situation. Nous travaillons avec des données composées d'événements enregistrés automatiquement dans le système : passage de trains, mouvements d'aiguillages, défaillances d'équipement, etc. Il n'y a pas de vérité terrain sur les causes de défaillance, ni d'information sur les réparations. Nous savons aussi qu'une partie des défaillances enregistrées ne résultent pas d'une défaillance physique des dispositifs, mais plutôt d'une absence de réponse, possiblement provoquée par des causes externes indépendantes.
L'étude se concentre sur un sous-ensemble des dispositifs : les compteurs d'essieux. Pour extraire une connaissance sur les défaillances à partir des données, nous proposons une procédure en deux étapes : d'abord, une détection et un filtrage des rapports de défaillance qui semblent provenir de conditions externes ; puis, à partir des relations entre événements consé-cutifs, une classification des défaillances par effets et fréquence. Cette procédure est une étape préliminaire pour une possible analyse de fiabilité (Schroeder et Gibson, 2007).
Cet article est structuré de la façon suivante. Dans la section 2, les compteurs d'essieux et le jeu de données utilisé sont décrits, suivis par les étapes de prétraitement des données. Dans la section 3, le modèle de traitement est présenté. Enfin, nous présentons nos résultats et conclusions. Notons que, dans cet article, la terminologie utilisée pour le domaine de la maintenance est cohérente avec celle du European Committee for Standardization (2010).
Compteurs d'essieux et jeu de données 2.1 Compteurs d'essieux
Un compteur d'essieux est composé de bobines inductives placées le long des voies et d'un estimateur, connectés électriquement. Quand l'essieu d'un train passe, il perturbe le champ électromagnétique entre les bobines, conduisant à un changement de tension dans le circuit, mesuré par l'estimateur et comparé avec un seuil prédéfini. Le passage du seuil est signalé au système et enregistré comme un passage de train (Rosenberger (2011), Wei et al. (2010 
Description des logs
Les données prennent la forme de logs journaliers enregistrés par le système de supervision ferroviaire. Ils proviennent d'une station polonaise, sur 11 mois, du 1er janvier au 17 novembre 2015. Nous en avons extrait des enregistrements provenant de 79 compteurs d'essieux, qui ont signalé 692 défaillances au total.
Nous ne disposons pas d'un historique des données et ne pouvons donc pas observer la vie complète des équipements. Les systèmes de surveillance enregistrent les événements dans des disques locaux de taille limitée. Quand de nouvelles données arrivent et que le disque est plein, les plus anciennes sont effacées. La période d'enregistrement ne correspond donc pas au cycle de vie des équipements ou au plan de maintenance, rendant une estimation directe de la distribution du temps de vie résiduel impossible (par exemple via l'estimateur Kaplan-Meier (Hosmer et al., 2008)).
Les maintenances, informations constructeur et autres données techniques ne sont pas pré-sentes. Nous ne connaissons donc pas les causes et solutions au problèmes signalés. Seuls sont enregistrés les défaillances détectées et le passage des trains, sous la forme de messages occupé ou libre, couplés à l'identifiant d'équipement et l'horodatage de l'événement.
Prétraitement des données
Parfois, plusieurs équipements signalent des défaillances dans un temps court (typiquement 30 minutes). Selon les experts, de tels rapports quasi-simultanés sur différents équipements ne sont pas des défaillances physiques, et doivent provenir d'événements externes : redémarrages systèmes, phénomènes naturels, coupures de courant, etc. nous parlerons ici d'interruptions de service. Des groupes de défaillances sont ici considérés comme interruptions de service si au moins 4 équipements sont affectés dans un intervalle de 30 minutes, et ils sont alors retirés des données avant analyse.
Introduction au modèle de traitement
Comme expliqué précédemment, les données analysées sont non-étiquetées, et il est difficile de construire explicitement un modèle du problème, ce qui nous a dirigé vers des techniques d'apprentissage non-supervisé. De nombreuses techniques de traitement peuvent être utilisées pour ce type de problème : recherche de règles d'association, partitionnement de données, cartes auto-adaptatives, réduction de dimensionnalité (par ex. analyse en composantes principales) (James et al., 2009).
Nous avons choisi une analyse de partitionnement des données qui offre une très bonne mé-thode de représentation de plusieurs modes de défaillance. En outre, notre base de défaillances est de taille modeste et sa représentation vectorielle est de faible dimension, ce qui rend la réduction de dimensionnalité peu utile.
La méthode de partitionnement choisie est le clustering hiérarchique, dans lequel une heuristique permet de décider du nombre de groupes à retenir à partir de l'analyse du dendrogramme généré (James et al., 2014, chapter 4). Les regroupements en clusters sont effectués à l'aide de la distance euclidienne avec une approche complete linkage pour les calculs de distances entre groupes 1 .
1. Les calculs ont été réalisés à partir de la bibliothèque SciPy (Jones et al.,01 ).
Représentation vectorielle des défaillances
Nous souhaitons différencier les types de défaillances selon la fréquence à laquelle elles apparaissent et leur effet sur le fonctionnement, mesuré par la période d'inopérance qu'elles induisent. Les variables suivantes ont été choisies pour décrire les défaillances : -t last : durée depuis la dernière défaillance (en secondes) ; -t next : durée avant la prochaine défaillance (en secondes) ; -op last : nombre de cycles d'opération depuis la dernière défaillance ; -op next : nombre de cycles d'opération avant la prochaine défaillance ; -t of f : durée de fonctionnement incorrect depuis la dernière défaillance (en secondes). La variable t of f est associée à la perturbation introduite par la défaillance dans le système et la sévérité de celle-ci. En particulier, si la défaillance a été résolue par un redémarrage du compteur, le temps avant que l'équipement soit à nouveau opérationnel devrait rester court. Cette variable est critique pour déterminer les effets des défaillances sur l'infrastructure.
Rappelons que nous ne savons pas si une indisponibilité longue de l'équipement est due à une réelle défaillance ou si, pour une raison quelconque, l'équipe de maintenance a pris un temps inhabituel pour atteindre et réparer l'équipement, ou même si l'équipement a été réparé rapidement mais n'a pas pu être utilisé par la suite.
Les première et dernière défaillances d'un enregistrement, pour lesquelles t last ou t next n'est pas connu, ont été incluses dans l'analyse si une période suffisante (fixée à deux semaines) existe entre le début des enregistrements et la défaillance. C'est cette durée qui est alors utilisée dans les calculs.
La plupart des variables a une distribution dense sur les valeurs faibles, le reste étant plus épars, ce qui montre que la plupart des événements a lieu dans des intervalles de temps plutôt courts. Pour certains événements, ces intervalles peuvent aller jusqu'à plusieurs jours.
Il y a une corrélation nette entre les variables t next et op next (et entre t last et op last ). Intuitivement, si le trafic est réparti de façon homogène à l'échelle d'une année, la relation entre les deux variables doit être quasi-linéaire sur un compteur. Ceci correspond globalement à nos observations, mais un certain nombre d'événements s'éloignent de cette relation.
Pour le partitionnement des données, chaque variable a été normalisée dans [0, 1] par division par son maximum global : les variables ont des unités différentes, et il n'est pas pertinent de les comparer directement. De plus, puisque l'écart entre les opérations est d'au plus quelques minutes, les variables temporelles à valeurs élevées auraient rendu insignifiant l'impact des autres variables.
Une fois le partitionnement effectué, nous caractérisons les clusters sur la base des distributions de variables présentées ci-dessus. Avec cinq variables, il est possible d'étudier les distributions manuellement, afin d'interpréter opérationnellement les résultats algorithmiques.
Résultats
À partir de l'analyse du dendrogramme, une coupure permettant d'obtenir 3 clusters a été choisie. Au total, 487 signalements ont été identifiés comme résultant d'un événement externe (section 2.2). Par ailleurs, 5 signalements ont eu lieu trop près du début ou de la fin des mesures pour que toutes les valeurs des variables puissent être fournies (section 3.1). Il y a 42 défaillances dans le cluster 1, 136 dans le cluster 2 et 22 dans le cluster 3.
FIG. 1 -Distribution des variables de partitionnement entre les trois clusters.
La Fig. 1 présente les distributions lissées des variables introduites dans la section 3.1 pour chaque cluster. Dans le cluster 1, un grand nombre de défaillances ont de hautes valeurs de t next et opnext. Les valeurs de t last and op last sont aussi généralement plus hautes que dans le cluster 2. Nous pouvons interpréter ces défaillances comme aléatoires et ne résultant pas de la tendance de l'équipement à tomber en panne, mais plutôt comme des défaillances exceptionnelles. Dans le cluster 2, les variables t last et t next ont des valeurs plutôt basses. Ces défaillances sont séparées entre elles par des durées courtes. Elles peuvent être interprétées comme systématiques, le compteur d'essieux tombant en panne de façon répétée. Le cluster 3 contient des défaillances à haut t last , et t next est légèrement plus haut que dans le cluster 2. Ces défaillances sont interprétées comme rares ou aléatoires, comme dans le cluster 1.
Les valeurs de t of f ne sont pas spécifiques à un cluster. Bien que le cluster 2 présente des valeurs plutôt plus hautes, la tendance est peu claire. Ce résultat est légèrement décevant, dans la mesure où nous nous attendions à ce que t of f permette de séparer les défaillances à conséquences sérieuses de celles qui n'ont eu que peu d'impact sur le trafic. Ainsi, le cluster dans lequel les valeurs de t of f auraient été en général plus hautes et les autres variables plus basses aurait contenu les défaillances les plus sérieuses : fréquentes, et à conséquences fortes.
Conclusion
L'étude présentée dans cet article est la première étape d'une analyse de fiabilité d'équi-pements de trafic ferroviaire en présence de données limitées et incomplètes. Les étapes pré-sentées fournissent une façon d'appréhender la validation des données et la construction d'un système de classification pour les défaillances, en fonction des spécificités du domaine. Bien que le problème soit loin d'être résolu, nous avons obtenus quelques résultats positifs.
L'approche par clustering hiérarchique a mis en évidence trois groupes dans les données, avec des caractéristiques différentes. Bien que n'expliquant pas complètement les modes de défaillances, les résultats sont prometteurs.
En perspective, on peut remarquer que certaines améliorations seraient intéressantes à étu-dier dans le but de proposer d'autres types de clusters, par exemple, l'ajout de nouvelles variables. Actuellement, l'espace décrit dans la section 3.1 est fortement hétérogène car la plupart des points ont des valeurs très basses pour toutes les variables, alors que quelques uns ont des valeurs élevées et proches des maximums. Par ailleurs, l'utilisation d'autres techniques de clustering de données sera étudiée dans des travaux futurs car elle permettrait de mettre en avant une caractérisation des défaillances selon d'autres angles de représentation.
Summary
This paper proposes an approach to analyze operation records of axle counters, a core part of railway infrastructure. Our aim is to introduce an efficient way to automatically extract knowledge regarding failures of such devices.
As the data provided does not contain a ground truth regarding causes of failures, failure information and their causes should be extracted from underlying relations between recorded events. After a data pre-processing step, the recorded events are clustered with respect to the relationships that can be highlighted among them. As a result, classes of events can be highlighted, from which a classification system can be proposed.
Beyond this specific application, the approach is a novel way to tackle reliability analysis problems.

Introduction
Dans de nombreux domaines d'application tel que le diagnostic, la santé ou le marketing, les praticiens s'intéressent aux événements qui sont corrélés à des événements indésirables ou qui les déclenchent. Souvent, l'occurrence de la situation indésirable s'explique par la présence d'une action spécifique, mais aussi par l'absence de certains événements (Cao et al., 2016). Par exemple, dans le cadre du marketing, si un client de supermarché n'a pas reçu de promotions depuis longtemps, il a une très forte probabilité de choisir la concurrence, alors que dans le cas contraire il resterait fidèle à son enseigne. Pour réagir au mieux, il est important de découvrir les événements ainsi que leurs contextes d'occurrence ou d'absence afin de déterminer la meilleure action à exécuter pour éviter la situation indésirable, comme l'attrition en marketing. De plus, il est important de connaître les caractéristiques temporelles des situations indésirables, i.e. de quelle manière il faut anticiper l'occurrence de ces situations. Par exemple, ne pas envoyer d'offre promotionnelle dans les trois jours précédant le jour habituel de courses d'un client peut conduire ce client à acheter moins de produits, mais lui envoyer une telle offre trop tôt peut n'avoir aucun effet. Une information temporelle, comme les délais acceptables, peut améliorer la précision des prédictions et de la recommandation d'action auprès des décisionnaires.
Notre but est d'extraire d'une base de séquence d'événements datés des motifs temporels indiquant l'absence de certains événements appelés événements négatifs (Cao et al., 2016). Ainsi, le motif négatif p = a ¬c b indique que a est suivi fréquemment de b sans la présence de c entre eux. De plus, nous souhaitons traiter la dimension temporelle, i.e. savoir quel laps de temps s'écoule entre une occurrence de a et une occurrence de b en l'absence de c.
Peu de travaux se sont intéressés à la fouille de motifs séquentiels négatifs et, à notre connaissance, le présent travail est le premier qui concerne la fouille de motifs temporels né-gatifs. PNSP (Positive and Negative Sequential Patterns mining) (Hsueh et al., 2008), étend GSP pour la fouille de motifs séquentiels négatifs. Toutefois, PNSP est incomplet. Sa stratégie d'élagage est incorrecte et nombre de motifs séquentiels négatifs pourtant fréquents ne sont pas extraits. Neg-GSP (Zheng et al., 2009) propose une version alternative s'appuyant sur une application partielle du principe d'Apriori aux motifs séquentiels positifs mais pas aux néga-tifs. Cependant la stratégie d'élagage de Neg-GSP bien que correcte est très inefficace car elle n'applique le principe d'Apriori qu'à la partie positive du motif. Proposé récemment, e-NSP (efficient NSP, Cao et al. (2016)) calcule le support des motifs séquentiels négatifs à partir du support de leurs sous-motifs séquentiels positifs, ce qui ne nécessite pas de parcours supplé-mentaire de la base de séquences. e-NSP produit les mêmes motifs que PNSP et souffre de la même incomplétude mais il est nettement plus performant que PNSP et Neg-GSP.
Nous proposons une formalisation et une sémantique pour les motifs temporels négatifs. Nous proposons une méthode pour résoudre le problème d'extraction de tels motifs à partir d'une base de séquences temporelles. La méthode d'extraction proposée s'appuie sur les algorithmes PrefixSpan (Pei et al., 2004), e-NSP (Cao et al., 2016) et TGSP (Yen et Lee, 2013). Il faut noter qu'aucun de ces algorithmes ne traite complètement le problème d'extraction de motifs qui nous intéresse. Enfin, la méthode est évaluée sur des données réelles.
Motifs temporels négatifs
Cette section introduit les motifs temporels négatifs qui étendent les motifs séquentiels (Pei et al., 2004), d'une part, avec des contraintes imposant l'absence de certains itemsets (aspect négatif) et, d'autre part, des contraintes sur le délai entre occurrences d'itemsets (aspect temporel). Nous proposons un formalisme pour de tels motifs et définissons leur sémantique. Dans la suite, [n] = {1, . . . , n} dénote l'ensemble des n premiers entiers strictement positifs.
Soit I un ensemble d'items. Formellement, un motif temporel négatif (MTN) est une sé-
encadre le délai entre le début de la séquence et l'occurrence de p 1 (resp. entre l'occurrence de p n et la fin de la sé-quence). Une contrainte d'absence (itemset négatif) peut être vide, de même qu'une contrainte temporelle (intervalle temporel).
À noter qu'un itemset, positif ou négatif, ne contient que des items positifs. De plus, un motif ne peut contenir deux itemsets négatifs consécutifs. En effet, une telle succession est difficilement interprétable (cf. Cao et al. (2016)). La partie positive du motif est la sous-séquence restreinte aux itemsets positifs.
− −−− → ¬qn a une occurrence dans la séquence s = s 1 , . . . , s m (s supporte p) ssi il existe des indices (e i ) i∈ [n] tels que ∀i ∈ [n], e i ∈ [m] ∧ j < k ⇒ e j < e k et :
2. ∀i ∈ [0, n], ∀j, e i < j < e i+1 , q i s j (satisfaction des contraintes d'absence). e 0 (resp. e n+1 ) est un indice virtuel marquant le début (resp. la fin) de la séquence s.
Soient ¬q i un itemset négatif de p, p i son itemset (positif) prédécesseur (ou un marqueur de début de séquence) et p i+1 son itemset (positif) successeur (ou un marqueur de fin de sé-quence). La définition précédente spécifie l'absence faible pour les itemsets négatifs : il existe une occurrence de (p i , p i+1 ) dans s qui ne contient pas q i . L'absence forte impose que toute occurrence de (p i , p i+1 ) dans s ne contient pas q i . . L'occurrence de e, 3 unité de temps après a, satisfait la contrainte temporelle entre a et e. De même, l'occurrence de d, 2 unités de temps après l'occurrence de e, satisfait la contrainte temporelle entre e et d.
La fouille de motifs temporels négatifs dans une base de séquences temporelles datées D consiste à extraire toutes les sous-séquences (motifs) incluses fréquemment dans des séquences de la base, i.e. ayant un support supérieur à un seuil σ donné a priori. Pour réduire la complexité de la recherche, nous reprenons la contrainte d'e-NSP imposant que tout itemset négatif doit être fréquent. Contrairement à e-NSP, la négation spécifie l'absence faible.
NTGSP : fouille de motifs temporels négatifs
Dans cette section nous présentons la méthode NTGSP -Negative Time Gap Sequential Pattern -pour l'extraction des motifs temporels négatifs (MTN) à partir de séquences d'itemsets datés. NTGSP emprunte à PrefixSpan (Pei et al., 2004) pour l'extraction de motifs séquen-tiels, à e-NSP (Cao et al., 2016) pour l'extraction de motifs négatifs et à TGSP (Yen et Lee, 2013) pour l'extraction d'intervalles temporels entre itemsets positifs.
Les quatre étapes de l'algorithme sont décrites ci-dessous. Le processus de fouille sera illustré sur la base D suivante et les paramètres σ = 2, µ = 3, ε = 1 et δ = 2 : 1. Extraction des motifs séquentiels La première étape utilise l'algorithme PrefixSpan (Pei et al., 2004) avec le seuil de support minimum σ pour extraire les motifs séquentiels de la base de séquences temporelles D.
Les motifs séquentiels de D sont (ab) c d, b b d et tous leurs sous-motifs. 2. Génération des motifs séquentiels négatifs candidats (MSC) Les MSC sont générés en passant à négatif un ou plusieurs itemsets de chacun des motifs générés à l'étape précédente.
Les MSC générés à partir du motif (ab) c d de l'étape 1 sont : supp(p 2 ) = |{s 1 , s 2 , s 4 }| = 3 ≥ µ. p 2 est donc un motif séquentiel négatif. 4. Extraction des motifs temporels négatifs (MTN) Finalement, NTGSP extrait les intervalles temporels représentant les délais admissibles entre les itemsets positifs des MSN par clustering des délais fournis par les occurrences de motifs dans les séquences temporelles de D. Pour ce faire NTGSP utilise l'algorithme de clustering CLIQUE (Agrawal et al., 2005). Soit p un MSN obtenu à l'étape 3 et comportant n itemsets positifs. Pour chaque occurrence de p dans une séquence de D, le vecteur des délais entre itemsets positifs de p peut se repré-senter par un point dans un espace de dimension n + 1. Nous cherchons à regrouper ces points proches pour construire la composante temporelle des MTN. CLIQUE décompose l'espace de dimension n + 1 en hypercubes unitaires de taille ε et élague ceux qui ne sont pas suffisamment denses relativement à un seuil δ. Chaque composante connexe du graphe des hypercubes unitaires denses constitue un cluster. Un cluster est ensuite décomposé en sous-hypercubes maximaux et les contraintes temporelles sont générées à partir des coordonnées des côtés de ces sous-hypercubes dans chaque dimension. Pour des raisons d'efficacité NTGSP utilise la sémantique d'absence faible et seule la première occurrence du motif dans une séquence temporelle est utilisée. Soit ε = 1 et δ = 2. Le vecteur de délais associé à p 3 = (ab) −→ ¬c d est de dimension 1.
Extraction des motifs séquentiels négatifs (MSN)
Aux occurrences de p 3 sont associés respectivement les vecteurs de délais (6), (7) et (6). Seule l'unité 6 est dense : elle contient 2 points. Elle constitue un cluster. La contrainte temporelle résultante est [6,6]. Nous obtenons donc le motif (ab) [6,6] − −− → ¬c d. Le tableau 1 fournit quelques exemples de motifs extraits des données EDF. Pour des raisons de confidentialité les événements ont dû être anonymisés. Cependant, pour montrer l'intérêt de NTGSP, le tableau contient une classe de motifs particulièrement importante, que nous avons appelés motifs duaux. Ils se présentent par paires (C ¬A U , C A ¬U ) où C dénote un contexte, A un événement lié à une action ou un état et U un événement indésirable, lié à une panne, par exemple. Le premier motif de la paire exprime que dans le contexte C et en l'absence de l'action A l'événement indésirable U se produit. Le deuxième motif indique dans le même contexte en présence de A l'événement indésirable U ne se produit pas.
Expérimentations
Les motifs 1 et 2 sont de tels motifs duaux. De plus, ils fournissent des informations temporelles importantes pour savoir dans quel laps de temps l'action inhibitrice doit être exécutée pour éviter l'événement indésirable. À noter le support élevé des motifs 1 et 2 : le motif 1 se produit dans 0.0562 × 375.143 = 21.083 interactions de clients et le motif 3 a un support de 4.0% et apparaît dans 15.000 séquences, ce qui est particulièrement significatif.
Les motifs (non duaux) 3 et 4 contiennent la même séquence d'événements mais des contraintes temporelles différentes qui correspondent à deux hypercubes (rectangles) maximaux issus de deux représentations différentes d'un cluster. 
TAB. 1 -Quelques exemples motifs temporels négatifs

Introduction
Les systèmes de stockage « not-only SQL » (NoSQL) ont connu un important dévelop-pement ces dernières années en raison de leur capacité à gérer de manière flexible et efficace d'importantes masses de données hétérogènes, Floratou et al. (2012); Stonebraker (2012). Les approches orientées documents sont couramment utilisées comme par exemple les systèmes MongoDB (Chodorow et Dirolf, 2010) ou CouchDB (Anderson et al., 2010). Ces systèmes reposent sur le principe de « schemaless » consistant à ne plus considérer un schéma unique pour un ensemble de données , appelé collection de documents (Chevalier et al., 2015). Cette flexibilité dans la structuration des données complexifie l'interrogation pour les utilisateurs qui doivent connaître les différents schémas des données manipulées (Chouder et al., 2017). Cet article traite de la problématique d'interrogation de données hétérogènes dans les systèmes NoSQL orientés documents.
Il existe différents types d'hétérogénéités (Shvaiko et Euzenat, 2005) : L'hétérogénéité structurelle désigne le problème de structures variables entre les documents. L'hétérogénéité syntaxique considère que différents attributs peuvent désigner le même concept tandis que l'hé-térogénéité sémantique considère qu'un attribut peut correspondre à différents concepts. Les travaux développés dans cet article se focalisent sur l'hétérogénéité structurelle des documents.
Pour permettre l'interrogation des données structurellement hétérogènes dans les systèmes NoSQL, deux approches sont essentiellement suivies : soit les données sont transformées pour être rendues homogènes dans un schéma unique (Tahara et al., 2014), soit les données sont conservées de manière hétérogène, mais les différents schémas possibles sont inférés pour permettre l'interrogation (Wang et al., 2015). La première approche a pour avantage de faciliter l'interrogation pour l'utilisateur qui manipule ainsi un schéma unique. Cependant cette approche nécessite des pré-traitements pouvant s'avérer coûteux et difficilement compatible avec des environnements dynamiques. La seconde approche consiste à inférer les différents schémas pour permettre leur interrogation. La variabilité des schémas rend la construction des requêtes plus complexe car elle nécessite la manipulation des différents schémas. Nos travaux se placent dans cette deuxième approche, en rendant transparente pour l'utilisateur l'hétérogé-néité structurelle des documents.
La section 2 expose en détail le problème d'hétérogénéité abordé dans cet article. La section 3 propose un état de l'art et dans la section 4 nous donnons une formalisation de nos proposition. Enfin, la section 5 présente les résultats de nos premières expérimentations.
Problème d'interrogation des documents structurellement hétérogènes
Dans les systèmes orientés documents les données sont représentées en utilisant les notations JavaScript Object Notation (JSON). Un document est considéré comme une paire (clé, valeur) où la clé est un identifiant unique et sa valeur est représentée au format JSON (Bourhis et al., 2017) FIG. 1 -Exemple de quatre documents au format JSON. Nous considérons uniquement des collections structurellement hétérogènes, en considérant qu'un attribut peut être situé à différentes positions dans les différents schémas (par exemple,
Considérons une collection
, ou ne pas être présent (par exemple, "details" est présent dans d 2 mais pas dans
Cette hétérogénéité structurelle complexifie l'interrogation d'une collection de documents, en particulier lorsque cette collection comporte de très nombreux documents structurés avec une grande variabilité. Pour obtenir un résultat correct, l'utilisateur doit écrire autant de requêtes qu'il n'y a de versions de schémas concernées par la formulation de l'interrogation.
Considérons que l'on souhaite obtenir la liste des titres et des années de parution des films. En se basant sur la seule connaissance du schéma du document d 1 , on peut formuler une requête avec les attributs "movie_title", "year" qui permet d'obtenir :
[{"movie_title" : "F ast and f urious", "year" : 2017}, {"movie_title" : "T he Hoobit"}, {"movie_title" : "Despicable M e 3", "year" : 2017}, {"movie_title" : "T itanic"}]
Le résultat est incorrect en raison de l'hétérogénéité structurelle de l'attribut "year" qui est imbriqué de différentes manières dans les documents d 2 et d 4 .
Une autre formulation peut être faite avec les attributs : "movie_title", "details.year". Cette requête produit un résultat comportant le même type d'incorrections. Pour obtenir un résultat complet, il est nécessaire de construire une requête très complexe tenant compte des différents schémas des documents.
Nous proposons dans cet article une approche permettant à un utilisateur d'exprimer simplement une requête à partir des attributs, sans avoir à tenir compte des différentes positions structurelles des attributs, tout en conservant les structures originelles des documents. La requête permet d'obtenir un résultat « complet », de manière transparente par rapport à l'hétéro-généité structurelle de la collection de documents (sans avoir à connaître et à manipuler avec exhaustivité les différents schémas).
Etat de l'art
Dans cette section, nous nous intéressons aux langages de requêtes de données semistructurées ainsi qu'aux systèmes de stockage existants pour lesquels nous analysons leur capacité à prendre en compte l'hétérogénéité structurelle des données.
Beaucoup de travaux ont étudié l'interrogation de documents semi-structurés et des BD de documents. XQuery (Li et al., 2004) est un langage de requête standardisé par le W3C 1 qui propose une syntaxe « SQL-like » pour interroger des documents XML . Il couvre la plupart des fonctionnalités offertes par SQL (interrogation, agrégations, fermeture, etc.). XQuery reprend le langage XPath (Clark et al., 1999) pour localiser et naviguer parmi les différents noeuds d'un document XML. Ces deux langages ne spécifient pas comment traiter de grandes collections de documents et très peu de mises en oeuvre supportent des échelles de volumes importantes. Dans le cadre des bases de données NoSQL, différentes propositions de langages d'interrogation ont été faites, en particulier pour interroger des documents JSON. Citons JSONiq (Florescu et Fourny, 2013) et SQL ++ (Ong et al., 2014) qui proposent une couche prenant en charge les requêtes utilisateurs sur des données semi-structurées ou structurées. L'utilisateur doit construire ses requêtes avec ces nouveaux langages, proches de XQuery ou SQL, mais qui nécessitent parfois des paramétrages fins des processeurs de requêtes, ex. : SQL ++ (Ong et al., 2014), pour répondre aux traitements particuliers de ces données. En bilan, tous ces langages ou systèmes offrent les outils pour interroger n'importe quelles données mais demandent à l'utilisateur de connaître les schémas de ces données et ne lui offrent pas de facilités pour interroger facilement des documents hétérogènes.
Une autre approche d'interrogation consiste à modifier la structure lors du stockage et à interroger les données sans langage de requête orienté document. Par exemple (Tahara et al., 2014) proposent le système Sinew qui aplatit les données et les charge dans un SGBD relationnel (tables). Jaql (Beyer et al., 2011) propose un nouveau langage de script pour interroger simultanément des documents stockés dans des magasins différents et les requêtes sont décou-pées pour être parallélisées en se basant sur le paradigme map-reduce (Thusoo et al., 2009). Au-delà des coûts d'évaluation des requêtes, l'utilisateur doit connaître la structure des documents pour les interroger correctement.
MongoDB est l'un des systèmes NoSQL orientés documents les plus utilisés apportant des solutions efficaces pour le passage à l'échelle (scalability) et la distribution. Dans sa version standard, le langage d'interrogation des données propre à MongoDB ne permet pas d'interroger des données structurellement hétérogènes de façon transparente. Pour y arriver, l'utilisateur doit construire des fonctions spécifiques prenant en charge les différentes structures possibles des données et les intégrer dans ses requêtes (parcours de tous les schémas possibles). Cela rend difficile et complexe l'interrogation et la rend très sensible à l'intégration de nouvelles données (nouveaux schémas). (Wang et al., 2015) et (Herrero et al., 2016) traitent la problématique de la découverte et l'intégration de nouveaux schémas. (Wang et al., 2015) proposent de ramener tous les sché-mas de documents dans un même « schéma type » (skeleton) afin d'aider l'utilisateur dans la découverte d'attributs ou de sous-schémas dans la collection. (Herrero et al., 2016) proposent au contraire d'extraire séparément tous les schémas présents dans la collection afin d'aider l'utilisateur à connaître tous les schémas et tous les attributs présents dans la collection de documents. Si elles permettent de découvrir les schémas des données, ces approches laissent à l'utilisateur la responsabilité de prendre en charge l'hétérogénéité des données lors de l'interrogation.
Interrogation de documents hétérogènes
Notre approche permet l'interrogation de collections de documents qui sont structurellement hétérogènes dans les systèmes NoSQL. L'utilisateur exprime sa requête en se basant, d'une part, sur sa connaissance de la nature des données à traiter et, d'autre part, sur la connaissance d'au moins un schéma de donnée existant dans la collection, mais sans devoir connaître tous les schémas et/ou tous les différents chemins qui mènent aux différents attributs. Cette requête est réécrite de manière transparente pour l'utilisateur afin de prendre en compte l'hé-térogénéité des documents (schémas multiples dans la collection).
Modélisation des données à schémas multiples
Définition 1 (Collection) Une collection, notée C, est un ensemble de documents
Tout document est considéré comme une paire (clé, valeur) où sa valeur est de la forme
, est une valeur objet, chaque a i,j est appelé attribut et chaque v i,j est elle-même une valeur atomique (numérique, chaîne, booléenne, nulle) ou complexe (objet, tableau) définis ci-après ; Une valeur atomique est formée comme suit.
-v i,j = n si n ∈ N * l'ensemble valeurs numériques (entiers ou réels) ; -v i,j = "s" si s est une chaîne de caractères formée dans l'ensemble des caractères U nicodeA * ; -v i,j = b si b ∈ B l'ensemble des booléens {true, f alse} ; -v i,j = ⊥ est la valeur nulle ; Une valeur complexe est formée comme suit.  Dans le cas du document d 2 , l'attribut "details" contient une valeur complexe, formant deux chemins d'accès "details.year" et "details.language" dans le schéma.
Définition 4 (Schéma de collection). Le schéma S d'une collection C est défini par
-p i ∈ S est un chemin appartenant au schéma d'au moins un document de la collection ; -i = {p pi,1 , . . . , p pi,k } ⊆ S, est l'ensemble des chemins d'accès à p i ; Notons que pour la suite de l'article, et par abus de langage, nous nommerons les chemins p i "attributs" ; nous parlerons donc de chemins du dictionnaire ou d'attributs dans le dictionnaire.
Exemple. Le dictionnaire de la collection décrite à la figure 1 est défini ci-dessous. Chaque entrée p i permet d'accéder aux différents chemins possibles de positionnement ;
(year, {year, details.year, versions.1.year, versions.2.year}) indique que l'entrée "year" correspond à 4 positions possibles dans les documents. 
Interrogation de données à schémas multiples
L'interrogation d'une collection de documents s'opère par une composition d'opérateurs unaires. Dans cet article, nous limitons l'interrogation aux opérations de projection et de sé-lection pouvant s'exprimer avec les commandes "find" et "aggregate" de MongoDB.
Noyau minimum fermé d'opérateurs élémentaires
Nous définissons un noyau minimum fermé d'opérateurs élémentaires. On note C in la collection de documents interrogée, et C out la collection de documents résultante.
Définition 6 (Projection) L'opérateur de projection réduit le schéma des documents à un sous-ensemble d'attributs ; on note π A (C in ) = C out où A ⊆ S in est un sous-ensemble d'attributs de S in (schémas de la collection C in ) Définition 7 (Sélection). L'opérateur de sélection permet de restreindre une collection de documents aux seuls documents satisfaisant un prédicat de sélection ; on note σ p (C in ) = C out où p est un prédicat (ou condition) de sélection. Un prédicat simple est une expression a k ω k v k avec a k ⊆ S in est un attribut, ω k ∈ {= ; > ; < ; = ; ≥ ; ≤ } est un opérateur de comparaison, et v i une valeur. Les prédicats peuvent se combiner avec les opérandes Ω = { ∨, ∧, ¬} formant un prédicat complexe.
On note N orm p la forme conjonctive normale du prédicat p, notée comme suit.
Définition 8 (Requête). Une requête Q est construite par composition d'opérateurs
Exemple. Considérons la collection des documents de la figure 1.
"F ast and f urious", "year" : 2017, "language" : "English"} q2 : La requête q 3 est construite par composition. On peut remarquer que les requêtes q 1 et q 3 ne retournent pas l'ensemble des documents possibles, car l'attribut "language" utilisé dans le prédicat de sélection est structurellement hétérogène entre les différents documents de la collection interrogée. De manière analogue la projection de l'attribut "year" dans la requête q 2 est également perturbée par l'hétérogénéité, ne permettant pas d'obtenir la valeur attendue pour les documents d 2 et d 4 où l'attribut projeté "year" est ignoré.
Extension de requêtes aux collections hétérogènes
L'hétérogénéité structurelle des documents complexifie l'interrogation car elle n'est pas gérée nativement par les opérateurs de la plupart des systèmes NoSQL ; par exemple MongoDB ("find") ne reconnaît pas de manière automatique les différentes structures des documents d'une collection ; les attributs non positionnés de manière compatible à la requête sont ignorés.
Notre approche consiste à faciliter l'interrogation pour les utilisateurs, par reformulation automatique des requêtes. Ce processus exploite le dictionnaire des données afin de reformuler la requête en prenant en compte les multiples schémas des documents de la collection interrogée. L'algorithme 1 décrit ce processus d'extension automatique de la requête utilisateur.
-Lors d'une projection, la liste des attributs projetés A i est étendue par l'union des chemins d'accès k à chaque attribut a k de la liste projetée. Ces chemins sont obtenus à partir du dictionnaire des données. -Lors d'une sélection, le prédicat de sélection p, en forme normale conjonctive, est étendu par l'ensemble des disjonctions formées à partir des chemins d'accès i,j de chaque attribut a i,j .
Algorithme 1 : Extension automatique de la requête utilisateur entrée :
Exemple. Considérons la requête q 3 (π movie_title, year (σ langugage= "English" (C)) de l'exemple précédent. Le moteur de réécriture des requêtes, à partir des entrées du dictionnaire suivantes :
(movie_title, {movie_title}), (year, {year, details.year, versions.1.year, versions.2.year}), (language, {language, details.language, versions.1.language, versions.2.language}) et en appliquant l'algorithme 1, permet d'obtenir la requête étendue suivante :
π movie_title, year, details.year, versions.1.year, versions.2.year (σ language="English" ∨ versions.1.language="English" ∨details.language="English" ∨ versions.2.language="English" 
Expérimentations
Nous avons implémenté un outil appelé Easy-Q afin de mettre en oeuvre l'algorithme de réécritures de requêtes proposé dans cet article ainsi que la construction du dictionnaire défini. Easy-Q procède à la création du dictionnaire d'une manière automatique au moment de l'insertion des données et le stocke dans une collection sous MongoDB. Il effectue aussi sa mise à jour. Notre outil prend en entrée la requête de l'utilisateur, procède à la réécriture en utilisant le dictionnaire afin d'extraire différents chemins possibles pour chaque prédicat et lance son exécution dans MongoDB.
Protocole expérimental
Pour l'ensemble de nos expériences nous avons utilisé le système orienté document MongoDB afin de stocker et exécuter nos requêtes. Nous avons choisi de travailler sur des collections de documents synthétisés. Les documents sont construits à partir d'une collection accessible sur internet qui décrit des films proposés par IMDB 2 et composés de 28 attributs qui sont tous liés à la racine (structure plate) : tous les documents sont donc homogènes.
Les caractéristiques des collections générées sont résumées dans le tableau 1. Les documents de la collection sont générés aléatoirement sans ordre particulier : les documents d'un même schéma sont aléatoirement répartis dans la collection. Les groupes d'attributs pour chaque schéma généré sont liés à ce schéma : le groupe est de la forme "groupe_xy" où x fait référence au numéro du groupe et y fait référence à un schéma (numérotés alphabéti-quement). Exemple : le groupe "groupe_1E" fait référence au premier groupe du cinquième schéma généré. Deux schémas différents n'ont donc pas de sous-chemin commun. Pour les niveaux intermédiaires d'imbrication nous générons des attributs intermédiaires entre le groupe et les attributs qui contiennent les valeurs. Exemple : si l'attribut "movie_title" fait partie du premier groupe du schéma 2, dans un document généré, le chemin vers cet attribut sera "groupe_1B.level0.movie_title".
Description des requêtes Les requêtes Q1, Q3, Q5 contiennent la forme conjonctive des prédicats alors que les requêtes Q2, Q4, Q6 contiennent la forme disjonctive des prédicats. 
Évaluation du module de réécriture de requêtes
La première évaluation porte sur le temps d'exécution de la même requête posée sur la collection homogène et la requête réécrite sur la collection hétérogène ; les deux requêtes retournant le même nombre de résultats. L'objectif est d'étudier le coût additionnel de notre solution par rapport à la simple exécution d'une requête sur un jeu de donnée homogène. Nous comparerons aussi ce coût à une interrogation de la collection hétérogène sans réécriture : la somme des coût des requêtes individuelles sur chaque schéma.
Afin d'évaluer la sélection, nous proposons d'exécuter les 6 requêtes de longueurs variables présentées ci-avant (la projection portera sur tous les attributs).
Nous avons utilisé les mesures suivantes pour chacune des requêtes : -QRewritten : Le temps d'exécution de la requête réécrite par notre système sur la collection hétérogène. -QSeparated : La somme des temps d'exécution des sous-requêtes sur la collection hétérogène ; requêtes sans réécriture sur tous les schémas possibles. -QBase : Le temps d'exécution de la requête sur la collection homogène. Notons que les requêtes conjonctives Q1, Q3, Q5 retournent au maximum 1% des documents tandis que les requêtes disjonctives Q2, Q4, Q6 retournent au minimum 70% des documents.
FIG. 2 -Évaluations de la requêtes réécrite
Les résultats présentés figure 2 montrent que notre solution ne dépasse jamais, en temps d'exécution, 2 fois en moyenne dans le cas des requête disjonctive et 1 fois et demi dans le cas des requêtes conjonctive, la durée de la requête de base (sur données homogènes) alors que la réécriture ajoute 10 disjonctions par critère de sélection (10 schémas possibles). De plus, les attributs de la collection hétérogènes sont tous imbriqués à différents niveaux qui peuvent atteindre 7 niveaux dans le cas du sixième schéma. Les évaluation montrent aussi de meilleures performances par rapport aux requêtes sur schémas séparés : jusqu'à 2 fois et demi plus rapide. Même si d'autres évaluation seront à mener, les expériences montrent une croissance du temps d'exécution de la requête réécrite linéaire par rapport à la taille de la collection et du temps d'exécution de la requête sur la collection homogène, à l'opposé de la croissance exponentielle observée dans le cas des requêtes cumulées.
Évaluation du module de création du dictionnaire
Le tableau 2 présente le temps nécessaire pour la création du dictionnaire pour 2, 4, 6, 8 et 10 schémas pour des collections de 100 Go. Le temps nécessaire pour la création du dictionnaire est influencé nettement par le nombre de schémas dans la même collection. Notons cependant qu'il s'agit d'une opération réalisée une seule fois sur la collection lorsque celle ci existe déjà. En cas d'alimentation continue d'une collection, le dictionnaire sera mis à jour au fur et à mesure de l'arrivée des données (documents). Nous avons aussi étudier l'effet d'une forte hétérogénéité sur un nombre important de schémas en testant jusqu'à 5000 schémas dans la même collection. Le dictionnaire à été généré avec pour chaque attribut 5000 schémas diffé-rents. Nous avons aussi évalué le temps mis pour la réécriture de la requête Q6 et nous avons un temps de réécriture très intéressant de moins de 1.5 secondes. Enfin, la taille du dictionnaire n'a jamais dépassé 12 Mo ce qui est restreint au vu des collections et de l'hétérogénéité traitées. 6 Conclusion L'hétérogénéité dans les systèmes orientés documents constitue un défi majeur lors de l'exploitation des données. Nous proposons une approche facilitant l'interrogation de documents à structures hétérogènes en simplifiant l'écriture des requêtes. Notre approche repose sur la construction d'un dictionnaire de données qui indexe tous les schémas d'une collection de documents, et qui est exploitée pour réécrire de manière transparente les requêtes des utilisateurs. Les requêtes réécrites permettent d'obtenir facilement l'ensemble des documents répondants à la requête initiale.
En perspectives de ces travaux, nous allons continuer la validation de l'algorithme de ré-écriture de requêtes en le validant sur d'autres systèmes orientés documents tels que couchBase. Nous étudions le passage à l'échelle sur des collections réelles de grande taille et nous menons des expériences sur des environnements distribués. A long terme, nous travaillerons sur l'extension du langage de requêtes par plus d'opérateurs (agrégation et jointure) ainsi que sur le support des hétérogénéités sémantique et syntaxique.

Introduction
À l'ère numérique, le processus d'apprentissage devient de plus en plus médiatisé et centré sur l'apprenant. L'échange d'informations se fait via un système d'information numérisé qui encourage la participation active des utilisateurs. Cet environnement a remis en question les méthodes traditionnelles de traitement de l'information, conduisant à l'innovation d'autres technologies pour faire face à la massification et à l'hétérogénéité des données. Dans ce travail, nous traitons le cas des MOOCs qui sont des systèmes d'information numériques dédiés à l'apprentissage en ligne et ouvert. Les MOOCs sont accessibles à un nombre massif d'apprenants de profils hétérogènes et animés par une équipe pédagogique de taille réduite qui se trouve incapable d'accompagner l'intégralité des participants.
Notre objectif est donc d'identifier, parmi ce nombre massif d'apprenants, ceux qui sont capables de partager des informations correctes et immédiates avec tout apprenant dans le besoin. Nous appelons ces apprenants "Apprenants leaders". Pour ce faire, nous proposons une approche de recommandation qui repose sur une méthode d'aide à la décision multicritère pour la prédiction hebdomadaire des trois classes de décision : Cl 1 des "Apprenants en risque" d'abandonner le MOOC, Cl 2 des "Apprenants en difficulté" et Cl 3 des "Apprenants leaders". Ensuite, la technique de filtrage démographique est appliquée afin de recommander une liste personnalisée des "Apprenants leaders" pour chaque "Apprenant en risque" ou "Apprenant en difficulté" en fonction de son profil. Etant donnée la primeur de MOOCs, peu de travaux existent sur la recommandation dont une majorité recommande les ressources pédagogiques répondant aux besoins des apprenants (Onah et Sinclair, 2015). A nos connaissances, le seul travail qui recommande une ressource humaine est celui de Labarthe et al. (2016) sauf que l'apprenant recommandé est identifié d'une manière intuitive.
2 KTI-MOOC : système de recommandation pour un MOOC L'objectif de notre système de recommandation KTI-MOOC (recommender system for the Knowledg Transfer Improvement within a MOOC) est la personnalisation du processus d'échange d'informations entre les apprenants du MOOC. Ainsi, l'utilisateur cible de notre système est un "Apprenant en risque" d'abandonner le MOOC ou bien un "Apprenant en difficulté". Ce sont les apprenants à qui nous envisageons recommander des "Apprenants leaders" qui représentent une source pertinente d'information.
Le profil d'un apprenant est représenté par la langue, le pays, la ville et le domaine d'études. Le voisinage d'un apprenant cible ("Apprenant en risque" ou "Apprenant en difficulté") est représenté par les apprenants qui lui sont plus proches considérant ces quatre informations en appliquant la distance euclidienne. Enfin, afin de recommander à un apprenant cible c la liste de leaders appropriés, nous devons lui prédire le taux d'appréciation r c,l pour chaque apprenant leader l, en utilisant les notes données par chaque voisin v pour ce même apprenant leader.
Dans la formule (1), v l (c) est le voisinage de l'apprenant cible qui a évalué l'"Apprenant leader" en question. La variable w c,v reflète le poids du voisin, calculé par son degré de similarité avec l'apprenant cible. Le taux r v,l est l'évaluation donnée par le voisin v à l'"Apprenant leader" l en question. Les "Apprenants leaders" en ligne disponibles et d'une valeur r v,l la plus élevée seront recommandés et affichés sur la page personnelle de l'apprenant cible. Afin de remédier au problème de démarrage à froid, s'il n'existe aucune appréciation vis à vis des apprenants leaders, le système recommande à l'apprenant cible des leaders de son voisinage.
Dès que l'apprenant est connecté, l'algorithme de recommandation s'exécute : s'il est un "Apprenant en risque" ou un "Apprenant en difficulté", alors il est un utilisateur cible, auquel sont calculés le voisinage et la fonction de prédiction afin d'en inférer les n-top (dans notre cas 3-top) "Apprenants leaders" appropriés. Sinon, cet apprenant sera recommandé.
Expérimentations et résultats
Les algorithmes sont codés avec Java et exécutés sur une machine personnelle avec Windows 7, Intel (R) Core T M i3-3110M CPU @ 2.4 GHz et 4.0 GB de mémoire.
La Figure 1 représente la page personnelle d'un apprenant cible. KTI-MOOC est un module intégré dans l'environnement du MOOC qui doit apparaitre sous forme d'une liste déroulante en bas et à droite de la page. La liste contient l'ensemble des "Apprenants leaders" personnalisée en fonction du profil de l'apprenant cible en question.
Dès qu'il clique sur un nom de la liste recommandée, une fenêtre de Chat est ouverte pour l'apprenant cible lui permettant (par ordre d'apparition sur la fenêtre de Chat) de : (1) Fournir un retour de pertinence sur l'"Apprenant leader" contacté (bouton like/dislike), (2) Ouvrir une La Figure 2 montre que l'algorithme de recommandation est plus rapide lorsque moins d'apprenants sont inscrits et moins d'évaluations sont données. Cela semble logique parce que la technique de filtrage démographique appliquée pour la recommandation traite les données démographiques de tous les apprenants ainsi que les évaluations qu'ils soumettent. La complexité de l'algorithme de recommandation est en θ(n).
FIG. 2 -Temps d'exécution de l'algorithme de recommandation
La couverture de l'espace item est le pourcentage des "Apprenants leaders" recommandés par rapport au nombre total des "Apprenants leaders". La Figure 3 représente les résultats des simulations effectuées sur des ensembles distincts de données en modifiant la taille de l'ensemble cible de recommandation. Les "Apprenants leaders" sont ordonnés selon un ordre croissant de leur fréquence de recommandation. Dans les courbes supérieures la recommandation concerne les "Apprenants en risque" et les "Apprenants en difficulté". En revanche, dans les courbes inférieures la recommandation concerne uniquement les "Apprenants en difficulté".
Nous constatons que la couverture sur l'espace item diminue en diminuant la taille de l'ensemble cible de la recommandation (exp. cibles est élevé plus les voisinages sont diversifiés. Ceci permet de réduire le taux d'appartenance d'un "Apprenant leader" aux voisinages identifiés. Cependant, dans tous les cas, plus que la moitié des "Apprenants leaders" a été recommandée.
Conclusion
Notre système de recommandation KTI-MOOC est conçu pour la personnalisation de l'échange d'informations entre les apprenants des MOOCs. Il vise à aider les apprenants à trouver un apprenant source d'une information pertinente. Il représente un module intégré dans l'environnement du MOOC qui doit apparaitre sous forme d'une liste déroulante sur la page personnelle de l'apprenant cible, en bas et à droite de la page. La liste contient l'ensemble des "Apprenants leaders" personnalisée en fonction du profil de l'apprenant cible en question. 
Summary
In order to help learners take advantage of the MOOC (Massive Open Online Course) they follow, we propose a tool to recommend to each of them an ordered list of "Leader learners" to support him during his learning process. The recommendation phase is based on a multicriteria approach for the periodic prediction of the "Leader learners". Given the heterogeneity of the learners' profiles, we recommend to each of them the appropriate leaders by using the Euclidean distance and the demographic filtering technique.

Introduction
Les systèmes de recommandation ont été principalement introduits pour aider les utilisateurs à faire face à la surcharge d'information sur le Web et pour augmenter le profit des entreprises. Leur tâche est de faire des suggestions d'items personnalisées à un utilisateur. Plusieurs domaines d'application ont bénéficié du développement de ces systèmes utilisés pour recommander des films à Netflix, des produits à Amazon, ou encore de la musique à Spotify. Néanmoins, le problème de recommandation d'hôtels se distingue des autres problèmes et soulève des difficultés particulières.
Les systèmes de recommandation d'hôtels souffrent d'un problème de démarrage à froid continu et cela pour trois raisons principales. Tout d'abord, le voyage est une activité relativement rare et la majorité des gens réservent un hôtel une ou deux fois par an. Ensuite, l'intérêt des voyageurs est susceptible de changer avec le temps. Ce phénomène est en particulier observé chez les personnes qui changent de statuts et passent de la réservation d'hôtels de la classe économique à la réservation d'hôtels luxueux. Enfin, la sélection d'un hôtel est généralement influencée par plusieurs facteurs contextuels comme par exemple la localisation, la tempora-lité, la météo et la raison du voyage. Les systèmes de recommandation basés sur le contexte offrent une solution efficace pour faire face à ces difficultés ( Adomavicius et Tuzhilin (2011)).
Dans cet article, nous proposons un système de recommandation d'hôtels qui combine deux nouvelles approches prenant en compte les dimensions géographique et temporelle ainsi que la raison du voyage. Le système regroupe dans un premier temps les utilisateurs qui partagent les mêmes affinités concernant les destinations visitées ainsi que les périodes de visite. Les modèles de recommandation sont ensuite construits indépendamment pour chaque groupe d'utilisateurs et l'apprentissage des modèles est guidé par la raison du voyage. Nos expérimen-tations sur des jeux de données réels extraits des bases de données d'AccorHotels 1 démontrent la contribution des données contextuelles à améliorer la qualité de recommandation.
La suite de cet article s'organise comme suit. Dans la section 2, nous donnons un aperçu de quelques approches de recommandation. Les sections 3 et 4 décrivent les méthodes sur lesquelles reposent le système de recommandation proposé. Enfin, nous présentons nos résultats expérimentaux dans la section 5, avant de conclure dans la section 6.
Préliminaires
Les systèmes de recommandation gèrent trois types d'entités : les utilisateurs, les items à recommander et les interactions entre utilisateurs et items. On distingue principalement deux approches pour la recommandation : le filtrage basé sur le contenu et le filtrage collaboratif. Dans le filtrage basé sur le contenu, les attributs descriptifs des items sont utilisés pour recommander à l'utilisateur des items similaires à ceux qu'il avait appréciés dans le passé. Les techniques de filtrage collaboratif exploitent la mesure de similarité entre les ensembles d'interactions effectuées par chaque utilisateur afin de sélectionner les items à recommander.
Pour traiter le problème de démarrage à froid continu rencontré lors de la recommandation d'hôtels, nous avons recours aux systèmes de recommandation basés sur le contexte. Ces systèmes tentent de reproduire le processus de prise de décision des voyageurs en considé-rant les facteurs contextuels qui influencent en général leurs décisions. Les recommandations sont alors guidées par le contexte actuel de l'utilisateur et par le comportement passé d'autres utilisateurs dans des situations similaires. Construire un système de recommandation robuste consiste alors à identifier les facteurs contextuels qui impactent les utilisateurs et à développer des modèles qui prennent en compte ces facteurs.
Cependant, les approches préalablement proposées échouent à s'adapter aux contraintes propres au problème de recommandation d'hôtels. La grande majorité des approches est conçue de façon à manipuler des données collectées explicitement comme les notes ou les commentaires. Elles ne peuvent être facilement transposées pour gérer des données collectées implicitement comme les réservations d'hôtels (utilisées dans notre cas de figure) ou les données de navigation. Par ailleurs, le contexte est traditionnellement modélisé comme une variable multidimensionnelle où chaque dimension correspond à un facteur contextuel. Les approches proposées supposent que les facteurs contextuels contribuent de façon égale au processus de prise de décision de l'utilisateur. Sachant que les voyageurs donnent plus de priorité à certains facteurs plutôt qu'à d'autres, ces approches ne sont pas adaptées au problème considéré.
TAB. 1 -Exemples de clusters de pays de résidence (liste non-exhaustive par cluster).
Cluster 1
France, Belgique, Andorre, Guinée, Guyane, Algérie, Tunisie, Sénégal Cluster 2
Italie, Allemagne, Suisse, Pays-Bas, Grèce, Roumanie, Russie, Turquie Cluster 3 États-Unis, Canada, Pérou, Portugal, Inde, Chine, Corée du Sud, Vietnam Cluster 4 ÉAU, Bahreïn, Koweït, Qatar, Maroc, Espagne, Tanzanie, Cuba
Une étude menée avec des experts du domaine révèle que les voyageurs sont influencés par plusieurs types de contexte qui sont principalement le contexte physique, social et psychologique. Le système de recommandation présenté intègre les dimensions géographique et temporelle (contexte physique) et la raison du voyage (contexte psychologique).
Intégration des influences géographique et temporelle
Nos jeux de données contiennent des utilisateurs répartis partout dans le monde et des hôtels répandus dans plus de 90 pays. La sélection d'un hôtel est généralement conditionnée par la destination à visiter. Nous considérons deux facteurs influençant le choix de destinations. Tout d'abord, le pays de résidence de l'utilisateur joue un rôle important dans le processus de décision. Les résidents d'un même pays suivent en général les mêmes tendances et visent dans la majorité de leurs déplacements des régions relativement proches du lieu de résidence. Ensuite, la popularité des destinations change en fonction des périodes de l'année marquée par les vacances et les saisons.
Afin d'intégrer l'influence des dimensions géographique et temporelle sur les utilisateurs, nous proposons de regrouper les pays de résidence dont les résidents ont des comportements similaires à l'égard du choix des destinations et des périodes de visite. La répartition des utilisateurs dans des clusters découle de ce regroupement de pays. Les modèles de recommandation sont ensuite construits indépendamment pour chaque cluster d'utilisateurs. Ces modèles, dits locaux (par opposition au modèle global construit pour l'ensemble des utilisateurs), sont capables de capter des préférences de granularité plus fine puisqu'ils couvrent des utilisateurs qui partagent des goûts similaires vis-à-vis des destinations.
Un pays de résidence est représenté par un vecteur comprenant un élément par destination et par mois. La valeur de cet élément est égale à la proportion de résidents ayant visité la destination en question durant le mois concerné. On applique l'algorithme K-moyennes afin d'obtenir K clusters de pays de résidence. La table 1 montre des exemples de pays par cluster.
Le clustering des pays de résidence suppose que les utilisateurs dont on dispose constituent un échantillon représentatif de la population de chaque pays. Le clustering dépend aussi de la distribution des hôtels AccorHotels dans tous les pays, sachant que celle-ci n'est pas uniforme.
Intégration de la raison du voyage
Le comportement du voyageur n'est pas le même lors d'un voyage d'affaires ou d'un voyage pour le plaisir. La raison du voyage n'est pas fournie explicitement par l'utilisateur mais peut être inférée à partir d'autres caractéristiques du séjour. Celles-ci couvrent, entre autres, le délai entre la date de réservation et la date du séjour, les jours de la semaine pendant lesquels le séjour est effectué (week-end ou non) et le nombre d'adultes et/ou d'enfants. D'autre part, on note que les préférences des utilisateurs peuvent changer entre deux réservations successives.
Nous proposons un modèle qui prend en compte la raison du voyage afin d'améliorer l'apprentissage des paramètres du modèle de recommandation. Le modèle proposé donne aussi plus de poids aux interactions les plus récentes qui représentent mieux les préférences actuelles de l'utilisateur. Le modèle se base sur BPR ( Rendle et al. (2009)), un modèle de factorisation de matrices, où nous utilisons un échantillonnage non uniforme du jeu de données afin d'apprendre le modèle.
Soit U l'ensemble des utilisateurs de taille m et H l'ensemble des hôtels de taille n. Soit R ∈ R m×n la matrice utilisateur-hôtel contenant m utilisateurs et n hôtels. La valeur de r uh est égale à 1 si l'utilisateur u a visité l'hôtel h, et 0 sinon. Le but de la factorisation de matrices est d'approximer la matrice R par le produit de deux matrices de facteurs latents P ∈ R m×k et Q ∈ R k×n , sachant que k min (m, n).
Plusieurs méthodes ont été proposées afin d'apprendre les paramètres du modèle, i.e., les matrices P et Q. BPR ( Rendle et al. (2009)) considère des paires d'items lors de l'apprentissage du modèle et optimise le ranking de ces items l'un par rapport à l'autre. L'hypothèse principale est qu'un item h observé pour un utilisateur u est préféré par rapport à un item h non observé pour u. Les données considérées pendant la phase d'apprentissage sont représentées par l'ensemble suivant :  Nous reportons dans la table 3 les résultats pour un cluster d'utilisateurs qui en comprend environ 60 mille, sachant que les conclusions sont similaires pour les autres clusters. Les ré-sultats montrent l'importance de l'intégration des données liées à la raison du voyage et de la mise en valeur des interactions les plus récentes. Ceci a un impact sur l'apprentissage du modèle qui génère de meilleures recommandations.
Conclusion
Nous proposons dans cet article un système de recommandation d'hôtels basé sur le contexte afin de faire face au problème de démarrage à froid continu rencontré lors de la re- 
Summary
In recent years, recommender systems have witnessed an increased interest from industry and academia. The deployment of such systems in the hotel industry needs to satisfy specific constraints, making the direct application of classical approaches insufficient. There is an inherent complexity to the problem, starting from the decision-making process for selecting accommodations, which is sharply different from the one for acquiring tangible goods, to the multifaceted behavior of travelers, often selecting accommodations based on contextual factors. Travelers recurrently fall into the cold-start status due to the volatility of interests and the change in attitudes depending on the context. In this paper, we propose a context-aware recommender system for hotel recommendation. The system is based on two novel approaches that take into account geography, temporality, and the trips' intent. Our experiments on a realworld dataset show the impact of taking into account contextual data in improving the quality of the recommendation.

Introduction
La recherche en microbiologie dispose aujourd'hui de très grandes quantités de données sur les habitats des microorganismes en raison de l'expansion des technologies de séquen-çage à haut-débit et de la croissance du volume des publications et des bases de données. De nombreux domaines de recherche en microbiologie ont l'usage de cette information, dont, en premier lieu, l'étude de la diversité microbienne. L'expression en langue naturelle de l'information sur les habitats microbiens est un frein majeur à son exploitation. Il est très fréquent que des habitats similaires soient décrits par des termes différents, ce qui rend difficile leur comparaison automatique. (Ivanova et al., 2010) souligne l'importance de la construction d'un référentiel commun pour standardiser les descriptions de ces habitats, nous proposons ici un tel référentiel sous la forme d'une ontologie, appelée OntoBiotope.
Contexte et motivation
Tous les domaines de la microbiologie produisent des descriptions d'habitat, en premier lieu sous forme d'articles -près de 7 millions d'habitats de microorganismes sont mentionnés dans PubMed selon Deléger et al. (2016). Les bases de données de ressources biologiques comportent toujours un champ «isolation», plus ou moins structuré et détaillé qui décrit le site où l'échantillon a été prélevé, comme BacDive, the Bacterial Diversity Metadatabase de DSMZ (https://bacdive.dsmz.de). Plus récemment, l'utilisation des technologies de séquençage à haut-débit génère un très grand nombre de séquences de microorganismes associées ici encore à leur lieu d'isolation et disponibles publiquement dans des bases de données comme GenBank.
Parallèlement, l'abondance de ces descriptions favorise l'émergence en biologie de questions transversales aux différents milieux, telles que les questions relatives à la provenance des organismes et aux parcours de contamination, à l'adaptation des microorganismes à différents milieux en lien avec des questions évolutives et génétiques. L'exploitation d'un tel volume de données requiert l'utilisation de méthodes automatiques. Les descriptions des milieux d'échan-tillonnage microbiens restent largement sous-exploitées par manque de solutions automatisées. La raison est double. L'analyse à grande échelle des descriptions des milieux de vie des microorganismes requiert (1) une classification de référence aux catégories de laquelle attacher les descriptions et (2) un moyen automatique d'associer les descriptions des habitats à ces catégories. Nous proposons dans cet article une solution qui répond à ces deux objectifs.
Le deuxième objectif relève de la fouille de texte pour extraire finement les informations, les catégoriser et les relier. Les progrès récents des méthodes permettent d'atteindre des performances qui les rendent aujourd'hui exploitables pour ce type de tâche mesurées par des compétitions internationales comme BioNLP Shared Task Bacteria Biotope (Bossy et al., 2015).
Le premier objectif relatif à la disponibilité d'une classification de référence, est un point critique. Pour être utilisable, elle doit répondre à plusieurs critères. Elle doit être suffisamment riche pour rendre compte de la grande diversité des habitats et permettre ainsi de distinguer des habitats dont les propriétés physico-chimiques diffèrent, mais sans être trop vaste, ce qui nuirait à sa maintenance et à son utilisation manuelle. Sa structure doit à la fois refléter les domaines d'études de biodiversité microbienne pour faciliter son appropriation par les utilisateurs microbiologistes, mais également regrouper les milieux très similaires de manière à en faciliter les traitements. Son organisation doit être hiérarchique pour permettre son utilisation à différents niveaux de précision.
Les classifications d'habitats de microorganismes sont peu nombreuses et ne répondent pas à ces critères. Par exemple, la classification ATCC (Floyd et al., 2005) est une liste de 37 entrées d'habitat environnemental, insuffisante de par sa petite taille et sa structure à plat. GOLD (Genome OnLine Database) utilise un vocabulaire contrôlé plus riche, mais non hiérarchisé pour indexer l'information d'isolation des échantillons biologiques (Reddy et al., 2014).
EnvO (Environment Ontology project : https://bioportal.bioontology.org/ ontologies/ENVO) est une ontologie hiérarchique de 7000 classes, soutenue par le Genomics Standards Consortium (GSC) destinée à l'annotation manuelle des environnements des organismes et des échantillons biologiques (Buttigieg et al., 2013), mais elle souffre de limitations pour la description des habitats d'organismes microscopiques. Ratkovic et al. (2012) ainsi que Cook et al. (2016) ont montré qu'EnvO n'était pas bien adaptée à l'extraction d'information en microbiologie. Le développement d'EnvO repose sur la réutilisation de classifications connues qui ont été conçues pour d'autres objectifs. La conséquence en est qu'elles ne sont généralement pas adaptées à la description des habitats microbiens. Elle sous-représente certains domaines importants en recherche microbienne comme la transformation des aliments. La classe des aliments réutilise FoodON, the United Nations Food classification où par exemple les fromages sont distingués par leur couleur (red marbled, white) ou leur présentation (sliced, dip). La transformation (cuisson, lavage) ou l'animal dont le lait est utilisé (vache, brebis) sont des concepts plus pertinents pour l'étude écologique. Un autre exemple est la classe des sols issue de la classification pédologique de Agriculture Organization soil classification. Elle n'est pas structurée selon les propriétés principales des sols comme l'acidité ou l'humidité qui sont critiques pour les microorganismes.
L'absence d'ontologie adaptée à la catégorisation automatique des descriptions d'habitats microbiens en microbiologie a motivé la construction de l'ontologie OntoBiotope.
Construction et principes de l'ontologie OntoBiotope
La partie Habitat qui fait l'objet de cet article est la partie principale de l'ontologie OntoBiotope. Elle respecte les critères définis ci-dessus. L'approche suivie pour sa construction est assistée par des outils automatiques et des outils d'édition. Une attention particulière est apportée à la terminologie pour permettre son usage à des fins d'extraction fine de l'information.
La construction s'est faite de manière ascendante et descendante. L'approche descendante structure successivement la classification en fonction des grands domaines d'étude de la microbiologie et de leurs subdivisions successives pour faciliter son appropriation par les microbiologistes. L'approche ascendante part de l'ensemble des termes particuliers qui dénotent des habitats pour les regrouper itérativement et hiérarchiquement. Ces termes ont été extraits automatiquement du champ «Habitat» de la base de données GOLD et du champ «Source» de la base de données GenBank par l'extracteur de terme BioYaTeA (Golik et al., 2013) intégré dans la suite Alvis (Ba et Bossy, 2016). L'analyse terminologique manuelle des termes extraits a été assistée par l'outil TyDI (Terminology Design Interface) suivant la méthode décrite par Nédellec et al. (2010). La formalisation en sous-arbres assure que les propriétés d'une classe sont partagées par toutes les sous-classes, afin de garantir que les informations indexées par une classe particulière pourront être retrouvées par l'interrogation par des classes parentes. OntoBiotope Habitat ne décrit pas les lieux géographiques, d'autres classifications pertinentes comme la base GeoNames leur sont dédiées. Le sous-arbre Food a été construit en adaptant FoodEx2, la nouvelle classification des aliments de l'EFSA (l'Autorité européenne de sécurité des aliments) et grâce à l'expertise des microbiologistes du projet Florilège sur la flore positive des aliments (Falentin et al., 2017).
Le format initial choisi est le format Open Biomedical Ontologies (OBO) développé par OBO Foundry dont l'éditeur Obo-Edit présentait au début du projet en 2010 de bonne propriétés d'utilisabilité pour des non-spécialistes. L'expressivité du format OBO est adaptée aux besoins du projet permettant la représentation du niveau lexical (synonymes) et du niveau conceptuel (classes et relations). Trois types de synonymes sont considérés, les synonymes exacts (exact synonym) comme les acronymes (perchloroethylene contaminated site / PCE contaminated site) ou les variations typographiques, les synonymes proches (close synonym) (polluted site / contaminated site) et les synonymes associés (related synonym) (PCP percolated soil / PCP contaminated soil).
OntoBiotope Habitat est distribuée par le portail d'ontologie AgroPortal (http://agroportal.lirmm.fr/ontologies/ONTOBIOTOPE), sous la licence Creative Commons with Attribution (CC-BY). La version publique ne contient pas d'autre relation que la relation hiérarchique. Elle est explorable en ligne et téléchargeable aux formats standards, OBO (format d'origine) et traduit en RDF/XML. Elle contient 2320 classes et 492 synonymes, elle est organisée dans une hiérarchie d'une profondeur de 13. La racine se divise en 11 grands domaines (figure 1).
Exemples d'utilisation
Nous avons proposé Ontobiotope Habitat comme ontologie de référence pour catégoriser les habitats microbiens et les relier aux bactéries par les trois dernières éditions de la compéti-tion internationale BioNLP Shared Task Bacteria Biotope (Bossy et al., 2015), organisées avec l'objectif de stimuler la recherche en extraction d'information dans le domaine de l'écologie microbienne. Ontobiotope a démontré son utilité pour annoter automatiquement des textes par les différents algorithmes participant aux compétitions.
FIG. 2 -Exemple de l'habitat "cheese" dans la base de données Florilège.
Nous avons développé une telle application en utilisant la Suite Alvis de text-mining pour analyser et catégoriser à grande échelle les habitats de microorganismes. Grâce à la catégo-risation, les données du texte deviennent comparables et peuvent être croisées avec d'autres données, en particulier les données génétiques. Le projet Florilège en est un exemple, il a pour objectif la mise à disposition de données relatives à la flore positive des aliments, espèces, habitats, phénotypes, usages, etc., dans une base de données structurée (Falentin et al., 2017). Elle intègre les informations de l'ensemble des résumés de la base bibliographique PubMed en microbiologie des aliments, analysée par la suite Alvis. L'ontologie OntoBiotope est utilisée pour indexer les données d'habitats. Les taxa sont normalisés par la taxonomie de référence du NCBI. La figure 2 donne un exemple de l'interface d'interrogation : le taxon Arcobacter cryaerophilus et ses habitats dont fresh village cheese par exemple ont été identifiés dans des textes, catégorisés et reliés par la relation lives_in. fresh village cheese est normalisé par cheese qui est l'objet de la requête. La Suite Alvis et l'application sont en cours de déploiement sur l'infrastructure européenne OpenMinTeD, ce qui la rendra publiquement utilisable (Ba et Bossy, 2016) sur différents corpus. A terme, l'application Florilège rendra accessible dans une interface unifiée les données de microbiologie de la littérature et des sources de données expertisées comme la collection allemande de bactéries DSMZ et le Centre International de Ressources Microbiennes (CIRM) dédié aux bactéries de l'INRA. Toutes les données extraites des textes sont également accessibles par le moteur de recherche sémantique AlvisIR (http:// bibliome.jouy.inra.fr/demo/ontobiotope/alvisir2/webapi/search). L'interface du moteur comme celle la base de données Florilège (http://genome.jouy. inra.fr/Florilege) permettent d'exprimer des requêtes pour une recherche hiérarchique et relationnelle (quel organisme vit où ?), c'est-à-dire que les résultats contiennent les relations correspondant à la classe recherchée, ainsi que les relations faisant intervenir les entités associées aux classes plus spécifiques suivant la hiérarchie de l'ontologie.
Perspectives
Les branches d'OntoBiotope sont à différents stades de développement en fonction des ontologies réutilisables répondant aux besoinx et des collaborations avec des experts. La partie être vivants et anatomie est ainsi une des parties les plus riches en nombre de concepts avec 51 classes dans la branche gastrointestinal part. Sa structuration nécessite une réflexion approfondie. Il ne serait pas judicieux de reprendre en l'état les classifications anatomiques médicales qui sont regroupées en grands systèmes ou par fonction, plutôt que structurées en fonction des propriétés physico-chimiques des milieux. Par contre, le lien vers les concepts d'anatomie médicale devra être préservé de façon à permettre l'intégration de données annotées par ces différentes ressources. Plusieurs sous-arbres complémentaires des habitats, notamment les propriétés des habitats, les phénotypes microbiens et leurs usages technologiques complètent OntoBiotope et seront publiés prochainement.
Remerciements
Ce travail a été financé par le programme Quaero d'Oséo, par le métaprogramme INRA MEM et par le projet européen H2020 OpenMinTeD (EC/H2020-EINFRA 654021).
Références
Ba, M. et R. Bossy (2016). Interoperability of corpus processing workflow engines : the case of AlvisNLP/ML in OpenMinTeD. In INTEROP 2016, LREC, Portoroz, Slovenia.

Introduction
La présence des réseaux comme outil de modélisation ou de conception des systèmes naturels et humains est au coeur de la science des réseaux. Dans cette discipline par nature pluridisciplinaire, l'étude des graphes dynamiques pose encore aujourd'hui de nombreux défis notamment lorsqu'il est question de les visualiser (Beck et al., 2014).
L'un des objectifs de la visualisation est de révéler à l'utilisateur la structure inhérente aux données. L'oeil humain est particulièrement habile à détecter les motifs graphiques cachées dans l'image montrée à l'écran (Ware, 2000(Ware, , 2005. On peut argumenter -et c'est le cas dans un grand nombre de champs scientifiques -que la structure tient à la notion de groupe (Stacey, 2005;Shavit, 2005) : des ensembles présentant une certaine homogénéité. Dans le cas des graphes, cette homogénéité tient à une densité de liens plus marquée dans le groupe qu'avec le reste du graphe. Ce sont ces groupes que l'on appelle les communautés (dans un graphe).
La visualisation d'un graphe dynamique trouve là un défi : montrer l'évolution de ces groupes à travers le temps. On parle ainsi souvent des communautés dynamiques (Rossetti et Cazabet, 2017), mais ce terme fait implicitement l'hypothèse de la stabilité dans le temps d'une communauté. Or, notre travail et plus particulièrement le domaine auquel nous nous intéressons remettent cette notion de stabilité en question.
Le domaine dans lequel notre travail s'inscrit est celui de l'étude des médias français. Nous considérons des données d'affiliation et construisons le graphe de co-participation à des émissions radiophoniques ou télévisuelles française sur la période 2010 -2016. L'étude des "communautés" dans ce contexte présente un intérêt particulier pour, par exemple, "lire" le biais potentiel des médias induit d'invitations répétées d'un même panel d'experts sur un sujet donné.
Nous proposons un tableau de bord offrant un ensemble d'outils destinés à sonder les communautés présentes dans un jeu de données extraits des co-participations aux émissions des médias. Ces communautés, celles proposées par différents algorithmes ou encore un groupe d'acteurs sélectionnés par l'utilisateur à l'écran, peuvent ensuite être examinées sous diffé-rents angles et à travers différents signaux mesurant leur niveau "d'activité", leur "stabilité" ou encore leur "taux de renouvellement". La construction du tableau de bord suit d'un travail dont les prémisses ont déjà fait l'objet d'une communication courte (Ren et al., 2017).
Calculer les communautés
Le point de départ est le chargement et la visualisation d'un graphe de co-participation sur la période 2010-2016. Typiquement, on pourra isoler les acteurs participants à des magazines diffusés de manière périodique et/ou sur un thème spécifique (actualité politique, variétés, sport, etc.).
Parmi les approches permettant d'identifier des communautés (Rossetti et Cazabet, 2017), celles misant sur une évolution incrémentale des communautés doivent être mises à l'écart pour les raisons évoquées plus haut. Nos expérimentations, comparaisons et évaluation informelles (auprès de collaborateurs de l'INA) nous ont conduit à considérer l'approche calculant les communautés sur le graphe rassemblant les co-participations sur l'ensemble de la période.
Ces graphes -parce qu'ils sont obtenus par projection à partir de graphe d'affiliation (Guillaume et Latapy, 2005)  (Borgatti et al., 2009) -sont typiquement très denses et leur lecture exige d'en simplifier la structure. Nous utilisons à cette fin l'approche de (Nick et al., 2013) capturant un squelette particulièrement efficace à recouvrer la structure des graphes d'affiliation.
Les sous-graphes obtenus par restriction à un intervalle de temps (où on ne considère que les émissions s'étant déroulées sur cet intervalle) présente une densité modérée et peuvent être examinés sans avoir recours à ce filtrage. Il reste intéressant de confronter deux stratégies pour tenter de faire émerger les communautés (voir la figure).
Mainmise sur les médias
Le calcul de communautés à lui seul ne suffit pas à l'analyste qui doit pouvoir questionner les représentations qui lui sont proposées pour explorer le monde des médias (sous l'angle des co-participation aux émissions radiophoniques et télé-visuelles).
Un ensemble d'indicateurs permet de sonder les communautés pour questionner leur "stabilité" et leur évolution dans le temps 1 .
1. La vidéo d'accompagnement montre bien plus qu'il ne nous est possible d'illustrer ici.
Ren et al.
Plusieurs courbes indiquent combien les acteurs d'une communauté ont en effet été invités les uns avec les autres, plutôt qu'avec d'autres personnes hors de leur groupe. Ce signal est indispensable puisque les communautés sont calculées sur l'ensemble de la période. Le diagramme en bâtonnets (figure de gauche) indique combien le groupe sé-lectionné "recouvre" l'ensemble des invités à une émission (1 bâtonnet = 1 émission).
Certaines courbes présentent des profils typiques et reflètent le rythme des actualités. La périodicité des évènements sportifs, par exemple, se lit très facilement (championnats annuels, Tour de France, etc.). Dans ce domaine, les communautés sont particulièrement bien tissées. Les communautés d'experts sur des questions de sécurité est un autre exemple (ils sont sur-sollicités, et sur toutes les chaînes, à certains moments pour commenter des évènements dramatiques). On lit facilement aux sursauts des courbes le calendrier de ces évènements tragiques.
Les communautés que nous étudions présentent encore une autre particularité : des personnes vont et viennent entre les communautés. Un diagramme de Sankey (Reda et al., 2011) (figure de gauche) permet de suivre comment une communauté se fait et se dé-fait, et comment elle se "renouvelle" (des journalistes prenant la relève dans le suivi de l'actualité). En effet, si une personne a pu être affectée à une communauté au vu de la globalité des co-invitations sur toute la période, sa pré-sence sur les émissions avec les membres de sa communauté fluctue dans le temps. D'autres acteurs "papillonnent" d'une communauté à une autre.
Ayant sélectionné un groupe de personnes, typiquement une communauté suggérée sur l'une des vues (graphe noeuds-liens ou Sankey), on peut avoir un retour de son activité (au sens du signal affichée sur la courbe présenté plus haut) sur un diagramme à bulles (figure de gauche). Les taille des bulles correspond au nombre d'émissions auxquelles ont participé les membres d'une communauté sur un mois.
Les dix chaînes les plus "fréquentées" sont extraites et placées verticalement, les fré-

Introduction
L'objectif de la recherche non supervisée est d'affecter un label à des points non labélisés où le nombre et l'emplacement des clusters sont inconnus. Nous nous sommes concentrés sur un algorithme de clustering modal où le nombre de clusters est défini en terme de modes locaux de la fonction de densité de probabilité qui génère les données. Le plus connu des algorithmes de clustering modal est le k-means. Comme ce dernier est basé sur la distribution de mélange normale, il est contraint à trouver des clusters ellipsoidaux ce qui peut être inapproprié pour des jeux de données complexes. Le Mean-shift est une généralisation du k-means en raison de sa capacité à calculer des clusters de topologie aléatoire définis comme les bassins d'attractions des modes locaux générés par la montée de gradient de (Fukunaga et Hostetler, 1975). Afin de calculer les chemins de la montée de gradient, les k plus proches voisins sont appropriés car ils s'adaptent à la topologie locale des données. La version actuelle des k plus proches voisins Mean-shift contient des goulots d'étranglement posés par une grille de recherche multiple pour le choix d'un nombre de voisins optimal et par le calcul exact des k plus proches voisins. Nous proposons ici un nouvel algorithme qui résout ces gouffres computationnels : (a) une échelle normale efficace du choix du nombre des plus proches voisins qui évite la recherche en grille, (b) le locality sensitive hashing (LSH) qui est une version approximée des k plus proches voisins et (c) une implémentation MapReduce distribuée.
Méthode 2.1 Le Mean-shift
Le Mean Shift introduit par (Fukunaga et Hostetler, 1975), génère pour un point x de dimension d une séquence de points qui suivent le chemin en montée de la densité de gradient en utilisant la relation de récurrence :
où X 1 , . . . , X n est un échantillon aléatoire obtenu d'une fonction de densité commune f , les k plus proches voisins de
est la distance du k-ème plus proche voisin. x 0 = x. L'équation (1) donne au Mean Shift son nom en raison du déplacement successif des itérations de x j vers la moyenne de ses k plus proches voisins pour la prochaine itération x j+1 . La convergence de la séquence {x 0 , x 1 , . . .} vers un mode local pour la version à noyau de l'équation (1) a été établie par (Comaniciu et Meer, 2002) pour une large classe de noyaux sur des fenêtres fixées. Cette convergence reste valide quand la fenêtre fixe est remplacée par la distance des plus proches voisins qui décroit avec l'augmentation du nombre d'itérations.
Le chemin de montée de gradient vers les modes locaux produit par l'équation (1) forme les bases de l'Algorithme 2.1 (NNMS), notre méthode des plus proches voisins Mean-shift. Les entrées du NNMS sont les échantillons de données X 1 , . . . , X n et les points candidats que nous souhaitons clusteriser x 1 , . . . , x m (Ils peuvent être X 1 , . . . , X n mais ce n'est pas un prérequis). Les paramètres de réglage sont les suivants : -le nombre de plus proches voisins k -le seuil sous lequel la convergence des itérations est considérée comme étant sufisante ε 1 -le nombre maximum d'itérations j max -le seuil sous lequel deux itérés finaux sont considérés comme étant membres du même cluster ε 2 -la cardinalité minimale des clusters formés c min Les sorties sont les labels des clusters des points candidats {c(x 1 ), . . . , c(x m )}. Il y a trois sous-routines à l'Algorithme 2.1. Les lignes 1-6 correspondent à la formation des chemins de la montée de gradient dans l'équation 1 qui sont itérés jusqu'à ce que la distance de la dernière itération soit infèrieure à ε 1 ou que le nombre maximum d'itérations j max soit atteint. Les sorties de ces lignes sont les itérés finaux x * 1 , . . . , x * m . Les lignes 7-8 concernent la fusion des itérés finaux dans le même cluster lorsque la distance les séparant est sous le seuil ε 2 , ceci créant un regroupement initial des x * 1 , . . . , x * m . Les lignes 9-13 déterminent si les plus petits clusters ont une cardinalité supérieure à s min sinon on fusionne les clusters concernés avec leur voisin le plus proche pour produire c(x * 1 ), . . . , c(x * m ). La ligne 14 assigne les labels de ces clusters aux données originales x 1 , . . . , x m .
Choix du nombre de plus proches voisins suivant une échelle normale
Le paramètre de réglage critique pour le Mean shift est le choix du nombre de plus proches voisins k. Le travaux pionniers de (Loftsgaarden et Quesenberry, 1965), (Fukunaga et  j := 0 ; x ,0 := x ; 3:
x ,1 := mean of k-nn of x ,0 ; 4: while x ,j+1 , x ,j > ε 1 or j < j max do 5:
x * := x ,j ; /* Création des clusters par fusions des itérés finaux */ 7: for 1 , , 2 := 1 to m do 8:
; /* Fusion des petits clusters */ 9: C * := cluster avec une cardinalité minimale ; 10: while card(C * ) < s min do
11:
C := plus proche cluster à C * ; 12:
C * := cluster avec une cardinalité minimale ; 14: for := 1 to m do c(x ) := c(x * ) ;
1973) établissent l'erreur quadratique optimale des sélecteurs locaux et globaux pour les estimateurs de densité des plus proches voisins, sachant que ces auteurs ne considèrent pas les sélecteurs basés sur les données. Une grille de recherche basée sur les données cherche à minimiser les indices de qualité de recherche non supervisée comme l'indice Silhouette considéré par (Wang et al., 2007). Notre proposition d'échelle normale pour le sélecteur est : (Duong et al., 2016). Elle suit l'assertion que la sélection des paramètres de réglages basés sur le gradient de densité plutôt que sur la densité elle-même est plus adéquate pour le mean shift (Chacón et Duong, 2013). La complexité de k NS est O(1) ce qui contraste avec le O(n) de la grille de recherche pour sélectionner le nombre optimal de plus proches voisins k sachant que le nombre de recherches de valeurs possibles est usuellement réglé pour être proportionnel à n.
Plus proches voisins approximés avec le Locality Sensitive Hashing
La tâche calculatoire la plus intensive dans NNMS est le calcul des k plus proches voisins plutôt que la sélection du nombre de plus proches voisins. En effet, pour chaque point candidat, cela requiert le calcul et le tri de la distance X i − x j , i = 1, . . . , n, j = 1, . . . , m, qui est O(mn log n). Dans les cas usuels où m est du même ordre de grandeur que n, cela empêche son application pour des jeux de données importants. Une approche de réduction de complexité prometteuse tient sur le calcul des approximés plus proches voisins plutôt que sur les exacts plus proches voisins. Parmi celles existantes, le locality sensitive hashing introduit par (Datar et al., 2004), (Datar et al., 2004) est une approche probabiliste basée sur une projection scalaire aléatoire de points multivariés x
est une variable aléatoire uniforme prise sur [0, w), w > 0. Une table de hashage dont les blocs sont basés sur des valeurs entières L(X i ; w), i = 1, . . . , n est alors construite. En raison de propriétés statistiques de la distribution normale, les points proches dans l'espace multidimensionnel de départ auront tendance à tomber dans les mêmes blocs scalaires et les points distants tomberont dans des blocs différents comme vérifié dans (Slaney et Casey, 2008). D'importantes valeurs de w impliqueront moins de blocs avec plus de précision dans la préservation des caractéristiques de X i , tandis que de petites valeurs de w entraineront plus de blocs avec moins de précision. Nous avons préféré paramétriser le LSH par le nombre de blocs M de la table de hashage. Nous avons fixé w = 1 sans perte de généralité L i ≡ L(X i ; 1). Ces projections scalaires sont ensuite triées dans leur ordre statistique
où 1{·} est la fonction d'indication. Afin de chercher les approximés plus proches voisins, le réservoir des potentiels plus proches voisins est réglé à la valeur du bloc contenant la valeur de hashage. Ce réservoir est élargi si nécessaire par concaténation avec les blocs voisins. Les approximés k plus proches voisins de x sont les k plus proches voisins contenus dans le réservoir réduit
est la distance seuil des plus proches voisins à x. L'erreur d'approximation dans les plus proches voisins à x induite par la recherche dans R(x) plutôt que dans toutes les données est probabilistiquement contrôllée, voir (Slaney et Casey, 2008). L'Algorithme 2 NNLSH est une approximation de la recherche des plus proches voisins avec le LSH et la fonction de hashage fourni par l'équation (3). Les entrées sont les échantillons de données X 1 , . . . , X n . Dans les lignes 2-6, pour chaque point candidat x , les approximés k-plus proches voisins k-nn(x ) sont calculés à partir du réservoir R(x ).
La proposition où le NNLSH est intégrée au NNMS a été faite par (Cui et al., 2011), ce qui réduit la complexité à O((mn/M ) log(n/M )). Le nombre de blocs M est un paramètre de réglage crucial. Malgré un fort intérêt pour le LSH (Har-Peled et al., 2012), il n'existe pas de méthode optimale pour sélectionner le nombre de blocs, nous examinerons donc des heuristiques de performance dans la prochaine section.
Implémenter les approximatifs plus proches voisins NNMS de manière distribuée avec un processus maître et N processus esclaves réduit la complexité à O(mn/(M N ) log(n/(M N ))). C'est notre proposition, le DNNMS dans l'Algorithme 3. Les entrées et sorties sont les mêmes que pour l'Algorithme 1. Pour la j-ème itération, les chemins de montée de gradient x j = [x 1,j ; . . . ; x m,j ] sont collectés dans une matrice m × d. Aux lignes 1 à 6, on itère jusqu'à une convergence globale x j+1 − x j ≤ 2 ≡ ≡x 1,j+1 − x 1,j , . . . , x m,j+1 − x m,j ≤ 2 ou jusqu'au nombre maximal d'itérations j max . Certains calculs redondants sont effectués lorsque certains des x ,j ont déjà convergé, mais cette forme de calcul est nécessaire pour une parallélisation effective en MapReduce (Dean et Ghemawat, 2008). Le paradigme MapReduce est plus efficace si les algorithmes en séries sont repensés, passant d'une itération sur chaque candidat à une itération sur l'ensemble des candidats simultanément. Les lignes 7-14 décrivent la fusion des regroupements reprise de l'Algorithme 1 sans modification majeure étant donné que le MapReduce n'est pas requis ici.
Algorithm 2 NNLSH -Approximés k plus proches voisins avec LSH
Entrées : {X 1 , . . . , X n }, {x 1 , . . . , x m }, k, M Sorties : {k-nn(x 1 ), . . . , k-nn(x m )} /* Création des tables de hashage avec M blocs */ 1: for i := 1 to n do H i := H(X i ) ; /* Recherche des approximés plus proches voisins dans les blocs adjacents */ 2: for := 1 to m do 3:
while card(R(x )) < k do 5:
Algorithm 3 DNNMS -Plus proches voisins Mean-shift distribué, avec approximés k plus proches voisins en utilisant le LSH Entrées : {X 1 , . . . , X n }, {x 1 , . . . , x m }, k, ε 1 , j max , ε 2 , s min , M Sorties : {c(x 1 ), . . . , c(x m )} /* Calcul des chemins de montée de gradient */ 1: j := 0 ; x 0 := [x 1,0 ; . . . ; x m,0 ] ; 2: x 1 := mean of k-nn of {X 1 , . . . , X n } to x 0 3: while x j+1 − x j > ε 1 or j < j max do 4:
x j+1 := mean of k-nn of {X 1 , . . . , X n } to x j ; /* cf Algorithme 2 */ On peut observer sur la figure 1a que notre Scala/Spark implémentation de la montée de gradient avec les k-plus proches voisins est scalable, le temps d'exécution diminue efficacement avec les nombre de noeuds esclaves. Après investigations des lacunes de notre première implémentation, nous avons observer que l'étape de labélisation n'était pas scalable comme montré sur la figure 1b. Cependant, en réutilisant les LSH afin de segmenter les tâches comme Algorithm 4 Labélisation distribuée avec les k plus proches voisins approximés Entrées : {x * 1 , . . . , x * m }, M 2 , 2 , 3 Sorties : {c(x 1 ), . . . , c(x m )} /* Création des tables de hashages à M 2 blocs */ 1: for i := 1 to m do H i := H(x * i ) ; /* Labelisation des données */ 2: for := 1 to m do 3:
; /* Calcul des barycentres */ C 1 , ..., C j := barycentre des clusters /* Fusion des plus proches cluster */ 6: for C 1 , C 2 := 1 to j do 7:
décrit dans l'algorithme 4, nous avons observé une scalabilité de la solution réprésentée sur la figure 1c. La troisième étape consistant à fusionner les petits clusters avec leurs plus proches voisins se déroule localement sur le noeud maître, elle prend les coordonnées des barycentres et la cardinalité du cluster associé pour sortir une labélisation finale qui sera appliquée en parallèle. En pratique si les valeurs 2 et 3 sont bien choisies, cette étape est immédiate, un mauvais choix de ces valeurs peut entrainer la génération d'un grand nombre de clusters et faire exploser le temps d'exécution. 
Influence du nombre de blocs du LSH
Notre nouvelle implémentation tire avantage du LSH durant la phase de montée de gradient mais aussi pendant l'étape de labélisation. Un paramètre clé sera celui du nombre de blocs M 1 , M 2 dans le LSH. Si on laisse ce dernier constant comme présenté sur la Figure 2, on constate que la complexité quadratique de la montée de gradient via les k plus proches voisins persiste tout comme pour l'étape de labélisation. Cependant, on peut observer sur la Figure 3  le temps d'exécution diminuer rapidement, en fonction du nombre de blocs, puis ralentir pour atteindre un seuil qui dépendra du nombre de données d'entrées. Une observation intéressante concerne l'augmentation linéaire du temps d'exécution lorsqu'on fixe un nombre d'élément par bloc constant avec l'augmentation de la taille du jeu de données comme illustré sur la Figure 4. Ce résultat permet de conforter notre version du Mean Shift en tant qu'algorithme pleinement scalable.
Application à la segmentation d'image
La résurgence d'intérêt dans l'algorithme Mean-shift est dûe à son application à la segmentation d'image (Comaniciu, 2003) où une image est transformée dans un espace colorimétrique dans lequel chaque cluster correspond à des régions segmentées de l'image originale. L'espace 3-dimentionnel L * u * v * de couleur (Pratt, 2001)    Le Berkeley Segmentation Dataset and Benchmark fournit une segmentation d'image humaine de leurs images à des fins de comparaisons. Dans la Fig.5(a-b) se trouvent deux détections de bordures faites par l'utilisateur #1107 et #1123. L'utilisateur #1107 se concentre sur la segmentation du feuillage d'arrière plan et le contour de la forme des fleurs, tout en ignorant les détails des pétales des fleurs. L'utilisateur #1123 quant Ã lui se concentre sur la segmentation des pétales individuelles dans le premier plan. Nous portons ici notre attention sur le NNMS et le DNNMS-200 (Fig. 5(c-d)). Le DNNMS-500 et le DNNMS-1000 (Fig. 5(e-f)) donne une qualité insuffisante de la détection des bords. Le NNMS et le DNNMS-200 sont capables de segmenter en une seule exécution avec un unique jeu de paramètres de réglage, simultanément le feuillage d'arrière plan et la forme des pétales de premier plan. On a ainsi une segmentation automatique combinant le résultat de deux experts humains se focalisant sur différentes zones de l'image. 
Conclusion
Nous avons introduit plusieurs améliorations à l'algorithme des plus proches voisins Meanshift. La première est une heuristique du choix d'une valeur optimale du nombre de plus proches voisins. La seconde est l'emploi d'une approximation des plus proches voisins via le locality sensitive hashing pour la phase de montée de gradient mais aussi pour la phase de labélisation. La troisième est une implémentation dans un écosystème distribué. Nous avons démontré que ces améliorations diminuent drastiquement le temps d'exécution tout en maintenant la qualité du regroupement vis à vis des exacts plus proches voisins. Ces améliorations rendent possible l'application du Mean-shift pour le clustering appliqué au Big Data dans un futur proche. Certaines améliorations restent cependant à faire sur les paramètres de réglages cruciaux, i.e le nombre de plus proches voisins ainsi que le nombre de blocs dans le locality sensitive hashing pour les approximés plus proches voisins est requis.

Introduction
La prolifération du nombre de publications scientifiques et d'enquêtes sur un sujet donné s'accompagne de l'essor croissant des méthodologies de synthèse des connaissances. Les mé-thodes agrégatives, souvent regroupées sous le terme de procédures « méta-analytiques », visent à combiner des résultats quantitatifs d'études indépendantes. Si l'historique de la méta-analyse remonte aux travaux de Pearson au début du XXème siècle sur l'analyse de plusieurs études portant sur le vaccin contre la fièvre typhoïde, puis à ceux de Fisher et Cochran dans les années 30, les articles de Cohen (1962), Light et Smith (1971) et Glass (1976), qui a introduit le terme de méta-analyse dans le contexte statistique, ont contribué au développement du domaine. Aujourd'hui, la médecine reste un terrain d'application privilégié mais la méta-analyse connaît également un fort engouement en sciences sociales où elle complète et précise les synthèses issues de méthodes interprétatives (Laroche, 2015).
Dans son schéma classique issu des travaux historiques, la méta-analyse vise essentiellement à estimer le degré de relation entre des variables d'intérêt en tenant compte des variations observées entre différentes études. Cependant, ses définitions dans la littérature offrent souvent un cadre plus large qui met l'accent sur la combinaison de résultats quantitatifs de multiples recherches pour produire une connaissance empirique sur un sujet donné (Littell et al., 2008). C'est dans ce cadre que se positionne cet article qui propose une nouvelle approche méta-analytique basée sur une analyse ordinale pour contribuer à établir une synthèse de résultats d'enquêtes d'opinions. Plus précisément, on considère ici un ensemble E = {e 1 , e 2 , . . . , e m } de m enquêtes portant sur une même thématique et X = {x 1 , x 2 , . . . , x n }, l'ensemble de toutes les modalités considérées sur E qui correspondent à des points de vue ou opinions que les personnes interrogées peuvent approuver ou non. On dispose de la fréquence d'approbations P A (x i , e k ) de la modalité x i dans l'enquête e k . La distribution observée des P A (x i , e k ) permet de déduire un ordre sur les modalités pour chaque enquête : x i est préférée à x j dans l'enquête e k si P A (x i , e k ) > P A (x j , e k ). L'objectif est d'établir, à partir de ces fréquences, un ordre global sur l'ensemble X de toutes les modalités qui permettra d'obtenir une vue générale des opinions des plus aux moins approuvées. Notre démarche n'est pas seulement basée sur les ordres partiels induits sur chaque enquête, difficilement combinables, ni sur les sommes ou moyennes des fréquences d'approbation, très variables d'une enquête à l'autre, mais sur des comparaisons par paires des modalités comparables dans chaque enquête.
D'un point de vue opérationnel, deux questions délicates se posent. La première difficulté est méthodologique : les enquêtes ayant été menées indépendamment par des organismes différents, les modalités de X ne sont pas nécessairement présentes dans chaque enquête et de plus un codage préalable est nécessaire pour homogénéiser les réponses. La deuxième difficulté, plus importante, est algorithmique car la recherche d'ordres consensus se heurte à des problèmes de complexité (Barthelemy et Monjardet, 1981). S'appuyant sur des travaux antérieurs sur la recherche d'ordres médians dans un tournoi (Barthelemy et al., 1989;Charon et al., 1997), nous proposons dans cet article d'améliorer une méthode de Branch & Bound pour calculer efficacement un ordre total sur X qui soit le «plus compatible» avec les ordres sur les opinions exprimées pour chacune des enquêtes de E.
Nous appliquons notre approche sur un ensemble d'enquêtes portant sur les motivations et les freins à l'intégration de l'Internet des Objets (IdO) dans les entreprises. Si ce sujet est très présent dans l'actualité économique, car les entreprises cherchent à tirer partie de cette technologie transformatrice, il a été encore peu exploré dans les sciences sociales. Nous avons donc recueilli les résultats de huit enquêtes récentes réalisées par des organisations et entreprises de premier plan dans le conseil et la technologie (World Economic Forum, The Economist Intelligence Unit, etc) qui ont interrogé des interlocuteurs positionnés en haut de la chaîne hiérarchique (directeur de département, de business unit, etc) dans 6237 entreprises réparties dans le monde. Il s'agit à notre connaissance de la première méta-analyse sur ce sujet et les premiers résultats obtenus semblent questionner des opinions souvent diffusées dans les discours du moment ainsi que des hypothèses adoptées dans les « Business Models » récemment développés dans le contexte de l'industrie 4.0.
Modélisation du problème et état de l'art
Dans cette partie nous considérons que les modalités renvoyant à des concepts similaires mais rédigées avec des variations ont été recodées sous une même formulation et nous renvoyons au paragraphe 4 consacré à l'application pour les détails du codage. Néanmoins, les enquêtes ayant été menées indépendamment, toutes les modalités x i n'apparaissent pas néces-sairement dans toutes les enquêtes, et les ordres induits sur les modalités dans chaque enquête e k ne sont donc pas directement comparables. Pour contourner cette difficulté intrinsèque à la nature des données, nous construisons préalablement un ordre partiel P sur X en comparant les modalités deux à deux ; ce qui nous permet de poser notre problème de méta-analyse comme un problème de recherche d'un ordre total «le plus compatible» avec l'ensemble ordonné (X, P ).
La recherche d'un ordre total compatible avec un ensemble ordonné a donné lieu à des travaux dans les domaines de la Théorie du choix social (vote) et de l'Agrégation des préfé-rences (Monjardet, 1973;Barthelemy et Monjardet, 1981). Le problème classique se modélise à l'aide d'un tournoi T . Un tournoi est un graphe complet orienté, qui peut être pondéré ou non, dont les sommets sont les éléments de X (ici les modalités) et pour lequel il existe un arc entre deux sommets x i et x j si x i est supérieur à x j pour P .
La construction de P se déduit naturellement de la comparaison des fréquences d'approbation des modalités. Pour toute paire de modalités {x i , x j } de X on compare leurs différences de fréquence d'approbation sur les seules enquêtes dans lesquelles elles ont été proposées :
Ceci permet de munir l'ensemble des modalités X d'un ordre sur les paires. Ainsi,
Si l'ensemble des relations entre paires n'induit pas de circuit, le tournoi est dit transitif et tout ordre qui respecte l'orientation des arcs de T est compatible avec l'ensemble ordonné (X, P ). En revanche, si il existe un circuit, par exemple x i → x j → x k → x i , alors aucun ordre total n'est compatible, puisque l'un des arcs {(x i , x j ), (x j , x k ), (x k , x i )} est orienté en sens contraire de l'ordre. Ces arcs orientés à l'opposé d'un ordre total sont appelés arcs-retour.
Exemple
Considérons le cas de trois enquêtes qui portent sur cinq modalités dont la table des pourcentages d'approbation est donnée dans le Tableau 1. Les cases vides correspondent aux modalités non proposées dans les enquêtes. Les comparaisons par paires de ces modalités, sur les seules enquêtes où elle sont simultanément proposées donnent le Tableau 2 des poids des arcs du graphe orienté.
Il lui correspond le graphe orienté de la Figure 1. Il ne contient qu'un seul circuit (x 1 , x 4 , x 2 ) dont les poids des arcs sont les seuls indiqués. L'arc de poids minimum est (x 2 , x 1 ) de poids 10. C'est le seul arc retour de l'ordre optimal (x 3 , x 1 , x 4 , x 5 , x 2 ) 
Méthodologie
Dans le cas pondéré qui nous intéresse dans la suite, l'ordre le plus compatible avec l'ensemble ordonné (X, P ) est celui pour lequel la somme des poids des arcs-retour est minimale. Cette somme est appelée écart à l'ensemble ordonné et, quand le tournoi modélise les comparaisons par paires d'éléments qui proviennent d'ordres totaux, un ordre associé à l'écart minimal est appelé ordre médian car il minimise la somme de la distance de Kendall aux ordres totaux. Ce n'est plus nécessairement vrai dans le cas d'ordres partiels que nous avons ici, mais nous conservons dans la suite le terme de médian pour désigner un ordre à écart minimum d'un ensemble ordonné quelconque.
La recherche d'un ordre médian, aussi connue sous le nom de problème de Kemeny (Kemeny, 1959), revient à rendre le tournoi transitif par retournement des arcs-retour dont la somme des poids est minimale. Ce problème a une longue histoire algorithmique, qui débute avec Slater (1961) pour les tournois non pondérés, puis s'est étendue aux tournois pondérés (Barthelemy et al., 1989;Charon et al., 1997). Le problème a été prouvé NP-difficile (Hudry, 1989).
En pratique, son calcul s'effectue par une méthode de Branch & Bound qui consiste à dé-velopper une arborescence dont les sommets sont les sections commençantes d'un ordre qui peut se prolonger en ordre médian (Guénoche, 1977). Cependant, lorsque le tournoi contient de nombreux circuits, le parcours de l'arborescence de recherche peut rapidement devenir inapplicable, même pour quelques dizaines de sommets (Barthelemy et al., 1989). En effet, la taille de l'arborescence peut dépasser le million de noeuds et le temps de recherche de la feuille à explorer pour prolonger la section commençante est pénalisant.
Dans notre contexte applicatif, l'ensemble ordonné (X, P ) n'est pas un tournoi : environ 40% des paires de modalités ne sont pas ordonnées puisque toutes les modalités n'apparaissent pas dans toutes les enquêtes. Cependant, nous avons développé une méthode de Branch & Bound fortement inspirée de celle utilisée pour la recherche d'un ordre médian dans un tournoi pondéré et nous améliorons l'exploration de l'arborescence eu égard aux travaux antérieurs en tenant compte du meilleur ordre construit sur chaque section commençante et d'un coût minimum de l'établissement d'un ordre sur les éléments non classés.
3 Algorithme pour un ordre médian Dans la suite, nous considérons un graphe pondéré G associé à l'ensemble ordonné (X, P ) construit selon le même principe qu'un tournoi où la pondération d'un arc entre deux modalités x i et x j est égale à w(x i , x j ). Nous cherchons un ordre total sur X dont la somme des poids des arcs-retour sur G soit minimale. Comme dans le cas de la construction d'un ordre médian sur un tournoi, nous développons une arborescence de recherche dont les sommets sont des sections commençantes, c'est à dire des débuts d'ordres totaux sur une partie de X que l'on prolonge aux étapes suivantes jusqu'à l'obtention d'un ordre total sur X. Dans la suite, on
L'algorithme de B & B peut se décrire en trois étapes :
1. Etablir par une (ou plusieurs) heuristique(s) une borne supérieure, notée B sup de l'écart d'un ordre total au tournoi ; elle doit être la plus faible possible ;
2. Initialiser les feuilles de l'arbre de recherche avec tous les sommets o i tels que
Tant que la section commençante prolongée n'est pas un ordre total -Considérer une section commençante (une feuille) de poids minimum ; -Tester le coût de son extension avec chaque sommet non placé dans la section ; -Si le poids de la section étendue reste inférieur ou égale à B sup , créer une nouvelle feuille dans l'arbre 3.1 Evaluation des sommets de l'arborescence L'efficacité de la recherche arborescente repose en grande partie sur le calcul d'une borne pour le poids W k (o 1 , o 2 , . . . , o k ) d'une section commençante. Ce poids doit être inférieur ou égal au poids de tout ordre total ayant cette section commençante. Il est défini par la somme de trois termes :
-la somme des poids des arcs-retour du sous-graphe de G induit par les sommets de X s . Cette somme est égale à 1≤i<j≤k w(o j , o i ) -la somme des poids des arcs de G dont l'origine y est dans Y et l'extrémité dans X s : y∈Y j=1,...,k w(y, o j ) -une borne inférieure B inf (Y ) du poids de l'ordre non encore déterminé sur la partie finissante Y .
Le calcul de la borne inférieure B inf est une adaptation du calcul d'une borne inférieure pour le problème de Slater proposée dans (Charon et al., 1996). Rappelons que, dans le cas d'un tournoi qui est un graphe complet, pour supprimer tous les circuits et donc construire un ordre médian, il suffit d'éliminer les circuits de longueur 3 par retournement d'arcs. On peut alors évaluer une borne inférieure du coût de ces suppressions de la façon suivante. La suppression d'un 3-circuit coûte au moins le plus petit poids des trois arcs, noté w min . Mais si on retourne l'arc de poids w min , on supprime également tous les 3-circuits qui contiennent cet arc. Il faudrait donc rechercher un ensemble d'arcs de poids minimum dans l'ensemble des 3-circuits arc-disjoints. A défaut d'optimum, ceci peut être approximé par un algorithme glouton. Il suffit de ranger tous les 3-circuits dans l'ordre des poids décroissants et de parcourir cette liste ; chaque fois que l'on retient un arc, on ignore les 3-circuits suivants qui contiennent cet arc. La somme des poids des arcs retenus est une borne inférieure de l'écart d'un ordre médian.
On remarque que cette procédure peut en fait s'appliquer à tout sous-graphe induit par un sous-ensemble de sommets du tournoi. On peut donc l'utiliser pour calculer une borne inférieure B inf (Y ) sur une partie finissante Y . Notons cependant que dans notre cas G n'étant pas un graphe complet, pour avoir la précision de la borne inférieure calculée sur un tournoi il faudrait considérer tous les circuits minimaux (sans corde) puisqu'il se peut qu'il n'y ait pas de 3-circuits. Ce calcul pouvant s'avérer très coûteux en temps, nous nous en tenons aux 3-circuits en acceptant une dégradation possible de la précision de la borne inférieure B inf (Y ).
Au total, l'évaluation
Puisqu'elle ne tient pas complètement compte de l'ordre non encore déterminé sur Y , elle est inférieure ou égale au poids de tout ordre total commençant par (o 1 , o 2 , . . . , o k ).
Par rapport à l'algorithme détaillé dans (Guénoche, 2017), nous avons conservé la procédure d'optimisation locale (qui teste les transpositions des extrémités des arcs-retour) et la procédure de décomposition du tournoi. Elles permettent de calculer un ordre approché et donc une borne supérieure de l'écart au tournoi. Mais nous avons introduit deux améliorations qui permettent de limiter l'exploration de l'arborescence de recherche :
-la prise en compte de la borne inférieure du coût d'un ordre sur toute partie finissante, décrite ci-dessus ; -la gestion du meilleur ordre sur une partie commençante. Plus, précisément, lors du premier calcul d'une partie commençante on l'enregistre avec son évaluation et on prolonge l'arborescence. Si l'on retrouve cette partie, la décision dépend de la comparaison de son évaluation avec celle obtenue précédemment : si elle est plus élevée on rejette la partie et on ne prolonge pas l'arborescence ; si elle est plus faible on la conserve en mettant à jour l'évaluation et on prolonge l'arborescence, et en cas d'égalité si on ne cherche qu'un seul ordre médian on peut également la rejeter. Cette stratégie nécessite la construction d'une structure de donnée pour les fonctions caractéristiques des parties de X afin de mémori-ser la valeur du meilleur ordre sur X s .
Nous avons mesuré expérimentalement que ces procédures permettent un gain significatif en temps de calcul et en taille de l'arborescence. Cette efficacité se mesure par le rapport entre la taille de l'arbre sans les utiliser et la taille de l'arbre quand on les applique.
Efficacité des nouvelles procédures
Nous avons réalisé des simulations en tirant deux pourcentages pour chaque paire de modalités. La comparaison des valeurs permet de quantifier la préférence de l'une en faveur de l'autre. On construit tout d'abord un tournoi en prenant comme poids de chaque arc la différence positive de ces pourcentages. Le tournoi généré est transitif car les arcs sont orientés dans le sens des indices croissant des modalités. Il est donc transitif et le seul ordre médian est l'ordre naturel.
Pour simuler le fait que toutes les modalités ne sont pas systématiquement évaluées, nous avons introduit un taux d'incomparabilité, noté Inc. Les Inc × n(n − 1)/2 valeurs tirées au hasard sont mises à 0, et ainsi les deux modalités sont incomparables. Et pour graduer l'écart à la transitivité, nous bruitons le tournoi par des échanges aléatoires de valeurs de poids symétriques : w(x j , x i ) ← w(x i , x j ) et w(x i , x j ) = 0. Plus ces échanges sont nombreux, plus on s'écarte de la transitivité et plus l'ordre médian est difficile à calculer et donc les arbres de recherche sont grands. Ce second paramètre est donc le taux d'échanges à partir du tournoi transitif noté Swap qui correspond au nombre de paires échangées, soit Swap × n(n − 1)/2.
Pour construire un ordre médian, on commence par calculer une borne supérieure, par application de l'heuristique d'optimisation locale, puis par décomposition en 1 + n 10 classes et enfin par une nouvelle optimisation locale, on calcule une borne supérieure de l'écart. C'est alors que démarre la construction de l'arborescence une fois avec les procédures à évaluer et l'autre fois sans ; cette dernière ne peut n'aboutir que si la place mémoire est suffisante. Notre programme limite la taille de l'arborescence à 2 000 000 de noeuds. Le Tableau 3 indique, pour n = 20, le rapport moyen des tailles des arbres, sur 100 essais pour swap = 5 ou10, pour 50 essais pour swap = 15 et pour 10 essais si swap = 20. On remarquera que nous n'avons pas pu établir de valeur moyenne de la taille de l'arbre sans appliquer les procédures pour swap = 20 et Inc = 30, non plus que quand Inc = 40 pour Swap = 15 ou 20, parce que l'un des arbres, construit sans les procédures, dépassait les limites autorisées. Pourtant, la taille moyenne des arbres avec les procédures reste inférieure à 6000 noeuds ! Ces estimations, montrent que l'on gagne un facteur très important sur la taille de l'arborescence. Ce facteur va croissant quand l'écart à la transitivité augmente et aussi quand le taux d'incomparabilité devient plus important. Ceci est prévisible, du fait que nos graphes sont loin d'être complets et donc qu'il y a beaucoup d'ordres sur les sections commençantes qui sont ex-aequo. C'est en n'en conservant qu'un seul qu'on gagne en efficacité.
Pour vérifier que l'on peut calculer des ordres médians sur des graphes plus importants, nous avons testé des séries de 5 graphes aléatoires. Pour n = 30, Swap = 10, Inc = 30, un ordre médian est construit à l'aide d'un arbre de 65 000 noeuds en moyenne, le maximum observé étant 228 072. De même pour n = 40, Swap = 5, Inc = 40, la taille moyenne des arbres étant 181 493 et le maximum 361 458 noeuds.
Application
Notre cadre applicatif concerne le déploiement de l'Internet des Objets (IdO) dans les entreprises. Le terme Internet des Objets a émergé à l'Auto-ID Center du MIT à la fin des années 1990 pour désigner « une infrastructure intelligente mettant en lien des objets, de l'information et des humains à travers des réseaux d'ordinateurs, avec la RFID comme technologie de base pour sa réalisation » (Brock, 2001). Différentes définitions ont été proposées depuis, et l'essor de l'IdO démarre véritablement avec le rapport « Internet Reports -The Internet of Things » de l'ITU (International Telecommunication Union) en 2005 qui présente à la fois les technologies mobilisées par l'IdO et le potentiel du marché. Aujourd'hui, les capacités offertes par l'IdO dans le secteur industriel visent essentiellement quatre usages -le monitoring, le contrôle, l'optimisation et l'autonomie -et les exemples de déploiement se multiplient (Porter et Heppelmann, 2014). Cependant, selon une étude américaine récente du Boston Consulting Group (Rose et al., 2016), nombre d'industriels ne perçoivent pas encore l'étendue des opportunités offertes par l'industrie 4.0 ou n'en font pas un impératif même si ils en reconnaissent le potentiel. Les raisons profondes de ces différentes attitudes, qui oscillent entre l'adhésion enthousiaste et la réticence craintive, sont dues aux présentations plus orientées vers le « grand public » que vers le milieu industriel.
Pour éclaircir la question, avant de mener une étude de terrain plus approfondie, nous avons analysé les résultats proposés par des enquêtes internationales récentes et nous proposons ici une première méta-analyse. Ces enquêtes, menées sur un total de plus de 6 000 cadres de haut niveau, comportent deux familles de questions : celles concernant les facteurs incitant à l'adoption de l'IdO et celles concernant les barrières limitant leur adoption. Dans l'analyse menée dans cet article, nous traitons ces deux familles de façon indépendante et calculons donc un ordre consensus pour chaque ensemble de modalités associé à ces deux familles. La première (facteurs incitatifs) comporte au total 38 modalités et la seconde (facteurs de réticence) 45 modalités.
Codage préalable des modalités
D'un point de vue méthodologique, les enquêtes ayant été menées indépendamment, un codage des modalités de réponses a été nécessaire pour construire une base d'analyse homogène. Ce codage est basé ici sur les concepts déployés dans la construction des « Business Models » (BM) en sciences de gestion. De façon générale, un BM repose sur le triptyque création de valeur (l'offre), délivrance de valeur (le service aux clients) et capture de valeur (les revenus) (Teece, 2010). A partir de ces trois éléments de base, différentes architectures à la fois descriptives et explicatives ont été proposées et nous retenons ici celle proposée par Osterwalder et Pigneur (2010) qui est reconnue comme l'une des plus complètes. Elle est organisée autour de neuf blocs qui sont liés entre eux par des relations identifiées : Ces blocs contiennent eux-mêmes des sous-blocs que nous ne détaillons pas ici. Nous avons utilisé cette structuration pour classer l'ensemble des modalités de réponses proposées dans les différents questionnaires. Puis, les modalités classées dans un même sous-bloc et jugées similaires ont été requalifiées en une seule modalité. Par exemple, les modalités « réaliser de la croissance sur des marchés connexes » et « adresser de nouveaux clients » ont été requalifiées en la modalité « pénétrer un nouveau marché » du bloc (2) « segment de clientèle ».
Résultats
A titre indicatif, la taille des arborescences de recherche est pour les modalités d'incitation de 13 742 sommets et pour les modalités de frein de 418 140 sommets.
Pour cette première analyse, nous avons retenu pour chaque ordre médian ses deux extré-mités (modalités pas ou peu dominées et modalités très dominées). Cependant, en pratique, il était nécessaire de définir un indice qui puisse guider cette dichotomie. Nous n'avons pas trouvé de question équivalente dans la littérature et nous avons donc développé un nouvel indice de contribution des modalités à l'ordre qui se définit comme suit. Pour chaque modalité o i de l'ordre médian O M , sa contribution C(o i ) est égale à la somme des poids des arcs sortants de o i ayant pour extrémités des sommets dominés (à droite de o i dans O M ) moins la somme des poids des arcs dominant o i , ayant des sommets d'origine à gauche de
Plus il y a d'arcs sortant à droite plus la modalité est dominante et inversement plus il y a d'arcs venant de gauche plus la modalité est dominée. Les modalités qui sont associées à peu d'arcs dans le graphe G ou qui en ont autant venant de part et d'autre sont plus « neutres » eu égard à la force des opinions exprimées et sont donc situées au milieu de l'ordre.
Pour l'adoption de l'IdO, les modalités les plus dominantes concernent majoritairement deux blocs du BM : le modèle de revenus (9) avec les modalités « améliorer la rentabilité », « nouveaux revenus » et les ressources et compétences (6) avec les modalités « collecter de nouvelles données », « optimiser la productivité des actifs », « améliorer l'efficacité opération-nelle des ressources ». S'y ajoute le bloc structure de coût (8) avec la modalité «réduire les coûts récurrents ou opérationnels». Ces résultats vont à rebours des discours souvent relevés dans la communication professionnelle ainsi que dans certaines conclusions de travaux théo-riques récents sur les Business Models qui soutiennent que la proposition de valeur est le bloc le plus impacté par l'introduction de l'IdO au sein des activités de l'entreprise (Arnold et al., 2016;Dijkman et al., 2015). Cette priorité ne ressort pas de notre méta-analyse : les modalités portant sur la création de valeur (e.g. « créer une nouvelle proposition de valeur », « changer de proposition de valeur », etc) se retrouvent à la fois en fin de l'ordre médian associé aux facteurs incitatifs de l'IdO et en fin de celui associé aux difficultés d'adoption. En revanche, l'impact de l'IdO sur la transformation à travers la ré-ingénierie des processus business, égale-ment soulignée dans la littérature (Ferretti et Schiavone, 2016), est confirmé par l'analyse (e.g. « améliorer l'efficacité opérationnelle des ressources », « optimiser la productivité des actifs »).
Pour les barrières rencontrées, le bloc (7) partenariats et environnement est le moins dominé avec les modalités « faiblesse de l'infrastructure publique », « absence de standard ouvert pour la technologie », « incertitude sur la stabilité des partenaires ». Ainsi, les entreprises conçoivent bien que l'IdO contribue à améliorer leurs revenus et leurs ressources et compé-tences, mais à condition que les interactions avec les parties prenantes externes soient plus efficaces.
Conclusions
Dans cet article nous avons proposé une nouvelle démarche de méta-analyse permettant d'extraire un ordre consensus sur les modalités évaluées dans un ensemble d'enquêtes d'opinion conduites indépendamment. Cet ordre permet de refléter les principales tendances exprimées et, en particulier, de distinguer les modalités dominantes dans l'ensemble des enquêtes de celles qui sont dominées, c'est-à-dire qui sont peu souvent préférées à d'autres. La recherche de l'ordre consensus a été posée ici comme une recherche d'un ordre médian des sommets dans un graphe orienté pondéré qui modélise les comparaisons par paire des modalités sur l'ensemble des enquêtes. Ce problème se ramenant à un problème NP-difficile nous avons adapté une approche de Branch & Bound pour le résoudre. Nous avons nettement amélioré un programme existant pour l'appliquer à un graphe orienté relativement peu dense, par rapport à un graphe orienté complet 1 . Il permet de construire un ordre optimal à l'aide d'arborescences de plusieurs centaines de milliers de sommets et s'avère bien adapté à la méta-analyse d'enquêtes où la taille des échantillons interrogés peut être grande mais où l'ensemble des modalités à ordonner ne dépasse pas quelques dizaines.
Avec peu de travaux préalables sur lesquels nous appuyer, pour commencer l'analyse, nous avons partitionné initialement l'ensemble des modalités de réponse en deux familles (facteurs incitatifs / facteurs de réticence) que nous avons traitées indépendamment. Cependant, les questions sur les « corrélations » entre ces différents facteurs se posent naturellement et nous prévoyons de les analyser prochainement. Elles soulèvent des problèmes statistiques non triviaux puisque nous ne disposons pas des réponses individuelles mais des réponses agrégées à l'échelle de chaque enquête.
Les premiers résultats obtenus concernant l'analyse des motivations et les freins à l'intégration de l'Internet des Objets dans les entreprises sont particulièrement intéressants car ils questionnent des hypothèses récentes de la littérature en sciences de gestion sur l'impact de l'IdO sur les « Business Models ». Par la mise en lumière de résultats contre-intuitifs et à rebours de la théorie, cette nouvelle approche de méta-analyse comporte des implications

Introduction
C'est à travers des messages du type « La rédaction vous conseille », « Lire aussi », « Sur le même sujet », « Vous pourriez également être intéressé par », que les portails d'actualités en ligne proposent aux utilisateurs des sélections personnalisées de contenus pouvant les intéresser afin de les assister dans leur exploration des articles d'actualités. Dans ce domaine d'application, les systèmes de recommandation font face à un flux abondant et continu d'actualités qu'il faut collecter, analyser et mettre à disposition des utilisateurs selon leurs préférences et dans les plus brefs délais. Par ailleurs, la recommandation des articles d'actualité diffère des domaines d'application traditionnels des systèmes de recommandation qui se caractérisent par une parcimonie de données reflétant les comportements des utilisateurs à cause de l'inaccessibilité de certains produits (produits non disponibles ou ayant un coût d'acquisition élevé). En effet, les articles d'actualité étant gratuits et accessibles à tous les utilisateurs, les approches de recommandation à adopter doivent être capables de prendre en considération un flux abondant d'interactions entre les utilisateurs et le contenu. À titre d'exemple, sur la plateforme de recommandation de Plista 1 , le nombre d'événements observés par seconde peut dépasser les 5000 et le temps de réponse maximal pour fournir une recommandation est limité à 100ms. Dans ce contexte, il est nécessaire qu'un système de recommandation adopte une approche temps-réel, capable de monter en charge et proposant des recommandations dynamiques qui s'adaptent rapidement aux comportements observés chez chaque utilisateur. De plus, vu la nature non structurée du contenu traité, il est judicieux d'adopter des approches sémantiques capables d'extraire les concepts et les thématiques invoquées dans les articles d'actualité afin de mieux cerner les préférences et les intérêts des utilisateurs.
Les contraintes fortes de la problématique de la recommandation continue, temps réel et à large échelle des articles d'actualité ont conduit à une large adoption d'approches non personnalisées et non sémantiques fondées sur critères de popularité et de nouveauté  pour leurs faibles complexités. Plusieurs approches récentes se focalisent sur les technologies de traitement des données massives (p.ex. Apache Spark 2 et Flink 3 ) afin d'assurer le passage à l'échelle  sans prendre en considération l'aspect séman-tique du contenu ni les préférences personnelles de chaque utilisateur. D'autres travaux ont intégré les connaissances extraites à partir du contenu dans le processus de recommandation pour mieux assimiler les facteurs et les thématiques qui influencent les attitudes du consommateur et améliorer ainsi la qualité des suggestions (Capelle et al., 2013). Néanmoins, ces travaux ne tiennent pas compte des contraintes liées à la recommandation en ligne en temps réel. En effet, les approches proposées sont généralement évaluées hors-ligne sur des jeux de données statiques et limités ne reflétant pas les conditions réelles de ce domaine d'application nécessi-tant une gestion continue du flux de données entrant et une capacité à générer en temps réel et à large échelle des recommandations personnalisées et dynamiques.
C'est dans ce contexte que s'inscrit ce travail dont l'objectif est de proposer une approche de recommandation personnalisée d'articles d'actualité traitant l'aspect sémantique du contenu et respectant les contraintes temps réel et de passage à l'échelle du domaine d'application. Notre approche est basée sur l'hybridation de plusieurs approches personnalisées et non personnalisées afin d'éviter le problème de démarrage à froid. Pour valider l'intérêt de la proposition, notre système de recommandation a été évalué pendant le challenge NEWSREEL 2017 dont l'objectif est de faire la recommandation d'articles d'actualité en temps réel et à large échelle pour un ensemble de magazines et de journaux en ligne.
Le reste de cet article est organisé comme suit. La section 2 détaille l'approche proposée ainsi que les composantes du système développé. La section 3 présente les différentes expé-rimentations menées ainsi que les résultats obtenus. Enfin, nous concluons cet article avec un résumé de la contribution et des travaux futurs.  . Les connaissances ainsi extraites serviront par la suite à modéliser les intérêts des utilisateurs (figure 1 (3)) en analysant les contenus des articles avec lesquels ils ont interagi.
Le traitement des données
Les algorithmes de recommandation
Pour faire face aux défis de personnalisation des recommandations et du démarrage à froid, nous proposons une approche hybride par commutation (Burke, 2002) qui sélectionne, selon le cas à traiter, exclusivement une de ses deux approches sous-jacentes. La première approche, nommée PN, est non personnalisée dans la mesure où elle se base sur les critères de nouveauté et de popularité pour évaluer la pertinence d'un article d'actualité. Cette approche est adoptée pour faire face au problème de démarrage à froid et qui se présente dans les cas où l'utilisateur actif est inconnu ou lorsque l'article considéré est nouveau et n'a pas encore été lu ou évalué. Par contre, dans les cas où les données sur les l'utilisateur ou l'article ne sont pas parcimonieuses, l'approche a recours au filtrage par similarité de contenu. Dans ce travail, deux variantes du filtrage par contenu ont été évaluées. La première, nommée KBP, modélise l'article d'actualité avec ses mots-clés alors que la deuxième, nommée NeBP, n'a recours qu'aux entités nommées mentionnées dans le texte.
La proposition de l'approche non personnalisée P N est motivée par des études récentes portant sur la recommandation d'actualité et affirmant la pertinence des critères de popularité et de nouveauté comme indicateurs de l'intérêt que susciterait un article d'actualité chez le consommateur (Kille et al., 2016). Pour estimer la pertinence P N (a) d'un article d'actualité a en fonction de sa nouveauté N (a) et sa popularité P (a), nous proposons l'équation 1 qui adopte une forme exponentielle (Ding et Li, 2005) pour modéliser la décroissance progressive de la popularité au fil du temps et qui a été observée par des travaux existants (Kille et al., 2016). La modélisation de la nouveauté est motivée par le fait que les utilisateurs sont attirés par l'actualité tandis que le facteur popularité permet de mettre en avant les articles ayant suscité un grand intérêt brusque (i.e. un grand nombre de lectures) rendant leurs probabilités de lecture supérieures à celles des articles supposés intéresser l'utilisateur.
Dans la formulation proposée, P (a) et N (a) représentent respectivement le nombre de lectures de l'article a et son âge (en jours), tandis que K est un paramètre spécifiant le taux de dégradation de la popularité (deux à quatre jours (Kille et al., 2016)). Cette formulation permet ainsi de sélectionner les articles ayant le meilleur compromis entre nouveauté et popularité et a le mérite d'être adaptée au contexte de la recommandation temps-réel grâce à sa complexité constante (O(1)).
Les approches KBP et NeBP proposent des recommandations personnalisées basées sur le contenu des articles d'actualité puisqu'elles suggèrent à un utilisateur donné les articles les plus similaires à ceux qu'il a déjà lus dans le passé. Cependant, les deux approches diffèrent au niveau de la représentation des articles et donc celle des profils des utilisateurs. En effet, KBP a recours à un vecteur de termes pondérés par leurs importances pour représenter un article ou l'historique des lectures d'un utilisateur alors que NeBP se limite aux entités nommées. Dans les deux variantes, nous adoptons la mesure de pondération TF-IDF (Pazzani et Billsus, 2007) pour quantifier l'importance d'un mot-clé ou d'une entité nommée dans un article d'actualité. Dans ce contexte, le profil de l'utilisateur s'enrichit au fur et à mesure des articles lus par les termes ou les entités nommées qui y sont présents. Enfin, nous avons recours à une variante de la mesure de similarité cosinus 4 pour calculer la concordance entre un article candidat et le profil de l'utilisateur actif et filtrer ainsi les actualités potentiellement capables de l'intéresser. Le calcul de similarité entre deux documents d 1 et d 2 ou entre un document d et un utilisateur u est proportionnel à la taille de leurs vecteurs représentatifs (resp. O(|d 1 |+|d 2 |) et O(|d|+|u|)). Dans ce travail, le choix du cosinus comme mesure de similarité a été effectué sur la base de leur complexité et sa popularité dans le domaine de la recherche d'information. Son apport par rapport à d'autres mesures devrait être validé par des expérimentations comparatives contrôlées hors ligne.
Expérimentations
Les expérimentations ont été menées sur la plateforme de recommandation d'articles d'actualité de Plista 5 dans le cadre de la compétition NEWSREEL 6 de la conférence CLEF 2017. Cette plateforme collecte les contenus ainsi que les données d'usage sur les sites des éditeurs d'actualités partenaires, les partage en temps réel avec les systèmes de recommandation participants qui doivent en retour fournir des suggestions ciblées en moins de 100ms. La pertinence des systèmes participants est alors évaluée en fonction du nombre de consultations que leurs Le système de recommandation implémentant l'approche hybride NeHyb a été déployé sans connaissances à priori. Au fil du temps, il a collecté et analysé des données émanant de 82336 articles et de 3755547 utilisateurs. Ces traitements ont été menés sur une machine dotée de deux processeurs Intel Xeon E5-26xx avec une fréquence de 2GHz chacun, une mémoire cache de 4Mo, 4Go de RAM et 94Go d'espace de stockage. L'expérimentation nous a permis de valider l'intérêt de l'approche proposée puisque le système déployé a été capable de traiter le flux abondant et véloce des données tout en respectant la contrainte temps réel. En effet, le système a pu traiter jusqu'à 4000 requêtes par minute avec un temps de réponse moyen de 47ms, une occupation moyenne du processeur de 6% et une occupation moyenne de l'espace mémoire de 240Mo.
Conclusions et perspectives
Dans ce travail, nous avons proposé et évalué en conditions réelles une approche hybride, sémantique et personnalisée pour la recommandation des articles d'actualité en ligne. L'approche est capable de respecter les contraintes de la recommandation en temps réel tout en permettant le passage à l'échelle grâce à sa faible complexité et à sa capacité à traiter les flux 7. http ://aksw.org/Projects/FOX.html

Données Ritter Données MSM2013 R(%) P(%) F(%) R(%) P(%) F(%)
Ritter ( Rappel -R(%), Précision -P(%), Mesure F -F(%).
Extraction de Localisation et Apprentissage
La combinaison de l'outil Ritter et de Stanford-NER filtré par DBpedia donne la meilleure mesure F. Pour MSM2013, la mesure F augmente de 69 % à 75 %. Lorsque l'on s'intéresse à une forte précision, c'est la combinaison de Ritter avec le filtrage BDPedia qui est la plus efficace (dernière ligne) alors que pour le rappel, il s'agit de la combinaison de Ritter avec Gate (avant dernière ligne).
Prévoir qu'un tweet contient un nom de lieu n'est pas simple car les tweets sont générale-ment écrits dans un langage pseudo-naturel. Les outils usuels de traitement automatique de la langue rencontrent alors des difficultés. Nous avons proposé un ensemble de caractéristiques pour représenter les tweets et nous avons étudié la pertinence de cette représentation dans un modèle prédictif basé sur un apprentissage automatique. Ces caractéristiques sont précisées et détaillées dans (Hoang et Mothe, 2018) Le modèle appris permet donc de prédire si un nouveau tweet contient une localisation ou non. Plus de détails sur cette approche sont disponibles dans (Hoang et Mothe, 2018). Dans nos travaux futurs, nous souhaitons analyser comment les localisations pourraient aider à la prédiction de la diffusion des tweets. Nous pourrions ainsi étanedre les travaux présentés dans Hoang et Mothe (2017).

Introduction
Les lacs de données (data lakes) ont été introduits par Dixon (2010). Ils proposent une manière, née avec les mégadonnées (big data), de stocker dans leur format natif des données volumineuses, variées et diversement structurées, en vue de les analyser (reporting, visualisation, fouille de données...).Ce concept s'oppose à celui des entrepôts de données, très intégrés et orientés sujet, mais qui ont l'inconvénient de diviser les données en silos étanches (Stein et Morrison, 2014). Toutefois, tout le monde s'accorde pour dire qu'un lac de données doit être bien conçu sous peine de devenir un marécage (data swamp) inexploitable (Alrehamy et Walker, 2015) ; c'est à dire qu'il doit permettre le requêtage des données (sélection/restriction) avec un bon temps de réponse et pas seulement leur stockage et leur accès « clé-valeur ». En revanche, les solutions pour y parvenir sont peu ou prou inexistantes dans la littérature et relèvent à l'heure actuelle de pratiques industrielles peu divulguées.
C'est pourquoi Pathirana (2015) a proposé un modèle conceptuel de métadonnées permettant l'indexation et l'interrogation efficace d'un lac de données patrimoniales. Ce modèle multidimensionnel est proche des modèles en flocons en usage dans les entrepôts de données, mais ne concerne que les métadonnées et non le corpus de documents lui-même. Il a été instancié au niveau physique dans différents systèmes de gestion de bases de données (SGBD) NoSQL. Cependant, le type de schéma employé est très difficile à faire évoluer lorsque ceux des sources de données évoluent ou que de nouvelles sources sont à prendre en compte, alors que c'est un point crucial dans la gestion d'un lac de données.
En conséquence, nous proposons dans cet article de : 1) remplacer le modèle multidimensionnel de Pathirana (2015) par un modèle ensembliste, en l'occurrence un data vault (Linstedt, 2011), qui est un modèle de données permettant des évolutions de schémas aisées et qui n'a, à notre connaissance, jamais été employé dans le contexte de la gestion de métadonnées ; 2) vérifier la faisabilité, d'une part, et l'efficacité de ce modèle en termes de réponse aux requêtes sur les métadonnées (à la manière d'un index), d'autre part, car il induit de nombreuses jointures. Pour cela, nous traduisons notre metadata vault conceptuel en différents modèles logiques (relationnel et orienté document) et physiques (PostgreSQL et MongoDB), ce qui permet également de comparer l'efficacité respective des deux modèles physiques.
État de l'art
Les lacs de données sont généralement construits pour intégrer de très grands volumes de données non structurées de manière rapide. Ils ne se limitent toutefois pas à une technologie de stockage (la plupart du temps HDFS -Hadoop Distributed File System), mais proposent un nouvel écosystème de données permettant rapidement, à la demande, de croiser des données, sans besoin de prétraitements coûteux tels que la construction d'un entrepôt de données. Les données sont immédiatement accessibles, contrairement, de nouveau, aux entrepôts de données qui sont rafraîchis périodiquement via une phase d'ETL (extraction, transformation, chargement) qui peut être coûteuse (Miloslavskaya et Tolstoy, 2016).
La modélisation ensembliste (ensemble modeling) est une approche utilisée dans l'industrie, qui vise à renormaliser les entrepôts de données afin de permettre une meilleure évo-lutivité, tant en termes de données que de schéma (Rönnbäck et Hultgren, 2013). Les deux approches qui s'en dégagent sont l'anchor modeling (Regardt et al., 2009) et les data vaults (Linstedt, 2011). Elles sont en fait très proches (Rönnbäck et Hultgren, 2013), avec une évo-lutivité un peu plus aisée et une évolution de schéma non destructive pour l'anchor modeling, mais un plus grand nombre d'objets à gérer en raison d'une modélisation en sixième forme normale (6NF), ainsi que des procédures de maintenance des attributs temporels (timestamps) non automatisées. Les data vaults sont plus proches de la modélisation multidimensionnelle traditionnelle et sont supportés par un plus grand nombre d'outils, ce qui a guidé notre choix.
Un data vault est défini au niveau logique relationnel comme un ensemble lié de tables normalisées orienté détail et suivi d'historique, qui prend en charge un ou plusieurs domaines fonctionnels d'une organisation. C'est une approche hybride englobant la 3NF et le schéma en étoile (Linstedt, 2011). Plus concrètement, un data vault est composé des éléments principaux suivants. Un centre (hub) est une entité de base qui représente un concept métier (un niveau hiérarchique de dimension dans un entrepôt de données classique, client ou produit, par exemple). Il contient principalement une clé (business key). Un lien (link) matérialise une association entre deux centres ou plus. Il correspondrait à une entité de faits dans un entrepôt classique. Un satellite contient des attributs relatifs à un centre ou un lien.
Modèle de métadonnées en data vault
Afin de proposer un cas d'utilisation susceptible d'illustrer notre propos et de servir de preuve de concept, nous avons exploité le corpus de données issu du projet TECTONIQ, qui vise à valoriser le patrimoine industriel textile de Lille Métropole (Kergosien, 2017). Ce corpus rassemble des données hétérogènes, fournies par différentes sources : descriptions de bâtiments industriels (documents XML orientés données), articles de presse liés à l'industrie textile (documents XML orientés documents), photos et plans de bâtiments et monuments liés à l'industrie textile (images JPEG) et livres d'histoire de France du domaine public (documents PDF). Il doit permettre divers types d'analyses. C'est pourquoi le stockage du corpus est effectué dans un lac de données et que Pathirana (2015) a proposé une modélisation multidimensionnelle de ses métadonnées. Nous nous proposons dans cet article de rendre ces métadonnées évolutives face à l'ajout de nouvelles sources de données, en détournant en quelque sorte les concepts des data vaults (centres, liens et satellites) pour stocker les métadonnées.
La Figure 1 illustre notre modèle conceptuel de métadonnées en data vault. Les rectangles arrondis bleus y représentent les centres, les rectangles gris les satellites et l'hexagone vert un lien. Les associations entre centres et liens sont toutes de cardinalité « plusieurs à plusieurs » pour assurer la plus grande généralité, tandis que les associations entre centres ou liens et satellites sont de cardinalité « un à plusieurs ».
FIG. 1: Modèle de métadonnées en data vault
Afin d'identifier les centres, nous ciblons les métadonnées indicatrices des caractéristiques de chaque document en entrée, qui sont susceptibles de constituer des clés. Ainsi, nous sélec-tionnons le titre (Hub_Title), la localisation (Hub_Location), la date (Hub_Date) et la catégorie des documents (Hub_Category). À chacun de ces centres, nous associons ensuite un satellite qui contient les attributs descriptifs du centre. Le centre Hub_Category est associé à quatre satellites qui forment une classification correspondant à chacune des source de données dont nous disposons. Les attributs descriptifs de ces satellites sont spécifiques à chacune des sources. Le lien document (Link_Document) permet d'associer tous les centres. Finalement, puisque notre modèle concerne seulement des métadonnées, chaque entité (centre, lien, satellite) est décrite par une référence directe (Source) à un document (fichier physique) du lac de données. Grâce à cette modélisation, toute nouvelle source de données ou évolution de schéma au sein du corpus de données peut être prise en charge par l'ajout de satellites, pour les cas les plus simples, voire de centres et de liens. De plus, les entités qui deviendraient obsolètes sont simplement identifiées grâce à leur horodatage (attribut Datetime).
Pour montrer que notre modèle conceptuel peut s'adapter à différents contextes, et pour les comparer, nous le déclinons en deux types de modèles logiques et physiques : un modèle relationnel avec PostgreSQL et un modèle NoSQL orienté document avec MongoDB. Le modèle relationnel des métadonnées est traduit du modèle conceptuel (Figure 1 Après avoir inséré les métadonnées dans le format natif des SGBD choisis via des scripts, nous avons mesuré le volume de stockage nécessaire. Nos mesures tiennent compte des index, de l'espace mémoire non utilisé et de celui qui est libéré lors de la suppression ou du déplacement des données. Le volume des métadonnées générées pour 245 fichiers est faible (moins d'1 Mo) dans les deux cas. Par ailleurs, MongoDB nécessite systématiquement moins d'espace que PostgreSQL. Cela est justifié par l'utilisation du format JSON, qui est très léger, dans MongoDB. En revanche, dans PostgreSQL, chaque table est stockée comme un vecteur de pages de taille prédéterminée (8 Ko), ce qui induit des pages non totalement remplies.
Dans le but de mesurer la performance de notre modèle de métadonnées, nous avons formulé cinq requêtes de type projection/restriction et de complexité croissante en termes de nombre de centres impliqués (et donc de jointures via le lien Link_Document), que nous appliquons sur les deux modèles physiques.  (5) est plus complexe que les précédentes. En effet, il n'est pas possible de déter-miner dynamiquement le satellite Catégorie à utiliser. Il faut donc exécuter la requête (5) en deux étapes : la première pour déterminer la catégorie et la seconde pour récupérer le reste des informations sachant la catégorie. Ces deux opérations induisent des temps de réponse très supérieurs à ceux des requêtes (1) à (4). MongoDB se montre une nouvelle fois plus efficace (trois fois plus rapide en moyenne) que PostgreSQL. Cette différence de temps d'exécution s'explique par la réécriture en interne par PostgreSQL d'une sous-requête induisant une jointure coûteuse, alors que MongoDB permet l'union rapide des collections. 
Conclusion
Partant de la modélisation des métadonnées d'un lac de données sous forme multidimensionnelle et constatant que l'évolution du schéma n'était pas garantie, nous avons proposé une modélisation en data vault des métadonnées. La traduction de notre modèle conceptuel de métadonnées en modèles logiques et physiques et des expériences menées sur le corpus TEC-TONIQ ont permis de montrer la faisabilité de notre approche en termes de volume de stockage et de temps de réponse aux requêtes formulées sur les métadonnées. La comparaison de deux modèles physiques (PostgreSQL et MongoDB) a également fait apparaître la supériorité des modèles logiques orientés document pour stocker ce type de métadonnées.
Les perspectives ouvertes par ce travail incluent de tester la robustesse du modèle de mé-tadonnées lors d'un passage à l'échelle des données sources, d'ajouter des sources de données pour vérifier la pertinence de la modélisation en data vault et de tester des requêtes plus complexes que des projections/restrictions. Il serait également intéressant de comparer l'efficacité de la modélisation en data vault et l'anchor modeling, les tenants de cette dernière argumentant que la modélisation en 6NF permet malgré tout de bons temps de réponse, grâce à la technique d'élimination de jointures employée dans les optimiseurs modernes. Finalement, différentes modélisations alternatives des métadonnées, indépendamment des techniques de modélisation employées, pourraient être envisagées et comparées. Les schémas des documents XML du corpus pourraient aussi être extraits automatiquement pour enrichir les métadonnées.

Introduction
Dans cet article, nous nous intéressons à l'optimisation d'une approche récente de clustering de réseau appelée la recherche de liens conceptuels. Il s'agit d'une nouvelle approche qui effectue des clusters de liens en exploitant la structure et les attributs des noeuds pour identifier les liens fréquents entre des groupes de noeuds au sein desquels les attributs sont communs. Ce travail est motivé par le fait que l'algorithme d'extraction d'origine effectue la recherche des clusters de liens de manière séquentielle, sans prendre en compte les parallélisations possibles.
Ainsi dans cet article, nous présentons PALM (Stattner et al., 2017), un algorithme parallèle qui vise à améliorer l'efficacité du processus d'extraction des liens conceptuels en explorant simultanément plusieurs zones de l'espace de recherche. Pour cela, nous démontrons que l'espace des solutions forme un treillis de concepts. Nous proposons ensuite une approche qui explore en parallèle les branches du treillis tout en réduisant l'espace de recherche en s'appuyant sur certaines propriétés des liens conceptuels. L'efficacité de l'algorithme est démontré en l'appliquant à un réseau de télécommunications et en comparant les performances avec l'algorithme d'extraction d'origine. Les résultats obtenus montrent un gain significatif sur le temps de calcul.
État de l'art
La recherche de clusters dans les réseaux sociaux, aussi appelée clustering de réseaux, est une des approches les plus répandues de la modélisation descriptive de réseaux. L'objectif est d'identifier des groupes de noeuds qui satisfont certaines propriétés. Les principales familles de méthodes pour le clustering de réseaux sociaux peuvent être classifiées comme suit.
(i) Le clustering basé sur les liens fait référence à une famille de méthodes qui recherche une partition des noeuds, appelé communauté, en tenant uniquement compte de la structure du réseau (Fortunato, 2010;Blondel et al., 2008). L'objectif est de décomposer le réseau en plusieurs communautés, définies comme des groupes de noeuds fortement connectés. Les algorithmes tentent ainsi d'identifier les groupes qui maximisent les liens intra-communautaires tout en minimisant les liens inter-communautaires (Newman, 2006).
(ii) Le clustering hybride est une approche de clustering de réseau qui vise à tenir compte des attributs des noeuds durant la phase d'extraction des clusters (Zhou et al., 2009;Yoon et al., 2011). En effet, dans de nombreuses applications la définition classique d'une communauté ne permet pas de comprendre pleinement les structures étudiées. Ainsi, les techniques de clustering hybrides tentent d'identifier les groupes de noeuds densément connectés, qui possèdent en plus une similitude dans leurs attributs.
(iii) La recherche de liens conceptuels est une nouvelle approche qui exploite les informations disponibles à la fois sur la structure du réseau et les attributs des noeuds, dans le but d'identifier l'ensemble des attributs les plus fréquemment connectés dans le réseau (Stattner et Collard, 2012). De tels groupes fournissent une connaissance sur les attributs qui structurent les liens au sein du réseau.
Liens conceptuels
Un item est une expression logique A = x où A est un attribut et x une valeur. L'item vide est noté ∅. Un itemset est une conjonction d'items, par exemple (A 1 = x et A 2 = y et A 3 = z), qu'on note par simplicité (xyz). Quand un itemset est une conjonction de k items non-vides, on parle de k-itemset.
Soient m et sm deux itemsets. Si sm ⊂ m, on dit que sm est un sous-item de m et que m est un super-itemset de sm. Par exemple sm = xy est un sous-itemset de m = xyz. Nous notons I V l'ensemble des itemsets construits à partir des noeuds de V . Definition 1. Lien conceptuel Posons m 1 et m 2 deux itemsets de I V et V m1 , V m2 , respectivement les ensembles de noeuds dans V qui satisfont m 1 et m 2 . Nous notons E (m1,m2) le cluster de liens connectant des noeuds de V m1 à des noeuds de V m2 , c'est-à-dire :
Le cluster E (m1,m2) est appelé "lien conceptuel" dans la mesure où il ne s'agit pas d'un lien du réseau, mais d'un cluster de liens entre deux groupes de noeuds qui peuvent être vus comme des "concepts", au sens de l'analyse de concepts formels, c'est-à-dire des objets qui partagent des attributs communs. Par exemple, si m 1 est l'itemset cd et m 2 est l'itemset ef j, le lien conceptuel E (m1,m2) = (cd, ef j) inclut tous les liens entre les noeuds qui vérifient cd et qui sont connectés à des noeuds qui vérifient ef j. Nous notons L V l'ensemble des liens conceptuels construits à partir des itemsets de I V .
Le support du lien conceptuel E (m1,m2) de L V , est le pourcentage de liens appartenant à
. Le lien conceptuel entre les itemsets m 1 et m 2 est fréquent si le support de E (m1,m2) est plus grand qu'un seuil de support minimum β donné, i.e. supp(E (m1,m2) ) > β.
Ainsi, nous définissons LC comme l'ensemble des clusters de liens fréquents (les liens conceptuels fréquents) extraits du réseau G.
Definition 2. Sous-lien conceptuel Soient deux itemsets sm 1 et sm 2 respectivement sous-
Propriété 1. Fermeture descendante Si un lien conceptuel est fréquent, tous ses sousliens le sont également. De la même façon, si un lien est non-fréquent, tous ses sur-liens sont également non-fréquents. Preuve. Soient sm 1 et sm 2 respectivement des sous-itemsets de m 1 and m 2 . Les propriétés
Definition 3. Liens conceptuels fréquents maximaux. Soit β un seuil de support donné, nous appelons liens conceptuels fréquents maximaux, tout lien conceptuel fréquent pour lequel il n'existe aucun super-lien qui soit également fréquent. 
Algorithme PALM
L'algorithme d'origine effectuait la recherche des liens conceptuels en 4 étapes. (i) Géné-ration des 1-itemsets candidats (ii) Génération des 1-liens conceptuels fréquents, à partir des 1-itemsets candidats (iii) Génération des t-itemsets candidats à l'itération t > 1 (iv) Génération des t-liens conceptuels à l'itération t > 1, à partir des t-itemsets candidats Les parties (iii) et (iv) sont répétées jusqu'à ce qu'il n'y ait plus de nouveaux candidats générés. Bien que cet algorithme permette effectivement d'extraire les liens conceptuels fré-quents et maximaux, la recherche s'effectue de manière séquentielle. De plus, l'exploration de l'espace de recherche n'est pas totalement optimisée, ce qui rend l'extraction des liens conceptuels particulièrement coûteuse en temps pour de grands jeux de données.
La première amélioration apportée par PALM concerne la phase de création des candidats. Dans le précédent algorithme, la génération des t-itemsets candidats est effectuée en fusionnant les itemsets m 1 et m 2 possédant t-2 items communs et tel que (m 1 , m 2 ) ∈ LI 2 (t−1) ou (m 1 , m 2 ) ∈ RI 2 (t−1) . Or, deux itemsets ont k items en commun si et seulement s'ils partagent au moins un sous-itemset de taille k. Ainsi, en conservant la structure du treillis de concepts nous pouvons explorer les (t-2)-itemsets fréquents et fusionner leurs sous-itemsets respectifs.
A l'étape (iv) de l'ancien algorithme, l'ensemble des liens conceptuels testés est constitué de toutes les combinaisons possibles faites à partir des itemsets de LI cand et des itemsets de RI cand . La fréquence de chacune de ces combinaisons est ensuite évaluée. Cette étape est la plus coûteuse en temps. Afin de réduire le temps de calcul nécessaire, nous réduisons le nombre de liens conceptuels candidats à tester en exploitant la propriété 1 de fermeture.
Enfin, plusieurs étapes de la recherche sont indépendantes et peuvent être parallélisées. En effet dans l'étape (i), nous explorions tous les noeuds du réseau afin d'extraire les 1-itemsets. Dans PALM les noeuds sont répartis sur T threads, chargés chacun de l'extraction des 1-itemsets de sa portion de noeuds, puis de la fusion dans une liste globale. Dans la partie (ii), les nouveaux clusters candidats sont générés par jointure des éléments précédents. Cette étape a également été parallélisée en répartissant le travail sur T threads.
Résultats expérimentaux
Le jeu de données utilisé est un réseau d'appels fourni par un opérateur de téléphonie mobile local. Il représente des abonnés le 1e juin 2009 de 5h à 15h. Les noeuds sont les abonnés et les liens sont les appels passés sur la période. Le réseau a une structure scale-free et est composé d'environ 246 000 noeuds et 510 000 liens collectés sur les 10h d'étude.
Dans une première approche, nous avons étudié l'évolution du temps de calcul (en secondes) avec différents seuils (β = 0.3 and β = 0.4) pour les deux algorithmes (cf Figure 2). Tout d'abord, nous observons que pour les deux seuils utilisés, le temps de calcul est toujours plus petit pour l'algorithme PALM. Nous observons cependant des tendances communes. En effet, le temps de calcul croît linéairement avec la taille du réseau, alors qu'il peut être approché par une fonction de puissance quand le nombre d'attributs augmente. Ces tendances ont été observées pour plusieurs seuils de support.
Pour mieux comprendre l'évolution du temps de calcul, nous nous sommes focalisés sur l'évolution de (a) la pente du temps de calcul et (b) l'exposant de la fonction de puissance. La Si nous nous concentrons sur l'évolution de la pente, nous pouvons observer que pour les deux algorithmes, la pente du temps de calcul croît quand le seuil de support décroît. Cependant contrairement à l'algorithme d'origine, PALM explore plus rapidement l'espace des solutions, en particulier pour de petits seuils de support. Un comportement similaire est observé si nous nous intéressons à l'exposant de la courbe du temps de calcul. Pour aller plus loin, nous avons cherché à comprendre quel était le gain sur le temps de calcul apporté par PALM (cf Figure 4). Dans notre contexte, le gain est défini comme la proportion de temps économisée par rapport à l'algorithme d'origine. Le gain fourni par l'algorithme PALM croît rapidement avec la taille du réseau et semble se stabiliser. En effet, le gain est d'environ 75% et reste stable même avec un nombre de liens élevé. Les mêmes tendances sont observées pour les résultats obtenus selon le nombre d'attributs.

Introduction
L'évaluation par des relecteurs est un processus scientifique par lequel les experts d'une discipline vérifient la qualité du travail de leurs pairs. L'examen et la validation des travaux scientifiques est une pierre angulaire de la recherche scientifique. Le nombre croissant de publications journalières dans un contexte académique de plus en plus compétitif (publier ou périr) et la digitalisation du monde de l'édition légitiment le développement d'outils informatiques d'aide au processus de reviewing.
Lorsqu'un chercheur propose un article à un éditeur, ce dernier est en charge de trouver un certain nombre de reviewers. Le temps nécessaire pour trouver ces reviewers constitue le principal goulot d'étranglement de l'édition scientifique, retardant parfois la date de publication d'un article de plusieurs mois. Nous proposons un outil de recherche d'experts scientifiques nommé Peerus Review 3 . Les experts recherchés sont les reviewers potentiels d'un article-requête émis sous la forme d'un titre et d'un résumé. L'algorithme de recherche d'experts s'appuie sur les données générées par l'entreprise DSRT afin d'estimer la probabilité de chaque scientifique de la base d'être un expert de l'article soumis. Cette estimation est réalisée en deux étapes, la première calculant les similarités 1. https://peer.us/ 2. https://review.peer.us/ 3. Inscription gratuite et démonstration disponible entre la requête et les articles de la base de données, puis la seconde agrégeant ces similarités par un système de votes où chaque article renforce le score de ses auteurs.
Modèle implémenté
On considère un graphe biparti G = (V, E) composé de deux types de noeuds V = V C ∪V D correspondant aux C auteurs et D articles, dont les liens E sont les associations auteur-article. On note A la matrice d'adjacence de G. On dispose en plus d'une matrice d'attributs X pour l'ensemble des articles. La recherche d'experts consiste à générer, étant donné un ensemble (A, X) (voir figure 1) et une requête q composée de texte, les probabilités (ou scores) s = (s 1 , s 2 , ..., s |C| ) des auteurs c = (c 1 , c 2 , ..., c |C| ) d'être experts de cette dernière.
En pratique, la requête est la concaténation du titre et du résumé et est transformée en vecteur par une technique de modélisation du langage. Nous avons testé différentes techniques telles que TF-IDF, LSA, LDA et word2vec. Cette dernière technique est actuellement utilisée et nécessite plus de traitements puisqu'elle ne fournit pas directement de représentation vectorielle pour les articles, mais seulement pour les mots qui les composent. Deux étapes sont ensuite nécessaires pour estimer les scores des auteurs :
-similarité requête-documents : la proximité entre la requête et les articles de la base est estimée. Pour ce faire, on calcule la similarité cosinus pour chaque pair de repré-sentations vectorielles requête-article. Pour word2vec, la distance entre deux articles est calculée grâce à la distance du cantonnier (ou métrique de Wasserstein) décrite dans Kusner et al. (2015) à partir des représentations lexicales de leurs mots 4 . -associations documents-auteurs : on utilise une technique de fusion de données pour attribuer un score à chaque auteur. Celle utilisée dans notre prototype est nommée rangréciproque et consiste dans un premier lieu à classer les articles selon l'ordre décrois-sant de leurs similarité à la requête, puis d'agréger les rangs rank d (q) des articles écrits par chaque auteur selon la formule RR auteur = d∈D(e) 1 rank d (q) où D(e) est l'ensemble des articles écrits par l'auteur e (correspondant à la ligne e de la matrice A) 5 . Ce modèle de votes est l'un des algorithmes confrontés dans Macdonald et Ounis (2006). Dans Balog et al. (2012), les systèmes de votes sont vus comme une altération du second modèle génératif présenté dans Balog et al. (2006) où la probabilité d'un auteur d'être un expert étant donnée une requête est estimée en utilisant la formule de Bayes : P (e|q) rang = P (q|e)P (e) = d∈D(e) P (q|d)P (d|e)P (e), P (q|d) étant la similarité entre l'article d et la requête q, P (d|e) étant la force d'association entre l'article d et l'auteur e et P (e) est la probabilité a priori de l'auteur e d'être un expert (souvent considérée comme uniforme).
R. Brochier et al. -l'utilisateur doit pouvoir vérifier, par lui-même, la véracité des résultats. Pour cela, l'intégralité des informations liées aux articles des auteurs est présentée (titre, résumé, noms d'auteurs, affiliation et date de publication). -Peerus Review doit pouvoir collecter la satisfaction des utilisateurs vis-à-vis des résul-tats retournés. C'est pourquoi l'utilisateur doit valider ou invalider un auteur qui lui est présenté afin de pouvoir consulter le suivant. Cela permet d'estimer la qualité de l'algorithme et l'utilisateur peut requérir un classement moyennant le vecteur requête avec les vecteurs des articles qu'il estime satisfaisants (option «recompute»).
Travaux futurs et conclusion
Notre prototype permet de rechercher des reviewers potentiels à un article soumis grâce à un système de votes reposant sur une technique de modélisation du langage indépendamment sélectionnée. Dans des travaux futurs, nous exploiterons la topologie des relations auteursarticles à travers des modèles de propagation dans les graphes (Serdyukov et al. (2008)). À plus long terme, nous explorerons les techniques d'apprentissage de représentations plongeant dans un même espace experts et articles scientifiques (Van Gysel et al. (2016)).
Références
Balog, K., L. Azzopardi, et M. De Rijke (2006). Formal models for expert finding in enterprise corpora. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pp. 43-50. ACM.

Introduction
Dans cet article, nous traitons de la prédiction des valeurs futures d'une série temporelle d'intérêt. Nous considérons un horizon de prédiction arbitraire, et un contexte multivarié, où les valeurs historiques de plusieurs séries temporelles sont disponibles en entrée. Vu le contexte particulier du Défi EGC 2018, nous nous intéressons à la prédiction du rayonnement solaire. La prédiction des valeurs futures de variables météorologiques a notamment un intérêt pratique quand il s'agit d'optimiser des sources d'énergie renouvelables (Barbounis et al., 2006). L'approche classique à la prédiction météorologique utilise des simulations physiques initialisées par des relevés sur le terrain (Lynch, 2008). De manière alternative, dans cet article nous adoptons une approche basée sur l'apprentissage automatique, complètement agnostique de la dimension physique. Plus précisément, la tâche de prédiction est vue comme un problème de régression, avec pour variable cible le rayonnement solaire à un horizon de prédiction donné. Le vecteur d'entrée peut potentiellement utiliser l'ensemble des valeurs historiques (i.e. observées jusqu'à l'instant présent). Ce choix engendre les sous-problèmes suivants :
-pré-traitement : les valeurs manquantes affectent généralement les modèles d'apprentissage. L'inspection et la correction préalable des données d'apprentissage est néces-saire.
-sélection de variables : dans le contexte des séries temporelles, il faut trouver un compromis entre le modèle naif n'utilisant que les valeurs présentes comme vecteur d'entrée, et le modèle exhaustif qui utilise l'historique complet. Ce dernier contient intuitivement plus d'information, mais l'estimation de paramètres trop nombreux engendre un excès de variance et de complexité. Après la revue des travaux existants dans la section 2, nous présentons les données du Défi EGC 2018 dans la section 3.1 1 . Notre pré-traitement est introduit dans la section 3.2. Une procédure de sélection de variables est ensuite décrite dans la section 3.3. Des modèles de ré-gression génériques sont ensuite entrainés avec les données résultant de ces processus selon le protocole décrit en section 4. Nos résultats expérimentaux permettent d'évaluer la performance respective de deux modèles de régression, de la procédure de sélection de variables, ainsi que de l'impact des sites géographiques où les données ont été enregistrées.
Travaux Existants
Considérons un ensemble de séries temporelles météorologiques, x désignant l'une d'entre elles. Ces séries sont indexées par un pas de temps t, de sorte que x t est la valeur d'une série donnée observée à un temps donné. Le pas de temps est supposé fixe, i.e. le laps de temps entre x t et x t+1 est constant pour tout t. Par commodité et cohérence vis à vis de la littérature du domaine, nous définissons la prédiction à l'horizon l pour la série temporelle x comme la tâche de prédire x t+l−1 connaissant les valeurs de la série jusqu'à x t−1 . Prédire à l'horizon 1 revient alors à prédire x t connaissant la série jusqu'à x t−1 . En d'autres termes, par commodité, l'instant présent est supposé être t − 1 dans notre article. Étant donné un ensemble de D séries temporelles {x d } d∈1...D , parmi lesquelles nous isolons une série d'intérêt avec l'index δ ∈ 1 . . . D, la tâche de prédiction de x δ,t+l−1 étudiée a un caractère multivarié via la connaissance de x d,t−1 pour tout d.
Les modèles ARIMA sont l'approche classique pour le traitement des séries temporelles. Ils sont optimisés selon l'horizon 1. Ils mélangent une partie auto-régressive (i.e. processus AR(p)) :
et une moyenne mobile (i.e. processus M A(q)) :
Dans l'équation (1), σ t est un bruit gaussien. Intuitivement, la prédiction réalisée selon l'équation (1) est une combinaison linéaire des p dernières valeurs observées (i.e. x t−1 à x t−p ), additionnée d'un résidu indépendamment distribué. La moyenne mobile permet d'introduire une dépendance entre résidus. Sous réserve de conditions formelles, les deux processus sont stationnaires. Cette propriété a pour conséquence importante que leur espérance est constante
Moufia
Possession Saint André Saint Leu Saint 55.45 TAB. 1 -Coordonnées GPS (latitude, longitude) des sites où les données ont été enregistrées. selon t. ARIMA permet de combiner et d'intégrer ces processus, et ainsi d'assouplir la condition de stationarité.
Les modèles GAM (Generalized Additive Models) (Hastie et Tibshirani, 1990) proposent plutôt une modélisation additive de fonctions continues. Ils ont déjà été utilisés dans le contexte de l'analyse de la pollution de l'air (Dominici et al., 2002), que l'on peut juger proche de notre travail. En théorie, l'optimisation est alors réalisée selon tous les horizons de prédiction possibles. Toutefois, GAM ne fournit pas de solution pour la sélection de variables dans un contexte multivarié. En effet, la plupart des modèles d'apprentissage automatique sont basés sur un vecteur d'entrée numérique et de taille fixe.
Diverses stratégies peuvent être utilisées pour adapter des réseaux de neurones à notre tâche de prédiction multivariée. Une approche de type divide-and-conquer basée sur des modèles MLP (Multi-Layer Perceptrons) a été proposée dans . Tandis que des méthodologies de sélection de variables sont bien établies pour les modèles ARIMA (e.g. BoxJenkins (Anderson, 1976;Box et al., 2015), Hyndman and Khandakar (Hyndman et Khandakar, 2007), parmi d'autres), aucune méthodologie équivalente ne s'est imposée pour les réseaux de neurones. Une méthode de sélection bayésienne pour les MLP est décrite dans , mais elle requiert l'apprentissage avec un vecteur délibérément redondant, dans lequel la sélection est réalisée a posteriori. De manière à gérer la variété implicite aux angles dans WD (i.e. un angle de 350 • est plus similaire à un angle de 10 • que de 120 • ), nous prenons le cosinus et le sinus de cette dernière, formant ainsi respectivement les variables UnitX et UnitY. La prise en compte de la forte tendance journalière et saisonnière du rayonnement solaire est assurée grâce à l'utilisation du quotient entre rayonnement direct et global k b (utilisé e.g. dans (Kylling et al., 2000)).
Cette variable construite a une interprétation intuitive (i.e. 0 pour un temps nuageux, 1 pour un temps ensoleillé), pertinente dans le contexte de panneaux photovoltaiques, notamment (Tapakis et al., 2016) :
Le rayonnement solaire peut également être normalisé par un modèle de rayonnement maximal théorique (Reno et al., 2012, Section 2.3). Pour une variable météorologique quelconque, des ensembles mensuels-horaires peuvent être calculés pour estimer des moyennes et variances spécifiques depuis un ensemble de données, utilisées ensuite pour la normalisation .
Dans cet article, nous nous intéressons à la prédiction horaire, i.e. prédire le rayonnement solaire une heure après un temps donné. Les séries temporelles étant fournies selon un pas de temps d'une minute, d'après la terminologie introduite en section 2, nous traitons le problème de prédire x t+59 .
Pré-traitement
Avant de s'intéresser à la tâche de prédiction elle-même, nous avons exploré les données fournies. Les fichiers ont été pré-traités grâce aux libraries R zoo (Zeileis et Grothendieck, 2005) et lubridate (Grolemund et Wickham, 2011), qui fournissent des outils adéquats au traitement des séries temporelles.
Nous avons tout d'abord vérifié la présence d'horodatages manquants ou erronés, et de valeurs manquantes. Tous les horodatages ont été correctement identifiés (i.e. bon format et secondes respectives à 0), mais des valeurs et horodatages manquants ont été trouvés. Par exemple, dans les données de Moufia, il n'y a pas de données pour les horodatages allant de 2014-01-22 08:33:00 à 2014-01-22 08:57:00. Quand l'horodatage est présent, toutes les variables sont presque toujours renseignées, à l'exception notable des données Saint André, où seule I G manque entre 2014-01-05 08:59:00 et 2014-01-05 14:59:00.
Les fichiers de données ont été complétés avec les horodatages manquants, et les valeurs manquantes ont été interpolées linéairement. Quand la plage d'interpolation est trop grande, des artéfacts visuels indésirables résultent de cette procédure (i.e. lignes droites, ou sinusoïdes dans le cas de UnitX and UnitY). Les plages temporelles associées doivent alors être exclues de l'analyse pour le site respectif. Toutefois, quand une seule valeur manque, cet artéfact est imperceptible. Afin d'implémenter cette identification visuelle, nous avons adapté un outil de visualisation de séries temporelles ( (Anderson, 2012) Toutes les périodes d'exclusion résultent de valeurs et horodatages manquants, sauf le début des données Moufia. Le processus stochastique que suivent ces dernières semble ainsi diffé-rer significativement des processus de rayonnement habituels (voir figure 1a). L'exploration visuelle nous a amené à constater qu'au contraire de toutes les autres variables, les séries de rayonnement ont un fort a priori la nuit. Les valeurs de rayonnement nocturne sont ainsi très proches de 0, et l'incertitude des capteurs à ce niveau engendre des valeurs de k b aberrantes. Nous corrigeons ce problème en fixant : en période nocturne, controllant ainsi la variance des valeurs nocturnes, et limitant ainsi leur influence sur la modélisation. Les horodatages nocturnes sont déterminés en utilisant des éphémérides disponibles publiquement pour l'île de la Réunion (Bruneau et Pinheiro, 2017).
Sélection de Variables
Selon les définitions en section 3.1, les prédictions horaires dans le contexte de données ayant un pas de temps d'une minute revient à prédire x δ,t+59 avec la connaissance des séries temporelles x d jusqu'à l'horodatage t − 1. Avant même de discuter de la fonction de prédiction utilisée, un dilemme se présente au sujet de son espace d'entrée :
-utiliser seulement les valeurs instantanées des D séries temporelles à t − 1, -en cas d'ajout de créneaux temporels t < t − 1, choisir une taille de vecteur d'entrée. L'importance de ce dilemme tient à ce que la taille du vecteur d'entrée influence fortement le temps de calcul et les besoins en termes de taille d'ensemble d'apprentissage. Dans le cas du MLP, une étape d'algorithme d'apprentissage est quadratique selon le nombre de dimensions du vecteur d'entrée , au mieux linéaire si un algorithme SGD (Stochastic Gradient Descent) est utilisé (Zhang, 2004). En d'autres termes, le vecteur d'entrée doit contenir suffisamment d'information de manière à assurer une bonne prédiction, mais être aussi parcimonieux que possible afin d'éviter une consommation de mémoire et un temps de calcul excessifs.
Comme indiqué dans la section 2, des méthodologies sont bien établies pour la sélection de variables dans les modèles ARIMA. Nous choisissons d'effectuer cette sélection de variables sur chacune des variables météorologiques. Nous utilisons la méthode itérative proposée par (Hyndman et Khandakar, 2007), et implémentée dans la librairie R forecast, conduisant à sé-lectionner un ensemble de créneaux passés pertinents pour la prédiction à l'horizon 1. En principe, cette sélection n'est pas adaptée à notre tâche de prédiction à l'horizon 60. Nous formons cependant l'hypothèse que cette sélection univariée fournit un vecteur d'entrée utile malgré tout à l'horizon 60 pour une méthode de régression générique. Implicitement, la modélisation de covariances entre les variables météorogologiques est déléguée à la procédure d'apprentissage. Nous réalisons cette sélection grâce aux données de Possession pour l'année 2014. Ce choix, qui demeure abritraire, est motivé par l'absence d'intervalle d'exclusion pour ce site. Alternativement, il aurait été possible de réaliser une sélection spécifique à chaque site, mais l'application d'un modèle utilisant cette sélection aux données d'un autre site créerait alors un nouveau problème méthodologique.
En pratique, l'estimation de modèles ARIMA d'ordre supérieur à 30 (i.e. valeur de p ou q des équations (1) et (2)) est coûteuse en temps de calcul. Les variables météorologiques ont une saisonnalité de 24 heures a priori très marquée : une approche naïve requerrait alors des modèles ARIMA d'ordre au moins égal à 1440, ce qui est impossible en pratique. Pour contourner ce problème, nous avons extrait des séries à pas de temps horaire des données originales (i.e. horodatages ayant leur minutes et secondes à 0). Nous avons estimé des modèles ARIMA sur ces séries construites, alors optimisés pour la prédiction de x t+59 avec la connaissance de x t−1 , x t−61 , etc... Par commodité nous définissons un index horaire T lié à la périodicité horaire, t étant alors lié à une période d'une minute. La conversion de ces index est illustrée en figure 2. Nous soulignons qu'avec ce système, x t−1 désigne la même valeur que x T −1 , et que l'horizon à une heure est indiqué de manière équivalente par x t+59 et x T .
FIG. 2 -Conversion entre horodatages ayant pour période une minute ou une heure. Comme dans un contexte programmatique l'horizon de prédiction est souvent choisi comme horodatage de référence, la conversion avec un index de tableau est également indiquée.
Les sélections automatiques réalisées selon (Hyndman et Khandakar, 2007) sont vérifiées à l'aide de graphes d'auto-correlation. Des créneaux passés ont été ajoutés à la sélection initiale jusqu'à avoir un graphe d'auto-correlation satisfaisant. Supposant que cette procédure engendre un modèle ARIMA(p, d, q), nous inférons la sélection de max(p, q) créneaux temporels (i.e. x t−1 . . . x t−max(p,q) sont concaténés au vecteur d'entrée pour l'apprentissage). Pour limiter le temps de calcul, max(p, q) est limité à 20. La sélection pour le pas de temps d'une minute, d'une heure, ainsi que les termes saisonniers sont concaténés. Les résultats pour chaque variable météorologique sont résumés en table 3. Nous notons que toutes les sélections sont relativement parcimonieuses, sauf pour Patm. Le vecteur résultant est de taille 70. Tous les modèles ARIMA ont utilisé une version différenciée des séries temporelles. Cela signifie que les données originales ne vérifiaient pas l'hypothèse de stationarité (i.e. moyenne est variance constante). Une manière simple d'établir la stationarité faible est de normaliser les séries temporelles selon des moyennes mensuelles-horaires .
Expériences
Protocole
Comme décrit en section 3.3 et résumé par la figure 2, pour un temps présent T − 1 notre protocole se concentre sur la prédiction de k b au temps T . En particulier, nous allons tester les
Variable
Créneaux sélectionnés
TAB. 3 -Créneaux temporels sélectionnés par ARIMA pour chaque variable météorologique. NB : comme T − 1 et t − 1 sont équivalents, seul t − 1 est indiqué quand opportun.
hypothèses suivantes : -si la sélection de variables parmi les créneaux t * < t − 1 offre une meilleure performance que l'utilisation des valeurs à t − 1 seules, -si un modèle appris sur les données d'un site réalise de meilleures performance sur ses données de test respectives qu'un modèle appris sur les données d'un autre site. Nous désignons désormais le vecteur formé des valeurs des 7 séries temporelles à t − 1 comme le vecteur instantané, et le vecteur résultant de la procédure décrite en section 3.3 comme le vecteur arima. Pour tester nos hypothèses, pour chaque site nous utilisons les données de 2014 pour l'apprentissage, et celles de 2015 pour le test. De ces ensembles, nous excluons les éléments associés à la prédiction, triviale, d'un créneau nocturne. Nous utilisons les deux modèles de régression suivants, avec leurs algorithmes d'appentissage associés :
-Xgboost : ce modèle est basé sur le Gradient Boosting (Friedman, 2001) et les Generalized Boosted Models (Ridgeway, 2007). Nous l'avons implémenté grâce à la librairie R Xgboost, qui réalise des ensembles d'arbres de régression. L'algorithme utilise plusieurs hyper-paramètres, tels que la vitesse d'apprentissage η, ou la profondeur maximale d'un arbre. Ces paramètres ont été réglés au moyen de la librarie R caret avec une validation croisée à 3 ensembles. La totalité de l'ensemble d'apprentissage est utilisée de manière séquentielle par Xgboost. Aucune normalisation de données n'a été utilisée dans les résultats présentés pour Xgboost, car empiriquement une meilleure performance est alors obtenue. -MLP : ce type de réseau de neurones utilise une seule couche cachée. Nous l'avons implémenté grâce à la librairie R mxnet (Chen et al., 2015). La complexité (i.e. le nombre de neurones de la couche cachée) des modèles spécifiques aux vecteurs instantané et arima est sélectionnée grâce à une validation croisée à 10 ensembles sur les données du site Possession. Les meilleures tailles de couches cachées ont été déterminées à 10 et 30, respectivement pour les vecteurs instantané et arima. Des MLP utilisant ces tailles ont ensuite été optimisés pour chaque site, toujours avec une validation croisée à 10 ensembles. Les réseaux de neurones étant sensibles à la normalisation des données en entrée, les séries temporelles utilisées pour la construction des vecteurs instantané et arima ont été normalisées grâces aux ensembles mensuels-horaires décrits dans  
Résultats
En  (Martín et al., 2010).
Les modèles appris ont tous des performances sensiblement meilleures que le modèle de persistence. L'utilisation du vecteur arima améliore sensiblement la qualité des modèles MLP et Xgboost l'utilisant, exception faite du MLP pour Saint-André. Le MAE atteint alors au mieux 18.3% et 20.2%, respectivement pour les modèles Xgboost et MLP. Cette amélioration est cependant à relativiser, car elle reste notamment limitée par rapport à l'impact d'utiliser le modèle Xgboost ou MLP. Par exemple, Xgboost utilisant le vecteur instantané est toujours meilleur que le MLP utilisant le vecteur arima.
Par la suite, nous retiendrons les modèles appris avec le vecteur arima. Avant d'appliquer les modèles retenus sur les données de test d'autres sites que ceux ayant servi à leur apprentissage, la table 5 indique la correlation moyenne entre sites pour les 7 variables météorologiques. Assez logiquement, les variables liées au vent telles que WS_Mean, UnitX et UnitY ont une corrélation inter-site faible, quasiment nulle pour les variables encodant la direction. Text et Patm ont la plus forte corrélation, de manière assez logique également car la température extérieure et la pression atmosphérique sont a priori assez homogènes sur un territoire de la taille de l'île de la Réunion. La variance assez élevée associée à Patm invite à modérer cette observation, suggérant plutôt des groupes homogènes.
Globalement, la De manière également intéressante, la meilleure performance des MLP n'est parfois pas obtenue sur leur site d'origine (e.g. le MLP de Sain Leu obtient son meilleur score sur Saint Pierre), et le meilleur MLP pour un site donné n'est pas toujours spécifique à ce site (e.g. le MLP de Possession est le meilleur MLP pour 4 sites). Cette dernière observation est sans doute partiellement due à l'utilisation des données de Possession pour la procédure de sélection de variables et de complexité de modèle.
Conclusion
Nous avons répondu à la dimension prédictive du Défi EGC 2018, plus précisément la prédiction de l'indice de rayonnement solaire k b à l'horizon d'une heure. Nous avons présenté une séquence de pré-traitements, une sélection de variables basée sur le modèle ARIMA, et des résultats expérimentaux obtenus grâce aux modèles Xgboost et MLP. Xgboost obtient alors les meilleures performances. Notre procédure de sélection de variables apporte un gain de performance, qui reste cependant marginal au fait de substituer Xgboost au MLP, notamment. Une rapide étude des corrélations inter-site montre que les données des 5 sites ne peuvent pas être simplement agrégées en un seul ensemble d'apprentissage. Après avoir entrainé nos modèles sur leur site respectif, nous avons observé leur performance sur les données de test d'autres sites. La normalisation mensuelle-horaire semble alors donner un avantage au MLP. Une modélisation des dépendances entre sites plus poussée est une extension possible à nos observations préliminaires.
L'approche adoptée dans cet article fait peu de cas de la nature séquentielle des données traitées. Prendre en compte cette nature peut a priori mener à des améliorations qualitatives, par exemple en adaptant une architecture de réseaux de neurones convolutionnelle (Krizhevsky et al., 2012) ou récurrente (Williams et Zipser, 1989) à notre problème de prédiction. Ces modèles ont déjà été utilisés dans des problèmes séquentiels apparentés, tels que l'analyse de sentiment dans un texte (Severyn et Moschitti, 2015) ou la traduction automatique (Liu et al., 2014).

Introduction
En quelques années seulement, le financement participatif a connu une forte croissance. Ce phénomène émergent est encore peu étudié, mais soulève de nombreuses questions dans divers champs scientifiques. L'une des principales interrogations est de connaître à l'avance le succès d'une campagne de financement participatif par des techniques de prévisions.
Nous proposons dans cet article de reprendre la méthode de prédiction du montant final levé lors d'une campagne de financement participatif utilisant l'algorithme k-NN proposée dans Blansché et al. (2017), et d'y apporter deux améliorations afin de faciliter le passage à l'échelle sur des données de plus grande taille et d'accroître les performances des prédictions. La première amélioration proposée consiste à dé-couper l'ensemble d'apprentissage en groupes homogènes à l'aide d'un algorithme de classification non supervisée. La recherche des k plus proches voisins pourra alors se limiter à un faible nombre de clusters plutôt qu'à l'ensemble d'apprentissage complet.
La seconde proposition consiste à chercher des attributs, pour décrire les campagnes de financement participatif, permettant de réaliser des prédictions plus précises.
Dans la section 2, nous présentons plus en détail la problématique qui nous intéresse. Dans la section 3, nous présentons notre première proposition, sur l'utilisation de la classification non supervisée et dans la section 4 décrivons la seconde, sur l'utilisation d'attributs extraits des séries temporelles. Enfin, dans la section 5, nous apportons une conclusion à notre travail et proposons plusieurs perspectives de recherches futures.
Problématique
Contexte
Le financement participatif (crowdfunding), qui consiste à faire appel à un grand nombre de personnes pour financer un projet (contrairement aux modes de financements traditionnels), a connu une forte croissance avec l'émergence d'Internet et des réseaux socionumériques. Il existe plusieurs formes de financement participatif, selon qu'il y ait une récompense ou non pour les contributeurs, que celle-ci prenne la forme de dividendes en cas de succès ou d'un produit livré, mais également selon la durée des campagnes de financement, limitée ou non. Kickstarter 1 est un site de financement participatif parmi les plus populaires. La durée des campagnes est limitée dans le temps, mais peut varier d'un projet à l'autre. Le créateur d'un projet choisit un seuil de financement en-dessous duquel le projet est considéré comme un échec, mais au-dessus duquel c'est un succès : plus de 60 % des projets ne sont pas financés (le montant levé n'atteint pas le seuil fixé par le créateur). Le seuil de financement est parfois inférieur au seuil de rentabilité du projet : le créateur peut vouloir afficher 100 % de financement rapidement pour augmenter sa notoriété (les contributeurs privilégient souvent les campagnes réussies) sans que cela ne corresponde à ses besoins. De plus, en cas d'échec d'une campagne, aucune somme n'est versée et le capital investi en amont est perdu et le créateur ne reçoit aucun fond : celui-ci peut alors s'assurer un remboursement partiel en choisissant un seuil de financement inférieur au seuil de rentabilité.
On peut décomposer l'évolution d'une campagne en n + 1 états répartis dans le temps. On notera t i (c) l'horodatage du i-ième état de la campagne c, en normalisant de sorte que 0 < t i (c) < 1. Ainsi t 0 (c) = 0 représente le début de la campagne et t n (c) = 1 représente la fin de la campagne. On notera alors m i (c) le montant levé à l'état i d'une campagne c.
Beaucoup de publications (Etter et al., 2013;Mitra et Gilbert, 2014;Li, 2016) s'intéressent à la prédiction du succès ou de l'échec d'une campagne de financement participatif (classification supervisée). Dans cet article, nous nous intéressons à un problème de régression : l'objectif est de prédire la valeur finale m n (c 0 ) d'une campagne 
Données utilisées
Dans Etter et al. (2013), les auteurs ont constitué une base de données portant sur 16042 campagnes Kickstarter datant de 2012 et 2013. Les séries temporelles ont été normalisées par un ré-échantillonnage en 1000 états. Il s'agit d'un petit échantillon : Kickstarter compte un total de plus de 375000 campagnes lancées depuis sa fondation en 2009 (58074 campagnes pour l'année 2016).
Sur la figure 1, on observe l'évolution du montant levé moyen au cours du temps. On remarque, en particulier pour les campagnes réussies, que la pente de la courbe est plus forte au début et à la fin de la campagne qu'au milieu de celle-ci. Sur la figure 2, on observe la répartition des montants levés à la fin d'une campagne. On remarque que la plupart des échecs sont très loin du seuil de financement et que la plupart des projets financés avec succès sont à peine au-dessus de ce seuil, bien que certains projets peuvent le dépasser très largement (plus de 500 % du montant demandé).
État de l'art
Méthodes « classiques »
Si les sommes levées étaient uniformément réparties durant la campagne, une approche naïve consisterait à faire une approximation linéaire. On définit alorsˆmalorsˆ alorsˆm n (c 0 ) = t n (c 0 ) × mi (c0) ti (c0) . Cette hypothèse n'est cependant pas vérifiée. Sur la figure 3 (évolu-tion des contributions selon le temps) on remarque que les apports en début et fin d'une campagne sont plus importants qu'au milieu (surtout en cas de succès). On peut également tenter de prédire la valeur finale en construisant un modèle classique de régression linéaire ou polynomial. Les méthodes d'autorégression (Taylor, 2008) consistent à expliquer une variable numérique par ses valeurs précédentes plutôt que par d'autres variables. Ces méthodes s'appliquent à beaucoup de problèmes d'analyse de séries temporelles telles que la prédiction boursière ou météorologique. La méthode ARIMA (AutoRegressive Integrated Moving Average) est l'une des approches d'autorégression les plus utilisées.
Utilisation de k-NN
La première approche proposée dans Etter et al. (2013)   Etter et al. (2013), pour déterminer les k voisins les plus proches, les auteurs comparent les campagnes en utilisant la distance euclidienne sur les états 0 à i. Cependant, nous avons montré que nous obtenons des prédictions de qualité équivalente en calculant la distance selon l'état i uniquement.
Expérimentations
Nous avons évalué l'efficacité à prédire le montant final des campagnes de financement participatif des différentes méthodes mentionnées sur les données de Etter et al. (2013). Pour comparer ces méthodes, nous avons utilisé la mesure RMSE (Root Mean Square Error), très souvent utilisée en régression (Hyndman et Koehler, 2006). Plus la valeur est faible, meilleure est la prédiction.
Dans cette expérimentation (et dans les suivantes), nous réalisons des prédictions pour chaque campagne après avoir produit un modèle sur un ensemble d'apprentissage. Nous avons ordonné les campagnes de financement participatif par date de fin. Pour la prédiction du montant final d'une campagne donnée, l'algorithme k-NN utilise un ensemble d'apprentissage constitué des 1000 dernières campagnes complètes. Ainsi nous Nous avons calculé les prédictions des différents modèles de régression à partir de différents états de la campagne en cours afin de pouvoir observer l'évolution des performances dans le temps (figure 4). On remarque que les plus mauvaises performances sont obtenues par la méthode d'autorégression ARIMA. L'approche naïve et la ré-gression polynomiale ont également des résultats peu satisfaisants. Étonnamment, les performances de la régression linéaire sont correctes, mais c'est l'approche proposée dans Blansché et al. (2017) qui produit les meilleures prédictions. Ce résultat semblent indiquer que la fouille de données, en exploitant un historique de campagnes passées, est une voie à privilégier pour prédire le montant final d'une campagne.
Classification non supervisée
Méthodologie
L'approche proposée est prometteuse, mais nous ne l'avons appliquée que sur une base de donnée de 16042 campagnes. Or il ne s'agit que d'un petit échantillon des campagnes lancées sur Kickstarter. Depuis sa création, plus de 375000 projets ont été lancés sur la plateforme et il peut y avoir simultanément plus de 4000 projets en cours de financement. De plus, pour chaque campagne, il est nécessaire de répéter la méthode de prédiction pour chaque nouvel état enregistré, afin d'avoir en permanence des prédictions à jour selon les données que l'on possède. La méthode proposée utilise le principe de l'algorithme k-NN et donc, pour chaque prédiction, il est nécessaire de calculer la distance par rapport à toutes les entrées de la base de données.
Nous proposons de réduire le nombre de calculs de distance en utilisant un algorithme de classification non supervisée, en s'inspirant d'une approche proposée dans Wang (2011). L'ensemble d'apprentissage est découpé en groupes homogènes (clusters) par un algorithme de classification non supervisée. Chaque cluster peut alors être ré-sumé par un individu représentatif, qui minimise la somme des carrés des distances avec les objets qui composent le cluster. Pour identifier les k plus proches voisins d'une nouvelle campagne, on détermine d'abord les k clusters les plus proches. On restreint alors l'ensemble d'apprentissage, et donc la recherche des k plus proches voisins, à l'union des observations contenues dans ces k clusters.
Nombre de points dans les séries temporelles
Dans les données de Etter et al. (2013), chaque série temporelle comporte 1000 états uniformément répartis. On peut légitiment se demander si ce nombre d'états est nécessaire ou s'il est possible d'avoir une représentation fidèle avec moins d'états.
Pour réduire le nombre d'états d'une série temporelle, il existe plusieurs techniques. La plus simple est de faire un ré-échantillonnage avec un nombre d'états réduits, pris uniformément dans temps. D'autres approches, comme le Piecewise Aggregate Approximation (Keogh et al., 2001), consistent à découper la série temporelle en segments et de calculer une moyenne pour chaque segment. Certaines méthodes cherchent des points d'intérêt de la série temporelle tels que les pics les plus saillants (Chung et al., 2001), mais cette approche ne convient pas aux séries temporelles de financement participatif, car ces séries sont souvent monotones croissantes. Les transformées de Fourier (Bloomfield, 2004) sont très utilisées pour traiter des séries temporelles bruitées comme les signaux sonores, mais ont peu d'intérêt pour notre problème.
Nous nous sommes finalement intéressés à l'approche la plus simple et avons étudié l'impact de la réduction du nombre d'état par ré-échantillonnage.
Mesure de dissimilarité
Dans un deuxième temps, nous nous sommes intéressés au choix de la mesure de dissimilarité. De nombreuses mesures ont été développées pour traiter ce type de données (Giusti et Batista, 2013;Pereira et R.F. de Mello, 2013;Aghabozorgi et al., 2015). Les plus populaires restent la distance euclidienne et la dissimilarité Dynamic Time Warping (DTW), développée spécifiquement pour les séries temporelles (Sakoe et Chiba, 1971). La distance euclidienne nécessite d'avoir des séries de longueur identique. La mesure DTW est une mesure « élastique » qui permet d'associer des points particuliers des séries malgré un décalage temporelle. Cette mesure est cependant très coûteuse en temps de calcul.
Expérimentations
Pour étudier l'impact de la longueur des séries temporelles, nous avons appliqué l'algorithme k-means sur les séries complètes (1000 états) et les séries réduites par un ré-échantillonnage uniforme en 100 états et 10 états, ainsi que des séries représentées uniquement par l'état final. Nous avons utilisé la distance euclidienne et la mesure 1000 états 1 1 0,95 0,79 100 états 1 0,95 0,79 10 états 1 0,78 1 état 1
Tab. 1 -Comparaison des clusters obtenus par la distance euclidienne selon la longueur des séries (κ de Cohen)
DTW pour construire les clusters. Le nombre de clusters a été déterminé selon l'indice de Calinski-Harabasz (Caliński et Harabasz, 1974). Pour chaque configuration, nous avons appliqué l'algorithme k-means 100 fois, en six clusters, et conservé le meilleur résultat en terme d'inertie intra-classe. Les clusters ont été comparés entre eux en utilisant l'indice κ de Cohen. En raison du coût de calcul très élevé de la mesure DTW, nous nous sommes limités à un échantillon de 1000 séries choisies aléatoirement. Nous avons choisi d'utiliser l'algorithme k-means qui est simple à mettre en oeuvre. Le calcul du centroïde, l'individu représentatif au centre d'un cluster, va dépendre de la mesure de dissimilarité utilisée. Si la distance euclidienne est utilisée, le barycentre une simple moyenne sur chaque état de la série. Si la mesure DTW est utilisée, le calcul du centroïde n'est pas trivial : l'algorithme DTW Barycenter Averaging (DBA) est alors la méthode la plus couramment employée (Petitjean et al., 2011). Pour une utilisation à grande échelle, l'algorithme k-means ne sera pas une solution envisageable. En effet, il sera nécessaire de mettre à jour les clusters régulièrement (chaque fois qu'une campagne se termine) et l'utilisation de k-means serait alors trop coûteuse en temps de calcul et nous perdrions le bénéfice de l'approche. Ainsi, l'algorithme k-means devra être remplacé par une approche incrémentale (Ning et al., 2010) qui sera en mesure de mettre à jour les clusters à faible coût lorsque de nouvelles campagnes viendront s'ajouter à l'ensemble d'apprentissage.
Les tableaux 1, 2 et 3 montrent les résultats obtenus. On remarque qu'en utilisant la distance euclidienne, les clusters obtenus sont très semblables quelque soit la longueur des séries temporelles (1000, 100 ou 10 états). La différence s'accentue un peu quand on n'utilise qu'un seul état. En utilisant la mesure DTW, on observe que le nombre d'états a un impact beaucoup plus fort sur les clusters. On remarque également que moins il y a d'états dans les séries temporelles, moins il y a de différences entre la distance euclidienne et la mesure DTW.
Nous avons enfin étudié l'impact du découpage de l'ensemble d'apprentissage en clusters sur la méthode de prédiction du montant levé en utilisant le même protocole expérimental que celui décrit dans la section 2.3.3. Nous avons utilisé deux mesures de dissimilarité pour construire les clusters : la distance euclidienne et la mesure DTW. Nous avons également fait varier la taille de l'ensemble d'apprentissage de 1000 à 15000 et mesuré le temps de calcul de différentes configurations :
-prédiction par k-NN, sans découpage en clusters ; -prédiction par k-NN avec un découpage en six clusters et recherche des voisins dans le cluster le plus proche (noté 6/1) ; 1000 états 1 0,69 0,52 0,58 100 états 1 0,63 0,62 10 états 1 0,8 1 état 1
Tab. 2 -Comparaison des clusters obtenus par la dissimilarité DTW selon la longueur des séries (κ de Cohen)
1000 états 100 états 10 états 1 état 0,53 0,69 0,79 1
Tab. 3 -Comparaison des clusters obtenus par la distance euclidienne et la dissimilarité DTW selon la longueur des séries (κ de Cohen)
-prédiction par k-NN avec un découpage en vingt clusters et recherche des voisins dans les cinq clusters les plus proches (noté 20/5). Sur la figure 5, nous pouvons voir l'évolution du temps de calcul selon la taille de l'ensemble d'apprentissage pour les trois configurations étudiées (ce temps de calcul ne prend pas en compte le temps de construction des clusters). On remarque que, comme nous pouvions nous y attendre, le temps de calcul évolue linéairement selon la taille de l'ensemble d'apprentissage. De plus, on remarque que le découpage en clusters améliore nettement le temps de calcul, de l'ordre de 50 % environ avec les configurations choisies.
Sur la figure 6, nous pouvons voir la performance de la prédiction des méthodes comparées, selon l'avancement dans la campagne de financement, avec un ensemble d'apprentissage composées de 1000 campagnes. Nous observons que les performances de notre approche avec et sans clustering sont similaires si l'on utilise la distance euclidienne pour créer les clusters. On observe même une amélioration en début de campagne (avant 10 % d'avancement). En revanche, en utilisant DTW, les performances sont très instables (une étude plus approfondie des clusters obtenus pourra nous permettre de comprendre l'impact négatif de DTW). On remarque également que les résultats sont meilleurs dans les configurations 20/6 que dans les configurations 6/1.
Sélection de variables
Méthodologie
Un des résultats importants de l'article Blansché et al. (2017) est que la recherche des k plus proches voisins à l'état i ne nécessite pas de calculer une mesure de distance sur l'ensemble de la série temporelle. Les prédictions obtenues en utilisant uniquement le dernier état connu de la série sont tout aussi performantes. Ce résultat remet en question la nécessité de représenter une campagne par une série temporelle de l'évolution du montant levé. Nous avons donc tenté une représentation vectorielle, par extraction  ( mi(c) nci(c) ). Les quatre premiers attributs sont calculés uniquement à partir des données sur le montant levé. Les quatre derniers attributs utilisent d'autres données, comme par exemple l'horodatage du début et de la fin de la campagne ou le nombre de contributeurs. Trois de ces attributs n'évoluent pas durant la campagne : la durée de celle-ci, le seuil de financement du projet et l'impulsion. Nous avons cherché quels sont le ou les attributs les plus pertinents pour prédire le montant levé final d'une campagne. Une recherche exhaustive, parmi les 255 combinaisons possibles, a été réalisée.
Expérimentations
Nous avons appliqué le même protocole expérimental que celui décrit dans la section 2.3.3. Pour chaque combinaison d'attributs, nous avons appliqué notre approche en utilisant la distance euclidienne sur l'espace vectoriel correspondant, centré et réduit, pour chercher les k plus proches voisins. Nous avons ensuite comparé les combinaisons entre elles selon la moyenne de performance sur chaque état. Nous pouvons difficilement Nous avons également observé les 10 % des combinaisons les plus efficaces (26 combinaisons) et quels attributs composent ces combinaisons : nous avons compté le nombre d'occurrences (cf. tableau 4). On observe que les trois attributs fixes dans le temps sont les trois attributs les plus utilisés parmi les meilleures combinaisons de variables et que l'attribut le plus utilisé est l'impulsion, ce qui confirme son importance.
Conclusion
Dans cet article, nous avons présenté deux propositions d'amélioration d'un algorithme de prédiction utilisant les k plus proches voisins. Nous avons évalué l'impact du découpage de l'ensemble d'apprentissage en groupes homogènes pour réduire le temps de calcul et l'utilisation d'attributs à la place de séries temporelles.
La plupart des articles de recherche sur la prédiction du résultat d'une campagne de financement participatif, qu'il s'agisse de prédire de son succès ou son échec ou bien le montant levé final, ont tendance à utiliser des séries temporelles (Li, 2016;Zhao et al., 2017;Fan-Osuala et al., 2018). Néanmoins, certains résultats que nous avons obtenus nous font penser que cette approche n'est peut-être pas la bonne. Il y a plusieurs éléments qui mènent à cette conclusion :
-quand on réduit la longueur des séries temporelles de 1000 états à 100, voire à 10 états, les clusters obtenus sont presque identiques ; -l'utilisation de la mesure DTW détériore les performances de l'algorithme de prédiction (par rapport à la distance euclidienne) ; -l'attribut qui maximise les performances de prédiction ne dépend pas du temps. Cette conclusion inattendue nous pousse à voir le problème sous un autre angle et à reconsidérer notre méthodologie afin de produire des prédictions plus efficaces.
Notre principale perspective est d'étudier les possibilités d'amélioration des performances (prédictions plus précises, plus tôt) en améliorant notre approche (en explorant les méthodes de raisonnement à partir de cas) ou en développant d'autres algorithmes, mais également en utilisant des données enrichies, internes au site de financement participatif (nombre de contributeurs, de commentaires, etc.) ou portant sur la réputation d'un projet, en particulier sur les réseaux socionumériques. Nous comptons également continuer à travailler sur la classification non supervisée de campagnes de financement, en particulier en utilisant un méthode incrémentale, moins coûteuse en temps de calcul.

Summary
L'apprentissage automatique, pardon le « machine learning », a envahi la sphère médiatique grâce à des succès impressionnants comme la victoire d'une machine au Go, ou la promesse de véhicules autonomes arrivant très prochainement sur nos routes. De fait, tant l'exploitation des données massives que la production de code machine à partir de l'expérience de la machine plutôt que par des humains, met l'apprentissage automatique au coeur de l'intelligence artificielle. Très certainement cela signifie que nous savons répondre à la question « qu'est-ce qu'un bon système d'apprentissage ? » et qu'il ne nous reste plus qu'à en décliner la réponse pour obtenir des systèmes adaptés à chaque domaine applicatif. Pourtant, la réponse à cette question a profondément évolué au cours des 60 dernières années, au point que les publications sur l'apprentissage automatique d'il y a quelques décennies semblent venir d'une autre planète et ne sont d'ailleurs plus enseignés aux étudiants. Et ceci pas seulement parce que les connaissances passées seraient jugées obsolètes, mais parce qu'elles ne semblent pas pertinentes. Avons-nous donc raison ? Nos précurseurs avaient-ils tort ? Et nos successeurs nous citeront-ils dans leurs manuels ? Dans cette présentation, nous examinerons quelques moments clés de l'histoire de l'apprentissage automatique correspondant à des tournants dans la manière de considérer ce qu'est un bon système d'apprentissage. Et nous nous demanderons si nous vivons un autre moment charnière dans lequel changent notre perspective, la question que nous cherchons à résoudre dans nos recherches, les concepts manipulés et la manière d'écrire nos papiers. 
Biography

Biography
Christopher Kermorvant, est ingénieur ENSIIE (1996) et docteur en informatique (2002). Depuis 20 ans, il travaille au développement d'applications utilisant des technologies de Machine Learning. Après une expérience académique en France (doctorant au laboratoire Hubert Curien de l'Université de Saint-Etienne) et à l'étranger (ingénieur de recherche au laboratoire IDIAP de l'Ecole Polytechnique Fédérale de Lausanne, post-doctorant au laboratoire MILA de l'Université de Montréal), il a dirigé pendant 8 ans une équipe de recherche au sein d'A2iA, une PME high-tech, spécialisée dans la reconnaissance d'écriture manuscrite. Avec son équipe, il a développé de nouveaux systèmes de reconnaissance d'écriture basés sur des réseaux de neurones profonds. Ces systèmes se sont classés premier lors d'évaluations internationales de reconnaissance d'écriture en français, anglais, arabe (Rimes, OpenHart, Maurdor) et sont depuis été intégrés dans la gamme des produits de la société A2iA.
Depuis 2015, il travaille en tant qu'expert indépendant en Machine Learning (www.teklia.com) pour aider les entreprises à développer des produits innovants basés sur des technologies de Machine Learning tout en poursuivant des projets de recherche collaborative en Digital Humanities.

Introduction
Depuis la prolifération des bases de données partiellement étiquetées, l'apprentissage automatique a connu un développement important dans le mode semi-supervisé [Chapelle et al. (2006)]. Cette tendance est due à la difficulté de l'étiquetage des données d'une part et au coût de cet étiquetage quand il est possible, d'autre part. L'apprentissage semi-supervisé est un cas particulier de l'apprentissage à partir de données faiblement étiquetées [Li et al. (2013)], qui consiste en général à modéliser une fonction statistique à partir de données regroupant à la fois des exemples étiquetés et d'autres non-étiquetés. Pour aborder une telle problématique, deux grandes familles d'approches existent : celle basée sur la propagation de la supervision en vue de l'apprentissage supervisé [Zhu (2006)] et celle basée sur la transformation de la partie étiquetée en contraintes en vue de leur intégration dans un processus de clustering (nonsupervisé) [Basu et al. (2008)]. Nous nous intéressons ici à la première famille d'approches avec une difficulté particulière. Il s'agit d'apprendre avec une partie supervisée relativement réduite par rapport à la partie non-supervisée. Dans ce paradigme semi-supervisé, la littérature a connu un essor important, depuis déjà une vingtaine d'années, notamment en classification, avec des approches populaires comme le self-training [Chapelle et al. (2006)], le co-training [Blum et Mitchell (1998)], Transductive-SVM ou S 3 VM [Joachims (1999), Bennett et Demiriz (1999)], les approches à base de graphes [Blum et Chawla (2001)] et les approches génératives [Nigam et al. (2000)]. Dans ce même paradigme, les problèmes de régression ont également suscité l'intérêt de plusieurs travaux de recherche, que nous pouvons citer sans vouloir être exhaustifs. Il s'agit d'approches diverses : à base de régression linéaire [Azriel et al. (2016); Ji et al. (2012)], logistique [Amini et Gallinari (2002)] ou Laplacienne [Cai et al. (2006), Belkin et al. (2006)] ; d'autres utilisant le principe du co-training [Zhou et Li (2005)] ; ou satisfaisant des contraintes plus spécifiques liées à l'ordre de préférences entre les données non-étiquetées [Zhu (2006)] ou à leur distribution géométrique dans les espaces multidimensionnels [Ryan et Culp (2015), Moscovich et al. (2016)].
Dans ce papier, nous proposons d'améliorer l'algorithme proposé par [Ji et al. (2012)]. L'idée est de bénéficier de la réduction de dimensions proposée dans leur travail et de remplacer la régression linéaire par une régression Laplacienne qui est censée reconnaître mieux la structure géométrique induite des données non-étiquetées. Pour ce faire, nous nous appuyons en plus sur les travaux de [Belkin et al. (2006)]. Ces deux approches seront donc à la base de l'approche proposée et seront décrites dans la section suivante. Ensuite, nous appliquons cette proposition sur des données des réseaux d'assainissement de la métropole de Lyon pour la reconstitution des dates de pose des canalisations. En effet, cette connaissance est primordiale pour les collectivités locales afin d'inspecter l'état de santé de ces réseaux. L'approche retenue ne prend pas en compte la dimension spatiale de l'évolution (temporelle) des réseaux car cette approche fait l'objet de travaux spécifiques en lien avec un géographe 1 .
Notations et formulations
Notons par : n le nombre total d'individus, m le nombre d'individus labellisés, (v i ) n i=1 les données d'entrée, z la variable cible, de dimension n (dont seulement m éléments ne sont pas manquants). Dans notre cas, les données d'entrée sont dans un espace multidimensionnel X et la cible est réelle. Notons également κ : X → X le noyau de Mercer [Mika et al. (1999)] que l'on utilise pour tenir compte des non-linéarités de la régression. On note K la matrice carrée symétrique réelle, définie pour (i, j) ∈ {1, ..., n} 2 par :
Cette matrice est potentiellement très grande par son nombre d'éléments (n 2 ). Dans notre application nous prendrons le noyau RBF :
, qui rajoute un hyperparamètre ω. L'extraction des s composantes principales de K de plus hautes valeurs propres définit les données d'apprentissage dans une nouvelle matrice X de n lignes par s colonnes. La régularisation Laplacienne fait intervenir la matrice Laplacienne d'un certain graphe L, dont la matrice d'adjacence en i, j est 1 si i et j sont ressemblantes (la distance dans l'espace des variables est inférieure à un seuil radius), 0 sinon, ainsi que deux régulariseurs τ et C. Enfin, les données de test sont notées (v ti ) b i=1 .
1. http://www.hireau.org 3 Approche proposée : LapS3L
Nous proposons une méthode, que nous appelons LapS3L, qui consiste à améliorer l'algorithme SSSL [Ji et al. (2012)], tout en remplaçant la deuxième étape de l'algorithme par la régression semi-supervisée LapRLS [Belkin et al. (2006)] dans le cas linéaire. Il y a de nombreuses motivations pour cela :
1. La première étape du SSSL est censée traiter le problème de la non linéarité, on peut donc espérer n'avoir à faire qu'une régression linéaire dans la seconde étape ;
2. La régularisation Laplacienne est une généralisation des moindres carrés : si les paramètres ν et γ lap sont nuls, alors le problème linéaire résolu avec la régularisation Laplacienne est exactement le même que le problème des moindres carrés utilisés pour la deuxième étape du SSSL.
L'approche que nous proposons est légèrement différente du principe déjà évoqué dans [Chapelle et al. (2006)] et mis en place dans le SSSL (décomposer le problème semi-supervisé en une étape non supervisée puis une étape complètement supervisée). La deuxième étape est aussi semi-supervisée, mais elle utilise des données que l'on peut traiter par une méthode linéaire. Par rapport au SSSL, on introduit une régularisation dans le calcul de w (ligne 9 dans l'algorithme 1).
, les s plus fortes valeurs propres et vecteurs propres associés de K 4:
∀i ∈ {1, ..., n} , ∀j ∈ {1, ..., s} , U i,j ← u j i
Chaque vecteur est une colonne 6:
, les indices des individus labellisés 8:
∀i ∈ {1, ..., n} , ∀j ∈ {1, ..., b} , K bi,j ← κ (v i , v t j)
13:
end procedure 16: end procedure
Application aux données d'assainissement
Les réseaux d'assainissement ont été construits et étendus pour et par la ville. Ce patrimoine existant impacte les pratiques de gestion : de nombreuses études [Ahmadi et al. (2014), Harvey et McBean (2014)] ont montré l'importance primordiale de la connaissance de la date de pose des conduites pour estimer leur état actuel de détérioration et prédire leur dégrada-tion. L'enjeu est donc pour la Métropole de Lyon de reconstituer les dates de pose des réseaux d'assainissement dont seulement 24 % du linéaire est connu.
Description des données
La base de données contient sept variables, dont quatre variables catégorielles (matériau, forme, structurant, type d'effluent) et trois variables continues (longueur, largeur, hauteur). Pour représenter les données catégorielles, nous utilisons un encodage où chaque modalité de chaque variable est représentée par une variable booléenne. La variable Forme possède plus de 300 modalités, ce qui porte la dimension des données à 346. Le choix des variables s'est fait en concertation avec les experts métiers de la Direction de l'Eau de la Métropole de Lyon. Les individus sont les conduites du réseau d'assainissement. Le réseau contient 85766 conduites, mais nous n'en gardons que 4000 (pour pouvoir traiter la matrice K). Enfin, la variable cible est la date de pose. Elle est comprise entre 1900 et 2014 (avant normalisation).
Résultats
En appliquant notre méthode d'apprentissage semi-supervisé à la base de données Assainissement, on obtient une erreur quadratique moyenne RMSE de 9.32. La valeur des paramètres a été obtenue par l'exploration d'une grille par validation croisée. Afin d'empêcher les variations dues au choix des échantillons, la validation est répétée 10 fois. On peut remarquer que l'approche LapS3L affiche des résultats meilleurs que l'algorithme SSSL non régularisé, et meilleurs que la régularisation Laplacienne (LapRLS) pour la mesure d'erreur quadratique. Pour expliquer ces résultats, on peut regarder l'évolution de l'erreur en fonction de la valeur du paramètre s (figure 1). L'erreur pour la méthode SSSL trouve un minimum en fonction de s, puis diverge rapidement. Pour comprendre ce phénomène, il suffit de se rappeler que la deuxième étape de l'algorithme SSSL consiste à effectuer une régression aux moindres carrés ordinaires. Si s augmente considérablement, la régression tentera de classifier des données correspondant à des valeurs propres trop faibles. Ces données sont assimilables à du bruit, et donc la méthode est victime de sur-apprentissage. La régularisation introduite dans la méthode proposée permet de mitiger ce risque dans une certaine mesure, raison pour laquelle la méthode proposée fournit de meilleurs résultats pour des valeurs de s plus grandes.
Conclusion
Nous avons proposé une approche de régression semi-supervisée qui combine deux approches intéressantes de l'état de l'art. La méthode proposée correspond aux exigences du cadre applicatif, et donne une erreur de régression quadratique plus faible que les deux mé-thodes de référence.
Cependant, la norme 2 employée dans la régularisation Laplacienne utilisée montre des limites dans le cas où le nombre de canalisations non datées augmente, alors que le nombre de canalisations datées reste constant [Alaoui et al. (2016)]. Bien que ce cas ne semble pas correspondre à notre problème, où il faut reconstituer les dates des canalisations anciennes uniquement, il serait intéressant de mettre en place une régularisation différente.

Introduction
La chute est la première cause de décès accidentel chez les personnes âgées de plus de 65 ans. L'évaluation régulière du risque de chute requiert des informations fiables et nombreuses sur la personne âgée et la collecte de ces informations est coûteuse en temps. Pour pallier ce problème, nous proposons un système chargé de collecter les informations au fil du temps, de les stocker et les gérer, en vue de fournir ces informations à la demande. Il s'agit donc d'un cas de la vie réelle où on ne dispose que d'informations incomplètes, incertaines ou incohérentes dans des situations de prise de décision. Ces informations sont très souvent variées et évolutives, risquent de devenir obsolètes et de contredire d'autres informations. Elles doivent ainsi être vérifiées continuellement afin de refléter fidèlement l'état et le comportement des personnes. L'objectif de ce travail est ainsi de maintenir un ensemble d'informations le plus fiable et complet possible, de façon à fournir à la demande des informations avec un degré de confiance élevé.
Pour cela, nous proposons un mécanisme de détection et de gestion des contradictions dans la base d'informations. La détection des observations obsolètes repose sur leur contradiction potentielle avec d'autres observations plus récentes et/ou plus jugées plus fiables portant sur des variables différentes. Détecter ce type de contradiction nécessite de combiner des connaissances sur les dépendances entre les variables et du raisonnement dans l'incertain du fait des observations incomplètes d'une part et de l'incertitude stochastique sur les connaissances. Notre proposition est basée sur l'utilisation d'un réseau bayésien (RB) (Jensen, 1996;Naïm et al., 2011;Pearl, 1988) à même de combiner modèle de connaissance et de raisonnement dans l'incertain. Un RB est un graphe dirigé sans circuit G défini sur un ensemble de noeuds X = {X 1 , X 2 , ..., X n }, associé avec une distribution de probabilités P sur X telle que Obs désigne une observation c'est à dire l'instanciation d'une ou plusieurs variables. La distribution de probabilités a posteriori P (V |Obs) représente l'état cognitif, ou la croyance, d'un observateur à propos de la variable V , dans une situation bien déter-minée. Dans cet article, les observations ne sont pas synchrones, puisque nous considérons des informations recueillies au fil du temps. En conservant les observations du passé, nous ajoutons un degré d'incertitude supplémentaire puisque nous ne pouvons plus affirmer avec certitude que l'observation est encore vraie. La prise en compte du vieillissement des observations ne peut être gérée directement dans le RB. Il ne s'agit pas non plus d'observations incertaines (Pearl, 1988;Valtorta et al., 2002;Mrad et al., 2015). Pour gérer l'incertitude liée au vieillissement des observations, nous proposons des fonctions de péremption, qui permettent de calculer pour chaque observation un degré de confiance.
Cet article commence par un rapide état de l'art sur la problématique de l'obsolescence des informations. Il présente ensuite l'architecture du système et le modèle de raisonnement permettant la gestion de l'obsolescence. La dernière partie présente la réalisation de notre application. 
Gestion de l'obsolescence des informations
Le rôle du système proposé est de maintenir à jour une base d'informations sur une PA, sur la base d'informations acquises par des dispositifs externes et/ou par interaction avec la personne. Le système peut décider d'interagir avec la PA pour confirmer les observations. Les opérations suivantes sont effectuées de façon cyclique : Base d'informations : regroupe l'ensemble des variables du RB, les observations associées, les dates de saisie, les fonctions de péremptions et les degrés de confiance (exemple en tableau 1). Seule la dernière valeur observée est conservée. Pour les variables qui n'ont pas ou plus de valeur observée, la base contient la valeur la plus probable sachant les observations récentes et la probabilité a posteriori associée. Ces mesures reflètent les croyances sur l'état de la PA. 
Tab. 1 -Base d'informations : les observations (avec date et niveau de confiance) et les croyances sur les variables non observées (distributions de probabilités a posteriori).
Module de raisonnement : gère la cohérence de la base d'information. La pré-sence d'une observation obsolète dans la base d'informations peut se manifester de deux façons. D'une part, quand une observation obsolète entre en contradiction avec des observations sur d'autres variables et qu'en conséquence l'observation simultanée de cet ensemble de valeurs est extrêmement improbable. Nous dirons que la base d'informations n'est pas dans un état possible. D'autre part, une observation obsolète peut générer un doute sur une variable non observée. Les activités d'inférence, de détection de contradictions, remise en cause des observations passées, suppression des observations obsolètes et vérification des doutes sont répétées jusqu'à ce que la base d'informations soit dans un état stable. Le module de raisonnement s'appuie sur les trois concepts suivants :
Une base d'informations est dans un état stable si et seulement si elle est dans un état possible et on n'a aucun doute sur ses variables n'ayant pas ou plus de valeur observée.
Une base d'informations est dans un état possible lorsque l'ensemble des observations OBS = obs est tel que P (OBS = obs) > Seuil(OBS), où Seuil(OBS) est une fonction qui associe à un ensemble de variables une probabilité en dessous de laquelle un ensemble d'observations n'est "pas possible". Cette définition relaxe le sens strict du mot possible qui correspond généralement à une probabilité non nulle. La valeur du seuil dépend logiquement de la taille du domaine des observations et des conventions adoptées dans la définition du RB pour caractériser les situations locales impossibles dans les TPC (Tables de Probabilités Conditionnelles). Nous avons adopté une définition très simplifiée de la fonction seuil.
On dit qu'on a un doute sur une variable n'ayant pas ou plus de valeur observée si la croyance sur cette variable est fortement remise en cause par les nouvelles observations.
Fig. 2 -Interfaces graphiques pour la gestion des interactions.
Module d'interaction : gère en permanence l'arrivée des nouvelles informations et fournit des informations fiables sur des variables cibles à la demande.
Réalisation et résultats de l'application
Notre application a été implantée sous l'environnement Windows, avec le le logiciel pyAgrum (Gonzales et al., 2014). Nous avons testé les méthodes proposées sur quelques scénarios à l'aide d'un premier démonstrateur (voir figure 2). Nous avons réa-lisé un premier test comparatif avec un raisonnement consistant à déclarer obsolète toute information dont le degré de confiance est sous un seuil fixé. Ceci peut conduire à deux écueils : supprimer une information qui n'est pas devenue obsolète ; ne pas supprimer des informations obsolètes. Comparativement, notre proposition évite systé-matiquement le premier écueil et partiellement le second puisque c'est la contradiction avec une information plus récente qui permet de détecter l'obsolescence. Les résultats préliminaires trouvés sont encourageants mais doivent être vérifiés et améliorés afin d'améliorer la performance du système.
Conclusion et perspectives
Nous avons proposé un système chargé de la gestion de l'obsolescence des informations dans le cadre de l'évitement de la chute chez les personnes âgées. Le système détecte d'éventuelles contradictions entre de nouvelles observations et les informations précédemment recueillies sur la personne, et met à jour de la base d'informations à l'aide d'un modèle probabiliste de connaissances générales sur les PA sous forme d'un réseau bayésien. Nos perspectives à court terme concernent une meilleure définition formelle du problème de la gestion d'une base d'informations non synchrones et non


Introduction
Dans le cadre de ce travail, nous nous focalisons sur l'étude de la sémantique des données d'observation associées aux tumeurs cérébrales. Associer une sémantique à une donnée d'observation consiste à mettre la donnée en relation avec d'autres entités participant, soit au phénomène observé (par exemple, la température corporelle d'un sujet), soit à un processus d'observation (par exemple, cette même valeur est reliée à une action d'observation, à son observateur, à l'instrument de mesure utilisé et l'instant de la mesure).
En termes de modélisation ontologique, il existe deux approches de modélisation qui sont adoptées pour décrire sémantiquement le contenu sémantique d'une image : l'approche cognitive (Cimino, 2006) et l'approche réaliste (Smith, 2006). L'approche cognitive oriente sa modélisation autour des "concepts" décrits par des "termes" faisant partie d'un lexique spéci-fique que nous manipulons pour l'attribution des propriétés (des qualités ou des dimensions) des données d'observation ; ces termes sont construits selon notre perception et connaissance des entités du monde réel. Contrairement à l'approche cognitive, l'approche réaliste aligne les termes des terminologies aux entités qui existent dans le monde indépendamment d'agents cognitifs reconnaissant leur existence. De plus, l'approche réaliste considère qu'il n'y a "qu'une seule réalité objective universelle" ; "chaque attribut du patient est lui-même une entité unique en réalité et on lui attribue son propre identifiant. Ainsi, les entités réelles référencées peuvent être de différents types : entités réelles, mesures, etc.
Jusqu'à aujourd'hui, les principaux formats existants pour la description des données d'observation sont : les comptes rendus radiologiques DICOM (Digital Imaging and Communications in Medicine) SR (Structured Report) (Clunie, 2000) et le modèle AIM (Annotation and Imaging Markup) (Channin et al., 2010)  Notre travail est basé sur l'hypothèse que l'utilisation des technologies du Web sémantique, en particulier les ontologies et leurs capacités de raisonnement, peut rendre plus explicite la sémantique des données d'observation en neuro-imagerie et faciliter leur exploitation et leur interprétation «avancées». La couverture de toutes les informations impliquées dans l'évalua-tion des tumeurs cérébrales est impossible car aucune source consensuelle n'existe pour spé-cifier les exigences précises de ce domaine. Pour surmonter cette difficulté, nous avons limité notre étude au domaine couvert par la terminologie VASARI 1 (Visually Accessible Rembrandt Images). VASARI constitue un cas d'utilisation représentatif de l'ensemble minimaliste des connaissances basiques qui nécessitent une modélisation formelle.
La terminologie VASARI est un vocabulaire d'annotation des gliomes cérébraux de haut grade en particulier le glioblastome multiforme (GBM) dans les images IRM (Imagerie par Ré-sonance Magnétique). Son objectif principal consiste à normaliser la description des tumeurs cérébrales et à faciliter leur interprétation par les neuro-radiologues. La terminologie VASARI contient trente critères d'imagerie et elle a été développée par des experts du domaine qui ont considéré la majorité des évaluations possibles des tumeurs cérébrales en IRM. Chaque critère d'imagerie de la terminologie VASARI est référencé par un numéro (F 1 , F 2 , etc.) et un ensemble de valeurs de scores possibles. Par exemple, le critère "F 1 :tumor location" de VASARI évalue la localisation de l'épicentre géographique et il définit sept valeurs d'étiquette possibles = frontal, temporal, insulaire, pariétal, occipital, tronc cérébral, cervelet.
L'objectif principal de ce travail est de faciliter l'identification, le partage et le raisonnement sur les résultats d'observation des tumeurs cérébrales via la formalisation de leurs significations sémantiques.
Matériel et méthode
L'annotation des caractéristiques d'imagerie des tumeurs cérébrales implique différents types d'entités : les entités physiques, les qualités liées aux objets physiques et les mesures de volume et de taille. Selon la terminologie VASARI, les entités physiques qui caractérisent certaines anomalies des tissus cérébraux sont : la tumeur cérébrale, l'épicentre de la tumeur cérébrale, les composantes de la tumeur cérébrale (à savoir : région de prise de contraste, région de non prise de contraste, partie nécrotique, composante oedémateuse et la bordure de la tumeur cérébrale) et la partie périphérique d'une tumeur cérébrale ou d'une partie de la tumeur cérébrale.
L'ontologie VASARI a été conçue selon l'approche réaliste. Notre méthodologie de modélisation se compose de cinq étapes principales qui peuvent être décrites comme suit : tout d'abord, nous avons analysé la signification de l'aspect étudié par chaque critère VASARI F i et nous avons trié ses configurations possibles pour établir la liste des valeurs possibles autorisées pour chaque critère. Deuxièmement, nous avons identifié les principales entités observées qui sont impliquées dans chaque critère. Troisièmement, nous avons défini les entités observées, soit avec des classes d'ontologies existantes soit avec de nouvelles classes ontologiques. Quatrièmement, nous avons spécifié les axiomes caractérisant ces entités. Enfin, nous nous sommes assurés que toutes les configurations possibles pour chaque critère F i sont bien modélisées de manière formelle.
Après une analyse de la signification des caractéristiques étudiées par les critères VASARI et de l'identification des différentes entités qu'elles impliquent, nous avons procédé à leur description formelle. Cette étape n'a pas été une tâche très simple pour nous étant donné que nous avons rencontré certains problèmes de modélisation que nous avons soulevés et discutés dans ce travail (Amdouni et Gibaud, 2016). Ces problèmes de modélisation concernent : les données d'observations négatives, les données de représentation de la connaissance spatiale et la représentation d'entités complexes touchant notamment à des mesures dérivées de mesures élémentaires.
Toute la construction ontologique (taxonomies des classes et des propriétés) s'appuie sur la version 2 de l'ontologie BFO (Basic Formal Ontology) (Grenon et al., 2004), ce qui facilite l'intégration d'ontologies spécialisées issues de la fonderie OBO (Smith et al., 2007). En particulier, nous avons réutilisé les ontologies suivantes : FMA, IAO, PATO, OBI, OGMS, UO et l'ontologie RO (toutes ces ontologies sont disponibles sur Bioportal du National Center of Biomedical Ontology (NCBO) 2 ). Nous avons développé l'ontologie VASARI en format OWL2 en utilisant la version 5 de l'outil Protégé. Nous avons utilisé Ontofox 3 pour l'extraction des ontologies OBO.
Dans notre travail expérimental, nous avons développé un outil d'annotation sémantique des données fondé sur les classes et relations de l'ontologie VASARI. Cet outil permet à l'utilisateur de transformer la description informelle des 30 critères VASARI en une description formelle. Pour faire le test, nous l'avons appliqué au corpus de données REMBRANDT 4 . REMBRANDT contient les observations relatives à 30 critères VASARI formulées par 3 radiologues et relatives à 34 patients atteints de GBM. Les résultats d'observation sont représentés FIG. 1 -Le modèle de base des classes principales dans l'ontologie VASARI dans un fichier Excel où chaque feuille de calcul contient des évaluations soumises par un radiologue. L'ensemble de données sémantiques résultant a été utilisé pour monter l'importance de la représentation réaliste des données d'observations en neuro-imagerie.
Résultats
L'ontologie VASARI est composée de huit modules d'ontologies et contient environ 570 classes OWL et 120 propriétés. La figure 1 met en évidence les quatre aspects sémantiques majeurs qui décrivent le domaine VASARI, à savoir : les structures pathologiques, la localisation anatomique, les qualités et dispositions, et les mesures.
Le logiciel d'annotation prend comme donnée d'entrée l'ensemble des valeurs des critères d'imagerie de la base REMBRANDT ainsi que le schéma de l'ontologie VASARI. Ensuite, pour annoter sémantiquement les données, le logiciel réalise quatre tâches principales : tout d'abord, il crée des instances des classes de l'ontologie VASARI en se basant sur les valeurs des critères. Deuxièmement, il décrit les critères d'imagerie en générant des triplets RDF qui éta-blissent des liens sémantiques entre les instances. Troisièmement, il crée dans un graphe RDF des affirmations à partir des triplets. Quatrièmement, il sérialise les données en RDF/XML et enregistre le graphe RDF en mémoire. On note que le logiciel stocke séparément le schéma de l'ontologie et les données d'instances (dans les prochains paragraphes, on emploie le terme Tbox pour faire référence au schéma et Abox pour désigner la base d'instances). En terme de performance, le logiciel génère le graphe RDF de l'ensemble de données contenues dans la base REMBRANDT en 1.06 s (soit ≈ 0.47s par feuille).
La figure 2 montre un exemple d'annotation de 10 observations du patient 9000_00_5316 de la base REMBRANDT avec notre ontologie : F1.tumor location= parietal, F2.side of tumor= left, F3.eloquent brain= none, F5.proportion of enhancing= 6-33%, F8.cyst= no ; F12.definition of the enhancing margin= well defined, F14.proportion of edema= 6-33%, F16.hemorrhage= no, F29.lesion size= 6,5cm. Considérons cet exemple pour illustrer quelques capacités d'in- -L'épicentre de la tumeur cérébrale de ce patient est localisée dans le lobe pariétal gauche vu que elle est déclarée située dans son lobe pariétal et dans son hémisphère cé-rébral gauche. L'axiome mis en jeu est : "fma:left temporal lobe" ≡ def. "fma:parietal lobe" and "fma:regionalPartOf" some "fma:left cerebral hemisphere". -La tumeur de ce patient est de type oedémateuse vu qu'elle contient une partie de type oedémateux. L'axiome mis en jeu est : "vasari:edematous cerebral tumor" ≡ def. "bfo:has continuant part at some time" some "vasari:cerebral edema component". -La périphérie de la tumeur qui prend la contraste est bien définie. L'axiome mis en jeu est : "vasari:well defined outisde margin of enhanncing cerebral tumor component" ≡ def. "vasari:outside margin of enhancing cerebral tumor component" "obi:has quality at some time" some "vasari:well defined".
Pour détecter les incohérences contenues dans la base de connaissances nous pouvons utilisé l'objet "ValidityReport" de l'API JENA. Cette structure encapsule tous les axiomes et assertions inconsistants qui sont détectés. Pour générer des explications sur les causes d'incohérences, nous avons utilisé la méthode "explainconsistency()". Cette méthode énumère tous les axiomes impliqués.

Introduction
Dans un monde où les technologies d'acquisition de données sont en croissance rapide, l'analyse exploratoire des bases de données hétérogènes et de grandes tailles reste un domaine peu étudié. Une technique fondamentale de l'analyse non supervisée est celle du clustering, dont l'objectif est de découvrir la structure sous-jacente des données en regroupant les individus similaires dans des groupes homogènes. Cependant, dans de nombreux contextes d'analyse exploratoire de données, cette technique de regroupement d'objets reste insuffisante pour découvrir les motifs les plus pertinents. Le co-clustering (Hartigan, 1975), apparu comme extension du clustering, est une technique non-supervisée dont l'objectif est regrouper conjointement les deux dimensions de la même table de données, en profitant de l'interdépendance entre les deux entités (individus et variables) représentées par ces deux dimensions pour extraire la structure sous-jacente des données. Cette technique est la plus adaptée, par example, dans des contextes comme l'analyse des paniers de consommation où l'objectif est d'identifier les sous-ensembles de clients ayant tendance à acheter les mêmes de produits, plutôt que de grouper simplement les clients (ou les produits) en fonction des modèles d'achat/vente.
Dans la littérature, plusieurs approches de co-clustering ont été développées. En particulier, certains algorithmes de co-clustering proposent d'optimiser une fonction qui mesure l'écart entre la matrice de données et la matrice de co-clusters (Cheng et Church, 2000). D'autres techniques sont basées sur la théorie de l'information (Dhillon et al. (2003)), sur les modèles de mélange pour définir des modèles de blocs latents (Govaert et Nadif, 2008), sur l'estimation Bayésienne des paramètres (Shan et Banerjee (2008)), sur l'approximation matricielle (Lee et Seung, 2011), ou sur le partitionnement des graphes (Dhillon, 2001). Cependant, ces méthodes s'appliquent naturellement sur des données de même type.
Dans Bouchareb et al. (2017), nous avons proposé une méthodologie permettant d'étendre l'utilisation du co-clustering au cas d'une table de données contenant des variables numé-riques et catégorielles simultanément. L'approche est basée sur une discrétisation de toutes les variables en fréquences égales, suivant un paramètre utilisateur, suivi par l'application d'une méthode de co-clustering sur les données discretisées. Dans ce papier, nous proposons une nouvelle famille de modèles permettant de formaliser cette méthodologie. Le modèle proposé ici ne nécessite aucun paramètre utilisateur et permet une inférence automatique des discrétisa-tions optimales des variables selon une approche regularisée, par opposition à la discrétisation définie par l'utilisateur proposée par Bouchareb et al. (2017). Un nouveau critère, mesurant la capacité du modèle à représenter les données, et de nouveaux algorithmes sont présentés.
Le reste de ce papier est organisé comme suit. En section 2, nous présentons le modèle proposé, le critère de sélection et la stratégie d'optimisation implémentée. La section 3 présente des résultats expérimentaux sur des données réelles, et la section 4 conclusion et perspectives.
Un modèle de co-clustering de données mixtes
Avant de présenter le modèle proposé, décrivons les données telles qu'elles sont vues par notre modèle. Les données sont composées d'un ensemble d'instances (identifiants de ligne de la matrice) et un ensemble de variables pouvant être numériques ou catégorielles. Nous définissons la notion d'une observation qui représente un 'log' d'une interaction entre une instance et une variable. Cette représentation nous permet de considérer le cas des valeurs manquantes dans les données mais aussi le cas de plusieurs observations par couple (instance, variable) comme dans les séries temporelles. Un example simple, illustrant cette représenta-tion, est donné par :
Cet example contient 4 instances (i 1 , . . . , i 4 ), 3 variables numériques (X 1 , X 2 , X 3 ), 2 variables catégorielles (X 4 , X 5 ) et un total de 21 observations.
Les paramètres du modèle
Le modèle de co-clustering est défini par une hiérarchie des paramètres. A chaque étage de la hiérarchie, les paramètres sont choisis en fonction des paramètres précédents. Définition 1. Le modèle de co-clustering des données mixtes est défini par : -la taille de la partition de chaque variable. Une partition est un regroupement des valeurs dans le cas d'une variable catégorielle et une discrétisation en intervalles dans le cas d'une variable numérique, -la partition des valeurs de chaque variable catégorielle en groupes de valeurs, -le nombre de clusters d'instances et de clusters de parties de variables. Ces choix définissent la taille de la matrice des co-clusters, -la partition des instances et des parties de variables selon le nombre de clusters choisi, -la distribution des observations sur les cellules de la matrice des co-clusters, -la distribution des observations associées à chaque cluster d'instances (resp. parties de variables) sur l'ensemble des instances (resp. parties de variables) dans le cluster, -la distribution des observations dans chaque partie de variable catégorielle sur l'ensemble des valeurs dans la partie.
Notations. Pour formaliser ce modèle, nous considérons les notations suivantes :
• N : le nombre total d'observations (connu), • K n : le nombre de variables numériques (connu),
• K c : le nombre de variables catégorielles (connu). X c l'ensemble de ces variables, • V k : le nombre de valeurs uniques de la variable catégorielle X k (connu), • J k : le nombre de parties de la variable X k (inconnu), : le nombre de valeurs dans la partie j k de la variable X k (déduit)
• n i. : le nombre d'observations associées à la i ` eme instance (inconnu), • n .kj k : le nombre d'observations associées à la partie j k de la variable X k (inconnu) • n v k : le nombre d'observations associées à la valeur v k de la variable catégorielle X k (inconnu) Un modèle de la définition 1 est complètement défini par le choix des paramètres ci-dessus notés inconnu.
Le critère Bayésien de sélection du meilleur modèle
Nous faisons l'hypothèse d'une distribution a priori des paramètres la moins informative possible, en exploitant la hiérarchie des paramètres avec un a priori uniforme à chaque niveau.
Étant donné les paramètres, la vraisemblance conditionnelle P (D|M) des données sachant le modèle peut être définie par une distribution multinomiale sur chaque niveau de la hiérarchie. Le produit de la probabilité a priori du modèle et de la vraisemblance, permet de calculer de manière exacte la probabilité a posteriori du modèle connaissant les données P (M|D). A partir de cette probabilité, nous définissons un critère de sélection de modèle C(M) = − log P (M|D), donné par théorème 1. Théorème 1. Parmi les modèles définis en définition 1, un modèle suivant un a priori hiérar-chique uniforme est optimal s'il minimise le critère :
S(A, b) est le nombre de Stirling de deuxième espèce donnant le nombre de répartitions possibles de A valeurs en, au plus, B groupes.
Les trois première lignes représentent le coût a priori du modèle tandis que les deux dernières représentent le coût de la vraisemblance. Pour des raisons de manque d'espace, la preuve de ce théorème n'est pas présentée dans ce papier.
Algorithme d'optimisation
En raison de leur grande expressivité, les modelés de co-clustering des données mixtes sont complexes à optimiser. Dans ce papier, nous proposons une heuristique d'optimisation en deux étapes. Dans la première étape, nous commençons par partitionner les variables en fréquences égales en utilisant un ensemble prédéfini des tailles de partitions et nous appliquons la mé-thodologie proposée en Bouchareb et al. (2017) pour trouver des co-clusters initiaux. Parmi les tailles testées, nous choisissons la solution initiale qui correspond à la valeur minimale du critère (1) comme point de départ. A partir de cette solution initiale, la deuxième étape est une post-optimisation qui effectue les fusions de clusters, les fusions de parties de variables, les déplacements de parties de variables entre clusters et déplacements de valeurs entre parties, qui minimisent le mieux le critère. Cette post-optimisation permet de choisir le meilleur modèle parmi un large sous-ensemble de modèles testés tout en améliorant l'interpretabilité, étant donné que le modèle optimisé est souvent très compact, comparé à la solution initiale.
Expérimentation
Pour valider l'apport du modèle proposé dans l'analyse exploratoire des données mixtes, nous l'avons appliqué sur les bases de données Iris et CensusIncome (Lichman, 2013).
La base Iris est composée de 150 instances, 750 observations, 4 variables numériques et 1 variable catégorielle. Les tailles des partitions de départ sont de 2 à 10 parties par variable. Globalement, le modèle obtenu permet d'obtenir un résumé de la base de données très riche en informations et exploitable à plusieurs niveaux de granularité pour piloter l'analyse exploratoire.

Introduction
C'est à travers des messages du type « La rédaction vous conseille », « Lire aussi », « Sur le même sujet », « Vous pourriez également être intéressé par », que les portails d'actualités en ligne proposent aux utilisateurs des sélections personnalisées de contenus pouvant les intéresser afin de les assister dans leur exploration des articles d'actualités. Dans ce domaine d'application, les systèmes de recommandation font face à un flux abondant et continu d'actualités qu'il faut collecter, analyser et mettre à disposition des utilisateurs selon leurs préférences et dans les plus brefs délais. Par ailleurs, la recommandation des articles d'actualité diffère des domaines d'application traditionnels des systèmes de recommandation qui se caractérisent par une parcimonie de données reflétant les comportements des utilisateurs à cause de l'inaccessibilité de certains produits (produits non disponibles ou ayant un coût d'acquisition élevé). En effet, les articles d'actualité étant gratuits et accessibles à tous les utilisateurs, les approches de recommandation à adopter doivent être capables de prendre en considération un flux abondant d'interactions entre les utilisateurs et le contenu. À titre d'exemple, sur la plateforme de recommandation de Plista 1 , le nombre d'événements observés par seconde peut dépasser les 5000 et le temps de réponse maximal pour fournir une recommandation est limité à 100ms. Dans ce contexte, il est nécessaire qu'un système de recommandation adopte une approche temps-réel, capable de monter en charge et proposant des recommandations dynamiques qui s'adaptent rapidement aux comportements observés chez chaque utilisateur. De plus, vu la nature non structurée du contenu traité, il est judicieux d'adopter des approches sémantiques capables d'extraire les concepts et les thématiques invoquées dans les articles d'actualité afin de mieux cerner les préférences et les intérêts des utilisateurs.
Les contraintes fortes de la problématique de la recommandation continue, temps réel et à large échelle des articles d'actualité ont conduit à une large adoption d'approches non personnalisées et non sémantiques fondées sur critères de popularité et de nouveauté  pour leurs faibles complexités. Plusieurs approches récentes se focalisent sur les technologies de traitement des données massives (p.ex. Apache Spark 2 et Flink 3 ) afin d'assurer le passage à l'échelle  sans prendre en considération l'aspect séman-tique du contenu ni les préférences personnelles de chaque utilisateur. D'autres travaux ont intégré les connaissances extraites à partir du contenu dans le processus de recommandation pour mieux assimiler les facteurs et les thématiques qui influencent les attitudes du consommateur et améliorer ainsi la qualité des suggestions (Capelle et al., 2013). Néanmoins, ces travaux ne tiennent pas compte des contraintes liées à la recommandation en ligne en temps réel. En effet, les approches proposées sont généralement évaluées hors-ligne sur des jeux de données statiques et limités ne reflétant pas les conditions réelles de ce domaine d'application nécessi-tant une gestion continue du flux de données entrant et une capacité à générer en temps réel et à large échelle des recommandations personnalisées et dynamiques.
C'est dans ce contexte que s'inscrit ce travail dont l'objectif est de proposer une approche de recommandation personnalisée d'articles d'actualité traitant l'aspect sémantique du contenu et respectant les contraintes temps réel et de passage à l'échelle du domaine d'application. Notre approche est basée sur l'hybridation de plusieurs approches personnalisées et non personnalisées afin d'éviter le problème de démarrage à froid. Pour valider l'intérêt de la proposition, notre système de recommandation a été évalué pendant le challenge NEWSREEL 2017 dont l'objectif est de faire la recommandation d'articles d'actualité en temps réel et à large échelle pour un ensemble de magazines et de journaux en ligne.
Le reste de cet article est organisé comme suit. La section 2 détaille l'approche proposée ainsi que les composantes du système développé. La section 3 présente les différentes expé-rimentations menées ainsi que les résultats obtenus. Enfin, nous concluons cet article avec un résumé de la contribution et des travaux futurs.  . Les connaissances ainsi extraites serviront par la suite à modéliser les intérêts des utilisateurs (figure 1 (3)) en analysant les contenus des articles avec lesquels ils ont interagi.
Le traitement des données
Les algorithmes de recommandation
Pour faire face aux défis de personnalisation des recommandations et du démarrage à froid, nous proposons une approche hybride par commutation (Burke, 2002) qui sélectionne, selon le cas à traiter, exclusivement une de ses deux approches sous-jacentes. La première approche, nommée PN, est non personnalisée dans la mesure où elle se base sur les critères de nouveauté et de popularité pour évaluer la pertinence d'un article d'actualité. Cette approche est adoptée pour faire face au problème de démarrage à froid et qui se présente dans les cas où l'utilisateur actif est inconnu ou lorsque l'article considéré est nouveau et n'a pas encore été lu ou évalué. Par contre, dans les cas où les données sur les l'utilisateur ou l'article ne sont pas parcimonieuses, l'approche a recours au filtrage par similarité de contenu. Dans ce travail, deux variantes du filtrage par contenu ont été évaluées. La première, nommée KBP, modélise l'article d'actualité avec ses mots-clés alors que la deuxième, nommée NeBP, n'a recours qu'aux entités nommées mentionnées dans le texte.
La proposition de l'approche non personnalisée P N est motivée par des études récentes portant sur la recommandation d'actualité et affirmant la pertinence des critères de popularité et de nouveauté comme indicateurs de l'intérêt que susciterait un article d'actualité chez le consommateur (Kille et al., 2016). Pour estimer la pertinence P N (a) d'un article d'actualité a en fonction de sa nouveauté N (a) et sa popularité P (a), nous proposons l'équation 1 qui adopte une forme exponentielle (Ding et Li, 2005) pour modéliser la décroissance progressive de la popularité au fil du temps et qui a été observée par des travaux existants (Kille et al., 2016). La modélisation de la nouveauté est motivée par le fait que les utilisateurs sont attirés par l'actualité tandis que le facteur popularité permet de mettre en avant les articles ayant suscité un grand intérêt brusque (i.e. un grand nombre de lectures) rendant leurs probabilités de lecture supérieures à celles des articles supposés intéresser l'utilisateur.
Dans la formulation proposée, P (a) et N (a) représentent respectivement le nombre de lectures de l'article a et son âge (en jours), tandis que K est un paramètre spécifiant le taux de dégradation de la popularité (deux à quatre jours (Kille et al., 2016)). Cette formulation permet ainsi de sélectionner les articles ayant le meilleur compromis entre nouveauté et popularité et a le mérite d'être adaptée au contexte de la recommandation temps-réel grâce à sa complexité constante (O(1)).
Les approches KBP et NeBP proposent des recommandations personnalisées basées sur le contenu des articles d'actualité puisqu'elles suggèrent à un utilisateur donné les articles les plus similaires à ceux qu'il a déjà lus dans le passé. Cependant, les deux approches diffèrent au niveau de la représentation des articles et donc celle des profils des utilisateurs. En effet, KBP a recours à un vecteur de termes pondérés par leurs importances pour représenter un article ou l'historique des lectures d'un utilisateur alors que NeBP se limite aux entités nommées. Dans les deux variantes, nous adoptons la mesure de pondération TF-IDF (Pazzani et Billsus, 2007) pour quantifier l'importance d'un mot-clé ou d'une entité nommée dans un article d'actualité. Dans ce contexte, le profil de l'utilisateur s'enrichit au fur et à mesure des articles lus par les termes ou les entités nommées qui y sont présents. Enfin, nous avons recours à une variante de la mesure de similarité cosinus 4 pour calculer la concordance entre un article candidat et le profil de l'utilisateur actif et filtrer ainsi les actualités potentiellement capables de l'intéresser. Le calcul de similarité entre deux documents d 1 et d 2 ou entre un document d et un utilisateur u est proportionnel à la taille de leurs vecteurs représentatifs (resp. O(|d 1 |+|d 2 |) et O(|d|+|u|)). Dans ce travail, le choix du cosinus comme mesure de similarité a été effectué sur la base de leur complexité et sa popularité dans le domaine de la recherche d'information. Son apport par rapport à d'autres mesures devrait être validé par des expérimentations comparatives contrôlées hors ligne.
Expérimentations
Les expérimentations ont été menées sur la plateforme de recommandation d'articles d'actualité de Plista 5 dans le cadre de la compétition NEWSREEL 6 de la conférence CLEF 2017. Cette plateforme collecte les contenus ainsi que les données d'usage sur les sites des éditeurs d'actualités partenaires, les partage en temps réel avec les systèmes de recommandation participants qui doivent en retour fournir des suggestions ciblées en moins de 100ms. La pertinence des systèmes participants est alors évaluée en fonction du nombre de consultations que leurs Le système de recommandation implémentant l'approche hybride NeHyb a été déployé sans connaissances à priori. Au fil du temps, il a collecté et analysé des données émanant de 82336 articles et de 3755547 utilisateurs. Ces traitements ont été menés sur une machine dotée de deux processeurs Intel Xeon E5-26xx avec une fréquence de 2GHz chacun, une mémoire cache de 4Mo, 4Go de RAM et 94Go d'espace de stockage. L'expérimentation nous a permis de valider l'intérêt de l'approche proposée puisque le système déployé a été capable de traiter le flux abondant et véloce des données tout en respectant la contrainte temps réel. En effet, le système a pu traiter jusqu'à 4000 requêtes par minute avec un temps de réponse moyen de 47ms, une occupation moyenne du processeur de 6% et une occupation moyenne de l'espace mémoire de 240Mo.
Conclusions et perspectives
Dans ce travail, nous avons proposé et évalué en conditions réelles une approche hybride, sémantique et personnalisée pour la recommandation des articles d'actualité en ligne. L'approche est capable de respecter les contraintes de la recommandation en temps réel tout en permettant le passage à l'échelle grâce à sa faible complexité et à sa capacité à traiter les flux 7. http ://aksw.org/Projects/FOX.html

Introduction
De nombreuses recherches portant sur le contexte urbain, telles que l'étude du dévelop-pement du commerce de proximité, les études démographiques ou la planification urbaine, nécessitent des données de population à une échelle assez fine, parfois jusqu'au niveau du bâti (Lu et al. (2010); Ural et al. (2011);Langford (2013) ;Sridharan et Qiu (2013)). Les modèles de transport par exemple, qui sont de plus en plus au coeur des questionnements urbains, néces-sitent généralement des données décrivant les populations des habitations et de lieux de travail. En effet, la majorité du transport urbain est liée aux déplacements entre habitation et travail, et la précision des modèles de transport les décrivant est fortement corrélée à la connaissance des populations à l'échelle des bâtiments. Comme ces données ne sont que rarement disponibles, il est important de pouvoir efficacement les inférer à partir de données à plus grande échelle.
Les données de recensement contiennent des informations précieuses pour l'estimation d'une cartographie à haute résolution des population des bâtiments résidentiels et d'activités.
Toutefois, la faible résolution temporelle -due à la fréquence de la collecte des données-et la faible résolution spatiale sont deux désavantages reconnus de cette source de données (Lu et al. (2010)). En conséquence, ce données sont souvent disponibles uniquement sous forme de projection en grille composées de carreaux très larges. Ces derniers sont trop grands pour permettre une estimation directe des populations à l'échelle des bâtiments, et nécessitent donc une étape de désagrégation à l'aide me méthodes diverses (Bakillah et al. (2014)). Sachant que cette étape a pour rôle de prendre en compte la topologie locale des bâtiments, ainsi que leur usage réel, elle nécessite donc des donnée additionnelles absentes des données de recensement.
Une première solution pour obtenir ces données de topologie urbaine repose sur l'usage du LiDAR (Light Detection And Ranging), mais cette solution demande des campagnes de mesure dédiées, reste extrêmement coûteuse, et comporte de nombreux biais (Harvey (2002)). Une seconde méthode se repose sur l'utilisation de campagnes de questionnaires, souvent utilisés pour la construction de modèles de transport. Mais là encore, un biais important est lié au très faible pourcentage de la population interrogée. Une autre source de données, les données d'usage et de couverture du sol (Land Use Land Cover) ont déjà été utilisées pour effectuer une estimation de la population par bâtiment (Mennis (2003); Reibel et Agrawal (2007)), mais elle ne peuvent être utilisées que pour des régions d'intérêt très précises. Elles peuvent par ailleurs être en partie inférées à partir des images satellites, plus faciles à obtenir. Mais si cette méthode est directement applicable au problème de l'inférence d'usage des sols, elle est plus difficilement applicable à l'obtention de donnée d'usage et de couverture du sol : l'intensité d'usage, qui est essentielle pour calculer des estimations de lieux d'emplois et de résidence réalistes, n'est pas directement extractible (Rodrigues et al. (2013)). Il a de plus été montré que les techniques basées sur l'utilisation de données satellitaires voient leur qualité baisser lorsque la densité de population augmente (Danoedoro (2006)). Ainsi, la reconstruction de la topologie des bâtiments reste très problématique sans utiliser des sources de données complémentaires. C'est ainsi le cas dans (Ural et al. (2011)) où il est noté que "l'empreinte et la hauteur des bâtiments sont d'abord déterminés à partir d'images aériennes, de modèle numériques de terrains, et de modèles de surface". Cependant la disponibilité de ce type de données et donc l'applicabilité de cette méthode sont très limitées.
Dans Liu et al. (2008), les auteurs estiment la densité de population à partir d'images satellitaires et de données de recensement. La relation entre la population et la topologie des bâtiments a aussi été étudiée dans le contexte de données LiDAR par Lu et al. (2010), où les auteurs ont montré que l'estimation de population était meilleure en utilisant la surface plutôt que le volume. Cela s'explique par l'homogénéité de l'habitat individuel, ainsi que par les erreurs de classification potentielles liées aux données volumétriques LiDAR. Dans ces travaux, les auteurs utilisent donc des données d'imagerie pour estimer la population locale à petit échelle. Ils ne descendent cependant pas à l'échelle de l'habitation individuelle. Leur méthode repose sur (1) l'identification du nombre d'unités d'habitation, (2) l'extraction des surfaces artificielles liées aux surfaces résidentielles, (3) la classification des types d'usage des sols, et enfin (4)   Touya et al. (2017) montrent que "les variations de contenu dans OSM sont parfois guidées par des désaccords entre les contributeurs, mais au final elles augmentent la qualité des données". Ainsi, la qualité supérieure des données d'OSM a été montrée pour le jeu de données officiel Meridian 2 pour la Grande-Bretagne (Haklay (2010)), et il a été montré que le réseau urbain Allemand est plus complet que celui décrit dans les jeux de données commerciaux (Neis et al. (2011)). Par exemple, celui de Hambourg est complet à 99.8% (Over et al. (2010)). Pour montrer la vivacité de cette communauté, on peut remarquer que le volume de points d'intérêt (Points Of Interest (POI)) en Chine a été multiplié d'un facteur 9 entre 2007 et 2013 (Liu et Long (2016)).
Avant que la qualité des données d'OSM n'atteigne un tel niveau, des données relatives à l'usage des sols à la fois précises et à jour étaient difficiles à trouver, notamment aux échelles continentale et régionale. Heureusement, ce n'est maintenant plus le cas.
Méthodologie
Contribution : Nous proposons dans ce travail une méthode basée sur l'évaluation des surfaces d'habitation pour estimer la population des bâtiments en utilisant uniquement des données libres. Cette méthode vise une applicabilité à n'importe quelle région d'intérêt, pour peu que les données OSM associées possèdent une qualité et une complétude suffisante.
Méthode générale : Tout d'abord, la liste des bâtiments de la zone d'intérêt est extraite de la base OSM. Cette liste est ensuite filtrée selon l'usage déclaré : bâtiment résidentiel ou bâtiment d'usage mixte. Ensuite, la surface d'habitation de chacun est évaluée à partir des annotations des "building parts" composant chaque bâtiment. Enfin, des données annexes de population disponibles sous forme de grille sont utilisées pour calculer le nombre d'habitants de chaque bâtiment, en prenant en compte les ratios des surfaces de ses différents usages. Dans ce travail, nous ré-agrégeons ensuite ces données sous forme de grille, afin de pouvoir comparer la qualité du résultat avec une vérité terrain, mais cette dernière étape n'est pas indispensable pour une application réelle.
Requête OSM
Les données OSM portant sur la région d'intérêt sont obtenues par l'intermédiaire de l'API Overpass, suivant la méthode de Boeing (2017) Toutes les données géographiques sont projetées dans le système de coordonnées UTM (Universal Transverse Mercator) pour réduire les biais en fournissant des distances correctes indépendamment de la zone d'intérêt choisie.
Classification
Les structures géographiques importées à partir de la base OSM (bâtiments, partie de bâ-timents, polygones et points d'intérêts) sont classifiées en fonction de leur usage des sols : "activité", "résidentiel", "mixte" ou "autre". Cela permet de reconstituer les structures des bâ-timents ayant un usage résidentiel complet ou partiel.  Afin de calculer la surface totale de résidence, nous multiplions le nombre d'étages par la surface associée à chaque bâtiment ou sous-partie de bâtiment. La surface associée à chaque usage des sols est calculée incrémentalement en considérant chaque composant de chaque bâtiment. Cela nous permet de prendre en compte le fait que différentes sous-parties peuvent être associées à différents usages, chacune contribuant différemment aux surfaces associées.
Estimation désagrégée de population
La procédure de désagrégation utilise l'information de surface totale d'habitation de chaque bâtiment, ainsi qu'un jeu de données de population sous forme de grille géo-référencée. En l'absence de toute donnée socio-économique géographique, nous faisons l'hypothèse d'une consommation constante de surface par habitant pour tous les bâtiments. Cela introduit un biais qui pourra être levé dans le futur en prenant en compte notamment le type d'habitation et les catégories socio-professionnelles.
Nous utilisons dans ces travaux des données de population libres disponibles sous forme de grille géo-référencée. Un premier jeu de données représente la population mondiale avec une résolution de 1km 2 (Doxsey-Whitfield et al. (2015)). Dans le cas particulier de la France, les données de population INSEE sont utilisables, avec une résolution spatiale de 200m 2 .
Calcul de la population par bâtiment : Nous répartissons cette population proportionnellement en fonction de la surface résidentielle totale calculée dans chaque carré G i de la grille échantillonnant la population. Les bâtiments à cheval sur plusieurs carrés de cette grille voient leur surface totale affectée à chaque carré en fonction du pourcentage d'inclusion de leur empreinte.
Si φ est l'opération d'obtention de la surface d'un polygone, et si R(x) est la fonction qui détermine la quantité totale de surface d'habitation du bâtiment x, alors la surface résidentielle totale du bâtiment B j se situant dans le carré G i est déterminée par :
Ainsi, la somme dees surface résidentielles définit la surface totale incluse dans G i :
Enfin, la population de chaque carré de la grille de population est répartie au sein de chaque bâtiment appartenant à ce carré selon pourcentage de leur surface résidentielle.
Résultats
Nous avons évalué notre méthode de désagrégation de données de population sur plusieurs villes. Pour ce faire, nous avons utilisé des données de population sur une grille d'une réso-lution de 1km 2 pour Manhattan, New York 4 , ainsi que pour des villes Françaises avec une résolution plus fine de 200m × 200m 5 .
Aucune des sources de données n'est exempte d'erreurs, ainsi les estimations de la population ne seront pas nécessairement cohérentes entre elles.
Comme la vérité terrain (population par bâtiment) n'est pas disponible, nous évaluons notre méthode en calculant des données de population à une résolution plus grossière à partir des données connues à une résolution plus fine, en agrégeant 5 × 5 carrés. Puis, nous appliquons notre méthode de désagrégation en partant sur ces données à résolution grossière, ce qui nous permet de comparer le résultat à une vérité terrain et de produire des histogrammes d'erreurs.
Notons que dans le cas de données manquantes (carrés sans données sur la population), l'agrégation est simplement effectuée sur les carrés pour lesquels la population est connue. Les estimations de la population par bâ-timent sont agrégées selon la résolution de la vérité terrain. Pour des bâtiments à cheval sur plusieurs carrés, la répartition de la population est faite de manière proportionnelle comme dans l'équation 1.
Remarquons que l'erreur croît aux bords de la région analysée pour une raison simple : des polygones larges pourraient ne pas être retrouvés aux bords de la ré-gion puisque seuls des polygones complète-ment contenus dans la boîte englobante sont trouvés. Ainsi, la procédure d'inférence de l'usage du sol peut donner lieu à une mauvaise classification, due à ces données manquantes.
Manhattan, New York
La disponibilité de données sur la hauteur ou le nombre d'étages des bâtiments est variable, mais un exemple remarquable est montré sur la figure 1. Elle montre clairement que l'exploitation de l'information sur le nombre des étages est incontournable pour des lieux tels Manhattan. Leur utilisation permet une évaluation plus précise de la surface associée à l'usage résidentiel de chaque bâtiment et améliore ainsi grandement l'estimation de la population par bâtiment.
La figure 2 montre tous les bâtiments retrouvés dans la base OSM, différenciés selon leur usage de sol classifié. La partie droite de la figure montre la surimposition de la grille grossière de population.
France
Plusieurs villes Françaises ont été traitées, allant de grandes villes telles Paris et Lyon, via des villes moyennes telle Toulouse, à des villes de la taille de Grenoble. La figure 4 montre tous les bâtiments récupérés de la base OSM, différenciés selon leur usage de sol classifié. La partie droite de la figure montre la surimposition de la grille fine de population. Le tableau 2 montre les médianes des erreurs relatives et absolues, obtenues pour la mé-thode de désagrégation. Les erreurs relatives sont similaires à travers les différentes villes, même lorsque les densités de population sont différentes. La figure 3 montre les histogrammes des erreurs absolues pour chacune des villes. On peut observer que Paris a la médiane des erreurs absolues la plus élevée même si l'erreur relative médiane y est la plus faible ; ceci est dû à sa densité de population, plus élevée que pour les autres villes testées. 
Conclusions et perspectives
Nous avons présenté une méthode pour l'estimation désagrégée de la population d'une ville, au niveau des bâtiments individuels. Elle n'utilise que des données ouvertes : la surface dédiée à l'usage résidentiel d'un bâtiment est estimée à partir de données OSM, et des données sur la population disponibles sur une grille sont utilisées afin de produire une estimation de la population par bâtiment.
Une validation est proposée qui procède en simulant les données de population à une réso-lution disponible au monde entier à partir de données plus fine disponible en France, puis en y A droite, surimposition de la grille fine de données de population. La couleur jaune indique une densité de population élevée.

Introduction
Un des objectifs de notre recherche est l'identification et l'extraction automatique d'information pertinente à partir de données textuelles provenant d'une multiplicité grandissante de domaines (littéraire, journalistique, scientifique, médical,technique, etc) et de sources (bases de données, bibliothèques numériques, blogues, etc). Afin de permettre un accès efficace à l'information et d'en simplifier l'utilisation nous devons prendre en compte le traitement de corpus de grande taille, la réduction du bruit contenu, le multilinguisme, ainsi que plusieurs tâches de traitement par l'ordinateur et l'utilisateur. C'est pourquoi il est nécessaire d'automatiser certains processus, notamment l'extraction d'entités nommées (noms propres, adresses, dates, etc 1. UNITEX/GRAMLAB a été principalement développé par Sébastien Paumier (2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008)(2009)(2010)(2011)(2012). Son développement se poursuit grâce à une communauté de développeurs et de linguistes.
2. Français, anglais, . . ., grec, russe, arabe (écriture de droite à gauche), thaï et coréen (absence de séparateurs).
Les dictionnaires électroniques
Les dictionnaires électroniques utilisés par UNITEX/GRAMLAB obéissent au format (1). Nous donnons ci-dessous des exemples d'entrées simples comme composées dans lesquelles Hum et Prof indiquent qu'il s'agit de noms (N) humains et de profession.
( 
Les grammaires locales
Les grammaires locales permettent de décrire des patrons linguistiques à l'aide de graphes. Chaque chemin qui conduit de l'état initial à l'état final est un motif accepté. A titre d'exemple, la grammaire locale de la figure 1 permet reconnaître un certain nombre de patrons linguistiques. Nous donnons ciaprès quelques phrases reconnues et non reconnues. 3 Recherche de motifs et annotation 3.1 Une grammaire qui fait appel au dictionnaire Une grammaire peut faire appel au dictionnaire. Le graphe de la figure 2 comprend deux chemins, le premier reconnaît un nom humain <N+Hum> suivi d'un verbe à la troisième personne du singulier de l'imparfait <V:I3s>, le second reconnaît un adverbe qui se termine en ment <ADV><<ment$>> suivi d'un verbe au participe passé <V:K>. De plus cette grammaire a la capacité d'écrire dans le texte et entoure les séquences reconnues de balises Motif et de construire ainsi une annotation. Le résultat de l'application de cette grammaire se présente sous la forme d'une concordance dont un échantillon est visible à droite de la figure. FIG. 2 -Grammaire de reconnaissance de date et les concordances produites
Utilisation d'un sous-graphe et de variables
La grammaire de la figure 3 effectue une normalisation d'une forme basique de date constituée au minimum du numéro de jour (le méta symbole <NB> reconnaît une suite de chiffres) et du mois en lettres, éventuellement précédé du nom de jour et/ou suivi de l'année. Elle utilise dans ce but les variables 3 jour et an qui mémorisent respectivement le numéro de jour et de l'année (si présente). La boîte grisée est un appel au sous graphe mois_norm qui transforme le nom du mois en son numéro 4 . La variable m mémorise la totalité de la date. Ensuite cette grammaire écrit une balise <Date> dans laquelle $an$/$mois$/$an$ constitue la forme normalisée de la date (ordre année, mois, jour) et les $ entourant chaque variable permettent d'afficher leur contenu. La figure 4 donne un extrait d'une concordance de dates normalisées et balisées. 
Conclusion
Nous avons effectué une présentation à la fois brève et détaillée de certaines possibilités d'Unitex/GramLab 6 . Notre logiciel s'utilise à travers deux interfaces écrits en Java UNITEX IDE (classique) et GRAMLAB IDE (orientée projet). Ils appellent le coeur du logiciel écrit en C/C++. Ce dernier est disponible sous la forme d'une API pour C et Java (JNI) qui de surcroît donne accès à un système de fichiers virtuels et à la persistance des ressources. Ces caractéristiques donnent la possibilité d'insérer d'UNITEX/GRAMLAB au sein de chaînes de traitement complexes. Il est utilisé par des universitaires (littéraires, linguistes, sociologues, etc) comme par des entreprises pour effectuer des tâches aussi diverses que l'analyse de textes littéraires, l'analyse de retours clients, la normalisation d'adresses, l'extraction d'opinions, etc. UNITEX/GRAMLAB n'est pas un logiciel dédié à une tâche particulière d'analyse de corpus ou d'extraction d'information mais grâce aux ressources dont il dispose et surtout grâce à celles dont il rend possible l'élaboration et l'utilisation, il permet à chacun de construire une solution la plus conforme à ses besoins et ses attentes.

Introduction
Le Web des Données -encore appelé Linked Data ou Web Sémantique (Berners-Lee et al., 2001) -est constitué de centaines de bases RDF (Klyne et al., 2014) inter-liées formant un vaste réseau de milliards de triplets RDF. Une base RDF est composée d'un ensemble de triplets, où chaque triplet s'exprime sous la forme (sujet, prédicat, objet). Les triplets peuvent être vus comme des phrases élémentaires sujet-verbe-complément, c'est à dire « Le 'sujet' a pour 'prédicat' la valeur 'objet' ». Chaque ressource est identifiée de manière unique au sein de la base RDF où elle est stockée. Les identifiants sont généralement des IRI 1 pour un accès à travers le Web 2 . Pour récolter et manipuler les données d'une base RDF, SPARQL (Prudhommeaux et Seaborne, 2008) est le langage de requêtes recommandé par le W3C. L'écriture d'une requête SPARQL reste cependant difficile pour la plupart des utilisateurs potentiels du Web des Données. En effet, une des raisons principales est qu'il est souvent nécessaire de connaître les IRI des ressources et propriétés manipulées pour pouvoir interroger les bases.
Notre contribution, avec la plateforme universal-endpoint.com, pour l'interrogation du Web des Données est triple. Premièrement, l'utilisateur peut rédiger des requêtes en SimplePARQL, à la manière de SPARQL où des ressources imprécises peuvent être utilisées au sein de triplets -en sujet, en prédicat et/ou en objet. Deuxièmement, la plateforme agit comme une plateforme de correspondance depuis laquelle l'utilisateur peut accéder à différentes bases du Web des Données à la fois. Troisièmement, l'utilisation de SimplePARQL peut-être vu comme une aide à la rédaction de requêtes SPARQL. La figure FIG. 1 présente le schéma de fonctionnement de la plateforme universal-endpoint.com.
Cet article est organisé comme suit. La Section 2 présente les différents services proposés par la plateforme universal-endpoint.com. La Section 3 conclue cet article.
FIG. 1 -Fonctionnement de la plateforme universal-endpoint.com 2 Les services de la plateforme universal-endpoint.com
Requêtes SimplePARQL
Dans les bases RDF du Web des Données, les ressources sont décrites par leurs liens aux autres ressources et leurs liens à des valeurs littérales. La sémantique des bases RDF est donc contenue dans ces relations. Cependant, il y a un écart entre la représentation structurée que perçoit l'utilisateur et celle physiquement présente dans une base RDF. Par exemple, l'information (Einstein, lieu-naissance, Ulm) pour l'utilisateur est concrètement stockée sous la forme (ressource1,propriété2,ressource3), (ressource1,foaf:name,"Einstein"), (propriété2,rdfs:label,"birth place"), (ressource3,rdfs:label,"Ulm"). Le rôle de l'approche SimplePARQL est de réduire cet écart. Pour cela, SimplePARQL est une généralisation du langage SPARQL, utilisant quasiment la même syntaxe et la même grammaire mais où les différentes composantes d'un triplet -le sujet, le prédicat et/ou l'objet -peuvent être des ressources imprécises (en plus de pouvoir être des variables, des IRI ou éventuellement des blank nodes).
Le premier travail théorique sur SimplePARQL date de 2015 (Djebali et Raimbault, 2015), et à cette même date un premier moteur de requêtes SimplePARQL, peu fonctionnel, était disponible. La présentation faite ici par l'exemple repose sur les fonctionnalités de la nouvelle version du moteur SimplePARQL disponible sur universal-endpoint.com depuis octobre 2017.
Supposons que nous cherchions le lieu de naissance d'Albert Einstein. Dans DBpedia, on utiliserait la requête SPARQL REQ. 1, tandis que dans Wikidata la requête REQ. 2. Dans les deux cas, la connaissance de (l'identifiant de) la ressource Einstein et de (l'identifiant de) la propriété birth place est nécessaire. Si ce n'est pas le cas, l'utilisateur -expérimenté -sera amené à écrire une requête plus complexe ou à écrire des requêtes préliminaires pour l'obtention de ces identifiants. Mais alors la vision simple et suffisante sous la forme d'une phrase élémen-taire exprimée par l'unique triplet « Albert Einstein a pour lieu de naissance la valeur inconnue recherchée » n'est plus perçue par l'utilisateur. A chaque fois, le filtrage sur la langue peut être précisé (e.g. /Einstein Albert/@de). Concrètement, une requête SimplePARQL est réécrite en un ensemble de requêtes SPARQL offrant la possibilité de matcher une ressource imprécise avec des ressources RDF dans la/les base/s interrogée/s. En fonction de la position de la ressource imprécise dans le triplet, les requêtes SPARQL sont générées selon des règles de réécriture et selon certaines priorités que nous avons définies (non présentées ici, mais disponibles en ligne sur la plateforme). Les diffé-rents résultats et les différentes pages de résultats sont obtenus selon ces stratégies de réécriture.
REQ. 1 -requête SPARQL pour
La requête REQ. 3 est un exemple de requête SimplePARQL contenant deux ressources imprécises : /Einstein Albert/ et /birth place/. Les résultats de cette requête, fournis par la plateforme universal-endpoint.com, sont présentés en figure FIG. 2 où en plus des valeurs trouvées pour la variable ?p (comme en SPARQL) la plateforme apporte la précision sur les correspondances qui ont été trouvées pour chaque ressource imprécise. REQ. 3 -requête SimplePARQL (multi-bases). Il est intéressant de noter que SimplePARQL cherchant à faire coïncider une ressource imprécise en explorant les voisinages des ressources dans la base RDF questionnée, on peut écrire sa requête par exemple en utilisant le français pour décrire une ressource imprécise. Ainsi la requête SimplePARQL "SELECT ?p WHERE{/Einstein Albert/ /lieu naissance/ ?p}" a des chances d'aboutir si un label en français est associé à la propriété correspondante dans la base.
Plateforme de correspondance
La plateforme universal-endpoint.com agit comme une plateforme de correspondance (un point d'accès central, un hub) depuis laquelle l'utilisateur peut accéder à différentes bases du Web des Données. La plateforme se charge d'interroger en SPARQL les bases sélectionnées via leurs endpoints SPARQL publics, puis de centraliser les réponses. L'utilisation de SimplePARQL trouve toute sa place ici, puisque les requêtes peuvent être écrites sans nécessité l'usage d'IRI qui sont propres à une base, et donc une même requête SimplePARQL peut être utilisée pour interroger plusieurs bases différentes.
Aide à la rédaction de requêtes SPARQL
L'utilisation de SimplePARQL peut-être vu comme une aide à la rédaction de requêtes SPARQL où après chaque requêtage en SimplePARQL, l'utilisateur peut choisir la ressource qu'il souhaite manipuler en lieu et place d'une ressource imprécise. Pour réaliser ceci, sur la figure FIG. 2 l'utilisateur clique simplement sur la double flèche correspondant à la ressource souhaitée pour remplacer la ressource imprécise dans la requête SimplePARQL. Ainsi, à partir d'une requête SimplePARQL l'utilisateur obtient au final (éventuellement par itérations successives) une requête SPARQL -sans ressource imprécise, puisque toutes désambiguïsées.
Conclusion
La plateforme universal-endpoint.com propose un ensemble de services pour un accès simplifié aux données de Web des Données. Les travaux futurs sont dans le groupement de résultats intra et inter-bases lorsque les ressources résultats correspondantes aux ressources imprécises sont similaires (soit même IRI soit ressources liées par une propriété owl:sameAs).

Introduction
De nombreuses entreprises souhaitent être en mesure d'analyser les données qui leur parviennent chaque jour. C'est le cas de l'entreprise EDF avec laquelle ce projet a été effectué. EDF surveille l'évolution des thématiques discutées dans différents types de corpus textuels (réclamations, mails, chatbot, etc.). Un plan de classement prédéfini permet de recourir à des algorithmes de classification supervisée performants afin de placer les différents documents dans des catégories prédéfinies au fur et à mesure de leur arrivée. Cependant, de nombreux documents se retrouvent mal ou même non classés. Cela peut être dû au fait que les catégo-ries évoluent au fil du temps et qu'il est nécessaire de réviser ces plans de classement. Être en mesure de détecter au plus tôt ces tendances nouvelles représente un atout important pour une entreprise. EDF souhaite pouvoir détecter les documents qui ont permis d'amorcer ces évolutions car ils peuvent avoir le potentiel d'anticiper la constitution de nouvelles catégories. Ils constituent une forme d'explication du changement en cours permettant une meilleure interaction avec les utilisateurs du système au sein d'EDF. Afin de surveiller ces évolutions, il est nécessaire de prendre en compte la notion de nouveauté. Dans ce contexte, l'analyse de nouveauté à partir du flux des documents est une piste envisagée sérieusement pour mieux appréhender ces évolutions.
Dans cet article nous voulons étudier la possibilité d'utiliser des méthodes basées sur des thématiques pour améliorer la détection de nouveauté. Nous avons développé une méthode gé-nérique et trois modèles afin de capter les documents nouveaux. Les thématiques construites dans chacun des modèles sont issues de modèles LDA (Blei et al. (2003)) mais il est tout à fait envisageable d'utiliser d'autres modèles comme PLSA (Hofmann (1999)) ou NMF (Lee et Seung (1999)). En plus des modèles développés et au vu de la difficulté d'accessibilité des jeux de données annotés pour la nouveauté, nous avons mis au point une méthodologie permettant d'ajouter artificiellement de la nouveauté dans des données textuelles. Cette méthodologie nous permet de tester plusieurs scénarios d'arrivée (fréquence, volume, etc.) et de mesurer la performance de nos systèmes.
Nous commencerons par définir la notion de nouveauté dans un flux de données textuelles. Nous présenterons ensuite la méthode générique et la déclinaison en trois modèles distincts. Nous montrerons comment nous ajoutons artificiellement de la nouveauté dans notre jeu de données et enfin nous présenterons les différents résultats que nous avons obtenus. En conclusion, nous verrons dans quelle mesure et par quels moyens notre système pourrait être amélioré et dans quels cas il serait particulièrement utile.
État de l'art
La détection de nouveauté peut être définie comme le fait de reconnaître des données qui sont différentes d'une certaine manière des données habituellement traitées. Il est courant de rapprocher les idées de nouveauté et de signaux faibles car les méthodes de détection sont souvent employées sur des jeux de données contenant un très grand nombre d'exemples normaux et peu de données considérées comme "anormales". À partir de cette définition, nous pouvons voir le problème de détection de nouveauté comme un problème de classification à deux classes où nous avons une classe de données "anormales" en faible volume qui doit être distinguée des autres possibilités. Au sein de la littérature, les termes novelty detection sont souvent rapprochés de termes comme anomaly detection et outlier detection . Le dictionnaire Merriam-Webster définit le terme novelty comme ceci : «qui ne ressemble à rien de ce qui a déjà été observé".» Dans (Pimentel et al. (2014)), la détection de nouveauté est classée en cinq catégories distinctes : (1) probabiliste, (2) basée sur la distance, (3) basée sur la reconstruction, (4) basée sur le domaine, et (5) basée sur des techniques de théorie de l'information. La première mé-thode cherche à estimer la densité d'une classe normale et suppose que des zones de basse densité ont peu de chances de contenir des données normales. La seconde approche part du principe qu'une nouveauté va apparaître loin de ses plus proches voisins. Pour (3), il est né-cessaire d'entraîner un modèle de régression et l'observation d'une erreur importante entre la prédiction et la valeur réelle donne du poids à un score de nouveauté. La quatrième méthode va utiliser des méthodes spécifiques aux domaines pour caractériser les données d'entraînement.
Généralement, elles définissent une frontière autour des données dites "normales". L'approche (5) va calculer l'apport informationnel des données d'entraînements grâce à l'entropie ou à d'autres techniques basées sur la théorie de l'information de Shannon. Elle se base sur le principe qu'une nouveauté va modifier significativement le contenu informationnel d'un jeu de données.
Les modèles que nous allons présenter se basent sur des mesures de distances (2 nde approche) pour calculer un score de nouveauté d'un document arrivant dans notre corpus. Les techniques de modélisation thématiques existantes permettent d'associer des termes et des documents qui ont des relations sémantiques et qui sont souvent utilisés au sein d'un même sujet. Afin de générer automatiquement des thématiques, nous utiliserons un modèle de Latent Dirichlet Allocation (Blei et al. (2003)). LDA est un modèle probabiliste utilisé pour décrire un corpus de D documents associés à un vocabulaire de taille V . Dans ce modèle, des variables latentes sont utilisées pour représenter des thématiques présentes dans chaque document. LDA utilise un processus génératif qui permet de simuler la création d'un document. A partir des paramètre α et β, le modèle détermine les variables cachées Z n correspondant aux thématiques. Ces thématiques sont décrites par une distribution de probabilité des termes du vocabulaire sur les thématiques (φ k = p(w n |z k ), où w n est le n-ième mot du vocabulaire) et une distribution de probabilité des thématiques sur les documents (θ d = p(z k |d)). La distribution φ k est, en partie, illustrée dans le tableau 1. Le modèle LDA permet la construction de thématiques à un temps donné. Il sera néces-saire, à terme, de s'intéresser à l'aspect temporel des données afin de construire des modèles pouvant évoluer dans le temps. Plusieurs méthodes ont été présentées ces dernières années et permettent de modéliser l'évolution des thématiques dans le temps, c'est le cas pour (Wang et McCallum (2006), Blei et Lafferty (2006), AlSumait et al. (2008), Wang et al. (2012), et Amoualian et al. (2016)). Ces modèles se basent sur des approches différentes pour modéli-ser l'évolution des thématiques. Certains utilisent des fenêtres temporelles et se basent sur les paramètres α et β pour mettre à jour le modèle (Blei et Lafferty (2006)) tandis que d'autres (AlSumait et al. (2008) lient les distributions de termes-thématiques pour déterminer les β. Il est intéressant de noter que des approches par prédiction (Wang et al. (2012)) ainsi que l'utilisation d'objets mathématiques complexes comme les copules (Amoualian et al. (2016)) ont fait leur preuve. Pour modéliser la détection au plus tôt, des modèles basés sur des mesures physiques comme la vitesse ou l'accélération sont apparus dans la littérature (Xie et al. (2016), He et Parker (2010). Ces méthodes permettent de détecter très tôt des événements qui apparaissent rapidement mais ne détectent pas les nouveautés qui peuvent apparaître progressivement.
Nous avons observé que ces articles ne proposent pas de méthodes d'évaluation quantitatives pour la détection de la nouveauté. C'est pourquoi nous avons décidé de ne pas nous baser sur ces modèles complexes mais plutôt sur des modèles LDA.
Les modèles 3.1 Méthode générique
Dans cette section, nous allons décrire le modèle de façon générique et expliquerons les différents moyens que nous pouvons utiliser afin de l'implémenter.
Nous observons un certain nombre de documents
correspond au moment où nous observons les documents arriver. w h et w c correspondent à la taille des fenêtres temporelles définissant l'historique et le contexte récent que l'on prend en compte. L'historique correspond à un sous-ensemble de documents 
Calcul de distance.
Comme nous l'avons dit précédemment, nous voulons détecter au plus tôt l'émergence de thématiques nouvelles ainsi que différents phénomènes complexes. Ces thématiques sont construites à partir des documents et c'est donc ces derniers qui induisent la nouveauté. Afin de la mesurer au niveau des thématiques, il faut aussi être capable de la mesurer au niveau des documents. Dans notre modèle, nous voulons leur associer un score de nouveauté qui dé-pend des documents de l'historique. Plus le score de nouveauté est grand, plus le document peut être considéré comme nouveau. Nous nous basons sur le fait que la nouveauté apparaît anormalement loin, en termes de distance, de ses plus proches voisins. Notre fonction de scoring score doit agréger des calculs de dissimilarité des documents par rapport à l'historique et au contexte. Cette dissimilarité peut être calculée par rapport à plusieurs ensembles. Une première idée consiste à calculer la dissimilarité entre les documents du contexte et de l'historique :
Plusieurs fonctions sont connues dans la littérature pour calculer la dissimilarité entre documents. Si nous considérons − → x un vecteur de dimension n où n correspond au nombre de mots dans le vocabulaire. − → x peut représenter un document sous la forme de sac de mots ou bien les mots les plus probables d'une thématique LDA. Nous pouvons utiliser la divergence Cosine, la divergence de Kullback-Leibler symétrique ou encore la divergence de Jensen-Shannon :
Modèle de comparaison documents-documents
La Figure 2 représente une comparaison des termes présents dans les documents de l'historique et du contexte récent. Le modèle consiste à calculer une distance entre tous les documents deux à deux : à chaque document arrivant dans la fenêtre de contexte, nous le comparons avec tous les documents de l'historique. Pour la comparaison entre les documents, nous pouvons utiliser les mesures citées dans la partie 3.2. Nous avons choisi d'utiliser une divergence cosine car elle est connue pour avoir une performance élevée (Strehl et al. (2000)). Les scores associés aux termes correspondent au TF-IDF de chaque terme estimé sur le corpus. Une fois les distances calculées, nous obtenons une matrice de distance avec, en ligne, les documents du contexte (là où nous voulons trouver la nouveauté) et, en colonne, les documents de l'historique. Nous avons vu dans l'état de l'art qu'un document nouveau est anormalement loin de ses voisins. Nous ne voulons donc pas comparer un document du contexte avec tous les documents de l'historique mais seulement avec les documents qui y ressemblent fortement donc avec ses plus proches voisins. Le but est d'identifier les documents isolés dont même les plus proches voisins sont anormalement loin. Afin d'agréger les résultats, nous faisons donc une moyenne sur les distances par rapport aux k plus proches voisins. Plus ce score est élevé, plus le document est susceptible d'être nouveau.
Modèle de comparaison thématiques-documents
La Figure 3 introduit la notion de thématiques dans le modèle. Ce dernier permet de comparer les termes des documents du contexte récent avec les termes les plus probables des théma-tiques de l'historique. Comme nous l'avons dit en présentant le modèle LDA, une thématique est décrite grâce à sa distribution de probabilité sur les termes et sur les documents. Nous prenons les 100 termes les plus probables par thématique et nous pouvons donc comparer les documents avec les thématiques dans un même espace. Pour ce modèle nous choisissons d'utiliser la distance cosine en prenant, pour score, la probabilité des termes dans la thématique avec laquelle on compare. Bien que la comparaison entre un score TF-IDF et une probabilité ne soit pas mathématiquement rigoureuse, nous considérons les probabilités comme un score d'appartenance à une thématique, ce qui nous permet d'utiliser les deux mesures dans la même expression. Ce modèle détermine rapidement si un document fait partie des données «normales»ou s'il se trouve loin des thématiques de l'historique.
Modèle de comparaison thématiques-thématiques
La Figure 4 représente une variante du modèle précédent dans le sens où, au lieu de comparer directement les documents du contexte avec les thématiques de l'historique, nous allons construire des thématiques sur ces documents puis déterminer si une ou plusieurs théma-tiques peuvent être considérées comme nouvelles (étape (a)). Pour comparer les thématiques du contexte avec celles de l'historique, nous calculons la distance entre les termes les plus probables de chaque thématique. Nous agrégeons les résultats de la même manière que dans la partie 3.3. Afin de sélectionner seulement les thématiques vraiment nouvelles, nous fixons un seuil qui correspond à : threshold = µ(score) + σ(score). Ce seuil traduit la notion
FIG. 3 -Comparaison des documents avec les thématiques de l'historique
d'anormalement distant des thématiques précédentes. Une fois les thématiques nouvelles identifiées, nous allons utiliser les documents les plus probables de celles-ci et comparer leurs termes avec les thématiques de l'historique (étape (b)). Cela permet de déterminer quels sont les documents responsables de la nouveauté de la thématique. Ensuite, nous allons comparer les termes des documents les plus probables au sein de ces thématiques nouvelles avec les termes des thématiques de l'historique (étape (b)). Cela permet de déterminer quels sont les documents responsables de la nouveauté de la thématique.
FIG. 4 -Comparaison des documents des thématiques nouvelles avec les thématiques de l'historique
Expérimentation
Méthodologie
Il n'existe pas de jeux de données facilement accessible où la nouveauté est annotée par document. Autrement dit, nous n'avons pas de vérité terrain qui permettrait d'observer si les systèmes classent bien les documents annotés comme nouveaux. On a donc dû constituer notre propre vérité terrain. Pour cela, nous avons utilisé un jeu de données où chaque texte est déjà associé à une catégorie. En utilisant ce type de jeu de données, nous pouvons simuler artificiellement de la nouveauté. Pour cela, nous avons constitué nos documents de l'historique en enlevant la totalité d'une catégorie. Pour la suite nous introduisons, dans nos documents de contexte, une partie des articles de la catégorie enlevée précédemment. Cela nous permet de contrôler la quantité de nouveauté que nous introduisons et nous pouvons donc étudier l'impact que cela a sur nos modèles.
Jeu de données
Les données de l'entreprise EDF sont des données sensibles, donc nous avons choisi un jeu de données public (Tang et al. (2012) Nous avons répété l'opération 100 fois pour avoir des documents différents à détecter et donc des résultats plus stables. Les résultats de nos mesures sont commentés dans la partie suivante.
Résultats
Les résultats que nous présenterons dans cette partie sont composés de deux parties. Dans un premier temps, nous allons observer des mesures d'AUC (Area Under Curve) moyennes qui permettent de quantifier la qualité de détection de nos systèmes. Pour rappel, l'AUC mesure l'aire sous une courbe ROC, c'est une mesure très utilisée dans le domaine de la recherche d'information et nous pouvons l'utiliser car nous avons ramené notre problème à un problème de classification binaire. Les articles cités dans l'état de l'art utilisent une mesure de perplexité pour mesurer la qualité de leur thématique mais ne propose pas de mesure quantitative pour la détection de la nouveauté. Dans un second temps, nous présenterons la précision à 100 de nos systèmes. Cette mesure nous permet de nous concentrer uniquement sur le début de classement de nouveauté : c'est-à-dire les documents qui sont fortement susceptibles d'être présentés à des experts métiers. Il est important de noter que notre méthodologie permet de détecter des nouvelles catégories mais ne mesure pas la nouveauté apparaissant au sein des catégories : par exemple, entre 1990 et 2005, nous pouvons imaginer que les articles de recherche sur le thème des bases de données ont beaucoup évolué et n'utilisent pas forcément les mêmes termes. Notre système n'intègre pas encore ces changements et cela peut expliquer les scores de détection pas à la hauteur de nos attentes.
Au niveau des mesures d'AUC, il est intéressant de constater qu'à partir de seulement 5 nouveaux documents introduits dans l'ensemble de contexte, on peut observer des différences dans les tendances de détections entre les modèles. En effet, dans la Table 3, quand on ajoute 5 nouveaux documents, on voit qu'il est plus difficile de détecter des catégories comme datamining et medical. Au contraire, les catégories theory et visu ont l'air d'être assez faciles à détecter (AU C > 0.7). Lorsqu'on utilise des modèles thématiques, que ce soit pour résumer l'historique (Table 4) ou pour la fenêtre de contexte courant (Table 5), nous remarquons que la tendance s'inverse et que la catégorie medical est plus facile à détecter. Bien sûr, les catégories database et datamining restent assez difficiles à détecter. Cela peut s'expliquer par le fait que ces deux catégories partagent beaucoup de termes en commun et que les thématiques LDA ne font pas la différence entre ces deux ensembles. Au contraire, il semble que les catégories theory et medical utilisent des termes plus spécifiques qui permettent d'avoir des thématiques plus facilement identifiables. Enfin il est intéressant de noter que, dans le troisième modèle (Table 5), la première étape identifie les nouvelles thématiques assez bien car on retrouve bien les nouveaux documents insérés dans leur liste des 100 documents les plus probables. Les résultats présentés dans la Table 6 montrent le nombre moyen de documents nouveaux que l'on retrouve dans les 100 premiers documents classés par score de nouveauté (2.5% du classement). C'est, en quelque sorte, un zoom sur le début de la courbe ROC. Ces 100 premiers documents sont destinés à être présentés à des experts métiers pour interprétation. La première colonne montre le niveau de détection lorsque l'on compare deux à deux les documents du contexte et de l'historique par rapport aux termes qu'ils utilisent. On peut voir que les scores sont très faibles pour les catégories datamining et medical : c'est-à-dire que les documents nouveaux ne sont pas les plus éloignés, en termes de distance, de leurs plus proches voisins. Pour ce modèle, qui compare les documents deux à deux, la quantité de nouveauté introduite n'a pas d'effets sur la détection. La deuxième colonne présente les mêmes résultats une fois que l'on a résumé nos documents de l'historique sous forme de thématique LDA. Pour cela nous avons utilisé le modèle présenté en partie 3.4. On peut voir une nette amélioration des scores relatifs aux catégories theory et medical. Une légère baisse de la catégorie visu et une baisse significative pour les catégories database et datamining. Cette dernière observation nous confirme que ces deux catégories posent des problèmes à cause des termes qu'elles utilisent : en effet, elles partagent beaucoup de termes et les nouveaux documents database sont forcément proches des thématiques relatives à la catégorie datamining dans l'historique.Lorsque l'on résume notre fenêtre de contexte sous forme de thématiques (partie 3.5), on observe la même tendance que précédemment, à part pour la catégorie visu qui semble rester assez constante. Cette méthode permet d'identifier certains types de documents nouveaux sans comparer tous les documents : l'étape d'identification des nouvelles thématiques permet de diminuer le nombre de documents à comparer (100 par thématiques nouvelles identifiées). Aussi, les modèles thématiques permettent de manipuler des matrices plus légères (en comparant thématiques de l'historique et du contexte) par rapport à la comparaison de documents deux à deux. 
Conclusion et perspectives
Dans cet article, nous avons commencé par apporter des précisions sur la définition de la nouveauté. Nous avons listé des articles de l'état de l'art sur des techniques qui nous ont inspirées ou nous inspireront pour la suite. Nous avons ensuite évalué la capacité des modèles thématiques à détecter de la nouveauté dans des flux de données textuelles en développant une méthodologie qui, à partir de données classées, nous permet d'introduire artificiellement de la nouveauté. Cette méthodologie pourrait être améliorée en utilisant d'autres jeux de données où les catégories n'évolueraient pas dans le temps : cela permettrait d'éviter la détection de nouveauté intra-classes. Nous avons montré que les modèles thématiques peuvent être utilisés pour la détection de nouveauté dans certains cas. Les modèles que nous avons développés sont des modèles simples basés uniquement sur des mesures de distances et nous observons des différences significatives par rapport à un modèle basé uniquement sur la comparaison des termes. Nous n'avons pas étudié l'influence des différents paramètres utilisés. Il serait intéres-sant d'observer à quel point le nombre de plus proches voisins utilisés ou encore le nombre de thématiques influent sur la précision du modèle. Pour la suite de ce travail, il est question de se baser sur des méthodes thématiques temporels présentés dans AlSumait et al. (2008) 


Introduction
Avec l'émergence du web, les systèmes de recommandation sont devenus des outils incontournables pour découvrir de nouveaux objets, en particulier pour les sites de e-commerce (Ricci et al. (2011)). La multiplication des contenus multimédias a également contribué à ce phénomène, comme le montre le challenge Netflix de 2009 1 . Cet engouement s'est propagé à d'autres domaines, que ce soit la recommandation d'articles scientifiques (Beel et al. (2016)) ou de lieux (Bobadilla et al. (2013)). Malgré ces avancées, peu de travaux portent sur les mobilités géographiques et résidentielles. Si celles-ci sont courantes (e.g., premier travail, mutation professionnelle, alternance ou stage), il s'avère dans bien des cas que les personnes qui démé-nagent ne connaissent pas leur future ville d'habitation et ses quartiers. Comme le choix d'un quartier a un impact significatif sur la vie des personnes, nous proposons un nouvel outil de comparaison entre quartiers permettant d'accompagner les personnes dans leur mobilité.
La recommandation de quartiers soulève différents problèmes. Premièrement, la notion de quartiers (i.e., définition, délimitations, perception) est floue et sujette à différentes interprétations (Authier et al. (2007)). Un second problème concerne la collecte de données. Les systèmes de recommandation de lieux exploitent essentiellement les réseaux sociaux géo-localisés, et les "likes", "checkins" ou évaluations servent de données pour construire et comparer des profils d'utilisateurs (Wang et al. (2013); Li et al. (2016) 
Un outil de comparaison entre quartiers
Le prototype inclut d'abord une étape de préparation (collecte des données) avant d'appliquer des algorithmes de comparaison ou de similarité entre quartiers.
Intégration de données. Pour collecter une quantité suffisante de données, nous utilisons l'unité géographique de l'INSEE appelée IRIS (Ilots Regroupés pour l'Information Statistique). Un IRIS est une maille du territoire, découpé de façon relativement homogène (même nombre d'habitant.e.s). Il est défini par un code, son contour (ensemble de coordonnées géo-graphiques), mais aussi par plus de 800 indicateurs bruts (e.g. le nombre de boulangeries, la répartition par type de logement ou par catégorie socio-professionnelle) répartis dans une vingtaine de sources INSEE. Ces informations sont complétées par d'autres sources pour les prix moyens au m 2 (au niveau de la commune ou d'un arrondissement). Les indicateurs collectés pour chaque IRIS sont très spécifiques et doivent être regroupés en indicateurs synthétiques pour pouvoir être traités de façon systématique sur un grand nombre d'IRIS. Nous avons donc une trentaine d'indicateurs synthétiques (e.g., loisirs, commerces de proximité). Le voisinage d'un IRIS est également pris en compte pour se rapprocher du concept de quartier (un IRIS étant généralement plus petit qu'un quartier). Les mappings requis par cette intégration ont été développés à la main, et les données intégrées sont stockées au format GeoJSON.
Comparaison de quartiers. Pour l'élaboration du prototype, nous nous sommes placés dans le cas hypothétique où les clients recherchent un quartier similaire à celui dans lequel ils habitaient auparavant. Huit algorithmes sont utilisés pour recommander ou regrouper des quartiers similaires. Ils prennent en entrée un ou plusieurs quartiers de départ (e.g., domicile actuel, autres quartiers appréciés) et un point d'arrivée (e.g., lieu du nouvel emploi). Les quartiers candidats sont ceux situés dans un rayon donné autour du point d'arrivée. L'algorithme similarité cosine permet, dans notre contexte, de calculer directement le degré de ressemblance entre 2. Airbnb locations, http://www.airbnb.fr/locations 3. Datavilles, http://www.rue89lyon.fr/?p=88778 N. Barret et al.
le quartier de départ et les quartiers candidats. Notre algorithme de l'écart-type construit un profil "utilisateur" à partir de plusieurs quartiers de départ. Notre hypothèse est qu'un écart-type faible, pour un indicateur, signifie que ce dernier est important. Chaque indicateur du profil a donc un poids calculé selon l'inverse de son écart-type. Ensuite la mesure cosine est appliquée entre le profil construit et les candidats. Les algorithmes de clustering (Birch, Kmeans, Mean Shift, Mini Batch KMeans et Spectral Clustering) peuvent être appliqués dans le cas de la recommandation et dans le cas d'un regroupement. Dans le premier cas, les quartiers candidats sont répartis dans des clusters, puis on détermine le cluster le plus proche pour le quartier de départ et les candidats appartenant au cluster choisi sont recommandés. Les algorithmes de clustering utilisés pour le regroupement permettent de détecter directement les quartiers les plus similaires au quartier contenant le point d'arrivée. Enfin l'algorithme de type SVM (one-class SVM) exploite plusieurs quartiers de départ comme des exemples positifs pour l'apprentissage. Un vecteur de caractéristiques représentatif est construit à partir des coefficients calculés pendant l'apprentissage, et ce vecteur est utilisé comme "quartier de départ fictif" d'un autre algorithme (e.g. similarité cosine ou clustering).
Scénarios d'utilisation
Le prototype VizLIRIS est développé en Python et utilise la librairie scikit-learn 4 pour les algorithmes de recommandation et regroupement ainsi que la librairie gdal 5 pour les aspects cartographiques. Plusieurs paramètres (e.g., rayon de recherche, nombre de recommandations) sont réglables au niveau de l'interface. Dans la suite, nous présentons deux cas d'utilisation. Scénario 2 : regroupement de quartiers. Bob est sociologue et étudie les populations de Paris. La fonctionnalité regroupement de VizLIRIS lui permet de trouver les quartiers les plus similaires dans une zone donnée. La figure 1b montre les groupes obtenus avec l'algorithme Birch dans un rayon de 3 kilomètres. Bob remarque par exemple que plusieurs zones partagent les mêmes caractéristiques de population, par exemple les berges (couleur violet foncé) ou la périphérie (couleurs bleu ciel et rouge).
Ces deux scénarios illustrent les capacités de VizLIRIS à recommander ou regrouper des quartiers. En terme de perspectives, il est envisagé d'ajouter des justifications pour aider à comprendre les résultats, de faciliter la sélection d'un algorithme et de développer un algorithme pour détecter les quartiers en regroupant les IRIS adjacents similaires.

Introduction
De nombreuses applications requièrent un stockage et une manipulation de bases de données distribuées (Özsu et Valduriez, 2011). Le plus souvent, la centralisation des données est impossible à cause de contraintes légales ou techniques. Ainsi, Zhang et Zaki (2006) soulignent l'importance d'étendre la découverte de connaissances aux bases de données distribuées. Par exemple, les données du web sémantique sont réparties sur plusieurs triplestores accessibles uniquement via des requêtes SPARQL. Dans ce contexte, les propriétés décrivant une même entité (e.g., Paris) sont réparties sur plusieurs sites (e.g., Wikidata ou GeoNames). Cet article vise à extraire directement des motifs au sein de telles bases de données distribuées.
Peu de travaux de la littérature se sont intéressés à la découverte de motifs dans des bases de données distribuées (Cheung et al., 1996;Otey et al., 2003;Jin et Agrawal, 2006;Kum et al., 2006). Ces propositions se sont focalisées sur une extraction exhaustive des motifs en fusionnant les extractions réalisées localement sur chacun des sites. Malheureusement, le volume de données à transmettre entre les différents sites exige un coût de communication bien supérieur à la centralisation des données car les motifs sont nombreux par nature et les multiples extractions génèrent de multiples doublons. De plus, le coût de calcul de ces extractions parallèles est prohibitif même si des techniques d'élagage les diminuent sensiblement en contrepartie de coûts de communication supplémentaires (Zhu et Wu, 2007;Zhu et al., 2011). Afin d'éviter le coût inéluctable d'une extraction exhaustive, nous proposons de fournir des motifs à la demande en bénéficiant de l'échantillonnage de motifs (Al Hasan et Zaki, 2009;Boley et al., 2011). L'utilisateur peut à tout moment obtenir un échantillon de motifs dont le tirage sera proportionnel à leur intérêt. En plus d'être peu coûteux à extraire, ils s'avèrent utiles dans de nombreuses tâches comme la classification (Boley et al., 2011), l'extraction d'outliers (Giacometti et Soulet, 2016) ou l'exploration interactive de données (Giacometti et Soulet, 2017).
Cet article cherche à tirer des motifs ensemblistes proportionnellement à une mesure d'inté-rêt dans une base de données distribuée en ayant les mêmes garanties que si toutes les données avaient été centralisées. Après avoir formalisé notre problème dans la section 3, nous proposons un algorithme générique appelé DDSAMPLING (Distributed Database Sampling) qui tire aléatoirement un motif au sein d'une base de données distribuée proportionnellement à une mesure d'intérêt (cf la section 4). De manière originale, il autorise une nouvelle classe de mesures d'intérêt (comprenant notamment la fréquence, l'aire et la contrainte de taille maximale). A notre connaissance, notre proposition est la première approche d'extraction de motifs qui autorise un partitionnement vertical ou hybride des données. Par ailleurs, nous démontrons l'exactitude de DDSAMPLING et étudions sa complexité. En deuxième lieu, dans la section 5, nous évaluons la qualité de DDSAMPLING face aux défaillances de communication ou aux pannes de certains sites sur différents jeux de données. De manière intéressante, la proportionnalité du tirage est peu altérée.
Travaux relatifs
Cet article concerne la découverte de motifs sur une base de données distribuée. Cette problématique diffère de la parallélisation d'algorithmes d'extraction où les calculs distribués sont opérés sur un jeu de données unique ou sciemment distribué pour accélérer les calculs.
Plusieurs approches de la littérature se sont intéressées à l'extraction de motifs fréquents sur une base de données distribuée. Cette tâche est complexe car quel que soit la fréquence exigée sur la base de données distribuée (fréquence globale), il n'est pas possible de contraindre la fréquence locale sur un fragment sans communiquer des informations entre sites. Dans ce cadre, Cheung et al. (1996) proposent le premier travail pour extraire tous les motifs globalement fréquents en identifiant les sites où les motifs sont les plus fréquents et en réduisant ainsi un peu les échanges. De manière plus drastique, Otey et al. (2003) proposent d'écono-miser les échanges en se limitant à la collection des motifs fréquents maximaux. Pour ne pas avoir à énumérer tous les motifs de chaque fragment et limiter les échanges, Jin et Agrawal (2006) imposent un seuil minimal de fréquence sur chaque fragment. A partir des différentes extractions locales, Kum et al. (2006) construisent une collection globale approchée des motifs fréquents. Un élagage centralisé proposé par Zhu et Wu (2007) repose sur la construction d'un arbre contenant pour chaque motif toutes ses occurrences (i.e., couples fragment/transaction), ce qui requièrt encore un volume d'échanges considérable. Plus récemment, Zhu et al. (2011) parviennent à mettre en oeuvre un élagage décentralisé au sein de l'extraction de chaque fragment en échangeant des filtres de Bloom. Cette approche réduit significativement les temps de calculs mais le coût des communications amoindri demeure important. En effet, le volume de motifs extraits génère invariablement un coût de communications énorme bien supérieur à celui de la centralisation des données. En outre, toutes ces approches d'extraction de motifs fréquents se restreignent à un partitionnement horizontal des données, une même transaction ne pouvant pas être distribuée sur deux fragments distincts. Enfin, les propositions de l'état de l'art requièrent d'avoir une capacité de calcul importante sur chaque fragment ce qui n'est pas toujours possible. Par exemple, le web sémantique offre un accès à des données distribuées via une requête SPARQL mais il n'est pas possible d'y exécuter une routine d'extraction. Ainsi, pour éviter des coûts de calcul sur chaque fragment et des coûts de communications prohibitifs, nous optons pour une extraction de motifs à la demande. L'extraction de motifs à la demande consiste à extraire en un temps très court un motif sans avoir pré-calculé auparavant une énorme collection de motifs. Par exemple, de nombreuses mé-thodes heuristiques (Dietterich et Michalski, 1983)   (Boley et al., 2011). Afin de marcher aléatoirement d'un motif X à un autre, les méthodes stochastiques requièrent d'accéder à l'intérêt global de tous les motifs voisins à X. Par exemple, dans le cas de la fréquence, il faudrait connaître la fréquence globale de tous les sous-ensembles et de tous les sur-ensembles de X, ce qui occasionnerait de nombreux coûts de communication. Pour cette raison, nous préférons adopter une procédure aléatoire de tirage en plusieurs étapes. Il est vrai que cette technique a déjà été déclinée pour plusieurs mesures d'intérêt (e.g., support, aire (Boley et al., 2011) ou mesure d'exception (Moens et Boley, 2014)) et plusieurs types de données (e.g., données numériques  ou séquentielles (Diop et al., 2018)). Néanmoins, le contexte des bases de données distribuées constitue un challenge orthogonal. Par simplicité, nous nous limitons dans cet article aux motifs ensemblistes, mais notre approche considère différentes mesures d'intérêt.
3 Formalisation du problème
Motifs ensemblistes et bases de données distribuées
Soit I un ensemble fini de littéraux nommés items. Un itemset ou motif X est un sousensemble non vide de I. La taille de l'itemset X, notée |X|, est sa cardinalité. L'ensemble de tous les itemsets définis sur I, dénoté par L, s'appelle le langage. Dans ce contexte, une base de données D est un ensemble de couples définis sur N×L : Intuitivement, une base de données distribuée est juste un ensemble de bases de données où les transactions de chaque fragment ne se chevauchent pas :
Définition 1 (Base de données distribuée/centralisée) Une base de données distribuée P = {D 1 , . . . , D K } est un ensemble de bases de données (appelées fragments) tel que pour tout
Dans la suite, sauf indication contraire, P = {D 1 , . . . , D K } désigne une base de données distribuée de K fragments et D correspond à sa base de données centralisée. On dit que la base de données distribuée P = {D 1 , . . . , D K } est un partitionnement de la base de données centralisée D. Par exemple, dans la figure 1, on constate que la base de données distribuée
} est un partitionnement de la base de données centralisée D. Dans ce cas, la transaction 2 est répartie sur les fragments D 2 , D 3 et D 4 . De manière plus générale, suivant la répartition des données sur les fragments, on distingue deux partitionnements particuliers de D. Si chaque transaction est contenue sur un seul fragment, on parle de partitionnement horizontal. Si chaque item est contenu sur un seul fragment, on parle de partitionnement vertical. Un partitionnement hybride est un partitionnement qui n'est ni horizontal, ni vertical (e.g., partitionnement de la figure 1). Par exemple, dans le cas des données du web sémantique, une entité est décrite sur plusieurs triplestores. Dans ce cas, il est pertinent d'extraire des motifs sur l'ensemble de la base de données distribuée pour rechercher des corrélations entre les propriétés de triplestores distincts.
Pour finir, comme indiqué auparavant, nous considérons un environnement avec une base de données distribuée où il est seulement possible de demander à un fragment k, la taille d'une transaction d'identifiant j et l'item à la position i d'une transaction j : port défini comme la proportion de transactions de la base de données D contenant le motif X : supp(X, D) = |{(j, T ) ∈ D : X ⊆ T }|/|D|. Avec l'exemple de la figure 1, on a supp(DF, D) = 2/5 car le motif DF apparaît dans les transactions 2 et 3. Il est courant d'associer aussi une utilité à un motif en fonction de sa taille comme c'est le cas avec la mesure d'aire : supp(X, D) × |X| × |D|. De ce fait, nous nous intéressons à une famille de mesures d'intérêt de la forme supp(X, D) × u(X) où u est une fonction d'utilité fondée sur la taille :
Définition 2 (Utilité fondée sur la taille) Une fonction d'utilité u est dite fondée sur la taille ssi il existe une fonction f u : N → R telle que u(X) = f u (|X|) pour tout motif X.
Dans la suite, et par simplicité, toute utilité d'un motif réfère à une fonction d'utilité fondée sur la taille. Par l'exemple, l'utilité u area = |X| × |D| permettra de considérer la mesure d'aire supp(X, D) × u area (X) et dans ce cas, on a f uarea () = × |D|. L'utilité u ≤M définit par 1 si |X| ≤ M et 0 sinon imposera une contrainte de taille maximale car avec la mesure induite supp(X, D)×u ≤M (X), un motif dont la cardinalité est strictement supérieure à M sera jugé inutile (quel que soit sa fréquence). Notons qu'avec la fonction d'utilité u freq (X) = 1, nous considèrerons tout simplement le support comme mesure d'intérêt. Enfin, de manière intéressante, notons que le produit ou la somme de deux fonctions d'utilité fondées sur la taille reste une fonction d'utilité fondée sur la taille. Ainsi, u area × u ≤M définira comme utile un motif avec une grande aire tant que sa cardinalité n'excède pas le seuil M . L'échantillonnage de motifs vise à tirer aléatoirement un motif X parmi le langage L avec une probabilité π = m(X)/Z où m est une mesure d'intérêt et Z une constante de normalisation. Formellement, nous noterons ce tirage de la manière suivante : X ∼ π(L). Dans la suite, L désigne le langage commun à tous les fragments de la base de données distribuée. Soient une base de données distribuée P = {D 1 , . . . , D n } et une fonction d'utilité fondée sur la taille u. Notre objectif est de tirer un motif 
Approche sans centralisation des données : DDSAMPLING
Algorithm 2 DDSAMPLING
Input: Une base de données distribuée P = {D1, · · · , DK } et une utilité fondée sur la taille u Output: 
X := X ∪ {itemAt(i , j, D k )} et ϑ := ϑ ∪ {i} 12: od 13: return X Cette section présente notre algorithme DDSAMPLING (pour Distributed Database Sampling, voir Algorithme 2) dont la force est de ne centraliser aucun item (sauf ceux qui seront tirés pour construire les motifs). Il prend en entrée une base de données distribuée et une fonction d'utilité fondée sur la taille afin de retourner un itemset X tiré proportionnellement suivant son intérêt supp(X, D) × u(X). L'idée clé de notre approche consiste à centraliser la taille de chacune des transactions des différents fragments (plutôt que tous les items). Ces seules informations permettent ensuite de calculer aisément les différentes distributions de tirage et permettent de localiser les fragments utiles à la construction de l'itemset à tirer. Matrice de pondération Nous commençons par introduire la matrice de pondération qui quantifie pour chaque transaction combien d'items sont stockés sur chaque fragment :
Définition 3 (Matrice de pondération) La matrice de pondération M de la base de données distribuée
En pratique, la matrice de pondération M est pré-calculée hors ligne (ligne 1) et utilisée à volonté par la suite pour tirer de nombreux échantillons au sein de l'algorithme 2. Par ailleurs, nous définissons la somme des valeurs de la ligne j, i.e.
, qui correspond à la taille de la transaction d'identifiant j. Par exemple, le tableau 1 présente la matrice de pondération pour la base de données jouet de la figure 1, et on a Calcul des poids ω(j) et ω (j) La première utilité de la matrice de pondération est de permettre un calcul aisé des poids ω(j) et ω (j). Pour rappel, ces poids correspondent respectivement à la somme des utilités des motifs de la transaction j et à la somme des utilités des motifs de taille dans la transaction j. Tout d'abord notons que la somme total ω(j) correspond à la somme de tous les poids ω (j) : ω(j) = Mj =0 ω (j). Concentrons-nous maintenant sur le calcul de la somme des utilités des motifs de taille dans la transaction j. Intuitivement, comme tous les motifs de même taille ont la même utilité, il suffit de dénombrer le nombre de motifs de taille et de multiplier cette quantité par l'utilité f u (). La propriété suivante formalise ce calcul :
Propriété 1 Soient M la matrice de pondération d'une base de données distribuée P = {D 1 , . . . , D K } et une fonction d'utilité fondée sur la taille u. Le poids des itemsets de longueur de la transaction j est défini par :
L'utilisation de cette propriété est illustrée dans le tableau 1 pour trois fonctions d'utilités fondées sur la taille, u freq , u area et u ≤2 . Par exemple, comme f u freq () = 1 pour toute taille , on a ω freq (1) = 3 =0 3 = 1 + 3 + 3 + 1 = 8 = 2 3 (où 3 est le nombre d'itemsets de taille dans une transaction de taille 3). En considérant l'aire, on a ω area (1) = 3 =0 3 × = (1 · 0) + (3 · 1) + (3 · 2) + (1 · 3) = 3 + 6 + 3 = 12 = 3 · 2 3−1 car f uarea () = . Enfin, avec la contrainte de cardinalité maximale de 2, on a ω 2 (1) = 2 =0 3 = 1 + 3 + 3 = 7. Ainsi, dans l'algorithme 2, le tirage de l'identifiant de transaction j et le tirage de la longueur de l'itemset qui sera échantillonné (ligne 3-5) reprennent le principe de l'algorithme 1. Seule la méthode de calcul des distributions diffère en s'appuyant sur la matrice M grâce à la propriété 1. Construction de l'itemset Les lignes 4 à 11 détaillent le tirage de l'itemset sur l'ensemble des fragments. L'idée est de tirer sans remise une position d'item i dans la transaction d'identifiant j (ligne 8) et de rechercher le fragment k disposant de cet item (ligne 9). Il faut alors calculer la position i de cet item au sein du fragment k (ligne 10) avant de l'interroger (ligne 11). On répète cette procédure fois (ligne 7) et pour éviter de tirer deux fois le même item, on maintient l'ensemble des positions déjà tirées ϑ. Avec l'exemple du tableau 1, si on a le tirage de la position 2 (i.e., i = 2) dans la transaction 1, le fragment retenu est k = 2 avec i = 2 − 1 = 1, et l'item itemAt(1, 1, D 2 ) = B est ajouté à X.
Analyse théorique de DDSAMPLING
La propriété suivante démontre que DDSAMPLING retourne un échantillon de manière exacte :
Propriété 2 (Correction) Soient P = {D 1 , ..., D K } une base de données distribuée et u une fonction d'utilité fondée sur la taille. L'algorithme 2 effectue le tirage aléatoire d'un motif X de L proportionnellement à supp(X, D) × u(X).
La complexité de la méthode DDSAMPLING se décompose en deux phases : celle du pré-traitement correspondant à la pondération des lignes de la matrice, et celle du tirage d'un motif. Pour chaque phase, il est pertinent de déterminer la complexité en temps et en communication.
Complexité temporelle Pour le prétraitement, DDSAMPLING remplit la matrice M avec une complexité de O(|D| · |P|). Ensuite, il pondère les lignes de la matrice avec une complexité de O(|D| · |I|) à cause de la fonction binomiale. La complexité temporelle du prétraitement est donc O(|D|·(|P|+|I|)). Pour le tirage d'un motif, la première étape consiste à sélectionner une ligne de la matrice, soit une complexité de O(log(|D|)). Deuxièmement, on tire uniformément un itemset de la transaction ayant pour identifiant le numéro de la ligne précédemment tiré avec une complexité de O(|I|). Finalement, nous obtenons une complexité temporelle de tirage d'un motif égale à O(log(|D|) + |I|). Complexité en communication Pour rappel, les coûts de communication correspondent aux requêtes sizeOf et itemAt. Dès lors, pour le prétraitement, la construction de la matrice de pondération M requiert O(|D 1 | + · · · + |D K |) échanges ce qui est souvent très inférieur au coût de la centralisation complète en O(||D 1 || + · · · + ||D K ||) = O(||D||) échanges. Dans notre exemple jouet, le calcul de la matrice de pondération requiert 10 requêtes contre 17 pour la centralisation complète de D. Pour le tirage d'un motif, il est nécessaire d'effectuer autant de requêtes itemAt que d'items contenus dans l'itemset à retourner. La complexité temporelle moyenne est donc égale à la longueur moyenne l des itemsets tirés, cette longueur étant donnée par la formule suivante : l = lmax
Ces expérimentations évaluent le coût de communication de DDSAMPLING par rapport à une solution centralisée et l'impact de défaillances sur sa performance (problèmes de communication ou de pannes de sites). Protocole expérimental Dans toutes les expérimentations réalisées, nous avons choisi comme fonction d'utilité des contraintes de taille maximale (u ≤M avec M ∈ [1..5]) et de taille minimale (u ≥m avec m = 1). Un tel choix permet d'éviter de tirer trop de motifs peu fréquents, en particulier lorsque les jeux de données considérés contiennent de longues transactions. Les expériences ont été conduites avec 5 bases de données de l'UCI (archive.ics.uci.edu/ ml). Pour obtenir des bases de données distribuées 1 , nous avons fragmenté uniformément chaque jeu de données en K = 10 fragments. Pour générer un partitionnement horizontal, chaque transaction est placée aléatoirement dans un fragment donné avec la même probabilité 1/K. Pour un partitionnement hybride, tous les items d'une transaction sont placés indépen-damment et aléatoirement (avec la même probabilité 1/K) dans un fragment donné. Enfin, pour générer un partitionnement vertical, chaque item est placé aléatoirement sur un site donné (avec la même probabilité 1/K). La table 2 récapitule les caractéristiques des jeux données utilisés dans les trois premières colonnes (nombre d'items, de transactions et taille globale). Toutes les expérimentations sont réalisées sur un PC de 2.71 GHz 2 Core CPU avec une RAM de 12 GB.  En suivante ce protocole, nous avons évalué la qualité des échantillons construits en faisant varier la probabilité p ∈ {0.00, 0.10, 0.20} qu'une communication avec un site échoue, ou qu'un nombre z ∈ {0, 2, 5} de sites soient en panne (les sites en panne étant tirés aléatoire-ment). Les résultats obtenus sont représentés à la figure 2. De manière générale, les courbes obtenues montrent que les défaillances n'altèrent pas significativement l'exactitude du tirage si leur niveau reste modéré (p ≤ 10% et au plus z = 2 sites sur K = 10 sont en panne), et ceci quel que soit le type de partitionnement. Plus précisément, dans le cas de problèmes de communication, les motifs les plus fréquents sont les plus tirés car ils sont généralement les plus courts et ont ainsi moins de chance d'être rejetés. Ils seront donc sur-représentés dans l'échantillon construit. Dans le cas de sites en panne permanente, on constate que la probabilité de tirage d'un motif reste proportionnelle à sa mesure d'intérêt. Pour le cas vertical, tous les motifs tirés le restent de manière exacte (leurs items étant toujours accessibles au cours du temps). Plus le nombre de sites en panne est important, moins il y a de motifs disponibles. Du coup, les motifs restants sont plus fréquemment tirés augmentant la pente des courbes. représente l'évolution du taux de rejet moyen en fonction de p pour M ∈ {1, 2, 3, 4, 5}. Pour une valeur de p fixe, on constate que ce taux de rejet augmente avec M . Néanmoins, il reste inférieur à 50% si p est inférieur à 0.1 ce qui constitue déjà un niveau élevé de défaillance. La partie droite de la figure 3 représente les taux de rejets en cas de panne de sites. Tout d'abord, ils sont plus faibles dans le cas d'un partitionnement horizontal où le taux de rejet est égal à z/K. En moyenne, ils sont plus élevés dans le cas de partitionnements hybride ou vertical. Enfin, l'écart-type des taux de rejets moyen est plus élevé dans le cas d'un partitionnement vertical. En effet, avec un partitionnement vertical, les taux de rejets seront plus ou moins élevés si les items des motifs les plus fréquents sont ou pas sur les sites en panne. Pour finir, on note que le taux de rejet moyen reste inférieur à 50% si moins de 10% des sites sont en panne. Comme pour le cas de défaillances de communication, ce taux de rejet reste acceptable du point de vue du temps de calcul nécessaire pour construire un échantillon.
Conclusion
Cet article propose le premier algorithme d'échantillonnage de motifs ensemblistes depuis une base de données distribuée. Il permet de considérer différentes mesures d'intérêt et surtout, des partitionnements hybrides ou verticaux. Grâce à la seule centralisation des tailles, les coûts de communications de l'approche sont raisonnables car les échanges d'items sont réalisés uniquement lorsque l'utilisateur demande des motifs. De plus, l'étude expérimentale souligne la robustesse de l'approche. Dans les travaux futurs, nous envisageons de remplacer le tirage exact des transactions par une méthode stochastique afin de ne plus avoir à centraliser les tailles de toutes les transactions de chacun des fragments. Il serait également intéressant de proposer un mécanisme de correction des poids afin de contre-balancer la panne d'un site.

Introduction
Un système de recommandation peut effectuer principalement trois sortes de tâches ) : prédire l'évaluation qu'un utilisateur associerait à un produit ; prédire les produits qu'un utilisateur aurait sélectionnés à travers une liste classée de N produits ; et enfin, prédire la classe d'appartenance d'un produit (classification).
Dans notre cas, la recommandation de pneumatiques, nous visons à proposer un classement de pneus pertinents vis-à-vis des utilisateurs. Pour cela, nous devrons aussi, selon l'approche utilisée, prédire l'évaluation que donnerait un utilisateur envers les produits. Cependant, le cas particulier d'un comparateur en ligne de pneumatiques, qui est notre cas applicatif, nous impose des contraintes au niveau des données. En effet, nous n'avons à notre disposition aucun profil d'utilisateur, aucune évaluation explicite d'un utilisateur envers un produit, ni aucune information validant le fait qu'un utilisateur ait bien acheté un produit après avoir été redirigé vers un site marchand. Nous devons donc nous appuyer sur les éléments restants, à savoir les interactions implicites des utilisateurs ainsi que leurs parcours (sessions) et les caractéristiques techniques des produits. Ces contraintes, ainsi que notre volonté d'explorer les modèles de factorisation de matrices et d'apprentissage profond, nous ont amenés à effectuer les choix suivants parmi les principales approches de l'état de l'art (cf. fig. 1). Deux approches ont ainsi retenu notre attention. Le Memory-Based Collaborative Filtering (e.g. Hu et al. (2008);Gong (2010); Bokde et al. (2015); He et al. (2016He et al. ( , 2017) qui se base sur l'hypothèse suivante : les utilisateurs ont tendance à acheter les mêmes produits que d'autres utilisateurs ayant des goûts similaires. On veut ainsi regrouper les utilisateurs similaires, afin de proposer à un utilisateur des produits qu'il na pas encore consultés mais avec lesquels les autres membres de son groupe ont interagi. Pour déterminer si des utilisateurs ont des goûts similaires, on peut par exemple comparer le score qu'ils ont attribué aux produits. On dispose alors de la note d'un utilisateur envers un produit (explicite) ou on la déduit des interactions (implicite). Enfin l'approche Session-Based (e.g. Hidasi et al. (2015); Quadrana et al. (2017); Tuan et Phuong (2017)), qui consiste à traiter le problème de la recommandation sous la forme d'une séquence d'événements (ordonnés chronologiquement) où l'objectif est de prédire l'élément suivant pour chaque interaction. Contrairement à une vision plus classique, cette méthode a l'avantage de pouvoir se passer des évaluations que les utilisateurs peuvent donner aux produits. Ainsi, elle se concentre uniquement sur les produits avec lesquels les utilisateurs vont interagir et de quelle manière. Dans cet article, nous présentons dans un premier temps les méthodes d'apprentissage qui feront l'objet d'une comparaison dans l'étude. Puis, nous détaillerons les données et le protocole expérimental utilisés. Enfin, nous terminerons par les résultats de notre étude comparative et statistique.
Méthodes d'apprentissage pour la recommandation
Notre objectif, est d'étudier les performances de ces deux approches à travers des techniques de factorisation de matrices et d'apprentissage profond. Notre choix s'est porté sur cinq modèles issus de l'état de l'art car ils présentent des caractéristiques variés et n'ont pas été évalués dans des conditions similaires (métriques, taille du classement et nombre de produits). Session-Based Implicite + Annexe CNN Tuan et Phuong (2017) TAB. 1: Résumé des caractéristiques des modèles sélectionnés pour l'étude.
Le premier modèle, Element-wise Alternating Least Square, possède deux implémenta-tions. Un algorithme offline qui sert à pré-entraîner le modèle grâce aux données de départ et un algorithme online pour traiter le nouveau flux de données en temps réel. Mais, la particularité de ce modèle est sa gestion des produits populaires. On entend par populaire le fait qu'un produit soit très présent sur l'ensemble des interactions utilisateur-produit. Pour cela, l'eALS calcule la confidence d'un produit, soit la probabilité qu'un produit n'ait pas été vu par les utilisateurs. En effet, plus un produit est populaire, plus les chances qu'il soit recommandé (ou du moins visible sur la page) pour les utilisateurs sont importantes. Dans ce cas, il faut apporter un poids plus important au fait que l'utilisateur n'ait pas interagi avec lui, non pas parce qu'il ne l'a pas vu, mais bien parce qu'il ne l'intéresse pas.
Le Neural Collaborative Filtering, est une hybridation de deux modèles pouvant être utilisés indépendamment pour produire une recommandation. Le GMF (Generalized Matrix Factorization) va modéliser les facteurs latents des interactions de manière linéaire. Le MLP (Multi-Layer Perceptron) va apprendre la fonction d'interaction, de manière non-linéaire, afin de modéliser lui aussi les interactions utilisateur-produit. Enfin, une fois que l'on a les poids de sortie de chacun des deux modèles, on les concatène pour produire le score d'un utilisateur envers un produit.
Le Deep Matrix Factorization est lui aussi une hybridation, mais entre une factorisation de matrices et un réseau de neurones DSSM (Deep Semantic Similarity Model), qui ne sont pas indépendants car le DSSM travaille sur la sortie de la factorisation de matrices (matrice d'interaction). La particularité du DSSM est qu'il peut calculer le vecteur latent d'un utilisateur sur une «branche» et le vecteur latent d'un produit sur une deuxième. Chaque branche va ainsi transposer le vecteur dans un nouvel espace à plus faible dimension, au fur et à mesure des couches. La fonction cosinus sera ensuite appliquée sur la concaténation de la sortie de chaque branche pour calculer le score d'une interaction utilisateur-produit.
Le GRU4Rec est un RNN (Recurrent Neural Network) utilisant l'architecture GRU (Gated Recurrent Unit). Ses différentes mémoires et portes lui permettent de traiter des séquences d'événements. Ainsi, l'idée est de prédire le prochain produit avec lequel l'utilisateur va interagir en fonction des produits précédents. Comme ce modèle utilise des interactions explicites, nous avons recréé artificiellement des données explicites via nos données implicites en attribuant des poids à chaque type d'interaction utilisateur-produit (cf. section 3 pour les types d'interactions). Ces poids agissent ainsi comme une note qu'aurait attribué un utilisateur envers un produit. Nous avons attribué ces poids de façon arbitraire en fonction de la proximité du type d'interaction avec un achat potentiel.
Enfin, le 3D CNN (Convolutional Neural Network) est un réseau de neurones qui utilise des convolutions en trois dimensions, afin d'encoder l'aspect temporel d'une séquence d'interactions entre un utilisateur et des produits. L'objectif est le même que pour le GRU4Rec, à la différence que l'on n'utilise pas seulement l'ID des produits, mais aussi des informations annexes. Dans notre implémentation, nous utilisons la nature de l'interaction utilisateur-produit ainsi que l'ID de l'utilisateur à qui appartient la session, afin de faire le lien avec d'autres sessions pouvant lui appartenir.
Étude comparative sur des données de pneumatique
Nos jeux de données proviennent tous d'un fichier log du comparateur en ligne de pneumatique de l'industriel. Ce fichier répertorie l'ensemble des interactions utilisateur-produit depuis septembre 2016 jusqu'en avril 2018. Le tableau suivant résume leur volumétrie. Parmi les types d'interaction utilisateur-produit possible, nous en avons sélectionné cinq selon leur présence dans le fichier log et leur pertinence pour la recommandation (présentés par ordre croissant de leur poids) : InfoProduit : Affichage de la fiche détaillée du produit cible. Quelques caractéristiques sont déjà visibles pour le top 3 des produits recommandés. VoirOffre : Affichage des revendeurs qui vendent le produit cible avec leur localisation. InfoMarchand : Affichage d'une fiche avec les informations détaillées du revendeur. AfficherNuméroMarchand : Génération puis affichage d'un numéro pour appeler le centre du revendeur. N'indique en aucun cas si l'utilisateur a bien appelé le centre. AcheterEnLigne : Redirection vers le site du revendeur. N'indique en aucun cas si l'utilisateur a bien acheté le produit par la suite.
Enfin, pour mener à bien des tests statistiques, nous avons créé quatre autres jeux de données en découpant en deux les deux premiers. On obtient ainsi les jeux Pattern A, Pattern B, Product A et Product B.
En ce qui concerne notre méthodologie, les cinq modèles effectuent leur recommandation sur tous les produits d'un jeu de données. Ils sont ensuite évalués par le HR (Hit Rank) et le NDCG (Normalized Discounted Cumulative Gain) sur des top 100 et 200. Le HR va mesurer la capacité du système à donner toutes les solutions pertinentes. Dans le cadre de la recommandation, cela correspond à indiquer si oui ou non le produit cible est bien présent dans la recommandation top N . Si l'on a recommandé le produit cible à chaque fois, alors on aura un score de 1. Le NDCG, va mesurer la qualité de notre classement. Ainsi, plus notre produit cible sera en haut du classement, plus la valeur sera proche de 1.
Pour l'étude statistique nous avons utilisé les tests suivants (Demsar (2006)) : 1. Test de Friedman : le but est de réfuter l'hypothèse suivante : «Tous les algorithmes ont des performances équivalentes». Une p-value de 0, 05 sera utilisée ; 2. Test de Nemenyi : une fois le test de Friedman validé, on peut utiliser celui-ci pour montrer qu'un algorithme ou un groupe d'algorithmes est significativement plus performant qu'un autre. La confiance utilisée sera de 0, 1 à cause du faible nombre de jeux de données pour mener cette étude.
6. Une session égale un utilisateur dans cette approche.
La table 3 présente nos résultats, avec en gras le meilleur algorithme de la ligne. Chaque valeur représente la moyenne obtenue sur dix expériences, tandis que l'intervalle de confiance est calculé par l'écart type.  
Conclusion
Dans ce papier, nous avons présenté cinq modèles appartenant à deux approches de la recommandation, Memory-Based CF et Session-Based. Notre étude sur des données réelles (cf. table 3), a montrée que la vision en session des données est significativement meilleure (cf. figures 2) que celle du collaborative filtering, pour une quantité d'information équivalente (cf. table 2 : eALS, NeuMF et GRU4Rec) et des données de même nature (implicites). La mauvaise performance des hybrides, peut s'expliquer en partie par la difficulté à les paramétrer (grand nombre de paramètres). En outre, il est à noter la bonne performance de notre implémentation du 3D CNN, qui pourrait être encore plus importante en utilisant plus d'informations annexes


Summary
Software Heritage est une initiative à but non lucratif dont l'objectif ambitieux est de collecter, préserver et partager le code source de tous les logiciels jamais écrits, avec leur historique de développement complet, en construisant une base de connaissances logicielle universelle. Software Heritage répond à une variété de besoins : préserver nos connaissances scientifiques et technologiques, améliorer le développement et la réutilisation des logiciels pour la société et l'industrie, favoriser la science ouverte et construire une infrastructure essentielle pour des études logicielles reproductibles à grande échelle. Nous avons déjà collecté plus de 4 milliards de fichiers sources uniques provenant de plus de 80 millions d'origines. Manipuler ce gigantesque ensemble de données est une mission complexe et nécessite de nouvelles approches pour stocker et requêter l'information d'une manière compatible avec la croissance explosive du développement logiciel collaboratif. Dans cette conférence, nous explorons quelques uns des nouveaux défis et opportunités que présente Software Heritage.
-5 -

Introduction
Avec plus de 4 milliards d'internautes, le nombre d'utilisateurs des médias sociaux en 2018 est estimé à 3,196 milliards, dont 9 sur 10 ont accès aux plateformes choisies via un appareil mobile. Environ 76% des utilisateurs de ces plateformes ont tendance à exprimer leurs sentiments en cliquant sur les boutons comme "J'aime", "Je n'aime pas", etc.. . 50% des internautes expriment leurs opinions et sentiments sur les médias sociaux à l'aide d'émoticônes, d'emojis ou de smileys. Concernant les personnes qui s'expriment en arabe, on retrouve du texte avec 30% de caractères en arabe, 26% de caractères en latin, pour exprimer principalement des idées en anglais ou en français, et environ 15% combinent les deux caractères (Salem, 2017).
Partant de ce constat, et du fait des nombreuses invasions qu'à connu l'Algérie, romaine, byzantine, arabe, turque, espagnole et française, où une réalité socio-linguistique assez complexe est constatée, nous nous intéressons aux posts exprimés dans le dialecte algérien (DALG) pour faire de l'analyse des opinions et des émotions. Nous devons prendre en compte sa diversité langagière où le multilinguisme est omniprésent dans la société algérienne, influençant ainsi le langage d'expression dans les réseaux sociaux. Dans les conversations usuelles, l'arabe dans sa variété soutenue n'est pas utilisé dans les conversations familiales , amicales, etc... Plus de 99% des Algériens utilisent le tamazight et l'arabe algérien. Il s'agit des langues maternelles de la région. Environ 73% parlent l'arabe algérien et 27% une variante de tamazight 1 .
Ceci s'est répercuté sur l'usage du dialecte dans les échanges sur les réseaux sociaux. Ces dialectes ont moins de normalisation et de standardisation (Saadane et Habash, 2015). Exemple, "Bonjour, labas" est une expression du DALG qui combine un mot français et le mot "labas" écrit en latin et qui correspond phonétiquement au mot arabe " ", signifiant "ça va ?". Comme les études de l'analyse d'opinions et d'émotions (AOE) arabophones se concentrent principalement sur l'Arabe Standard Moderne (ASM) (Al-Smadi et al., 2017), (Baly et al., 2017), il existe peu de ressources (lexiques, ontologies, corpus, ...) pour le DALG.
Afin de pallier à cela, nous avons développé une plateforme ouverte pour l'annotation manuelle de tweets exprimés en DALG. Elle nous permettra d'abord de constituer un corpus annoté, pour ensuite créer un modèle de prédiction de polarité pour enrichir les métadonnées du modèle à entraîner.
Pour cela, nous avons organisé notre article comme suit. Dans la section 2, nous présentons certains travaux liés à notre problématique en précisant leurs limites. Puis, nous introduisons notre plateforme d'annotation multilingue TWIFIL dans la section 3 en mettant l'accent sur certaines spécificités et défis du DALG. Avant de conclure et rappeler quelques perspectives à notre travail, des tests sont réalisés dans la section 4.
Travaux connexes
Les approches proposées pour l'AOE en arabe se concentrent essentiellement sur l'ASM, et ne fournissent que la classification de la polarité des sentiments (neutre, positif et néga-tif). De plus, les recherches sur les dialectes nord africains sont si rares qu'on peut dire qu'ils sont inexistants. Les premières solutions ont appliqué les outils du Traitement Automatique du Langage Naturel (TALN) conçus pour l'ASM directement sur le DALG. Cependant les performances étaient très faibles. Ceci met l'accent sur la nécessité du développement de solutions et de ressources propres à l'analyse du DALG (Saadane et Habash, 2015). (Saadane et Habash, 2015) ont présenté une orthographe conventionnelle pour le DALG qui peut être utilisée dans la plupart des applications de TALN, comme l'analyse des sentiments, où ils ont défini des règles phonétiques standards à suivre afin de faciliter la traduction automatique des variantes du DALG et de l'arabe classique. (Mataoui et al., 2016) ont proposé une approche d'AOE basée sur le lexique pour le DALG. Un ensemble de ressources se résumant en lexique des mots de négation, lexique des mots d'intensification, une liste d'émoticônes avec leurs polarités assignées et un dictionnaire des phrases communes du DALG ont été proposés. Puis, le calcul de la polarité s'est basé sur un ensemble de données annotées manuellement et les ressources citées précédemment.
SIAAC : Sentiment Polarity Identification on Arabic Algerian Newspaper Comments est un corpus proposé par (Rahab et al., 2017). Il est dédié à la classification de polarité des textes recueillis sur le site Internet d'Echorouk (un journal algérien). Les classificateurs Support Vector Machines et Naïve Bayes ont donné des résultats satisfaisants en terme de précision dans les deux modèles. L'utilisation de bigrams a également augmenté leur précision.
Il est évident, d'après les travaux cités, que les ressources accessibles au public pour l'AOE du DALG sont rares. Et celles disponibles, comme celle de (Mataoui et al., 2016) ne donne que la polarité des textes sans aucune information sur l'émotion exprimée ou son émetteur.
Pour pallier à ce problème, nous avons utilisé l'opinion publique en créant une plateforme ouverte pour l'annotation de tweets que l'on présentera dans ce qui suit.
Plateforme d'annotation collaborative
Le DALG est moins normalisé que I'ASM. Il a un vocabulaire inspiré de l'arabe, modifié phonologiquement et morphologiquement (Meftouh et al., 2012). Nous devons tenir compte de ce genre de spécificités et bien d'autres dans le développement de notre plateforme.
Défis et spécificités de DALG
Nous présentons un certain nombre de spécificités et défis du DALG que nous allons considérer dans le traitement du dialecte pour TWIFIL :
Alternance codique, ou code-switching : il s'agit d'une alternance d'au moins deux codes linguistiques (langues ou dialectes) dans une conversation ou même dans une phrase. L'internaute algérien altèrne entre deux ou plusieurs langues, dans le contexte d'une même conversation. Par exemple, " la serviette" emploie un mot arabe et un mot français, qui signifie "donne-moi la serviette". Cependant, le DALG est aussi formé par une transformation des mots des langues qui ont inspiré les Algériens à travers les âges. Prenons le mot " " qui s'inspire du mot arabe " " qui signifie "une tasse", où la dernière lettre est passée de " " à "". Codage d'une langue en utilisant l'alphabet d'une autre langue : Il s'agit d'expressions arabes écrites en lettres latines ou "arabizi". Par exemple "khdma kbira", écrite en arabe comme " " signifie "tâche énorme". Ou bien, faire l'inverse, comme dans l'exemple " " qui correspond à l'expression anglaise "bye bye".
On peut aussi combiner le code-switching et le codage des langues dans différents alphabets.
Utilisation de chiffres au lieu de lettres ou de mots : Les chiffres ressemblant à certaines lettres et syllabes arabes ont été exploités par la jeunesse algérienne dans les réseaux sociaux. Par exemple, le 7 remplace la lettre "" et le 9 remplace le " ". Dérivés du DALG : en Algérie, les régions est et ouest ont des accents totalement différents. Prenons l'expression "femme" en arabe, à l'est c'est" : m'ra", à l'ouest ce sera " : shiira". Idiomes et expressions : Elles sont utilisées généralement à des fins sarcastiques ou pour passer un message indirectement. Par exemple " , , ...." est une façon de demander un potde-vin. Le sens exact de l'expression " " est "café" et " " est un nom commun. Ces diversités linguistiques requièrent une attention particulière, c'est pourquoi les dialectes écrits sont des langues très riches et variées 2 .
TWIFIL
TWIFIL (TWIter et proFIL) est une plateforme publique accessible à tous via le web 3 ou mobile 4 . Elle a été conçue pour le DALG afin de faciliter la génération de corpus et la construction d'un dictionnaire dialectal algérien. Les lexiques (L1, L2) proposés par (Mataoui et al., 2016) ont été utilisés puis enrichis.
Annotation des données et enrichissement du dictionnaire : les annotateurs contribuent à l'enrichissement de la platforme en donnant : -la polarité du texte partagé ; -l'émotion ressentie en lisant le tweet ; -le domaine du texte ; -une estimation de l'âge et du genre de l'auteur ; -la polarité et l'émotion d'un mot ou d'un idiome du dialecte algérien. -Les utilisateurs peuvent également contribuer à enrichir l'aspect lexical du dialecte, en ajoutant de nouveaux mots, différentes orthographes des mots et différents mots liés, ainsi que des idiomes (ces mots/idiomes sont validés par un administrateur). TWIFIL a été rendu public le 13/09/2018. Nous avons récolté 7000 tweets (2623 positifs, 2468 négatifs et 1909 neutres). Ils ont été validés par les administrateurs et annotés par 24 personnes de différents domaines et différents âges. Les annotations ont été exploitées comme suit :
-Pour la polarité, nous avons exporté la moyenne des valeurs données de l'intervalle -10 (Très négatif) à 10 (Très positif). Avec 0 pour la polarité neutre. -Pour l'émotion, nous avons choisi l'émotion la plus ressentie du tweet. Même chose pour le contexte et le genre. -Pour l'âge de l'auteur du tweet, nous avons considéré la moyenne de la médiane des classes d'âges choisies par les annotateurs (de douze à soixante-six ans organisés en classe de trois ans par classe (12-15)...  Le corpus nous a servi à faire de la prédiction/classification de polarité. Cependant il peut être utilisé pour plusieurs applications dans le cadre de l'AOE. On peut faire de la prédic-tion/classification des sentiments et même il est possible de prédire le genre et/ou l'âge des utilisateurs, etc.
Prétraitement des données : les algorithmes d'apprentissage nécessitent généralement des données numériques. Un prétraitement des tweets bruts collectés et annotés est nécessaire. Le processus que nous avons suivi est décrit par la figure 1. Le vecteur de mots (bag of words) est remplacé par un vecteur booléen de 3000 entrées (nombre de mots les plus courants dans le corpus) où la présence d'un terme du vocabulaire dans le texte est spécifiée.
FIG. 1 -Prétraitement des données
Tests
En guise d'illustration de l'emploi du corpus, nous nous sommes focalisés sur la tâche de classification automatique d'opinion en nous basant sur les réseaux de neurones. Il faut noter que cette tâche ne fait pas appel à la totalité des informations offertes par le corpus. Nous avons aussi effectué une série de tests pour choisir la meilleure longueur du vecteur booléen, ainsi que la meilleure architecture du réseau de neurones, obtenue grâce à la fonction "softmax" en 50 itérations et 100 neurones par couches interne. Les résultats sont prometteurs avec une faible perte (Figure 3 4 Conclusion L'analyse de l'opinion et de l'émotion du DALG est difficile en raison de la morphologie complexe de la langue. Certaines spécificités liées à ce dialecte ont été présentées. Nous avons mis en avant le manque de ressources accessibles au public pour l'analyse des sentiments.
Ceci, nous a amené à développer une plateforme ouverte pour l'annotation publique de Tweets en DALG "TWIFIL", créant ainsi un corpus qui a servi à faire de la prédiction/classification

Introduction
D'ici 2050, les villes auront 2,5 milliards d'habitants de plus qu'aujourd'hui 1 et plus des deux-tiers de la population mondiale sera urbaine. Cette projection impose de repenser les méthodes de gestion urbaine pour faire face aux challenges qui en découleront. Dans cette optique, une approche consiste à tirer parti des données décrivant les activités des citadins pour découper les villes en zones fonctionnelles, capables de fournir une meilleure compréhension de la structuration spatiale et socio-économique des cités.
En effet, les zones fonctionnelles urbaines sont un formidable outil d'aide à la décision pour la planification urbaine et sont également utiles pour la recommandation de circuits touristiques ou de sites d'implantation de nouveaux commerces. Cependant, délimiter automatiquement des zones fonctionnelles urbaines peut s'avérer complexe car le concept de zone fonctionnelle n'a pas, à l'heure actuelle, de définition harmonisée et les segmentations de villes obtenues dans l'état de l'art sont difficiles à valider comme montré dans Farmer et Fotheringham (2011). Toutefois, cela n'influe pas sur l'intérêt scientifique porté à la délimitation des zones fonctionnelles, au vu de la multiplicité des travaux de recherche traitant du sujet. Lesdits travaux se fondent le plus souvent sur des méthodes de clustering que l'on peut répartir en deux grandes familles. D'une part, les approches statistiques, développées par les auteurs qui assimilent les zones fonctionnelles urbaines aux zones résidentielles, commerciales et autres. D'autre part celles à base de graphes qui sont proposées par des chercheurs considérant les zones fonctionnelles urbaines comme des régions marquées par de fortes interactions internes. Nos travaux se positionnent dans le second groupe car l'objectif de notre étude est d'identifier des zones hétérogènes abritant de fortes interactions socio-économiques. En d'autres termes, nous recherchons des villes au sein de la ville.
Dans cet article nous présentons donc une approche à base de graphe pour délimiter les zones fonctionnelles en utilisant à la fois des données de mobilité et des points d'intérêt (POI), sachant que dans les autres approches basées sur les graphes seules les données de mobilité sont exploitées. Le reste de l'article est organisé comme suit : nous donnons plusieurs défini-tions de la notion de zone fonctionnelle dans la section 2. Nous présentons ensuite plusieurs travaux proches de notre proposition dans la section 3. Puis nous décrivons notre méthodologie pour extraire les zones fonctionnelles dans la section 4 et présentons les résultats obtenus en section 5 avant de conclure dans la section 6.
Le concept de zone fonctionnelle
Le problème de la délimitation des zones fonctionnelles est fréquemment et diversement abordé dans la littérature. Cependant, à notre connaissance, aucune défintion consensuelle n'a pu être donnée jusqu'ici à la notion de zone fonctionnelle. Plusieurs propositions coexistent, chacune ayant sa particularité. Le plus souvent elles mettent l'accent sur les interactions socioéconomiques au sein des zones fonctionnelles.
Par exemple, l'OCDE (OCDE, 2002) définit une zone fonctionnelle comme une unité territoriale résultant de l'organisation de relations sociales et économiques de sorte que ses frontières ne reflètent pas de particularités géographiques ou historiques. En général, elle est organisée autour d'un ou de plusieurs noeuds de sorte que les zones alentour soient connectées à ce(s) noeud(s) à travers différents systèmes (transports, communications, travail, échanges)  (CSIL, 2015). Dans le même ordre d'idées, Karlsson (2007), postule qu'une zone fonctionnelle est caractérisée par une agglomération d'activités et par des infrastructures de transport intrarégionales qui facilitent la mobilité des individus et des produits à l'intérieur de ses frontières.
D'autres auteurs se font plus précis, en restreignant les interactions socio-économiques au cadre du marché du travail et donc aux déplacements travail-maison. Ainsi, selon Antikainen (2005), une zone fonctionnelle est une zone de mobilité domicile-travail. Elle est une agglomération de lieux de travail qui attirent la main d'oeuvre des régions environnantes. Sa qualité la plus importante est sa capacité à dépasser les frontières administratives. Farmer et Fotheringham (2011), abondent dans le même sens en considérant les zones fonctionnelles comme des régions géographiques au sein desquelles les interactions en termes de trajets domicile-travail sont maximisées et entre lesquelles ces interactions sont plutôt minimisées.
Au delà des interactions socio-économiques, une zone fonctionnelle est aussi définie comme un territoire possédant une fonction particulière (zone résidentielle, commerciale, touristique, etc.) qui dépend des activités humaines s'y déroulant. Suivant cette conception, une zone fonctionnelle correspond à une certaine utilisation des terres (Zhi et al., 2014;Gao et al., 2017).
En dépit de leur diversité, ces définitions permettent d'identifier des caractéristiques essentielles concernant les zones fonctionnelles. Premièrement et fondamentalement, elles sont délimitées de sorte que les interactions socio-économiques sont plus fortes en leur sein qu'entre elles. Deuxièmement, elles sont hétérogènes parce qu'elles regroupent différents types d'activités. Troisièmement, elles ne correspondent pas à un découpage administratif, géographique ou historique. Enfin, elles peuvent être associées à des utilisations spécifiques des terres.
Précisons que lorsque les zones fonctionnelles sont délimitées à l'échelle de centres urbains, elles sont qualifiées de zones fonctionnelles urbaines et c'est précisément l'identification de ce type de zones qui nous intéresse dans cet article. Par ailleurs, du fait des caractéristiques sus-citées, la détermination des zones fonctionnelles impose de disposer de données décri-vant des interactions socio-économiques. Dans notre contexte, nous utiliserons des données de mobilité et des POI pour leur identification.
Travaux proches
L'identification des zones fonctionnelles urbaines, dans la plupart des travaux récents, rime avec la détermination de l'utilisation des terres. Habituellement, délimiter des zones fonctionnelles urbaines revient à délimiter des zones résidentielles, commerciales, administratives, touristiques, etc. Cette tendance traduit une perception de la zone fonctionnelle urbaine comme étant un espace remplissant une fonction spécifique aux yeux des usagers qui le fréquentent. Plusieurs méthodes, essentiellement statistiques, ont été proposées afin de délimiter automatiquement ce type d'espace. Ainsi, dans Yuan et al. (2012), les auteurs présentent une approche basée sur une régression de mélanges de Dirichlet multinomiaux (DMR pour DirichletMultinominal Regression) pour identifier les zones fonctionnelles de Beijing. Ils utilisent à la fois des données décrivant les déplacements des taxis et les POI dans leur modèle qui est une amélioration du LDA (Latent Dirichlet Allocation). D'autres méthodes utilisant la factorisation de matrices (Wang et al., 2018), l'algorithme Expectation Maximization (Long et Shen, 2015), le clustering hiérarchique (Tu et al., 2018) pour ne citer que celles-là, ont aussi été dé-veloppées. Elles usent des données provenant des GSM, des trajectoires de taxis, des media sociaux, des carte de bus, d'images satellites, etc.
Toutefois, les zones fonctionnelles urbaines ne sont pas exclusivement associées à l'utilisation des terres dans la littérature. Certains auteurs les conçoivent comme des régions présentant des interactions socio-économiques plus fortes en leur sein que vis à vis de l'extérieur. Nous partageons cette conception car elle correspond à notre sens à la caractéristique fondamentale et traditionnelle de la notion de zone fonctionnelle. Les méthodes développées pour extraire les zones fonctionnelles suivant cette conception présentent une constante. Elles modélisent sous forme de graphe les interactions qui ont lieu dans un territoire puis recherchent des communautés au sein de ce graphe. Ceci s'explique par le fait que premièrement, les graphes permettent de modéliser naturellement les interactions et deuxièmement, la notion de communauté dans un graphe sert à formaliser aisément le concept de zones fonctionnelles. Une communauté étant un ensemble de noeuds plus fortement inter-connectés entre eux qu'avec l'extérieur. Cette formalisation est explicitée dans Farmer et Fotheringham (2011). Les auteurs y proposent une approche pour identifier les zones fonctionnelles, fondée sur l'extraction de communautés dans un graphe obtenu à partir des flux travail-domicile sur tout le territoire Irlandais. Les poids des liens du graphe dépendent à la fois de la distance séparant deux noeuds et du nombre de dé-placements observés entre eux. La particularité de leur travail réside dans l'utilisation d'une méthode spectrale pour maximiser la modularité. Pour découvrir les zones fonctionnelles de Shanghai, Fan et al. (2015) recherchent également des communautés, mais plutôt par le biais de l'algorithme Fast-Newman, dans un graphe de cellules de Voronoï construit à partir des trajectoires de taxis. Demsar et al. (2014) se démarquent des deux travaux précédents en cherchant à détecter des communautés recouvrantes et donc des zones fonctionnelles qui se chevauchent dans la ville de Londres, à partir de données sur la circulation des taxis.
Ces approches à base de graphes n'exploitent que des données d'interaction, plus préci-sément de mobilité. Elles ne prennent pas en compte la composition socio-économique des territoires étudiés. Or les zones fonctionnelles ne sont pas que des zones de fortes interactions, ce sont aussi des zones hétérogènes rassemblant différents types d'activités. Par conséquent, l'intégration des données socio-économiques peut aider à améliorer la qualité du processus de détermination des zones fonctionnelles. C'est la raison pour laquelle nous proposons une mé-thode de délimitation des zones fonctionnelles fondée sur l'extraction de communautés et qui enrichit celles déjà existantes en alliant l'utilisation des POI aux données de mobilité. Nous décrivons cette nouvelle approche dans la section suivante.
Méthodologie
En nous référant aux définitions de la section 2, l'une des caractéristiques principales des zones fonctionnelles est d'avoir de plus fortes interactions internes que vis à vis de l'extérieur. La détection de telles zones a été formalisée par Farmer et Fotheringham (2011) comme un problème de détection de communautés dans un graphe d'interaction. Problème qui peut être résolu en maximisant la modularité.
La modularité, proposée par Newman (2006), est une mesure évaluant la qualité d'une partition des sommets d'un graphe. Cette mesure utilise le fait qu'une communauté est plus connectée en interne que vers l'extérieur. Elle est formellement définie par l'équation 1 pour un graphe de matrice d'adjacence A et une partition P des sommets de ce graphe :
où C est une partie de P, i et j sont deux sommets de C avec k i , k j leur degré respectif et m est le nombre d'arêtes dans le graphe.
L'idée sous-tendant cette formulation consiste à comparer, pour chaque partie, ou communauté, le nombre de liens internes (somme des A ij ) avec le nombre de liens attendus dans un modèle de référence (somme des k i k j /2m). La qualité est ainsi d'autant meilleure que le nombre d'arêtes observées est supérieur à ce qui était attendu. Pour la modularité le modèle de référence est le modèle configurationnel (Bender et Canfield, 1978) qui préserve uniquement la distribution de degrés du graphe original mais mélange toutes les arêtes. La modularité prend des valeurs comprises entre -0,5 et 1.
Notre méthode s'inspire de celle de Farmer et Fotheringham (2011) et repose également sur la maximisation de la modularité mais (i) nous y intégrons des informations sur les points d'intérêt pour contrôler l'hétérogénéité des zones identifiées et (ii) nous remplaçons la méthode spectrale d'optimisation par une autre plus efficace. Les trois étapes de notre approche sont : segmentation du territoire en unités spatiales, construction des graphes modélisant les interactions entre ces unités et clustering des unités spatiales pour extraire des zones fonctionnelles.
Segmentation du territoire
Le découpage en grille est une méthode classique de segmentation du territoire très utilisée car simple à implémenter et produisant des zones de surface identique (Wang et al., 2016;Liu et al., 2015;Pei et al., 2014;Liu et al., 2012). Cependant, le découpage qui en résulte ne respecte pas l'aménagement urbain, manque de sémantique (Yuan et al., 2012) et dépend d'un paramètre fixant la taille des cellules.
Afin de pallier ces insuffisances, nous avons préféré diviser l'espace d'étude en zones disjointes en utilisant le réseau routier. Les unités spatiales obtenues sont a priori homogènes d'un point de vue socio-économique et évitent les limitations du découpage en grille. Nous utiliserons le terme de zones formelles pour les identifier dans la suite de l'article, comme dans Bednarz et al. (1994).
Graphe d'interaction
Les interactions entre zones formelles sont modélisées par un graphe de flux, pondéré par le nombre de trajets entre les zones sources et destinations durant une certaine période (en pratique nous ignorons la direction ce qui rend la matrice symétrique). Afin d'obtenir des régions fonctionnelles connexes, les pondérations sont ajustées avec une pondération géographique : les poids sont calculés grâce à une fonction Gaussienne, voir équation 2 telle que proposée par Farmer et Fotheringham (2011) :
où A ij est l'interaction pondérée entre les zones formelles i et j, W ij représente le nombre de trajets entre i et j, d ij est la distance euclidienne entre i et j, h est un paramètre contrôlant la variance de la Gaussienne et donc la compacité des zones.
Extraction de zones fonctionnelles
L'optimisation de la modularité est un problème NP-difficile (Brandes et al., 2006) mais de nombreuses heuristiques ont été proposées pour trouver rapidement des solutions de bonne qualité. Selon Fortunato (2010), la méthode de Louvain proposée par Blondel et al. (2008) est l'une des méthodes les plus efficaces à la fois en temps et en qualité, c'est la raison pour laquelle nous l'avons sélectionnée pour calculer les partitions.
Afin d'extraire les zones fonctionnelles nous avons donc appliqué la méthode de Louvain sur divers graphes d'interaction obtenus en faisant varier le paramètre h (la variance) de la Gaussienne. Puis, pour chaque partition, nous avons mesuré l'hétérogénéité des régions géné-rées en calculant leur entropie moyenne, comme définie par l'équation 3, grâce à la distribution des points d'intérêt.
où E R est l'entropie (Shannon, 1948) de la région R calculée par :
avec p i la proportion de la i-ème catégorie de points d'intérêts dans la région R. Notre intuition est que plus une région est hétérogène, plus son entropie sera élevée et plus y devrait y avoir des interactions spatiales en son sein. Enfin, nous sélectionnons la partition qui maximise le produit de la modularité par l'entropie moyenne. Ce produit constitue une règle de sélection multi-critères pour s'assurer que les zones fonctionnelles aient de fortes interactions internes et une hétérogénéité suffisante.  , 2013). Chaque trajet comporte un identifiant, l'origine de la demande (envoi par le central, demande à un arrêt ou dans la rue, etc.), un identifiant anonymisé du téléphone du demandeur, l'arrêt de taxi en cas de demande à un arrêt, l'identifiant du taxi, un horodatage, le type de jour (vacances, jour ouvré, week-end), l'absence éventuelle de données GPS, et les coordonnées GPS collectées toutes les 15 secondes.
Le jeu de données contient 1710670 enregistrements. Après un léger nettoyage pour exclure les trajets incomplets, trop courts ou trop longs, il nous reste 1438924 trajectoires. Les trajectoires très courtes peuvent avoir leurs origines et leurs destinations associés à la même zone formelle et les trajectoires très longues peuvent avoir une extrémité en dehors de la zone d'étude. Dans cette étude nous n'utilisons que la source et la destination des trajectoires que nous stockons dans une base de données Postgresql indexée avec l'extension Postgis, utile pour manipuler et effectuer des calculs sur des données spatiales.
Outre ces données, 7710 points d'intérêt ont été collectés depuis openstreetmap 2 . Ces POI sont étiquetés par des types de base (distributeur de billets, banque, église, hotel, etc.) que nous avons regroupés en dix catégories plus générales pour en réduire le nombre. Ces catégo-ries sont : Logement, Lieux de travail et services publics, Alimentation et loisirs, Tourisme, Transports, Business et industries, Education, Santé, Espaces verts, Lieux de culte.
Enfin, les zones formelles sont extraites de l'atlas urbain de 2012 disponible en ligne 3 . Il s'agit de données vectorielles avec des informations cadastrales. Le processus de segmentation fournit 2453 zones formelles qui sont représentées sur la figure 1. 
Experimentations et résultats
Nos expériences ont été menées sur un ordinateur portable HP Zbook, possédant un processeur octocoeur Intel i7-6700HQ ayant une fréquence de 8 * 2.60Ghz et 16Gb de Ram. Les opérations de SIG sont effectuées avec QGIS sous Ubuntu 16.04 LTS.
A partir des zones formelles et des données origines-destinations, nous avons construit des graphes d'interaction pour des valeurs de la variance de la Gaussienne (le paramètre h de l'équation 2) comprises entre 0,05 et 3 avec un pas de 0,05. Les régions n'ayant aucune interaction sont supprimées lors de la phase de construction du graphe. La méthode de Louvain est ensuite exécutée 10 fois sur chaque graphe et la partition de plus forte modularité est conservée. L'entropie moyenne est ensuite calculée ainsi que le produit modularité par entropie moyenne. Les résultats sont présentés figure 2. Notons que la condition d'arrêt de notre expérimentation est la convergence de l'entropie moyenne. Or d'après la figure 2 l'entropie moyenne stagne pour des valeurs de variance comprises entre 1,5 et 3. Ce qui explique que nous n'ayons pas effectué d'expérimentations au delà.
La variance qui maximise le produit entre entropie moyenne et modularité est h = 0, 35. La partition correspondante contient 11 communautés qui correspondent aux zones fonctionnelles illustrées figure 3. Les figures 4 et 5 montrent deux exemples avec des variances valant respectivement 0, 1 et 2. On peut noter une corrélation claire entre la taille des communautés et la variance même si nous n'avons pas formalisé cette corrélation. Après comparaison, les zones fonctionnelles obtenues ne correspondent pas aux frontières administratives des districts de Porto représentées figure 6 même si on peut noter quelques similarités. Cela confirme que la ville a une structure cachée qui n'est accessible que via l'exploitation des données de mobilité. La diversité des activités de chaque zone est présentée figure 7 (les couleurs identifiant les zones et les POI ne sont pas corrélées). La figure 7 montre que les communautés ont une bonne variété de types de POI et que les zones résidentielles sont dominantes dans la majorité des communautés. Cette situation est normale étant donné que fournir des logements est l'une des principales, sinon la principale, fonction d'une ville.
Bien que les communautés détectées soient de grands blocs contigus, il arrive que certaines de leurs zones formelles se retrouvent en dehors de leurs frontières. Cela vient du fait que quelques zones formelles géographiquement éloignées restent fortement interconnectées même en pénalisant grandement les distances élevées avec le calcul de l'équation 2. Ce phé-nomène peut être géré en assignant aux zones formelles l'identifiant de la communauté dans laquelle elles sont incluses ou, au contraire, il est possible de les laisser en l'état pour ne pas perdre l'information de la forte connectivité.
D'après les expérimentations de Farmer et Fotheringham (2011), la variance h devrait être calculée de manière adaptative pour chaque origine à partir des distances des trajets partant de cette origine et aboutissant à d'autres zones formelles. Néanmoins, ce calcul génère des zones fonctionnelles très dispersées comme illustré figure 8. La méthode de Farmer et Fotheringham (2011)  dit, nous déterminons la variance h pour l'ensemble des zones formelles en ajustant la distribution des distances avec une loi gaussienne. Cependant, ce mode de calcul génère également des zones fonctionnelles très dispersées comme illustré figure 9. En raison des résultats de mauvaise qualité, obtenus avec ces deux modes de sélection automatique de la variance, nous ne sommes pas en mesure de comparer notre modèle à celui proposé par Farmer et Fotheringham (2011) et qui nous sert de modèle de référence.
Conclusions et perspectives
Cet article propose une méthode à base de graphes pour délimiter les zones fonctionnelles d'une ville. Cette méthode consiste à diviser la ville en zones formelles à partir du réseau routier puis à construire un graphe d'interaction entre ces zones en utilisant les trajectoires de taxis. Les noeuds du graphe correspondent aux zones formelles et deux noeuds sont connectés si il existe un ou des trajets de taxis allant d'une zone à l'autre. Les arêtes sont pondérées en fonction du nombre de trajets et de la distance entre zones formelles. L'utilisation d'une fonction à décroissance gaussienne permet de pénaliser les longues distances et d'obtenir un ensemble de graphes en modifiant la valeur de la variance. Les graphes obtenus sont ensuite décomposés en communautés maximisant la modularité. Chaque communauté correspond à une zone fonctionnelle. Le meilleur découpage en zones fonctionnelles est identifié en utilisant l'information apportée par l'entropie des points d'intérêts au sein des communautés. Toutes les expériences ont été menées sur la ville de Porto grâce à des trajectoires de taxis enregistrées pendant un an et des POI.
Ce travail peut être approfondi en étudiant la stabilité des zones détectées au cours du temps. En particulier, les données pourraient être divisées en sous-ensembles couvrant diffé-rentes périodes et les zones fonctionnelles identifiés pour chaque sous-ensemble. On détermi-nerait ensuite des coeurs de communautés (Gfeller et al., 2005) pour l'ensemble des partitions obtenues. Par ailleurs, il peut également être pertinent de calculer des communautés recouvrantes pour obtenir un découpage plus réaliste qui ne soit pas une partition stricte. En outre, l'utilisation des trajectoires est pour l'instant minimale étant donné que seules les zones de dé-part et d'arrivée sont exploitées. Les zones traversées pourraient aider à affiner la détection des zones fonctionnelles. Enfin, la dimension temporelle peut également apporter une information forte pour la pondération des arêtes du graphe.

Introduction
Les coûts de maintenance d'un hélicoptère représente des frais très importants tout au long du cycle de vie de l'appareil et sont donc décisifs lors de l'achat d'un hélicoptère. De ce fait, la maintenance s'inscrit à part entière dans la conception d'un produit (Lefebvre, 2009). L'optimisation de la maintenance est donc un point d'intérêt croissant pour le secteur de l'industrie aéronautique. L'un des facteurs d'optimisation consiste à tirer profit des données collectées lors de l'utilisation des hélicoptères, des données dites d'opérations ou en service.
Dans ce contexte, les techniques d'analyse de données se posent comme une alternative intéressante pour anticiper les opérations de maintenance en se basant sur l'analyse des données de vols et de maintenance. Chez Airbus Helicopters (AH), la détection d'anomalies repose principalement sur la surveillance des données de vibration. L'analyse des vibrations générées par des éléments dynamiques fournit un état de santé, permettant de déterminer si l'hélicoptère concerné est capable d'opérer un nouveau vol.
Les travaux de cette étude s'inscrivent dans le cadre général de la maintenance conditionnelle basée sur l'état et l'utilisation de l'appareil au sein d'AH. Il s'agit d'exploiter les données collectées via trois systèmes en place : HUMS (Health and Usage Monitoring System), MIS (Maintenance Information System), et MRO (Maintenance Repair and Overhaul).
Les données du HUMS concernent plus de 400 appareils associés à plus de 50 opérateurs différents. Cela représente environ 300 vols déchargés par jour. Les données sont recueillies, traitées, valorisées puis mises à disposition des clients d'AH par des applications Web. AH dispose donc d'un historique de données de vols qui n'a encore été que partiellement exploité à des fins d'analyse pour la maintenance.
Analyse de deux paramètres de la BTP
Dans ce travail, nous avons été confrontés à des données extrêmement complexes, provenant d'une multitude de sources. Nous avons alors décidé de nous focaliser sur la Boite de Transmission Principale (BTP), et d'étudier plus précisément deux paramètres connus des experts et fortement corrélés : la température et la pression de l'huile.
Maintenance prédictive d'hélicoptère à partir de données d'usage
Nous avons développé une méthode de visualisation des séries temporelles dans le but de définir des hypothèses de normalité de fonctionnement de la BTP à partir des données brutes sur un très grand nombre de vols. Etant données deux séries temporelles pour la pression et la température, l'approche que nous avons suivie a été la suivante (Daouayry et al., 2018) :
-Construction de matrices de co-occurences de la température et de la pression sur une fenêtre de temps donnée (typiquement 6h), -Proposition d'une représentation graphique des matrices de co-occurrences, -Visualisation sous forme de vidéo de l'historique des vols (plusieurs centaines d'heures) en affichant une succession de fenêtres glissantes. En pratique, une sélection pertinente d'un sous-ensemble des vols opérés a été faite pour valider l'approche. La représentation graphique de différentes matrices de co-occurrences a permis de comparer différents états de fonctionnement de la BTP (par exemple décollage, vol stationnaire, atterrissage).
Premiers résultats
Nous avons implémenté cette méthode de visualisation des series temporelles avec les outils internes AH. L'idée était d'offrir un moyen visuel aux experts pour interagir avec les données et comprendre les phénomènes sous-jacent à la BTP. Ce travail a aussi permis de définir une notion de "normalité de fonctionnement de la BTP", comme un ensemble de motifs à la fois "répétables" dans le temps et reproductibles pour l'ensemble des vols opérés, sans présen-ter d'anomalie particulière. À travers ces visualisations opérationnelles, plusieurs motifs ont pu être identifiés sur des hé-licoptères. Ces motifs peuvent être considérés comme la "normalité" de fonctionnement de la BTP. On peut noter qu'en dépit de sa simplicité, ce travail a rendu possible l'analyse par les experts de AH de l'historique complet des données de température et de pression sur les vols disponibles par hélicoptère, ce qu'ils ne pouvaient pas faire avant. La visualisation sous forme de vidéo s'est avérée précieuse pour eux, avec des résultats faciles à interpréter et sur lesquels de nombreuses questions métiers ont émergé.
Les perspectives de ce travail sont nombreuses. Actuellement, la détection des anomalies peut se faire à la main en visualisant des vidéos mais pourrait très bien s'automatiser. Une anomalie de fonctionnement de la BTP pourrait se définir comme un état éloigné des motifs représentant le fonctionnement normal, qu'il serait possible d'identifier "à la volée" par des calcul de distance entre matrices de co-occurrences. Les matrices de co-occurrences peuvent aussi s'étendre à plus de deux paramètres pour capter plus finement l'analyse de la BTP. 

Introduction
Une anomalie peut être définie comme une déviation par rapport à ce qui est défini comme normal. La détection d'anomalie constitue une tâche importante dans de nombreux domaines tels que l'analyse de données, la reconnaissance d'image médicale, la détection d'intrusion dans les systèmes informatiques, ou encore la fraude à la carte bancaire. Avec le développe-ment croissant des volumes de données issues des applications métier, la détection d'anomalie représente par ailleurs un enjeu de plus en plus important dans de nombreux domaines industriels.
Dans le contexte industriel de la SNCF 1 , l'objectif de ce travail est de détecter un comportement anormal dans les traces de messages au sein de son système d'information (SI). L'historique des séries temporelles observées par le passé est indexé à l'aide d'un arbre iSAX pour permettre son archivage et son interrogation. En effet, les arbres iSAX (Shieh et Keogh, 2008) sont des structures d'indexation multidimensionnelles très performantes pour les séries temporelles. Elles permettent des recherches par similarité très efficaces sur des critères de distance, mais sont aussi parmi les seules à supporter la pondération des portions de signaux et la déformation temporelle dynamique (dynamic time warping) durant la recherche. Elles ont également montré qu'elles restaient opérationnelles même lorsque la base de séries temporelles dépasse le milliard de séries.
Sur ces données, nous nous intéressons à la détermination du score de détection d'anomalie CFOF proposé récemment par (Angiulli, 2017). L'intérêt particulier de ce score est qu'il soit actuellement le seul pour lequel la robustesse vis-à-vis de l'augmentation de la dimensionnalité des données ait été établie de façon formelle et expérimentale. Notre contribution principale est de montrer qu'il est possible de tirer parti des propriétés des arbres iSAX pour calculer de façon efficace une approximation du score CFOF. La méthode proposée a été testée sur des jeux de données synthétiques, ainsi que sur des données SNCF. Dans tous les cas, les scores approchés sont très proches des scores réels. Enfin, l'interprétation des résultats sur les données réelles confirme la pertinence de l'utilisation de la méthode.
Positionnement par rapport aux approches existantes
La détection d'anomalies est toujours un domaine très actif obtenant de nombreux résultats, comme par exemple les travaux récents de Abràmoff et al. (2015) pour aider au diagnostic de la rétinopathie diabétique grâce à l'utilisation d'un réseau de neurones convolutionnel. Il existe de multiples méthodes appliquées à des domaines très différents (Chandola et al., 2009(Chandola et al., , 2012. La partie 2.1 suivante décrit quelques unes de ces méthodes.
Différentes méthodes
On peut trouver parmi ces méthodes des approches non supervisées telle que l'utilisation de forêt d'arbres d'isolation dans l'article de Ting et al. (2008). Ces arbres d'isolation travaillent chacun sur un échantillon des données, et les données marquées comme isolées par plusieurs arbres sont considérées comme anormales. Il existe aussi des méthodes supervisées, comme par exemple celle de Mukkamala et al. (2002), qui utilise un classifieur de type Support Vector Machine dans le cadre de détection d'intrusions système (programme américain Defense Advanced Research Projects Agency 1998). Parmi ces méthodes supervisées, on notera égale-ment l'utilisation de règles et de motifs, comme notamment dans la technique proposée par Li et al. (2007) pour détecter des trajectoires anormales d'objets.
Méthodes basées sur une notion de proximité
Ces méthodes sont largement utilisées, avec de nombreuses variantes. On peut y trouver trois sous-familles (Aggarwal, 2013) : les méthodes basées sur du clustering, celles basées sur des distances et celles basées sur des densités.
Les méthodes basées sur du clustering observent si un objet appartient ou non à un cluster. L'objet est considéré anormal s'il ne se trouve rattaché à aucun cluster (Mukkamala et al., 2002). Les méthodes basées sur des distances les plus typiques utilisent simplement, pour calculer le score d'anomalie d'un objet q, les distances entre q et ses k plus proches voisins (Knox et Ng, 1998). La troisième sous-famille, basée sur des densités, contient des techniques qui vont tenir compte des objets dans une zone proche autour de l'objet q à évaluer. Ces objets et leur distribution influenceront le score attribué à q (Breunig et al., 2000).
Une des meilleures méthodes de détection par densité est celle basée sur le Local Outlier Factor (Breunig et al., 2000). Cependant, sa pertinence diminue très fortement lorsque la dimensionnalité de l'espace augmente, à cause du phénomène communément appelé malédic-tion de la dimensionnalité. En effet, lorsque la dimension augmente, les valeurs des distances entre objets tendent à être plus similaires (alors que les objets, eux, ne le sont pas forcément) et les méthodes sont confrontées au problème dit de concentration des objets. Comme montré par Angiulli (2017), le score Local Outlier Factor n'échappe pas à cela, et il tend vers 1 pour tous les objets lorsque la dimensionnalité augmente. C'est pourquoi Angiulli (2017) propose une nouvelle méthode appelé CFOF (Concentration Free Outlier Factor) ayant la propriété de résister à ce phénomène de concentration. Cette méthode sera plus amplement détaillée dans la section 3.1. Angiulli (2017) propose une technique efficace de calcul de CFOF par échantillonnage, où les scores CFOF de tous les objets d'un échantillon sont calculés par rapport à tous les autres objets du même échantillon. Cette technique tire partie d'une factorisation des opéra-tions nécessaires au sein de chaque échantillon. La qualité des estimations dépend de la taille de l'échantillon, et si cette technique est bien adaptée lorsque l'on souhaite calculer les scores pour tous les objets d'une base, elle ne l'est pas lorsque l'on veut calculer seulement le score d'un nouvel objet vis-à-vis d'un historique de référence.
Nous nous plaçons dans le cadre d'un stockage existant d'un historique de séries temporelles dans un arbre iSAX (Shieh et Keogh, 2008), qui est une structure d'indexation particulièrement performante pour l'interrogation et la recherche par similarité. La méthode que nous présentons permet de calculer une approximation du score CFOF d'une nouvelle série en tirant parti des propriétés de l'arbre iSAX.
3.1 Rappel de la définition du score CFOF Le calcul du score CFOF (Angiulli, 2017) d'un objet q vis-à-vis d'un ensemble d'objets de référence R consiste à chercher la taille du voisinage minimale k m telle que q soit dans les k m plus proches voisins d'au moins une fraction des objets de R. Le score CFOF(q) est alors la valeur de k m normalisée par rapport à la taille de R. Ce score est paramétré par le seuil (dans [0; 1]) qui détermine la proportion d'objets de R devant inclure q dans leur voisinage (de taille k m ). Ce paramètre rend la valeur du score plus ou moins sensible au nombre d'objets de référence auxquels q doit ressembler, et comme le montrent les expériences de la section 4 son réglage est simple en pratique.
De façon plus formelle, la mesure se définit comme suit. Soit nn k (x) le kième plus proche voisin d'un objet x, c'est-à-dire tel qu'il existe seulement k − 1 objets plus proches de x que ne l'est l'objet nn k (x). L'ensemble des k plus proches voisins de x est noté NN k (x) et inclut tous les objets tel que
Le score CFOF de détection d'anomalies est actuellement le seul pour lequel il a été montré de façon formelle et constaté de façon expérimentale (Angiulli, 2017) qu'il n'était pas sensible au phénomène de concentration quand la dimensionnalité des données augmente.
Principe général de la méthode de calcul proposée
Dans notre contexte, l'historique de référence (par exemple une mesure au fil du temps) est découpé en séries de longueur fixe pouvant se chevaucher ou non. Une série de longueur D (contenant D valeurs) est alors considérée comme étant un objet dans un espace à D dimensions, et chacune des séries est stockée sous la forme d'un objet indexé dans l'arbre iSAX.
Pour un nouvel objet q (une nouvelle série), qui n'est pas dans l'arbre iSAX, la méthode que nous proposons permet de calculer une approximation du score CFOF de q par rapport aux objets de référence stockés dans l'arbre iSAX, en tirant parti des propriétés de cet arbre.
Dans les arbres iSAX, comme dans tout arbre d'indexation en général, une feuille contient un ensemble d'objets similaires, mais un arbre iSAX possède deux autres propriétés dont nous allons tirer parti :
1. Les feuilles ne se chevauchent pas, et une zone de l'espace multidimensionnel n'est représentée que par une feuille.
2. L'indexation utilisée permet de borner les distances lors des recherches dans l'arbre. Par exemple, pour un objet p (contenu dans l'arbre ou non), il est possible au niveau de tout noeud N (intermédiaire ou feuille) de connaître une borne inférieure de la distance entre p et l'objet le plus proche de p indexé dans le sous-arbre à partir de N .
La première propriété est importante car elle va permettre de précalculer et de stocker des statistiques liées à la distribution des objets pour tout l'espace couvert par l'arbre, et ce à une granularité qui est celle de la feuille. La seconde propriété sera quant à elle mise à profit pour élaguer, lors du calcul d'un score CFOF, les zones de l'espace ne pouvant pas contenir d'objets participant au voisinage en cours de détermination.
Pour calculer le score CFOF d'un nouvel objet q par rapport aux objets de référence stockés dans l'arbre iSAX, une des principales difficultés est d'arriver à obtenir pour chacun des objets p stockés dans l'arbre la valeur du rang de q dans le voisinage de p. Ce rang vaudra 1 si q est le plus proche voisin de p, 2 si q est le second plus proche voisin de p, etc. La valeur de ce rang, notée v-rang p (q), est définie plus précisément comme étant la valeur de k pour laquelle nn k (p) = q. Lorsque ces valeurs v-rang p (q) sont connues pour tous les objets p de référence, l'obtention de CFOF(q) est simple. Il suffit de placer ces valeurs dans une liste triée par ordre croissant, que nous noterons l v-rang , et de prendre dans l v-rang l'élément d'indice × |R|| (obtenu par arrondi supérieur). La valeur de cet élément correspond à la taille de voisinage minimale k m telle que q soit dans les k m plus proches voisins d'au moins une fraction des objets de R. La valeur de CFOF(q) est alors la valeur normalisée de k m par rapport à la taille
Revenons sur l'étape clef de calcul de v-rang p (q). Pour determiner cette valeur nous devons obtenir le nombre d'objets de référence r tels que distance(p, r) ≤ distance(p, q). Afin d'éviter d'avoir à compter chaque objet r un par un, nous proposons de calculer une approximation de v-rang p (q) en utilisant des fonctions de répartition des distances des objets dans différentes zones de l'espace. Le calcul de v-rang p (q) s'effectue alors en parcourant toutes les zones non vides de l'espace, c'est-à-dire toutes les feuilles N f de l'arbre iSAX, et en utilisant une fonction de répartition pour déterminer le nombre d'objets r de N f qui sont tels que distance(p, r) ≤ distance(p, q). Pour des vecteurs dont les composantes suivent une loi normale, le module de ces vecteurs suit une loi du χ (apparentée à la loi du χ 2 ). C'est une géné-ralisation d'autres lois, notamment de la loi de Maxwell qui décrit la distribution des modules de vitesses de particules en 3 dimensions. Un aspect intéressant de cette loi du χ est qu'elle tend rapidement vers une loi normale quand la dimensionnalité augmente. Notons F µ,σ (x) la fonction de répartion de la loi normale de moyenne µ et d'écart type σ. Soit, pour une feuille N f et un objet p, des approximations˜µapproximations˜ approximations˜µ et˜σet˜ et˜σ de la moyenne et de l'écart type des distances entre p et les objets contenus dans N f . La fraction d'objets de N f situés à une distance de p inférieure ou égale à distance(p, q) peut alors être approximée par F ˜ µ,˜ σ (distance(p, q)). C'est ce principe qu'utilise l'algorithme détaillé dans la section suivante.
Algorithme d'approximation de CFOF à partir d'un arbre iSAX
Le pré-traitement nécessaire à l'exécution de l'algorithme est le calcul des approximations˜µ approximations˜ approximations˜µ et˜σet˜ et˜σ pour chaque objet de référence p vis-à-vis de chaque noeud feuille N f . Pour un objet p et un noeud feuille N f , ˜ µ est noté dist(N , p) et˜σet˜ et˜σ est noté˜σnoté˜ noté˜σ N (p). Le principe de l'algorithme proposé est indépendant de ces approximations, et nous détaillerons leurs calculs ensuite dans la Section 3.4.
En plus des valeurs˜µvaleurs˜ valeurs˜µ et˜σet˜ et˜σ, l'approximation du score CFOF d'un objet q à partir des objets de référence contenus dans un arbre iSAX, nécessite d'autres paramètres d'entrée : la racine N racine de l'arbre ISAX, l'ensemble R des objets de référence (en pratique cet ensemble peut aussi être obtenu à partir de l'arbre) et le paramètre . L'algorithme 1 décrit le calcul de l'estimation CFOF q à partir de ces entrées.
Le principe général est celui présenté dans la section précédente. Pour chaque objet de référence p l'algorithme calcule v-rang p (q) par cumul dans la variable v-rang (boucle commençant à la ligne 2). Chaque valeur de v-rang est insérée dans la liste triée l v-rang , permettant de retourner la valeur de l'approximation de CF OF (q) (ligne 24).
Pour le calcul de v-rang p (q), pour un p donné dans la boucle principale, l'algorithme parcourt l'arbre à partir de sa racine (boucle interne commençant ligne 6) en tenant à jour une liste liste N des noeuds restants à visiter. Le noeud courant est ôté de cette liste ligne 7. Ensuite l'algorithme utilise les bornes minDist et maxDist sur la distance entre p et les objets contenus dans le sous-arbre associé au noeud courant, qui sont des bornes fournies par la structure d'indexation iSAX. Deux élagages sont alors réalisés : 1. Si la distance minimale entre p et les objets représentés par le noeud courant est supérieure à la distance dist entre p et q (ligne 8), alors le sous-arbre ne contient pas d'objet à compter dans v-rang p (q). L'exploration du sous-arbre peut alors être évitée de façon certaine. 2. Si c'est la distance maximale qui est inférieure à la distance dist (ligne 10), alors tous les objets du sous-arbre sont plus proches de p que ne l'est q. Il est donc possible de réaliser un second type d'élagage sûr, en comptant tous ces objets directement dans v-rang p (q) sans parcourir le sous-arbre. Ceci est réalisé ligne 11 avec nbrObj(N courant ) représentant le nombre d'objets du sous-arbre du noeud N courant .
S'il n'y a pas eu d'élagage, deux cas sont possibles selon que N courant soit un noeud feuille ou pas. Si c'est un noeud feuille (ligne 12), alors l'algorithme va comptabiliser dans v-rang p (q) les objets du noeud qui sont plus proches de p que ne l'est q, en approximant ce décompte avec la fonction de répartion F ˜ µ,˜ σ . Enfin, si le noeud courant est un noeud interne de l'arbre sans possibilité d'élagage (ligne 16), alors les noeuds enfants immédiats de N courant dans l'arbre sont ajoutés à la liste des noeuds restants à visiter.
Calcul des paramètres dist(N , p) et˜σet˜ et˜σ N (p)
Même si le principe de l'algorithme et des élagages présentés Section 3.3 sont indépendants des paramètres˜µparamètres˜ paramètres˜µ et˜σet˜ et˜σ de la fonction de répartition F , ces paramètres vont influer sur la qualité globale de l'approximation de CFOF réalisée. Nous indiquons ici les valeurs utilisées dans les expériences présentées Section 4, et qui ont permis d'obtenir une très bonne approximation du score CFOF tant sur des données synthétiques de distributions gaussiennes que sur les données réelles traitées dans le système d'information de la SNCF.
Tout d'abord pour˜µpour˜ pour˜µ, c'est-à-dire dist(N , p) pour un objet p et un noeud N , c'est la moyenne quadratique aussi appelée RMS (Root Mean Square) qui est utilisée :
avec N dénotant ici l'ensemble des objets contenus dans le noeud lui-même. Soit C le barycentre des objets de N (en prenant une masse unitaire pour chaque objet). Par le théorème de Huygens nous avons :
Algorithme 1 : Calcul de l'approximation du score CFOF de q dans un arbre iSAX Data : L'objet q, l'ensemble R des objets de référence, la racine N racine de l'arbre, les valeurs
// noeuds restants à parcourir
Rien à faire, ignorer simplement N courant L'autre étape de pré-traitement est le calcul des écarts types˜σtypes˜ types˜σ, c'est-à-dire des valeurs˜σ valeurs˜ valeurs˜σ N (p). Pour cela, nous utilisons une moyenne pondérée des écarts types pris sur chacune des dimensions, afin de privilégier les dimensions où p s'écarte le plus de C (le barycentre des objets du noeud N ). Soit D le nombre de dimensions et σ 1 , σ 2 , . . . , σ D les écarts types des objets de N pour chaque dimension. Soit (p 1 , p 2 
Évaluations et analyses
Nous avons évalué notre approche sur deux ensembles de tests. Le premier, construit sur le même jeu de données que celui utilisé par Angiulli (2017), est destiné à estimer la capacité de notre algorithme à effectuer une approximation correcte du score CFOF. Le second utilise un jeu de données issu du SI de la SNCF, et sert à évaluer sa capacité à détecter des anomalies réelles identifiées et qualifiées par les experts.
Évaluation sur le jeu de données Clust2
La première évaluation a pour but de vérifier la qualité de notre estimation du score CFOF. Pour ce faire, nous utilisons le jeu de données Clust2 de Angiulli (2017), généré de la manière suivante : pour un ensemble de dimensions D ∈ [2, 5, 10, 20, 50], 10000 points sont générés selon deux distributions normales. La première est centrée sur l'origine avec un écart-type de 1 sur chaque dimension, et la seconde est centrée sur (4 . . . 4) avec un écart-type de 0.5. Dans cette première évaluation, le paramètre CFOF est fixé à 0.1. Pour chaque point, nous calculons d'une part le score CFOF réel, et d'autre part son approximation avec l'algorithme 1.
Les résultats de cette évaluation sont présentés sur la figure 1. Elle montre une comparaison du score exact (en bleu) et du score estimé (en jaune) pour les jeux de données en 2, 5, 10, 20 et 50 dimensions, par ordre croissant de score CFOF exact (et donc par ordre d'anormalité croissante). Nous voyons que quelle que soit la dimension des données, la courbe d'estimation suit la courbe du score exacte avec beaucoup de précision. Nous voyons aussi qu'au fur et à mesure que l'on augmente le nombre de dimensions, une faible transition a lieu : jusqu'en dimension 10 le score estimé est légèrement inférieur au score réel pour toutes les valeurs du score. À partir de la dimension 20, le score devient légèrement surestimé pour les faibles valeurs de score (pour les 2000 premiers points). Cette tendance s'accentue en 50 dimensions, où le score est surestimé pour les 6000 premiers points.
La figure en 20 dimensions représente par ailleurs, une estimation de l'apport lié à l'utilisation de l'arbre iSAX. En effet, la structure hiérarchique de l'arbre reflète l'organisation multi-dimensionnelle du nuage des points qui y sont stockés, et notamment l'anisotropie de sa distribution spatiale. Nous pouvons évaluer l'apport de cette structure en faisant la supposition que pour chaque noeud, les objets stockés sont distribués uniformément entre les distances minimale et maximales aux objets de ce noeud. Nous pouvons calculer un score CFOF avec cette estimation, en nous attendant à ce que ce score soit de plus mauvaise qualité que celui obtenu en exploitant la structure de l'arbre. C'est ce que l'on constate sur la courbe en 20 dimensions, avec le score tracé en vert. Là aussi, nous voyons que le score est surestimé pour les 2000 premiers objets, et sous-estimé pour les objets suivants, mais avec une différence bien plus grande par rapport à notre algorithme. Le dernier graphique de la figure 1 présente un zoom sur les 500 derniers scores en 20 dimensions, montrant que l'apport de la structure arborescente iSAX est indispensable et que sans elle l'approximation du score CFOF n'est pas exploitable.
Concernant la rédution du temps d'exécution, sur le jeu en 20 dimensions, le temps moyen de calcul de notre approximation du score CFOF est de 40 secondes par objet (machine Linux équipée d'un Intel Xeon Silver 4114 à 2.2 GHz), et de plus de 40 fois cette valeur pour le calcul du score exact. 
Évaluation sur données réelles de la SNCF
La seconde évaluation de notre algorithme porte sur un jeu de données réel issu du contexte industriel de la plateforme de médiation SNCF appelée CanalTrain. Cette évaluation sert à mettre en avant trois points importants dans la cadre de l'application industrielle. Tout d'abord, nous comparons le score CFOF estimé et le score réel sur les vraies données métier, ce qui nous permet de constater que notre estimation est de qualité suffisante pour l'application industrielle envisagée. Ensuite, l'interprétation des résultats montre une utilisation possible dans le cadre de flux de messages, où les nouvelles séries sont construites par agrégation des données arrivant, et leurs scores CFOF sont calculés par rapport à l'arbre iSAX contenant les séries relatives à un historique donné. Enfin, plusieurs scores sont calculés en faisant varier le paramètre . Nous montrons que ce paramètre permet de régler la sensibilité de la détection d'anomalie : avec un paramètre faible, seules les anomalies les plus grossières sont identifiées. Avec un paramètre plus élevé, les anomalies plus fines sont aussi identifiées et donnent lieu à un score CFOF plus élevé. Dans notre application, le réglage de ce paramètre se fera en fonction du taux de faux négatifs levés et des retours des experts, mais nous montrons d'ores et déjà qu'un paramètre unique permet de piloter la détection.
L Ces séries sont ensuite insérées dans l'arbre, qui constitue la base de référence à laquelle nous allons comparer le reste des données. La même méthode de d'extraction à partir des donnée brutes de séries temporelles est appliquée aux données disponibles sur la période du 1 er au 24 septembre 2018, ce qui nous donne 1122 séries. Nous estimons ensuite le score CFOF de chacune de ces séries par rapport aux séries de référence stockées dans l'arbre. À des fins d'évaluation et de comparaison, nous calculons aussi leur score exact, mais dans l'application finale cette phase sera omise. Nous voyons que les deux courbes correspondantes, présentées sur la figure 2, sont presque tout le temps confondues. Plusieurs valeurs du paramètre sont évaluées, afin de voir dans quelle mesure nous pouvons contrôler le taux de faux négatifs.
Pour ce jeu de données, les exécutions ont également été réalisées sur un Intel Xeon Silver 4114 à 2.2 GHz, et se terminent avec un temps moyen de calcul de notre approximation du score CFOF de 160 secondes par objet, alors que le temps mesuré pour le calcul du score exact est plus de 25 fois supérieur.
Toutes les anomalies détectées avec un paramètre de 0.001 correspondent à des incidents métiers réels. Nous en discutons trois en particulier ici. Une première anomalie, identifiée autour du 7 septembre, provient de l'accumulation de messages provenant d'une plateforme en amont. Notre méthode permet de détecter d'une part un pic anormal du nombre de messages entre le 6 et le 7 septembre, et d'autre part une baisse anormale le matin du 7. Une deuxième anomalie, représentative des anomalies souvent détectées, est visible le 19 septembre au matin. Nous voyons que le nombre de messages a arrêté d'augmenter de 3h à 6h du matin, avant de brusquement grimper à plus de 10000 messages par minute. C'est un cas d'anomalie assez classique dans lequel la plateforme n'arrive pas à traiter tous les messages arrivant suite à une saturation, lorsqu'une ou plusieurs files d'attente en amont de CanalTrain vident massivement leurs buffers de messages accumulés. Le même type de comportement, mais plus atténué, peut être observé le 6 septembre autour de 14h. Un des premiers constats techniques, avant la purge et le redémarrage des noeuds de la plateforme, était une saturation de la mémoire vive suivie d'une saturation du CPU. L'anomalie la plus frappante se situe dans la nuit du 20 au 21 septembre, où l'on constate que le nombre de message s'est totalement écroulé, et tous les flux de la plateforme CanalTrain sont soudain stoppés. Cette anomalie, qui a dans les faits été résolue très tardivement, provenait du dysfonctionnement du système de gestion des messages (ActiveMQ) interne à CanalTrain. Un redémarrage s'est avéré nécessaire, mais la cause du comportement anormal du composant n'a cependant toujours pas pu être identifiée. On constate qu'avec le paramètre ∈ (0.001, 0.005, 0.01) l'anomalie est marquée en début et en fin par deux pics de scores élevés, tandis qu'avec = 0.05 et = 0.1, l'anomalie est signalé du début à la fin. En effet, une valeur de plus élevée rend le coefficient CFOF plus exigeant en terme de similarité.
Les évaluations présentées dans cette section mettent en avant deux points importants : (1) notre méthode d'approximation du score CFOF à partir de l'arbre iSAX permet d'obtenir une qualité suffisante pour l'application envisagée de détection d'anomalies, (2) le paramètre permet de régler la sensibilité de la détection, et de l'adapter aux motifs recherchés.

Introduction
De nombreux algorithmes d'apprentissage non supervisé se basent sur une mesure de similarité ou distance entre instances. Bien qu'il existe un grand nombre de métriques décrites dans la littérature, en pratique, l'ensemble de métriques disponibles est grandement réduit par les caractéristiques des données et de l'algorithme choisi. Le choix d'une distance peut impacter fortement la qualité d'un clustering.
Shi et Horvath proposent dans (Shi et Horvath (2006)) la méthode des Unsupervised Random Forest (URF), dérivant des random forests (RF, Breiman (2001)). Leur méthode permet de calculer des distances entre instances non étiquetées en utilisant les forêts d'abres aléatoires. Une fois la forêt construite, les données d'entraînement sont passées dans chacun des arbres. Chaque feuille contenant un nombre limité d'instances, et toutes les instances terminant dans les mêmes feuilles pouvant être considérées comme similaires, il est possible de définir une mesure de similarité : si deux objets i et j sont dans la même feuille, la similarité globale entre ces deux objets est incrémentée. Cette mesure est par la suite normalisée via une division par le nombre d'arbres dans la forêt. L'utilisation des forêts d'arbres aléatoires est rendue possible dans le cadre non supervisé grâce à la génération d'instances synthétiques, permettant la classification binaire entre ces dernières et les instances observées, non étiquetées. Deux méthode de génération d'instance sont présentées dans leur travaux, addCl1 et addCl2.
L'utilisation d'arbres de décision en tant que méthode pour obtenir une similarité permet de limiter les prétraitements nécessaires, notamment dans le cas de jeux de données hétérogènes.
Malgré ses avantages, la méthode présente deux principaux inconvénients. Premièrement, la phase de génération d'instances n'est pas efficace sur la plan computationnel. Dans la mesure où les arbres obtenus dépendent des instances générées, il est nécessaire de constuire plusieurs forêts avec différentes instances générées et d'effectuer une moyenne de leurs ré-sultats. Deuxièmement, les instances synthétiques peuvent biaiser le modèle construit vers la discrimination d'objets sur certains attibuts spécifiques.
En parallèle, P. Il est intéressant de développer deux paramètres de l'algorithme des ET : K, et n min , la taille minimale d'un noeud pour qu'il puisse subir une coupure. Le paramètre K, ayant des valeurs dans l'ensemble {1, ..., n f eatures }, influence le caractère aléatoire des arbres. En effet, pour des valeurs faibles, la dépendance des arbres vis-à-vis des étiquettes est diminuée. Dans le cas extrême où K = 1 (i.e. uniquement un attribut est sélectionné pour la réalisation de la coupure), la dépendance entre les arbres et les étiquettes est éliminée.
Nous proposons d'étendre l'approche de Shi et Horvath (2006) et d'utiliser les ET avec une nouvelle approche où la génération d'instances n'est plus nécessaire, que nous nommons unsupervised extremely randomized trees (UET). Nous étendons cette méthode afin de pouvoir l'utiliser sur des données hétérogènes.
Ce document est un résumé du travail présenté à PAKDD 2018 (Dalleau et al. (2018)).
Unsupervised Extremely Randomized Trees
Les méthodes de génération d'instances synthétiques ne sont pas efficace computationnellement. Au lieu de générer de nouvelles instances, une autre approche possible est d'assigner aléatoirement de nouvelles étiquettes. Cette méthode que nous proposons et nommons addCl3, peut-être implémentée de la manière suivante. Soit n obs le nombre d'instances dans le jeu de données. Une liste contenant n obs 2 fois l'étiquette 0 et n obs − − n obs 2 l'étiquette 1. Pour chaque instance, une étiquette est échantillonée sans remplacement dans cette liste. Avec addCl3, deux instances similaires peuvent être étiquetées différement et finir dans deux feuilles différentes. Cependant, en fixant K = 1, la construction des arbres ne dépend plus de l'étiquette. L'algorithme des ET semble donc être un algorithme intéressant à utiliser avec addCl3. L'algorithme 1 présente les UET.
Description de l'algorithme
Algorithme 1 : Unsupervised Extremely Randomized Trees Data : 
Optimisation des paramètres
Deux paramètres sont importants : le nombre d'arbres n trees , et n min , vu précédemment. Nous avons évalué l'influence de ces paramètres sur 3 jeux de données, Iris, Wine and Wisconsin 1 . Notre évaluation, basée sur l'évolution de l'Adjusted Rand Index (ARI), montre que (i) la différence d'ARI n'est pas statistiquement significative pour n trees > 50 et (ii) la valeur optimale de n min semble être autour de 30% de la taille du jeu de données. La significativité des différences a été évaluée par le test de Kruskall-Wallis.
3 Évaluation empirique 3.1 Évaluation de certaines caractéristiques des UET Nous avons comparé la différence entre la similarité moyenne entre les instances appartenant au même groupe et la similarité moyenne entre instances n'appartenant pas au meme groupe, que l'on dénote ∆. La procédure est répétée 20 fois. Nous calculons ¯ ∆, la moyenne des ∆, ainsi que σ, l'écart type. Cette approche permet une comparaison agnostique à une méthode de clustering donnée. Capacité à discriminer des clusters Nous avons généré trois jeux de données pour cette expérience : deux jeux de données sans structure de cluster, NoC4 et NoC50, ainsi qu'un jeu de données avec une structure de clustern C4. Les résultats, présentés Table 3.1, montre que notre méthode semble être capable de donner des ¯ ∆ significativement plus grands lorsqu'une structure de cluster existe, tout en donnant des ¯ ∆ ≈ 0 lorsqu'il n'y en a pas.
Invariance aux transformations monotones d'attributs L'une des propriétés intéressantes des arbres de décision est leur invariance à ce type de transformations. En effet, comme pré-cisé dans Friedman (2006), cette propriété confère une résistance aux outliers, ainsi qu'à tout changement d'échelle entre variables. Une évaluation empirique sur deux jeux de données synthétiques semble confirmer cette propriété.
Robustesse aux variables corrélées Nos expérimentations semblent indiquer que les UET sont robustes à la présence de variables corrélées. Cette propriété est intéressante dans la mesure où elle permet de limiter les étapes de prétraitement dans le cas où des variables fortement corrélées sont présentes dans les données.
Performance des UET sur des données numériques, catégorielles et hétérogènes
Nous avons appliqué le même protocole à des jeux de données synthétiques et réels de 3 types différents : continu, catégoriel, et hétérogènes. Pour les 6 jeux de données d'évaluation, contenant entre 4 et 10 variables et 2 classes, 0.30 < ¯ ∆ < 0.49, et 0.001 < σ < 0.008.
Évaluation comparative avec des résultats de la littérature
Dans les sous-sections précédentes, nous avons comparé les différences de similarité intraet inter-cluster. Il est aussi intérssant de comparer les résultats obtenus avec de véritables clusterings présentés dans la littérature. Le protocole suivant est adopté dans cette partie. Pour chaque jeu de données, UET est appliqué 10 fois, et la moyenne des matrices de similarité est calculée. Cette matrice est par la suite transformée en matrice de distance par l'équation DIS ij = 1 − SIM ij et un clustering hiérarchique agglomératif est réalise. La qualité du clustering est évalué par la Normalized Mutual Information (NMI). Cette procédure est répé-tée 20 fois, et la moyenne et l'écart type de la NMI sont calculés. Les résltats obtenus sont présentés Nous avons par ailleurs comparé les résultats obtenus avec la méthode proposée et les URF. Pour ce faire, nous avons utilisé l'implémentation proposée par les auteurs et comparés les ARI obtenues en utilisant l'algorihtme de partitionnement autour des médoïdes. 2000 arbres et 100 forêts sont utilisés pour les URF, avec une valeur de m try = √ n f eatures . Les matrices de similarités sont obtenues par moyennage de 20 matrices de similarité. Ces expériences ont été réalisées sur une machine équipée d'un processeur Intel i7-6600U. Tout en obtenant des clusterings compétitifs avec la littérature, un gain concernant le temps de calcul est parfois observé, parfois de plusieurs ordres de grandeur.
Comparaison avec la distance euclidienne
Enfin, nous avons comparé les clusterings obtenus en utilisant les matrices obtenues par notre méthode avec des clusterings obtenus en utilisant la distance euclidienne. Pour les jeux de données categoriels, la distance euclidienne est calculée après application de One-Hot encoding afin de transformer les variables. Une comparaison de la NMI montre que les UET sont performants face à la distance euclidienne, donnant des NMI meilleures ou comparables dans tous les cas testés.
Conclusion et perspectives
Dans ce travail, nous présentons une nouvelle méthode permettant le calcul de similarités utilisant des arbres aléatoires. Cette approche étend celle des Unsupervised Random Forest, en utilisant des arbres extrêmement aléatoires. Notre approche est applicable à des jeux de données homogènes ou hétérogènes et possède des propriétés intéressantes que nous avons évalué empiriquement, telles que l'invariance aux transformations monotones des variables ou la résistance aux variables corrélées. Ces propriétés permettent de réduire les tâches de prétraitement. Une évaluation empirique de l'approche que nous proposons sur des jeux de données synthétiques et réels donne des résultats compétitifs vis-à-vis de la littérature.
Summary
In this paper we present a method to compute similarities on unlabeled data, based on extremely randomized trees. The main idea of our method, Unsupervised Extremely Randomized Trees (UET) is to randomly split the data in an iterative fashion until a stopping criterion is met, and to compute a similarity based on the co-occurrence of samples in the leaves of each generated tree. We evaluate our method on synthetic and real-world datasets by comparing the mean similarities between samples with the same label and the mean similarities between samples with different labels. Our empirical study shows that the method effectively gives distinct similarity values between samples belonging to different clusters, and gives indiscernible values when there is no cluster structure. We also assess some interesting properties such as invariance under monotone transformations of variables and robustness to correlated variables and noise. Finally, we performed hierarchical agglomerative clustering on synthetic and real-world homogeneous and heterogeneous datasets using UET. Our experiments show that the algorithm outperforms existing methods in some cases, and can reduce the amount of preprocessing needed with many real-world datasets.

Introduction
Les graphes sont des structures mathématiques constituant un outil de modélisation et de représentation universel utilisé dans une large gamme d'applications réelles. La recherche de motifs de graphes (RMG) est l'une des opérations fondamentales sur laquelle reposent la recherche et l'analyse des graphes de données. Soit G(V, E, l, Σ) un graphe de données et Q(V q , E q , f v ) un motif de graphe (requête) où : V (V q ), E (E q ), l (f v ) et Σ représentent respectivement l'ensemble de noeuds, l'ensembles des arêtes, la fonction des étiquettes et l'univers des étiquettes dans le graphe (la requête). Le problème RMG consiste à trouver toutes les correspondances de Q dans G, notées par M (Q, G). Typiquement, ce problème est défini en termes de : -l'isomorphisme de sous-graphes (Ullmann, 1976) : M (Q, G) est constitué de tous les sous-graphes G de G auxquels Q est isomorphe, i.e., il existe une fonction bijective h : V q −→ V telle que (u, u ) ∈ Q si et seulement si (h(u), h(u )) ∈ G ; ou -la simulation de graphes (Henzinger et al., 1995) : M (Q, G) est une relation binaire
FIG. 1: Un graphe de données et un graphe requête
Avec la taille grandissante des graphes de données, le nombre des correspondances d'une requête peut être excessivement important. Inspecter tous les résultats est une tâche ardue, en plus du fait que les utilisateurs ne portent d'intérêt qu'aux meilleures réponses. De plus, dans plusieurs applications, les algorithmes d'appariement utilisent des requêtes ciblées qui visent à trouver des correspondances d'un noeud de sortie au lieu de l'appariement entier (Fan et al., 2013). Ce genre d'applications ne cherche pas les correspondances exactes, et pour cette raison plusieurs approches à base de simulation de graphes ont été proposées. Malgré les relaxations posées par la simulation de graphes et ses variantes d'une part, et le fait qu'il est presque impossible de connaitre la structure de graphe d'une autre part, nous avons constaté que ces approches sont aussi restrictives car elles n'acceptent pas l'appariement avec des noeuds manquants (sans affecter la qualité des résultats). Dans plusieurs applications réelles, ce type de relaxation est très utile car il permet d'éviter le problème de réponses vides.
Ceux-ci mettent en évidence le besoin de trouver les k-meilleures (top-k) réponses d'un noeud de sortie en permettant la relaxation en termes de noeuds manquants.
Exemple : Un réseau de collaboration est représenté par G dans la figure 1b. Dans ce graphe, un noeud v i représente une personne avec son travail (étiquette du noeud) et une arête (v i , v j ) indique une relation de supervision. Exemple (v 3 , v 7 ) indique que la personne v 3 avec le travail A est le superviseur de la personne v 7 avec le travail D. Une compagnie émet la requête Q (figure 1a) pour trouver des correspondances dans G. Dans cet exemple, u * 0 est le noeud de sortie de Q. Cela signifie que seules les correspondances de ce noeud sont demandées.
Dans cet exemple, l'isomorphisme de sous-graphes ne parvient pas à identifier des correspondances pour la requête Q. Avec la simulation de graphes, on peut vérifier que
Cependant, le résultat d'une recherche utilisant la requête ciblée ne contient que le noeud v 3 . En outre, on peut vérifier que, dans un tel cas, le noeud v 2 peut être considéré comme un résultat potentiel, car il est possible que le superviseur d'une personne occupant le poste C puisse également être un superviseur des personnes supervisées par C. Mais la simulation classique ne parvient pas à identifier ce type de relation.
Les approches à base de la simulation relaxée de graphes peuvent bénéficier d'une notion plus générale de la simulation de graphes. Mais elles risquent de perdre de leur efficacité en terme de temps de recherche.
Dans cet article, nous étudions les défis ci-dessus et nous proposons un nouveau modèle, appelé la simulation relaxée de graphes (SRG), afin d'éviter le problème des réponses vides. Nous proposons des algorithmes qui permettent de trouver les top-k réponses tout en rédui-sant le coût de la recherche grâce à des techniques d'optimisation. De plus, nous menons des expérimentations approfondies pour attester l'efficacité de l'approche proposée.
État de l'art
Plusieurs travaux ont été proposés pour la recherche de motifs de graphes. Ce problème a été traité par l'isomorphisme de sous-graphes et la simulation de graphes.
L'isomorphisme de sous-graphes consiste à énumérer toutes les occurrences exactes d'une requête dans un graphe. Ce problème est NP-complet et il est largement étudié dans la littéra-ture. Récemment, plusieurs travaux ont été proposés pour faire face aux limites de l'isomorphisme de sous-graphes. (Zhang et al., 2010) ont étudié l'appariement approximatif en utilisant une distance d'édition bornée par la requête. La simulation de graphe présente une alternative efficace à l'isomorphisme de sous-graphes en permettant un certains nombre d'assouplissements sur les correspondances. (Fan et al., 2010) introduisent un modèle de simulation bornée en terme de nombre de sauts dans le graphe. Le modèle, simulation forte, proposé par (Ma et al., 2014) étend la simulation classique en imposant deux conditions supplémentaires (dualité et localité). Une autre étude récente, réalisée par (Gao et al., 2016), étend la simulation de graphes en permettant l'absence des noeuds à un saut. Cependant, elle perd la notion de simulation et affecte la qualité des résultats pour les requêtes avec des noeuds feuilles. (Li et al., 2017) proposent la combinaison de la taxonomie des étiquettes avec la simulation de graphes.
Le problème de top-k a beaucoup été étudié pour toutes les représentations de données. (Ilyas et al., 2008) présentent un état de l'art de ce problème dans les systèmes de bases de données relationnelles. Ce problème a été également étudié pour les requêtes XML, (Guo et al., 2003), et les graphes de données, (Fan et al., 2013).
La simulation relaxée de graphes
Dans cette section nous présentons notre modèle qui permet d'éviter le problème des ré-ponses vides et nous décrivons notre approche pour la recherche des motifs.
Bien que les résultats des approches à base de simulation de graphes soient intéressants en terme de temps de recherche, nous avons constaté qu'elles sont incapables de capturer des correspondances significatives ce qui entraine un certain nombre de réponses vides. Nous avons donc proposé un nouveau modèle, appelé la simulation relaxée de graphes (SRG), qui permet d'identifier des correspondances avec un certain type de relaxation.
SRG est basé sur la notion d'ensemble de satisfaction. Un ensemble de satisfaction d'un noeud u ∈ Q (Sat u ) est un ensemble d'ensembles d'étiquettes qui permet de vérifier si un noeud v ∈ G correspond au noeud u. Selon le besoin dans SRG, l'ensemble de satisfaction peut être calculé comme suit : (1) le premier ensemble dans Sat u contient les étiquettes des fils du noeud u, (2) les autres ensembles dans Sat u sont construits à partir de toutes les combinaisons possibles en remplaçant chaque élément dans le premier ensemble par les étiquettes des fils du noeud correspondant, s'ils existent. La simulation relaxée de graphes. Soient G = (V, E, l, Σ) un graphe de données et Q = (V q , E q , f v ) un motif de graphe. Le graphe G correspond au motif Q par SRG s'il existe une relation binaire R ⊆ V q × V qui vérifie :
En général, les graphes de données sont très grands, ce qui rend l'ensemble de résultats excessivement large. En revanche, l'intérêt de l'utilisateur peut être résumé ou regroupé dans les top-k réponses du noeud souhaité u * (Ilyas et al., 2008). Dans ce but, nous avons défini une fonction de classement basée sur les ensembles de satisfaction où l'importance d'un noeud v ∈ G est donnée par le score γ(S ui ) de l'ensemble de satisfaction couvert par v :
représente le nombre de substitutions dans S ui ⊆ Sat u , i.e., le nombre des noeuds manquants remplacés par leurs noeuds fils. Ensuite, nous utilisons γ() pour calculer la pertinence δ() d'un noeud v * qui correspond au noeud de sortie u * :
avec S best représente les meilleurs ensembles de satisfaction couverts lors du processus de recherche des correspondances, et |E q | représente le nombre des arêtes dans la requête Q. La recherche à base de SRG permet d'avoir plus de résultats en évitant le problème des réponses vides. Ce gain en qualité influe sur l'efficacité en terme de temps de recherche. Pour cette raison, nous avons conçu des algorithmes qui visent à trouver les top-k réponses et réduire le coût de recherche. Notre approche se déroule en deux phases. La première vise à encoder les informations de voisinage des noeuds d'un graphe en utilisant la structure de données probabiliste "filtre de Cuckoo" . Cette dernière est très efficace pour les tests d'appartenance à un ensemble et elle permet de réduire de façon significative la complexité en temps. La seconde phase vise à trouver les k meilleures correspondances selon le modèle SRG. Pour qu'un noeud v ∈ G corresponde à un noeud u ∈ Q, les trois conditions suivantes doivent être vérifiées :
la liste des étiquettes des fils de v couvre au moins un ensemble parmi les ensembles de Sat u . Nous utilisons le filtre de Cuckoo pour vérifier la dernière condition. L'algorithme de recherche choisit des correspondances potentielles du noeud de sortie u * et effectue une vérification de correspondance des noeuds de sous graphe induit par le noeud choisi. le résultat final est une liste des k meilleures correspondances trouvées (voir Habi et al., 2018). L'analyse de la complexité en temps de notre approche a donné O(|V |.D+|V |.(|V |+|E|)).
Résultats
Dans cette section, nous décrivons et discutons les résultats expérimentaux afin d'évaluer nos méthodes.
Nos expérimentations portent sur quatre graphes (Epinions, Amazon, Google, et LiveJournal) issus de Stanford Large Network Dataset Collection 1 (nous ne présenterons ici que les Trois séries d'expérimentations sont menées : la première est la simulation de graphes amé-liorée par la terminaison anticipée (top-kGS) (Fan et al., 2013), la deuxième est la simulation relaxée de graphes (top-kRGS), et finalement la simulation relaxée de graphe améliorée par l'utilisation du filtre de Cuckoo (top-kCuckoo).
Discussions
Taux d'échec : La figure 2a montre les résultats du taux d'échec des deux modèles (simulation de graphes et simulation relaxée de graphes). Dans cette expérience, nous avons fixé k = 10 et nous avons fait varier (|V q |, |E q |) de (5, 6) à (20, 60). Nous observons que la simulation relaxée réduit efficacement le taux d'échec (dans toutes les expérimentations).
Temps de recherche : Les figures 2b et 2c montrent le temps moyen de recherche en fonction respectivement de la taille de la requête et de k. Les résultats montrent que les trois algorithmes sont sensibles à la variation de k. En outre, top-kGS et top-kCuckoo surperforment toujours top-kRGS. Top-kCuckoo et top-kGS ont un temps de recherche presque similaire pour les requêtes de petites tailles mais top-kCuckoo prend plus de temps pour les grandes requêtes, ce qui s'explique par le fait qu'il identifie plus de correspondances.
Ces expérimentations montrent les performances de notre approche en terme de qualité avec un temps de recherche quasi similaire au top-kGS.
Conclusion
Dans cet article, nous avons abordé le problème de réponse vide dans le contexte de la recherche de motifs de graphes. Nous avons proposé un nouveau modèle appelé la simulation relaxée de graphes (SRG) basé sur la simulation de graphes. Notre modèle permet de prendre en compte les noeuds manquants sans affecter la qualité des résultats et de fournir une bonne flexibilité pour plusieurs applications. En outre, nous avons également développé un algorithme efficace utilisant le filtre de Cuckoo pour calculer les k meilleures réponses. Par conséquent, notre approche convient très bien aux grands graphes. Nos expérimentations valident l'efficacité de cette approche.

Introduction
D'une manière générale, le domaine de la visualisation des connaissances s'intéresse à l'utilisation de représentations visuelles pour améliorer la création et le transfert de connaissances entre au moins deux personnes (Eppler et Burkhard, 2004). La visualisation des connaissances désigne ainsi tous les moyens graphiques pouvant être utilisés pour construire et transmettre des informations complexes (Burkhard, 2004), (Bertschi et al., 2011), (Ursyn, 2015).
Dans ce travail, nous proposons une nouvelle approche d'évaluation du risque en prenant en compte les interactions qui existent entre les facteurs de risque. Cette approche est décrite dans la figure 1. Nous nous focaliserons sur la dernière étape de ce processus à savoir la visualisation du modèle conceptuel (sous forme d'un graphe). L'approche proposée pour cette étape est un outil de visualisation dynamique de connaissance exploitant une représentation sous forme d'un graphe. Il permet une visualisation dynamique des interactions entre les facteurs de risques pour aider à la compréhension du déclenchement des effets en cascade entre les facteurs de risque. L'objectif que nous poursuivons est la visualisation dynamique du parcours du graphe des facteurs de risque guidé par les interactions existantes entre ces facteurs.
Le parcours du graphe permet un accès aux connaissances relatives aux interactions entre ces facteurs, une réorganisation des connaissances en fonction des cas à traiter, des retours arrière sur les décisions prises quand elles ont abouti à une impasse pour la stratégie de prévention. Les traces de la navigation sont collectées, car elles révèlent des processus de raisonnement la plupart du temps implicites.
FIG. 1 -Etapes de la méthodologie proposée par (Azzi et al., 2018) 2 Conception de MCVGraphViz La démarche de conception que nous suivons correspond à celle présentée dans la Figure 2. Elle est inspirée de celle proposée en visualisation d'information par (Card et al., 1999). Les données sont représentées dans un modèle conceptuel. L'approche adoptée consiste à charger les données dans un premier module pour construire les primitives graphiques. Pour chaque élément du graphe nous déterminons la façon dont il est dessiné. Enfin, le visuel correspondant est affiché dans l'interface de visualisation. Cette architecture permet à l'utilisateur d'interagir et de manipuler le graphe. Les interactions de l'utilisateur sont finalement retransmises à différents niveaux de cette architecture, en fonction du besoin. Par exemple, lorsque l'utilisateur sélectionne, zoom ou se déplace dans l'écran, c'est au niveau du rendu visuel que l'action est transmisse.
FIG. 2 -Approche de visualisation dans MCVGraphViz
MCVGraphViz est construit sur une architecture client-serveur. La visualisation du graphe s'effectue côté client. L'objectif principal est de développer une interface de visualisation dynamique. Il était donc important d'intégrer les variables graphiques et les fonctionnalités interactives à fournir aux utilisateurs. Ainsi, la conception et le développement de l'interface graphique de visualisation du graphe sont entièrement fondés sur SVG. L'avantage présenté par l'utilisation du format vectoriel SVG dans les applications de gestion des connaissances est qu'il offre de nombreuses fonctionnalités interactives aux utilisateurs, conformes aux stratégies ergonomiques de communication homme-machine.
Primitives graphique
Les primitives graphique concerne la façon dont les variables vont être visualisées dans l'interface. Comme nous l'avons annoncé précédemment nous disposons d'un graphe ce qui nous amène à représenter deux variables : des noeuds et des liens. Les noeuds sont symbolisés par des cercles pleins dont : (i) le diamètre est défini par le nombre de liens associé avec ce dernier. Cela permet d'identifier rapidement un noeud d'intérêt ; (ii) la couleur est définie par la catégorie à laquelle noeud appartient. En utilisant cette variation du diamètre et de la couleur, il devient très simple de détecter les noeuds les plus importants dans le graphe et s'ils sont connectés à des noeuds de taille plus réduite ou au contraire de taille plus importante. Afin de différencier le noeud correspondant à l'entrée du graphe, une variation de luminosité est intégrée autour du noeud. Les relations entre les noeuds sont représentées par des lignes dirigées et étiquetées indiquant le sens de la relation. Pour différencier ces liens : (i) l'épaisseur dépend de la probabilité de l'association entre deux noeuds ; (ii) le grain est défini par la catégorie des noeuds associés. Ce procédé permet d'identifier la relation d'intérêt entre les noeuds considérés. Concernant la clarté du graphe, si cela n'est pas très important lorsque le nombre de noeuds et de liens est faible, cela peut avoir un réel impact lorsque la taille du graphe est importante. En effet, un graphe dense peut conduire à une visualisation non compréhensible à cause du chevauchement des éléments du graphe. Pour résoudre ce problème, nous avons pris en la variable positionnement bien connue en visualisation. Afin d'éviter tout croisement inutile entre les éléments du graphe (noeud, relations, étiquette, etc.), chacun est positionné de la manière adaptée.
Aspect dynamique 2.2.1 Interaction avec la visualisation
La plupart des interactions dynamiques commencent par une opération de sélection, dans laquelle un ou plusieurs composants du graphique sont isolés pour certaines actions, telles que la mise en évidence, le masquage, le déplacement ou l'obtention de détails. Par exemple, pour désencombrer un graphique, il est possible de sélectionner un noeud et de désactiver le reste des éléments qui ne sont pas connectés. Ainsi, nous obtenons une visualisation moins chargée tout en conservant leurs liens. De même, il est possible de sélectionner, de déplacer ou de modifier la forme d'un lien pour éliminer un croisement ou améliorer l'esthétique d'un graphique. La sélection peut impliquer un objet unique, les objets d'une région établie à partir d'une distance, ou un ensemble d'objets satisfaisant un ensemble de contraintes spécifiées par l'utilisateur (par exemple, tous les noeuds directement connectés à un noeud spécifique). L'un des problèmes les plus importants relatif à la sélection d'éléments dans un graphe se produit dans des régions denses du graphique où les éléments sont si rapprochés qu'une sélection sans ambiguïté est difficile ou impossible. Cette situation conduit à la nécessité d'utiliser d'autres types d'interaction, tels que le zoom ou les techniques de distorsion.Un autre élément visuel connu est celui du mouvement perceptible par l'utilisateur. Son utilisation est courante dans certaines zones de visualisation pour mettre en évidence les modifications d'un ensemble de données ou informations sélectionné par l'utilisateur (par exemple, clic sur un noeud, survole avec la souris, etc.). MCVGrapViz est conçu pour visualiser les connaissances contenues dans un modèle conceptuel en permettant son interprétation. Dans ce contexte, l'utilisateur peut avoir besoin d'informations supplémentaires sur certains éléments du graphe (noeud, relation, etc.). Par conséquent, MCVGraphViz fournit cette fonctionnalité. L'avantage de l'approche adoptée dans MCVGraphViz est qu'elle permet d'une part de visualiser l'ensemble du graphe et d'autre part d'affiner l'accès aux connaissances représentées dans le modèle (par exemple, un système d'info-bulles).
Acquisition de nouvelles connaissances
Nous faisons l'hypothèse que les parcours du graphe reflètent des connaissances implicites réutilisables dans le contexte d'une évaluation du risque. En effet, chaque action réalisée (choix d'un noeud, clic souris sur un noeud, enchaînement de liens entre des noeuds, etc.) exprime une connaissance propre à l'auteur du parcours. Un système dédié à la gestion de traces permet de collecter les traces, de les stocker et de les manipuler à l'aide d'opérations génériques appelées transformations qui sont de différents types (Fuchs, 2017) : filtrage, fusion de traces, etc. La trace s'enrichit à chaque nouvelle interaction au fur et à mesure que l'activité se déroule, en prenant en compte le contexte de chaque action réalisée. C'est donc un objet dynamique, qui peut être disponible en temps réel (Besnaci et al., 2015) et qui peut être modifié (Champin et al., 2013), requêtée ou visualisée. L'utilisation des traces d'interactions vise le plus souvent : (i) l'analyse a posteriori des usages et des activités ; (ii) la qualification des usages (Rossi et al., 2005) ; (iii) l'extraction de motifs fréquents dans l'activité tracée (Georgeon et al., 2006) et étudier les comportements des utilisateurs à partir de ces motifs. MCVGraphViz collecte les actions effectuées au cours de la navigation, constitue une trace et la stocke sous la forme d'un chemin (noeud de départ, noeud de destination).
Résultats
Nous allons présenter ici un cas d'utilisation avec un jeu de données issues de l'étude de (Meneton et al., 2016). Ce jeu de données présente les interactions entre les facteurs de risque cardiovasculaires sous forme d'un modèle conceptuel. Le nombre de concepts et de relations dans ce modèle conceptuel est respectivement 13 et 58. Nous pouvons ainsi voir dans la figure 3 l'interface de visualisation développée dans MCVGraphViz (démonstration accessible : http://www-limics.smbh.univ-paris13.fr/MCVGraphViz/).
Conclusion
Nous avons présenté un nouvel outil de visualisation dédié aux connaissances structurées sous forme de graphe. Cet outil permet de visualiser le graphe, et d'interagir avec le modèle conceptuel représentant les connaissances du domaine. Nous avons réutilisé les résultats recensés dans l'état de l'art pour obtenir des visualisation équivalentes aux outils dédiés à la Besnaci, M., N. Guin, et P. Champin (2015). Acquisition de connaissances pour importer des traces existantes dans un système de gestion de bases de traces. In IC 2015 : 26es Journées


Introduction
Avec la croissance rapide de l'information en ligne, la nécessité de développer des mé-thodes pour trouver, filtrer et gérer ces ressources de manière rapide et efficace devient d'autant plus prégnante. La classification de données textuelles consiste à classer de façon automatique des textes dans une ou plusieurs catégories. Cette dernière a déjà été appliquée à plusieurs problématiques notamment l'extraction d'information (Kushmerick et al., 2001), l'analyse de sentiments (Dey et Haque, 2009), la détection de SPAM (Jindal et Liu, 2007), etc.
Afin d'exploiter des algorithmes d'apprentissage automatique sur des données textuelles, il est souvent nécessaire de représenter le texte sous la forme d'un vecteur de taille fixe, ceci afin de plonger la donnée dans un espace métrique.
De nombreuse méthodes de "vectorisation" ont été développées au fil des années. La plus utilisée étant la méthode dite sac de mots (bag of words) (Harris, 1954) qui consiste à décrire un texte par les occurrences (fréquences) des mots qui le composent. Cette méthode considère que tous les mots dans un document donné ont le même poids ce qui est problématique pour les mots peu discriminants. Dans le contexte d'une classification, afin de pallier ce problème, Karen Spärck Jones a introduit la notion de pondération tf-idf (fréquence du terme dans le document-proportion inverse de documents qui contiennent le mot) (Jones, 1972) qui consiste à donner plus de poids aux mots qui apparaissent dans moins de documents. Ainsi, si deux mots m 1 et m 2 ont une même fréquence dans un document, si m1 n'est présent que dans ce document, il aura un poids plus important que m 2 . Toutefois, ces représentations engendrent des plongements dans des espaces vectoriels de très grande dimension et ne prennent pas en considération la sémantique des mots. Cela signifie par exemple que les mots "se nourrir", "manger" et "conduire" sont équidistants dans cet espace vectoriel malgré le fait que sémantiquement, "manger" devrait être plus proche de "se nourrir" que de "conduire".
Partant de l'hypothèse que les mots utilisés dans les mêmes contextes ont tendance à avoir une signification (sens) similaire, Scott Deerwester et al. ont introduit la notion d'analyse sé-mantique latente (LSA) (Landauer et al., 1998). LSA est une méthode statistique qui permet d'extraire des motifs dans les relations entre les termes et les concepts dans des documents en appliquant une décomposition en valeurs singulières (SVD) sur une matrice termes-documents.
Chaque composante (vecteur propre) générée représente un concept exprimé sous la forme d'une combinaison linéaire des vecteurs associés aux termes, ce qui permet de réduire grandement la dimensionnalité.
Avec l'arrivée du modèle word2vec, proposé par Mikolov (Mikolov et al., 2013), de nouvelles opportunités sont apparues dans le domaine de la vectorisation des termes. Il existe deux architectures de modèle word2vec : la première, appelée CBOW prédit un mot en fonction de la connaissance seule de son contexte local d'occurrence (les mots qui l'entourent), et la seconde architecture, appelée skip-gram, prédit le contexte local d'occurrence d'un mot en fonction du mot lui-même. word2vec permet de capturer une certaine sémantique lexicale, dans la mesure où les mots ayant une signification voisine auront une représentation similaire, plus précisément, ils seront proches dans l'espace vectoriel dans lequel ils seront plongés. Par exemple, "se nourrir" et "manger" sont proches, alors que "manger" et "conduire" sont plus distants. Le résultat de la soustraction ou l'addition de vecteurs de mots porte également une signification. Par exemple, les vecteurs de mots peuvent être utilisés pour retrouver des relations d'analogie entre les termes à l'aide d'une algèbre linéaire simple : "Roi" -"homme" + "femme" = "Reine" (Mikolov et al., 2013).
Nous présentons dans cet article, une méthode efficace et peu coûteuse de vectorisation, basée sur word2vec et LSA à des fins de catégorisation de documents. Elle assure la capture de la sémantique d'un document, en préservant au mieux l'intégralité des mots contenus dans le texte.
Notre méthode consiste, dans un premier temps à vectoriser un document en considérant la moyenne des vecteurs word2vec des mots qui le composent, puis, dans un deuxième temps à combiner cette vectorisation word2vec à une vectorisation obtenue par LSA pour ce même document. La combinaison proprement dite est le résultat de la concaténation des deux vectorisations précédentes.
Etat de l'art
De nombreux travaux ont expérimenté une combinaison de word2vec avec différentes variantes de vectorisation. Basé sur word2vec, le modèle doc2vec (Le et Mikolov, 2014) propose une représentation vectorielle au niveau du document. Le principe de doc2vec consiste à représenter chaque paragraphe et chaque mot par un vecteur de base unitaire (one hot vector). Le vecteur de paragraphe et les vecteurs de mots sont moyennés ou concaténés. Le vecteur résul-tant est utilisé pour prédire le mot suivant dans un contexte donné. Néanmoins cette approche nécessite des données peu bruitées, ainsi qu'un grand nombre d'itérations pour converger. Sa complexité dépend de la taille du vocabulaire (Chen, 2017). Ronghui Ju et al (Ju et al., 2015) ont développé un modèle qui combine i) une représentation basée sur LSA, ii) un sac-demot associé à la pondération tf-idf et iii) une vectorisation word2vec. Le principe consiste à créer une matrice tridimensionnelle (m * n * v) où m est le nombre de mots, n le nombre de documents et v la dimension du vecteur word2vec. Dans ce tenseur tridimensionnel chaque document est représenté par une matrice, chaque ligne est le produit du sac-de-mot pondéré par tf-idf et de la représentation word2vec des termes. A l'issue de la création de cette matrice qui associe les termes aux documents, une décomposition en valeurs singulières (SVD) est appliquée sur chacune des matrices [termes x documents]. Le résultat est ensuite exploité pour entraîner un réseau neuronal convolutif (CNN) (LeCun et al., 1999). LDA2vec est éga-lement une approche combinant word2vec avec cette fois ci LDA (Allocation de Dirichlet latente). C'est un modèle développé par Christopher Moody (Moody, 2016) qui est inspiré de la variante skip-gram de word2vec. Il combine une analyse LDA (Blei et al., 2003) et une représentation word2vec. Le principe de cette méthode est de projeter les vecteurs word2vec des termes et les vecteurs de documents dans un même espace vectoriel. Cela consiste, d'une part, à représenter un document par une somme pondérée des vecteurs des thèmes. Celle-ci est obtenue en multipliant les "topic-vectors" générés par LDA et le vecteur de distribution de thèmes. Le résultat est ensuite combiné avec la représentation word2vec d'un mot pivot afin de générer un vecteur de contexte, qui sera utilisé pour prédire le contexte de ce mot pivot.
Modèles utilisés 3.1 Word2vec
Word2vec (Mikolov et al., 2013) constitue une famille de modèles de plongement lexical (word embedding) permettant de créer des représentations vectorielles de mots à partir de grands corpus. Word2vec peut être utilisé via deux architectures différentes : CBOW (sac de mots continus) et Skip-gram (saut de gramme). CBOW construit la représentation vectorielle d'un mot via la prédiction de son occurrence à partir de la connaissance des mots avoisinants. La notion de sac de mots continus implique que l'ordre des mots n'est pas pris en compte. Par ailleurs, la deuxième architecture, Skip-gram, construit la représentation vectorielle d'un mot via la prédiction de son contexte d'occurrence. Les mots avoisinants les plus proches auront un poids plus important par rapport aux plus éloignés (c'est la notion de skip-gram). Pour les deux architectures, ces prédictions sont réalisées à l'aide d'un réseau de neurones à propagation avant (Zell, 1994) qui ne contient que 3 couches (couche d'entrée, couche cachée, couche de sortie) ce qui permet une exécution rapide.
FIG. 1 -Les deux architectures de word2vec.
Pour les deux architectures, le nombre de mots avoisinants (ou la fenêtre) est défini par l'utilisateur. On désigne les mots à entraîner par w t , t ∈ {1 · · · T }. Dans la suite, nous nous focalisons sur l'architecture Skip-gram, que nous avions choisi pour nos expériences. L'objectif du Skip-gram est de maximiser la moyenne des log-probalités :
Où c est la taille du contexte. Une trop grande valeur de c augmente le temps d'entraînement.
La formulation de Skip-gram définit p(w t+i |wt) en utilisant la fonction softmax˘ a:
Où v et v ′ sont les représentations vectorielles "entrée" et "sortie" du mot w et w e /w s sont les mots entrée/sortie.
Analyse sémantique latente
L'analyse sémantique latente (LSA) est une méthode utilisée en traitement du langage naturel pour caractériser une forme de sémantique lexicale dite distributionnelle. Elle est basée sur la décomposition en valeurs singulières de la matrice [termes x documents] qui aboutit à la factorisation de cette matrice sous la forme :
où X est la matrice approchée [termes x documents], T k est la matrice des k premiers vecteurs propres (vecteurs "terme") à gauche, P k est la matrice des k premiers vecteurs propres (vecteurs "document") à droite et S k est une matrice diagonale tronquée à l'ordre k.
LSA, comme word2vec, suppose que les mots dont les significations sont proches s'observent dans des parties similaires des textes qui composent le corpus (hypothèse distributionnelle). La différence entre les deux méthodes est que LSA construit une projection linéaire des vecteurs "terme" dans un espace de dimension réduite, le contexte de co-occurrence des termes étant global au document, tandis que word2vec propose une réduction de dimension via une projection non-linéaire en exploitant un contexte local de co-occurrence. Par ailleurs, la proximité sémantique des termes est appréhendée via la similarité cosinus associée aux pondé-rations tf et tf-idf pour LSA, tandis que les architectures word2vec proposent une distribution de probabilité associé à une pondération binaire des entrées du vocabulaire exploité.
Combinaison de Word2vec et LSA
Word2vec permet de créer des représentations vectorielles de mots dans un espace sé-mantique qui capture en partie le sens du mot. Partant de l'hypothèse que le sens général d'un texte est défini par la combinaison du sens des mots qui le composent, l'idée générale consiste à considérer la représentation vectorielle d'un texte sous la forme d'une combinaison des vecteurs des mots qui le forment. L'approche naïve se borne à effectuer la moyenne des vecteurs de mots obtenus par une représentation word2vec. Cette moyenne est une estimation de l'idée générale du texte. Cependant, toute approche basée sur un moyennage est sensible aux valeurs extrêmes, peu pertinentes dans le cas de données très asymétriques et souvent, la moyenne calculée ne correspond à aucune des valeurs observées. Par ailleurs, à partir de la moyenne word2vec, il est difficile, voire impossible, de remonter aux mots qui forment le texte. En contre-partie, une représentation vectorielle type LSA conserve la connaissance des occurrences des mots importants qui caractérisent les textes. C'est cette complémentarité des deux approches qui motive l'approche proposée. En concaténant la moyenne des vecteurs word2vec et la représentation vectorielle des textes fournie par LSA nous obtenons une représentation vectorielle en basse dimension au niveau du document qui capture la sémantique générale d'un texte tout en conservant une description lexicale/conceptuelle du document. A notre connaissance, cette combinaison n'a pas été explorée.
La concaténation du vecteur moyen word2vec d'un document à sa représentation LSA est illustrée en figure 1. Nous avons choisi d'appliquer LSA sur une matrice (termes x documents) pondérée par le tf-idf. 
Expérimentation
Données utilisées
Afin d'estimer les performances des différentes méthodes évaluées, nous avons choisi quatre jeux de données possédants les caractéristiques suivantes :
1. 20NewsGroup (Joachims, 1997), noté par la suite 20NG, est une base documentaire anglophone constituée de 20K documents contenants des commentaires extrait d'un forum de discussion, répartis dans 20 catégories. Ces documents contiennent du bruit sous la forme de méta-données présentes notamment dans les entêtes et les pieds de pages qui donnent des informations sur les auteurs des commentaires ou sur leur organisme de rattachement. Ces méta-données peuvent être bénéfiques lors d'une tâche de classification dans certains cas en biaisant l'apprentissage.
2. RCV1 (Lewis et al., 2004) est un corpus de 800K documents contenant des dépêches en anglais produites par l'agence de presse Reuters, classées dans 103 catégories. Les dépêches sont multi-étiquettes, i.e. multi-label, en ce sens qu'une même dépêche peut appartenir à plusieurs catégories.
TED-FR est un sous corpus issue du corpus fd-TED (Ahmia et al., 2018) 1 . contenant des appels d'offres se rapportant aux marchés publics européens. Les documents contiennent beaucoup de bruit qui prend la forme d'information juridico-commercialadministratif. Les documents sont classés en 45 catégories (également multi-label).
Pour cette expérimentation, nous avons uniquement considéré les annonces en français traduites en entier ce qui constitue 800K documents.
4. TED-FILTRE, est un corpus de 2000K documents obtenus à partir du corpus complet fd-TED en filtrant les passages juridico-commercial-administratifs, ce qui réduit considérablement la taille des textes. Il comporte également 45 catégories.
Protocole expérimental
Nous décrivons dans cette section le protocole mis en oeuvre afin de comparer notre mé-thode de vectorisation (combinaison LSA + word2vec) à d'autres types de vectorisation couramment employés dans la littérature sur les jeux de données mentionnés précédemment.
Pour toutes les méthodes testées les pré-traitements suivants s'appliquent. Nous retirons tout d'abord les mots vides (ou stop-words) adaptés aux différentes langues de nos jeux de données. Ensuite, dans le cas spécifique des corpus issus du fd-TED, les codes CPV (système de classification pour les marchés publics) contenus dans le texte sont remplacés par un motclé neutre (%digit%), car ces codes définissent la classe des documents. Par ailleurs, nous supprimons également les termes dont le nombre d'occurrences est inférieur à 5 dans un jeu de données.
Une fois les pré-traitements effectués, nous considérons les vectorisations suivantes : les pondérations tf et tf-idf sans et avec application de LSA (notés respectivement tf, tf-idf, LSA tf, LSA tf-idf par la suite), la moyenne des vecteurs word2vec (noté W2V par la suite), ainsi que la combinaison tf-idf avec LSA + word2vec (notée LSA+W2V par la suite).
Dans notre expérimentation, nous avons choisi 100 dimensions pour LSA et W2V ce qui conduit à une vectorisation combinée en 200 dimensions.
-MLP : Un perceptron multicouches (Rumelhart et al., 1986) avec deux couches cachées de 200 neurones chacune, la fonction d'activation utilisée est la sigmoid : e x e x +1 . -SGD : Une machine linéaire à vecteurs supports optimisée par descente de gradient stochastique (Zhang, 2004). -NB : Un classifieur Bayésien multinomial naïf (Kibriya et al., 2004) avec α = 1 pour les pondérations tf et tf-idf. Un classifieur Bayésien naïf gaussien (Hand et Yu, 2001) est utilisé pour les autres types de vectorisations car ces dernières peuvent contenir des valeurs négatives, avec une variance par défaut de σ = 10 −9 . La performance de ces différents algorithmes est évaluée en fonction des différentes vectorisations décrites précédemment. Toutes les combinaisons ne peuvent cependant pas être évaluées pour des raisons de complexité algorithmique. Il n'est en effet pas concevable en pratique de combiner par exemple une vectorisation en très grande dimension tf-idf à un classifieur MLP compte tenu de la combinatoire engendrée.
La classification 'multi-label' a été effectuée en utilisant lapproche 'One v.s. Rest', qui consiste à entraîner un modèle binaire par classe. Les individus dune classe sont considérés comme positifs et tous les autres comme négatifs. Un seuil de décision (0.5) est ensuite exploité (si le logarithme de la probabilité d'un individu par rapport à une classe est supérieure au seuil) pour décider de laffectation.
Résultats expérimentaux
Cette section présente les résultats expérimentaux obtenus pour les différents couples (vectorisation ; algorithme) présentés.
Les algorithmes de classification sont évalués sous l'angle de quatre métriques : -l'exactitude empirique ( accuracy) qui représente le pourcentage des observations éti-quetées de manière identique à l'annotation : A = vp+vn vp+f p+vn+f n , -La précision P = vp vp+f p , -Le rappel R = vp vp+f n où vp est le nombre de vrais positifs, f p est le nombre de faux positifs et f n est le nombre de faux négatifs, -La F1-mesure F = (1+β 2 ).P.R β 2 .P +R (avec β=1), où vp, f p, vn, f n représentent respectivement les nombres de vrais positifs, faux positifs, vrais négatifs, faux négatifs.
Les métriques (précision, rappel, F1-score) utilisées sont présentées sous la forme dune moyenne pondérée par rapport au support de chaque classe. Il sagit donc dune macro-moyenne pour tenir compte du déséquilibre des classes.
Par ailleurs, 80% du jeu de chacune des bases de données présentées en section 4.1 sont utilisés en tant que données d'apprentissage et 20% en tant que données de test, en procédant à une validation croisée (k=5).
Les tableaux 1, 2, 3 et 4 présentent les résultats de classification obtenus sur l'ensemble des quatre corpus tels que définis en section 4.1. TAB. 1 -Résultats obtenus pour les méthodes testées sur le jeu de données 20NG.
Pour le corpus 20NG, la meilleure exactitude est obtenue avec le classifieur SGD pour la vectorisation tf-idf. Ce résultat s'explique par le fait que la pondération tf-idf arrive à mieux représenter un document en considérant tous les mots qui le forme, ce qui lui permet de séparer de manière plus efficace les classes les plus similaires. En effet, 20NG est une base contenant des classes assez similaires sémantiquement (exemple : religion.misc et religion.christian), et d'autres classes plus éloignées (exemple : religion.misc et sci.electronics).
Pour les autres jeux de données, notre modèle LSA+W2V obtient les meilleurs résultats. Pour le TED, le rappel est très largement supérieur à celui obtenu avec les autres méthodes. Ce jeu de donnés se révèle constituer une tâche difficile de classification, notamment par le fait qu'il contienne beaucoup de bruit, et que les mots réellement discriminants par rapport aux classes ne représentent qu'une partie du texte. LSA (tf-idf ) se classe en deuxième position avec une exactitude empirique de 49,45% et word2vec en quatrième position avec une exactitude empirique de 45,23% comme indiqué dans le tableau 2. TAB. 4 -Résultats obtenus pour les méthodes testées sur le jeu de données TED-FILTRE.
Notre modèle obtient respectivement une exactitude empirique de 55.27%, 58.51% et 77.47% sur les jeux de données TED-FR, RCV1, TED-FILTRE, comme le montrent les tableaux 2, 3 et 4. Les résultats plus faibles obtenus sur le TED-FR s'expliquent en grande partie par la nature de la base, qui est très bruitée notamment par la présence d'information juridicoadministrative (laquelle représente la majorité des textes), et la difficulté de la classification multi-étiquettes en 45 classes.
Conclusion
Nous avons introduit dans cet article un modèle simple de vectorisation en basse dimension des textes d'un corpus qui combine les méthodes LSA et word2vec. Nous avons comparé cette vectorisation aux autres approches classiques de vectorisation proposées dans l'état de l'art.
En particulier, notre méthode obtient de meilleurs résultats comparativement à toutes les autres méthodes testées pour trois des quatre jeux de données traités sur des tâches de classification supervisées. Sur le quatrième jeu de données qui comporte des classes très similaires du point de vue sémantique, notre méthode en basse dimension est pénalisée par rapport aux approches qui représentent les documents sous la forme de sac-de-mots. L'exploitation de cette approche de vectorisation sur une tâche de clustering de documents textuels constitue une perspective à ce travail.

Introduction
La fouille d'opinion est devenue un champ de recherche important du Traitement Automatique des Langues (TAL) et sa maturité est attestée par les nombreux états de l'art dont elle a fait l'objet [Chapate et all. (2015); Feldman (2013); Liu (2012), etc.]. Son objectif est d'analyser l'ensemble des opinions des internautes sur un objet donné. La tâche se veut plus précise avec la fouille d'opinion ciblée (ABSA ; Aspect Based Sentiment Analysis) [Liu (2012)] : les cibles correspondent aux différentes caractéristiques des objets concernés et les avis des internautes sont analysés en fonction de ces cibles. L'ABSA a fait l'objet de différents challenges, parmi lesquels SemEval-2014, 2015et 2016[Pontiki (2016].
Ciblée ou non, la fouille d'opinion est une tâche extrêmement complexe pour de multiples raisons : forme indirecte de l'expression, humour, subjectivité, nécessité d'un traitement fin de la négation, etc. Les approches utilisées sont nombreuses et variées, généralement divisées entre méthodes statistiques, généralement supervisées [Pang et Lee (2008)] et méthodes basées sur un lexique d'opinion 1 , à base de règles [Neviarouskaya et al. (2010)] ou statistiques [Wilson et al. (2005)], une tendance actuelle étant de combiner différentes approches.
La fouille d'opinion ciblée peut être partagée en trois sous-tâches : (i) détection de phrases ou portion de phrases porteuses d'opinion, (ii) détection de la cible relative à l'opinion émise, (iii) détection de la polarité et éventuellement, de l'intensité de l'opinion en question. Nos expérimentations concernent les deux dernières. Par ailleurs, les travaux présentés analysent des avis laissés par les internautes concernant les livres : un domaine plus complexe que ceux géné-ralement envisagés, où l'opinion est souvent exprimée de manière indirecte ou complexe et les cibles parfois peu différenciées. La langue, intermédiaire entre la langue académique et celle des tweets, rend possible l'utilisation d'une analyse syntaxique de surface (chunking). Notre étude s'est plus particulièrement attachée à explorer les améliorations que peut apporter ce type d'analyse, jamais testée en ABSA précédemment à notre connaissance. La section 2 décrit les corpus et la tâche de détection des cibles, où le chunking semble inopérant ; la section 3 montre les avantages de la méthode pour la détection de la polarité.
2 Corpus et détection des cibles
Corpus et annotations
En fouille de données ciblée, l'absence de corpus disponible en langue française dans le domaine des livres qui nous intéressait, nous a conduit à combler ce manque : nous avons annoté 900 avis d'internautes collectés sur le site Amazon.fr. Le schéma d'annotation proposé se veut générique et très précis ; il distingue 5 cibles principales, elles-mêmes divisées en attributs pour un total final de 21 classes possibles. L'annotation distingue (a) l'expression de l'opinion ; (b) l'entité à laquelle cette opinion se rattache (si exprimée) ; (c) la cible et (d) une valeur entière comprise entre -2 et 2, qui exprime la polarité de l'opinion en même temps que son intensité. Au total, le corpus comporte 3300 expressions d'opinion ainsi annotées ].
Pour les tâches que nous nous proposions de réaliser, nous avons réduit le nombre de cibles à 8 en effectuant des regroupements et nous n'avons pas tenu compte de l'intensité des opinions. Comme corpus de tests, nous avons présélectionné 340 phrases ou portions de phrases porteuses d'opinion dans la partie non annotée du corpus et nous avons choisi pour chacune d'elles l'une des 8 cibles et la polarité de l'opinion correspondantes.
Détermination des cibles
Pour ce qui concerne la détection des cibles, le domaine étudié se distingue des domaines classiquement étudiés par deux particularités. La première est que la désignation des entités n'y suffit pas. Par exemple, dans la phrase : « le livre est bien écrit », la cible est le Style alors que l'entité, le livre, ne la désigne pas. La deuxième est que les classes diffèrent très fortement de par leurs importances relatives : avec près de 45% des annotations, la classe qui désigne une opinion globale sur l'ouvrage (General), est très fortement prévalente ; à l'inverse, la classe Illustrations en regroupe moins de 1%.
Nous avons testé plusieurs modèles statistiques classiques entraînés avec les noms, adjectifs, verbes et adverbes (sauf mots grammaticaux) figurant dans les expressions d'opinion et les entités annotées : kNN, Random Forest, Neural Network, SVM, Fuzzy classification, SOM (Kohonen package in R), etc. La prévalence de la classe General pose problème à tous les classifieurs, jusqu'à aboutir à une totale inefficacité de certains qui classent la quasi-totalité des tests en General 2 . Nous avons également utilisé Word2Vec pour pouvoir prendre en compte 2. Pour plus d'informations, on pourra se reporter à Villaneau et al. (2018) les mots du corpus de test absents du corpus d'entraînement (SVMW2V). Par ailleurs, les essais d'introduction de paramètres linguistiques n'ont pas donné de résultats concluants. Les meilleurs résultats ont été obtenus en utilisant une méthode en sac de mots lemmatisés avec les SVM (noyau linéaire) et les Random Forest (ntree=500)  Pour la reconnaisance des différents chunks, nous avons réadapté un outil précédemment utilisé dans une étude de patrons sémantiques [El Maarouf et al. (2011)]. La difficulté rencontrée à ce stade est la recherche d'un compromis entre les erreurs inévitables du chunking -en particulier causées par les agrammaticalités du texte -et la précision nécessaire.
Le lexique d'opinion
Le lexique d'opinion est l'une des pierres angulaires d'un système de détection de l'opinion dans une approche linguistique [Breck et Cardie (2017)]. Nous avons d'abord utilisé un lexique obtenu par compilation de deux lexiques existants : la norme émotionnelle ValEmo [Syssau et Font (2005)] et une extension de la norme F-POL [Vincze et Bestgen (2011)]. Le lexique obtenu s'est avéré trop général : par exemple le verbe dormir qui y est positivement connoté, exprime généralement une opinion négative lorsqu'il est utilisé dans une critique de livres.
De plus, nous sommes arrivés à la conclusion énoncée par Taboada et al. (2011) que « more words may lead to including more noise" »[plus de mots peut conduire à plus de bruit] et nous avons réduit le vocabulaire à celui qui exprime clairement une opinion sur un livre. En revanche, nous avons introduit un lexique d'expressions courantes en langue française, une étude plus approfondie étant en dehors du cadre de notre étude.
Traitement de la négation
Le traitement de la négation est une pierre d'achoppement en analyse d'opinion. Wiegand et al. (2010) en présente diverses approches, les deux problèmes principaux étant d'une part, sa détection et d'autre part, la détermination de sa portée.
Bon nombre de difficultés se retrouvent dans d'autres langues, notamment l'anglais : fausses négations (« non seulement »), doubles négations, patrons spécifiques à chaque mot négatif, négation exprimée de manière indirecte ou subtile [Pang et Lee (2008); Asmi et Ishaya (2012)], etc. L'un des problèmes spécifique à la langue française est l'usage de l'adverbe ne : fréquemment omis dans la langue orale, son omission tend à se répandre dans la langue écrite informelle, jusqu'à conduire à des expressions ambigües dans la langue écrite, telles que « il y en a plus », qui peut signifier aussi bien il y en a davantage que il n'y en a plus.
Dans notre approche compositionnelle, une fonction est attachée à la négation : elle modifie le score des éléments qui sont dans la portée de cette dernière. Nous avons testé plusieurs fonctions proposées dans la littérature sans constater de modification globale des résultats obtenus : il semble donc que le choix exact de la fonction soit beaucoup moins significatif que la détermination de la portée. Globalement, il convient de prend en compte le fait qu'une négation atténue l'intensité tout en étant généralement l'indice d'une opinion plutôt négative.
Les chunks et leur usage
Le chunking permet de prendre en compte partiellement les problèmes liés à l'association des mots si l'on associe un traitement spécifique à chaque forme de chunk. Les chunks verbaux jouent un rôle majeur dans notre approche. Outre la prise en compte du temps et du mode, ils transmettent ou non une éventuelle négation aux chunks qui les entourent en fonction de la nature du verbe qui est à leur tête. Le rôle essentiel des chunks nominaux est la prise en compte des associations entre adjectif et nom, certains adjectifs induisant une polarité stable, positive ou négative (par exemple mauvais, excellent) alors que d'autres accentuent, atténuent ou inversent la polarité du nom qu'ils qualifient (grand, faux). Les chunks adjectivaux ou adverbiaux permettent de prendre en considération les modifieurs qui peuvent, comme les adjectifs, atténuer, renforcer ou inverser la polarité (très, trop, un peu, etc.)  [Zhang et al. (2012)].
Mises en oeuvre et résultats
En guise de baseline, nous avons utilisé plusieurs méthodes statistiques classiques pour classifier les 340 phrases de test entre opinion positive et opinion négative. SVM (avec un noyau linéaire), Glmnet (régression logistique), NeuralNet (20-5) (NN) et Random Forest (500 arbres) (RF) obtiennent des F1-scores compris entre 0,768 et 0,803 (cf. table 1), par une approche en sacs de mots utilisant comme variables les lemmes des noms, verbes, adjectifs et adverbes présents dans les annotations. Contrairement à ce qui avait été observé dans la détec-tion des cibles, un vote majoritaire entre méthodes n'améliore en rien les résultats.
Les résultats obtenus par l'approche basée sur le chunking (Chv1) avec le lexique utilisé s'avèrent décevants : avec un F1-score de 0,774, ils sont inférieurs à ceux obtenus par trois des quatre baselines. Le nombre important de phrases qui obtiennent un score nul suggère un problème de lexique. 
Conclusion et perspectives
La cible d'une opinion peut généralement être déterminée en étudiant les mots utilisés pour l'exprimer : les approches lexicales et statistiques s'avèrent très efficaces dans cette tâche et nos expérimentations ne remettent pas en cause cette prépondérance. En revanche, elles suggèrent que, combinée à un lexique d'opinion très ciblé, une analyse de surface est une approche efficace pour déterminer la polarité de l'opinion. Elles suggèrent également que la pertinence d'un lexique d'opinion est très dépendant du domaine et, très probablement, du corpus luimême, ce qui, en soi, restreint la généricité des approches basées sur de tels lexiques. Ces résultats demandent à être confortés et validés sur d'autres corpus et d'autres domaines.

Introduction
Les jeux de données multi-relationnelles suivent un schéma (modèle de données), où des entités (objets) de plusieurs catégories sont décrites par leurs caractéristiques (attributs) et où des relations relient les objets des différentes categories. Les experts des domaines concernés les exploitent dans le cadre de différentes tâches : consultation ou exploration, requêtage, extraction de motifs, ou encore classification, au sens d'organiser dans une hiérarchie de géné-ralisation des ensembles d'objets similaires. Cet article se concentre sur la tâche d'exploration fondée sur une telle organisation hiérarchique.
L'analyse formelle de concepts, ou AFC (Ganter et Wille, 1999), et ses extensions apportent des méthodes qui contribuent à l'exploitation des données, notamment multi-relationnelles (Liquière et Sallantin, 1998;Kötters, 2013;Ferré, 2015;Ferré et al., 2005;Hacene et al., 2013). Parmi ces méthodes, l'analyse relationnelle de concepts (ARC) a été spécialement conçue pour les tâches sus-citées (Hacene et al., 2013). Cette méthode construit un ensemble de classsifications interconnectées, qui peuvent être utilisées pour extraire des motifs et des règles d'implication portant sur les liens inter-objets ainsi que sur des abstractions de ces liens.
L'ARC a pour caractéristique principale de construire des abstractions des liens inter-objets en appliquant des opérations d'échelonnage sur des groupes d'objets. Ces abstractions per-mettent de grouper des objets qui ont au moins un / seulement un / tous / au moins 30% / etc. (parmi) leurs liens sortants pour une certaine relation vers un autre groupe d'objets identifié ; ces quantifieurs sont appelés quantifieurs d'échelonnage. Ceci permet de propager des regroupements d'objets à travers des chaînes de liens inter-objets. Combinée avec la nature incrémentale et exploratoire de l'ARC, où à chaque étape on peut choisir les relations à considérer, la variété des quantifieurs rend le processus très finement réglable (Braud et al., 2018).
D'un côté, ces possibilités de réglage rendent les résultats de l'ARC très expressifs, avec des descriptions utilisant des quantifieurs autres que les classiques quantifieurs universel et existentiel. D'un autre côté, la multiplicité des choix à réaliser lors d'une tâche d'analyse peut submerger l'analyste. Pour traiter ce problème, une adaptation de l'ARC a été proposée par Dolques et al. (2015), où les relations sont explorées de façon graduelle grâce à une configuration pas-à-pas plutôt que globale. Néanmoins, à notre connaissance, aucun outil n'existe qui permette de guider l'utilisateur et de rendre le processus plus intuitif.
Dans cet article, nous proposons d'introduire de la connaissance dans le processus de l'ARC pour faciliter l'analyse : de la connaissance a priori fournie par l'utilisateur pour contraindre le processus au départ, et de la connaissance fournie par le processus à l'utilisateur, qui peut ainsi mieux décider comment le régler. Nous proposons trois sur-couches du processus de l'ARC : la première permet d'exprimer des contraintes sur les quantifieurs, pour en faire un choix cohérent ; la seconde permet de traduire des requêtes de haut niveau (du niveau schéma), qui sont difficiles à exprimer par les utilisateurs, en des expressions d'un langage contrôlé ; la troisième permet de donner des métriques quantitatives sur les treillis et sur les règles d'implication obtenus par des réglages voisins du réglage courant afin d'aider les utilisateurs à affiner leur analyse. Nous appliquons nos propositions à un jeu de données sur la qualité des eaux de rivières, issu du projet FRESQUEAU 1 , qui concerne les relations entre l'état physico-chimique de l'eau et les caractéristiques des taxons (macroinvertebrés) qui y vivent. Ce jeu de données est réduit à des fins d'illustration : des jeux de données plus importants, et dans d'autres domaines, peuvent être traités par des techniques de seuillage ou de sélection de concepts, L'article est organisé de la manière suivante. La section 2 présente le type d'exploration relationnelle que l'ARC permet et pourquoi il peut être difficile pour les experts de conduire une analyse. La section 3 présente l'ARC et son utilisation comme outil de requête. La section 4 décrit les trois sur-couches et les illustre sur les données FRESQUEAU. Nous concluons et dressons quelques perspectives dans la section 5.
Exploration de jeux de données relationnelles
De nombreuses données sont multi-relationnelles de façon inhérente, impliquant des relations de différentes natures. Ceci motive le développement de méthodes pour extraire des motifs relationnels ou des règles d'association relationnelles, induire des arbres de décision relationnels, ou encore construire des classes en utilisant des distances relationnelles (Džeroski, 2003). Dans le domaine de l'AFC, les données relationnelles sont envisagées par le biais de représentations à base de graphes (Liquière et Sallantin, 1998;Kötters, 2013;Ferré, 2015), sous forme d'expressions logiques (Ferré et al., 2005) ou sous forme tabulaire, comme dans l'ARC (Hacene et al., 2013).
Quand les experts ont une connaissance vague des données et quand leurs questions sont générales, une approche exploratoire s'avère adaptée (Wildemuth et Freund, 2012;Palagi et al., 2017). L'exploration des données peut alors être complètement libre, ou guidée par ces questions générales, souvent au niveau du schéma des données (concepts et relations).
Nous utilisons ici comme illustration un cas d'exploration typique pour un hydroécologue étudiant l'effet de l'état physico-chimique des eaux d'une rivière sur les caractéristiques (ou traits de vie) des taxons qui y vivent. Un extrait du modèle de données est montré sur la figure 1 : les sites échantillonnés (par différentes techniques de prélèvement) ont une certaine abondance de taxons (identifiants d'êtres vivants, ici des macroinvertebrés, organisés en genres et familles) qui ont une certaine affinité pour certaines modalités de traits de vie (p. ex. modalités de taille maximale, stade aquatique -oeuf, larve, nymphe -mode de respiration, mode de locomotion). Les sites échantillonnés sont aussi décrits par des mesures sur des paramètres physico-chimiques (PC) (p. ex. nitrites, minéraux, matière organique, température) organisés en catégories selon leur nature. Pour chaque relation, le niveau varie en 5 degrés d'intensité dé-pendant du nombre d'individus d'un taxon sur le site pour l'abondance, de la part de population d'un taxon montrant une modalité d'un trait de vie pour l'affinité et de la valeur mesurée sur le site échantillonné pour une mesure de paramètre PC. Pour les besoins de l'analyse, chaque relation entre les catégories d'objets est divisée en 5 relations correspondant aux niveaux.
FIG. 1 -Extrait du modèle de données du projet FRESQUEAU
Une question générale posée par les hydroécologues est : quels sont les liens entre les traits de vie des taxons et les valeurs des paramètres physico-chimiques ? L'exploration du jeu de données pour répondre à cette question peut prendre plusieurs formes, comme l'extraction de règles impliquant les relations, ou le regroupement d'objets de différentes catégories (comme les sites échantillonnés ou les taxons) selon leurs attributs et les objets d'autres catégories avec lesquels ils sont connectés. Par exemple, les experts peuvent être intéressés par les réponses à la question suivante : trouver des groupes de sites échantillonnés qui ont (1) un certain niveau d'abondance pour un groupe de taxons, ayant eux-même en commun un groupe de traits de vie avec un certain niveau d'affinité et qui ont (2) à un certain niveau, des paramètres physico-chimiques d'un certain groupe. Les résultats peuvent révéler, par exemple : un groupe de sites contenant des taxons avec une durée de vie longue, et contenant beaucoup de matières organiques ; un groupe de sites avec un haut niveau de minéraux dissous, et contenant des taxons qui se déplacent en rampant (mode de locomotion). Pour une étude plus précise, la question générale et les groupes extraits peuvent être raffinés dans plusieurs directions. Pour en donner seulement un exemple, considérons la question suivante (les termes en caractères gras indiquent des points de variabilité dans la question) :
-trouver des groupes de sites échantillonnés qui ont (1) un certain niveau d'abondance pour un groupe de taxons (et plus de 60% des taxons de chaque site sont dans ce groupe), ayant eux-même seulement des traits de vie dans un même groupe de traits de vie avec un certain niveau d'affinité et qui ont (2) à un certain niveau de concentration, tous les paramètres physico-chimiques d'un certain groupe. D'une part, reformuler la question générale avec ces indications est important pour les experts car ils ont besoin de réponses précises et peuvent aussi vouloir changer le focus de leur analyse. D'autre part, le raffinement peut être fait dans de nombreuses directions, et les experts peuvent être rapidement perdus. Par ailleurs, choisir un raffinement plutôt qu'un autre peut conduire à des ensembles de résultats trop restrictifs, ou au contraire, à des ensembles trop larges. Finalement, les résultats sont des ensembles de groupes d'objets reliés qui respectent un certain schéma de question. Ces groupes peuvent être nombreux et peuvent se spécialiser les uns les autres, comme nous le montrons plus loin.
3 L'ARC, un outil pour l'interrogation de données L'analyse relationnelle de concepts (Hacene et al., 2013) étend l'analyse formelle de concepts (Ganter et Wille, 1999) aux jeux de données multi-relationnelles et ajoute aux approches citées précédemment une panoplie d'opérateurs et une approche itérative qui permet de suivre la propagation de la connaissance, de faciliter la compréhension du processus de formation des motifs, des règles ou des classifications et de favoriser le raisonnement, y compris abductif. Les jeux de données multi-relationnelles y sont représentés par des familles relationnelles de contextes (RCF), composées de contextes formels et de relations. Chaque contexte formel (ou contexte objets-attributs) représente un ensemble d'objets d'une certaine catégorie par ses attributs. Une relation (ou contexte objets-objets) connecte les objets des différentes catégories (ou de la même catégorie).
Définition 1 (Famille relationnelle de contextes (RCF)) Une famille relationnelle de contextes est une paire (K, R) où :
Le tableau 3 montre un exemple simplifié de RCF inspiré de notre application dans le domaine hydro-écologique (a_abondance est ici une relation binaire pour simplifier l'illustration). Le contexte formel Taxon introduit :
-les taxons Aeschnidae (Aes.), Agabus (Agb.), Agraylea (Aga.), Agriotypus (Agi.), Ancylus (Anc.), Anisus (Ani.), Anodonta (Ano.), Anthomyiidae (Ant.). -cinq attributs décrivant leur micro-habitat (rochers, graviers, sable, macrophytes, débris organiques). Le contexte formel SiteEchantillon décrit 8 sites par les caractéristiques de leur flux (torrent, eau calme) et leurs composants chimiques. Le contexte objets-objets a_abondance connecte les 8 sites aux taxons qui y ont été trouvés durant l'échantillonage.
L'AFC peut être appliquée aux contextes formels Taxon et SiteEchantillon pour former des hiérarchies de groupes d'objets partageant des attributs communs (concepts). 
Définition 2 (Concept formel) Etant donné un contexte objets-attributs K = (G, M, I), un concept associe un ensemble maximal d'objets avec l'ensemble maximal des attributs qu'ils partagent, pour former une paire C = (Extension(C), Intension(C)) telle que : Extension(C) = {g ∈ G|∀m ∈ Intension(C), (g, m) ∈ I} est l'extension du concept (objets couverts par le concept) ; Intension(C) = {m ∈ M |∀g ∈ Extension(C), (g, m) ∈ I} est l'intension du concept (attributs partagés).
Les concepts formels sont ordonnés par une relation de spécialisation/généralisation, denotée par C , fondée sur la relation d'inclusion ensembliste. Etant donnés deux concepts formels C 1 = (E 1 , I 1 ) and C 2 = (E 2 , I 2 ), C 2 C C 1 si et seulement si E 2 ⊆ E 1 (et de façon équiva-lente I 1 ⊆ I 2 ). C 2 est une spécialisation (i.e., sous-concept) de C 1 . C 1 est une généralisation (i.e., super-concept) de C 2 . L'intension de C 2 hérite des attributs de l'intension de C 1 , tandis que l'extension de C 1 hérite des objets de l'extension de C 2 . L'ordre C munit l'ensemble des concepts de K d'une structure de treillis, appelée treillis de concepts de K.
La figure 2 montre les treillis de concepts associés aux contextes formels des sites échan-tillonnés (à gauche) et des taxons (à droite). Le treillis des sites échantillonnés met en évidence le groupe de sites échantillonnés dans des eaux calmes (C_SiteEchantillon_5) versus le groupe des sites échantillonnés dans des torrents (C_SiteEchantillon_4). Les sites échan-tillonnés dans des eaux calmes sont ensuite séparés en trois sous-groupes en fonction de la présence de calcium (Ca, pour C_SiteEchantillon_1), de magnésium (Mg, pour C_Site-Echantillon_2), ou de glyphosate (C3H8NO5P, pour C_SiteEchantillon_3).
L'ARC permet d'introduire des attributs relationnels pour compléter les contextes formels initiaux afin de prendre en compte l'information relationnelle. Une relation r j ⊆ G k × G l est utilisée pour construire les attributs relationnels de K k en utilisant les relations entre les objets de G k et les concepts construits sur les objets de G l . La figure 3 illustre la notion d'at- la partie intermédiaire contient l'intension privée des attributs hérités des super-concepts (elle ne contient que les attributs "introduits") ; la partie inférieure contient l'extension privée des objets hérités des sous-concepts (elle ne contient que les objets "introduits").
tribut relationnel avec quelques exemples. Un attribut relationnel est composé d'un quantifieur d'échelonnage, du nom de la relation, et du concept cible. Par exemple : -l'attribut relationnel ∃a_abondance(Concept_T axon_2) est associé aux sites échan-tillonnés se3, se7 et se8 car ils ont au moins un lien a_abondance avec un taxon de l'extension du Concept_T axon_2. -l'attribut relationnel ∃∀a_abondance(Concept_T axon_3) est associé au site échan-tillonné se6, car il a au moins un lien a_abondance et ces liens sont seulement dirigés vers les taxons de l'extension du Concept_T axon_3. -l'attribut relationnel ∃∀ ≥60% a_abondance(Concept_T axon_2) est associé aux sites échantillonnés se7 et se8 car ils ont au moins un et au moins 60% de leurs liens a_abondance vers les taxons de l'extension du Concept_T axon_2. -l'attribut relationnel ∃⊇a_abondance(Concept_T axon_1) est associé aux sites échan-tillonnés se3 et se6 car ils sont reliés à au moins un et à tous les taxons de l'extension de Concept_T axon_1 à travers un lien a_abondance. En utilisant cette information relationnelle, on peut étendre les contextes formels avec les attributs relationnels et ainsi construire de nouveaux treillis de concepts. Par exemple, à gauche de la figure 4 (resp. à droite) est représenté le treillis de concepts associé au contexte formel des sites échantillonnés étendu avec tous les attributs relationnels possibles composés du quantifieur d'échelonnage ∃∀ (resp. ∃∀ ≥60% ) et des concepts du treillis de concepts des taxons de la figure 2. La figure 4 montre aussi la relation de généralité entre les quantifieurs d'échelonnage. Dans notre exemple, ∃∀ est plus général que ∃∀ ≥60% (dénoté par ∃∀ ∀ S ∃∀ ≥60% ), avec pour conséquence que si un objet possède un attribut relationnel formé avec ∃∀, il possède aussi son équivalent (même relation/même concept) formé avec ∃∀ ≥60% ; il existe ainsi une forme de projection entre les attributs relationnels introduits dans le treillis de gauche vis-à-vis de ceux introduits dans le treillis de droite, et entre les treillis (Braud et al., 2018).
Les questions des hydroécologues pourraient être traitées comme des requêtes à une base de données. Ce que l'AFC apporte est une organisation inhérente des réponses à leurs requêtes par regroupement hiérarchique (Messai et al., 2005;Azmeh et al., 2011). Par exemple, si la FIG. 3 -Attributs relationnels construits à partir des concepts Taxon et de la relation a_abondance entre SiteEchantillon et Taxon question d'un expert est trouver les sites échantillonnés en eau calme, le treillis à gauche de la figure 2 organise les réponses via C_SiteEchantillon_5 et ses sous-concepts. L'expert voit ainsi quelles réponses correspondent à sa question avec le minimum de caractéristiques additionnelles, alors qu'à mesure qu'il/elle descend dans le treillis, des caractéristiques sont ajoutées aux groupes. Il/elle voit aussi quels sont les sites échantillonnés parmi les réponses "équivalentes" (qui ont exactement les mêmes caractéristiques), ou quelles caractéristiques apparaissent ensemble. Toutes ces informations l'aident à naviguer dans les réponses possibles à sa question, à comprendre des propriétés de ces réponses et à formuler des hypothèses. Grâce à l'ARC, ceci s'étend à l'information relationnelle. Par exemple, le treillis de concepts de la figure 4 (à droite) met en évidence le fait que les sites échantillonnés dans les eaux calmes (C_SE_5 ) ont une proportion significative de leurs taxons qui apprécient les débris organiques. Il représente plus spécifiquement les réponses à la question générale : "trouver les groupes de sites échantillonnés qui ont plus de 60% de leurs taxons dans un certain groupe de taxons". C_SE_5 et ses sous-concepts montrent l'organisation des réponses à la question trouver les sites échantillonnés en eau calme ou de façon alternative trouver les sites échan-tillonnés qui ont plus de 60% de leurs taxons ayant pour micro-habitat des débris organiques. Cette spécificité de l'AFC et de l'ARC porte l'attention de l'expert sur la structuration des données et des relations et l'amène à naviguer dans les réponses et dans le jeu de données.
Dans le cas général, le modèle de données peut être cyclique, impliquant un processus itératif qui converge après un nombre d'étapes dépendant du jeu de données. Par exemple, on peut avoir la relation inverse est_abondant allant des taxons aux sites échantillonnés. Alors, quand les concepts des sites échantillonnés sont construits, un nouveau treillis de concepts des taxons peut être construit grâce au quantifieur d'échelonnage choisi et aux attributs relationnels en résultant pour est_abondant et ainsi ouvrir la voie à d'autres questions.
FIG. 4 -Treillis de concepts SiteEchantillon (SE) avec ∃∀ (LHS) et ∃∀ ≥60% (RHS)
4 Des guides pour le processus d'exploration de l'ARC L'outil RCAexplore 2 permet des usages variés de l'ARC : changer à chaque étape le quantifieur d'échelonnage, les contextes formels et les relations considérées, et les concepts calculés. Cette diversité a sa contrepartie dans la difficulté à choisir les bons paramètres pour une question donnée. Pour résoudre cette difficulté, nous avons développé trois sur-couches implantées en python et qui communiquent avec RCAExplore : la première sur-couche concerne l'étape de modélisation, elle permet à l'utilisateur d'exprimer des contraintes sur les relations ; la deuxième et la troisième interviennent à l'étape de choix des quantifieurs permettant la construction des attributs relationnels : l'une pour faciliter l'interprétation des expressions des attributs, l'autre pour anticiper leurs effets, en termes de nombre de concepts et de règles produites. Ces développements ont été faits en proximité avec les hydroécologues du projet Fresqueau, ce qui justifie le caractère appliqué de la présentation que nous en faisons ci-dessous. Notre expérience avec l'ARC nous conduit toutefois à penser que ces outils s'appliqueront de même à d'autres domaines.
Contraindre le choix des quantifieurs d'échelonnage. RCAExplore offre la possibilité de choisir parmi plusieurs quantifieurs pour chaque relation, mais parfois, des relations sont sémantiquement liées et les quantifieurs qui leur sont associés doivent alors être cohérents. Par exemple, dans notre jeu de données, chaque relation générale (p. ex. a_abondance) est représentée par plusieurs relations pour capturer la notion de niveau (p. ex. cinq relations a_abondance_de_niveau_i). Si plusieurs relations abondance sont sélectionnées ensemble, il serait alors cohérent de leur appliquer le même quantifieur. Pour cela, nous regroupons les relations en classes d'équivalence : les relations d'une même classe d'équivalence sont considérées de la même façon au cours du processus et on leur applique le même quantifieur d'éche-lonnage à chaque étape. Néanmoins, le quantifieur d'échelonnage pour une classe peut être différent d'une étape à l'autre. Cette information est analysée quand un quantifieur d'échelon-nage est associé à une relation via l'interface utilisateur, afin de propager la contrainte sur les relations de la même classe. L'utilisateur peut accepter ou modifier les propositions du système.
Aide à l'interprétation. Une autre difficulté rencontrée par l'utilisateur est de comprendre l'impact du choix des quantifieurs d'échelonnage sur son analyse. Pour traiter cette difficulté, nous avons développé un interpréteur qui traduit automatiquement les choix réalisés sur l'interface utilisateur en une expression formatée respectant un langage fixé. La partie haute de la figure 5 montre une telle expression, correspondant aux choix faits dans la partie basse. Cette expression est composée à partir d'éléments de la forme : Tableau de solutions voisines. La troisième sur-couche consiste à calculer des métriques de dimensionnement sur des ensembles de solutions voisines de la requête courante, i.e. qui pourraient être obtenues en variant progressivement les quantifieurs suivant la relation de gé-néralité comme le présentent Braud et al. (2018). Ces métriques peuvent guider l'expert dans la navigation entre les différents réglages des quantifieurs. Dans le tableau 2, nous examinons ce qui se passe quand les quantifieurs (pour l'exemple, ∃∀ ≥n% ) sont modifiés sur la relation a_abondance de niveau 3. Dans l'étude pratique, pour répondre aux questions sur la connexion entre les paramètres physico-chimiques et les traits de vie des taxons, nous construisons des treillis dans lesquels les experts doivent naviguer, ainsi que des règles. La dimension des résultats se compte en particulier en nombre de concepts des treillis (colonne 3) et certaines règles d'implication non redondantes de prémisse de taille 1 entre attributs relationnels, intéressant les experts (colonne 4). La dernière colonne donne la taille maximale du support des règles (nombre d'objets dans une extension de concept). Dans le treillis des taxons, les règles d'intérêt extraites sont de la forme suivante : Une règle ma révèle un lien entre l'état physico-chimique du cours d'eau et le niveau de présence de certains groupes de taxons. Une règle aa révèle la présence simultanée de taxons. Ces deux types de règles sont cohérents avec les questions des hydroécologues. Une règle mm donne des résultats déjà connus, comme p. ex. des relations entre les différentes formes de l'azote, qui dépendent des processus chimiques. Les règles am sont a priori moins pertinentes car les taxons (ici macroinvertébrés) n'ont pas d'effet attendu sur les paramètres physico-chimiques. L'outil offre la possibilité de ne pas calculer les types de règles, comme ce dernier, s'ils n'intéressent pas l'expert. En exploitant le tableau 2, l'expert peut d'abord remarquer que le jeu de données contient des échantillons très diversifiés en terme de physicochimie et de population de taxons, ce qui explique la faible valeur du support maximal des règles en général. Si l'expert est intéressé par les classifications, il remarque qu'il n'y pas de grande différence quand le quantifieur d'échelonnage est spécialisé (de 1661 concepts à 1641). Il note que les treillis obtenus pour ∃∀ ≥60% et ∃∀ ont le même nombre de concepts ; il est donc inutile de raffiner ce quantifieur jusqu'à 100% si le but est de réduire la taille des résultats pour en faciliter l'analyse. Si l'expert s'intéresse aux règles mesure/abondance (ma), il peut en examiner le nombre dans la colonne 4, successivement 19, 32, 11 et 7. Il peut alors décider d'utiliser le quantifieur ∃∀ ≥60% qui donne un nombre raisonnable de règles reposant sur une connexion relativement élevée entre les mesures physico-chimiques et les taxons.
Conclusion
Dans cet article, nous avons présenté plusieurs sur-couches de l'outil en accès libre RCAExplore de manière à guider des experts métier, qui n'auraient pas une connaissance approfondie de l'ARC, dans l'exploration de leurs données. L'ARC permet d'extraire des classifications, des règles et des motifs avec une grande variété de filtres procurés par les quantifieurs, mais les modifications apportées aux résultats par ces quantifieurs peuvent ne pas être très intuitives et le nombre de concepts générés peut se révéler important. L'application des contraintes permet de prendre en compte des groupes cohérents de relations et de leur appliquer des quantifieurs de manière homogène. L'interprète de requêtes inclus dans l'outil permet de les clarifier et aide à la fois à les formuler et à comprendre les résultats obtenus. Les scripts calculent des métriques et extraient des règles d'implication respectant certaines formes. Les métriques permettent aux experts de disposer d'un aperçu sur les treillis (nombre de concepts, nombre et support des règles générées), et ainsi de réorienter l'analyse, en étendant ou en restreignant la recherche. Les règles d'implication informent les experts sur les relations entre objets du domaine.
Pour le futur, nous projetons de développer une version de RCAExplore qui soit un outil d'exploration de données totalement intégré permettant de passer des données brutes à des résultats construits exploitables par les experts métier. Pour cela, une interface devra rendre transparent l'enchaînement de l'appel des différentes parties de l'outil, mettre à disposition tous les résultats, certains étant actuellement stockés dans des fichiers et présenter un menu pour faciliter le processus itératif. Les langages de contraintes et d'interprétation mériteront d'être affinés pour se rapprocher du langage naturel, de manière à procurer une interface plus simple d'utilisation. Nous devrons aussi analyser la réaction des utilisateurs à cette interface et voir en quoi elle facilite effectivement l'usage de l'ARC.
Enfin il sera pertinent de travailler sur la complémentarité entre l'ARC et d'autres approches. Nous avons déjà réalisé une comparaison avec la recherche de motifs temporels et l'apprentissage inductif ; d'autres travaux existent sur les liens entre l'ARC et le bi-clustering, ou, plus généralement, l'apprentissage (Kaytoue et al., 2015). Ces comparaisons pourront être étendues aux bases de données relationnelles, aux bases de données inductives, et également à OWL/RDF et SPARQL.
Remerciements. Ce travail a été partiellement financé par l'AFB (Agence française de biodiversité). Merci à Corinne Grac (UMR 7362 LIVE -ENGEES) pour son aide.

Introduction
L'intérêt pour la fouille d'opinions (Ravi et Ravi, 2015) affiché tant par le monde académique que par l'industrie a stimulé de nombreuses recherches durant la dernière décennie comme le confirme une étude bibliométrique récente (Piryani et al., 2017). Ces travaux, qui combinent des techniques variées du traitement automatique de la langue à l'apprentissage, cherchent à découvrir automatiquement les opinions des consommateurs sur un sujet donné à partir de corpus souvent hétérogènes (forums, enquêtes sur le web, réseaux sociaux) et fortement bruités. Si les algorithmes automatiques permettent aujourd'hui d'extraire avec succès cadre d'une analyse d'opinions sur la réalité augmentée.
2 Cadre méthodologique et travaux connexes
La Q-méthode
La méthode Q, initialement développée par un psychométricien (Stephenson, 1981), se décompose en trois phases. La première phase vise à constituer un ensemble de q items qui tente de couvrir une large part des différentes opinions possibles. Cet ensemble est construit à partir de différentes sources (entretiens, éléments issus de la littérature, descriptions d'objets, etc) et nous renvoyons à (McKeown et Thomas, 2013) pour les détails méthodologiques de cette collecte. Dans la deuxième phase, ces items sont soumis à un échantillon de n individus qui doivent les classer en fonction de leur degré d'adéquation/d'inadéquation à leur propre point de vue. Ce classement s'effectue en deux temps. Tout d'abord, les items sont rangés dans trois classes ("d'accord", "neutre", "pas d'accord") sans contrainte d'effectifs. Un deuxième classement est ensuite effectué sur une échelle discrète plus fine : par exemple de la classe -3 à la classe +3 où -3 correspond à un profond désaccord et +3 à un profond accord. Le classement, appelé Q-sort, est contraint par une distribution des fréquences selon les degrés d'approbation fixée selon une courbe Gauss : ces contraintes conduisent à un arbitrage global entre les affirmations et non à une série d'évaluations autonomes affirmation par affirmation. Dans la troisième phase, une analyse en composantes principales -souvent complétée par des rotations -est réalisée sur la matrice nxq comprenant les résultats des classements des q items par les n individus (Zabala, 2014). Cette analyse permet d'identifier des points de vue synthétiques partagés par plusieurs individus. Elle peut être complétée par des variables exogènes ainsi que par des commentaires ex-post des répondants.
L'analyse de la dynamique
À notre connaissance, le travail que nous présentons est la première tentative de prise en compte de la dynamique de classement dans la Q-méthode. Dans d'autres contextes, quelques pistes ont été explorées mais elles restent rares. En fouille d'opinions, quelques travaux ont tenté de modéliser la notion d'hésitation (Lu et al., 2007;Pathak et Towari, 2016). Ils portent essentiellement sur une adaptation des règles d'association qui, s'inspirant de la théorie des ensembles flous, associe chaque item à une fonction continue dont l'information associée est ensuite intégrée dans le calcul du support et de la confiance. On retrouve également des tentatives de modélisation de la notion d'hésitation et d'incertitude des choix dans le contexte de l'aide multicritère à la décision (Roy, 1988;Bouyssou, 1989) qui s'appuie aussi parfois sur des procédures de classement. Ces travaux portent essentiellement sur leur modélisation mathématique et leur intégration dans des algorithmiques d'aide à la décision mais les publications ne comportent pas d'expérimentations sur des données réelles. Sur le plan méthodologique, les travaux les plus proches de la démarche que nous explorons se trouvent en en psychologie comportementale et en analyse sensorielle. En psychologie comportementale, l'analyse des traces de la souris lors de la résolution de tâches contribue à mieux cerner des processus cognififs mis en jeu dans des tâches de décision (Freeman et al., 2011;Hehman et al., 2014). En analyse sensorielle où les méthodes de classement sont très populaires, l'objectif général est d'appréhender les dimensions sensorielles qui permettent de discrimer des stimuli et l'analyse de la dynamique tente de fournir des garanties sur la stabilité des résultats finaux grâce à une compréhension plus fine des facteurs mis en jeu dans le processus de discrimination. Dans ce cadre, (Cadoret et al., 2011) se sont intéressés au processus associé à une tâche de classement hiérarchique, et l'étude des séquences de partitions emboîtées proposées par les sujets a montré que des dimensions sensorielles communes pouvaient émerger de stratégies de classement différentes. Plus récemment, (Lê et al., 2016) ont étudié les traces du doigt lors d'un classement libre d'images sur tablette. Une expérimentation portant sur des parfums a confirmé qu'une même dimension sensorielle devait être considérée différemment dans l'analyse si elle était identifiée au cours du temps de façon similaire par la majorité des sujets ou si elle émergeait de différentes combinaisons de processus. Dans notre cadre, nous analysons les traces de la souris (trajectoires géométriques et séquences temporelles de tous les évènements) recueillies aux échelles individuelles pour les deux étapes de construction du Q-sort. L'objectif de l'analyse est double. Dans un premier temps, il s'agit d'identifier différentes stratégies mises en oeuvre dans le processus de classement et de repé-rer dans la masse les comportements individuels atypiques qui peuvent influencer les résultats globaux. Et dans un deuxième temps, il s'agit de renforcer la robustesse des résultats agrégés finaux en intégrant dans leur calcul les variations observées à l'échelle individuelle.
3 Protocole expérimental
Recueil des traces
Avec l'essor de la Q-méthode plus d'une quinzaine d'interfaces ont été développées (ex. PQ-method) avec un intérêt croissant pour le recueil en ligne (ex. Ken-Q Data). Cependant, aucune ne mémorise la dynamique de construction du Q-sort. Nous avons donc développé un prototype (Q-Connect - Figure 1) qui mémorise tous les évènements ("drag", "drop", "mouvement", etc) avec leurs caractéristiques spatiales (coordonnées sur l'écran) et temporelles (en millisecondes). Lors de l'expérimentation, l'ordre des items sur l'écran est aléatoire et les participants doivent lire préalablement le contenu de l'ensemble des items présenté sur l'écran. Ces précautions visent à limiter les biais potentiels liés au placement.
Les données
Les données expérimentales de notre recherche ont été recueillies dans le cadre d'une étude sur la perception de la réalité augmentée pour le grand public. Dans ce cadre, la réalité augmentée désigne une technologie permettant aux consommateurs d'intéragir avec le produit d'une manière personnalisée et adaptée au futur contexte d'utilisation. Le caractère encore récent de ces possibilités (ou affordances) conduit à investiguer comment les possibilités offertes sont perçues pour les utilisateurs finaux. À cette fin, un travail qualitatif a été tout d'abord engagé au moyen de focus groupes pour identifier quelles associations, idées, opinions, émotions étaient suscitées auprès des utilisateurs potentiels de cette technologie. Sur cette base, un échantillon d'affirmations a été extrait en cherchant à conserver la diversité des expressions, pour constituer la base d'un Q-sort qui a été utilisé dans le cadre d'une étude internationale comparant les FIG. 1 -Interface de Q-Connect pour l'étape 2 -Les items en bas pré-classés en trois classes dans l'étape 1 peuvent être sélectionnés pour être classés sur la distribution contrainte en haut de l'écran.
perceptions de la réalité augmentée dans quatre pays (Gauttier et al., 2016) puis dans une étude ultérieure centrée sur des cas individuels (Gauttier et Gauzente, 2018). Les Q-sorts associés à ces données ont été recueillis sur un petit échantillon composé de 13 participants. Il est évident que l'échantillon devra être agrandi à l'avenir mais les premiers ré-sultats obtenus permettent néanmoins de rendre compte de stratégies différentes et de préciser des aspects méthodologiques qui n'avaient été considérés jusque-là que très empiriquement.
Résultats
Les analyses s'organisent autour de trois directions principales : (i) l'exploration des processus de classement, (ii) les variations inter-individuelles, et (iii) l'intégration des nouvelles connaissances issues de la captation du processus dans l'analyse.
Processus de classement
La Figure 2 confirme que, dans chaque étape de classement, plus de la moitié des items ont été déplacés sur différentes classes avant la décision finale. Et la Figure 3 montre que certains items sont associés à des décisions plus stables que pour d'autres : par exemple, à l'étape 2, l'item 5 n'a été classé que dans les classes proches -3 à -2 alors que l'item 15 a parcouru l'ensemble des classes de l'échelle. De plus, lorsqu'on analyse la séquence des choix on observe que certains items ont été classés plus souvent en premier, alors que d'autres ne le sont jamais (Figure 4). En se focalisant sur ces derniers (les items 8 et 15 de la Figure 3) on voit apparaître deux situations distinctes : l'item 8 ("C'est pas intéressant pour voir des objets réels, mais pour visualiser comment certaines situations pourraient évoluer (notre apparence physique, un lieu, une maladie...") n'est jamais placé en premier mais une fois qu'il est placé dans une classe il ne bouge plus. À contrario l'item 15 ("Quand on prévisualise un produit chez soi par la RA, on n'a ni le conseil des vendeurs, ni le plaisir d'aller en magasin") est souvent déplacé avant de trouver sa place définitive. L'explication proposée par l'experte qui a piloté les focus groupes préalables à la mise en oeuvre de la Q-méthode met en évidence la portée différente des deux items. L'opinion 8 était consensuelle et semblait transcender la spécificité des situations. En revanche, l'opinion 15 nécessitait une phase réflexive plus approfondie sur le contexte : à quelle situation on a été confrontée ? quel vendeur on a rencontré ? etc L'analyse de la dynamique permet également de mieux évaluer le rôle des deux étapes dans la construction du Q-sort. Dans la méthodologie, cette mise en oeuvre en deux temps est en effet préconisée par de nombreux chercheurs pratiquant cette méthode (Watts et Stenner, 2012), car elle permet d'alléger la tâche de classement pour le répondant, en particulier dans le cas de nombreux items. Lorsque l'on étudie le passage de l'étape 1 à l'étape 2 ( Section 4.1), on voit que, hormis quelques rares « outliers » dont l'identification est aussi intéressante, le codage de l'étape 1 se transmet par "glissement" sur celui de l'étape 2. Il semble donc que l'attitude, 
Variabilité inter-individuelle
Différents travaux en psychométrie ont confirmé le lien entre les temps de réponse à des questionnaires et la solidité des attitudes individuelles (Johnson, 2004) TAB. 1 -Passage des classes de l'étape 1 à l'étape 2 (effectifs). 28 items classés en disagree ("plutôt pas d'accord") dans l'étape 1 sont placés lors du premier déplacement en -3 ("pas du tout d'accord") à l'étape 2.
la tâche effectuée et un très lent (individu 6) que l'on retrouve dans les deux étapes et qui a contrario semble correspondre à un participant très impliqué mais indécis dans ses décisions. Pour approfondir l'analyse du temps sur les deux étapes, nous avons observé les temps pour chaque déplacement dans l'ordre des déplacements ( Figure 6). Une régression par quantiles permet de caractériser l'évolution de la médiane des temps. Pour l'étape 1 la médiane est assez stable pour les premiers déplacements puis elle diminue légèrement. Il semble donc que les attitudes soient similaires pour les items avec un petit effet d'apprentissage sur la fin du classement en trois classes. En revanche, pour l'étape 2, après une petite stabilité au début, la décroissance est beaucoup plus marquée. Nous faisons l'hypothèse suivante : au tout début de l'étape 2 la découverte de la contrainte imposée pour le classement par la distribution normale nécessite un temps d'adaptation, mais ensuite les attitudes étant déjà en partie construites dans l'étape 1 le processus se déroule plus rapidement.
FIG. 5 -Distribution des temps totaux de déplacements des items pour chaque participant.
Intégration des connaissances dans l'analyse
L'analyse des traces associées à la dynamique du classement permet d'identifier des participants ayant des comportements atypiques, des items pour lesquels l'interprétation peut être FIG. 6 -Temps des déplacements d'items pour chaque participant dans l'ordre des 24 premiers déplacements (les autres ne concernant que les individus qui ont effectué des modifications). Chaque point correspond à un individu. En rouge la médiane (régression par quantile avec un polynôme de degré 2).
complexe et des comportements caractéristiques dans les deux étapes du processus. Se pose alors la question de l'intégration de ces nouvelles connaissances dans l'analyse quantitative de l'ensemble des données. La méthode Q étant basée sur une analyse en composantes principales -que nous effectuons ici sur les covariances -une première stratégie consiste à introduire des pondérations différentes sur les individus et les items. Nous avons testé deux pondérations (35% et 65%) sur les deux items (8 et 15) placés tardivement et les deux participants (5 et 6) aux temps extrêmes. Le Section 4.3 donne les résultats sur les premiers facteurs pour la pondération des items. Comme attendu la variance globale diminue, mais les pourcentages de la variance totale augmentent pour les premiers rangs, ce qui renforce la validité d'un modèle factoriel parcimonieux. La Section 4.3 donne les résultats sur les premiers facteurs pour la pondération des individus. L'effet de la pondération de l'individu 5 (très rapide) est très sensible mais, elle est négligeable pour l'individu 6 (très lent). Cela tend à montrer que 6 n'a pas de différence dans les réponses avec les autres participants mais simplement de la lenteur. Cela rejoint les travaux en psychométrie de (Fox et al., 2013) : l'hésitation n'est pas en soi un problème, simplement un effet de la personnalité ou d'attitudes moins accessibles, probablement en raison d'une moindre familiarité avec le sujet. En revanche, nous avons cherché à préciser l'impact de l'atypicité d'un individu dans l'analyse. Nous avons tout d'abord vérifié si la fluctuation de la part expliquée sur les premières composantes principales en sous-pondérant l'individu 5 était purement aléatoire ou atypique. La p-value pour le test sur la première composante vaut 0.047 ; ce qui semble confirmer l'hypothèse. Elle est cependant plus élevée sur les quatre premières (p = 0.0038) avec la présence d'un autre individu (10) qui dévie aussi un peu des comportements « moyens ». Pour confirmer le caractère atypique de l'individu 5, nous avons effectué un test à plus large spectre en comparant l'impact sur les premières valeurs propres de la pondération sur les douze paires contenant l'individu 5 avec les 66 autres paires possibles ne le contenant pas. Pour les quatre premières composantes, un test non paramétrique de la médiane permet de confirmer l'hypothèse. TAB. 3 -Effet de la pondération des individus sur l'ACP.
Conclusion et perspectives
Le travail présenté dans cette communication est une première étude exploratoire de l'analyse de la dynamique des processus de classement. L'objectif in fine est de prendre en compte les informations extraites d'une telle analyse dans l'exploitation des classements finaux. Cette information a été jusque là ignorée dans la très grande majorité des cas. Et, malgré les limitations de notre cadre expérimental, nous montrons l'intérêt d'identifier les variations de comportements et les stratégies mises en oeuvre dans les prises de décision. Des travaux complémentaires sont déjà programmés pour confirmer et approfondir ces premiers résultats sur une autre enquête. Nous nous sommes centrés ici sur une méthodologie, la Q-méthode, qui est tradidtionnellement appliquée à des échantillons de taille réduite. Cependant, nous n'avons guère trouvé non plus dans la littérature relative à la fouille d'opinions de travaux prenant en compte les processus individuels de construction des opinions collectées sur les média sociaux. Mais, au-delà des questions soulevées par l'identification des caractéristiques de ces processus et leur prise en compte effective dans les analyses, d'un point de vue méthodologique, il ne s'agit pas ici d'opposer les échelles d'observation mais bien au contraire de les associer à l'avenir pour comprendre plus finement les déterminants des phénomènes sociaux étudiés. En s'inspirant de la biologie intégrative qui commence à intégrer avec efficacité dans les études un ensemble de données "omics" (genomics, transcriptomics, proteomics, metabolomics) permettant de combiner les informations extraites à différentes échelles du vivant, il nous semble que la voie est ouverte aujourd'hui pour une "science humaine intégrative" (Kuntz, 2016). L'hétérogénéité

Introduction
Anticiper la réaction des utilisateurs vis-à-vis d'une information dans les médias sociaux peut permettre de prévoir l'ampleur et l'évolution du phénomène de diffusion. Ainsi, grâce à ces prévisions, il serait possible de cibler de façon pertinente des utilisateurs pour améliorer l'image d'une marque d'un point de vue marketing, ou pour atténuer la propagation de rumeurs ou d'une "infox 1 ". Cependant, la majorité des travaux de modélisation de la diffusion dans les médias sociaux reposent en majorité sur des hypothèses épidémiologiques [Hethcote (1989); Anderson et May (1992)] et non sur l'observation de données réelles pour décrire les caractéristiques de la diffusion. Dans des approches relativement récentes, des chercheurs [Li et al. (2018); Zhou et al. (2017); Hoang et al. (2016)] ont introduit des modélisations du phé-nomène de diffusion au niveau microscopique, c'est-à-dire au niveau des utilisateurs, prenant en compte des variables de diffusion observées sur des cas réels de diffusion. Notre approche innove dans le sens où elle se place au niveau des populations des pays et qu'elle consiste à identifier en plus du nombre (N ) de populations atteintes, le rayon géographique (R) d'influence autour de ces populations, l'instant de diffusion de l'information (T ), la durée de la diffusion (I) et le pays auquel appartiennent ces populations. Le modèle WorldSpread que nous proposons permet de décrire le processus de diffusion en fonction de ces variables et d'identifier les populations qui sont atteintes par une information en connaissant la population à l'origine de l'information et sa thématique. Nous apportons ainsi avec le modèle WorldSpread une contribution à la modélisation du processus de diffusion au niveau macroscopique en prenant en compte la dimension géographique des diffuseurs. Le reste du papier est organisé comme suit. La section 2 présente la méthodologie de collecte et d'extraction des données. La section 3 détaille le modèle WorldSpread proposé. La section 4 est consacrée à la présentation des résultats obtenus. Enfin la section 5 conclut et présente nos travaux futurs.
Collecte et extraction de données
Dans notre approche, nous avons utilisé les sujets tendance de Twitter. Un sujet tendance est une information très diffusée dans le média social à un instant précis, souvent représenté par un hashtag (mot ou ensemble de mots précédés du caractère dièse résumant le sujet). La méthodologie que nous proposons s'effectue en trois grandes étapes. Dans un premier temps, nous avons collecté toutes les cinq minutes, la liste des 50 meilleurs sujets tendance pour les 62 pays disponibles dans l'API de Twitter. Nous avons sélectionné les sujets diffusés par au moins 4 populations de pays dans un intervalle de 3 jours suivant la première diffusion. Dans un second temps, nous avons ordonné chronologiquement l'adoption des sujets pour chaque population de pays. Ensuite, nous avons classé les sujets automatiquement en fonction du pays C où le sujet apparaît en premier. De plus, nous avons classé les sujets manuellement selon les neuf thématiques S suivantes : les célébrités, les jeux, les films/TV, la musique, les nouvelles, la politique, le sport, la technologie et les autres. Dans un troisième temps, nous avons extrait 3 paramètres pour chaque population ayant adopté le sujet : le temps d'adoption, le temps d'infection et le rayon d'impact. Le temps d'adoption T est le temps écoulé entre l'apparition du sujet dans le pays d'origine et l'apparition de ce sujet dans un autre pays. Le temps d'infection I est le temps écoulé entre la première apparition du sujet dans la liste des tendances du pays et le moment où il disparaît de la liste. Le rayon d'impact R est la distance géographique entre la capitale du pays à l'origine du sujet et la capitale d'un pays où le sujet apparaît ensuite. Enfin, nous avons utilisé l'outil d'extraction de motifs fréquents SPMF (Fournier-Viger et al. (2016)) afin de découvrir les populations de pays souvent impactées ensemble. Dans notre approche, nous avons utilisé deux jeux de données. Un jeu de données d'observation (n o 1) collecté en juin 2017 est composé d'environ 2900 sujets. Un jeu de données d'évaluation (n o 2 ) collecté en octobre 2017 contient environ 3000 sujets.
Le modèle WorldSpread
Formulation du problème
Dans la suite de ce papier, nous appelons sujet, une information relative à une thématique, qui est susceptible de se propager. Par exemple le sujet #AgentsofSHIELD correspond à une série télévisée et sa thématique est "films/TV". Étant donné C le pays d'origine du sujet, S la thématique du sujet et t = [t 0 , t 1 , ..., t m ] l'ensemble des instants d'apparition des sujets dans de nouveaux pays relatifs à C et S, où t 0 est l'instant d'apparition dans le pays source. Notre objectif est ici double. Premièrement, il s'agit de définir le nombre de pays diffusant le sujet aux différents instants t j , c'est-à-dire une liste n = [n 0 , n 1 , ..., n m ] où n j est le nombre de nouveaux pays diffuseurs au temps t j . Deuxièmement, il s'agit d'identifier également les noms des pays diffusant le sujet aux différents instants t j , c'est-à-dire une liste Lc = [Lc 1 , ..., Lc m ] contenant, pour chaque instant, les pays qui diffusent l'information.
Description du modèle WorldSpread
Le modèle que nous proposons repose sur la formalisation de nos observations sur des données réelles. Ainsi, à un pays source C et une thématique S sont associés N total le nombre total de pays ayant diffusé la thématique ainsi que les 4 listes suivantes :
-T = [T 1 , T 2 , ..., T k ] la liste des instants de la diffusion où un ou plusieurs nouveaux pays ont adopté la thématique S, -N = [N 1 , N 2 , ..., N k ] la liste des nombres de pays ayant adopté la thématique S à chaque instant T j , -I = [I 1 , I 2 , ..., I k ] la liste des temps d'infection relatifs aux pays ayant adopté la thé-matique S à chaque instant T j , -R = [R 1 , R 2 , ..., R k ] la liste des rayons d'adoption (distance géographique par rapport au pays C) relatifs aux pays ayant adopté la thématique S à chaque instant T j . En plus de ces variables, nous générons également le réseau du pays source du sujet qui repose sur un graphe dirigé G S,C , où S est la thématique du sujet. La construction de ce graphe s'effectue en deux étapes. Premièrement, pour chaque pays C, nous générons en fonction de la thématique S le graphe local dirigé G S,C = (C S , E S ) où C S = {C, C 1 S , C 2 S , ..., C n S } est l'ensemble des noeuds représentant les pays qui apparaissent fréquemment dans la diffusion provenant de C pour la thématique S et E S ={E 0 S , E 1 S , ..., E n−1 S } est l'ensemble des liens dirigés de C aux noeuds de C S . Il existe un lien dirigé d'un noeud C 1 vers un noeud C 2 si le sujet se diffuse de C 1 à C 2 directement. Deuxièmement, nous construisons pour chaque pays C selon la thématique S un graphe global dirigé G S,C = (C S , E S ) comme l'union des G S,Cn pour chaque C n de C S . L'ensemble des noeuds C S est l'union de l'ensemble des noeuds de C S avec l'ensemble des noeuds des graphes locaux des noeuds des G S,C k S pour tout C k S de C S et l'ensemble des arêtes dirigées E S est l'union de l'ensemble des arêtes des
L'objectif du modèle est de décrire l'évolution de la diffusion d'un sujet de manière à pouvoir identifier : le moment où une population est susceptible de diffuser le sujet, le laps de temps pendant lequel elle va diffuser le sujet, la distance géographique qui sépare la population de celle à la source du sujet, et le pays auquel appartient la population.
Evaluation du modèle 4.1 Evalutation quantitative
Afin d'évaluer l'"erreur" ou la distance entre les valeurs obtenues sur le jeu d'observation et le jeu d'évaluation, nous nous sommes focalisés sur l'évolution des nombres de pays diffusant l'information au cours du temps et nous avons observé les différences relatives entre les courbes de diffusion. En considérant le cas (États-Unis,Sport), les valeurs obtenues sont présentées dans la figure 1. 
Ainsi, plus la valeur est proche de 0, et plus le modèle décrit fidèlement le phénomène. Nous avons observé la distribution de l'erreur pour les 140 couples (C,S) dont nous disposons d'au moins un exemple à la fois dans le jeu de données 1 et dans le jeu de données 2 (voir figure 2). Ainsi, nous avons remarqué que nous obtenons une erreur strictement inférieure à 0.6 pour 43% de ces couples.
L'erreur semble plus importante lorsque le nombre d'exemples de diffusion est faible. En effet, en observant le nombre moyen d'exemples en fonction de l'erreur, nous avons constaté que moins il y a d'exemples, plus l'erreur est élevée (voir figure 3).
Evalutation qualitative
Décrire de manière numérique la diffusion d'un sujet en connaissant le pays source C et sa thématique S peut s'avérer utile pour estimer la vitesse de propagation à d'autres populations. Néanmoins, cette première description ne permet pas de cibler précisément les populations susceptibles de diffuser une information. Ainsi, dans un second temps, nous nous sommes intéressés à l'identification des pays diffusant un sujet au cours du temps toujours en prenant en compte le pays source C et la thématique S. L'objectif de notre approche est double. Premiè-rement, il s'agit de proposer un ensemble de pays P = {C 1 , C 2 , ...C k } allant probablement diffuser le sujet dont la thématique est S et le pays source est C. Deuxièmement, il est question de suggérer un ordre chronologique O dans l'adoption du sujet par les pays de cet ensemble. Pour chaque exemple de diffusion du jeu de donnée 2, nous avons calculé le rappel A P et la précision B P :
-A P = nombre de pays de P correctement identifiés nombre de pays de P * 100 -B P = nombre de pays de P correctement identifiés nombre de pays total de pays diffusant * 100
De plus nous avons mesuré le taux de bonnes chronologies en utilisant : H = nombre de chronologie correctes identifiées nombre d'exemples de diffusion * 100
Pour estimer la qualité de l'identification des pays du modèle, nous avons calculé A P , B P pour chaque exemple de diffusion relatif au top 10 des couples (C,S) contenant le plus d'exemples de diffusion. Pour un couple (C,S) donné, nous avons calculé H ainsi que µ A , µ B respectivement la moyenne des A P , B P . Ensuite, nous avons calculé M H , M A et M B respectivement les moyennes des H, µ A et µ B pour les 10 couples (C,S). En évaluant le modèle sur le jeu de donnée 2, nous avons noté que M A = 69.7%, M B = 66.04% et M H = 64, 5% . Ainsi, en moyenne 2 3 de l'ensemble des pays diffusants sont identifiés correctement par WorldSpread. De plus, en moyenne dans 2 3 des cas l'ordre chronologique proposé est correctement identifié.
Conclusion et perspectives
Dans ce travail, nous avons introduit WorldSpread, un modèle original et innovant qui permet de décrire simplement la diffusion entre des populations sur le média social Twitter à la fois sur le plan quantitatif et sur le plan qualitatif, en connaissant le pays source C et la thé-matique S de l'information. Notre approche repose sur l'utilisation de deux jeux de données réelles collectées à trois mois d'intervalle et contenant chacun près de 3000 exemples de diffusion regroupés manuellement dans 9 thématiques. Nous avons constaté que le modèle permet d'établir une description assez précise du phénomène de diffusion notamment pour les couples (C,S) comptant le nombre d'exemples le plus important. Une perspective intéressante serait d'intégrer en plus dans le modèle WorldSpread le nombre d'utilisateurs diffusant l'information au sein des populations concernées. Décrire le processus en prenant en compte cette variable supplémentaire pourrait permettre d'estimer le nombre d'utilisateurs impactés selon le pays en prenant en compte la population source et la thématique de l'information. D'un point de vue marketing, la stratégie à adopter serait de répandre une information au sein d'une population réceptive afin de maximiser sa diffusion. Du point de vue d'un modérateur, un tel modèle pourrait permettre de cibler les populations à informer afin d'atténuer la propagation de rumeurs ou de fausses informations. 
Summary
Dissemination models proposed in social media are based for the most part on epidemiological hypotheses and not on the observation of real data to describe the characteristics of the diffusion. Such models can not faithfully reproduce the phenomenon of diffusion because they do not consider the observed factors that may influence this phenomenon. Our approach is innovative because we consider the populations of the countries and that it consists in identifying in addition to the number of populations reached, the geographical radius of influence around these populations, the moment of diffusion of the information, the duration of the diffusion and the country to which these populations belong knowing the population at the origin of the information and its theme.

Introduction
Le calcul de règles d'association (Agrawal et al. (1993)) est un problème important en fouille de données qui a donné lieu à une littérature foisonnante. Dès sa naissance, et pour pallier au grand nombre de motifs produits, l'accent a été mis sur la recherche d'ensembles réduits de règles contenant une information jugée intéressante. Comme souvent lorsque deux critères sont à optimiser -ici le nombre de règles et l'information contenue -, l'un d'eux prend le pas. Ainsi, dans le domaine des règles d'association, le nombre de règles est souvent vu comme primordial.
Le problème de la représentation de la totalité des règles -et donc de l'ensemble de l'information -est celui qui nous intéresse ici. Dans le cas de données binaires bidimensionnelles, le premier à être considéré, la question n'est plus ouverte. Nous savons que les règles peuvent être représentées de façon minimale par les ensembles fermés. Ce résultat, basé sur le fait que les fermés sont des représentants uniques de leurs classes d'équivalence vis à vis du support, a donné lieu à de nombreuses combinaisons avec les mesures d'intérêt utilisées pour réduire le nombre de règles au détriment de l'information.
Dans ce papier, nous considérons le cas des données binaires multidimensionnelles. Bien qu'il ait été moins étudié que le cas bidimensionnel, des moyens de réduire le nombre de règles ont déjà été proposées en généralisant la mesure d'intérêt la plus connue : la fréquence. Cependant, à notre connaissance, aucun résultat n'existe sur des représentations condensées de l'entièreté des règles. Nous nous proposons ici d'y remédier en montrant que, tout comme dans le cas bidimensionnel, les ensembles n-fermés d'une transformation d'un tenseur booléen multidimensionnel sont suffisants pour dériver le support de toutes les associations et donc la confiance de toutes les règles.
Dans la Section 2, nous rappelons les définitions et propriétés connues et utiles des tenseurs, ensembles fermés et règles dans les tenseurs bi-et multidimensionnels. Dans la Section 3, nous montrons que le support naturel de toute association peut être calculé à partir du support d'associations particulières impliquant n − 1 dimensions. Dans la Section 4, nous présentons une transformation du tensor permettant la dérivation du support d'associations par rapport à des ensembles de dimensions. Enfin, dans la Section 5, nous utilisons les résultats produits pour conclure que les n-ensembles fermés contiennent une information nécessaire et suffisante.
Définitions
Matrices, tenseurs et fermetures
Di∈D D i une relation n-aire entre les éléments des dimensions. Ensemble, D et R forment le tenseur booléen T = (D, R), une matrice binaire n-dimensionnelle représentant des données. Ce tenseur est aussi appelé contexte n-dimensionnel ou n-contexte dans le domaine de l'analyse formelle de concepts. Le tenseur illustré dans la Figure 1 servira d'exemple tout au long de ce papier.
Dans T , un ensemble n-fermé est un tuple (X 1 , . . . , X n ) avec X i ⊆ D i tel que i∈{1,...,n}
et chaque composant est maximal pour cette propriété. Ainsi, dans le cas classique bidimensionnel, les 2-ensembles fermés sont les paires (A, B) dans lesquelles A et B sont maximaux tels que ∀a ∈ A, ∀b ∈ B, (a, b) ∈ R. Dans ce cas, à la fois A et B sont dits fermés. Lorsque X est un sous-ensemble de l'une des deux dimensions, nous utiliserons c(X) pour noter le plus petit (pour l'inclusion) ensemble fermé contenant X.
Dans notre exemple, nous trouvons les 3-ensembles fermés suivants :
L'ensemble des 2-ensembles fermés, ordonnés par la relation d'inclusion sur l'un de leurs deux composants, forme un treillis complet (Ganter et al. (1997)). De la même façon, l'ensemble des n-ensembles fermés, ordonnés par la relation d'inclusion sur l'un de leurs n-composants, forme un n-treillis complet (Voutsadakis (2002)). Les n-ensembles fermés peuvent être calculés grâce à DATA-PEELER (Cerf et al. (2008)).  
Règles d'association dans le cas bidimensionnel
En l'absence de restrictions, il y a 2 2|D2| règles possibles. Il est donc nécessaire de n'en considérer qu'une partie ; idéalement la plus intéressante. Pour ce faire, un certain nombre de mesures d'intérêt ont été proposées (Zhang et al. (2009)). La première est la fréquence. Une règle A → B est dite fréquente, par rapport à un seuil t ∈ [0, 1], si et seulement si |s(A∪B)| ≥ t × |D 1 |. Ne s'intéresser qu'aux règles fréquentes permet ainsi de réduire significativement le nombre de motifs. Le seuil de fréquence peut être combiné à un seuil de confiance pour réduire encore le nombre de règles.
Cependant, le nombre d'ensembles fréquents peut se révéler toujours trop élevé. Pour y remédier, d'autres méthodes ont été proposées. Les propriétés suivantes font maintenant partie du folklore :
La première indique que les règles importantes sont celles dont la conclusion contient la prémisse tandis que la seconde signifie qu'il suffit de ne considérer que les règles entre ensembles fermés.
Dans l'exemple de la Figure 2, la fermeture de {p 2 } est {p 2 } et celle de {p 2 , p 5 } est {p 2 , p 4 , p 5 }. De ce fait, les confiances de p 2 → p 2 p 5 et p 2 → p 2 p 4 p 5 sont toutes deux égales à 1/3. Cependant, construire des bases de règles d'association en utilisant toutes les règles entre ensembles fermés comparables n'est toujours pas suffisamment efficace puisqu'il peut y avoir jusqu'à 2 |D2| ensembles fermés. Afin de réduire encore plus le nombre de règles, il a été montré qu'il était suffisant de ne considérer que les règles de la forme A → B telles que A = c(A), B = c(B), A ⊂ B et il n'y a aucun fermé X entre A et B. Cela correspond à une règle par arête dans le diagramme de Hasse du treillis des 2-ensembles fermés. La confiance de n'importe quelle règle peut alors être calculée en trouvant un chemin entre sa prémisse et sa conclusion dans le diagramme et en multipliant les confiances des règles parcourues. Sur le diagramme de Hasse illustré dans la Figure3, nous constatons que la confiance de ∅ → p 4 p 5 peut être obtenue en multipliant les confiances de ∅ → p 4 et p 4 → p 4 p 5 , ce qui nous donne 3 5 × 2 3 = 0.4. Luxenburger (Luxenburger (1991)) a montré que des ensembles de règles plus petit peuvent être obtenu en considérant uniquement un arbre couvrant du diagramme de Hasse. Cependant, l'utilisation de ces règles pour dériver des confiances implique de résoudre des problèmes d'optimisation linéaire, ce qui se révèle être trop chronophage pour la plupart des applications.
Puisque, par définition, les ensembles ont le même support que leur fermeture, les ensembles fréquents ont des fermetures fréquentes. Les deux approches pour réduire le nombre de règles peuvent donc être combinée en ne calculant que les règles entre ensembles fermés fréquents voisins (Lakhal et Stumme (2005)).
Règles d'association dans le cas multidimensionnel
Différentes généralisations des règles d'association dans les relations n-aires ont été étu-diées. Dans Nguyen et al. (2011), les auteurs proposent ce qui est, pour nous, la plus générale. Nous la présentons ici et l'utilisons dans le reste de ce travail.
une relation n-aire entre les dimensions. Nous voulons extraire des "règles d'association" du tenseur (D, R). Cependant, contrairement au cas bidimensionnel, les motifs composant les règles peuvent impliquer différentes dimensions.
Soit D ⊆ D un ensemble de dimensions. Sans perte de généralité, nous supposons que 
Dans le cas bidimensionnel, le support d'une association sur une dimension est un sousensemble de l'autre dimension. De la même façon, dans le cas multidimensionnel, le support d'une association est calculé sur les dimensions qui ne sont pas dans son domaine. Soit X une association, le support de X, noté s(X), est l'ensemble {t ∈ Di∈dom(X) D i | ∀x ∈ X, x.t ∈ R} des tuples dans le produit cartésien des dimensions absentes du domaine de X qui forment un élément de R avec un élément de X.
Dans notre exemple, nous avons que s(p
Dans notre exemple, p 1 → p 3 .m 1 est une règle d'association sur le domaine {D produits , D magasins }.
Soit X → Y une règle sur dom(X Y ). Si dom(X) est différent de dom(X Y ), les supports s(X) et s(X Y ) sont définis sur des ensembles différents et ne peuvent donc pas être comparés pour calculer la confiance de la règle. Le support de la prémisse doit donc être défini différemment. Le support de X par rapport à un domaine D ⊇ dom(X) est défini par
Grâce à ce support, nous pouvons définir la confiance naturelle de X → Y sur le domaine
Ces règles d'association multidimensionnelles conservent la propriété que
Le nombre de ces règles est, évidemment, encore plus élevé que dans le cas bidimensionnel. Dans Nguyen et al. (2011), les auteurs utilisent la fréquence et la confiance pour le réduire. Il paraîtrait donc naturel d'imiter le cas bidimensionnel et de représenter aussi l'ensemble des règles d'associations n-dimensionnelles avec des n-ensembles fermés. Cependant, peu de résultats existent sur le sujet.
Transformations de tenseurs
Cette section présente les définitions de transformations de tenseurs que nous utilisons dans nos preuves.
Le tenseur peut être transformé en : -"fixant" des éléments d'une dimension -combinant des dimensions La première opération consiste en la restriction du tenseur à un sous-ensemble de l'une de ses dimensions. Soient  Figure 5 illustre cette transformation.
FIG. 5 -Transformations T ({D clients },{D produits ,Dmagasins}) du tenseur T présenté dans la Figure 1.
Dériver le support d'associations
Nous cherchons à identifier un ensemble restreint de règles d'association multidimensionnelles suffisantes pour dériver la confiance de toutes les autres. Pour ce faire, nous commencerons par montrer que la taille du support de n'importe quelle association intéressante peut être dérivée de la taille des supports de n-ensembles fermés. Nous supposons uniquement qu'une des dimensions n'apparait dans le domaine d'aucune association intéressante. Nous estimons cette supposition raisonnable car, en pratique, une dimension contient habituellement les "objets" ou "transactions" et ses éléments n'apparaissent pas dans les règles. Sans perte de généralité, nous supposerons que cette dimension est D 1 .
Tel que mis en évidence dans la Figure 6, le tenseur n-dimensionnel T peut être vu comme un empilement de tenseurs (n − 1)-dimensionnels. De ce fait, la taille du support d'une association X est la somme des tailles de ses supports dans les différentes couches composant le tenseur.
. . .
6 -Les tenseurs n-dimensionnels sont un empilement de tenseurs (n − 1)-dimensionnels. Supposons que nous voulons connaître la taille des supports de p 1 et p 1 p 3 dans notre exemple T et que D clients est la dimension n'apparaissant pas dans les règles. Le produit cartésien de la seule dimension support qui n'est pas D clients est {m 1 , m 2 , m 3 }. Les supports de p 1 dans T {m1} , T {m2} et T {m3} sont, respectivement, de taille 1, 1 et 2 donc |s(p 1 )| = 4 dans T . Les tailles des supports de p 1 p 3 dans les mêmes tenseurs sont 1, 1 et 1 donc |s(p 1 p 3 )| = 3 dans T . Ainsi, p 1 → p 1 p 3 a une confiance de 3 4 .
Proposition 2. Soient X une association dans T et Z un élément du produit cartésien de dimensions supports de X. Le support de X dans T Z est le support de X Z dans T .
Preuve. ⇒. Soit S le support de X dans T Z . Par définition, ∀s ∈ S, ∀x ∈ X, s.x ∈ R Z . Puisque T Z est construit à partir de T en intersectant les couches correspondantes à Z, nous avons que s.x ∈ R Z implique que s.Z.x ∈ R. Par conséquent, S est un sous-ensemble du support de X Z dans T . ⇐. Soit S le support de X Z dans T . Par définition, ∀s ∈ S, ∀x ∈ X, s.Z.x ∈ R. De la construction de T Z nous obtenons que s.Z.x ∈ R implique que s.x ∈ R Z . Par conséquent, S est un sous-ensemble du support de X dans T Z et donc S = S .
Dans notre exemple, le support de p 1 .m 3 dans T est {c 1 , c 2 }. Le support de m 3 dans T p1 est aussi {c 1 , c 2 }.
La Proposition 2 implique que les supports des associations -et donc la confiance des règles -dans les tenseurs mentionnés dans la Proposition 1 peuvent être dérivés des supports des associations sur le domaine D 1 dans T . Des Propositions 1 et 2, nous pouvons déduire que le support de n'importe quelle association dans T peut être dérivé des supports des associations sur le domaine D 1 .
Règles entre associations sur différents domaines
Dans la Section 3, nous avons montré que la taille du support de n'importe quelle association peut être dérivée des tailles des supports d'associations sur D 1 . Ce résultat est suffisant lorsque nous voulons calculer la confiance d'une règle entre deux associations sur le même domaine. Cependant, les règles de la forme X → Y avec dom(X) ⊂ dom(X Y ) requièrent la connaissance du support s dom(XY ) (X) de X par rapport à dom(X Y ). Dans cette section, nous montrons que le tenseur peut être transformé pour unifier les domaines des prémisses et conclusions de façon à ce que le support de n'importe quelle association par rapport à n'importe quel domaine puisse être dérivé d'associations sur D 1 .
Comme nous l'avons présenté dans la Section 2.3, le support d'une association X par rapport à un domaine D est l'union des supports de toutes les associations pouvant être construites en augmentant de façon minimale X pour que D soit son domaine. Ce support peut être vu comme le support d'une association augmentée de façon à ce que chaque dimension additionnelle contienne un élément qui représente une disjonction sur l'entièreté de la dimension.
En d'autres termes, le tenseur T ↑ est construit à partir de T en ajoutant un élément à chaque dimension (à l'exception de D 1 ) et en projetant les croix sur ces éléments jusqu'à saturation tel qu'illustré dans la Figure 7.
Preuve. Soit t un élément du support de X D↑ in T ↑ . De la construction de T ↑ , nous déduisons que,
En suivant ce raisonnement récursivement sur les différentes dimensions dans D, nous obtenons que, pour tout x ∈ X, il existe un tuple a ∈ Di∈D\dom(X) D i tel que t.a.x ∈ R. Ainsi, par définition, t ∈ s D (X) dans T . Par conséquent, s(X D↑ ) dans T ↑ est un sousensemble de s D (X) dans T .
Soit t un élément de s D (X) dans T . Si t n'est pas dans le support de X D↑ , alors cela signifie qu'il n'y a aucun tuple a ∈ Di∈D\dom(X) D i tel que t .a.x ∈ R. Cela contredit notre supposition initiale. Par conséquent, s D (X) dans T est un sous-ensemble de s(X D↑ ) dans T ↑ et ils sont donc égaux.
La Proposition 5 implique que le support de X par rapport à un domaine D dans T est le même que le support de X D↑ dans T ↑ . A partir de cela et des Propositions 1 et 2, nous pouvons déduire que la taille du support de n'importe quelle association X par rapport à n'importe quel domaine dans T peut être dérivé des tailles des supports d'associations sur D 1 dans T ↑ .
Dans notre example T décrit dans la Figure 1 
Fermés et supports
Dans les Sections 3 et 4, nous avons montré que le support de n'importe quelle association par rapport à n'importe quel domaine peut être dérivé des supports des associations sur D 1 dans le tenseur T ↑ . Il ne nous reste plus qu'à montrer que la connaissance de l'ensemble des n-ensembles fermés est suffisante pour retrouver ces supports.
Conclusion
Nous avons montré que le support des associations et donc la confiance des règles d'association dans un tenseur booléen multidimensionnel peuvent être dérivés de la connaissance des n-ensembles fermés d'une transformation du tenseur. Cela généralise les résultats connus dans le cas bidimensionnel puisque, dans ce contexte, la transformation ne modifie pas les 2-ensembles fermés.
Cependant, le fait que les supports des associations fréquentes puissent être dérivés des fermés fréquents, dans le cas bidimensionnel, ne se généralise pas au cas multidimensionnel. Les nombres d'associations fréquentes et de n-ensembles fermés mériteraient donc d'être à l'avenir comparés.

Introduction
Un des leviers de la création de services innovants est l'agrégation et l'analyse de données hétérogènes provenant de sources internes ou externes à l'entreprise (De Filippi, 2013). Cependant, l'ouverture de grandes masses de jeux de données entre organisations soulèvent de nombreux problèmes :
-Les données libérées découlent de connaissances métiers qui ne sont pas nécessai-rement possédées par leurs consommateurs. Cela complexifie la compréhension des données (étape critique, notamment pour la création de modèles par les data scientists). -Le grand volume de données complexifie l'identification de jeux de données pertinents pour un besoin donné (une tâche à laquelle s'attaque Google Goods (Halevy et al., 2016)). -De nombreux cas d'utilisations nécessitent de casser les barrières entre des silos de données. Cette tâche est rendue complexe par de multiples formes d'hétérogénéité des données (Pluempitiwiriyawej et Hammer, 2000). -L'ouverture des données à des tiers constitue une évolution importante des usages. Les entreprises sont encore réticentes quant à ce partage par crainte des transgressions des consommateurs de la donnée (p. ex. l'appropriation d'un jeu de données sans accord préalable). Symétriquement, le consommateur attend des garanties quant à la qualité, la crédibilité et la traçabilité des données. Ce papier présente Dataforum, un écosystème ouvert et auditable offrant des solutions aux challenges soulevés précédemment. Ce système, développé par Orange Labs, s'appuie sur les technologies du Web sémantique, du traitement automatique du texte et de l'apprentissage automatique.  (Brümmer et al., 2014) pour spécifier la provenance des données et CCREL (Abelson et al., 2008)  
Modèle de métadonnées
Sémantisation des jeux de données
FIG. 2 -Chaîne de sémantisation pour la caractérisation automatique de jeux de données ("Structure identification" et "Spatial coverage identification" sont des travaux en cours)
Pour référencer un grand nombre de jeux de données et proposer des dispositifs de recommandation, il est nécessaire de disposer d'outils de caractérisation automatique adaptés à la très grande hétérogénéité des données en termes de contenu et de contenant. La plateforme propose une chaîne d'algorithmes (voir figure 2), activable à la demande par le producteur de données, générant automatiquement des métadonnées pour décrire le contenu d'un ensemble de données (langues, thèmes, mots clés, couverture géographique) et ses contenants (formats et taille des fichiers). Les traitements sémantiques se basent sur des techniques à base de trigrammes pour l'identification de la langue et à base de modèles de Markov associés à des traitements linguistiques et des techniques de recherche d'informations pour les autres méta-données sémantiques. La chaine permet de traiter sept langues : anglais, français, néerlandais, espagnol, slovaque, roumain et allemand.
Recherche et recommandation
Le moteur de recherche de Dataforum permet à des non-initiés de sélectionner des critères de recherche (mots-clefs, thème, licence, langage, etc.) afin de générer automatiquement des requêtes SPARQL. L'utilisateur obtient les résultats de sa recherche sous forme de "cartes" synthétiques et peut obtenir de plus amples informations via une vue graphe (présentée dans la figure 1). La richesse du modèle sem4DS permet également de fournir des suggestions aux utilisateurs en fonction des jeux de données récemment consultés. Dataforum utilise pour cela Reperio, un moteur de recommandation développé à Orange Labs (Meyer, 2012). Dans la version actuelle de la plateforme, les mots-clefs et les thèmes sont utilisés pour mesurer les similarités entre jeux de données. Grâce à une visualisation du graphe de similarité, l'utilisateur peut naviguer de manière intuitive dans les jeux de données de proche en proche (noeuds = jeux de données, arcs = relations de similarité).
Perspectives
Le système présenté est un prototype de recherche en cours de déploiement pour répondre aux besoins des propriétaires de données et des data scientists de l'entreprise Orange.

Introduction
Les agents conversationnels se sont démocratisés au travers des smartphones (Google Assistant, Siri) et des assistants vocaux (Google Home, Amazon Echo...). Les utilisateurs peuvent installer des applications tierces qui interagissent avec ces périphériques via des briques logicielles propriétaires mises à la disposition des développeurs. Notre cas d'utilisation est la recherche de professionnels au travers des assistants vocaux (par exemple, un plombier ou un restaurant) et entre dans la famille des systèmes de dialogues orientés tâches. Ces systèmes sont conçus pour remplir une tâche particulière, en interagissant avec un utilisateur et n'ont pas pour but de participer à des conversations non structurés sans rapport avec les tâches à accomplir. Dans ce papier, nous décrivons les méthodes et algorithmes utilisées par notre système de dialogue orienté tâches et comment les intégrer au sein de l'éco-système technique associé aux assistants vocaux pour ajouter de l'intelligence aux application tierces. En effet, même si des algorithmes de traitement du langage naturel sont utilisés au sein des assistants vocaux, les applications tierces sont contraintes de suivre un graphe de dialogue prédéfini pour associer les requêtes utilisateurs aux fonctionnalités des applications. Notre approche tire partie d'une fonctionnalité particulière des assistants vocaux pour transférer le contrôle de la conversation de l'assistant vocal vers notre système de dialogue personnalisé afin de nous permettre d'entrainer notre modèle de conversation dynamiquement au contact des utilisateurs.
État de l'art
Les applications tierces sont crées à partir de bibliothèques propriétaires. Certaines d'entreelles permettent de concevoir des applications portables sur plusieurs plateformes. Pour la plupart, ces bibliothèques partagent les concepts d'intentions et d'entités.
Intentions : la phrase parlée de l'utilisateur n'est pas toujours disponible au niveau application. À la place, le périphérique analyse l'entrée audio et renvoie une intention à l'application. Une intention est une correspondance entre une requête utilisateur et un ensemble d'actions utilisateurs définies par l'application. Par exemple, la phrase "Je cherche un restaurant" peut être associée à l'intention "recherche_restaurant". Sur Amazon Echo, ni l'audio, ni sa transcription ne sont transmis à l'utilisateur, uniquement l'intention détectée.
Entités : ce sont les valeurs des paramètres associés à chaque intention et sont extraites des requêtes utilisateurs. Par exemple, dans "je cherche un restaurant à Londres", le mot "Londres" pourrait être extrait comme une entité "localisation".
Les applications tierces sont supposées être dirigées par un graphe de dialogue statique contrôlé par les différentes intentions et entités détectées durant le dialogue. Le système doit connaitre les différentes transitions entre l'intégralité des état de dialogue possible.
L'apprentissage par renforcement a été étudié pour l'entrainement de systèmes de dialogue. Dans RLDS (Singh et al., 1999), le dialogue est considéré comme un processus de décision Markovien et les états du dialogue sont estimés grâce à un oracle. Les travaux récents visent à entrainer les systèmes de dialogue par l'apprentissage par renforcement combiné à des réseaux de neurones profonds . Cependant, ces algorithmes néces-sitent de grosses quantités de données et ne peuvent pas être déployé en production sans un entrainement préalable. Pour résoudre ce problème, une approche utilisant un magicien d'Oz (Rojas-Barahona et al., 2017) a été proposée. Celle-ci est cependant difficile à utiliser en pratique car un opérateur humain doit répondre à la place de l'agent pendant une longue période de temps. Une seconde approche utilise des bots utilisateurs (Li et al., 2017, codés en dur, ayant chacun un comportement reflétant un cas d'utilisation. Cette approche permet d'entrainer l'agent aux contacts des bots, remplaçant le besoin d'utilisateurs. Ces deux méthodes ne sont pas utilisables dans notre cas car elles nécessitent la requête textuelle, qui n'est pas disponible sur notre cible, Amazon Echo.
Le système de dialogue
Nous définissons un dialogue comme une séquence de tours de dialogue entre un utilisateur et le système. Après chaque entrée utilisateur, le système renvoie une réponse. Cette réponse peut être parlée ou bien être une action effectuée par le périphérique. Les kits de développement Alexa et DialogFlow sont basés sur la même architecture. Après une requête utilisateur, le pé-riphérique lui associe une intention et en extrait des entités. Ensuite, la réponse correspondante au prochain noeud du graphe de dialogue est retournée. Afin de gagner en flexibilité, les déve-loppeurs tiers peuvent utiliser un webhook. Un webhook permet d'envoyer des informations à une API au travers d'une requête POST pour requêter une base de données ou mettre à jour des informations utilisateur. Nous avons cependant utilisé le webhook pour court-circuiter le graphe de dialogue du périphérique en routant toutes les intentions vers le même appel d'API. Les intentions et entités sont traitées dans notre système de dialogue personnalisé et la réponse textuelle est ensuite envoyée au périphérique vocal pour être lue à l'utilisateur.
Le système de décision
Notations. Soient i ∈ N + un index de conversation et t ∈ N + le nombre de tours de conversation depuis le début du dialogue i.
L'algorithme de bandits linéaires Un dialogue est un problème à information partielle avec un compromis entre l'exploration et l'exploitation. En effet, ne connaissant pas la politique de dialogue optimale à son initialisation, il est nécessaire pour l'algorithme d'explorer afin de trouver les actions permettant ensuite de maximiser sa récompense cumulée (la somme des r i,t ). Ce problème est connu comme le problème des bandits contextuels. LinUCB (Li et al., 2010;Chu et al., 2011) est un algorithme de bandits contextuels qui utilise des modèles linéaires pour prédire les récompenses des actions tout en maintenant un intervalle de confiance supérieur sur les récompenses pour engendrer une exploration optimiste des actions. Bien qu'étant linéaire, LinUCB possède de fortes garanties théoriques et peut-être compétitif sur le court terme face à des algorithmes basés sur des réseaux de neurones (Allesiardo et al., 2014) ou des forêts aléatoires (Féraud et al., 2016) grâce à sa rapidité de convergence.
La fonction de récompense utilise un retour en N-étapes, inspiré de l'apprentissage TD (Sutton, 1988), pour permettre au modèle d'apprendre des dépendances temporelles. Cela permet au modèle d'être récompensé avec des récompenses obtenues plusieurs itérations après avoir joué une action et un facteur de "discount" est utilisé de manière à favoriser les récom-penses immédiates plutôt que celles sur le long terme. Ceci permet d'éliminer les actions inutiles n'ayant pas d'impact positif sur les performances.
Soit γ ∈ [0, 1] un facteur de "discount". La récompense r i,t est générée après avoir joué l'action k i,t . Chaque action k i,t ∈ H i est récompensée avec la récompense r i,t γ t−t . Au temps T , la récompense cumulée de l'action k i,t , avec t ≤ T est : y i,t = r i,t + γr i,t+1 + γ 2 r i,t+2 + ... + γ T −t r i,T Nous utilisons une version modifiée de LinUCB (voir Algorithme 1) où chaque action peut avoir des contextes de tailles différentes. La linéarité de LinUCB permet aussi de mettre à jour les modèles immédiatement lorsqu'une récompense intermédiaire est disponible.
Le gestionnaire de dialogue
Deux objets principaux sont manipulés, les contextes et les actions. Les actions sont des réponses possibles à une requête utilisateur. Par exemple "Je vous propose param :POI à param :localisation". Des appels à des systèmes externes peuvent aussi être associés aux actions, comme une réservation de table ou l'achat de billets d'avions. Les contextes sont construits par le système et reflètent l'état de la conversation. Les variables principales des contextes sont les Algorithme 1 LinUCB Asynchrone avec relations temporelles // Initialisation des modèles linéaires
Observer la récompense r i,t for j ∈ {0, .., t} do b ki,j ← b ki,j + x i,j,ki,j γ t−j r i,t end for end for intentions et les entités. Bien que les intentions et les entités soient créés manuellement, des méthodes existent pour les créer de manière automatique depuis des historiques de dialogues existants (Bouraoui et Lemaire, 2017). Afin d'aider l'algorithme d'apprentissage, plusieurs méthodes permettent de le guider et de réduire l'espace d'exploration.
Des intentions et des actions génériques : une intention "recherche_restaurant" peut être remplacée par "recherche_professionnel" ayant pour paramètre "param :professionnel". Procé-der de la sorte permet de déléguer l'analyse générale de la requête au périphérique et d'analyser nous même les entités détectées. Définir les bonnes intentions a été particulièrement important car notre cible principale, Amazon Alexa, ne restitue pas la requête utilisateur, mais uniquement le nom de l'intention et les valeurs des entités.
Contraindre le graphe avec des déclencheurs : pour guider les choix de l'algorithme et s'assurer de sa sureté, nous avons ajouté des déclencheurs (règles métiers) contrôlant les actions disponibles dans [K] i,t . Certaines règles peuvent forcer l'algorithme à choisir une action particulière (avec un ensemble d'action de taille 1) tandis-que d'autres peuvent simplement retirer une action de l'ensemble. Ceci est important pour permettre de protéger certains actions derrière une action de confirmation.
Construire de contexte de dialogue : chaque système de dialogue peut contextualiser des variables différentes, dépendantes des tâches à accomplir, et chaque action peut avoir des contextes de tailles différentes. Quelques exemples de variables : le nombre de tours de dialogue, l'intention détectée, la liste des actions précédentes, le nombre de résultats de recherches (utile pour les actions de filtrage). Ces variables sont regroupées avant chaque déci-sion pour former les vecteurs de contextes. Il est important de préciser que LinUCB est robuste aux contextes générés par un adversaire. Cela signifie que l'algorithme peut-être utilisé même lorsque les contextes ne sont pas tirés depuis des distributions stationnaires.
Les récompenses sont définie manuellement et sont déclenchées lorsque l'utilisateur effectue une action entrainant une conversion (déclenchement d'un appel téléphonique, d'un achat, d'une réservation). Les noeuds du dialogue n'entrainant pas de récompense seront peu à peu négligés puis éliminés au profit de chemins aux récompenses plus élevées.
Les utilisations pratiques du système de décision
Bien que cette approche ouvre de nombreuses possibilités, cette architecture n'est pas supposée fournir un dialogue contrôlé par une IA de bout en bout comme l'envisagent les mé-thodes d'apprentissage profond. Elle peut être utilisé principalement : L'optimisation de la formulation. La formulation des réponses peut impacter l'expérience utilisateur. Même si la sémantique des deux phrases parlées est identique, une mauvaise formulation peut porter à confusion tandis-qu'une bonne formulation pour augmenter le taux de complétion des tâches.
Le remplacement des règles expertes. Ce type de système de dialogue comprend un certain nombre de règles expertes. Certaines sont factuelles, comme la protection d'une action de réservation par une action de confirmation, et peuvent raisonnablement être définies manuellement. Cependant d'autre règles relèvent de l'heuristique et bénéficient grandement d'un apprentissage dynamique. Par exemple, que faire si l'utilisateur n'a pas précisé la localisation de sa recherche ? Est-ce qu'il faut utiliser la géo-localisation du périphérique ? Ou bien la ville entrée dans son profil ? La localisation de la dernière recherche ? Qu'en est-il si cette recherche a été effectuée il y a 30 secondes, ou bien 2 jours ? Le système de décision peut résoudre ce type de problème. Après avoir ajouté ces informations au vecteur de contexte, la règle sera apprise de manière automatique par le système de décision.

Introduction
L'apprentissage non supervisé (ou clustering) permet de calculer un modèle de la structure de données lorsqu'aucune autre information n'est connue. Les objets sont regroupés en "clusters", en fonction de leur similarité. Cette classification est une représentation compacte de la distribution sous-jacente des données. De nombreux algorithmes ont été proposés, qui peuvent être classés en plusieurs familles (Bishop et al., 1998). Dans cet article, nous nous intéressons à la famille des algorithmes à base de prototypes, dans laquelle chaque cluster est représenté par un prototype : un nouvel objet dans l'espace de représentation. Les approches basées sur des prototypes sont très populaires en raison de la compacité du modèle obtenu (les prototypes), du pouvoir descriptif de ces prototypes et de la faible complexité de calcul du modèle (chaque objet est comparé à un ensemble de prototypes généralement réduit). Cette faible complexité explique à elle seule la popularité des approches à base de prototypes pour des applications réelles. Habituellement, le meilleur choix de prototype est le barycentre du cluster. Le prototype est ainsi défini comme étant l'objet minimisant la somme des distances carrées avec tous les objets du cluster. Si les objets sont décrits comme des vecteurs numériques dans un espace euclidien, la définition des prototypes du cluster est simple. Dans ce cas, un prototype est un vecteur défini dans le même espace, calculé comme la moyenne vectorielle des objets appartenant à son cluster. En fait, la plupart des algorithmes basés sur des prototypes ne sont adaptés qu'aux vecteurs définis dans un espace euclidien. Cependant, dans de nombreux cas, les objets ne peuvent pas être facilement définis dans un espace euclidien sans perte d'informations et/ou sans pré-traitement coûteux (images, réseaux, séquences, textes, par exemple). La similarité entre un tel objet n'est généralement pas une distance euclidienne et le calcul habituel des prototypes n'est plus valide. Une représentation commune pour ce type de données consiste à décrire les relations (les similarité) entre les objets à l'aide d'une matrice de dissimilarité ou de distance. Pour cette raison, elles sont parfois appelées données relationnelles. Peu de travaux ont encore été réalisés sur la classification relationnelle basée sur des prototypes, mais certains auteurs ont travaillé sur des adaptation de K-moyens (Ordonez et Omiecinski, 2004;Hathaway et al., 1989;Rossi et al., 2007;Cherki et al., 2016). Le problème principal de la classification relationnelle basée sur des prototypes est la définition des prototypes basée uniquement sur les distances entre les objets. Habituellement, les prototypes sont représentés par une combinaison linéaire des données d'entrée. Mais, en ce qui concerne la puissance de traitement et l'utilisation de la mémoire, cette mise en oeuvre est très coûteuse. L'approche présentée dans le présent document a une faible complexité, afin de pouvoir traiter de gros volumes de données, tout en offrant une représentation appropriée de la structure des données en minimisant la perte d'informations. Les algorithmes de classification proposés sont indé-pendants de la représentation des objets, afin d'utiliser toutes l'information disponible. Nous proposons deux algorithmes basés sur cette approche : une version batch, dans laquelle l'ensemble de données est conservé en mémoire pendant tout le processus d'apprentissage et une version incrémentale dans laquelle les objets sont présentés un par un. Les deux approches sont beaucoup plus rapides et nécessitent beaucoup moins de mémoire que d'autres approches pour données relationnelles. Nous démontrons ces propriétés théoriquement et expérimentalement sur un ensemble de données relationnelles artificielles et réelles.
Clustering relationnelle basé sur Coordonnées Barycentrique
La Figure 1 est un exemple illustrant l'idée générale de l'approche proposé. Dans le système de coordonnées barycentriques Hille (2005), l'espace de représentation est défini par un ensemble unique de P points de support choisis parmi les objets O. Ces points de support peuvent être des objets choisis au hasard parmi O et représentent un espace virtuel de dimension P − 1 (Figure 1.b). Soit S ⊂ {1, . . . , N } un sous-ensemble fini d'index, avec P = |S| | N . Nous définissons l'ensemble des points de support O S = {o i , i ∈ S} ⊂ O associé à une représentation inconnue dans X par X S = {s i ; s i = x i , i ∈ S} ⊂ X. Nous visons à représenter chaque cluster par un prototype {µ 1 , µ 2 , ..., µ K } avec K le nombre de prototypes (Figure 1.c). Le prototype µ k du cluster k est défini comme une combinaison linéaire normalisée de X S (les points de support) :
C'est également la définition des coordonnées barycentriques d'un objet dans l'espace défini par les points de support. En d'autres termes, β k sont les coordonnées barycentriques de µ k par rapport au système de points de support X S . Tout objet o de la base de données peut ainsi être défini à l'aide de coordonnées barycentriques : o i = P p=1 β i p s p avec les coordonnées β i satisfaisant P p=1 β i p = 1. Pour évaluer la distance entre un objet o i et un prototype µ k , nous  
En utilisant la symétrie de D S , nous obtenons β i comme solution du système linéaire suivant :
Notez que la dernière équation du système représente la contrainte normalisée P p=1 β i p = 1. Par conséquent, nous pouvons calculer les coordonnées barycentriques β i pour chaque donnée o i en utilisant (4). Nous sommes donc capables de calculer les distances entre un objet et un prototype à partir de l'équation (2). Le problème à optimiser pour trouver les coordonnées de chaque prototype reste une minimisation de l'inertie. Nous proposons deux algorithmes pour Choisir au hasard K coordonnées β i pour initialiser β k . while la convergence n'est pas atteinte do Assigner chaque objet au prototype le plus proche en utilisant (2). Mettre à jours β k en utilisant (5). end calculer les coordonnées des prototypes : une version batch, où l'ensemble de données est conservé en mémoire et une version incrémentale, où les objets sont présentés un par un.
La version batch de l'algorithme proposé est décrite dans l'algorithme 1). On suppose que l'ensemble du jeu de données puisse être stocké dans la mémoire, ce qui permet de calculer et de stocker les coordonnées barycentriques β i de tous les objets. En minimisant la distance carrée et en utilisant l'équation (2), nous calculons les coordonnées du prototype µ k du cluster k dans le système de coordonnées barycentriques défini par O S . Les coordonnées barycentriques du prototype de C k sont donné par :
L'idée du processus incrémentale est de présenter les objets du jeu de données un par un, de manière aléatoire. La mise à jour des prototypes est calculée de manière incrémentielle pour chaque objet présenté. Comme nous pouvons calculer les coordonnées barycentriques de o i en termes de points de support O S (voir l'équation (4)), la règle de mise à jour de β k peut être écrite ainsi :
où γ est le poids (ou le taux d'apprentissage) définissant l'importance de o i dans les nouvelles coordonnées barycentriques. La procédure résultante est donnée dans l'algorithme 2.
Algorithme 2 : Algorithme proposé version incrémentale Entrée : objets O, fonction d, K, P , γ. Sortie : coordonnées des prototypes β k Choisir au hasard P points de supports
Initialiser au hasard K coordonnées β k tel que P p=1 β k p = 1. while la convergence n'est pas atteinte do Sélectionner o i ∈ O au hasard et calculer β i avec (4).
Assigner o i à son prototype le plus proche µ k * en utilisant (2). Mettre à jours µ k * en calculant β k * selon (6). end Les algorithmes proposés sont comparés à sept algorithmes de classiques de clustering, adaptés aux matrices de dissimilarité. Pour étudier expérimentalement l'effet du nombre d'objets sur le temps de calcul et l'utilisation de la mémoire, nous avons généré des données gaussiennes avec 10 clusters et 10 dimensions. Nous avons augmenté progressivement le nombre d'objets pour observer l'augmentation du temps de calcul (Table 3). Notez que seule la version batch de l'approche proposée est présentée ici, car le temps de calcul de la version incrémen-tale est exactement proportionnel. Dans la table 3, "-" signifie que l'algorithme n'a pas assez de mémoire pour terminer. Comme prévu, le temps de calcul de l'algorithme proposé augmente beaucoup plus lentement que pour les autres approches. L'algorithme proposé peut traiter des ensembles de données volumineux pour un coût temporel et une consommation de mémoire très raisonnables. Ce n'est pas le cas pour les autres algorithmes.
Time (s) 500 1,000 2,000 5,000 10,000 20,000 40,000 50,000 100,000 1,000,000 10,000,000 Dans le tableau 3, nous avons examiné notre algorithme en version batch et incrémentale avec le score NMI (Normalized Mutual Information) (Strehl et Ghosh, 2003) et nous l'avons comparé à différents algorithmes testés sur 7 ensembles de données de types variés (vecteurs, textes, séquences et distributions). L'indice de qualité Silhouette donne des résultats similaires. Dans cette expérience, nous avons également utilisé 10 points de support. Les résultats dé-montrent la qualité des approches proposées par rapport aux algorithmes de l'état de l'art. Les qualités internes et externes de nos algorithmes sont, la plupart du temps, au moins aussi bonnes que celles de nos concurrents sur les jeux de données expérimentaux. Les versions batch et incrémentale sont de qualité très similaire. 

Introduction
L'augmentation de données consiste à générer des données artificielles permettant d'amé-liorer la diversité des données d'entraînement d'un classifieur dans le but d'améliorer ses performances. Les réseaux de neurones convolutionnels profonds (CNN) ont récemment prouvés leur efficacité dans le domaine de la classification de séries temporelles (CST). Cependant, les techniques d'augmentation de données n'ont pas encore été complètement explorées pour la CST. Dans cet article, nous proposons de tirer parti d'une technique d'augmentation des données basée sur DTW spécialement développée pour les séries temporelles, afin d'améliorer les performances d'un réseau de neurones profond résiduel (ResNet) pour la CST.
Méthode
Nous avons choisi d'améliorer la capacité de généralisation du réseau ResNet proposé dans Wang et al. (2017). En adoptant une architecture déjà validée, nous pouvons attribuer toute amélioration des performances du réseau uniquement à l'augmentation des données. Pour plus de détails concernant l'architecture, nous conseillons la lecture de Ismail Fawaz et al. (2018).
La méthode d'augmentation des données que nous avons choisie de tester avec cette architecture profonde a été proposée afin d'augmenter le jeu d'entraînement défini pour un NN-DTW dans un problème de simulation de démarrage à froid (Forestier et al., 2017). Pour plus de détails concernant la méthode de pondération, les lecteurs peuvent se référer à Forestier et al. (2017). En ce qui concerne le calcul de la séquence moyenne, nous avons adopté l'algorithme DBA (Petitjean et al., 2016) dans notre processus d'augmentation de données.
Résultats
Nous avons évalué notre méthode d'augmentation de données pour ResNet sur l'UCR archive (Chen et al., 2015). Pour former les modèles d'apprentissage profond, nous avons tiré parti de la puissance de calcul élevée de 60 GPUs d'un cluster de calcul 1 . Nos résultats montrent que l'augmentation de données peut considérablement améliorer la précision d'un modèle d'apprentissage profond tout en ayant un impact négatif léger sur certains jeux de données dans le pire des cas (cf. figure 1a). Nous avons testé une technique d'ensemble prenant en compte les décisions de deux ResNets (formés avec et sans augmentation de données). Les résultats de l'ensemble (cf. figure 1b) sont en conformité avec le consensus récent dans la communauté, dans lequel les ensembles ont tendance à améliorer la précision individuelle des classifieurs (Ismail Fawaz et al., 2018).
Conclusion
Dans cet article, nous avons montré qu'il est possible d'atténuer le sur-apprentissage de petits jeux de données de séries temporelles en utilisant une technique récente d'augmentation de données basée sur DTW et une version pondérée de l'algorithme DBA.
Références
Chen, Y., E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, et G. Batista (2015) 

Introduction
La détection d'anomalies est définie comme la recherche de structures dans un jeu de données qui ne correspondent pas au comportement attendu (Chandola et al., 2009).
Dans cet article, nous étudions la résistance au bruit et à la rareté de la détection d'anomalies par arbre de décision dans le cas de systèmes physiques simulés. En fait, la question est de savoir si l'apprentissage supervisé est adapté à des problèmes de détection d'anomalies, où la classe positive (anomalie) est extrêmement moins fréquente que la classe négative (normale), en présence de bruit bien sûr. La fonction cachée étant simple (un hyperplan, sur une seule variable), toutes les techniques d'apprentissage supervisées pourraient être utilisées (perceptron, SVM, plus proche voisin, etc.). Nous avons choisi les arbres de décision car ils fournissent un modèle explicite et peuvent gérer un grand nombre de données.
La suite de notre article s'organise comme suit. La Section 2 expose le contexte et la problématique de la détection d'anomalies des systèmes physiques que nous considérons : le pneumatique et la pompe à chaleur. Ensuite, nous détaillons dans la Section 3 notre générateur de données artificielles. La Section 4 présente notre modèle de détection d'anomalies et les diffé-rents résultats obtenus suite à son évaluation. Enfin, une conclusion est établie dans la Section 5 et propose quelques perspectives. 
Problématique
Dans cet article, nous nous intéressons à deux systèmes physiques, le pneumatique et la pompe à chaleur. Cette section présente deux anomalies typiques de ces systèmes et comment les représenter sous la forme d'une même tâche d'apprentissage automatique.
Le pneumatique est un composant essentiel dans un véhicule qui assure sa sécurité et son confort. Des statistiques récentes montrent que la perte de pression du pneumatique est la principale cause de sa défaillance et par conséquent, de plusieurs accidents. Ainsi, des textes de réglementation européenne imposent la présence d'un système de surveillance de la pression des pneumatiques (SSPP) dans tous les nouveaux véhicules dès 2012. Ce dispositif doit être capable de détecter une perte de pression d'au moins 20% de la pression à chaud (El Tannoury, 2012). La crevaison d'un pneumatique est un problème simple à modéliser et à comprendre, cf. Figure 1. Ceci nous permet de mieux évaluer notre modèle d'apprentissage en nous focalisant sur la problématique du déséquilibre des données et de la robustesse de ce modèle.
La pompe à chaleur est un dispositif thermodynamique qui transfère l'énergie du milieu le plus froid vers le milieu le plus chaud. Il s'agit d'un système physique compliqué qui fait intervenir plusieurs variables dans son fonctionnement. Il peut y avoir donc une grande diversité de pannes. Parmi ces pannes, nous considérons la perte de fluide frigorigène ou réfrigérant. Cette perte de fluide pourrait se représenter comme une perte de pression (ou de poids) de fluide et conduire à une courbe similaire à celle de la crevaison d'un pneumatique. Cependant, les pompes à chaleur ne sont pas forcément équipées de capteurs mesurant cette pression. Des experts indiquent que cette perte est détectable en observant la vitesse de dégivrage qui diminue.
Les anomalies que nous considérons pour le pneumatique et pour la pompe à chaleur se ramènent à observer une variable continue, respectivement la pression du pneumatique et la vitesse de dégivrage, qui est constante dans le cas normal, mais qui diminue en cas de fuite. La tâche d'apprentissage consiste donc simplement à apprendre un seuil sur cette unique variable d'entrée. Dans la suite de cet article, nous prendrons l'exemple de la pression du pneumatique. Il ne s'agit ici que d'une première étape, avant de considérer une représentation plus complexe de ces systèmes physiques ou d'autres anomalies. L'intérêt principal de cette première étape est d'étudier si une technique d'apprentissage supervisée peut être mise en oeuvre sur un tel problème, où les anomalies sont extrêmement moins fréquentes que le fonctionnement normal, et avec quelle performance.
Génération de données artificielles bruitées
Un jeu de données réel a une taille donnée et une complexité fixe et inconnue (Barse et al., 2003). L'intérêt de générer des données artificielle est de pouvoir contrôler la complexité de la fonction cible et d'avoir autant de données que souhaité.
Nous fixons arbitrairement mais de façon réaliste la pression nominale à 2 bars, considérant que le pneumatique se dégonfle si la pression est inférieure, et la pente de dégonflement à 0.01, c'est-à-dire que le dégonflement se fait linéairement en 40 unités de temps, avant que le seuil d'alerte de 1.6 bar soit atteint. Les paramètres que nous faisons varier sont le nombre N d'observations et la durée T de la phase normale à l'issue de laquelle le dégonflement commence. La durée T nous permet de contrôler le déséquilibre de nos données en testant différentes fré-quences de déséquilibre. Ces fréquences représentent la fréquence d'anomalies dans un jeu de données. Par exemple, une fréquence 1 pour 10 signifie que dans un jeu de données de 10 observations, une seule observation représente une anomalie. Les cinq fréquences que nous utilisons sont : 1 pour 10, 1 pour 100, 1 pour 1 000, 1 pour 10 000, 1 pour 100 000. Le nombre N d'observations est obtenu en répétant plusieurs cycles terminés chacun par un dégonflement.
Les données que nous obtenons à la sortie de notre générateur sont des fichiers d'enregistrements (fichiers logs) qui décrivent les mesures de la pression datées et classées par ordre chronologique. Cependant, comme notre objectif est de détecter une anomalie à un instant t précis, nous transformons nos fichiers logs en un format attribut-valeur et suite à cette transformation, chaque ligne de données représentera une observation de la valeur de la pression et la classe associée.
Dans notre simulation du comportement du pneumatique, nous ne modélisons pas explicitement d'autres paramètres qui peuvent influencer son comportement (Température extérieure, échauffement du roulement, etc). Afin de prendre en considération l'incertitude liée aux mesures de la pression et de nous rapprocher le plus possible du comportement réel du pneumatique, nous ajoutons un bruit de type Gaussien à la pression générée précédemment. La classe associée à chaque ligne ne change pas, bien sûr. Afin de mieux évaluer la résistance au bruit de la détection d'anomalies par arbre de décision, nous avons choisi de tester deux bruits gaussiens :
-Premier niveau de bruit : Un bruit gaussien avec une espérance µ égale à 0 et un écart type σ égale à 0.01. Cet écart-type correspond volontairement à la pente du dégonfle-ment du pneumatique. Sans bruit, le dégonflement serait détecté dès que la pression devient strictement inférieure à 2 bars. -Deuxième niveau de bruit : Un bruit gaussien avec une espérance µ égale à 0 et un écart type σ égale à 0.03, donc 3 fois plus "fort".
d'une approche typique pour traiter des données déséquilibrées, one-class SVM (Scholkopf et al., 1999), montrent qu'elle ne résiste pas au déséquilibre de nos données et génère un grand nombre de fausses alertes. Les forêts d'arbres d'isolation, d'après l'algorithme et les tests publiés (Liu et al., 2008), risquent d'être moins précises car elles sous-échantillonnent les données alors que nous cherchons des anomalies jusqu'à 10 fois plus rares que les tests publiés, et plus lentes puisqu'elles construisent -aléatoirement qui plus est-un arbre séparant les données alors que nous ne cherchons qu'un seuil/noeud et considérons plus de 100 fois plus de données. Détecter le dégonflement du pneumatique consiste à trouver la pression qui représente le seuil de séparation entre le comportement normal et le comportement anormal. Ainsi, nous imposons que la profondeur de notre arbre de décision ait une valeur de 1 en conservant les autres paramètres par défaut. Nous utilisons dans ce travail l'implémentation CART de l'arbre de décision de la librairie Scikit-learn (Pedregosa et al., 2011) de Python dont le critère de segmentation est l'indice de diversité de Gini. Les évaluations sont réalisées avec 80 millions de données pour chaque expérience. Elles sont divisées en 75% pour la phase d'entraînement et 25% pour la phase de test.
La performance de notre modèle est évalué par la précision, le rappel et la F-mesure. Rappelons que l'exactitude (accuracy en anglais) n'est pas une mesure de performance adaptée à notre problème. En effet, en travaillant avec un jeu de données déséquilibré, un programme d'apprentissage a tendance à construire des modèles qui prédisent correctement la classe majoritaire et ont donc une exactitude élevée. Mais cette exactitude ne sera pas utile parce qu'elle ne reflète pas l'aptitude à détecter les anomalies et donc la classe minoritaire.
Nous interprétons les résultats de notre modèle d'apprentissage supervisé en fonction en des deux niveaux de bruits afin d'établir un bilan de sa résistance au bruit et au déséquilibre des données.
-Premier niveau de bruit : Avec le premier niveau de bruit, la performance de l'arbre de décision est assez impressionnante même avec des données très déséquilibrées. En effet, quand la classe majoritaire (classe normale) devient plus importante, l'arbre de décision devient plus exigeant en terme du seuil de décision qui baisse. L'arbre de décision "préfère" rater quelques exemples positifs en les attribuant à la classe normale (et donc le rappel diminue) plutôt que de déclencher de nombreuses fausses alertes en classant mal les exemples négatifs de plus en plus nombreux. Il maintient ainsi une précision élevée. Vu que la F-mesure est une moyenne de la précision et du rappel, elle diminue lentement. -Deuxième niveau de bruit : Avec le deuxième niveau de bruit, nous observons un comportement similaire : le seuil de décision diminue en augmentant le déséquilibre. Néanmoins, l'arbre de décision est moins performant qu'avec le premier niveau de bruit : il n'arrive pas à maintenir une précision parfaite (égale à 1) même pour un déséquilibre minimal. Ceci est dû à l'augmentation du bruit. Pourtant pour la cinquième fréquence, nous observons une précision égale à 1 avec les deux niveaux de bruits. C'est étonnant vu qu'il s'agit d'un très grand déséquilibre de données. Nous supposons qu'il s'agit d'une aberration probablement liée aux limites d'implémentation de notre générateur. En effet, vu que nous ajoutons un bruit Gaussien à nos données, les exemples négatifs (dont la pression est nominale, 2 bars) suivent une loi normale également. Si nous reprenons l'exemple de la cinquième fréquence avec le premier niveau de bruit, nous aurons  Table 1. Nous avons donc une chance sur un million, en théorie, de tirer une observation de valeur inférieure à ce seuil mais il est probable qu'en pratique notre générateur ne génère aucune donnée au delà de 5 σ. Ceci explique la valeur de la précision, égale à 1, lors du dernier test puisque notre modèle n'a trouvé aucune valeur de la classe normale inférieure au seuil de décision pour la classer comme étant dégonflement. De même pour le deuxième niveau bruit suite à l'augmentation du bruit et donc de σ, le seuil se rapproche de 2 − 0, 03 * 4, 8 = 1, 856. Sur 80 millions d'exemples négatifs générés, aucun ne descend en dessous de ce seuil. Même en générant un nombre encore plus grand d'exemples négatifs, il est probable qu'aucun ne serait inférieur à ce seuil.
Conclusion
Malgré l'aspect temporel de nos données, notre objectif est de faire une détection d'anomalie à chaque instant. Nous constatons que nous n'avons pas eu besoin de prendre en compte les valeurs précédentes de la pression et que la valeur courante de la pression a suffi.
Nous avons généré un très grand nombre de données (80 millions pour chaque fréquence et niveau de bruit) afin d'avoir au moins 200 exemples positifs dans le jeu de test dans le cas des jeux de données les plus déséquilibrés. Nous avons observé que nous avons probablement atteint la limite pratique du notre générateur aléatoire de bruit gaussien utilisé.
Ces expériences permettent de constater qu'un arbre de décision est capable de traiter plusieurs millions d'exemples et d'apprendre un seuil permettant d'obtenir d'excellents précision, rappel et F-mesure alors que la fréquence de la classe positive descend jusqu'à 1 pour 100 000.
Les prochaines étapes sont de tester sur des données réelles. Nous espérons que ce sera l'occasion de considérer des fonctions cachées plus complexes (sur plusieurs attributs) et qui nécessiteront éventuellement de prendre en compte l'historique de la séquence de données.

Introduction
Les modèles de propagation actuels proposent de représenter la diffusion d'informations dans un réseau de données. Les problèmes de diffusions de rumeurs ou de maladies sont étudiés pour comprendre et modéliser la propagation d'informations (Kempe et al. (2003); Kermack et McKendrick (1932)). Ces modèles de diffusion ont également été adaptés aux réseaux éco-nomiques et notamment bancaires pour étudier les faillites.
Les événements économiques et sociaux semblent donc pouvoir être représentés de la même façon : une baisse de production de l'orge peut augmenter le prix des céréales, ce qui peut impacter les producteurs et distributeurs de bières. Notre idée est de proposer un modèle de propagation représentant ce genre de scénarios. Pour cela nous proposons un problème appelé Infection Classification Problem (ICP). Le but d'ICP est de déterminer pour chaque élément du réseau s'il est concerné par l'événement. Dans cette optique nous avons étudié deux événements connus : L'ouragan Katrina qui a frappé la Nouvelle-Orléans et la tentative d'acquisition de Monsanto par Bayer. Pour répondre à ICP nous proposons deux modèles de diffusions appelés Hybrid Linear Threshold (HLT) et Adapted Threshold (AT) qui sont basés sur le modèle de seuil Linear Threshold. Pour chacun des modèles nous utilisons un seuil pour déterminer à partir de quel instant un noeud est impacté par un événement. Les modèles diffèrent par la manière dont le seuil est calculé, pour HLT, nous utilisons la valeur des poids des arcs entrants tandis que nous utilisons la valeur du poids du noeud pour AT.
État de l'art
Un réseau est représenté par un graphe dirigé G = (V, E) où les noeuds V sont des entités et les arcs E représentent les interactions entre les entités. Dans le cas de réseaux sociaux, les noeuds représentent des personnes et les arcs les interactions sociales entre les personnes.
Parmi les problèmes étudiés d'infection de réseaux, Kempe et al. (2003) proposent d'étu-dier l'Influence Maximization Problem, où le but est de déterminer quels noeuds initiaux impactent le plus d'autres noeuds. Pour cela ils proposent deux modèles, Independent Cascade Model (IC) et Linear Threshold Model (LT). Ces modèles, à partir de noeuds initiaux, propagent l'information dans le graphe. Ces noeuds initiaux, ou noeuds sources, transmettent l'information de la contagion à leurs voisins qui peuvent alors se retrouver infectés et la transmettre à leur tour.
Independent Cascade Model (IC) : Goldenberg et al. (2001) proposent un modèle où, à chaque étape t, un noeud impacté u peut aléatoirement impacté un ou plusieurs voisins relié par l'arc (u, v) via une fonction probabiliste p (u,v) . Si l'information est transmise, le noeud v devient impacté et peut alors transmettre l'information au temps t + 1.
Linear Threshold Model (LT) : Granovetter (1978) proposent un modèle basé sur un seuil. Chaque noeud v a un seuil s(v) et il y a un poids w sur chacun de ses arcs entrants. À une étape t, si la somme du poids des arcs venant des voisins impactés est supérieure au seuil s(v), v devient impacté :
Dans notre papier, nous utiliserons le modèle LT car il est déterministe pour un seuil fixe (Lu et al. (2011)), ce qui nous permet de garantir un résultat toujours identique pour les expé-rimentations, contrairement au modèle IC.
D'autres problèmes proposent d'étudier la minimisation de l'influence ou l'influence compétitive (Yang et al. (2017);Caliò et Tagarelli (2018)). Mehmood et al. (2016) proposent le Typical Cascade Problem où le but est de trouver quels noeuds sont généralement impactés pour des noeuds sources donnés.
Les modèles de diffusions ont également permis de représenter les risques de faillites dans les milieux bancaires (Chinazzi et Fagiolo (2015); Kenett et Havlin (2015)), la simulation des risques du aux échanges entre banques (Montagna et Kok (2016)) ou encore la formation de réseaux économiques (Kantemirova et al. (2018)).
Problème de classification de noeuds impactés
Nous voulons déterminer, pour un graphe et un événement donné, quels noeuds sont impactés. Nous définissons pour cela un nouveau problème appelé Infection Classification Problem (ICP). Soit :
-L IV ⊂ V la liste des noeuds initialement impacté par l'infection i -L T V ⊂ V une liste de noeuds cibles dans laquelle : -L RV ⊂ L T V sont les noeuds devant être impactés à la fin de la propagation -L AV ⊂ L T V sont les noeuds ne devant pas être impactés à la fin de la propagation -L RV ∩ L AV = ∅ -Lorsque la propagation s'arrête pour un modèle de diffusion nous avons :
-L T P la liste des noeuds impactés par le modèle et qui devaient l'être -L F P la liste des noeuds impactés par le modèle et qui ne devaient pas l'être -L T N la liste des noeuds non impactés par le modèle et qui ne devaient pas l'être -L F N la liste des noeuds non impactés par le modèle et qui devaient l'être Avec ces listes nous pouvons calculer la précision et le rappel :
La précision offre une évaluation du bruit en calculant le pourcentage de noeuds bien classé parmi les noeuds impactés. Le rappel offre une évaluation du silence en calculant le pourcentage de noeuds impactés parmi ceux qui sont censés l'être. La moyenne harmonique de ces valeurs, appelée F-measure, permet d'évaluer ICP. Le but des modèles appliquer à ICP étant de maximiser cette valeur.
Modèles de diffusion
Nous proposons de résoudre ICP à l'aide de 2 modèles dont le développement a été motivé par l'adaptation d'un graphe à un événement typé. En effet, l'acquisition d'une société n'impactera pas les noeuds de la même manière que la sortie d'un nouveau produit, même si les sources sont identiques. Pour cela nous définissons une infection i = (L IV , type) tel que :
-L IV ⊂ V la liste des sources : ∀v ∈ L IV , v ∈ I t0 -type, le type de l'infection Nous utilisons les données présentes dans le graphe (voir Section 5) tel que le poids, le type des liens ou des noeuds ou encore la distance aux sources pour adapter la propagation.
Calcul du seuil : Pour déterminer pour chacun des noeuds leur seuil respectif nous utilisons leur type, le type de l'infection et soit le poids du noeud soit celui des arcs entrant.
Poids des arcs : Le poids des arcs est adapté selon le type des noeuds de l'arc, du lien et de l'événement. Selon ces types un pourcentage est appliqué au poids initial de l'arc.
Valeur d'infection : Nous utilisons le rapport entre la somme des infections provenant des voisins et le seuil propre au noeud pour donner une valeur à l'infection, transmise aux voisins. L'idée repose sur la virulence pour représenter un impact plus ou moins fort.
Fonction d'utilité : Nous utilisons la distance entre le noeud infecté et la source de l'infection pour diminuer la valeur d'infection : plus un noeud est distant des sources moins l'information transmise sera virulente.
Nous avons créé deux modèles de propagation qui utilisent la notion de seuil. Comme pour le modèle LT les noeuds ont un seuil et deviennent impactés lorsque l'information transmise par leurs voisins devient supérieure à ce seuil. La différence avec le modèle LT est que nous adaptons l'infection du graphe à un événement donné à l'aide des fonctionnalités. Ces deux modèles se distinguent par la façon dont est calculé le seuil :
Hybrid Linear Threshold (HLT) : Nous utilisons le type de l'événement et du noeud pour déterminer un pourcentage qui est appliqué sur le total du poids des arcs entrants pour déterminer la valeur du seuil.
Adapted Threshold (AT) : Le type de l'événement et du noeud sont utilisés pour déter-miner un pourcentage qui est appliqué sur le poids du noeud (qui est calculé indépendemment des arcs).
Expérimentations
Pour tester notre modèle, nous utilisons un multi-graphe composé de 3,8 millions de noeuds et de 9,6 millions d'arcs. Ce graphe représente une situation économico-sociale en 2016. Nos noeuds sont typés (organisations, lieux, personnes, etc.).
Le graphe est construit par agrégations d'hypothèses détectées à l'aide d'un traitement automatique du langage sur différents documents (news, rapports économiques). Basiquement, si deux entités nommées (= noeuds) sont présents dans une même phrase, ils se retrouvent liés par un arc dans le graphe. Plus une information est présente, plus son poids est important dans le graphe. Un label est utilisé pour désigner le type de chaque arc afin de représenter l'interaction entre deux noeuds. Les arcs sont dirigés et peuvent être multiples (par exemple pour représenté des situations de coopérations et de compétitions entre deux mêmes noeuds).
Pour tester nos modèles, nous avons défini avec des analystes économiques deux événe-ments : Katrina et Bayer-Monsanto. Pour évaluer la qualité des modèles, nous avons défini avec les analystes économiques une liste de 3792 noeuds considérés comme importants : ce sont nos noeuds cibles dans ICP. Ces noeuds ont été choisis à partir du poids des noeuds et des arêtes entrantes. Pour chaque événement les analystes ont défini quels noeuds cibles étaient concernés, 55 pour Katrina et 92 pour Bayer-Monsanto.
Nous avons testé quatre modèles pour résoudre le problème ICP : Linear Threshold Model (LT) : Basé sur l'article de Granovetter (1978). Chaque noeud à son propre seuil fixé par un pourcentage s'appliquant sur la somme du poids des arcs entrants.
Dynamic Linear Threshold Model (DLT) : Basé sur l'article de Litou et al. (2016), DLT calcule un seuil par un pourcentage appliqué sur les poids des arcs entrants. Le poids des arcs change dans le temps en exploitant une distribution suivant une Loi de Poisson. Les noeuds peuvent également renoncer à l'infection et modifier leurs seuils en fonction du temps.
Hybrid Linear Threshold Model (HLT) : Les poids des arcs et l'information transmise sont calculés en fonction du type de l'événement, des noeuds et des arcs. Le seuil est calculé selon un pourcentage, défini par le type de l'événement et du noeud, s'appliquant sur le poids des arcs entrants.
Adapted Threshold Model (AT) : Le seuil est calculé selon un pourcentage, défini par le type de l'événement et du noeud, s'appliquant sur le poids initial du noeud.
Les résultats des données sont présentés en La Table 2 présente les résultats pour le problème ICP. Les colonnes Précision et Rappel donnent respectivement une évaluation du bruit et du silence (voir Section 3). La colonne FMeasure donne l'évaluation de chaque algorithme pour le problème ICP.
Pour ICP, les modèles LT et DLT donnent tous deux de bons résultats pour la précision car ils impactent un petit nombre de noeud cibles voisins des noeuds sources L IV , qui se retrouvent généralement être dans les cibles désirées L T P . En conséquence le bruit est très bas. Cependant, ils n'impactent pas suffisamment de noeuds cibles et se retrouvent avec un silence important. AT, qui produit de meilleurs impacts sur les noeuds cibles, donne de meilleurs ré-sultats que les autres modèles. Les fonctionnalités adaptés aux attributs du graphe permettent également à HLT de surpasser le standard LT duquel il est tiré.
Conclusion et ouvertures
Dans ce papier nous proposons un nouveau problème de diffusion, ICP, dont le but est de classifier, pour un graphe et un événement donnés, les noeuds impactés par cet événement. Nous proposons deux modèles de diffusions inspirés des modèles de seuil. Le premier modèle, HLT, calcule le seuil à partir du poids des arcs entrants tandis que le second, AT, calcule le seuil à partir du poids du noeud. Nous utilisons ces modèles pour simuler des événements économiques sur un graphe typé et pondéré. Nos premières expérimentations montrent que nos modèles surpassent ceux de la littérature.
Plusieurs pistes restent à explorer pour améliorer nos résultats. Étant donné que ICP est un problème de classification, avec plus d'événements, nous pourrions utiliser des techniques

Introduction
Dans le cadre des approches non supervisées, un algorithme de classification tente de diviser les données en plusieurs classes de sorte à ce que les données (appelés également individus, observations ou points) qui se trouvent dans la même classe soient les plus similaires possibles, et inversement, les points appartenant à des classes différentes soient les plus dissemblables possibles.
Parmi les algorithmes de classification, l'algorithme de k-means (centres fixes) est probablement un des plus connus (Forgy, 1965) et constitue l'objet de notre étude. Sa première version est apparue dès les années 50 (Jain, 2010). Ce dernier, repose sur un traitement itératif (i.e. les instructions de l'algorithme doivent être réalisées plusieurs fois avant de converger vers une classification stable) et répétitif (i.e. un même calcul est potentiellement effectué plusieurs fois sur les mêmes données). Dans 'k-means', les résultats d'une itération ne sont pas conservés pour alimenter l'itération suivante. Cette caractéristique engendre des dégradations de performances notamment lorsque la dimension (nombre d'attributs) est important et lorsque ces dimensions possèdent de nombreuses valeurs de densité variable.
Contribution : Afin de permettre à k-means d'offrir de meilleures performances notamment sur de gros volumes de données, nous proposons une nouvelle version de k-means basée sur un principe de pré-agrégats. Cette extension repose sur les principes sont les suivants : (i) pré-calculer et stocker les différents calculs effectués lors des itérations successives, (ii) réuti-liser les pré-calculs stockés pour accélérer les itérations futures. En section 2, nous discutons l'état de l'art. Ensuite, nous exposons notre modèle (section 3) et nos expérimentations (section 4).
Etat de l'art
Approches k-means. L'utilisation de la version standard de k-means nécessite un temps d'exécution proportionnel au produit du nombre de classes et du nombre de points par itération. Ce temps d'exécution total est relativement coûteux en termes de calcul, en particulier pour les grands ensembles de données (Alsabti, 1997). Plusieurs extensions de la version standard du k-means ont été proposées pour accélérer les temps d'exécution (Hung et al., 2005) :
-accélération de l'algorithme par la parallélisation et la distribution des données et des traitements. Cette solution est basée sur les paradigmes MapReduce ou MPI (Zhang et al., 2013)  (Zhao, Weizhong ;Ma, Huifang ;He, 2009) ; -accélération par réduction du nombre de calculs à effectuer pour chaque itération (propriétés de l'inégalité triangulaire) (Elkan, 2003)  (Hamerly, 2010) ; -accélération par organisation ou structuration des données (Hung et al., 2005). Approches de pré-agrégations. Les précédentes optimisations n'utilisent pas le concept de pré-calcul d'agrégats déjà été utilisé dans plusieurs domaines.
-La structuration multidimensionnelle permet d'anticiper les calculs analytiques qui sont donc pré-calculés et stockés dans des cubes de données (Gray, 1996). Dans (Deshpande, 1998), les auteurs partitionnent les données en blocs uniformes (« chunks ») qui sont réutilisés lors des calculs suivants. -Dans le cadre des données statistiques, des travaux intégrant le pré-calcul des agrégats ont été proposés par Wasay et al. (2017). Dans ce système baptisé « Datacanopy », un arbre binaire contenant des pré-agrégats est défini. Les pré-agrégats des noeuds fils sont imbriquées dans ceux des pères. Tous ces travaux reposent sur des calculs d'agrégats statiques. Or, dans le cadre de l'apprentissage automatique, il n'est pas possible d'anticiper les calculs à pré-agréger. Notre approche repose donc sur un principe de stockage « à chaud » des pré-agrégats.
Contribution
Chaque classe a un centroïde {G j | j = 1, . . . , K} tel que G j est la moyenne des valeurs des données de la classe C j .
Les centroïdes intermédiaires.
. . , K} deux ensembles des centroïdes des K classes. G t−1 et G t représentent les centroïdes des classes respectivement aux itérations t-1 et t.
Les classes des identifiants des observations. Soit CID = {CID j | j = 1, . . . , K} l'ensemble des indices des classes. Les différentes classes des CID et C se réfèrent aux mêmes données. Un indice CID j est formé à partir des indices des données de la classe C j lui correspondant.
Les agrégats. Soit M = {M cle } un ensemble d'agrégats correspondants à tous les centroïdes des classes utilisés dans le processus de k-means. A chaque agrégat est associé un indice de CID (clé) permettant de l'identifier. M est vide au début du processus et il est alimenté au fur et à mesure que de nouveaux agrégats sont calculés.
Algorithme de k-means étendu
Dans cette section nous présentons l'algorithme de k-means étendu (voir Fig. 1  
Expérimentations
Pour valider notre approche, nous utilisons la plate-forme de calcul OSIRIM de l'IRIT (cluster de 24 noeuds ayant chacun 8 processeurs de 7.5 Go de mémoire). Nous utilisons un jeu de données synthétiques composé de deux ensembles : données sphériques (DS) i.e. plusieurs groupes de données homogènes (gaussienne isotropique) et des données homogènes (DH), i.e. un seul groupe compact de données (mélange de gaussiennes) où chaque donnée représente des valeurs réelles comprises dans l'intervalle [−10; 10].
Protocole expérimental Pour chaque expérimentation, 4 paramètres sont renseignés : d la dimension de la donnée ∈ [1 ;97000], t le nombre de données à générer ∈ [2000 ;202000], k le nombre de classes ∈ [4 ;20] et D le type de distribution (sphérique, homogène). Chaque expérimentation est exécutée 10 fois. Quelque soit la version de k-means (standard ou étendue) et nous avons toujours le même partitionnement dû à la même initialisation.
Différence de temps d'exécution entre les deux k-means avec des données sphériques et homogènes
Dans cette section, nous évaluons, pour les données sphériques et homogènes, la différence de temps d'exécution (qu'on appellera DFEMS) entre les deux versions de k-means. Elle est calculé en faisant la différence entre le temps pris par le k-means étendu et le k-means standard ; il est positif lorsque k-means étendu est favorable par rapport à k-means standard (cf tab.1).  La version étendue présente de meilleurs résultats en termes de temps d'exécution : sur 1987 expériences sur des données sphériques toutes dimensions confondues, la version éten-due présente 1353 cas favorables (68%) où le DFEMS atteint 1758s. Nous constatons égale-ment que la performance de la version étendue dépasse de plus de 100 secondes la version standard dans 25% des cas. Le nombre de cas favorables est encore plus important lorsqu'il s'agit de données sphériques avec des dimensions d >= 2000, où on atteint un taux de cas favorables de 97% et avec 55% présentant un DFEMS supérieur à 100s. Les figures (Fig. 2) montre une ligne horizontale en rouge séparant les cas favorables (croix vertes) des cas défa-vorables (cercles oranges) à k-means étendu. A gauche de la figure on montre le pourcentage en gain de temps d'exécution de la version étendue par rapport au k-means standard : à partir de 3.15 Go de données, le k-means étendu est pratiquement toujours favorable allant jusqu'à jusqu'à 30% plus vite. Dans la figure 3, on observe que la version étendue de k-means est très favorable à partir de données de dimension supérieure à 2000 (ligne verticale verte) avec une DFEMS allant jusqu'à 1758s (97% de gain) alors que le temps moyen d'exécution de k-means standard est de 3473s. Les cas défavorables se concentrent principalement dans le sous-espace de dimension inférieure à 2000. Dans la figure (Fig. 3), nous évaluons le comportement des deux algorithmes avec des données homogènes. Dans la partie gauche de la figure, le pourcentage de gain en temps par rapport au temps d'exécution de k-means standard est d'environ de 0 à 21% lorsque la dimension des données homogènes est égale ou supérieure à 42000. La même allure est observée dans la partie droite de la figure, la version étendue est avantageuse à partir de la dimension 42000. A partir de cette dimension, le temps moyen d'exécution de k-means étendu est de 1755 secondes. Le gain pouvant atteindre les 372 secondes.
L'approche est beaucoup plus avantageuse sur les DS que les DH puisque dans k-means appliqué à ces derniers beaucoup plus de centroïdes sont utilisés et peu de centroïdes réutilisés comparés au k-means appliqué sur les DS.
Conclusion
Dans cet article, nous avons proposé une approche pour optimiser le calcul de l'algorithme d'apprentissage non supervisé de données, k-means. Cette approche repose sur un algorithme qui pré-calcule et stocke les résultats intermédiaires, appelés pré-agrégats dynamiques, pour être réutilisés dans les itérations suivantes.
Nos expérimentations comparent notre version de k-means étendue avec la version standard. Notre approche montre un gain allant jusqu'à 30% sur des données de grande dimension.
Plusieurs perspectives sont envisagées. Nous allons entraîner cette version étendue de kmeans avec des données plus massives encore pour mieux évaluer le coût du calcul des pré-agrégats. Nous envisageons également d'élargir notre approche à d'autres algorithmes d'apprentissage automatique des données.

Introduction
Les graphes de connaissances (GC) sont de plus en plus utilisés pour représenter et partager les données sur le Web. Le Web sémantique définit des standards pour la représentation (RDF), l'interrogation (SPARQL) et le raisonnement (RDFS/OWL) sur ces GC. Des milliers de GC sont disponibles : ex., DBpedia, Wikidata (préc. Freebase), YAGO, WordNet. La nature ouverte des GC implique souvent qu'ils soient incomplets, et différentes techniques d'apprentissage automatique ont été étudiées pour les compléter.
La tâche de prédiction de liens (Nickel et al., 2016) consiste à prédire les (parties d')arcs manquants. Supposons que le directeur du film Avatar soit absent du GC, on souhaite le prédire, c-à-d. l'identifier parmi tous les noeuds du GC. Le principe est de trouver des régularités dans les connaissances existantes et de les exploiter afin de classer les noeuds du GC. Plus haut est un noeud dans le classement, meilleure est la prédiction. La prédiction de liens a été introduite pour les réseaux sociaux avec un seul type d'arc (Liben-Nowell et Kleinberg, 2007) puis a ensuite été étendu à des données multi-relationnelles et appliquée aux GC (Nickel et al., 2016). Comparé à la classification supervisée, la prédiction de liens affronte plusieurs défis. Premièrement, il y a autant de problèmes de classification qu'il y a de relations, qui comptent souvent dans les centaines. Deuxièmement, 3. des performances supérieures à l'état de l'art sur un benchmark difficile.
Le reste du papier est organisé comme suit. La section 2 discute l'état de l'art en prédiction de liens. La section 3 rappelle la définition des CNN et leur calcul efficace. La section 4 présente notre méthode de prédiction de liens, utilisant les CNN et la théorie de Dempster-Shafer. La section 5 présente des résultats expérimentaux positifs sur le benchmark FB15k-237 ainsi que sur la base Mondial. Enfin, la section 6 conclut et ouvre des pistes de travaux futurs. Nickel et al. (2016) ont récemment publié une synthèse sur l'apprentissage automatique pour les graphes de connaissances, où la prédiction de liens est la tâche d'inférence principale. Ils identifient deux types d'approches qui diffèrent par le type de modèle : les modèles à base de traits latents et ceux à base de traits observés. Les premiers sont de loin les plus étudiés. Avant d'aller plus loin, il est utile de poser le vocabulaire utilisé dans le domaine. Les noeuds sont appelés entités, les étiquettes d'arcs sont appelés relations et les arcs sont des triplets (e i ,r k ,e j ), où e i est la tête, e j est la queue et r k est la relation de e i à e j . Les approches existantes ignorent les valeurs littérales (nombres, dates, chaînes, etc.) ou bien les traitent comme des entités, sans prendre en compte la sémantique de leur domaine.
État de l'art
Les modèles à base de traits latents apprennent des embeddings des entités et relations dans des espaces vectoriels de petite dimension, puis font des inférences sur un triplet (e i ,r k ,e j ) en combinant les embeddings des deux entités et de la relation. Les méthodes existantes varient selon la façon d'apprendre ces embeddings et selon la façon des les combiner. Ces méthodes sont fondés sur différentes techniques telles que : factorisation de matrices ou de tenseurs, réseaux de neurones et descente de gradients. Par exemple, une des premières méthodes pour les GC, TransE (Bordes et al., 2013), modélise une relation comme une translation dans l'espace d'embedding des entités, et évalue un triplet candidat d'après la distance entre la tête translatée et la queue. Bordes et al ont aussi introduit deux jeux de données, FB15k et WN18 respectivement extraits de Freebase et Wordnet, qui sont devenus des benchmarks de référence pour la prédiction de liens. Toutanova et Chen (2015) ont cependant montré qu'une méthode très simple pouvait battre les méthodes existantes à cause d'un biais dans les jeux de données : de nombreux triplets de test ont leur inverse parmi les triplets d'entraînement. Ils ont extrait un sous-ensemble plus exigeant de FB15k, appelé FB15k-237, où tous les triplets inverses sont supprimés. Récemment, Schlichtkrull et al. (2018) ont amélioré de façon significative les performances sur FB15k-237 en utilisant des réseaux convolutifs de graphes pour apprendre les embeddings.
Les modèles à base de traits observés font leurs inférences directement à partir des arcs du GC. L'inférence par marche aléatoire (Lao et al., 2011) utilise des séquences de relations comme traits et définit la valeur de chaque trait par des marches aléatoires dans le GC. Des poids sur les traits sont appris par régression logistique pour chaque relation cible, et ensuite utilisés pour évaluer les triplets candidats. La méthode a démontré une amélioration par rapport à la génération de règles avec la PLI (Programmation Logique Inductive, Muggleton (1995)). AMIE (Galárraga et al., 2015) parvient à générer de telles règles de façon plus efficace en taillant des algorithmes de PLI sur mesure pour les GC. Ils introduisent aussi une nouvelle mesure de règles qui améliore la précision des inférences dans l'hypothèse du monde ouvert en vigueur dans les GC. Ces deux méthodes ont l'avantage de produire des explications intelligibles pour les inférences, contrairement aux traits latents. Cependant, elles requièrent une phase d'entraînement distincte pour chacune des centaines de relations cibles, tandis que les traits latents sont généralement appris conjointement en une seule phase.
Une différence clé de notre approche est qu'il n'y a pas de phase d'entraînement et que tout l'apprentissage est fait lors de l'inférence. Il s'agit donc d'une approche à base d'instances (ou de raisonnement par cas) plutôt qu'une approche à base de modèles. Étant donné un triplet incomplet (e i ,r k ,?) nous calculons des Concepts de plus proches voisins (CNN) à partir des traits observés de la tête e i , où les CNN ont une représentation équivalente aux corps des règles d'AMIE. À partir de là, nous inférons un classement d'entités candidates pour la queue de la relation r k . En fait, comme r k n'est pas utilisé dans le calcul des CNN, plusieurs relations cibles peuvent être inférées pour presque le même coût qu'une seule relation. En effet le coût principal réside dans le calcul des CNN, lequel est facilement contrôlé car l'algorithme est any-time.
Concepts de plus proches voisins (CNN)
Dans cette section, nous rappelons brièvement les définitions relatives aux CNN, ainsi que les aspects algorithmiques et pratiques du calcul de leur approximation dans un temps donné. Plus de détails sont disponibles dans des publications précédentes (Ferré, 2017(Ferré, , 2018.
Définitions théoriques
Un graphe de connaissance (CG) est défini par une structure K =E,R,T , où E est l'ensemble des noeuds, aussi appelés entités, R est l'ensemble des étiquettes d'arcs, aussi appelées relations, et T ⊆ E × R × E est l'ensemble des arcs orientés et étiquetés, aussi appelés triplets. Chaque triplet (e i ,r k ,e j ) représente le fait que la relation r k relie l'entité e i à l'entité e j . Un exemple de triplet est (France, capitale, Paris). Cette définition est très proche des graphes RDF, où les entités sont des URI ou des littéraux (ou des blank nodes) et les relations sont des URI appelées propriétés. Elle est aussi équivalente aux ensembles de faits logiques où les entités sont des constantes et les relations des prédicats binaires.
Les requêtes basés sur des motifs de graphe jouent un rôle central dans notre approche car elles sont utilisées pour caractériser les CNN, et peuvent être utilisées comme explications des inférences. Un motif de triplet (x,r,y) ∈ V ×R×V est un triplet avec des variables (prises dans V ) à la place des entités. Un filtre exprime une condition booléenne sur les variables. On ne considère ici que les égalités entre une variable et une entité : x = e. Les éléments de requêtes sont l'ensemble des motifs de triplet et des filtres qui peuvent être composés à partir des entités, relations et variables. Un motif de graphe P est un ensemble d'éléments de requêtes. Les égalités sont équivalentes à autoriser des entités dans les motifs de triplets mais ont deux avantages : (1) simplifier le traitement de motifs de triplet qui n'ont ainsi qu'une seule forme; (2) ouvrir des perspectives pour des filtres plus riches (ex., intervales de valeurs : x∈ [a,b]). Une requête Q=(x 1 ,...,x n )←P est la projection d'un motif de graphe sur un sous-ensemble de ses variables. De telles requêtes ont une forme concrète en SPARQL avec la syntaxe SELECT ?x1...?xn WHERE { graph pattern }. Les requêtes peuvent être vues comme des règles anonymes, c-à-d. des règles similaires à celles d'AMIE (Galárraga et al., 2015) mais sans la relation dans la tête de la règle. Par exemple, la requête (x,y) ← (x,parent,z),(z,sibling,y),(y,sex,w),w = female sélectionne tous les couples personne-tante, c-à-d. les paires (x,y) où y est une soeur d'un parent de x.
Nous définissons maintenant les réponses à une requête. Un matching d'un motif P sur le GC K = E,R,T est une fonction µ des variables de P vers des entités de E telle que µ(t) ∈ T pour tout motif de triplet t ∈ P et µ(f) s'évalue à vrai pour tout filtre f ∈ P , où µ(t) et µ(f) sont obtenus à partir de t et f en remplaçant chaque variable x par µ(x). Les réponses ans(Q,K) d'une requête Q = (x 1 ,...,x n ) ← P est l'ensemble des n-uplets (µ(x 1 ),...,µ(x n )) pour tout matching µ de P dans K. On notera que plusieurs matchings peuvent produire la même réponse et que les doublons sont ignorés. Dans la suite, nous considérons des requêtes avec une seule variable projetée, dont les réponses sont assimilées à un ensemble d'entités.
Dans Graph-FCA (Ferré, 2015), un concept de graphe est defini comme une paire C =(A,Q) telle que A = ans(Q) et Q est la requête la plus spécifique qui vérifie A = ans(Q). Cette requête la plus spécifique Q peut être calculée à partir de A avec le produit catégoriel de graphes (voir intersection PGP dans (Ferré, 2015)), ou avec l'anti-unification de Plotkin (1971)
Dans un précédent travail (Ferré, 2017), nous avons utilisé les concepts de graphe pour définir la distance conceptuelle entre deux entités e 1 et e 2 comme le plus petit concept δ(e 1 ,e 2 )=(A,Q) dont l'extension A contient à la fois e 1 et e 2 , et où l'intension Q=x←P est une requête qui caractérise ce que les deux entités ont en commun. Cela est bien défini car les concepts sont organisés en treillis (comme prouvé dans (Ferré, 2015)). Les "valeurs de distances" ont donc une représentation symbolique via l'intension de concept Q. Les distances conceptuelles sont organisées en ordre partiel plutôt qu'en ordre total, contrairement aux mesures de distance classiques. Une distance numérique dist(e 1 ,e 2 )=|ext(δ(e 1 ,e 2 ))| peut être dérivée de la taille de l'extension parce que plus e 1 et e 2 sont proches, plus leur distance conceptuelle est spécifique et plus l'extension est petite. De façon duale, une similarité numérique sim(e 1 ,e 2 )=|int(δ(e 1 ,e 2 ))| peut être dérivée de la taille de l'intension (nombre d'éléments de requête) parce que plus e 1 et e 2 sont similaires, plus leur distance conceptuelle est spécifique et plus l'intension est grande.
Partant d'une entité e dont on veut trouver des entités similaires, les distances conceptuelles entre e et chacune des autres entités induisent une partition de l'ensemble d'entités E, où deux entités e 1 ,e 2 appartiennent au même cluster si elles partagent la même distance conceptuelle, c-à-d. δ(e,e 1 ) = δ(e,e 2 ). Chaque cluster S l est représenté symboliquement par la distance conceptuelle partagée δ l . S l est un sous-ensemble de l'extension de δ l et est appelé l'extension propre de δ l : S l =proper(δ l )⊆ext(δ l ). Chaque paire (S l ,δ l ) est appelé un Concept de plus proches voisins (CNN, Concept of Nearest Neighbours) et nous notons CNN (e,K) leur collection pour un graphe K.
S. Ferré
Discussion. Comme CNN (e,K) est une partition de l'ensemble des entités, le nombre de CNN ne peut qu'être inférieur au nombre d'entités, et en pratique il est très inférieur. C'est intéressant parce qu'en comparaison le nombre de concepts de graphe est exponentiel dans le nombre d'entités dans le pire cas. L'espace de recherche des approches à base de PLI est l'ensemble des règles, lequel est encore plus large que l'ensemble des concepts de graphe. Calculer les CNN pour une entité donnée est donc une tâche nettement plus abordable que la fouille de règles, bien que l'espace de représentations soit le même. La question que nous commençons à étudier dans ce papier est de savoir si ces CNN sont utiles pour l'inférence et comment ils se comparent avec les autres approches.
Comparé aux mesures numériques utilisées dans les approches de plus proches voisins, les CNN définissent un ordre plus subtil sur les entités. Tout d'abord, parce que les distances conceptuelles sont seulement partiellement ordonnées, il se peut que parmi deux entités aucune ne soit plus similaire à e que l'autre. Cela reflète le fait qu'il peut y avoir plusieurs façons d'être similaire à quelque chose, sans qu'une soit nécessairement préférée à l'autre. Par exemple, laquelle est la plus similaire à une "grande vieille maison"? une "petite vieille maison" ou une "grande maison neuve"? Ensuite, il se peut que deux entités soit à exactement la même distance et donc soit indiscernables en terme de similarité. Enfin, l'intension de concept fournit une explication intelligible de la similarité avec e.
Aspects algorithmiques et pratiques
Nous esquissons ici les aspects algorithmiques et pratiques du calcul de CNN (e,K). Plus de détails sont disponibles dans (Ferré, 2018). Le principe de l'algorithme est de raffiner de façon itérative une partition de l'ensemble des entités, convergeant vers la partition induite par les extensions propres des CNN. Chaque cluster S l est associé à une requête Q l =x←P l et à une ensemble d'éléments de requête candidats H l . La correspondance avec les CNN est que lorsque H l est vide, (S l ,δ l ) où δ l = (ans(Q l ),Q l ) est un CNN, c-à-d. S l est l'extension propre d'un CNN dont la distance conceptuelle a pour intension Q l . Tant que H l n'est pas vide, S l est susceptible d'être l'union de plusieurs extensions propres (manque de discernement) et Q l n'est pas nécessairement la requête la plus spécifique qui matche toutes les entités de S l (manque de précision dans la similarité conceptuelle). Dans ce cas, on obtient une surestimation des distances conceptuelles pour certaines entités de S l .
Initialement, il y a un seul cluster S 0 = E où P 0 est le motif vide et H 0 est la description de e. La description d'une entité e est le motif de graphe obtenu en extrayant le sous-graphe autour de e et, pour toute entité e i dans le sous-graphe, en remplaçant e i par une variable y i et en ajoutant le filtre y i = e i . Ici, nous choisissons d'extraire le sous-graphe qui contient tous les arcs partant de e jusqu'à une certaine profondeur.
Ensuite à chaque itération, un cluster S de motif P et d'ensemble d'éléments candidats H est partagé en deux clusters S 1 ,S 0 en utilisant un élément h∈H comme trait discriminant. L'élément h doit être choisi de telle sorte que P ∪ {h} forme un motif connecté et contenant la variable x. Actuellement, cet élément est choisi de façon à faire un compromis entre exploration en profondeur et en largeur de la description de e, mais d'autres stratégies sont possibles. Les nouveaux clusters sont définis comme suit :
Les équations pour S 1 ,S 0 assurent qu'on a bien une partition après chaque partage. Les éventuels clusters vides (S i = ∅) sont ôtés de la partition. En conséquence, bien que l'espace de recherche soit l'ensemble des sous-graphes de la description de e, qui a une taille exponentielle dans la taille de la description, le nombre de clusters reste toujours inférieur au nombre d'entités. Discussion. L'algorithme ci-dessus termine car l'ensemble H décroit à chaque partage. Cependant, dans le cas de grandes descriptions ou de grands GC, le temps de calcul peut être trop long pour une utilisation au moment de l'inférence. Nous pouvons facilement contrôler ce temps d'exécution avec un timeout car l'algorithme est any-time. En effet, il peut produire à tout moment une partition des entités, avec une surestimation de la distance conceptuelle pour chaque cluster. Des expériences passées (Ferré, 2018) ont montré que l'algorithme a la bonne propriété de produire plus de la moitié des concepts dans une petite fraction du temps total de calcul.
En fait, l'algorithme converge vers une approximation des CNN, dans le sens où la distance conceptuelle peut encore être une surestimation pour certaines entités après termination. La raison vient de ce que les motifs de graphe sont contraints à être des sous-ensembles de la description de e. Afin d'avoir des résultats exacts, il faudrait autoriser la duplication des variables et de leurs arcs adjacents, ce qui augmenterait considérablement l'espace de recherche.
Des expériences sur des GC ayant jusqu'à un million de triplets ont montré que l'algorithme peut calculer tous les clusters pour des descriptions de centaines d'arcs en quelques secondes ou minutes. En comparaison, dans le même délai, les approches par relâchement de requêtes ne parviennent pas à faire plus de 3 relâchements, ce qui est largement insuffisant pour identifier des entités similaires dans la plupart des cas; et les approches calculant les similarités symboliques avec chaque entité ne passent pas à l'échelle des GC qui ont des dizaines de milliers de noeuds.
Prédiction de liens
Le problème de la prédiction de liens est d'inférer une entité manquante dans un triplet (e i ,r k ,e j ), c-à-d. soit d'inférer la queue à partir de la tête et de la relation, soit d'inférer la tête à partir de la queue et de la relation. Comme les deux problèmes sont symétriques, nous décrivons ici uniquement l'infé-rence de la queue. Dans la suite, nous considérons donc e i et r k comme étant fixés (ils n'apparaissent pas dans les indices) et e j comme étant variable. Notre approche de la prédiction de liens s'inspire du travail de Denoeux (1995) que nous adaptons aux concepts de plus proches voisins (CNN). Deoeux définit une règle de classification à partir des k-plus proches voisins en se basant sur la théorie de Dempster-Shafer (D-S). Chaque k-plus proche voisin x l d'une instance à classer x est utilisé comme support de l'appartenance de x à la classe c l de x l . Le degré de support est fonction de la distance entre x et x l , de telle sorte que le choix de k est bien moins sensible. La théorie D-S permet de fusionner les k supports en un support global, et ainsi de définir une mesure de croyance pour chaque classe.
Nous adaptons le travail de Denoeux à l'inférence de e j dans un triplet (e i , r k , e j ) comme suit. Étant donnée une partition des entités {(S l ,Q l ) avec Q l = x ← P l } l , comme approximation de CNN (e i ,K), chaque cluster (S l ,Q l ) est utilisé comme support à l'inférence de la queue e j relativement à la relation r k . Le degré de support dépend de la distance extensionnelle d l entre e i et les entités de S l , définie par
c-à-d. le nombre de réponses de la requête Q l , et de la confiance φ l,j de la règles d'association (x,r k ,e j )←P l , qui est définie par
S. Ferré
c-à-d. la proportion des entités réponses de Q l qui ont un r k -lien vers l'entité e j . Puisque dans un GC une entité-tête peut être liée à plusieurs entités-queues via la même relation, nous considérons un problème de classification distinct pour chaque entité-queue candidate e j ∈E, avec deux classes : c 1 j (e j est une entité-queue) et c 0 j (e j n'est pas une entité-queue). Pour chaque cluster S l et chaque entité candidate e j ∈E, le degré de support peut ainsi être formalisée en définissant une affectation de croyance de base m l,j sur les ensembles de classes.
représente le degré de croyance que e j soit une entité-queue, tandis que m l,j ({c 1 j ,c 0 j }) représente le degré d'incertitude. m l,j ({c 0 j }) est mis à 0 pour refléter l'hypothèse du monde ouvert des GC selon laquelle un fait manquant n'est pas considéré comme faux. La constante α 0 détermine le degré maximum de croyance, qui peut être inférieur à 1 pour refléter l'incertitude concernant la véracité des triplets existants. Le degré de croyance décroit de façon exponentielle avec la distance. Enfin, nous rendons ce degré proportionnel à la confiance de l'inférence de e j par Q l . Dans Denoeux (1995), ce facteur confiance n'existe pas car il vaudrait 1 pour la classe du plus proche voisin et 0 pour toute autre classe.
En appliquant la thérie D-S pour fusionner les supports venant de tous les clusters de notre partition {(S l ,Q l )} l , nous arrivons à l'équation suivante pour la croyance en chacune des entités candidates e j .
Bel j =Bel j ({c 1 j })=1−Π l (1−m l,j ({c 1 j })) À partir de la croyance en chaque entité e j , il est possible de classer les entités par croyance décroissante, ou bien de sélectionner un sous-ensemble d'entités en choisissant un seuil de croyance minimale. Ensuite, les classements d'entités peuvent être évalués par des mesures telles que Hits@N (la proportion des tâches d'inférences où l'entité correcte est parmi les N premières entités) et MRR (Mean Reciprocal Rank, la moyenne des inverses du rang de l'entité correcte). Les sélections d'entités peuvent être évaluées avec des mesures telles que précision, rappel et score F1.
On notera que la méthode ci-dessus se généralise facilement à l'inférence conjointe de la relation r k et de l'entité-queue e j . Il suffit d'utiliser les indices k,j partout où l'index j est utilisé : φ l,k,j serait la confiance dans l'inférence de la relation r k et de l'entité e j par Q l , c 1 k,j serait la classe des entités liées à e j via r k et Bel k,j serait la croyance en l'inférence d'un tel lien.
Évaluation
Méthodologie
La page "expériences" 1 fournit des liens vers le code source, les jeux de données et les logs. En complément, nous utilisons aussi le jeu de données Mondial (May, 1999) qui contient des informations géographiques mondiales. Comme la tâche considérée est d'inférer des relations entre entités, nous l'avons simplifié en enlevant les triplets contenant des nombres ou des dates, en enlevant les triplets de nommage des entités et en déréifiant les relations n-aires. Les triplets restant utilisent 20 relations et sont partagés en trois parties : train (7979 triplets), validation (778) et test (970).
Tâche. Nous suivons le même protocole qu'introduit par Bordes et al. (2013) et suivi par les travaux ultérieurs. La tâche consiste à inférer, pour chaque triplet test, l'entité-queue à partir de la tête et de la relation et aussi l'entité-tête à partir de la queue et de la relation. Nous appelons entité test l'entité connue et entité manquante l'entité à inférer. Nous évaluons la performance de notre approche en utilisant les mesures : MRR (Mean Reciprocal Rank), H@{1,3,10} (Hits@{1,3,10}). Comme dans les travaux existants nous utilisons les versions filtrées de ces mesures pour prendre en compte le fait qu'il peut y avoir plusieurs entités valides dans un classement d'entités. Par exemple, si l'entité correcte est au rang 7 mais que 2 des 6 premières entités forment des triplets valides (c-à-d. qui appartiennent au jeu de données), alors on considère que l'entité correcte est au rang 5.
Méthode. Comme notre approche n'a pas de phase d'entraînement, nous pouvons utiliser les deux parties train et validation comme exemples pour notre inférence. Notre approche a seulement deux paramètres pour le calcul des CNN : la profondeur de la description de l'entité test et le temps de calcul alloué (timeout). Nous étudions la sensibilité à ces deux paramètres. Pour l'inférence d'un classement d'entités, nous posons α 0 = 0.95 et utilisons tous les CNN calculés (pas de sélection des k-plus proches CNN). L'implémentation de notre approche a été intégrée à SEWELIS comme amélioration d'une précédente contribution sur l'édition guidée de graphes RDF (Hermann et al., 2012). Nous avons exécuté nos expériences sur une machine Fedora 25, avec un CPU Intel(R) Core(TM) i7-6600U @ 2.60GHz et une mémoire 16GB DDR4.
Baselines. Nous avons choisi la même tâche et les mêmes mesures que Schlichtkrull et al. (2018) de façon à pouvoir récupérer les résultats de plusieurs approches existantes à base de traits latents (LinkFeat, DistMult, CP, TransE, HolE, ComplEx, R-GCN), et les comparer avec notre approche à base de CNN. Nous nous comparons également à une approche à base de règles, AMIE, que nous avons exécuté avec ses paramètres par défaut. Comme suggéré par les auteurs d'AMIE (équation 8, Galárraga et al. (2015)), nous classons les entités e j en agrégeant les confiances φ l,j des règles R l permettant d'inférer le triplet (e i ,r k ,e j ) : φ j = 1 − Π l (1 − φ l,j ). Nous ajoutons également une méthode naïve, Freq, qui consiste simplement à classer les entités e j selon leur fréquence décroissante d'usage dans les triplets de relation r k , que l'on définit par freq j =|ans(x←(x,r k ,y),y =e j )|. Le classement obtenu est indépendant de l'entité test e i et agit donc comme un classement par défaut.
Résultats et discussion
Le tableau 1 compare les résultats de notre approche (CNN) aux autres approches. CNN est exécuté avec des timeouts compatibles avec une interaction utilisateur (0.01s,0.1s,1s) et une profondeur de description égale à 10, qui garantit de capturer à peu près tous les traits pertinents de l'entité test. CNN surpasse les performances des autres approches sur FB15k-237 dès 0.01s pour les mesures les plus fines (MRR, H@1, H@3) et dès 1s pour la mesure H@10. CNN-1s atteint un MRR de 0.286 comparé à 0.248 pour la meilleure autre approche, R-CGN, soit une marge de 3,8%. Cette marge est encore plus grande pour les mesures H@1 (6,2%) et H@3 (4,8% Nous étudions maintenant l'impact de la profondeur de description sur les résultats. On s'attend à priori à ce qu'une plus grande profondeur fournisse plus d'information mais soit plus coûteuse en calcul. Cependant, en variant la profondeur de 1 à 20, nous observons de très faibles écarts-types sur les quatre mesures, tous entre 0.003 et 0.007. Cela montre qu'une grande part de l'information utile se trouve déjà à la profondeur 1. De plus, c'est une bonne propriété que l'accroissement de la profondeur ne dététiore pas les résultats car cela signifie que cette profondeur peut être fixée à une valeur élevée sans problème. L'explication de cette propriété est que l'algorithme de partitionnement itératif commence avec les triplets peu profonds et se poursuit avec des triplets de profondeur croissante.
Le tableau 2 détaille les résultats de notre approche, en distingant la prédiction des queues et la prédictions des têtes. Il montre clairement qu'il est nettement plus facile de prédire les queues que les têtes. Ce n'est pas surprenant sachant que dans les GC, les relations sont généralement orientées dans le sens le plus déterministe. Par exemple, la relation entre films et genres est orientée des films vers les genres parce que chaque film a seulement un ou quelques genres alors que pour chaque genre il y a de nombreux films. Le tableau 3 détaille davantage les résultats pour une sélection de 10 des relations les plus fréquentes du jeu de données FB15k-237, en considérant seulement la prédiction des queues. Afin de donner une idée de la difficulté de ces prédictions pour chaque relation, nous donnons le nombre de têtes et queues uniques dans les parties train+valid, ainsi que le MRR de la baseline Freq. Les résultats montrent que le MRR augmente de façon significative avec notre approche pour toutes les relations, sauf type_of_union dont le MRR de la baseline est déjà très élevé (0.971). Par exemple, le MRR de profession augmente de 16,7% et celui de award augmente de 19%. Pour la moitié des relations Hits@1 est supérieur à 0.5, ce qui signifie que choisir la première entité du classement serait correct dans plus de 50% des cas. Cela inclut des relations avec un grand nombre de queues uniques et donc potentiellement difficiles à prédire, par exemple la relation nationality avec 100 queues uniques et H@1 = 0.668.
Exemples d'inférences et explications
Pour compléter cette évaluation, nous illustrons notre méthode d'inférence en inspectant quelques exemples en détail. Dans FB15k-237, la langue du film "Dragon Ball Z : Bojack Unbound" est correctement prédite comme étant "Japanese" avec MRR=1, comparé à MRR=0.2 pour Freq. CNN-1s génère 26 concepts, parmi lesquels la meilleure explication (en terme de croyance) est que le film est produit au Japon et a Toshiyuki Morikawa comme acteur. Le lieu de vie de "Tabu" est correctement prédit comme étant "Mumbai" avec MRR=1, comparé à MRR=0.059 pour Freq. CNN-1s génère 32 concepts, parmi lesquels la meilleure explication est que "Tabu" a reçu le prix "Filmfare Award for Best Actress", ce qui indique que un certain nombre de personnes ayant reçu ce prix vivent à Mumbai. Les prédictions suivantes dans le classement sont d'autres villes en Inde. Dans Mondial, le continent de la Suède est correctement prédit comme étant l'Europe car c'est un pays de type monarchie constitutionnelle (requête à 3 éléments), comme le Danemark. Matterhorn est correctement prédit comme étant localisé en Suisse car c'est une montagne des Alpes qui est aussi localisée en Italie



Introduction
Les documents et les ressources médicales connaissent, de plus en plus, une croissance exponentielle et des changements simultanés. Ceci complique la tâche des experts en médecine dans leur suivi des nouveautés médicales pour la prise de bonnes décisions. Ainsi, et dans le but d'automatiser le processus d'analyse de ces documents et les transformer en connaissances, les techniques de la fouille de textes et d'analyse sémantique sont fortement nécessaires. Noter que l'analyse sémantique est un processus permettant de produire une représentation formelle d'un texté ecrit en langage naturel (Kiryakov et al., 2004). Cette formalisation puissê etre utilisée par exemple dans le raisonnement et l'inférence oú egalement pour déterminer si deux textes sont en relation d'implication (Gyawali et al., 2017). Dans notre travail qui se place dans le cadre d'une coopération entre l'hôpital La Rabta de Tunis et le laboratoire SMART (Straegies for Modelling and ARtificial inTelligence), nous sommes intéressésintéressésà assister les médecins cardiologues dans la prise des décisions au niveau des traitements des patients qui souffrent d'une dissection aortique. Une dissection aortique est une déchirure partielle de la paroi de l'aorte pouvantàpouvantà tout momentévoluermomentévoluer vers la rupturecompì ete, avec comme conséquences le décès (Criado, 2011). Il s'agit donc d'une urgence absolue dans son diagnostic et dans sa prise en charge. Alors, afin d'´ elaborer le système d'aidè a la déci-sion, nous avons besoin, bien entendu, de modéliser les connaissances relativesàrelativesà cette maladie. Ces connaissances sont généralement décrites dans des documents spécialisés appelés les guides de bonnes pratiques. Elles sont constituées d'un ensemble de r` egles et de recommandations (ex. In patients with abdominal aortic diameter of 25-29 mm, new ultrasound imaging should be considered 4 years later ). L'objectif de cet articléarticlé etant donc de présenter notamment notre travail d'analyse sémantique et la transformation des recommandations en r` egles SWRL (semantic web rule language). Ex.
lessT han(?d, 29)− > recommendedDiagnosis(?p, "ultrasounImaging")). SWRL est un langage de r` egles intégré directement dans OWL (Ontology Web Language). Il permet de définir des r` egles sous forme d'implications logiques entre conditions et conclusions. Nous pensons que l'utilisation des techniques de web sémantique et des mécanismes d'inférence des ontologies pourraitêtrepourraitêtre une bonne aide pour bienélaborerbienélaborer notre système d'assistance médical. Ainsi, afin d'analyser le texte, nous avons utilisé une ontologie existante du domaine cardiovasculaire et des outils de traitement du langage naturel. En effet, l'analyse sémantique des textes a ´ eté bienétudiéebienétudiée dans la littérature [ (Gyawali et al., 2017), (Petrucci et al., 2016)]. Cependant, l'ensemble de ces travaux se sont focalisés sur une transformation en formules logiques simples ou en des ontologies légères. Nous nous intéressons plutôtplutôtà produire des r` egles SWRL.
Le reste de l'article sera comme suit : la section 2 présente quelques travaux connexes. La section 3 détaille notre approche proposée. La section 4 discute les résultats obtenus. La section 5 conclut ce travail et donne quelques perspectives.
´ Etat de l'art
L'extraction des connaissancesàconnaissancesà partir du texte s'est trouvée au coeur de plusieurs domaines de recherche vu l'augmentation vertigineuse, d'année en année, du volume de données disponibles sous forme de corpus de textes (Upadhyay et Fujii, 2016;Ristoski et Paulheim, 2016). Uné etude comparative des approches et d'outils d'extraction des connaissances est présentée dans (Gangemi, 2013). Nous nous intéressons dans cette sectionàsectionà présenter essentiellement des travaux qui ontétudiéontétudié la transformation du texte en formules logiques (analyse sémantique). (Park et Lee, 2007) ont proposé une méthode semi-automatique pour extraire des r` egles de documents Web. Leur méthode nécessite, en entrée, une ontologie de domaine existante et une sélection manuelle des pages Web pertinentes. L'approche n'utilise que des techniques simples de TAL, ce qui l'empêche de gérer des textes complexes. (Gangemi et al., 2017) ont proposé un système appelé FRED qui extrait les relations n-aires sur la base de structures de représentation du discours et les transforme ensuite en représentation RDF (Resource Description Framework). Une méthode présentée par (Petrucci et al., 2016), permettant de convertir des textes sous forme d'axiome de logique de description. (Gyawali et al., 2017) ont proposé une méthode qui convertit des documents techniques d'Airbus en des axiomes DLàDLà l'aide des grammaires d'arbres adjoints.
Dans notre travail, nous nous intéressons non seulementàseulementà la transformation de texte en logique de description (axiomes OWL) maiségalementàmaiségalementmaiségalementà la génération automatique de r` egles SWRL. Nous pensons que l'analyse d'un texte composé d'un ensemble de recommandations sous forme de r` egles SWRL est plus utile pourélaborerpourélaborer notre système d'aidè a la décision.
Approche proposée
Notre approche d'analyse sémantique des recommandations médicales est constituée de trois grandesétapesgrandesétapes détaillées ci-dessous.
1. Pré-traitement. Le pré-traitement de notre texte repose sur les méthodes de tokenisation, la lemmatisation, l'analyse syntaxique, etc. Cette phase visè a nettoyer notre corpus de texte des mots inutiles (de bruit). Notre corpus (voir tableau 1) est composé d'un ensemble de recommandations médicales définies par la Société européenne de cardiologie ESC.
In complicated Type B AD, surgery may be considered. In patients with acute contained rupture of TAA, urgent repair is recommended.
In complicated Type B AD, TEVAR is recommended.
Tab. 1 -Quelques r` egles du corpus.
2. Annotation sémantique. Il s'agit d'annoter l'ensemble de recommandations par l'ontologie de domaine cardiovasculaire CVDO 1 . Le choix de l'ontologie est fait après une recherche sur les ontologies existantes (essentiellement via le portail bioportal 2 ) qui nous a amenéamenéà conclure que l'ontologie CVDO est la plus adaptéè a nos besoins et couvre le plus notre domainé etudié (maladies cardiovasculaires). Ceci est dit, l'ontologie CVDO manque de relations et sera par la suite enrichie par le résultat de l'analyse sémantique. Ainsi, le processus d'annotation sémantique prend le texte pré-traité et l'ontologie CVDO, puis essaie de trouver les correspondances entre eux. Chaque terme du texte peutêtrepeutêtre associéassociéà un ou plusieurs concepts de l'ontologie (classes, propriétés d'objet, propriétés de données, etc.). Dans notre travail, nous nous sommes intéressésintéressésà trois types de correspondances (Mahfoudh et al., 2016) : 1) correspondance syntaxique en utilisant la distance Levenshtein ; 2) correspondance morphologique en utilisant la lemmtatisation et 3) correspondance sémantique en utilisant l'ontologie WordNet.
3. Analyse sémantique. Pour la transformation du texte en r` egles SWRL, nous avons défini un ensemble de patrons selon la nature de la recommandation médicale. La transformation est basée sur le résultat de l'annotation sémantique. Nous identifions pour chaque ligne de recommandation le pattern ayant la même structure et les classer ensemble. Un exemple de pattern est présenté ci dessous.
Patron1 : r` egles avec expressions de classe.
if the patient has a disease related to heart disease.  Ensuite, il faut générer l'arbre correspondante de la recommandation qui sera automatiquement convertie en r` egle SWRL en cherchant sa correspondance avec les patrons prédéfinis (figure 2).
Expérimentations
Afin d'´ etablir une correspondance entre l'ontologie et le texte, nous avons déve-loppé un prototype avec le langage Java. Les correspondances morphologiques et la lemmatisation sont assurées par l'API Standford CoreNLP. La bibliothèque NLTK de Python est utilisée pour le pré-traitement du texte. Nous avons obtenu 30% de correspondance entre l'ontologie CVDO et les textes de recommandations médicales. Ceci refì ete que l'ontologie CVDO couvre une partie de texte mais elle manque aussi d'autres qui doiventêtredoiventêtre ajoutésajoutésà la fin de l'analyse sémantique. Pour la construction des patterns et la transformation des r` egles, nous avons utilisé le framework Jena et l'API SWRLAPI. 60 % des r` egles SWRL sont correctement extraites du corpus de recommandations. Dans certains cas, une recommandation peut correspondrè a un ou deux patterns simultanément, ce qui a réduit le taux de précision.
Conclusion
Nous avons présenté, dans cet article, une méthode d'analyse sémantique des recommandations médicales avec une ontologie de domaine. Notre approche consistè a transformer ces recommandations en r` egles SWRL. L'analyse sémantique est menée par une annotation sémantique du texte de recommandation par les concepts de l'ontologie. Nous avons combiné l'utilisation de l'ontologie OWL et des techniques de TAL pour capturer des connaissances et les représenter sous forme de r` egles dans le langage SWRL.
Nous rappelons que notre objectifétaitobjectifétait d'´ elaborer un système d'aide médicale permettant aux médecins de prendre des décisions concernant les maladies aortiques. Nous prévoyons, dans un futur travail, d'incorporer le résultat de l'analyse sémantique dans l'ontologie CVDO afin de l'enrichir et l'exploiter dans des tâches d'inférence. Nous nous baserons sur un travail précédent sur l'enrichissement et le peuplement d'ontologie (Mahfoudh et al., 2015). Une fois enrichie, il faut interroger cette ontologie afin de répondre, via des interfaces IHM adaptées, aux requêtes des médecins et leur assister dans leur prise de décision.

Introduction
La reconnaissance des entités nommées (REN) dans un texte est une tâche consistant à repérer des éléments textuels et à les classer dans des catégories/types prédéfinies (i.e. noms de personnes, d'organisations, de marques, d'équipes sportives, etc.). La REN est souvent considérée comme l'une des briques de fondation des systèmes visant à structurer un texte tout-venant. Cette brique est généralement conçue de façon indépendante des autres briques du Traitement Automatique du Langage Naturel (TALN) et s'inscrit dans un flot d'exécution linéaire : les briques situées après la REN ne peuvent plus intervenir sur la reconnaissance des entités nommées, et celles situées avant ne peuvent bénéficier des résultats de la REN. Par exemple, la résolution des coréférences et l'extraction des relations sont généralement situées après la REN ce qui impose de facto des limites aux systèmes. Soient trois exemples :
(1) Paris visite Paris.
(2) Paris est triste. Elle pleure dans le salon.
(3) Firadsicht est une ville.
Dans l'exemple (1), les systèmes académiques et industriels testés 1 reconnaissent les deux mentions de "Paris" comme des lieux. Pourtant, l'application d'un module d'extraction de relations permettrait de construire le triplet "Paris_lieu, rencontrer, Paris_lieu" et de mettre en évidence l'inconsistance de l'annotation (un lieu ne peut rencontrer un lieu) pour conduire le système à réviser les solutions proposées. Dans l'exemple (2), les systèmes annotent "Paris" comme un lieu. La résolution de la coréférence "Elle" (i.e. "Paris") conduirait le système à mettre en évidence que la ville ne peut pas "pleurer dans le salon", conduisant ainsi le système à revoir son annotation. L'exemple 3 met en évidence les limites des systèmes dès lors que l'entité à annoter est absente des ressources dictionnairiques ("Firadsicht" dans l'exemple n'est pas reconnue). Un constat plus global est que les briques élémentaires à la compréhension du langage sont aujourd'hui encore (trop) cloisonnées tel que récemment souligné par Cartier (2015).
Nous nous focalisons dans ce qui suit sur une approche de REN symbolique (à base de règles). Les systèmes existants sont confrontés à plusieurs contraintes, en sus des contraintes d'ordre analytique présentés ci-avant. Tout d'abord, ils reposent généralement sur un ensemble de règles pour lesquelles la priorité d'application est cruciale , impactant directement la qualité des résultats. Ensuite, les systèmes symboliques actuels s'appuient gé-néralement sur des ressources de grand volume qu'il faut alors développer ou enrichir (Sagot et Stern, 2012) (Nouvel et al., 2012) (Nouvel et al., 2016). Enfin, le texte à analyser est géné-ralement traité comme une suite de phrases indépendantes, ce qui réduit considérablement le contexte mis à disposition du système.
Nous proposons une façon de décloisonner à la fois les briques d'analyse du TALN et les phrases en mettant en place une approche itérative pour la reconnaissance d'entités nommées : chaque brique du TALN sollicitée lors d'une itération apporte à la REN de nouveaux éléments de contexte qui la mènent à des solutions plus pertinentes. Ces nouvelles solutions impliquent potentiellement les briques à réviser les éléments précédemment fournis, et ainsi de suite. Par ailleurs, le système tisse des liens entre les phrases grâce à la résolution des coréférences dans le but de propager à travers le texte les solutions identifiées par le système. Il s'ensuit que les types d'entités proposés par le système demeurent des candidats jusqu'à la dernière itération. Dans la suite, nous décrivons le système (section 2) et présentons les résultats de l'évaluation (section 3).
Description du système 2.1 Quelle typologie ?
Même si, dans la littérature, des centaines de types d'entités nommées ont été définies (Sekine et Nobata, 2004), les campagnes d'évaluation ou shared tasks les plus récentes ainsi que les solutions industrielles n'en utilisent que quelques dizaines tout au plus, selon les besoins 1. Une quinzaine au total, y compris pour l'anglais, dont : Google, Alchemy (IBM), Gate (Tablan et al., 2013), SEM (Dupont et Tellier, 2014), NERC-fr (Azpeitia et al., 2014), Polyglot-Ner (Al-Rfou et al., 2015), AllenNLP (Gardner et al., 2018) applicatifs. Dans notre travail, nous utilisons l'ontologie NERD 2 qui propose un consensus entre les types des systèmes les plus populaires (Rizzo et Troncy, 2012). L'intérêt d'utiliser une telle ontologie réside notamment dans le fait que les solutions retournées par le système permettent d'inférer de nouvelles solutions par la relation de subsomption. Ainsi, une entité typée nerd:SportsTeam (équipe de sport) est nécessairement typée nerd:Organization (organisation). L'enjeu est donc de faire en sorte que le système retourne les types les plus spécifiques afin d'inférer, selon le cas d'application visé, des types plus génériques. Inversement, à partir d'un type identifié par le système (par exemple nerd:Organization), celui-ci peut être guidé par les types spécifiques possibles pour affiner la solution proposée (par exemple nerd:Airline, nerd:Band ou encore nerd:Company). Ce dernier cas n'est pas exploité par le système actuel et constitue une perspective à nos travaux.
Dans le cadre de ce travail, nous avons développé une extension à NERD en ajoutant des classes telles que les mesures, les noms de méthodes/théories, les récompenses, les lignes de transport, etc 3 . Au total, le système dispose de 115 classes et sous-classes.
Algorithmique
Le système est constitué de six phases principales dont les deux dernières sont itératives (cf. Fig. 1). Le texte est d'abord soumis à une analyse syntaxique (phase 1) qui fournit les dé-pendances entre chaque mot (token) de la phrase (par exemple les dépendances sujet et objet). La structure obtenue est soumise à une série d'expressions régulières permettant le repérage des mesures, e-mails, URLs, numéros de téléphone, etc. (phase 2). Les expressions temporelles sont ensuite reconnues dans la phase 3. La phase 4 projette les éléments du lexique de contexte-clés permettant de repérer des éléments de contexte en vue de la désambiguïsation des entités nommées. Par exemple, des termes tels que "société", "organisation", "entreprise" sont des éléments clés pour l'identification des organisations. La ressource lexicosémantique JeuxDeMots (Lafourcade et Joubert, 2008) a été utilisée pour construire de tels lexiques 4 . Les termes repérés sont des couleurs, des ingrédients, des métiers, des matériaux, des instruments de musiques, des nationalités, des sports, etc. La phase 4 projette également le lexique des entités nommées ce qui permet de générer les premiers candidats. La phase 5 correspond à l'application des règles en n itérations. Une fois les n itérations effectuées, la phase 6 intervient (une unique fois) dans le but d'augmenter la couverture en proposant de nouveaux candidats. La base de connaissances DBpedia est utilisée à titre expérimental : par l'extraction de n-grammes de mots, des mentions exactes et partiellement exactes sont recherchées dans la base pour augmenter la couverture du système. Il s'avère que les types fournis par la base (via la propriété rdf:type) sont représentés par des classes décrites dans plusieurs ontologies. Nous avons donc projeté ces classes sur celles de l'ontologie NERD en exploitant la propriété sameAs et les valeurs associées fournies par NERD 5 . Par exemple, dbo:Place, wm:LOC, ou encore ner2:location sont projetées sur nerd:Location. La phase 5 est ensuite à nouveau amorcée car de nouveaux éléments issus de la phase 6 pourraient impliquer le dé-2. http://nerd.eurecom.fr/ontology/ 3. Cette extension sera publiée prochainement. 4. http://www.jeuxdemots.org. Nous utilisons la relation de synonymie. 5. De nombreuses équivalences de classes ont été ajoutées manuellement pour couvrir les ontologies utilisées par DBpedia.
FIG. 1 -Schéma de l'approche
clenchement de nouvelles règles. Au bout de n itérations, le système fournit alors les résultats définitifs.
Le système exploite de nombreux éléments de contexte, y compris ceux qui pourraient être éloignés de l'entité à catégoriser, en couvrant plusieurs tâches du TALN dont l'extraction de la terminologie, l'extraction de relations et la résolution de la coréférence. Nous faisons l'hypothèse que, même si ces tâches menées de façon indépendante ne sont pas entièrement résolues, leurs traitements partiels mais combinés apportent des indices pertinents conduisant un système de REN à de meilleures performances. Ces traitements sont sollicités, sous forme de règles, lors de la phase 5 :
-Acronymes. Ce module identifie les acronymes entièrement en capitales par un ensemble d'heuristiques. Chaque acronyme est associé à sa forme complète lorsqu'elle existe dans le texte. Par exemple, dans "L'Organisation des Nations Unies (ONU) attire notre attention.", le sigle ONU est lié à "Organisation des Nations Unies" ; tous deux reçoivent les mêmes types candidats. -Complétion. Ce module s'appuie sur les informations d'ordre morphosyntaxique pour compléter une entité nommée (notamment avec les adjectifs, noms communs et noms propres) qui serait partiellement identifiée. On évite ainsi que dans le nom du personnage "La Panthère rose", seul "La Panthère" soit reconnu. -Coordination. La coordination exprime généralement une relation entre plusieurs élé-ments de même nature. Par exemple, dans "J'ai visité Lisbonne, Ajouda et Benfica.", le fait de connaître le type de Lisbonne (i.e. lieu) permet au système de proposer le même type pour les deux autres entités : Ajouda et Benfica. -Coréférences. L'analyse des coréférences est actuellement limitée aux coréférences pronominales, basée sur la sortie d'un analyseur syntaxique. Par exemple, dans "Paris est triste. Elle pleure dans le salon.", "Elle" et "Paris" sont liés par un lien d'identité. -Descripteurs. Les descripteurs, ou marqueurs, sont des mot-outils qui peuvent aider à désambiguïser une entité nommée. Par exemple, "à" et "en" sont plus probablement placés avant un lieu qu'avant la mention d'une équipe de sport. -Contexte gauche et droit. Les relations de dépendances permettent d'identifier le contexte gauche et droit de l'entité, y compris lorsque les éléments textuels sont distants de l'entité nommée. Par exemple, dans "J'ai visité la ville, celle que j'avais toujours rêvé de visiter, Benfica.", "Benfica" est directement lié à "ville". -Preuve interne (McDonald, 1996). Une entité nommée est parfois constituée d'élé-ments permettant de déduire son type. Par exemple, "Association Valentin Haüy" et "Fédération Française de Football" contiennent "Association" et "Fédération" qui permettent au système de proposer le type candidat "organisation".
-Appartenance (Lopez et al., 2014). Ce module exploite l'expression d'appartenance entre deux entités nommées pour déduire une relation hiérarchique entre elles. Par exemple, dans "J'ai testé Revitalift de L'Oréal Paris.", sachant que "L'Oréal Paris" est une marque, la relation d'appartenance exprimée par la préposition "de" indique que "Revitalift" appartient à la marque et est probablement un produit. -Relations. Ce module a pour objectif d'extraire des triplets de la forme "sujet, prédicat, objet". Par exemple, dans "Lilwenn est président du groupe.", le triplet "Lilwenn, être, président_Function" est généré. Ce triplet est soumis à une base de connaissances (gé-nérée à partir de JeuxDeMots) contenant le triplet "Person, être, Function" qui permet de déduire que le sujet est ici une personne. Les triplets sont générés en exploitant le résultat de l'analyse syntaxique (en particulier les relations sujet et objet, attribut du sujet, etc.), évitant ainsi le traitement proposé par Ezzat (2014) qui consiste à analyser les éléments textuels situés entre le prédicat et ses arguments. 
Sélection des candidats
Un candidat contient principalement trois données : le type de l'entité concernée, la règle ayant conduit à ce type, l'URI de l'entité concernée dans le cas de l'utilisation de DBpedia au cours de la phase 6. Un intérêt de l'approche proposée est de conserver les candidats pour une sélection définitive la plus tardive. À l'issue de chaque itération (phase 5), pour chaque 6. Grâce au triplet "Personne, téléphoner à, Personne" entité nommée, un candidat est sélectionné tout en préservant les autres candidats. Si au cours du processus le candidat sélectionné est confronté à une inconsistance, celui-ci est remplacé par un candidat plus adapté. Pour ce faire, chaque règle est associée à un score de confiance déterminé empiriquement 7 . Par exemple, les candidats générés par le module descripteurs ont un score plus bas que les règles fondées sur le contexte gauche immédiat. Pour une entité donnée, si plusieurs candidats existent et proviennent de différentes règles, une agrégation des scores (somme) est calculée pour chaque type. Le candidat de meilleur score est retourné.
Exemple d'application
Nous décrivons pas à pas les phases du système à partir d'un exemple concret (4)  -"Chevy Chase" : candidat "lieu", par déclenchement des règles suivantes : descripteurs ("à Chevy Chase"), coordination contenant un élément typé ("Chevy Chase, Maryland, États-Unis") -"Maryland" : même cas que "Chevy Chase". -"Bowman" : candidat "personne>sportif" par déclenchement de la règle comparaison ("Bowman [...] comme lanceur_sportif "). -"il" : réfère à "Matt Bowman" et par extraction de relation fait partie du triplet "il, jouer, match" qui permet de déduire que "il" est un sportif, donc "Matt Bowman" est un sportif. -"Cardinals" : candidat "équipe de sport" grâce au contexte gauche "matchs des". -Phase 5 : Deuxième itération -"Matt Bowman" : candidat "personne>sportif" par propagation de la coréférence "il" résolue dans l'itération précédente ; également candidat par présence de la preuve interne "Bowman" (annoté lors de l'itération précédente). -"Mets" : candidat "équipe sportive" par extraction du triplet "Matt Bowman_sportif, choisi par, Mets". 
Évaluation
Dès l'apparition de la tâche de reconnaissance d'entités nommées, il a été démontré et largement accepté qu'une grande quantité de ressources constituait les fondations d'un système de REN (Wakao et al., 1996). Dans le contexte industriel, il a souvent été noté que le dévelop-pement des ressources linguistiques pour la tâche de REN a un coût non négligeable (Ezzat, 2014). De fait, notre système limite les dépendances avec de telles ressources. Concrètement, hors DBpedia, le système dispose de ressources contenant seulement 26 000 entités nommées (dont 22 000 prénoms), de 1 500 termes (contexte-clé), et de 250 triplets (exemple d'un triplet : "Person, épouser, Person").
Les outils utilisés dans le cadre de cette évaluation sont à l'état de l'art : -l'analyseur syntaxique Talismane (Urieli, 2013) -l'analyseur d'expressions temporelles HeidelTime (Strötgen et Gertz, 2010)  ce corpus a été annoté manuellement en adoptant le format CoNLL et l'encodage BIO et est accessible sur Internet 9 . Il contient 3 125 entités nommées annotées avec les types de l'ontologie NERD. Ce corpus est noté Wikipedia-test dans la suite. Il n'a pas été utilisé lors du développement du système. Le tableau 1 donne un aperçu de la constitution des corpus de test. La colonne T montre le nombre de tokens annotés pour chaque type, la colonne TU donne le nombre de tokens uniques. Enfin, la colonne TA donne le nombre de tokens ambigus pour chaque type (i.e. le nombre de tokens ayant au moins deux types différents).
Dans un premier temps, nous avons expérimenté l'apport de la phase 6 (DBpedia) sur le corpus Wikipedia-train. Les résultats obtenus montrent que l'apport de cette phase, qui in-8. http://tln.lifat.univ-tours.fr/Tln_Corpus80jours.html 9. https://www.emvista.com
FIG. 2 -Impact du nombre d'itérations sur Wikipedia-train
tervient en fin de processus, est quasiment nul. Cela s'explique par le fait que l'analyse du contexte suffit à catégoriser les entités nommées : plus de 98% des entités ont été catégori-sées grâce au contexte et non grâce à leur présence dans une base de données. Par conséquent, la phase 6 a été abandonnée dans la suite de l'expérimentation d'autant plus que celle-ci est coûteuse en temps d'exécution. Néanmoins, la suppression de cette phase ne doit pas être géné-ralisée : par exemple, dans le cas des SMS et des tweets, le contexte est très limité et la syntaxe n'est pas standard : appliquer cette approche à de tels genres textuels ne serait pas pertinent.
Dans un deuxième temps, nous avons expérimenté l'impact du nombre d'itérations sur les résultats en utilisant le corpus Wikipedia-train. La Fig. 2 met en évidence que plusieurs itérations augmentent le F-score du système de façon significative. Sur ce corpus, une stabilité apparaît à partir de la quatrième itération. Nous montrons ainsi que le système est en mesure de proposer de nouvelles solutions à chaque itération. Une perspective à ce travail consiste à définir le cas d'arrêt optimal, automatiquement. Dans la suite de l'expérience, nous avons fixé un nombre d'itérations égal à 6.
Enfin, une expérience a consisté à attribuer le même score à chaque module de règles. Sur le corpus Wikipedia-train, la performance du système est diminuée de moitié ce qui montre la pertinence des scores attribués empiriquement. Une perspective consiste à expérimenter plus finement ces scores qui ont un impact immédiat sur les résultats.
Nous avons comparé les résultats de sept systèmes industriels et académiques sur Wikipediatest et 80 jours : Google, Alchemy (IBM), Gate (Tablan et al., 2013), NERC-fr (Azpeitia et al., 2014), SEM (Dupont et Tellier, 2014), mXS  et notre système symbolique noté "Emvista". Les mesures classiques de précision (P), rappel (R) et F-score (F) sont utilisées. Le tableau de synthèse (cf. Tab. 4) présente les résultats en termes de micro mesures afin de tenir compte du déséquilibre des classes.
Sur le corpus "80 jours", le tableau 2 donne la première et la deuxième places (en terme de F-score) à Google et à Emvista. Avec un F-score global de 0,76, notre système obtient la première place (cf. Tab. 4). Le même scénario est observé sur le corpus . Notre système obtient le meilleur F-score global (0,81), proche du deuxième (0,80).
Il est intéressant de remarquer que, malgré la différence stylistique entre les deux corpus, le système d'Emvista est le plus robuste en ce sens qu'il ne montre que 0,05 point de différence entre les deux corpus (cf. Tab. 4). Cela peut s'expliquer par le fait qu'il est plus autonome vis-à-vis des ressources que les autres systèmes. Enfin, les types de situations induisant le système en erreur sont majoritairement 1) les problèmes de frontières à gauche et à droite des entités nommées, 2) les erreurs de l'analyse syntaxique qui impacte directement la qualité du NER, 3) les erreurs relatives à la résolution des coréférences.
jours
Wikipedia  
Conclusion
Nous avons expérimenté un système visant à compenser l'utilisation d'une très faible quantité de ressources par une analyse du contexte faisant intervenir différentes briques du TALN au-delà des frontières imposées par les phrases : gestion de la coordination, extraction de relations, identification des acronymes, analyse des coréférences, etc. Le point essentiel du système réside dans le fait qu'il repose sur un flot d'exécution itératif, évitant ainsi de fixer un ordre d'exécution des différentes briques et règles. À chaque itération, le système est en mesure de Une perspective immédiate à ce travail est l'amélioration des briques de TALN, particuliè-rement la brique de résolution des coréférences de sorte à tisser plus de liens entre les phrases. Il va de soi que cette brique est dépendante des résultats de la REN et d'extraction des relations, entre autres. De fait, nous développons un système itératif couvrant l'ensemble des briques du Traitement Automatique du Langage Naturel, dans le but d'expérimenter le décloisonnement total des différentes tâches du TALN. 
Summary
Named entity recognition (NER) seeks to locate and classify named entities into predefined categories (persons, organizations, brandnames, sports teams, etc.). NER is often considered as one of the main modules designed to structure a text. In this article, we describe our symbolic system which is characterized by 1) the use of limited resources, and 2) the embedding of results from other modules such as coreference resolution and relation extraction. The system is based on the output of a dependency parser that adopts an iterative execution flow that embeds results from other analysis blocks. At each iteration, candidate categories are generated and are all considered in subsequent iterations. The advantage of such a system is to select the best candidate only at the end of the process in order to take into account all the elements provided by the different modules. The system is compared to academic and industrial systems.

Introduction
Au sein du groupe SNCF, la documentation métier est aujourd'hui en pleine mutation, avec des métiers qui se digitalisent, plus mobiles et marqués par de nouveaux modes de consommation de l'information. Dans le cadre du programme PRISME de transformation en matière de sécurité ferroviaire, SNCF cherche à simplifier l'accès à l'information et la production de contenus dans la documentation métier. L'évaluation de nouveaux systèmes intelligents d'accès aux contenus, d'aide à l'interprétation et à la saisie participe à cette démarche.
Dans ce contexte, nous avons mis en oeuvre des traitements sur un corpus de référentiels métiers SNCF pour répondre à deux objectifs. Le premier est celui de guider l'utilisateur dans sa recherche documentaire à travers la structuration des résultats de recherche, en regroupant en thématiques (classification non supervisée) les documents retournés en réponse à une requête. Le second objectif est d'aider les rédacteurs à qualifier automatiquement de nouveaux documents selon des thèmes définis dans une arborescence construite par les experts métiers (classification supervisée). Les enjeux dans notre contexte d'application industrielle sont de disposer de systèmes performants (réponse rapide et pertinente), maîtrisés (résultats interpré-tables et explicables) et adaptés aux besoins des utilisateurs de ces systèmes (pertinence des descripteurs par rapport aux connaissances de l'utilisateur). Dans le cas des deux objectifs pré-sentés, et dans ce contexte applicatif, le choix de la représentation des contenus (descripteurs lexicaux) constitue une étape préalable à l'application d'algorithmes d'apprentissage.
Les travaux présentés ici évaluent l'usage des plongements lexicaux (word embeddings) comme moyen de représenter les contenus. Ils abordent en particulier la question de l'apprentissage et de l'évaluation de ces plongements sur un corpus en langue française et spécialisée. Si nous nous intéressons à ces plongements, c'est tout d'abord parce qu'ils aboutissent à des représentations des contenus en faible dimension, ce qui permet d'accélérer les traitements et donc de proposer des réponses rapides à l'utilisateur. Par ailleurs, cette efficacité en calcul ne se fait pas au détriment de la qualité des résultats puisque ces vecteurs encapsulent une information sémantique riche contrairement aux représentations creuses dites one-hot.
Nous décrirons Section 2 les spécificités du corpus sur lequel nous travaillons et verrons que ces particularités ont donné lieu à des pré-traitements adaptés. Ensuite, nous détaillerons Section 3 la méthode d'apprentissage utilisée pour apprendre les plongements lexicaux. Nous discuterons Section 4 le protocole d'évaluation ainsi que les premiers résultats. Enfin, nous ouvrirons la discussion en considérant la polysémie des mots du vocabulaire spécialisé.
Corpus et pré-traitements
Les données sont constituées de 7029 textes, avec un contenu technique relatif à la sécurité, à l'exploitation et à l'utilisation du réseau ferroviaire. À cette base documentaire s'ajoutent un lexique ferroviaire et une base d'acronymes fournis par la SNCF.
Initialement au format pdf, le corpus a été converti au format txt et nettoyé (élimination d'erreurs liées à des problèmes de conversion, suppression de métadescripteurs des documents). Plusieurs pré-traitements ont ensuite été appliqués au corpus pour obtenir une repré-sentation vectorielle robuste du contenu des documents. Le contenu a été découpé en unités lexicales selon les standards d'Unitex. Le corpus a ensuite été lemmatisé par une version du Lefff (Lexique des Formes Fléchies du Français, (Sagot (2010)) adaptée pour prendre en compte certaines spécificités du corpus SNCF : les problématiques SNCF étant étroitement liées à un ancrage territorial, les noms de communes françaises ont été ajoutés ; des variantes de graphie récurrentes dans le corpus (même mot avec ou sans accent, ou graphie tantôt avec oe ou avec oe) ont également été ajoutées. Le lexique a ensuite été filtré, pour conserver uniquement les lemmes de type noms propres, noms communs, verbes, adverbes et adjectifs, et les termes répertoriés dans les ressources lexicales fournies par SNCF (lexique et acronymes).
À l'issue des pré-traitements, la taille du vocabulaire est de 18k mots et la taille du corpus de l'ordre de 10 7 . Ces pré-traitements ont été appliqués itérativement de façon à améliorer la qualité des embeddings, notamment en ce qui concerne le vocabulaire très spécifique du corpus (voir Section 4.2). Empiriquement, nous avons constaté une réduction du bruit dans les résultats de l'apprentissage que nous décrivons à la section suivante.
3 Plongements lexicaux L'objectif est d'apprendre des vecteurs denses pour représenter notre vocabulaire : un espace de représentation des contenus de dimension réduite permet des temps de réponse plus rapide qu'avec une matrice creuse de grande dimension, et les résultats de classification et de clustering bénéficient de l'utilisation des plongements (Kim (2014)). Dans notre cas, le corpus est de petite taille en comparaison des corpus utilisés pour l'apprentissage de plongements lexicaux à l'état de l'art (Mikolov et al. (2013)). Nous privilégions donc une approche basée sur la décomposition en valeur singulière (SVD) et décrite par Levy et al. (2015). Celle-ci est efficace sur de petits corpus et ses performances sont comparables à l'état de l'art.
Dans un premier temps, nous créons la matrice de cooccurrence termes-termes en utilisant une taille de fenêtre contextuelle de 5 mots. La matrice ainsi générée est une matrice symétrique creuse, de dimension égale à la taille du vocabulaire. Dans un second temps, les fréquences de cooccurrence de chaque paire de termes sont pondérées de façon à refléter leur significativité. Pour ce faire, nous calculons une variante de l'information mutuelle, la PPMI. Enfin, nous procédons à une réduction de dimension par application de la SVD. La taille du nouvel espace de représentation est un paramètre du modèle, elle est dans notre cas fixée arbitrairement à 200, une valeur prise dans l'intervalle des valeurs communes de l'état de l'art.
Nous souhaitons attirer l'attention sur le fait que, sur un tel corpus, il est important d'apprendre des plongements spécifiques. À titre d'exemple, prenons le mot courant manette. Parmi ses 10 plus proches voisins avec notre approche, 7 sont des acronymes SNCF, ce qui aurait été impossible d'obtenir avec des plongements appris sur un autre corpus.
Évaluation des plongements lexicaux
Protocole d'évaluation
Afin d'évaluer la qualité de l'espace de représentation construit sur le corpus SNCF, nous proposons dans cette section un protocole d'évaluation sollicitant plusieurs experts SNCF. L'évaluation consiste à valider la pertinence de l'association de deux mots donnés. Par exemple, l'association de train et wagon est pertinente, tandis que l'association de billet et passage à niveau ne l'est pas. Nous proposons de solliciter l'expert sur la pertinence de l'association entre un mot donné et ses 6 plus proches voisins dans l'espace de représentation généré via l'approche décrite Section 3. Le résultat d'évaluation de l'association est binaire : pertinente ou non pertinente. Ce protocole permet de simplifier le travail des experts dont le temps est pré-cieux. Ceci nous permet de couvrir suffisamment le corpus et ainsi de garantir la significativité statistique des résultats. Enfin, une tâche claire laisse moins de place au subjectif (c'est le cas avec plusieurs degrés de similarité).
Les mots évalués ont été regroupés et proposés aux experts selon 4 catégories : Mots issus du lexique SNCF (Lexique) ; Acronymes SNCF polysémiques (Acr. poly) ; Acronymes SNCF non polysémiques (Acr.) ; N-grammes fréquents dans le corpus (ngram fréq., n ∈ {1, 2, 3}). Pour chaque catégorie, on considère la fréquence d'apparition des mots dans le corpus pour échantillonner notre vocabulaire. Deux fois 20 mots de chacune des catégories sont présentés aux experts : 20 parmi les mots les plus fréquents, et 20 parmi ceux dont la fréquence se situe au niveau de la médiane de la distribution. Ainsi chaque expert se voit proposer 160 mots auxquels sont associés les 6 mots les plus proches, soit 960 paires de mots à juger comme étant pertinentes ou non. Le nombre d'associations différentes proposées à l'évaluation est détaillée par catégorie et fréquence Les experts. Neuf experts SNCF ont participé. Il s'agit de responsables ou chefs en poste depuis en moyenne 10 ans (de 9 mois à 25 ans à la SNCF) exerçant à des postes variés : documentation métier, sécurité système, qualité et performance, organisation de travaux, etc.
L'interface. Une plateforme regroupant plusieurs formulaires web a été développée par le LIUM. Pour chacune des 4 catégories, un tableau est proposé contenant le mot et ses 6 voisins. Les 6 voisins sont associés à une case à cocher. Si l'expert estime que le mot voisin n'est pas en relation avec le mot courant alors il coche la case. S'il estime que l'association des deux mots est correcte, il n'a aucune action à faire. De plus, une fonctionnalité permettant d'indiquer que le mot n'est pas connu est proposée afin de bien faire la différence entre une association qui ne serait pas pertinente et une association qui ne peut être évaluée car au moins l'un des mots n'est pas connu. Nous estimions le temps d'annotation d'un formulaire à moins d'une heure (environ 20 secondes pour les 6 associations à un mot). Les experts ont en effet mis de une demi-heure à une cinquantaine de minutes pour annoter un formulaire.
Résultats
Analyse quantitative. La Table 2  En second lieu, on constate que les termes fréquents (++) sont mieux connus que les autres et donnent ainsi lieu à moins d'associations inconnues. L'accord inter-évaluateur est également plus élevé dans cette sous-catégorie, quelque soit la catégorie des termes. Ces deux constats montrent la difficulté de conduire une évaluation concernant le vocabulaire spécifique : les acronymes qui sont très spécifiques sont souvent inconnus (en particulier les polysémiques), le vocabulaire moins fréquent est moins connu. Acr. freq.
Acr. peu freq.
Acr. poly. freq.
Acr. poly. peu freq.
n-gram freq.
n-gram peu freq.
lex. freq.
lex. peu freq.
FIG. 1:
Nombres d'associations jugées pertinentes en fonction de la catégorie (couleur) et de la proximité du voisinage (abscisse) comprise entre 1 (le mot le plus proche), et 4 (le quatrième mot le plus proche dans l'espace appris). Les mots les plus proches sont jugés plus pertinents.
Les résultats de l'évaluation montrent que 30 à 40% des paires proposées ne sont pas jugées pertinentes. Ces chiffres pris dans l'absolu ne sont pas très informatifs, ils mettent simplement en avant la difficulté de la tâche. En revanche, ils vont servir de référence pour les prochaines évaluations prévues avec les experts. Cela montre également que le nombre de 6 associations par terme est peut-être trop élevé et conduit à plus d'erreurs. En effet, la Figure 1 nous montre que les premières associations proposées sont systématiquement plus pertinentes.
Analyse qualitative. La catégorie des acronymes a été jugée comme étant la plus compliquée à évaluer. Ce résultat n'est en outre pas surprenant étant donné l'importante polysémie des acronymes au sein du groupe. L'acronyme n'est bien souvent pas connu de l'évaluateur. Deux stratégies sont alors mises en oeuvre : i) l'expert recherche la signification de l'acronyme et évalue les associations proposées ; ii) il indique via l'interface que l'acronyme lui est inconnu. Dans de rares cas, les mots proposés ont permis d'élucider le sens d'un acronyme. Dans le cas des associations pertinentes, les experts remarquent que les mots proposés apparaissent potentiellement au sein de la même phrase. Cela signifie que les experts réfléchissent au contexte de l'acronyme pour évaluer son sens. D'autre part, les stratégies mises en oeuvre ici pour inférer le sens des acronymes lorsqu'il est inconnu montrent qu'il est difficile d'évaluer une paire de mots de manière indépendante des autres paires de mots.
La spécificité locale (au sens géographique) de certains acronymes a été évaluée comme surprenante et non pertinente par rapport à la réalité du terrain. Par exemple, pour le sigle GL (Grande Ligne) est proposé Saint Denis. En réalité, les trains grandes lignes passent bien par Saint-Denis, mais une proposition comme Gare du Nord aurait été jugée plus pertinente et plus dimensionnante. Cet exemple illustre la difficulté à intégrer des connaissances métiers dans la modélisation du vocabulaire de spécialité.

Introduction
La détection d'anomalies est un domaine de recherche très actif traité par plusieurs communautés scientifiques telles que : la sécurité informatique, la médecine, l'industrie et la finance. De façon générale, ce problème consiste à détecter les données qui sont significativement différentes des données bénignes ou normales. De nos jours, les données sont de plus en plus représentées par les graphes car ces derniers ont la faculté de modéliser les interactions complexes de façon simple et intuitive. Un graphe G = (V, E) est un outil de représentation de données formé d'un ensemble de sommets V et d'un ensemble de liens (arêtes) E entre les sommets. Lorsque les données sont représentées par des graphes, le problème de détection d'anomalies revient à repérer les graphes qui sont différents des graphes correspondants aux objets normaux observés par le système. De plus, les graphes en flux (graph stream) sont de plus en plus utilisés. En effet, dans la plupart des applications de surveillance en temps réel, la structure complète des graphes n'est pas connue, car les graphes grandissent et évoluent au fil du temps. De même, lorsque les graphes sont trop volumineux pour être chargés entièrement en mémoire centrale, les traiter dans le modèle de flux de données où le flux est en général une séquence d'arêtes est une nécessité. La détection d'anomalies dans un flux d'arêtes pose plusieurs défis comme le traitement incrémental des arêtes, la gestion de l'espace mémoire occupé par le flux et la détection des anomalies en temps réel.
Dans ce travail, nous nous intéressons au problème de la détection d'anomalies dans un flux de graphes hétérogènes et étiquetés. Notre application principale est la sécurité des systèmes informatiques. Chaque graphe dans le flux représente une fenêtre d'une activité particulière du système (accès mémoire, authentification, etc.). Ceci explique l'hétérogénéité du flux.
Considérons un flux d'arêtes provenant de différents graphes hétérogènes orientés et éti-quetés. Chaque arête du flux est représentée par un 6-upplet <sommet source, l s , sommet destination, l d , l e , id graphe > où l s , l d et l e représentent les étiquettes du sommet source, du sommet destination et de l'arête respectivement. Le flux d'arêtes forme des graphes dynamiques qui évoluent au fil du temps. Les arêtes qui partagent le même id graphe appartiennent au même graphe. De plus les arêtes qui proviennent de graphes différents peuvent être entrelacées et donc plusieurs graphes peuvent évoluer simultanément. La problématique considérée ici est de détecter, dans ce flux, les graphes anormaux à n'importe quel moment t.
Un graphe anormal est défini comme étant un graphe qui est significativement différent des graphes bénins ou normaux connus par le système. Ainsi, la détection d'anomalies dans ce flux d'arêtes peut être vue comme un problème de comparaison/classification de graphes : au fur et à mesure que les graphes évoluent avec l'arrivée de nouvelles arêtes, on les re-classifie en normaux ou anormaux selon leur similitude avec des graphes d'entraînement qui représentent un comportement normal du système. Les deux problématiques sous-jacentes sont donc : (1) comment calculer la similarité entre les graphes ? et (2) comment les classifier ?
Comparer deux graphes est un problème complexe dont les solutions sont généralement exponentielles (Bunke et Allerman, 1983). Pour obtenir des approches de comparaison polynômiales, la méthode la plus utilisée est de décomposer les deux graphes à comparer en sous-structures plus simples et de comparer les sous-structures obtenues (Riesen et al., 2015). Il existe plusieurs méthodes de calcul de similarité entre les graphes en utilisant leurs sousstructures. Les méthodes basées sur les noyaux de graphes (Shervashidze et al., 2011(Shervashidze et al., , 2009) sont les plus rapides mais ne sont pas applicables dans le cas de graphes dynamiques car elles pré-calculent un espace fixe de sous-structures pour représenter les graphes alors que dans un flux la structure complète des graphes change au fil du temps. Les méthodes basées sur la distance d'édition de graphes (GED pour Graph Edit Distance) ne peuvent être utiles non plus car on doit recalculer la GED à chaque arrivée d'une nouvelle arête ce qui est très coûteux. Rappelons que la GED définit la similarité entre deux graphes par la séquence minimale d'opérations d'éditions (i.e., insertions ou suppressions de noeuds ou d'arêtes) nécessaires pour transformer un graphe en l'autre (Sanfeliu et Fu, 1983). Les approximations les plus rapides de la GED sont de complexité polynomiale (Fischer et al., 2017;Riesen et al., 2015). La phase de classification est une problématique importante. En effet, l'inconvénient principal de la représentation par graphes est le manque de méthodes appropriées pour la classification et le clustering dans l'espace des graphes. Cela est dû principalement au fait que certaines opérations de base nécessaires dans la classification ne sont pas disponibles pour les graphes (Riesen et al., 2007). Une technique pour pallier ce problème consiste à transformer les graphes en vecteurs pour pouvoir appliquer les algorithmes classiques de classification, on parle dans ce cas de plongement de graphes (graph embedding). D'une manière générale, le plongement de graphes consiste à faire associer à chaque graphe un point dans un espace vectoriel, de telle sorte que les points qui sont associés aux graphes similaires soient proches (Foggia et al., 2014). Une des approches de plongement de graphes qui a prouvé son efficacité pour la classification consiste à représenter un graphe G par un vecteur E G = (d 1 , ..., d M ) contenant les distances d'édition entre le graphe G et M graphes prototypes sélectionnés dans l'ensemble des graphes d'entraînement (Riesen et Bunke, 2009). Cependant, cette méthode est inefficace dans le cas de flux d'arêtes, car le coût nécessaire pour mettre à jour la GED de manière incré-mentale (càd à chaque arrivée d'une nouvelle arête) est de complexité quadratique O(n 2 ) où n représente le nombre de sommets dans le graphe (Mills-Tettey et al., 2007;Toroslu et Üçoluk, 2007).
Nous proposons dans cet article une nouvelle approche pour la détection d'anomalies en temps réel dans un flux de graphes hétérogènes et étiquetés tout en prenant en compte les défis de la gestion de manière incrémentale des arêtes lors de la comparaison des graphes, et de la limitation d'espace mémoire.
La suite de cet article est organisée en 4 sections : La deuxième section présente l'état de l'art. La section 3 est consacrée à la description de l'approche proposée. La section 4 présente la complexité de la méthode en termes de temps et d'espace, ainsi que les résultats que nous avons obtenus par expérimentations. Enfin, la dernière section conclut l'article en présentant quelques perspectives.
État de l'art
Le problème de la détection d'anomalies dans les graphes a fait l'objet de plusieurs travaux (Akoglu et al., 2015;Ranshous et al., 2015). Cependant, la plupart des approches existantes ne portent pas sur la détection de graphes anormaux mais plutôt sur la détection d'objets anormaux dans les graphes tels que les sommets anormaux (Akoglu et al., 2010;Papalexakis et al., 2012), les sous-graphes anormaux (Noble et Cook, 2003) ou les communautés et les événe-ments anormaux (Sun et al., 2010;Aggarwal et Subbian, 2012). Parmi les méthodes proposées pour la detection de graphes anormaux dans un flux, nous pouvons citer Classy (Kostakis, 2014), une approche distribuée pour la détection des programmes malveillants dans un flux de graphes orientés et étiquetés représentant des appels de fonctions (call graphs). Classy compare deux graphes avec une approximation de la GED qui utilise le recuit simulé. De plus, afin d'accélérer le processus de la classification des nouveaux graphes entrants, elle utilise une borne inférieure de la GED de complexité temporelle O(n) où n est le nombre de sommets dans le graphe. Cependant, Classy est conçue pour des flux de graphes entiers et non pour un flux d'arêtes. Spotlight (Eswaran et al., 2018) est un approche basée sur le sketching des graphes pour la détection des graphes anormaux dans un flux d'arêtes de graphes bipartis, orientés et pondérés. Dans cette méthode, l'anomalie est définie comme la disparition ou l'apparition soudaine d'un sous-graphe dense dans un graphe. Le point fort de cette approche est qu'elle arrive à représenter chaque graphe par un vecteur de taille fixe et réduite appelé sketch. Chaque dimension du sketch représente la somme des poids des arêtes d'une région (sous-graphe) du graphe. Les graphes anormaux peuvent être détectés en repérant les sketchs les plus éloi-gnés des sketchs normaux dans l'espace vectoriel. Spotlight ne traite que les graphes bipartis simples et ne détecte qu'un type spécifique d'anomalies (l'apparition ou disparition soudaine d'un sous-graphe dense). StreamSpot (Manzoor et al., 2016) est applicable directement à notre problématique. StreamSpot decompose un graphe en k−shingles qui sont des arbres de profondeur k et utilise une extension de la similarité cosinus pour les comparisons. Cependant, l'inconvénient principal de cette similarité est qu'elle ne prend en compte que le nombre des sous-structures communes entre les deux graphes à comparer et ne fait aucune comparaison entre les sous-structures. Cela rend cette similarité non précise dans le cas où les graphes sont très denses ou dans le cas où les sous-structures sont grandes. Pour remédier à ce problème les auteurs ont divisé les sous-structures en petits morceaux de taille fixe. Cependant, le choix de la taille C des morceaux influe significativement sur la précision de la similarité. En effet, un petit C rend la plupart des paires de graphes similaires, tandis qu'un grand C rend les paires de graphes plus dissemblables. L'inconvénient de cette solution est donc le re-calibrage du paramètre C à chaque arrivée d'un nouveau type de graphes bénins. De plus, pour être incrémentale, l'approche sauvegarde dans un cache de taille limitée les arêtes des graphes. Cependant, lorsque ce cache est plein, StreamSpot supprime les anciennes arêtes pour stocker les nouvelles qui arrivent. Par conséquent, une partie de chaque graphe sera perdue, ce qui influe sur le la précision de la détection.
Approche proposée
Dans cette section, nous allons présenter notre approche en commençant par décrire la représentation que nous proposons pour les graphes, ensuite nous décrirons le processus de détection d'anomalies basé sur cette représentation.
Représentation des Graphes
La décomposition des graphes en sous-structures
Nous décomposons chaque graphe en un ensemble de sous-structures locales appelées branches. Notre décomposition est une extension de la décomposition en branches de Zheng et al. (2013)  branche est représentée par un couple (r, ES) où r est l'étiquette du noeud racine et ES est un vecteur contenant les arêtes voisines du noeud r. Nous utilisons la structure de données introduite dans (Lopresti et Wilfong, 2003) et appelée "structure d'arêtes" pour les vecteurs ES. Supposons qu'il existe α étiquettes d'arêtes différentes l 1 , ..., l α , le vecteur ES de la branche associée au sommet r contient 2α entiers non négatifs, (x 1 , ..., x α , y 1 , ..., y α ), tel que x i est le nombre d'arêtes sortantes de r marquées par l i et y j est le nombre d'arêtes vers r marquées par l j . Pour simplifier, on scinde le vecteur ES en 2 vecteurs ES OUT et ES IN contenant la structure des arêtes sortantes de r et celles entrantes vers r respectivement. Plus formellement, ES OUT = (x 1 , ..., x α ) et ES IN = (y 1 , ..., y α ). La table 1 illustre la représentation des branches de l'exemple de la figure 1. 
Calcul de similarité entre les branches
Nous proposons d'utiliser la GED pour calculer la similarité entre deux branches. Ce choix est justifié par le fait que cette métrique nous permet de traiter n'importe quel type de graphe (i.e., orienté ou pas, simple ou multigraphe et étiqueté ou non étiqueté). La distance d'édition entre deux branches est définie comme suit :
Definition 1 (Zheng et al., 2013) Soient Br 1 = (r 1 , ES 1 ) et Br 2 = (r 2 , ES 2 ) deux branches, la distance d'édition entre les deux branches est :
Nous calculons la similarité entre deux branches par :
où BED nrm (Br 1 , Br 2 ) est la distance d'édition normalisée entre les deux branches définie comme suit :
Sélection des branches prototypes
Nous utilisons la stratégie du prototype couvrant en tenant compte des classes indépen-dantes pour la sélection des branches prototypes, (En anglais SPS-C « Spanning Prototype Class-wise ») (Riesen et Bunke, 2009). Cette stratégie prend en compte toutes les distances par rapport aux prototypes sélectionnés auparavant. La première branche prototype est la mé-diane du classe. Chaque branche prototype supplémentaire sélectionnée par notre stratégie SPS-C représente la branche la plus éloignée des branches prototypes déjà sélectionnées. Cette stratégie prend en compte toutes les distances par rapport aux prototypes déjà sélectionnés et tente de couvrir l'ensemble des branches bénignes le plus uniformément possible (Riesen et Bunke, 2009).
Soient M le nombre total de branches à sélectionner et K le nombre des différentes classes de graphes normaux, la technique consiste à choisir M K branches prototypes de chaque classe c comme suit :
avec p i = arg max br∈C\Pi−1 min p∈Pi−1 BED(br, p)
Plongement de graphes par sous-structures pondérées
Soit P = {sp 1 , ..., sp M } un ensemble contenant M branches prototypes extraites de graphes normaux dans la phase d'entraînement. Pour chaque graphe G, nous maintenons en mémoire une matrice M G où BED(i, j) représente la distance d'édition entre la i-ème branche du graphe G et la j-ème branche prototype.
Nous représentons un graphe G par le vecteur E G de M composantes, où la i-ème composante représente la somme des similarités pondérées entre les branches de G et la i-ème branche prototype (i.e., E G = (e 1 , ..., e M ) tel que e j = N i=1 Sim(i, j)w i ). Le terme w i = |Bri| |G| représente le poids de la branche d'index i. C'est le rapport entre le nombre d'arêtes dans la branche Br i et le nombre d'arêtes dans le graphe. Le terme Sim(i, j) représente la similarité entre les branches i et j calculée par la formule 3. Le produit Sim(i, j)w i représente l'impact de la branche i par rapport au prototype j. Nous calculons la distance entre deux graphes G 1 et G 2 en utilisant la distance euclidienne (norme L2) entre leurs deux vecteurs caractéristiques
Il est clair que si deux graphes G 1 et G 2 contiennent des branches similaires, la distance entre eux d(G 1 , G 2 ) sera petite et vice versa. De plus, il est évident qu'en utilisant ce plongement, les graphes anormaux seront les plus éloignés des autres graphes car les branches qu'ils contiennent sont les moins similaires aux branches prototypes.
Détection d'anomalies
La phase d'entraînement : Avant d'entamer la détection d'anomalies, le système doit être entraîné pour qu'il reconnaisse les modèles de graphes normaux existants. Dans la phase d'entraînement nous avons repris l'algorithme de clustering utilisé dans la méthode StreamSpot (Manzoor et al., 2016), qui consiste à regrouper les graphes d'entraînement dans k clusters en utilisant l'algorithme k-Medoid, le paramètre k est choisi de telle sorte qu'il maximise le coefficient silhouette (Rousseeuw, 1987). Cette technique permet de bien séparer les clusters des graphes les uns des autres. Un seuil d'anomalie est ensuite attribué à chaque cluster en utilisant l'inégalité de Cantelli (Grimmett et Stirzaker, 2001). Ce seuil est fixé à 3 fois l'écart type des distances plus la distance moyenne entre les graphes du cluster et le graphe médoïde. Ensuite, nous calculons le centroïde de chaque cluster qui est la moyenne des vecteurs caractéristiques des graphes du cluster.
La  
Expérimentation
Nous avons testé notre approche sur une configuration Intel I7 8700K @ 3.7 GHz avec 32Go RAM, en utilisant le dataset sbustreamspot-data 1 crée par Manzoor et al. (2016). Ce dataset consiste en un flux de graphes représentant des activités et des connexions dans un système informatique dans le but de détecter des cyber-attaques. Il contient un scénario d'activités malicieuses et 5 scénarios d'activités bénignes. Les activités bénignes représentent des navigations internet normales, telles que : la navigation sur YouTube, téléchargement de fichiers, navigation sur des sites d'information, consultation d'emails et des jeux vidéo en ligne. Le scénario d'attaque consiste en un téléchargement à la dérobée déclenché par une visite d'une URL malveillante. Les caractéristiques des 6 types de graphes sont présentées dans le tableau 2. Les graphes normaux ont été regroupés en 3 sous-ensembles de données :
-ALL regroupe tous les graphes d'activités bénignes . Nous avons testé notre approche en utilisant différents taux d'entraînement τ = {25%, 50%, 75%} sur les 3 sous-ensembles de données ALL, YDC et GFC. De plus, afin d'observer le comportement de notre approche à l'arrivée des nouveaux graphes non vus au préalable, nous avons contrôlé le nombre de graphes qui arrivent et grandissent simultanément en créant des groupes aléatoires de P graphes de tests. Dans toutes les expérimentations, les métriques de performances sont calculées périodiquement à chaque 10000 arêtes. Nous avons utilisé les métriques suivantes comme mesures de performance : -F1 score : est la moyenne harmonique de la précision et le rappel. Il combine à la fois la précision et le rappel. -BACC (Balanced Accuracy) : l'exactitude équilibrée est la moyenne arithmétique des taux des vrais positifs et des vrais négatifs.   
Conclusion et perspectives
Dans cet article, nous avons présenté une nouvelle représentation de graphes par des vecteurs pour la détection d'anomalies dans un flux de graphes hétérogènes. Cette representation  
Summary
In this work, we propose a new approach to detect anomalous graphs in a stream of directed and labeled heterogeneous graphs. Our approach uses a new representation of graphs by vectors. This representation is flexible and allows to update the graph vectors as soon as a new edge arrives. In addition, it is applicable to any type of graph and optimizes memory space. Moreover, it allows the detection of anomalies in real-time.

Introduction
La gestion de la relation client est l'analyse des données des interactions des clients. Ce qui est important pour l'industrie, ce n'est pas de déterminer si un document exprime globalement une opinion positive ou négative, mais au contraire de détecter si le client exprime des opinions sur des sujets précis. Par exemple, il peut être satisfait des frais de livraison, tout en étant très mécontent du délai de livraison. Les langues couvertes sont le français (considéré comme la langue native), l'anglais (natif mais moins développé donc non utilisé lors de l'apprentissage), l'espagnol, l'allemand, le portugais et l'italien. Le logiciel s'appelle AKIO Analytics.
Prétraitement, catégories et flux de données
Concernant le prétraitement, nous avons un pipeline linguistique comprenant un tokeniseur, un correcteur orthographique et grammatical, un tagger-chunker statistique, un analyseur syntaxique en dépendance, un annotateur de la négation. L'entrée peut être de quatre types : a) la chaîne brute d'origine, b) une suite de formes fléchies corrigées, c) une suite pleine de lemmes, d) une suite filtrée de lemmes corrigés. Le filtrage des lemmes consiste à ne prendre que les parties du discours comme les noms ou les adverbes de négation et d'ignorer d'autres mots comme les déterminants. Nous gérons trois types de catégories : les modalités d'expression, les thèmes et les opinions. Les catégories sont précises et nombreuses (179 catégories). Une modalité d'expression sera par exemple "question", un thème sera StoreDelivery et une opinion sera MissingItemNeg. Nous traduisons le corpus de développement automatiquement du français vers l'espagnol, nous transférons les marques de catégories depuis le document français et ensuite, de manière monolingue, en espagnol, nous apprenons un modèle de classification qui sera appliqué en exploitation. Concernant le choix des options, si on se focalise sur les FM au dessus de 70, l'option des formes brutes avec SMO n'est pas réaliste car le temps d'apprentissage est trop long. En l'état actuel de nos évaluations, nous optons pour le classifieur SGD avec les lemmes corrigés filtrés, étant entendu que ce choix pourrait être remis en question à la lumière de futurs développements.

Introduction
Notamment à travers les sites de vente en ligne ou les plateformes d'intermédiation, les utilisateurs sont de plus en plus confrontés à un nombre de possibilités rendant impossible l'analyse exhaustive de l'ensemble des choix possibles par l'utilisateur. Il est alors devenu indispensable de recourir à un système de recommandation dans ces situations. Dans cet article, nous abordons le problème de recommandation séquentielle dont le but est de prédire la prochaine action d'un utilisateur à partir de sa séquence d'actions passées. Dans la suite une action est assimilée à un item. Pour ce faire, la préférence à long terme de l'utilisateur et sa dynamique à plus court terme sont prises en compte. Considérons tout d'abord les méthodes récentes traitant ce problème. Modéliser la préférence utilisateur. Les techniques de factorisation de matrices (Koren et Bell. (2011)) permettent de modéliser les interactions entre les utilisateurs et les items en dé-composant la matrice d'interaction en un produit de deux matrices de rang k. La prédiction qu'un utilisateur u choisisse l'item i est estimée par le produit scalaire du vecteur de longueur k associé à u, par le vecteur de longueur k associé à i. Cependant, la matrice d'interaction est généralement creuse, ce qui rend la décomposition peu précise. Pour essayer de pallier ce problème, d'autres méthodes comme FISM (Kabbur et al. (2013)) décomposent une matrice de similarité d'items en deux matrices de rang k. Plus un item i est similaire aux items déjà choisis par l'utilisateur, plus i a des chances d'être recommandé. Modéliser la dynamique séquentielle. Une autre tendance dans les systèmes de recommandation est de prendre en compte les informations séquentielles présentes dans l'historique des utilisateurs. La dynamique à court terme est généralement modélisée à l'aide de chaînes de Markov. La matrice de transition est décomposée par le produit de deux matrices de rang k. Ainsi, la probabilité d'avoir l'item i sachant que l'item j appartient à l'historique de l'utilisateur est estimée par le produit scalaire des deux matrices. Modèle unifié. Plusieurs approches récentes cherchent à unifier la préférence utilisateur et la dynamique séquentielle pour obtenir de meilleures performances, comme par exemple la mé-thode FPMC (Rendle et al. (2010)). Plus récemment, Fossil (He et McAuley (2016)) propose d'associer une approche de similarité entre items comme FISM avec des chaînes de Markov d'ordre L. PRME (Feng et al. (2015)) a amélioré FPMC et Fossil en remplaçant le produit scalaire par des distances Euclidiennes. En effet, les méthodes utilisant des distances permettent une meilleure généralisation. Dernièrement, TransRec (He et al. (2017)) unifie la préférence utilisateur et la dynamique séquentielle en utilisant des translations dans un espace Euclidien.
Ces méthodes combinent la dynamique de long et court terme en n'utilisant uniquement des chaînes de Markov d'ordre fixe. Pour pallier ce problème, nous proposons une nouvelle méthode, REBUS, qui utilise des séquences fréquentes pour identifier les items les plus pertinents des historiques des utilisateurs. Ces items permettent de mieux capturer la dynamique séquentielle. Notre contribution se résume au développement d'un nouveau modèle, REBUS, qui unifie la préférence utilisateur et la dynamique séquentielle en les plongeant dans un même espace euclidien. De plus, l'ordre personnalisé des chaînes Markov est déterminé grâce à l'utilisation de séquences fréquentes. La figure 1 résume le fonctionnement de REBUS. Dans une étude empirique sur 13 jeux de données, nous démontrerons que REBUS surpasse l'état de l'art des systèmes de recommandation séquentielle. Les données et le code sont disponibles 1 .
FIG. 1 -La préférence utilisateur : plongement des items de l'historique de l'utilisateur ; La dynamique séquentielle : plongement des items de la séquence fréquente ; REBUS recommande l'item le plus proche de la somme des deux plongements.
2 Le modèle REBUS 2.1 Evaluer la dynamique séquentielle par un contexte personnel Le point de vue défendu dans cet article, est que l'utilisation de séquences plus longues et variées permet de mieux représenter la dynamique séquentielle et donc d'améliorer la recommandation. C'est pourquoi nous proposons d'utiliser des chaînes de Markov d'ordre variable, contrairement aux méthodes existantes qui utilisent des chaînes d'ordre fixe. L'approche utilisée pour trouver le contexte le plus adapté pour un utilisateur u au pas de temps t, comporte deux étapes : (1) La construction d'un ensemble de contextes commun pour tous les utilisateurs et (2) l'identification du contexte le plus adapté à un utilisateur u au pas de temps t. Identifier des contextes pertinents. Les contextes pris en compte sont ceux qui apparaissent dans au moins minCount séquences d'utilisateurs et ont une taille inférieure ou égale à L. Nous proposons de les identifier à l'aide d'un extracteur de sous-chaînes fréquentes. L'ensemble généré est appelé F. Personnalisé le contexte de u au pas de temps t. L'objectif est de déterminer la sous-chaine m s [1,t] u la mieux adaptée à u au temps t. Pour ce faire, on sélectionne la séquence de F la plus longue et récente présente dans s 
Le modèle proposé
REBUS est un modèle qui plonge les items dans un espace euclidien de telle sorte que la projection d'un item est influencée par la préférence utilisateur et la dynamique séquentielle. L'utilisation d'une distance euclidienne possèdent deux avantages : (1) cela permet d'avoir une meilleure généralisation car les distances conservent l'inégalité triangulaire, (2) cela permet d'effectuer un seul calcul de distance tout en unifiant les dynamiques à long et court terme. L'objectif est d'apprendre un vecteur P i pour chaque item i de I de telle sorte que la prédictionˆp prédictionˆ prédictionˆp u,i,t que u choisisse i au temps t soit aussi proche que possible que ce qui est observé dans les données. Le modèle est alors défini par :
où (1) β i est un terme de biais, (2) le premier terme, la dynamique à long terme, est la moyenne des vecteurs associés aux items j de l'historique de l'utilisateur, (3) le second terme modélise la dynamique à court terme à l'aide du paramètre η r qui augmente avec r, le rang de l'item dans la séquence représentant l'historique, pour donner plus d'importance aux items récents 2 . REBUS apprend les paramètres P et   (2017)) qui unifie la préférence utilisateur et la dynamique séquentielle avec des translations dans un espace euclidien. Nous évaluons notre modèle REBUS avec deux configurations : (1) avec la dynamique séquentielle basée sur les sous-chaînes fréquentes, (2) avec la dynamique séquentielle basée sur les chaînes de Markov d'ordre 1, notée REBUS 1M C . Protocole expérimental. Pour chaque jeu de données, nous avons séparé les séquences des utilisateurs en 3 parties : (1) l'item le plus récent qui sera utilisé pour évaluer les méthodes et appelé l'item vérité, (2) le deuxième item le plus récent qui sera utilisé pour la validation des méthodes lors de la phase d'apprentissage et (3) tous les autres items qui seront utilisés pour entraîner les méthodes. La précision de l'approche est mesurée par l'AUC et le HIT50. Études des performances. Les performances des différentes méthodes pour chaque jeu de données sont résumées dans le tableau 2. Nous pouvons observer que les résultats de la mé-trique AUC de REBUS surpassent les autres approches sur la plupart des jeux de données. RE-BUS obtient également de bonnes performances sur la métrique HIT50 avec un rang moyen de 3.2 sur 9 méthodes. PRME est très performant sur les jeux de données denses tels que ML30 et ML50. Cependant, il montre ses limites sur les jeux de données éparses. Cela permet de conclure qu'avoir des vecteurs latents indépendants n'est pas un avantage sur des jeux de données éparses. En utilisant des chaînes de Markov d'ordre personnalisé, nous remarquons que cela permet d'obtenir de meilleurs résultats par rapport à l'utilisation des chaînes de Markov TAB. 2 -AUC et HIT50 pour les différentes expérimentations. La ligne Amélioration montre les gains/pertes de REBUS comparé à la meilleure des autres méthodes (en gras).
Expériences
Exemples de recommandations. La figure 2 montre quelques exemples de recommandation de notre approche sur les jeux de données Amazon-Games et Amazon-Office. REBUS capture la dynamique séquentielle et recommande des items qui sont similaires à l'item vérité. Par exemple, pour le premier utilisateur, REBUS recommande Final Fantasy X-2 car l'utilisateur avait acheté les éditions précédentes de Final Fantasy (Les carrés rouges autour des items). L'item vérité, The Legend of Dragoon, est un jeu similaire. On voit dans cet exemple que la dynamique séquentielle est capturée par une séquence avec 4 items et un joker. Les 2 derniers exemples utilisent des séquences compactes de 2 items.
Conclusion
Nous avons proposé une nouvelle méthode REBUS utilisant des distances dans un espace Euclidien pour tenter de régler le problème de recommandation séquentielle. Notre approche utilise des chaînes de Markov d'ordre personnalisé grâce à l'exploitation de sous-chaines fré-quentes. REBUS apprend une représentation de la préférence utilisateur et la dynamique sé-

Introduction
Estimer de manière efficace la similarité entre des séquences symboliques est une tâche récurrente dans de nombreux domaines d'application, en particulier en bio-informatique, traitement des textes ou encore dans les domaines de la sécurité et sûreté des systèmes cyberphysiques. De nombreuses mesures de similarité ont été définies pour estimer la similarité entre deux séquences symboliques, comme la distance d'édition (Levenshtein, 1966) et son implémentation proposée par Wagner et Fisher (Wagner et Fischer, 1974), BLAST (Korf et al., 2003), les distances de Smith et Waterman (Smith et Waterman, 1981), de Needleman et Wunch (Needleman et Wunsch, 1970) ou les noyaux séquentiels locaux (Vert et al., 2004).
Dépasser le modèle de sac de mots pour tenir compte de la séquentialité des données textuelles est un problème difficile en général. Nous présentons dans cet article une nouvelle approche pour caractériser la similarité entre séquences symboliques en introduisant la notion de recouvrement de séquences. Dans un contexte de classification de données séquentielles, pour lequel chaque catégorie est représentée par un sous-ensemble de séquences, les approches orientées "modèle de langage" sont attractives dans la mesure où elles offrent un cadre formel bien établi susceptible de fournir, par exemple, une probabilité pour qu'un modèle génératif puisse produire la séquence de test à classer. La difficulté d'inférer des statistiques robustes pour des sous-séquences (n-grammes) de taille supérieure à 2 ou 3 (rares en général) constitue une limite pour ces approches. Pourtant, si, une ou deux phrases ou parties significatives de phrases d'un texte à classer se retrouvent dans une seule séquence d'apprentissage, alors on pourrait être amené, malgré la rareté de l'évènement, à considérer que celui-ci est significatif et discriminant. Cette observation est à la base de l'hypothèse sous-jacente à l'élaboration de la similarité par recouvrement : si à partir des séquences d'apprentissage associées à une classe, il est possible de recouvrir complètement la séquence de test avec un minimum de sousséquences, alors on dispose d'un modèle génératif parcimonieux qui permet "d'exprimer" avec le minimum de "mots" la séquence de test. C'est ainsi une manière de compresser au mieux la séquence de test en indexant les sous-séquences issues de l'ensemble d'apprentissage (une sous-séquence étant caractérisée par l'identifiant de la séquence d'apprentissage dont elle est issue, l'indice de début, et l'indice de fin de la sous-séquence). La règle de décision consiste alors à affecter à la séquence de test la catégorie de la classe la plus "compressante". Fondamentalement, cette similarité est basée sur un ensemble de séquences dites de référence à partir duquel un vocabulaire de sous-séquences peut être extrait et utilisé pour recouvrir de manière "optimale" une séquence quelconque. Un lien peut-être établi avec les approches "matching pursuit" développées pour caractériser des séries temporelles (Mallat et Zhang, 1993). Ce principe de recouvrement de séquences a été introduit avec succès dans le contexte de la dé-tection d'intrusion sur des machines hôtes d'un réseau (Marteau, 2018). Nous ré-introduisons ci-dessous la définition formelle de cette similarité pour en dériver un modèle discriminant pour la classification de textes.
2 Similarité par recouvrement de séquences FIG. 1 -Exemple : recouvrement (optimal) de la séquence (s) en exploitant les sous-séquences des séquences de l'ensemble (S).
La notion de recouvrement de séquences est simple et illustrée en Fig. 1. La séquence s est recouverte par des sous-séquences extraites des séquences de l'ensemble S. Sur cet exemple, le recouvrement est optimal dans la mesure où il est construit avec un nombre minimal de sousséquences. Le recouvrement est total dans le sens où tous les symboles de s sont couverts.
La similarité par recouvrement met en vis-à-vis i) la taille du recouvrement optimal (exprimée en nombre minimal de sous-séquences nécessaires) de s obtenu en utilisant les sousséquences extraites des séquences de S, avec ii) la taille de la séquence s elle-même (exprimée en nombre d'éléments), notée |s|. La similarité est construite de telle sorte qu'elle est maximale égale à 1 si le recouvrement optimal est de taille 1 (une seule sous-séquence est nécessaire pour recouvrir s), et minimale égale à 1/|s| si le recouvrement est composé uniquement de sousséquences de taille unitaire.
Définitions et notations
Soit Σ un alphabet fini et soit Σ * l'ensemble de toutes les séquences (ou chaînes) définies sur Σ. On note la séquence vide.
Soit S ⊂ Σ * un sous-ensemble quelconque de séquences définies sur Σ, et soit S sub l'ensemble de toutes les sous-séquences que l'on peut extraire des éléments de S ∪ Σ. Notons M (S sub ) l'ensemble de tous les multi-ensembles 1 que nous pouvons construire à partir des éléments de S sub .
c ∈ M (S sub ) est appelé recouvrement partiel de la séquence s ∈ Σ * si et seulement si : 1. toutes les sous-séquences qui composent c sont aussi des sous-séquences de s, 2. les copies de tout élément indistinguable de c correspondent à différentes occurrences d'une même sous-séquence dans s.
Si c ∈ M (S sub ) recouvre entièrement s, ce qui signifie que nous pouvons trouver un arrangement contigu de tous les éléments de c qui recouvre entièrement s, alors on dira que c est un recouvrement total de s. Enfin, nous appelons recouvrement S-optimal de s tout recouvrement total de s composé d'un nombre minimal de sous-séquences extraites des séquences de S sub .
Soit c * S (s) un recouvrement S-optimal de s. La mesure de similarité par recouvrement entre une séquence non vide et un ensemble quelconque de séquences S ⊂ Σ * est définie de la manière suivante :
où |c * S (s)| est le nombre de sous-séquences qui composent le recouvrement S-optimal de s, et |s| est la longueur de la séquence s. Notons qu'en général c * S (s) n'est pas unique, mais puisque tous les recouvrements de ce type ont la même cardinalité, |c * S (s)|, S (s, S) est bien défini.
Propriétés de S (s, S) :
1. Si s est une sous-séquence non vide de S sub , alors S (s, S) = 1 est maximal.
2.
A contrario, dans le cas de plus grande dissimilarité, le recouvrement S-optimal de s a une cardinalité égale à |s|, ce qui signifie qu'il est uniquement composé de sousséquences de longueur 1. Dans ce cas, S (s, S) = 1 |s| est minimal. 3. Si s est non vide, S (s, ∅) = 1 |s| (notons que si S = ∅, alors S sub = Σ).
D'autre part, puisque est une sous-séquence de toute séquence de Σ * , on convient que pour tout S ⊂ Σ * , S (, S) = 1.0
Pour illustrer le calcul de la similarité par recouvrement, considérons l'exemple suivant : 
Construction d'un recouvrement S-optimal pour toute séquence s
L'algorithme brute-force permettant de construire un recouvrement S-optimal d'une sé-quence quelconque s consiste en un algorithme incrémental qui, 1) détermine la plus longue sous-séquence de s contenue dans S sub qui est de plus un préfixe de s. Cette première sousséquence est le premier élément du recouvrement S-optimal recherché. Puis, 2), l'algorithme recherche la plus longue sous-séquence de s suivante dans S sub et qui commence à la fin du premier élément du recouvrement trouvé. Cette deuxième sous-séquence est ajoutée au recouvrement en construction, et on itère tant que la fin de la séquence s n'est pas atteinte. Dans (Marteau, 2018), la preuve que cet algorithme fournit un recouvrement S-optimal pour tout S et toute séquence s est proposée.
Par ailleurs, cette version brute-force peut être accélérée en y intégrant une recherche dichotomique de préfixes plus rapide en général. Cette solution, décrite dans (Marteau, 2018), est présentée succinctement sous la forme des algorithmes 1 et 2.
Pseudo-distance pour comparer des paires de séquences symboliques (chaînes de caractères)
La similarité par recouvrement définie (Eq. 1) entre une séquence s et un ensemble de séquences S permet de définir une mesure de similarité sur l'ensemble Σ * . Pour toute paire de séquences non vides s 1 , s 2 ∈ Σ * nous définissons cette mesure de la manière suivante :
où S est défini par l'équation Eq. 1. Pour des raisons de complétude, nous posons : S seq (, ,) = 1.0, et pour toute séquence
Finalement nous définissons simplement la pseudo distance δ c sur Σ * :
ce qui conduit à
Proposition 2.1. δ c (., .) est une semi-métrique sur Σ * (cette mesure est non négative, symé-trique et vérifie la propriété de séparation, mais elle ne vérifie pas l'inégalité triangulaire).
Algorithme 1 : Find the first break location in s between positions t b and t e 1 Function breakDichoSearch(s, t b , t e , S) input : s ∈ Σ * , a test sequence input : t b < t e < |s|, the index segment in which looking for the break input : S ⊂ Σ * , a set of sequences output : t, the searched breaking index position 
Complexité algorithmique
Une implémentation de l'algorithme 2 s'appuyant sur des arbres de suffixes permet de garantir une complexité algorithmique bornée supérieurement par O(k · |s| · log(|s|)), où k = c * S (s) est la taille du recouvrement S-optimal de s.
Cette complexité algorithmique ne dépend pas de |S|, ce qui signifie que la taille de S peut être en théorie très grande. En pratique, tant que les arbres de suffixe tiennent en mémoire (RAM), l'algorithme sera donc relativement efficace.
Pour la pseudo-distance δ c (s 1 , s 2 ), la complexité algorithmique s'exprime en O(k 1 · |s 1 | · log(|s 1 |) + k 2 · |s 2 | · log(|s 2 |)) où k 1 = c * {s2} (s 1 ) est la taille du recouvrement {s 2 }-optimal pour s 1 et k 2 = c * {s1} (s 2 ) est la taille du recouvrement {s 1 }-optimal pour s 2 . En comparaison, la distance de Levenshtein relève d'une complexité quadratique O(|s| 2 )).
Algorithme 2 : Find using a binary search a S-optimal covering for s input : S ⊂ Σ * , a set of sequences input : s ∈ Σ * , a test sequence output : c * , a S-optimal covering for s 
Exemples et premières expériences
Nous présentons ci-dessous quelques exemples qui illustrent certaines caractéristiques de la similarité (ou pseudo-distance) par recouvrement pour la comparaison de chaînes de caractères. Une implémentation python 3 est disponible sur https://github.com/pfmarteau/ STree4CS et permet de "rejouer" ces exemples ou d'en produire d'autres.
Distances par recouvrement sur des paires de chaînes de caractères
Le tableau 1 présente les distances par recouvrement obtenues pour quelques paires de chaînes de caractères. Nous utilisons la distance de Levenshtein (Levenshtein, 1966)  Ces exemples montrent que la distance par recouvrement est peu sensible aux permutations de sous chaînes comme dans ("little big man", "big little man"), ce qui n'est pas le cas pour la distance de Levenshtein. La paire des séquences les plus éloignées pour la distance par recouvrement est ("european", "american") alors que pour la distance de Levenshtein, il s'agit de ("indian", "indoeuropean").
Détection de plagiat
Nous montrons sur l'exemple suivant la capacité de la similarité par recouvrement à retrouver des passages d'un texte original dispersés au sein d'un texte plagié. Cet exemple est tiré d'un article visant à prévenir le plagiat diffusé par l'université de Princeton 5 .
Texte source original "From time to time this submerged or latent theater in Hamlet becomes almost overt. It is close to the surface in Hamlet's pretense of madness, the "antic disposition" he puts on to protect himself and prevent his antagonists from plucking out the heart of his mystery. It is even closer to the surface when Hamlet enters his mother's room and holds up, side by side, the pictures of the two kings, Old Hamlet and Claudius, and proceeds to describe for her the true nature of the choice she has made, presenting truth by means of a show. Similarly, when he leaps into the open grave at Ophelia's funeral, ranting in high heroic terms, he is acting out for Laertes, and perhaps for himself as well, the folly of excessive, melodramatic expressions of grief."
Texte plagié : des passages du texte source ont été repris verbatim, d'autres légère-ment modifiés sans référencement (les passages correspondants sont soulignés) "Almost all of Shakespeare's Hamlet can be understood as a play about acting and the theater. For example, in Act 1, Hamlet adopts a pretense of madness that he uses to protect himself and prevent his antagonists from discovering his mission to revenge his father's murder. He also presents truth by means of a show when he compares the portraits of Gertrude's two husbands in order to describe for her the true nature of the choice she has made. And when he leaps in Ophelia's open grave ranting in high heroic terms, Hamlet is acting out the folly of excessive, melodramatic expressions of grief".  'an',' be',' u','nd','ers','to','od',' as ','a ','pl','a','y ','a','b','out ','acting ','and ','the t','heater','. ','F','or ','ex','am','pl','e,','in ','A','ct ','1',',','Hamlet a','d','op','ts ','a ','pretense of madness',' th','at ','he ','us','es ','to protect himself and prevent his antagonists from ', 'dis', 'co', 'ver', 'ing ', 'his m', 'is', 'sion', ' to ', 'reven', 'ge', ' his ', 'fa', 'ther's ', 'm', 'ur', 'de', 'r', '. ', 'H', 'e a', 'l', 's', 'o pr', 'esent', 's t', 'ruth by means of a show', ' when he ', 'com', 'p', 'ar', 'es ', 'the p', 'or', 'tr', 'a', 'it', 's of ', 'G', 'ert', 'ru', 'de', "s ', 'two ', 'h', 'us', 'b', 'and', 's in', ' or', 'de', 'r to ', 'describe for her the true nature of the choice she has made', '. ', 'A', 'nd ', 'when he leaps in', ' Ophelia's ', 'open grave ', 'ranting in high heroic terms, ', 'Hamlet ', 'is acting out ', 'the folly of excessive, melodramatic expressions of grief. '] Les petites différences entre les passages plagiés qui sont soulignés dans le texte original et les sous-séquences du recouvrement proposé sont dues à la non-unicité du recouvrement optimal. Un simple post-traitement peut facilement corriger ces différences. Bien sûr, si le texte plagié est réécrit avec la même structure de texte mais en utilisant des mots synonymes, la similarité par recouvrement, dans sa version actuelle, ne pourra pas détecter le plagiat.
5. https://www.princeton.edu/pr/pub/integrity/pages/plagiarism/ 4.3 Séquences de gènes promoteurs pour la bactérie E-Coli (Harley et Reynolds, 1987) Les séquences de gènes promoteurs sont des séquences qui définissent l'endroit où la transcription d'un gène par l'ARN polymérase commence. La tâche considérée issue de l'archive UCI 6 consiste en une classification binaire dont l'objectif est de décider si la séquence testée comporte un gène promoteur ou non. Une procédure de type 'leave-one-out' est proposée pour évaluer les méthodes de classification. Le classifieur basé sur la similarité par recouvrement (CS) exploite la règle de décision :
Celle-ci stipule que la classe préditê y pour une séquence s est la classe qui maximise la similarité par recouvrement entre s et l'ensemble S y des séquences d'apprentissage appartenant à la classe y.  (Quinlan, 1986;Towell et al., 1990). CS 1/106 1 arbre de suffixe par classe (Marteau, 2018).
TAB. 2 -Taux d'erreur de classification pour les méthodes testées sur le jeu de données "Promoter Gene Sequences" proposé par l'archive UCI.
Les résultats présentés dans le tableau 2 montrent une très bonne capacité du classifieur CS a discriminer les séquences comportant un gène promoteur, comparativement aux autres méthodes testées dans la littérature sur cette tâche.
Nous considérons ici les textes comme des séquences de mots, chaque mot étant assimilé à un symbole. Nous nous inspirons du classifieur de Bayes Naïf dans sa version multinomiale pour définir une pondération (naïve) des mots du vocabulaire conditionnée à la tâche de classification. Étant donnée la classe y, la pondération w yi associée au mot x i est : 6) où N yi = x∈T x i est le nombre de fois que le mot x i apparaît dans le corpus d'apprentissage T , N y = n i=1 N yi est le nombre total des occurrences des mots dans la classe y ∈ Y , n est la taille du vocabulaire, S y est l'ensemble des séquences d'apprentissage associées à la classe y et S est l'ensemble des séquences d'apprentissage. α est un paramètre de lissage qui évite les probabilités nulles lorsque aucune occurrence de mot n'est observée étant donné la classe y.
En considérant que les classes sont équiprobables 7 , cette pondération associée au mot x i est normalisée afin de définir une mesure de probabilité sur les classes et une entropie associée
La similarité entre une séquence s et une classe y est estimée de la manière suivante :
où H 0 = log 2 |Y | (les classes sont considérées équiprobables). Le terme log 2 (|S y |) relève d'une heuristique qui pénalise les classes caractérisées par un très grand nombre de séquences.
En pratique, l'équation 8 découle directement de l'équation 1 par apport d'un double effet de pondération. En premier lieu si l'on considère l'élément e du recouvrement optimal de s obtenu pour la classe y, sa pondération a priori, sans connaissance de la classe y, est (1 − max xi∈e H i /H 0 ). Autrement dit, si aucun terme x i de la séquence e n'est discriminant, max xi∈e H i → H 0 , et la pondération a priori associée à e tend vers 0. Dans ce cas, l'élément e n'entre plus dans le calcul de la similarité. A contrario, si e contient au moins un mot très discriminant, max xi∈e H i → 0 et la pondération a priori associée à e tend vers 1. En deuxième lieu, la pondération conditionnée à la connaissance de la classe y de l'élément e dépend du terme (1 − max xi∈ê xi∈ê w yi ). Autrement dit, si un terme est très discriminant et identifie la classe y, i.e. P (x i | y) ∼ 1, alors (1 − max xi∈ê xi∈ê w yi ) → 1 et l'élément e est comptabilisé à hauteur de sa longueur |e| dans le calcul de la similarité. Si au contraire (1 − max xi∈ê xi∈ê w yi ) → 0, i.e. aucun mot de e n'est caractéristique de la classe y, alors, il sera comptabilisé à hauteur de sa longueur diminuée d'une unité (|e| − 1).
La règle de décision est la même que celle proposée pour le classifieur à base de similarité par recouvrement simple (Eq.5). L'unique méta paramètre pour cette approche discriminante 7. Considérer que les classes ne sont pas équiprobables semble trop pénaliser les classes à faible effectif. est le paramètre de lissage α introduit dans l'Eq.6. Il s'apparente au paramètre utilisé pour construire les estimateurs de Laplace qui entrent en jeu dans les classifieurs de Bayes naïfs.
Dataset
Expérimentation sur des données textuelles
Nous évaluons l'approche discriminante précédente sur deux jeux de données : 1) "Twenty Newsgroup" 8 qui comporte 18846 documents répartis en 20 classes ; nous considérons éga-lement une tâche plus simple qui est composée de 3759 documents répartis en 4 classes ( alt.atheism, comp.graphics, sci.med, soc.religion.christian). 2) un ensemble de documents collectés sur des flux RSS en anglais (1384 documents) et en français (1585 documents) 9 répartis en 6 classes inhomogènes (art-culture, économie, politique, santé-médecine, science, sport). Nous utilisons deux itérations de validation croisée 5 étapes, 80% des documents étant utilisés pour l'apprentissage des modèles et 20% pour les tests, avec un brassage aléatoire (graines initiales identique pour toutes les méthodes) entre chaque itération.
Outre l'approche discriminante basée sur la similarité par recouvrement (DCS) associée à la règle de décision définie par l'équation 5, sont évalués : le classifieur de Bayes Multinomial naïf (NB, (Kibriya et al., 2004)), une machine à support vecteur linéaire optimisée par descente de gradient stochastique (SGD, (Zhang, 2004)), un perceptron multicouche (MLP, (Rumelhart et al., 1986)), un réseau convolutif (CNN, (LeCun et Bengio, 1998)), un réseau type CapsulesNet (CNN-Cap, (Sabour et al., 2017)) et deux réseaux récurrents intégrant un modèle d'attention (Bi-LSTM et Bi-GRU (Du et Huang, 2018) avec attention). L'agrégation des méthodes DCS et SGD par ajout des scores (DCS+SGD) est également évaluée. Tous les méta-paramètres des méthodes précédentes ont été optimisés de manière à minimiser le taux d'erreur de classification sur la base de la répartition "train/test" du corpus twenty-newsgroup, proposé par la boite à outil scikit-learn 10 pour la configuration 4 classes, l'ensemble d'apprentissage comprenant 2257 documents et l'ensemble de test 1502 documents. Les CNN et RNN

Introduction
La plateforme web ANCORE 1 (Analyse de Comportements Relatifs Exceptionnels) permet de découvrir, dans le contexte des votes du parlement européen, des groupes d'individus et des contextes montrant des différences remarquables concernant leur entente, ou mésentente, usuelle. Par exemple, alors que, globalement, les votes des députés européens du Rassemblement National et du Front de Gauche témoignent d'un fort désaccord entre ces deux partis, on observe une convergence des votes pour la thématique "Relations extérieures avec l'UE".
La plateforme (section 2) met en oeuvre l'algorithme de fouille de motifs exceptionnels DEBuNk (Belfodil et al. (2017)). La visualisation des résultats intègre un module de géné-ration automatique de textes facilitant la compréhension et l'interprétation des résultats. La démonstration développe un scénario de vérification (section 3) d'affirmation et montre comment apporter un éclairage sur les accords et les désaccords de groupes parlementaires, tels que reflétés par les votes.
La plateforme ANCORE
Module DEBuNk (Belfodil et al. (2017)). Il relève d'une tâche EMM (Exceptional Model Mining (Duivesteijn et al. (2016) . Comparer cette similarité avec celle obtenue en considérant tous les scrutins sim * (g 1 , g 2 ) permet d'évaluer l'impact du contexte c sur les groupes g 1 et g 2 (cf. 1, 2 et 3 de la Figure 1). L'espace à explorer est si important qu'une évaluation par force brute se révèle impossible. DEBuNk met en oeuvre plusieurs techniques (opérateurs de fermeture , heuristiques optimistes. . .) pour fournir une liste exhaustive des motifs pertinents avec leur qualité p = (c, g 1 , g 2 ), ϕ(p) tout en validant des contraintes données a priori par l'utilisateur : seuil minimal de qualité, taille minimale d'un groupe, taille minimale d'un contexte. Modules GAT et VIZ. La plateforme ANCORE vise à pallier la difficulté d'interprétation des résultats issus de DEBuNk en fournissant une description claire, en anglais, exprimant de manière neutre la signification de chacun de résultats.
La difficulté la plus importante réside dans la description des groupes identifiés. Ceux-ci pouvant être formés à partir des propriétés individuelle des députés (nom, prénom, date de naissance, genre), des groupes parlementaires (ex : S&D), des partis politiques (ex : LR), des propriétés des pays (population, système politique, langues, GDP, nombres de sièges, appartenance à l'espace Schengen...), il en résulte un grand nombre de possibilités. 
C. de Lacombe et al.
L'approche classique de GAT consiste à utiliser des modèles de phrases prédéfinis permettant de décrire chacune des associations possibles entre propriétés. Le nombre de possibilités étant ici très important, les modèles de phrases sont générés automatiquement en fonction des résultats de DEBuNk. Une propriété est décrite par un adjectif et le reste par un complément. Le paramètre mis en adjectif est choisi selon l'ordre de priorité suivant : genre, nationalité, parti européen, parti national. La date de naissance est toujours donnée en complément.
Les modèles de phrase prennent la forme d'arbres capturant la structure syntaxique de la phrase. Ces modèles sont complétés en fonction des résultats fournis et un réalisateur de surface SimpleNLG (Gatt et Reiter (2009)) permet de produire la forme finale de la phrase en appliquant les règles grammaticales : accords en nombre et en genre des verbes et des adjectifs.
En complément des informations inclues dans le motif et de la transcription textuelle, le module VIZ fournit d'autres éléments synthétiques comme par exemple l'Agreement-O-Meter qui décrit grâce à une jauge le degré d'accord usuel entre les deux groupes et l'intensité du changement observé. L'utilisateur peut aussi naviguer dans les données correspondant à un motif pour mieux l'analyser et le comprendre : scrutins correspondant au motif, code couleur indiquant le niveau d'accord/désaccord, lien vers leur descriptif officiel, votes individuels . . .).
Scénario
L'article "Les groupes au Parlement européen, des alliances parfois surprenantes" 4 . En parlant du PPE, le principal groupe politique de la 8e législature, il affirme que la volonté de rassembler le plus de partis possible occasionne parfois "des alliances parfois surprenantes". Un exemple en particulier est donné, celui du parti Fidesz (Hongrie). Nous pouvons nous poser plusieurs questions pour étudier cette affirmation, dont :
-Le Fidesz est-il en conflit par rapport au reste du PPE ? -Le Fidesz a-t-il des conflits avec certains partis spécifiques du PPE ? -Existe-t-il d'autres conflits au sein du PPE ? Nous étudions ici les deux premières questions. La troisième le sera lors de la démonstration. Le Fidesz par rapport au reste du PPE. Grâce à DEBuNk, nous recherchons les sujets conflictuels entre le Fidesz et le reste du PPE. En filtrant les résultats pour n'afficher que les motifs qui concernent l'ensemble du PPE, le premier chiffre qui ressort est que le Fidesz vote en accord avec le reste du groupe pour 94% des scrutins. Les motifs de désaccord les plus importants trouvés concernent l'agriculture et le fonctionnement administratif de l'UE. Le Fidesz par rapport à d'autres partis du PPE. Avec les mêmes paramètres de recherche, on s'intéresse cette fois aux motifs qui opposent le Fidesz à un autre parti. Celui qui présente le plus grand désaccord exceptionnel avec le Fidesz est le Partido Popular espagnol, avec une chute de 91% à 10% d'accord, pour deux contextes (figure 2) identifiés par les sujets : [2 Internal market -4.10 Social policy] d'une part et [4.10.07 The elderly] d'autre part. Parmi les autres partis présentant de forts désaccords avec le Fidesz sur des sujets spécifiques, on trouve plusieurs partis, tous constitués de 3 députés ou moins.
Nous recherchons ensuite les motifs d'accord exceptionnel avec d'autres partis membres du PPE ; pour obtenir des résultats, nous devons baisser le seuil minimal de qualité, ce qui tend à montrer que le PPE a globalement une ligne cohérente. Cette requête fait ressortir deux partis :

Introduction
Un entrepôt de données (ED) est une grande base de données conçue pour analyser les données. La taille d'un ED peut atteindre des dizaines de teraoctets (To). Il est modélisé à l'aide d'un schéma en étoile ou en flocons de neige, comprenant une ou plusieurs tables de faits et plusieurs dimensions.
Plusieurs techniques de partitionnement horizontal ont été utilisées pour améliorer les performances des entrepôts de données distribuées (EDD), comme l'équilibrage des charges de données ou les stratégies de placement et de distribution des bases de données (Zamanian et al., 2015;Lu et al., 2017). On peut distinguer deux types de partitionnement : statique et dynamique. Dans les techniques statiques, on effectue le placement et la distribution des données avant de traiter une requête en se basant soit sur le schéma de l'entrepôt (Eltabakh et al., 2011;Dittrich et al., 2010), soit sur une charge de requêtes stable (Arres et al., 2015). Dans les techniques dynamiques, la distribution des données se fait au moment de l'élaboration du plan d'exécution de la requête (Zamanian et al., 2015;Tang et al., 2018). Certaines techniques, comme dans (Eltabakh et al., 2011;Dittrich et al., 2010), n'utilisent pas de charge de requêtes, mais plutôt des fichiers logs, cependant, elles ne sont pas adaptables dans le contexte des EDD et les requêtes de jointure en étoile.
Dans Hadoop, les bases de données sont constituées de tables, dont les données sont accessibles via un langage de requêtes tels que Hive-QL (Thusoo et al., 2009) ou Spark-SQL (Armbrust et al., 2015). Les tables, dans Hive ou dans Spark-SQL, sont similaires aux tables d'une base de données relationnelle. Dans un entrepôt, les données des tables sont sérialisées et chaque table possède un répertoire HDFS correspondant. Hadoop utilise des techniques de partitionnement et d'équilibrage de charges pour améliorer l'exécution des requêtes. Cependant, la distribution aléatoire des blocs, effectuée par HDFS, peut diminuer les performances des EDD, plus particulièrement avec les requêtes OLAP.
Les requêtes OLAP se composent de plusieurs opérations, comme le filtrage, la projection, la jointure et l'agrégation. Chaque opération peut s'exécuter lors de la phase Map ou celle de Reduce. Ainsi, chaque opération génère un coût d'E/S ou de CPU. L'opération de jointure est la plus coûteuse et peut générer un coût de communication considérable. Elle peut nécessiter n−1 ou 2 * (n − 1) cycles MapReduce, où n est le nombre des tables utilisées dans la requête. Pour minimiser le nombre de ces cycles et améliorer le traitement des requêtes, plusieurs travaux ont été proposés (Purdil˘ a et Pentiuc, 2016;Brito et al., 2016). Cependant, autant que nous sachions, il n'existe aucun travail antérieur pouvant exécuter l'opération de jointure en étoile en une seule étape de Spark sur la plateforme Hadoop.
Dans ce papier, nous proposons un nouveau schéma de placement de données massives d'un EDD sur un cluster de noeuds homogènes, en utilisant l'équilibrage des charge de données sans se baser sur une charge de requêtes donnée. Nous prenons en compte les caractéristiques physiques du cluster et la distribution des clés primaires et étrangères des dimensions. Notre stratégie permet d'exécuter plusieurs opérations d'OLAP dont la jointure en étoile avec un seul cycle de Spark. Pour développer et évaluer notre approche, nous avons utilisé le langage Scala, la plateforme Hadoop-YARN avec Spark, le système Hive et le banc d'essai TPC-DS.
Le reste de cet article est structuré comme suit. La Section 2 résume les travaux liés aux différents types de jointure en MapReduce. Dans la Section 3, nous détaillons notre approche. Nous présentons nos expérimentations dans la Section 4 et nous concluons dans la Section 5.
Etat de l'art
La plupart des algorithmes de jointure en MapReduce reposent sur des techniques de partitionnement dynamiques, comme repartition join et broadcast join (Blanas et al., 2010), multiway join (Afrati et Ullman, 2011) ainsi que le travail récent de (Kalinsky et al., 2016). Par contre, il y a peu de travaux qui utilisent des techniques statiques comme (Dittrich et al., 2010) et (Azez et al., 2015). Ce genre de technique nécessite la connaissance au préalable du type de traitement à faire. De plus, bien que le partitionnement dynamique effectué par les algorithmes de (Afrati et Ullman, 2011) et (Kalinsky et al., 2016 sont performants pour l'exécution de la jointure en étoile, cependant, le temps des transferts des données entre les noeuds durant la phase Shuffle peut être considérable, surtout dans le cas des dimensions de grande taille. D'autre part, la technique de réplication totale de certaines dimensions, utilisée dans les travaux de (Blanas et al., 2010) et (Abouzeid et al., 2009), est une méthode inappropriée pour les tables de grande taille. Ainsi, la méthode de pré-jointure effectuée par l'approche JOUM (Azez et al., 2015), n'est pas une solution fléxible pour les entrepôts de données massives et peut occuper un espace disque trop important. D'autres travaux, comme (Purdil˘ a et Pentiuc, 2016;Brito et al., 2016), ont proposé des méthodes pour réduire le nombre des cycles MapReduce dans l'opération de jointure. Telle que la stratégie de Purdil˘ a et Pentiuc (2016) qui exécute la jointure en étoile en deux itérations MapReduce ; Brito et al. (2016)   (Blanas et al., 2010)) qui nécessite plusieurs cycles de Spark. Zamanian et al. (2015) utilisent un schéma de partitionnement dynamique implé-menté dans un système de traitement des bases de données parallèles, appelé "predicate-based reference". Leur stratégie assure automatiquement la localité des données en co-partitionnant les tables qui partagent la même clé de jointure dans le même "Bulk" (partition).
Notre stratégie de placement est une technique de partitionnement statique comme (Dittrich et al., 2010). Nous traitons le problème des données dupliquées, comme dans (Zamanian et al., 2015), en utilisant l'algorithme de clustering "K-means Balanced". Avec notre technique et indépendamment de la charge de requêtes utilisée, nous pouvons exécuter le filtrage (c.à.d., prédicats dans la clause Where), la projection et la jointure en étoile en un seul cycle de Spark. Nous avons utilisé le moteur de traitement parallèle Spark et la plateforme Hadoop-YARN. L'ED utilisé est en schéma en étoile. Nous avons employé la technique de compartiment de données (Bucketing), qui existe en Hive et Spark-SQL, appelée Sort-Merge-Bucket (SMB) join.
L'approche proposée
Notre approche consiste à construire des fragments horizontaux des tables de faits et de dimensions, puis de les distribuer de façon équilibrée sur les différents noeuds d'un cluster, afin d'exécuter la jointure en étoile localement et en un seul cycle de Spark. Nous disposons au préalable du schéma de l'ED et des caractéristiques physiques du cluster. Notre démarche est composée de deux phases : (1) construction des buckets des tables de faits et de dimensions ; (2) placement des buckets qui partagent la même clé de jointure dans le même noeud. La 
Formulation du problème
Soit un schéma en étoile d'un ED, E={F , D1, D2,..,Dk}, tel que F est la table des faits et Dd, d ∈ 1..k, sont les dimensions. Notons par F K={f k 1 , f k 2 ,..,f k k }, l'ensemble des clés étrangères dans F en provenance de différentes dimensions, et par P K={pk 1 , pk 2 ,..,pk k }, l'ensemble des clés primaires de différentes dimensions. Nous symbolisons par "index" la clé de bucketing qui va servir à partitionner les tables de faits et de dimensions, tel qu'"index ∈
FIG. 1 -les étapes de l'approche
F K", ou bien "index" pourrait être une nouvelle colonne à ajouter à toutes les tables de l'ensemble E. Nous notons par N B le nombre des fragments (buckets) construits. Nous notons aussi par CF = {bucketF 0 , bucketF 1 , ..., bucketF N B−1 }, l'ensemble des buckets de la table des faits F , par CDd = {bucketDd 0 , bucketDd 1 , ..., bucketDd N B−1 }, ceux des dimensions, et par W CF = {{bucketsF 0 , bucketF 1 , ..., bucketF N B−1 } l'ensemble des tailles des buckets de la table des faits F et W CDd = {{bucketDd 0 , bucketDd 1 , ..., bucketDd N B−1 }, d ∈ 1...k, celles des dimensions. Nous notons par "group", l'ensemble des buckets qui partagent la même clé de jointure "index", composé d'un bucket de F et un bucket de chaque dimension Dd, d ∈ 1..k. N ={n 1 , n 2 ,.., n e } est l'ensemble des noeuds du cluster.
Notre objectif est de savoir comment choisir la clé index et le nombre N B, pour obtenir des tailles plus ou moins égales des buckets de l'ensemble W CF et W CDd , d ∈ 1..k, et comment distribuer ces buckets sur les noeuds du cluster afin d'exécuter des opérations d'OLAP dont la jointure en étoile localement et en un seul cycle de Spark. Dans ce que suit, nous détaillons notre approche.
Construction des Buckets
Cette phase est composée de trois parties : (1) détermination du nombre de buckets "N B" et de la clé de partitionnement "index" ; (2) construction des buckets de la table des faits "CF " ; et (3) construction des buckets des dimensions "CDd, d ∈ 1..k".
Détermination du nombre NB et de la clé index
Pour déployer notre stratégie de partitionnement, la détermination du nombre idéal N B et de la clé de partition index, est une tâche primordiale. a) Détermination du nombre N B : nous devons sélectionner celui-ci comme suit :
tel que min_N B est le nombre minimum des buckets, et max_N B le nombre maximum. Pour déterminer ces valeurs, nous employons les règles suivantes :
--Règle 1. Pour paralléliser notre traitement, il est préférable d'exploiter tous les CPU cores. Donc, au minimum, nous avons min_N B = N ct , tel que N ct est le nombre total des CPU cores des DataNodes du cluster. C.à.d., N ct est le nombre total des CPU cores affectés aux executors de Spark 1 . Notre démarche consiste à affecter pour chaque partition de Spark un CPU cores (dans notre cas une partition est un group, voir les notations dans la Section 3.1).
--Règle 2. Le choix d'un grand nombre N B peut pénaliser le traitement distribué, dû à l'augmentation du nombre d'E/S et à la taille des méta-données persistant en mémoire dans le NameNode. Pour palier à cela, le max_N B est déterminé comme suit :
tel que V E est la taille de l'entrepôt E, V M est la somme des tailles mémoires (RAM) de tous les DataNodes, et T est la dimension ayant la plus faible taille dans
Notre raisonnement ici porte sur la première partie de la formule (2) ; ceci signifie que si nous avons un espace mémoire suffisant (c.à.d. V M ≈ V E ), max_N B ≈ min_N B, nous pouvons exécuter des partitions de grande taille. Cependant, si la taille de la mémoire est faible (V M V E ), le max_N B augmente. Dans ce cas, le traitement de petites partitions est préférable. La seconde partie de l'équation, c.à.d. max_N B ≤ |T |, permet d'éviter l'obtention des buckets vides dans CF et CDd.
--Règle 3. Pour accélérer la sélection du meilleur nombre N B, nous commençons par exécuter des requêtes avec N B= N ct , et à chaque fois, nous incrémentons la valeur N B, c.à.d. N B = N B + N ct , jusqu'à ce que N B=max_N B, ou lorsque le temps d'exécution des requêtes augmente. b) Détermination de la clé index : le premier objectif de déterminer la clé index est de créer des "groups" de buckets. Nous distinguons deux techniques pour choisir la clé de jointure index : (1) nous choisissons la clé index comme l'une des clés étrangères de la table des faits F , tel que le nombre de ses valeurs distinctes soit supérieur ou égal au nombre N B (pour éviter l'obtention des buckets vides) et pour qu'elle ait la meilleure distribution homogène de ses valeurs (c.à.d. la plus faible valeur du coefficient d'asymétrie 2 ) ; (2) dans la deuxième technique, nous ajoutons une nouvelle clé de type entier à toutes les tables de l'entrepôt E. Cependant, afin d'obtenir une meilleure classification des enregistrements de F et déterminer 1. un executor est un processus intelligent permettant de lancer d'une façon autonome les tâches d'une application. 2. dans notre cas, le coefficient d'asymétrie Sk est calculé par la formule : Sk= n (n−1)(n−2)
où n est le cardinal de l'ensemble Dist, x i est l'élément i de Dist, σ est l'écart type de Dist et µ est la moyenne de Dist, (c.f. https ://en.wikipedia.org/wiki/Skewness). les valeurs d'index pour minimiser l'écart-type des ensembles W CF et W CDd , d ∈ 1..k, nous appliquons l'algorithme de clustering "K-means balanced" (c.f. dans la Section 3.4).
Construction des buckets de la table des faits
La construction de l'ensemble CF est basée sur les deux paramètres : le nombre "N B" et la clé de partition "index". Quelque soit la technique utilisée pour choisir la clé index, nous appliquons la formule suivante pour construire les buckets de l'ensemble CF : enregistrements du bucketF i ≡ enregistrements de F qui ont la même valeur de la clé "index".
Construction des buckets des dimensions
Nous construisons à present les ensembles de buckets, CDd, d ∈ 1..k, pour les tables de dimensions. Nous distinguons deux techniques : (1) dans la première, nous la désignons par F kKey, nous partitionnons la dimension Dm, dont la clé étrangère f k m vérifie les deux conditions expliquées dans la Section 3.2.1.b, et nous créons ses buckets en utilisant la même formule dans 3.2.2 (où nous remplaçons F par Dm). Cependant pour partitionner les Di, i ∈ 1..k/m, nous devons d'abord construire une table intermédiaire IDi, composée de deux colonnes, la première contenant la clé étrangère f k i et la deuxième, contient une clé, notée f k m ; ses valeurs sont calculées par la formule : f k m [j] modulo N B, tel que les f k m [j] sont les valeurs de la clé étrangère f k m dans F et j ∈ 1..|F |. Puis, nous supprimons tous les enregistrements dupliqués dans la table IDi. Enfin, nous faisons une jointure entre Di et IDi pour obtenir une nouvelle dimension D i qui contient la même clé de jointure de la dimension Dm. Et nous construisons l'ensemble CD i, en utilisant la formule 3.2.2. La Fig. 2 donne un exemple comment créer une nouvelle dimension avec cette technique. (2) Dans la deuxième technique, notée par N ewKey, après avoir ajouter la clé index à la table des faits, en utilisant l'algorithme "K-means balanced" et la création de l'ensemble CF , nous devons ajouter la clé index à toutes les dimensions pour créer les CDd, d ∈ 1..k. Pour ce faire, nous suivons ces deux étapes : (a) premièrement, nous construisons une table intermédiaire IDd pour chaque dimension Dd, d ∈ 1..k. IDd est composée de deux attributs, f k d et index, tel que f k d est la clé étrangère dans F qui correspond à la dimension Dd, et index est la clé de partition ajoutée à F . La table IDd a la même taille, c.à.d. le même nombre d'enregistrements que F . Avant de faire la jointure entre IDd et Dd, pour obtenir une nouvelle dimension D d qui contient la clé de jointure index, nous supprimons tous les enregistrements dupliqués dans la table IDd ; (b) nous construisons après les ensembles CD d en appliquant la même formule dans 3.2.2 (où nous remplaçons F par D d). Notons que la taille d'une nouvelle dimension D d change selon la valeur du nombre N B et la façon de déterminer la clé index. La Fig. 3 montre un exemple de construction d'une nouvelle dimension et ses buckets avec la technique N ewKey.
Placement des Buckets
Dans la phase II de notre approche, nous distribuons équitablement les groups construits sur les noeuds du cluster. Formellement, si on note par group i = bucketF i k d=1 bucketD d i , i ∈ 0..N B − 1, nous commençons par placer le group 0 sur le noeud 1, le group 1 sur le noeud 2,..., et le group p−1 sur le noeud e, tel que e=p modulo N B et p <= N B. Nous recommeno¸nsrecommeno¸ns l'opération de la même manière, nous plaçons le group p sur le noeud 1, le group p+1 sur le 
Détermination de la Clé Index
Nous remarquons que la taille des buckets dépend des deux paramètres N B et index. Nous pouvons déterminer la valeur du N B en suivant les étapes de la Section 3.2.1. Cependant, le choix des valeurs de la clé index est une tâche très difficile à gérer. Donc, puisque en général, les tailles des dimensions sont négligeables par rapport à la table des faits, nous cherchons comment minimiser l'écart-type de l'ensemble W CF . Cependant, il y a un facteur important qui influe directement sur la taille des nouvelles dimensions construites. Celui-ci est la similarité des enregistrements des buckets de F . Pour palier à ce problème, nous proposons une solution approximative basée sur l'algorithme "K-means Balanced". Les étapes de notre méthode sont : 1. A partir de la table F , nous construisons la matrice MV, tel que :
2. Une fois MV obtenue, nous construisons les N B clusters de MV. Donc, nous avons choisi l'algorithme K-means balanced (Malinen et Fränti, 2014) pour regrouper les vecteurs dans MV, pour deux raisons : la première est d'obtenir des tailles presque égales de buckets de CF ; la deuxième est de minimiser l'erreur quadratique moyenne (MSE) et d'augmenter la similarité interne des clusters. MSE est calculée comme suit :
ici, les X i sont des vecteurs de la matrice MV, les C j sont les centres des groupes ou des clusters et n = |M V |.
Nous obtenons (n mod N B) clusters de taille n N B , et N B-(n mod N B) clusters de taille n N B .
3. Finalement, nous affectons les valeurs obtenues à l'issue du clustering aux valeurs de la clé index.
Dans notre technique de regroupement, nous n'avons pas inclus un autre facteur qui influe sur la taille des nouvelles dimensions. Celui-ci est le nombre d'attributs d'une dimension. Certaines dimensions peuvent avoir peu d'attributs alors que d'autres en ont plusieurs (voire des centaines). Cependant, avec les formats de stockage en colonne comme Parquet et ORC, seuls les attributs sollicités par les requêtes sont chargés en mémoire.
Transformation des requêtes
Dans notre approche, nous faisons quelques transformations aux niveaux des prédicats de jointure pour obtenir des résultats non erronés. Dans la technique N ewKey, 
Expérimentations
Configuration de l'Environnement
Dans cette Section, nous présentons les étapes d'implémentation de notre approche. Premièrement, nous générons l'ED en utilisant le banc d'essai TPC-DS. Nous stockons directement en HDFS Parquet format. Nous avons implémenté les deux phases de notre approche. Nous avons utilisé un cluster de 5 machines esclaves (DataNodes) et une machine maître (NameNodes), caractérisées par : des CPU Pentium I7 avec 8 cores, une mémoire vive de 16 GB et un disque dur de 1 TB. Nous avons installé sur tous les noeuds Hadoop-YARN V-2.9.1, Hive V-2.3.3, Spark V-2.3.2, le banc d'essai TPC-DS, le langage Scala et l'outil "SBT".
Nous avons configuré Spark comme suit : spark.executor.instances=10, spark.executor.memory=6 GB, spark.executor.cores=3 CPU cores, la taille des blocs HDFS est de 128 MB et le facteur de réplication est égal à 3. Pour chaque noeud, nous gardons 4 GB de mémoire et 2 CPU cores pour le "système d'exploitation", "executors", et pour "Application Master". Avec cette configuration, nous pouvons exécuter 10 × 3 = 30 tâches au même temps.
Génération des données
Nous adoptons l'application spark-sql-perf 3 en utilisant Scala et Spark, pour générer une partie de DW, composé d'une table des faits et de 9 dimensions. Suite aux limitations physiques de notre cluster, nous générons la table des faits store_sales par partition, où, nous avons choisi la clé étrangère ss_store_sk de la dimension store comme clé de partition, puisque l'attribut ss_store_sk possède le moins de valeurs distinctes comparé aux clés des autres dimensions (voir Tab. 1). TAB. 2 -Les 6 requêtes sélectionnées
Implémentation de l'Approche
Pour implémenter la première phase de notre approche, nous avons utilisé trois composants de Spark : Dataf rame, Dataset et ArrayBuf f er. Pour placer les goupes des buckets, nous ne changeons pas la politique de placement par défaut d'HDFS comme (Eltabakh et al., 2011). Car la modification du nouveau framework API d'Hadoop V-2.x, est une tâche très difficile à effectuer. Notre stratégie est comme le balancer externe d'Hadoop.
Évaluation
Pour évaluer notre approche, nous générons un ED, noté DW, dont la taille est de 500 GB en format CSV (environ 143 GB en format compressé Parquet, voir les caractéristiques dans Tab. 1). Nous avons sélectionné puis adapté 6 requêtes de TPC-DS (voir Tab. 2). Nous supprimons "Group by" clause les agrégations, puisque cette opération s'exécute dans la phase Reduce et nécessite d'autre cycles (stages) de Spark. Les caractéristiques des requêtes sont : dans Q1, nous joignons 2 dimensions de faible taille avec la table des faits, ainsi nous sélectionnons quelques attributs avec l'utilisation de 2 filtres ; Q2 est comme Q1, mais avec la sélection de plusieurs attributs ; dans Q3, nous joignons 2 dimensions, dont une de large taille, avec la table de faits, sans utiliser des filtres ; la requête Q4 est comme Q1, avec l'ajout d'une dimension de grande taille customer_address ; Q5 est comme Q4, sans utiliser des filtres ; et Q6 est comme Q5, mais nous joignons 4 dimensions avec la table de faits.
Nous avons exécuté les 6 requêtes avec 4 approches : (1) SSH (partitionnement et distribution par défaut d'Hadoop et Spark), qui utilise Shuffle Hash join (SH join est comme répartition join de (Blanas et al., 2010)  
Discussion
Concernant les résultats de la Fig. 4, notre approche SSM BO a permis d'améliorer les temps d'exécution de 25% à 60% par rapport à SHB. Pour toutes les requêtes, des mauvais résultats sont obtenus par les 2 approches SSH et SSM BO (en appliquant la technique F kKey). Dans SSH, ceci est dû au taux élevé du shuffle durant la phase Reduce. Alors que dans SSM BO , nous obtenons de mauvais résultats bien que nous ayons partitionné toutes les tables avec une seule clé tout en activant SM B join de SPARK-SQL. Dans ce cas, l'optimiseur Catalyst de Spark doit planifier d'autres cycles pour calculer la condition de jointure (F.f k m modulo N B = D d.f k m ), ce qui augmente le coût de CPU.

Introduction
L'interprétation d'un fichier vectoriel (PDF, PS, EMF, WFM, IGES, STEP, DXF...), donne une suite ordonnée d'objets graphiques. Dans ce travail nous nous intéressons à l'identification des symboles dans les fichiers vectoriels de type schémas ou plans produits par des logiciels de DAO/CAO (Dessin Assisté par Ordinateur/Conception Assistée par Ordinateur) ou de PAO (Publication Assistée par ordinateur). Ces logiciels dessinent toujours les objets graphiques complexes (symboles) ou simples (rectangles, ellipses) de la même façon, et avec le même ordre chronologique. Partant de ce constat, et des travaux de Péré-Laperne et Couture (2017), ainsi que de ceux de Péré-Laperne (2018) qui précisent comment passer de l'espace des objets graphiques 2D à une chaîne de codes en 1D , sachant que les fonctions de transformation sont insensibles aux translations, aux rotations, ou aux homothéties, l'identification des symboles dans ces fichiers revient à rechercher les sous-chaînes de codes dans ces chaînes de codes issues des fichiers d'entrée. Cette identification des symboles est une des étapes de la méthode (A)KDD (Antropocentric Knowledge Discovery in Database) décrite par Péré-Laperne et Couture (2017) pour la restructuration des documents déstructurés. Cette méthode s'appuie sur les premiers travaux de Fayyad et al. (1996) pour l'extraction des connaissances des données.
Dans la section suivante, nous dressons un bref état de l'art. Dans la section 3, nous faisons le lien entre les répétitions et les paquets de l'algorithme de tri par paquet et nous proposons un algorithme linéaire au niveau du temps et de l'espace d'exécution. Dans la section 4 nous présentons les bénéfices de l'algorithme proposé , avant de conclure dans la dernière section.
État de l'art
Dans le domaine des objets graphiques vectoriels, l'état de l'art sur l'identification des symboles est inexistant. Des travaux existent sur l'identification des symboles dans des fichiers images (bitmap), mais, les problèmes sont très différents. Ces derniers manipulent des pixels, alors que nous utilisons des entités de plus haut niveau (segments, poly lignes, lettres, etc...), qui, même déstructurées, contiennent beaucoup plus d'informations que les pixels.
Comme introduit précédement pour identifier les symboles il faut identifier les répétitions. Les travaux de Gusfield (1997), puis ceux de Saha et al. (2008)), pour identifier des répétions dispersées dans l'ADN, se sont heurtés à la taille de la cible génomique. Ces travaux utilisent les arbres des suffixes ce qui entraîne une occupation mémoire (en octets) supérieure à 15 fois la taille du génome. L'utilisation du tableau des suffixes par Franěk et al. (2003) et Narisawa et al. (2007) ramène cette taille à 5 fois la taille du fichier d'entrée (1 fois pour les données en entrée et 4 fois pour le tableau des suffixes). Depuis, les travaux de Puglisi et al. (2010), Yusufu et Yusufu (2015), ou ceux plus récents de Louza et al. (2017) confirment l'abandon de l'arbre des suffixes pour utiliser le tableau des suffixes, pour des raisons d'occupation mémoire. Ces algorithmes nécessitent la création du tableau des suffixes (SA), des plus longs préfixes communs ( TAB. 1 -Extrait des résultats de Puglisi et al. (2010) 3 Algorithme proposé Nous partons de la chaîne de départ, de longueur n, notée S : S est une séquence de n codes de l'ensemble Σ, dans notre cas |Σ| ≤ 256. Pour identifier un symbole, dans un schéma, à partir de la chaîne de codes 1D, nous devons trouver les sous-chaînes de codes qui se répètent au moins T inf fois et qui sont composées d'au moins L inf codes. Prenons deux exemples. Le premier, pour identifier les cartouches dans un dossier électrique de 100 folios, le cartouche est présent sur chaque folio, donc si on cherche les sous-chaînes qui se répètent plus de 90 fois (T inf =90) et qui sont composées d'au moins 50 codes (L inf =50 un cartouche est composé d'un très grand nombre de codes voir Fig.1), on est sûr d'avoir identifié les cartouches. Le deuxième pour identifier les protections. Le symbole protection, dans un tel dossier de 100 folios, est présent au moins une trentaine de fois et le nombre de codes qui le compose est d'une quarantaine (T inf =30 et L inf =40), Donc, si on identifie les sous-chaînes qui se répètent au moins 30 fois, et qui ont une longueur supérieure à 40, on est sûr d'identifier les protections.
FIG. 1 -A gauche le folio, à droite les symboles
Dans les travaux de l'état de l'art, le tableau SA est créé pour obtenir la totalité des suffixes triés, puis le tableau LCP permet de déterminer le nombre de codes identiques entre 2 suffixes triés. Ensuite les autres tableaux servent à identifier les répétitions. Dans ces méthodes, ce qui prend le plus de temps, c'est la création du tableau SA car tous les suffixes sont triés.
Algorithm 1 TriPaquet(R,p)
If |R| < t then insertionSort(Rpd) {tri par insertion si nombre suffixe < t} |Σ|. Ils affirme que c'est le problème majeur de presque tous les algorithmes de tri (Small subarrays are of critical importance in the performance of MSD string sort. We have seen this situation for other recursive sorts quicksort and mergesort). Donc, si on supprime le tri des paquets les plus petits, on diminue de façon considérable le temps de tri, c'est l'objectif de notre algorithme.
Avant de le décrire, il faut définir la relation qui existe entre les répétitions et les paquets du tri. Une répétition est un ensemble de sous-chaînes répétées. Une répétition dans S est définie par R S,u = (L r ; i 1 , i 2 ..., i Tr ), où T r ≥ 2 et 0 ≤ i 1 < i 2 ... < i Tr ≤ n − 1, la longueur de L r est égale à la longueur des sous-chaînes, la taille T r est égale au nombre de sous-chaînes et les i i sont les adresses des sous-chaînes. Pour l'algorithme de tri, un paquet est égal à la répétition R S,u = (p + 1; i 1 , i 2 ..., i Tr ) ou p est la profondeur de la récursivité et les i i sont les adresses suffixes du paquet à trier et T r le nombre de suffixe du paquet. On démontre (assez facilement) que toutes les répétitions correspondent à des paquets et que tous les paquets sont les répétitions de la chaîne S.
Algorithm 2 Répétition(R,p)
bP rof ond := T rue {Indicateur du paquet le plus profond de la récursivité est mis à vrai} {Et profondeur < limite alors appel recursif} if ConditionsP ourEnregistrer then {Si les conditions pour enregistrer sont vrais} EnregistreRepetition {Alors on enregistre la répétition} bP rof ond := F alse {Indicateur du paquet le plus profond est mis à faux} Dans notre algorithme, l'appel de la récursivité n'est exécuté que si le nombre de suffixes du paquet est ≥ T inf et si la profondeur de la récusivité est ≤ L inf . L'indicateur bP rof ond est mis à vrai en début de procédure, et, remis à faux en fin de procédure. En fin procédure, l'enregistrement des répétitions ne se fait que si bP rof ond est vrai, le nombre de suffixes du paquet est ≥ T inf , la profondeur p est ≥ L inf et à condition que les suffixes ne se chevauchent pas. La colonne "Tri insertion" (Tab.2) est conforme aux résultats de Sedgewick et Wayne (2015), et montre la nécessité de changer de tri quand la taille du paquet est petite. La première colonne contient les valeurs des seuils (c'est à dire le nombre de suffixes du paquet) qui déclenche le changement de tri. La colonne "Pas de tri" correspond au fait que l'on ne fait plus de tri en dessous du seuil indiqué par la première colonne. Là encore, on vérifie très bien les propos de Sedgewick et Wayne (2015). Ce qui prend du temps c'est le tri des petits paquets. Or pour identifier les répétitions il n'est pas nécessaire de les trier. . L'espace mémoire occupé par notre algorithme est légèrement supérieur à ceux utilisant les tableaux des suffixes, il faut rajouter l'espace occupé par les compteurs : dans notre cas 1024 octets ainsi que 4 fois la taille du plus grand paquet. On peut diminuer l'espace total occupé à 8 fois la taille du plus grand paquet plus la taille des compteurs, cela complexifie l'algorithme et diminue ses performances d'environ 10%.
Résultats
La méthode (A)KDD est centrée utilisateur, c'est lui qui fixe les seuils T inf et L inf . La stratégie consiste à identifier en premier les symboles présents le plus grand nombre de fois et/ou les plus grands, puis, ceux qui apparaissent le moins souvent et/ou les plus petits. A chaque itération les symboles identifiés sont supprimés du fichier d'entrée, de cette façon l'ité-ration suivante se fait sur un fichier de taille inférieure, elle est plus rapide.
Conclusion
Nous avons proposé un algorithme de détection des répétitions qui est très efficace dans la recherche des répétitions sur des fichiers textuels. Nous avons utilisé des fichiers textuels car c'est le seul moyen de vérifier la performance de l'algorithme par rapport à l'état de l'art. Les premiers résultats obtenus pour la détection des symboles dans les fichiers graphiques sont très encourageants (non décrit dans cet article). Ils feront l'objet de nos prochains travaux. Nous utiliserons les mémoires caches de façon optimale, comme le propose Kärkkäinen et Rantala (2009). Et surtout, nous paralléliserons l'algorithme par paquets, qui se prête très bien à ce type d'optimisation. La recherche des répétitions est l'algorithme principal de la méthode (A)KDD : (Antropocentric) Knowledge Discovery in Database), il est utilisé pour détecter les symboles, mais aussi, les types de segments (pointillés, tirets, axes), les hachures ,les remplissages, les formes simples (cercle, arc, ellipse, rectangle, triangle, carré) et les fonctions.

Présentation de l'approche
Avec l'émergence du numérique de la dernière décennie, nous faisons face à de grands volumes de données issus de plusieurs secteurs d'activité tels que Commerce, Biologie, Mé-decine, Télécommunication, etc. Ces différents jeux de données véhiculent une quantité d'informations prodigieuses et pertinentes. Cependant, l'exploitation optimale de ces masses de données reste encore difficile. Ainsi, la mise en place de nouvelles solutions d'analyse de données est devenue un véritable défi pour la communauté scientifique.
Dans ce contexte, la batterie de résultats fournie par l'Analyse Formelle des Concepts s'avère d'une grande importance dans le processus d'extraction de connaissances à travers les treillis de Galois. Cependant, leur vraie exploitation a été toujours freinée par le nombre exorbitants des concepts formels extraits. Dans le but de filtrer ces derniers, plusieurs approches ont été définies. Parmi ces approches, nous nous intéressons à celles qui ont utilisé les métriques de qualité pour garder les concepts les plus intéressants.
Plusieurs mesures de qualité ont été proposées dans la littérature telles que la stabilité définie par (Kuznetsov (1990)) 1 , le couplage et la cohésion proposés par (Paul et Scott (2008)), la séparation introduite par (Klimushkin et al. (2010)), la distance définie par (Eklund et al. (2012)), etc. Ce nombre important engendre de nouveaux problèmes, entre autres, le choix de mesures de qualité à utiliser.
Cet article propose une nouvelle approche multi-critères permettant de sélectionner les top-k meilleurs concepts d'un ensemble de concepts formels, ayant en entré un ensemble de mesures de qualité. En outre, il importe de souligner que leur importance peut être éventuelle-ment pondérée par un utilisateur et/ou expert.
Parmi les méthodes multi-critères, nous proposons d'utiliser l'approche TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution) introduite par (Hwang et Yoon (1981)) pour générer les top-k concepts formels. L'idée fondamentale de cette méthode consiste à choisir une solution, qui se rapproche le plus de la solution idéale (meilleure sur tous les critères) et de s'éloigner le plus possible de la pire solution (qui dégrade tous les critères).
Extraction et navigation dans les top-k concepts formels
Dans le but d'illustrer cette approche, un prototype appelé FC-SWEEPER a été développé. Ce dernier permet à un utilisateur de sélectionner un jeu de données décrivant un contexte formel et de construire le treillis de concepts. Cette construction peut se faire à travers l'énumé-ration de tous les concepts formels qui peuvent être générés du contexte et ceci en appliquant l'algorithme LCM (Uno et al. (2004)). Dans le cas où le treillis s'avère très dense, l'utilisateur peut choisir d'extraire une couverture de concepts formels en se basant sur l'algorithme QUA-LITYCOVER (Mouakher et Ben Yahia (2016)) ou GRECOND (Belohlavek et Vychodil (2009)). Par la suite, l'utilisateur définit les critères à maximiser ou à minimiser, lance l'exécution de la méthode TOPSIS et visualise le résultat des top-k concepts formels.
Nous envisageons de poursuivre cette approche en l'améliorant selon deux directions : la proposition de nouveaux algorithmes de génération des top k concepts, capables de passer à l'échelle sur des jeux de données de taille réelle ainsi que d'autres méthodes d'agrégation plus fines qui ne nécessitent pas de fixer au préalable le poids des critères, e.g. les algorithmes d'extraction des motifs non dominés (Bouker et al. (2014)).

Introduction
Les molécules chimiques s'apparentent bien plus à un réseau/graphe d'atomes qu'à une information de type vectorielle, contenant un nombre fini de descripteurs. Cette information structurelle renferme ainsi une grande partie de l'information caractéristique des molécules : elle définit la manière dont celles-ci vont se conformer dans l'espace, ce qui conditionnera considérablement leurs propriétés macroscopiques où leur réactivité potentielle en présence d'une autre espèce. Le traitement de cette information structurelle est spécifiquement dénommé Quantitative Structure-Activity Relationship (QSAR). On distingue deux cas d'usage des modèles QSAR : le premier est purement applicatif, il vise à estimer le comportement macroscopique d'une molécule vis-à-vis d'une propriété donnée. Le second cas d'usage est la restitution d'information. Pouvoir interpréter directement un modèle QSAR permet d'identifier quelle serait la relation liant les éléments structurels aux propriétés macroscopiques de la molécule. Dans ce cas, il est nécessaire de s'appuyer sur des prédicteurs conservant un certain sens chimique.
Modèles QSAR "Sac de Fragments"
Parmi les nombreux types d'approche exploitant ces modèles (Wu et al., 2018), nous nous intéressons dans cet article aux approches orientées graphe. Les molécules sont assimilables à des graphes, dans lesquels les atomes tiennent le rôle de noeuds, tandis que les liaisons atomiques covalentes tiennent le rôle d'arêtes. Au sein même de cette famille d'approches, on distingue les méthodes de type fragmentation , qui visent à séparer le graphe entier en plusieurs sous-graphes plus aisés à analyser individuellement. De part leur correspondance naturelle avec les groupements fonctionnels et structure bien connues, ces mé-thodes offrent une ré-interprétabilité accrue par rapport à des descripteurs plus artificiels.
Principe
On décrit ici un modèle de type sac de fragments (Bag of Fragments), similaire à celui proposé dans (Baskin et Varnek, 2008). La méthodologie est la suivante : disposant d'une notation nous permettant d'assurer que deux graphes isomorphes 1 seront représentés par le même identifiant dans le contexte de l'étude, nous générons à partir de chaque noeud (atome) un sous-graphe centré sur lui-même, de longueur arbitraire. Ces sous-graphes pourront ainsi faire l'objet de descripteurs topologiques, en d'autres termes de nouvelles variables explicatives (prédicteurs), dont la valeur pour une molécule donnée correspondra au nombre d'occurrences de ce sous-graphe et de ses isomorphes dans cette molécule. On projette ainsi les molécules sur un espace de fragments (sous-graphes), d'où le terme de "sac" de fragments. Cette projection peut ensuite être exploitée par des méthodes d'analyse classiques .
FIG. 1: Projection d'un graphe (molécule) sur un espace de fragments
Ces fragments sont utilisables en l'état pour la prédiction de certaines propriétés (densité, LogP 2 par exemple), mais se révèlent insuffisants pour d'autres propriétés telles que le point d'ébullition par exemple. On provoque en effet une perte d'information en ne tenant pas compte du contexte d'apparition de ces fragments.
Afin d'exploiter au mieux cette approche Bag of Fragments, et d'être le plus précis possible pour la suite de l'étude, il est nécessaire d'utiliser une notation qui permette de garantir au maximum que des fragments identiques, appartenant à la même classe d'isomorphisme, soient regroupés sous la même modalité de descripteur topologique. On utilise ici une méthode à base de projection vers un arbre canonique et de notation de Newick modifiée, pour exprimer les différentes modalités de fragments.
Vocabulaire généré
Selon les fragments que l'on considère (taille, prise en compte ou non du label, etc.), on génère un vocabulaire de fragments, dont la taille va être fonction du caractère précis ou au contraire approximatif de ceux-ci. En revanche, on constate un effet de la taille de ce vocabulaire sur la modélisation de QSAR, qui ne donne d'ailleurs pas nécessairement l'avantage aux fragments les plus précis. Dans (Ruggiu et al., 2010) notamment, on constate qu'en autorisant un niveau d'imprécision 3 sur les fragments considérés, ceux-ci peuvent s'avérer être plus discriminants sur certaines tâches de prédiction. Cela pourrait être révélateur d'une forme de polysémie à considérer, voire même d'une sémantique véhiculée par ces fragments.
3 Étude lexicographique des fragments moléculaires L'observation précédente, qui révèle une apparente influence de polysémie, amène à formuler l'hypothèse d'une analogie avec ce que l'on observe au niveau des lexèmes en traitement automatique du langage naturel (TALN).
On propose pour étayer cette hypothèse l'étude suivante : considérant des fragments de taille t (un atome "feuille" est distant d'au plus t liaisons avec l'atome "racine"), et un corpus extrait de PubChem 4 recouvrant un espace chimique assez large (1 510 000 molécules), quels sont les écarts de fréquence entre les fragments les plus courants et les plus exotiques ? Le cas échéant, cette répartition se rapproche-t-elle d'une loi statistique connue ? dans un repère log-log, où l'on trace la fréquence d'un terme (fragment) en fonction de son rang (figure 2a). Apparaissent alors plusieurs lois de puissance, phénomène caractéristique d'une famille de distributions dites Zipfienne (Newman, 2005) , que l'on retrouve empiriquement sur les corpus textuels. On retrouve ainsi une distribution similaire sur le corpus projet Gutemberg 6 en langue anglaise, exception faite du nombre de termes (figure 2b). On constate également que la taille du vocabulaire généré semble soumis à une explosion combinatoire quand t augmente 7 , ce qui a pour conséquence de rendre très difficile une exploitation statistique de ces fragments pourtant plus précis sans disposer au choix d'un très grand nombre d'observations couvrant cet espace de fragments très vaste, ce qui est difficile à obtenir pour une propriété cible, ou d'une méthode permettant de rapprocher des fragments différents, mais porteurs d'une information similaire.
Vers une utilisation des fragments en contexte
Les observations empiriques précédentes indiquent que des fragments de graphes pris en tant que prédicteurs deviennent rapidement difficiles à analyser d'un point de vue statistique, au vu de l'explosion de dimensionalité que cela implique. Composer avec des observations caractérisées par des vecteurs creux à dimensionalité très élevée est une des problématiques avec lesquelles compose le traitement automatique des langages naturels (TALN), par troncature de vocabulaires (termes trop fréquents ou trop rares), ou bien grâce aux travaux plus récents dans le domaine des plongements (embeddings), techniques de réduction de dimension basées sur une contextualisation des termes (Mikolov et al., 2013), (Le et Mikolov, 2014), (Pennington et al., 2014). Il est à noter que des apprentissages de représentation ont été récemment adaptés pour les graphes selon plusieurs variantes (Goyal et Ferrara, 2017)  (Narayanan et al., 2017).
On propose ici de continuer à considérer des fragments de taille t comme des termes (lexèmes) qui composent un graphe, structure d'ordre supérieure. On embarque ainsi les informations portées par les arêtes dans cette structure de base que sont les fragments. On génère au préalable pour chaque noeud v de chaque graphe son fragment de taille t associé, noté ϕ v . On considère ensuite une distance n de voisinage maximale, définissant I v l'ensemble des voisins de ce noeud. Pour chaque i ∈ I v , nous allons générer un ensemble de n-grams de fragments (figure 3). 
Étude de similarités
Suivant le protocole précédent, les embeddings des 153 004 fragments de taille t = 2 sont calculés sur un corpus de 1 510 000 molécules avec un modèle Skip-Gram de paramètre n = 4. Les fragments étant projetés sur un nombre de dimensions arbitraire m = 100, on peut définir une similarité entre deux fragments. La figure 4 illustre une tendance qui semble se retrouver en opérant une recherche de fragments les plus proches d'un fragment de référence au sein du corpus étudié, au vu de la similarité cosinus. Dans le cas où l'on soumet un fragment de référence comportant des atomes appartenant au groupe des halogènes 8 , on constate bien souvent la présence parmi les fragments les plus proches, de fragments identiques à la référence, à une substitution d'halogène près. Ce type de substitution concerne donc deux atomes aux propriétés similaires, ce qui du point de vue chimique fait sens. Les embeddings de fragments en contexte, contrairement aux fragments bruts, semblent donc capable, sur cet exercice, d'inférer des proximités sémantiques intéressantes, tâche nécessitant pourtant une certaine connaissance de ce domaine. Des études complémentaires sont requises pour confirmer l'intérêt de tels embeddings sur des tâches de régression et de classification de molécules.
Conclusion
Dans cet article nous avons présenté un état de l'art sur l'utilisation des modèles Bag of Fragments dans l'élaboration de QSAR. Des études lexicographiques menées sur ces fragments sur un corpus conséquent exposent empiriquement des similarités étonnantes vis-à-vis des lois distributionnelles typiques des langages naturels, et nous encourage à formuler une analogie entre ces domaines, notamment pour résoudre des problématiques bien adressées en TALN. En ce sens, nos premières expérimentations (application requête-réponse par exemple) portant sur des plongements de fragments (embeddings), pris dans leur contexte d'apparition, paraissent très encourageantes, et nous amènent à approfondir cette analogie en élaborant de nouvelles méthodes de caractérisation de graphe, dans l'espoir d'améliorer notamment la prédiction de propriétés physico-chimiques des molécules.
8. Fluor F, Chlore Cl, Brome Br, Iode I, tous situés dans la 17-ième colonne du tableau périodique, et présentant des propriétés chimiques très homogènes

Introduction
A cause des nouvelles technologies de l'information et de la communication, le nombre de connaissances augmente exponentiellement. Cette massification empêche leur repérage, leur mise à jour et leur actualisation. Vu la capacité cognitive de l'homme à assimiler aisément et instantanément des représentations graphiques relatives à une quantité importante d'informations, les organisations doivent trouver, aujourd'hui, des stratégies pour représenter visuellement son patrimoine afin de partager, échanger, mobiliser et valoriser ces connaissances.
Dans ce travail, nous traitons la cartographie des connaissances comme moyen d'identification et de partage des connaissances. En effet, la cartographie est la représentation visuelle de concepts interconnectés sur une carte.
Notre objectif, dans ce papier, est de proposer un système de cartographie CK-Cartography pour l'identification et le partage des savoir-faire et des savoirs factuels. une description qui peut être factuelle ou prescriptive (Ghrab et al., 2017). Pour la construction de CK-Cartography, nous adoptons la méthode d'Eppler (2001) pour la cartographie des connaissances et l'ontologie COOK pour la définition des concepts (Ghrab et al., 2017).
La cartographie des savoirs générée par CK-Cartography est basée sur un langage graphique iconique pour la représentation graphique de chaque concept sur la cartographie. Nous associons pour chaque concept un pictogramme ou un ensemble de combinaison de pictogrammes selon le type du concept. La méthode de construction de la cartographie et le langage graphique sont implémentés dans CK-Cartography.
Fonctionnalités de CK-Cartography
CK-Cartography affiche différents types de carte (carte des processus, carte des savoir-faire et des savoir-factuels, carte de rangement, carte des personnels) (Figure1). Ces cartes offrent une vue générale de l'ensemble des concepts et une vue détaillée à la demande de l'utilisateur. Chaque carte a sa propre fonctionnalité.
La carte de processus visualise l'ensemble des processus d'une organisation avec ses différents niveaux. La carte des savoir-faire et des savoir-factuels ( 
FIG. 3 -Capture d'écran de la carte de rangement
La carte des personnels donne une idée sur les actions individuelles et collectives des acteurs(individu ou collectif) dans une organisation, la localisation de ses acteurs, les savoir-faire mobilisés dans ces actions et les processus pour lesquels les acteurs participent. A travers cette carte, il est possible de connaître les acteurs détenant des savoir-faire ou des savoirs factuels cruciaux, la localisation d'un tel acteur en cas de besoin, la proposition de solutions pour le transfert et le partage de savoir-faire ou de savoirs factuels en tenant compte de la localisation géographique de chacun (par exemple pour un acteur qui va être remplacé par un autre, savoir la localisation de ce nouveau acteur, identifier ses savoir-faire, trouver une solution pour communiquer avec lui (mail, tel, etc.).
La carte de rangement a pour objectif de ranger les savoir-faire cruciaux les uns par rapport aux autres afin de leur attribuer des priorités et les classer dans des classes d'équivalence (Figure 3). Cette carte permet aux décideurs (par exemple comité de direction d'une organisation) de prendre la bonne décision (privilégier un savoir-faire sur un autre savoir-faire tout en tenant compte des objectifs de l'organisation).
Conclusion
Notre système CK-Cartography est conçu pour l'identification et le partage des savoirfaire et des savoirs factuels entre les différents acteurs d'une organisation (détenteurs et utilisateurs de savoirs). La cartographie des savoir-faire et des savoirs factuels générée par CKCartography traite les concepts (savoir-faire, savoir factuel, acteur, collectif, organisation, processus, action) à visualiser et leurs dimensions (tacite/explicite, interne/externe, individuelle/collective). La spécificité de CK-Cartography est qu'il est basé sur la génération de plusieurs types de cartes selon le type d'usage. La description des concepts cartographiés est assurée par l'ontologie COOK qui propose des définitions rigoureuses et fines des concepts mis en jeu. Les concepts à visualiser dans la cartographie des savoirs sont identifiés et évalués à travers des méthodes, déjà validés, d'identification des processus sensibles et des connaissances cruciales. Ghrab, S., I. Saad, G. Kassel, et F. Gargouri (2017). A core ontology of know-how and knowing-that for improving knowledge sharing and decision making in the digital age. Journal of Decision System 26(2), 138-151.
Summary
CK-Cartography (Crucial Know-How and Knowing-That Cartography) is built around two use scenarios: (i) give a general and detailed visibility of Know-How and Knowing-That of the organization and (ii) help to decision making. CK-Cartography provides answers to queries related to these two scenarios. These queries are translated into maps which each map have a particular specification. The first purpose of Know-How and Knowing-That Cartography is the identification of Know-How and Knowing-That for a capitalization purpose. The second purpose is multicriteria Know-How and Knowing-That characterization and evaluation.

Introduction
Les données séquentielles sont présentes dans de nombreux contextes applicatifs (analyses de textes ou de vidéos, exploitation de traces d'interactions, supervision de processus industriels, exploration de données en biologie moléculaire, etc). Le cas d'utilisation qui motive nos propres travaux est celui de la supervision industrielle, où les suites d'états du système étudié constituent une séquence. Nous voulons fouiller ces collections de séquences étiquetées par des experts métiers pour découvrir des règles sur les co-occurences de pannes. Ceci permettrait, d'une part, de mieux comprendre le système étudié, d'autre part, de pouvoir construire un moteur de règles explicables, offrant alors des perspectives de prédiction et d'anticipations de certains dysfonctionnements. Une formalisation simple de ce contexte est de considérer qu'il s'agit de découvrir des motifs séquentiels co-occurrents à une variable cible (étiquette ou classe). La découverte de règles caractérisant une classe ou une étiquette a été très étudiée (Novak et al. (2009)), notamment dans le cadre de la découverte de sous-groupes (Wrobel (1997)). La découverte de sous-groupes dans des données étiquetées consiste à trouver des motifs (e.g., des motifs séquentiels), également appelés descriptions, qui définissent en intention des objets (e.g., des séquences) pour lesquelles une mesure de qualité indique qu'ils se répartissent d'une façon particulière entre les classes ou étiquettes. Nous décidons de travailler à l'exploitation de collections de séquences d'itemsets étiquetées. Pour la découverte de sous-groupes, les approches classiques d'énumération exhaustives comme celle de SD-MAP (Atzmüller et Puppe (2006)) posent rapidement un problème de passage à l'échelle. Dans le cas simple des itemsets, deux facteurs influent sur la faisabilité du calcul : le nombre d'items et le nombre de transactions dans la base de données. Concernant notre type de données séquentielles, nous avons comme facteur supplémentaire le nombre d'itemsets pour chaque séquence. De fait, l'espace de recherche devient rapidement trop grand pour pouvoir être exploré exhaustivement et il faut se contenter d'approches heuristiques qui ne peuvent détecter que certains des sous-groupes intéressants. De plus, les meilleurs motifs (et donc sous-groupes) ne doivent pas être redondants : fournir des motifs très similaires à un expert limite la confiance dans la méthode et freine la dissémination de tels outils. Diverses approches heuristiques ont été proposées mais partagent les deux problèmes qui sont le besoin de paramétrisation et la redondance des ré-sultats obtenus. Récemment, plusieurs approches heuristiques prometteuses d'échantillonnage dans l'espace des motifs ont été étudiées (Boley et al. (2011);Egho et al. (2017); Diop et al. (2018)). Il y a eu notamment le travail sur l'algorithme Misère ) qui propose la détection par échantillonnage de sous-groupes dans des séquences d'items et donc un contexte de données séquentielles simplifié. Les auteurs montrent l'intérêt qu'il y a dans un échantillonnage qui se construit à partir de l'une des séquences présentes dans les données. Nous proposons une exploration de l'espace de recherche pour des séquences d'itemsets qui développe certaines des propositions présentes dans Misère pour l'échantillonnage de motifs suivi d'un processus d'exploration locale et d'extraction d'optima locaux. Notre méthode présente plusieurs avantages hérités de Misère : elle donne des résultats à tout moment de l'exécution, bénéficie de la recherche aléatoire pour limiter la redondance des résultats, et est indépendante de la mesure de qualité utilisée. Nous l'améliorons en étendant le langage de motifs (séquence d'itemsets), et en permettant la découverte d'optima locaux, sans paramé-trage de l'utilisateur. D'un point de vue technique, nous utilisons une représentation verticale des données sous la forme de bitsets pour l'amélioration des performances et avons porté une attention particulière à la consommation mémoire. Notons enfin que la méthode est facilement parallélisable. La suite de cet article est organisée comme suit : la Section 2 introduit formellement le problème, la Section 3 discute l'état de l'art, la Section 4 détaille notre proposition qui est ensuite validée empiriquement dans la Section 5.
Formalisation du problème traité
Soit I un ensemble d'éléments. Tout sous-ensemble X ⊆ I est appelé un itemset. Une séquence s = X 1 , ..., X n est une liste ordonnée de n > 0 itemsets. n est la longueur de la séquence, et  S-extension, I-extension Une séquence s b est une S-extension de s a =< X 1 , X 2 , ..., X n > par un item x si ∃i, 0 ≤ i ≤ n tel que s b =< X 1 , ..., {x} i , ..., X n >. Une séquence s b est une I-extension de s a =< X 1 , X 2 , ..., X n > par un item x si ∃1 ≤ i ≤ n tel que s b =< X 1 , ..., X i ∪ {x}, ..., X n >. Par exemple, {ab}, {b}} est une I-extension de {a}, {b}}
Voisinage direct vertical ou horizontal On appelle voisinage direct vertical d'un motif s l'ensemble des motifs identiques à s si (1) on applique à s une I ou S-extension et (2) on applique à s une suppression d'un item. On appelle voisinage direct horizontal d'un motif s l'ensemble des motifs identiques à s si l'on modifie un item de s par un autre item du jeu de données.
Optimum local Soit N (s) le voisinage direct d'un motif s. r est un optimum local de S par une mesure de qualité ϕ s.s.i ∀r ∈ N (r ), ϕ(r ) ≥ ϕ(r).
Motif non θ-redondant Un ensemble de motifs est non θ-redondant si ∀s1, s2 ∈ S × S, sim(s1, s2) ≤ θ. Notre mesure de similarité est l'indice de Jaccard défini par :
Découverte de sous-groupes diversifiés Soit une base de données D, un entier k, une mesure de similarité sim. La découverte de sous-groupes diversifiés a pour but l'extraction de l'ensemble des meilleurs motifs non θ-redondants de taille inférieure ou égale à k, au regard d'une mesure de qualité ϕ et d'une classe cible c.
État de l'art
Les approches classiques d'énumération exhaustive de motifs séquentiels de type SPADE (Zaki (2001)) peuvent être adaptées à notre problème, comme, par exemple (Zhou et al. (2016)). Cependant, la taille de l'espace de recherche devient rapidement trop grande et une exploration exhaustive est souvent inapplicable en pratique.
Les classifieurs de données séquentielles comme, par exemple (Dafé et al. (2015)), pourraient à première vue nous intéresser. Cependant, ils résolvent le problème de la classification d'une donnée nouvelle, et non pas celui de la découverte d'un motif caractéristique de classe. Sur les méthodes d'échantillonnage dans l'espace des motifs, les auteurs de (Diop et al. (2018)) proposent une technique qui garantit que la probabilité de tirage d'un motif est proportionnelle à sa fréquence. Cette méthode est intrinsèquement liée à la recherche de motifs fréquents, et ne peux donc pas être facilement adaptée à notre problème. Notons aussi que les approches d'échantillonnage visant à limiter la taille du jeu de données, par exemple Toivonen (1996), traitent un problème bien différent de celui de l'échantillonnage dans l'espace des motifs. L'approche de référence dans le cas de la découverte de sous-groupes mais aussi celle de motifs d'exception est la recherche en faisceaux ("Beam Search") décrite par exemple dans (Duivesteijn et al. (2016)). L'idée est d'explorer l'espace de recherche étage par étage, en gardant les motifs les plus intéressants, qui généreront les motifs potentiellement intéressants de l'étage suivant. Cette méthode gloutonne présente le désavantage de devoir ajuster un paramètre en fonction du jeu de données (la taille du faisceau). De plus les motifs trouvés peuvent être redondants. Une approche visant à résoudre le problème de découverte de règles de classifications a été proposée dans Egho et al. (2017). Une mesure de qualité ("level") a été proposée. L'algorithme associé, baptisé misère, est basé sur de l'échantillonnage de motifs et la géné-ralisation des éléments tirés afin de proposer de nouveaux motifs de classification. La force de misère réside dans le fait qu'un objet du jeu de données est tiré et le motif généré possède donc un support non nul. Cette contribution est perfectible : la séquence tirée aléatoirement dans le jeu de données ne contient pas forcément l'étiquette cible, et misère effectue simplement un nouveau tirage à chaque itération, sans garantie de tomber sur un optimum local. Notons enfin que misère traite des séquences d'items plutôt que des séquences d'itemsets. Dans Bosc et al. (2018), les auteurs ont proposé une exploration de l'espace de recherche basée sur la recherche arborescente de Monte Carlo dans le cadre des itemsets étiquetées. Cette méthode est donc également basée sur de l'échantillonnage, où chaque tirage renvoie de l'information sur l'espace de recherche. Ces suites d'informations permettent de guider la recherche selon un compromis d'exploitation/exploration. Cette méthode présente cependant l'inconvénient de devoir stocker tous les éléments rencontrés (construction de l'arbre), ce qui peut présenter un budget mémoire élevé. De plus, son adaptation aux données séquentielles est difficile.
Notre algorithme permet à la fois d'extraire des règles de co-occurrences motif-classe (en maximisant une mesure de qualité bien connue appelée W RAcc, Lavrac et al. (1999)). Cependant, notre méthode est assez générique pour permettre l'utilisation de n'importe quelle
mesure, sans exiger de propriétés particulières, comme la monotonie du support. Aucun paramétrage n'est nécessaire. Contrairement à misère, nous avons la garantie de trouver des optima locaux grâce au hill-climbing. Nous avons cependant matérialisé la limite de l'espace de recherche (la frontière du bas sur le schéma) comme étant les derniers motifs dont toutes les I-extensions et S-extensions produiront des motifs de support nul. On peut prouver facilement que tout élément de cette frontière correspond en fait à au moins un élément du jeu de données. La forme de l'espace de recherche est donc totalement dépendante du jeu de données.
Échantillonnage de données et optimisation locale
Mesure de qualité
Pour extraire des règles de co-occurences motifs-classe pertinentes, nous choisissons d'utiliser la Weighted Relative Accuracy (W RAcc, Lavrac et al. (1999)). Elle permet de comparer la proportion d'éléments possédant l'étiquette ciblée dans le sous-groupe à la proportion d'élé-ments possédant l'étiquette ciblée dans la population totale.
Weighted Relative Accuracy Soit c ∈ C une étiquette et s un motif séquentiel :  
Tirage
Dans un premier temps, une séquence est tirée aléatoirement parmi les séquences du jeu de données ayant l'étiquette cible. Ceci permet de s'assurer que le motif généré par la suite contiendra au minimum un élément du jeu de donnée et le motif aura donc un support non nul.
Plusieurs stratégies sont possibles : -Tirer uniformément un élément du jeu de données avec remise. C'est la stratégie la plus simple que nous utilisons aujourd'hui. -Tirer uniformément sans remise : ceci permet de ne pas réutiliser un motif déjà utilisé afin de favoriser la diversité, dans une certaine mesure. -Tirer avec remise, avec diminution de la probabilité de prendre un élément déjà choisi.
Généralisation
Dans un second temps, cette séquence est généralisée, c'est à dire qu'un nouveau motif plus général est construit à partir de cette séquence. Pour cela, chaque item du motif est examiné, et a une probabilité d'être supprimé de 0.5.
Recherche d'optima locaux
Dans une troisième phase, une recherche d'optimum local est lancée. Autrement dit, un calcul du voisinage est effectué afin de trouver le motif maximisant la mesure de qualité dans le voisinage. Une première méthode possible est de calculer l'ensemble des voisins directs verticaux et horizontaux. Le voisin direct maximisant la mesure de qualité cible est choisi. Cette procédure est répétée jusqu'à ne plus avoir de voisins qui aient une meilleure mesure de qualité : on a trouvé un optimum local.
On pourrait également ne calculer que les voisins directs verticaux. Ceci permettrait de limiter le nombre d'éléments voisins (et donc leur W RAcc) à calculer. De plus tout élément appartenant au voisinage direct horizontal est atteignable avec la sélection successive de deux éléments du voisinage direct vertical (suppression puis addition d'item ou l'inverse). Enfin, étant donné le grand facteur de branchement dans l'espace de recherche, le calcul de l'ensemble des voisins et de leurs mesures (souvent dépendante du calcul du support) est coûteux. Pour contrer cela, une technique est de générer aléatoirement un nombre de voisins compris entre 1 et le nombre réel de voisins directs, valant, pour m items possibles et n itemsets dans la séquence, m(2n + 1) (voir preuve ci-dessous).
Démonstration : Pour une séquence s, soit n sa longueur (son nombre d'itemsets), k sa taille, et m le nombre d'items possibles. On a alors :
Filtrage
Afin de limiter la redondance des motifs trouvés, une procédure de filtrage est nécessaire, même si la composante aléatoire de notre algorithme permet déjà une bonne diversité des résultats. Une première possibilité, que nous utilisons, est de stocker l'ensemble des résultats obtenus (optima locaux) dans une structure de données, et de les filtrer en fin d'algorithme à la manière de Bosc et al. (2018).
Une autre possibilité, est de stocker les motifs dans une file de priorité F de taille k. A chaque fois qu'un nouveau motif est trouvé, si sa qualité est meilleure que le minimum des motifs de F, alors il est ajouté, et le ou les autres éléments similaires dans F sont supprimés. Cette technique permet de minimiser l'utilisation de mémoire utilisée. Notre coût mémoire est alors minimal. On constate cependant que les résultats sont de moins bonne qualité via ce filtrage heuristique.
Représentation verticale
Des algorithmes de découverte de motifs séquentiels comme SPAM (Ayres et al. (2002)) utilisent une représentation verticale afin d'améliorer le temps de calcul. Cette technique suppose un ordre lexicographique sur l'énumération, permettant de réutiliser une représentation verticale d'un motif pour l'étendre (I ou S-extension) en ajoutant un élément en fin de sé-quence. Dans notre cas, on doit pouvoir ajouter un élément à tout endroit de la séquence. Il faut donc pouvoir calculer la représentation verticale sans cette pré-condition. Pour cela nous proposons de garder dans une table de hashage l'ensemble des représentations verticales des itemsets rencontrés (mémoïsation) et nous les recombinons unes par unes afin de construire la représentation verticale de la séquence voulue. Dans le cas où un ensemble d'éléments nouveau est rencontré, nous le calculons.
Un exemple est donné dans Figure 2. Dans ce cas, le motif {ab}, {c}} a été généré. Imaginons que {c}} soit un motif déjà généré, mais pas {ab}}. Nous sommes donc dans un cas qui n'est pas géré par SPAM, puisque l'on ajoute un nouvel itemset avant une séquence dont on connaît la représentation verticale. L'algorithme de calcul du support va d'abord chercher à trouver la représentation verticale de {ab}. Puisqu'elle n'a pas encore été trouvée, elle va être générée et ajoutée à la structure de mémoïsation. Une fois ceci fait, les représentations verticales sont combinées comme décrit dans Figure 2.
Expérimentations
Nous évaluons empiriquement notre approche, à travers les questions suivantes : Quelle est la qualité de nos résultats comparativement à l'état de l'art ? La représentation verticale permet-elle d'obtenir un gain de performances ? Comment évolue la consommation mémoire ? Est-il plus intéressant d'effectuer une recherche d'optimum local en calculant seulement le voisinage direct vertical ou les voisinages directs verticaux et horizontaux ?
Pour l'évaluation empirique, nous évaluons notre algorithme baptisé HillSeqS sur divers jeux de données (décrits dans le tableau 3) : promoters, splice sont issus du célèbre répertoire UCI 1 , aslbu, block et context sont associés à la publication (Mörchen et Ultsch (2007)). Enfin, nous utilisons sc2 un jeu de données associé à la publication (Bosc et al. (2017)). Dans ce dernier, chaque itemset correspond à des actions d'un joueur sur Starcraft 2 durant une fenêtre de temps avec une étiquette qui correspond au type de la faction gagnante. Notre approche est au pire comparable aux autres algorithmes, au mieux supérieure, et ce sans nécessiter de paramétrage. Elle s'avère relativement stable selon le jeu de données, contrairement aux expérimentations avec notre implémentation Beam Search paramétrée pour un nombre de faisceaux de 30, une valeur qui donne de bons résultats en moyenne. Notons qu'un "Beam Search" souffre du problème de redondance des motifs et que, pour des comparaisons plus justes, nous avons ajouté l'étape de filtrage à chaque étape de la recherche en faisceau. Notre algorithme est moins performant sur le jeu context qui possède des séquences très longues. Une explication pourrait être que ce jeu de données possède des optima locaux "longs" à atteindre, et que beaucoup d'entre eux sont de mauvaises qualités. Une améliora-tion possible serait donc de relancer la recherche en cas "d'enfermement" dans la recherche d'optimum local.
La recherche d'optimum local peut être trop longue de sorte que le nombre d'optima locaux trouvés peut être inférieur à k. Si le peu de motifs trouvés sont de mauvaises qualités, ce qui est le cas pour le jeu context, la moyenne des WRAcc sera plus faible. Une piste d'amélioration pourra être de sortir de la recherche d'optimum si l'amélioration de la qualité du motif courant n'est pas significative. Le graphique Figure 5 présente les moyennes des WRAcc pour les 5 meilleurs motifs non-redondants trouvés et différents budgets de temps, sur le jeu promoters. A chaque pas de temps, chaque algorithme a été relancé et c'est pourquoi on observe parfois une légère diminution locale de la qualité des résultats sur les approches à composante aléatoire misère et HillSeqS. On constate une augmentation avec le temps de la qualité des résultats pour notre approche. Ceci est dû au fait que chaque tirage garantit la découverte d'un optimum local : on exploite des zones de l'espace de recherche de manière aléatoire et un budget temps plus important permet une plus grande exploration. Notons cependant que notre algorithme, tout comme misère, ne garantit pas une recherche exhaustive. Les consommations en mémoire en fonction du temps pour HillSeqS, pour misère et pour Beam Search sont présentés dans Figure 7. Le jeu de données utilisé est sc2. La consommation mémoire de notre algorithme est très faible : nous ne stockons dans la file de priorité que les optima locaux, un nombre négligeable au vu de tous les motifs calculés. La consommation mémoire de misère augmente au fil des tirages effectués. Ceci s'explique par notre implémentation qui reprend le pseudo-code de ) où chaque élément est ajouté dans une file de priorité : tous les éléments sont conservés. Le pic observé sur Figure  7 est dû à la mise en mémoire des jeux de données.
Le graphique Figure 8 montre que le calcul du voisinage direct vertical uniquement (en bleu) donne de meilleurs résultats en moyenne que le calcul du voisinage direct vertical et horizontal. Une explication pourrait être que le changement d'un item de la séquence (parcours horizontal) est atteignable en deux étapes verticales : la suppression et l'ajout d'un item. Ainsi, on diminue le nombre de calculs de voisins lors de la recherche d'un optimum local, ce qui augmente le nombre d'échantillonnages possibles, menant à des résultats plus intéressants en moyenne. De plus, on peut voir dans Table 4 que dépendamment du jeu de données, la représentation verticale permet un net gain de performances : la représentation sous forme de bitsets permet d'augmenter le nombre d'itérations, mais cette augmentation est variable. Ceci peut s'expliquer par la composante aléatoire de l'algorithme, et par la « forme » des zones autour des optima locaux : la vitesse de convergence vers un optimum local est variable. Un autre paramètre influençant cette hétérogénéité des augmentations est le nombre d'itemsets possibles. A chaque nouvel itemset trouvé, un calcul sur tout le jeu de données est effectué, ce qui est beaucoup plus coûteux que de faire appel à la structure de mémoization. De même, la longueur des séquences influence l'efficacité de la représentation sous forme de bitsets, et ce potentiellement différemment de la façon dont elle influence l'implémentation naïve, ceci pouvant expliquer la variation des augmentations.
Le motif suivant est donné par notre algorithme sur le jeu de données sc2. Il correspondant à une suite de constructions lors d'une partie et caractérise la victoire du joueur de type "Terran" lors des matchs "Terran vs Zerg", avec une WRAcc de 0.048 : {Barracks}, {SupplyDepot}, {Hatchery}, {SpineCrawlerBarrack}, {Barracks}}

Introduction
Les données récoltées par les capteurs sont de plus en plus mises en question au vue de la difficulté de vérifier leurs sens. Cette dernière est due à la grande masse de données récoltées mais aussi à cause d'autres facteurs comme la temporalité, l'incertitude (Ba et al., 2016), l'incomplétude, etc. L'information traitée à partir de ces données peut alors être mise en cause. Ainsi la détermination de connaissances précises et fiables nécessite donc un système vérifiant leur véracité (Lukoianova et Rubin, 2014).
Nous sommes dans un contexte de récolte des données web. Nous nous focalisons sur l'analyse des fichiers de log spécifiques à la récolte des bannières publicitaires. Le système de recueil enregistre a priori toutes les informations qui tournent autour de la publicité sur une page web (e.g. le nom du site, l'url, la catégorie, la méta-catégorie, etc.). Cet axe de travail est au départ d'une chaîne de traitements non abordée dans cet article. L'information finale dépend fortement des données d'entrées récoltées par les capteurs.
Dans ce contexte, il est important d'avoir un système vérifiant leur qualité. Nous souhaitons développer un prototype pour l'analyse de la qualité de ces données en tenant compte de leurs spécificités. Nos données sont imprécises. En effet, le nombre des bannières récoltées dans une période du temps peut varier sans forcément que le nombre réel de bannières affichées varie. Cela peut être dû à des bogues de développement, à l'emploi de technologies visant à gêner la récolte des données, etc. De plus, nous avons observé des discontinuités remarquables dans la récolte en fonction des sites.
Notre modèle d'analyse doit donc prendre en considération toute la complexité des données. Afin d'explorer les informations fournies par notre modèle, nous proposons un système visuel interactif. Dans cet article, nous présentons dans un premier temps les approches utilisées pour la définition de notre modèle. Nous présentons ensuite notre outil, appelé MMS Explore, qui met en oeuvre les indicateurs nécessaires pour évaluer la qualité des flux temporels.
Approches intégrées
Afin d'analyser la qualité du recueil, notre outil exploite les approches et notions suivantes (pour les définitions formelles, cf. Z. b. Othmane (2018)) : -Quantiles : Nous proposons de projeter les données dans leur quantile par rapport à un ensemble étudié. C'est une approche qui apporte un nouveau mode de comparaison des données en les passant du volume brute au volume relatif . Le volume relatif (quantiles) donne naissance à des chronologies de positionnement temporel interne et externe du recueil (Utkin et al., 2014). Le positionnement interne d'un média permet une comparaison de ses données à un instant vis à vis de ses données connues lors de l'analyse. On positionne la donnée récoltée à un instant t par un capteur donné dans l'ensemble des données récoltées connues par ledit capteur. Le positionnement externe correspond à la position d'une donnée à un temps t par rapport à toutes les autres données d'une sélection définie. On positionne donc la donnée par rapport à l'ensemble des autres données récoltées par les capteurs sélectionnés (Destercke et al., 2015). Ces deux informations nous permettent de construire des indicateurs de cohérence et de variabilité du flux analysé. Pour l'absence d'une donnée, l'approche lui affecte une position distinct dite Q O . Cette nouvelle position reflète l'ignorance totale sur la valeur de la récolte (absence/non-existence). Cette approche permet de couvrir de l'imprécision des données et de gérer l'incertitude résultante en les représentant dans un domaine approprié (Cappiello, 2015). Le choix des paramètres de ce domaine tel que le nombre de quantile, l'échelle, etc. sont paramétrables dans notre outil (Z. b. Othmane, 2018). -Variabilité : la variabilité est définie selon deux versions permettant de calculer un score par rapport au mouvement du recueil d'une sélection. Ces définitions permettent d'avoir des scores relatifs au mouvement interne et externe, i.e. sur la volatilité d'un mouvement vis à vis du reste mais aussi sur sa cohérence intrinsèque. Les scores engendrés nous ont permis de détecter de possibles problèmes au niveau du recueil. Ces derniers sont représentés dans notre outil par des points singuliers pouvant mettre en évidence l'existence de possibles dysfonctionnements (anomalies) au niveau des capteurs. -Stabilité : La stabilité est la mise en relation de la variabilité externe avec la variabilité interne (dite cohérence). Son objectif est de quantifier la stabilité totale, i.e. une mesure globale de la qualité du fonctionnement d'un capteur. Ce score peut se présenter comme une indicateur statique déterminé suite à une mesure sur un temps précis ou sur une catégorie précise ou bien pendant une période de temps. Ce quantifieur est défini pour un média (un capteur), une variable étudiée, et une période. -La discontinuité de la récolte : Nous sommes dans un contexte des données temporelles volumineuses, la détection des arrêts et reprises de la récolte d'un ensemble des capteurs est délicat (Cappiello et al., 2018). Notre outil intègre des graphiques aidant Z. Ben Othmane et al.
à juger la continuité des enregistrements (présence/absence par exemple). Un système d'alerte allant sur 4 positions complète le dispositif. Il indique le niveau d'urgence, i.e. le degré de la nécessité d'une intervention rapide.
Présentation de l'outil
Notre outil de visualisation interactive exploite les données de logs organisées par variables d'étude. La partie Back-end s'occupe de la préparation des données et indicateurs nécessaires à l'interface visuelle interactive contenant les tableaux de bords appropriés (partie Front-end). Ces deux parties interagissent ensemble selon les choix interactifs de l'utilisateur. Le logiciel dans ses deux parties peut appeler des scripts externes pour affiner les analyses.
L'outil propose divers dashboards, complémentaires et interagissant entre eux, qui intègrent les approches d'analyses cités ci-dessus pour valider la qualité d'un recueil. L'interactivité assurée par notre outil permet de mieux comprendre la qualité de la récolte en permettant une exploration profonde.
Afin de permettre à l'utilisateur d'avoir une idée globale sur la qualité des flux temporels, notre outil propose des :
-Visualisations en graphiques ordinaires : des graphiques permettant une navigation dans divers axes d'analyse. -Visualisations des mesures statistiques : des mesures qui se changent automatiquement à la suite d'une sélection ou un axe de travail. -Visualisations binaires : un mode de représentation d'existence/absence de la donnée selon un axe de temps ou une catégorisation appropriée. -Visualisations en méta-plan : un type de visualisation informant sur les positionnements des données. -Visualisations analytiques : appels externes à des scripts de fouille de données offrant des visualisations analytiques. Notre outil propose principalement deux types d'indicateurs : les indicateurs statiques visant à fournir une information précise par un score bien défini (voir Fig. 2), et les indicateurs dynamiques qui apparaissent dans des flux d'informations progressifs, e.g. indicateur de stabilité de n médias sur une période de temps (voir Fig. 1).
FIG. 1 -Information sur la stabilité de la récolte

Introduction
La chimie moléculaire se définit comme l'étude d'entités discrètes (appelées molécules) et correspond à la communauté la plus large de chimistes. Des centaines de millions de molécules sont connues, contenant généralement moins d'une centaine d'atomes et moins d'un millier d'électrons. Les propriétés chimiques de ces molécules dépendent des positions des noyaux et des électrons qui peuvent être calculées de manière approchée par des méthodes issues de la mécanique quantique. Avec la démocratisation de la puissance de calcul, la chimie informatique est devenue une partie essentielle de la recherche en chimie moléculaire. Mais, selon les différents paramètres utilisés, ces calculs peuvent durer de quelques heures à quelques milliers d'heures par molécule. L'apprentissage automatique et plus généralement l'intelligence artificielle appliquée à des données de chimie moléculaire promet de révolutionner la chimie dans un futur proche (Schneider, 2018;Tabor et al., 2018). Avec la récente abondance de données en chimie quantique moléculaire, de nombreux travaux ont vu le jour à un rythme accru depuis 2017. Les modèles employés sont majoritairement de deux types : les réseaux de neurones (Schütt et al., 2017(Schütt et al., , 2018Gubaev et al., 2018;Hy et al., 2018;Sinitskiy et Pande, 2018) et les méthodes à noyaux de type Support Vector Machine (SVM) ou Gaussian Process Regressions (GPR) (Nakata et Shimazaki, 2017;Bartók et al., 2017;Musil et al., 2018). Aujourd'hui, les travaux se concentrent sur la prédiction de valeurs finales, au sens où si l'énergie totale de la molécule est l'objet d'étude, alors un modèle prédisant cette énergie est entraîné. La plupart des travaux présentent des résultats prometteurs, mais travaillent sur des jeux de données très restrictifs en termes de taille et de variété de molécules ; principalement le jeu de données QM9 avec 1 million de couples géométrie/énergie sur seulement 7165 molécules contenant au maximum 23 atomes.
Les propriétés moléculaires les plus étudiées en chimie quantique concernent la réacti-vité d'une molécule (localisation des électrons les plus énergétiques, etc.) ou ses propriétés d'absorption et d'émission de lumière visible qui dépendent des états excités de la molécule. Dans tous ces cas, une description précise de l'état fondamental est nécessaire. Cela signifie connaître la position d'équilibre des noyaux, ce que l'on appelle la géométrie convergée de l'état fondamental, et connaître les fonctions d'onde des électrons. Ainsi prédire la géométrie complète à partir d'une méthode d'apprentissage automatique serait une importante avancée, permettant notamment d'économiser beaucoup de temps de calculs et permettant à terme d'accélérer et guider le criblage de nouvelles molécules. Un point crucial pour l'apprentissage automatique est la disponibilité de données homogènes ou tout du moins comparables. Or, les calculs en chimie quantique sont toujours des méthodes approchées car la résolution analytique de l'équation de Schrödinger n'est pas possible pour des systèmes contenant plusieurs électrons. Ne sont donc comparables que des calculs effectués avec les mêmes approximations de calculs (sur l'opérateur mathématique ou sur les fonctions d'onde électronique). Des bases de données de calculs homogènes sont très rares en chimie moléculaire. Il existe des bases de données expérimentales de tailles importantes dont la plus conséquente est le projet PubChem contenant plus de 96 millions de molécules (Wang et al., 2009). Il existe aussi au moins cinq bases de données théoriques pour des systèmes de la chimie des solides (comme NoMaD par exemple), mais leurs méthodes de calcul sont malheureusement radicalement différentes et assez incompatibles avec la chimie moléculaire (fonctions mathématiques localisées contre fonctions mathématiques périodiques). À l'échelle moléculaire, depuis 2013 le projet "Clean Energy" d'Harvard contient plus de 2 millions de molécules calculées afin d'en estimer leurs potentiels comme matériau photovoltaïque (https://cepdb.molecularspace.org/). Malheureusement, les données des calculs ne sont pas disponibles et ces calculs auraient aussi pu servir à bien d'autres applications. Finalement, une base de données de calculs en chimie molé-culaire, PubChemQC (Nakata et Shimazaki, 2017), a été construite par un laboratoire japonais. Elle avait pour objectif ambitieux de calculer avec des paramètres constants tous les composés de la base PubChem. Le projet est au point mort après 3,5 millions de composés calculés, mais il s'agit de la source de données primaires, libre d'accès, la plus homogène et la plus large en chimie moléculaire. Elle est beaucoup plus représentative de l'espace moléculaire que le jeu de données QM9. Nous avons donc utilisé cette source pour l'apprentissage de nos modèles.
Préliminaires
Notre objectif à terme est de pouvoir se passer du calcul de mécanique quantique ou tout du moins de prédire un bon point de départ pour l'accélérer de façon substantielle. Le premier problème qu'il faut résoudre est donc de prédire précisément la position des atomes (section 3), problème qui peut être décomposé en la prédiction de la longueur d'une liaison covalente (section 4) et d'angles. Cette longueur de liaison covalente entre deux atomes est un équilibre entre la répulsion des noyaux de charge positive, la répulsion entre les électrons de charge négative et l'attraction entre les électrons et les noyaux. Ainsi la distance d'équilibre dépend de la nature des atomes (carbone, hydrogène, oxygène...) participant à la liaison, mais est également influencée par les atomes au voisinage de la liaison car ils peuvent par exemple attirer à eux une partie des électrons et donc modifier l'équilibre de la liaison. L'influence des atomes du voisinage peut être plus ou moins forte selon leurs positions relatives à la liaison.
Les calculs dont les résultats sont disponibles sur la base PubChemQC (Nakata et Shimazaki, 2017) ont été réalisés à l'aide du logiciel de chimie quantique GAMESS avec comme paramètres la fonctionnelle B3LYP (approximation sur l'opérateur hamiltonien), l'ensemble de fonctions de base 6-31G* (approximation sur les fonctions monoélectroniques), le tout en closed shell et phase gazeuse. Nous avons récupéré pour cette étude la géométrie issue de l'optimisation de l'état fondamental. Ce sont ces données qui serviront de cibles à nos modèles prédictifs. Nous avons effectué un premier filtre grossier afin d'enlever les molécules vides ou contenant un unique atome. Afin de limiter la taille des entrées de nos modèles, nous avons fixé une taille maximale de 60 atomes (bien supérieure aux 23 atomes du jeu de données QM9), ce qui permet de garder la quasi-totalité des molécules de cette base. Nos travaux préliminaires de curation manuelle des données nous permettent d'affirmer qu'une partie de ces calculs sont faux, au sens où il n'arrivent pas à optimiser l'état fondamental de la molécule initialement demandée. Il s'agit de calculs qui ont convergé vers une autre molécule par une modification de certaines fonctions chimiques ou en plusieurs autres molécules par une dissociation. Nous considérons dans un premier temps que ces données sont valorisables en terme d'apprentissage. Cette hypothèse ne peut pas être vérifiée actuellement faute de procédure automatique de nettoyage de la base de données, qui aurait permis de comparer les performances de nos modèles avec ou sans ces calculs.
Afin d'évaluer la qualité des prédictions lors de l'entraînement et pour guider les modèles lors de la procédure d'optimisation des poids, nous utilisons l'erreur quadratique moyenne (Root-Mean-Square Error ou RMSE). PourˆyPourˆ Pourˆy i la valeur prédite pour la variable y i pour un exemple i, le RMSE de N prédictions se définit comme suit :
Lors de la prédiction d'une géométrie complète, nous adaptons cette fonction afin de prendre en compte la prédiction d'un vecteur de distances restreint aux sorties correspondant à des atomes en entrée. En effet, le nombre d'atomes variant d'une molécule à une autre, il faut masquer le vecteur de sortie. PourˆyPourˆ Pourˆy i,j la valeur prédite pour la variable y i,j pour l'atome j d'une molécule i possédant A i atomes, le PRMSE de N prédictions se définit comme suit :
Sans le masquage du PRMSE, le modèle apprendrait surtout à prédire des valeurs nulles pour les sorties ne correspondant pas à des atomes en entrée, ce qui constitue une tâche très simple et éloignée de nos objectifs.
L'ensemble de nos traitements ont été réalisés en Python à l'aide des bibliothèques TensorFlow et Scikit-Learn.
Introduction de bruit. Afin de prédire des géométries moléculaires optimisées à partir de géométries moléculaires non convergées, la situation idéale serait que les modèles apprennent à partir d'un ensemble de géométries non convergées issues de mesures ou de résultats de mé-canique moléculaire, c'est à dire un modèle théorique moins sophistiqué. Malheureusement, la base PubChemQC ne fournit que les géométries optimisées en mécanique quantique et lors de nos essais de génération nous avons été confrontés aux problèmes de l'ordre des atomes, compliquant sérieusement le calcul du RMSE. Nous avons choisi dans un premier temps d'insérer un bruit contrôlé afin de valider notre méthodologie. Le modèle devra alors prédire le bruit afin de le soustraire à la géométrie bruitée. L'introduction de bruit ne garantit donc pas que le modèle se généralisera aux données réelles, mais nous permet de valider la méthode. Le bruit que l'on introduit est un bruit gaussien (normal et identiquement distribué) de moyenne nulle. Le paramètre d'écart-type σ permet de contrôler l'amplitude avec précision, tout en générant quelques cas extrêmes. Le déplacement des atomes doit être suffisamment important pour que la tâche d'optimisation de la géométrie moléculaire soit difficile et comparable à des cas d'utilisation réels, mais suffisamment modérée pour que l'on n'inverse pas la position de couples d'atomes. Le déplacement des atomes est réalisé sur les coordonnées, ie. avant le calcul des distances. Nous avons estimé qu'il était réaliste chimiquement bruiter les coordonnées des atomes avec un bruit gaussien de paramètre σ = 17, 32 pm. Dans approximativement 95% des cas (i.e. 2σ), l'atome se retrouve ainsi à une distance comprise entre 0 et 60 pm de sa position initiale, ce qui est raisonnable. Pour cette tâche nous disposons de 2,5 millions de molécules. Afin d'évaluer la performance de notre modèle, nous calculons le PRMSE après introduction du bruit, ce qui correspond à environ 17, 31 pm. Cela revient à réduire l'erreur à environ 60 % de sa valeur initiale, et donc à prédire 40 % du bruit. Il s'agit d'un gain qui pourrait être non négligeable, même si ce n'est pas réellement utilisable pour optimiser la géométrie des molécules. Toutefois, l'analyse détaillée révèle un comportement inattendu du modèle et remet en cause la nature du bruit introduit.
En effet, l'analyse statistique des données bruitées révèle qu'ajouter le bruit sur les coordonnées plutôt que sur les distances a plus éloigné les atomes de l'origine du repère en moyenne (0.82 pm, cf. table 2). Les prédictions de notre modèle s'étendent entre -9,6 pm et 1,2 pm, alors qu'elles devraient s'étendre entre -94,8 pm et 97,2 pm. Le modèle n'arrive donc pas à suffisamment déplacer les atomes pour obtenir les géométries convergées. Pire, il semble tout juste capable de prédire une partie du biais de déplacement en prédisant en moyenne -0.23 pm avec très peu de dispersion. Cet effet est d'autant plus flagrant sur la figure 1. Il est possible de remarquer aussi que le modèle, malgré un très grand nombre de paramètres, prédit un faible nombre de valeurs discrètes. Le modèle apprend très peu, voire n'apprend rien en terme de chimie. Nous avons essayé d'introduire un bruit plus faible ou de l'introduire directement sur les distances, mais nous avons obtenu des résultats similaires. Cette expérience, montre la complexité du problème à résoudre, cependant la tâche ne nous semble pas impossible et nous donnerons quelques pistes à la fin de cet article. 
Prédiction d'une distance particulière
Les modèles décrits dans cette section ont pour objectif de prédire la distance entre des atomes partageant une liaison covalente au sein d'une molécule. L'objectif n'est donc plus de résoudre le problème de prédiction d'une géométrie moléculaire convergée complète, mais plutôt d'en résoudre une version locale simplifiée. Modèles. En plus des informations géométriques, nous ajoutons la masse et le numéro atomique de chaque atome au voisinage de la liaison. Le numéro atomique est encodé de façon booléenne (one-hot encoding). Cela a pour but de ne pas instaurer de relation d'ordre entre les différents atomes et donc a priori de mieux guider les modèles lors de l'apprentissage. Cela implique toutefois de déterminer une limite aux numéros atomiques des atomes acceptés par un modèle. En effet, cet encodage coûte un attribut pour chaque numéro atomique accepté et cela pour chaque atome au voisinage de la liaison. Afin de travailler sur des modèles de taille raisonnable, nous acceptons les atomes de numéro atomique inférieur ou égal à celui du fluor, ce qui correspond à 9 attributs encodant le numéro atomique pour chaque atome du voisinage. La classe positionnelle de chaque atome par rapport à la liaison est également représentée en one-hot encoding. Ainsi, il faut 15 attributs par atome dans le voisinage. La grande majorité des molécules de notre jeu de données étant de taille inférieure à 60 et les deux atomes composant la liaison n'apparaissant pas dans les entrées, nous choisissons de limiter le voisinage de la liaison à 58 atomes, soit une couche d'entrée de taille 870. Les molécules possédant un nombre variable d'atomes et l'entrée des modèles étant de taille fixe, nous effectuons une procédure de padding des données : lorsqu'une liaison possède moins de 58 voisins, les blocs correspondant aux atomes non définis valent zéro. La table 3 illustre les entrées de nos modèles. Ceux-ci possèdent 3 couches cachées entièrement connectées de largeur 870 et un unique neurone de sortie dont l'objectif est de prédire la distance entre les deux atomes de carbone. Nous avons pris quelques précautions afin d'éviter le sur-apprentissage de nos modèles, notamment avec le taux de désactivation aléatoire des neurones (dropout) et la dégradation des coefficients (weight decay) (cf. table 4). Les résultats que nous présentons sont les performances réalisées sur des données mises de côté avant l'entraînement, soit 20 % du jeu de données.
Données et modèles
FIG. 2 -Analyse graphique du modèle tentant de prédire des distances carbone-carbone sans horizon. À gauche, l'histogramme de distribution des erreurs. Au centre, l'histogramme de distribution des erreurs en échelle logarithmique. En haut à droite, le tracé des distances pré-dites (en ordonnée) en fonction des distances cibles (en abscisse) à mettre en relation avec l'histogramme de distribution des distances cibles en bas à droite.
Résultats
Le tableau 5 fournit les résultats de l'analyse statistique des erreurs de prédiction des modèles. Les deux modèles obtiennent des performances très satisfaisantes qui permettent d'envisager leur utilisation en pratique. La restriction au plus proche voisinage améliore significativement les performances sur notre jeu de données. Les analyses graphiques des erreurs et des prédictions (figure 2 et 3) des modèles prédisant les longueurs de liaisons entre des atomes de carbone font nettement apparaître la diminution des erreurs importantes. Malgré la quantité de données disponibles, l'espace réel présente une concentration importante sur deux types de distances. Le modèle sans horizon a tendance à ramener, entre autres, les liaisons très courtes (< 130 pm) vers 140 pm. Avec le seuil de 200 pm, une meilleure continuité des prédictions entre les différents types de liaisons apparaît. Soit le modèle sans horizon, plus complexe, ne dispose pas d'assez d'exemples pour bien prédire les distances ayant un faible effectif, soit il n'a pas encore convergé. En ajoutant l'horizon, le modèle est plus simple et possède suffisamment d'exemples pour converger rapidement vers une meilleure solution. 
Conclusion et perspectives
Nous avons réalisée une tentative ambitieuse en essayant de prédire la géométrie complète de molécules à partir d'une base de données (PubChemQC) large, diversifiée et imparfaite. La tâche que nous avons tentée d'accomplir avec ces modèles est théoriquement possible, cependant l'approche directe, la plus simple, est particulièrement inefficace. Le fait que le modèle effectue des prédictions constantes et l'impossibilité de produire de meilleurs résultats à l'issue de la recherche par quadrillage ont mené à l'abandon de la méthode pour prédire des géomé-tries moléculaires convergées, au profit d'une méthode plus locale. Toutefois, nous pouvons essayer d'en tirer quelques explications et de nouvelles pistes. Premièrement, les modèles que nous avons entraînés sont des modèles aux architectures relativement simples, avec un nombre de neurones et de connexions limité par les capacités matérielles actuelles. Des architectures plus complexes auraient pu mener à de meilleures performances pour les mêmes données. Un autre écueil pourrait être le manque de données. Même si nous travaillons sur un jeu de données conséquent, il s'agit peut-être d'une quantité insuffisante pour une tâche aussi complexe. Il est également possible que le problème soit lié à notre méthodologie et notamment à l'ajout du bruit sur les données à prédire. Enfin, il est probable, et c'est cette piste de travail que nous souhaitons privilégier pour la suite, qu'il nous manque les bons descripteurs des molécules en entrée des modèles. En effet, les travaux récents mêlant chimie moléculaire et apprentissage obtiennent des résultats très convaincants en utilisant des filtres de convolution reflétant les lois fondamentales de la physique et ayant les propriétés recherchées pour ce type d'application : invariance à l'indexation et à la translation/rotation des atomes (Schütt et al., 2018). La même logique a été déclinée pour l'utilisation de méthodes à noyaux (Bartók et al., 2017;Musil et al., 2018). Les travaux de Sinitskiy et Pande (2018) utilisent une représentation discrétisée dans l'espace (volume 3D) et entraînent des réseaux de neurones convolutifs. Il faut tout de même noter que des distances interatomiques ont été utilisées avec succès par Schütt et al. (2017) afin de prédire l'énergie totale d'une molécule en fonction de sa géométrie. Nous avons donc choisi dans un premier temps d'étudier un sous-problème plus simple.
Les modèles tentant de prédire la distance carbone-carbone travaillent sur des données parfaites, c'est à dire qu'il prédisent des longueurs de liaisons dans des molécules dont la géométrie a déjà été optimisée. Cela nous permet de confirmer notre capacité à effectuer des prédictions d'ordre géométrique en utilisant des distances interatomiques. Afin de prédire avec une haute précision l'immense majorité des distances de la base de données, de la connaissance métier a été introduite dans le modèle d'apprentissage par le biais d'un seuil. Ce seuil permet de mieux discriminer l'environnement proche ayant un fort impact sur la distance calculée. Cette information, relativement simple, limite aussi la taille des données à fournir au modèle. Nous avons également entraîné des modèles sur des liaisons plus simples comme la liaison carbone-hydrogène et la liaison oxygène-hydrogène et les performances sont du même ordre de grandeur. En complément, nous avons testé des modèles de type support vector machine (SVM) et Kernel Ridge Regression (KRR) sans obtenir de résultats aussi convaincants. Au final, seule une dizaine de cas sur plusieurs millions d'exemples semble poser des problèmes. Une application inattendue de notre modèle est la mise en évidence d'un défaut de curage de la PubChemQC avec des résultats ayant mal été calculés Ainsi notre modèle a été capable de s'entraîner sur des données imparfaites sans sur-apprendre et sa capacité en généralisation permet de mettre en exergue une partie des données de mauvaise qualité dans les données d'origine. Notre modèle peut donc être utilisé afin de vérifier qu'une molécule ne possède pas une longueur de liaison carbone-carbone aberrante ou au contraire, mettre en avant les situations exceptionnelles, importantes en réactivité chimique. Cette piste nous intéresse particulièrement dans le cadre du projet QuChemPedIA, dont un des volets vise à fournir une base de données libre, collaborative et nettoyée pour la chimie quantique moléculaire. La suite de ce travail sur les modèles localisés serait de constituer une procédure itérative combinant différents modèles (réseaux de neurones et modèles à noyaux) et d'ajouter la notion d'angles.

Introduction
Au-delà des traditionnels systèmes de recherche d'information qui permettent de sélection-ner des documents à partir de mots clefs, on peut s'intéresser plus globalement aux modèles de pertinence (Huang et Soergel, 2013), (Brouard et Nie, 2004) permettant de mettre en relation différentes entités. La recherche de tels modèles qui suppose l'élicitation de la relation liant la requête et l'information pertinente reste un sujet de recherche actuel et la définition de méthodes d'évaluation pour ces modèles est un prérequis. L'évaluation traditionnelle repose sur le paradigme de Cranfield (Harman, 2013) dans lequel un ensemble figé de documents et de jugements de pertinence réalisés par des experts pour des requêtes données servent de base de comparaison aux réponses des systèmes. L'avantage du paradigme de Cranfield est de permettre un certain contrôle. Les experts sont censés fournir des jugements fiables et le nombre de documents étant limité, il est possible d'établir des mesures de rappel (en considérant l'ensemble des réponses pertinentes existant dans le corpus). Néanmoins ce mode d'évaluation suppose la mise à disposition de ressources humaines importantes (beaucoup de jugements de pertinence à produire) et par conséquent ne peut être mis en oeuvre que sur des corpus de taille relativement limité. Ces ensembles de documents, requêtes et jugements de pertinence associés sont par ailleurs fabriqués de façon artificielle. Considérant l'existence de banques de connaissances accessibles en ligne, la recherche de solutions s'appuyant sur une évaluation participative des systèmes de recherche d'information se développe (Kazaï, 2018). Une approche consiste à rendre accessible un système d'interrogation et de recueillir in situ les actions des utilisateurs en vue d'analyses ultérieures. L'intérêt est notamment de pouvoir construire des corpus de taille beaucoup plus importante et directement liés à une véritable utilisation. L'application WIB se situe dans cette approche. Elle permet l'utilisation en aveugle de différents modèles de pertinence en vue de leur comparaison. Dans la suite, nous donnons quelques élé-ments d'ingénierie sur la prise en main des bases d'articles Wikipédia. Nous continuons en présentant l'application et les différentes tâches qu'elle permet de résoudre. Nous donnons dans la dernière partie quelques éléments architecturaux de l'application qui garantissent une intégration aisée de nouveaux modèles de pertinence. Nous concluons sur le potentiel partage de l'application avec la communauté scientifique. 
Un outil de comparaison
En l'état actuel, l'application s'appuie sur le modèle de pertinence ECHO (Brouard, 2012) mais l'objectif est d'intégrer d'autres modèles pour les comparer entre eux. Toutes les actions utilisateurs regroupées par session d'utilisation sont stockées dans une base de données. Ces actions et en particulier les choix d'ajout à la requête peuvent être interprétés comme des retours indiquant une satisfaction de l'utilisateur et pourront étre exploitées par la suite pour comparer les différents modèles. L'application a été conçue avec le souci de permettre une intégration facile d'autres modèles de pertinence. Toutes les interactions de l'application WIB avec le modèle de pertinence sous-jacent se matérialise par un appel AJAX. Pour pouvoir être intégré à l'application un nouveau modèle de pertinence doit se conformer à une API bien définie et être mentionnée dans le fichier de configuration de l'application. Cette API consiste en un simple script php recevant les requêtes composites au format JSON et retournant les 100 premiers résultats pour chaque facette (termes, documents, catégories) dans ce même format.
Conclusion et perspectives
Le calcul de certains indices de performance vont être ajoutés à l'application pour pouvoir comparer les différentes instances entre elles. Le code de cette application pourra à terme être mis à la disposition de tous permettant à chacun de créer sa propre instance avec son modèle de pertinence ou ses systèmes dédiés mais quelques efforts sont encore nécessaires pour faciliter le déploiement de nouvelles instances de l'application. L'instance de l'application (s'appuyant sur le modèle ECHO) qui est déjà en ligne : http ://echo.imag.fr/apps/echopedia/ nous permettra d'obtenir un premier retour d'utilisation.

Introduction
La classification multi-label a reçu une attention considérable au cours de la dernière décennie et récemment, stimulée par des applications impliquant de grands ensembles de données, elle a été étendue à des problèmes où le nombre de labels peut dépasser le million (Agrawal et al., 2013). Dans ce nouveau contexte, appelé apprentissage multi-label extrême (noté XML en anglais pour eXtreme Multi-Label Learning) où les données sont très creuses et ont un très grand nombre de dimensions, les algorithmes classiques ne parviennent pas à passer à l'échelle ou voient leurs performances prédictives se dégrader. Pour tenter de surmonter ces difficultés, la recherche s'est récemment orientée vers trois directions : (i) utiliser des astuces d'optimisation et la parallélisation sur des "superordinateurs" (Yen et al., 2017), (ii) réduire la dimension des données pour obtenir un problème latent soluble avec des approches multi-label classiques (Bhatia et al., 2015) ou (iii) partitionner hiérarchiquement le problème initial en sous-problèmes à petite échelle (Prabhu et Varma, 2014). La décomposition arborescente de la troisième stratégie présente plusieurs avantages. En découpant l'apprentissage en sous-tâches, elle réduit la complexité de l'entraînement et des prédictions et ouvre la voie à la parallélisation. Et, sa séquence de décisions successives permet une grande expressivité.
Motivés par ces propriétés, nous présentons une nouvelle approche arborescente rapide et précise appelée CRAFTML (Clustering-based RAndom Forest of predictive Trees for extreme Multi-label Learning). Comme FastXML (Prabhu et Varma, 2014) qui fait partie des meilleures approches arborescentes pour l'apprentissage multi-label extrême, CRAFTML est une forêt d'arbres de décision supervisés où les conditions de séparation des instances à chaque noeud sont multivariées. Mais CRAFTML a deux différences de fond avec FastXML : (i) il implémente de la diversité entre les arbres avec une stratégie qui s'approche de celle des "forêts aléatoires" mais qui s'en distingue non seulement car il remplace les sélections aléatoires par des projections aléatoires pour préserver plus d'informations mais aussi car il les applique aux labels en plus des attributs ; (ii) il utilise une nouvelle stratégie de séparation qui est plus simple et qui a une très faible complexité. Des expériences numériques sur huit jeux de données de la littérature multilabel extrême montrent que CRAFTML dépasse les approches arborescentes XML avec un temps d'apprentissage inférieur et une consommation de mémoire plus faible. Il est également compétitif avec les autres approches de l'état de l'art XML.
Le reste de cet article de résumé est organisé comme suit. La section 2 rappelle brièvement les travaux récents dans le domaine de l'apprentissage multi-label extrême. La section 3 décrit notre nouvelle proposition CRAFTML. La section 4 compare les performances de CRAFTML avec l'état de l'art XML. Les références de toutes les méthodes citées ici sont disponibles dans l'article original (Siblini et al., 2018).
Travaux connexes
En raison de l'intérêt croissant de l'apprentissage multi-label au cours de la dernière décennie, de nombreux algorithmes ont été proposés (Zhang et Zhou, 2014). Plusieurs expé-rimentations numériques ont mis en évidence les bonnes performances de la forêt aléatoire multi-label RF-PCT et de la méthode hiérarchique HOMER. Cependant, ces algorithmes ne sont pas adaptés aux dimensions (10 5 à 10 7 ) de l'apprentissage multi-label extrême. Trois stratégies différentes sont aujourd'hui développées pour résoudre le problème de passage à l'échelle : les astuces d'optimisation et de parallélisation, la réduction de dimension et la décomposition hiérarchique arborescente.
Dans ce résumé, nous nous concentrons particulièrement sur les méthodes arborescentes (LPSR, FastXML, son extension PFastReXML et PLT). De façon générale, celles-ci transforment le problème initial à grande échelle en une série de sous-problèmes à petite échelle en partitionnant de manière hiérarchique l'ensemble des instances ou l'ensemble des labels. Ces différents sous-ensembles sont associés aux noeuds d'un arbre. L'ensemble initial associé à la racine est partitionné en k sous-ensembles qui sont eux-mêmes associés aux k noeuds enfants de la racine. Le processus de décomposition est répété jusqu'à ce qu'une condition d'arrêt soit vérifiée sur les sous-ensembles. Dans chaque noeud, deux problèmes d'optimisation sont soulevés : (i) construire une partition pour optimiser un critère donné, et (ii) définir une condition ou construire un classifieur pour décider, à partir des attributs d'une instance donnée, du sous-ensemble de la partition auquel l'associer. Dans la phase de prédiction, une nouvelle instance suit un chemin de la racine jusqu'à une feuille (arbre d'instances) ou plusieurs feuilles (arbre de labels) déterminées par les décisions locales successives dans les noeuds puis, des prédictions en sont déduites. Pour un arbre de labels, les labels associés aux feuilles atteintes sont celles prédites avec une probabilité non nulle. Pour un arbre d'instances, la prédiction est donnée par un classifieur local appris uniquement à partir des instances de la feuille atteinte.
Dans la littérature XML, trois approches arborescentes récentes ont été proposées : LPSR et FastXML respectivement basées sur un seul arbre d'instances k-aire et une forêt d'arbres d'instances binaires (k = 2), et PLT basé sur un arbre de labels. LPSR vise à regrouper dans un même sous-ensemble les instances qui partagent des attributs et des labels communs. FastXML vise à minimiser une fonction de perte basée sur le "nDCG" (normalized Discounted Cumulative Gain) qui tend à regrouper des instances avec des labels communs. FastXML a été récemment étendu à une nouvelle version appelée PFastReXML afin de mieux prendre en compte la distribution à longue traîne des labels. PLT construit un arbre de labels en séparant récursivement les labels en sous-ensembles pour regrouper ceux qui co-occurrent dans les mêmes instances. La récursion s'arrête lorsque les sous-ensembles terminaux contiennent un seul label. Un classifieur multi-label est ensuite appris à chaque noeud pour permettre d'estimer des probabilités de suivre les différents chemins racine-feuille de l'arbre conditionnellement aux attributs. L'apprentissage est tel que les chemins se terminant sur les feuilles associées aux labels pertinents ont une forte probabilité.
Pour aller vers des stratégies de séparation moins complexes que l'état de l'art et pour intégrer une diversité entre les arbres adaptée à l'apprentissage extrême, nous introduisons une nouvelle approche arborescente appelée CRAFTML.
CRAFTML
CRAFTML construit une forêt d'arbres d'instances k-aires en suivant le schéma commun des méthodes basées sur les arbres d'instances rappelé dans la section 2. Sa stratégie de partitionnement de noeuds consiste à regrouper les instances avec des labels communs dans les mêmes noeuds/feuilles en visant à satisfaire deux contraintes : chaque arbre/noeud doit apprendre sur des instances projetées aléatoirement pour assurer la diversité et le processus de partitionnement doit effectuer des opérations de faible complexité pour assurer le passage à l'échelle. Par conséquent, l'apprentissage d'un noeud dans CRAFTML suit les trois étapes ci-dessous :
1. Projeter aléatoirement, dans des espaces de dimension réduite, les attributs et les labels des instances du noeud. Ces projections, dont les coefficients sont générés sur demande avec une graine et un "hash", sont orthogonales et creuses de type hashing trick qui est empiriquement efficace sur les données XML (Siblini et al., 2018).
2. Partitionner les instances en k sous-ensembles temporaires en appliquant l'algorithme des k-moyennes sphériques (algorithme de Loyd) sur la matrice des labels projetés. La métrique du cosinus est utilisée car elle est rapide et adaptée aux données creuses. Les centroïdes des clusters sont initialisés avec la stratégie k-means ++ qui améliore la stabilité et les performances de l'algorithme par rapport à une initialisation aléatoire.
projetés. Puis, partitionner les instances en k sous-ensembles finaux (noeuds enfants) selon la classe que leur associe le classifieur.
Les étapes 2 et 3 sont accélérées avec une stratégie d'échantillonnage sans remise. Une fois qu'un arbre a été formé, chaque feuille stocke un vecteur correspondant à la moyenne des vecteurs de labels de ses instances. Dans la phase de prédiction, pour chaque arbre, l'instance d'entrée suit un chemin racine-feuille déterminé par les décisions successives des classifieurs des noeuds parcourus et la prédiction fournie est le vecteur stocké dans la feuille atteinte. La forêt agrège les prédictions des arbres avec une simple moyenne. Les principales similitudes et différences entre CRAFTML et les autres approches arborescentes de l'état de l'art sont résumées dans le tableau 1. Grâce aux projections aléatoires, à la simplicité du séparateur, et à l'exploitation du caractère creux des données, les complexités spatiales et temporelles de CRAFTML sont faibles et indépendantes du nombre d'attributs et de labels (Siblini et al., 2018) ce qui permet à l'approche de passer à l'échelle en XML.
Comparaisons expérimentales
Nous comparons CRAFTML avec les meilleures méthodes arborescentes de l'état de l'art (FastXML, PFastReXML, LPSR, PLT) présentées dans la section 2 sur huit ensembles de données apprentissage/test de référence en XML détaillés dans le repository 1 . Un hyperparamètrage standard a été determiné pour CRAFTML et a été fixé pour l'intégralité des expérimentations (Siblini et al., 2018). Il assure une comparaison équitable avec FastXML et son extension. Le nombre de labels d y , le nombre d'attributs d x , le nombre d'instances d'apprentissage n et le nombre d'instances de test n S des jeux de données sont rappelés dans le tableau 2.
CRAFTML a de meilleures performances prédictives que les autres approches dans la plupart des cas. Pour WikiLSHTC-325K et Amazon-670K, la domination de PFastreXML s'explique en partie par le fait que ce dernier est entraîné avec les propensions des labels calculées avec des informations externes supplémentaires (hiérarchie des labels de Wikipedia et Amazon). Les comparaisons en temps de calcul et en consommation mémoire ne sont pas 1. http://manikvarma.org/downloads/XC/XMLRepository.html
Bibtex
Delicious EURLex-4K Wiki10-31K dx = 1836, dy = 159 dx = 500, dy = 983 dx = 5000, dy = 3993 dx = 101938, dy = 30938 n = 4880, nS = 2515 n = 12920, nS = 3185 n = 15539, nS = 3809 n = 14146, nS = 6616  P@1  P@3  P@5  P@1  P@3  P@5  P@1  P@3  P@5  P@1  P@3  P@5  détaillées ici mais montrent que CRAFTML est très compétitif (Siblini et al., 2018). Comparé aux autres méthodes, son temps d'entraînement observé est inférieur en moyenne et la taille de son modèle est plus petite. Ces mesures sont cohérentes avec les résultats théoriques : en raison de la stratégie d'échantillonnage et de la réduction de la dimension résultant des projections aléatoires, les complexités de CRAFTML sont les plus faibles. Son temps de prédiction est cependant plus élevé que les autres même si sa complexité est équivalente. Nous avons également comparé CRAFTML aux autres approches non parallélisées de l'état de l'art XML (SLEEC, PDSparse, AnnexML) et les performances prédictives sont comparables mais CRAFTML est plus rapide et/ou consomme moins de mémoire (Siblini et al., 2018). En comparaison avec les méthodes conçues pour la parallélisation et nécessitant des ordinateurs avec des centaines de coeurs (PPDSparse, DISMEC) les conclusions sont mixtes et dépendent du jeu de données. Les jeux WikiLSHTC-325K et Amazon-670K semblent favoriser les deux approches parallèles par rapport à toutes les méthodes arborescentes. Cependant, en termes de complexités, CRAFTML est beaucoup plus intéressant que DISMEC et PPDSparse.
Conclusion
Notre nouvelle méthode d'apprentissage multi-label extrême CRAFTML est compétitive avec les méthodes de l'état de l'art extrême. Et, contrairement à ces dernières, elle ne s'appuie pas sur un schéma d'optimisation complexe. Elle combine des blocs d'apprentissage simples et rapides (par exemple un clustering avec k-means, un classifieur multi-classe très naïf), ce qui permet d'envisager des extensions et d'atteindre les performances requises par les défis sociétaux et techniques actuels. Avec la dimensionnalité croissante des données, l'apprentissage automatique recourt de plus en plus aux supercalculateurs. Mais cet accès est loin d'être disponible partout aujourd'hui et son coût va fixer des limites à l'avenir. Par conséquent, (i) des algorithmes d'apprentissage machine économes en ressources et évolutifs sont nécessaires pour favoriser la démocratisation des nombreuses applications du monde réel qui dépendent encore

Introduction
Le méthane est le deuxième gaz à effet de serre après le dioxyde de carbone mais il a un pouvoir de réchauffement global 23 fois supérieur. En Europe, la quasi-totalité des émissions de méthane sont d'origine agricole parmi lesquelles deux tiers proviennent de l'élevage des ruminants. Pendant la digestion, la matière végétale est dégradée par l'écosystème microbien du rumen et produit entre autres, des gaz de fermentation dont le méthane. Les composés phénoliques des plantes peuvent avoir un effet sur les fermentations ruminales et diminuer la production de méthane. Cependant, la très grande diversité des structures chimiques possibles pour ces composés ne permettait pas de tester pour tous, leur activité.
Ainsi, notre stratégie a été d'effectuer un essai de criblage par fermentations in vitro sur 208 plantes et d'identifier parmi celles-ci, les plantes bio-actives contre le méthane. Parallèlement le profil en composés phénoliques de chacune de ces plantes a été déterminé. L'analyse par HPLC-DAD confirmait la présence d'un composé par un pic. Celui-ci était alors caractérisé par son temps de rétention sur la colonne et son spectre dans l'ultra violet. A ce stade, les composés ne pouvaient pas être identifiés car il y avait en moyenne plus d'une centaine de pics par plante. La priorité était de sélectionner quelques composés susceptibles d'être responsables de l'effet observé sur le méthane.
Compte-tenu de la très grande fluctuation des profils en termes d'importance relative des composés et de leur faible taux de présence dans les plantes, il n'était pas possible d'établir des corrélations entre les composés et l'effet observé par les méthodes classiques de l'analyse statistique. Nous avons donc eu recours à la fouille de données, et plus particulièrement aux règles d'association de classe (Srikant et al., 1997), pour faire émerger les composés susceptibles d'avoir un effet positif. Nous nous focalisons ici sur la recherche des composés actifs et non sur les synergies car il était impératif de sélectionner d'abord un nombre raisonnable de composés. En effet, les phases suivantes qui sont, d'une part, l'identification des composés et, d'autre part, la vérification in vitro de l'effet escompté, sont onéreuses en temps et en coût.
L'article s'organise donc de la façon suivante. La section 2 présente les données et la façon dont elles ont été acquises. Le reste de l'article se consacre à la procédure d'extraction des composés prometteurs (sections 3 et 4).
Présentation des données
Les substrats qui ont été utilisés pour les fermentations in vitro et pour la détermination des profils en composés phénoliques, ont été obtenus à partir de 208 espèces de plantes.
Les variables prémisses
Les composés phénoliques sont extraits des substrats par un traitement éthanol : eau puis séparés avec une chaîne HPLC. Le profil chromatographique de chaque plante est enregistré à 280 nm. Un composé de référence, la flavone, a été utilisé pour le calcul des temps de ré-tention relatifs (T i ) des pics. L'alignement des séquences est réalisé en repositionnant les T i des standards. Les composés des plantes étant inconnus, ils sont identifiés (noms des variables prémisses) par leur temps de rétention relatif
Il a été détecté au total dans le jeu de 208 plantes, 1 075 composés différents. Le nombre de composés différents trouvés en moyenne par plante est de 106. Le nombre d'occurrences d'un composé dans le jeu de plantes est très variable allant d'une unique apparition à une fréquence d'apparition de près de 58%. En moyenne, la fréquence d'apparition est de 10%. Les données des variables prémisses sont les aires des pics si le composé est présent.
Les données brutes sont donc structurées en une matrice 208 (plantes) x 1 075 (composés) à 280 nm contenant les valeurs numériques des aires des pics. Cette matrice a un taux de remplissage faible de 10%. Les composés omniprésents dans les plantes (fréquence > 30%) ont été retirés du jeux de données pour éviter le risque de faux-positifs, c'est-à-dire 28 composés. Les données ont ensuite été binarisées avant la fouille comme indiqué dans la partie suivante.
La variable cible
La particularité de ce travail de fouille de données est qu'il ne comporte qu'une seule variable cible : le méthane. La production de méthane mesurée in vitro est un vecteur colonne de dimension 208 sans aucune données manquantes, dont les valeurs (moyenne de 3 répétitions) sont des ratios compris entre 0,10 et 1,33. Cette variable a été transformée pour calculer un index anti-méthanogène. Toute plante qui a un index supérieur à 0 a un effet anti-méthanogène très significatif (p<0,01). L'index est converti en données binaires et nommé indM eO. L'effet anti-méthanogène est présent (index >0) chez 64 plantes, soit environ 30% de l'effectif, pour lesquelles indM eO a pris la valeur 1.
Extraction des composés potentiellement prometteurs
La technique d'extraction des règles d'association (Agrawal et Srikant, 1994) nécessite que les données soient sous la forme binaire. Comme nous l'avons dit dans la section 2, la particularité de cette base de données est qu'elle est éparse puisque les plantes ne possèdent qu'une centaine de composés en moyenne parmi les 1 075 qui ont été détectés. Nous allons donc discrétiser les variables numériques prémisse de la façon suivante : la valeur 1 sera attribuée pour tous les composés exprimés, c'est-à-dire pour toutes les valeurs supérieures strictement à 0 ; et la valeur 0 pour les composés non exprimés.
L'extraction a été effectuée en utilisant la bibliothèque arulesViz (Hahsler, 2017) du logiciel R. Nous avons retenu comme seuil minimum pour le support 1 , la valeur de 0,025, soit vérifié par au moins 6 individus (substrats), et comme seuil minimum pour la confiance 2 , la valeur de 0,50. La base de données possède 30% de plantes anti-méthanogènes ce qui se traduit par sup(indMe0)=0,30. Par conséquent, le seuil retenu pour la confiance nous garantit que les règles extraites sont obligatoirement dans la zone attractive, c'est-à-dire la zone où conf (T ⇒ indM eO) > sup(indM eO).
2 892 règles de classe ont été extraites dont 26 de niveau 2, c'est-à-dire les règles composées de 2 items et par conséquent avec un seul composé en prémisse.
Afin d'aider les biologistes dans le choix des composés prometteurs, nous avons proposé la visualisation représentée dans la figure 1. Un tel graphique n'est intéressant et lisible que dans le cas d'un nombre limité de règles et avec des valeurs pour la confiance pas trop proches de 1.
Ce graphique nous restitue les informations suivantes :
1. Le nombre d'individus vérifiant la prémisse T i ou support absolu sup abs (T i ) du composé T i grâce à la longueur du segment de droite (segments rouge et bleu).
2. Le nombre d'exemples ou le support absolu sup abs (T i indM eO) de la règle T i ⇒ indM eO grâce à la longueur du segment de droite qui se situe à gauche de la droite d'équation x = 0 (segment rouge).
3. Le nombre de contre-exemples ou le support absolu sup abs (T i indM eO) grâce à la longueur du segment de droite qui se situe à droite de la droite d'équation x = 0 (segment bleu). On rappelle que sup abs (T i ) = sup abs (T i indM eO) + sup abs (T i indM e0).
FIGURE 1 -Visualisation des 9 meilleures règles T i ⇒ indM eO.
absolu sup abs (indM eO) n'est pas représenté sur le graphique car d'une part, c'est la même valeur pour toutes les règles, et d'autre part, la valeur de son support est environ six fois plus élevée que la valeur du support de toutes les règles extraites. On note que dans le cas où deux règles ont la même valeur pour la mesure choisie sur l'axe des ordonnées, nous effectuons une translation de la représentation de la seconde règle selon l'axe des abscisses. C'est le cas notamment pour les composés T0.9756 et T0.4404.
Après avoir extrait les composés potentiellement prometteurs sur la base de données binaires, nous confrontons ces résultats avec les données initiales, c'est-à-dire avec les données numériques, afin de prendre en compte l'intensité d'expression des composés.
Sélection des composés les plus prometteurs
Plus un composé est fortement présent dans une plante, plus la valeur de celui-ci sera élevée. Ainsi, pour notre problématique, une règle de classe sera d'autant plus intéressante que le composé phénolique sera fortement exprimé, donc aura de fortes valeurs. Par conséquent, les règles qui vont particulièrement nous intéresser sont celles où les fortes valeurs pour le composé T i sont présentes, règles que nous pouvons formaliser de la façon suivante : T i ≥ v ⇒ indM e0 avec v une valeur prise par le composé T i .
Afin de détecter ce type de règles, nous retenons la stratégie suivante que nous expliquons en nous appuyant sur un exemple.
La figure 2 restitue l'ensemble des valeurs prises par le composé T0.6696, et ceci par catégorie de substrats, c'est-à-dire ceux pour lesquels il n'y a aucun effet sur les émissions de méthane et ceux pour lesquels il y a un effet positif (diminution de méthane). Nous recherchons donc la valeur optimale v opt du composé où la proportion de substrats ayant un effet positif est supérieure à la proportion de substrats n'ayant aucun effet.
Pour se faire, nous allons évaluer toutes les règles pour chacune des valeurs prises par le composé T i , excepté évidemment la valeur minimale. Comme nous voulons que ces règles vérifient le support minimum déterminé par l'utilisateur, nous n'allons évaluer qu'un sousensemble des règles possibles. Afin de formaliser notre stratégie d'extraction, nous définis-sons les notations suivantes. Soit t i le nombre de valeurs distinctes prises par le composé T i et soit {v i1 , .., v ik , .., v iti } avec k ∈ {1, .., t i } l'ensemble des valeurs ordonnées prises par le composé. Soit s le support absolu minimum déterminé par l'utilisateur. Nous recherchons donc la ou les meilleures règles au regard d'une mesure de qualité (confiance, leverage, ...) choisie par l'utilisateur parmi toutes les règles suivantes :
Voici un exemple de règle extraite : T 0.6696 ≥ 60, 33 ⇒ indM eO avec une valeur pour la confiance de 0,875 et une valeur pour le support de 0,034. La valeur de la confiance de la règle binaire T 0.6696 ⇒ indM eO extraite précédemment est de 0,54 et la valeur du support de 0,034. Il y a une amélioration importante de la confiance lorsque le composé T 0.6696 est présent sous la forme d'un pic majeur (> 1 000 mAU).
Afin de nous guider dans le choix final de ces règles, nous utilisons une nouvelle mesure, l'intensité d'expression de la règle Int exp , qui va nous renseigner sur l'intensité de la règle par rapport à l'intensité moyenne du composé. C'est le rapport entre la moyenne des valeurs prises par la règle numérique, c'est-à-dire la moyenne des valeurs supérieures à v ik , et la moyenne des valeurs prises par le composé T i :
Int exp (T i ≥ v ik ⇒ IndM eO) = moy(T i ≥ v ik ) moy(T i ) Ainsi, plus l'intensité de la règle est supérieure à 1, meilleure sera celle-ci. La règle T 0.6696 ≥ 60, 33 ⇒ indM eO a une intensité d'expression Int exp de 1,51. C'est une règle prometteuse, donc un composé à étudier.
A l'issue de cette étape, 7 composés ont montré un effet seuil qui permet d'améliorer encore la confiance : T 0.9792, T 0.0492, T 0.9756, T 1.0008, T 1.0464, T 0.6696, T 0.5784.
Conclusion

Introduction
Le "biclustering" est une technique de fouille de données qui permet de retrouver des motifs sous la forme d'une sous-matrice dans une matrice ou un tableau de données à deux entrées [Madeira et Oliveira (2004)]. Il est lié au "clustering" [Arabie et al. (1996)] dont l'objectif est de regrouper des lignes de la matrice en fonction des similarités qu'elles présentent. Pour une sous-matrice considérée, la similarité entre les lignes doit alors être vérifiée pour toutes les colonnes. D'une façon alternative, le biclustering consiste à regrouper les lignes et les colonnes d'une sous-matrice simultanément, en fonction d'un certain critère de similarité. Par exemple, dans le biclustering à colonnes constantes (CC), pour chaque ligne d'une sous-matrice les valeurs dans les colonnes sont constantes et (possiblement) différentes d'une colonne à l'autre.
Pour mettre en oeuvre le biclustering, nous utilisons dans cet article l'analyse formelle de concepts (FCA pour "Formal Concept Analysis" [Ganter et Wille (1999)]) et les "Pattern Structures", une extension de la FCA pour traiter les tableaux numériques [Ganter et Kuznetsov (2001); Kaytoue et al. (2011)]. La FCA est très liée au biclustering car elle cherche aussi à regrouper les lignes et colonnes d'un tableau de données binaires en fonction d'une similarité qui consiste à partager les mêmes croix (ou 1) pour chaque ligne d'une sous-matrice.
Nous présentons ici deux méthodes pour découvrir des biclusters dits "à changements de signes cohérents" (CSC), comme défini dans [Madeira et Oliveira (2004)], à partir d'une matrice de signes. Cette matrice est obtenue en deux étapes : (i) en "échelonnant" ("scaling") la matrice numérique d'origine où ont été recherchés les biclusters à colonnes constantes (CC),
(ii) en appliquant des "partition pattern structures" dans la matrice des signes pour retrouver des partitions de lignes exhibant des changements de signes cohérents.
Le biclustering a été beaucoup utilisé pour analyser des données biologiques et en particulier des données d'expression de gènes. Ainsi l'algorithme de biclustering CSC SAMBA est proposé dans [Tanay et al. (2002)], où dans la matrice des données, les gènes sont en ligne et les conditions expérimentales en colonne. Il s'agit alors de découvrir des sous-matrices où les conditions affectent les gènes de façon cohérente, c'est-à-dire que chaque couple de conditions produit toujours le même effet, positif ou négatif, dans la sous-matrice. La matrice des données est considérée comme un graphe biparti, où les sommets se divisent en gènes et conditions, et où une arête existe entre un sommet de type "gène" g et un sommet de type "condition" c si c affecte g. Un bicluster correspond alors à une biclique du graphe biparti, ce qui est équivalent à la recherche de concepts en FCA standard.
Dans le même esprit, l'algorithme QUBIC proposé dans [Li et al. (2009)] recherche aussi tous les biclusters CSC dans une matrice de données. Un graphe pondéré est construit, où chaque gène est représenté par un sommet, et deux sommets sont reliés par une arête en fonction du degré de leur similarité. Ce degré de similarité fournit la pondération attachée à l'arête. Ensuite, la tâche de biclustering consiste à retrouver les sous-graphes de pondération maximale.
Pour notre part, nous explorons dans cet article la découverte de biclusters CSC en utilisant les techniques de FCA et l'extension des patterns structures pour traiter les données numériques. Tout d'abord nous décrivons la relation existante entre biclustering et FCA (section 2). Puis nous présentons une approche originale de recherche de biclusters à changements de signes cohérents et les expérimentations réalisées (section 3), avant de conclure le papier.
Biclustering et FCA
Biclustering
Dans un tableau de données numériques, considéré comme une matrice, il est intéressant de rechercher des sous-matrices où les éléments ont tous la même valeur (table 1a). Cette tâche correspond à la recherche de biclusters à valeurs constantes, ce qui revient exactement à calculer les concepts en FCA. Il peut aussi être intéressant de découvrir des biclusters dits "à colonnes constantes" (CC) où les valeurs sont identiques pour chaque colonne de la sousmatrice (table 1b), ce qui a directement des applications en recommandation où le bicluster représente un groupe d'individus (en ligne) donnant la même note à un produit (en colonne). Enfin, dans un bicluster "à changements de signes cohérents" (CSC, table 1c), les éléments de la sous-matrice sont considérés comme des symboles, soit '' ou soit ''. Les signes sont corrélés dans le "même sens" ou dans le "sens opposé", par exemple la colonne 1 en table 1c) est identique à la colonne 2 et opposée à la colonne 3.
Dans cet article, nous étudions les deux types de biclusters CC et CSC, que nous appliquons à la fouille de "motifs graduels" [Di-Jorio et al. (2009)]. La recherche de biclusters de type CC en FCA a été introduite dans [Codocedo et Napoli (2014)] et s'appuie sur les "partition pattern structures". Elle est expliquée dans la section suivante.
TAB. 1 -Trois types de biclusters : (a) "à valeurs constantes", (b) "à colonnes constantes" (CC), et (c) "à changements de signes cohérents" (CSC).
TAB. 2 -Un tableau de données numériques avec 4 objets (en ligne) et 5 attributs (en colonne).
Biclustering de type CC et "partition pattern structures"
La FCA est un formalisme mathématique qui s'appuie sur la théorie des treillis et qui est utilisé en classification et en fouille de données [Ganter et Wille (1999)]. La FCA s'applique à un tableau de données binaire, calcule les concepts formels, qui correspondent à des rectangles maximaux de croix dans le tableau binaire, et les organise en un treillis de concepts. Les "pattern structures" généralisent la FCA et sont utilisées pour traiter des données complexes comme les données numériques (entre autres).
Les "partition pattern structures" (PPS) ont été étudiées dans le cadre de la recherche de dépendances fonctionnelles [Baixeries et al. (2014)] mais aussi pour la recherche de biclusters de type CC dans une matrice numérique. Ici une partition correspond à un regroupement des objets (en ligne) en fonction des valeurs de leurs attributs (en colonne). Ainsi, un exemple de "partition pattern concept" (pp-concept) est donné par ({m 1 , m 4 }, {{g 1 , g 2 }, {g 3 , g 4 }}) dans la table 2. Deux biclusters de type CC sont obtenus, ({g 1 , g 2 }{m 1 , m 4 }) et ({g 3 , g 4 } {m 1 , m 4 }). Par manque de place, nous ne pouvons détailler les calculs qui le sont en revanche dans [Codocedo et Napoli (2014)].
Biclustering CSC et "partition pattern structures"
Dans cette section, nous présentons deux approches pour mettre en oeuvre les biclustering CSC en s'appuyant sur les "partition pattern structures" (PPS). D'abord nous décrivons comment résoudre le problème avec un échelonnage (scaling) de la matrice des données. Ensuite, nous montrons comment les biclusters CSC peuvent être retrouvés directement avec PPS. Dans la première approche (algorithme 1), à partir d'une matrice binaire nous construisons une nouvelle matrice dans laquelle chaque colonne décrit la cohérence entre deux colonnes de la matrice d'origine. Puis nous appliquons PPS pour calculer les biclusters CC.
Considérons la matrice des signes données dans la table 3a. Cette matrice peut être échelon-née pour produire l'apposition des trois matrices binaires données dans la table 3b, où chaque couple de colonnes (c i , c j ) constitue une nouvelle colonne. Dans une colonne (c i , c j ), la valeur Input : Une matrice des signes binaires S avec un ensemble de colonnes C Output : Un ensemble de biclusters CSC extrait de S 1 all_csc := ∅ ; 2 foreach c x ∈ C do 3 À partir de tous les couples d'attributs (c x , c y ), x < y, construire une matrice échelonnée T ;  Pour éviter l'échelonnage et la combinaison des colonnes deux à deux, nous pouvons appliquer directement PPS à la matrice originale. C'est l'objet du second algorithme (algorithme 2), où les biclusters CSC sont découverts en examinant les pp-concepts engendrés.
Un pp-concept (A, d) est composé d'un ensemble d'attributs A et d'un ensemble de composants de partitions p. De plus, comme tout couple (p, A) est un bicluster CC, un bicluster CSC est soit un bicluster CC ou encore un couple de biclusters CC de signes opposés.
Par exemple, dans la matrice donnée en table 3a, trois biclusters CC sont engendrés à partir du pp-concept ({c 2 , c 3 , c 4 }, {{r 1 , r 2 }, {r 3 }, {r 4 }}).
-
biclusters CC biclusters CSC ({r 2 , r 4 }{c 1 c 2 , c 1 c 3 , c 1 c 4 }) ({r 2 , r 4 }{c 1 , c 2 , c 3 , c 4 }) ({r 1 , r 3 }{c 1 c 2 , c 1 c 3 }) ({r 1 , r 3 }{c 1 , c 2 , c 3 }) ({r 1 , r 2 , r 3 , r 4 }{c 2 c 3 }) ({r 1 , r 2 , r 3 , r 4 }{c 2 , c 3 }) ({r 1 , r 2 , r 4 }{c 2 c 3 , c 2 c 4 }) ({r 1 , r 2 , r 4 }{c 2 , c 3 , c 4 })
TAB. 4 -Quelques biclusters CC de la table 3b et leurs biclusters CSC correspondant dans la table 3a.
Input : Une matrice binaire de signes S Output : Un ensemble de biclusters CSC de S 1 all_csc := ∅ ; 2 all_ppc := tous les pp-concepts de S 3 foreach ppc ∈ ppcs do 4 Créer une nouvelle partition en fusionnant deux "partitions opposées" ;
5
Ajouter les éléments de la nouvelle partition à all_csc ; 6 end 7 return all_csc ; Algorithme 2 : Biclustering CSC sans échelonnage.
Ici, b x est "opposé" à b z car le signe de b x dans chaque colonne (' ) est l'inverse du signe correspondant dans b z (' ). De fait, ({r 1 , r 2 , r 4 }, {c 2 , c 3 , c 4 }) est un bicluster CSC. Quant à b y , il est lui-même un bicluster CSC puisqu'il n'est en "opposition" avec aucun autre bicluster CC.
Dans nos expérimentations, nous avons comparé les temps d'exécution des deux approches appliquées à des jeux de données numériques générés de façon aléatoire. À partir d'un tableau de données de taille m × n, les deux méthodes construisent une matrice binaire de signes de taille C m

Introduction
Parmi les différentes techniques d'analyse exploratoire de données, la découverte de sousgroupes (Klösgen (1996)) vise à identifier des régions dans les données qui se détachent par rapport à une cible. Le principe est d'identifier des ensembles d'objets définis en intention qui sont fortement associés à certaines valeurs de la cible. Dans cet article nous proposons de géné-raliser cette approche au cas où l'on a plusieurs attributs cibles numériques. On cherche alors à la fois un sous-groupe d'objets défini par une conjonction de restrictions sur un ensemble d'attributs descriptifs et un sous-ensemble d'attributs cibles dont les valeurs sont fortement correlées sur cet ensemble. L'exploration conjointe de l'espace des descriptions et de l'espace des cibles permet de rechercher des corrélations pouvant être expliquées par d'autres variables descriptives de manière complètement non-supervisé.
Pour cela, nous introduisons le problème de découverte de sous-groupes corrélés sur les rangs basé sur l'exploration conjointe de motifs graduels identifiant des corrélations de rang et de sous-groupes afin d'identifier des contextes pour lesquels les corrélations sont exceptionnellement fortes par rapport au reste des données. Les motifs recherchés sont composés d'un ensemble D de conditions sur les attributs descriptifs, qu'ils soient numériques ou nominaux, et de C, un modèle de corrélation de rang sur des attributs numériques qui capture des corrélations de rang (positives ou négatives) basées sur une généralisation de τ de Kendall.
Nous présentons un algorithme d'énumération s'appuyant sur des propriétés d'élagage avec des bornes supérieures. Une étude empirique sur plusieurs jeux de données démontre la pertinence et l'efficacité de notre méthode.
Travaux connexes
Il existe plusieurs travaux visant à la découverte de motifs à forte co-variations entre des attributs numériques ou ordinaux. De telles approches sont connues sous plusieurs vocables : itemsets corrélés par les rangs (Calders et al. (2006)), dépendances graduelles (Hüllermeier (2002)), itemsets graduels (Do et al. (2010(Do et al. ( , 2015) ou motifs de co-variation (Prado et al. (2013)). La plupart de ces approches considèrent la fouille de motifs pour lesquels la corré-lation des attributs numériques est supérieure à un seuil défini par l'utilisateur. Par exemple, dans un article fondateur, Calders et al. (2006) proposent un processus d'extraction de motifs sous contrainte de correlation. Ils utilisent la mesure de corrélation τ de Kendall et proposent un processus d'élagage permettant la fouille de ce type de motifs. Cette approche a été géné-ralisée dans Prado et al. (2013) pour découvrir des corrélations positives ou négatives entre un nombre quelconque (≥ 2) d'attributs. On peut noter que Calders et al. (2006) et Prado et al. (2013) introduisent des mesures supplémentaires qui prennent en compte la distribution de paires d'objets qui supportent les motifs : dans Calders et al. (2006), ces mesures caractérisent un seul attribut catégoriel avec des motifs corrélés sur les rangs, alors que Prado et al. (2013) introduit le concept de motifs émergents selon un seul attribut numérique ou un graphe. Ces deux approches ne considèrent néanmoins qu'une seule cible et ne visent pas à trouver des motifs exceptionnels en fonction de cibles multiples. Do et al. (Do et al. (2010(Do et al. ( , 2015) utilisent une mesure de support basée sur la longueur du chemin le plus long entre les objets ordonnés par les attributs numériques. Cette mesure a plusieurs inconvénients, que ce soit au niveau calculatoire ou sémantique. Plus récemment, Downar et Duivesteijn (2017) ont proposé une approche pour trouver des sous-groupes dans le cas où deux cibles numériques interagissent de manière inhabituelle. L'interaction entre les deux cibles est modélisée par plusieurs mesures de corrélation (par exemple, le coefficient de corrélation de Pearson, la corrélation de rang τ de Kendall). La principale limitation de ce travail est que les deux attributs numériques cibles doivent être spécifiés apriori et l'approche ne fonctionne pas avec un ensemble arbitraire d'attributs numériques.
Sous-groupes corrélés sur les rangs
Un sous-groupe corrélé sur les rangs est un ensemble d'attributs fortement corrélés sur un ensemble d'objets défini en intention. Cet ensemble d'objets est identifié par une description, c'est-à-dire une conjonction de conditions sur des attributs descriptifs (par opposition aux attributs cibles). Plus formellement, un tel motif est composé de deux parties : un ensemble d'attributs corrélés positivement ou négativement, et une conjonction de restrictions sur certains attributs descriptifs. Les objets qui satisferont la description constituent le sous-groupe de données sur lequel les corrélations sont évaluées.
Avant d'introduire formellement le langage de motifs qui nous intéresse, établissons une notation. Dans ce qui suit, un ensemble de données est noté D = (O, C, R) où O est un ensemble de n objets, C un ensemble d'attributs numériques qui associe à chaque objet une valeur réelle (∀c ∈ C, c : O → R). Des corrélations sont recherchées parmi les attributs de cet ensemble. R est un ensemble d'attributs qui peuvent être soit numériques soit catégoriels et dont la restriction de leurs domaines de valeurs identifie les sous-groupes.
Evaluer la corrélation d'un ensemble d'attributs
Les mesures de corrélation évaluent la force de l'association entre deux attributs ainsi que la direction de la relation. Trois types de corrélations sont utilisés en statistique : la corrélation de Pearson, le τ de Kendall et les corrélations de rang de Spearman. La corrélation de Pearson est la mesure la plus utilisée, mais elle nécessite des attributs continus, et pas seulement ordinaux. De plus, elle est basée sur des hypothèses fortes (les deux attributs doivent être distribués selon une loi normale, être corrélés avec relation linéaire et homoscédastique) qui ne sont généralement pas satisfaites dans la pratique. Des mesures de corrélation de rang (par exemple, la mesure de corrélation de rang τ de Kendall, ou de rang de Spearman) sont mieux adaptées car elles ne reposent pas sur les hypothèses mentionnées ci-dessus.
Contrairement au coefficient de Spearman, la mesure τ de Kendall est facile à interpréter et peut être facilement utilisée pour la fouille de motifs (Calders et al. (2006)).
Définition 1 (Motif corrélé sur le rang). Un motif corrélé sur les rangs C est un ensemble d'au moins deux attributs signés de C noté C = {(a, s) | a ∈ C et s ∈ {−, +}} avec, par convention, le signe du premier attribut dans l'ordre canonique à +.
et < s est la relation binaire classique sur R : < quand s = +, et > quand s = −. La mesure de corrélation τ de Kendall généralisée à un nombre quelconque d'attributs est alors :
Corrélations de rang contextualisées
L'objectif du processus de fouille proposé est de trouver des sous-groupes d'objets O, définis en intention, pour lesquels la valeur de τ (C, O) sur les attributs C est plus forte que sur l'ensemble de tous les objets τ (C, O). Ces sous-groupes sont définis au moyen de conjonctions de restrictions sur les attributs de R : Définition 2 (Sous-groupe et support). Un sous-groupe d'objets est défini en intention par la description D = f 1 , . . . , f |R| avec chaque f est une restriction sur le domaine de valeurs de l'attributs d ∈ R. En fonction du type de d , la restriction f est définie par :
Nous avons maintenant tous les ingrédients pour définir les sous-groupes corrélés.
Définition 3 (Sous-groupes corrélés). Un sous-groupe corrélé est une paire (C, D) avec C un motif de corrélation sur C et D une description sur R qui définit un sous-groupe d'objets en
Exemple 1 L'intérêt de ce modèle est illustré sur l'exemple du Tableau 1. Ces données dé-crivent un ensemble de baux commerciaux décrits par la date de début et de fin de bail, la localisation GPS du commerce, et son type. Les attributs de la cible décrivent l'environnement géographique du commerce sur la durée du bail, c'est-à-dire le nombre de commerces de chaque type se trouvant dans un rayon de 300m (pharmacie, boulangerie, boucherie). Un attribut supplémentaire indique la durée du bail du magasin (durée de vie). Sur cet exemple, le motif C = Duree_De_V ie + , Boulangerie − , Boucherie + est fortement corrélé sur le sous-  o1  1991  2000  1 3 boulangerie  10  5  7  1  o2  2000  2013  3 3 boulangerie  13  7  5  3  o3  1975  1992  2 1 boulangerie  18  3  2  7  o4  1986  2005  2 3 boulangerie  20  9  1  9  o5  1999  2008  2 3 Pharmacie  10  7  2  2  o6  1995  2014  5 3  boucherie  20  8  3  1  o7  1980  1999  4 4 boulangerie  20  6  3  1 TAB. 1 -Exemple de données et de motif.
Certains sous-groupes corrélés peuvent être considérés comme équivalents car ils partagent le même support. Ces motifs appartenant à une même classe d'équivalence peuvent être retirés par un opérateur de fermeture.
Définition 4 (Les opérateurs de fermeture). Suivant le formalisme de l'analyse formelle de concepts (Wille (1982)), on définit deux fonctions H and M qui permettent d'associer à un sous-groupe corrélé l'ensemble des paires d'objets qui le supportent et réciproquement :
ci-dessus (Définition 1).
Etant donné un ensemble de paires d'objets
Le couple (H(C, D), M (X)) forme un concept formel.
Les sous-groupes corrélés et fermés qui capturent le mieux les corrélations locales dans les données doivent avoir une valeur de corrélation élevée par rapport à ce qui est observé sur l'ensemble des données. Une mesure appropriée de ce phénomène est la précision relative pondérée (WRAcc) (Lavrač et al. (1999)). Cette mesure prend en compte l'accroissement de la précision par rapport à la corrélation par défaut, c'est-à-dire la corrélation sur l'ensemble de tous les objets.
Définition 5 (WRAcc). Le caractère exceptionnel d'un sous-groupe corrélé (C, D) est évalué en utilisant la mesure Wracc, définie comme suit :
Notre tâche de fouille peut maintenant être entièrement exprimée comme le problème suivant :
Problème 1 (Fouille de sous-groupes corrélés et fermés). Sois S la collection de sous-groupes corrélés et fermés définie comme :
Nous voulons également un ensemble concis de motifs inattendus qui maximisent la mesure WRAcc. Cependant, il est bien connu (Xin et al. (2006)) qu'en général ces motifs sont très redondants, certains étant une petite variation des autres. Nous proposons de limiter cette redondance à l'aide de l'appproche suivante :
Problème 2 (Fouille des top-k sous-groupes fermés, corrélés, exceptionnels et diversifiés). Sois K un sous-ensemble de S contenant les top-k sous-groupes fermés et corrélés par rapport à la mesure WRAcc et qui sont aussi diversifés. La diversité entre deux motifs est évaluée par la mesure de Jaccard, définie comme suit :
Etant donné un seuil δ, l'ensemble K des k sous-groupes les plus diversifiés, est défini par :
|K| = k
Le point 2 est très difficile à garantir dans un processus incrémental. Cela est dû à la non-transitivitée de la mesure de similarité. En effet, si un sous-groupe corrélé (C, D) est exclu de K par un motif similaire (C , D ) de meilleure qualité, il n'y a aucune garantie que d'autres sous-groupes corrélés exclus par similitude avec (C, D) soient également similaires à (C , D ). Nous relaxons donc cette condition de la manière suivante :
Algorithme
Nous énumérons récursivement les sous-groupes corrélés par une recherche en profondeur DFS à l'aide de l'algorithme LOCOM (voir Algorithme 1). Étant donné le motif (C, D) actuellement exploré, l'algorithme retourne toutes ses spécialisations qui sont des sous-groupes corrélés exceptionnels. Pour le premier appel, le motif
De plus, pour éviter de générer des motifs plusieurs fois, nous utilisons des ordres arbitraires, C sur C et R sur R. L'ordre canonique entre les motifs est donc défini par avec :
Si X n'est pas vide (lignes 4 à 37), les sous-groupes corrélés actuels sont spécialisés soit en ajoutant un attribut signé en C, soit en réduisant la valeur du domaine d'un attribut dans R. Si cet attribut est catégorique, son domaine est limité à une seule valeur (ligne 17). S'il est numé-rique, deux sous-intervalles peuvent être générés : un réduit d'une seule valeur sur la gauche (ligne 25) et l'autre à droite (ligne 32). Pour éviter de générer deux fois le même intervalle, la réduction sur la droite n'est autorisée que lorsqu'aucune réduction sur la gauche précédente ne s'est produite (Kaytoue et al. (2011)). La fonction de fermeture est utilisée pour faire des sauts dans le processus d'énumération, et obtenir directement le motif le plus spécifique couvrant les mêmes paires d'objets.
La fonction Propager (X, (C c , D c )) est utilisée pour éliminer rapidement les candidats peu prometteurs de X. Trois techniques d'élagage sont utilisées pour arrêter le processus d'énumé-ration (ligne 6) : l'anti-monotonicité de la mesure support σ(D), et deux bornes supérieures, une sur le τ de Kendall, et l'autre sur la mesure WRAcc.
Algorithme 1 : LOCOM((C, D), X, gauche)
Entrées : (C, D) le motif en cours de construction, X l'ensemble des couples attributs-valeurs de C ∪ R, à énumérer. left : Un tabeau de |R n | valeurs booléennes indiquant si les intervalles de l'attribut numérique correspondant ont été réduits sur le côté gauche. Il y a aussi des variables globales :
-β, α, δ : les seuils utilisés pour les contraintes -minWRAcc : la valeur WRAcc minimale des k motifs Sorties : K, Liste des top-k motifs diversifiés actuels.
si (a est le ième attribut symbolique de R) alors
Dans cette section, nous présentons nos principaux résultats expérimentaux. Nous commençons par décrire les jeux de données réels utilisés, ainsi que les questions auxquelles nous voulons répondre. Après l'étude quantitative, nous donnons quelques exemples de motifs trouvés. Pour garantir la reproductibilité, le code source et les données sont librement accessibles 1 . Nous exposons dans cette section qu'un échantillon des études que nous avons réalisées. L'ensemble complet des figures issues de ces expérimentations sont accessibles via le pointeur précédent.
Jeux de données et objectifs
Nous considérons 4 jeux de données réels bien connus. Le premier 2 , SA-heart, décrit des individus d'une région d'Afrique du Sud présentant des anomalies cardiaques. Les trois autres jeux de données, issus de la collection UCI Machine Learning 3 , décrivent différent domaines d'applications (Abalon, Seismic-bumps, German Credit). Cette étude expérimentale vise à ré-pondre à différentes questions : Comment se comporte LOCOM vis-à-vis des différents paramètres, des caractéristiques des jeux de données et d'une baseline ? des propriétés d'élagage de LOCOM ?
Nous montrons dans le tableau 2 les meilleurs motifs (par rapport à la WRAcc) obtenus par notre approche sur les différents jeux de données.
Etude quantitative
Comme baseline, nous considérons l'algorithme PAIRMINING (Prado et al. (2013)), dérivé de Calders et al. (2006) afin de gérer les variations positives et négatives. Pour chaque contexte, l'algorithme PAIRMINING recherche des motifs graduels. Les temps d'exécution de LOCOM et PAIRMINING en fonction des paramètres α et β pour le jeu de données SA-heart sont décrits dans la figure 2. Comme attendu, l'algorithme LOCOM est meilleur que PAIRMINING, excepté quand α est suffisamment élevé et que quasiment aucun motif n'est retourné. Plus intéressant, cette baseline ne finit pas dans de nombreuses configurations.
Nous étudions ensuite le comportement de notre algorithme plus en détail. Plus particuliè-rement, la figure 1 retourne le temps d'exécution, le nombre d'éléments explorés avec ou sans l'exploitation de la borne supérieure U B WRAcc en fonction de k. La distribution des valeurs WRAcc des motifs découverts est également affichée sur cette figure. L'optimisation basée sur U B WRAcc permet d'accélérer la découverte des top-k sous-groupes corrélés grâce à un élagage de l'espace de recherche plus efficace. Ce gain atteint même un facteur de 2 sur certains jeux de données.
Etude qualitative
Ces motifs sont cohérents par rapport aux connaissances du domaine. Par exemple, pour SA-heart, les maladies coronariennes (chd) sont positivement corrélées avec l'âge et l'obésité. L'âge et la consommation d'alcool sont anti-corrélés pour des valeurs de tensions artérielles (Sdb) élevées. Les motifs découverts sur Abalone mettent en évidence des corrélations entre les différentes mesures de poids sur l'ensemble de données (σ(D) ≥ 0, 99).
Motifs SA-heart (α = 0.05, β = 0.5 et δ = 0.2).  Ces mesures sont également corrélées avec l'âge de l'abalone (Rings) quand ils sont plutôt petits. Dans les données Seismic-bumps, les nombres de secousses sismiques (Number-of-bumps) dont l'énergie varie entre [10 6 , 10 7 ) (Number-of-bumps5) et [10 7 , 10 8 ) (Number-ofbumps6) sont corrélés dans des contextes où l'on a de l'extraction de charbon.
Dans les données German credit, l'âge est anti-corrélé avec le nombre de crédits (Numberof-credit) pour les hommes célibataires qui sont propriétaires de leur appartement (Type of apartment = own).
Conclusion et perspectives
Dans cet article, nous avons introduit le problème de la découverte des sous-groupes corrélés sur les rangs avec un nombre arbitraire de cibles numériques (supérieur ou égal à 2). Cela permet de mettre en évidence des sous-groupes d'objets -identifiés par des conditions sur des attributs numériques et/ou nominaux -pour lesquels la corrélation de rang entre un sousgroupe d'attributs (numériques ou ordinaux) signés est exceptionnellement supérieure à celle évaluée sur l'ensemble des données. Les motifs de corrélation de rang que nous considérons sont basés sur une généralisation du τ de Kendall qui permet de représenter un sous-ensemble d'attributs numériques qui co-varient d'une manière positive ou négative. Nous avons défini LOCOM, un algorithme de type Branch-and-Bound qui exploite certaines propriétés d'éla-gage basées sur le calcul de bornes supérieures et sur des propriétés de fermeture. Une étude empirique sur plusieurs ensembles de données démontre l'efficacité de LOCOM. Ce travail ouvre de nouvelles perspectives de recherche. Par exemple, d'autres mesures et paradigmes peuvent être étudiées pour évaluer l'intérêt des sous-groupes, en particulier l'intérêt subjectif qui permet de prendre en compte les connaissances a priori de l'utilisateur Bie (2011). Une autre direction intéressante consiste à concevoir des méthodes d'exploration instantanée en abandonnant la complétude de l'algorithme et en échantillonnant directement l'espace des motifs Boley et al. (2011).

Introduction
L'étude des ensembles fermés, soient-ils fréquents ou non, est un des sujets centraux de la fouille de données. L'analyse formelle de concepts (FCA) (Ganter et Wille (1999)) est un des formalismes qui permettent d'étudier ces ensembles fermés, grâce à la structure qu'ils ont lorsqu'ils sont ordonnés par inclusion : le treillis des concepts. De nombreuses études utilisent ce formalisme, que ce soit dans des applications (Poelmans et al. (2013)) ou pour des résultats plus théoriques en complexité d'énumération (Gély et al. (2009)) ou de comptage (Kuznetsov et Obiedkov (2008)).
Il est bien connu que le nombre maximum d'ensembles fermés dans une table de données n × m, avec n plus petit que m est 2 n . Dans ce papier, nous cherchons à généraliser au cas 3-dimensionnel la construction qui atteint 2 n , puis nous cherchons une borne supérieure au nombre maximum d'ensembles fermés en trois dimensions -nombre que nous appellerons par la suite f 3 (n). Pour ce faire, nous commençons par rappeler les définitions basiques en deux dimensions, puis en trois dimensions. La Section 3 donne une construction qui permet d'atteindre 3.36 n ensembles fermés. Dans la Section 4, nous donnons une esquisse de preuve pour une borne supérieure de 3.38 n . Les résultats présentés dans cet article sont disponibles en version longue sur ArXiv (Bazin et al. (2018)).
Définitions
Nous présentons notre travail dans le formalisme de l'analyse formelle de concepts (Ganter et Wille (1999) Un exemple de ces deux définitions est donné dans la Figure 1. L'ensemble d'objets O contient sept objets, l'ensemble d'attributs A contient cinq attributs. Dans ce contexte, la paire ({o 2 , o 7 }, {a 2 , a 4 , a 5 }) est un concept. Afin d'alléger les notations, et quand cela n'induit aucune confusion, nous écrirons les ensembles sans leurs accolades. Ainsi, notre concept devient (o 2 o 7 , a 2 a 4 a 5 ).
Le passage en trois dimensions se fait naturellement (il a été fait pour la première fois dans (Lehmann et Wille (1995)). Un 3-contexte est un quadruplet (O, A, C, R) où O, A et C sont des ensembles, appelés respectivement ensemble d'objet, d'attributs et de conditions, et R est une relations ternaire entre ces ensembles.
Dans ce modèle, un 3-concept est une boite maximale de croix dans le 3-contexte. Plus précisément, c'est un triplet (O, A, C) pour lequel on a que O × A × C ∈ R et on ne peut augmenter aucun de ces ensembles sans perdre la propriété. L'ensemble des 3-concepts suit une orientation différente de celle en deux dimensions, mais les concepts peuvent tout de même être ordonnés en un 3-treillis.
Les figures 2 et 3 donnent deux manière de visualiser un 3-contexte.
3 2 n , 3 n et plus si affinité
En deux dimensions, dans un contexte de taille n × n, il est possible d'atteindre 2 n concepts. Cela signifie que toutes les parties de l'ensemble [n] sont fermées. Le contexte permettant d'atteindre 2 n concepts est appelé contranominal scale en anglais, nous l'appellerons contexte anti-diagonal ici, pour la raison suivante. Basé sur un ensemble S de taille n, il correspond au contexte (S, S, =). Soit X un sous ensemble de S. Les concepts du contexte anti-diagonal sont de la forme (X, X), avec X = S \ X. Toutes les parties de S sont fermées, le treillis de concepts correspondant est le treillis booléen de dimension n. 1 Un équivalent du contexte anti-diagonal en 3-dimension est le contexte (S, S, S, S 3 \ {(a, a, a) | a ∈ S}), montré en 
FIG. 4 -Le contexte anti-diagonal sur un ensemble de taille 3. Ce 3-contexte a 3 3 = 27 3-concepts.
Ce 3-contexte, basé sur un ensemble de taille n, donne 3 n 3-concepts. Introduit par Lehmann et Wille (1995), il a été également étudié par Biedermann (1998Biedermann ( , 1999. La comparaison avec le cas 2-dimensionnel s'arrête ici, car on peut construire des contextes donnant plus de 3 n concepts.
Observation 1 Il existe un 3-contexte 5 × 5 × 5 avec quatre cent vingt-huit concepts. Ce contexte est donné en Figure 5.
FIG. 5 -Ce contexte a 428 concepts.
On remarque que 428 est strictement supérieur à 3 5 = 243. Notons également que 428 > 3.36 5 . Cela nous permet de conclure à l'existence de petits contextes ayant beaucoup de concepts, mais pas sur le cas général. Afin d'étendre cette observation, nous présentons maintenant une construction permettant de coller deux contextes et de multiplier leur nombre de concepts.
Soient
2. gardant les croix existantes, et en en ajoutant de nouvelles dans les cellules empruntant des coordonnées aux deux contextes.
Deux exemples sont montrés dans la Figure 6 : en deux dimensions puis en trois dimensions.
Proposition 2 Soient K 1 et K 2 deux 3-contextes avec respectivement N 1 et N 2 concepts. Alors le contexte K résultant de la fusion de K 1 et K 2 par la procédure décrite ci-dessus a N 1 × N 2 concepts.
Cela nous permet de répéter notre contexte 5 × 5 × 5 pour créer de grands contextes avec beaucoup de concepts. Nous avons donc le théorème suivant :
Théorème 3 Il existe une constante c telle que, pour tout entier n, f 3 (n) ≥ c3.36 n .
Approche pour une borne supérieure
Notre approche pour obtenir une borne supérieure ne s'appuie pas directement sur les concepts d'un contexte, mais sur l'équivalence qu'ils ont avec les traverses minimales d'une certaine classe d'hypergraphes. Dans le cadre des 3-concepts, ils sont équivalents aux traverses minimale des hypergraphes 3-uniformes (chacune des arêtes a arité 3), 3-partis. Chaque arête correspond alors à un "trou" du contexte.
FIG. 6 -La procédure revient à coller les deux contextes sur une diagonale, tout en remplissant le reste de croix. Les parties grisées en deux dimensions représentent ces croix. Elles sont absentes en trois dimensions, pour des raisons évidentes de lisibilité.
En utilisant ce formalisme, nous avons utilisé une approche de measure and conquer (Kullmann (1999);Fomin et al. (2009)) pour borner le nombre de traverses minimales dans un hypergraphe de cette classe. Cette approche nous permet d'obtenir le théorème suivant :
Théorème 4 Pour tout entier n, f 3 (n) ≤ 3.38 n .
Preuve (version courte) Une version longue de cette preuve peut-être consultée dans (Bazin et al. (2018)).
Notre preuve utilise le theorème de Kullmann (1999) qui borne le nombre de feuilles d'un arbre muni de probabilités de transition sur ses arêtes. Nous commençons par montrer, pour un hypergraphe H, qu'il est possible de construire un tel arbre de façon à ce que H soit la racine et les feuilles les traverses de H. L'essentiel de la preuve se résume alors à identifier les probabilités de transition correspondantes aux différentes configurations possibles dans l'hypergraphe. Cela nécessite une étude de cas fastidieuse. Une fois les probabilités trouvées, le théorème nous assure que le nombre maximum de traverses minimales dans un hypergraphe tri-parti 3-uniforme à 3n sommets est inférieur à 1.5012 3n et donc que le contexte correspondant à une tripartition des sommets en trois dimensions égales possède, au plus, 3.38 n concepts.
Questions sans réponses
Bien que nous donnions un encadrement assez petit de la valeur de f 3 (n), nous n'avons pas de certitude quant à sa véritable valeur. Une recherche plus approfondie pourrait éventuellement permettre de trouver des 3-contextes ayant plus de concepts, qui pourraient alors être répliqués en utilisant la construction multiplicative.
De même, la borne supérieure que nous donnons peut possiblement être améliorée, soit en utilisant une évaluation plus fine en terme de measure and conquer ou par une tout autre approche.
La question reste ouverte par rapport au nombre maximum de concepts dans un contexte de dimension d (Voutsadakis (2002)).

Introduction
La théorie des réseaux complexes a mis en avant l'existence de propriétés communes aux réseaux modélisant des systèmes réels. En particulier, la plupart de ces réseaux possèdent une structure communautaire, i.e. une partition de l'ensemble des noeuds telle que les noeuds de chaque partie sont plus connectés entre eux qu'avec l'extérieur (Newman, 2006). Un cas typique est celui des réseaux sociaux, où les utilisateurs se regroupent autour de thèmes. La structure de communautés est particulièrement importante pour l'étude du réseau puisqu'elle permet de se placer à un niveau intermédiaire (mésoscopique) entre le niveau local (voisinage uniquement) et le niveau global (la totalité du réseau).
Une façon courante d'extraire les communautés d'un réseau consiste à trouver une partition de ses noeuds maximisant la modularité (Newman, 2006), c'est à dire qui maximise la densité des liens au sein des communautés en minimisant le nombre de liens entre communautés. Il est dans ce cas nécessaire de connaître la totalité du réseau afin d'en déterminer ses communautés. Dans le cas des très grands réseaux tels que l'internet ou les réseaux sociaux en ligne, cette condition est parfois impossible à remplir, soit parce qu'on ne connaît pas le réseau complet, soit parce qu'il est difficile de le stocker en mémoire. De plus, définir l'ensemble des communautés d'un réseau comme une partition stricte de ses noeuds est souvent éloigné de la réalité, puisque cela empêche un noeud d'appartenir à plusieurs communautés (Palla et al., 2005).
Extraction de communautés ego-centrées par apprentissage supervisé d'espaces prétopologiques
Ainsi, nous nous concentrons sur la notion de communauté locale à un (ou plusieurs) noeud(s) d'intérêt. On cherche dans ce cas à détecter la ou les communautés de ce noeud, on parle de communautés ego-centrées (Chen et al., 2009;Danisch et al., 2013). Cette approche permet de faire une optimisation locale, bien moins gourmande que l'approche globale, et réalisable même dans le cas où la totalité du graphe n'est pas connue. Par ailleurs, cette approche est adaptée à la mise en évidence de communautés chevauchantes, aboutissant ainsi à des résultats plus en accord avec la réalité.
Cet article a pour objectif de présenter une méthode prétopologique d'extraction de communautés locales. La théorie de la prétopologie est une généralisation de la théorie des graphes (Dalud-Vincent, 2017) et est de ce fait particulièrement adaptée à l'étude et à la modélisation de réseaux. Elle permet notamment de représenter des relations de natures différentes entre ensembles d'éléments, là où un graphe ne décrit des relations qu'entre paires d'éléments. Il est par exemple possible d'imaginer une prétopologie sociale définie à partir de différents types de relations entre ses utilisateurs (amis, collègues, familles, . . . ).
Après une présentation de travaux liés à l'extraction de communautés (Section 2), nous introduisons (Section 3) les concepts clés de la prétopologie ainsi que la classe (générique) des espaces prétopologiques définis par une fonction d'adhérence logique. Cette dernière formalisation offre la possibilité d'apprendre un espace prétopologique adapté aux caractéristiques du réseau. La Section 4 établit le cadre de nos expérimentations et décrit les prédicats qui composent nos règles logiques. La Section 5 expose les résultats obtenus sur quatre jeux de données réels et synthétiques. Une comparaison entre les méthodes classiques et les méthodes prétopologiques est faite.
Travaux connexes
De nombreux travaux se sont déjà attelés à la tâche de détection de communautés egocentrées. On peut en distinguer au moins trois types : les méthodes guidées par une mesure locale inspirée de la modularité (Newman, 2006), celles basées sur des algorithmes de propagations ou encore des méthodes reposant sur des algorithmes d'apprentissage de plongements de graphes (Grover et Leskovec, 2016).
Les méthodes guidées par une mesure de modularité proposent de construire une communauté ego-centrée en ajoutant successivement des noeuds à un ensemble initial de noeuds d'intérêts. La fonction objectif de ces méthodes est une variante de la modularité adaptée au cas des communautés locales. À chaque itération de l'algorithme, le noeud apportant le plus grand gain au score de modularité locale est inséré dans la communauté. L'algorithme s'arrête quand il n'est plus possible d'améliorer le critère ou lorsque la communauté détectée est suffisamment grande (Clauset, 2005). Certaines méthodes ajoutent une étape d'élagage afin de corriger de potentielles erreurs (Chen et al., 2009;Luo et al., 2008).
Par ailleurs, Danisch et al. (2013) définissent une méthode inspirée de la propagation de l'opinion ou de la chaleur dans un graphe. Si l'on considère le noeud d'intérêt comme une source de chaleur, cette chaleur se transmet en suivant les liens du réseau, aboutissant à un score de température pour chaque noeud indiquant la proximité entre le noeud d'intérêt et le reste du réseau. Une communauté ego-centrée peut alors être extraite en ne conservant que les noeuds dont le score dépasse un seuil fixé. Cette méthode offre la possibilité d'extraire des communautés à différents niveaux de granularité.
G. Caillaut et al
Enfin, certains travaux démontrent la pertinence de considérer des méthodes d'apprentissage de plongements lexicaux dans le contexte de l'étude des graphes (Figueiredo et al., 2017;Grover et Leskovec, 2016). Pour faire une analogie, si l'on considère qu'un sommet repré-sente un mot, on peut générer des phrases décrivant les chemins empruntés par des marches aléatoires. Ces phrases permettent alors l'apprentissage de plongements pour chaque noeud du réseau, plongements ensuite utilisés par des méthodes d'apprentissage pour en extraire des communautés.
On observe ainsi que de nombreuses approches ont été proposées pour résoudre le problème de détection de communautés. Il est probable qu'il n'existe pas de méthode générique pour réaliser cette tâche de façon optimale. Chaque méthode permet toutefois d'extraire une portion d'information qu'il serait regrettable de ne pas exploiter. C'est pourquoi nous proposons une méthode alternative, basée sur la théorie de la prétopologie, permettant de tirer profit des qualités des différentes approches en les combinant. La prétopologie permet de décrire un processus d'expansion à partir d'une combinaison de plusieurs "sources d'informations". C'est par ce processus d'expansion que nous suggérons d'extraire les communautés d'un réseau.
Éléments de Prétopologie
Un espace prétopologique est défini par un couple (E, a) avec E un ensemble fini non-vide d'éléments et a : P(E) → P(E) sa fonction d'adhérence vérifiant les propriétés 1 et 2.
L'opérateur d'adhérence modélise ainsi un processus d'expansion d'une partie A de E. Il est usuellement défini par un ensemble V de voisinages sur E (Belmandt, 1993) où V ∈ V est une application réflexive de E vers P(E).
Contrairement aux opérateurs de la topologie, l'opérateur d'adhérence prétopologique n'est pas nécessairement idempotent, on peut alors l'appliquer de façon successive sur un ensemble A ∈ P(E) jusqu'à obtenir un ensemble K tel que A ⊆ K ⊆ E et a(K) = K. On appelle K le fermé de A et on le note F (A). Si |A| = 1 alors on appelle F (A) un fermé élémentaire. Caillaut et Cleuziou (2018) introduisent une nouvelle classe d'espaces prétopologiques dont la fonction d'adhérence est définie par une formule logique Q en forme normale disjonctive (DNF).
Cette définition de l'opérateur d'adhérence possède d'une part l'avantage d'être plus gé-nérale que la définition précédente, mais surtout permet d'envisager l'apprentissage de règles de combinaison de voisinages, et donc d'espaces prétopologiques. Cleuziou et Dias (2015) proposent la méthode LPS (Learning Pretopological Spaces) qui consiste à apprendre une fonction numérique pour définir l'opérateur d'adhérence. Cette fonction impose quelques restrictions, notamment le fait d'être nécessairement linéaire. C'est pourquoi Caillaut et Cleuziou (2018) proposent la méthode LPSMI (Learning Pretopological Spaces Multi-Instance) consistant en l'apprentissage d'une règle logique, plus souple qu'un modèle linéaire.
Extraction de communautés ego-centrées par apprentissage supervisé d'espaces prétopologiques
Ces méthodes proposent d'apprendre un espace prétopologique en se basant sur ses fermés élémentaires. Étant donné un ensemble S * de fermés élémentaires cibles et une liste de pré-dicats, LPSMI apprend une DNF Q composée des prédicats donnés en entrée et telle que les fermés élémentaires S * puissent être obtenus par la fonction d'adhérence logique a Q (.).
Nous proposons d'appliquer LPSMI pour apprendre un espace prétopologique dont les fermés correspondraient à des communautés locales. Cependant, LPSMI utilise une fonction d'optimisation spécialisée pour l'apprentissage d'espaces prétopologiques de type V. Ces espaces sont définis par une fonction d'adhérence possédant la propriété d'isotonie.
Un espace de type V impose une contrainte forte sur la façon dont les communautés peuvent se chevaucher 1 . Soient trois points x, y, z ∈ E, si y ∈ F ({x}) et y ∈ F ({z}), alors F ({y}) ⊆ F ({x}) ∩ F ({z}) 2 . Par conséquent, la communauté centrée sur y sera, selon les cas, comprimée entre celle de x et celle z, ou alors elle imposera la présence de noeuds indésirables dans les communautés centrées sur x et sur z. Ce formalisme interdit donc de nombreuses formes de structures qui se retrouvent pourtant dans des cas réels.
C'est pourquoi nous prétendons qu'une prétopologie de type V n'est probablement pas adaptée à la modélisation des communautés à partir de fermés élémentaires. Nous proposons alors de recourir à une variante de LPSMI plus simple et qui n'impose pas ces contraintes de formes sur les communautés.
Méthode d'extraction de communautés
L'algorithme d'apprentissage LPSMI reprend le principe de l'apprentissage de concepts et consiste à construire une formule logique Q en forme normale disjonctive de manière gloutonne, c'est-à-dire par ajouts successifs de littéraux. La construction de la règle logique est guidée par un critère objectif exploitant la propriété d'isotonie des espaces de type V. Dans le but de s'affranchir de cette contrainte nous proposons la méthode LPSFM dont la seule différence avec LPSMI est son critère objectif. La construction d'une règle logique par LPSFM est guidée par le score de F-mesure entre les communautés réelles (cibles) et les communautés prétopologiques obtenues. Nous noterons critère MI le critère utilisé par LPSMI et critère FM celui utilisé par LPSFM. Nous détaillons dans cette section les différences entre ces deux critères ainsi que les prédicats utilisés pour l'apprentissage des règles logiques.
Fonctions objectives pour l'apprentissage
Étant donné un ensemble de fermés élémentaires cibles S * et un espace prétopologique (E, a), les deux critères MI et FM proposent d'évaluer la qualité de l'espace prétopologique en mesurant la correspondance entre ses fermés élémentaires et S * . Les critères MI et FM partagent donc le même objectif, ils sont toutefois fondamentalement différents.
Le critère MI s'appuie sur les propriétés structurelles des espaces prétopologiques de type V (Éq. 5) pour évaluer finement non seulement la qualité d'un prédicat au regard des fermés 1. Si on considère qu'un fermé élémentaire exprime une communauté ego-centrée.
G. Caillaut et al
élémentaires qu'il génère mais aussi son potentiel à travers ses fermés non-élémentaires. Il en résulte que l'espace appris sera nécessairement de type V 3 . Il est donc primordial que les prédicats composant la DNF Q respectent les propriétés des espaces de type V, c'est à dire lorsqu'un ensemble A ∈ P(E) se propage par l'opérateur d'adhérence à un élément x ∈ E, tout sur-ensemble de A doit aussi se propager à x. Soit q un prédicat défini sur P(E) × E, q est de type V s'il respecte :
Le critère FM est quant à lui beaucoup plus simple puisqu'il ne tient pas compte du potentiel d'un prédicat, il ne s'appuie que sur les fermés élémentaires générés. Caillaut et Cleuziou (2018) montrent qu'en pratique ce critère est moins efficace que le critère MI lorsque la tâche consiste spécifiquement à apprendre des espaces de type V. Il reste cependant un recours pré-cieux pour guider l'apprentissage d'espaces prétopologiques non contraints (non V). Toute DNF qui induit une fonction d'adhérence respectant les deux propriétés décrites par les équa-tions 1 et 2 est alors autorisée. Les seules propriétés à satisfaire pour les prédicats considérés sont alors les propriétés 7 et 8 suivantes :
Construction des prédicats/descripteurs d'un réseau
Nous proposons un ensemble de prédicats spécifiquement dédiés à la tâche d'extraction de communautés ego-centrées. Chaque prédicat peut être vu comme un descripteur, il permet de capturer une caractéristique du réseau. Dans la suite, nous notons E l'ensemble des éléments du réseau (qui peut ne pas être connu en totalité), A un sous-ensemble de E et x un élément de E. L'ensemble des prédicats que nous proposons se décompose en trois catégories décrites ci-après. La diversité de cet ensemble de prédicat est un bel exemple illustrant la capacité d'analyse multi-critères offerte par le formalisme prétopologique.
Les prédicats topologiques. Soient V (x) les voisins du noeud x dans le réseau et V (A) = x∈A V (x) l'union des voisinages de chaque élément de A. Nous considérons que ces voisinages sont réflexifs, tels que x ∈ V (x) (et par conséquent A ⊆ V (A)).
Un premier prédicat de base est défini à partir de la matrice d'adjacence du réseau. On le note q adj (A, x) et il est vrai lorsqu'un élément de A est connecté à x.
Nous proposons quatre prédicats supplémentaires définis par les voisinages de A et de x permettant de capturer différentes variantes d'intensités d'interactions entre A et x. 
FIG. 1: Exemple
Parmis ces quatre prédicats, on peut montrer que seul q r2 est un prédicat de type V. On note ces prédicats q X (A, x) avec X ∈ {clauset, luo, chen}. Le prédicat q X (A, x) est vrai lorsque l'ajout de x à la communauté A améliore sa modularité, notée mod X (A).
Ces prédicats ne respectent pas les propriétés des espaces de type V et ne peuvent donc pas être exploités par l'approche LPSMI.
Prédicats définis par une mesure de proximité. Le prédicat q danisch (A, x, k) est défini à partir de la mesure de proximité carryover-opinion (Danisch et al., 2013). Ce prédicat est vrai lorsqu'il existe un élément de A dont la proximité avec x est supérieure à un seuil k dans [0, 1].
D'autres prédicats pourraient être envisagés, à partir des approches récentes de représen-tations vectorielles des noeuds d'un graphe (e.g. node2vec). Cependant celles-ci nécessitent de connaître l'intégralité du réseau et ne tiennent pas compte du caractère local de la tâche considérée dans cette étude. En outre, les expérimentations que nous avons menées sont venues confirmer l'absence d'efficacité de ce type de prédicat pour l'extraction de communautés ego-centrées.
Extraction de communautés à partir d'un espace prétopologique
Afin d'illustrer le principe d'extraction d'une communauté ego-centrée par un fermé élé-mentaire dans un espace prétopologique, nous considérons le réseau de la figure 1a. Soit l'espace prétopologique (E, a Q ) avec E l'ensemble des noeuds du réseau et Q la DNF définie par Q = q danisch (A, x, 0.5) ∧ q r1 (A, x, 0.5) ; soit la matrice de proximités carryover-opinion donnée par la table 1b ; la communauté ego-centrée issue du noeud a est obtenue par le fermé élémentaire F Q ({a}) dans l'espace prétopologique (E, a Q ) :
Le fermé obtenu correspond effectivement à une communauté identifiable intuitivement sur le réseau. L'obtention du fermé résulte de deux applications successives de l'adhérence a Q . Par définition de Q, l'expansion d'un sous-ensemble A à un nouvel élément x nécessite que les deux prédicats q danisch (A, x, 0.5) et q r1 (A, x, 0.5) soient satisfaits. Ainsi, le singleton {a}, s'étend aux éléments b et c par une première application de l'adhérence car : On retrouve encore une fois la communauté identifiable {a, b, c, d} à laquelle s'est ajouté l'élément e ce qui est tout à fait cohérent du point de vue local au noeud d.
Cet exemple montre qu'un espace prétopologique correctement défini permet d'extraire la structure complexe latente d'un réseau. Cette notion d'espace prétopologique "correctement défini" nécessite que la DNF définissant l'espace prétopologique soit pertinente. C'est ce problème les méthodes d'apprentissage LPSMI et LPSFM tentent de résoudre.
Expérimentations
Il n'existe pas à notre connaissance de travaux visant à résoudre de manière supervisée le problème d'extraction de communautés égo-centrées. C'est pourquoi nous positionnons ces nouvelles contributions supervisées (LPSMI et LPSFM) par rapport aux approches nonsupervisées existantes, en toute objectivité.
Jeux de données
Le premier réseau synthétique est composé de 60 noeuds répartis dans trois communautés de tailles égales. Il est construit sur un modèle aléatoire très simple : tout d'abord, chaque Le premier réseau réel que nous avons utilisé est le célèbre Zachary's karate club (Zachary, 1977). Il modélise les interactions entre les 34 membres d'un club de karaté et est composé de deux communautés connues.
Le second réseau issu de données réelles (figure 2b) représente les interactions entre les équipes universitaires de la division 1-A 4 qui se sont rencontrées lors des matchs de football américains sur la saison 2006. Il est identique au réseau utilisé par Chen et al. (2009). Le réseau est composé de 179 noeuds (équipes) et 787 arcs (matchs) ; 115 équipes sont réparties dans 11 communautés pré-identifiées et 64 équipes ne sont dans aucune communauté.
Protocole expérimental et résultats
Nous proposons de mesurer la qualité d'une méthode d'extraction de communautés egocentrées en calculant, via la F-mesure, la correspondance entre les communautés extraites et les communautés réelles ou connues. Soit E l'ensemble des noeuds d'un réseau, pour chaque noeud x ∈ E, une communauté ego-centrée C(x) est extraite puis comparée à la communauté attendue C * (x). Pour les méthodes prétopologiques proposées, c'est le fermé élémentaire qui défini la communauté ego-centrée extraite (C(x) = F ({x})). Les scores de précision (P), G. Caillaut et al rappel (R) et leur moyenne harmonique (FM ou F-mesure) sont rappelées en (12).
Nous comparons les scores obtenus par les méthodes prétopologiques avec les scores obtenus en utilisant les méthodes de Clauset, Luo, Chen (Clauset, 2005;Luo et al., 2008;Chen et al., 2009) et Danisch (Danisch et al., 2013). Les méthode de Clauset, Luo et Chen construisent les communautés ego-centrées par accumulation des noeuds maximisant une mesure de modularité. La méthode de Danisch repose sur l'idée qu'il existe une forte différence entre la proximité d'un noeud à sa communauté locale par rapport à cette proximité aux autres sommets du réseau. Danisch montre que la courbe de la proximité carryover-opinion pour un noeud donné est une succession de plateaux suivis de brusques décroissances. Des communautés locales à différents niveaux de granularité peuvent alors être obtenues suivant la pente que l'on considère comme marqueur de fin de la communauté. Nous avons calculé les performances de cette méthode en considérant qu'une communauté regroupe les 2, 3 ou 4 premiers plateaux, respectivement notés "Danisch2", "Danisch3" et "Danisch4" dans la suite.
La méthode LPSMI ne peut combiner que les prédicats de type V tandis que LPSFM dispose de l'ensemble des prédicats décrits dans la section précédente pour apprendre les espaces prétopologiques. Nous avons fixé arbitrairement k = 0.3 pour les prédicats topologiques et avons construit deux prédicats q danisch avec les seuils k = 0.15 et k = 0.3, nous les noterons respectivement q danisch (k = 0.15) et q danisch (k = 0.3).
Les résultats provenant des méthodes LPSMI et LPSFM ont été obtenus par validation croisée à cinq plis (5-fold cross-validation). Afin de comparer équitablement toutes les méthodes entre elles, les scores présentés dans la table 1 sont ceux obtenus en évaluant les méthodes sur les cinq mêmes jeux de test représentant chacun 20% des communautés à retrouver. Il est important de noter que les communautés de référence utilisées dans ces expérimentations sont déduites des partitions des noeuds des réseaux (hormis le réseau LFR qui contient des chevauchements). De ce fait, les communautés cibles correspondent à des approximations de communautés ego-centrées, les scores peuvent donc ne pas refléter la qualité réelle de chaque modèle ; ils restent cependant un bon indicateur. Enfin, les meilleures règles logiques apprises par LPSFM sont présentées dans la table 2.
Les résultats obtenus viennent d'abord confirmer l'analyse selon laquelle les espaces préto-pologiques de type V (LPSMI) sont inadaptées à la modélisation des structures de communautés locales. S'affranchir des contraintes des espaces de type V permet en revanche de construire de espaces prétopologiques tout à fait pertinents comme le montrent les scores obtenus par LPSFM. La nouvelle approche proposée obtient globalement, et de manière significative, de meilleurs scores que les méthodes existantes. Cela montre l'intérêt de la supervision pour la tâche d'extraction de communautés. En effet, cela permet de prendre en considération les caractéristiques d'un réseau donné, et donc de produire un modèle adapté à celui-ci. À l'inverse, les performances des approches non-supervisées sont fortement liées au réseau sur lequel elles s'appliquent. 
TAB. 2: Exemples de règles apprises par l'algorithme LPSFM. Les clauses sont affichées dans l'ordre dans lequel elles sont ajoutées dans la DNF.
phénomène s'inverse lorsque l'on considère le réseau Karaté. Danisch semble proposer une approche qui ne souffre pas de ce problème mais elle reste sensible aux seuils.
Les modèles prétopologiques appris par LPSFM ont été appliqués sur des jeux de données différents du jeu d'apprentissage de sorte à évaluer leur capacité de généralisation. Les résultats sont présentés dans la table 3. Les mesures ont été effectuées sur l'ensemble du réseau et non plus sur des jeux de test ; c'est pourquoi les valeurs de la diagonale diffèrent des scores de LPSFM présentés dans la table 1.
De toute évidence, les modèles prétopologiques parviennent mal à se généraliser. Les règles apprises (table 2) ne se ressemblent pas, ce qui semble suggérer que les réseaux eux mêmes et leur structure communautaire en particulier ne se ressemblent pas. Il n'est donc pas étonnant qu'un modèle appris spécifiquement pour un réseau ne convienne pour un autre. Nous verrons néanmoins en perspective que nous disposons d'une piste de travail pour lever ce verrou.
Enfin notre méthode d'apprentissage d'espaces prétopologiques ouvre la voie vers des approches exploitant l'intégralité des aspects d'un réseau en permettant de combiner tous types de descripteurs entre eux. La règle apprise sur le réseau Erd˝ os-Rényi propose par exemple Erd˝ os-Rényi Karaté Foot LFR Erd˝ os-Rényi 0, 85 ± 0, 01 0, 62 ± 0, 07 0, 29 ± 0, 21 0, 07 ± 0, 00 Karaté 0, 75 ± 0, 03 0, 80 ± 0, 04 0, 47 ± 0, 14 0, 34 ± 0, 11 Foot 0, 41 ± 0, 00 0, 59 ± 0, 00 0, 97 ± 0, 00 0, 41 ± 0, 00 LFR 0, 54 ± 0, 01 0, 74 ± 0, 00 0, 60 ± 0, 00 0, 65 ± 0, 00 TAB. 3: Scores de généralisation pour les modèles prétopologiques (LPSFM).
de combiner des descripteurs topologiques de bas niveau (q r2 ) avec des descripteurs de plus haut niveau (q luo ). Cette formulation logique apporte une compréhension précise du modèle appris : il est clair que la règle apprise sur le graphe Erd˝ os-Rényi est guidée par les deux prédi-cats q luo et q danisch (k = 0.3). Cette règle montre que, sur ce réseau précis, ces deux prédicats sont (1) complémentaires puisqu'ils n'apparaissent pas ensemble dans les clauses 2 et 3 et (2) trop permissifs puisqu'ils doivent être restreints par d'autres prédicats. D'autre part, l'importance des clauses conjonctives est déterminé par l'ordre dans lequel elles apparaissent dans la formule logique apprise : la clause q luo ∧ q danisch (k = 0.3) est ainsi celle apportant le plus d'informations utiles à la détection des communautés du réseau Erd˝ os-Rényi.
Conclusion
Dans cette étude nous avons proposé une formalisation du problème d'extraction de communautés ego-centrées fondée sur les techniques récentes d'apprentissage supervisé d'espaces prétopologiques. Nous avons défini une première collection de descripteurs locaux, s'appuyant sur les principales méthodologies existantes pour cette tâche et montré expérimentalement sur des données réelles ou simulées, d'une part qu'il existe des modèles de structuration prétopo-logiques adaptés aux réseaux étudiés et d'autre part que ces modèles peuvent être appris de façon supervisée.
Nos travaux démontrent la pertinence de la prétopologie dans l'extraction de communautés dans les réseaux puisqu'elle permet de gagner en performance par rapport aux méthodes classiques de détection de communautés ego-centrées. Les outils mis à disposition par la prétopolo-gie permettent d'exprimer naturellement et élégamment des interactions de différentes natures entre ensembles d'éléments. Cette capacité se révèle indispensable lorsqu'il s'agit d'exploiter différents niveaux d'informations présentes dans un réseau. Cette étude ouvre la voie à de nombreuses perspectives de recherche dont nous dégageons quelques pistes de travail.
Notre étude a permis de mettre en évidence qu'une prétopologie de type V n'est pas adaptée à la tâche d'extraction de communautés ego-centrées au moyen des fermés élémentaires. Cependant, il est toutefois possible qu'une définition différente permette l'extraction de communautés de bonne qualité depuis un espace prétopologique de type V.
D'autre part, si les modèles prétopologiques permettent l'extraction de bonnes communautés, ces modèles semblent difficiles à généraliser aux réseaux sur lesquels ils n'ont pas été entraînés, or cet entraînement représente un coût évident et potentiellement rédhibitoire 5 . Apprendre un modèle prétopologique de façon non-supervisée permettrait de lever ce verrou. Dans cet objectif, nous avons observé les bonnes capacités de généralisation du modèle appris sur le réseau LFR. Ce résultat ainsi que de récents travaux (Lu et al., 2018) semblent montrer qu'il est possible d'utiliser des réseaux générés artificiellement pour obtenir des données étiquetées. Une nouvelle approche consisterait alors à générer automatiquement des données d'entraînement à partir d'un réseau synthétique structurellement proche du réseau réel ciblé.
Références Belmandt, Z. (1993). Manuel de prétopologie et ses applications.
5. Bien qu'en pratique, on peut observer que peu d'exemples suffisent à guider efficacement l'apprentissage.

Introduction
L'un des défis principaux de l'exploration de données est la détection de changement dans les ensembles de données dynamiques. Ce phénomène est connu sous le nom de "dérive de concept" (Gama, 2010;Silva et al., 2013). Une des applications directes, qui constitue notre intérêt pratique dans le présent document, est la détection de changement dans l'intérêt des utilisateurs sur la base des données enregistrées au cours de leur navigation en ligne. Cette tâche, appelée "profilage de l'utilisateur", revêt une grande importance économique pour les entreprises du secteur de la publicité en ligne. Les tâches de profilage visent à reconnaître les "états d'esprit" des utilisateurs à partir de leurs navigations sur différent sites Web ou leurs interactions avec des "points de contact" numériques (différentes façons dont une marque interagit et affiche des informations aux utilisateurs). Il est très important de pouvoir détecter les changements d'intérêt d'un utilisateur ou son déménagement dans une autre ville ou un autre pays afin d'ajuster la stratégie publicitaire le concernant. Ces profils sont calculés à partir d'une très grande base de données de navigation sur Internet, qui liste les séquences d'URL ou les points de contact visités par un grand nombre de personnes. Chaque URL d'un "point de contact" est caractérisée par des informations contextuelles et sémantiques. Dans ce contexte, chaque utilisateur est décrit comme une série temporelle de catégories d'URL et d'emplacements physiques. Les catégories d'URL sont calculées à l'aide d'une approche de classification adaptée aux données complexes (Rastin et al., 2016;Rastin et Matei, 2018). Les emplacements sont enregistrés à l'aide des informations de géolocalisation collectées lors de la navigation de l'utilisateur, mais sont limités à une série de codes postaux. La détection de changements dans les séries temporelles implique l'extraction de périodes "stables", séparées par une période de variation généralement courte. Il y a donc deux stratégies principales : soit l'algorithme cherche à détecter les différentes périodes de stabilité dans la série chronologique, soit il détecte la période de variation (Last, 2002;Aggarwal et al., 2003;Han, 2005;Cao et al., 2006). La détec-tion de la stabilité ou de l'homogénéité est liée à la tâche de classification des flux de données. Dans cet article, nous considérons une fenêtre temporelle glissante avec un pas d'une journée, afin d'obtenir pour chaque fenêtre une distribution de lieux ou d'intérêts. Nous proposons dans cet article une nouvelle approche basée sur le traitement de signaux, décrite dans la section 2, adaptée à la tâche de profilage. Nous avons ensuite testé l'algorithme sur des données simulées pour valider sa qualité par rapport aux approches traditionnelles ; les résultats sont présentés à la section 3. Enfin, nous avons appliqué le cadre proposé à un jeu de données industrielles réelles, comme indiqué à la section 4. Une conclusion est donnée à la section 5.
Proposed approach
Algorithm 1 Détection de changements dans un signal de profil utilisateur Entrée : Vecteur de signal v de longueur N . sortie : Liste des changements détectés.
Lissage :
5:
Initialiser la liste locale des sauts L e = ∅ 8:
Calcul de la fonction de coût en fonction des différences finies du premier ordre :
for k ← 1, length(v j−1 ) do 10:
Calcul des maxima locaux de la fonction de coût :
12:
Afin de détecter les changements de profil des utilisateurs, nous avons appliqué l'algorithme de détection de changement décrit ci-dessous. Cet algorithme détecte des "sauts" inhabituels dans un signal caractérisant des variations de profil d'un utilisateur. Pour construire un tel signal, nous avons défini comme profil de référence la distribution d'étiquettes ou de codes postaux dans les premières fenêtres temporelles. Ensuite, la fenêtre est décalée d'un jour à la fois, afin de produire une série de distribution. La similarité entre deux distributions de probabilité (fenêtre de référence et fenêtres décalées) est calculée par la divergence de JensenShannon (JS) (Manning et Schütze, 1999;Dagan et al., 1997), une version symétrisée et lissée de la divergence de Kullback-Leibler D(P Q) entre deux distributions discrètes. Notez que toutes les probabilités égales à P ou Q sont ignorées dans le calcul, ce qui signifie que deux distributions totalement différentes auront une valeur JS de 1. L'approche proposée a été testée sur des ensembles de données artificielles pour validation, puis appliquée sur les ensembles de données réels pour analyser les changements de profil et d'état d'esprit des utilisateurs. L'algorithme 1 décrit l'approche de détection de changement multi-échelles. L'idée est la suivante : un processus de lissage itératif élimine les fluctuations aléatoires du signal (lignes 5 et 6), puis détecte des variations anormalement élevées (lignes 12 à 14). Les signaux sont des fonctions continues fragmentées présentant des discontinuités à certains emplacements
. Nous considérons ici que v j k sont les moyennes d'une fonction v discrétisée sur les intervalles I j,k = 2 −j [k, k + 1[. Dans une approche multi-échelle basée sur des coefficients, une stratégie pour détecter les singularités au niveau j est basée sur un critère qui utilise les différences de premier ou de second ordre de v j , la détection des singularités de saut est effectuée à chaque niveau indépendamment. Nous calculons ensuite le nombre N j de singularités au niveau j et définissons j max comme le plus grand niveau j tel que N j−1 = N j . Nous définissons également le niveau j min comme le plus petit j tel que N j = N jmax . Une singularité détectée dans I j,k pour j min < j < J est dite recevable s'il existe une singularité dans I j+1,2k ou I j+1,2k+1 .
Validation expérimentale
Pour valider la qualité de notre algorithme dans un environnement contrôlé, nous l'avons testé sur des ensembles de données artificielles. Pour générer ces données, nous avons considéré trois catégories de variations de profils : soit le profil de l'utilisateur change avec le temps en un profil totalement nouveau, soit il devient partiellement différent, soit il reste stable. Nous avons généré 10000 signaux pour chacune de ces catégories. Pour construire un signal, nous avons d'abord généré deux ensembles de 1 à 5 étiquettes aléatoires, chacun représen-tant des profils possibles avant et après le changement. Un seul ensemble est créé pour simuler l'absence de changement. Pour simuler un changement partiel, nous avons forcé les deux ensembles à partager 1 ou 2 étiquettes. Nous avons simulé une période de deux mois. Cent "time-stamps" aléatoires ont été générés au cours de cette période, chacun associé à une éti-quette du premier ou du second ensemble, selon une date de changement choisie au hasard. Pour démontrer l'efficacité de l'approche proposée, nous avons évalué ses performances en termes de temps de calcul et avons calculé les moyennes des différences absolues entre la date de modification détectée et prévue et les avons comparées à un ensemble d'algorithmes de l'état de l'art : Jump penalty, PWC bilateral, Robust jump penalty, Soft mean-shift, Total variation, Robust TVD et Medfiltit (Little et Jones, 2011). Le tableau 3 présente les résultats de la comparaison. Les bonnes performances de l'algorithme proposés sont dûs à l'utilisation d'une fonction de lissage bien adaptée à la fonction de coût et surtout aux applications successives de ce lissage de façon itérative, permettant de détecter des variations stables dans le signal. Le processus est par ailleurs peu complexe, ce qui explique les faibles temps de calculs observés.
Algorithmes
Profil  Afin de suivre l'évolution des habitudes de déplacement des utilisateurs, la géolocalisation (codes postaux) associées à des horodatages sur une période de 74 jours pour 598 utilisateurs ont été utilisés. L'objectif de ces données est de pouvoir détecter le moment où un utilisateur déménage dans un endroit différent ou passe du temps en dehors de sa zone habituelle. Lors de la création du signal, nous avons utilisé une fenêtre de 10 jours. Dans Figure 1.a, la dissimilarité de Jensen-Shannon augmente fortement pendant deux jours, reste stable pendant trois jours, puis augmente à nouveau. Deux changements sont détectés, le premier étant un changement partiel. Ce type de signal peut être interprété comme un mouvement en deux étapes, avec une période pendant laquelle l'utilisateur passe du temps aux deux endroits avant de se déplacer définitivement. Un autre cas intéressant est celui où l'utilisateur part en vacances ou pour son travail quelque temps, avant de retourner à son lieu de résidence habituel. Les figures 1.b et 1.c montrent deux exemples pour ce cas.
Pour suivre les changements réels d'intérêt individuel, nous avons utilisé un ensemble de données du journal de navigation de 142794 utilisateurs, fournissant à chaque utilisateur une liste des "time-stamps" associés à l'URL visitée à ce moment, sur une période de 30 jours. .c illustre ce type d'utilisateurs. Comme vous le voyez, ces signaux montent et restent stables sur une période de temps puis diminuent. Cela signifie que la différence entre la fenêtre de référence et les fenêtres décalées augmente pendant un certain temps, mais qu'à la fin de la période enregistrée, la distribution des catégories d'URL visitées revient à une distribution similaire à la distribution de référence.
Conclusion
Dans cet article, nous avons proposé un nouvel algorithme multi-échelles de détection de changement pour analyser les variations de profils individuels des utilisateurs en fonction de leurs données de navigation et de géolocalisation. Nous avons d'abord créé, pour chaque utilisateur, un signal de l'évolution de la répartition de l'intérêt des utilisateurs en ligne et un autre signal basé sur la distribution des emplacements physiques enregistrés au cours de leur navigation. Ensuite, nous avons proposé un algorithme de détection de sauts capable de détecter automatiquement les changements. Nous avons détecté différents scénarios : au cours de la pé-riode analysée, certains utilisateurs ont conservé le même profil, certains ont eu un changement net de profil et d'autres n'ont montré qu'un changement provisoire. Les tests expérimentaux effectués sur des signaux simulés ont montré que l'approche proposée est plus rapide et fait moins d'erreurs pour cette tâche que les algorithmes de l'état de l'art.

