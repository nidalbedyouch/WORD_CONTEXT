[
  {
    "id": "230",
    "text": "Introduction\nUn système de recherche d\u0027information (SRI) est un module logiciel qui sélectionne, à partir d\u0027une collection de documents, une liste de documents potentiellement pertinents en réponse à une requête utilisateur. Le processus suivi par un SRI est composé de 3 étapes.\nIndexation. Cette étape permet de passer d\u0027un document textuel à un document qui peut être utilisé dans la RI. Elle se base sur l\u0027extraction des mots les plus importants des textes. Lors de cette étape, les mots vides tels que le, la, les sont généralement éliminés ; les termes sont ensuite racinisés, c\u0027est-à-dire que des règles de transformation sur les termes sont appliquées afin d\u0027obtenir un radical, limitant les variantes des termes à une forme unique ; enfin une pondération reflète l\u0027importance des différents radicaux obtenus. Dans un cadre non dynamique, l\u0027indexation est réalisée sur l\u0027ensemble des documents, avant toute recherche.\nCalcul des scores de pertinence des documents. Lorsqu\u0027une requête est soumise au système, des scores de pertinence sont attribués aux termes qui la composent, en tenant compte de leur présence dans les documents. Ces scores sont ensuite combinés pour calculer le score global de chacun des documents de la collection. Il existe de nombreux modèles de pondéra-tion. La plupart sont basés sur les facteurs T F et IDF . L\u0027expression T F (Term Frequency) correspond à la fréquence du terme dans le document, tandis que l\u0027IDF (Inverse Document Frequency) désigne la fréquence inverse du terme dans le document, inversement proportionnel au nombre de documents qui contiennent le terme.\nReformulation de la requête. Cette étape permet de créer une requête plus adéquate à la RI que celle initialement formulée par l\u0027utilisateur. Le principe de la reformulation automatique est de modifier la requête de l\u0027utilisateur en ajoutant des termes significatifs ou en ré-estimant leurs poids. Dans sa version automatique, il s\u0027agit de considérer les premiers documents restitués comme pertinents et d\u0027ajouter des termes issus de ces documents ; de nouveaux poids sont également estimés et les scores des documents recalculés pour fournir la réponse finale du système. Ce paramètre n\u0027est pas étudié dans le travail présenté dans cet article.\nChacune de ces étapes fait intervenir différents paramètres : par exemple lors de l\u0027indexation, il est possible de choisir entre différents outils de racinisation, lors du calcul des scores de pertinence des documents, différents modèles de pondération peuvent être choisis. Les différents paramètres étudiés dans cet article sont présentés en figure 1.\nL\u0027efficacité d\u0027un SRI est évaluée en calculant des mesures de performance comme le rappel, la précision et d\u0027autres mesures associées. Depuis ses débuts, le domaine de la RI est très actif pour fournir de nouvelles propositions correspondant à une évolution de ces trois étapes. Lorsqu\u0027un nouveau modèle de RI est proposé, ses paramètres sont étudiés, mais sans considé-rer les effets croisés. Par exemple dans Ponte et Croft (1998), ce sont les paramètres du modèle lui-même qui sont étudiés sans regarder l\u0027influence du choix de l\u0027algorithme de racinisation. Les modèles d\u0027apprentissage d\u0027ordonnancement (Learning to rank en anglais) considèrent de nombreux paramètres tels que les fréquences TF et IDF, la taille des documents et des caractéristiques comme les scores BM25, LMIR, PageRank des documents (Qin et al., 2010). Ces approches ont pour objectif d\u0027optimiser l\u0027ordonnancement des documents mais ne cherchent pas à connaître l\u0027impact des paramètres. Quelques travaux visent à sélectionner les variables importantes, donc à étudier leur influence (Laporte et al., 2014;Naini et Altingovde, 2014).\nAinsi, généralement, les paramètres sont étudiés de façon indépendante, sans considérer les effets croisés des paramètres. Compte tenu du nombre de paramètres, l\u0027étude des effets croisés est difficile et implique au préalable de collecter des données suffisantes pour le faire. Cet article s\u0027attaque à ce problème. Ainsi, dans cette étude, nous nous appuyons sur un ensemble de données massif (2 millions de configurations) dans lequel les différents paramètres varient.\nLa littérature du domaine ne s\u0027est que peu intéressée à une analyse de cette nature. Presque tous les articles et les thèses du domaine de la RI rapportent des études montrant la variation de mesures de performance en fonction d\u0027un ou plusieurs paramètres, mais il ne s\u0027agit pas d\u0027une analyse en parallèle de paramètres variés. Quelques travaux se sont cependant intéressés à utiliser les méthodes d\u0027analyse pour étudier les résultats de moteurs de recherche sur un ensemble de requêtes. Banks et al. (1999)   Compaoré et al. (2011) présentent une étude qui a les mêmes objectifs que ceux de ce papier. Cependant, le nombre d\u0027éléments analysés et donc les combinaisons de paramètres est bien moindre. Dans leurs études, les auteurs montrent que les paramètres qui ont le plus d\u0027influence sont différents en fonction que l\u0027on considère les besoins d\u0027information faciles ou difficiles. Bigot et al. (2014) utilisent les résultats d\u0027une analyse pour sélectionner la configuration de système la plus adaptée en fonction de la difficulté du besoin d\u0027information. L\u0027objectif de l\u0027analyse que nous présentons dans le présent papier est d\u0027étudier à grande échelle les caractéristiques des SRI dans le but de déterminer les meilleures combinaisons de paramètres selon certaines mesures de performance. Ce travail a été mené dans le cadre du projet ANR CAAS (Contextual Analysis and Adaptive Search) ANR-10-CORD-001-01.\nLa suite de cet article est structurée comme suit. La section 2 présente la méthode utilisée pour obtenir les données ainsi que les données elles-mêmes. La section 3 présente l\u0027analyse de la dépendance entre les paramètres et leur influence mutuelle. La section 4 s\u0027attache à déterminer quelles sont les valeurs de paramètres les plus susceptibles de conduire à de bonnes performances du moteur de recherche correspondant. La section 5 conclut cet article.\nVariantes de moteurs de recherche et paramétrage\nLes données ont été générées via l\u0027interface RunGeneration présentée dans Louédec et Mothe (2013) et qui est une sur-couche à la plateforme Terrier.\nTerrier et RunGeneration\nLa plateforme de RI TERRIER (Ounis et al., 2006) possède de nombreuses possibilités de paramétrage, tant au niveau de l\u0027indexation (indexation par blocs de différentes tailles, choix de la racinisation, etc.) que de la recherche (différents modèles de pondération pour la mise en correspondance entre la requête et les documents, différentes normalisations des poids) et de la reformulation de requêtes. Une fois paramétré, Terrier permet, pour un besoin d\u0027information ou un ensemble de besoins, de retrouver les documents susceptibles de répondre à ce besoin.\nL\u0027interface RunGeneration a comme objectif de faciliter le paramétrage d\u0027une chaine de traitement sous Terrier. Une fois les paramètres sélectionnés au travers de l\u0027interface, celle-ci crée le fichier \"terrier.properties\" indispensable à Terrier et contenant l\u0027ensemble des paramètres. Le second objectif de l\u0027interface RunGeneration est de permettre de lancer plusieurs combinaisons de paramètres simultanément, ce qu\u0027il n\u0027est pas possible de faire en utilisant la plateforme Terrier. Ainsi plusieurs indexations et recherches sont effectuées sur les mêmes données via une seule intervention de l\u0027utilisateur. Celui-ci peut par exemple demander en une\nFIG. 1 -Paramètres de la génération des données.\naction plusieurs indexations de documents avec des paramètres différents. L\u0027interface a été développée pour fonctionner sur les versions 3.0 et 3.5 de Terrier 1 (Louédec et Mothe, 2013). Ainsi, lors de la génération des données utiles à notre analyse, le principe est le suivant : pour chaque combinaison de paramètres, une liste de documents retrouvés en réponse au besoin d\u0027information est constituée. Cette réponse du système est alors évaluée sur la base de mesures de RI. Ainsi, pour une combinaison de paramètres, nous connaissons la valeur de chacune des caractéristiques correspondant aux paramètres du moteur et aux mesures de performance.\nParamètres utilisés lors de la génération de données\nLa figure 1 indique les variables utilisées lors de la génération des données ainsi que les modalités de ces variables paramètres. Le nombre de combinaisons obtenues est de 2 263 800. Les variables correspondant à des paramètres du système sont qualitatives.\nCollection d\u0027évaluation utilisée\nCompte tenu du nombre de combinaisons et des temps nécessaires pour générer les données, dans cette étude, nous n\u0027avons utilisé qu\u0027une seule collection de documents : la collection TREC-8 de la tâche adhoc de TREC 2 . Elle comprend environ 530 000 documents soit 2 Go ; chaque document est composé en moyenne de 532 mots. La collection comprend également 50 besoins d\u0027information et les jugements de pertinence des documents associés à ces besoins. Sur ce jeu de données, nous n\u0027avons pas pris en compte les paramètres de reformulation de requêtes afin de ne pas rendre le nombre de combinaisons possibles trop grand pour être généré. Plutôt nous avons fait l\u0027hypothèse qu\u0027une première analyse permettrait de faire ressortir les paramètres principaux qui eux pourront être combinés avec les paramètres de reformulation. En effet, les principes de reformulation (implantés dans Terrier) s\u0027appuient tous sur l\u0027utilisation des premiers documents retrouvés suite à une première recherche. Aussi, optimiser la précision dans ces premiers documents, optimise à priori la reformulation de requêtes.\nCaractéristiques d\u0027évaluation associées\nAfin d\u0027évaluer les résultats obtenus nous avons utilisé trec_eval 3 qui calcule plus de 100 mesures de performance telles que bpref , AP et P @5. Cependant, nous avons restreint les variables utilisées à celles qui sont les moins corrélées. Ainsi, nous avons conservé les 6 mesures de performance préconisées dans Baccini et al. (2012). Elles sont résumées dans la table 1. Elles sont toutes quantitatives à valeur continue et leurs valeurs sont comprises entre 0 et 1. \nDistribution des valeurs des variables de mesure de performance\nLa figure 2 montre la distribution des valeurs des variables correspondant aux mesures d\u0027évaluation de la performance. On note que le minimum 0 est atteint pour chacune des variables ; en revanche le maximum 1 n\u0027est atteint que pour la P @30 et la iprec@recall0. L\u0027ensemble de ces figures montre que les valeurs sont faibles ; la valeur 0 est la plus fréquente, montrant ainsi que beaucoup de configurations échouent dans la RI. Par ailleurs, comme les données ne suivent pas une loi normale, il faudra faire attention aux analyses réa-lisées par la suite pour ne choisir que celles qui s\u0027appliquent à des variables qui ne suivent pas une loi normale. Cependant, les tests et méthodes que nous utilisons dans la suite restent valident car nous travaillons sur un grand jeu de données.\nCorrélations entre variables de même type\nNous avons étudié la corrélation d\u0027une part entre les variables correspondant aux paramètres du SRI et d\u0027autre part entre variables d\u0027évaluation des moteurs.\nNous avons analysé le lien entre les paramètres du moteur de RI, pris deux à deux afin de savoir si certaines de ces variables paramètres ont des rôles similaires ou sont liées entre elles. Pour cela, nous avons réalisé le test du chi 2 . Soit l\u0027hypothèse H0 «les deux variables sont indépendantes» contre H1 «les deux variables ont un lien». Nous rejetons l\u0027hypothèse H0 si la p-value est inférieure à 0, 05. Après avoir réalisé le test pour chacune des variables, nous concluons que toutes les variables qualitatives sont indépendantes deux à deux. Elles ont donc chacune leur rôle spécifique.\nEn revanche, en ce qui concerne les variables quantitatives correspondant aux mesures de performance, nous avons constaté qu\u0027il existe une corrélation. Cette corrélation est plus ou moins importante en fonction des mesures que l\u0027on compare. La figure 5  Ainsi les mesures de performance, déjà réduites à 6 pour plus de 100 au départ sont assez redondantes dans leur capacité à mesurer les performances des systèmes puisque corrélées, même si elles ne mesurent pas le même phénomène. Les paramètres du système en revanche n\u0027étant pas corrélés, cela a un sens de chercher à optimiser chacun de ces paramètres.\nCorrélations entre variables paramètres et variables d\u0027évaluation\nAfin d\u0027étudier l\u0027effet des paramètres sur les mesures de performance, nous avons effectué une analyse de la variance (ANOVA).\nSoit l\u0027hypothèse H0 «Le paramètre n\u0027a pas d\u0027effet sur la mesure de performance» contre H1 «le paramètre a un effet sur la mesure de performance». Nous rejetons H0 si la p-value est \u003c 0, 05.\nNous avons étudié cette corrélation sur chacune des mesures de performance. La table 2  paramètres ont un effet significatif. Ces trois paramètres sans effet pourront donc être fixés dans la génération éventuelle d\u0027autres données. Pour être réellement exhaustif, il faudrait vérifier que ces paramètres n\u0027ont pas d\u0027influence lorsque l\u0027on change de collection, mais compte tenu de leur nature, la probabilité que cela soit le cas est forte.\nEffet significatif\nEffet non significatif TrecQueryTagsProcess, Topic BlocSize, IgnoreEmptyDocuments RetrievingModel, Stemmer IgnoreLowIdfTerms TAB. 2 -Effets significatifs et non significatifs pour l\u0027ensemble des mesures de performance.\nVariables ayant le plus d\u0027influence\nNous avons utilisé la méthode Stepwise qui est une régression linéaire multiple Bendel et Afifi (1977) pour étudier l\u0027influence relative des différentes variables paramètres. Cette mé-thode ajoute les variables les plus significatives du modèle et retire les moins significatives pas à pas. Dans le cadre de notre étude, elle a pour objectif de sélectionner les paramètres qui ont le plus d\u0027influence sur les mesures de performance. C\u0027est une combinaison de la mé-thode Forward et de la méthode Backward. La première méthode part du modèle vide et ajoute les variables les plus significatives du modèle progressivement, tandis que la seconde part du modèle complet et élimine progressivement les variables les moins significatives du modèle.\nAprès avoir réalisé cette analyse sur les différentes mesures de performance, nous observons que les trois variables supprimées sont BlocsSize, IgnoreEmptyDocuments et IgnoreLowIdf T erms, comme dans le cas de l\u0027étude des corrélations précédente.\nAu final cette méthode sélectionne donc les paramètres T opic, T recQueryT agsP rocess, RetrievingM odel et Stemmer. La variable T opic est la plus significative du modèle, suivi de T recQueryT agsP rocess, puis de RetrievingM odel et de Stemmer. Le besoin d\u0027information considéré est le paramètre dont dépend le plus les résultats. Cela est un résultat important concernant la variabilité des résultats. On aurait pu penser que le modèle de recherche utilisé pouvait être le plus important des paramètres. La formulation du besoin d\u0027information est éga-lement importante puisque le paramètre T recQueryT agsP rocess correspond aux parties du besoin d\u0027information pris en compte lors du traitement. Lorsque seul le titre est considéré, il correspond à quelques mots, taille typique des requêtes sur le web. Lorsque les autres champs sont également considérés, il peut s\u0027agir de requêtes plus longues, donnant un contexte précis du besoin d\u0027information. Cette première analyse avait pour objet de déterminer les paramètres du moteur les plus importants ou qui influencent le plus la performance d\u0027une recherche. Dans la section suivante, nous déterminons quelles sont les valeurs de paramètres les plus susceptibles de conduire à de bons résultats.\n4 Paramètres des SRI pour des classes de précision L\u0027objectif de cette analyse est d\u0027étudier les valeurs des paramètres du moteur de recherche qui peuvent être associées à des valeurs de précision. Nous nous sommes appuyés dans cette étude sur la AP qui est la mesure consensuelle lorsqu\u0027il s\u0027agit de comparer globalement plusieurs systèmes et qui est utilisée en particulier dans la campagne d\u0027évaluation TREC (trec.nist.gov) (Voorhees, 2007).\nClassification mixte\nLa classification mixte est une méthode de classification qui a pour objectif d\u0027obtenir, à partir des facteurs issus d\u0027une analyse des correspondances multiples (ACM), des classes d\u0027individus les plus cohérentes possibles en constituant les groupes les plus homogènes.\nDans notre cas d\u0027étude, nous utilisons cette méthode afin d\u0027associer à une classe de valeur de AP les paramètres de moteurs. L\u0027idée sous-jacente est de favoriser les combinaisons de paramètres qui sont plutôt associées à des valeurs fortes de AP et d\u0027éviter les combinaisons de paramètres plutôt associées à des valeurs faibles de AP .\nCette étude nécessite d\u0027appliquer une ACM, méthode qui s\u0027applique sur des variables qualitatives. Afin de transformer le paramètre d\u0027évaluation qualitatif considéré (l\u0027AP ) en valeurs qualitatives, nous avons créé des classes de valeurs de AP . Les classes ont été définies de sorte d\u0027avoir des effectifs comparables. Nous noterons dans la suite la classe map1 la classe ayant FIG. 6 -Valeur des paramètres pour les classes de AP.\nles valeurs de AP les plus faibles jusqu\u0027à map4 la classe ayant les valeurs de AP les plus fortes. La figure 6 présente les résultats du croisement entre les classes de AP et les variables paramètres selon la méthode de classification mixte.\nNous pouvons observer dans la figure 6 les modalités présentes dans chacune de ces classes. La classe 1 contient des mesures de performance à valeurs faibles, la classe 2 des mesures de performance à valeurs moyennes, la classe 3 des mesures de performance à valeurs très faibles et la classe 4 des mesures de performance à valeurs élevées.\nLa combinaison de paramètres la plus représentative pour la classe 4, c\u0027est-à-dire pour la classe ayant des mesures de performance à valeurs élevées est :\n-IgnoreEmptyDocuments \u003d TRUE -Stemmer \u003d PS -RetrievingModel \u003d LemurTFIDF -TrecQueryTagsProcess \u003d TITLE -IgnoreLowIdfTerms \u003d TRUE -IgnoreEmptyDocuments \u003d TRUE -Stemmer \u003d PS -RetrievingModel \u003d LemurTFIDF -TrecQueryTagsProcess \u003d TITLE -IgnoreLowIdfTerms \u003d TRUE La combinaison de paramètres la plus représentative pour la classe 3, c\u0027est-à-dire pour la classe ayant des mesures de performance à valeurs très faibles (map1) est :\n-IgnoreEmptyDocuments \u003d FALSE -Stemmer \u003d Crop -RetrievingModel \u003d DFI0 -TrecQueryTagsProcess \u003d NARR -IgnoreLowIdfTerms \u003d FALSE Cette combinaison de paramètres est donc à éviter, de même, que la combinaison de paramètres la plus représentative pour la classe 1, c\u0027est-à-dire pour la classe ayant des mesures de performance à valeurs faibles (map2).\nConclusions et perspectives\nDans cet article, nous nous sommes intéressés à une analyse massive de résultats de recherche d\u0027information obtenus par un paramétrage du système. Ainsi, de nombreux paramètres ont été analysés, en étudiant les effets croisés de ceux-ci. Nous avons pu distinguer les requêtes en fonction de leur niveau de difficulté et définir les paramètres qui ont le plus d\u0027influence en fonction de ces classes ainsi que leurs valeurs les plus adaptées. Un aspect qui reste à étudier est l\u0027influence de la collection sur les résultats obtenus. En effet, nous nous sommes ici intéressés à une collection unique (TREC8).\nDans la suite de ces travaux, nous allons travailler sur des méthodes sélectives de recherche d\u0027information, c\u0027est à dire des méthodes qui adaptent le traitement en fonction des cas rencontrés. Ainsi, toutes les requêtes ne seront pas traitées de la même façon par le moteur, mais les paramètres du système seront au contraire différents en fonction du type de requêtes.\nLe projet CAAS, financé par l\u0027ANR dans le cadre de l\u0027appel Contint 2010, a permis de développer le travail présenté ici. Nous remercions également Anthony Bigot et Sébastien Déjean pour leurs précieux conseils.\n"
  },
  {
    "id": "231",
    "text": "Introduction\nL\u0027analyse d\u0027opinions est une tâche de fouille de textes qui consiste en l\u0027identification et la classification des textes subjectifs en plusieurs catégories d\u0027opinions (polarités). Dans la dernière décennie, beaucoup de travaux se sont penchés sur cette problématique, en prenant le problème sous différents angles (principalement statistique et/ou linguistique). Cependant, la question de visualisation n\u0027a pas bénéficié de cet intérêt. La plupart des travaux proposent une visualisation basique (e.g., graphiques en secteurs), ce qui est clairement insuffisant dans un contexte de big data où l\u0027utilisateur a d\u0027autant plus besoin d\u0027explorer les données dans l\u0027ensemble, mais aussi dans le détail.\nDans ce travail, nous nous situons dans un contexte de veille sur le Web et nous nous intéressons au problème d\u0027analyse d\u0027opinions dans un contexte de veille. Ainsi, nous proposons une méthode de visualisation d\u0027opinions basée sur l\u0027utilisation de termes clés afin de restituer le maximum d\u0027information à l\u0027utilisateur. Notre méthode est implémentée au sein de la plateforme de veille AMIEI 1 . La section suivante présente la problématique de recherche que nous traitons. La section 3 présente le processus général de veille avec la plateforme AMIEI. La section 4 présente notre approche pour l\u0027analyse d\u0027opinions et la visualisation des résultats. Enfin, la section 5 présente un exemple d\u0027application sur un corpus de tweets politiques.\nContexte et Problématique\nL\u0027analyse d\u0027opinions est un domaine de recherche qui se concentre sur l\u0027identification et la classification des opinions dans les données textuelles. Beaucoup de travaux se sont intéressés à l\u0027une ou l\u0027autre de ces problématiques mais la plupart se sont intéressés à la classification d\u0027opinions, i.e., l\u0027association d\u0027un texte à une catégorie d\u0027opinions (e.g., opinion positive vs. négative).\nLa problématique a été majoritairement approchée sous un angle statistique et/ou linguistique. D\u0027un point de vue statistique, le texte est représenté sur l\u0027espace de descripteurs (e.g., termes) afin qu\u0027il puisse être traité par les outils d\u0027apprentissage statistique, e.g., Pak et Paroubek (2010); Pang et al. (2002). Ces méthodes sont connues pour leur généricité (bon rappel). De l\u0027autre part, les méthodes de linguistique, également appelées méthodes à base de règles, ont été largement déployées pour l\u0027analyse d\u0027opinions, e.g., Kennedy et Inkpen (2006); Wilson et al. (2005). Ces méthodes sont connues pour leur spécificité (bonne précision). Enfin, d\u0027autres travaux ont tenté de mixer la généricité de la statistique et la spécificité de la linguistique afin de proposer des méthodes à la fois robustes et précises (méthodes hybrides), e.g., Dermouche et al. (2013); Kamps et al. (2004); Turney et Littman (2003).\nDans ce travail, nous nous intéressons au problème de visualisation de l\u0027opinion. En effet, la problématique de visualisation n\u0027a pas été suffisamment étudiée dans ce domaine en se contentant de visualiser les proportions de chaque polarité d\u0027opinion sur un graphique. Cette méthode est clairement insuffisante dans le cas où l\u0027on veut savoir davantage sur ses données. Par exemple, dans le domaine industriel, il serait intéressant d\u0027identifier les idées redondantes et les concepts qui sont présents dans une catégorie d\u0027opinion et pas dans une autre. Une telle visualisation a des applications directes dans plusieurs domaines, e.g., la veille stratégique et économique, la CRM, la e-réputation, etc.\nAcquisition de l\u0027information\nCette phase permet l\u0027acquisition de l\u0027information selon plusieurs modes : -Un moteur de recherche pour faire des recherches ponctuelles pouvant être capitalisées. -Un automate de collecte pour des opérations récurrentes à des fins de capitalisation. \nCapitalisation et traitement\nPartage de l\u0027information\nLe partage et la diffusion des informations acquises et validées, ainsi que les résultats de l\u0027analyse se font à travers un portail de consultation, permettant la recherche et le partage des informations organisées par thématique avec une gestion des droits d\u0027accès à partir de profils prédéfinis. Le partage peut également se faire via \"Mon espace\" ; un module permettant de personnaliser, pour chaque utilisateur, son accès à la plateforme AMI EI.\nPour l\u0027analyse d\u0027opinions, la plateforme AMIEI offre la fonctionnalités suivantes : -Indicateurs classiques de l\u0027opinion globale dans un corpus de documents (distribution des documents sur les classes de polarité). -Visualisation des termes clés pour chaque classe de polarités. Un terme clé doit être fréquent et discriminant vis-à-vis de la classe d\u0027opinion qu\u0027il caractérise. -Evolution des termes clés à travers le temps. -Soit c la classe d\u0027opinion du document d (classe la plus probable). -Evaluer chaque terme w i du document d selon un critère de spécificité (pouvoir discriminatif du terme au regard de la classe d\u0027opinions). Ici, nous choisissons comme critère le gain informationnel (IG). Ensuite, trier les termes w i du document selon ce critère : IG(w m |c) \u003e IG(w n |c) \u003e ... \u003e IG(w p |c). -Les K premiers termes sont ceux qui \"expliquent\" le mieux cette classification. Nous précisons que les termes discriminants de deux classes différentes sont deux ensembles disjoints. En effet, un terme ne peut être responsable d\u0027affecter un texte qu\u0027à une seule classe.\nMéthode et implémentation\nVisualisation\nLa visualisation est une étape clé dans le processus d\u0027analyse d\u0027opinions, d\u0027autant plus dans un contexte de big data. En effet, l\u0027information utile est encore plus enfouie et difficile à retrouver, ce qui nécessite des techniques de visualisation efficaces et adaptées à ce contexte particulier. Dans AMIEI, nous proposons de visualiser l\u0027opinion contenue dans un corpus de FIG. 2 -Visualisation en nuage de termes (extrait).\nFIG. 3 -Visualisation en fisheye (extrait).\ndocuments par un \"nuage de termes\" construit à partir de l\u0027ensemble des termes discriminants responsables de la classification (cf. section 4.1.). Chaque terme discriminant est ainsi repré-senté par une taille proportionnelle à sa fréquence dans le corpus des textes. Nous proposons également une visualisation temporelle du nuage de terms en utilisant la technique de fisheye.\nEtude de Cas\nNous réalisons une expérimentation sur un corpus composé de 50000 tweets issus d\u0027une collecte massive réalisée par la plateforme de veille AMIEI dans la soirée du 02 Mai 2012 avec le tag \"#ledebat\" (400000 tweets collectés). Ces tweets sont relatif au débat télévisé du second tour de l\u0027élection présidentielle française de 2012 ayant opposé F. Hollande et N. Sarkozy. Nous appliquons, comme prétraitement, la suppression de mots outils et de numériques.\nLes Figures 2 et 3 représentent la visualisation du résultat d\u0027analyse du corpus Politique. Pour une meilleure lisibilité, seulement une sélection de termes fréquents est représentée ici. La polarité des termes est représentés par une couleur (vert pour le positif et rouge pour le négatif). Ces résultats nous ont permis de cerner les termes et les concepts les plus importants dans chaque catégorie d\u0027opinion. A partir de cette visualisation, nous pouvons tirer plusieurs enseignements dont voici quelques uns : -Le concept de \"changement\" dans toutes ses variantes (slogan phare de la campagne du candidat F. Hollande) est largement repris par les internautes, et ce de manière positive.\n"
  },
  {
    "id": "232",
    "text": "Introduction\nLe projet ANR IMAGIWEB 1 consiste à analyser et à suivre l\u0027évolution de l\u0027image (au sens de l\u0027opinion) sur la toile, d\u0027une part des personnages politiques à travers le réseau social Twitter, et d\u0027autre part de l\u0027entreprise EDF vis-à-vis du nucléaire en utilisant des blogs comme données. Ce projet regroupe différents partenaires parmi lesquels un laboratoire de recherche en science politique, des entreprises et des laboratoires de recherche en fouille de données.\nDans un premier temps, les tweets et blogs récoltés sont annotés manuellement pour relater l\u0027opinion qu\u0027ils véhiculent. Par la suite, l\u0027enjeu sera de détecter automatiquement les opinions grâce à des méthodes de fouille d\u0027opinion. Au-delà de la détection des opinions, pour mieux comprendre et analyser le contenu des tweets et des blogs, l\u0027enjeu est aussi de les visualiser et de les explorer. Ainsi, un autre objectif du projet consiste à fournir à l\u0027utilisateur, qu\u0027il soit politologue, sociologue, marketeur ou encore analyste, un outil pour explorer les données (issues de tweets ou de blogs) et pour analyser en ligne l\u0027opinion selon différents points de vue (sujets, temps, ...). L\u0027analyse OLAP (OnLine Analytical Processing) permet de répondre à cet objectif de navigation, d\u0027analyse et de visualisation.\nL\u0027OLAP sur des données textuelles correspond à une thématique de recherche récente avec des enjeux scientifiques importants. En effet, si l\u0027OLAP a su montrer tout son potentiel analytique sur des \"données classiques\", la prise en compte de données textuelles nécessite une adaptation ou une évolution de l\u0027OLAP pour prendre en compte les spécificités de ces données (Ravat et al., 2007;Zhang et al., 2009). Quelques travaux de recherche encore plus récents portent sur l\u0027analyse OLAP de tweets, un cas particulier de données textuelles (Ben Kraiem et al., 2014;Bringay et al., 2011). Dans ce contexte, l\u0027objectif de ce papier est de (1) démontrer l\u0027intérêt de l\u0027analyse OLAP pour ce type de données en se basant sur des cas d\u0027étude réels, (2) relater une implémentation concrète \"classique\" en utilisant des outils existants.\nPour ce faire, dans la section 2 nous commençons par présenter les deux cas d\u0027étude. Dans la section 3, nous évoquons les aspects de modélisation multidimensionnelle et de navigation. Dans la section 4, nous exposons la mise en oeuvre, avant de conclure dans la section 5.\nDeux cas d\u0027étude\nDans le cadre du projet IMAGIWEB, deux cas d\u0027étude sont traités : des tweets à caractère politique et des billets de blogs traitant de l\u0027entreprise EDF et du nucléaire. Pour chacun des cas, un processus d\u0027annotation manuelle concernant l\u0027opinion véhiculée a été mis en place.\nDonnées tweets et besoins d\u0027analyse\nDans le cadre du projet IMAGIWEB, les tweets ont été recueillis grâce à l\u0027API Streaming de Twitter. Ce sont des tweets en français, à caractère politique, portant sur Nicolas Sarkozy et François Hollande, avant et après les élections prési-dentielles de 2012. Les données extraites sont le contenu du tweet, le pseudonyme du twittos, la date du tweet, l\u0027image (à savoir François Hollande ou Nicolas Sarkozy, c\u0027est à dire l\u0027entité sur laquelle porte le tweet), l\u0027URL qui mène vers le tweet.\nUne annotation est faite par un annotateur sur un extrait ou un passage d\u0027un tweet. L\u0027annotateur détermine l\u0027opinion contenue dans le passage (avec une polarité allant de -2 pour une opinion très négative à +2 pour une opinion très positive en passant par le zéro si l\u0027opinion est neutre ou par le NULL s\u0027il n\u0027y a pas d\u0027opinion) ainsi que la cible (le sujet sur lequel porte le passage) et la sous-cible. Les cibles et sous-cibles ont été déterminées par les membres du projet. Citons comme exemple de cible \"bilan\", \"compétences\", \"positionnement\". Pour la cible \"positionnement\", les sous-cibles sont \"alliance\", \"écologie\", \"économie\" et \"sociétal\". Enfin l\u0027annotateur donne un niveau de confiance dans son annotation. 4073 tweets ont été annotés manuellement, ce qui a donné lieu à 5674 annotations.\nLes données tweets constituent le terrain d\u0027analyse des chercheurs en science politique et en sociologie. Les politologues souhaitent pouvoir suivre l\u0027évolution dans le temps des deux images que sont François Hollande et Nicolas Sarkozy à travers Twitter. L\u0027analyse de ces données, à la fois des tweets eux-mêmes et de leurs annotations, constitue un premier enjeu du projet.\nDonnées blogs et besoins d\u0027analyse\nLes blogs à analyser concernent tout ce qui touche à EDF et au nucléaire. À partir d\u0027un ensemble de blogs, tous les articles, en français, avec au moins une occurrence du sigle EDF ou des mots \"Electricité de France\" et de \"nucléaire\" ont été collectés. Les données contiennent le titre de l\u0027article, l\u0027URL du site web dont provient l\u0027article, la date, le contenu textuel et l\u0027image (sécurité, emploi ou prix). Les données blogs contiennent également le passage annoté (à chaque article correspond un ou plusieurs passages), la cible (\"politique\", \"tarifs\" ou encore \"risques\"), la sous-cible (par exemple \"démantèlement/durée de vie\" ou \"expertise/incident\" pour la cible \"risques\"), la polarité et la confiance. 560 articles ont été annotés manuellement en 3420 annotations (6,1 annotations par article en moyenne).\nPar rapport aux besoins, les marketeurs d\u0027EDF souhaitent centrer leur analyse sur les notions de cibles, de polarité. Ils souhaitent également pouvoir naviguer dans les données selon le type de structure (organisme) dont est issu le blog. Cette information peut être portée par l\u0027extension du site web. Par exemple, une organisation à but non lucratif aura généralement un site web avec l\u0027extension \".org\" alors qu\u0027une société aura un site web avec une extension \".com\". Dans le modèle associé aux tweets (cf. figure 1), deux faits sont observés : ANNOTATION et TWEET. À ces faits sont associées plusieurs dimensions, comme le temps, la cible ou encore l\u0027annotateur. Dans la dimension temps, on retrouve plusieurs niveaux de granularité de l\u0027information avec deux hiérarchies : {jour, semaine, année} et {jour, mois, trimestre, année}.\nLe fait TWEET va permettre de compter le nombre de tweets et de RT Confiance donne un indice quant à la confiance accordée à la polarité par l\u0027annotateur. Le modèle permet également de retrouver l\u0027image, la cible, l\u0027annotateur, et bien sûr, le temps. La dimension annotateur rendra possible la comparaison de l\u0027annotation automatique à celle manuelle le moment venu. Les dimensions image, cible et temps sont cruciales pour l\u0027analyse. Une des particularités de ce modèle est de retrouver la polarité et la confiance aussi bien en mesure qu\u0027en dimension. Cela permet de visualiser les données selon différentes manières. La polarité en tant que dimension permet par exemple de visualiser le nombre de fois où la polarité +2 est affectée alors qu\u0027en la plaçant en tant que mesure, elle peut être agrégée avec des fonctions comme la somme ou la moyenne. Le modèle pour les blogs est assez similaire à celui des tweets. On retrouve deux faits Article et Annotation. Les mesures et les fonctions d\u0027agrégat associées sont identiques. On retrouve également plusieurs dimensions en commun, à savoir la polarité, la confiance, la cible et le temps. Toutes ces notions similaires sont en fait celles associées au besoin commun concernant l\u0027analyse de l\u0027opinion. En revanche, notons comme différence que les blogs disposent d\u0027un titre grâce à la dimension Blog et qu\u0027ils sont également porteurs d\u0027informations sur la structure qui héberge l\u0027article (grâce à l\u0027extension du site web) via une dimension Structure.\nÀ partir du modèle multidimensionnel, pour introduire la navigation, la notion de cube OLAP est utilisée. Ainsi, deux cubes ont été créés concernant les données issues de Twitter et il en est de même pour les blogs. La navigation se caractérise par l\u0027application d\u0027opérateurs tels que le Drill Down qui permet d\u0027aller vers un niveau plus détaillé selon la hiérarchie de dimension définie préalablement dans le modèle, en appliquant une fonction d\u0027agrégat sur la mesure qui est observée. Il s\u0027agit par exemple de passer de l\u0027observation de la polarité moyenne par trimestre à l\u0027observation par mois selon la hiérarchie temporelle. L\u0027opérateur inverse s\u0027appelle le Roll Up. Notons également l\u0027existence de l\u0027opérateur Slice \u0026 Dice qui permet de sélectionner certaines valeurs pour certains axes d\u0027analyse. Par exemple, dans un cube qui permet d\u0027observer le nombre de tweets par mois et par cible, il serait possible de sélectionner quelques cibles sur lesquelles on souhaite se focaliser.\nMise en oeuvre\nDans le cadre du projet IMAGIWEB, nous avons retenu MySQL comme SGBD en raison de contraintes techniques du projet. Nous avons également choisi de développer notre propre ETL (Extract Transform Load, phase correspondant à l\u0027alimentation des données) car nous souhaitions pouvoir apporter des transformations très particulières en lien avec le contenu textuel (relatives à la fouille de texte) pour la suite du projet. Enfin, nous avons préféré le serveur OLAP Pentaho Mondrian en lui greffant l\u0027interface graphique Saiku pour l\u0027étendue de sa communauté et la prise en main de son environnement. L\u0027implémentation résultante permet de naviguer dans les données en construisant des tableaux de bord très facilement pour l\u0027utilisateur comme nous l\u0027illustrons par la suite sur les données Twitter. Notons qu\u0027il y a un menu sur l\u0027interface qui permet également de représenter les données issues de la navigation sous forme de différents types de graphiques qui sont générés très simplement par l\u0027utilisateur.\nInitialement, le politologue peut par exemple observer la polarité moyenne en fonction du temps en trimestre pour les entités Hollande et Sarkozy. Puis, pour observer de façon plus précise, il peut obtenir le détail par mois (ce qui correspond au niveau OLAP à une opération de Drill Down), en se focalisant simplement sur Hollande (réalisant ainsi une opération de Slice), obtenant ainsi les résultats figurant dans la figure 2.\nAinsi, le politologue peut constater une baisse importante de popularité entre le mois de Mai et le mois de Juin (polarité de -0.346 à -0.658). Il peut ensuite détailler les cibles sur lesquelles cette baisse est plus importante, en ajoutant la dimension Cible dans les résultats. Le tableau 1 qui en résulte permet d\u0027observer que l\u0027opinion des Twittos a particulièrement diminué sur ses performances (-0.294 à -1.000) mais aussi sur son positionnement et son projet.\nFIG. 2 -Polarité moyenne et nombre de tweets en fonction du temps en mois pour Hollande\nL\u0027intérêt pour le chercheur en science politique est ici, sur la base de la navigation, de pouvoir établir des liens entre l\u0027opinion exprimée sur le web et des évènements de la vie politique, d\u0027observer également à quel point le Web est un miroir ou non de l\u0027opinion publique au sens large (comparaison avec les sondages d\u0027opinion classiques).\nConclusion\nDans le cadre du projet IMAGIWEB, l\u0027analyse OLAP était une des pistes à explorer pour visualiser les données. La mise en oeuvre de l\u0027architecture décisionnelle a répondu à de réels \n"
  },
  {
    "id": "233",
    "text": "Introduction\nDe nos jours, le maintien opérationnel d\u0027un Système d\u0027Information est devenu un des critères essentiels pour toute entreprise, ou personne cherchant à délivrer un service, ou simplement souhaitant communiquer. Le côté déplaisant de l\u0027interconnexion mondiale des Systèmes d\u0027Information réside dans un phénomène appelé \"Cybercriminalité\". Des personnes, des groupes mal intentionnés ont pour objectif de nuire aux informations d\u0027une entreprise, d\u0027une personne voire d\u0027un Etat. Conséquemment, la détection des intrusions doit permettre de protéger le Système d\u0027Information. L\u0027objectif de cet article est de présenter dans un premier temps l\u0027état de l\u0027art en matière de détection d\u0027intrusions et dans un second temps d\u0027aborder les travaux menés afin de faciliter la visualisation des flux. La première partie de cet article sera consacrée à l\u0027étude de l\u0027existant dans laquelle nous présenterons les différentes approches de détection d\u0027intrusions et leurs limites. Ensuite, nous nous intéresserons à la motivation de nos travaux et nous proposerons une solution. Nous détaillerons par la suite, la première phase de nos travaux ainsi que les résultats et nous terminerons par une conclusion et les perspectives.\nÉtude de l\u0027existant\nUne multitude d\u0027outils (Antivirus, IDS, IPS, HIDS, Firewall) permettent aujourd\u0027hui de mettre en place une sécurité \"relative\" pour l\u0027ensemble du Système d\u0027Information. Les principaux risques résiduels sont l\u0027absence de constat en temps réel sur le signalement des comportements anormaux et sur l\u0027exploitation des vulnérabilités. Il convient donc de répondre en fournissant des contremesures dans des délais raisonnables.\nLes différentes solutions de détection d\u0027intrusions\nLes systèmes de détection des intrusions sont divisés selon les 3 familles distinctes :\n-NIDS (Network Intrusion Detection System) est une sonde chargée d\u0027analyser l\u0027activité réseau du segment où elle est placée et de signaler les transactions anormales (Bhruyan et al (2011)). -HIDS (Host-Based Intrusion Detection System) est basée sur l\u0027analyse d\u0027un hôte selon les produits utilisés, une HDIS surveille le trafic à destination de l\u0027interface réseau, l\u0027activité système et logiciel, les périphériques amovibles pouvant être connectés. -HYBRIDES, qui rassemble les informations des NIDS et HIDS et produit des alertes aussi bien sur des aspects réseau qu\u0027applicatifs. Il existe aussi une variante appelée \"IPS\" (Intrusion Prevention System) étant capable d\u0027appliquer une politique de sécurité lors d\u0027une intrusion. Un autre concept nommé CIDN 1 décrit par Fung (2011) offre la possibilité de partager des informations au travers un espace communautaire sur Internet. Les différentes solutions s\u0027appuient sur deux méthodes, la première est fondée sur une comparaison d\u0027une tentative d\u0027intrusion par rapport à une base de signatures. Ce type de système recherche dans les trames réseau un schéma qui correspond à une signature connue via de l\u0027extraction de motifs. Il est possible d\u0027ajouter de nouvelles signatures, c\u0027est à dire de créer une expression régulière qui correspondra par son contenu à une activité malveillante ou abusive. La seconde méthode repose sur des modèles comportementaux appelés \"profils\". Ils sont utilisés pour détecter les comportements déviant des profils définis. Les anomalies peuvent \"signaler\" une intrusion ou un nouveau comportement. Dans le second cas, il convient d\u0027ajouter ces nouveaux comportements afin de diminuer les \" faux positifs\". Le concept de détection des anomalies repose sur une analyse statistique et un apprentissage temporel des comportements. Plusieurs principes de mise en place sont disponibles comme \"IDES\" 2 (Lunt et al, 1992), ou \"EMERALD\" 3 (Porras et Neumann, 1997 \nLimitation des solutions existantes\nLes principales limites des outils présentés dans les chapitres précédents résident dans le fait que l\u0027analyse des événements et journaux systèmes est souvent considérés comme fastidieuse. De plus, ils ne prennent pas encore en compte l\u0027évolution quasi permanente d\u0027un Système d\u0027Information. Par exemple, la sécurité d\u0027un entrepôt de données peut être mise en cause par la non réévaluation du ou des serveurs hébergeant ce dernier. La structuration organisationnelle et l\u0027analyse de risques s\u0027avèrent donc indispensables. \nMotivations et proposition\nRéalisation de la première phase\nCette phase constitue un tout en soi dans la mesure où la visualisation des données pour les utilisateurs est un enjeu crucial en terme de prise de décisions sur les problématiques de sécurité. Il s\u0027agit du préambule à la \"fouille de données\" qui sera effectuée dans les phases suivantes. Un des principaux équipements de sécurité est le \"Pare-Feu\" , les données brutes envoyées en temps réel par l\u0027ensemble des équipements de filtrage sont traitées selon une extraction de motifs .\nDescription des données\nLe réseau SP1 propose des services à destination de 14 millions de personnes. Les données peuvent être considérées comme sensibles et portent sur une quantité de 9.2 Teraoctets et plusieurs dizaines de millions d\u0027euros par jours. Ces données sont hétérogènes et proviennent de plusieurs sources différentes. Le contenu des variables listées ci-dessous est exporté vers les conteneurs de données. La phase 1 se focalisera uniquement sur l\u0027analyse et la représentation graphique de ces dernières.\n-adresse ip source, adresse ip de destination, port de destination, protocole (udp et tcp) -date et heure de la connexion -numéro de la règle du pare feu appliquée, action appliquée par la politique de filtrage.\nLe tableau 2 synthétise le volume en nombre de lignes traitées par les équipements de filtrage. \nScénario de visualisation\nLa représentation graphique de l\u0027ensemble des flux autorisés selon la période souhaitée relève du problème de vision de grands graphes (voir figure 2), mais il est possible d\u0027extraire des \"sous graphes\" basés sur du \"requêtage\" qui visent à sélectionner les modalités de certaines variables (adresses source et de destination ainsi que les services et protocoles). En revanche, l\u0027analyse d\u0027un graphique fondé sur les flux rejetés (même agrégés) comme le montre la figure 3 s\u0027avère simple mais aussi efficace. Une adresse IP tente de se connecter à plusieurs autres adresses sur le port \"135\". Une recherche de répertoires partagés peut être à l\u0027origine de ce type de comportement. \nRésultat\nA l\u0027issue de la phase 1, Le traitement des informations recueillies sur les différents équipe-ments de filtrage permet de visualiser rapidement les tentatives de connexion depuis plusieurs sources vers plusieurs destinations. Ceci rend possible de soulever des interrogations sur cette transaction et de mettre en place une action de surveillance. D\u0027autres options ont été créées afin d\u0027offrir une visualisation des règles de filtrage les plus utilisées. En cas de doute sur une 8. https ://www.perl.org/ 9. Logiciel de visualisation graphique, http ://www.graphviz.org 10. AfterGlow, outil de génération graphique, http ://afterglow.sourceforge.net/ 11. Responsables de la sécurité du système d\u0027information, ingénieurs sécurité, administrateurs réseau adresse Ip, il est possible de lister toutes les activités de cette dernière selon des critères de temps, de destination, de protocoles et de ports utilisés.\n"
  },
  {
    "id": "234",
    "text": "Introduction\nDans cet article, nous proposons une nouvelle méthode de classification non supervisée de documents multilingues de corpus comparable bruité afin d\u0027améliorer l\u0027extraction des lexiques de traduction. Nous nous basons sur l\u0027approche de (Rouane et al., 2007) dans la réingénierie des modèles UML et (Mimouni et al., 2012) dans la RI qui ont profité d\u0027un couplage entre l\u0027aspect formel et le relationnel afin de prendre en compte des relations entre les objets d\u0027un même contexte. Nous avons choisi d\u0027effectuer un couplage entre l\u0027Analyse Formelle de Concepts (AFC) et les modèles vectoriels. En effet, l\u0027AFC, appliquée dans un contexte de fouille de textes, permet d\u0027extraire des classes de documents sous formes de CFs. D\u0027un autre côté, les modèles vectoriels basés sur les vecteurs des extensions des CFs extraits, permettent d\u0027aligner les CFs des différentes langues en calculant le degré de similarité des Concepts Fermés monolingues extraits, dans l\u0027objectif de générer des CFs multilingues.\nExtraction de Concepts Fermés à partir de corpus comparables\nEn classification de documents, un Concept Fermé est le couple \u003c T, D \u003e, avec T l\u0027ensemble des termes des documents qui appartiennent à tous les documents D, et D, l\u0027ensemble des documents qui contiennent tous les termes de T . Dans notre contexte de recherche, un Concept Fermé représente une classe de documents regroupés selon un ensemble de termes représentatifs. L\u0027extraction des Concepts Fermés, à partir d\u0027un corpus comparable français-anglais, est précédée par une étape de pré-traitement linguistique du corpus comparable bilingue mais aussi une réorganisation du contenu des documents du corpus en question est né-cessaire. Les concepts en sortie sont de la forme : CF \u003d\u003c {t 1 , t 2 , . . . , t n }, {d 1 , d 2 , . . . , d m } \u003e tel que {t 1 , t 2 , . . . , t n } (ou extension) est l\u0027ensemble des termes qui composent un termset fermé et {d 1 , d 2 , . . . , d m } (ou intension) l\u0027ensemble de documents dans lesquels {t 1 , t 2 , . . . , t n } sont apparus ensemble avec une fréquence supérieure ou égale à minsupp. La sortie est composée de l\u0027ensemble des Concepts Fermés français CF f r et des Concepts Fermés anglais CF en séparément.\n"
  },
  {
    "id": "239",
    "text": "?\nrabdesselam/fr/ Résumé. Les résultats de toute opération de classification ou de classement d\u0027objets dépendent fortement de la mesure de proximité choisie. L\u0027utilisateur est amené à choisir une mesure parmi les nombreuses mesures de proximité existantes. Or, selon la notion d\u0027équivalence topologique choisie, certaines sont plus ou moins équivalentes. Dans cet article, nous proposons une nouvelle approche de comparaison et de classement de mesures de proximité, dans une structure topologique et dans un objectif de discrimination. Le concept d\u0027équivalence topologique fait appel à la structure de voisinage local. Nous proposons alors de définir l\u0027équivalence topologique entre deux mesures de proximité à travers la structure topologique induite par chaque mesure dans un contexte de discrimination. Nous proposons également un critère pour choisir la \"meilleure\" mesure adaptée aux données considérées, parmi quelques mesures de proximité les plus utilisées dans le cadre de données quantitatives. Le choix de la \"meilleure\" mesure de proximité discriminante peut être vérifié a posteriori par une méthode d\u0027apprentissage supervisée de type SVM, analyse discriminante ou encore régression Logistique, appliquée dans un contexte topologique. Le principe de l\u0027approche proposée est illustré à partir d\u0027un exemple de données quantitatives réelles avec huit mesures de proximité classiques de la littérature. Des expérimentations ont permis d\u0027évaluer la performance de cette approche topologique de discrimination en terme de taille et/ou de dimension des données considérées et de sélection de la \"meilleur\" mesure de proximité discriminante.\nIntroduction\nLa comparaison d\u0027objets, de situations ou d\u0027idées sont des tâches essentielles pour évaluer une situation, pour classer des préférences ou encore pour structurer un ensemble d\u0027éléments matériels ou abstraits, etc. En un mot pour comprendre et agir, il faut savoir comparer. Ces comparaisons que le cerveau accomplit naturellement, doivent cependant être explicitées si l\u0027on veut les faire accomplir à une machine. Pour cela, on fait appel aux mesures de proximité. Une mesure de proximité est une fonction qui mesure la ressemblance ou la dissemblance entre deux objets d\u0027un ensemble. Ces mesures de proximité ont des propriétés mathématiques et des axiomes précis. Mais est-ce que ces mesures sont toutes équivalentes ? Peuvent-elles être utilisées dans la pratique de manière indifférenciée ? Produiront-elles les mêmes bases d\u0027apprentissage qui serviront comme entrée pour l\u0027estimation de la classe d\u0027appartenance d\u0027un nouvel objet. Si nous savons que la réponse est non, alors comment pouvoir décider laquelle utiliser ? Certes, le contexte de l\u0027étude ainsi que le type de données considérées peuvent aider à sélectionner quelques mesures de proximités, mais laquelle choisir parmi cette sélection ?\nOn retrouve cette problèmatique dans le cadre d\u0027une classification supervisée ou d\u0027une discrimination. L\u0027affectation ou le classement d\u0027un objet anonyme à une classe dépend en partie de la base d\u0027apprentissage utilisée. Selon la mesure de proximité choisie, cette base d\u0027apprentissage change et par conséquent le résultat du classement aussi.\nOn s\u0027intéresse ici au degré d\u0027équivalence topologique de discrimination de ces mesures de proximité. Plusieurs études d\u0027équivalence topologique de mesures de proximité ont été proposées Batagelj et Bren (1992, 1995; Rifqi et al. (2003); Lesot et al. (2009);Zighed et al. (2012) mais pas dans un objectif de discrimination. Cet article met donc l\u0027accent sur la façon de construction la matrice d\u0027adjacence induite par une mesure de proximité, tout en tenant compte des classes d\u0027appartenance des objets, connues a priori, en juxtaposant des matrices d\u0027adjacence intra-groupe et inter-groupes Abdesselam (2014). Un critère de sélection de la \"meilleure\" mesure est proposé. On vérifie en effet a posteriori qu\u0027elle est bien une bonne mesure discriminante en utilisant la méthode des SVM multi-classes.\nTAB. 1 -Quelques mesures de proximité.\nOù, p désigne la dimension de l\u0027espace, Cet article est organisé comme suit. Dans la section 2, après avoir rappelé les notions de structure, de graphe et d\u0027équivalence topologique, nous présentons la façon dont a été construite la matrice d\u0027adjacence dans un but de discrimination, le choix de la mesure du degré d\u0027équivalence topologique entre deux mesures de proximité ainsi que le critère de sélection de la \"meilleure\" mesure discriminante. Un exemple illustratif est commenté en section 3. Une conclusion et quelques perspectives de cette approche sont données en section 4.\nLe tableau 1 présente quelques mesures de proximité classiques utilisées pour des données continues, définies sur R p .\nEquivalence topologique\nL\u0027équivalence topologique repose en fait sur la notion de graphe topologique que l\u0027on dé-signe également sous le nom de graphe de voisinage. L\u0027idée de base est en fait assez simple : deux mesures de proximité sont équivalentes si les graphes topologiques correspondants induits sur l\u0027ensemble des objets restent identiques. Mesurer la ressemblance entre mesures de proximité revient à comparer les graphes de voisinage et à mesurer leur ressemblance. Nous allons tout d\u0027abord définir de manière plus précise ce qu\u0027est un graphe topologique et comment le construire. Nous proposons ensuite une mesure de proximité entre graphes topologiques qui servira par la suite à comparer les mesures de proximité.\nGraphe topologique\nSur un ensemble de points x, y, z, . . . de R p , on peut, au moyen d\u0027une mesure de proximité u définir une relation de voisinage V u qui sera une relation binaire sur E × E. Pour simplifier la compréhension mais sans nuire à la généralité du propos, considérons un ensemble d\u0027objets E \u003d {x, y, z, . . .} de n \u003d |E| objets plongés dans R p . Ainsi, pour une mesure de proximité u donnée, nous pouvons construire un graphe de voisinage sur un ensemble d\u0027individus dont les sommets sont les individus et les arêtes sont définis par une propriété de la relation de voisinage.\nDe nombreuses définitions sont possibles pour construire cette relation binaire de voisinage. On peut, par exemple, choisir l\u0027Arbre de Longueur Minimale (ALM) Kim et Lee (2003), le Graphe de Gabriel (GG) Park et al. (2006), ou encore le Graphe des Voisins Relatifs (GVR) Toussaint (1980), dont tous les couples de points voisins vérifient la propriété suivante :\n) V u (x, y) \u003d 0 sinon c\u0027est-à-dire, si les couples de points vérifient ou pas l\u0027inégalité ultratriangulaire (1), condition ultramétrique.\nLa figure 1 montre, dans R\n, un exemple de graphe topologique GVR parfaitement défini par la matrice d\u0027adjacence V u associée, formée de 0 et de 1. Sur le plan géométrique, cela signifie que l\u0027hyper-Lunule (intersection des deux hypersphères centrées sur les deux points) est vide.\nComparaison de mesures de proximité\nOn dispose de p variables quantitatives explicatives (prédicteurs) {x j ; j \u003d 1, p} et d\u0027une variable qualitative cible à expliquer y, partition de n \u003d ? q k\u003d1 n k individus-objets en q modalités-groupes {G k ; k \u003d 1, q}.\nPour toute mesure de proximité u i donnée, on construit, selon la propriété (1), la matrice d\u0027adjacence binaire globale V ui qui se présente comme une juxtaposition de q matrices d\u0027adjacence symétriques Intra-groupe {V\nA noter que la matrice d\u0027adjacence partitionnée globale V u i ainsi construite, n\u0027est pas symétrique. En effet, pour deux objets\n• Le premier objectif est de regrouper les différentes mesures de proximité considérées, selon leur similitude topologique pour mieux visualiser leur ressemblance dans un contexte de discrimination.\nPour mesurer le degré d\u0027équivalence topologique de discrimination entre deux mesures de proximité u i et u j , nous proposons de tester si les matrices d\u0027adjacence associées V u i et V u j sont différentes ou pas. Le degré d\u0027équivalence topologique entre deux mesures de proximité est mesuré par la quantité :\n0 sinon.\n• Le second objectif consiste à établir un critère d\u0027aide à la sélection de la \"meilleure\" mesure de proximité ; celle qui parmi toutes les mesures considérées, discrimine au mieux les q groupes.\nOn note, V u * \u003d diag (1 G 1 , . . . , 1 G k , . . . , 1 G q ) la matrice d\u0027adjacence symétrique blocdiagonale de référence \"discrimination parfaite\" associée à la mesure de proximité inconnue, notée u * , où, 1 n k désigne le vecteur d\u0027ordre n k dont toutes les composantes sont égales à 1 et 1 G k \u003d 1 n k t 1 n k , la matrice carrée d\u0027ordre n k dont tous les éléments sont égaux à 1.\nOn peut ainsi établir le degré d\u0027équivalence topologique de discrimination S(V u i , V u * ) entre chaque mesure de proximité u i considérée et la mesure de référence u * . Enfin, afin d\u0027évaluer autrement le choix de la \"meilleure\" mesure de proximité discriminante proposée par cette approche, nous avons appliqué a posteriori une technique de classement par SVM Multiclasses (MSVM) sur la matrice d\u0027adjacence associée à chacune des mesures de proximité considérée y compris à la mesure de référence u * .\nExemple d\u0027application\nPour illustrer notre approche, nous considérons ici un jeu de données bien connu et relativement simple, celui des Iris Fisher (1936); Anderson (1935). Ces données ont été proposées comme données de référence pour l\u0027analyse discriminante et la classification par le statisticien Ronald Aylmer Fisher en 1933. Les données complètes se trouvent notamment dans UCI (2013). Quatre variables (longueur et largeur des sépales et pétales) ont été observées sur 50 fleurs de chacune des 3 espèces d\u0027Iris (Iris Setosa, Iris Virginica, Iris Versicolor).\nComparaison et classement des mesures de proximité\nLes principaux résultats de l\u0027approche proposée sont présentés dans les tableaux et le graphique suivants. Ils permettent de visualiser les mesures qui sont proches les unes des autres selon l\u0027objectif de discrimination.\nPour ce jeu de données, le tableau 2 récapitule les similarités entre les 8 mesures de proximité et montre que la mesure u T ch de Tchebychev est la plus proche de la mesure u * de référence.\nUne Analyse en Composantes Principales (ACP) suivie d\u0027une Classification Hiérarchique Ascendante (CHA) ont été effectuées à partir de la matrice de similarités entre les 8 mesures de proximité considérées, afin de les partitionner dans des groupes homogènes et de visualiser leurs ressemblances.\nL\u0027application d\u0027un algorithme de construction d\u0027une CHA selon le critère de Ward, Ward Jr (1963), permet d\u0027obtenir le dendogramme de la figure 2. Le vecteur similarités S(V ui , V u * ) de \nTAB. 3 -Classement de la mesure de référence.\nAu vu des résultats présentés dans le tableau 3 de la partition en 5 classes de mesures de proximité, la mesure de référence inconnue u * , projetée en élément supplémentaire, serait donc plus proche des mesures de la classe 3, c\u0027est-à-dire, de la mesure de Tchebychev u T ch qui serait pour ces données, la \"meilleure\" mesure de proximité parmi les 8 mesures considé-rées. Ce résultat confirme celui constaté dans le tableau 2, à savoir, une plus grande similarité S(V u T ch , V u * ) \u003d 68.10% de la mesure de Tchebychev avec celle de référence u * .\nLes mesures discriminantes selon les MSVM\nCette partie consiste à valider les résultats du choix de la meilleure mesure au vu de la matrice de référence a posteriori en utilisant les MSVM. Nous utilisons ici le modèle M SV M LLW , Lee et al. (2004), considéré comme le plus fondé théoriquement du fait que sa solution donne un classifieur qui converge vers celui de Bayes.\nMesure\nErreur Pour ce faire, nous allons appliquer une des techniques de sélection du modèle consistant à tester plusieurs valeurs du paramètre et à choisir celle qui minimise l\u0027erreur test calculée par validation croisée. Dans cet exemple, nous testons 10 valeurs du paramètres (entre 1 et 100) pour toutes les bases de données. Aprés simulations, la valeur choisie est C \u003d 1.\nLes principaux résultats du modèle M SV M LLW , appliqué sur chacune des matrices d\u0027adjacence induites par les mesures de proximité considérées, sont présentés dans le tableau 4.\nLe meilleur taux d\u0027erreur est celui donné par les mesures de Tchebechev u T ch et Euclidienne u E , qui est aussi celui enregistré pour la matrice de référence.\nL\u0027application du modèle MSVM montre que les mesures de proximité de Tchebechev u T ch et Euclidienne u E sont les plus adaptées pour différencier et séparer au mieux les 3 espèces d\u0027Iris. Ce résultat confirme celui obtenu précédemment, à savoir le choix de la mesure de Tchebychev u T ch comme la mesure plus proche, parmi les huit mesures considérées, de la mesure de référence et donc la plus discriminante.\nExpérimentations\nNous avons procédé à des expérimentations sur d\u0027autres jeux de données afin d\u0027essayer d\u0027évaluer l\u0027effet des données, de leur taille et/ou de leur dimension sur les résultats de la classification des mesures de proximité toujours dans un but de discrimination. Est-ce que, par exemple, les mesures de proximité se regroupent différemment selon le jeu de données utilisé ? Selon la taille de l\u0027échantillon et/ou le nombre de variables explicatives considérées dans un même ensemble de données ?\nPour répondre à ces questions, nous avons donc appliqué l\u0027approche proposée sur différents jeux de données, présentés dans le tableau 5, qui proviennent tous du référentiel UCI (2013). L\u0027objectif est de comparer les résultats des classifications des mesures de proximité de toutes ces expérimentations ainsi que la \"meilleure\" mesure discriminante proposée pour chacun de ces jeux de données.\nEtant donné un ensemble de données explicatives X (n,p) à n objets et p variables, et une variable à expliquer ou à discriminer Y q à q modalités-classes.\nPour analyser l\u0027effet du changement de dimension, nous avons considéré le jeu de données \"Waveform Database Generator\" pour générer 3 échantillons n?4 de taille n \u003d 2000 objets et de dimension p égale respectivement à 40, 20 et à 10 variables.\nDe même, pour évaluer l\u0027effet du changement de la taille de l\u0027échantillon, nous avons éga-lement généré 3 autres échantillons n?5 de taille n égale respectivement à 3000, 1500 et à 500 objets et de même dimension p égale à 30 variables.\nLes principaux résultats de ces expérimentations, à savoir les équivalences topologiques des mesures de proximité discriminantes et l\u0027affectation de la mesure de référence u * dans la classe la plus proche, sont présentés dans le tableau 6.\nPour chacune de ces expérimentations, nous avons retenu une partition en cinq classes de mesures de proximité afin de les comparer et de bien distinguer les mesures de la classe d\u0027appartenance de la mesure de référence, c\u0027est-à-dire les mesures les plus discriminantes.\nLes regroupements des mesures de proximité obtenus pour les trois jeux de données n?4 sont pratiquement identiques, il n\u0027y a donc pas vraiment d\u0027effet de la dimension. \nune nouvelle approche d\u0027équivalence entre mesures de proximité dans un contexte de discrimination. Cette approche topologique est basée sur la notion de graphe de voisinage induit par la mesure de proximité. D\u0027un point de vue pratique, dans ce papier, les mesures que nous avons comparées sont toutes construites sur des données quantitatives. Mais ce travail peut parfaitement s\u0027étendre aux données qualitatives en choisissant la bonne structure topologique adaptée.\nNous envisageons d\u0027étendre ce travail à d\u0027autres structures topologiques et d\u0027utiliser un critère de comparaison, autre que les techniques de classification, afin de valider le degré d\u0027équivalence entre deux mesures de proximité. Par exemple, évaluer le degré d\u0027équivalence topologique de discrimination entre deux mesures de proximité en appliquant le test non paramètrique du coefficient de concordance de Kappa, calculé à partir des matrices d\u0027adjacence associées, Abdesselam et Zighed (2011). Cela va permettre de donner une signification statistique du degré de concordance entre les deux matrices de ressemblance et de valider ou pas l\u0027équivalence topologique de discrimination, c\u0027est-à-dire si vraiment elles induisent ou pas la même structure de voisinage sur les groupes d\u0027objets à séparer.\nEnfin, les expérimentations menées sur différents jeux de données ont montré qu\u0027il n\u0027y pas du tout d\u0027effet de la dimension et pas vraiment d\u0027effet de la taille de l\u0027échantillon aussi bien sur les regroupement des mesures de proximité que sur le résultat du choix de la meilleure mesure discriminante.\nproximity measures in a topological structure and a goal of discrimination. The concept of topological equivalence uses the structure of local neighborhood.\nThen we propose to define the topological equivalence between two proximity measures, in the context of discrimination, through the topological structure induced by each measure. We also propose a criterion for choosing the \"best\" measure adapted to data considered among some of the most used proximity measures for quantitative data. The choice of the \"best\" discriminating proximity measure can be verified retrospectively by a supervised learning method type SVM, discriminant analysis or Logistic regression applied in a topological context.\nThe principle of the proposed approach is illustrated using a real quantitative data example with eight conventional proximity measures of literature. Experiments have evaluated the performance of this discriminant topological approach in terms of size and/or dimension of the relevant data and of selecting the \"best\" discriminant proximity measure.\n"
  },
  {
    "id": "240",
    "text": "Introduction\nCe papier propose un nouveau mécanisme d\u0027optimisation pour l\u0027algorithme de classification automatique évidentielle semi-supervisée SECM , qui est le premier à reposer sur des contraintes exprimées sous la forme de données étiquetées. Les algorithmes de classification évidentielle (Masson et Denoeux, 2008) reposent sur le cadre théorique des fonctions de croyance et permettent de représenter tous les types d\u0027affectations partielles grâce au concept de partition crédale qui étend la notion de partition stricte, floue et possibiliste. Ces méthodes évidentielles ont été étendues dans le cadre semi-supervisé (Antoine et al., 2012 pour pouvoir tirer partie de contraintes de type Must-Link (ML) et Cannot-Link (CL) qui spécifient si deux données doivent ou non appartenir à la même classe. La transformation des informations disponibles a priori en ce type de contraintes peut néanmoins induire une perte de connaissance. L\u0027algorithme SECM a été proposé récemment pour tirer partie de données partiellement étiquetées . Cependant, l\u0027algorithme SECM initial repose sur une optimisation stricte qui respecte l\u0027ensemble des contraintes et notamment la positivité des masses de croyances associées à l\u0027affectation d\u0027un point à une classe. Cette contrainte théorique entraîne la formation d\u0027un problème complexe. Nous proposons donc de modifier le mécanisme d\u0027optimisation en relâchant la contrainte de positivité, à l\u0027instar de ce qui est fait dans (Bouchachia et Pedrycz, 2006), et en s\u0027assurant a posteriori de l\u0027optimisation que les masses de croyances sont positives. Nos résultats expérimentaux montrent que notre heuristique ne dégrade pas les performances de l\u0027algorithme SECM et permet de gagner de manière significative en complexité sur nos jeux de tests.\nCe papier est organisé comme suit : la section 2 présente les concepts fondamentaux de la théorie des fonctions de croyance et les principales méthodes de classification automatique sous contraintes. L\u0027algorithme semi-supervisé SECM  est ensuite décrit dans la section 3 et un nouveau schéma d\u0027optimisation est proposé. Enfin, les résultats de l\u0027algorithme sont présentés dans la section 4. Le papier conclut sur l\u0027intérêt de la nouvelle méthode d\u0027optimisation.\n2 Travaux existants 2.1 Les fonctions de croyance L\u0027intérêt principal d\u0027un algorithme de classification évidentielle est de pouvoir représenter le doute concernant l\u0027affectation d\u0027un point à un cluster. Pour ce faire, ces méthodes reposent sur la théorie de l\u0027évidence de Dempster-Shafer, également appelée théorie des fonctions de croyance (Shafer, 1976;Smets et Kennes, 1994). Soit ? une variable prenant ses valeurs dans un ensemble fini ? \u003d {? 1 , . . . , ? c } appelé cadre de discernement. La connaissance partielle concernant la valeur de ? peut être représentée par une fonction de masses m, qui est une application de l\u0027ensemble des parties de\nLes sous-ensembles A ? ? tels que m(A) \u003e 0 sont appelés les éléments focaux de m. La quantité m(A) s\u0027interprète comme la quantité de croyance allouée à A et qui, faute d\u0027information complémentaire, ne peut être allouée à aucun autre sous-ensemble de A. L\u0027ignorance totale correspond à m(?) \u003d 1 alors qu\u0027une certitude totale se rapporte à l\u0027allocation complète de la masse de croyance sur un unique singleton de ?. Si tous les ensembles focaux de m sont des singletons, alors la fonction de masses de croyances est équivalente à une distribution de probabilités. La quantité m(?) peut être interprétée comme la croyance que la valeur réelle de ? n\u0027appartient pas à ?. Quand m(?) \u003d 0, la fonction de croyance est dite normalisée. La connaissance exprimée par une fonction de croyance peut aussi être représentée par une fonction de plausibilité pl : 2 ? ? [0, 1] définie comme suit :\nB?A? \u003d? La quantité pl(A) est interprétée comme le degré maximal de croyance qui peut potentiellement être affecté à l\u0027hypothèse selon laquelle la vraie valeur de ? appartient à A. Quand une décision doit être prise concernant la valeur de ?, il est intéressant de transformer une fonction de masses en probabilité pignistique (Smets et Kennes, 1994) :\n??A où |A| dénote la cardinalité de A ? ?. Quand il existe m(?) ? \u003d 0, une étape de normalisation doit précéder la transformation pignistique. La normalisation de Dempster, qui consiste à diviser toutes les masses par 1 ? m(?), est une méthode classique de normalisation.\nAlgorithme des c-moyennes évidentielles\nLa version évidentielle des k-moyennes, ECM, est un algorithme de classification automatique qui construit une partition crédale à partir des données. Dans ce formalisme, la connaissance partielle concernant l\u0027appartenance d\u0027un objet x i est représentée par une fonction de croyance m i sur l\u0027ensemble ? des classes possibles. Ainsi, un degré de croyance peut être affecté aux singletons (comme dans les approches floues et possibilistes) mais également à tous les sous-ensembles de ?. Soit {x 1 , . . . x n } un ensemble d\u0027individus dans R p à classer dans un ensemble ? \u003d {? 1 , . . . ? c } de c classes. Pour chaque objet x i , la fonction de croyance m i est calculée en plaçant une grande (resp. petite) quantité de croyance sur le sous-ensemble proche (resp. éloigné) en terme de distance de x i . La distance d ij est une métrique définie entre un objet x i et une représentation dans R p d\u0027un sous-ensemble A j ? ?. Similairement à l\u0027algorithme des c-moyennes floues, chaque classe ? k est représentée par un prototype v k . Pour chaque sous-ensemble A j ? ?, A j ? \u003d ?, un centre v j est calculé comme le barycentre des centres associés aux classes composant A j :\nLa distance d 2 ij peut être définie comme une distance euclidienne (Masson et Denoeux, 2008). Plus récemment, une variante a été proposée pour prendre en compte une distance de Mahalanobis (Antoine et al., 2012). Similairement aux travaux de (Gustafson et Kessel, 1979), cette distance permet de détecter des clusters ayant différentes formes géométriques, grâce à une matrice de covariance floue S k associée à chaque cluster ? k et qui doit être optimisée.\nEnsuite, similairement à ce qui est fait pour les prototypes, pour chaque sous-ensemble de A j qui n\u0027est pas un singleton, une matrice S j est calculée en moyennant les matrices incluses dans\nL\u0027algorithme ECM minimise la fonction objectif suivante en fonction des matrices M, V et S précédentes :\nComme m i? correspond à la croyance que x i est un point aberrant, son cas est traité séparément du reste des autres sous-ensembles. Le paramètre ? indique la distance de l\u0027ensemble des objets à l\u0027ensemble vide. Il est intéressant de remarquer à ce niveau qu\u0027une pénalité des sous-ensembles A j ? ? avec une grande cardinalité a été introduite avec la pondération |A j | ? . L\u0027exposant ? permet de contrôler le degré de cette pénalisation.\nTout comme pour les c-moyennes floues, la partition est construite selon un processus itératif qui optimise alternativement les matrices M, V et S. La complexité d\u0027un algorithme évidentiel est linéaire avec le nombre de données mais exponentiel avec le nombre de classes. En conséquence, il est crucial pour ce type de méthodes de minimiser les calculs réalisés dans les phases d\u0027optimisation comme cela est proposé dans cet article.\nAlgorithmes semi-supervisés\nLa plupart des méthodes de classification automatique ont été améliorées pour prendre en compte la connaissance experte sous la forme de contraintes soit entre paires de données de type Must-Link (ML) ou Cannot-Link (CL) qui indiquent si deux points doivent ou non appartenir au même cluster, soit sous la forme de données étiquetées (Wagstaff et al., 2001). Citons par exemple des algorithmes de type k-moyennes (Wagstaff et al., 2001;Basu et al., 2002), hiérarchiques (Davidson et Ravi, 2005), basés sur la densité (Ruiz et al., 2010;Lelis et Sander, 2009), des méthodes spectrales (Wang et Davidson, 2010) ainsi que des algorithmes dédiés aux flux de données (Ruiz et al., 2009). D\u0027autres travaux se sont intéressés à l\u0027intégration de contraintes dans l\u0027algorithme des c-moyennes floues (Grira et al., 2006;Pedrycz, 1985;Bensaid et al., 1996;Pedrycz et Waletzky, 1997a). Pour palier les limitations des algorithmes flous en présence de bruit ou de points aberrants, des méthodes possibilistes (Krishnapuram et Keller, 1993;Sen et Davé, 1998) et plus récemment évidentielles ont été proposées (Masson et Denoeux, 2008. Ces dernières ont également été étendues au cas semi-supervisé pour bénéficier des avantages des modèles basés sur les fonctions de croyance dans la prise en compte de la connaissance experte. Les travaux proposés reposent soit sur des contraintes ML et CL (Antoine et al., 2012 soit, plus récemment, sur des données partiellement étiquetées avec l\u0027algorithme SECM . D\u0027un point de vue formel, deux approches ont été proposées dans la littérature pour prendre en compte les contraintes et les étiquettes pendant le processus de classification automatique. En premier lieu, il est possible de modifier le processus des algorithmes de classification, soit durant la phase d\u0027initialisation (Basu et al., 2002), soit pendant la phase de convergence. Dans ce dernier cas, on peut soit imposer un respect strict des contraintes comme dans COP Kmeans (Wagstaff et al., 2001), soit modifier la fonction objectif pour pénaliser les solutions qui ne respectent pas complètement les contraintes (Pedrycz et Waletzky, 1997b). Par exemple, dans (Bouchachia et Pedrycz, 2003), les auteurs décrivent un FCM amélioré dont la fonction objectif introduit un terme de pénalité qui considère à la fois l\u0027appartenance actuelle des points i aux classes k notée u ik , mais également l\u0027appartenance telle qu\u0027elle devrait être à partir des contraintes de l\u0027expert notée˜unotée˜ notée˜u ik comme le montre l\u0027équation (7).\nTAB. 2 -Plausibilités calculées à partir de la partition crédale.\noù U et V dénotent respectivement la matrice d\u0027appartenance et les coordonnées des centres des clusters. ? est un paramètre de régulation qui permet d\u0027équilibrer l\u0027importance du respect des contraintes dans la fonction objectif.\nEn second lieu, d\u0027autres méthodes proposent d\u0027adapter la métrique en fonction des contraintes et étiquettes fournies par l\u0027expert comme dans l\u0027algorithme MPC k-means (Bilenko et al., 2004). Par exemple, dans (Bouchachia et Pedrycz, 2006), les auteurs proposent une mé-thode pour adapter une distance de Gustafson-Kessel (Gustafson et Kessel, 1979). Nous proposons dans ce papier de considérer un modèle de contraintes flexibles avec une modification de la fonction objectif qui pénalise les solutions ne respectant pas les données étiquetées.\n3 Algorithme SECM 3.1 Formalisation du problème L\u0027idée principale de l\u0027algorithme proposé dans  est d\u0027ajouter un terme de pénalité dans la fonction objectif de ECM afin de prendre en compte un ensemble d\u0027objets étiquetés. La démarche suivie est la même que dans (Bouchachia et Pedrycz, 2003) mais rapportée aux algorithmes évidentiels. L\u0027expression d\u0027un objet étiqueté peut se traduire sous la forme d\u0027une fonction quantifiant la croyance sur l\u0027appartenance de l\u0027objet à une classe. Considérons dans un premier temps une partition crédale connue et définie par le tableau 1. Elle représente la connaissance partielle de l\u0027appartenance de quatre objets à deux classes. Il est alors possible de calculer la plausibilité de chaque objet x i pour chaque classe ? k , comme illustré par le tableau 2. On remarque alors qu\u0027une plausibilité nulle permet de déduire avec certitude qu\u0027un élément n\u0027appartient pas à une classe. Ainsi, l\u0027observation de pl i (? 1 ) \u003d 0 permet de déduire que x 1 , un objet atypique, et x 4 , un objet affecté avec certitude dans la classe ? 2 , ne font pas partie de la classe ? 1 . En revanche, les objets dont la plausibilité pour une classe est élevée ont des chances d\u0027appartenir à cette classe. Ainsi, pl i (? 1 ) \u003d 1 apparaît pour l\u0027objet x 2 , qui appartient à la classe ? 2 avec certitude, et pour l\u0027objet x 3 , qui appartient soit à ? 1 , soit à ? 2 .\nSupposons maintenant que l\u0027on ne dispose pas de la partition crédale, mais qu\u0027il existe des contraintes sous formes d\u0027étiquettes. Par exemple, l\u0027objet x i est inclus dans la classe ? k . Il est alors possible d\u0027imposer la contrainte pl i (? k ) \u003d 1. L\u0027effet sera d\u0027exiger :\n-une croyance élevée pour les fonctions de masse ayant un sous-ensemble comprenant ? k , donc toutes les fonctions de masses qui ont un degré de croyance plus ou moins fort sur le fait que x i appartienne à ? k , -des valeurs faibles pour toutes les fonctions de masses ayant un sous-ensemble qui n\u0027incluent pas ? k . La contrainte entre un objet x i et la classe ? k est donc respectée pour de nombreuses solutions allant de la certitude totale que x i appartienne à ? k jusqu\u0027à l\u0027incertitude complète de l\u0027affectation de x i entre ? k ou plusieurs autres classes de ?. La contrainte est donc flexible car elle permet si nécessaire de garder un doute quant à l\u0027affectation de l\u0027objet à la classe. Par conséquent, cela limite l\u0027influence négative d\u0027une contrainte bruitée.\nLorsqu\u0027un expert crée des contraintes d\u0027étiquettes, il peut avoir un doute entre plusieurs classes pour un unique objet. Par exemple, l\u0027objet x i appartient à une des classes du sousensemble A j ? ?. Cette information se modélise alors sous la forme d\u0027une contrainte sur la plausibilité de A j : pl i (A j ) \u003d 1. Cela revient à favoriser les fonctions de masses ayant au moins une classe dans A j . Cette contrainte, qui généralise la précédente, permet d\u0027établir un terme de pénalité à ajouter à la fonction objectif de ECM :\nLa nouvelle fonction objectif est alors la suivante :\nsous les contraintes (5)  \nOptimisation\nL\u0027optimisation du nouveau critère consiste, de la même manière que pour l\u0027algorithme ECM, à minimiser alternativement les matrices M, V et S. Le terme de pénalité J S ne dé-pendant ni de V, ni de S, leur mise à jour est similaire à ECM, et leur formule est présen-tée dans (Masson et Denoeux, 2008). La partition crédale M est au contraire présente dans J S . En fixant ? à 2 alors la minimisation de la fonction objectif par rapport à M devient un problème quadratique à contraintes linéaires. Ce problème peut être résolu par une méthode classique d\u0027optimisation (Ye et Tse, 1989), néanmoins de nombreux auteurs se trouvant dans un contexte similaire proposent d\u0027optimiser directement la fonction objectif sans prendre en compte les contraintes de positivité sur la partition (6), afin de réduire le temps de convergence de l\u0027algorithme.\nAfin de résoudre le problème de minimisation contraint par (5), des multiplicateurs de Lagrange ? 1 , . . . ? n sont introduits et le Lagrangien défini : \nA j ??,A j ? \u003d? Annuler les dérivées partielles permet d\u0027obtenir les équations suivantes :\nAj ??,Aj ? \u003d? En utilisant (15) et (16) dans (17), il est possible d\u0027écrire :\nCette équation peut finalement être utilisée dans (15) et (16) pour obtenir la mise à jour des fonctions de masse, ?i \u003d 1, n et ?j/A j ? ?, A j ? \u003d ? : \nA j ??,A j ? \u003d? Comme le numérateur de la première partie de l\u0027équation (19) permet l\u0027obtention de valeurs négatives, la fonction de masse m ij peut être négative. Ces valeurs négatives sont accentuées par l\u0027importance donnée aux contraintes. Si l\u0027utilisateur reste dans un cas normal d\u0027utilisation des contraintes, c\u0027est-à-dire ? ? 0.8 (cf. partie 4.2), les valeurs de m ij \u003c 0 seront proches de 0. Une fonction de réajustement est donc envisageable sans que l\u0027optimisation soit dégradée :\navec \nExpérimentations\nLes expériences menées sur plusieurs jeux de données consistent à comparer les résultats obtenus par SECM lorsque la mise à jour des fonctions de masse utilise l\u0027optimisation de (Ye et Tse, 1989), noté SECM-classic, avec SECM et l\u0027optimisation proposée, noté SECM-do.\nDonnées et méthode d\u0027évaluation\nJeux de données : Plusieurs jeux de données issue de l\u0027UCI Machine Learning Repository ont été employés. Le tableau 3 indique leurs caractéristiques ainsi que la métrique utilisée pour les expériences. Il faut noter que LettersIJL correspond au jeu de données Letters modifié comme (Bilenko et al., 2004  Protocole expérimental : Une expérience consiste, pour un certain pourcentage de contraintes, à exécuter 25 fois l\u0027algorithme SECM avec 25 jeux de contraintes différents. Afin d\u0027éviter les optima locaux, chaque exécution teste cinq initialisations aléatoires des centres de gravité et récupère les résultats obtenus par l\u0027initialisation ayant la fonction objectif minimale.\nRésultats\nJeux de données réelles : La figure 1 montre l\u0027évolution de l\u0027indice de Rand moyen obtenu avec SECM-classic et SECM-do par rapport au pourcentage de contraintes pour Iris et Wine. Des résultats similaires ont été trouvés avec Ionosphere et LettersIJL. Le coefficient ? est fixé à 0.5. Il est ainsi possible de remarquer (1) que l\u0027ajout progressif de contraintes améliore l\u0027indice de Rand et (2) que l\u0027algorithme SECM-do présente de meilleurs résultats que SECM-classic. Pour ces expériences, nous avons également constaté que les valeurs des fonctions objectif de SECM-do sont plus petites que celles de SECM-classic. Des résultats similaires ont été trouvés pour ? \u003d 0.3 et ? \u003d 0.8. La nouvelle optimisation permet donc d\u0027obtenir un meilleur minimum grâce à sa relaxation des contraintes, ce qui implique de meilleurs résultats de classification. Il faut également noter que pour ? \u003d 0.8, les résultats obtenus prouvent que la fonction de réajustement de SECM-do ne dégrade en rien les solutions.\nPour ces mêmes expériences, le temps CPU a été observé afin de comparer la vitesse d\u0027exé-cution des deux algorithmes. Le tableau 4 présente les résultats obtenus. Il est ainsi aisé de voir que l\u0027algorithme SECM-do est plus rapide que l\u0027algorithme SECM-classic. à une forte valeur. De plus, nous avons choisi de réduire l\u0027incertitude trouvée par la partition finale en fixant ? à une valeur élevée. Ainsi, ECM avec c \u003d 2, ? \u003d 3 et ? 2 \u003d 1000 trouve la partition crédale dure représentée par la figure 2(b). Nous pouvons remarquer que ECM ne permet pas d\u0027isoler correctement l\u0027avion. Dans une seconde expérience, nous introduisons des contraintes sur la partition comme illustré Figure 2(c). Chaque pixel de la première (respectivement seconde) zone est affectée à ? 1 (respectivement ? 2 ). L\u0027algorithme SECM est alors exécuté avec les mêmes paramètres que ECM. La partition crédale résultante est présentée Figure 2(d). Nous pouvons constater que les contraintes ont permis de lever l\u0027indétermination de la plupart des pixels alloués à ?.\nConclusion\nNous avons présenté dans cet article une nouvelle méthode d\u0027optimisation pour l\u0027algorithme de classification automatique intitulé SECM. Ce dernier est une variante de l\u0027algorithme évidentiel ECM prenant en compte des contraintes d\u0027étiquettes. Il repose sur la minimisation d\u0027une fonction objectif avec des contraintes linéaires et non linéaires, ce qui impose l\u0027utilisation de méthodes d\u0027optimisation avancées. De plus, l\u0027utilisation des fonctions de masses liées aux méthodes évidentielles rend la complexité de l\u0027algorithme linéaire par rapport au nombre d\u0027objets et exponentielle par rapport aux nombre de classes. Nous proposons donc de relâ-cher les contraintes de positivité sur les fonctions de masses, c\u0027est-à-dire sur les contraintes non linéaires, afin de réduire l\u0027optimisation de la partition à la méthode des multiplicateurs de Lagrange. Le respect des contraintes de positivité est ensuite vérifié par une méthode de réajustement. Nous avons montré sur un ensemble de jeux de données que cette nouvelle technique permet non seulement d\u0027augmenter la rapidité de SECM mais qu\u0027elle permet également d\u0027améliorer les performances en trouvant de meilleurs minima. Les travaux futurs porteront sur l\u0027étude de nouveaux formalismes plus rapides permettant de conserver la majeure partie de l\u0027expressivité des méthodes évidentielles avec une complexité largement réduite pour permettre de traiter des jeux de données avec un nombre de classes plus important.\n"
  },
  {
    "id": "241",
    "text": "Introduction\nLes systèmes de recommandation basés sur le contenu suivent généralement un processus en deux étapes : (i) Création d\u0027une représentation du besoin des utilisateurs ainsi que des informations à recommander. (ii) Comparaison des représentations afin d\u0027évaluer la pertinence d\u0027une information pour un utilisateur en fonction de son profil. Notre approche consiste à automatiser l\u0027indexation à l\u0027aide de processus d\u0027inférence sur une ontologie d\u0027indexation intégrant les vocabulaires contrôlés (e.g. thésaurus, nomenclatures, listes) définis par les documentalistes pour modéliser le domaine. Le respect de la vision métier sur le domaine permet une supervision simplifiée pour les documentalistes, garantissant la qualité de l\u0027indexation.\nAutomatisation du processus d\u0027indexation\nLa classification multi-label consiste à associer des étiquettes à des items (Tsoumakas et Katakis, 2007). Cet article propose une méthode pour enrichir sémantiquement une ontologie en adoptant des processus d\u0027apprentissage automatique pour indexer et décrire l\u0027indexation de façon à réduire l\u0027écart entre le point de vue des experts et les règles d\u0027indexation. L\u0027approche proposée repose sur les quatre phases suivantes :\nPhase 1 : utilisation du travail d\u0027indexation déjà fait par les documentalistes et d\u0027un processus d\u0027analyse de texte pour extraire des mots-clés afin de générer une matrice qui présente la fréquence de chaque mot-clé en fonction de chaque étiquette.\nPhase 2 : utilisation de la matrice afin de définir des règles capables de déterminer si un document doit être associé à une étiquette sur la base des mots-clés qu\u0027il contient. Deux seuils de fréquence sont définis, ? et ?. Les mots-clés dont la fréquence est supérieure au seuil ? sont considérés comme des indices fiables. La présence d\u0027un seul de ces mots est considérée comme suffisante pour que le document soit associé à l\u0027étiquette. Le seuil de fréquence infé-rieur est ?. Dans ce cas, nous avons besoin d\u0027une combinaison de ?-termes (dont la fréquence est supérieure à ?) pour prendre la décision d\u0027associer un document avec l\u0027étiquette. Plus d\u0027informations sur les règles d\u0027indexation peuvent être trouvées dans (Werner et al., 2014).\nPhase 3 : la classification fournit deux types de résultats. Le premier est la découverte de la classe de subsomption la plus spécifique. Le second est la déduction des classes d\u0027équiva-lence lorsque les contraintes logiques sont équivalentes. D\u0027une part, cela signifie que lorsqu\u0027un document est étiqueté (lors de la phase 4) par une classe qui possède des subsumants, ce document est également marqué par les classes subsumantes. D\u0027autre part, lorsqu\u0027un document est étiqueté avec une classe qui a des classes d\u0027équivalence alors ce document est également étiqueté avec ces classes équivalentes. Ces deux éléments peuvent permettre la classification multi-label. De plus, sachant que les étiquettes peuvent être organisées de façon hiérarchique il peut s\u0027agir d\u0027un processus de classification hiérarchique multi-label (HMC).\nPhase 4 : la phase de réalisation consiste à trouver toutes les classes les plus spécifiques des individus. Cette phase est mise en oeuvre par le moteur d\u0027inférence. Les phases 3 et 4 utilisent des raisonneurs comme FaCT ++, HermiT ou Pellet.\n"
  },
  {
    "id": "242",
    "text": "Introduction\nUn flux de données est une séquence, potentiellement infinie, non-stationnaire (la distribution de probabilité des données peut changer au fil du temps) de données arrivant en continu. Dans le cas d\u0027un flux, l\u0027accès aléatoire aux données n\u0027est pas possible et le stockage de toutes les données arrivant est infaisable. Le clustering de flux de données nécessite un processus capable de partitionner des observations de façon continue avec des restrictions au niveau de la mémoire et du temps. Dans la littérature, de nombreux algorithmes de clustering de flux de données ont été adaptés à partir des algorithmes de clustering traditionnel, par exemple, la méthode DbScan (Cao et al. (2006); Isaksson et al. (2012)) basée sur la densité, la méthode de partitionnement k-means (Ackermann et al. (2012)), ou encore la méthode basée sur le passage de message AP (Affinity Propagation) (Zhang et al. (2008)). Dans cet article, nous proposons le modèle G-Stream, qui permet de découvrir des clusters de formes arbitraires dans un flux de données en constante évolution. Les caractéristiques et les principaux avantages de G-Stream sont décrits ci-dessous : (a) La structure topologique qui est représentée par un graphe dans lequel chaque noeud représente un cluster. Les noeuds (clusters) voisins sont reliés par des arêtes. La taille du graphe est évolutive. (b) L\u0027utilisation d\u0027une fonction d\u0027oubli afin de réduire l\u0027impact des anciennes données dont la pertinence diminue au fil du temps. Les liens entre les noeuds sont également pondérés. (c) Contrairement à de nombreux algorithmes qui utilisent un nombre important de données pour initialiser leur modèle, G-Stream utilise seulement deux noeuds au départ. (d) Toutes les fonctions de G-Stream sont effectuées en-ligne. (e) L\u0027utilisation de la notion de réservoir pour maintenir, de façon temporaire, les données très éloignées des prototypes courants. L\u0027article est organisé comme suit : d\u0027abord, la section 2 décrit plusieurs travaux liés au problème de clustering de flux de données. Ensuite, la section 3 présente notre nouvelle approche de clustering de flux de données, appelée G-Stream. Puis, dans la section 4, nous rapportons une évaluation expérimentale. Enfin, la section 5 conclut cet article et présente nos futurs travaux de recherche.\nTravaux similaires\nCette section présente un bref état de l\u0027art qui concerne les problèmes de clustering de flux de données. Nous mettons ainsi en évidence les algorithmes les plus pertinents proposés dans la littérature pour faire face à ce problème. La plupart des algorithmes existants (par exemple, CluStream proposé par (Aggarwal et al. (2003)), DenStream de (Cao et al. (2006)), StreamKM++ de (Ackermann et al. (2012)) divisent le processus de clustering en deux phases : (a) En-ligne, dans cette phase, les données sont résumées, (b) Hors-ligne, dans cette phase, les clusters finaux sont calculés à partir de la quantification fournie par la phase en-ligne. Les deux algorithmes CluStream et DenStream utilisent une extension temporelle du Clustering Feature vector proposée par (Zhang et al. (1996)) (appelée micro-clusters) afin de maintenir des résumés statistiques sur les données ainsi que leur temps d\u0027arrivée, ceci durant la phase en-ligne. En créant deux types de micro-clusters (potentiel et outlier micro-clusters), DenStream surmonte l\u0027un des principaux inconvénients de CluStream, sa sensibilité au bruit. Dans la phase hors-ligne, les micro-clusters trouvés lors de la phase en-ligne sont considérés comme des pseudo-points et seront transmis à une variante de k-means dans l\u0027algorithme CluStream (resp. une variante de DbScan dans l\u0027algorithme DenStream), afin de déterminer les clusters finaux. StreamKM++ est une extension de l\u0027algorithme k-means++ pour le flux de données. Les auteurs de (Isaksson et al. (2012)) ont proposé SOStream, qui est un algorithme de clustering de flux de données, basé sur la densité, inspiré à la fois du principe de l\u0027algorithme DbScan et celui des cartes auto-organisatrices (SOM) de (Kohonen et al. (2001)). L\u0027algorithme E-Stream, qui est proposé par (Udommanetanakit et al. (2007)), classe l\u0027évolution des données en cinq catégories : apparition, disparition, auto-évolution, fusion et division. Il utilise une autre structure de données pour sauvegarder des statistiques sommaires, nommée ?-bin histogramme. (Zhang et al. (2008)) présentent une extension de l\u0027algorithme Affinity Propagation pour le flux de données, appelé StrAP et qui utilise un réservoir pour maintenir d\u0027éventuels outliers. Les auteurs de (Bouguelia et al. (2013)) ont proposé une version incrémentale de l\u0027algorithme GNG de (Fritzke (1994)), appelée AING. où ? 1 \u003e 0, qui est une constante définissant le taux de décroissance du poids au fil du temps. t désigne le temps courant et t 0 est le temps d\u0027arrivée de la donnée. Le poids d\u0027un noeud est calculé à partir des poids des données qui lui sont affectées :\noù m est le nombre de données affectées au noeud c au temps courant t. Quand le poids d\u0027un noeud est inférieur à une valeur donnée, alors ce noeud est considéré comme obsolète et sera supprimé (ainsi que ses liens). Gestion des arêtes : la procédure de gestion des arêtes effectue des opérations liées à la mise à jour des arêtes du graphe (les étapes 14-19 de l\u0027algorithme 1). Lors de l\u0027incrémentation de l\u0027âge des arêtes, l\u0027instant de création d\u0027une arête est pris en compte. Contrairement à la fonction d\u0027oubli, l\u0027âge des liens sera renforcé par la fonction exponentielle f 2 (t) \u003d 2 ?2(t?t0) où ? 2 \u003e 0, définit le taux de croissance au temps courant t, t 0 est le temps de création de l\u0027arête. L\u0027étape suivante consiste à ajouter une nouvelle arête reliant les deux noeuds les plus proches. La dernière étape consiste à supprimer chaque lien dépassant un âge maximum.\nGestion du réservoir : l\u0027objectif de l\u0027utilisation d\u0027un réservoir est de maintenir, temporairement, les données éloignées. Comme nous l\u0027avons mentionné précédemment, chaque noeud a un seuil de distance. Les premières données du flux sont affectées aux noeuds les plus proches sans prendre en considération les seuils de distances. Le seuil de distance de chaque noeud est mis-à-jour en prenant la distance maximale du noeud au point le plus éloigné qui lui est affecté. Lorsque le réservoir est plein, ses données sont re-transmises à l\u0027apprentissage. Elles sont placées au début du flux de données, DS, afin de les traiter en premier. Les seuils de distance des noeuds sont mis-à-jour en conséquence.\nÉvaluation expérimentale\nDans cette section, nous présentons une évaluation expérimentale de l\u0027algorithme GStream. Nous avons comparé notre algorithme avec l\u0027algorithme GNG ainsi qu\u0027avec deux algorithmes pertinents de clustering de flux de données. Nos expériences ont été réali-sées sur la plateforme MATLAB en utilisant des données réelles et synthétiques. Les bases de données réelles, Shuttle (43500x9) et KddCup1 (49402x34), ont été prises à partir du répertoire UCI. Les bases DS1 (9153x2) et DS2 (5458x2) sont générées à l\u0027aide du programme disponible sur http://impca.curtin.edu.au/local/software/ synthetic-data-sets.tar.bz2. Comme nous l\u0027avons expliqué dans la section 3, les \nConclusion\nDans ce papier, nous avons proposé, G-Stream, une méthode efficace pour le clustering topologique en-ligne de flux de données évolutives. Dans G-Stream, les noeuds ainsi que les arêtes composant la structure topologique sont pondérés. A partir de deux noeuds, G-Stream compare les données arrivant aux prototypes courants ; il sauvegarde celles très éloignées dans un réservoir ; il apprend les seuils de distance automatiquement ainsi que plusieurs noeuds sont créés à la fois. L\u0027évaluation expérimentale sur des bases de données réelles et synthétiques a démontré l\u0027efficacité de G-Stream à découvrir des clusters de formes arbitraires. Les résultats obtenus sont prometteurs. Nous envisageons à l\u0027avenir d\u0027appliquer le principe des fenêtres adaptatives, de rendre notre algorithme le plus autonome possible et de le développer en Spark.\n"
  },
  {
    "id": "243",
    "text": "Introduction\nLe Web Sémantique a été lancé en 2001 par le W3C 1 pour promouvoir le partage et la créa-tion de données structurées sur le Web en proposant des recommandations pour la description de données (RDF), d\u0027ontologies (RDFS, OWL), et des méthodes et outils associés (SPARQL, ...) pour gérer les connaissances. Actuellement le Web Sémantique correspond à des centaines de bases RDF communautaires (e.g. DBPedia Notre contribution est de fournir une méthode originale d\u0027évaluation de mises à jour, inspirée du raisonnement par cas, utilisant exclusivement les données de la base RDF mise à jour (i.e. ne nécessitant pas l\u0027utilisation d\u0027une ontologie ou de méta-données). Par cette méthode, une mise à jour candidate est évaluée positivement si ses modifications dans la base RDF rendent -selon certains critères -la partie cible mise à jour dans la base plus structurellement similaire à d\u0027autres parties de la base. Notre méthode d\u0027évaluation de la cohérence peut être décomposée en 3 étapes : (i) extraction des contextes de la mise à jour depuis la base, (ii) ré-cupération des parties de la base similaires à la mise à jour et à ses contextes et (iii) évaluation par similarité de la cohérence des données de la mise à jour par rapport à la base.\nEn Section 2 nous définissons une mise à jour RDF et ses contextes (première étape de notre approche). En Section 3 nous détaillons notre méthode d\u0027évaluation de mise à jour en définissant quelles sous-parties de la base sont prises en compte lors de l\u0027évaluation d\u0027une mise à jour (Section 3.1), et enfin comment évaluer la cohérence d\u0027une mise à jour RDF par rapport à une base (Section 3.2).\nMise à jour RDF\nNous introduisons ici quelques définitions pour formaliser les notions de mises à jour RDF et de contextes associés dans une base RDF.\nNous rappelons quelques vocabulaires du Web Sémantique et introduisons quelques termes utilisés dans la suite de l\u0027article. Ainsi, nous considérerons comme synonymes les termes document RDF, base RDF et ensemble de triplets RDF ; pour un document RDF D nous noterons R D l\u0027ensemble des ressources -sujet, prédicat ou objet -des triplets de D ; nous appellerons une ressource noeud une ressource étant soit sujet, soit objet d\u0027un triplet ; pour un document RDF D, nous noterons N D l\u0027ensemble des ressources noeud de D ; nous appellerons document RDF connexe un document RDF dans lequel il y a un chemin connectant chaque ressource noeud du document à une autre, en d\u0027autres termes si le graphe RDF représentant le document est un graphe connexe ; nous appellerons degré d\u0027une ressource noeud le nombre de triplets la contenant dans une base RDF, en d\u0027autres termes son degré dans le graphe RDF.\nNotons aussi que implicitement nous désignons toujours les données d\u0027une base RDF sans (avant) que les modifications d\u0027une mise à jour ne lui soient appliquées. Enfin, toute mise à jour d\u0027une base RDF peut être vue en tant que combinaison de deux sections : une section d\u0027ajout qui contient ce que la mise à jour ajoute à la base et une section de suppression qui contient ce que la mise à jour supprime dans la base.\nDéfinition 1 (Mise à jour RDF). Une mise à jour RDF u d\u0027une base RDF B est un couple d\u0027ensembles de triplets RDF (A, R) tels que :\nUne mise à jour RDF qui ajoute des informations à une base doit apporter de nouveaux éléments liés à des données déjà existantes. Une mise à jour qui supprime des informations peut uniquement supprimer des données déjà existantes dans la base. Les sections d\u0027ajout et de suppression ne contiennent pas de triplets en commun (l\u0027ordre d\u0027application de la suppression ou de l\u0027ajout dans la base n\u0027a pas d\u0027importance). Une mise à jour contient nécessairement une section d\u0027ajout et forme un document connexe (autrement il s\u0027agit de 2 mises à jour distinctes).\nDe la Définition 1 nous pouvons classer les mises à jour RDF en deux catégories : les mises à jour d\u0027ajout, définies par A \u003d ? et R \u003d ?, et les mises à jour de modification, définies par A \u003d ? et R \u003d ?. Le cas des suppressions « pure » est discuté en conclusion.\nPour comparer les données d\u0027une mise à jour à une base RDF selon notre évaluation, nous utilisons le voisinage dans la base de toutes les ressources de la mise à jour. Ainsi les contextes d\u0027une mise à jour sont obtenus grâce aux voisinages des sections d\u0027ajout et de suppression.\nDéfinition 2 (Contextes de mise à jour RDF). Soient un ensemble de triplets RDF B, une mise à jour RDF u \u003d (A, R) candidate à B et n ? N un rang de voisinage. Soit la fonction voisinage n B (r) : N B ? B retournant tous les triplets de B connectés à r par un chemin de longueur égale ou inférieure à n, appelée fonction de voisinage de r.\nLes contextes d\u0027une mise à jour u candidate à B sont les deux ensembles de triplets RDF I u et F u définis par :\n-I u appelé le contexte initial de u dans B tel que\nLe contexte initial représente l\u0027état initial de la partie de la base autour des ressources de la mise à jour candidate, le contexte final représente l\u0027état théorique de la base si la mise à jour était appliquée. 3 Évaluer la cohérence par mesure de la similarité\nNous considérons qu\u0027une mise à jour est cohérente avec une base si on peut trouver suffisamment de sous-parties de la base suffisamment similaires avec les contextes de la mise à jour. Nous procédons en 3 étapes : (i) recherche dans la base de sous-parties structurellement comparables aux contextes de la mise à jour, (ii) quantification de la similarité entre chaque sous-partie et les contextes de la base, (iii) conclusion sur la cohérence de la mise à jour.\nContexte initial I u1 .\nContexte final F u1 .\nFIG. 2 -Contextes de u 1 (En pointillés : voisinage dans la base des ressources de u 1 )\nTrouver des références dans la base\nDeux ensembles de triplets RDF peuvent être structurellement comparés si leurs ressources noeuds sont liées de façon similaire, incluant (au moins) une ressource commune.\nDéfinition 3 (Ensembles de triplets RDF comparables). Soit deux ensembles de triplets RDF connexes G et H.\nG est comparable à H s\u0027il existe une fonction de transformation f :\nDeux ensembles comparables contiennent au moins une ressource commune. De plus, en théorie des graphes, on dira qu\u0027un ensemble de triplets RDF est comparable à un autre si il est homomorphique à une partie d\u0027un autre, sans considérer l\u0027orientation des arcs. Comparer une mise à jour à une base entière signifie comparer structurellement les contextes initial et final de la mise à jour à chaque sous-partie de la base comparable à la mise à jour. Nous appelons références ces sous-parties de la base dépendantes de la mise à jour.\nDBPedia, en tant que références pour u 1 en Fig. 1 (Élé-ments en commun avec u 1 en traits épais orange).\nDe la base DBPedia, deux références pour u 1 peuvent être extraites, notées D 1 et D 2 et représentées en Fig. 3. D 1 et D 2 contiennent plusieurs ressources en commun avec u 1 . D 1 suit le même modèle que u 1 avec une boisson liée à une ville liée à une personne, alors que D 2 concerne une boisson liée à une entreprise liée à une personne.\nÉvaluer la cohérence d\u0027une mise à jour\nDans notre approche, si les modifications d\u0027un mise à jour rendent la partie ciblée de la base plus similaire à d\u0027autres parties (existantes) de la base alors nous évaluons positivement cette mise à jour.\nNous proposons d\u0027évaluer la similarité de la mise à jour par rapport à chacune de ses réfé-rences à l\u0027aide d\u0027une mesure de la similarité structurelle entre deux graphes. Dans l\u0027évaluation en Définition 5, nous supposons l\u0027usage d\u0027une mesure donnant un score de similarité dans R + tel que plus le score est élevé, plus la similarité est grande (un score de 0 signifie aucune similarité). Plusieurs mesures sont utilisables telles que la distance d\u0027édition entre deux graphes, le coefficient de Jaccard, etc. . On note similarity la mesure de similarité entre deux graphes RDF avec similarity(u, La valeur de similarité seule n\u0027importe pas dans notre évaluation, seul le signe de la diffé-rence entre l\u0027état final et initial indique si la mise à jour apporte des informations similaires à ce qui est déjà connu.\nExemple 4. Dans cet exemple, nous choisissons d\u0027utiliser une mesure de similarité en considérant dans chaque ensemble de triplets l\u0027ensemble des ressources et l\u0027ensemble des couples de ressources (sujet, relation) et (relation, objet) où le score est calculé simplement -pédago-giquement -avec similarity \u003d +1 pour chaque ressource commune et +2 pour chaque couple de ressources communes. La différence de similarité entre la référence D 1 et les contextes de u 1 est positive (similarity(F u1 , D 1 ) ? similarity(I u1 , D 1 ) \u003d 12 ? 11) et celle entre D 2 et les contextes de u 1 est nulle (similarity(F u1 , D 2 ) ? similarity(I u1 , D 2 ) \u003d 9 ? 9), ainsi, avec un nombre minimum de références de 1, on a eval(u 1 , B, 2, 1) \u003d true.\nLa mise à jour u 1 est donc cohérente avec la base DBPedia : les modifications de la mise à jour créent des données structurellement similaires à des parties de la base. Cette mise à jour peut être appliquée à la base.\nConclusion\nDans cet article nous proposons une approche d\u0027intégration, ou de mise à jour, de données dans des bases RDF par une évaluation de la cohérence des mises à jour en fonction de leur\n"
  },
  {
    "id": "244",
    "text": "Introduction\nNetworks are studied in numerous contexts such as biology, sociology, online social networks, marketing, etc. Graphs are mathematical representations of networks, where the entities are called nodes and the connections are called edges. Very large graphs are difficult to analyse and it is often beneficial to divide them in smaller homogeneous components easier to handle. The process of decomposing a network has received different names : graph clustering (in data analysis), modularization, community structure identification. The clusters can be called communities or modules ; in this paper we use those words as synonyms.\nAssessing the quality of a graph partition requires a modularization criterion. This function will be optimized to find the best partition. Various modularization criteria have been formulated in the past to address different practical applications. Those criteria differ in the definition given to the notion of community or cluster.\nTo understand the differences between the optimal partitions obtained by each criterion we show how to represent them using the same basic formalism. In this paper we use the Mathematical Relational Analysis (MRA) to express six linear modularization criteria. Linear criteria are easy to handle, for instance, the Louvain method can be adapted to linear quality functions (see Campigotto et al. (2014)). The six criteria studied are : the Newman-Girvan modularity, the Zahn-Condorcet criterion, the Owsi´nskiOwsi´nski-Zadrozny criterion, the Deviation to Uniformity, the Deviation to Indetermination index and the Balanced Modularity (details in section 3). The relational representation allows to understand the properties of those modularization criteria. It allows to easily identify the criteria suffering from a resolution limit, first discussed by Fortunato et Barthelemy (2006). We will complete this theoretical study by some experiments on real and synthetic networks, demonstrating the effectiveness of our classification. This paper is organized as follows : Section 2 presents the Mathematical Relational Analysis approach, we introduce the property of balance for linear criteria and its relation to the property of resolution limit. In Section 3, we present the six linear modularization criteria in the relational formalism. Next, Section 4 presents some experiments on real and artificial graphs to confirm the theoretical properties found previously.\nRelational Analysis approach\nThere is a strong link between the Mathematical Relational Analysis 2 and graph theory : a graph is a mathematical structure that represents binary relations between objects belonging to the same set. Therefore, a non-oriented and non-weighted graph G \u003d (V, E), with N \u003d |V | nodes and M \u003d |E| edges, is a binary symmetric relation on its set of nodes V represented by its adjacency matrix A as follows :\nWe denote the degree d i of node i the number of edges incident to i. It can be calculated by summing up the terms of the row (or column) i of the adjacency matrix :\nN 2 the density of edges of the whole graph.\nPartitioning a graph implies defining an equivalence relation on the set of nodes V , that means a symmetric, reflexive and transitive relation. Mathematically, an equivalence relation is represented by a square matrix X of order N \u003d |V |, whose entries are defined as follows :\n2. For more details about Relational Analysis theory see Marcotorchino et Michaud (1979) and Marcotorchino (1984).\nModularizing a graph implies to find X as close as possible to A. A modularization criterion F (X) is a function which measures either a similarity or a distance between A and X. Therefore, the problem of modularization can be written as a function to optimize F (X) where the unknown X is subject to the constraints of an equivalence relation 3 .\nWe define as well ¯ X and ¯ A as the inverse relation of X and A respectively. Their entries are defined as ¯ x ii \u003d 1 ? x ii and ¯ a ii \u003d 1 ? a ii respectively. In the following we denote ? the optimal number of clusters, that means the number of clusters of the partition X which maximizes the criterion F (X).\nLinear balanced criteria\nEvery linear criterion is an affine function of X, therefore in relational notation it can be written as :\nwhere the function ?(a ii ) depends only on the original data (for instance the adjacency matrix). In the following we will use K to denote any constant depending only on the original data.\nDefinition 1 (Property of linear balance) A linear criterion is balanced if it can be written in the following general form :\nwhere ?(.) and ¯ ?(.) are non negative functions depending only on the original data and\n3. In fact, the problem of modularization can be written in the general form :\nsubject to the constraints of an equivalence relation :\nx ii ? {0, 1} Binary\nThe exact solving of this 0 ? 1 linear program due to the size of the constraints is impractical for big networks. So, heuristic approaches are the only reasonable way to proceed.\nBy replacing ¯\nx by its definition 1 ? x ii , equation (4) can be rewritten as follows :\nFrom this expression we can deduce the importance of the property of balance for linear criteria. If the criterion is a function to maximize, the presence and/or absence of the terms ? ii and ¯ ? ii has the following impact on the optimal solution :\nthe solution that maximizes F (X) is the partition where all nodes are clustered together in a single cluster, so ? \u003d 1 and\n) and\nthen the optimal solution that maximizes F (X) is the partition where all nodes are separated, so ? \u003d N and\nIn other words, the optimization of a linear criterion who does not verify the property of balance will either cluster all the nodes in a single cluster or isolate each node in its own cluster, therefore forcing the user to fix the number of clusters in advance.\nWe can deduce from the previous paragraphs that the values taken by the functions ? and ¯ ? create a sort of balance between the fact of generating as many clusters as possible, ? \u003d N , and the fact generating only one cluster, ? \u003d 1.\nIn the following we will call the quantity N i\u003d1 N i \u003d1 ?(a ii )x ii the term of positive agreements and the quantity N\nii the term of negative agreements.\nDifferent levels of balance\nWe define two levels of balance for all linear balanced criterion :\nDefinition 2 (Property of local balance) A balanced linear criterion whose functions ? ii and ¯ ? ii satisfy\nwhere K L is a constant depending only upon the pair (i, i ) (therefore not depending on global properties of the graph) has the property of local balance.\nSome remarks about definition 2 : -Since K L depends only on properties of the pair (i, i\n) , that is local properties, we call this property local balance.\n-When we talk about global properties we refer to the total number of nodes, the total number of edges or other properties describing the global structure of the graph. -In the particular case of local balance where\n), that is ? ii and ¯ ? ii sum up to a constant, we have the following situation : whereas ? ii increases ¯ ? ii decreases and vice versa.\nLet us consider the special case where ?(a ii ) \u003d a ii , the general term of the adjacency matrix. A null model is a graph with the same total number of edges and nodes and where the edges are randomly distributed. Let us denote the general term of the adjacency matrix of this random graph ¯ ?(a ii ). A criterion based on a null model considers that a random graph does not have community structure. The goal of such a criterion is to maximize the deviation between the real graph, represented by ?(a ii ) and the null model version of this graph, represented by ¯ ?(a ii ) as shown in equation (5).\nThat implies\n. This constraint implies that ¯ ? ii depends upon the total number of edges M . Consequently, the decision of clustering together two sub-graphs depends on a characteristic of the whole network and the criterion is not scale invariant because it depends on a global property of the graph.\nThe definition of null model for linear criteria can be generalized as follows :\nDefinition 3 (Criterion based on a null model) A balanced linear criterion whose functions ? ii and ¯ ? ii satisfy the following conditions :\nwhere g(K G ) is a function depending on global properties of the graph K G is a criterion based on a null model. K G can be for example the total number of edges or nodes. We can deduce from definitions 2 and 3 that a linear criterion can not be local balanced and based on a null model at the same time.\nIn the particular case where ¯ ? decreases if the size of the network increases, it becomes negligible for large graphs. As explained previously, if this term tends to zero, the optimization of the criterion will tend to put together the nodes more easily. For instance, a single edge between two sub-graphs would be interpreted by the criterion as a sign of a strong correlation between the two clusters, and optimizing the criterion would lead to the merge of the two clusters. Such a criterion is said to have a resolution limit.\nThe resolution limit was introduced by Fortunato et Barthelemy (2006), where the authors studied the resolution limit of the modularity of Newman-Girvan. They demonstrated that modularity optimization may fail to identify modules smaller than a scale which depends on global characteristics of the graph even weakly interconnected complete graphs, which represent the best identifiable communities, would be merged by this kind of optimization criteria if the network is sufficiently large. According to Kumpula et al. (2007) the resolution limit is present in any modularization criterion based on global optimization of intra-cluster edges and extracommunity links and on a comparison to any null model.\nIn section 4 we will show how criteria having a resolution limit fail to identify certain groups of densely connected nodes.\nModularization criteria in relational notation\nGraph clustering criteria depend strongly on the meaning given to the notion of community. In this section, we describe six linear modularization criteria and their relational coding in Table 1. We assume that the graphs we want to modularize are scale-free, that means that their degree distribution follows a power law.\n1. The Zahn-Condorcet criterion (1785, 1964) : C.T. Zahn (see Zahn (1964)) was the first author who studied the problem of finding an equivalence relation X, which best approximates a given symmetric relation A in the sense of minimizing the distance of the symmetric difference. However the criterion defined by Zahn corresponds to the dual Condorcet\u0027s criterion (see Condorcet (1785)) introduced in Relational Consensus and whose relational coding is given in Marcotorchino et Michaud (1979). This criterion requires that every node in each cluster be connected to at least as half as the total nodes inside the cluster. Consequently, for each cluster the fraction of within cluster edges is at least 50% (see Conde-Céspedes (2013) for the demonstration).\n2. The Owsí nski-Zadro? zny criterion (1986) (see Owsi´nskiOwsi´nski et Zadro? zny (1986)) it is a generalization of Condorcet\u0027s function. It has a parameter ?, which allows, according to the context, to define the minimal percentage of required within-cluster edges : ?. For ? \u003d 0.5 this criterion is equivalent to Condorcet\u0027s criterion. The parameter ? defines the balance between the positive agreements term and the negative agreements term. For each cluster the density of edges is at least ?% (see Conde-Céspedes (2013)).\n3. The Newman-Girvan criterion (2004) (see Newman et Girvan (2004)) : It is the best known modularization criterion, called sometimes simply modularity. It relies upon a null model. Its definition involves a comparison of the number of within-cluster edges in the real network and the expected number of such edges in a random graph where edges are distributed following the independence structure (a network without regard to community structure). In fact, the modularity measures the deviation to independence. As mention in the previous section, this criterion, based on a null model and it has a resolution limit (see Fortunato et Barthelemy (2006)). In fact, as the network becomes larger M ?? ?, the term ¯ ? ii \u003d ai.a .i 2M tends to zero for since the degree distribution follows a power law.\n4. The Deviation to Uniformity (2013) This criterion maximizes the deviation to the uniformity structure, it was proposed in Conde-Céspedes (2013). It compares the number of within-cluster edges in the real graph and the expected number of such edges in a random graph (the null model) where edges are uniformly distributed, thus all the nodes have the same degree equal to the average degree of the graph. This criterion is based on a null model and it has a resolution limit. indeed ? ?? 0 as N ?? ?.\nThe Deviation to Indetermination (2013)\nAnalogously to Newman-Girvan function, this criterion compares the number of within-cluster edges in the real network and the expected number of such edges in a random graph where edges are distributed following the indetermination structure 4 (a graph without regard to community structure), introduced in Marcotorchino (2013) and . The Deviation to Indetermination is based on a null model, therefore it has a resolution limit.\nThe Balanced modularity (2013) This criterion, introduced in Conde-Céspedes et\nMarcotorchino (2013), was constructed by adding to the Newman-Girvan modularity a term taking into account the absence of edges ¯ A. Whereas Newman-Girvan modularity compares the actual value of a ii to its equivalent in the case of a random graph ai.a .i 2M , the new term compares the value of ¯ a ii to its version in case of a random graph\n. It is based on a null model and it has a resolution limit.\nwhere 4. There exists a duality between the independence structure and the indetermination structure (see Marcotorchino (1984), Marcotorchino (1985) and Ah-Pine et Marcotorchino (2007)).\nThe six linear criteria of Table 1 verify the property of balance, so it is not necessary to fix in advance the number of clusters, more specifically : From Tables 1 and 2 one can easily deduce that for the criteria having a resolution limit the quantity ¯ ? ii decreases when the size of the graph becomes larger.\nTests with real and artificial networks\nWe modularized six real networks of different sizes : Jazz (Gleiser et Danon (2003)), Internet (Hoerdt et Magoni (2003)), Web nd.edu (Albert et al. (1999)), Amazon (Yang et Leskovec (2012) 5 ) and Youtube (Mislove et al. (2007)). We ran a generic version of Louvain Algorithm (see Campigotto et al. (2014) and Blondel et al. (2008)) until achievement of a stable value of each criterion. The number of clusters obtained for each network is shown in Table 3. Table 3 shows that the Zahn-Condorcet and Owsi´nskiOwsi´nski-Zadro? zny criteria generate many more clusters than the other criteria having a resolution limit, for which the number of clusters is rather comparable. Moreover, this difference increases with the network size. Notice that the number of clusters for the Owsi´nskiOwsi´nski-Zadro? zny criterion decreases with ?, that is the minimal required fraction of within-cluster edges, so the criterion becomes more flexible.\nOnly ground-truth overlapping communities are defined on these previuos real networks. This fact makes difficult to judge the quality of the obtained partitions. That si why we generated five benchmark LFR graphs (see Lancichinetti et al. (2008)) of different sizes 1000, 5000, 10000, 100000 and 500000. The input parameters are the same as those considered in Lancichinetti et Fortunato (2009). The average degree is 20, the maximum degree 50, the exponent 5. the data was taken from http://snap.stanford.edu/data/com-Amazon.html. of the degree distribution is -2 and that of the community size distribution is -1. In order to test the existence of resolution limit we chose small communities sizes, ranging from 10 to 50 nodes, and a low mixing parameter, 0.10. So, the communities are clearly defined. Figure 1 shows the average number of clusters for 100 runs of the generic Louvain algorithm. Network size: N FIGURE 1 -Average number of cluster for artificial LFR graphs (logarithmic scale). Figure 1 shows clearly the difference between the behaviour of those criteria having a resolution limit (NG, DU, DI and BM) and the behaviour of criteria locally defined (ZC and OZ). As the size of the network increases the four criteria suffering from resolution-limit detect fewer clusters than those predefined. The number of clusters is rather comparable for these four functions, one reason can be the fact that the term of negative agreements tends to zero when the network gets bigger. Conversely, the criteria locally defined identified more clusters than those predefined, specially ZC. The criterion which best approaches the real number of clusters is OZ with ? \u003d 0.2. Figure 2 shows the average Normalized Mutual Information for the partitions in Figure 1. Figure 2 shows that the average NMI decreases with the network size for criteria having a resolution limit. The criterion with the highest NMI is OZ with ? \u003d 0.2 which guarantees an within-cluster density of 20%.\nNumber of clusters\nConclusions\nWe presented six linear modularization criteria in relational notation, Zahn-Condorcet, Owsi´nskiOwsi´nski-Zadro? zny, the Newman-Girvan modularity, the Deviation to Uniformity index, the Deviation to Indetermination index and the Balanced-Modularity. This notation allowed us to easily identify the criteria suffering from a resolution limit. We found that the first two criteria had a local definition whereas the others, based on a null model, had a resolution limit. These findings were confirmed by modularizing real and artificial graphs using a generic version of the Louvain algorithm. We compared the number of clusters found by the six criteria and the Normalized Mutual information for artificial graphs. The results showed that those criteria ba-\n"
  },
  {
    "id": "245",
    "text": "Introduction\nEn apprentissage automatique, la précision et le rappel sont des mesures classiques pour évaluer les résultats et la performance des algorithmes utilisés. Ces mesures sont essentiellement utilisées en apprentissage supervisé (Sokolova et al., 2006), en classification simple (Jain, 2010) et croisée Hanczar et Nadif (2013) et en recherche d\u0027information (Manning et al., 2008). Dans ce dernier cas, la performance de l\u0027algorithme employé est évaluée à partir de la similarité entre l\u0027ensemble de documents retrouvés et l\u0027ensemble des documents cibles. Cette similarité se base sur la précision et le rappel. De la même manière en classification simple (resp. croisée), les algorithmes identifient des groupes (resp. biclusters) d\u0027éléments qui sont comparés à des groupes (resp. biclusters) de référence. En apprentissage supervisé, l\u0027évaluation d\u0027un classeur se fait en comparant les classes prédites avec les vraies classes sur un ensemble de test. On mesure la similarité entre les classes prédites et les vraies classes en calculant leur préci-sion et rappel. Cependant cette approche ne tient pas compte du taux de vrais négatifs. Pour ces raisons, on préfère dans certains cas utiliser le couple sensibilité-spécificité que le couple précision-rappel dans ce contexte. La précision et le rappel sont donc deux mesures très utilisées dans les procédures d\u0027évaluations de nombreux domaines. Il est extrêmement fréquent de combiner ces deux valeurs afin de construire des indices de performance tel que la F-mesure ou l\u0027indice de Jaccard (Albatineh et Niewiadomska-Bugaj, 2011).\nPar défaut les indices de performance donnent la même importance à la précision et au rappel. Or dans de nombreux cas, on peut vouloir privilégier l\u0027un par rapport à l\u0027autre. Par exemple, en génomique des groupes de gènes ayant des profils d\u0027expression similaires sont identifiés en utilisant des méthodes de classification. Ces groupes sont comparés à des classifications de gènes issues de bases de connaissance afin d\u0027estimer leur pertinence biologique (Datta et Datta, 2006). L\u0027objectif de ces analyses est de capturer le plus d\u0027information biologique dans les groupes de gènes, on veut donc privilégier le rappel par rapport à la précision dans ce contexte. Certains indices de performance ont une variante introduisant un paramètre permettant de contrôler le compromis précision-rappel comme c\u0027est le cas de la F-mesure qui est une généralisation de l\u0027indice de Dice. Pour d\u0027autre mesures, le contrôle du compromis précision-rappel est plus difficile, comme c\u0027est le cas de l\u0027indice de Jaccard. Dans cet article nous analysons les différents indices de performance en fonction du compromis précision-rappel. Nous proposons également un nouvel outil d\u0027analyse qu\u0027est l\u0027espace de compromis qui présente de nombreux avantages par rapport à l\u0027espace précision-rappel.\nDans la section 2, nous présentons les différents indices de performance étudiés ainsi que leurs variantes sensibles au compromis. Dans la section 3, nous rappelons les propriétés de l\u0027espace précision-rappel. Nous analysons le comportement des différents indices dans cet espace. Dans la section 4, nous définissons l\u0027espace de compromis et nous montrons comment représenter les performances par les courbes de compromis. Dans la section 5, nous montrons les avantages à travailler dans l\u0027espace de compromis en particulier pour la sélection de modèles et la comparaison d\u0027algorithmes. Nous illustrons ces propriétés avec un exemple dans le contexte du biclustering. Dans la section 6, nous exposons nos conclusions et perspectives.\n2 Indices basés sur le couple précision et rappel\nDéfinitions\nSoit D un ensemble des données contenant N éléments. Nous appelons groupe cible le sous-ensemble T ? D que nous recherchons. Un algorithme dont l\u0027objectif est de retrouver le groupe cible produit un groupe X. Pour mesurer la qualité de ce groupe X, un indice de performance est utilisé afin d\u0027évaluer la similitude entre T et X. Ces indices de performances sont généralement basés sur deux valeurs : la précision et le rappel. La précision représente la proportion de X qui recouvre T quant au rappel il exprime la proportion de T retrouvé par X. Ces deux indices prennent les formes suivantes :\nLes principaux indices de performances utilisés sont une combinaison de la précision et du rappel. Dans cet article nous étudierons les quatre plus populaires : l\u0027indice de Kulczynski, Fmesure, Folke et Jaccard. Ces travaux pourront être facilement étendus à d\u0027autres indices. Par défaut chacun de ces indices donne la même importance à la précision et au rappel. Cependant on peut construire des versions pondérées permettant de privilégier la précision par rapport au rappel ou inversement.\nL\u0027indice de Kulczynski\nL\u0027indice de Kulczynski est la moyenne arithmétique de la précision et du rappel.\nUne version pondérée introduit le paramètre R ? [0, +?] qui permet de contrôler le compromis entre la précision et le rappel. Plus R est grand, plus le rappel est important, le point d\u0027équilibre est atteint pour R \u003d 1. Nous réécrivons cet indice en effectuant le changement de variable suivant : ? \u003d R R+1 , ? ? [0, 1] contrôle désormais le compromis et le point d\u0027équilibre est atteint pour ? \u003d 0.5.\nLa F-mesure\nLa F-mesure, appelée aussi indice de Dice, est le rapport entre l\u0027intersection et la somme des tailles du groupe X et du groupe cible T . C\u0027est aussi la moyenne harmonique entre la précision et la rappel. \nL\u0027indice de Folke\nL\u0027indice de Folke correspond à la moyenne géométrique de la précision et du rappel.\nIl est possible de pondérer le moyenne géométrique en introduisant un paramètre ? ? [0, 1]. Plus ? est grand plus le rappel est important et le point d\u0027équilibre est atteint pour ? \u003d 0.5.\nL\u0027indice de Jaccard\nL\u0027indice de Jaccard est le rapport entre l\u0027intersection et l\u0027union du groupe X et le groupe cible T .\nIl n\u0027est pas facile de définir une version pondérée de l\u0027indice de Jaccard à cause de la pré-sence du terme pre.rec au dénominateur. Nous voulons un indice pondéré ayant les proprié-tés suivantes : I Jac (T, X, ?) ? [0, 1] ; I Jac (T, T, ?) \u003d 1 ; I Jac (T, X, 0.5) \u003d I Jac (T, X) ; I Jac (T, X, 0) \u003d pre ; I Jac (T, X, 1) \u003d rec. Pour cela nous proposons l\u0027indice suivant :\n3 L\u0027espace précision-rappel L\u0027espace précision-rappel, illustré dans la figure 1, est un espace à deux dimensions dans lequel les abscisses et ordonnées représentent respectivement le rappel et la précision (Buckland et Gey, 1994). Une performance est représentée par un point dans cet espace (le point blanc par exemple). Le principe de l\u0027espace précision-rappel est proche de celui de l\u0027espace ROC qui représente le taux de vrais positifs en fonction du taux de faux positifs (Fawcett, 2006). Plusieurs relations ont d\u0027ailleurs été identifiées entre ces deux espaces (Davis et Goadrich, 2006). Un point dans l\u0027espace précision-rappel représente tous les groupes de taille |X| \u003d |T | rec pre ayant une intersection avec le groupe cible de |T ? X| \u003d |T |rec. Le point (1,1) (point noir), maximisant la précision et le rappel, représente le groupe idéal et dans ce cas il y a une parfaite correspondance avec le groupe cible (X \u003d T ). Le point (1, |D| rec puisqu\u0027on a |D| ? |X|. Tous les groupes dont la performance se situe sur la droite pre \u003d |T | |D| rec sont ceux dont |T ? X| est minimale. Cette droite représente tous les groupes dont |T ? X| est nulle. La plupart des algorithmes a un paramètre permettant de contrôler la taille du résultat X. Pour chaque taille de X on obtient des valeurs de précision et rappel différentes. La performance d\u0027un algorithme peut donc être représentée par un ensemble de points et approximée par une courbe dans l\u0027espace précision-rappel. Dans la figure 1, on donne un exemple de courbe précision-rappel. On peut tirer plusieurs informations sur les performances de ces différents groupes même sans se référer à un indice en particulier. Si un point domine un autre, c-à-d si sa précision et son rappel sont supérieurs, alors on peut conclure qu\u0027il aura une meilleure performance quelque soit l\u0027indice utilisé. Les points noirs représentent les points dominants de la courbe, il ne sont dominés par aucun autre point et représentent les performances des meilleurs groupes. Il n\u0027y a pas de rapport de domination entre ces types de points, il est nécessaire d\u0027utiliser un indice pour les comparer.\nLe comportement des différents indices de performances peut se visualiser en dessinant leur iso-ligne dans l\u0027espace précision-rappel. Une iso-ligne est un ensemble de points dans l\u0027espace précision-rappel ayant tous la même valeur d\u0027indice (Flach, 2003;Hanczar et Nadif, 2013). La figure 2 montre les iso-lignes des indices de Kulczynski, F-mesure, Folke et Jaccard. Les lignes en gras représentent les iso-lignes lorsque ? \u003d 0.5. Pour les quatre indices, nous observons que les iso-lignes ont une symétrie autour de l\u0027axe pre \u003d rec, ceci signifie que la précision et le rappel ont la même importance. Par contre les différents indices ne considèrent pas la différence entre précision et rappel de la même façon. Cette différence n\u0027est pas prise en compte dans l\u0027indice de Kulczynski, alors que les autres indices la pénalisent. L\u0027indice de Folke pénalise moins que la F-mesure et l\u0027indice de Jaccard. Ces deux derniers sont équivalents car ils sont compatibles,\nDans la figure 2, les lignes en pointillées représentent les iso-lignes pour ? \u003d 0.2 et les lignes pleines ? \u003d 0.8. La modification de la valeur de ? déforme les iso-lignes, ce qui permet de donner plus d\u0027importance à la précision ou au rappel. A noter que pour pre \u003d rec les indices de Kulczynski, F-mesure et Folke retournent la même valeur quelque soit ?. L\u0027indice de Jaccard a un comportement différent, il pénalise le fait que ? s\u0027approche de 0.5. Dans la L\u0027espace de compromis, que nous proposons, offre un nouvel outil de visualisation des performances des résultats ou des algorithmes en fonction du compromis précision-rappel. Il y a certaines similitudes avec les \"cost curves\" utilisées en apprentissage supervisé (Drummond et Holte, 2006). L\u0027espace de compromis représente en abscisse ? et en ordonnée l\u0027indice de performance. La performance d\u0027un groupe X est représentée dans cet espace par une courbe f (?). On a une correspondance entre les points de l\u0027espace précision-rappel et les courbes Cette dernière courbe définit le domaine d\u0027application des indices de performances pour un problème donné, illustré dans la figure 3 par les zones blanches. Un point situé dans l\u0027une des zones grises, signifie que le groupe correspondant à de moins bonnes performances que le groupe maximal et peut donc être considéré comme non informatif. On constate que le domaine d\u0027application de l\u0027indice de Kulczynski est beaucoup plus petit que celui des autres indices. Cela est dû au fait que cet indice ne pénalise pas la différence entre précision et rappel. La ligne en pointillé représente le groupe contenant un unique élément appartenant au groupe cible. Le groupe parfait est représenté par la droite f (?) \u003d 1. A l\u0027inverse les groupes ayant une intersection nulle sont représentés par la droite f (?) \u003d 0. Les groupes aléatoires sont représentés par les courbes partant du point (0, |T | |D| ).\nCourbe optimale de compromis\nComme nous l\u0027avons illustré dans la figure 1, la performance d\u0027un algorithme peut être représentée par une courbe dans l\u0027espace précision-rappel. A chaque point de cette courbe correspond une courbe dans l\u0027espace de compromis. On peut représenter la courbe précision-rappel par un ensemble de courbes dans l\u0027espace de compromis. La figure 4 donne la représentation de la courbe précision-rappel de la figure 1 dans l\u0027espace de compromis pour les différents indices de performance. On s\u0027intéressera particulièrement à l\u0027enveloppe supérieure de cet ensemble de courbes, représentée en gras dans la figure 4 que nous appellerons courbe optimale de compromis. Cette dernière représente les meilleurs performances de l\u0027algorithme pour tous les compromis. On s\u0027aperçoit que les courbes formant l\u0027enveloppe supérieure correspondent tous à des points dominants de la courbe précision-rappel. Les points dominés ont toujours leur courbe en dessous de la courbe optimale de compromis. Dans le cas de l\u0027indice de Kulczynski, les courbes formant l\u0027enveloppe supérieure correspondent aux points de l\u0027enveloppe convexe de la courbe précision-rappel. Ces courbes de compromis permettent d\u0027analyser les résultats bien plus facilement que les courbes précision-rappel. 5 Application des courbes de compromis 5.1 Sélection de modèles L\u0027utilisation de l\u0027espace de compromis permet d\u0027identifier très facilement le résultat optimal pour un compromis donné. Ceci est illustré dans la figure 5 à travers un problème de classification croisée. Nous avons généré une matrice de données aléatoires dans laquelle un bicluster a été introduit, ce dernier suit un modèle additif selon la définition de Madeira et Oliveira (2004). Nous utilisons l\u0027algorithme CC (Cheng \u0026 Church) pour retrouver ce bicluster (Cheng et Church, 2000). La similarité entre le bicluster retourné par l\u0027algorithme et le bicluster recherché est alors calculée par les différents indices de performance. Cet algorithme dispose d\u0027un paramètre permettant de contrôler la taille du bicluster retourné, nous pouvons donc représenter les performances de cet algorithme par une courbe précision-rappel (figure 5). A partir de cette courbe il n\u0027est pas facile de déterminer le meilleur bicluster pour un compromis de précision-rappel donné. Même en ajoutant les iso-lignes au graphique, la comparaison des différents biclusters n\u0027est pas intuitive. Dans la figure 5 est représentée la courbe optimale de compromis pour la F-mesure. A partir de cette courbe on peut instantanément identifier le meilleur bicluster pour un compromis donné. On a aussi une décomposition de la valeur de ? en une série d\u0027intervalles qui sont délimités sur le graphique par les lignes verticales pointillées, pour lesquels le meilleur bicluster est donné. Sur notre exemple on constate qu\u0027il y a sept intervalles, nous nous intéresserons donc qu\u0027aux sept biclusters correspondants, identifiés sur la figure par leur taille. Pour le dernier intervalle (? \u003e 0.74) le meilleur bicluster est la matrice entière, la courbe optimale de compromis est confondue avec la courbe du bicluster maximal. Notons qu\u0027il n\u0027est pas possible d\u0027identifier visuellement ces biclusters dans l\u0027espace précision-rappel car ils ne correspondent ni à l\u0027ensemble des points dominants ni à l\u0027enveloppe convexe de la courbe précision-rappel (sauf dans le cas de l\u0027indice de Kulczynski). Il est également très facile de travailler avec des contraintes sur la précision ou le rappel dans l\u0027espace de compromis. Nous rappelons que la précision et le rappel se lisent à l\u0027extrémité de chaque courbe de compromis. Lorsqu\u0027on demande une précision minimale pre min , il suffit de considérer unique les courbes de compromis qui partent au-dessus du seuil minimum c-à-d f (0) \u003e pre min . De même avec un rappel minimum rec min , on ne conserve que les courbes qui arrivent au-dessus du seuil de rappel c-à-d f (1) \u003e rec min .\nComparaison d\u0027algorithmes\nL\u0027espace de compris simplifie également grandement la comparaison des algorithmes. Nous reprenons l\u0027exemple de classification croisée précédent dans lequel un autre algorithme, ISA (Bergmann et al., 2003), est testé et comparé à CC. Les performances de ce nouvel algorithme sont représentées dans l\u0027espace précision-rappel et l\u0027espace de compromis par la courbe grise dans la figure 6. Dans l\u0027espace précision-rappel les deux courbes se croisent plusieurs fois, aucun des deux algorithmes n\u0027est donc absolument meilleur que l\u0027autre. Il est difficile de voir dans quelles conditions CC est meilleur que ISA et inversement. Dans l\u0027espace de compromis on visualise immédiatement quel est le meilleur algorithme pour chaque valeur de compromis. Pour ? \u003c 0.28 CC est meilleur que ISA, pour 0.28 \u003c ? \u003c 0.83, ISA est meilleur, pour ? \u003e 0.83 les deux algorithmes retournent un bicluster contenant toute la matrice de donnée et ont donc des performances identiques. La distance entre les deux courbes permet de FIG. 5 -Identification des meilleurs biclusters dans l\u0027espace précision-rappel et l\u0027espace de compromis. A gauche, la courbe précision-rappel. A droite, la courbe optimale de compromis.\nvisualiser la différence de qualité entre les deux algorithmes. Dans l\u0027espace précision-rappel les courbes des deux algorithmes se croisent trois fois, laissant penser qu\u0027il y a deux intervalles de ? pour lesquelles CC est meilleur (de même pour ISA). Les courbes optimales de compromis montrent que l\u0027identité du meilleur algorithme ne change qu\u0027une fois, en ? \u003d 0.28. Dans l\u0027espace précision-rappel, CC a une meilleurez précision que ISA ; 14 fois sur 20 ce qui laisse penser que CC est plus souvent meilleur que ISA. L\u0027espace de compromis nous montre qu\u0027au contraire l\u0027intervalle [0, 0.28] pour lequel CC est meilleur est deux fois plus petit que celui de ISA [0.28,0.83]. Cet exemple illustre bien la facilité de la comparaison d\u0027algorithmes dans l\u0027espace de compromis.\nFIG. 6 -Identification du meilleur algorithme. A gauche, les courbes précision-rappel. A droite, les courbes optimales de compromis.\n"
  },
  {
    "id": "246",
    "text": "Introduction\nLes requêtes skyline sont importantes dans les applications qui nécessitent la localisation des réponses selon plusieurs critères. Ayant un ensemble de points dans un espace vectoriel de d dimensions, un algorithme traitant ce type de requêtes doit retourner l\u0027ensemble des points de S dits non dominés. Il est meilleur pour ce type d\u0027algorithmes de fonctionner progressivement Kossmann et al. (2002) car les utilisateurs sont souvent impatients de recevoir des réponses. La relation de dominance se définit comme suit Börzsonyi et al. (2001) : Soit S un ensemble de données de d dimensions (d critères) sur lequel va porter l\u0027opérateur skyline. Soit D l\u0027ensemble de toutes les dimensions D \u003d {d 1 , , d d }. Soient p et q deux points de S. La relation de dominance (?) suivant D est pour 1 ? i, j ? d ,p domine q ?? {?d i ? D, p(i) ? q(i)} et {?d j ? D, p(j) \u003c q(j)}. Lorsque aucun point ne domine l\u0027autre on dit qu\u0027ils sont non dominés ou concurrents. L\u0027opérateur skyline renvoie l\u0027ensemble des points concurrents, suivant toutes les dimensions D : SkyD(S) \u003d {p ? S/ ? S : q ? p} Dans ce papier, nous présentons une solution analytique pour déduire l\u0027ensemble de points candidats afin d\u0027éviter de parcourir l\u0027ensemble S entièrement. Nous donnons un nouveau théo-rème pour l\u0027élimination des points non candidats. Notre méthode est basée sur le tri Tan et al. (2001) et à la différence avec ce travail, où les tests entre les points balayent tout l\u0027ensemble S, nous donnons des théorèmes pour la déduction des points les plus évidents et qui constituent les premières solutions à présenter. Nous montrerons que la combinaison de DC Divide-andConquer avec notre méthode fournit des résultats meilleurs que lorsqu\u0027il est appliqué tout seul. Le reste de ce papier se présente comme suit. La section 2 présente les travaux liés à cette problématique. La section 3 donne notre approche. La section 4 présente les résultats des expérimentations. La conclusion et les travaux futurs sont donnés dans la section 5. Börzsonyi et al. (2001) était le premier travail ayant adapté l\u0027optimisation au sens de Pareto dans les bases de données. Intuitivement, le calcul du skyline consiste à comparer chaque point p avec tous les autres et si aucun point ne le domine alors p est un point skyline. L\u0027algorithme BNL Börzsonyi et al. (2001) utilise cette technique directe. Il met en mémoire une liste candidate et teste à chaque fois si un nouveau point p domine un ou plusieurs points déjà insérés. Si c\u0027est le cas, il est inséré et l\u0027ensemble des points dominé est écarté sinon il passe au point suivant. Cette méthode peut être utilisée facilement et ne requiert pas de prétraitement, sauf qu\u0027elle est gourmande en mémoire et en temps de calcul. DC Börzsonyi et al. (2001) divise l\u0027entrée en plusieurs partitions et détermine le skyline de chaque partition. Par la suite, les skyline sont fusionnés et les points dominés sont écartés. Cette méthode est meilleure que BNL mais souffre des multiples duplications lors de la fusion. Notons que ni BNL ni DC ne fonctionnent en on-line. L\u0027algorithme Bitmap Tan et al. (2001) consiste à encoder dans des vecteurs bitmap toutes les informations de chaque point selon le nombre de points distincts sur chaque axe. La comparaison des vecteurs bitmap se fait par la suite. Bitmap est progressive mais nécessite beaucoup d\u0027opérations et de codage en commençant par la détermination des points distincts dans chaque axe car il y aura beaucoup de tests dupliqués. Index Tan et al. (2001) consiste à trier les données sur chacun des d axes dans un ordre croissant. Afin de déter-miner le skyline, les points sont testés de façon circulaire. Le problème est que la récupération des coordonnées des points peut prendre du temps. Cette méthode est bien adaptée pour les applications on-line, elle retourne aussitôt les premiers points, sauf que les auteurs ne déduise pas les skyline induit par le tri. NN Kossmann et al. (2002) est un algorithme qui utilise les RTrees pour indexer les données. Il partitionne l\u0027espace selon chaque axe selon le point le proche voisin de l\u0027origine. NN est progressif et est efficace dans un espace à deux dimensions, mais il souffre du problème de duplications des éliminations pour 3 dimensions et plus. A partir de 4 dimensions, il devient difficile de l\u0027appliquer. Les auteurs proposent différentes techniques pour remédier à ces problèmes. Branch and Bound Skyline Papadias et al. (2003) exploite les R-Tree, la méthode de Branch and Bound et NN afin de calculer, en on-line, le skyline. Son plus grand problème est qu\u0027il souffre de requêtes redondantes. Yuan et al. (2005) proposent Skycube. Il calculent les skyline fils de toutes les combinaisons possibles des points dans le treillis. Lorsqu\u0027ils passent au niveau supérieur ou inférieur du treillis ils fusionnent ces fils.\nTravaux liés\nLa méthode DCRD\nNotre méthode DCRD pour Divide-and-Conquer for Reduced Data est une méthode analytique qui détermine l\u0027espace candidat en se basant sur le tri. L\u0027utilisation directe de l\u0027espace Pareto est simple pour un espace à 2 dimensions, mais au-delà de 3 dimensions, il faut ajouter des méthodes efficaces pour calculer cet espace. Nous donnons un nouveau théorème qui permet de déduire cet espace. Dans Index, les tests de dominance se font entre tous les points sans l\u0027exploitation de la concurrence induite par le tri. Par exemple, il est impossible qu\u0027un point A, ayant la valeur minimale unique sur un axe X, soit dominé par un autre point. Ce qui nous mène à donner le théorème 1. Preuve. Suite à la discussion précédente, sur l\u0027axe i, il est impossible qu\u0027un autre point puisse dominer le point p, puisqu\u0027aucune valeur sur cet axe ne sera inférieure à celle de p. Si pour tous les autres axes, le point q domine p, il sera impossible qu\u0027il le domine sur i, d\u0027où p est soit concurrent avec q soit le domine. Définition du conflit entre les points ayant des valeurs minimales sur le même axe. Il est fréquent que deux points ou plus aient la même valeur minimale sur un axe. Ainsi, il faut résoudre ce conflit de dominance avant de passer au calcul du skyline définitif. Ainsi, nous donnons le théorème 2 suivant : Théorème 2 : Existence de plusieurs points minimaux sur un axe i Soient p et q deux points de S tel qu\u0027il existe un axe i avec p Preuve. Ceci revient à résoudre le conflit entre p et q dans les autres sous-espaces. Le test montrera soit la dominance soit la concurrence entre eux sur les autres axes.\nPhase 1 : Tri des données et déduction des premiers points skyline\ntrier les valeurs de S par ordre croissant ; -Extraire l\u0027ensemble Sky des premiers points skyline en utilisant les théorèmes 1 et 2 ;\nPhase 2 : Réduction de l\u0027espace de données\nCette phase consiste à limiter l\u0027espace de données en filtrant l\u0027ensemble de données selon deux points appelés M in sys et M ax sys , autrement dit, on détermine l\u0027espace de dominance dans lequel se trouvent tous les points candidats. M in sys est le même que le point idéal de Pareto. Le point M ax sys est un point virtuel qui permet de délimiter cet espace. Il sert à éli-miner un espace important non utile. En deux dimensions, il se confond au point nadir mais, à plus de dimensions, ils sont différents. Le point nadir est un point pour lequel la fonction à optimiser est maximale ? et ce n\u0027est pas notre cas car M ax sys est dominé. D\u0027une manière analytique, nous déterminons les points M in sys et M ax sys à partir des coordonnées des points skyline déduits de la phase précédente : M in sys et M ax sys possède chacun d composantes. Chaque composante M in sys [i] (resp. M ax sys [i]) de M in sys (resp. M ax sys ) est la valeur minimale(resp. maximale) de toutes les composantes des points de Sky. Calcul de l\u0027espace des candidats. Dans cette étape, l\u0027ensemble des données candidats est ré-duit à un hypercube. Pour l\u0027obtenir, nous énonçons et appliquons le théorème3 suivant :  \n. p ne peut dominer q sur l\u0027axe d. Donc, il y a au moins un point m appartenant au skyline actuel qui domine p.\nPhase 3 : Calcul du skyline final\nA l\u0027issue de la phase 2, l\u0027espace déduit contient les points candidats. Il ne reste que de les comparer entre eux et éliminer les points dominés, nous appliquons ainsi DC.\nExpérimentations\nLes expérimentations ont été réalisées dans une machine dotée d\u0027Intel Core i5 2,50 GHz, et de 4 Go de RAM, sous Windows 7, 64 bits. Le programme est écrit sous Java. MySQL 5.1.41 est utilisé comme système gestion des bases de données. Nous avons utilisé les mêmes bases de données synthétiques de Börzsonyi et al. (2001)  Kossmann et al. (2002). Il s\u0027agit de trois types de données : corrélées, anti-corrélées et indépen-dantes. Nous avons calculé le temps nécessaire en secondes pour retourner le skyline en tenant compte de la dimensionalité et la cardinalité.\nComparaison entre DC et DCRD dans le type anti-corrélé. En fixant la dimension à 5 et en variant la cardinalité de 10000 à 100000 points. La figure 1 montre que pour ce type de données, DCRD a rendu les résultats dans des temps meilleurs puisqu\u0027il y a eu des éliminations de points inutiles. Pour 10000 points et en variant la dimension de 1 à 10, nous remarquons sur la figure 6 que DCRD a commencé à rendre les réponses rapidement à partir de d\u003d6. On déduit ainsi que le Comparaison entre DC et DCRD dans le type corrélé Ce type de données est intéressant et facile à manier ; même un algorithme naïf pourrait présenter de bonnes performances. En comparant ces deux méthodes sur la cardinalité et en fixant le nombre d\u0027attributs à 5, nous remarquons sur la figure 3 que DCRD a dépassé de loin DC. Ceci est dû au nombre important de points qui ont été éliminés. La faiblesse de DC est qu\u0027il exécute des tests de dominance sur tout l\u0027ensemble de données d\u0027une façon aveugle. De même, en comparant DC et DCRD selon la dimension, nous avons fixé la cardinalité à 10000 et nous avons varié le nombre d\u0027attributs. Puisque le nombre de points éliminé est important, la figure 4 montre que DCRD a été très rapide alors que DC a consommé plus de temps. Ceci est toujours le cas puisque DC ne fait aucun traitement préalable et exécute des partitions sur tout l\u0027ensemble d\u0027entrée. Ceci a un effet sur les temps de réponse.\n"
  },
  {
    "id": "247",
    "text": "Introduction\nL\u0027explosion d\u0027internet, couplée à l\u0027effet de la mondialisation, a pour résultat d\u0027interconnecter les personnes, les entreprises, les états. Le côté déplaisant de cette interconnexion mondiale des Systèmes d\u0027Information réside dans un phénomène appelé \"Cybercriminalité\". Des personnes, des groupes mal intentionnés ont pour objectif de nuire dans un but pécuniaire ou pour une \"cause\", aux informations d\u0027une entreprise, d\u0027une personne voire d\u0027un Etat. Il n\u0027est pas rare que des faits de \"cyber-attaques\" soient relatés dans les médias envers des grandes socié-tés comme \"Google\",\"Visa\",\"Sony\", \"Apple\". La sécurité d\u0027un Système d\u0027Information se doit d\u0027être présente afin de garantir la confidentialité, l\u0027intégrité, la disponibilité de l\u0027information. De ce fait, il existe une multitude d\u0027équipements de sécurité qui permettent de détecter les comportements anormaux. Un des principaux équipements de sécurité est le \"Pare-Feu\" 1 ou plus communément appelé \"Firewall\". Il a pour mission comme le décrit Al-Shaer et Hamed (2003) de filtrer , selon une politique fondée sur les flux autorisés à pénétrer dans un réseau selon leurs sources, leurs destinations et les services souhaités (navigation internet, transfert de fichiers, etc... ). Par son positionnement, il donne une visibilité totale de l\u0027ensemble des flux. Cet équipement offre aussi la possibilité \"d\u0027historiser\" vers des journaux les flux ayant été autorisés ou interdits. L\u0027exploitation et l\u0027analyse des journaux d\u0027événements liés aux équi-pements de sécurité sont devenues primordiales pour la maîtrise des flux et la détection des intrusions ainsi que pour la vérification du bien fondé de la politique de filtrage mise en place (Golnabi et al, 2006). Dans ce contexte, les constructeurs d\u0027équipements de filtrage mettent à disposition des logiciels permettant d\u0027analyser les flux. Ces derniers nécessitent un accès et une connaissance dudit équipement. La détection des anomalies et des comportements anormaux est conséquemment réservée à ces seuls utilisateurs. La problématique de la représentation des événements de sécurité est tellement répandue que plusieurs outils ont même été regroupés au . Le principe est de modéliser un système de \"monitoring\" et de visualisation des données réseau en temps réel permettant de détecter rapidement les tentatives d\u0027intrusions.\nComposition du projet \"D113\"\nLe projet \"D113\" est composé de quatre phases qui s\u0027inscrivent dans le cadre d\u0027un travail de thèse en sécurité s\u0027appuyant sur des données issues des différents équipements et outils de sécurité. Les différentes phases du projet se déclinent selon la liste suivante.\n- Notre démonstration de logiciel portera uniquement sur la phase 1 qui est le préambule à la \"fouille de données\" qui sera effectuée dans les phases suivantes. La première phase constitue un tout en soi dans la mesure où la visualisation des données pour les utilisateurs est un enjeu crucial en termes de prise de décisions sur les problématiques de sécurité. Ces trois sites sont opérationnels, c\u0027est à dire que les données traitées et analysées dans les sections suivantes correspondent à des données de production. Pour des raisons de confidentialité les adresses IP ont été anonymisées. Le réseau SP1 est doté de son propre conteneur de données qui est alimenté par les événements envoyés en temps réel par le \"Firewall\". Les réseaux SAB1 et SQ1 mutualisent un même conteneur. Les données brutes envoyées par l\u0027ensemble des équipements filtrants sont traitées selon une extraction de motifs.\nDescription des données\nLe contenu des variables listées ci-dessous sont exportées vers des conteneurs de données. \nConclusions et perspectives\nA l\u0027issue de la phase 1, l\u0027ensemble des événements liés au filtrage est exporté en temps réel vers des conteneurs de données. Dans un souci de performance et compte-tenu de l\u0027importance\n"
  },
  {
    "id": "248",
    "text": "Introduction\nLes travaux de cette dernière décennie dans le domaine de la découverte de connaissances, comme ceux notamment de ) et de (Tiwari et al. (2010)), témoignent du vif intérêt pour le problème de l\u0027extraction des ensembles d\u0027items fré-quents, des motifs séquentiels, des motifs structurels (dans les données de type arbres, graphes ou treillis), et la recherche de méthodes efficaces pour extraire ces motifs. Dans cet article, nous nous intéressons à la notion de proportion analogique, essentiellement étudiée dans le domaine de l\u0027intelligence artificielle, pour extraire de nouveaux types de motifs dans les bases de données. Les proportions analogiques relient quatre objets A, B, C, D du même type dans une assertion de la forme « A est à B ce que C est à D ». Ils permettent d\u0027exprimer l\u0027identité (ou la proximité) des rapports existant entre deux paires d\u0027éléments. Des exemples typiques de cette notion en langage naturel sont : « le veau est à la vache ce que le poulain est à la jument », « l\u0027aurochs est au boeuf ce que le mammouth est à l\u0027éléphant ». Ces relations permettent d\u0027exprimer que ce qui distingue A de B est comparable à ce qui distingue C de D. Les exemples ci-dessus montrent la diversité (et la potentielle complexité) des sémantiques possibles du connecteur « est à » intervenant dans une proportion analogique. Dans le premier exemple, ce connecteur représente une relation de filiation tandis que dans la seconde, il exprime une évolution possible. Le connecteur « ce que » de la proportion représente généralement l\u0027identité ou la similarité. Quand les éléments A, B, C et D sont des valeurs numériques, la relation peut être définie en utilisant les proportions mathéma-tiques classiques, comme la proportion géométrique : A/B \u003d C/D (e.g., 1/3 \u003d 2/6) ou la proportion arithmétique : A ? B \u003d C ? D (e.g., 5 ? 3 \u003d 9 ? 7). Quand les objets A et B, resp. C et D, représentent les mêmes entités à différents moments ou états de leur vie (par exemple, A et B décrivent le même endroit à deux moments différents), la proportion analogique peut exprimer des évolutions similaires. De manière générale, les proportions analogiques permettent de trouver des parallèles entre quatre événements ou situations.\nNous cherchons à exploiter la notion de proportion analogique dans le contexte des bases de données relationnelles afin d\u0027extraire des combinaisons de quatre n-uplets liés par une telle relation. Notre objectif est de découvrir des parallèles entre des paires de n-uplets, i.e., des paires d\u0027éléments qui sont dans les mêmes rapports. Ces parallèles ne reflètent pas forcément une relation de proximité (A est aussi proche de B que C est proche de D), mais plutôt une transformation semblable (On passe de A à B comme on passe de C à D). Ces parallèles sont d\u0027une importance majeure puisqu\u0027il permettent de modéliser des règles d\u0027évolution reproductible dans les systèmes écolo-giques (les états de deux littoraux qui évoluent dans les mêmes directions : apparition et disparition des mêmes espèces, évolution d\u0027une pollution d\u0027une région à une autre), des mouvements sociétaux (extension d\u0027une crise géopolitique ou comparaison avec des successions d\u0027événements passés), ou des déplacements parallèles d\u0027objets.\nLes contributions de cet article sont les suivantes. Nous proposons une méthode pour identifier les proportions analogiques dans les bases de données. À cette fin, nous suivons une approche vectorielle pour définir la notion de proportion analogique adaptée au modèle relationnel. Puis nous montrons qu\u0027il est possible de ramener le problème d\u0027énumération de toutes les combinaisons de quatre n-uplets liés par une relation d\u0027analogie, à un problème de clustering moyennant un prétraitement et l\u0027utilisation d\u0027une métrique. Ceci permet de rassembler des paires d\u0027éléments qui définissent des vecteurs égaux ou presque. Nous analysons ensuite les résultats de notre approche appliquée à un jeu de données réelles.\nNotre article est organisé comme suit. En section 2, nous introduisons la notion de proportion analogique et nous proposons une définition graduelle de celle-ci adaptée au contexte des bases de données. La section 3 présente notre approche de découverte des proportions analogiques et l\u0027algorithme qui en découle, tandis que la section 4 détaille les expérimentations effectuées. Enfin, nous présentons les travaux relatifs à notre proposition (Section 5) puis nous concluons (Section 6).\nProportions analogiques et modélisation\nLes proportions analogiques\nCette section s\u0027appuie sur les références (Miclet et Prade (2009)) et (Lepage (2012)). Une proportion analogique est une assertion de la forme « A est à B ce que C est à D», notée par la suite Un algorithme naïf pour énumérer les proportions analogiques issues d\u0027un ensemble d\u0027objets de cardinalité n, a une complexité temporelle en n 4 . En utilisant un point de vue vectoriel de la notion de proportion analogique, les objets A, B, C et D désignent des points d\u0027un espace à n dimensions. S\u0027ils forment une relation d\u0027analogie alors ces points forment un parallèlogramme. Par exemple, la figure 1 montre la relation de proportion analogique existant entre les points A(1, 2), B(4, 4), C(3, 1), D(6, 3) représentés dans un repère orthonormé.\nAinsi quatre objets A, B, C et D sont en proportion analogique si et seulement si\nLa relation de proportion analogique liant A, B, C, et D peut être alors symbolisée par le vecteur ? ? ? AB ( ou ? ? ? CD). Dans ce cas particulier (la conformité est la relation d\u0027identité), il est possible de définir un algorithme dont la complexité temporelle est en n 2 : celui-ci calcule tous les vecteurs existant entre les paires de n-uplets et rassemblent toutes les paires de nuplets définissant des vecteurs égaux en une classe d\u0027équivalence (Lepage (2012)). Une classe d\u0027équivalence, représentée par un vecteur, rassemble ainsi des paires de points qui, prises deux à deux, sont en proportion analogique « selon ce vecteur ». Il est alors aisé de générer l\u0027ensemble de toutes les relations d\u0027analogie à partir de chacune des classes d\u0027équivalence.  \nSupposons que\nModéliser les proportions analogiques selon une approche géométrique\nLa modélisation des proportions analogiques dans le contexte des bases de données relationnelles est influencée par les propriétés du modèle relationnel. Soit un ensemble d\u0027attributs {A 1 , . . . , A m }, un schéma de relation est défini comme un sous-ensemble d\u0027attributs S \u003d {A i1 , . . . , A in }. Une relation définie en termes d\u0027un schéma de relation S est un sous-ensemble fini du produit cartésien des domaines de chacun des attributs de S. Chaque élément d\u0027une relation est appelé n-uplet qui peut être représenté par 1. La propriété de permutation des moyens permet d\u0027éviter de calculer à la fois\nun point décrit par n dimensions. Une base de données est un ensemble fini de relations. D\u0027autres contraintes additionnelles comme les dépendences fonctionnelles et les dépendances d\u0027inclusion permettent de restreindre le contenu des relations. Il serait intéressant d\u0027en tenir compte dans la recherche des proportions analogiques mais nous nous limiterons ici au cas de la recherche de proportions existantes entre quatre points. Ainsi les n-uplets A, B, C, et D d\u0027une relation sont considérés comme des points à n dimensions, et sont dénotés comme suit :\nComme dit précédemment, A, B, C, et D sont liés par une relation de proportion analogique si et seulement si\nL\u0027égalité est difficile à obtenir quand on considère des jeux de données réels. Il convient alors de rendre cette définition plus flexible, notamment en donnant une vision plus graduelle de la relation de proportion analogique. Deux vecteurs ne doivent plus être égaux mais presque égaux ce qui revient à mesurer dans quelle mesure || ? ? ? AB ? ? ? ? CD|| est proche de 0. On cherche alors à évaluer la « distorsion » entre les deux vecteurs. Pour permettre la commensurabilité des dimensions lorsque les attributs portent sur des domaines différents, les valeurs des vecteurs sont normalisées afin qu\u0027elles appartiennent à l\u0027intervalle [0, 1]. Pour cela, chaque valeur v du domaine actif d\u0027un attribut peut être remplacée par la valeur suivante :\nv ? min att max att ? min att où min att et max att désignent respectivement la valeur minimale et la valeur maximale du domaine actif de l\u0027attribut.\nPlusieurs stratégies peuvent être utilisées pour mesurer à quel point l\u0027expression || ? ? ? AB ? ? ? ? CD|| est proche de || ? ? 0 ||. Différentes normes peuvent être utilisées comme la norme de Minkowsky (norme p), qui donnera la longueur du vecteur correctif permettant de passer de ? ? ? AB à ? ? ? CD, ou, la norme infinie qui donnera la coordonnée maximale de ce vecteur correctif.\nDéfinition 1 : Distorsion analogique fondée sur une norme infinie Soit A, B, C, et D quatre n-uplets de n dimensions.\nLa norme infinie retourne la plus grande différence de dimension entre les deux vecteurs. Les deux vecteurs sont d\u0027autant plus égaux que le changement maximal sur une dimension est proche de zéro.\nDéfinition 2 : Distorsion analogique fondée sur la norme p Soit A, B, C, et D quatre n-uplets de n dimensions.\nDans ce cas, la définition met l\u0027accent sur la longueur du vecteur correctif permettant de passer de\nDans tous les cas (Définition 1 ou Définition 2), la relation de proportion analogique est d\u0027autant plus vraie que la distorsion est proche de 0.\nProposition : Les deux définitions de distorsion vérifient les propriétés fondamentales des proportions analogiques. En effet, les propriétés suivantes sont vérifiées :\npuisque les deux définitions reposent sur une valeur absolue des différences entre chaque coordonnée.\n-la permutation des moyens : la relation Dist(\nDans la suite, nous utiliserons la norme infinie qui est la plus drastique et possède un meilleur pouvoir de discrimination dans la mesure où elle évite tout effet de compromis entre les composantes des vecteurs.\nDécouvrir les proportions analogiques\nNotre objectif est de découvrir toutes les proportions analogiques présentes dans un ensemble de données et si possible de dégager des tendances, i.e., les différents vecteurs représentatifs des proportions analogiques découvertes. Une approche naïve pourrait consister à énumérer tous les vecteurs et à calculer la distorsion entre chaque paire de vecteurs, puis à ne garder que les paires de vecteurs dont la valeur de distorsion ne dé-passe pas un certain seuil. Cependant, une telle approche poserait la question du choix du seuil, très dépendant des données. Il nous semble par ailleurs préférable d\u0027utiliser une technique permettant de fournir une vue synthétique des motifs découverts. Une approche de type clustering semble tout à fait appropriée dans ce contexte. Un argument supplémentaire en faveur d\u0027une telle approche est lié à l\u0027objectif final que nous nous sommes fixé, à savoir l\u0027extension des langages d\u0027interrogation de bases de données avec des requêtes analogiques, i.e., des requêtes visant à découvrir des proportions analogiques existant dans un ensemble de données. En effet, l\u0027identification de classes d\u0027équivalence regroupant les paires de points représentant des vecteurs (presque) égaux permettrait la définition d\u0027index, utiles pour optimiser l\u0027évaluation de telles requêtes. Le problème ici étudié, qui consiste à regrouper des vecteurs à n dimensions égaux ou presque, se ramène à un problème de clustering classique dès lors que l\u0027on dispose d\u0027une métrique. Or les définitions des distorsions analogiques (Dist( ? ? u , ? ? v )) satisfont les conditions qui caractérisent les métriques, soit :\n-l\u0027identité des indiscernables : Dist( ? ? u , ? ? v ) \u003d 0 ssi ? ? u \u003d ? ? v , -la propriété de symétrie et -l\u0027inégalité triangulaire. Différentes approches de clustering sont donc utilisables, comme les k-means et l\u0027approche hiérarchique (Xu et al. (2005)). Notre objectif étant de tester l\u0027approche et de la valider sur des jeux de données réels, puis d\u0027identifier les relations découvertes, nous avons choisi de reprendre un algorithme de clustering hiérarchique (Xu et al. (2005)) dont les étapes sont énoncées dans l\u0027Algorithme 1. Cet algorithme requiert une étape préalable de construction des vecteurs à partir des points de la relation. \nData\nExpérimentations\nDans cette section, nous illustrons notre approche avec des données électorales afin de découvrir des parallèles entre les résultats des votes de différentes régions, puis des évolutions des résultats de votes d\u0027une année à l\u0027autre. Pour cela, nous avons exploité les jeux de données ouverts décrivant les résultats des élections présidentielles en    Figure 2).\nLes expérimentations visaient à montrer que le cadre proposé permet de mettre en évidence des parallèles existant dans les données, ce que montrent nos premiers résul-tats. Vu la nature de la relation de proportion analogique, l\u0027approche peut évidemment s\u0027appliquer à bien d\u0027autres domaines, comme la recherche de trajectoires parallèles d\u0027objets mobiles, moyennant une adaptation, ou dans les domaines environnemental et sociétal, pour découvrir des évolutions analogues.\n5 État de l\u0027art L\u0027originalité de l\u0027approche présentée ici tient à la nature même du type de régularité que l\u0027on cherche à découvrir dans les données. La majeure partie des travaux en fouille de données visent à découvrir des caractéristiques fréquentes (vues comme des valeurs d\u0027attribut ou des séquences de valeurs d\u0027attribut lorsqu\u0027un aspect temporel est pris en compte) dans un ensemble de données. Avec les proportions analogiques, qui sont des relations quaternaires, nous cherchons à vérifier l\u0027existence de \"parallèles\" entre des couples d\u0027objets d\u0027une collection. Une proportion analogique, lorsqu\u0027elle met en parallèle les situations de deux éléments à deux moments différents, peut être vue comme une sorte de règle d\u0027évolution. Dans un tel contexte, la recherche de proportions analogiques peut constituer une alternative aux approches de la littérature visant à -extraire des règles d\u0027évolution dans des graphes (Berlingerio et al. (2009)), ou à -découvrir des trajectoires parallèles d\u0027objets en mouvement (Vlachos et al. (2002); Chen et al. (2005); Lee et al. (2007); Li et al. (2013)), ou encore à -classer des séquences d\u0027événements (Studer et al. (2010); Guigourès et al. (2014); Lin et al. (2003); Malinowski et al. (2013); Zhou et al. (2013)).\nQuoi qu\u0027il en soit, l\u0027approche proposée fournit un cadre plus général que celui dédié spécifiquement à l\u0027extraction de règles d\u0027évolution dans les données. En effet, la notion de proportion analogique n\u0027implique pas l\u0027existence d\u0027une dimension temporelle et peut servir à décrire des parallèles de nature très variée entre deux paires d\u0027objets.\n"
  },
  {
    "id": "249",
    "text": "Introduction\nActuellement, la recherche et la détection de similitudes s\u0027effectuent en deux phases : une première phase de recherche de sources candidates, suivie d\u0027une seconde de comparaison de ces sources possibles avec le document que l\u0027on suspecte d\u0027être un plagiat. La phase de collecte est de plus en plus optimale grâce à l\u0027amélioration de l\u0027efficacité des moteurs de recherche en local et sur le Web. C\u0027est à la seconde phase que cet article s\u0027intéresse. Une fois qu\u0027une source candidate est trouvée, elle doit être comparée avec le document sur lequel pèse les soupçons. À l\u0027heure actuelle, la plupart des logiciels anti-plagiat, une fois une liste de sources candidates constituée, se contentent de comparer mot à mot le document analysé avec chaque source possible. Cette technique permet seulement de détecter les similitudes de types « copier/coller ». Bien que cette approche ait prouvé son efficacité et suffise la plupart du temps, en France près d\u0027un étudiant sur deux a déjà eu recours au « copier/coller » (Gibney, 2006), une énorme faille persiste. En effet, le fait de reformuler ou tout simplement de paraphraser un texte, en utilisant des synonymes par exemple, rend la plupart des techniques actuelles caduques. Certains articles (Callison-Burch et al., 2008;Bannard et Callison-Burch, 2005) se sont tout de même intéressés à la détection de reformulations paraphrastiques avec des approches d\u0027alignement. Malgré le fait que ces approches soient plus robustes à l\u0027ajout et à la suppression de mots ainsi qu\u0027à l\u0027utilisation de synonymes, elles restent toutefois inefficaces face aux reformulations non paraphrastiques comme le passage de la forme active à la forme passive. L\u0027approche proposée consiste à comparer les deux textes, phrase par phrase, et non plus mot à mot et à rechercher si une phrase de l\u0027un des textes comporte le même sens qu\u0027une phrase dans l\u0027autre texte. Ceci repose sur l\u0027hypothèse que lorsqu\u0027on paraphrase ou reformule un texte, on garde le sens de celui-ci et ainsi on garde les mots-clés principaux, porteurs du plus de sens de chaque phrase. Après avoir défini quelques notions et présenté l\u0027état de l\u0027art, nous décrirons d\u0027abord comment segmenter le texte en unités de sens, pour ensuite procéder sur chacune de ces unités à l\u0027extraction des mots porteurs de sens, afin de rechercher des concordances de mots de même concept dans un autre texte. Enfin, nous testerons trois algorithmes utilisant notre approche et nous déterminerons le meilleur seuil pour chacun d\u0027entre eux. Le seuil est le nombre minimum de concepts identiques dans deux phrases permettant d\u0027affirmer que l\u0027une est la reformulation de l\u0027autre. Pour finir, nous présenterons l\u0027évaluation de notre approche en comparant la méthode retenue aux méthodes classiques de détection des paraphrases par alignement.\n2 La comparaison au-delà du « copier/coller »\nLa notion de comparaison\nLa « comparaison de deux documents » est un terme assez vague. Pour comparer correctement deux documents, il faut repérer leurs points communs (leurs similitudes) et leurs diffé-rences. Les similitudes étant plus simples à détecter, il est de convention de chercher à repérer celles-ci en premier lieu et d\u0027en déduire ensuite les différences, représentées alors par le reste du document. Cependant, la plupart des comparaisons textuelles se limitent au « copier/coller », or ce ne sont pas les seules similitudes pouvant être recensées dans un texte.\nLa notion de similitudes\nBien que l\u0027on puisse avoir au sein d\u0027un document des tableaux, images, graphiques ou tout autre type de données, cet article traite seulement des similitudes d\u0027ordre textuel. On distingue plusieurs types de similitudes allant de la ressemblance jusqu\u0027à l\u0027identité même (SimacLejeune, 2013b). Les ressemblances sont les types de similitudes les plus difficiles à repérer et sont pour cause le point faible des logiciels anti-plagiat actuels. Dans notre cas, on distingue trois types majeurs de similitudes textuelles, de la plus simple à détecter à la plus complexe :\n-la copie, qui consiste à copier mot à mot tout ou partie d\u0027un texte dans un autre. Pour exemple, considérons la phrase suivante présente dans un texte : « En cinquante ans, grâce à des efforts considérables dans la recherche et l\u0027élaboration de la fusion, la performance des plasmas a été multipliée par 10\u0027000. » Elle sera recopiée à l\u0027identique dans un autre texte ; -la paraphrase, aussi appelée reformulation paraphrastique, qui consiste à reprendre une phrase d\u0027un texte pour la détailler ou l\u0027expliciter. Elle conserve donc l\u0027ordre des éléments évoqués, autorisant simplement le changement de vocabulaire, l\u0027ajout, la suppression et la substitution de mots. Toujours en considérant la phrase de l\u0027exemple précédent, une paraphrase possible serait : « En une cinquantaine d\u0027années, grâce à un immense effort de recherche, la performance des plasmas produits par les machines de fusion a été multipliée par 10000. » On remarque la conservation des concepts, mais aussi la substitution ou la suppression de certains d\u0027entre eux ; -la reformulation, qui autorise elle toutes modifications textuelles à condition que le sens de la phrase soit conservé. Cela donne souvent lieu à un changement d\u0027ordre des concepts. La reformulation de la phrase exemple serait : « La performance des plasmas produits par les machines de fusion a été multipliée par 10,000 grâce à un immense effort de la recherche bien que cela ait pris une cinquantaine d\u0027années. »\nLa notion de concept\nUn concept est une idée, un sens représenté par un mot ou un groupe de mots. Les reformulations et paraphrases exploitent les propriétés paradigmatiques des mots (leur capacité à se substituer mutuellement) et entraînent ainsi des changements de vocabulaire mais elles conservent les concepts et les idées exprimées (Duclaye, 2003). Il est alors, dans le cadre de la détection de similitudes, plus judicieux de représenter un mot par un concept plutôt que par son identité ou sa définition. Par exemple, il est plus judicieux de représenter un mot par un tableau de tous les mots par lesquels il peut être substitué (un tableau de ses synonymes, lui compris) plutôt que seulement par lui-même.\nÉtat de l\u0027art\nLorsque les processus anti-plagiat comparent deux documents, ils recherchent les éléments de l\u0027un également présents dans l\u0027autre. Ils tentent de détecter des similitudes, toutes informations communes laissant penser qu\u0027un plagiat a pu avoir lieu. La comparaison mot à mot est certes efficace pour trouver les zones de « copier/coller » mais les plagiaires ne se contentent plus de copier des éléments depuis une source, ils essaient à présent de camoufler leurs emprunts d\u0027idées derrière des modifications syntaxiques. Les recherches de Barron-Cedeño et al. (2013) se concentrant sur la détection de paraphrases appliquée dans le cadre de la détection du plagiat démontrent que le phénomène de paraphrasage nuit aux systèmes anti-plagiat et rend la détection de similitudes plus difficile. Il faut donc tenter de détecter les paraphrases et les reformulations par des moyens différents, car bien que souvent associés ces deux termes représentent des opérations textuelles bien distinctes. Toutefois, les travaux linguistiques ayant portés sur leur définition, s\u0027accordent sur le fait que ce sont des opérations de modifications de texte, certes bien différentes, mais qui conservent toutes deux le sens (Harris, 1957;Martin, 1976;Duclaye, 2003).\nDes recherches (Gülich et Kotschi, 1983;Eshkol-Taravella et Grabar, 2014) se sont attardées à chercher des marqueurs de reformulations afin de mieux les repérer par la suite et d\u0027étudier leur fonctionnement et leur construction. D\u0027autres recherches se sont cantonnées à étudier les limites de la détection des paraphrases (Vila et al., 2011) en estimant au contraire qu\u0027il n\u0027existait pas de caractérisation complète sur le plan linguistique et computationnelle de la paraphrase.\nFace à ces difficultés, des chercheurs se sont concentrés sur des approches alternatives ne permettant pas de détecter concrètement des reformulations mais de tout de même déterminer qu\u0027un texte en contient :\n-les approches stylométriques (Iyer et Singh, 2005) qui suggèrent qu\u0027en analysant des statistiques de fréquences de mots ou bien d\u0027autres caractéristiques d\u0027un texte on peut en reconnaître l\u0027auteur, et ainsi, si un passage du document ne possède pas les mêmes caractéristiques que le reste du document, on peut en déduire que ce passage aura été emprunté à un autre auteur (Oberreuter et Velásquez, 2013;van Halteren, 2004;Jardino et al., 2007) ; -les approches de calcul de distances (Simac-Lejeune, 2013a) qui propose de calculer une distance « sémantique » entre deux textes après avoir extrait les mots clefs de chaque texte, exposant ainsi l\u0027emprunt probable de l\u0027un dans l\u0027autre.\nEn dehors de ces approches, la majorité des travaux portent sur la détection des reformulations paraphrastiques, comme les recherches de Eshkol-Taravella et Grabar (2014) portant sur leur détection dans des corpus oraux. Les approches les plus répandues sont les méthodes par alignement (Callison-Burch et al., 2008;Bannard et Callison-Burch, 2005). Servant la plupart du temps dans un contexte bi-linguale (alignement d\u0027un texte et de sa traduction), elles consistent à aligner deux textes par leurs mots ou groupes de mots en communs et ainsi de repérer les mots ou groupes de mots différents mais équivalents. Certaines recherches (Shen et al., 2006), visant à produire des paraphrases, se sont également avérées intéressantes. En effet, étudiant la possibilité de générer automatiquement des paraphrases, un processus d\u0027assemblage puis de désassemblage s\u0027est dégagé, remettant ainsi sur le devant de la scène les méthodes d\u0027alignement. Proche de ces méthodes avec alignement, on peut citer le travail de Fenoglio et al. (2007) traitant de la comparaison de versions de documents textuels à la façon des serveurs de versions. Il met en lumière les transformations élémentaires (déplacements, insertions, suppressions et remplacements de blocs de caractères), identifiées depuis longtemps par les spécialistes de la génétique textuelle (de Biasi, 2000;Grésillon, 1994) comme éléments fondateurs d\u0027une paraphrase.\nToutefois, le cadre théorique le plus souvent adopté est la théorie linguistique Sens-Texte (Kahane, 2003) élaborée dans les années 1960 par Mel\u0027? cuk, notamment son système de paraphrasage (Žolkovskij et Mel\u0027? cuk, 1967;Mel\u0027? cuk, 1992;Mili´cevi´cMili´cevi´Mili´cevi´c, 2007) comme dans le travail de Mili´cevi´cMili´cevi´Mili´cevi´c (2010). Ce dernier met également en avant des approches sémantiques qui permettent de s\u0027approcher d\u0027une détection de reformulations. La plupart des règles sémantiques de paraphrasage trouvées jusqu\u0027ici mettent en jeu un découpage du texte en proposition et des liens communicatifs et rhétoriques entre celles-ci (Danlos, 2006), coïncidant ainsi, dans la plupart des cas, à la définition d\u0027une reformulation qui se contente d\u0027être une paraphrase avec changement d\u0027ordre des propositions.\nLa reformulation non paraphrastique étant bien plus complexe à détecter que sa voisine la paraphrase, les études se concentrant uniquement sur elle se font plus rares. Mais dès lors qu\u0027on sait que la reformulation conserve également le sens du texte (Harris, 1957;Martin, 1976;Duclaye, 2003) et que le mécanisme de paraphrase le plus utilisé est le changement de lexique (Barron-Cedeño et al., 2013), on peut envisager d\u0027appliquer plus ou moins les mêmes approches sémantiques que pour la paraphrase ou bien même, des approches plus naïves de recherche de correspondances de concepts.\n3 Notre approche\nSegmentation\nDans un premier temps, l\u0027idée est de segmenter le document que l\u0027on suspecte être un plagiat. Plusieurs algorithmes de segmentation ont été évalués :\n-la segmentation par nombre de blocs : on découpe le document en un certain nombre de blocs de même taille (de même nombre de mots), peu importe la taille finale de chaque bloc ; -la segmentation par taille de blocs : on découpe le document par blocs d\u0027une certaine taille (un certain nombre de mots), peu importe le nombre de blocs créés ; -la segmentation par pourcentage que représente un bloc sur l\u0027ensemble du document (e.g. une segmentation comme celle-ci avec en paramètre un pourcentage de 1% pour un bloc reviendrait à une segmentation en 100 blocs, chaque bloc représentant 1%) ; -la segmentation par granularité (Simac-Lejeune, 2013b) : il s\u0027agit d\u0027une segmentation hiérarchique, on découpe le texte en nb blocs de même taille, puis on redécoupe chaque bloc ainsi obtenu en nb blocs de même taille, et ainsi de suite sur une profondeur limite définie. Ceci permettant d\u0027affiner l\u0027analyse niveau par niveau ; -la segmentation par paragraphe : chaque segment représentant un paragraphe ; -la segmentation par phrase : chaque segment représentant une phrase du document ; -la segmentation par proposition : chaque segment représentant une unité minimale de sens, les délimiteurs étant la ponctuation de fin de phrase mais aussi les virgules, les conjonctions de coordination et divers mots de liaison ou de causalité. Chaque algorithme a fait l\u0027étude, via de nombreux tests et corrections, à l\u0027optimisation de ses paramètres afin de mettre l\u0027accent sur la rapidité du processus de découpage mais aussi sur la pertinence des métadonnées extraites dans chaque segment. Il est important que chaque segment conserve un sens afin d\u0027être potentiellement sujet à une reformulation. Une segmentation en unité de sens a donc été choisie. Un découpage par phrase ou par proposition est à privilé-gier car une segmentation à faible granularité, comme celle par paragraphe, donne lieu à des segments trop volumineux pour l\u0027étape d\u0027extraction qui suivra. Au contraire, une segmentation à trop grand niveau de granularité pourrait, en plus d\u0027entraîner un temps d\u0027exécution plus important (plus de segments à traiter), occasionner une perte d\u0027informations dans sa globalité (aucune liaison entre les concepts extraits). C\u0027est pourquoi la segmentation qui a été retenue est une fusion du découpage par phrase et du découpage par taille de blocs : une segmentation par phrase mais d\u0027une taille minimale (en mots). On conserve ainsi une unité de sens (une ou plusieurs phrases) tout en gardant une taille suffisamment importante pour pouvoir obtenir une pertinence raisonnable des métadonnées extraites mais suffisamment petite pour être considé-rée comme indépendante et donc éventuellement reformulée. Après divers tests, le seuil a été fixé à 15 mots, taille moyenne des phrases dans la langue française. Avec un seuil si faible, c\u0027est l\u0027une des méthodes de segmentation évaluée les plus chronophages mais pour notre étude elle garantit un rapport taille/pertinence optimal.\nExtraction de mots clefs (mots porteurs de sens)\nLa seconde étape du processus est une étape d\u0027extraction des mots porteurs de sens de chaque segment c\u0027est-à-dire des mots représentant les concepts que le plagiaire a été obligé de réutiliser s\u0027il voulait conserver le sens de la phrase, même s\u0027il a pu les remplacer par des synonymes. Pour déterminer les mots porteurs de sens d\u0027un texte, l\u0027étiqueteur morphosyntaxique TreeTagger (Schmid, 1994) a été utilisé. Il détermine la classe lexicale, le \"Part Of Speech\" de chaque unité lexicale (token) du texte. De façon plus commune, on peut dire que pour chaque mot ou élément du texte, TreeTagger détermine s\u0027il s\u0027agit d\u0027un nom, d\u0027un verbe, d\u0027un adjectif, d\u0027une ponctuation, etc. L\u0027étiquetage morphosyntaxique permet d\u0027identifier les mots clefs d\u0027un texte par leur classe lexicale. Plutôt que de discriminer les mots vides (stop words) par leur taille, ceci pouvant générer des erreurs (e.g. un mot de moins de trois lettres n\u0027est pas pertinent, un contre exemple est le mot « as » qui peut être important, et le mot « mais » qui est simplement une conjonction), on les discriminera par leur \"Part Of Speech\".\nDans notre cas, les mots pertinents à conserver sont un peu plus riches sémantiquement que les mots clefs habituels. On ne conserve pas seulement les noms communs et propres, il est important de garder aussi les adjectifs, les verbes et également les adverbes, en réalité tout mot porteur de sens au sein d\u0027une phrase. On néglige donc les méthodes les plus courantes pour extraire des mots clefs, les méthodes fréquentielles (Lee et Baik, 2004) qui consiste pour chaque mot du texte à calculer sa fréquence d\u0027apparition dans le texte. C\u0027est pour cela que le terme de mots clefs est ici un abus de langage et que nous allons préférer le terme de mots porteurs de sens d\u0027une phrase. Tout mot porteur de sens d\u0027une phrase doit être conservé, peu importe son nombre d\u0027occurrences dans le texte.\nUn filtre de mots vides a été ajouté à la sortie de TreeTagger afin d\u0027être certain de la pertinence des mots porteurs de sens extraits. Ainsi en couplant les deux techniques, l\u0027efficacité de l\u0027étiquetage est passée d\u0027environ 96% à quasiment 100%.\nConsidérons la phrase suivante : « Ce peu de masse disparue crée une grande quantité d\u0027énergie comme le démontre la fameuse formule d\u0027Einstein E\u003dmc2. » Ses mots porteurs de sens extraits seraient « peu, masse, disparue, crée, grande, quantité, énergie, démontre, fameuse, formule, Einstein, E\u003dmc2 ».\nThésaurus -chargement d\u0027un dictionnaire de synonymes\nParallèlement à cela, un dictionnaire de synonymes est chargé. Pour chaque mot, on a donc accès à un tableau contenant tous les mots de la langue par lesquels il peut être substitué. L\u0027efficacité de notre approche dépendant en grande partie du contenu de cette ressource, il est important de faire la différence entre des synonymes et des mots de substitution possibles. Par exemple, pour le mot « père », « papa » serait un synonyme alors que « parent » serait un mot de substitution envisageable. Autre exemple, le mot « île » a pour synonyme « îlot », « archipel » ou bien encore « atoll » mais aucunement les mots « tâche » ou « pâté » qui eux se trouvent pourtant dans notre tableau et peuvent servir de mot de substitution. En effet, on peut très bien imaginer dans un poème une phrase telle que « cette tâche au milieu de l\u0027océan » faisant référence à un îlot. En règle générale, « îlot » et « tâche » ne sont pas synonymes mais ici, ils représentent le même concept.\nDès lors, un concept est un mot porteur de sens ainsi que tous ses mots de substitution possibles contenus dans son tableau.\nLe tableau 1 représente une partie des mots de substitution correspondant aux mots porteurs de sens extraits sur la phrase exemple lors de l\u0027étape précédente. On remarque, comme dans l\u0027exemple de l\u0027îlot cité précédemment, que le terme « énergie » laisse place à « assiduité » qui n\u0027a strictement rien à voir avec le contexte de notre phrase mais qui dans un autre contexte aurait très bien pu être un synonyme envisageable. \nCorrespondance\nLa dernière étape de notre approche consiste à comparer chaque phrase d\u0027une source candidate avec les mots porteurs de sens de chaque segment du texte en cours d\u0027analyse ainsi qu\u0027avec leurs mots de substitution possibles contenus dans le tableau défini précédemment. On appellera seuil de correspondance le nombre de concepts communs à partir duquel on peut estimer qu\u0027une phrase est la reformulation d\u0027une autre. S\u0027il y a plus de concepts pertinents communs entre deux phrases que le seuil de correspondance défini, c\u0027est sans doute que l\u0027une est une reformulation de l\u0027autre.\nPlusieurs algorithmes mettant en oeuvre cette méthode ont été développés, certains plus robustes que d\u0027autres face aux changements de genre, de nombre, de casse typographique ou bien d\u0027ordre des mots (e.g. phrase passée de la forme active à la forme passive et vice versa). L\u0027efficacité de la détection dépend de l\u0027algorithme choisi, du seuil de correspondance défini, et du nombre et de la pertinence des « synonymes » disponibles dans le dictionnaire chargé.\nNous proposons trois algorithmes, trois implémentations différentes de l\u0027approche décrite précédemment.\n-un premier (tableau 2 -A) qui compare la présence des concepts dans l\u0027ordre et tels qu\u0027ils sont présents dans les phrases. Il ne supporte donc ni le changement de casse typographique, ni la dérivation et la flexion ; -un second (tableau 2 -B) qui compare également la présence des concepts dans l\u0027ordre des phrases mais en comparant leurs lemmes en minuscules, il supporte donc le changement de casse typographique, la dérivation et le changement de genre et de nombre ;\n-un troisième (tableau 2 -C), plus naïf, qui reprend le principe du précédent, en comparant cette fois la présence des concepts dans les deux phrases sans prendre l\u0027ordre en compte. Il est ainsi robuste aux reformulations non paraphrastiques de type mise à la forme passive. Le tableau 2 résume les différentes variations de la langue supportées par chaque algorithme. Considérons maintenant la phrase : « La célèbre équation d\u0027Einstein E \u003d mc 2 exprime le phénomène suivant : une importante quantité d\u0027énergie est apparue et un peu de la masse a disparu. » ainsi que sa reformulation : « Ce peu de masse disparue crée une grande quantité d\u0027énergie comme le démontre la fameuse formule d\u0027Albert Einstein E\u003dmc2. » Si on opère la comparaison de type C sur ces deux phrases, on retrouve bien, malgré le changement de vocabulaire et d\u0027ordre des mots, la correspondance de nos concepts, ici en gras.\nA noter l\u0027importance du seuil de correspondance, il y a dans cette exemple 11 concepts identiques, avec un seuil de correspondance de 11 ou moins, la phrase est reconnue comme reformulation, alors qu\u0027avec un seuil de correspondance supérieur ce ne sera plus le cas.\nÉvaluation et tests 4.1 La base de tests et protocole\nLa base de tests est composée de 150 textes, représentant chacun un passage d\u0027un document, allant de plus de 100 mots pour le plus petit à environ 9000 mots pour le plus grand. Cela représente 400 comparaisons de textes deux à deux. Afin de tester correctement les performances des algorithmes évalués, aussi bien des paraphrases que des reformulations plus complexes sont présentes dans le corpus, ainsi que des textes « pièges » traitant du même sujet et donc employant le même vocabulaire mais n\u0027étant pas pour autant des reformulations d\u0027un autre texte du corpus.\nCi-dessous la répartition des types de textes présents dans le corpus : -10 différents chapitres tirés d\u0027un même roman ; -20 chapitres de la bible (deux traductions différentes pour 10 chapitres) ; -25 textes de Wikipédia (différentes versions à différentes dates de 10 articles) ; -35 extraits de travaux d\u0027élèves (avec leurs sources provenant du Web) ; Les extraits de travaux d\u0027élèves proviennent pour la plupart de rapports et mémoires scientifiques ou économiques. L\u0027intégralité des textes sont en français.\nRésultats\nDans un premier temps, on compare les trois méthodes décrites précédemment, leur efficacité et leur temps moyen d\u0027exécution respectifs étant différents selon le seuil utilisé, on détermine d\u0027abord le seuil optimal pour chacune d\u0027entre elles. Le tableau 3 représente le rapport précision/rappel des trois algorithmes allant du seuil 1 à 7. Un seuillage de 4 semble mieux convenir aux algorithmes A et B, tandis qu\u0027un seuillage de 5 semble idéal pour l\u0027algorithme C. Prenant en compte la F-mesure et privilégiant le rappel plutôt que la précision, l\u0027algorithme C se montre être le plus efficace sur la base de tests. Le tableau 4 représente le temps d\u0027exécution de la procédure (segmentation, extraction de mots porteurs de sens, chargement du thésaurus et comparaison) des trois algorithmes en utilisant leur meilleur seuillage en fonction du nombre moyen de mots contenus dans les deux textes à comparer (moyenne du nombre de mots des deux textes). La méthode C s\u0027avère être la plus rapide (200 secondes en moyenne pour un texte d\u0027environ 4000 mots contre 250 pour la méthode A et 212 secondes pour la méthode B) en plus d\u0027avoir un meilleur rapport préci-sion/rappel, respectivement 0.745 et 0.807, car malgré le fait qu\u0027elle soit utilisée avec un seuil plus grand (5 contre 4 pour les deux autres implémentations) et qu\u0027elle fasse donc forcément un plus long parcours, elle ne vérifie pas l\u0027ordre des mots et néglige donc des permutations et suppressions de tableau. On remarque néanmoins une précision générale assez basse due aux faux positifs générés par les propriétés paradigmatiques des mots contenus dans le thésaurus.\nLe tableau 5 compare la méthode retenue (l\u0027algorithme C avec un seuil de 5) avec une mé-thode d\u0027alignement basée sur la méthode de Bannard et Callison-Burch (2005) mais appliquée sur un corpus mono-lingue. Ces deux approches possèdent des performances similaires face à la détection de « copier/coller », environ 84% d\u0027efficacité, en revanche notre méthode montre de biens meilleurs résultats sur la détection des reformulations non paraphrastiques (un rappel de 0.80 contre 0.24 pour une méthode avec alignement). TAB. 5 -Evaluation de notre méthode par rapport à une méthode à alignement en fonction des types de similitudes à détecter.\nConclusions\nLa méthode retenue montre des résultats similaires aux méthodes avec alignement sur la détection de copies exactes et de paraphrases et se montre beaucoup plus robuste face aux reformulations. Néanmoins, sa précision est fortement impactée par le thesaurus utilisé, qui peut engendrer des faux positifs pour les raisons évoquées dans la partie 3.2 Extraction de mots clefs, et la segmentation, qui peut être faussée par du texte enrichi (tableau, liste à puces). Nous conviendrons également que cette technique est plutôt coûteuse en temps et en ressources (chargement du thesaurus en mémoire).\nUn seuil adaptatif évoluant en fonction de la taille des phrases pourra également être mis en place dans de futurs travaux. Pour des phrases standards comportant entre 8 et 15 mots, il sera préférable de fixer le seuil à 5, en revanche si la phrase excède la vingtaine de mots, il faudra définir le seuil entre 10 et 12 mots communs.\nAu final, cette approche reste naïve et gourmande aussi bien en temps qu\u0027en ressources, néanmoins elle permet de détecter des reformulations jusque là impossibles à détecter avec des méthodes conventionnelles à alignement et constitue donc une alternative intéressante. Elle est à privilégier pour la détection de reformulations non paraphrastiques.\n"
  },
  {
    "id": "250",
    "text": "Introduction\nLa plupart des logiciels anti-plagiat se concentrent sur une détection extrinsèque de plagiat, c\u0027est-à-dire sur le fait de trouver des similitudes entre un document et un corpus de sources probables. Or ce système est inutile si le document ayant été plagié ne se trouve pas dans le corpus fouillé. Néanmoins, il existe un autre type de détection, la détection intrinsèque qui exploite des données extraites de l\u0027intérieur même du document. La détection d\u0027auteurs par étude du style d\u0027écriture du document est la forme de détection de plagiat intrinsèque la plus répandue. Cette approche diverge selon les travaux car elle soulève plusieurs problèmes, allant du découpage du texte de façon pertinente, au choix et à la collecte des données stylistiques à surveiller, en passant par la manière de découper et de classer les différents passages du document par auteur. C\u0027est sur ce dernier point que l\u0027article va essentiellement se concentrer.\nLa détection d\u0027auteur 2.1 La notion de stylométrie\nLa stylométrie ou l\u0027étude stylométrique d\u0027un texte est une analyse à mi chemin entre une analyse linguistique et statistique. Elle exploite des variables stylométriques, qui sont des caractéristiques linguistiques du texte, afin d\u0027établir des statistiques sur le document étudié. Effectuer l\u0027analyse stylométrique d\u0027un document consiste à surveiller les variations du style d\u0027écriture du document en surveillant l\u0027évolution des variables stylométriques au sein de celuici afin d\u0027en détecter les irrégularités et ainsi pouvoir déterminer si certains passages, appelés phases stylistiques, sortent de la norme par rapport à la majorité du texte.\nÉtat de l\u0027art\nDès le XIX e siècle, Mendenhall (1887) suggère qu\u0027en analysant des caractéristiques internes d\u0027un texte on peut en reconnaître l\u0027auteur. Depuis, les techniques d\u0027études stylomé-triques de document ont fait d\u0027importantes avancées et de nombreuses recherches (Stein et Eissen, 2007;Layton et al., 2013;Jayapal et Goswami, 2013) appliquent cette découverte à la détection de plagiat. Certaines de ces recherches se concentrent sur l\u0027extraction et la surveillance des données stylométriques les plus pertinentes. Stein et Eissen (2007) ainsi que Zamani et al. (2014)    (Cheng, 1995), un algorithme multidimensionnel des k-moyennes non paramétrique.\nSegmentation\nDans un premier temps, l\u0027idée est de segmenter le document. Il est important que chaque segment conserve un sens afin d\u0027être autonome et donc d\u0027être potentiellement écrit par une personne différente. Une segmentation en unité de sens est donc à privilégier. S\u0027appuyant sur le travail de Zechner et al. (2009), c\u0027est une segmentation pseudo sémantique qui a été retenue : un découpage par phrase d\u0027une taille minimale (en mots). Le seuil a été fixé à 15 mots, taille moyenne des phrases dans la langue française.\nExtraction de la stylométrie\nLa seconde étape du processus consiste à extraire la stylométrie de chaque segment. Pour ce faire, il faut au préalable détecter la langue de chaque segment au moyen d\u0027un module im-plémentant la technique de catégorisation de texte à base de n-grammes de Cavnar et Trenkle (1994). Ensuite, l\u0027étiqueteur morphosyntaxique TreeTagger (Schmid, 1994)  \nConstruction des courbes\nUne fois la segmentation et les calculs stylométriques opérés, on obtient donc plusieurs valeurs par segment (i.e. une valeur par variable stylométrique). Une suite de valeurs brutes sans cohérence n\u0027étant pas exploitable, on représente la stylométrie du document sous la forme de courbes, avec en abscisse, la position des segments (la ligne de vie du document) et en ordonnée, les valeurs des variables stylométriques observées. Ceci a pour avantage, en plus de permettre une représentation visuelle, de faciliter la comparaison et la manipulation des valeurs entre elles, les algorithmes de manipulation de courbes étant courant.\nIl est possible que le style d\u0027un même auteur varie énormément au fil d\u0027un même texte. La fatigue ou la maturité lors de longs écrits peuvent entraîner du bruit ou des variations brusques. On convient alors qu\u0027un lissage est nécessaire. C\u0027est le lissage par la moyenne glissante sans pondération (Chou, 1975) qui a été utilisé dans cet article.\nRegroupement\nIl reste à déterminer les phases stylistiques de façon automatique. Un algorithme d\u0027apprentissage automatique non supervisé (i.e. sans intervention humaine) est idéal dans ce cas de figure qui s\u0027apparente au clustering car il faut déterminer à quel auteur (i.e. à quel cluster) chaque donnée s\u0027apparente. Sachant que le nombre d\u0027auteurs et donc de clusters n\u0027est pas connu à l\u0027avance, c\u0027est le Mean Shift multidimensionnel (Cheng, 1995) qui se dégage. En effet, cet algorithme permet de clustériser un ensemble de points sans connaître à l\u0027avance le nombre k de clusters. L\u0027idée dans notre cas est de déterminer le nombre k à partir d\u0027un seuil. On dé-finit alors empiriquement un nombre k de départ assez grand, admettons 10 et un seuil, entre 2% et 15% en fonction de la moyenne de la variable stylométrique observée (seuil adaptatif). Tant qu\u0027il existe deux clusters voisins avec une différence de stylométrie inférieure au seuil, on relance un KMeans avec k \u003d k ? 1. Une fois toutes nos phases identifiées et k définitif, s\u0027il existe deux clusters (non voisins cette fois-ci étant donné que les voisins ont déjà été réunifiés) avec une différence de stylométrie inférieure au seuil, on en déduit qu\u0027ils sont du même auteur.\nOn prend en considération plusieurs variables stylométriques en même temps, tout comme le fait van Halteren (2004). L\u0027idée est de surveiller plusieurs variables stylométriques afin qu\u0027elles se « complètent » mutuellement. On augmente ainsi le taux de certitude de l\u0027existence d\u0027une zone par le fait qu\u0027une zone est définie comme telle si la majorité des courbes fléchissent de telle façon à la dessiner. De plus, la zone de flexion retenue est maintenant désignée par la moyenne des zones de flexion de toutes les courbes surveillées, ceci réduisant considérable-ment l\u0027erreur d\u0027approximation et rendant plus sûr notre prise de décision. Pour exemple, sur la FIG. 1 -Mean Shift sur plusieurs variables stylométrique.\nfigure 1 chaque zone de flexion des courbes est représentée par une ligne verticale pointillée de la même couleur que la courbe dont elle dépend. Les lignes pointillées noires plus épaisses représentent les découpages retenus (les moyennes des trois flexions des trois courbes).\nPour faciliter l\u0027observation des écarts et des flexions, les courbes sur cette figure ont été normalisées (mises à la même échelle), leurs valeurs stylométriques sont donc faussées.\nÉvaluation et tests 4.1 La base de tests et protocole\nLa base de tests est composée de 500 textes contenant en moyenne 7 000 mots. Les textes sont constitués d\u0027un (l\u0027intégralité du texte) à cinq passages, chaque passage étant potentiellement écrit par un auteur différent. Un texte peut contenir plusieurs passages écrits par un même auteur. On recense en totalité dans la base, une dizaine d\u0027auteurs différents. La langue prédomi-nante au sein des textes est le français, cependant pour tester l\u0027adaptabilité et le plurilinguisme du système de nombreux passages sont en anglais ou en italien. Afin de tester correctement la procédure évaluée, des passages traitant du même sujet et donc employant le même vocabulaire ont été utilisés dans le but de tromper la stylométrie extraite. De plus, l\u0027intégralité des textes est annotée, de telle sorte à savoir précisément de quel mot à quel mot les textes sont écrits par un auteur ou par un autre.\nRésultats\nNotre procédure présente une précision de 0.89 et un rappel de 0.34. Il est néanmoins important d\u0027étudier plus en détails les limites de cette procédure et de nuancer un rappel si faible. La figure 2 est un diagramme à bulles représentant les performances du découpage stylomé-trique. L\u0027axe des abscisses représente la taille en segment de la phase stylistique concernée et celui des ordonnées l\u0027écart moyen de la variable stylométrique observé entre cette phase et ses voisines, son unité est notée us pour unité stylométrique. Les bulles représentent les différents \nConclusions\nNotre approche montre des résultats exploitables lorsque les phases stylométriques à identifier ne sont pas trop importantes (n\u0027excèdent pas 190 segments soit environ 4000 mots) et lorsque la différence de stylométrie est suffisamment grande (supérieur à 0.20 us). En revanche dans tous autres cas, les limites de notre approche se font ressentir. Pour palier ces problèmes, un seuil adaptatif pourra être défini en fonction du type de variable stylométrique surveillée. De plus, avec du recul, nous convenons qu\u0027un Mean Shift n\u0027était sans doute pas la meilleure option de clustering. Dans la suite de nos travaux nous implémenterons d\u0027autres classifieurs (hiérarchique, DBSCAN, etc.).\nPour conclure, bien que perfectible, cette approche permet de détecter différents styles d\u0027écriture au sein d\u0027un même texte et notre contribution malgré ses limites permet bien de regrouper automatiquement les phases stylistiques par auteur.\n"
  },
  {
    "id": "251",
    "text": "Introduction\nLe risque chimique ou alimentaire se manifeste lorsque les produits chimiques sont dangereux pour la santé et consommation humaine ou animale, et pour l\u0027environnement. Si certains produits et substances sont maintenant clairement identifiés comme dangereux (e.g. l\u0027amiante, l\u0027arsenic, le plomb), nos connaissances actuelles sur d\u0027autres substances sont moins complètes. Nous nous intéressons en particulier au risque alimentaire (e.g. l\u0027arsenic, les nitrates, la listeria, la dioxine) et au risque chimique (e.g. le bisphénol A, les phtalates). Ces substances entrent souvent dans la composition de produits courants et peuvent avoir l\u0027effet nuisible sur l\u0027organisme humain. Le contrôle sur la commercialisation de ces substances est effectué par des organismes sanitaires dédiés, comme EFSA (European Food Safety Authority) ou ANSES (Agence nationale de sécurité sanitaire de l\u0027alimentation, de l\u0027environnement et du travail). Les experts se retrouvent face à une littérature scientifique abondante et doivent l\u0027étudier pour avoir une base solide pour la prise de décisions. L\u0027objectif de notre travail consiste à proposer une aide automatique pour l\u0027analyse de la littérature scientifique afin de détecter les phrases indicatives du risque induit par ces substances. Nous abordons cette tâche comme une problématique de catégorisation : les phrases des textes doivent être catégorisées dans les classes du risque. Nous présentons les données (section 2) et approches utilisées (sections 3 et 4). Nous discutons ensuite les résultats obtenus et concluons avec les pistes pour les travaux futurs (section 5).\nNotre objectif est de catégoriser les phrases des corpus dans les classes de risque. L\u0027é-valuation est effectuée par rapport aux données de référence. Une liste de mots vides et des ressources linguistiques sont aussi utilisées. Le travail est effectué avec le matériel en anglais.\nCorpus. Les corpus proviennent de la littérature scientifique, qui est le matériel typique utilisé par les experts. Le corpus du risque chimique (80 000 occ.) contient le rapport sur le bisphénol A (EFSA Panel, 2010). Le corpus du risque alimentaire (\u003e240 000 occ.) a été constitué à partir de 115 documents officiels publiés entre 2000 et 2010 sur une dizaine de substances, comme l\u0027arsenic, la dioxine ou le nitrate (Blanchemanche et al., 2013). Trois sections (introduction, conclusion et résumé) sont traitées car elles comportent les résultats principaux.\nClassifications du risque. Les classifications du risque (alimentaire (Blanchemanche et al., 2013) et chimique (Maxim et van der Sluijs, 2014)) sont structurées hiérarchiquement et décrivent différents aspects révélateurs de la nocivité des substances chimiques :\n-significativité des résultats (The Panel concluded that the current NOAEL for BPA (5 mg/kg b.w./day) would be sufficiently low to exclude any concern for this effect) ; -hypothèse scientifique (Despite this lack of evidence, the possibility of poultry and egg consumption as an exposure route to HPAIV remains a concern to food safety experts). Le risque est présent lorsque la nocivité des substances est apparente dans la littérature scientifique, ou lorsque les expériences présentées montrent des imprécisions et incertitudes.\nRessources linguistiques. Des ressources linguistiques sont utilisées avec l\u0027approche par apprentissage supervisé pour enrichir l\u0027annotation. Nous supposons que ces différentes expressions, souvent liées à la notion d\u0027incertitude, sont indicatrices de la notion du risque chimique :\n-l\u0027incertitude (e.g. possible, should, may, usually) indique des doutes existant au sujet des résultats obtenus expérimentalement, leur interprétation, etc. ; -la négation (e.g. no, neither, lack, absent, missing) indique que de tels résultats n\u0027ont pas été observés, que l\u0027étude ne respecte pas les normes, etc. ; -les limitations (e.g. only, shortcoming, insufficient) indiquent des limites, comme la taille insuffisante de l\u0027échantillon traité, le faible nombre de tests ou de doses testées, etc. ; -l\u0027approximation (e.g., approximately, commonly, estimated) indique d\u0027autres insuffisances liées aux valeurs imprécises de substances, d\u0027échantillons, de doses, etc. Avec la recherche d\u0027information, nous utilisons des ressources pour l\u0027extension de requêtes :\n-101 805 paires de synonymes provenant de la langue générale (Fellbaum, 1998) et spé-cialisée (Grabar et Hamon, 2010), -des clusters de mots générés avec des méthodes distributionnelles à partir des corpus (Brown et al., 1992;Liang, 2005). Données de référence. Les données de référence sont obtenues grâce à l\u0027annotation par des spécialistes en évaluation du risque. Un expert a annoté 425 phrases couvrant 55 classes du risque chimique. Plusieurs experts ont participé dans l\u0027annotation du corpus du risque alimentaire et fournissent des données de référence pour 657 phrases monoclasses couvrant 27 classes et 389 phrases multiclasses, pour un total de 1 046 phrases annotées. Plusieurs des classes contiennent très peu de phrases annotées et nous gardons celles qui fournissent un nombre suffisant d\u0027exemples (minimum de 10 pour le risque alimentaire, 5 pour le risque chimique).\nListe de mots vides. La liste de mots vides contient 176 mots (e.g., \u0026 about again all almost and any by do to etc). Cette liste contient essentiellement des mots grammaticaux.\n3 Approche par apprentissage supervisé Méthode. Nous utilisons différents algorithmes de la plateforme Weka (Witten et Frank, 2005) avec le paramétrage par défaut. Les phrases sont l\u0027unité de travail. Nous visons la dé-tection de phrases liées au risque : (1) de manière générale G pour détecter les phrases relatives au risque ; (2) de manière précise D pour associer ces phrases aux classes de risque. Les descripteurs sont fournis par l\u0027annotation sémantique et linguistique : forms (les formes de mots comme elles apparaissent dans le corpus), lemmas (mots lemmatisés), lf (combinaison de formes et de lemmes), tag (les étiquettes morpho-syntaxiques des formes (e.g. noms, verbes, adjectifs)), lft (combinaison de formes, lemmes et étiquettes morpho-syntaxiques), stag (étiquettes sémantiques de mots (e.g. incertitude, négation, limitations)), all (combinaison de tous les descripteurs). Les descripteurs sont pondérés de trois manières : freq (fréquence brute des descripteurs), norm (fréquence normalisée par la taille du corpus), tfidf (pondération tfidf (Salton et Buckley, 1987)). Nous effectuons une validation croisée. Les mesures d\u0027évaluation sont la précision, le rappel et la F-mesure (moyenne harmonique de la précision et du rappel). Résultats. Les résultats présentés sont obtenus avec J48 (Quinlan, 1993). Avec l\u0027expéri-ence G, les performances avec le risque alimentaire (autour de 0,8) sont meilleures que celles du risque chimique (0,61-0,64). Les performances sont assez stables avec les différents descripteurs et pondérations. L\u0027exploitation de formes, d\u0027étiquettes sémantiques et les différentes combinaisons de descripteurs donnent des résultats légèrement supérieurs. Bien que très simplistes, les étiquettes morpho-syntaxiques (e.g. noms, verbes, adjectifs) sont assez efficaces sur les deux corpus. Les étiquettes sémantiques seules (stag) sont parmi les plus efficaces pour détecter le risque chimique, mais montrent une F-mesure assez faible pour le risque alimentaire. À la figure 1, nous présentons l\u0027expérience D avec les descripteurs lft (formes, lemmes et étiquettes morpho-syntaxiques). Les résultats sont élevés avec les classes du risque alimentaire et deux classes du risque chimique (Facteur d\u0027incertitude et Hypothèses scientifiques). Le tfidf donne de meilleurs résultats dans la plupart des cas, mais la pondération norm est aussi compétitive. Les descripteurs lft fournissent de meilleurs résultats que les autres descripteurs. Les résultats sont meilleurs avec le risque alimentaire, où il existe plus de données d\u0027apprentissage.\nApproche de recherche d\u0027information\nMéthode. Nous considérons les libellés des classes comme les requêtes et les phrases des corpus comme les réponses potentielles à ces requêtes. Nous exploitons le système de recherche d\u0027information Indri (Strohman et al., 2005), qui utilise un modèle probabiliste basé sur le champ aléatoire de Markov et offre plusieurs fonctionnalités, comme par exemple :\n-la racinisation (Porter (Porter, 1980) et de Krovetz (Krovetz, 1993)) réduit un mot à sa racine (e.g., suppression de pluriels et de chaînes finales comme -ment et -ique) ; -le et booléen (band) permet de combiner plusieurs mots clés ; -les fenêtres ordonnées ou non ordonnées permettent de spécifier l\u0027ordre des mots clés ; -la pondération (tfidf (Salton et Buckley, 1987) et okapi (Robertson et al., 1998)) permet de relativiser le poids des mots-clés ; -la pondération des synonymes (wsyn) permet d\u0027indiquer l\u0027importance des mots clés. Pour l\u0027expansion des requêtes, nous retenons les mots supplémentaires des ressources linguistiques (synonymes et clusters) si ces mots montrent au moins 0,3 % de précision. L\u0027évaluation est effectuée avec plusieurs mesures : précision, rappel, F-mesure et MAP (Mean Average Precision), cette dernière prenant en compte l\u0027ordre des réponses. Pour la baseline, les mots des libellés de classes sont utilisés, sans la racinisation ni l\u0027expansion de requêtes. Résultats. Le tableau 1 indique la MAP et la F-mesure de différentes expériences : baseline, utilisation de raciniseurs, pondération des mots clés et des clusters. Nous obtenons de meilleurs résultats avec les libellés du risque chimique, car ils sont plus explicites. La F-mesure est en général plus élevée que la MAP. Les résultats sont améliorés avec la racinisation, la pondération tfidf et okapi, et les clusters. Plusieurs autres expériences n\u0027ont pas été concluantes (e.g. exploitation des définitions, pondération des synonymes, fenêtres ordonnées des mots clés des requêtes, et booléen). Krovetz, la pondération et les clusters fournissent les meilleurs ré-sultats (figure 2). Les raciniseurs améliorent le rappel et donc les performances globales, tandis que l\u0027utilisation de la pondération des mots clés (okapi ou tfidf) améliore surtout les valeurs de la MAP : les phrases retournées sont alors les mêmes, mais leur ordre devient plus correct. Les ressources linguistiques supplémentaires sont favorables pour certaines classes. Elles permettent surtout d\u0027améliorer le rappel.\nDiscussion et Conclusion\nL\u0027apprentissage supervisé est plus performant que la recherche d\u0027information, tandis que cette dernière, étant moins supervisée, permet de traiter un plus grand nombre de classes. La recherche d\u0027information permet aussi de varier plus facilement les paramètres selon que l\u0027on voudrait privilégier la précision ou le rappel. La pondération montre toujours un effet favorable. Dans une expérience similaire avec le risque alimentaire, des résultats comparables aux nôtres sont obtenus (Blanchemanche et al., 2013). Notons que nous avons aussi testé une approche non supervisée à base de règles, qui montre des résultats très faibles : rappel quasinul pour une précision entre 0,5 et 0,6. Il existe plusieurs possibilités pour combiner les deux approches testées : combinaison des sorties pour augmenter le rappel ; le vote des approches pour améliorer la précision ; l\u0027utilisation des noeuds décisionnels des modèles d\u0027apprentissage supervisé pour l\u0027extension de requêtes ; l\u0027exploitation des sorties de recherche d\u0027information et du système à base de règles par l\u0027apprentissage supervisé.\nEn conclusion, nous utilisons l\u0027apprentissage supervisé et la recherche d\u0027information pour détecter des phrases relatives au risque induit par les substances chimiques. Nous abordons la tâche comme une problématique de catégorisation : les phrases des textes doivent être caté-gorisées dans les classes de risque. Deux corpus et deux classifications du risque sont utilisés. Les résultats par apprentissage automatique sont les plus performants. Les résultats indiquent aussi que l\u0027expression de l\u0027incertitude linguistique (e.g., likely, should, assume) est associée avec la notion du risque chimique. Dans les travaux futurs, nous allons tester d\u0027autres paramètres pour améliorer les performances des approches testées et nous allons combiner les résultats de ces approches de différentes manières. Ces résultats peuvent être utilisés par les experts travaillant sur la gestion du risque pour la prise de décisions et évalués par eux.\nRemerciements. Ce travail est soutenu par le projet PNRPE DICO-Risk.\nRéférences\nBlanchemanche, S., A. Rona-Tas, A. Duroy, et C. Martin (2013). Empirical ontology of scientific uncertainty : Expression of uncertainty in food risk analysis. In Society for Social Studies of Science, pp. 1-27.\n"
  },
  {
    "id": "252",
    "text": "Introduction\nCes dernières années, nous assistons à la sémantisation des données statiques et dynamiques (flux de données). Toutefois, vu la spécificité de ces derniers ni les technologies du web sémantique ni celles des Systèmes de Gestions de Flux de Données (SGFD) ne peuvent les traiter. Pour ce faire, les chercheurs proposent aujourd\u0027hui de nouveaux systèmes tels que C-SPARQL (Barbieri et al., 2010), CQELS (Phuoc) et SPARQL Stream (Calbimonte et al.). Lorsque le débit du flux en entrée de ces systèmes dépasse les seuils supportés, deux solutions existent : 1-Allouer au système autant de ressources que nécessaires (Hoeksema et Kotoulas, 2011)  manière, nous préservons le niveau sémantique de l\u0027information et protégeons la cohérence des données du graphe en mémoire.\nConclusion\nNous avons proposé dans cet article une approche orientée graphe pour la réduction de charge des systèmes de traitement de flux de données sémantiques. Notre approche, permet d\u0027améliorer la qualité des résultats des requêtes des systèmes de traitement de flux de données sémantiques, en protégeant la sémantique et la cohérence des données de ces flux, contrairement à une application naïve de l\u0027approche orientée triplet RDF utilisée jusqu\u0027à présent.\n"
  },
  {
    "id": "253",
    "text": "Introduction à Sélection basée sur le Degré de Pertinence\nLes librairies digitales sont actuellement très répandues. Elles renferment des quantités d\u0027informations énormes et nécessitent des mécanismes efficaces d\u0027indexation et de manipulation. Les moteurs de recherche du type général ne peuvent pas les indexer car ils exigent que l\u0027information qu\u0027ils manipulent soit composée d\u0027entités indépendantes. Dans le besoin de traiter rapidement et efficacement les requêtes, des méthodes basées des approches diffé-rentes ont été inventées. On rencontre alors, des méthodes se basant sur les réseaux bayésiens comme CORI Callan et al. (1995), d\u0027autres méthodes qui se basent sur les statistiques TF*IDF. Il existe aussi des méthodes qui se basent sur le modèle de langage et la pseudo-pertinence. Ces méthodes utilisent des résultats déjà obtenus pour de réponses futures. Puisque le modèle centralisé souffre du problème de passage à l\u0027échelle, certaines méthodes ont été mises pour tourner sur les systèmes pair-à-pair. La méthode CORI a été une source d\u0027inspiration et a été utilisée comme moyen de classification dans beaucoup de travaux. Cette méthode fonctionne sur un système bayésien pour localiser des réponses probables aux utilisateurs. La fonction de score donnée dépend de certains paramètres obtenus à partir d\u0027expérimentations sur des datasets. Ce paramétrage fait que CORI est devenue instable. Ces paramètres doivent être réajustés pour chaque nouvelle collection. Afin de réduire le nombre de collections interrogées, Abbaci et al. (2002) présente la méthode CS. Celle-ci définit ndoc le nombre de documents à retourner et tient compte uniquement des deux premiers termes lors de l\u0027évaluation des requêtes longues. Bien que l\u0027objectif de réduction de flux est atteint, CS produit des faux positifs et faux négatifs importants à cause des restrictions imposées. Soit un système distribué où un serveur appelé courtier est lié à un ensemble de serveurs. Le courtier détient un index Terme/Serveur qui indique pour chaque terme t i la liste des serveurs qui le manipulent. Chaque serveur S i est responsable d\u0027une collection de documents c i et manipule un index Terme/Documents. Cet index définit pour chaque terme t i la liste des documents où il figure. Par cette définition, le courtier sélectionne de façon déterministe le sous-ensemble de serveurs pertinents. Ces index permettent de réduire la charge du système. Un document est jugé pertinent s\u0027il partage au moins un terme avec la requête. Plus un document partage de termes avec la requête plus son degré de pertinence s\u0027élève, induisant ainsi que le score d\u0027une Sélection basée sur la pertinence\ncollection est proportionnel aux nombre de documents pertinents qu\u0027elle contient. Sur cette définition, pour une requête q, le score d\u0027une collection c i se calcule selon la fonction SDP suivante :\nT F ti est la fréquence du terme t dans la collection c i . L\u0027expérimentation des trois méthodes sur le dataset Reuters21578, sur un système distribué. La figure Fig. 1 présente la comparaison entre les trois méthodes en fonction du recall. Nous avons réalisé des expéri-mentations intensives en faisant varier le Top-k. Nous remarquons que les valeurs pour cette métrique sont plus grandes dans SDP que dans les autres méthodes. CS (CS2 pour ndoc\u003d2, CS3 pour ndoc\u003d3, CS5 pour ndoc\u003d5) a retourné un recall plus faible. C\u0027est certainement à cause du ndoc qui influence la recherche. Avec ndoc\u003d2 et 3 ; le recall n\u0027atteint pas 1 c-à-dire il existe des documents pertinents et rares où le système n\u0027arrive pas à les sélectionner. CORI c\u0027est placé au-dessus de CS.\n"
  },
  {
    "id": "254",
    "text": "Introduction\nAvec les avancées technologiques en terme d\u0027acquisition des données scientifiques (images satellitaires, capteurs, etc.), les scientifiques s\u0027intéressent de plus en plus à des applications importantes en terme de surveillance et suivi de l\u0027environnement. Les données collectées sont généralement hétérogènes, multiéchelles, spatiales et temporelles (série temporelle d\u0027images satellites, aériennes, modèles numériques de terrain, nature du sol ...) et sont destinées à comprendre et prédire des phénomènes résultant de processus complexes et d\u0027origine pluridisciplinaire (données climatiques, géologiques, ...). L\u0027explosion de cette information spatiale, temporelle et des systèmes d\u0027informations géographiques nécessitent l\u0027investissement dans des méthodes d\u0027extraction de connaissances et nous nous intéressons à celles qui reposent sur la détection de motifs locaux comme, par exemple, la découverte de motifs séquentiels (Agrawal et Srikant, 1995;Mannila et al., 1997;Masseglia et al., 1998) ou de motifs plus complexes comme des sous-graphes Inokuchi et al. (2000) ou des sous-arbres Zaki (2002). Nos besoins concernent l\u0027étude spatiale et temporelle des évolutions d\u0027objets et de leurs interactions. Les objets peuvent être caractérisés par plusieurs attributs et leurs évolutions que l\u0027on appelle parfois dynamique se décrivent par les évolutions des attributs, par leur emplacement géographique, leur existence (apparition/disparition) et leur structure topologique (fusion/division). Pour certaines applications, nous pouvons transformer la base de données spatio-temporelles dans une base de données transactionnelles Hai et al. (2012) ou dans une base de séquences pour les analyser. Cependant, ces transformations peuvent s\u0027avérer très fastidieuse et les résultats peuvent être difficilement interprétables. Des domaines de motifs plus sophistiqués et applicables à l\u0027étude de phénomènes spatio-temporels ont donc été proposés. Ainsi, plusieurs travaux se sont intéressés à l\u0027extraction de motifs dans des graphes étiquetés Inokuchi et al. (2000). Quelques travaux ont été menés dernièrement sur des graphes attribués (Fukuzaki et al., 2010;Miyoshi et al., 2009;Desmier et al., 2013). Les difficultés dans la fouille de graphes attribués résident dans l\u0027explosion combinatoire de l\u0027exploration de l\u0027espace de recherche. En effet, cette espace de recherche porte à la fois sur les combinaisons de graphes et les combinaisons d\u0027attributs. Dans un travail présenté dans (Sanhes et al. (2013a,b)), Sanhes et al. ont proposé de travailler à la modélisation de données spatio-temporelles dans des DAG attribués, autrement dit un unique graphe orienté acyclique attribué (a-DAG) (cf. Figure 1) : les sommets sont des objets spatiaux caractérisés par un ensemble d\u0027attributs ou caractéristiques et les arcs dénotent la proximité spatio-temporelle entre ces objets (par exemple le voisinage spatial entre deux objets de deux pas de temps consécutifs). Le but est de trouver les transitions ou cheminements de caractéristiques pouvant montrer une tendance attendue ou surprenante, expliquer un phénomène particulier, ce qui revient à chercher dans un a-DAG les chemins fréquents d\u0027attributs. On trouve quelques travaux s\u0027attaquant à la fouille de graphes orientés FIGURE 1: Exemple de a-DAG construit sur des objets représentés dans des images temporelles acycliques mais étiquetés (et non pas attribués) tels que Chen et al. (2004);Termier et al. (2007). Ces méthodes recherchent des sous-graphes dans un ensemble de graphes, et de plus les sommets sont plutôt labélisés ou considérés comme labélisés et non attribués. Dans notre cas, on est en présence d\u0027un seul graphe orienté acyclique et attribué (a-DAG) ce qui pose des problèmes très différents. Le domaine de motif proposé pour la première fois dans Sanhes et al. (2013b) est appelé domaine des chemins pondérés dans un a-DAG. Lorsque nous avons voulu travailler à une implémentation efficace de l\u0027algorithme d\u0027extraction de ce type de motif, le seul à notre connaissance qui calcule des motifs dans des DAG attribués, nous avons étudié de près ses propriétés et nous avons découvert qu\u0027il était juste mais incomplet. Nous présentons ici un nouvel algorithme permettant de réaliser l\u0027extraction de tels motifs de façon complète. Non seulement nous proposons une correction du premier algorithme mais aussi nous étudions des optimisations nécessaires au passage à l\u0027échelle en introduisant des structures de données complémentaires comme un graphe de motifs. Nous montrons que la performance de l\u0027extraction est améliorée de plusieurs ordres de magnitude sur des jeux de données artificiels et nous l\u0027appliquons aussi à des données réelles pour motiver qualitativement l\u0027usage des chemins pondérés.\nDans la section suivante, nous présenterons les concepts et définitions nécessaires à la compréhension de l\u0027algorithme. En Section 3, nous prouvons l\u0027incomplétude de l\u0027algorithme existant. Nous proposons une solution de complétude et une optimisation basée sur une structure de graphe de motifs en Section 4. Nous montrerons les performances de l\u0027algorithme complet et optimisé sur des jeux de données artificielles en Section 5. Et enfin, nous conclurons en Section 6. \nUn chemin P est une séquence d\u0027itemsets P \u003d P 1 2 · · · n tel qu\u0027il existe un chemin O \u003d v 1 2 · · · n dans le graphe où P i est inclus dans l\u0027ensemble des items de v i (notion à différentier de la définition classique d\u0027un chemin dans un graphe). On dit que alors que O est une occurrence du chemin P et l\u0027ensemble des occurrences de P est noté occu G (P ).\nPar exemple dans le graphe de la figure 2 les occurrences du chemin de taille 3 ah sont 2 3 6 , 2 3 8 , 2 4 7 , 2 5 7 , et 5 7 8 . Un chemin pondéré P est un chemin où un poids est associé à chaque arc P i i+1 constituant P . Ce poids correspond au nombre d\u0027occurrences distinctes de P i i+1 dans le graphe. Pour l\u0027exemple précédent, le chemin ah et ses occurrences permettent de construire le motif pondéré : ah 4 cd 5 i. En effet, le nombre d\u0027occurrences de ah dans occur G (P ) st 4, et le nombre d\u0027occurrences de cd dans occur G (P ) est 5. Une telle représentation permet de voir que l\u0027itemset ah apparaît 4 fois avant l\u0027apparition du chemin cd et que l\u0027itemset i apparaît 5 fois après l\u0027apparition du chemin ah Dorénavant, ? G (P i i+1 ) désignera le poids de l\u0027arc entre les itemsets\nRelation d\u0027inclusion L\u0027opérateur d\u0027inclusion sur un couple de chemins pondérés est défini de la manière suivante :\nAutrement dit, P est inclus dans P\u0027 s\u0027il existe une sous-séquence Q de P\u0027 tel que les itemsets de P sont inclus un à un dans ceux de Q avec les mêmes poids au niveau des arcs. On dit que P est un sur-chemin ou super-chemin pondéré de P . A partir de la mesure proposée par Bringmann et Nijssen (2008), nous définissons le support d\u0027un chemin pondéré P, noté ?(P ), comme étant le poids minimal de ses arcs.\nChemin pondéré condensé Un chemin pondéré P est un condensé s\u0027il n\u0027admet aucun surchemin pondéré. Pour simplifier, nous appellerons motif un chemin pondéré et les occurrences d\u0027un motif seront tout simplement des chemins.\nCette méthode permet bien l\u0027extraction des motifs condensés de manière juste mais ne les génère pas tous. En effet, toutes les graines condensées forment bien des motifs condensés mais un motif condensé de taille supérieure à 2 peut contenir des graines non condensées. Effectivement, il existe des motifs condensés au sens de l\u0027inclusion qui peuvent être formés par certains motifs de taille 2 qui ne sont pas générés par la première étape.\nPour illustrer l\u0027incomplétude de l\u0027algorithme, nous montrons un contre-exemple sur le graphe de la figure 3. Dans ce a-DAG, les graines générées par la première étape de l\u0027algorithme ne permettent pas de construire le motif condensé a 1 bc 1 de.\n:a 2 :a\nCondensés représentés dans le a-DAG :\n• chemins de tailles 2 : a de où ? ne vaut plus 2 car toutes les occurrences de P ne sont pas utilisées : 2 4 n\u0027est pas relié à de. Dans ce cas on parle d\u0027extension avec perte d\u0027occurrences. Par conséquent il faut déterminer les occurrences utilisées et mettre à jour les différents poids (?) ainsi que les itemsets du motif. En réalité avec la séparation des 2 étapes, l\u0027information structurelle est perdue pendant le parcours en profondeur.\nAlgorithme complet et optimisé\nDans ce paragraphe, nous proposerons une solution permettant de corriger la complétude et nous proposerons par la même occasion une version optimisée utilisant une structure de graphe permettant de stocker les motifs que l\u0027on appellera graphe de motifs.\n... ?n I n un motif. Nous pouvons étendre P sans perte d\u0027occurrences s\u0027il existe un itemset I n+1 fermé fréquent dans l\u0027ensemble des sommets accessibles par V n (nous appellerons I n+1 un fermé fréquent local à V n ), tels que V n+1 supporte I n+1 et qu\u0027il existe au moins un arc de chaque sommet de V n vers V n+1 , c\u0027est-à-dire que toutes les occurrences de P sont conservées. De manière analogue, nous pouvons étendre P avec perte d\u0027occurrences lorsque toutes les occurrences de P ne sont pas utilisées lors de l\u0027extension. Dans ce cas nous obtenons un motif P \u003d I 1 ? 1\n... \nStratégie de l\u0027algorithme complet\nÀ partir des notions introduites précédemment, nous pouvons présenter la stratégie géné-rale de l\u0027algorithme complet (cf. algorithme 1). Cet algorithme est basé sur un parcours en profondeur de l\u0027espace de recherche pour étendre les motifs condensés.\nEn partant de l\u0027ensemble des sommets du graphe, l\u0027algorithme effectue un parcours en profondeur dans l\u0027espace de recherche pour étendre le motif condensé P initialisé à ?. La ligne 1 exprime le cas d\u0027arrêt de l\u0027algorithme : l\u0027ensemble des sommets destinations V P est vide. Le parcours en profondeur se fait aux lignes 2 et 3. L\u0027extension de P se fait avec l\u0027itemset Y fermé fréquent par rapport aux sommets de V P (ligne 4). Nous notons P le motif ainsi étendu. Lorsqu\u0027il s\u0027agit d\u0027une extension avec perte d\u0027occurrences, il est nécessaire de mettre à jour les occurrences du motif (ligne 5). Ce nouveau motif P est potentiellement un motif ou un sous-motif condensé. Nous supprimons les motifs qui sont inclus dans P , et insérons P dans C l\u0027ensemble des motifs condensés (lignes 7 et 8). Puis nous continuons le parcours (ligne 9).\nLa complexité de l\u0027algorithme ne permet pas le passage à l\u0027échelle à cause de nombreux tests coûteux d\u0027inclusion de motifs pour vérifier sa maximalité. Nous proposons ci-dessous une implémentation optimisée basée sur une structure de graphe pour stocker les motifs. Cette nouvelle structure va permettre d\u0027éviter les tests trop coûteux de comparaison entre motifs.\nAlgorithme 1 : DepthFirstMining\nEntrées : P motif courant (? à l\u0027appel initial) V P ens. des sommets destinations de P (V P \u003d V G à l\u0027appel initial) C ens. des motifs condensés (? à l\u0027appel initial) Mettre à jour les occurrences de P 6\nSupprimer dans C les motifs Q inclus dans P 7\nInsérer P dans C.  \nImplémentation optimisée\nNous allons nous servir de la maximalité des chemins pondérés condensés recherchés pour optimiser l\u0027algorithme qui se traduit par la maximalité des itemsets (itemsets fermés) du chemin et sa taille. Les tests d\u0027inclusions de l\u0027algorithme se font sur les itemsets et sur les chemins. Pour éviter ces tests nous allons définir une structure de graphe permettant le stockage des motifs trouvés au fur et à mesure du parcours de l\u0027espace de recherche. Cette structure est appelée graphe de motifs. • V m ? P(V ) est l\u0027ensemble des sommets • E m ? P(V ) × P(V ) est l\u0027ensemble des arcs • ? m : la fonction d\u0027attributs définie par : V m ?? P(I) V ?? X où X représente l\u0027itemset maximal caractérisant les sommets de V . Réciproquement, on associe à un itemset X l\u0027ensemble des sommets noté V X supportant l\u0027itemset X. Un sommet du graphe G m est identifié par un ensemble de sommets du graphe G. Un motif condensé est alors un chemin c \u003d V 1 n de G m de longueur maximale (cf. figure 5), i.e. V 1 est une source (pas d\u0027arc incident) et V n est un puits (pas d\u0027arc sortant).\nProcédure cherCondRec(X : itemset, V X : ens. de sommets, min_sup : entier) sommets_a_remonter : var. globale contenant les sommets à backtracker\nBdT (E + (V X )) la base de données transactionnelles construites à partir de ...\n?n?1 I n , c\u0027est une séquence d\u0027itemsets et chaque itemsets I i du motif P représente un sommet dans le graphe des motifs qui n\u0027est autre que V Ii ensemble des sommets du graphe a-DAG contenant l\u0027itemsets I. Le motif P est alors identifié de manière unique par la séquence V I1 In dans le graphe des motifs. Au moment de la construction du graphe des motifs et à l\u0027insertion d\u0027un nouveau sommet V i dans le graphe des motifs, il suffit de vérifier s\u0027il est déjà présent dans le graphe des motifs alors il a déjà été parcouru sinon il est inséré et le parcours de l\u0027espace de recherche continue. L\u0027algorithme final se déroule en 2 grandes étapes suivantes : -Recherche des motifs condensés par un parcours en profondeur de l\u0027espace de recherche (procédure 2). Les motifs sont stockés dans le graphe des motifs. Pendant la recherche, les sommets pour lesquels il y a eu extension avec perte d\u0027occurrences sont marqués pour être traités par la phase de backtracking.\n-Phase de backtracking sur les sommets marqués (pour lesquels il y a eu extension avec perte d\u0027occurrences) pour mettre à jour les occurrences des motifs (cf. procédure 3). La première étape fait appel à la procédure cherCondRec qui effectue un parcours en profondeur de l\u0027espace de recherche. Cette procédure étend récursivement les motifs condensés au fur et à mesure de leur construction dans G m . A une étape de la construction du motif P , soit V X le sommet à étendre dans le graphe des motifs G m supportant l\u0027itemset X, on calcule V X utile ensemble de tous les sommets ayant au moins un arc sortant de V X . On calcule tous les items accessibles par X, on obtient une base de données transactionnelles dont les transactions sont les arcs sortants et les items sont les items accessibles par X (ligne 3 et 4). Pour chaque itemset maximal Y dans cette base transactionnelle, on va étendre V X par V Y . Deux cas se présentent :\n• V X \u003d V X utile : tous les sommets de V X sont utilisés pour l\u0027extension, il y a extension sans perte. Il suffit alors de créer le sommet V Y et le relier à V X par un arc dans G m .\n• V X \u003d V X utile : l\u0027extension est réalisé avec perte d\u0027occurrences, on duplique le motif P en remplaçant le sommet V X par V X utile (à marquer pour être traiter dans la phase de Backtracking). On insère un nouveau sommet V Y et on crée un arc entre\nProcédure backtrackRec(V X utile : sommet du graphe des motifs, V toBacktrack : sommet du graphe des motifs, min_sup : entier)\nSoit V i utile ensemble des sommets de V i ayant au moins un arc sortant vers V X utile .\n4\nInsérer V i utile ? \u003e V X utile dans le graphe des motifs G m 5 // Lors de l\u0027insertion, mise à jour des attributs. backtrackRec(V i utile , V i )// backtracking vers le haut 6 L\u0027étape de backtracking décrite par la procédure récursive backtrackRec retraite chaque sommet marqué en visitant la branche du motif dans le sens inverse pour mettre à jour les occurrences des motifs et les informations tels que les poids et les itemsets. La figure 6 montre le déroulement de l\u0027algorithme sur l\u0027exemple du graphe de la figure 5. Les arcs du graphe de motifs sont en bleu. Les arcs en rouge définissent le cas d\u0027extension avec perte d\u0027occurrences, les arcs en vert montrent la phase de backtracking avec la mise à jour des poids et des itemsets.\nExpérimentations et résultats\nNous avons appliqué la méthode sur des jeux de données artificielles pour montrer la performance que nous avons comparé avec la méthode incomplète. Pour montrer l\u0027intérêt de ce nouveau domaine de motifs, nous avons utilisé un jeu de données réelles pour le problème de suivi du phénomène de l\u0027érosion.\nDans un premier temps, nous avons créé artificiellement trois jeux de données afin d\u0027observer l\u0027impact de la taille des a-DAG sur les performances notés « V20K, E60K » pour un a-DAG   données artificiel ressemble plus aux jeux de données tirés d\u0027une application spatio-temporelle.\n"
  },
  {
    "id": "255",
    "text": "Introduction\nLa quantité d\u0027information dans le Web a augmenté ces dix dernières années. Ce phénomène a favorisé la progression de la recherche dans le domaine des systèmes de recommandation. Les systèmes de recommandation consistent en un filtrage de l\u0027information dans le but de ne présenter aux utilisateurs que les éléments qui sont susceptibles de l\u0027intéresser, quel que soit le domaine. Les éléments à recommander sont également appelés items et peuvent être de différents types : des produits, services, informations, etc. Les systèmes de recommandation se doivent de sélectionner les informations les plus intéressantes en fonction du but recherché, tout en conciliant nouveauté, surprise et pertinence. Un système de recommandation se base sur des caractéristiques de références acquises de manière automatisée selon plusieurs méthodes différentes. Les caractéristiques de références peuvent provenir de : -L\u0027item (l\u0027objet à recommander) lui-même, on parle alors « d\u0027approche basée sur le contenu » (ou content-based approach) Balabanovi´cBalabanovi´c et Shoham (1997). Le filtrage basé sur le contenu calcule la similarité entre les objets afin de trouver l\u0027objet le plus semblable aux goûts de l\u0027utilisateur. Dans ce cas, l\u0027utilisateur se voit recommander des items similaires à ceux qu\u0027il a préférés dans le passé. -L\u0027utilisateur et l\u0027environnement social, on parle alors « d\u0027approche de filtrage collaboratif » (ou collaborative filtering). Le principe du filtrage collaboratif Breese et al. (1998)   Koren et al. (2009). Pour remplir leurs fonctions, les technologies de recommandation font aujourd\u0027hui face à des défis scientifiques majeurs. Comment intégrer l\u0027hétérogénéité des sources d\u0027information pour modéliser les préférences, comment prendre en compte le contexte, comment traiter efficacement ces masses d\u0027information, quels types d\u0027interfaces faut-il considérer ? Par ailleurs, les deux approches citées précédemment présentent des inconvénients principalement liés au démarrage à froid et à la montée en charge du système d\u0027où la nécessité de mettre en place des algorithmes performants et robustes. Ceci est l\u0027objectif de cette étude en vue d\u0027amélio-rer la qualité des systèmes de recommandation en introduisant de la sémantique aux données et en distribuant les traitements afin de minimiser les temps de calcul. La sémantique est ici représentée par une ontologie du domaine (domaines des films pour les expérimentations).\nArchitecture globale\nAfin de fournir une généricité dans le domaine d\u0027application, un passage à l\u0027échelle et une recommandation précise, nous proposons un système à trois couches : une couche de pré-analyse, une couche sémantique et une couche de recommandation.\nLe module de pré-analyse met en oeuvre un filtre de comptage afin d\u0027étudier en profondeur l\u0027intérêt implicite des utilisateurs : le filtre permet de compter le nombre de fois qu\u0027une valeur d\u0027un attribut figure parmi les items évalués par les utilisateurs, pour cela nous utilisons un filtre de Bloom. Un filtre de Bloom Broder et Mitzenmacher (2004) est un tableau de bits qui permet de tester d\u0027une manière rapide l\u0027appartenance d\u0027un élément à un certain ensemble. Le FBC, décrit dans Broder et Mitzenmacher (2004) est une extension du filtre de Bloom standard qui fournit la possibilité de supprimer des éléments du filtre. Le vecteur de bits y est remplacé par un tableau d\u0027entiers, où chaque position est utilisée comme compteur. L\u0027insertion d\u0027un élément est réalisée en incrémentant de 1 les entiers aux positions renvoyées par les fonctions de hachage. Le retrait est réalisé en décrémentant de 1 ces entiers. La question d\u0027appartenance d\u0027un élément au filtre est traitée en regardant si tous les entiers aux positions renvoyées par les k fonctions de hachage sont strictement positifs. Nous proposons de se baser sur l\u0027ontologie du domaine afin d\u0027extraire les attributs des items. Ensuite, nous utilisons les filtres de bloom avec compteur afin de stocker l\u0027intérêt implicite des utilisateurs dans les attributs des éléments. Ceci se fait en suivant les étapes : (1) Pour chaque utilisateur, nous créons un filtre de bloom avec compteur vide, (2) pour chaque élément noté par cet utilisateur, nous extrayons ses attributs et enfin (3) nous insérons ces attributs dans le filtre . Ainsi, le filtre contient tous les attributs des items précédemment notés par l\u0027utilisateur.\nLe module sémantique exploite l\u0027ensemble des données ainsi que l\u0027ontologie dans le but de définir les relations entre les utilisateurs, les items et les attributs. Ceci se traduit par la transformation sémantique des notes des utilisateurs. Nous nous intéressons tout d\u0027abord au nombre d\u0027occurrence des attributs qui ont été notés par un utilisateur. Nous appelons cette occurrence « la fréquence d\u0027apparition » ou « coïncidence » : cette valeur correspond au nombre de fois que les valeurs des attributs se répètent dans les items notés par l\u0027utilisateur. Cette valeur est extraite à partir des filtres de bloom avec compteurs.\nLa deuxième étape consiste à calculer la valeur sémantique (SV) en se basant sur la fré-quence d\u0027apparition. L\u0027équation utilisée est la suivante (1).\nAvec F le nombre total des attributs, N u le nombre total des items notés par l\u0027utilisateur \"u\". C j est la fréquence d\u0027apparition de l\u0027attribut j dans l\u0027ensemble des items qui ont été notés par l\u0027utilisateur et W j étant le poids calculé à partir de la phase de la sélection des attributs par une analyse des composantes principales.\nE[r u, * ] est la moyenne des notes de l\u0027utilisateur et r u,i est la valeur du rating initial donnée à l\u0027item \"i\". L\u0027utilisation de N permet de normaliser l\u0027équation sémantique. Cette équation a l\u0027intérêt de pouvoir prendre en compte des valeurs positives et/ou négatives comme note.\nL\u0027équation sémantique peut être appliquée à deux niveaux dans la recommandation. D\u0027une part, nous pouvons appliquer cette équation à toutes les notes disponibles dans la base de données initiale, ce qui permet de mieux expliquer l\u0027intérêt des utilisateurs pour les caractéristiques définissant les items notés (ajouter du sens à la note). D\u0027autre part, nous pouvons faire le choix d\u0027appliquer l\u0027équation sémantique à la sortie de la recommandation. Supposons que le module de recommandation renvoie un résultat des top K items (les K items les plus pertinents) pour un utilisateur donné, avec une estimation de la note pour ces top K. Ces notes seront transformées en une note sémantique suivant l\u0027équation (1) et les items proposés seront réordonnés en conséquence en top K\u0027, K\u0027 pouvant être inférieur ou égal à K.\nEnfin, le module de recommandation utilise une technique de filtrage collaboratif basée sur une méthode de factorisation de la matrice pour générer des recommandations précises. Nous avons fait le choix d\u0027utiliser la factorisation de matrice car cette technique a montré son efficacité comme méthode de filtrage collaboratif pour la recommandation Koren et Bell (2011 Nous avons testé le module sémantique au travers des deux approches : -Application du module sémantique au jeu de données (Semantic dataset). Dans cette approche, l\u0027application du module sémantique (que nous appelons \"sémantisation\") porte sur les données en entrée du système de recommandation. Il s\u0027agit donc de traiter l\u0027ensemble du jeu de données, avant son analyse par le système de recommandation et le filtrage collaboratif. Cette approche nous permet de retrouver des objets non pris en compte a priori. Cependant, puisque le module sémantique doit analyser tout le jeu de données, le temps de calcul peut être grandement augmenté par rapport à une analyse non sémantique. -Application du module sémantique au résultat de la recommandation Top K (semantic top K). Dans cette première approche, nous cherchons à \"sémantiser\" les données en sortie du système de recommandation. Le système de recommandation fournit classiquement une liste de K objets, ordonnés par ordre décroissant de préférence. La sé-mantisation réordonne ces objets, et fournit une nouvelle liste plus pertinente. Cette approche est extrêmement légère, et permet d\u0027améliorer la recommandation sans (trop de) perte de temps. De plus, avec cette approche, les paramètres de pondérations peuvent être personnalisés par l\u0027utilisateur plutôt que de considérer l\u0027ensemble du jeu de données. Néanmoins, une analyse a priori des données de l\u0027utilisateur est nécessaire avant de personnaliser les paramètres de pondérations. Dans la section suivante, nous présentons comment ces deux approches modifient les recommandations, et la qualité des résultats obtenus.\nF-Mesure\nCette métrique est généralement utilisée dans l\u0027évaluation des systèmes de recommandations. Cette métrique n\u0027évalue pas la qualité de la prédiction des notes, mais la pertinence des items qui sont proposés aux utilisateurs. La F-mesure est une façon courante de combiner le rappel et la précision dans une seule métrique afin de faciliter la comparaison. le rappel étant la probabilité qu\u0027un item choisi soit pertinent et la précision calcule la probabilité qu\u0027un item pertinent soit choisi. tel que nous pouvons le constater dans la figure 1, nos approches donnent\nFIG. 2 -ILS in \"genre\" attribute.\nde meilleurs résultats que la technique de matrice de factorisation (SVD++) sans sémantique. l\u0027amélioration est plus prononcée pour le cas de la\" sémantisation\" du jeu de données.\nILS\nILS (Intra-List Similarity), appelée également ILD (Intra-List Diversity) mesure la diversité/similarité entre les items dans la liste des top-k présentée à l\u0027utilisateur. Un bon système de recommandation doit trouver l\u0027équilibre entre ces deux concepts diversité et similarité. En effet, des items trop diversifiés peut provoquer une confusion chez l\u0027utilisateur, alors que recommander toujours les mêmes items peut ennuyer celui-ci. Le figure 2 représente cette mesure en se concentrant sur le attribut genres des films dans le top-k. Des valeurs élevées correspondent à une grande similarité. Ainsi, nous pouvons constater que notre approche permet de retourner des items plus similaires dans le top-k. Ceci est du au fait que nous prenons en compte l\u0027intérêt pour les attributs afin d\u0027identifier les items susceptibles d\u0027intéresser l\u0027utilisateur.\nConclusion\nNous avons proposé un système de recommandation qui repose sur deux concepts : relations sémantiques entre les données manipulées et un filtrage collaboratif basé sur la factorisation des matrices. Dans le but d\u0027améliorer la pertinence des recommandations, nous avons étudié en profondeur l\u0027intérêt implicite des utilisateurs pour les attributs des items. pour cela, nous appliquons une équation sémantique permettant de modifier les notes initiales des utilisateurs pour refléter leur intérêt pour les items.\nNotre système de recommandation opère en plusieurs étapes : Nous comptons le nombre de fois qu\u0027un attribut figure dans les items notés par les utilisateurs. Pour cela, nous nous sommes appuyés sur l\u0027utilisation d\u0027un filtre de bloom avec compteur. Ensuite, après passage par le module sémantique (transformation des notes des utilisateurs en appliquant l\u0027équation sémantique) , les recommandations sont générées en utilisant une technique de matrice de factorisation (SVD++).\nL\u0027approche proposée dans ce papier a montré un intérêt pour la recommandation de films en utilisant le jeu de données MovieLens combiné à une ontologie de films, peuplé par les données de IMDB. Nous avons fait le choix de travailler avec ce jeu de données car il est disponible et public et c est celui qui est le plus utilisé dans les expérimentations autour des systèmes de recommandation. Toutefois, nous avons conçu notre approche indépendamment du jeu de données que nous avions utilisé. Notre approche peut être utilisée comme une boite noire nécessitant de se connecter à une base de données des ratings disponibles, mais aussi à l\u0027ontologie du domaine de l\u0027application. Nous envisageons de tester notre approche sur des jeux de données provenant d\u0027autres applications telles que la recommandation nutritionnelle ou de tourisme. Nous envisageons également d\u0027intégrer dans notre approche la prise en compte des notes négatives (attributs non aimés par l\u0027utilisateur).\n"
  },
  {
    "id": "257",
    "text": "Introduction\nTraditionnellement, les données hydrométriques se présentent sous la forme de séries temporelles, représentant des mesures effectuées régulièrement par des stations : ces mesures peuvent concerner différents aspects comme les hauteurs et les débits de l\u0027eau dans les cours d\u0027eau, les quantités de précipitations, etc. Comme ces mesures sont souvent prélevées par un réseau distribué de capteurs, le problème des données manquantes est inévitable. Allant d\u0027une simple valeur manquante à une longue plage de valeurs manquantes, les lacunes peuvent avoir des causes multiples : dysfonctionnement des capteurs, maintenance des stations de mesure, erreurs humaines, etc. (Harvey et al. (2010)).\nLe réseau hydrométrique au Luxembourg fournit un bon cas d\u0027utilisation. Il est constitué de différentes stations hydrométriques permettant de mesurer notamment les débits des cours d\u0027eau. Les mesures sont ensuite fréquemment utilisées dans les modèles numériques de prévi-sion hydrologique ou pour calculer des statistiques sur les écoulements (e.g. temps de retour des crues ou des sécheresses).\nEn conséquence, lorsque certaines séries de mesures présentent beaucoup de lacunes (par exemple : les données de la station de HallerBach au Luxembourg, Figure 1), cela pose de nombreux problèmes et il est nécessaire d\u0027apporter un soin très particulier à combler ces lacunes avec une bonne précision.\nAfin de combler ces lacunes, les méthodes classiques d\u0027analyse de données ont été appliquées dans le domaine hydrologique (Salas (1980)), et des travaux récents tentent de fournir des solutions toujours plus efficaces (Harvey et al. (2010); Mwale et al. (2012)  Kotsiantis (2013)). -Les réseaux de neurones artificiels ont récemment été utilisés, notamment via des perceptrons (Tfwala et al. (2013)) ou des cartes auto-adaptatives (Mwale et al. (2012)). -L\u0027algorithme espérance-maximisation (EM) est souvent utilisé pour reconstruire des données manquantes (Van Hulse et Khoshgoftaar (2008)). -Enfin, différentes techniques de prédiction de séries temporelles peuvent être utilisées suivant les caractéristiques des séries (ARMA, ARIMA, etc.). Habituellement, les experts en hydrologie se servent de divers scripts pour corriger les données (R, MATLAB). Ainsi est-il important pour eux de pouvoir disposer d\u0027un outil interactif pour à la fois intégrer les diverses sources de données, visualiser les séries temporelles, et enfin choisir le mode d\u0027estimation de valeurs manquantes le plus adapté.\n2 gapIT : un outil pour estimer les données manquantes 2.1 Technologie gapIT est une application développée en JAVA et basée sur le logiciel d\u0027analyse et de traitement de données Cadral (Pinheiro et al. (2014) \nInspection et caractérisation des valeurs manquantes\nLes valeurs manquantes ne se corrigent pas de la même manière selon le contexte (taille des trous, saison durant laquelle les valeurs sont manquantes, probabilité qu\u0027une crue soit en cours, etc.) (Gyau-Boakye et Schultz (1994)  Premièrement, la phase de sélection des stations de référence est critique, car elle permet à l\u0027expert de choisir les séries temporelles qui serviront à compléter les séries temporelles lacunaires. Pour ce faire, l\u0027outil propose plusieurs approches complémentaires :\n-la sélection des stations les plus proches géographiquement ; -la sélection des stations se trouvant sur le même cours d\u0027eau (en amont et/ou en aval) ; -la sélection des stations ayant les séries temporelles les plus similaires sur la période concernée ; la similarité est calculée en utilisant la déformation temporelle dynamique (Dynamic Time Warping) (Berndt et Clifford (1994)). Deuxièmement, l\u0027outil propose diverses méthodes d\u0027estimation : interpolation, régressions multiples, arbres de régressions, réseaux de neurones (perceptron multi-couches), algorithme des plus proches voisins, etc.\nTroisièmement, l\u0027utilisateur peut ensuite appliquer via le logiciel la méthode d\u0027estimation sélectionnée en se basant sur les séries temporelles choisies pour évaluer les valeurs man-quantes. Or, il est important de déterminer la précision des estimations produites. Pour ce faire, des trous fictifs sont créés dans la fenêtre de temps proche du trou réel à remplir (par exemple : un trou avant, un trou après, élargissement du trou en cours d\u0027examen ou à un endroit choisi par l\u0027utilisateur). Ensuite, les mesures suivantes sont calculées : l\u0027erreur absolue moyenne (MAE), la racine carrée de l\u0027erreur quadratique moyenne (RMSE) et le coefficient Nash-Sutcliffe car c\u0027est un indicateur très commun en hydrologie (Nash et Sutcliffe (1970)).\nPour finir, l\u0027outil est capable de calculer automatiquement la configuration optimale (stations de référence et algorithme). Dans ce cas, l\u0027utilisateur garde la possibilité de modifier la configuration à son gré de manière à obtenir un nouveau résultat plus proche de ses attentes.\n3 Exemple : les débits des cours d\u0027eau au Luxembourg gapIT a été utilisé pour estimer les débits d\u0027écoulement manquants pour des stations sélec-tionnées au Luxembourg, sur la période allant du 1er janvier 2007 au 31 décembre 2013. Le jeu de données utilisé correspond à des mesures effectuées toutes les quinze minutes dans 24 stations. Afin de tester l\u0027efficacité de l\u0027outil, un ensemble de trous fictifs a été créé pour ces stations. Pour obtenir un ensemble représentatif, les trous générés sont de différentes tailles, se situent durant différentes saisons, etc. Ensuite, pour chacun de ces trous fictifs, toutes les techniques d\u0027estimation proposées par l\u0027outil ont été testées, et pour chacun des cas, les taux d\u0027erreur ont été mesurés (MAE, RMSE, Nash-Sutcliffe).\nAinsi, nous avons constaté que les réseaux de neurones et les arbres de régression permettent d\u0027obtenir les taux d\u0027erreur les plus faibles. De plus, si l\u0027on considère le meilleur ré-sultat concernant chaque trou, alors on voit que les taux d\u0027erreur sont globalement très faibles. Cela signifie que pour ces cas, une estimation correcte est possible en utilisant les données pré-sentes (Figure 4). En revanche, dans un certain nombre de cas, les meilleurs taux d\u0027erreurs sont élevés. Après analyse, il s\u0027avère que pour les stations concernées, il n\u0027existe pas suffisamment de stations assez proches, similaires ou dépendantes afin de créer une estimation assez précise. \n"
  },
  {
    "id": "258",
    "text": "Contexte\nLa recrudescence des documents textuels disponibles sur le web incite de plus en plus travaux à l\u0027exploitation de ces données de manières automatiques. Pour faire interagir ces données entre elles de manière efficace, il faut développer des moyens basés non seulement sur la ressemblance syntaxique mais également sur la correspondance sémantique.\nGEOLSemantics est une entreprise qui propose une solution logicielle de traitement linguistique basée sur une analyse linguistique profonde. Le but est d\u0027extraire automatiquement, d\u0027un ensemble de textes, des connaissances structurées, localisées dans le temps et l\u0027espace. Pour représenter ces connaissances, nous avons opté pour les technologies du web sémantique. Nous représentons nos extractions sous forme de triplets RDF et exploitons une ontologie pour apporter de la cohérence. Cette approche permet de relier les résultats de nos extractions aux connaissances du Linked Open Data, tels que Dbpedia et Geonames.\nLors de l\u0027analyse linguistique, il arrive que l\u0027information traitée contienne des imperfections. Dans notre travail, nous intéressons à l\u0027incertitude. Notre première contribution porte sur une catégorisation de l\u0027incertitude lors des différentes phases d\u0027extraction. Notre seconde contribution se situe au niveau de la représentation de l\u0027incertitude dans le graphe RDF.\n2 Acquisition de l\u0027information avec incertitude L\u0027analyse des textes comporte plusieurs étapes distinctes allant du simple découpage du texte en mots à la représentation de son contenu. Parmi ces étapes, nous retrouvons : (i) l\u0027analyse syntaxique, il s\u0027agit de la mise en évidence des structures d\u0027agencement des catégories grammaticales, afin d\u0027en découvrir les relations formelles ou fonctionnelles. (ii) l\u0027analyse sé-mantique, l\u0027objectif principal de cette analyse est de déterminer le sens des mots des phrases. (iii) l\u0027extraction de connaissances permet de mettre en évidence des entités nommées et des relations relatives à un concept particulier. Grâce à des déclencheurs qui indiquent qu\u0027une relation relative à un concept peut être présente et extraite. Un déclencheur correspond généra-lement à un concept présent dans l\u0027ontologie, ce qui permet de guider la règle d\u0027extraction par la suite. (iv) la mise en cohérence permet de consolider les connaissances extraites notamment le regroupement des entités nommées, la résolution des dates relatives. Cette étape peut être Gestion de l\u0027incertitude. suivi par un enrichissement à partir des données du Linked Open Data. Cependant, la fiabilité de l\u0027information est très souvent remise en cause. Notre démarche est de considérer le cycle de vie de la connaissance depuis son acquisition jusqu\u0027à son stockage dans la base de connaissances pour cela, nous identifions trois catégories : Pré-extraction de la connaissance : il s\u0027agira lors de cette étape de considérer les modalités de publication de l\u0027information à savoir : la date et le lieu de publication, la fiabilité accordée à la source, qu\u0027il s\u0027agisse de l\u0027auteur ou de l\u0027organisme de publication... Pendant l\u0027extraction de la connaissance : l\u0027incertitude pourra concerner aussi bien l\u0027information véhiculée que la règle d\u0027extraction à appliquer. Post-extraction de la connaissance : l\u0027incertitude peut intervenir au niveau des règles de mise en cohérence ou bien au niveau du choix de la base de référence.\nLe formalisme de représentation de connaissances choisie est le RDF tout en nous basons sur une ontologie développée pour prendre en compte les concepts relatifs à un domaine particulier. Notre approche consiste à considérer l\u0027incertitude comme une connaissance à part entière telle que le décrit l\u0027ontologie suivante. La classe Uncertainty, nous permet de modéliser l\u0027incertitude. Elle est décrite par trois proprié-tés : weight : une propriété littérale pour quantifier l\u0027incertitude identifiée, hasUncertainProp : une propriété objet qui servira d\u0027intermédiaire entre le domaine initial de la propriété et la propriété en question, isUncertain : propriété objet qui aura pour co-domain le top-concept, cela veut dire que tout concept de l\u0027ontologie pourra être visé par une incertitude. Cette ontologie est indépendante de tout domaine d\u0027application. Dès lors, elle peut être ajoutée à toute autre ontologie voulant prendre en compte l\u0027incertitude.\nConclusion et perspectives\nDans cet article nous nous intéressons au traitement de l\u0027information incertaine dans le cadre d\u0027une extraction de connaissances à partir de texte. Le traitement repose sur les technologies du web sémantique pour permettre de faire le lien avec les données du Linked Open Data. Notre démarche consiste à identifier les différentes situations où une incertitude remettant en cause la validité de l\u0027information peut subsister. Nous proposons une ontologie pour modéliser l\u0027information incertaine et la représenter au format RDF. Nous travaillons actuellement sur développement d\u0027un ensemble de patterns pouvant faciliter l\u0027interrogation du graphe RDF prenant en compte notre représentation de l\u0027incertitude. Nous prévoyons par la suite de développer un raisonneur basé sur le formalisme des logiques possibilistes afin de permettre l\u0027inférence sur les données incertaines.\nSummary\nThe knowledge representation area needs some methods that allow to detect and handle uncertainty. Indeed, a lot of text hold information whose the veracity can be called into question. These information should be managed efficiently in order to represent the knowledge in an explicit way. As first step, we have identified the different forms of uncertainty during a knowledge extraction process, then we have introduce an RDF representation for these kind of knowledge based on an ontologie that we developped for this issue.\n"
  },
  {
    "id": "259",
    "text": "Introduction\nL\u0027utilisation d\u0027ontologies s\u0027est montrée très efficace dans bien des domaines. La taille et la dynamique des domaines considérés demande toutefois l\u0027exploitation combinée de plusieurs ontologies, d\u0027où la nécessité d\u0027établir des correspondances sémantiques, ou mappings (Euzenat et Shvaiko, 2007), entre ontologies. Ainsi, la qualité des résultats produits par les systèmes utilisant des ontologies dépend de la validité des mappings entre ontologies, ce qui oblige des experts du domaine à réviser leur définition lorsque les ontologies évoluent. Si cette maintenance peut être effectuée manuellement sur de petits ensembles de mappings, une approche plus automatique est nécessaire lorsqu\u0027ils sont volumineux, comme dans le domaine médical. L\u0027existence de mappings erronés est souvent dûe à l\u0027évolution des ontologies, les erreurs d\u0027alignement mises à part (Dos . Il est alors fondamental de comprendre l\u0027évolution des ontologies pour pouvoir agir sur les mappings afin de garantir leur validité. Ce faisant, nous avons proposé un ensemble de patrons de changement permettant de caractériser l\u0027évolution des concepts d\u0027une ontologie en analysant les changements dans la définition des concepts . Nous avons également observé sur des jeux de données réelles le comportement des mappings dans le temps, ce qui nous a permis d\u0027identifier un ensemble d\u0027actions d\u0027adaptation pouvant s\u0027appliquer aux mappings pour les faire évoluer (Dos . L\u0027objet de cet article formalise, sous forme d\u0027heuristiques, le lien entre les patrons de changement et les actions d\u0027adaptation pour faire évoluer les mappings lorsque les ontologies liées évoluent. Aprés avoir introduit les concepts de notre approche (Section 2), en particulier les patrons de changement et les actions d\u0027adaptation, nous présentons les heuristiques proposées (Section 3) et le cadre expérimental emprunté au domaine biomédical pour les évaluer (Section 4) et les discuter par rapport à l\u0027existant (Section 5) avant de conclure (Section 6).\nPréliminaires\nNous présentons ici les notions et notations utilisées pour définir notre approche. Soit O t , semT ype) relie deux concepts c s ? C X et c t ? C Y par la relation sémantique semT ype, telle que semT ype ? {?, \u003c, \u003e, ?} (cf. tableau 1). \nTAB. 1 -Notations pour la formalisation des heuristiques\nPatrons de changement caractérisant l\u0027évolution d\u0027ontologies\nNos travaux ont montré que l\u0027évolution des ontologies rend nécessaire d\u0027adapter les mappings. Nous pensons qu\u0027une compréhension précise de cette évolution va nous renseigner sur la façon d\u0027adapter les mappings au cours du temps. Pour caractériser l\u0027évolution des ontologies, nous avons proposé un ensemble de patrons de changement . Contrairement à ceux de la littérature (Djedidi et Aufaure, 2009), (Javed et al., 2013), (Gröner et al., 2010), nos patrons considèrent les changements syntaxiques et sémantiques au niveau des attributs des concepts. Ce choix a été motivé par les résultats expérimentaux obtenus montrant que la définition des mappings repose sur certaines valeurs d\u0027attributs (Dos .\nLes Une Copie Totale caractérise le changement à travers lequel une valeur d\u0027attribut devient également la valeur d\u0027un attribut d\u0027un autre concept. Par exemple, un attribut a 1 d\u0027un concept c 1 a pour valeur \"portal systemic encephalopathy\" au temps j. Au temps j + 1, a 1 a la même valeur, mais un attribut a 2 d\u0027un concept c 2 aura également cette valeur.\nUne Copie Partielle est une copie d\u0027une partie de la valeur d\u0027un attribut. Un attribut a 1 d\u0027un concept c 1 a la valeur \"familial hyperchylomicromenia\" au temps j. Au temps j + 1, a 1 garde cette valeur, mais un attribut a 2 aura \"familial chylomicromenia\" comme nouvelle valeur.\nUn Transfert Total correspond au transfert de la totalité de la valeur d\u0027un attribut à un autre attribut. Contrairement au cas TC, la valeur originale de l\u0027attribut n\u0027est pas conservée.\nUn Transfert Partiel définit le transfert d\u0027un partie de la valeur d\u0027un attribut. Par exemple, un attribut a 1 peut valoir \"eye swelling\" au temps j. Au temps j + 1, cette valeur sera supprimée partiellement de a 1 mais un attribut a 2 vaudra \"head swelling\" (i.e., le terme \"swelling\" est déplacé de a 1 vers a 2 entre j et j + 1).\nLes Patrons de changement sémantiques (SCP) s\u0027intéressent à l\u0027évolution de la séman-tique de la valeur des attributs au cours du temps. Nos observations ont montré qu\u0027à travers leurs évolutions successives, les concepts pouvaient devenir plus généraux, plus spécifiques ou rester équivalents, modifiant ainsi la relation sémantique des mappings. Les 4 SCP que nous proposons sont : Equivalent (EQV), Plus Spécifique (MSP), Moins Spécifique (LSP) et Recouvrement Partiel (PTM).\nEquivalent stipule que les changements syntaxiques au niveau de la valeur de l\u0027attribut ne modifient pas sa sémantique. Par exemple, la valeur d\u0027un attribut peut passer de \"Diabetes type 1\" à \"Diabetes type I\" sans en affecter le sens.\nPlus Spécifique identifie un changement rendant un concept plus spécifique que sa nouvelle version. Le changement menant de \"kappa light chain disease\" à \"kappa chain disease\" rend le premier plus spécifique du fait de la suppression du qualificatif \"light\". Moins Spécifique décrit l\u0027effet inverse.\nRecouvrement Partiel identifie un changement au niveau de la sémantique ne pouvant être caractérisé par les autres SCP. Considérons la valeur originale \"focal atelectasis\" et son évolu-tion \"helical atelectasis\". Ces deux valeurs font toutes deux référence à la notion de \"atelectasis\", mais ne peuvent être déclarées ni équivalentes, ni plus ou moins spécifiques. ModSemTypeM(m st , semT ype) consiste en la modification de semT ype (la relation sé-mantique) à cause des modifications sur les concepts sources (c s ) et/ou cibles (c t ).\nLes actions d\u0027adaptation de mappings\nNoAction(m st ) est appliquée lorsque les modifications sur les concepts sources et/ou cibles n\u0027entraînent pas de changements sur la sémantique des mappings.\nHeuristiques d\u0027adaptation des mappings\nNous présentons dans cette section des heuristiques indiquant sous quelles conditions adapter les mappings afin qu\u0027ils restent valides dans le temps. Ces heuristiques ont été établies expérimentalement à partir de l\u0027observation de l\u0027évolution des mappings entre des jeux de données réelles et de l\u0027analyse de l\u0027impact des patrons de changements sur leur évolution. Des heuristiques ont été définies pour chaque type d\u0027adaptation.\nHeuristiques pour les mappings de type Move et Derive\nSoit Cand l\u0027ensemble des concepts dits candidats regroupant les concepts du contexte de c s en j + 1 pour lesquels il existe un changement de type LCP entre un de leurs attributs et un attribut de c s . Soit la fonction topA(c s , c t , n) retournant les n attributs expliquant le mieux le mapping entre c s et c t . Soit la fonction SLCP (a 1 , a 2 ) retournant VRAI s\u0027il existe un changement de type LCP entre a 1 et a 2 et FAUX sinon. Soit la fonction SCP (a 1 , a 2 ) retournant les changements de type SCP entre a 1 et a 2 ou ? si aucun changement de type SCP n\u0027a été identifié.\nMoveM. Expérimentalement, nous avons observé que l\u0027adaptation d\u0027un mapping de type MoveM correspondait à l\u0027existence de changements de type LCP entre attributs. Plus précisé-ment, nous avons observé que lorsque l\u0027adaptation du mapping est de type MoveM, il n\u0027existe qu\u0027un seul attribut de c s au temps j expliquant le mapping avec un changement de type LCP avec un attribut du contexte de c s au temps j + 1. De ce fait, nous appliquons l\u0027adaptation de type MoveM lorsqu\u0027il n\u0027existe qu\u0027un seul concept candidat avec un changement de type LCP  Figure 1). Intuitivement, cela signifie que le mapping suit l\u0027évolution des attributs qui l\u0027expliquent.\nSoit les concepts \u0027128829008\u0027 \"Acute myeloid leukemia, 11q23 abnormalities (morphologic abnormality)\" et \u0027C6924\u0027 \"Acute Myeloid Leukemia with 11q23 MLL Abnormalities\" issus de SNOMED CT et de NCI. Deux changements de type LCP se sont produits lors de l\u0027évolu-tion de \u0027C6924\u0027. Un transfert total s\u0027est produit sur l\u0027attribut qui expliquait le mieux le mapping (\"Acute Myeloid Leukemia with 11q23 Abnormalities\"). Une copie totale a eu lieu pour un autre attribut qui expliquait aussi le mapping. Ces deux changements concernent le même concept candidat \u0027C82403\u0027 \"Acute Myeloid Leukemia with t 9 11 p22 q23 MLLT3-MLL\" dans la nouvelle version de l\u0027ontologie, mais deux attributs différents expliquant le mapping. Suite à l\u0027évolution de \u0027C6924\u0027, le mapping relie dorénavant les concepts \u0027128829008\u0027 et \u0027C82403\u0027.\nDeriveM. De façon similaire à l\u0027action MoveM, DeriveM est appliquée lorsque plusieurs changements entre attributs de type LCP sont reconnus suite à une évolution. Le mapping original est préservé et c s existe dans la nouvelle version de l\u0027ontologie. Le fait que plusieurs changements de type LCP concernent les attributs pertinents de c s et qu\u0027il existe donc plusieurs concepts candidats conduit à la création de nouveaux mappings entre ces candidats et c t (cf. Figure 2). Formellement, nous définissons cette heuristique de la façon suivante :\nEn guise d\u0027illustration, considérons le mapping \u0027plus spécifique que (\u003c) entre \u002741452004\u0027 -\"Uterus acollis (disorder)\" dans SNOMED CT et \u0027752.3\u0027 -\"Other anomalies of uterus\" dans ICD-9-CM. L\u0027analyse de l\u0027évolution de 752.3 permet d\u0027observer plusieurs changements de type LCP concernant des concepts candidats différents et également plusieurs changements de type SCP. Par example, il y a une copie totale (TC) de l\u0027attribut \"Other anomalies of uterus\" expliquant le mapping vers un attribut de \u0027752.33\u0027 qui fait apparaître une équivalence. En plus, l\u0027attribut \"Bicornuate uterus\" est totalement transféré (TT) dans \u0027752.34\u0027 faisant également apparaître une relation d\u0027équivalence. L\u0027action d\u0027adaptation à réaliser, selon notre heuristique, est DeriveM en l\u0027appliquant aux deux concepts candidats concernés par les changements de type LCP et en gardant la même relation sémantique que la relation du mapping original.\n3.2 Heuristiques associées à la modification de relation sémantique L\u0027application de l\u0027action ModSTR dépend des changements de types SCP trouvés entre attributs. Nous avons identifié deux scénarios différents. Le premier concerne la modification de la relation du mapping original m 0 st alors que c s ne change pas (cf. Equation 3). Le second scénario concerne la modification de la relation du mapping original suite à un MoveM ou DeriveM (cf. Equation 4). Dans le premier cas, la nouvelle relation sémantique lie le concept source (en terme de contenu) au temps j + 1 et le concept cible alors que dans le second cas, c s est remplacé par un concept candidat (un concept appartenant au contexte de c s ). Le type de la relation sémantique après évolution est obtenu en combinant le type de la relation du mapping original semT ype et le type d\u0027un changement de type SCP détecté entre attributs. Soit la fonction getSemType qui fournit la relation sémantique obtenue en combinant le type d\u0027un mapping original semType (x) avec des relations identifiées par des patrons de type SCP, SCPs (y). Par example, si la relation sémantique entre c \nVoici les heuristiques associées au premier et deuxième scénario :\nst ? {?, \u003c, \u003e, ?}, semT\nct ? {?, \u003c, \u003e, ?}, semT , nous sélectionnons les concepts candidats au temps j + 1 à partir du contexte de c s au temps j. Les expérimentations à partir desquelles cette heuristique a été proposée ont montré que la similarité entre les attributs expliquant un mapping et les attributs des concepts du contexte de c s après évolution était très faible lorsque le mapping était supprimé (Dos . Ceci a conduit à introduire une condition portant sur la similarité dans l\u0027heuristique proposée.\nNo action. L\u0027heuristique pour NoAction considère que des patrons de type LCP et SCP ne sont pas observables (cf. Equation 6). Elle s\u0027applique dans des cas où les changements portant sur un concept lié par un mapping n\u0027ont pas d\u0027effet sur les attributs expliquant ce mapping, ou lorsque la similarité avec de nouveaux attributs du contexte est faible (Dos .\nEvaluation\nCette section porte sur l\u0027évaluation des heuristiques proposées. Nous étudions si les changements identifés dans une ontologie conduisent à des adaptations correctes (i.e. MAAs) des mappings affectés. Cette évaluation a été réalisée sur plusieurs versions d\u0027ontologies biomédi-cales (SNOMED CT et ICD-9-CM) et plusieurs versions de mappings associés.\nProtocole d\u0027expérimentation\n4. Deux types de mesure, représentées respectivement par and sont proposées pour évaluer de manière rigoureuse les actions proposées (cf. Résultats obtenus dans le tableau 2). Cette distinction est bien appropriée à l\u0027évaluation des actions MoveM, DeriveM and ModSTR. Le symbole exprime la précision, le rappel et la F-mesure lorsque les actions proposées sont correctes par rapport aux actions attendues, en plus du concept candidat ou de la relation sémantique. Le symbole correspond aux cas où seul le type d\u0027action est correct, le concept candidat ou la relation sont erronés (par exemple, MoveM ou DeriveM est proposé mais pas avec le bon concept candidat à j+1). La mesure est plus contrainte, elle pourrait conduire à des valeurs plus faibles. Ainsi, si nous observons un MoveM avec un concept c obs du contexte et si notre mécanisme propose un MoveM avec le même concept c obs , nous choisissons de mesurer la précison, le rappel et la F-mesure de (action d\u0027adaptation, concept lié et relation sémantique exacts). En revanche, si seule l\u0027action MoveM est correcte, nous faisons le calcul correspondant à Nous procédons de la même façon pour ModSTR, en considérant en plus de l\u0027action d\u0027adaptation du mapping, le type de la relation sémantique.\n5. Enfin, nous effectuons un calcul global qui calcule la précision, le rappel et la F-mesure pour chaque jeu de données, indépendamment des types d\u0027action d\u0027adaptation.\nRésultats et discussion\nLes résultats de l\u0027adaptation des mappings basés sur nos heuristiques sont présentés dans la Mapping Move. Pour ce type d\u0027action, les résultats varient selon que l\u0027évolution concerne ICD-9-CM ou SNOMED CT uniquement pour la précision. Ainsi, nos heuristiques peuvent générer des adaptations plus ou moins correctes selon les jeux de données. Les différences de résultats entre et étant très faibles, on remarque que, la plupart du temps, lorsque notre système propose un MoveM, c\u0027est avec un bon concept candidat du contexte.\nMapping Derive. Les résultats obtenus sont plus élevés que ceux de Move pour SNOMED CT. L\u0027action DeriveM est complexe car l\u0027action propose plusieurs DeriveM par mapping. Aucune action DeriveM n\u0027a été observée pour ICD-9-CM. Là encore, les conditions d\u0027application des heuristiques sont différentes selon les jeux de données. L\u0027explication peut provenir du processus de maintenance, de matching ou de la granularité des ontologies alignées.\nModification de la relation sémantique. L\u0027évaluation de cette heuristique a été difficile car les mappings dont nous disposions contenaient peu de relations différentes (la plupart était des équivalences). Néanmoins, les résultats montrent que les nouvelles relations proposées quand une action de type ModSemTypeM est détectée, sont pertinentes (surtout pour SNOMED CT). Ce type d\u0027adaptation est généralement combiné avec un MoveM ou un DeriveM ce qui oblige à choisir la bonne action avant de changer la relation. Parfois, les changements dans l\u0027ontologie ne sont pas l\u0027unique raison pour changer la relation. Cela influe négativement sur nos résultats. Enfin, considérer la nouvelle version des mappings comme référence peut aussi poser problème. C\u0027est un ensemble de mappings qui a subi des évolutions mais ce n\u0027est pas à proprement parler un jeu de données de référence. Une relation sémantique proposée peut être considérée comme fausse par notre processus d\u0027évaluation alors qu\u0027elle peut être correcte d\u0027un point de vue sémantique. L\u0027intervention d\u0027un expert serait nécessaire pour y remédier.\nMapping Remove. La précision et le rappel sont relativement bons. Ils ne varient que légèrement selon l\u0027ontologie. La F-mesure minimale est de 0.54. La prise en compte du retrait de l\u0027attribut expliquant le mieux le mapping dans l\u0027heuristique semble être une bonne décision.\nApplication d\u0027aucune action. Ce type d\u0027adaptation couvre le plus grand nombre de cas que ce soit en valeur absolue (5892 attendus dans SNOMED CT et 3139 dans ICD-9-CM) ou relative. Les résultats montrent une grande efficacité des heuristiques. On constate que, bien que les concepts source ou cible évoluent, si les attributs expliquant les mappings qui les concernent restent inchangés, ces mappings n\u0027évolueront pas non plus. Les très bons ré-sultats obtenus avec l\u0027application de cette heuristique permettent aux experts de se concentrer sur l\u0027étude d\u0027une toute petite partie des mappings (en comparaison à l\u0027ensemble initial), ces mappings étant plus difficiles à adapter et pouvant nécessiter une intervention humaine.\nRésultats globaux. Nous avons analysé les résultats en combinant tous les types d\u0027adaptation pour avoir une idée de la qualité du processus général d\u0027adaptation. La F-mesure est élevée (un min de 0.85 dans ICD-9-CM). L\u0027action d\u0027adaptation NoAction influe beaucoup sur ces résultats. Néanmoins les résultats sont globalement acceptables et prometteurs même si certaines actions sont difficiles à appliquer. Ceci montre que les conditions d\u0027application des heuristiques que nous proposons sont adaptées. L\u0027amélioration des résultats liés à certaines actions nécessiterait de rechercher quels autres éléments influencent les types d\u0027adaptation et d\u0027étudier comment les prendre en compte dans les heuristiques.\nEtat de l\u0027art\nNous distinguons trois principaux types d\u0027approches pour la maintenance des mappings due à l\u0027évolution d\u0027ontologies. La première consiste à identifier et réparer les mappings invalides (Meilicke et al., 2008)  (Ivanova et Lambrix, 2013). Ces approches effectuent des raisonnements logiques pour identifier les mappings produisant des incohérences logiques. Ce mécanisme ne s\u0027applique que sur des ontologies formelles, qui n\u0027existent pas toujours.\nLes travaux s\u0027inscrivant dans la deuxième catégorie reposent sur des techniques de ré-alignement total ou partiel d\u0027ontologies. Si le premier type d\u0027approche ne considère aucune information provenant de l\u0027évolution, le second recoupe l\u0027ensemble des concepts modifiés au niveau des ontologies avec ceux impliqués dans des mappings pour ne réaligner que ce sousensemble de concepts (Khattak et al., 2012). Ces approches sont coûteuses en temps de calcul et de validation lorsque les ontologies sont volumineuses (Shvaiko et Euzenat, 2013).\nLa troisième catégorie, inspirée du monde des bases de données (Velegrakis et al., 2003), fait référence à des approches qui profitent au maximum des informations provenant de l\u0027évo-lution d\u0027ontologies pour éviter de les réaligner totalement. Tang et Tang (2010) ont proposé une méthode visant à trouver l\u0027impact minimal de la propagation des changements au niveau d\u0027une ontologie. Martins et Silva (2009) suggèrent d\u0027adapter les mappings en procédant de la même façon que lors de la modification des concepts de l\u0027ontologie qui évolue. Cependant, dans leur approche, les mappings ne sont adaptés que lorsque les concepts sont supprimés. L\u0027originalité de nos travaux, par rapport à cet état de l\u0027art, réside dans : (i) l\u0027importance donnée aux modifications des ontologies sous-jacentes, (ii) à leur caractérisation et (iii) à la prise en compte du changement au niveau de la relation sémantique des mappings, ce dernier point étant souvent négligé dans les approches existantes qui ne considèrent qu\u0027un seul type de relation (Dos Reis et al., 2012).\nConclusion\nDans cet article, nous avons proposé un ensemble d\u0027heuristiques guidant l\u0027adaptation des mappings entre ontologies. Ces dernières formalisent le lien existant entre les changements identifiés au niveau des éléments ontologiques et les actions d\u0027adaptation à appliquer sur les composants des mappings pour préserver leur validité. L\u0027approche décrite a été validée expé-rimentalement sur des données réelles du monde biomédical. Le manque de données de ré-férence nécessite l\u0027implication d\u0027experts du domaine mais le travail réalisé permet de réduire considérablement le temps nécessaire aux experts pour valider les adaptations proposées.\n"
  },
  {
    "id": "260",
    "text": "Introduction\nQui n\u0027a pas dit, un jour, en écoutant la radio : \"Mais ça ressemble à Supertramp ou à une musique de Chopin ou à une musique baroque\" ? Sur la base d\u0027un court morceau écouté on peut en effet identifier directement l\u0027auteur ou le placer dans une catégorie même si on ne connait pas forcément le morceau. Si c\u0027est un chanteur, on le reconnait facilement au timbre de sa voix, pour un morceau de musique classique, l\u0027interprétation peut varier et on détecte plutôt la ligne musicale. Pour les documents textuels, ce problème d\u0027authentification d\u0027auteur présumé est récurrent, et la fouille de texte peut s\u0027avérer très utile. Ainsi, par exemple pour authentifier une élégie de Shakespeare en 1995 1 des techniques telles que le comptage exclusif des mots et la prise en compte de mots rares ont été employés avec succès (Foster (1996)). Le champ littéraire n\u0027est cependant pas le seul concerné. Le problème d\u0027authentification d\u0027un auteur apparait aussi dans bien d\u0027autres applications, comme dans le domaine juridique par exemple pour l\u0027authentification d\u0027un testament ou dans le cadre des investigations anticriminelles ou antiterroristes pour identifier la provenance d\u0027une demande de rançon ou de posts émis sur des forums de discussion du Dark Web (Abbasi et Chen (2005)). Le marketing peut également être intéressé par le profiling des auteurs des blogs ou des commentaires sur le Web.\nDans le cas de textes écrits, on peut plus généralement distinguer trois variétés de problèmes liés à la détermination d\u0027un auteur inconnu :\n-l\u0027extraction de profil (Author Profiling) : il s\u0027agit d\u0027indiquer à partir d\u0027un texte des éléments du profil de son auteur comme, par exemple, la tranche d\u0027âge et le genre ( Rangel et al. (2013)) ou l\u0027appartenance à une catégorie particulière comme celle des criminels potentiels ) -la reconnaissance de l\u0027auteur (Author Verification ou Author Recognition) : il s\u0027agit de vérifier parmi une liste des auteurs possibles lequel est le bon -l\u0027identification d\u0027un auteur : il s\u0027agit de décider si un texte donné a été écrit par l\u0027auteur d\u0027un autre groupe de documents Ainsi, dans ces trois problèmes, on s\u0027interroge sur l\u0027auteur d\u0027un document écrit, et dans ces trois cas pour répondre, il est nécessaire de représenter de façon appropriée le document à explorer et de pouvoir le comparer avec d\u0027autres. Toutefois, nous pensons qu\u0027il est illusoire de rechercher une empreinte d\u0027un auteur sur un texte qu\u0027on pourrait comparer avec des empreintes extraites d\u0027autres textes et qui serait unique au même titre qu\u0027une empreinte digitale. Nous pensons qu\u0027il faut utiliser divers espaces de représentation pour les textes à analyser selon la langue d\u0027origine ou encore le genre ou la qualité du document. Dans cet article, où nous nous intéressons plus spécifiquement à des problèmes d\u0027identification d\u0027auteurs à partir de documents rédigés dans diverses langues et de différents types (textes littéraires courts ou longs, articles de presse ou publications, blogs) nous avons exploré différents modes de représentation des documents. Nous avons ensuite proposé de formaliser l\u0027identification d\u0027auteurs comme un problème de classement que nous avons résolu de trois façons : à l\u0027aide d\u0027un algorithme original de comptage de similarité (DCM), puis avec deux autres méthodes qui exploitent ce comptage, par une technique de vote (DCM-voting), par apprentissage automatique (DCM-classifier).\nNotre article est organisé de la manière suivante : après la section 2 consacrée aux travaux relatifs à l\u0027identification d\u0027auteurs, nous définissons plus formellement le problème dans la section 3, puis nous décrivons les trois méthodes proposées pour le résoudre dans la section 4. La section 5 présentera les résultats des expériences réalisées afin d\u0027évaluer l\u0027intérêt de ces approches et de les comparer à celles de l\u0027état de l\u0027art. Des conclusions seront présentées dans la dernière section.\nEtat de l\u0027art\nL\u0027identification d\u0027auteur peut être définie comme un problème de classification de textes : Etant donné un ensemble, grand ou réduit à un seul élément, de documents d\u0027un même auteur, il faut déterminer si un nouveau document a été écrit par le même auteur que les autres \".\nIl s\u0027agit donc d\u0027un problème de classement supervisé binaire dont la réponse attendue est binaire (\"oui\" ou \"non\") ou une probabilité d\u0027appartenance à l\u0027ensemble de documents fournis. Toutefois, une des spécificités de ce problème de classement est que seuls des exemples d\u0027une des deux classes sont donnés : les documents rédigés par l\u0027auteur, mais la seconde classe n\u0027est pas explicitée. De plus, parfois le nombre d\u0027exemples positifs est réduit à un seul document, ce qui rend la tâche particulièrement difficile.\nPour pallier l\u0027absence d\u0027exemples négatifs, on peut essayer d\u0027en produire. C\u0027est la voie explorée par différents auteurs parmi lesquels figurent (Seidman (2013)) qui construisent une classe d\u0027\"imposteurs\" choisis aléatoirement sur la base des dix mots les plus fréquents figurant dans les documents disponibles pour remplir la classe du \"non\". D\u0027autres auteurs, comme Zhang et al. (2014) et Halvani et al. (2013), transforment ce problème de classification à deux classes en un problème avec plusieurs classes, soit en rajoutant des classes extérieures, soit en transformant la classes initiale en plusieurs. Les même auteurs (Halvani et al. (2013)) augmentent la taille de la classe des documents connus quand celle-ci est réduite à un seul document. Ainsi, ces approches permettent de revenir à un problème classique de classement supervisé mais, lors de la construction des exemples négatifs, elles sont confrontées au risque de choisir des documents trop proches ou trop éloignés des documents déjà fournis.\nOutre la question des données disponibles pour résoudre le problème, l\u0027identification d\u0027auteurs est ensuite confrontée à deux autres questions classiques en fouille de texte : comment représenter les documents et, une fois l\u0027espace de représentation choisi, quelles méthodes appliquer pour résoudre le problème de classement ?\nComme nous l\u0027avons déjà remarqué, l\u0027identification d\u0027auteurs peut être réalisée à partir de documents très différents : méls (de Vel et al. (2001); Chaurasia et Kumar (2010)), programmes, parties des oeuvres littéraires ou parties des documents de la vie de tous les jours, texte plat (Zhang et al. (2014)), extraits de chat (Inches et Crestani (2013)) ou séquences de commandes Unix (Szymanski et Zhang (2004)). Le choix des caractéristiques examinées, on parle des caractéristiques stylométriques, dépend du type de document, parfois de la langue et aussi de la qualité du texte initial. Les caractéristiques dites \"spécifiques\" aux applications portent plutôt sur le comptage des tabulations et autres séparateurs ou l\u0027analyse des caracté-ristiques spécifiques, telles que la position des parenthèses et des crochets fermants pour les programmes, et des lignes vides pour les méls. Les caractéristiques sémantiques sont prises en compte plutôt pour des textes issus du web (forum et chat), comme, par exemple, l\u0027usage des abréviations ou des mots démonstratifs fréquents (\"well\") ou des transcriptions concentrées des expressions orales (\"sse u\"). On peut également considérer des caractéristiques syntaxiques comme des fautes d\u0027orthographe ou les abréviations. Si on se place dans un cadre générique (authentification d\u0027auteur dans diverses langues et dans divers genres), on utilise plutôt des caractéristiques de type caractère ou mot, ou suites de caractères ou de mots (n-grams) (Chaurasia et Kumar (2010); Szymanski et Zhang (2004)). On peut aussi avoir recours à un étiqueteur lexical et syntaxique mais son usage augmente considérablement le temps de traitement (Juola et Stamatos (2013)) et les résultats vont dépendre de sa qualité (Vilariño et al. (2013)). Lorsque le choix de ces caractéristiques est fait, les documents peuvent être transformés en vecteurs en utilisant, le plus souvent, tf-idf comme pondération ou uniquement la fréquence. Ensuite, selon la représentation du texte adoptée, on peut comparer les documents à l\u0027aide de fonctions \"classiques\" de similarité telles que le cosinus, la corrélation, moins souvent des mesures de compression de données comme la Fast Compression Distance (Cerra et al. (2014)) ou la Common N-Gram dissimilarity (Layton et al. (2013)).\nPour ce qui concerne la résolution du problème de classement lui-même, on peut appliquer des méthodes \"classiques\" telles que les k plus proches voisins (k-NN) (Zhang et al. (2014); Ghaeini (2013); Halvani et al. (2013)) ou les SVM (Vilariño et al. (2013)). Certains auteurs (Dam (2013); Layton et al. (2013) ;Jankowska et Milios (2013)) proposent des méthodes basées sur le choix d\u0027un seuil ou d\u0027un vote et des formules de calcul de l\u0027éloignement entre le document d\u0027auteur inconnu et les autres. Les différences entre ces approches résident dans la phase de prétraitement, dans l\u0027extraction des caractéristiques et dans le choix du seuil et de la fonction de dissimilarité.\nPar rapport aux travaux antérieurs, notre contribution se situe dans un cadre plus large avec l\u0027objectif de proposer une méthodologie générique, applicable à des collections très différentes tant par le genre des documents que par le langage. Ceci nécessite la mise en place d\u0027une approche permettant de choisir automatiquement la représentation textuelle la mieux adaptée pour un corpus donné.\nDéfinition du problème et représentation des documents\nLe problème d\u0027identification d\u0027auteur peut être défini de la façon suivante. Etant donné un corpus composé de documents d\u0027un même type (mel, blog, roman, code, etc.) écrits dans un même langage (anglais, français, Java, etc.) on dispose pour chaque problème p d\u0027un ou plusieurs documents A p du corpus qui ont été rédigés par un même auteur et d\u0027un document u p dont l\u0027auteur est inconnu. L\u0027objectif est de déterminer si u p a été écrit ou non par le même auteur que les documents de A p . Si on dispose d\u0027un échantillon d\u0027apprentissage, autrement dit d\u0027un ensemble de problèmes P tel que pour chaque problème p ? P on sait si le document inconnu u p a été rédigé ou non par le même auteur que les documents associés A p , alors on peut formaliser le problème comme un problème de classement supervisé binaire et le résoudre à l\u0027aide de méthodes d\u0027apprentissage automatique. La difficulté consiste alors à déterminer d\u0027une part un ou des espaces de représentation des documents appropriés et d\u0027autre part à construire à partir de ces représentations des facteurs descriptifs des documents inconnus permettant de prédire efficacement si chaque document u p a été ou non produit par le même auteur que les documents de A p qui lui sont associés. Parmi les modèles les plus connus et les plus utilisés pour représenter des documents figure le modèle tf-idf introduit par Salton et al. (1975). Un document d est représenté par un vecteur (w 1 , . . . , w j , . . . , w |T | ) tel que le poids w j du terme t j dans d correspond au produit de la fréquence tf j du terme t j dans d par le pouvoir discriminant idf (j) de t j . Ce modèle est très efficace notamment pour identifier des termes (caractères, mots ou séquences de mots ou caractères correspondant à des n-grams) qui sont fréquents dans un document et rares dans les autres. Mais, comme nous l\u0027avons souligné en introduction, d\u0027autres caractéristiques peuvent être prises en compte pour représenter les documents. De plus, nous pensons qu\u0027il n\u0027existe pas un modèle de représentation universel adapté à tous les documents mais que le choix de cet espace de représentation doit dépendre du type de documents et du langage.\nCeci nous a conduit à considérer d\u0027autres espaces de représentation indiqués dans le tableau 1. Outre le modèle tf-idf défini à partir des mots, avec élimination des mots outils à l\u0027aide d\u0027un dictionnaire (R5) ou en considérant leur fréquence (R4), des suites de mots ou de caractères (R1, R2, R3), nous avons introduit trois autres modèles de représentation (R6, R7 et R8) visant à caractériser le style d\u0027écriture du document. Dans le modèle R6, la moyenne et l\u0027écart type du nombre de mots par phrase sont associés au document. Le modèle R7 attribue à chaque document une mesure de diversité du vocabulaire définie comme le nombre de mots différents employés divisé par le nombre total d\u0027occurrences de mots (i.e. la longueur du document). Le modèle R8 correspond au modèle de Salton dans lequel on considère les caractères de ponctuation au lieu des termes (mot ou caractère n-grams). Enfin, le modèle R678 correspond à la concaténation des trois modèles précédents : chaque document est représenté par un vecteur indiquant la moyenne par phrase des caractères de ponctuation \" :\" , \" ;\" , \",\", la moyenne et l\u0027écart type du nombre de mots par phrase et la diversité du vocabulaire. Ponctuation nombre moyen de signe de ponctuation par phrase caractères pris en compte : \",\" \" ;\" \" :\" \"(\" \")\" \" !\" \" ?\" R678 Concaténation R6 + R7 + R8 TAB. 1: La liste de espaces de représentation considérés\nDCM, DCM-voting et DCM-classifier\nAyant choisi un des espaces de représentation, on peut comparer les documents deux à deux à l\u0027aide de mesures de similarité comme le cosinus et le coefficient de corrélation ou avec la distance euclidienne et appliquer une des trois méthodes (DCM, DCM-voting, DCMClassifier) que nous avons proposées pour résoudre le problème d\u0027identification d\u0027auteur. La première méthode DCM permet de traiter directement le problème d\u0027identification p ? P , en considérant uniquement les similarités entre les vecteurs décrivant les documents suivant un des espaces. Les deux autres méthodes, basées sur DCM, permettent de combiner diffé-rentes représentations des documents, par une méthode de vote dans le cas de DCM-voting, à l\u0027aide d\u0027une méthode d\u0027apprentissage supervisée nécessitant la construction d\u0027attributs prédic-tifs pour DCM-classifier. Ces différentes méthodes sont décrites dans les sections suivantes.\nMéthodes de comptage de similarités : DCM et DCM-voting\nEtant donné un problème p ? P défini par un ensemble A p de documents rédigés par un même auteur et un document u p dont l\u0027auteur est inconnu, représentés dans un même espace, et un seuil de décision ? , la méthode DCM, décrite par l\u0027algorithme suivant, fournit en sortie la valeur True si l\u0027auteur de u p est le même que celui des documents de A p ou la valeur False dans le cas contraire. Cette méthode exploite les similarités (ou distances) entre tous les documents disponibles. Elle consiste à assigner le document u p au même auteur que les documents de A p si la plupart d\u0027entre eux sont plus proches de u p qu\u0027ils ne le sont des autres documents de A p . Plus précisément la plus grande similarité de chaque document d x ? A p aux autres documents de A p est calculée puis comparée à la similarité de d x à u p . Si la première est inférieure à la seconde, un compteur est incrémenté. Après examen de tous les documents de A p (fin de la boucle for extérieure), il comptabilise la proportion de documents de A p qui sont les plus proches de u p que des autres de A p et si cette proportion est supérieure au seuil fixé ?, alors l\u0027auteur du document inconnu u p est le même que celui des autres documents. Cette méthode présente l\u0027avantage d\u0027être simple et rapide à mettre en oeuvre et elle permet de traiter un problème d\u0027identification p ? P indépendamment des autres. En revanche, elle n\u0027exploite qu\u0027un seul mode de représentation des documents.\nPour pallier ce défaut, on peut avoir recours à une méthode de vote DCM-voting consistant simplement à appliquer la méthode DCM en considérant plusieurs espaces de représentation des documents, de préférence en nombre impair, puis à affecter le document inconnu à la classe majoritairement retournée par les différentes exécutions. Cependant, comme nous l\u0027avons souligné précédemment, tous les espaces de représentation ne sont pas équivalents et il serait souhaitable de pouvoir ajuster leur poids dans la décision finale ; ce qui est difficile à faire en pratique même pour un expert ; de même que le choix du seuil ?. Pour toutes ces raisons, nous proposons une autre méthode plus générale permettant d\u0027exploiter simultanément plusieurs modes de représentation des documents et d\u0027ajuster automatiquement, par apprentissage automatique, leur importance dans l\u0027identification de la classe des documents inconnus.\nLa méthode DCM-classifier\nDans le cadre de l\u0027apprentissage supervisé, on suppose que pour un sous-ensemble P A de problèmes de P , on sait en fait si les documents inconnus u p ont ou non été produits par le même auteur que les documents qui lui sont associés i.e. on dispose en plus de la classe class(u p ) des documents inconnus à savoir même auteur ou auteur différent. Ce sousensemble P A est décomposé en un échantillon d\u0027apprentissage P a utilisé pour construire un modèle de décision et un échantillon test employé pour l\u0027évaluer. La phase d\u0027apprentissage permet de mettre en relation des facteurs descriptifs (ou attributs) des documents avec leur classe de façon à pouvoir ensuite identifier l\u0027auteur d\u0027un nouveau document dont la classe est inconnue uniquement à partir de ces facteurs descriptifs. Il est clair que la qualité du modèle dépend largement du pouvoir prédictif de ces facteurs que nous proposons de définir de la façon suivante.\nPour chaque espace de représentation R v , v ? {1, .., V }, chaque document u p est décrit par deux attributs count v (u p ) et mean v (u p ) respectivement définis à l\u0027aide d\u0027une mesure de similarité s par :\nUn dernier attribut T OT count (u p ), basé sur tous les espaces de représentation est égale-ment calculé afin d\u0027avoir une description plus synthétique. Il est défini par :\nAinsi lors de l\u0027apprentissage, on considère les documents u p de chaque problème p de P a décrit par ces attributs descriptifs prédictifs et par leur classe réelle (\nCompte tenu du caractère numérique de ces attributs descriptifs, plusieurs méthodes d\u0027apprentissage supervisé peuvent alors être employées ( SVM, etc). Dans le cadre des expérimentations, nous avons privilégié les arbres de décision qui ont l\u0027avantage d\u0027intégrer une phase de sélection des attributs en fonction de leur pouvoir prédictif ; ce qui permet de favoriser selon la famille de problèmes considérés tel ou tel espace de représentation. De plus, ils permettent aussi d\u0027ajuster automatiquement les paramètres du modèle. Les résultats obtenus sur chaque corpus des collections ev2013, app2014 et ev2014 ont été évalués à l\u0027aide des indicateurs habituels de précision, de rappel et avec la mesure F 1.\nExpérimentations et résultats\nLe taux d\u0027erreur indiqué par la mesure F 1 étant très synthétique, pour comparer les mé-thodes, on a également utilisé l\u0027indicateur de performance AU C qui mesure l\u0027aire de la courbe ROC (Davis et Goadrich (2006)).\nPour la collection ev2014, seuls les indicateurs de performances calculés par la plateforme du challenge pour chaque corpus sont disponibles : AU C, l\u0027indicateur c@1, le produit des deux indicateurs et le temps d\u0027exécution. L\u0027indicateur c@1 permet de donner plus d\u0027importance à une réponse correcte par rapport à l\u0027absence de décision (i.e. une probabilité d\u0027appartenance à la classe de 0.5). Cet indicateur est défini par :\noù n est la taille du corpus, n c est le nombre de réponses correctes, n u le nombre de problèmes laissés sans décision.\nRésultats sur la collection 2013\nPour la méthode DCM, nous avons utilisé la représentation R1 (caractère 8-grams) qui avait donné les meilleurs résultats sur la collection d\u0027apprentissage de 2013 et fixé ? à |Ap| 2 alors que pour DCM-voting, nous avons privilégié les espaces de représentation R1, R2, R3, R4 et R678 (cf. Tableau 1). Pour les quatre espaces ayant trait aux mots ou aux n-grams caractères, les documents sont représentés sous format vectoriel avec la pondération tf ? idf .\nLa table 3 présente les résultats produits par les trois méthodes sur la collection eval13 ainsi que les résultats des gagnants de la compétition par corpus puis sur l\u0027ensemble de la collection. La faible précision de DCM-classifier pour le corpus espagnol peut s\u0027expliquer par le manque de problèmes pour cette langue (seulement quatre) dans le corpus d\u0027apprentissage rendant difficile la construction d\u0027un modèle performant. Les trois méthodes produisent des résultats satisfaisants cependant, DCM et DCM-voting sont limités aux problèmes contenant au moins deux textes connus. Si on compare les résultats obtenus par nos méthodes avec ceux des gagnants de la compétition par corpus, alors il n\u0027y a que sur le corpus grec que la méthode DCM-classifier l\u0027emporte avec un score de 85%. Par contre, sur l\u0027ensemble de la collection, DCM-voting comme DCM-classifier obtiennent des résultats meilleurs ou équivalents à ceux du gagnant pour tous les critères d\u0027évaluation (F1, précision et rappel) . TAB. 4: Résultats de la 10-cross validation de DCM-classifier sur app2014\nRésultats sur les collections 2014\nLa collection 2014 contient un nombre plus élevé de problèmes et de types de document que la collection de 2013 et elle s\u0027avère plus difficile à traiter puisque plus de la moitié des problèmes ont un seul document connu (|A| \u003c 2) ; ce qui rend les méthodes DCM et DCMvoting inadaptées et inefficaces. Pour cette raison, seule DCM-classifier a été évaluée, d\u0027abord sur la collection d\u0027apprentissage en utilisant la technique de 10-validation croisée qui consiste à séparer le corpus à traiter en deux groupes, un pour entrainer le modèle et l\u0027autre pour le tester, puis sur la collection d\u0027évaluation dans le cadre du challenge.\nLes résultats obtenus par validation croisée sur l\u0027ensemble d\u0027apprentissage sont présentés dans la table 4. Ils confirment les performances de la méthode DCM-classifier.\nLe tableau 5 contient les résultats officiels obtenus lors de la compétition PAN14 in Author Identification, Stamatatos et al. (2014). Ils permettent de comparer notre méthode à celles des autres participants. DCM-classifier nous a permis d\u0027être classé en deuxième position lors de la compétition. Elle fournit de bons résultats en un temps relativement court. Il convient de noter que les temps de traitement affichés par le gagnant de la compétition sont en moyenne supé-rieurs à trois heures alors que ceux de DCM-classifier sont de l\u0027ordre de quelques secondes.\nUn des avantages de la méthode DCM-classifier, basée sur les arbres de décision, est de mettre en évidence les caractéristiques qui permettent le mieux d\u0027identifier l\u0027auteur d\u0027un do- 1/6 1% GR T OTcount 1/6 8% SP TAB. 6: Classement des attributs cument selon le type de corpus considéré. En effet, les documents sont décrits par des attributs calculés sur différents espaces de représentation mais l\u0027apprentissage intègre une phase de sé-lection de ceux qui sont les plus discriminants. Ainsi, on peut en déduire pour chaque corpus l\u0027importance de chaque attribut.\nLes figures 1 et 2 présentent les différents espaces de représentation utilisés pour deux corpus de langues différentes, on voit que ces espaces sont très différents et que les poids rattachés le sont aussi. La table 6 indique de manière synthétique la liste des attributs les plus utilisés sur l\u0027ensemble des corpus de la collection d\u0027évaluation 2014. Ce résultat confirme l\u0027intérêt de combiner plusieurs espaces de représentation pour résoudre le problème d\u0027identification d\u0027auteurs.\n"
  },
  {
    "id": "261",
    "text": "Introduction\nLes systèmes de recommandation (SR), visent à recommander à des utilisateurs des ressources pertinentes pour eux. Le filtrage collaboratif (FC) (Resnick et al., 1994) est une des approches les plus populaires de la recommandation.\nBien que la qualité des recommandations fournies par le FC soit considérée comme satisfaisante en moyenne (Castagnos et al., 2013), certains utilisateurs ne reçoivent pas de recommandations de qualité. Le manque de données sur ces utilisateurs est une des raisons possibles expliquant cette mauvaise qualité. Ce problème est appelé démarrage à froid (Schein et al., 2001). Parmi les autres raisons évoquées dans l\u0027état de l\u0027art se trouve la trop grande différence des préférences de ces utilisateurs, par rapport à celles des autres (Haydar et al., 2012). C\u0027est sur cette raison que nous nous focalisons dans cet article. En effet, le filtrage collaboratif suppose une cohérence entre les préférences des utilisateurs ; ces utilisateurs ne respectant pas ce critère, il semble normal qu\u0027ils se voient proposer des recommandations de mauvaise qualité. Ces utilisateurs peuvent aussi être considérés comme des données aberrantes, ou outliers. Nous choisissons de les appeler des utilisateurs atypiques.\nNotre objectif ici est d\u0027identifier ces utilisateurs atypiques. Nous proposons, dans ce travail préliminaire, plusieurs mesures permettant de les identifier, en exploitant uniquement leurs préférences.\nLa section 2 se focalise sur les systèmes de recommandation et l\u0027atypisme. Dans la section 3, nous proposons des mesures d\u0027identification des utilisateurs atypiques. Ensuite, nous pré-sentons les expérimentations menées pour valider ces mesures et nous concluons notre travail.\nÉtat de l\u0027art\nLa recommandation sociale, ou filtrage collaboratif (FC), exploite les préférences d\u0027utilisateurs (en général des notes sur des ressources) pour estimer des préférences inconnues. L\u0027approche à base de mémoire, et notamment les k plus proches voisins (knn), exploite les similarités de préférences entre utilisateurs. Bien que simple à mettre en oeuvre, intégrant dynamiquement les nouvelles préférences et fournissant des recommandations de qualité, cette approche ne passe pas à l\u0027échelle. L\u0027approche à base de modèle souffre moins du problème de passage à l\u0027échelle. La technique de factorisation de matrices (Hu et al., 2008), la plus répan-due, forme un sous-espace de caractéristiques latentes, dans lequel utilisateurs et ressources sont représentés, qui permet d\u0027estimer les préférences inconnues.\nDans la littérature, les utilisateurs que nous appelons atypiques sont nommés déviants, anormaux, etc. (Del Prete et Capra, 2010) et la définition qui en est faite varie légèrement. Les travaux dédiés à leur identification sont peu nombreux. La mesure d\u0027anormalité (Del Prete et Capra, 2010;Haydar et al., 2012) aussi appelée coefficient de déviance, déviance, etc., est la plus utilisée pour les identifier. Elle représente la propension qu\u0027a un utilisateur à noter différemment des autres. Elle exploite l\u0027écart entre les notes d\u0027un utilisateur sur des ressources et la note moyenne sur ces ressources (équation (1)).\noù n u,r est la note que l\u0027utilisateur u a donné à la ressource r, n r la note moyenne sur r, R u l\u0027ensemble des ressources notées par u et u leur nombre. Les utilisateurs dont l\u0027anormalité est très élevée sont considérés comme atypiques. Bien que peu complexe, cette mesure ne tient pas compte du comportement propre à chaque utilisateur et les ressources sur lesquelles les utilisateurs ne sont pas unanimes vont injustement augmenter l\u0027anormalité. (Bellogín et al., 2011) définit un indicateur de clarté qui identifie les utilisateurs ambigus (instables) dans leur notation, basé sur la mesure de l\u0027entropie. Il a l\u0027inconvénient d\u0027identifier comme instables des utilisateurs dont les préférences évoluent ou dont les préférences diffèrent en fonction des domaines des ressources. Cette mesure ne nous paraît pas adéquate car l\u0027approche sociale peut leur proposer des recommandations de bonne qualité. (Bellogín et al., 2011;Haydar et al., 2012;Griffith et al., 2012) identifient un lien entre l\u0027erreur commise sur chaque utilisateur et ses caractéristiques (nombre de notes, de voisins, etc.). (Haydar et al., 2012), forme des clusters d\u0027utilisateurs et identifie un cluster d\u0027atypiques : des utilisateurs avec une forte erreur et un fort indice d\u0027anormalité. Cependant, nous pensons que les atypiques ne sont pas toujours similaires entre eux (ce qui en ferait d\u0027ailleurs des utilisateurs non atypiques au sens de la recommandation sociale), le clustering échouera probablement à former des clusters d\u0027atypiques. C\u0027est dans ce sens que va le travail présenté dans (Ghazanfar et Prugel-Bennett, 2011), qui clusterise des utilisateurs et propose de considérer comme atypiques les utilisateurs qui ne sont proches du centre d\u0027aucun des clusters formés.\nL\u0027identification d\u0027utilisateurs atypiques peut être associée à l\u0027identification de données aberrantes (outliers) : un outlier est une donnée qui dévie tellement des autres données que cela laisse penser qu\u0027elle a été générée par un mécanisme différent. Les méthodes statistiques et le clustering sont également très utilisées dans le domaine de la détection d\u0027outliers (Aggarwal, 2013).\nNouvelles mesures d\u0027identification d\u0027utilisateurs atypiques\nPartant des travaux de la littérature, nous proposons de nouvelles mesures permettant l\u0027identification d\u0027utilisateurs atypiques.\nCorrKMax -Nous pensons que l\u0027approche knn échoue sur les utilisateurs n\u0027ayant pas suffisamment d\u0027utilisateurs similaires. CorrKM ax représente la similarité moyenne qu\u0027a un utilisateur u avec ses k utilisateurs les plus similaires (équation (2)).\noù CorrP earson(u, v) est la corrélation de Pearson entre u et v. V (u) représente les k utilisateurs les plus similaires à u. Nous pensons que les utilisateurs associés à une faible valeur de CorrKM ax(u) recevront des recommandations de mauvaise qualité.\nAnormalité CR (Anormalité avec Controverse sur les Ressources) -Cette mesure repose sur la mesure d\u0027anormalité de l\u0027état de l\u0027art. Elle suppose que l\u0027écart sur une ressource controversée n\u0027a pas le même sens qu\u0027un même écart sur une ressource consensuelle, ce qui n\u0027est pas considéré par la mesure d\u0027anormalité de l\u0027état de l\u0027art. Nous proposons de pondérer les notes par le degré de controverse de la ressource, fonction de l\u0027écart-type de ses notes. L\u0027Anormalite CR d\u0027un utilisateur u est présentée dans l\u0027équation (3).\noù contr(r) est la controverse associée à une ressource r. Cet indice est basé sur l\u0027écart-type normalisé des notes sur r. Où contr(r) \u003d 1 ? ?r??min ?max??min , avec ? r est l\u0027écart-type des notes de r. ? min et ? max sont respectivement le plus petit et le plus grand écart-type de notes possibles parmi les ressources.\nLe calcul d\u0027Anormalité CR est d\u0027une complexité comparable à celle de l\u0027anormalité de l\u0027état de l\u0027art. Elle peut donc être calculée fréquemment et ainsi prendre en compte les nouvelles préférences des utilisateurs.\nAnormalite CRU (Anormalité avec Controverse sur les Ressources et profil Utilisateur) -Ni Anormalité ni Anormalité CR ne tiennent compte du comportement général de l\u0027utilisateur auquel elles s\u0027appliquent. Un utilisateur sévère peut être identifié comme atypique alors qu\u0027il n\u0027est atypique que dans sa manière de noter, et non dans ses préférences et qu\u0027il recevra probablement des recommandations de qualité. Pour éviter ce biais, nous proposons de centrer les notes de chaque utilisateur par rapport à sa moyenne de notes. L\u0027Anormalité d\u0027un utilisateur u, notée Anormalité CRU (u) est calculée selon l\u0027équation (4) :\noù n Cr représente la moyenne des notes centrées des utilisateurs sur r, contr C (r) est l\u0027indice de controverse associé à r, calculé à partir de l\u0027écart-type des notes sur la ressource, centrées par rapport aux utilisateurs. Le calcul de Anormalité CRU (u) est certes plus coûteux en temps que Anormalité CR (u), mais devrait permettre une identification plus précise des utilisateurs atypiques. Notons que ces deux dernières mesures sont indépendantes de l\u0027approche de recommandation utilisées (knn ou factorisation de matrices), à l\u0027opposé de la mesure CorrKM ax.\nExpérimentations\nDans cette section, nous évaluons les mesures d\u0027identification des utilisateurs atypiques que nous proposons, en comparaison avec celles de l\u0027état de l\u0027art.\nNous utilisons le corpus de données de l\u0027état de l\u0027art MovieLens, composé de 100 000 notes (de 1 à 5) de 943 utilisateurs sur 1 682 films (ressources). Une division du corpus en 2 sous-corpus de 80% (pour l\u0027apprentissage) et 20% (pour le test) est effectuée. L\u0027état de l\u0027art souligne que les utilisateurs pour lesquels le système manque de données (démarrage à froid) reçoivent de mauvaises recommandations. De façon à ne pas biaiser notre évaluation, nous écartons du corpus les utilisateurs associés à du démarrage à froid : ceux avec moins de 20 notes dans le corpus d\u0027apprentissage (Schickel-Zuber et Faltings, 2006). Le corpus est alors réduit à 821 utilisateurs. La mesure Anormalité de l\u0027état de l\u0027art présente une corrélation de 0,453 avec la RMSE. Cette corrélation significative confirme le lien existant entre l\u0027anormalité d\u0027un utilisateur et l\u0027erreur commise par une approche knn : plus un individu est anormal (atypique), plus l\u0027erreur commise sera élevée. A l\u0027opposé, moins il est anormal, moins l\u0027erreur sera élevée.\nCorrélations entre les mesures et l\u0027erreur de recommandation\nAnormalité CR augmente la corrélation de 11% (0,504). La controverse associée aux ressources permet donc d\u0027améliorer l\u0027estimation de la qualité des recommandations fournies aux utilisateurs. Anormalité CRU prend également en compte le profil de l\u0027utilisateur. Une corréla-tion de 0,546 est obtenue (amélioration supplémentaire de 8%, et donc de 20% par rapport à l\u0027état de l\u0027art). La prise en compte des particularités de notation des utilisateurs permet donc d\u0027améliorer l\u0027estimation de la qualité des prédictions fournies aux utilisateurs.\nLa faible corrélation de la mesure CorrKMax (-0,22) indique que, contrairement à notre intuition, la qualité des voisins d\u0027un utilisateur n\u0027est pas corrélée à la qualité des recommandations dans le cadre d\u0027une approche de recommandation knn.\nErreur en prédiction sur les utilisateurs atypiques\nUne corrélation représente le lien entre deux variables sur un ensemble d\u0027observations. Cependant, il est possible qu\u0027un lien existe sur une seule partie des observations, ce qui ne sera pas reflété par la corrélation. Ici, nous nous intéressons uniquement aux utilisateurs qualifiés d\u0027atypiques, c\u0027est-à-dire à ceux ayant des valeurs d\u0027anormalité les plus extrêmes. Par consé-quent, dans la suite des expérimentations, nous nous intéressons uniquement à la répartition FIG. 1 -Répartition de la RMSE des utilisateurs atypiques avec l\u0027approche knn des erreurs observées sur les utilisateurs considérés comme atypiques. Pour représenter la ré-partition de ces erreurs, nous exploitons les quartiles et la médiane de ces erreurs, sur les 4 mesures d\u0027anormalité. Plus les erreurs sont élevées, plus nous pouvons considérer que la mesure est de qualité. Nous comparons ces répartitions à celle associée à l\u0027ensemble total des utilisateurs, dénommé Complet (Figure 1).\nNous utilisons un pourcentage d\u0027utilisateurs atypiques fixe, car les mesures n\u0027ont pas des valeurs d\u0027anormalité comparables. Nous avons fixé exprimentalement ce seuil à 6% des utilisateurs, cela correspond à environ 50 utilisateurs parmi les 821 utilisateurs.\nL\u0027erreur médiane sur l\u0027ensemble des utilisateurs (Complet) est de 0, 82. Celle de Anormalité est de 1, 26, ce qui correspond à une augmentation de l\u0027erreur de plus de 50%, elle est d\u0027ailleurs équivalente au troisième quartile de l\u0027ensemble complet. Cependant, 25% des utilisateurs qualifiés d\u0027atypiques ont une RMSE plus faible que la RMSE médiane sur l\u0027ensemble des utilisateurs. Par conséquent, Anormalité semble sélectionner un grand nombre d\u0027utilisateurs dont les recommandations sont de bonne qualité. Anormalité CR et Anormalité CRU pré-sentent de meilleurs résultats qu\u0027Anormalité. Anormalité CRU est la plus performante : plus de 75% des utilisateurs sélectionnés ont une RMSE supérieure à 1,25 : 75% des utilisateurs Anormaux CRU font partie des 25% de l\u0027ensemble complet des utilisateurs à recevoir les moins bonnes recommandations. Enfin, environ 50% des utilisateurs sélectionnés avec CorrKM ax reçoivent de bonnes recommandations, ce qui confirme les premières conclusions obtenues avec la corrélation.\nNous pouvons conclure que Anormalité CRU identifie de façon fiable les utilisateurs atypiques : ceux recevant des recommandations de mauvaise qualité avec une approche knn.\nConclusion et perspectives\nNotre objectif dans ce premier travail était d\u0027identifier, en recommandation sociale, les utilisateurs qui reçoivent des recommandations de mauvaise qualité, avant que des recommandations ne leur soient proposées. Nous avons fait l\u0027hypothèse que ces utilisateurs avaient des préférences différentes des autres utilisateurs : des utilisateurs atypiques. Nous avons proposé plusieurs mesures exploitant la similarité de préférence avec les autres utilisateurs, l\u0027écart des notes par rapport aux autres, le consensus de notation sur les ressources et le profil de nota-tion des utilisateurs. Nous avons montré que, sur un corpus de l\u0027état de l\u0027art, ces informations permettaient de prédire fiablement la mauvaise qualité des recommandations faites à un utilisateur. Nous pouvons donc conclure que les utilisateurs présentant des préférences différentes des autres utilisateurs reçoivent effectivement des recommandations de mauvaise qualité. Ces mesures peuvent donc être utilisées pour anticiper une mauvaise recommandation et la suite de ce travail portera naturellement sur la prise en compte des préférences atypiques des utilisateurs pour leur fournir des recommandations de qualité.\n"
  },
  {
    "id": "262",
    "text": "Introduction\nDepuis les débuts du TALN, la compréhension de texte fait l\u0027objet d\u0027un suivi particulier de plusieurs recherches. C\u0027est en faveur du développement rapide de la tâche d\u0027extraction d\u0027information que la tâche de REN s\u0027est manifestée. Elle consiste à rechercher les expressions réfé-rentielles (Ehrmann (2008)), qui recouvrent classiquement les noms désignant des personnes, des lieux, des organisations, des expressions temporelles et celles numériques, mais peuvent aussi se rapporter à des notions plus techniques comme les maladies. Dès la campagne MUC-6, la tâche de REN s\u0027est ainsi polarisée sur trois types d\u0027entités (Grishman et Sundheim (1996)), à savoir : ENAMEX (personnes, organisations et lieux), TIMEX (expressions temporelles), NUMEX (expressions numériques). Cette première définition a été étendue dans la campagne CoNLL (Tjong Kim Sang et De Meulder, 2003) où 4 classes ont été normalisées : personnes, organisations, lieux, Divers. Dans les campagnes d\u0027évaluation ESTER2 (Galliano et al., 2009) 8 catégories ont été normalisées à savoir personnes, fonctions,organisations, lieux, productions humaines, dates, montants et événements.\nTravaux Connexes\nAuparavant, la visibilité de la langue amazighe au Maroc était quasiment nulle. Récem-ment, et grâce aux revendications qui se sont faites à l\u0027aide de l\u0027IRCAM 1 , elle a été soumise à un processus de codification et de standardisation. Face à l\u0027augmentation vertigineuse des informations en langue amazighe, disponibles librement sur le Web, plusieurs recherches ont été entamées dans ce sens. Il y en a celles qui se concentrent sur la reconnaissance optique des caractères (OCR) (Es-Saady et al., 2012) et celles qui se focalisent sur le TALN que nous pouvons classer en deux grandes catégories : (1) ressources informatiques, y compris des études sur la construction des corpus amazighe (Boulaknadel et Ataa Allah, 2011) et (2) les outils du TAL qui ont été réalisés comme le concordancier (Boulaknadel et Ataa Allah, 2010), l\u0027analyseur morphologique (Nejme et al., 2013a,b) et (Ataa Allah et Boulaknadel, 2010). Quant au domaine de la REN, il a acquis un certain intérêt à travers les travaux réalisés de Talha et Boulaknadel (Talha et al., 2014b,a;Boulaknadel et al., 2014).\nAperçu général de notre approche\nOn distingue traditionnellement trois grandes approches : Approches symboliques qui reposent sur l\u0027utilisation de grammaire formelle construite par la main. Approches statistiques qui permettent d\u0027apprendre, des modèles d\u0027analyse de textes sur de large corpus annoté auparavant, et ensuite établir automatiquement une base de connaissances à l\u0027aide de plusieurs modèles numériques comme le CRF, SVM, etc. Au-delà de ces deux approches, il existe une autre qualifiée d\u0027hybride qui représente un arrangement entre ses antécédents. Dans notre contribution, nous proposons un système fondé sur une approche symbolique, vu la non disponibilité d\u0027un large corpus, où le repérage s\u0027effectue en se basant sur un ensemble de gazetteers et de règles qu\u0027on a construit manuellement tout en exploitant le principe de transducteurs à états finis disponibles sous GATE. \nArchitecture du système\nNotre système de repérage d\u0027entités nommées permet l\u0027identification des bornes des EN, ainsi que leur catégorisation dans des classes prédéfinies. Son architecture, détaillée sur la figure 1, comporte 3 modules qui effectuent un traitement séquentiel immédiat des données : \nPrétraitement Morphologique\nManipuler des textes écrits en langue amazighe nécessite une analyse préliminaire qui consiste à : La suppression des espaces supplémentaires existants entre les mots et l\u0027élimi-nation de tous les mots non-amazighes figurant dans le corpus. Notre analyse comprend deux phases :\n-La segmentation du texte amazighe en des phrases.\n-L\u0027identification des entités linguistiques de base « tokenisation ». Ces deux phases citées au dessus sont implémentées en utilisant, respectivement, les modules de GATE : le « Sentence Splitter » et le « Tokeniser ».   \nConstitution des gazetteers\nÉvaluation\nDiscussion\nLes entités nommées qui n\u0027ont pas été identifiées, correspondent soit à des entités qui ne font pas partie de nos ressources, soit à des entités qui font partie de nos ressources, mais sont ambiguës. Certaines entités sont cependant ambiguës pour cause d\u0027homographie, ou encore le cas d\u0027entités poly-référentielles, une même entité nommée peut convenir à plusieurs classes. La prépondérance des entités mal classées implique un manque d\u0027information que ce soit au niveau du contexte syntaxique ou de la présence des indices externes, qui, en plus de déter-mination des mots d\u0027arrêt qui permettent de décider ou s\u0027arrêter, augmente les probabilités d\u0027erreurs de délimitation. Une analyse approfondie a conduit aux constats suivants :\n-Enrichir nos gazetteers (Personne, Organisation, Localisation, DATE, NUM).\n-Effectuer un traitement syntaxique supplémentaire afin de mieux saisir la structure syntaxique des phrases amazighes avant d\u0027effectuer le repérage des entités nommées. -Étendre le nombre de règles linguistiques pour chaque classe d\u0027entité nommée.\nConclusions et perspectives\nDans cet article nous avons proposé un système de repérage des entités nommées amazighes à base de règle. L\u0027évaluation du système montre que les résultats obtenus sont assez encourageants et nous invitent à explorer de nouveaux modes de repérage d\u0027entités nommées, afin de tirer le meilleur parti de notre approche et affiner le repérage des entités nommées amazighes.\nRéférences\nAtaa Allah, F. et S. Boulaknadel (2010). Light morphology processing for amazighe language.\nIn proceeding of the Workshop on Language Resources and Human Language Technology for Semitic Languages, Volume 17.\n"
  },
  {
    "id": "267",
    "text": "Introduction\nLorsque l\u0027on cherche à comparer deux documents, on recherche tout élément présent dans l\u0027un qui est également présent dans l\u0027autre, ces éléments sont dénommés \"similitudes\". Les plus évidentes à voir à l\u0027oeil humain sont les similitudes exactes, les parties copiées d\u0027un document directement dans l\u0027autre. Cependant, reproduire informatiquement cette capacité humaine est une opération délicate. Ce procédé est souvent gourmand en temps, car passant par une comparaison mot à mot afin d\u0027identifier les séquences de mots identiques dans les deux textes. De ce fait, des méthodes beaucoup moins gourmandes ont vu le jour. Basées sur un système de n-grammes, elles extraient des séquences de n mots se suivant d\u0027un texte et en cherche la présence dans l\u0027autre. C\u0027est dans l\u0027optique de proposer une alternative à ces méthodes que nous allons décrire dans cet article une nouvelle approche de construction de séquences communes.\nAprès avoir présenté l\u0027état de l\u0027art, nous décrirons dans un premier temps le processus d\u0027intersection des deux textes, ensuite, la phase de construction des plus longues séquences communes et pour finir, nous présenterons l\u0027évaluation de notre approche en la comparant aux méthodes naïves de comparaison mot à mot et à la méthode classique n-grammes.\n2 Le « copier/coller » 2.1 Le phénomène « copier/coller » Le « copier/coller » touche particulièrement les étudiants, en Europe, 34,5% (Guibert et Michaut, 2011) d\u0027entre eux auraient déjà recopié tout ou partie d\u0027un document pour le présen-ter comme travail personnel. Cette fréquence rejoint celle de travaux américains (Park, 2003) estimant à environ 30% la proportion d\u0027étudiants ayant produit un travail reprenant des phrases d\u0027Internet sans en citer la source. Une étude européenne (Gibney, 2006) révèle que près d\u0027un étudiant français sur deux (46%) a déjà fait usage du plagiat pendant son cursus, contre environ un tiers des étudiants anglais et 10% des étudiants allemands. Ces résultats, qui paraissent déjà impressionnants, pourraient pourtant encore être sous-évalués. En effet, toujours selon la même étude, 40% des étudiants ne comprennent pas ce que signifie réellement le plagiat et n\u0027assimilent pas le « copier/coller » à de la tricherie. La recherche de « copier/coller » entre deux textes joue donc un rôle essentiel dans la prévention du plagiat et la protection du droit d\u0027auteur.\nÉtat de l\u0027art\nLes « copier/coller » sont en théorie les similitudes textuelles les plus facilement repé-rables et identifiables. En effet, la détection de celles-ci équivaut à comparer l\u0027égalité entre deux textes. Pour effectuer cette recherche automatiquement on est obligé de procéder à une comparaison mot à mot. Cette opération, étant beaucoup trop chronophage pour être intégrée dans des solutions à but commercial ou hébergées en ligne, comme des services anti-plagiat, des techniques alternatives ont dû être mises au point.\nLes méthodes les plus efficaces restent les méthodes classiques dites n-grammes (Torrejón et Ramos, 2013), qui consistent à construire puis comparer à partir de textes, des séquences de n éléments pouvant être des syllabes, des mots, des entités nommées, etc. La recherche de Barron-Cedeño et Rosso (2009) prouve qu\u0027en prenant des \"n-words\" (séquence de n mots se suivant) de petites tailles, deux ou trois par exemple, les résultats sont bien meilleurs qu\u0027en utilisant des longues séquences avec un n important. Sur le même principe mais plus originale, on peut citer la méthode de Stamatatos (2009), utilisant des n-grammes mais lors d\u0027une détection intrinsèque, c\u0027est-à-dire sans utilisation de document externe, on ne cherche pas des similitudes avec d\u0027autres documents mais on étudie l\u0027intérieur même du document analysé pour y repérer des irrégularités, des zones suspectes. Les n-grammes les plus pertinents ne sont pas toujours des séquences de mots, comme en atteste le travail de Shrestha et Solorio (2013), des n-grammes de mots vides (stop words) et d\u0027entités nommées sont également utilisés pour dé-tecter des parties de textes similaires entre deux documents. Toutefois, les méthodes les plus répandues sont les méthodes \"fingerprint\", créant une empreinte du document pour la comparer avec celle d\u0027autres documents. La plupart de ces méthodes (Kent et Salim, 2010) utilisent également des n-grammes pour construire l\u0027empreinte des documents.\nLes méthodes \"fingerprint\" divisent la plupart du temps le document en grammes de longueur n, ainsi les empreintes de deux documents peuvent être comparées et les points (i.e. grammes) concordants, identifiés comme étant des passages identiques dans les textes. Certaines méthodes de \"fingerprint\" (Stein et Eissen, 2006, 2007Lyon et al., 2001) vont audelà de la recherche de similitudes exactes et introduisent la notion de « similarités proches » pouvant ainsi détecter les paraphrases. Toujours dans cette optique, des recherches plus ré-centes (Simac-Lejeune, 2013; Kong et al., 2013) ne se contentent pas de comparer des mots ou groupes de mots d\u0027un document à un autre mais tentent d\u0027établir une corrélation « sémantique » entre deux documents par une approche utilisant des mots-clefs.\n3 Notre approche 3.1 Intersection de deux textes L\u0027idée de cette première étape est d\u0027effectuer une intersection de deux textes, afin d\u0027obtenir un tableau des mots présents dans les deux textes tout en conservant la position qu\u0027ils ont dans l\u0027un des deux. La procédure utilisée durant cette étude est la suivante :\n1. passage en minuscule des deux textes à comparer ; 2. transformation en tableaux de ces deux phrases en segmentant sur les espaces et les caractères de ponctuation (lemmatisation) (chaque tableau représente une phrase et chaque cellule d\u0027un tableau contient un lemme de la phrase à laquelle il correspond) ; 3. intersection des deux tableaux créés en conservant les offsets (positions) des mots du premier tableau et donc de la première phrase.\nConstruction de séquences maximales communes\nLa seconde et dernière étape consiste à construire, à partir du tableau obtenu à l\u0027étape précédente, les séquences d\u0027un minimum de n mots se suivant dans le premier texte, se suivant donc dans le tableau et étant également présentes dans le second texte. Le seuil n est le nombre de mots se suivant à partir duquel on peut déterminer qu\u0027une séquence est la copie d\u0027une autre et qu\u0027elle n\u0027est pas due au hasard. Nous pourrions dès lors nous poser la question : à partir de combien de mots se suivant une séquence peut être considérée comme réellement copiée ? En effet, il existe des séquences de trois mots ou plus, suffisamment fréquentes dans la langue, pour fausser la comparaison, comme les séquences « il était une fois » ou « nulle par ailleurs ». Cependant, les résultats des travaux de Barron-Cedeño et Rosso (2009) démontrent que sur de larges textes, il est tout aussi efficace de fixer un n petit, à deux ou trois par exemple.\nLa procédure de construction des séquences est la suivante :\n1. on déplace une fenêtre de glissement de n éléments dans le tableau en fonction du seuil n choisi afin de constituer des \"n-words\" se suivant donc forcément dans le premier texte ; 2. pour chaque \"n-word\" constitué, on vérifie son existence dans le second texte ; 3. tant qu\u0027une correspondance est trouvée et que la séquence existe bien dans les deux textes, on construit la séquence de taille n + 1 en y concaténant le mot suivant du tableau ; 4. dès que la séquence ne s\u0027y trouve plus, on récupère la séquence maximale commune (la séquence essayée précédemment avant que le test échoue) et on recommence depuis l\u0027étape 1 en déplaçant la fenêtre de glissement sur le mot suivant et en reprenant le n initial.\nÉvaluation et tests 4.1 La base de tests et protocole\nLa base de tests est composée de 200 textes, allant de 100 mots à environ 20 000 mots (avec une moyenne de 1500 mots), représentant 500 comparaisons de textes deux à deux annotés manuellement afin de savoir quel passage est réellement la copie d\u0027un autre. Pour tester correctement les performances des algorithmes évalués, la base comporte aussi bien des passages entièrement copiés que des paraphrases ou des reformulations plus complexes, ainsi que des textes « pièges » traitant du même sujet et donc employant le même vocabulaire mais n\u0027étant pas pour autant un « copier/coller » ou une reformulation quelconque d\u0027un autre texte présent dans le corpus. L\u0027intégralité de ces textes est en français. Ci-dessous la répartition des comparaisons :\n-120 comparaisons effectuées afin de détecter des textes entièrement « copier/coller » de façon exact ; -80 comparaisons afin de détecter des textes entièrement paraphrasés ou reformulés ; -200 comparaisons entre des textes ne comportant que quelques passages rigoureusement identique (copier/coller) ; -100 comparaisons entre deux textes ne comportant que quelques passages « similaires » (paraphrases ou reformulations de phrases et/ou paragraphes). Ces comparaisons sont réparties entre des travaux d\u0027élèves (mémoires financiers et scientifiques) avec leurs sources, différentes versions à différentes dates d\u0027un même article de Wikipédia et des extraits du corpus de la PAN-CLEF 2014 en matière d\u0027alignement de textes.\nRésultats\nLes résultats obtenus sur le corpus de test, par la méthode naïve de comparaison mot à mot, la méthode classique des n-grammes et notre méthode, sont représentées dans le tableau 1. La méthode n-grammes évaluée est celle décrite dans l\u0027article de Barron  (2009) disant que prendre un n de petite taille augmente l\u0027efficacité de détection, sachant que prendre des bigrams favorise le rappel, tandis que prendre un n supérieur favorise la précision. Ce phénomène s\u0027explique par le fait que prendre un petit n forme des séquences courtes, on ne manque ainsi aucune correspondance mais on favorise les faux positifs, baissant alors la précision. En revanche prendre un n plus important construit des séquences plus longues, réduisant ainsi la correspondance de chaînes et donc le rappel mais augmentant le taux de certitude des concordances et donc la précision. Cet article ne pose pas la question d\u0027optimisation de la détection en fonction du n choisi, on fixe donc n \u003d 2 pour la suite de notre évaluation.\nOn constate dans le tableau 1 que notre méthode donne de meilleur résultat que celle des n-grammes (0.76 de précision contre 0.72 et 1 de rappel contre 0.78 avec n \u003d 2 pour les deux méthodes). Toutefois, on peut voir dans le tableau 2 qu\u0027en moyenne elle est 15% moins rapide et 30% plus coûteuse en mémoire que la méthode n-grammes (en allouant 6. \nConclusions\nNotre approche montre donc des résultats supérieurs aux méthodes n-grammes classiques, dans le sens où elle recherche une séquence de taille minimale n et agrandit si possible la séquence trouvée afin d\u0027obtenir une séquence maximale commune. En revanche, elle est tout aussi dépendante du nombre n choisi que les méthodes n-grammes. Son temps d\u0027exécution et son usage de la mémoire restent supérieurs à ceux des méthodes n-grammes bien que nettement inférieurs à ceux de la méthode mot à mot.\nPour des travaux futurs, nous envisageons de confronter nos résultats à des méthodes ngrammes plus sophistiquées comme celles décrites dans l\u0027article de Shrestha et Solorio (2013).\nPour conclure, bien que moins rapide, notre méthode montre une précision équivalente aux méthodes n-grammes tout en proposant un rappel nettement supérieur.\nSummary\nPlagiarism detection most commonly use the most naive phase of similarities search, the detection of copy and paste. In this paper, we propose an alternative method to the standard verbatim comparison approach. The idea is to carry out an intersection of two texts to get a table of common words and to keep only the maximum sequences of consecutive words in one of the texts which also exists in the other. We show that this method is faster and less expensive in memory that commonly used scan texts methods. The goal is to detect identical passages between two texts faster than verbatim comparison methods, while operating more efficient than the n-grams.\n"
  },
  {
    "id": "269",
    "text": "Introduction\nLe BiClustering consiste à réaliser un clustering simultanément sur les observations et les variables. Govaert et al ont introduit une adaptation de l\u0027algorithme k-means au biclustering nommée \"Croeuc\" qui permet de découvrir tous les biclusters en même temps. Dans Labiod et Nadif (2011), les auteurs ont proposés une approche de factorisation CUNMTF, qui généra-lise le concept de la NMF Lee et Seung (1999). D\u0027autres modèles probabiliste de biclustering sont proposés dans Govaert et Nadif (2008). Le Biclustering a de nombreuses applications et devient un challenge de plus en plus important avec l\u0027augmentation des volumes de données. Cependant les bons algorithmes de clustering sont encore extrêmement utiles, il est donc néces-saire de les adapter aux nouvelles architectures massivement distribuées utilisant le paradigme MapReduce.\nLe paradigme MapReduce Dean et Ghemawat ( Dans ce papier nous proposons une nouvelle approche globale de biclustering basé sur les cartes auto-organisatrices et le calcul distribué. Le modèle de biclustering BiTM (Biclustering using Topological Maps) a deja donnée lieu à une publication dans Chaibi et al. (2014). Dans ce papier nous proposons une adaptation de ces travaux a l\u0027architecture MapReduce avec une implémentation de BiTM sous Spark, une technologie open source de calcul distribué incluant plusieurs paradigmes de programmation. Les principales problématiques abordées dans ce papier sont la minimisation de la fonction et la taille des données en entrée et en sortie des fonctions primitive (Map et Reduce) d\u0027un algorithme de biclustering topologique.\n"
  },
  {
    "id": "270",
    "text": "Introduction\nLes tweets sont des messages courts ne dépassant pas 140 caractères. Cette contrainte impose l\u0027utilisation d\u0027un vocabulaire particulier pour les rédiger et donc elle rend indispensable de connaitre leurs contextes pour les comprendre. Pour ces raisons, nous allons nous concentrer sur la tâche de contextualisation des tweets attribuée à INEX2014 1 . Les participants devaient fournir un contexte, pour permettre aux lecteurs de bien comprendre le tweet en utilisant un système de recherche d\u0027information SRI et système de résumé automatique SRA. Dans cet article, nous proposons une nouvelle approche de contextualisation de tweets basée sur les règles d\u0027association inter-termes.\nCet article est organisé comme suit : Dans la section 2, nous détaillons notre nouvelle approche. la section 3 sera consacrée aux différentes expériences menées , finalement nous conclurons dans la section 4. \nApproche proposée\nConclusion\nDans cet article, nous avons décrit une nouvelle approche de contextualisation de tweet basée sur règles d\u0027association inter-termes. Les résultats ont confirmé que la synergie entre les règles d\u0027association entre termes et l\u0027expansion de tweets est fructueuse. Dans un travail en cours, nous proposons d\u0027ajouter une phase de désambiguïsation pour réduire le bruit dans nos résultats. \nSummary\nTweets are short messages that do not exceed 140 characters. Since they must be written respecting this limitation, a particular vocabulary is used. To make them understandable to a reader, it is therefore necessary to know their context. In this paper, we describe our approach for the tweet contextualization. This approach allows the extension of the tweet\u0027s vocabulary by a set of thematically related words using mining association rules between terms.\n"
  },
  {
    "id": "271",
    "text": "Introduction\nL\u0027objectif des systèmes de recommandation est de prédire les choix et les préférences individuelles en fonction des comportements et des préférences observées. Le filtrage collaboratif est la technique la plus utilisée par les systèmes de recommandation. Il consiste à comparer les données d\u0027un utilisateur avec des données similaires d\u0027autres utilisateurs, basée sur les habitudes d\u0027achat et de navigation (Goldberg et al., 1992). Il permet aux commerçants de fournir des recommendations aux clients pour de futurs achats. Dans la suite, les données sont représentées par une matrice U de taille (n × p) où chaque ligne représente un utilisateur, les colonnes représentent des items, et chaque cellule (u ij ) de U est la note attribuée par un utilisateur i pour un item j. Les notes (u ij ) peuvent être binaires, ou réelles et dans ce cas U est appelée matrice réelle de notations. La matrice U peut être obtenue de manière explicite (en gardant les évaluations fournies par les utilisateurs pour des articles donnés) ou de manière implicite (en considérant qu\u0027un utilisateur préfère implicitement acheter ou pas les éléments présentés sur des pages Web visitées).\nDans le filtrage collaboratif (désormais désigné par FC), plusieurs approches sont utilisées. Les techniques de FC actuelles telles que celles basées sur la corrélation entre utilisateurs (Bobadilla et al., 2013) ou sur la factorisation matricielle (Koren, 2009;Sarwar et al., 2000;Delporte et al., 2014) sont couramment utilisées, mais nécessitent un temps de calcul très coûteux et ne peuvent être déployées en ligne. Dans ce contexte, la classification croisée ou co-clustering, qui consiste à regrouper simultanément les utilisateurs et les items, est une bonne solution. Elle est particulièrement appropriée dans les systèmes de de recommendation. Il est ainsi, par exemple, intéressant de disposer de groupes d\u0027utilisateurs appréciant un groupe de films. Dans (George et Merugu, 2005), les auteurs ont proposé une approche de FC basé sur un algorithme de classification croisée pondérée (COCLUST) qui implique le regroupement simultané des utilisateurs et des articles. Malheureusement, dans cette approche la prise en compte des données manquantes n\u0027est pas appropriée, conduisant ainsi à une faible qualité de recommandation. Nous proposons donc de faire un meilleur usage de cet algorithme par une prise en compte plus efficace des données manquantes. D\u0027autre part, en exploitant le potentiel des résultats de la classification croisée, nous développons un outil interactif de visualisation et d\u0027interprétation simultanée des groupes d\u0027utilisateurs et des groupes d\u0027items.\nLe reste du papier est organisé comme suit. La section 2 présente le système de FC basé sur la classification croisée (COCLUST). Les sections 3 et 4 fournissent des détails sur nos approches de gestion des notes manquantes et de visualisation. La section 5 démontre l\u0027efficacité des approches proposées sur des données réelles. Enfin, la section 6 conclut et présente les directions pour des recherches futures.\nNotation. Soit U la matrice des notes, une classification croisée en K × L co-clusters (blocs ou sous-matrices résultant d\u0027une classification croisée) par COCLUST conduit à une partition de l\u0027ensemble des utilisateurs en K classes et une partition de l\u0027ensemble des items en L classes. Notons Z \u003d (z ik ) la matrice de classification binaire de taille (n × K) dé-finie par z ik \u003d 1 si l\u0027utilisateur i appartient à la k i` eme classe et 0 sinon. De la même manière notons W \u003d (w j ) la matrice de classification binaire de taille (p × L) définie par w j \u003d 1 si l\u0027item j appartient à la i` eme classe et 0 sinon. Par commodité, nous utiliserons également z \u003d (z 1 , . . . , z n ) avec z i ? {1, . . . , K} (respectivement w \u003d (w 1 , . . . , w p ) avec w j ? {1, . . . , L} ; le vecteur des étiquettes des items). Enfin, nous utiliserons les indices i, j, k et pour désigner implicitement les lignes (utilisateurs), les colonnes (items), les classes en ligne (classes d\u0027utilisateurs) et les classes en colonnes (classes des items) respectivement.\n2 Classification croisée par COCLUST Partant de l\u0027algorithme weighted Bregman co-clustering (Banerjee et al., 2004), les auteurs dans (George et Merugu, 2005) ont proposé de s\u0027attaquer au problème de recommandation moyennant une reconstitution des données observées suivant une classification croisée donnée. Plus précisément, à partir de la réorganisation en co-clusters obtenus par l\u0027algorithme COCLUST, les auteurs proposent une matrice d\u0027approximation de U sparse par une matricêmatricê U \u003d (ˆ u ij ) non sparse où chaque cellule est définie de la manière suivante :\navec u k , u k. , u .. , u i , u j sont respectivement les moyennes calculées sur l\u0027ensemble des valeurs observées dans le co-cluster (k, dans la classe des utilisateurs k, dans la classe des items, pour chaque utilisateur et pour chaque item. Notons donc que dans cette formulationû formulationˆformulationû ij dépend de i, j, k et Sachant que les partitions Z and W sont inconnues, le critère à minimiser par COCLUST est le suivant :\noù M \u003d (m ij ) est une matrice binaire de taille (n × p) où m ij \u003d 1 si u ij est observé et m ij \u003d 0 si u ij est manquant. Une solution (optimum local) de ce problème peut être obtenue par une minimisation alternée ; sachant Z puis sachant W (Banerjee et al., 2004) jusqu\u0027à la convergence (Algorithm 1). A la convergence la prédiction est obtenue en utilisant (1).\nAlgorithm 1 Training based on Co-clustering.\n. , u i and u j ; ( ? k, i and j) 2. Update Z :\nComme indiqué précedemment les estimations au cours des itérations de COCLUST sont basées uniquement sur les données observées. Malheureusement et étant donné que le taux des données manquantes est très élevé (la sparsité de certaines matrices peut être de l\u0027ordre de 99 %), les prédictions sont biaisées impliquant des qualités de recommandation discutables. D\u0027autre part, George et Merugu (2005) ont proposé de remplacer la moyenne d\u0027un co-cluster vide (qui ne contient aucune note observée) par la moyenne globale. Cette stratégie peut fortement perturber la qualité de la classification croisée, et de plus elle ne garantit pas la convergence de COCLUST, comme le montre figure 1. Pour plus d\u0027explications, nous avons rapporté  \nGestion des notes manquantes dans le FC\nPour surmonter le problème des données manquantes dans le FC, deux approches sont principalement utilisées. La première consiste à travailler uniquement sur les valeurs observées, et la deuxième consiste à utiliser les procédures d\u0027imputation. L\u0027imputation par la moyenne est la plus couramment utilisée, elle consiste à remplacer les notes manquantes d\u0027un item/utilisateur par la moyenne de ses notes observées.\nCes approches peuvent être efficaces, si peu de valeurs sont manquantes, et que le mé-canismes des données manquantes est Missing completely at random ou Missing at random (Little et Rubin, 2002). Malheureusement le taux de notes manquantes dans le filtrage collaboratif est très élevé, ce qui rend ces approches inefficaces dans ce contexte. En effet, elles peuvent conduire à des estimations fortement biaisées, ce qui impacte négativement la qualité des recommandations. Pour illustrer ce propos, nous avons rapporté dans figure 3 un exemple d\u0027une matrice utilisateur-item, avant (figure 3a) et après l\u0027imputation par les moyennes des items (figure 3b). Si nous voulons ordonner les items en fonction des préférences des utilisateurs, l\u0027ordre le plus fiable serait : i1, i4, i2, i3 (tels que i1 est l\u0027élément le plus apprécié). En revanche si nous utilisons la matrice après imputation pour trier ces éléments de la même manière, nous obtiendrons l\u0027ordre suivant : i2, i4, i1, i3 qui est absurde, puisque i1 arrive seulement en troisième position et i2 arrive en première position. Cela est dû aux estimations fortement biaisées des moyennes des utilisateurs i2 et i4. Dans ce qui suit, nous allons présenter une nouvelle méthode d\u0027imputation basée sur la version en ligne de l\u0027algorithme kmeans sphé-rique (OSPK-means) (Zhong, 2005). Notre approche repose sur les deux étapes principales, 1) Partitionner l\u0027ensemble des utilisateurs en k classes, en utilisant l\u0027algorithme OSPK-means et en tenant compte des valeurs manquantes, 2) Estimer les notes manquantes, en se basant sur les résultats de la classification. Et enfin remplacer celles-ci dans la matrice U. Ci-dessous, nous décrivons de manière détaillée les différentes étapes de notre approche :\nEtape de Classification\nDans le but de partitionner l\u0027ensemble des utilisateurs en k groupes, nous proposons les procédures suivantes : Initialisation : Dans la version initiale de OSPK-means, l\u0027initialisation se fait par un tirage aléatoire de K centres initiaux parmi l\u0027ensemble des utilisateurs. Cependant cette stratégie n\u0027est pas efficace dans notre cas. En effet la probabilité de choisir un utilisateur avec très peu de notes observées, comme un centre de gravité initial est élevée. D\u0027autre part, sélectionner les centres initiaux uniquement parmi l\u0027ensemble des utilisateurs ayant noté beaucoup d\u0027items, permettrait seulement la détection de certains groupes. Afin de surmonter ces difficultés, nous proposons la procédure d\u0027initialisation suivante :\n1. Générer une partition aléatoire des utilisateurs en k classes.\n2. Estimer les centres initiaux comme suit : soit µ kj la j i` eme composante du centre k alors :\n; sinon où S est un seuil proportionnel à la taille de la classe k, et peut être défini par l\u0027utilisateur. Intuitivement, cette stratégie permet d\u0027estimer la j i` eme composante du centre de la k i` eme classe à partir des données disponibles, mais seulement s\u0027il y a suffisamment de notes observées pour dans cette classe. En revanche, quand peu de valeurs sont observées pour une composante j l\u0027estimation de celle-ci est pénalisée, en divisant par le seuil S. Etape de mise à jour : Lorsque l\u0027utilisateur choisi dans l\u0027étape d\u0027affectation ne dispose pas de suffisamment de notes-observées, l\u0027assignation de celui-ci n\u0027est pas fiable. Par conséquent le centre correspondant ne doit pas être déplacé dans le sens de cet utilisateur. Pour résoudre ce problème, nous introduisons une fonction binaire (h(u) ? {0, 1}) qui annule la mise à jour dans ce cas. L\u0027Algorithme 2 fournit plus de détails sur cette étape de classification.\nAlgorithm 2 Classification.\nInput : n normalized users u i ( i \u003d 1) in R p , K : number of user clusters, ? : learning rate, B : number of batch iterations ; Output : K Centroids µ k in R p , and z \u003d (z 1 , . . . , z n ) ; Steps : 1. Random initialization of the partition z ; 2. Estimation of initial centroids :\n; otherwize. for b \u003d 1 to B do for i \u003d 1 to n do 3. Assignment : for each user u i , compute z i :\nz i +?h(ui)ui ; t \u003d t + 1 ; end for end for\nEstimation des notes manquantes\nDans cette étape les notes manquantes sont estimées, en se basant sur les résultats de la classification. Cependant, une pondération des notes s\u0027avère encore nécessaire. Nous avons choisi d\u0027accorder plus d\u0027importance aux utilisateurs representant le mieux leur classe d\u0027appartenance en pondérant par cos(u i , µ k ) tout en atténuant l\u0027effet des utilisateurs qui ne sont pas en accord avec la préférence globale pour un item au sein de leur classe d\u0027appartenance à l\u0027aide de p(u ij ). Soit u a un utilisateur actif, k \u003d z a , la note pour pour un item j prend la forme suivante :\noù r med est la note médiane (r med \u003d 3 if u ij ? {1, 2, 3, 4, 5}, p(u ij ? r med ) est la probabilité qu\u0027un item j soit apprécié au sein d\u0027un groupe k, tel que :\nDans la section suivante, nous proposons d\u0027exploiter les résultats de classification croisée, dans le but de fournir aux utilisateurs une représentation interactive basée sur des graphes bipartis. Cette dernière permet non seulement de faciliter l\u0027interprétation des résultats, mais aussi de donner un sens aux préférences des utilisateurs dans le contexte du filtrage collaboratif.\nVisualisation des résultats de la classification croisée\nIl y a très peu de travaux qui se sont intéressés à l\u0027aspect visualisation dans le contexte des systèmes de recommandation. En effet ces systèmes sont souvent évalués pour leur capacité à faire de bonnes recommandations, mais leur fonctionnement reste abstrait pour les utilisateurs. Parmi les quelques travaux de visualisation on peut citer la méthode de visualisation des données du FC (Mei et Shelton, 2006) qui consiste à représenter les utilisateurs à côté des items qu\u0027ils aiment, sur le même espace euclidien. On peut aussi citer PeerChooser (Smyth et al., 2008) qui est un système de FC interactif qui permet de visualiser sous forme de graphe les interactions entre un utilisateur actif et son voisinage, tout en offrant la possibilité de modifier ce dérnier. Il existe aussi des travaux qui proposent de visualiser sur un plan à deux dimensions la liste d\u0027items à recommander pour un utilisateur actif, en utilisant les techniques classiques telles que l\u0027ACP, MDS, SOM.\nDans ce travail nous proposons une nouvelle approche de visualisation dans le contexte des systèmes de recommandations. Contrairement aux autres méthodes citées ci-dessus, notre approche est globale, c\u0027est à dire qu\u0027elle ne se focalise pas uniquement sur l\u0027utilisateur actif. Elle exploite la dualité inhérente de la classification croisée pour mieux mettre en évidence les affinités entre certains types de groupes d\u0027utilisateurs et certains types de produits. Plus pré-cisément, nous proposons de représenter les relations de préférences entre des groupes d\u0027utilisateurs et groupes d\u0027items, au moyen des graphes bipartis. Notre approche peut être décrite comme suit : 1) Classifier la matrice utilisateur-item en K classes d\u0027utilisateurs et L classes d\u0027items. Dans nos expérimentations nous avons utilisé COCLUST, après la gestion des données manquantes, présentée dans la section précédente, 2) Construire une matrice résumant les résultats de la classification croisée, dans laquelle chaque groupe de de lignes et chaque groupe de colonnes est représenté par les utilisateurs et les items les plus populaires (qui ont le plus de votes) respectivement, 3) Calculer la relation de préférence entre chaque groupe d\u0027utilisateurs et chaque groupe d\u0027items, à l\u0027aide de la formule (4), 4) Construire le graphe biparti, étape décrite en détail dans la partie expérimentale.\nSoit U \u003d (u ij ) la matrice résumée de l\u0027étape 2, avec n utilisateurs et p items. Et soit E \u003d (e ij ) une matrice binaire de n × p, tel que e ij \u003d 1 si l\u0027utilisateur i aime l\u0027item j et e ij \u003d 0 sinon. Alors la corrélation entre la k \nIntuitivement la corrélation (de préférence) 4, entre un groupe d\u0027utilisateurs k et un certain groupe d\u0027items représente la proportion des items populaires dans la i` eme classe ayant été appréciée par les utilisateurs les plus populaires de la classe k. La section suivante présente les résultats expérimentaux démontrant l\u0027efficacité des approches proposées.\nAlgorithm 3 Bipartite procedure.\nInput : U, K and L ; Output : C : correlation matrix between clusters ; Steps : 1. Compute (Z, W) into K row clusters and L column clusters ; 2. Compute U with the relevant users and items. for k \u003d 1 to K do for l \u003d 1 to L do 4. Compute C \u003d (c k ) the correlation matrix between clusters, by using (4) end for end for 5. Build the bipartite graph\nRésultats expérimentaux\nDans nos expériences, nous avons choisi les deux jeux de données de MovieLens 1 (ML-100K et ML-1M) qui sont beaucoup utilisés dans le domaine. L\u0027échantillon ML-1M est constitué de 6040 utilisateurs, 3952 films, et de 1 million de notes observées. L\u0027ensemble ML-100K contient 100,000 notes fournies par 943 utilisateurs pour 1664 films. La proportion des notes observées dans ce dernier est seulement de 6, 4%. Les évaluations des utilisateurs (u ij ) appartiennent à l\u0027intervalle : [1; 5], et les notes manquantes sont codées par : NA. Les données MovieLens fournissent également certaines informations démographiques sur les utilisateurs, telles que : le sexe, l\u0027âge, la profession, code postal ; et des informations de base sur les films tels que : le titre, le genre, la date de sortie, etc. A noter qu\u0027un film peut être de plusieurs genres à la fois. Nous proposons dans la suite de réaliser la comparaison des courbes ROC et de la F-measure, des systèmes de FC suivants : COCLUST, le FC incrémental basé sur la décompo-sition en valeurs singulières SVDCF (Sarwar et al., 2002), et COCLUST++ (COCLUST après la gestion des valeurs manquantes). Ces comparaisons sont réalisées sous recommenderlab (Hahsler, 2011), que nous avons combiné avec le langage C pour implémenter les différentes méthodes ci-dessus. Les courbes de figure. 4a sont construites en faisant varier le nombre d\u0027items à recommander de 1 à 40. Les deux figures 4a et 4b montrent une amélioration significative des performances de COCLUST, grâce à la gestion des données manquantes que nous proposons. On remarque aussi, une faible qualité des recommandations pour SVDCF, qui est due à une gestion des données manquantes inappropriée. En effet dans cette dernière approche (Sarwar et al., 2002), les notes manquantes sont remplacées par les moyennes des items dont les estimations sont fortement biaisées. En d\u0027autres termes cette imputation favorise les items avec très peu de notes observées, comme illustré dans la section 3 (figure 3). La figure 4d montre que même avec l\u0027étape d\u0027imputation COCLUST++ reste plus rapide que SVDCF.\nEn ce qui concerne les possibilités de visualisation exploitant la classification croisée, la figure 5 montre un exemple de graphe biparti, qui est construit comme suit 1) Classification de l\u0027ensemble ML-100k, en 6 classes utilisateurs et 8 classes d\u0027items, en utilisant COCLUST++, 2) Calculer les corrélations entre les groupes d\u0027utilisateurs et d\u0027items, via la formule (4), 3) Construire le graphe biparti où les rectangles de gauche représentent des groupes d\u0027utilisateurs, tandis que ceux de droite des groupes d\u0027items. Seuls les liens qui correspondent à de fortes corrélations sont représentés. Pour chaque groupe d\u0027utilisateurs, les deux professions les plus populaires sont présentées, de même les deux genres les plus populaires dans chaque classe  \nConclusion\nDans ce papier nous avons proposé une meilleure exploitation du potentiel de la classification croisée dans les systèmes de FC. Pour ce faire, nous avons développé une nouvelle stratégie pour une gestion efficace des données manquantes. Nous avons ensuite proposé une nouvelle approche interactive basée sur des graphes bipartis, permettant d\u0027interpréter et de comprendre les résultats de la classification croisée dans le contexte du FC. Les résultats expé-rimentaux montrent une amélioration importante des performances de la classification croisée dans le FC, grâce à une meilleure gestion des notes manquantes. Nous avons aussi montré, comment les représentations interactives basées sur des graphes bipartis peuvent aider les uti-\n"
  },
  {
    "id": "272",
    "text": "Introduction\nDans le contexte d\u0027une fouille exploratoire, le recours à des techniques de réduction de dimensionnalité permet classiquement de contourner la difficulté de représenter des résultats de clustering réalisés sur des données à haute dimensionnalité (HD). Les étiquettes de clusters, associées par exemple à des couleurs catégorielles, peuvent alors être appliquées aux points d\u0027un nuage 2D ou 3D.\nLes techniques de réduction de dimensionnalité sont susceptibles d\u0027introduire des artéfacts de déchirement et de recollement (Aupetit, 2007). Les algorithmes de clustering ne sont pas sujets à ces artéfacts, mais peuvent mener à des résultats sous-optimaux, ou avoir été mal paramétrés. L\u0027objet de cet article est de proposer un outil interactif de fouille visuelle combinant le meilleur de ces deux approches. Il utilise une projection 2D obtenue par t-SNE, une technique de réduction de dimensionnalité non-supervisée (van der Maaten et Hinton, 2008). Nous ne proposons pas un algorithme de clustering per se, mais plutôt une manière itérative d\u0027amélio-rer conjointement un clustering initial calculé de manière non-supervisée dans un espace HD, et une représentation 2D associée.\nUne présentation générale de notre outil est proposée en section 2. Les clusters sont amendés grâce à des techniques de diffusion d\u0027étiquettes, présentées en section 3. Réciproquement, l\u0027adaptation de la projection 2D aux clusters est évoquée dans la section 4. Les exemples donnés en section 5 et tout au long de cet article utilisent le jeu de données COIL-20 (Nene et al., FIG. 1 -Diagramme résumant la logique de l\u0027outil. 1996). Il contient 1440 images, réparties en 10 classes. Les images sont décrites par les intensités de leurs 1024 pixels sur une échelle de gris.\nPrésentation de l\u0027outil\nLa logique de l\u0027outil est résumée dans la figure 1. Il est paramétré par un jeu de données HD (i.e., \u003e 3), et par un clustering non-supervisé réalisé dans l\u0027espace HD.\nUne projection 2D initiale est calculée de manière non-supervisée par l\u0027algorithme itératif t-SNE. Elle est matérialisée par un nuage de points, dont les colorations sont associées aux étiquettes de clusters via un ensemble de couleurs catégorielles. À partir d\u0027une projection et d\u0027un clustering donnés, l\u0027utilisateur peut déclencher les actions suivantes :\n-Sélection d\u0027une restriction : optionnellement, l\u0027utilisateur peut limiter le rayon de son action à un ensemble de clusters sélectionnés directement en cliquant la légende (voir Figure 2e). -Sélection de pivots : en cliquant sur un des points dans le nuage (voir Figure 2d), l\u0027utilisateur définit un pivot pour la diffusion d\u0027une nouvelle étiquette de cluster. Un inspecteur interactif est à sa disposition pour associer une sémantique à chaque pivot (voir Figure 2c). -Diffusion d\u0027étiquettes : les étiquettes des pivots sélectionnés sont diffusées en utilisant la proximité 2D entre éléments. -Modification des dissimilarités : la répartition en clusters peut être utilisée pour influencer les dissimilarités sous-jacentes à l\u0027algorithme t-SNE. L\u0027utilisateur peut paramétrer le niveau de cet impact. Les actions de l\u0027utilisateur modifient la visualisation et les clusters, dont le nouvel état peut servir d\u0027entrée à une nouvelle itération du diagramme en figure 1. Des itérations sont effectuées jusqu\u0027à ce que l\u0027utilisateur soit satisfait du résultat. fastidieuses, nous proposons une diffusion semi-automatique basée sur la sélection de pivots par l\u0027utilisateur. La diffusion peut être calculée selon deux algorithmes issus de la littérature : -Propagation probabiliste : les étiquettes peuvent métaphoriquement sauter de manière probabiliste depuis les pivots associés, puis d\u0027élément en élément. Ce processus converge, et le résultat peut être obtenu sous une forme analytique impliquant de simples produits de matrices (Zhu et Ghahramani, 2002). -Coupes de l\u0027arbre de couverture minimal : l\u0027arbre de couverture minimal d\u0027un graphe peut être calculé grâce à l\u0027algorithme de Kruskal (Kruskal, 1956). Des coupes dans cet arbre isolent des composantes connexes du graphe.\nDiffusion d\u0027étiquettes\nCes opérations sont réalisées relativement à la distribution visuelle des éléments ; les distances 2D entre éléments dans la projection sont donc utilisées dans les algorithmes.\nL\u0027utilisateur commence par sélectionner un ou plusieurs pivots dans la restriction en cours (qui peut englober tous les éléments si aucun cluster n\u0027a été sélectionné dans la légende). Ces derniers peuvent être vus comme des prototypes de clusters, existants et à redécouper, ou à créer. Selon ses préférences, l\u0027utilisateur peut alors les propager exhaustivement, ou paramé-trer interactivement la coupe de l\u0027arbre de couverture minimal pour isoler des composantes connexes. Dans ce dernier cas, il peut aussi choisir de regrouper les composantes sans pivot dans un cluster résiduel, ou laisser leur étiquette telle qu\u0027avant l\u0027interaction.\nModification des dissimilarités\nPlutôt que d\u0027utiliser la distribution 2D des éléments pour modifier les clusters comme dans la section 3, l\u0027utilisateur peut utiliser la répartition en clusters pour modifier la projection 2D, de manière par exemple à renforcer la séparation des clusters.\nConsidérons le graphe complet entre les éléments dans la restriction en cours, pondéré par les dissimilarités dans l\u0027espace HD. Nous voulons utiliser l\u0027information portée par les clusters pour amender les dissimilarités HD entre éléments. Pour préserver la structure interne des clusters, nous proposons de restreindre la modification au sous-graphe multipartite induit par les clusters. La fonction cumulative normalisée de la distribution Beta est alors appliquée aux poids d\u0027arêtes associés :\nLes bornes a et b permettent d\u0027adapter la transformation aux valeurs de dissimilarité, e.g., à la valeur maximale observée dans la restriction, ou à la cohésion interne des clusters. Les changements trop disruptifs sont ainsi évités. L\u0027utilisateur peut alors paramétrer interactivement un rapprochement (respectivement un éloignement) des clusters en augmentant le paramètre ? (respectivement ?).\nL\u0027algorithme t-SNE se base sur des dissimilarités HD entre les éléments pour estimer leurs positions dans le nuage de points. Classiquement, ces dissimilarités sont initialisées par une distance Euclidienne dans l\u0027espace HD. Dans l\u0027outil, les dissimilarités peuvent être modifiées dynamiquement.\nL\u0027algorithme t-SNE peut être interprété comme une variante d\u0027algorithme force et ressort. La métaphore physique suivie par cette classe d\u0027algorithmes convertit des changements discontinus des forces en présence en mouvements continus. Ainsi, la discontinuité obtenue à l\u0027application de l\u0027équation (1) est convertie en mouvements continus, facilitant leur suivi par un utilisateur. Après une modification de dissimilarités, ce dernier peut alors suivre le changement progressif induit par son action.\nLes dissimilarités HD demeurent latentes à la projection 2D, et ne peuvent pas être observées directement dans la visualisation. Pour pallier cette limitation, nous avons incorporé l\u0027outil ProxiViz (Heulot et al., 2012), qui permet de mapper interactivement les dissimilarités sur le diagramme de Voronoï du nuage de points (voir Figure 3). L\u0027utilisateur dispose ainsi d\u0027une information plus complète avant de procéder à ses modifications. Ceci peut par exemple permettre de prendre en compte d\u0027éventuels artéfacts de projection (Aupetit, 2007).\nExemples\nAu cours de ses manipulations avec l\u0027outil, l\u0027utilisateur est confronté à la situation de la figure 4a. Le cluster vert, identifié par l\u0027algorithme de clustering dans l\u0027espace HD, est éclaté en 3 composantes dans la visualisation. L\u0027utilisateur souhaite les regrouper, en respectant le voisinage des composantes dans la projection.\nIl commence par vérifier la pertinence d\u0027un tel regroupement en utilisant l\u0027outil ProxiViz. Les composantes du cluster vert sont bruitées par les clusters violet et orange. Il commence FIG. 3 -Le survol du nuage de points déclenche l\u0027outil ProxiViz. Les dissimilarités HD par rapport au point de la cellule survolée sont mappées sur une échelle de gris, et colorent les cellules de Voronoï des éléments respectifs. Les étiquettes de clusters sont rappelées en colorant le contour des points, ainsi que la cellule du point survolé.\nFIG. 4 -a)\nLe cluster vert est réparti en 3 composantes, bruité par les clusters violet et orange. Un pivot est sélectionné pour chaque composante. b) L\u0027exécution d\u0027itérations de t-SNE sur la restriction en cours ne réunit que partiellement le cluster. c) Après mise à jour des dissimilarités, le cluster est effectivement regroupé. donc par définir une restriction à ces 3 clusters, et sélectionne des pivots pour isoler les composantes (voir Section 3). Les composantes sont ensuite réunies simplement en éditant la légende interactive.\nEn déclenchant t-SNE sur la restriction en cours, le cluster est partiellement réuni (voir Figure 4b). L\u0027utilisateur influence leur rapprochement en mettant à jour les dissimilarités (voir Section 4 et Figure 4c).\nConclusion\nDans cet article, nous avons exposé notre outil de clustering visuel et interactif. Les techniques de diffusion d\u0027étiquettes et de modification des dissimilarités permettent d\u0027enrichir mutuellement une projection 2D et un clustering itérativement mis à jour. La métaphore physique suivie par le nuage de points et les moyens de contrôle offerts par l\u0027interface permettent à l\u0027utilisateur de suivre le changement progressif induit par ses actions. Nous avons illustré l\u0027intérêt de l\u0027approche au travers d\u0027exemples.\n"
  },
  {
    "id": "273",
    "text": "Introduction\nLa découverte de motifs locaux introduite par Agrawal et Srikant (1994) consiste à extraire des informations pertinentes décrivant une portion des données. Evaluer et garantir la qualité des motifs extraits demeure une problématique très ouverte malgré le nombre important de propositions (Giacometti et al., 2013). Chacune de ces propositions repose explicitement ou implicitement sur une mesure d\u0027intérêt dont la qualité dépend de la complexité du modèle sous-jacent et de son ajustement aux données. Le modèle repose en général sur des fondements statistiques dont la complexité et la compréhension sont bien connues. A l\u0027inverse, l\u0027ajustement aux données reste une notion difficile à appréhender. Pourtant, c\u0027est probablement cette notion qui distingue la fouille de données des statistiques traditionnelles. L\u0027ajustement aux données est souvent connoté négativement et synonyme de sur-apprentissage par rapport aux données. De notre point de vue, l\u0027ajustement aux données n\u0027est pas un biais d\u0027apprentissage mais un moyen pour lever certaines hypothèses sur le modèle en les remplaçant par des mesures sur les données. Nous proposons d\u0027étudier l\u0027ajustement aux données à travers les interrelations entre motifs lors de l\u0027évaluation d\u0027une mesure d\u0027intérêt ou d\u0027une contrainte d\u0027extraction.\nLa qualité d\u0027une mesure repose sur sa capacité à isoler un motif singulier qui dévie des autres motifs communs. Pour cette raison, une mesure se doit de mettre en relation le motif évalué avec d\u0027autres motifs, dits motifs liés. Par exemple, la confiance de la règle d\u0027association X ? Y met en relation la fréquence de X ? Y (motif évalué) par rapport à la fréquence de X (motif lié). La qualité de la règle augmente avec la fréquence de X ? Y tandis qu\u0027elle diminue si la fréquence de X augmente (lorsque les autres fréquences restent constantes). Ces variations de la confiance sont en partie conformes aux deux axiomes formulés par PiatetskyShapiro (1991). De manière intéressante, ces axiomes permettent d\u0027étudier formellement le comportement des mesures d\u0027intérêt dédiées à l\u0027évaluation des règles d\u0027association. Dans cet article, nous proposons de généraliser ce principe en introduisant la notion de motif lié pour s\u0027attaquer à l\u0027évaluation de n\u0027importe quelle méthode de découverte de motifs.\nL\u0027objectif de ce travail est de formaliser la qualité et la sémantique des mesures d\u0027intérêt et contraintes en analysant les interrelations entre les motifs nécessaires à l\u0027évaluation de chaque motif extrait. Ce travail s\u0027inscrit dans la lignée des travaux sur l\u0027analyse des propriétés formelles vérifiées par les mesures d\u0027intérêt Piatetsky-Shapiro (1991); Tan et al. (2004); Geng et Hamilton (2006); Lenca et al. (2008); Hämäläinen et al. (2010) en les étendant aux contraintes d\u0027extraction. Nous formaliserons l\u0027interrelation entre motifs en introduisant l\u0027ensemble de motifs liés. Cet ensemble regroupe tous les motifs susceptibles d\u0027impacter l\u0027évaluation d\u0027un motif donné. Nous distinguerons les motifs liés positivement qui permettent d\u0027accroître la mesure d\u0027intérêt de ceux qui la font décroître, i.e., les motifs liés négativement. Nous formulerons alors trois axiomes que devraient satisfaire une mesure d\u0027intérêt ou contrainte. Chacun de ces axiomes impose des contraintes topologiques que doivent respecter les deux ensembles de motifs liés. Enfin, nous proposerons des critères d\u0027analyse de la complexité et de la sémantique d\u0027une mesure d\u0027intérêt ou contrainte. Nous introduirons finalement la complexité en évaluation qui repose sur la cardinalité de l\u0027ensemble des motifs liés.\nTravaux relatifs\nA notre connaissance, très peu de travaux se sont intéressés à l\u0027interrelation entre les motifs lors de l\u0027évaluation d\u0027une mesure d\u0027intérêt ou d\u0027une contrainte. De manière plus générale, l\u0027évaluation de la qualité des méthodes de découverte de motifs est une tâche ardue et peu étudiée.\nProtocoles expérimentaux\nLa plupart des méthodes d\u0027évaluation ou d\u0027analyse de la qualité concernant la découverte de motifs repose sur des protocoles expérimentaux où l\u0027objectif est de vérifier la conformité du résultat avec un étalon-or. Dans un contexte supervisé, il est possible d\u0027exploiter directement la variable cible comme référence. Ensuite, le cadre précision/rappel, l\u0027analyse ROC (Fawcett, 2006), la validation croisée (Kohavi, 1995), etc sont utilisés pour évaluer l\u0027écart entre les motifs extraits et la référence. L\u0027évaluation de la qualité des méthodes de découverte de motifs dans un contexte non-supervisé s\u0027avère bien plus difficile comme le rappellent de Lin et Chalupsky (2004). En effet, la validation des motifs extraits ne peut pas s\u0027appuyer sur un étalon-or explicite. Plusieurs stratégies sont alors mises en oeuvre pour obtenir un succédané de cet étalon-or.\nPremièrement, il est possible de s\u0027appuyer sur la connaissance d\u0027experts d\u0027un domaine (Carvalho et al., 2005). Avec un cas d\u0027utilisation, des experts du domaine sont sollicités pour juger la justesse des motifs découverts et ainsi, évaluer la méthode d\u0027extraction. La stratégie de redécouverte teste si un processus parvient à retrouver les connaissances bien établies dans un certain domaine. Deuxièmement, il est parfois possible de construire l\u0027étalon-or (Gupta et al., 2008;Zimmermann, 2013). Par exemple, Gupta et al. (2008) propose un protocole d\u0027évaluation quantitative des algorithmes d\u0027extraction de motifs approximés fréquents : (1) l\u0027extraction des motifs fréquents dans un jeu de données classique, (2) l\u0027ajout de bruit dans ce jeu de données, (3) l\u0027extraction des motifs approximés fréquents et enfin, (4) la comparaison des véritables motifs fréquents avec les motifs approximés fréquents. Ici, l\u0027étape 1 explicite l\u0027étalon-or. Enfin, l\u0027hypothèse nulle peut parfois être construite expérimentalement. Gionis et al. (2007)  Ces protocoles expérimentaux sont clairement utiles, mais néanmoins ils souffrent de plusieurs limites. Tous ces protocoles expérimentaux reposent sur l\u0027évaluation de la collection de motifs extraits. Ils ne peuvent donc fournir qu\u0027un résultat a posteriori, i.e., après l\u0027implémen-tation d\u0027un prototype et de son application sur un jeu de données. L\u0027évaluation est forcément dépendante du jeu de données considéré et les résultats sont difficilement généralisables à n\u0027importe quel jeu de données, de n\u0027importe quel domaine.\nOutils formels\nPlusieurs outils formels ont été proposés dans la littérature pour analyser qualitativement les méthodes de découverte de motifs. Premièrement, la taille des représentations condensées est souvent utilisée comme une mesure objective d\u0027évaluation de leur intérêt (Calders et al., 2004). Par exemple, une représentation condensée fondée sur les motifs fermés est toujours plus compacte qu\u0027une représentation condensée fondée sur les motifs libres. Les motifs fermés sont donc jugés comme plus intéressants. Cependant, les représentations condensées les plus compactes ne sont pas forcément les plus utilisées. Par exemple, les itemsets non-dérivables (NDI) sont rarement utilisés malgré leur taux de compression impressionnant. La sémantique complexe des NDI expliquerait cette impopularité pour certaines personnes.  (2010) les ont étendus aux motifs ensemblistes. A notre connaissance, de tels axiomes n\u0027ont jamais été appliqués à des contraintes ou des algorithmes de construction de modèles. De plus, ils se sont essentiellement concentrés sur les mesures dédiées à la recherche de corrélations. Comment généraliser ces axiomes à n\u0027importe quelle mesure d\u0027intérêt ou contrainte ?\nEnfin, à notre connaissance seuls deux travaux ont étudié l\u0027interrelation entre les motifs lors de l\u0027évaluation d\u0027une mesure d\u0027intérêt. Crémilleux et Soulet (2008) ont défini informellement la notion de contrainte globale. Il s\u0027agit de contraintes dont l\u0027évaluation met en correspondance plusieurs motifs. Giacometti et al. (2011) ont ensuite formalisé cette notion de contrainte globale en utilisant une algèbre relationnelle étendue spécifiquement pour la découverte de motifs. Notre cadre propose une définition formelle plus générale et plus précise, mais surtout permet de mieux analyser l\u0027interrelation entre les motifs lors de l\u0027évaluation. En particulier, nous ré-pondrons aux deux questions énoncées ci-avant.\n3 Formalisation de l\u0027interrelation entre motifs Fu et al., 2000) Bouncer and Picker Algorithme de sélection avec différentes heuristiques (Bringmann et Zimmermann, 2009) TAB. 1 -Définition de plusieurs méthodes de découverte de motifs fondées sur la fréquence\nMéthode de découverte de motifs\nDans cet article, nous modélisons une méthode de découverte de motifs par une mesure\n-le langage L correspond à l\u0027ensemble des parties de I (i.e., L \u003d 2 I ) 1 et -? est l\u0027ensemble de tous les jeux de données possibles sachant qu\u0027un jeu de données est un multi-ensemble de L.\nSi le jeu de données visé est clair, M (X, D) est simplement noté M (X). Sans perte de géné-ralité, nous considérons que l\u0027intérêt d\u0027un motif X augmente avec M (X) i.e., si le motif X est plus intéressant que\nNotre modélisation d\u0027une méthode de découverte de motifs par une mesure d\u0027intérêt M est suffisamment souple pour englober les principaux outils de la littérature :\n-Evaluation par mesure d\u0027intérêt : l\u0027évaluation par mesure d\u0027intérêt consiste à affecter un score ou un rang à chaque motif reflétant sa qualité (e.g., la all-confidence proposée par Omiecinski (2003)). -Extraction sous contraintes : l\u0027extraction sous contraintes consiste à extraire tous les motifs satisfaisant un prédicat de sélection qui détermine la pertinence d\u0027un motif. Par exemple, il est courant d\u0027utiliser un seuil minimal sur une mesure d\u0027intérêt pour filtrer les motifs (e.g., la contrainte de support minimal (Agrawal et Srikant, 1994) \nNotion de motifs liés\nLa qualité d\u0027une méthode de découverte de motifs repose sur sa capacité à isoler un motif singulier qui dévie des autres motifs communs. Pour cette raison, cette méthode doit mettre en relation le motif évalué avec d\u0027autres motifs, dits liés. Pour analyser cette interrelation entre motifs, nous proposons de déterminer son ensemble de motifs liés en identifiant l\u0027impact de chacun de ces motifs sur le motif évalué. L\u0027impact d\u0027un motif Y sur le motif évalué X peut se mesurer en observant s\u0027il existe deux jeux de données D et D quasi-équivalents où la seule variation de Y modifie l\u0027évaluation de X. Typiquement, la all-confidence d\u0027un itemset X introduite par Omiecinski (2003) correspond à la plus petite confiance des règles d\u0027association Y ? Z incluses dans X. Il est bien connu que la all-confidence peut être réécrite comme le ratio entre la fréquence de X et la fréquence maximale de ses items : f req(X)/ max i?X f req({i}). Dans ce cas, quand X est évalué, les motifs liés de X sont luimême et tous ses items. En effet, augmenter la fréquence d\u0027un item peut faire décroître la allconfidence de X. A l\u0027inverse, augmenter la fréquence de X augmente aussi sa all-confidence. Cet exemple conduit à deux observations d\u0027importance :\n1. Il y a deux catégories de motifs liés : ceux qui peuvent améliorer la qualité du motif évalué (ici, X) et ceux qui peuvent la détériorer (ici, les items qui constituent X).\n2. Les motifs liés impactent la mesure d\u0027intérêt qu\u0027on analyse (ici, la all-confidence) via une autre mesure d\u0027intérêt élémentaire (ici, la fréquence).\nSuivant ces deux observations, nous définissions formellement la notion de motif lié en nous appuyant sur la définition d\u0027équivalence avec exception :  Illustrons la définition 2 avec la all-confidence. Comme la all-confidence ne peut croître qu\u0027avec la fréquence de X, all-conf + f req (X) est égal à {X}. Nous verrons qu\u0027il est courant voire souhaitable que le motif évalué soit aussi un motif lié. Pour l\u0027ensemble des motifs liés négativement, nous obtenons que all-conf ? f req (X) \u003d {{i}|i ? X} car seuls l\u0027augmentation de la fréquence d\u0027un item de X peut faire diminuer all-conf (X). Le tableau 2 donne d\u0027autres exemples d\u0027ensembles de motifs liés. Une force de la notion de motifs liés est de bien identifier les motifs « réellement » impliqués dans l\u0027évaluation. Par exemple, la définition des motifs libres donnée dans le tableau 1 implique tous les sous-ensembles de X (avec le ?Y ? X). Pourtant, seuls les sous-ensembles directs sont des motifs liés.\nDans le tableau 2, les mesures, contraintes, algorithme de construction de modèles choisis reposent exclusivement sur la fréquence (comme mesure m qui implique les motifs liés). Il est à noter que les définitions de motifs libres/fermés auraient pu être présentées avec d\u0027autres mesures élémentaires (e.g., fréquence disjonctive ou fonction d\u0027agrégat). Les bordures néga-tive et positive pourraient être analysées avec d\u0027autres mesures suivant la contrainte monotone ou anti-monotone considérée (Mannila et Toivonen, 1997). De même, la notion de top-k motif est pertinente avec d\u0027autres mesures d\u0027intérêt que la fréquence. En changeant la mesure élé-mentaire m introduite dans la définition 2, l\u0027analyse des interrelations entre motifs se ferait de manière analogue.\noù k \u003d |X| et n \u003d |I| ; singletons correspond à {{i}|i ? X} ; sous-ensembles directs correspond à {X \\ {i}|i ? X} ; sur-ensembles directs correspond à {X ? {i}|i ? I \\ X} ; sous-ensembles correspond à 2 X \\ {X} ; treillis correspond à L \\ {X}.\nTAB. 2 -Analyse des méthodes suivant leurs ensembles de motifs liés\nAxiomes de qualité\nEn s\u0027inspirant de ce qui a été fait pour les mesures d\u0027intérêt dédiées aux règles d\u0027association, cette section énonce trois axiomes que devraient satisfaire une mesure ou une contrainte idéale. Le tableau 2 illustre la satisfaction ou non des axiomes par les différentes mesures et contraintes.\nRéflexivité\nRevenons sur l\u0027exemple de la all-confidence. Nous avons constaté que l\u0027augmentation de la fréquence de certains motifs avait un impact sur la all-confidence de X. Donc la fréquence est une mesure élémentaire d\u0027importance pour la all-confidence. Par ailleurs, il est souvent considéré que l\u0027intérêt d\u0027un motif augmente avec sa fréquence. Il paraît donc naturel qu\u0027augmenter la fréquence de X augmente également la all-confidence de X. Plus généralement, si l\u0027intérêt d\u0027un motif X augmente avec la mesure élémentaire m, l\u0027intérêt de ce motif X selon la mesure d\u0027intérêt M devrait également augmenter lorsque m(X) croît. Comme all-conf + f req (X) \u003d {X}, la all-confidence est bien une mesure réflexive par rapport à la fréquence. A l\u0027inverse, l\u0027extraction des motifs libres n\u0027est pas réflexive par rapport à la fréquence puisque f ree + f req (X) \u003d {X \\ {i}|i ? X} n\u0027inclut pas X. Tandis qu\u0027un motif est jugé plus intéressant quand sa fréquence augmente, il a moins de chance d\u0027être libre (car sa fréquence sera plus proche de celle de ses sous-ensembles). Par conséquent, la contrainte de liberté pourrait même être qualifiée d\u0027irréflexive par rapport à la fréquence (i.e., X ? f ree ? f req (X)). Nous reviendrons dessus dans la sous-section suivante. L\u0027axiome 1 est une généralisation de plusieurs propositions de la littérature où la mesure m est restreinte au support. Pour les règles d\u0027associations, Piatetsky-Shapiro (1991)  \nExclusivité\nPour être facilement compréhensible par l\u0027utilisateur final, le comportement d\u0027une mesure M doit toujours rester le même vis-à-vis de chaque motif lié. En d\u0027autres termes, un motif lié ne devrait pas permettre d\u0027augmenter la mesure M dans certains cas, et de la diminuer dans d\u0027autres.\nAxiome 2 (Exclusive) Une mesure d\u0027intérêt M est exclusive par rapport à m ssi aucun motif est à la fois lié positivement et négativement à un motif donné pour m :\nCet axiome est largement vérifié par les méthodes de la littérature comme le montre le tableau 2 (colonne A2). Par exemple, comme all-conf + f req (X) ? all-conf ? f req (X) \u003d ?, la allconfidence est exclusive par rapport à la fréquence. A l\u0027inverse, l\u0027extraction des motifs libres fréquents n\u0027est pas exclusive puisque X appartient à la fois à f ree\u0026f req + f req (X) à cause de la contrainte de fréquence et à f ree\u0026f req ? f req (X) à cause de la contrainte de liberté. Le non-respect de l\u0027axiome 2 complexifie la lecture d\u0027une méthode d\u0027extraction puisqu\u0027il devient nécessaire de se reporter au jeu de données ou à d\u0027autres motifs pour comprendre les motifs extraits. Par exemple, lors de l\u0027extraction des motifs libres fréquents, un motif X peut ne pas être extrait soit si sa fréquence est trop basse, soit si sa fréquence est trop élevée. A l\u0027inverse, pour les motifs fermés fréquents, un motif n\u0027est pas extrait si sa fréquence est trop faible (aussi bien si le motif est non-fréquent ou non-fermé). Nous estimons donc que la violation de l\u0027axiome 2 pourrait expliquer en partie l\u0027échec des motifs NDI même s\u0027ils constituent une représentation condensée extrêmement compacte.\nLa combinaison des axiomes 1 et 2 (notée A1+2 dans le tableau 2) implique naturellement que X ? M ? m (X). Si une mesure M viole cette propriété, M est dite irréflexive selon m. L\u0027extraction des motifs libres et celle de la bordure négative des motifs fréquents sont irréflexives.\nExhaustivité\nLa pertinence d\u0027un motif est d\u0027autant plus forte que son intérêt dépend de la variation de nombreux autres motifs. Pour cette raison, tous les motifs du langage L devraient avoir un impact sur la mesure M . En d\u0027autres termes, l\u0027ensemble des motifs liés devrait idéalement être égal à la totalité du langage L.\nAxiome 3 (Exhaustive) Une mesure d\u0027intérêt M est exhaustive par rapport à m ssi tous les motifs du langage sont liés à tout motif pour m :\nCet axiome exprime que l\u0027interrelation entre les motifs lors de l\u0027évaluation d\u0027une mesure M devrait concerner tous les motifs. Chaque variation du jeu de données mesurable à travers m(X) devrait avoir une incidence sur l\u0027évaluation de M . Bien sûr, la plupart des méthodes d\u0027extraction (mesures ou contraintes de la littérature) ne satisfont pas cet axiome. Cependant, nous pensons que cet axiome donne la direction à suivre et nous revenons longuement dans la section suivante sur la forme de l\u0027ensemble de motifs liés.\nComplexité et sémantique\nEn pratique, l\u0027axiome 3 est peu vérifié. Néanmoins, les méthodes d\u0027extraction proposées tendent plus ou moins à le satisfaire. Cette section propose d\u0027étudier deux grandes caractéris-tiques de l\u0027ensemble des motifs liés à savoir sa taille et sa forme.\nComplexité en évaluation\nSuivant l\u0027axiome 3, nous affirmons que la pertinence d\u0027un motif pour une mesure élémen-taire m est encore plus forte lorsque sa pertinence dépend de la variation de la pertinence de nombreux autres motifs selon m. Par conséquent, l\u0027intérêt d\u0027une mesure M (au sens de sa globalité) selon m se mesure avec la taille de l\u0027ensemble de motifs liés : De manière similaire, on peut déterminer que la complexité en évalua-tion de la productivité est exponentielle par rapport à la taille du motif évalué puisque tous les sous-ensembles sont impliqués dans l\u0027évaluation de cette contrainte. Suivant la complexité en évaluation, on dira donc que la productivité est plus intéressante (car plus globale) que la all-confidence car les interrelations sont plus nombreuses.\nA notre connaissance, la complexité en évaluation est le premier indicateur pour mesurer l\u0027interrelation entre les motifs lors de l\u0027évaluation d\u0027une mesure d\u0027intérêt. Cette complexité permet de comparer plusieurs mesures d\u0027intérêt entre elles. La colonne |M ± f req (X)| du tableau 2 indique la complexité en évaluation des mesures et contraintes définies dans le tableau 1. Il se dégage clairement 3 grandes classes correspondant à 3 complexités en évaluation : constant, linéaire et exponentiel.\nBien que le tableau 1 ne comporte qu\u0027un échantillon restreint de la découverte de motifs, la complexité en évaluation des méthodes d\u0027extraction de motifs semble avoir augmenté durant ces deux dernières décennies. Au-delà de l\u0027intérêt des motifs extraits, nous pensons que la complexité en évaluation reflète aussi la difficulté algorithmique à les extraire. Ainsi, l\u0027amé-lioration des techniques d\u0027extraction pourrait expliquer cette augmentation de la qualité des motifs extraits.\nTAB. 3 -Liens entre l\u0027ensemble des motifs liés, la sémantique et la complexité\nSémantique de l\u0027ensemble de motifs liés\nContrainte globale Le tableau 3 schématise les principales topologies observées notamment au sein du tableau 2 en les organisant en trois grandes classes de complexité évoquées dans la section précédente. Ces classes font écho à la notion de contrainte globale introduite par Crémilleux et Soulet (2008) puis définie formellement par Giacometti et al. (2011). Il s\u0027agit des prédicats de sélection dont la complexité est au moins linéaire :\nPropriété 1 (Contrainte globale) Une contrainte q : L ? {1, 0} est globale ssi il existe une mesure élémentaire m telle que la complexité en évaluation de q selon m est au moins linéaire. Propriété 2 (Webb et Vreeken (2013); Hämäläinen et al. (2010)) Une mesure d\u0027intérêt M se comportant bien doit vérifier ?X ? L : 2 X \\ {X} ? M ? f req (X) Non-redondance Toutes les méthodes de découverte de motifs visant à réduire les redondances exploitent les sous-et/ou sur-ensembles du motif évalué. La majorité des représenta-tions condensées s\u0027appuient exclusivement sur les sous-ensembles ou sur-ensembles directs. Modèle Nous avons constaté que tous les algorithmes de construction de modèles ont leurs motifs liés qui couvrent l\u0027intégralité du treillis comme c\u0027est le cas pour Bouncer and Picker (Bringmann et Zimmermann, 2009). Les modèles sont souvent vus comme une amélioration des représentations condensées. La complexité en évaluation confirme que les modèles sont plus intéressants que les représentations condensées. La complexité des motifs top-k fréquents se rapproche de celle des modèles sans toutefois l\u0027atteindre.\n2. Dans ce contexte, « un bon comportement » signifie que les corrélations doivent être évaluées plus favorablement que les non-corrélations.\nConclusion\nCet article a introduit la notion de motifs liés qui nous semble centrale pour analyser l\u0027interrelation des motifs pour les méthodes de découverte de motifs. Une force de notre approche est son large spectre d\u0027application qui va au-delà des mesures d\u0027intérêt pour traiter aussi bien l\u0027extraction sous contraintes que la construction de modèles. Pour la première fois, des axiomes de qualité concernent la problématique de la non-redondance. L\u0027introduction de la complexité en évaluation permet de dépasser le stade qualitatif pour mieux comparer plusieurs méthodes.\nPlusieurs axes de progression subsistent au sein de notre cadre. La définition actuelle des motifs liés repose sur une notion d\u0027équivalence entre jeux de données où seule une mesure élé-mentaire m est impliquée dans l\u0027évaluation de M ; comment tenir compte qu\u0027une autre mesure m peut potentiellement impacter M en parallèle ? Une réflexion sur la définition de mesure élémentaire et des interrelations entre mesures élémentaires est nécessaire pour répondre à cette question. Par ailleurs, d\u0027autres axiomes importants sur les mesures d\u0027intérêts mériteraient d\u0027être étendus en s\u0027appuyant sur la notion de motifs liés. Bien que l\u0027incidence de la mesure élémentaire m dans l\u0027évaluation de M soit cruciale sur la sémantique des motifs liés, nous n\u0027avons pas encore étudié les implications des propriétés de m sur celles de M .\n"
  },
  {
    "id": "274",
    "text": "Introduction et contexte\nLe problème de la prédiction de liens tel qu\u0027il est formulé dans l\u0027article de Liben-Nowell et al. (2007) peut être compris comme une tâche de classification binaire. Des outils classiques d\u0027apprentissage tels que les arbres de classification, les SVM ou les réseaux de neurones ont été utilisés pour le résoudre sur des réseaux biologiques et de collaboration (Pujari et al. (2012)). Cependant, ces méthodes ne permettent pas à l\u0027utilisateur de faire varier le nombre de prédic-tions selon ses besoins. Pour ce faire, il est possible de calculer un score pour chaque paire de noeuds, corrélé à la probabilité d\u0027existence d\u0027un lien entre ces noeuds, on obtient alors un classement, et l\u0027utilisateur effectue la prédiction en sélectionnant les T paires les mieux classées. Le score peut être basé sur la structure connue du réseau, mais également sur d\u0027autres sources d\u0027information : par exemple les attributs des noeuds, la dynamique des contacts ou la localisation géographique (Scellato et al. (2011)). Pour combiner les informations capturées par différents scores, on utilise des méthodologies d\u0027apprentissage de classements. Parmi les méthodes à disposition, certaines sont non-supervisées et peuvent être vues comme des mé-thodes de consensus, telles que celles décrites dans Dwork et al. (2001). Il existe également des méthodes supervisées : une solution consiste à se ramener à un problème de classification en effectuant une transformation deux-à-deux, plutôt que de considérer des éléments à ordonner, on examine des couples d\u0027éléments dont on cherche à dire lequel doit être classé audessus de l\u0027autre (Herbrich et al. (1999)). Malheureusement, cette méthode n\u0027est pas adaptée à de grands réseaux où le nombre d\u0027éléments à classer est élevé. Plus généralement, la plupart des méthodes d\u0027apprentissage de classements ont été créées pour des tâches de recherche d\u0027information, où l\u0027on souhaite une précision élevée sur un petit nombre d\u0027éléments (e.g., Burges et al. (2011)). Nous souhaitons ici au contraire pouvoir fixer le nombre de prédictions, quitte à perdre en précision, pour prédire des liens non-observés dans de grands réseaux sociaux, et nous définissons dans ce but une méthodologie simple mais efficace d\u0027apprentissage supervisé. 2 Classements non-supervisés Scores de classement. Le but de ce travail est de montrer comment notre méthode permet de combiner des informations issues de différentes sources, nous ne présentons ici que quelques caractéristiques structurelles permettant d\u0027associer à chaque paire de noeuds un score à partir duquel est construit un classement. On trouve dans la littérature beaucoup d\u0027indices, nous en avons choisi quelques classiques et proposons de les généraliser à des réseaux pondérés. Certaines de ces caractéristiques sont dites locales car elles ne considèrent que des paires de noeuds à distance au plus 2. On appelle N (i) l\u0027ensemble des voisins du sommet i : -nombre de voisins communs (CN) :\nDonnées\nD\u0027autres caractéristiques sont dites globales car elles sont calculées sur l\u0027ensemble de la structure du réseau et permettent de classer des paires de noeuds distantes :\n-indice de Katz (Katz) : calculé à l\u0027aide du nombre de chemins de longueur l de i à j (au sens d\u0027un multigraphe pour un réseau pondéré), noté ? ij (l), selon l\u0027expression : p.w(k, k )/W (k) et revenant en i avec une probabilité 1 ? p, l\u0027indice est la probabilité pour que ce marcheur soit en j dans l\u0027état stationnaire de la marche.\n-attachement préférentiel (PA), basé sur l\u0027observation dans les réseaux sociaux que les noeuds de fort degré tendent à créer plus de nouveaux liens : s PAw (i, j) \u003d W (i).W (j) 2 . Enfin nous utilisons une méthode pour agréger des classements dans le but d\u0027améliorer la qualité de la prédiction. Il s\u0027agit de la méthode de Borda, initialement définie pour obtenir un consensus dans un système de vote. On affecte à chaque paire un score correspondant à la somme sur l\u0027ensemble des classements des nombres de paires moins bien classées, soit :\nRésultats. Certaines métriques usuelles d\u0027évaluation des problèmes de classification, comme la courbe ROC, ne sont pas adaptées au problème, en raison de la forte asymétrie des classes. En effet, les graphes étant peu denses, on s\u0027attend à ce que le taux de faux positif soit élevé dans une large gamme de prédictions et rende l\u0027observation de la courbe ROC peu instructive. Comme nous souhaitons pouvoir ajuster le nombre de prédictions pour trouver un bon compromis entre précision (Pr) et rappel (Rc), nous visualisons les résultats à l\u0027aide du F-score et des courbes Pr-Rc.\nNous traçons sur la Figure 1 les résultats obtenus sur le graphe d\u0027apprentissage G learn pour prédire les liens A 2 ? A 2 en utilisant certains des scores définis précédemment (pas la totalité pour une question de lisibilité). On voit que l\u0027allure des courbes de F-score varient significativement d\u0027un indice à un autre, ce qui indique que chaque score permet de détecter des liens différents, ce dont nous chercherons à tirer parti dans RankMerging. Comme on peut s\u0027y attendre pour une méthode de consensus, la méthode de Borda améliore les performances de la classification, en particulier la précision sur la prédiction des paires les mieux classées. Étant donnée la difficulté de la tâche, la précision est peu élevée en moyenne : lorsque Rc est plus grand que 0.06, Pr est inférieur à 0.3 pour toutes les méthodes. Nous n\u0027utilisons que des scores structurels, rendant improbable la prédiction de liens entre paires de sommets éloignées dans le graphe, le rappel est donc limité à des valeurs relativement faibles, car augmenter le nombre de prédictions diminuerait drastiquement la précision.\nMéthode RankMerging\nLes différentes méthodes de classements précédemment citées ne sont pas sensibles aux mêmes types d\u0027information structurelle. Nous décrivons une méthode générique d\u0027apprentissage supervisé qui permet d\u0027agréger les classements en tirant partie de la complémentarité des sources d\u0027information. Celle-ci ne nécessite pas qu\u0027une paire soit bien classée selon chaque critère, comme pour une règle de consensus, mais qu\u0027elle soit bien classée selon au moins un. La procédure est dénommée RankMerging, une implémentation et un guide utilisateur sont à disposition sur http://lioneltabourier.fr/program.html.\n2. Katz et RWR sont calculés à l\u0027aide de sommes infinies, que nous approximons à l\u0027aide des quatre premiers termes afin de réduire le coût du calcul. La forte asymétrie des classes tend à diminuer les performances de l\u0027indice PA, nous limitons la prédiction avec cet indice aux paires de sommets distantes d\u0027au plus 3.\n3. Pour que cette méthode ne biaise pas en faveur des prédicteurs qui classent un grand nombre d\u0027éléments, on considère qu\u0027une paire classée dans r? mais pas dans r ? est aussi classée dans r ? , avec un rang équivalent à toutes les autres paires non-classées et en-dessous de toute paire classée. Pour plus de détails, voir Dwork et al. (2001). L\u0027idée centrale est de déterminer à chaque pas de l\u0027algorithme le classement qui prédit le nombre le plus élevé de tp dans les prochaines étapes. Dans ce but nous définissons une fenêtre W i pour chaque classement : c\u0027est l\u0027ensemble des g liens non-prédits classés à partir du rang ? i dans r i 4 . On appelle qualité ? i le nombre de tp dans W i . À chaque étape, le classement r i dont la fenêtre a la qualité la plus élevée est sélectionné (en cas d\u0027égalité, on choisit aléatoirement), et on ajoute alors la paire classée au rang ? i de r i au classement de sortie de la phase d\u0027apprentissage (r L M ). On met à jour ? i et le curseur de fin de la fenêtre, de manière à ce que chaque W i contienne toujours exactement g paires, puis on met à jour les qualités ? i . Au cours du processus, on enregistre à chaque pas les valeurs des curseurs ? 1 ...? ? , qui indiquent la contribution de chaque classement au classement agrégé. Cette procédure est itérée jusqu\u0027à ce que le classement agrégé r L M contienne le nombre de paires T fixé par l\u0027utilisateur. Cet algorithme présente l\u0027intérêt majeur de ne parcourir qu\u0027une seule fois chaque classement, soit une complexité en O(?N ) pour ? classements de taille N (on note que N ? T ).\nLa phase de test consiste à agréger les classements obtenus sur le réseau G test en utilisant les paramètres ? 1 ...? ? appris durant la première phase. L\u0027implémentation pratique est simple : à chaque étape on regarde quel classement a été choisi à l\u0027étape correspondante de l\u0027apprentissage, et on sélectionne la paire la mieux classée du classement correspondant pour le graphe de test. Si celle-ci n\u0027est pas déjà dans le classement agrégé de la phase de test (r Protocole expérimental et résultats. Nous évaluons les performances de RankMerging en les comparant à des techniques existantes. En premier lieu, nous confrontons les résultats à la méthode non-supervisée de Borda, vue précédemment. Nous comparons aussi à des méthodes de classification supervisées, même si celles-ci ne sont pas conçues pour varier le nombre T de prédictions possibles 6 . Nous utilisons les implémentations proposées dans la boîte à outils Python scikit learn (scikit-learn.org) des méthodes suivantes : les k-plus proches voisins (NN), les arbres de classification (CT) et AdaBoost (AB), en faisant varier les paramètres de manière à obtenir plusieurs points dans l\u0027espace Pr-Rc 7 . Suivant la description de la méthode faite précédemment, nous mesurons les ? i obtenus sur G learn pour découvrir les liens A 2 ? A 2 , puis nous les utilisons pour agréger les classements obtenus sur G test pour prédire les liens B ? B, en utilisant le facteur d\u0027échelle f ? 1.5. La sélection des caractéristiques pertinentes n\u0027est pas problématique ici. En effet, l\u0027utilisateur peut agréger autant de classements qu\u0027il le souhaite car, d\u0027une part, l\u0027addition d\u0027un nouveau classement a un faible coût computationnel, et d\u0027autre part, la méthode est conçue pour qu\u0027un classement n\u0027apportant pas d\u0027information nouvelle soit simplement ignoré pendant l\u0027agréga-tion. Le paramètre g de l\u0027algorithme est ici fixé par une simple extrapolation : on mesure la valeur de g qui permet de maximiser la qualité de l\u0027agrégation sur G learn , et on emploie la même pour l\u0027agrégation sur G test (ici g \u003d 200). Sur la Figure 2, nous traçons le F-score et la courbe précision-rappel obtenus pour RankMerging (g \u003d 200), en agrégeant les classements des indices suivants : AA w , CN w , CN , Jacc w , Katz w (? \u003d 0.01), PA w , RWR w (p \u003d 0.8) et la méthode de Borda appliquée à ces sept indices. RankMerging permet d\u0027améliorer les prédictions : la mesure de l\u0027aire sous la courbe Pr-Rc indique une amélioration de 8.3% par rapport à la méthode de Borda 8 . On pouvait s\u0027y attendre, dans la mesure où RankMerging est une méthode supervisée et utilise l\u0027information contenue dans le consensus de Borda. En fait, la méthode est conçue pour que n\u0027importe quel classement non-supervisé puisse être agrégé sans perte de performance. Nous vérifions cela en pratique en retirant un à un les classements et en constatant que l\u0027amélioration ne fait que décroître, et ce quel que soit l\u0027ordre dans lequel on retire les différents classements 9 . En ce qui concerne les méthodes de classifications supervisées, nous pouvons constater que leurs performances sont élevées uniquement pour un faible nombre de prédictions (inférieur à 2000), mais ces méthodes n\u0027étant pas conçues pour faire varier le nombre de prédictions, elles produisent de très faibles performances en dehors de leur domaine optimal.\n"
  },
  {
    "id": "275",
    "text": "Introduction\nLe problème de prédiction de séquences est un problème important en fouille de données, défini de la façon suivante. Soit un alphabet Z \u003d {e 1 , e 2 , ..., e m } contenant un ensemble d\u0027éléments (symboles). Une séquence est une suite d\u0027éléments totalement ordonnée s \u003d 1 , i 2 , ...i n où i k ? Z (1 ? k ? n). Un modèle de prédiction M est un modèle entraîné avec un ensemble de séquences d\u0027entraînement. Une fois entraîné, le modèle peut être utilisé pour effectuer des prédictions. Une prédiction consiste, à prédire le prochain élément i n+1 d\u0027une séquence 1 , i 2 , ...i n en utilisant le modèle M . La prédiction de séquences a des applications importantes dans une multitude de domaines tels que le préchargement de pages Web (Deshpande et Karypis, 2004;Padmanabhan et Mogul, 1996), la recommandation de produits de consommation, la prévision météorologique et la prédiction des tendances du marché boursier.\nUn grand nombre de modèles de prédictions ont été proposés pour la prédiction de sé-quences. Un des modèle les plus connus est PPM (Prediction by Partial Matching) (Cleary et Witten, 1984). Ce modèle, basé sur la propriété de Markov, a engendré une multitude d\u0027approches dérivées telles que Dependancy Graph (DG) (Padmanabhan et Mogul, 1996), All-korder-Markov (Pitkow et Pirolli, 1999) et Transition Directed Acyclic Graph (TDAG) (Laird et Saul, 1994). Bien que des propositions ont été faites pour réduire la complexité temporelle et spatiale de ces modèles (Begleiter et al., 2004), l\u0027exactitude de leurs prédictions a subi peu d\u0027amélioration. D\u0027autre part, un certain nombre d\u0027algorithmes de compression ont été adaptés pour la prédiction de séquences tels que LZ78 (Ziv et Lempel, 1978) et Active Lezi (Gopalratnam et Cook, 2007). De plus, des algorithmes d\u0027apprentissage machine comme les réseaux de neurones et la découverte de règles d\u0027association séquentielles ont été employés pour faire de la prédiction de séquences (Fournier-Viger et al., 2012;Sun et Giles, 2001). Néanmoins, ces modèles souffrent de limites importantes. Premièrement, la plupart d\u0027entre eux partent de l\u0027hypothèse Markovienne qu\u0027un événement ne dépend que de son prédécesseur. Or, ce n\u0027est pas le cas pour de nombreuses applications, ce qui nuit à l\u0027exactitude des prédictions. Deuxièmement, tous ces modèles sont construits avec perte d\u0027information par rapport aux séquences d\u0027entraî-nement. Donc, ils n\u0027utilisent pas toute l\u0027information disponible dans les séquences d\u0027entraîne-ment pour effectuer les prédictions.\nPour pallier ces limites, un modèle nommé Compact Prediction Tree (CPT) (Gueniche et al., 2013) a été récemment proposé. Il utilise une structure en arbre pour compresser les sé-quences d\u0027entraînement sans perte ou avec une perte minime d\u0027information. De plus, il emploie un algorithme de prédiction conçu pour tenir compte du bruit et de plusieurs événements anté-rieurs lors d\u0027une prédiction plutôt que seulement le dernier. Il a été montré que ce modèle peut obtenir des prédictions jusqu\u0027à 12 % plus exactes que PPM, DG et All-K-order-markov sur des jeux de données provenant de divers domaines, ce qui constitue un gain important. Néanmoins, une limite de CPT est sa complexité temporelle et spatiale élevée. Dans cet article, nous pallions ces problèmes en proposant trois stratégies pour réduire la taille et le temps de prédiction de CPT. De plus, nous présentons une comparaison expérimentale avec davantage de modèles de prédiction de la littérature : All-K-order Markov, DG, Lz78, PPM et TDAG. Les résultats expérimentaux sur 7 jeux de données réels montrent que le modèle résultant nommé CPT+ est jusqu\u0027à 98 fois plus compact et est jusqu\u0027à 4.5 fois plus rapide que CPT. De plus, CPT+ conserve une exactitude très élevée par rapport aux autres approches de la littérature.\nLe reste de cet article est organisé de la façon suivante. La section 2 décrit brièvement le modèle CPT. Les sections 3 et 4 proposent respectivement de nouvelles stratégies pour réduire la taille du modèle CPT et ses temps de prédiction. La section 5 présente l\u0027évaluation expérimentale avec plusieurs jeux de données et les principaux modèles de prédictions de la littérature. Finalement, la section 6 est dédiée à la conclusion et aux travaux futurs.\nLe processus d\u0027entraînement\nLe processus d\u0027entraînement génère trois structures distinctes à partir des séquences d\u0027entraînement : (1) un Arbre de Prédiction (AP), (2) un Dictionnaire de Séquences (DS) et (3) un Index Inversé (II). Pendant l\u0027entraînement, les séquences sont considérées les unes après les autres pour construire incrémentalement ces trois structures. À titre d\u0027exemple, la figure 1 illustre la création des structures de CPT par insertion successive des séquences\n(1) Insertion de ?í µí±¨, í µí±©, í µí±ª? . Chacun des noeuds de l\u0027arbre représente un élément et chacune des séquences d\u0027entraînement est représentée par un chemin partant de la racine de l\u0027arbre et se terminant par un noeud interne de l\u0027arbre ou une feuille. La construction de cet arbre a une basse complexité. Insérer une séquence de m éléments demande de parcourir/créer au plus m noeuds. La construction complète de l\u0027arbre est O(n) où n est le nombre de séquences à insérer. Tout comme un arbre préfixe, cet arbre est une représentation compacte des séquences d\u0027entraînement, car les séquences partageant un préfixe commun partagent un chemin dans l\u0027arbre. Par exemple, à la figure 1, les séquences s 1 , s 2 et s 3 partagent le même chemin correspondant au préfixe B Dans le pire cas, le gain spatial offert par cette compression est nul, mais en pratique, tout dépendant de la densité et de la similarité des séquences du jeu de données utilisé, l\u0027arbre peut offrir une réduction spatiale très importante allant jusqu\u0027à 98% (Gueniche et al., 2013).\nLe Dictionnaire de Séquences est une structure qui permet d\u0027extraire chacune des sé-quences d\u0027entraînement de l\u0027arbre de prédiction. Lors de la construction du modèle CPT, un identifiant unique est assigné à chaque séquence. Il est égal à 1 pour la première séquence insérée (dénoté s 1 ) et est incrémenté d\u0027un pour chaque séquence subséquente (s 2 , s 3 , ...). Le dictionnaire de séquences associe chaque identifiant de séquence s a à un pointeur vers un noeud de l\u0027arbre. Ce noeud représente le dernier élément de la séquence s a dans l\u0027arbre. Grâce à cette structure, il est possible de parcourir chaque séquence d\u0027entraînement dans l\u0027arbre de prédiction du dernier au premier élément.\nL\u0027Index Inversé permet d\u0027identifier rapidement dans quelles séquences apparaît un ensemble d\u0027éléments d\u0027une séquence à prédire. L\u0027index inversé contient un vecteur de bits v e pour chaque élément e de l\u0027alphabet Z présent dans les séquences d\u0027entraînement. Le k-ième bit d\u0027un vecteur de bit v e prend la valeur 1 si l\u0027élément e apparaît dans la séquence s k , sinon il prend la valeur 0. Par exemple, à la figure 1, le vecteur de bit de l\u0027élément C après l\u0027insertion des séquences s 1 , s 2 , s 3 , s 4 et s 5 est 10110, car C apparaît dans les séquences s 1 , s 3 et s 4 . L\u0027index inversé est utilisé pour déterminer rapidement les séquences d\u0027entraînement contenant un ensemble d\u0027éléments d\u0027une séquence à prédire. Cela est réalisé en faisant l\u0027intersection des vecteurs de bits des éléments. Par exemple, déterminer l\u0027ensemble des séquences contenant les éléments A et C est réalisé par l\u0027opération 11101 ? 10110, donnant le résultat 10000, autrement dit {s 1 }. Grâce à l\u0027index inversé, cette tâche est très rapide ; O(i) où i est le nombre d\u0027éléments dans l\u0027ensemble.\nLe processus de prédiction\nLe processus de prédiction de CPT utilise les trois structures décrites précédemment. Soit une séquence s \u003d 1 , i 2 , ...i n de n éléments et y, un nombre entier représentant le nombre d\u0027éléments de s à considérer pour faire une prédiction. Le suffixe de taille y de s dénoté P y (s) est défini comme étant P y (s) \u003d n?x+1 , i n?x+2 ...i n La prédiction du prochain élément de s est effectuée de la façon suivante : CPT identifie tout d\u0027abord les séquences similaires à P y (s), c.à.d. qui contiennent les derniers y éléments de P y (s) dans n\u0027importe quel ordre et positions. Puis, pour chaque séquence similaire, CPT considère son conséquent. Le consé-quent d\u0027une séquence u est la sous-séquence débutant après le dernier élément en commun avec P y (s) jusqu\u0027à la fin de u. Chaque élément e dans un de ces conséquents est ensuite stocké dans une structure nommé Table de Compte (TC) avec son nombre d\u0027occurrences (ce nombre est une estimation de la probabilité P (e|P y (s))). L\u0027élément ayant le plus grand nombre d\u0027occurrences est l\u0027élément prédit par CPT. La mesure de similarité utilisée pour déterminer les séquences similaires est de nature stricte, mais est relâchée dynamiquement par le processus de prédiction, pour deux raisons. Premièrement, avec une mesure de similarité trop stricte, une séquence à prédire peut n\u0027être similaire à aucune séquence d\u0027entraînement, et donc aucune prédiction n\u0027est possible. Deuxièmement, une mesure de similarité trop stricte ne permet pas de considérer qu\u0027une séquence peut-être partiellement similaire à une autre. Or, dans les applications réelles, il y a souvent des éléments présents dans les séquences qui sont du bruit. Pour relâcher la mesure de similarité, CPT suppose qu\u0027un ou plusieurs éléments présents dans le suffixe de la séquence à prédire sont du bruit et qu\u0027ils peuvent être ignorés lors du calcul de similarité. Le calcul de similarité pour un suffixe P y (s) est fait par niveau, où à chaque niveau k \u003d 1, 2, ..., |P y (s)| ? 1 toutes les sous-séquences de taille |P y (s)| ? k de P y (s) sont générées. Chacune des sous-séquence u est utilisée pour trouver les séquences similaires dans l\u0027ensemble de séquences d\u0027entraînement et pour mettre à jour la TC. Ce relâchement de la mesure de similarité se poursuit pour la séquence à prédire d\u0027un niveau à l\u0027autre tant que TC n\u0027a pas été mise à jour un nombre minimum de fois.\nStratégies de compression de l\u0027arbre de prédiction\nBien que CPT offre des prédictions plus exactes que les principaux modèles de prédiction de la littérature selon une étude antérieure (Gueniche et al., 2013), une limite importante de CPT est sa complexité spatiale. Il a été montré que la taille des structures de CPT est inférieure à All-k-order Markov, mais demeure nettement supérieures à d\u0027autres modèles comme DG et PPM. L\u0027arbre de prédiction étant la structure la plus imposante de CPT, nous proposons ci-après deux stratégies pour réduire sa taille.\nStratégie 1 : Compressions des Chaînes Fréquentes (CCF). Certaines répétitions peuvent être identifiées dans les séquences d\u0027entraînement. Dépendamment du jeu de données, ces répé-titions peuvent être nombreuses et fréquentes. La compression des chaînes fréquentes consiste à identifier les sous-chaînes fréquentes d\u0027éléments apparaissant dans les séquences d\u0027entraîne-ment, puis à remplacer les sous-chaînes fréquentes par des éléments individuels.\nSoit une séquence s \u003d 1 , i 2 , ..., i n Une séquence c \u003d m+1 , j m+2 , ..., j m+k est une sous-chaine de s, dénoté c s, si et seulement si 1 ? m ? m + k ? n. Pour un ensemble de séquences d\u0027entraînement S, une sous-chaîne d est fréquente si |{t|t ? S ? d t}| \u003e minsup pour un seuil minsup fixé par l\u0027utilisateur.\nLa compression des chaînes fréquentes est effectuée pendant la phase d\u0027entraînement de CPT en trois étapes : (1) identifier les chaînes fréquentes dans l\u0027ensemble des séquences d\u0027entraînement, (2) créer un nouvel élément dans l\u0027alphabet Z pour représenter chaque sous-chaîne fréquente et (3) remplacer les sous-chaînes fréquentes par l\u0027élément correspondant lors de la construction de l\u0027arbre de prédiction de CPT. L\u0027identification de séquences fréquentes dans un ensemble de séquences est un problème populaire en fouille de données, pour lequel un grand nombre d\u0027algorithmes ont été proposés. Pour cette tâche, nous avons adapté un des algorithmes les plus performants nommé PrefixSpan (Pei et al., 2001), afin de ne découvrir que les séquences fréquentes d\u0027éléments consé-cutifs (sous-chaînes). De plus, nous avons ajouté la contrainte que les sous-chaînes fréquentes doivent respecter des contraintes de longueur minimale minSize et maximale maxSize (deux paramètres).\nLes sous-chaînes fréquentes identifiées sont stockées dans une nouvelle structure nommée Dictionnaire des chaînes fréquentes (DCF). Cette structure associe un nouvel élément non présent dans l\u0027alphabet Z (dans les séquences d\u0027entraînement) à chaque sous-chaîne fréquente. Le DCF permet de rapidement convertir une sous-chaîne en son élément correspondant et viceversa. Lors de l\u0027insertion des séquences d\u0027entraînement dans l\u0027arbre de prédiction, le DCF est utilisé pour remplacer chaque sous-chaîne par son élément correspondant.\nÀ titre d\u0027exemple, l\u0027illustration (1) de la figure 2 affiche la compression de l\u0027arbre de pré-diction de l\u0027illustration (5) de la figure 1 par la stratégie CCF. La sous chaîne fréquence B a été remplacée par un nouveau symbole x, réduisant le nombre de noeuds de l\u0027arbre de pré-diction.\nLa stratégie de compression de séquences CCF a un effet seulement sur l\u0027arbre de pré-diction où son nombre de noeuds et sa hauteur tendent à diminuer grandement. La stratégie CCF est transparente pour le processus de prédiction de CPT. En effet, lors de l\u0027extraction de séquences similaires, les branches de l\u0027arbre de prédiction sélectionnées sont décompressées à la volée par DCF. L\u0027identification et le remplacement de branches simples sont faits en un seul parcours de l\u0027arbre de prédiction. L\u0027index inversé et le dictionnaire de séquences n\u0027étant pas influencés par cette approche, le seul changement au processus de prédiction est la décompression dynamique des branches simples lorsque nécessaire. La complexité de ce remplacement est de O(n * (1 ? t)) où s est le nombre de séquence et t le taux de recouvrement de l\u0027arbre, ce dernier est défini comme le \"ratio\" de noeuds qui partagent plusieurs séquences par le nombre total de noeuds dans l\u0027arbre.\nStratégie de réduction des temps de prédiction\nStratégie 3 : Prédiction avec réduction du Bruit Amélioré (PBA). Tel qu\u0027expliqué pré-cédemment, pour prédire le prochain élément s n + 1 d\u0027une séquence s \u003d 1 , i 2 , ..., i n CPT utilise le suffixe de taille y de s dénoté P y (s) (les y derniers éléments de s), où y est un paramètre propre à chaque jeu de donnée. CPT prédit le prochain élément de s en parcourant les séquences similaires à son suffixe P y (s). La recherche de séquences similaires est rapide (O(y)). Toutefois, le mécanisme de réduction du bruit lors des prédictions (décrit à la section 2) ne l\u0027est pas, car il requiert de considérer non seulement P y (s) pour une prédiction, mais aussi toutes les sous-séquences de P y (s) de taille t \u003e k. Plus y et k sont grands, plus le nombre de sous-séquences à considérer l\u0027est aussi, et donc le temps de prédiction. Lors d\u0027une tâche de prédiction, certains éléments dans une séquence à prédire peuvent être considérés comme du bruit si leur simple présence affecte de façon négative le résultat de la prédiction. La stratégie PBA se base sur l\u0027hypothèse que le bruit observé dans une séquence est constitué des éléments ayant une faible fréquence, où la fréquence d\u0027un élément est le nombre de séquences d\u0027entraînement contenant l\u0027élément. Pour cette raison, PBA enlève seulement les éléments qui ont une faible fréquence pendant la phase de prédiction. Puisque la définition du bruit de CPT+ est plus restrictive que celle de CPT, un moins grand nombre de sous-séquences sont considé-rées. Cette réduction à un impact positif et tangible sur les temps de calculs tel que présentés dans notre évaluation expérimentale (section 5). Le pseudo-code illustrant la stratégie PBA est présenté ci-après (Algorithme 1). L\u0027algorithme prend en paramètres le préfixe P y (s) à prédire, les autres structures de CPT, un taux de bruit et un nombre minimum de mise à jour à faire à la TC pour faire une prédiction. Le taux de bruit représente le pourcentage d\u0027éléments dans une séquence qui doivent être considérés comme du bruit ; un taux de bruit de 0 indique que les séquence n\u0027ont pas de bruit alors qu\u0027un taux de bruit de 0.4 signifie que 40% des éléments d\u0027une séquence pourrait être du bruit. PBA est récursive de nature et considère un nombre minimal de sous-séquences dérivées de P y (s) pour faire une prédiction. Le bruit est d\u0027abord retiré de chaque sous-séquence. Puis la TC est mise à jour. Lorsque le nombre minimal de mise à jour est atteint, une prédiction est faite comme dans CPT en utilisant la TC. La stratégie PBA est une généralisation de la stratégie de réduction du bruit utilisée par CPT. En effet, selon les paramètres utilisés, il est possible de reproduire le fonctionnement original de CPT. Les trois contributions principales apportées par PBA sont l\u0027imposition d\u0027un nombre minimal de mise à jour de la TC pour faire une prédiction, la définition du bruit basé sur la fréquence d\u0027un élément et la réduction relative du bruit par rapport à la longueur de la séquence.\nAlgorithme 1 : L\u0027algorithme de prédiction avec PBA input : PS : le suffixe P s, CPT : les structures de CPT, TB : le taux de bruit output : Seq : un ou plusieurs éléments prédits file.ajouter(PS); while nombreMiseAjour \u003c minNombreMiseAJourTC ? file.nonVide() do suffixe \u003d file.prochain(); elementsBruit \u003d selectionnerElementsMoinsFrequents(TB); foreach elementBruit ? elementsBruit do suffixeSansBruit \u003d copierSuffixeSansBruit(suffixe, elementBruit); if suf f ixeSansBruit.length \u003e 1 then file.ajouter(suffixeSansBruit); end mettreAJourCountTable(CPT.countTable, suffixeSansBruit); nombreMiseAjour++; end retourne faireUnePrediction(CPT.countTable); end\nÉvaluation expérimentale\nNous avons effectué une série d\u0027expériences pour comparer la performance de CPT+, CPT et les principaux modèles de prédiction de la littérature All-K-order Markov, DG, Lz78, PPM et TDAG. Pour implémenter CPT+, nous avons obtenu et modifié le code source proposé dans l\u0027article original de CPT (Gueniche et al., 2013). Pour permettre la reproduction des expériences, le code source des modèles et jeux de données sont fournis à l\u0027adresse http: //goo.gl/LE4uYO. Tous les modèles sont implémentés en Java 8. Les expériences ont été réalisées sur une machine dotée d\u0027un processeur deux coeurs Intel i5 de 4ème génération avec 8 Go de mémoire vive et un SSD en SATA 600. Tous les modèles de prédiction utilisés ont été configurés empiriquement pour tenter de donner des valeurs optimales à chacun de leurs paramètres. PPM et LZ78 n\u0027ont pas de paramètres, DG et AKOM ont respectivement une fenêtre de 4 et un ordre de 5, finalement, par soucis d\u0027espace, TDAG à une hauteur maximale de 6. CPT à 4 paramètres et CPT+ en a 8, leurs valeurs sont elles aussi déterminées via une exploration expérimentale de l\u0027espace de valeurs possibles. Ces valeurs sont accessible dans les fichiers sources du projet. Les paramètres propres à l\u0027expérience se limitent à la longueur minimale et maximale des séquences utilisées, la taille du suffixe à considérer pour une séquence à prédire et la quantité d\u0027éléments à prédire pour chacune des séquences.\nDes jeux de données ayant des caractéristiques variées ont été utilisés (cf. Table 1) : sé-quences courtes/longues, séquences denses/éparses, petit/grands alphabets et divers types de données. Les jeux de données BMS, Kosarak, MSNBC et FIFA consistent en des séquences de pages Web visitées par des utilisateurs sur un site Web. Dans ce scénario, les modèles de pré-diction sont appliqués pour prédire la prochaine page Web que visitera chaque utilisateur. Le jeu de données SIGN est un ensemble de phrases exprimées en langage des signes, transcrites à partir de vidéos. Bible Word et Bible Char sont deux jeux de données qui proviennent de la Bible, livre religieux, le premier est l\u0027ensemble des phrases découpées en mots et le second est l\u0027ensemble des phrases découpées en caractères.\nPour l\u0027évaluation des prédictions des modèles, une prédiction est soit un succès, un échec, ou une abstention (si un modèle ne peut effectuer une prédiction). Deux mesures sont utilisées. La couverture est le nombre d\u0027abstentions divisé par le nombre de séquences à prédire. L\u0027exactitude (alias précision) le nombre de succès divisé par le nombre de séquences à prédire.\nNom\nNombre Expérience 1 : comparaisons des optimisations. Dans cette première expérience, nous avons tout d\u0027abord évalué les améliorations spatiales présentées à la section 3 en terme de taux de compression et de temps de calcul à l\u0027entraînement. Les autres mesures de performance telles que le temps de prédiction, la couverture et l\u0027exactitude ne sont pas affectées par la compression de l\u0027arbre de prédiction. Pour un arbre de prédiction A avec s noeuds avant compression et s2 noeuds après compression, le taux de compression tc a de A est défini comme tc \u003d 1 ? (s2/s), et est compris entre 0.0 et 1.0 non inclusivement. Plus la valeur est haute, plus la compression est importante. Les deux stratégies de compression sont évaluées d\u0027abord individuellement (dénotées CCF et CBS) puis en conjonction (dénoté CPT+). Toute compression permet d\u0027obtenir un gain spatial au prix d\u0027un coût temporel. La figure 3 présente cette relation pour chacune des stratégies de compression. Les résultats présentés à la figure 3 montrent que le taux de compression de l\u0027arbre varie selon le jeu de données de 58.90% à 98.65%. CCF offre un taux de compression moyen de 48.55% avec un faible écart type de 6.7%. alors que CBS à un taux de compression moyen de 77.87% avec un écart type beaucoup plus prononcé de 15.9%. L\u0027efficacité de CBS est dépen-dante au jeu de données ; dans le cas de MSNBC, qui est le jeu de données le moins affecté par les stratégies de compression, la faible cardinalité de son alphabet permet à MSNBC d\u0027être naturellement compressé grâce au fort recouvrement des branches de son arbre de prédiction. En effet, MSNBC ne possède que 17 éléments uniques et même si la taille moyenne des sé-quences ressemble à celle des autres jeux de données, la taille de son arbre avant compression est très petite. Le jeu de données où les stratégies de compression CCF et CBS sont les plus effectives est SIGN. SIGN a un très faible nombre de séquences, mais chacune d\u0027elle est très longue (en moyenne 93 éléments). Ces caractéristiques font en sorte que son arbre de prédic-tion a un faible taux de recouvrement et donc une importante partie de ses noeuds n\u0027ont qu\u0027un seul fils ; ce qui rend ce jeu de données un candidat idéal pour la stratégie CBS. CBS offre un taux de compression de 98.60 % pour SIGN.\nLa figure 3 présente également les temps d\u0027entraînement engendrés par les deux stratégies de compression de CPT, CBS et CCF. La mesure utilisée est un facteur multiplicatif du temps d\u0027entraînement. Par exemple, un facteur de x pour CBS signifie que CBS a eu une phase d\u0027entraînement x fois plus longue. Pour tous les jeux de données à l\u0027exception de SIGN, CBS est plus rapide que CCF. Il est intéressant d\u0027observer que le temps pris par la combinaison des deux stratégies de compression n\u0027est pas simplement une addition de leur coût d\u0027entraînement.\nCBS et CCF sont appliqués indépendamment à CPT et pourtant l\u0027utilisation de CBS réduit les temps de calcul de CCF grâce à une diminution du nombre de branches qui ont besoin d\u0027être compressées.\nNous avons également évalué le gain en temps de prédiction et l\u0027exactitude (précision) obtenue en appliquant la stratégie PBA. La figure 4 (gauche) illustre les temps de prédiction de CPT+ (avec CBA), et ceux de CPT. Les gains temporels sont importants pour la plupart des jeux de données notamment pour SIGN et MSNBC où les temps d\u0027entraînement sont jusqu\u0027à 4.5 fois moindres. Pour les jeux de données Bible Word et FIFA, les temps de prédiction sont plus élevés pour obtenir un gain en exactitude comme le montre la figure 4 (droite). L\u0027effet de CBA sur l\u0027exactitude des prédictions est positif pour tous les jeux de données sauf MSNBC. Cette amélioration s\u0027élève jusqu\u0027à 5.47% dans le cas de Bible Word. CBA se montre donc une stratégie effective pour à la fois réduire les temps de prédiction et augmenter l\u0027exactitude des prédictions. Expérience 2 : Mise à l\u0027échelle. Nous avons également comparé la complexité spatiale de CPT+ (avec ses deux stratégies de compression) avec celle de CPT et All-K-order Markov, DG, Lz78, PPM et TDAG, en termes de mise à l\u0027échelle par rapport au nombre de séquences. Les deux seuls jeux de données utilisés sont FIFA et Kosarak à cause de leur grand nombre de séquences (573,060 et 638,811 respectivement). L\u0027accroissement du nombre de séquences dans cette expérience est quadratique et s\u0027arrête à 128,000 séquences dû aux énormes temps de calcul requis pour réaliser chaque expérience. La figure 5 présente les résultats. Le taux de compression de CPT+ tend à baisser très légèrement avec l\u0027accroissement du nombre de sé-quences, ce phénomène est causé par un recouvrement de plus en plus important des branches dans l\u0027arbre de prédiction ; car la taille de l\u0027alphabet étant constante, plus de séquences sont utilisées et plus de branches s\u0027unifient. Les modèles DG et PPM ont une croissance linéaire, car ils sont basés sur la taille de l\u0027alphabet et indirectement sur le nombre de séquences d\u0027entraînement. Les autres modèles ont tous une croissance beaucoup plus importante que DG et PPM, notamment TDAG et LZ78.\nExpérience 3 : Comparaison avec les autres modèles de prédiction Dans l\u0027expérience 1, nous avons comparé l\u0027exactitude des prédictions de CPT+ avec celle de CPT afin d\u0027évaluer la contribution de la stratégie PBA. Dans cette expérience, nous effectuons une comparaison de l\u0027exactitude celle des autres principaux modèles de prédiction de la littérature All-K-order Markov, DG, Lz78, PPM et TDAG, sur les mêmes jeux de données. Il est à noter que nous ajoutons dans cette comparaison deux modèles de prédictions (Lz78 et TDAG) qui n\u0027ont pas été utilisés dans l\u0027article original proposant CPT. \nConclusion\nDans cet article, nous avons présenté trois stratégies pour réduire la taille et le temps de prédiction de CPT, nommées CCF (Compression des Chaînes Fréquences), CBS (Compression des Branches Simples) et PBA (Préduction avec réduction du Bruit Améliorée). Les résultats expérimentaux sur 7 jeux de données réels ont montré que le modèle résultant nommé CPT+ est jusqu\u0027à 98 fois plus compact que CPT, et que cette compression demeure lorsque le nombre de séquence augmente. En termes de temps d\u0027exécution, CPT+ s\u0027est montré jusqu\u0027à 4.5 fois plus rapide que CPT. Finalement, CPT+ s\u0027est montré comme étant le modèle offrant les pré-dictions généralement les plus exactes dans une comparaison avec les principaux modèles de la littérature CPT, All-K-order Markov, DG, Lz78, PPM et TDAG.\nComme travaux futurs, nous adapterons CPT+ pour la prédiction de séquences dans le contexte d\u0027un flux infini de séquences. CPT+, de par sa nature incrémentale, pourrait être adapté à ce problème.\n"
  },
  {
    "id": "276",
    "text": "Introduction\nCe travail s\u0027inscrit dans le cadre de l\u0027apprentissage supervisé et plus précisément des systèmes de classification à base de règles floues (Ishibuchi et al. (1992). Ces systèmes ont la spécificité d\u0027être facilement interprétables grâce à l\u0027utilisation de termes linguistiques.\nDans ces systèmes, les règles floues peuvent être fournies par un expert humain. Comme l\u0027acquisition des connaissances humaines est une tâche complexe, plusieurs travaux se sont consacrés à l\u0027automatisation de la construction des règles à partir des données numériques (Ishibuchi et al., 1992), (Dehzangi et al., 2007). Cette construction comprend deux phases : une partition floue de l\u0027espace des entrées puis la construction d\u0027une règle floue pour chaque sous-espace flou issu de cette partition. Dans ces systèmes, un nombre élevé d\u0027attributs conduit à une explosion du nombre de règles générées, ce qui entraîne une dégradation de la compréhensibilité des systèmes, et affecte le temps de réponse nécessaire aussi bien à la phase d\u0027apprentissage qu\u0027à la phase de classification.\nDe ce fait, l\u0027optimisation du nombre de règles floues ainsi que du nombre d\u0027antécédents paraît comme une clé pour améliorer les systèmes de classification à base de règles floues. Dans ce cadre, plusieurs approches ont été proposées dans la littérature. On peut citer l\u0027approche de sélection des règles pertinentes par algorithme génétique (Ishibuchi et al., 1995) ou par le concept d\u0027oubli . Une autre approche consiste à réduire le nombre d\u0027attributs par une sélection des attributs les plus significatifs (Lee et al., 2001).\nDans ce papier, nous nous intéressons à la technique de regroupement des attributs dans les prémisses des règles. Dans cette approche, initialement introduite dans un cadre non flou par Borgi (1999), les attributs prédictifs sont regroupés en blocs, les attributs de chaque bloc sont traités séparément et apparaissent ensemble dans une même prémisse. Une première extension de ce travail dans un cadre flou, pour la génération de règles dans les systèmes d\u0027infé-rence floue, a été réalisée par Soua et al. (2012). Cette approche de regroupement des attributs, nommée SIFCO, présente l\u0027avantage de décomposer le problème d\u0027apprentissage en des sousproblèmes de complexité inférieure, et de réduire ainsi le nombre de règles générées. De plus, cette approche permet d\u0027obtenir des règles plus intelligibles car de taille réduite.\nLe regroupement d\u0027attributs dans (Borgi, 1999) et (Soua et al., 2012) se fait par recherche de corrélation linéaire : les attributs linéairement corrélés sont regroupés et traités séparément. Dans cet article, nous proposons une méthode qui se base sur le concept des règles d\u0027association (RA) introduit par Agrawal et al. (1993). Les RA vont nous permettre de déterminer les attributs \"liés\" ou \"associés\" qui seront regroupés dans les mêmes règles.\nL\u0027article est organisé comme suit : dans la partie 2, nous présentons les systèmes de classification à base de règles floues. Nous décrivons, dans la partie 3, le principe de regroupement des attributs comme présenté dans (Borgi, 1999) et (Soua et al., 2012). Notre approche de regroupement des attributs par RA est décrite dans la partie 4 et les résultats des tests expéri-mentaux sont présentés dans la partie 5. Nous concluons l\u0027article en présentant les principales perspectives de ce travail.\nApprentissage à base de règles floues\nOn se place dans le cadre des problèmes de classification supervisée dont le but est d\u0027affecter une classe à un objet décrit par des variables descriptives (des attributs). Nous nous intéressons au système de classification floue proposé dans (Ishibuchi et al., 1992). Afin de simplifier les notations, nous désignons ce système par l\u0027acronyme SIF. Deux phases sont à distinguer dans ce système : la phase d\u0027apprentissage dans laquelle on construit le modèle de classement à partir des données d\u0027apprentissage, et la phase de classification qui sert à associer une classe à un objet inconnu en utilisant ce modèle.\nPhase d\u0027apprentissage\nLa méthode de génération des règles floues que nous adoptons correspond à l\u0027utilisation d\u0027une grille floue simple, proposée par Ishibuchi et al. (1992). Pour illustrer cette approche, nous supposons, par souci de clarté, que notre problème d\u0027apprentissage est un problème bidimensionnel (2 attributs : X 1 et X 2 ). Les m exemples d\u0027apprentissage considérés sont notés E p \u003d (X p1 , X p2 ) ; p \u003d 1, 2, . . . , m , ils appartiennent chacun à l\u0027une des C classes : y 1 , y 2 , . . . , y C . Il est à noter que dans les SIF les attributs considérés sont numériques ; chacun des attributs X 1 et X 2 est partitionné en k sous-ensembles flous {A 1 , A 2 , . . . , A k } où chaque sous-ensemble A i est défini par une fonction d\u0027appartenance triangulaire symétrique. Un exemple de grille floue simple est présenté dans Fig \n-y k ij est la conclusion de la règle, elle correspond à l\u0027une des C classes -CF k ij est le degré de certitude de la règle, il traduit sa validité.\nLa conclusion et le degré de certitude de chaque règle sont déterminés comme suit :\n1. Pour chaque classe y t , calculer la somme des compatibilités des exemples d\u0027apprentissage appartenant à cette classe, par rapport à la prémisse de la règle :\nTrouver la classe y a qui a la plus grande valeur de compatibilité\n3. Déterminer le degré de certitude CF\nDans les travaux portant sur la construction de règles de classification floues, les attributs ne sont pas nécessairement partitionnés en un même nombre de sous-ensembles flous. Plusieurs types de grilles floues ont été étudiés comme par exemple la grille floue rectangulaire .\nPhase de classification\nDans cette phase, le système décide, à partir de la base de règles générées, notée S R , de la classe y a à associer à un individu E \u003d (X 1 , X\n2 ) de classe inconnue. 1. Pour chaque classe y t ; t \u003d 1, 2, . . . , C, calculer ? yt par :\n2. Trouver la classe y a qui maximise ? yt :\nFIG. 1 -Grille floue simple.\nRegroupement d\u0027attributs\nL\u0027approche de regroupement d\u0027attributs se base sur le concept des ensembles d\u0027apprentissage artificiel, qui repose sur la combinaison des décisions de plusieurs apprenants pour améliorer l\u0027exécution du système global (Valentini et Masulli, 2002). L\u0027idée de ce concept est de répartir l\u0027information -qui peut correspondre aux exemples d\u0027apprentissage, aux attributs descriptifs ou encore aux classes -entre plusieurs apprenants, chaque apprenant réalise la phase d\u0027apprentissage sur l\u0027information qui lui a été fournie, et les opinions \"individuelles\" des différents apprenants sont ensuite combinées pour atteindre une décision finale. Dans notre cas, l\u0027information à répartir correspond aux attributs descriptifs : chaque classifieur utilise un sous-ensemble des attributs initiaux et construit une base de règles locale, puis les différentes bases locales obtenues sont combinées pour former le modèle final (voir Fig. 2).\nCette approche, vérifiée expérimentalement dans (Soua et al., 2012) et (Borgi, 1999), permet de garantir une réduction conséquente du nombre de règles sans trop altérer les taux de bonnes classifications. Pour un problème de n attributs et k sous ensembles flous pour chaque attribut, le nombre de règles générés par les SIF, noté N R SIF , vaut k n . Lorsqu\u0027on découpe le problème d\u0027apprentissage en g sous-problèmes et on applique sur chacun d\u0027eux la même démarche de génération de règles que les SIF, on obtient un nombre de règles N R regrp égal à :\noù n i est le nombre d\u0027attributs liés dans le i ème groupe g i . Il a été démontré dans (Soua et al., 2012) que si les groupes d\u0027attributs, issus de l\u0027approche de regroupement, forment une partition de l\u0027ensemble des attributs de départ (\nPar conséquent :\nFIG. 2 -Approche de génération de règles par regroupement des attributs (Soua et al., 2012).\nApproche proposée : regroupement des attributs par RA\nNotre contribution réside au niveau de la méthode de regroupement d\u0027attributs ; nous proposons une nouvelle méthode n\u0027utilisant pas la recherche de corrélation linéaire, mais qui se base sur le concept des règles d\u0027association (Agrawal et al., 1993). Les algorithmes d\u0027extraction des RA déterminent les associations intéressantes entre les attributs en analysant leurs apparitions simultanées dans les enregistrements de la base de données. Cette méthode peut être très intéressante pour les bases de données pour lesquelles il n\u0027existe aucune relation de type corrélation linéaire entre les attributs.\nGénéralement, les algorithmes d\u0027extraction des RA déterminent les associations entre des variables de type booléen. Comme les données traitées dans les SIF sont quantitatives, il est nécessaire de commencer par les transformer en des valeurs booléennes, puis d\u0027appliquer le concept des RA sur ces valeurs. Ensuite, à partir des associations trouvées entre ces valeurs booléennes, nous déduisons les associations entre les attributs de départ. Enfin et dans le but de garantir une réduction du nombre de règles, nous nous proposons de filtrer les groupes d\u0027attributs associés de manière à obtenir une partition de l\u0027ensemble des attributs de départ. Nous décrivons dans ce qui suit ces différentes étapes.\nGénération des itemsets fréquents : liaisons locales entre attributs\nL\u0027existence d\u0027une liaison entre deux variables dépend de la réponse à la question : est-ce que la connaissance des valeurs de l\u0027une permet de prédire les valeurs de l\u0027autre ? Le concept des RA répond à cette question en associant les valeurs qui apparaissent souvent ensemble dans les transactions de la base de données considérée.\nLes RA ont été introduites par Agrawal et al. (1993)  -la génération des itemsets fréquents (tous les itemsets ayant un support supérieur à un seuil prédéfini minSupp). -la génération des règles d\u0027association à partir de ces itemsets fréquents ; une RA doit avoir une confiance supérieure à un seuil prédéfini par l\u0027utilisateur minConf .\nDans ce travail, nous nous intéressons au premier sous-problème et nous cherchons à déter-miner les groupes d\u0027attributs liés. Pour déterminer les itemsets fréquents, plusieurs algorithmes ont été proposés (Agrawal et al., 1993)  (Agrawal et Srikant, 1994)  (Savasere et al., 1995). L\u0027algorithme Apriori proposé dans (Agrawal et Srikant, 1994) est le plus connu et il est largement utilisé mais il ne traite que des données booléennes. Dans les problèmes courants, la majorité des données sont quantitatives et qualitatives et nécessitent des algorithmes applicables à ce type de données. Une extension de Apriori a été proposée par Srikant et Agrawal (1996) ; ils ont proposé de faire une correspondance entre des variables quantitatives ou qualitatives et des variables booléennes par le codage disjonctif complet. Pour une variable qualitative, chaque catégorie correspond à un élément booléen. Pour une variable quantitative, on discrétise l\u0027attribut en des intervalles, puis on fait correspondre une variable booléenne à chaque intervalle.\nDans notre cas, les attributs étant continus, nous recourons au codage disjonctif complet des attributs. Le partitionnement des attributs se fait par une discrétisation régulière à intervalles égaux. Nous obtenons donc des intervalles que nous assimilons à des valeurs booléennes. Nous appliquons ensuite l\u0027algorithme Apriori sur ces intervalles et obtenons ainsi des itemsets fré-quents ou des groupes d\u0027intervalles liés.\nDétermination des attributs liés : liaisons globales entre attributs\nDans l\u0027étape précédente, nous avons déterminé les groupes d\u0027intervalles liés. Notre but étant de faire un regroupement des attributs et non pas de leurs intervalles, on se propose de développer une procédure qui permet de déterminer la liaison entre un groupe d\u0027attributs à partir des liaisons trouvées entre leurs intervalles.\nNous définissons pour cela une grille d\u0027association qui représente les associations entre les valeurs (intervalles) d\u0027un groupe d\u0027attributs. Chaque axe de la grille concerne un attribut. La Fig. 3 présente 3 exemples de grille avec deux attributs X 1 et X 2 ; X 1 est décomposé en 6 valeurs (val de valeurs des attributs X 1 et X 2 . Quand deux valeurs forment un itemset fréquent, la case correspondante à leur intersection est grisée : on appelle cette case une région liée.\nLa liaison entre deux valeurs de deux attributs n\u0027entraîne pas forcément la liaison entre les deux attributs puisque d\u0027une part, ces attributs peuvent avoir très peu de régions liées (exemple (2) de la Fig. 3) et d\u0027autre part, le nombre de données dans ces régions peut être très faible par rapport au nombre total de données (exemple (3) de la Fig. 3).\nDire que, plus le nombre de régions liées est grand, plus l\u0027association entre les attributs est forte, n\u0027est pas toujours suffisant. En effet, le principe de RA détermine si une région est liée en analysant son support, et ce dernier reflète la densité de données, c.à.d. la fréquence d\u0027apparition des données dans la région. Ainsi, une seule région liée peut entraîner une association plus significative que plusieurs régions liées si cette unique région a une densité plus importante que la densité totale de l\u0027ensemble des autres régions liées. Nous proposons donc de prendre en compte aussi bien le nombre de régions liées que leurs densités. Pour cela, nous commençons par définir le poids d\u0027une région, appelé aussi coefficient de pondération. Ce poids caractérise la densité de données dans cette région, ce qui revient à son support.\nPour les valeurs respectives val Nous nous inspirons ensuite du principe des RA généralisées où une taxonomie (Fig. 4) existe entre les variables. D\u0027après (Srikant et Agrawal, 1995), les associations trouvées à un niveau donné peuvent remonter au niveau supérieur en sommant leurs supports, à condition qu\u0027il n\u0027y ait pas de recouvrement. Avec l\u0027exemple de la Fig. 4, si les itemsets (Veste, Botte) et (Veste, Espadrille) sont extraits, alors il n\u0027est pas possible de les généraliser à l\u0027itemset de niveau supérieur (Vêtements, Chaussure) en sommant leurs supports, car Veste, Botte et Espadrille peuvent figurer dans une même transaction. Dans notre cas, les variables quantitatives sont partitionnées en des valeurs sous forme d\u0027intervalles ; la présence de deux valeurs d\u0027un seul attribut n\u0027est donc pas possible dans le même enregistrement. En formant une taxonomie entre un attribut et ses intervalles (Fig. 5), et comme il n\u0027y a pas de recouvrement entre les FIG. 4 -Exemple de taxonomie pris de Srikant et Agrawal (1995).\nFIG. 5 -Taxonomie entre un attribut et ses valeurs.\nintervalles, on peut calculer le degré d\u0027association des attributs comme la somme des supports de leurs valeurs (intervalles) liées. En utilisant ce principe, et en ne comptabilisant que les régions liées, nous définissons un degré d\u0027association entre deux attributs ou plus, par la somme des coefficients de pondération de leurs régions liées. Donc, pour deux attributs X 1 et X 2 , le degré d\u0027association ? s\u0027écrit : Dans le cas général d\u0027un ensemble d\u0027attributs X \u003d {X n1 , X n2 , . . . , X n l }, le degré d\u0027association de ces l attributs est :\n-r i1i2...i l X est la région formée par les intervalles val\n. . , k l sont respectivement les tailles des partitions de X n1 , X n2 , . . . , X n l .\nLe degré ? est compris entre 0 et 1, on peut alors définir un seuil d\u0027association ? min au delà duquel on considère que les attributs de l\u0027ensemble X sont liés.\nChoix des groupes d\u0027attributs associés\nLa procédure présentée dans 4.1 et 4.2 est basée sur le principe de l\u0027algorithme Apriori ; elle fournit donc tous les groupes d\u0027attributs associés de différentes tailles. Il est à noter que ces groupes ne constituent pas forcément une partition de l\u0027ensemble des attributs de départ : on peut avoir des relations d\u0027inclusion entre deux groupes d\u0027attributs, ou une intersection non vide. Afin de garantir une réduction du nombre de règles générées, nous nous proposons de sélectionner un ensemble de groupe d\u0027attributs de manière à former une partition de l\u0027ensemble des attributs de départ (voir partie 3). La sélection se base sur les deux critères suivants : \nExpérimentation\nNotre système, baptisé SIFRA, utilise l\u0027approche de regroupement des attributs dans le cadre des SIF (Ishibuchi et al., 1992) comme cela est fait dans SIFCO (Soua et al., 2012) mais avec une nouvelle méthode de regroupement des attributs, celle que nous avons proposée et qui se base sur le concept des règles d\u0027association. Après avoir déterminé les groupes d\u0027attributs associés, nous utilisons la démarche proposée par Ishibuchi et al. (1992) pour générer les règles floues (partie 2.1), pour chaque groupe d\u0027attributs. La classification d\u0027un objet inconnu se fait par la méthode de classification de Ishibuchi et al. (1992) (partie 2.2).\nNous avons testé notre système SIFRA sur des bases de données qui diffèrent par le nombre d\u0027attributs, le nombre d\u0027exemples et le nombre de classes (Tab. 1). Pour évaluer la capacité de généralisation de notre méthode, nous avons adopté la technique de validation croisée d\u0027ordre 10 ( Kohavi, 1995). Dans le tableau 2, nous présentons le taux de bonne classification suivi entre parenthèses du nombre de règles générées. Les meilleurs taux de classification sont présentés en gras. Le terme \"imp\" fait référence à l\u0027impossibilité de générer les règles floues à cause du nombre de règles très élevé (supérieur à 10 5 ). Pour la phase de regroupement des attributs, nous avons utilisé une discrétisation à intervalles égaux et avons fixé le nombre d\u0027intervalles à 3. D\u0027autres tailles de discrétisation ainsi que d\u0027autres méthodes de discrétisation pourront être étudiées dans de prochains travaux. Au niveau de la phase d\u0027apprentissage, nous avons utilisé une partition floue homogène et une partition floue supervisée. Pour la partition homogène, nous avons testé plusieurs valeurs de la taille de partition k. Comme dans SIFCO, la valeur de k qui permet d\u0027obtenir le meilleur taux de bonne classification dépend fortement des données de la base. Pour la partition floue supervisée, nous avons adopté la méthode MDLP de Fayyad et Irani (1993 Nous présentons dans Tab. 2 une comparaison de notre méthode SIFRA avec les deux méthodes SIF et SIFCO. Chacune des 3 méthodes possède des paramètres d\u0027entrée à définir, à savoir la taille de la partition foue k, le seuil et la méthode de corrélation pour SIFCO, les seuils minSupp et ? min pour SIFRA. Pour comparer la performance des 3 méthodes et pour simplifier la lecture des résultats, nous présentons dans Tab. 2, pour chaque méthode, le meilleur taux de bonne classification obtenu en faisant varier ses paramètres d\u0027entrée. D\u0027après Tab. 2, il est clair que notre approche SIFRA fournit des taux de bonne classification très satisfaisants comparée à la méthode SIF, et des taux similaires ou meilleurs comparée à SIFCO. Comparée aux SIF, notre approche permet d\u0027améliorer la performance de classification et de diminuer notablement le nombre de règles (en particulier avec les bases Wine, Vehicle et Sonar pour lesquelles la génération des règles avec SIF est impossible vu l\u0027explosion de leur nombre). Comparée à SIFCO, notre méthode donne le meilleur taux de classification pour la base Iris avec un nombre de règles plus élevé mais qui reste faible (33). Pour Lupus, Wine et Sonar, les mêmes taux ont été obtenus par SIFRA et SIFCO. Concernant la base Vehicle, notre approche améliore considérablement le taux de bonne classification (67.73% contre 54.97% avec SIFCO) mais avec un nombre de règles plus important. Ce différentiel du nombre de règles s\u0027explique par le fait que les groupes d\u0027attributs liés détectés par SIFRA (basé sur les RA) contiennent plus d\u0027attributs que les groupes détectés par SIFCO (basé sur une recherche de corrélation linéaire) (équation 7). Avec ces données, les liaisons entre attributs déterminées par notre approche semblent donc être plus pertinentes que celles trouvées avec SIFCO. 6 Conclusion\n"
  },
  {
    "id": "277",
    "text": "introduction\nLes méthodes à noyau utilisées en analyse exploratoire des données (K-PCA, K-LDA, K-CCA, etc.) ou pour traiter des tâches de classification ou de régression (machines à support vectoriel, SVM) nécessitent, dans leurs fondements, l\u0027usage de noyaux définis (positifs ou né-gatifs). Pourtant, bon nombre d\u0027études relativement récentes en fouille de données temporelles présentent des résultats produits par de telles méthodes exploitant des noyaux temporellement élastiques (NTE) non définis Haasdonk (2005)  Zhang et al. (2010) ou régularisés par des mé-thodes spectrales Narita et al. (2007). L\u0027émergence de nouvelles méthodes de régularisation pour NTE offre aujourd\u0027hui des alternatives à l\u0027exploitation des noyaux élastiques non définis que nous nous proposons d\u0027évaluer de manière comparative sur des jeux de données simples mais potentiellement explicites. D\u0027une manière générale, les procédures de régulation ont été développées pour approximer des noyaux non définis par des noyaux définis (ou semi-définis). Les premières approches appliquent directement des transformations spectrales aux matrices de Gram issues des noyaux non définis. Ces méthodes Wu et al. (2005)  Chen et al. (2009) consistent à i) changer le signe des valeurs propres négatives ou décaler ces valeurs propres en utilisant la valeur de décalage minimal nécessaire pour rendre le spectre des valeurs propres TAB. 1 -Liste des noyaux analysés positif, et ii) reconstruire la matrice de Gram issue du noyau avec les vecteurs propres d\u0027origine afin de produire une matrice semi-définie positive. D\u0027autres approches sont basées sur la recherche de la matrice de corrélation (matrice symétrique positive semi-définie ayant une diagonale unitaire) la plus proche de la matrice de Gram issue du noyau non défini, la proximité étant prise au sens d\u0027une norme (norme de Frobenius pondérée) Higham (2002).\nCependant ces procédures de convexification sont difficiles à interpréter géométriquement Graepel et al. (1998) et l\u0027effet attendu du noyau d\u0027origine non défini peut être, selon les études, soit perdu ou pour le moins atténué par ces méthodes agissant directement sur le spectre matriciel, soit encore minime voir négatif comparativement à l\u0027exploitation directe de la matrice non régularisée Chen et Ye (2008). Dans le contexte de l\u0027alignement de séquences ou de séries temporelles, des approches de régularisation plus directes pour les NTE consistent à remplacer les opérateurs min ou max par un opérateur de sommation ( ) dans les équations récursives qui définissent les distances élastiques. Il en résulte qu\u0027au lieu de ne considérer que le meilleur chemin d\u0027alignement possible entre deux séries temporelles, le noyau régularisé effectue la somme des couts (ou gains) de tous les chemins d\u0027alignement possibles avec un mécanisme de pondération qui cherche à favoriser les bons alignements et à pénaliser les mauvais alignements. Ces principes ont été appliqués avec succès par Saigo et al. (2004) pour la mesure (non définie) de Smith et Waterman (1981) très utilisée pour la comparaison de séquences symboliques, et plus récemment pour la speudo distance Dynamic Time Warping (DTW, Velichko et Zagoruyko (1970), Sakoe et Chiba (1971)) Cuturi et al. (2007), .\nNous développons dans cet article, sur la base d\u0027une analyse en composantes principales à noyau (K-PCA), une étude expérimentale permettant d\u0027évaluer les noyaux listés en table 1 sur de tâches de classification (supervisée et non-supervisée) de séries temporelles dans des sous espaces de dimension réduite. En nous limitant à des ensembles de séries temporelles de taille fixe, nous proposons ainsi de comparer expérimentalement au travers d\u0027une analyse K-PCA un noyau Gaussien construit à partir de la distance Euclidienne (noyau défini positif, non temporellement élastique, ce noyau servant de base de référence), un noyau Gaussien construit à partir de la pseudo distance DTW (noyau non défini en général, mais élastique), une version régularisée du noyau précédent basée sur la recherche de la matrice de corrélation la plus proche Higham (2002), et enfin le noyau DTW régularisé suivant la méthode proposée par , K DT W , et une version normalisée, K Faisant suite aux travaux de Cuturi et al. (2007), la technique de régularisation développée dans  s\u0027attache à transformer les équations récursives définissant la DTW (Dynamic Time Warping) de manière à produire une mesure de similarité notée K DT W constituant un noyau défini positif, c\u0027est-à-dire s\u0027apparentant à un produit scalaire dans un espace de Hilbert à noyau reproduisant. K DT W se distingue de l\u0027approche proposée par Cuturi et al. en prenant la forme d\u0027un noyau de convolution tel que défini par Haussler (1999) tout en imposant une condition sur les coûts locaux d\u0027alignement moins restrictive. Pour rappel, un noyau défini sur R est est une fonction continue symétrique K :\nDefinition\nLa définition récursive du noyau K DT W est la suivante : • ? ? R + est un paramètre d\u0027ajustement qui permet de pondérer les contributions locales, i.e. les distances entre les positions localement alignées, et\nAinsi, fondamentalement, l\u0027opérateur min (ou max) est remplacé par un opérateur de sommation et une fonction de corridor symétrique (la fonction h dans l\u0027équation récursive cidessus) est introduite pour, éventuellement, limiter la sommation et et donc la complexité algorithmique. Enfin, un nouveau terme récursif nécessaire à la régularisation (K xx ) est ajouté, de telle sorte que la preuve de la propriété de positivité du noyau peut être comprise comme une conséquence directe du théorème de convolution d \u0027 Haussler (1999).\nNormalisation\nLe noyau K DT W effectue la somme sur l\u0027ensemble des chemins d\u0027alignement possibles des produits des termes locaux d\u0027alignement e\nPour les séries temporelles de grandes tailles, ces produits deviennent infimes et K DT W incalculable lorsque ? est trop faible. Ainsi, le domaine de variation de K DT W s\u0027amenuise en convergeant vers 0 lorsque ? tend vers 0 sauf lorsque l\u0027on compare deux séries temporelles identiques (la matrice de Gram correspondante souffre ainsi d\u0027une dominance diagonale). Comme proposé dans , une manière de palier ce problème consiste à considérer le noyau normalisé : . Si l\u0027on oublie la constante de proportionnalité, cela revient simplement à élever le noyau\n, ce qui montre que˜Kque˜ que˜K DT W est lui aussi défini positif (Berg et al. (1984), Proposition 2.7). Une correction de dominance diagonale similaire (sous-polynomial, i.e. t \u003c 1) a initialement été proposée dans Schölkopf et al. (2002). L\u0027effet de ce type de normalisation, sur le jeu de données SwedishLeaf (c.f. Table 2) est illustré en figure 1. La distribution des valeurs de la matrice de Gram associée au noyau non normalisé K DT W (évalué avec ? \u003d 1) présente une très forte accumulation autour des valeurs très faibles (pour ? \u003d 1, K DT W M in \u003d 2.1e ? 77 et K DT W M ax \u003d 1.9e ? 06 sur le jeu de données testé), tandis que la distribution des valeurs de la matrice de Gram associée au noyau normalisé˜Knormalisé˜ normalisé˜K DT W \u003d K t DT W est plus diffuse. Les valeurs du noyau sont par ailleurs bornées :  \nComplexité algorithmique\nLa définition récursive précédente permet de montrer que la complexité algorithmique liée au calcul du noyau K DT W est O(n 2 ), où n est la longueur des deux séries temporelles mises en correspondance, et lorsqu\u0027aucun corridor n\u0027est spécifié. Cette complexité est ramenée à O(c.n) quand un corridor symétrique de taille c est exploité par le biais de la fonction symmétrique h.\nL\u0027analyse en composantes principales non-linéaire, encore appelée ACP à noyau ou Kernel-PCA, Schölkopf et al. (1998) peut être vue comme une généralisation de l\u0027ACP classique : elle permet d\u0027engendrer une réduction de dimensionnalité non linéaire du point de vue de l\u0027espace de représentation initial des données. Le principe consiste à projeter, par le biais d\u0027une fonction non-linéaire ?(.), les données initiales dans un espace en général de plus haute dimension de sorte que l\u0027image de la variété (non linéaire) contenant les données initiales devienne plus facilement linéairement séparable dans le nouvel espace, appelé espace des caractéristiques. Il suffit alors d\u0027effectuer une ACP classique dans cet espace linéaire pour obtenir une réduction de dimensionalité non linéaire dans l\u0027espace des données initiales. Si l\u0027on exploite un noyau K(., .) défini positif, celui-ci induit de manière implicite une fonction nonlinéaire dite de mapping ?(.) telle que ?x, y, K(x, y) \u003d\u003c ?(x)\nT , ?(y) \u003e. Cette fonction ?(.) n\u0027a pas besoin d\u0027être connue explicitement (on évoque ici l\u0027astuce du noyau).\nAlgorithm 1 ACP non linéaire 1: Choix du noyau (défini positif) k 2: Construction de la matrice de Gram à partir des données :\n..,m 3: Centrage de la matrice de Gram (on retire la moyenne des données projetées dans l\u0027espace des caractéristiques) : \nL\u0027algorithme 1 présente succinctement les étapes de l\u0027ACP non-linéaire, qui, à partir du choix d\u0027un noyau défini positif, extrait les valeurs et vecteurs propres de la matrice de Gram centrée associée, puis projette toute donnée (initiale ou de test) dans un espace des caracté-ristiques de dimension réduite (d). Il est clair que l\u0027ACP non-linéaire nécessite que le noyau utilisé soit défini positif.  Keogh et al. (2006). Ils sont de tailles modestes pour permettre (éventuellement) une visualisation la plus explicite possible en faible dimension. Le nombre de catégories varie de 2 à 50 et la longueurs des séries varie de 60 à 463.\nPour les 13 jeux de données listés en Table 2, et les 5 noyaux listés en Table 1, une ACP non linéaire est pratiquée puis les données sont projetées dans l\u0027espace des caracté-ristiques obtenu en faisant varier le nombre de vecteurs propres, c\u0027est à dire la dimension de l\u0027espace réduit. Par exemple, en figure 2 les projections des séries temporelles du jeu de données Gun_Point sont présentées dans un espace des caractéristiques de dimension 3 pour les noyaux Gaussien-Euclidien, Gaussien-DTW régularisé par matrice de corrélation la plus proche (MCPP), K DT W et K t DT W . La valeur du paramètre t, exposant du noyau K t DT W , est estimé directement à partir des données d\u0027apprentissage en évaluant les valeurs extrêmes prises par le noyau K DT W non normalisé. A titre d\u0027exemple, on considère le jeu de données Gun_Point, pour lequel a matrice de Gram évaluée sur le noyau Gaussien DTW n\u0027est pas dé-finie. Comme le montre la figure 2, les projections dans le sous-espace des caractéristiques de dimension 3 sont très proches pour le noyau Gaussien DTW et sa version régularisée par matrice de corrélation la plus proche. La séparation des classes est, sur cet exemple, bien meilleure pour le noyau KDT W et sa version normalisée KDT W t . A l\u0027issue de l\u0027ACP non linéaire, nous proposons une expérience de classification supervisée basée sur la règle du plus proche voisin (1-PPV) et une expérience de clustering basée sur l\u0027algorithme des K-moyennes (K correspondant ici au nombre effectif de catégories du jeux de données). Pour chaque jeux de données et pour chaque noyaux testés la classification supervisée et non supervisée sont effectuées dans le sous espace des caractéristiques défini par K-PCA en faisant varier la dimension du sous-espace de 1 à 20 (pour dix dimensions, cela correspond à une réduction dimensionnelle variant de 83% à 98% pour les jeux de données testés).\nLa qualité de la classification est ensuite évaluée à partir du taux d\u0027erreur de classification estimé à partir d\u0027une validation croisée en 5 sous-échantillons. La précision et l\u0027information mutuelle normalisée (IM N ) sont utilisées pour évaluer la qualité d\u0027une classification non supervisée sur des données La précision est définie comme la fraction des individus correctement étiquetés, étant donné une correspondance 1-vers-1 entre les vraies classes et les clusters dé-couverts. Si p dénote une permutation quelconque des indices des clusters {˜c{˜c i } proposés (ou des vraies classes {c j }), la précision est alors définie par : L\u0027information mutuelle normalisée, IM N , entre la vraie classification C et celle prédite˜Cprédite˜ prédite˜C est définie par :  Par ailleurs, le noyau Gaussien-DTW et sa variante régularisée à partir de la matrice de corré-lation la plus proche conduisent à des résultats très similaires et sensiblement moins bons comparativement aux noyaux KDT W et KDT W t . La régularisation par matrice de corrélation la plus proche ne semble donc pas apporter de bénéfice significatif en terme de classification supervisée ou non supervisée sur ces jeux de données par rapport au noyau DTW non défini. Il obtientles taux d\u0027erreur de classification les plus faibles sur 10 des 13 jeux de données (CBF est mieux classé par le noyau Gaussien-DTW, ECG200 par le noyau Gaussien-Euclidien et Lighting2 par le noyau Gaussien-DTW régularisé par matrice de corrélation la plus proche). Pour la classification non supervisée, K t DT W obtient également les meilleurs résultats d\u0027après les mesures Précision et IMN pour 8 des 13 jeux de données. Pour cette tâche, K DT W est meilleur sur les jeux de données yoga et Gun_Point, tandis que le noyau Gaussien-Euclidien se distingue sur ECG200 et les noyaux Gaussien-DTW sur Lighting2. Sur les jeux de données pour lesquels K t DT W n\u0027arrive pas en tête, ce noyau se positionne entre le noyau Gaussien-Euclidien et les noyaux Gaussien-DTW. Contrairement à la régularisation par matrice de corrélation la plus proche, le principe de régularisation mise en oeuvre dans K DT W et K t DT W modifie en profondeur la nature même de la fonction de similarité sous-jacente à la mesure DTW en apportant en général une meilleure capacité à séparer ou regrouper les séries temporelles dans des espaces de dimension réduite.\nRésultats et analyse\nConclusion\nNous avons évalué expérimentalement la capacité de quelques noyaux (élastiques, nonélastiques, définis, non-définis) à classer des séries temporelles de manière supervisée ou non dans des espaces de caractéristiques à dimension réduite obtenus par ACP non linéaire. Les résultats présentés montrent que les approches récentes de régularisation de noyaux élastiques offrent des alternatives bien meilleures que les principes classiques de régularisation basés sur des approches spectrales portant directement sur les valeurs propres des matrices de Gram construites à partir des noyaux non définis. Le noyau DTW régularisé K DT W exploité dans cet article dans sa version normalisé K t DT W offre un bon compromis entre les noyaux définis non-élastiques (tel que le noyau Gaussien-Euclidien) et les noyaux non définis élastiques (tel que le noyau Gaussien-DTW). Non seulement celui-ci conserve une caractéristique d\u0027élasti-cité temporelle tout en étant défini positif, mais il se marie également bien avec les approches à noyau tel que l\u0027ACP non linéaire. Sa capacité à proposer des espaces de caractéristiques discriminantes en dimension réduite en font un outil en général efficace pour l\u0027analyse exploratoire d\u0027ensembles de séries temporelles. Ces résultats confirment et complètent ainsi ceux présentés par  et  dans le cadre d\u0027une classification supervisée par machine à support vectoriel en apportant un éclairage sur la normalisation de ce type de noyau.\n"
  },
  {
    "id": "278",
    "text": "Introduction\nÀ cause de l\u0027explosion du nombre d\u0027informations stockées et partagées sur Internet, et l\u0027introduction de nouvelles technologies pour capturer ces données, l\u0027analyse des données incertaines est devenue essentielle dans de nombreuses applications pour la prise de décision. Pour gérer et traiter l\u0027incertitude des données, plusieurs modèles ont été proposés, ce qui a donné naissance à différents types de bases de données imparfaites. Nous pouvons citer les plus connues : les bases de données probabilistes présentées par Dalvi et Suciu (2007); Aggarwal et Yu (2009), les bases de données possibilistes introduites par Bosc et Pivert (2010) et les bases de données évidentielles basées sur la théorie de Dempster-Shafer proposées par Bell et al. (1996). L\u0027utilisation des bases de données évidentielles offre plusieurs avantages à savoir : (i) Elles permettent de modéliser l\u0027incertitude et l\u0027imprécision des données ; et (ii) cela représente une généralisation des deux modèles ; probabiliste et possibiliste à la fois. Dans cet article, nous nous nous intéressons aux requêtes Skyline sur des données incertaines où l\u0027incertitude est modélisée par la théorie de l\u0027évidence, ce qui constitue un travail pionnier.\nLe reste cet article est organisé comme suit. La section 2 contient un rappel sur l\u0027opérateur Skyline, les notions de base de la théorie de l\u0027évidence et les bases de données évidentielles. Dans la section 3, nous définissons formellement la relation de dominance et modélisons le Skyline évidentiel. Nos expérimentations sont données dans la section 4. Enfin, la section 5 conclut l\u0027article.\nNotions de base\nDans cette section, nous présentons d\u0027abord les requêtes Skyline sur des données classiques Borzsonyi et al. (2001). Ensuite, nous présentons les notions de base de la théorie de l\u0027évidence et les bases de données évidentielles.\nLes requêtes Skyline\nPour simplifier, nous supposons que la valeur la plus élevée, est la plus préférée.  \nDéfinition 1 (Relation de Dominance) Étant donnés deux objets\nLa théorie de l\u0027évidence\nLa théorie de l\u0027évidence a été introduite par Shafer (1976) dont le but est d\u0027évaluer subjectivement la vérité d\u0027une proposition. Cette théorie, aussi connue sous le nom de \"théorie des fonctions de croyance\" est une généralisation de la théorie bayésienne Dempster (1968). Elle représente un ensemble d\u0027hypothèses désigné par le cadre de discernement.  \nLes bases de données évidentielles (BDE)\nLes BDE permettent de représenter les données manquantes, incertaines ou imprécises. \nLes requêtes Skyline en présence de données évidentielles\nDans cette section, nous introduisons la relation de dominance entre les objets dont l\u0027incertitude est modélisée par la théorie des fonctions de croyance, par la suite, nous présentons la définition du Skyline évidentiel. Étant donné un ensemble d\u0027objets O \u003d {O 1 , O 2 , . . . , O n } défini sur un ensemble d\u0027attributs A \u003d {a 1 , a 2 , . . . , a d }, avec o i .a k représente l\u0027ensemble des éléments focaux de l\u0027objet o i et l\u0027attribut a k ; par exemple 1 , o 1 .wl \u003d { 16, 18}, 0.1 20}, 0.9 et o 1 .r \u003d { 0.5 100}, 0.5 Le degré de croyance qu\u0027une valeur incertaine de l\u0027objet o i définie sur l\u0027attribut a k est préférée ou égale à une autre valeur de l\u0027objet o j , est donné par Bell et al. (1996) :\nDans notre exemple, nous avons bel(o 1 .wl ? o 3 .wl) \u003d 0.3 · (0.1 + 0.9) + 0.7 · (0.1 + 0.9) \u003d 1, et bel(o 1 .r ? o 3 .r) \u003d 0.7 · 0.7 + 0.3 · 0.7 \u003d 0.7.\nÉtant donnés deux objets\nLe Tableau 2 présente les degrés de croyance que chaque objet en ligne, domine un autre objet en colonne.\nLes objets o i et o j comparés sont supposés différents, ce qui fait que la relation ? suffit pour exprimer que o i domine o j (au sens de (2)). Remarquons que cette définition se réduit à la \nPar exemple, o 1 0.9-domine o 2 et o 4 . Mais, il ne 0.9-domine pas o 4 car bel(o 1 ? o 4 ) \u003d 0.3 \u003c 0.9. Intuitivement, un objet est dans le Skyline s\u0027il n\u0027est pas dominé par rapport à un certain seuil b. \nDéfinition 7 (b-dominant skyline) Le skyline de O, désigné par b-Sky O , comprend les objets dans O qui ne sont pas b-dominés par aucun autre objet, i.e., b-Sky\nThéorème 1 Étant donnés deux seuils de croyance\nPreuve 1 Supposons qu\u0027il existe un objet\nLe théorème 1 indique que la taille de b-dominant skyline est plus petite que celle de b ? -dominant skyline si b \u003c b ? . Les utilisateurs ont donc la possibilité de contrôler la taille des objets que contient le Skyline évidentiel en faisant varier le seuil de croyance. obtenus. Dans chaque expérimentation., nous varions un seul paramètre, tandis que les autres paramètres prennent leurs valeurs par défaut. Le Tableau 3 montre ces paramètres et leurs symboles ; les valeurs par défaut sont en gras. La figure 1.a montre que la taille du skyline évidentiel augmente avec l\u0027augmentation de n. Dans la figure 1.b, on montre que la taille du skyline évidentiel augmente aussi de façon significative avec l\u0027augmentation de d. \nÉtude Expérimentale\nConclusion\nDans cet article, nous avons abordé le problème des requêtes skyline dans le cadre des bases de données évidentielles et nous avons introduit un nouveau type de skyline. Notre étude expérimentale a démontré la faisabilité et la flexibilité du skyline évidentiel. Comme perspective, nous envisageons de développer des techniques de classement des objets retournés par l\u0027opérateur skyline évidentiel.\n"
  },
  {
    "id": "282",
    "text": "Introduction\nL\u0027analyse en composantes principales (ACP, Jolliffe (1986)) est une des méthodes, si ce n\u0027est la méthode, d\u0027analyse exploratoire les plus couramment utilisées. Elle a été ré-interprétée sous un formalisme probabiliste par Tipping et Bishop (1999), montrant que les composantes principales pouvaient être estimées par maximum de vraisemblance dans le cadre d\u0027un modèle à variables latentes. Avec l\u0027avénement des données de grande dimension, la problématique consistant à sélectionner un petit nombre de variables d\u0027intérêt parmi l\u0027ensemble des variables disponibles est devenue primordiale. Un des soucis majeurs de l\u0027ACP dans cette optique est que les composantes principales sont définies comme une combinaison linéaire de l\u0027ensemble des variables initiales. Des versions parcimonieuses de l\u0027ACP (Zou et al., 2004) ainsi que de sa version probabiliste (Guan et Dy, 2009) ont été proposées récemment. La version parcimonieuse de Zou et al. (2004) repose sur l\u0027ajout d\u0027une pénalisation de type 1 au problème des moindres carrés, qui nécessite le choix du coefficient de pénalisation de façon heuristique. Dans Guan et Dy (2009), une version sparse bayésienne de l\u0027ACP probabiliste est proposée. Nous proposons dans ce travail une alternative fréquentiste utilisant un algorithme EM pour l\u0027inférence. La procédure d\u0027estimation obtenue à l\u0027avantage d\u0027être particulièrement simple, et ne nécessite pas le choix de loi a priori. Elle offre en outre la possibilité de considérer le problème du choix de la pénalité comme un problème de choix de modèles.\nAnalyse en composantes principales probabiliste\nSoit y un vecteur aléatoire observé de dimension p, et x un vecteur aléatoire latent (non observé), de dimension d, relié à y par l\u0027équation suivante :\noù W est une matrice p × d, µ est le vecteur moyenne (supposé nul dans la suite, µ \u003d 0), et ? N (0, ? 2 I). Conditionnellement au vecteur x, la distribution des vecteurs observés est :\nEn supposant x ? N (0, I), la distribution marginale du vecteur observé est :\nCe modèle est un modèle de type \"factor analysis\" (Bartholomew et al., 2011), popularisé par Tipping et Bishop (1999) sous le nom d\u0027analyse en composantes principales probabiliste (Probabilistic Principal Component Analysis (PPCA)). En effet, une estimation par maximum de vraisemblance des paramètres du modèle à l\u0027aide d\u0027un algorithme EM (Dempster et al., 1977), considérant le vecteur latent x comme manquant, conduit à estimer les colonnes de W par les vecteurs propres de la matrice de covariance empirique, vecteurs qui ne sont rien d\u0027autres que les axes principaux classiques. Soit y 1 , . . . , y n un échantillon i.i.d de vecteurs observés. L\u0027algorithme EM consiste à maximiser de façon itérative la log-vraisemblance complétée par les données non observées x 1 , . . . , x n :\n3 Une version parcimonieuse de l\u0027analyse en composantes principales probabiliste\nDans ce travail, nous considérons une version parcimonieuse de l\u0027analyse en composantes principales probabiliste. L\u0027objectif est d\u0027obtenir des axes principaux déterminés uniquement grâce à un nombre restreint de variables initiales, et ainsi faciliter leur interprétation. De plus, comme nous le verrons par la suite, l\u0027approche probabiliste de l\u0027ACP permet de sélectionner le paramètre de pénalité par des méthodes classiques de sélection de modèles.\nDans l\u0027optique d\u0027introduire de la parcimonie au sein de axes principaux, nous considérons une pénalité 1 sur les colonnes de la matrice W. La vraisemblance complétée à maximiser est alors la suivante :\noù w \u003d (w 1 , . . . , w pp ) t est la colonne de W et ? \u003e 0 est le paramètre de pénalisation. L\u0027algorithme EM est un algorithme itératif qui alterne deux étapes (E et M), décrites ci-après.\nLa q-ème itération de l\u0027étape E consiste à calculer l\u0027espérance de pen c sous la loi p(x|y, ? (q) ), où ? (q) \u003d (W (q) , ? 2(q) ) est la valeur courante de l\u0027estimation des paramètres du modèles :\nest une constante indépendante des paramètres du modèle et où\nDe sorte à faciliter la maximisation, nous considérons l\u0027approximation de la norme 1 par la forme quadratique suivante (Fan et Li, 2001) :\nj w j . La maximisation de Q(?, ? (q) ) en fonction de W n\u0027ayant pas de solution analytique, nous utilisons une approche alternative consistant à maximiser Q(?, ? (q) ) en fonction de chaque éléments de la matrice W successivement. On obtient alors, en dérivant Q(?, ? (q) ) par rapport à w j et en égalant à 0 :\nCette dérivation élément par élément ne conduit pas nécessairement au maximum de Q(?, ? (q) ), mais suffit pour faire augmenter la log-vraisemblance à chaque étape de l\u0027algorithme. On obtient alors un algorithme GEM (Generalized EM) qui conserve les mêmes propriétés de convergence qu\u0027un algorithme EM classique. L\u0027estimateur de la variance résiduelle est quant à lui :\net s\u0027avère être identique à celui de la version non sparse de l\u0027analyse en composantes principales probabiliste (Tipping et Bishop, 1999).\n4 Sélection de ? par l\u0027heuristique de pente Dans Zou et al. (2004), le choix de la pénalité ? est réalisé de manière heuristique en se basant sur l\u0027éboulis des valeurs propres. L\u0027idée de la stratégie que nous proposons est d\u0027estimer le modèle sur une grille de valeurs de ?, et d\u0027utiliser un outil de sélection de modèles pour choisir le meilleur modèle. Les outils classiques de sélection de modèles sont par exemple les critères AIC (Akaike, 1974) et BIC (Schwarz, 1978), qui pénalisent la log-vraisemblancêvraisemblancê ?) de la façon suivante :\n, où ? est le nombre de paramètres libres du modèles et n le nombre d\u0027observations. La valeur de ? dépend directement de la valeur de ? puisqu\u0027elle est égale au nombre d\u0027éléments non nuls dans W plus un (pour la variance résiduelle). Même si ces critères sont largement utilisés et asymptotiquement consistants, ils sont aussi connus pour être plus efficaces sur simulations que sur données réelles.\nPour surmonter ce problème, Birgé et Massart (2007) ont récemment proposé une mé-thode dirigée par les données pour calibrer la pénalité des critères pénalisés, connue sous le nom d\u0027heuristique de pente. L\u0027heuristique de pente a été proposé initialement dans un cadre d\u0027un modèle de régression gaussien homoscédastique, mais a ensuite été étendue à d\u0027autres situations. Birgé et Massart (2007) ont démontré qu\u0027il existait une pénalité minimale et que de considérer une pénalité égale au double de la pénalité minimale permettait d\u0027approcher le modèle oracle en terme de risque. La pénalité minimale est en pratique estimée par la pente de la partie linéaire de la log-vraisemblance pen c ( ˆ ?) exprimée en fonction de la complexité du modèle. Le critère associé est alors défini par :\noùˆsoùˆ oùˆs est l\u0027estimation de la pente de la partie linéaire de pen c ( ˆ ?). Une revue détaillée et des conseils d\u0027implémentation sont donnés dans Baudry et al. (2012).\nIllustrations numériques\nNous choisissons pour illustrer notre méthodologie un jeu de données classique issu de l\u0027UCI machine learning repository : le jeux de données USPS. Le jeu original contient 7291 images représentant des chiffres manuscrits de 0 à 9. Chaque chiffre est une image en niveaux de gris de taille 16 × 16, représentée par un vecteur de dimension 256. Pour cette expérience, nous avons extrait un sous ensemble de 1756 images correspondant aux chiffres 3, 5 et 8. Nous réalisons sur ces données une ACP ainsi que l\u0027ACP parcimonieuse que nous proposons. Pour cette dernière, nous fixons le nombre maximum d\u0027itérations de l\u0027algorithme EM à 500 et le seuil de convergence à 10\n, et nous considérons une grille de valeurs de ? de 0 à 150, avec un pas de 1. La méthode de l\u0027heuristique de pente (figure 1) conduit à choisir ? \u003d 126. Dans un but illustratif, nous discutons ici les résultats concernant les deux premières composantes principales. La figure 2 représente la projection des 1756 images dans le premier plan principal de l\u0027ACP ainsi que les deux premières composantes principales, tandis que la figure 3 propose la même représentation pour l\u0027ACP parcimonieuse. Nous pouvons noter que les deux méthodes définissent un premier plan principal relativement discriminant vis-à-vis des trois types d\u0027images. Tout l\u0027intérêt de l\u0027ACP parcimonieuse est que les composantes principales   \n"
  },
  {
    "id": "283",
    "text": "Introduction\nAvec la récente explosion du nombre d\u0027images et de données satellites disponibles, la conception de systèmes capables d\u0027interpréter automatiquement de telles données est devenue un domaine florissant. En effet, les satellites modernes sont capables d\u0027acquérir des images à très hautes résolutions (THR) avec une définition de plus en plus élevée sur un large domaine spectral. Or, les algorithmes capables de traiter un tel volume de données en un temps raisonnable sont pour le moment assez rares.\nLa segmentation de telles données d\u0027imageries peut se faire en utilisant des algorithmes basés sur les champs de Markov, (Roth et Black, 2011). Les champs de Markov reposent sur la notion de voisinage pour modéliser les dépendances qui peuvent exister entre des données telles que des pixels adjacents, ou des super-pixels adjacents (groupes de pixels).\nDans ces modèle, on considère S \u003d {s 1 , ..., s N }, s i ? 1..K un ensemble de variables aléatoires représentant les états (labels) des données. Ces états sont supposés liés dans l\u0027espaces par des relations de voisinages et émettent des observations X \u003d {x 1 , ..., x N } où les x i sont des vecteurs contenant les attributs de chaque donnée (RGB pour les modèles les plus simples). L\u0027objectif est alors de déterminer la configuration idéale de S, c\u0027est à dire de trouver les valeurs des s i afin d\u0027obtenir une segmentation optimale.\nUne des méthodes possibles pour résoudre ce problème est l\u0027utilisation du couple ICM-EM, (Zhang et al., 2001). Le choix de l\u0027algorithme ICM (Besag, 1986) vient du fait que cet algorithme a une complexité plus faible que celles des algorithmes plus récents utilisés pour les champs de Markov, ce qui est un atout non-négligeables pour traiter le volume important des données d\u0027une image à très haute résolution. De plus, la plupart des données issues de telles images sont déjà pré-traitées et l\u0027ICM est donc suffisant pour les segmenter. Il est également important de préciser qu\u0027à ce jour, l\u0027algorithme ICM est le seul à avoir été adapté pour pouvoir optimiser le modèle d\u0027énergie contenant des informations sémantiques utiles dans le cadre des image satellites que nous avons proposé dans de précédent travaux (Sublime et al., 2014). L\u0027objectif de l\u0027algorithme ICM est d\u0027optimiser itérativement une fonction d\u0027énergie locale dérivant du logarithme de P (x|s, ? s )P (s). Bien que cet algorithme ait montré son efficacité pour résoudre ce problème, il a cependant plusieurs défauts tels que son critère d\u0027arrêt basé sur l\u0027évolution de l\u0027énergie global et l\u0027absence de garantie de convergence. En effet, cet algorithme essaye d\u0027optimiser une fonction non-convexe, et il a été montré qu\u0027après un processus d\u0027optimisation relativement rapide de forme parabolique, l\u0027algorithme ne se stabilisait pas toujours et pouvait même se mettre à diverger entrainant ainsi une détérioration des résultats (Zhang, 1989). Une des difficultés récurrente de la segmentation d\u0027image dans le cadre non-supervisé est qu\u0027il est difficile d\u0027évaluer la qualité des résultats. Dans le cas de l\u0027ICM, le critère d\u0027arrêt repose sur l\u0027énergie globale liée aux champs de Markov (somme des énergies locales) et sur l\u0027hypothèse que cette énergie va se stabiliser. Le problème d\u0027une telle approche est précisément que cette énergie globale ne se stabilise pas toujours, l\u0027algorithme n\u0027ayant pas de garantie de convergence, et que même dans le cas où elle se stabilise cette stabilisation n\u0027intervient pas né-cessairement au moment où les résultats de la segmentation sont les meilleurs. De plus, en se basant sur l\u0027énergie globale avec un nombre de données élevé, il y a un risque non négligeable d\u0027\"overflow\" ou d\u0027arrondi de cette énergie.\nDans cette article, nous proposons un nouveau critère d\u0027arrêt pouvant être facilement calculé et qui repose sur un modèle d\u0027énergie adapté aux images satellites à très haute résolution.\nAlgorithme proposé\nOn note V x le voisinage d\u0027une observation x, et A \u003d {a i,j } K×K la matrice de voisinage de l\u0027image, où chaque a i,j est la probabilité de passer de l\u0027état i à l\u0027état j entre deux données voisines. A partir de ces notations nous utilisons comme modèle d\u0027énergie locale le modèle défini lors de nos précédents travaux (Sublime et al., 2014) :\nLa fonction d\u0027énergie décrite précédemment utilise une énergie locale dérivant de la loi normale dans laquelle µ s est la valeur moyenne associée à l\u0027état s et ? s sa matrice de covariance. Le dernier membre de cette énergie qui décrit l\u0027énergie de voisinage est une fonction positive calculée à partir des éléments de la matrice A. Le facteur ? x,v représente le poids de v en tant que voisin de la donnée x selon la proportion de frontière qu\u0027il occupe. Ce modèle repose sur l\u0027idée que des données voisines ayant des clusters différents ne sont pas nécessaire-ment incompatibles, et peut être vu comme une version relaxée du graph-cuts (Boykov et al., 2001).\nLa matrice de voisinage A de ce modèle d\u0027énergie contient des informations sémantiques telles que les affinités des différents clusters ou leur compacité sur l\u0027image. En effet, la diagonale de la matrice A contient la probabilité pour chaque cluster d\u0027avoir un voisinage plus ou moins composé d\u0027éléments du même cluster. Cette diagonale peut par conséquent être considérée comme un indicateur de compacité des clusters. Les éléments non-diagonaux quant à eux fournissent des informations sur les affinités des clusters.\nÉtant donné que l\u0027objectif principal de l\u0027utilisation des champs de Markov en segmentation d\u0027image est d\u0027obtenir des zones homogènes, nous avons décidé d\u0027utiliser cette information de compacité contenue dans notre matrice de voisinage et d\u0027en faire le nouveau critère d\u0027arrêt de notre algorithme C-ICM : \"Compactness-based Iterated Conditional Modes\".\nIl est en effet raisonnable de supposer qu\u0027il faut arrêter l\u0027algorithme lorsque la compacité des clusters cesse d\u0027augmenter. Aussi, nous utilisons les variations de la trace de la matrice A comme nouveau critère d\u0027arrêt. Ce critère présente plusieurs avantages : Les calculs sont faciles et plus rapides que ceux pour obtenir l\u0027énergie globale, ce critère permet également de repérer les cas où un cluster commence à en absorber d\u0027autres, ce qui arrive assez souvent avec l\u0027ICM. Un tel cas de figure provoquerait rapidement la convergence de la trace de A vers 1. Dans l\u0027algorithme (1), nous montrons comment nous avons adapté le framework EM-ICM pour notre critère d\u0027arrêt : Comme on peut le voir sur la Figure (2), le critère d\u0027arrêt basé sur l\u0027énergie aurait conduit l\u0027algorithme à s\u0027arrêter après la 8ème itération. Or, sur la Figure (1) l\u0027état de fusion des zones homogènes n\u0027est pas encore assez avancé après l\u0027itération 8 : La mer et une partie des bâtiments sont encore très fragmentés. Le critère de compacité aurait de son côté amené l\u0027algorithme à s\u0027arrêter après l\u0027itération 43 ce qui aurait conduit à des zones nettement plus homogènes : routes, eau, gros bâtiments, etc.\nOn notera également que les deux critères se stabilisent définitivement après l\u0027itération 47, itération après laquelle l\u0027image n\u0027évolue presque plus. Sur l\u0027image prise après la 50ème itéra-tion de l\u0027algorithme, on constate même le début assez marqué d\u0027un phénomène de détérioration avec certains clusters qui ont commencé à déborder sur d\u0027autres.\nDe cette première expérience, nous pouvons tirer les conclusions suivantes : Tout d\u0027abord, elle confirme la difficulté évoquée dans notre introduction de trouver un critère d\u0027arrêt idéal. En effet, le critère basé sur l\u0027énergie aurait ici arrêté la segmentation trop tôt et l\u0027énergie rebondit deux fois avant d\u0027atteindre sa stabilisation finale. Ensuite, on voit que notre critère basé sur la compacité semble plus stable : il n\u0027y a pas de rebond. Enfin, on notera que lorsque les deux critères semblent finalement se mettre d\u0027accord pour arrêter la segmentation (itération 43 pour la compacité, et itération 47 pour la stabilisation définitive de l\u0027énergie), on s\u0027aperçoit que nous sommes déjà dangereusement proche de la zone à partir de laquelle la segmentation commence à se détériorer.\nDonnées THR Strasbourg\nNotre seconde expérience a été effectuée sur un jeu de données construit à partir d\u0027une image satellite à très haute résolution de la ville de Strasbourg (Rougier et Puissant, 2014). Ce jeu de données pré-traitées utilise le modèle des super-pixels (agglomérats de pixels) avec des voisinages irréguliers : chaque super-pixel a entre 1 à 15 voisins. L\u0027image est représentée sous forme de 187.058 super-pixels ayant chacun 27 attributs radio-métriques et géométriques. On constate à l\u0027issue de cette expérience que notre indice de compacité tombe d\u0027accord avec l\u0027indice de qualité de clustering pour déterminer quand arrêter l\u0027algorithme. On notera tout de même qu\u0027il y a peu de différence en terme de qualité de résultats entre le moment d\u0027arrêt décidé par le critère de compacité, et celui décidé par le critère classique d\u0027énergie. Cependant, sur une image satellite à très haute résolution de cette taille, 2 itérations supplé-mentaires coûtent plusieurs minutes de calcul pour avoir dans le cas de cette expérience un résultat légèrement moins bon. Notre critère d\u0027arrêt semble donc être ici un choix plus judicieux pour décider d\u0027arrêter l\u0027algorithme au bon moment et économiser du temps de calcul.\nConclusion\nDans cet article, nous avons proposé une amélioration de l\u0027algorithme ICM pour la segmentation des images satellites à très haute résolution. Notre algorithme C-ICM introduit un nouveau critère d\u0027arrêt basé sur un modèle d\u0027énergie spécifique permettant d\u0027avoir des informations sur les relations entre les différents clusters. Notre critère repose ainsi sur la compacité et l\u0027homogénéité des clusters dans la segmentation plutôt que sur le traditionnel critère d\u0027énergie globale. Nos expériences préliminaires ont montré des résultats intéressants qui pourraient mener à une amélioration globale de l\u0027efficacité et à une vitesse accrue du traitement des\n"
  },
  {
    "id": "284",
    "text": "Introduction\nLe modèle des graphes conceptuels (Sowa, 1984;Chein et Mugnier, 2009) permet de repré-senter des connaissances sous la forme d\u0027un graphe étiqueté. Le modèle des graphes conceptuels utilise une représentation graphique visuelle des connaissances afin de faciliter la compré-hension pour les utilisateurs. La méthode d\u0027interrogation du modèle est basée sur l\u0027opération principale des graphes conceptuels, un homomorphisme de graphes appelé la projection : cette opération permet de déterminer si les connaissances exprimées dans un graphe conceptuel appelé graphe requête peuvent être déduites de celles exprimées dans la base de connaissances, représentée par un graphe conceptuel appelé graphe fait.\nLes objectifs de ce modèle sont proches d\u0027une partie de ceux des langages du Web sé-mantique tels que RDF, RDF-Schema ou OWL (Manola et al., 2004;Brickley et Guha, 2004;McGuinness et al., 2004) qui sont généralement interrogés en utilisant SPARQL (Garlik et al., 2013), une recommandation officielle du W3C disponible dans plusieurs outils. SPARQL offre plus de flexibilité par rapport aux graphes conceptuels dans l\u0027interrogation d\u0027une base de connaissances. D\u0027une part, SPARQL permet d\u0027exprimer une disjonction entre plusieurs parties d\u0027une requête (SPARQL utilise le mot « union ») et d\u0027identifier des parties comme obligatoires ou optionnelles. D\u0027autre part, SPARQL permet d\u0027interroger la base grâce à quatre types de requêtes : l\u0027interrogation, la sélection, la description et la construction. La requête d\u0027interrogation permet de savoir si la connaissance représentée par la requête est présente dans la base. La requête de sélection permet de trouver et extraire de la base des connaissances identifiées dans la requête comme importantes. La requête de description permet d\u0027obtenir des informations sur des connaissances de la requête. La requête de construction permet de déduire de nouvelles connaissances à partir de celles contenues dans la base.\nCet article propose de combiner la simplicité de la représentation visuelle des graphes conceptuels avec la puissance du modèle d\u0027interrogation du Web sémantique pour améliorer l\u0027expression des requêtes des graphes conceptuels. La contribution de cet article est triple. D\u0027abord, le graphe d\u0027interrogation est introduit. La notion de graphe d\u0027interrogation permet d\u0027exprimer des conditions de disjonction -un « ou » entre deux de ses sous-graphes -et des conditions d\u0027option -un sous-graphe est préféré mais non-nécessaire -dans un graphe conceptuel. Ensuite, le graphe d\u0027interrogation nous permet de définir un langage d\u0027interrogation pour les graphes conceptuels formé de quatre types de requêtes : requête d\u0027interrogation, requête de sélection, requête de description et requête de construction. Une requête d\u0027interrogation permet de savoir si le graphe de la requête est déductible du graphe fait. Une requête de sélection permet de trouver et extraire du graphe fait des sommets identifiés comme étant importants dans le graphe requête. Une requête de description permet d\u0027obtenir des informations complémentaires du graphe fait liées à un sommet particulier du graphe requête. Une requête de construction permet de déduire de nouvelles connaissances en utilisant les connaissances extraites du graphe fait. Enfin, l\u0027opération basique de calcul utilisée pour interroger et obtenir des réponses est introduite : la projection d\u0027un graphe d\u0027interrogation dans un graphe fait. Cette projection est définie en utilisant la projection classique d\u0027un graphe conceptuel dans un graphe fait.\nAucun langage générique d\u0027interrogation pour les graphes conceptuels n\u0027a encore été proposé, mais différentes idées ont été mises en avant. De même que la requête de sélection, la possibilité d\u0027identifier certains sommets dans un graphe requête pour facilement exploiter le résultat d\u0027une projection a déjà été proposé dans Sowa (1984). Notre approche est différente de Sowa (1984) dans lequel les sommets sont simplement marqués puisque nous proposons de les nommer pour pouvoir facilement les identifier dans le résultat de la projection. Dans la communauté de SPARQL, Corby et Faron-Zucker (2007) propose une implémentation de la recherche de motifs de graphes de SPARQL en utilisant la projection classique des graphes conceptuels. Notre approche, à l\u0027inverse, est de transposer les idées de SPARQL dans le modèle des graphes conceptuels.\nL\u0027article est organisé comme suit. La section 2 rappelle les bases du modèle des graphes conceptuels et de la projection. La section 3 présente le modèle des graphes d\u0027interrogation. La section 4 présente la projection d\u0027un graphe d\u0027interrogation dans un graphe conceptuel fait ainsi que la requête d\u0027interrogation, La section 5 présente la requête de sélection, La section 6 présente la requête de description, La section 7 présente la requête de construction. Enfin, la section 8 présente notre implémentation et donne quelques éléments pour comparer notre langage d\u0027interrogation des graphes conceptuels avec SPARQL.\nModèle des graphes conceptuels\nLe modèle des graphes conceptuels (Chein et Mugnier, 2009)  \nHomme Femme aPourPère(Humain, Homme)\n, ?) avec T C partiellement représenté par l\u0027arbre de gauche, T R partiellement représenté par l\u0027arbre de droite, et la signature ? de chaque relation est précisée à coté de chaque type de relation.\nExemple. La figure 1 montre un extrait du vocabulaire utilisé dans les exemples suivants.\nUn graphe conceptuel est un multigraphe biparti défini sur un vocabulaire. Un des ensembles de sommets est appelé l\u0027ensemble des sommets concepts, et l\u0027autre ensemble est appelé l\u0027ensemble des sommets relations, représentant les liens entre les concepts. Chaque sommet est étiqueté. Un sommet relation est étiqueté par un type de relations et un sommet concept est étiqueté par un couple formé d\u0027un type de concepts et d\u0027un marqueur. Si un concept c est le i-ème argument d\u0027une relation r, alors il y a une arête entre c et r étiquetée par i, et le type de concepts de c doit respecter les contraintes de la signature de r. Un sommet concept individuel est référencé par un marqueur individuel de I. Un sommet concept générique est ré-férencé par le marqueur générique * . Nous considérons les graphes sous forme normale : deux sommets concepts différents ne peuvent pas être étiquetés par le même marqueur individuel.\nUn graphe conceptuel défini sur V est un quadruplet G \u003d (C, R, E, l) qui satisfait les conditions suivantes :\n-(C, R, E) est un multigraphe fini, non-orienté et biparti. C est l\u0027ensemble des sommets concepts, R est l\u0027ensemble des sommets relations, E est l\u0027ensemble des arêtes \nLa projection est l\u0027opération d\u0027interrogation du modèle des graphes conceptuels. Soient un graphe requête G r et un graphe fait G f définis sur le même vocabulaire, G r se projette dans G f si les informations représentées par G r se déduisent de celles représentées par G f .\nLes sommets de G f qui correspondent aux sommets de G r sont appelés les images des sommets de G r par ?.\nUn bloc est défini comme un ensemble de sommets du graphe associé à un type de bloc. Tous les sommets concepts de l\u0027ensemble doivent avoir leurs sommets relations voisins dans l\u0027ensemble : ceci permet à un sommet concept du bloc d\u0027être caractérisé par les relations auxquelles il est lié. Différents types de blocs permettent d\u0027identifier la condition d\u0027un bloc : l\u0027Option (le bloc est dit optionnel), la Disjonction (le bloc est dit de disjonction) et le Standard (le bloc est dit standard).\nUn bloc optionnel permet d\u0027exprimer que la partie du graphe qu\u0027il contient est facultative : on recherche dans le graphe fait un graphe avec la partie optionnelle, mais si elle n\u0027est pas trouvée, un graphe privé de la partie optionnelle conviendra. Un bloc de disjonction est composé de blocs fils et exprime une disjonction entre ses blocs fils : on recherche dans le graphe fait un graphe avec seulement un des fils.\nUn bloc du graphe G est un couple b \u003d (S, T ) où -S ? C ? R est l\u0027ensemble des sommets du bloc tel que :\n?c ? S ? C, ?r ? R, si rc ? E alors r ? S -T ? {Standard, Option, Disjonction} est le type du bloc. Une arborescence est utilisée afin de structurer les blocs d\u0027un graphe requête. Les blocs sont les sommets de l\u0027arborescence. Les notions de bloc racine, bloc père et blocs fils sont définies grâce au vocabulaire lié aux arborescences. La structuration est hiérarchique, elle impose que la relation père-fils entre blocs vérifie l\u0027inclusion des sommets du bloc fils dans ceux du bloc père et que si deux blocs ont un sommet en commun, alors un de ces blocs est un ancêtre de l\u0027autre.\nStandard Disjonction\nDéfinition 5. Soit un graphe conceptuel G \u003d (C, R, E, l).\nUne structuration H de G est une arborescence (B, A, r) où la racine r est le bloc (C ? R, Standard), B étant l\u0027ensemble des blocs de G qui forme l\u0027ensemble des sommets de l\u0027arborescence, et A l\u0027ensemble des arcs. La structuration est telle que : H) est un couple où G est un graphe conceptuel et H est une structuration de G. \nProjection et requête d\u0027interrogation\nLes graphes d\u0027interrogation sont utilisés comme base de construction pour chacun des quatre types de requêtes.\nUn graphe d\u0027interrogation peut être développé en un ensemble de graphes conceptuels en suivant les conditions d\u0027option et de disjonction. Cet ensemble de graphes conceptuels, appelé l\u0027ensemble des graphes développés du graphe d\u0027interrogation, représente l\u0027ensemble des graphes conceptuels dont on cherche à savoir s\u0027ils sont déductibles du graphe fait. Cet ensemble permet de définir une projection d\u0027un graphe d\u0027interrogation dans un graphe fait comme étant une projection d\u0027un graphe développé dans le graphe fait. Pour obtenir un graphe développé à partir d\u0027un graphe d\u0027interrogation, des opérations sont appliquées sur le graphe d\u0027interrogation pour aboutir à un graphe ne contenant que des blocs standards. Les blocs optionnels seront retirés ou leur type sera changé en Standard. Le type des blocs de disjonction sera changé en Standard et seulement un et un seul de ses fils sera conservé.\nSoit b ? B, un bloc optionnel. Le graphe G i dans lequel le type de b est changé en Standard est un développement\nLe graphe G i dans lequel le type de b est changé en Standard et dans lequel tous les fils de b sauf un sont retirés est un développement de G i .\nL\u0027ensemble des graphes développés GD de G i est l\u0027ensemble de tous les graphes conceptuels associés aux graphes d\u0027interrogation ne contenant que des blocs standards obtenus grâce à une suite d\u0027applications de l\u0027opération de développement à partir de G i . Exemple. Les graphes conceptuels de la figure 4 forment l\u0027ensemble des graphes développés\nPour définir la projection d\u0027un graphe d\u0027interrogation dans un graphe fait, on construit les projections des graphes développés dans le graphe fait. La projection d\u0027un graphe développé est généralement une projection du graphe d\u0027interrogation. L\u0027exception est due à la sémantique de l\u0027option, en effet, la partie optionnelle d\u0027un graphe doit être présente dans la réponse si elle est présente dans le graphe fait : dans ce cas les projections qui ne contiendraient pas le bloc optionnel ne sont pas des projections du graphe d\u0027interrogation. Notons que ces dernières projections sont prolongées 2 par la projection contenant les informations optionnelles. On définit donc l\u0027ensemble des projections d\u0027un graphe d\u0027interrogation dans un graphe conceptuel comme l\u0027ensemble des projections des graphes développés du graphe d\u0027interrogation privé des projections prolongées.\nL\u0027ensemble des projections du graphe d\u0027interrogation G i dans G f est : \nRequête de sélection\nDans une requête d\u0027interrogation, l\u0027image de certains sommets peut être plus importante que l\u0027image des autres sommets de la requête. Cet article propose un type de requête dans lequel on peut identifier les sommets jugés importants et dont on veut retenir les images par la projection du graphe d\u0027interrogation : la requête de sélection.\nUne requête de sélection est donc un couple composé d\u0027un graphe d\u0027interrogation et d\u0027une fonction de nommage qui associe à des noms à sélectionner, des sommets du graphe.\nDéfinition 11. Soit un ensemble de noms N .\nUne requête de sélection est un couple\nUne réponse à une requête de sélection est une fonction qui associe à chaque nom de l\u0027ensemble des noms à sélectionner de la requête, un sommet du graphe fait. Si le graphe d\u0027interrogation se projette dans le graphe fait, les noms à sélectionner seront associés avec l\u0027image dans le graphe fait, si elle existe, du sommet du graphe d\u0027interrogation auxquels ils sont liés. Un sommet peut ne pas avoir d\u0027image par la projection, dans ce cas, le nom n\u0027est associé à aucun sommet. \nUne réponse à la requête de sélection R s est une fonction partielle de N dans C f ? R f associant à chaque élément n de N le sommet ?(select(n)) de G f s\u0027il existe.\nExemple. La figure 7 montre une requête de sélection R s \u003d [G i , select], où la fonction de nommage select, définie sur N \u003d {?prénom, ?surnom}, est représentée sur G i . La requête permet de connaître le prénom, et s\u0027il existe le surnom, de tous les humains du graphe fait. Les deux réponses à l\u0027exécution de R s dans G f (figure 5) sont présentées en partie droite de la figure 7.\nRequête de description\nLa requête de sélection permet d\u0027obtenir les images des sommets qui intéressent l\u0027utilisateur, la requête de description permet, elle, d\u0027obtenir non seulement les images des sommets qui intéressent l\u0027utilisateur, mais aussi leur description. La description d\u0027un sommet est formée par les sommets voisins qui apportent des informations sur ce sommet. Une requête de description est un couple composé d\u0027un graphe d\u0027interrogation et d\u0027une fonction de nommage qui à un nom de l\u0027ensemble des noms à décrire, associe un sommet du graphe. Étant donné que seuls les concepts peuvent être décrits, l\u0027ensemble à décrire ne doit contenir que des noms se référant à des sommets concepts.\nDéfinition 13. Soit un ensemble de noms N .\nUne requête de description est un couple\nUne réponse à une requête de description est un sous-graphe du graphe fait dans lequel figurent les images des concepts à décrire, les relations qui sont liées à ces concepts par une arête étiquetée par « 1 », et les concepts liés à ces relations. Seules les relations reliées par une arête étiquetée par « 1 » sont sélectionnées puisque se sont les relations dont le concept à décrire est le sujet. \nUne réponse à la requête de description R d est un sous-graphe\nExemple. La figure 8 montre une requête de description R d \u003d [G i , desc], où la fonction de nommage desc, définie sur N \u003d {?père}, est représentée sur G i . La requête permet de décrire tous les humains qui sont pères. Les trois réponses à l\u0027exécution de R d dans G f (figure 5) sont présentées en partie droite de la figure 8 : H2 et H3 sont pères, mais H2 est père à la fois de H1 et de F1.\nRequête de construction\nUne requête de construction est composée de deux graphes : un graphe d\u0027interrogation, appelé graphe condition, et un graphe conceptuel, appelé graphe modèle,. Le principe d\u0027une requête de construction est d\u0027une part d\u0027extraire des informations du graphe fait à l\u0027aide du graphe condition, qui doit se projeter dans le graphe fait, et d\u0027autre part d\u0027exploiter ces informations afin de construire un nouveau graphe conceptuel, appelé graphe réponse, à partir du graphe modèle. Une requête de construction peut être vue comme une règle (Salvat, 1998) dont la partie condition est définie par le graphe condition et la partie conclusion est obtenue par le graphe modèle. Pour lier les sommets du graphe condition à ceux du graphe modèle, on utilise une fonction lien qui associe à un sommet du graphe condition, un sommet du graphe modèle. Un sommet du graphe condition qui est lié à un sommet du graphe modèle doit avoir la même étiquette que ce dernier. Notons qu\u0027en utilisant l\u0027extension des types conjonctifs (Chein et Mugnier, 2004), il serait possible que deux sommets liés ne possèdent pas la même étiquette.\nest un graphe conceptuel appelé graphe modèle, tels que G et G m sont définis sur le même vocabulaire, lien est une fonction partielle définie comme suit : lien :\nUne réponse à une requête de construction est un graphe réponse construit sur le modèle du graphe modèle comme suit. Les sommets du graphe modèle liés aux sommets du graphe condition sont étiquetés par les images des sommets du graphe condition dans le graphe fait. Les sommets qui ne sont pas liés dans le graphe modèle gardent leurs étiquettes. Il se peut que des sommets du graphe condition ne possèdent pas d\u0027image par la projection, les sommets liés au graphe modèle sont alors retirés du graphe réponse, ainsi que toutes les relations éventuel-lement connectées à ces sommets. Ce cas se présente lorsqu\u0027un sommet du graphe modèle est lié à un sommet du graphe condition qui appartient à un bloc option, ou de disjonction.\nDéfinition 16. Soient un graphe conceptuel G f \u003d (C f , R f , E f , l f ), une requête de construction R c \u003d [G i , G m , lien] avec G i \u003d (G, H) où G \u003d (C, R, E, l) et avec G m \u003d (C m , R m , E m , l m ), une projection ? de G i dans G f , et l\u0027ensemble S ? des sommets de G qui ont une image dans G f par ?.\nUne réponse à la requête de construction R c est un graphe conceptuel appelé graphe ré-ponse G r \u003d (C r , R r , E r , l r ), tel que : -C r \u003d C m \\ {c \u003d lien(c ) ? C m |c ? C, c / ? S ? } -R r \u003d (R m \\ {r \u003d lien(r ) ? R m |r ? R, r / ? S ? }) \\ {r ? R m |rc ? E m , c / ? C r } -E r \u003d {nn ? E m |n, n ? C r ? R r } -l r est définie de la façon suivante ?s ? C r ? R r ? E r :\nsi ?s ? C ? R tel que s \u003d lien(s ), l r (s) \u003d l f (?(s )) sinon, l r (s) \u003d l m (s)\nExemple. La figure 9 montre la requête R c \u003d [G i , G m , lien] où la fonction lien est représentée directement sur le graphe via les pointillés. Cette requête peut s\u0027apparenter à une règle qui dit que si un humain a pour parent un autre humain, alors ce dernier est un parent du premier, mais si cet humain n\u0027a pas de parent, personne n\u0027est son parent. Les quatre réponses à l\u0027exécution de R c dans G f (figure 5) sont présentées en partie droite de la figure 9.\n8 Conclusion\n"
  },
  {
    "id": "286",
    "text": "Introduction\nLes réseaux sociaux sont l\u0027objet d\u0027une recherche intense depuis plusieurs années (Carrington et al., 2005;Newman et al., 2006;Scott et Carrington, 2011). Leur étude donne lieu à différentes questions concernant leur évolution, qu\u0027il s\u0027agisse d\u0027analyser comment les interactions se sont mises en place, ou alors de comprendre l\u0027état du système qu\u0027ils décrivent. Parmi ces interrogations, l\u0027étude des phénomènes de propagation dans les réseaux a suscité un intérêt soutenu au sein de la communauté, multipliant les domaines d\u0027applications, allant de la sociologie (Granovetter, 1978;Macy, 1991) à l\u0027épidémiologie (Hethcote, 2000;Dodds et Watts, 2005;Bertuzzo et al., 2010) en passant par la publicité virale et le placement de produits (Domingos et Richardson, 2001;Chen et al., 2010).\nNous nous intéressons dans cet article à l\u0027étude de la propagation dans les réseaux sociaux. Notre objectif est de proposer une méthodologie permettant de comparer des modèles préexistants et documentés de propagation. Le grand nombre et les différentes variations de ces derniers offrent un assortiment de solutions compliquant le choix d\u0027un modèle particulier. Afin de faciliter cette tâche, il convient de pouvoir comparer effectivement les modèles et non seulement les résultats finaux obtenus suite à leur application.\nCette ambition rejoint Kempe et al. (2003) qui proposent une généralisation de différents types de modèles de propagation. Ces résultats permettent de voir les modèles dans un cadre entièrement mathématique où chacun des algorithmes devient une solution à un problème d\u0027optimisation commun. A l\u0027inverse, nous adoptons une perspective résolument algorithmique dont l\u0027objectif est de venir en appui à une approche exploratoire.\nIl n\u0027existe à notre connaissance pas de formalisme unifiant toutes les approches permettant d\u0027effectuer une comparaison des modèles, de leur formulation, leur complexité ou leurs performances. La première contribution de cet article est donc de proposer un tel cadre unificateur basé sur un formalisme solide : la réécriture de graphes.\nLa propagation est généralement vue comme un phénomène global au réseau bien qu\u0027elle émerge en réalité de la somme d\u0027une multitude d\u0027évènements y agissant localement. La plupart des modèles consistent donc en un ensemble de règles décrivant les situations dans lesquelles une entité peut influencer ses voisins. Bien que chacun de ces évènements soit décrit localement et succinctement, l\u0027application répétée de transformations locales permet de faire émer-ger le comportement du modèle au niveau global. Dans ce formalisme, un modèle correspond alors à un ensemble de règles de transformation couplé à une stratégie qui régule et ordonne l\u0027application de ces mêmes règles.\nLes modèles auxquels nous nous intéressons par la suite considèrent un réseau social dont la topologie est fixée. Les règles décrivent alors comment évoluent les états des sommets du ré-seau. Dans leurs travaux, Kejžar et al. (2008) se sont intéressés, de façon similaire, à l\u0027évolution du caractère topologique d\u0027un réseau social. Partant d\u0027un réseau pré-existant, les auteurs proposent une série de règles permettant de modifier les connexions entre les acteurs du réseau, autorisant ainsi la création ou suppression de liens. Leur travail rejoint donc notre approche consistant à exploiter la réécriture comme mécanisme pour exprimer leurs modèles d\u0027évo-lution des réseaux. Cependant, leur article est davantage orienté vers l\u0027analyse des résultats asymptotiques probabilistes sur l\u0027évolution de la taille et la densité des réseaux ainsi produits. L\u0027intérêt de notre approche basée sur une description commune des modèles tient égale-ment à la possibilité d\u0027étudier et de comparer ceux-ci de manière expérimentale. La plupart des travaux s\u0027intéressent à l\u0027objectif atteint au terme d\u0027une propagation (couverture du réseau, vitesse de propagation, etc.), en revanche, il est plus difficile d\u0027établir des résultats expliquant comment se déroule la propagation et pourquoi cet objectif est atteint. L\u0027utilisation d\u0027un formalisme commun nous permet, au contraire, la réalisation de ce type d\u0027investigation.\nDe plus, cette méthodologie prend un sens particulier lorsque l\u0027étude des modèles se fait de manière visuelle et interactive. En manipulant le modèle (en lançant des simulations, en isolant une règle, etc.), l\u0027utilisateur est à même de développer une connaissance du modèle ainsi que de mesurer et suivre son comportement au fil du déroulement. Pour ces raisons, nous présentons une plate-forme de visualisation analytique -exploitant une version étendue de PORGY (Pinaud et al., 2012) (voir Fig. 1) -pour, simultanément, construire les réseaux et règles de réécriture, simuler la propagation selon différentes stratégies (i.e. les modèles) et comparer les traces d\u0027exécution de ces dernières à l\u0027aide de divers critères.\nL\u0027article introduit d\u0027abord la terminologie propre aux modèles de propagation des réseaux et décrit deux modèles particuliers (section 2). Ces modèles sont ensuite exprimés à l\u0027aide de règles de réécriture illustrant ainsi le pouvoir d\u0027expression et l\u0027utilisabilité du formalisme (section 3). Nous montrons enfin comment la plate-forme de visualisation peut être utilisée pour étudier les modèles et exhiber leurs différences (section 4).\nFIG. 1: Interface de PORGY :\n(1) le réseau social sur lequel on applique la propagation ; (2) édition d\u0027une règle ; (3) portion de l\u0027arbre de dérivation, conservant une trace complète des calculs réalisés (le graphe (1) représente un des sommet de celui-ci) ; (4) courbe montrant l\u0027évolution du nombre de sommets actifs ; (5) autre représentation de l\u0027arbre de dérivation ; (6) éditeur de stratégies.\nModélisation de la propagation dans les réseaux sociaux\nUn réseau social (Brandes et Wagner, 2003)   (2010)) construits au fil du déroulement de la propagation et qui peuvent être utilisés pour mesurer l\u0027influence d\u0027un utilisateur sur ses voisins ou représenter sa tolérance à la réalisation d\u0027une action (plus un utilisateur est sollicité, plus il sera enclin à s\u0027activer ou inversement).\nFace à cette diversité, nous nous limitons dans la suite de l\u0027article à illustrer la faisabilité de notre approche sur deux modèles représentatifs : un modèle à cascades indépendantes (IC, Kempe et al. (2003)) utilisé comme base pour de nombreux cas, et un modèle à seuils linéaires (LT, Goyal et al. (2010)) qui exploite un principe d\u0027activation non probabiliste contrairement au modèle précédent :\nLe modèle à cascades indépendantes IC. Ce modèle comporte de nombreuses variations (e.g. Gomez-Rodriguez et al. (2010); Watts (2002)) permettant, par exemple, la propagation d\u0027opinions divergentes dans un même réseau (Chen et al., 2011). Nous le décrivons sous une forme basique, telle que proposée par Kempe et al. (2003).\nSoit un sous-ensemble de sommets A 0 ? V activés au temps t \u003d 0 et les probabilités p v,w , définies pour toutes paires de sommets voisins {v, w} pour représenter l\u0027influence de v ? A 0 sur w / ? A 0 . Une série de nouveaux ensembles de sommets activés A t+1 est calculé à partir de A t . Pour chacun des sommets dans A t , on visite les voisins w ? N (v) qui n\u0027ont pas déjà été activés (mais qui peuvent déjà avoir été visités) ; en d\u0027autres mots, w ? N (v) \\ ? t i\u003d0 A i . Un sommet w peut alors devenir actif avec une probabilité p v,w , auquel cas il est ajouté à A t+1 . L\u0027algorithme s\u0027arrête lorsque A t+k est vide (pour k ? 0).\nModèle à seuil linéaire LT. Ce modèle suit un déroulement différent du précédent. Il le rejoint cependant en ce qu\u0027un sommet ne change plus d\u0027état dès lors qu\u0027il est activé. On suppose donné, soit aléatoirement, soit appris selon un historique d\u0027actions connues, les probabilités p v,w . Chaque sommet w ? V est aussi équipé d\u0027un seuil ? w . Soit S w l\u0027ensemble des voisins du sommet w qui sont activés. On détermine l\u0027ensemble A t+1 en calculant pour chaque sommet w non encore activé la valeur d\u0027influence jointe p w (S) \u003d 1 ? v?Sw (1 ? p v,w ). Le sommet w devient ainsi actif dès que l\u0027influence de ses voisins excède son seuil d\u0027activation, c\u0027est à dire lorsque p w (S) ? ? w . \nRéécriture de graphes\nLes éléments de base du calcul de la réécriture sont des sommets du graphe, additionnellement équipés de ports, auxquels les arêtes vont se connecter. Plus généralement, les sommets, ports et arêtes vont avoir des propriétés associées à une valeur (par exemple la probabilité p v,w ou le nom donné à un port) qui permettront de les distinguer entre eux, une combinaison spécifique des ces propriétés pouvant être identifiée comme un état.\nUne règle de réécriture consiste en un couple L ? R où L et R sont eux-mêmes des graphes (souvent petits). L et R sont respectivement appelés les membres gauche et droit de la règle. L\u0027application d\u0027une règle sur un graphe G se fait en localisant dans G un sousgraphe H isomorphe à L et en le \"remplaçant\" par R. La notion d\u0027isomorphisme doit toutefois être étendue pour tenir compte des états des sommets, des ports et des arêtes. La règle doit également, le cas échéant, préciser comment traiter les arêtes incidentes aux ports de H qui ne sont pas mentionnées dans L. De plus, une règle peut aussi calculer les valeurs de plusieurs propriétés de R en fonction de celles de H.\nDes exemples de règles sont donnés figure 2. La règle 2a concerne une paire de sommets, l\u0027un dans l\u0027état activé (vert), l\u0027autre étant non activé (rouge), connectés par une arête allant du port In du sommet activé vers le port Out du sommet non activé. La règle maintient la connexion entre les sommets et modifie l\u0027état du sommet rouge, le faisant passer dans l\u0027état violet signifiant que le sommet a été visité (ses propriétés ont été lues et il est possible de tenter de l\u0027activer). La règle 2b ne concerne qu\u0027un seul sommet ; son application est donc potentiellement réalisable sur tout sommet du graphe à condition qu\u0027il soit dans le même état (ici visité, symbolisé par la couleur violette). La règle consiste simplement à modifier l\u0027état du sommet, le faisant passer de l\u0027état violet à l\u0027état vert (activé).\nLe défi consiste la plupart du temps à savoir prédire le comportement de la réécriture ré-pétée de règles sur un graphe. En effet, l\u0027exécution des règles n\u0027est pas déterministe puisque leur ordre d\u0027application n\u0027est, a priori, pas précisé, mais aussi parce qu\u0027elles peuvent être appliquées sur de multiples instances H du membre gauche L de la règle. Il devient alors intéressant de savoir si le calcul de la réécriture converge et s\u0027il est confluent.\nDans cet optique, il peut également être tentant de chercher à conditionner l\u0027ordre d\u0027application des règles afin de guider le comportement du calcul. A cette fin, il est possible de définir une stratégie d\u0027application des règles. Une stratégie permet de choisir un ensemble des règles à appliquer, préciser leur ordre, le nombre de répétitions, et l\u0027endroit où celles-ci peuvent être ap-pliquées. Pour plus de détails sur le langage de stratégies utilisé par la plate-forme PORGY, sa formalisation et ses propriétés en tant que langage formel, le lecteur pourra consulter l\u0027article de Fernandez et al. (2014).\nTraduction des modèles de propagation\nLe premier défi qui se pose à nous est de pouvoir donner, pour chacun des modèles présen-tés dans la section 2, un ensemble de règles et une stratégie d\u0027application de ces dernières qui permet d\u0027émuler le fonctionnement du modèle.\nNotre démarche de traduction d\u0027un modèle de propagation quelconque en une série de règles de réécriture et leur stratégie d\u0027application est aisément généralisable. Celle-ci a pu être appliquée à tous les modèles rencontrés dans la bibliographie étudiée. Pour la clarté de la discussion concernant les étapes à suivre, nous présentons uniquement la traduction du modèle de propagation à cascades indépendantes (section 2). Ce dernier illustre tout à fait les opérations à réaliser et toute procédure de traduction d\u0027autres modèles suit un déroulement similaire.\nLe motif (membre gauche d\u0027une règle) principal à rechercher pour faire évoluer la propagation consiste à identifier un couple de sommets voisins dont l\u0027un est activé et l\u0027autre ne l\u0027est pas. Il est ainsi nécessaire de conserver pour chaque sommet son état actuel. De manière similaire, chaque arête devra préserver les probabilités d\u0027influence p v,w et p w,v que ses extrémités v et w pourront imposer l\u0027une sur l\u0027autre lorsque l\u0027un des sommets s\u0027activera.\nLa stratégie employée consiste, pour chaque sommet non actif, à calculer puis stocker l\u0027influence de ses voisins actifs en ne conservant que la valeur pour le sommet qui a l\u0027influence la plus forte. Le sommet non actif passe alors dans l\u0027état visité (règle de la figure 2a). Le parcours du voisinage est, de cette manière, contrôlé par la stratégie tandis que les actions à effectuer sur le réseau sont contrôlées par les règles. La procédure décrite ci-dessus forme une stratégie qui sera répétée tant qu\u0027un sommet actif peut influencer un de ses voisins (il reste une arête non marquée qui permet d\u0027appliquer la règle 2a). Chaque application de règle entraîne l\u0027ajout d\u0027un sommet sur l\u0027arbre de dérivation et d\u0027une arête (de couleur violette) pour montrer la succession des opérations. Les points de départ et d\u0027arrivée d\u0027une stratégie (enchaînement de plusieurs règles) sont eux représentés par une arête verte (Fig. 1). Cet arbre peut rapidement atteindre une taille conséquente, rendant la lisibilité difficile. Nous pouvons néanmoins le filtrer et ne conserver que les arêtes vertes et les sommets correspondants (Fig. 4).\nVisualisation analytique et comparaison des modèles\nNous détaillons dans cette partie comment la plate-forme de visualisation PORGY (Pinaud et al., 2012) est utilisée pour comparer deux applications des modèles de propagation présentés au début de cet article. Nous avons utilisé le modèle de Wang et al. (2006) pour générer un réseau social aléatoire de 300 sommets. Le réseau obtenu a 597 arêtes. Les conditions de départ sont identiques pour les deux modèles : même ensemble initial de sommets activés et même distribution de probabilités d\u0027influence entre les sommets. Notre objectif n\u0027est pas de montrer que tel modèle de propagation est meilleur que tel autre (ceci nécessiterait de nombreuses simulations pour calculer les résultats moyens sur les modèles probabilistes) mais plutôt de comprendre comment les modèles fonctionnent 1 . Les applications successives de la stratégie décrite précédemment permettent aux sommets actifs de transmettre l\u0027information ou l\u0027action représentant le sujet de la propagation à leur voisins. Chaque exécution de règle va créer un état intermédiaire du graphe d\u0027origine qui sera conservé dans la trace de la propagation (passage des différents sommets de non visité à visité puis potentiellement actif ). Cet historique va pouvoir être exploité pour étudier et comparer le graphe à un instant donné ou pour reconstituer et suivre le chemin emprunté par le processus d\u0027activation des sommets.\nUn arbre de dérivation (voir fig.1) est ainsi créé et maintenu pour fournir toutes ces informations. En visualisant des états successifs, nous pouvons observer cette progression. La figure 3 présente quelques vignettes d\u0027une vue de type Small-Multiples, qui est une partie du graphe analysé, et montre l\u0027évolution de l\u0027état de ses sommets. Les différents temps t représentent les applications successives des stratégies (un sommet non visité à t peut donc se retrouver activé à t + 1). L\u0027arbre de dérivation nous permet immédiatement de montrer quel est le modèle qui nécessite le moins d\u0027étapes de calcul ou le moins de lancements de stratégie avant d\u0027arriver à terme car sa branche est la plus courte (figure 4). Nous avons ainsi une première approximation de la complexité en temps des algorithmes.\nLier la profondeur de l\u0027arbre (donc le temps) avec d\u0027autres mesures nous permet de considérer l\u0027évolution de différents paramètres tout au long de la propagation. Nous pouvons, par exemple, aborder la notion de vitesse de propagation, valeur indiquant l\u0027évolution du nombre de sommets actifs en fonction du temps. La figure 4 présente l\u0027évolution de cette valeur pour une exécution des modèles à cascades indépendantes (partie droite, courbe du haut) et à seuils linéaires (courbe du bas). Les courbes présentées n\u0027ont pas les mêmes échelles puisqu\u0027elles sont calculées indépendamment pour chacun des modèles. Malgré ceci, nous pouvons observer que le modèle à cascades va parvenir à activer environ 80% des sommets contre seulement 18% pour celui à seuils (pour un même nombre d\u0027étapes de réécritures), démontrant le fort impact sur les performances des modèles, dans un premier temps, des valeurs utilisées pour l\u0027initialisation des probabilités d\u0027influence, et dans un second temps, du choix de l\u0027ensemble de sommets initialement activés. Nous avons utilisé une loupe (fonctionnalité de PORGY) sur le haut de chaque axe pour rendre les valeurs lisibles. Nous pouvons aussi noter qu\u0027après la première application de la stratégie, le nombre de sommets activés est très proche pour chacun des modèles, les différences apparaissant et se confirmant par la suite. Il peut aussi être intéressant de voir l\u0027état du graphe quand le nombre de sommets activés atteint un seuil. Puisque les différentes vues sont liées, la sélection d\u0027un sommet/arête (en bleu sur la figure 4) de l\u0027arbre de dérivation entraîne sa sélection dans la courbe et vice-versa. De manière similaire, la sélection d\u0027un sommet lors d\u0027une étape de la propagation sera immédia-tement répercutée à l\u0027ensemble des étapes contenant ce même élément, rendant la sélection visible même sur les sommets de l\u0027arbre de dérivation. D\u0027après la méthodologie employée par l\u0027application PORGY, tant qu\u0027un élément n\u0027est pas modifié par une règle, il n\u0027est jamais changé. En conséquence, la sélection d\u0027un sommet d\u0027intérêt dans l\u0027un des graphes intermé-diaires représentant le réseau permet de savoir directement quand cet élément a changé d\u0027état, surtout si l\u0027on travaille sur la version complète de l\u0027arbre de dérivation (montrant le détail des applications de règles).\nFinalement, le type de mesure évoluant selon le temps peut être généralisé à d\u0027autres propriétés des modèles de propagation. La notion de sommet visité, introduite précédemment, peut présenter un intérêt, dans le cas où un message doit seulement être vu et non nécessai-rement redistribué ou propagé par les utilisateurs. Cette vitesse de connaissance du contenu de la propagation est observable de manière similaire à la vitesse de propagation. De plus, en considérant ces deux mesures, nous pouvons en proposer une troisième exprimant l\u0027efficacité d\u0027une propagation, calculable grâce au rapport entre le nombre de sommets activés à un instant t et ceux visités/influencés au moment précédent t ? 1.\nConclusion et travaux futurs\nNous avons présenté un formalisme basé sur la réécriture de graphes vu comme un langage commun à l\u0027expression de tous les modèles de propagation sur réseaux. Lorsque la propagation n\u0027entraîne pas de modifications de la topologie du graphe, le modèle consiste en un ensemble réduit de règles gérant les transitions d\u0027état des sommets du graphe. Le stockage des états des sommets, les règles et le langage de stratégie qui pilote leur application facilite la gestion du caractère probabiliste des modèles.\nNous envisageons d\u0027étendre notre étude à un panel plus large de modèles de propagation afin de démontrer le caractère \"universel\" de notre approche. Cela exige aussi de pouvoir multiplier les simulations sur des réseaux de tailles conséquentes. Cet aspect pose un défi en raison de la complexité liée à la recherche de motifs correspondant aux membres gauches des règles -d\u0027abord parce que nous faisons face à un problème NP-Complet (isomorphisme de sous-graphes), mais aussi à cause de l\u0027explosion combinatoire qu\u0027elle engendre et qui doit être gérée à l\u0027aide du langage de stratégie (d\u0027application des règles).\nLa formulation des modèles à l\u0027aide de réécritures offre une possibilité nouvelle qui permettra de combiner propagation dans le réseau et évolution de la topologie du réseau sur lequel la propagation a lieu. Là encore, un langage de stratégie facilitera la gestion de l\u0027application simultanée ou alternée de ces deux types de transformations. Il n\u0027existe pas, à notre connaissance, de tels modèles. Nous espérons ainsi pouvoir proposer des modèles réalistes d\u0027évolution de réseaux, dont le réalisme tiendrait à la fois aux caractères structurels des réseaux produits, mais aussi à leur qualité en terme de circulation de l\u0027information.\ncan only be made at the cost of describing models based on a common formalism and independant from them. We propose to use graph rewriting to formally describe the propagation mechanisms as local transformation rules applied according to a strategy. This approach makes complete sense when supported by a visual analytics framework dedicated to graph rewriting. The paper first presents several models and illustrates them through selected simulations. We then show how our visual analytics framework allows to interactively manipulate models, and underline their differences based on measures computed on simulation traces.\n"
  },
  {
    "id": "287",
    "text": "Introduction\nL\u0027évolution d\u0027ontologie est un sujet posé avec l\u0027apparition des méthodologies de construction d\u0027ontologies. Il s\u0027est avéré indispensable de penser à maintenir et faire évoluer les ontologies, après leur construction, afin d\u0027assurer leur réutilisation et leur continuité. Ce besoin s\u0027est rapidement développé avec la prolifération des ontologies et leur large utilisation. À titre d\u0027exemple, depuis Janvier 2010, 59 versions de l\u0027ontologie Gene Ontology 1 (une des plus fameuses ontologies) ont été publiées à raison d\u0027une version par mois. Ainsi, afin de définir et gérer le processus d\u0027évolution, plusieurs méthodologies ont été proposées dans la littérature (Klein, 2004;Stojanovic, 2004;Djedidi et Aufaure, 2010;Khattak et al., 2013). Les premiers travaux ont pensé à définir ce qu\u0027est une évolution d\u0027ontologie. D\u0027où la définition proposée par (Stojanovic, 2004) : \"l\u0027évolution d\u0027ontologie est l\u0027adaptation, dans le temps, d\u0027une ontologie aux besoins de changement et la propagation cohérente des changements aux artefacts dépen-dants\". Cette définition a ouvert le débat sur la signification des changements ontologiques, leurs formalisations et leurs types (Klein, 2004;Stojanovic, 2004). Ainsi, un changement ontologique est une modification d\u0027une ou plusieurs entités ontologiques (classe, propriété, axiome, individus, etc.). Il peut viser la modification de la structure de l\u0027ontologie (ex. ajout de classe, ajout de propriété) et on parle dans ce cas de l\u0027enrichissement d\u0027ontologie. Il peut viser égale-ment l\u0027ajout d\u0027individus et on parle alors du peuplement d\u0027ontologie. Les changements ontologiques sont souvent de trois types (Stojanovic, 2004) : 1) les changements élémentaires qui représentent une opération primitive et non décomposable qui affecte une seule entité ontologique (ex. renommer une classe) ; 2) les changements composés (composites) qui affectent une entité ontologique et ses voisins (ex. suppression d\u0027une classe) ; 3) les changements complexes qui expriment un enchainement de plusieurs changements élémentaires et/ou composés (ex. fusion de classes). En effet, les changements composés et complexes sont des changements utiles et demandés par l\u0027utilisateur. Ils englobent plusieurs modifications en une seule opéra-tion, ce qui lui permet d\u0027adapter son ontologie d\u0027une manière plus facile sans se perdre dans les détails des changements élémentaires (Klein et Noy, 2003). Cependant, la définition et la formalisation de ces changements sont des tâches non triviales comme leur application peut affecter la cohérence de l\u0027ontologie. Deux types de cohérence sont généralement distingués dans la littérature : 1) la cohérence conceptuelle qui se réfère aux règles structurelles et contraintes du langage de représentation de l\u0027ontologie (ex. inexistence de concepts isolés) ; 2) la cohé-rence sémantique qui se réfère à la cohérence logique de l\u0027ontologie dans le sens où elle ne doit pas comporter des contradictions logiques (ex. ne pas avoir des relations contradictoires entre deux concepts). En effet, la préservation de la consistance de l\u0027ontologie et la résolution des incohérences résultantes de l\u0027application des changements ontologiques sont encore des problématiques insuffisamment étudiées. Ainsi, nous proposons dans cet article une nouvelle formalisation des changements ontologiques composés et complexes permettant : 1) d\u0027éviter les inconsistances d\u0027une manière a priori en utilisant les concepts des grammaires de graphes typés ; 2) de réduire le nombre de changements élémentaires constituant les changements composés/complexes. Les changements étudiés traitent à la fois le niveau structurel de l\u0027ontologie (l\u0027enrichissement de l\u0027ontologie) et aussi le niveau assertionnel (le peuplement d\u0027ontologie).\nLe reste de l\u0027article sera organisé comme suit : la section 2 présente un tour d\u0027horizon sur les principales approches d\u0027évolution d\u0027ontologie. La section 3 introduit les concepts de base des grammaires de graphes. La section 4 propose une nouvelle formalisation des changements ontologiques composés et complexes. Enfin, une conclusion synthétise le travail présenté et donne les perspectives envisagées.\nÉtat de l\u0027art\nDe nombreuses approches ont été proposées dans la littérature pour définir et implémenter le processus d\u0027évolution d\u0027ontologies. Le Tableau 1 présente certaines approches tout en préci-sant les langages utilisés, l\u0027implémentation, la gestion des inconsistances et leurs spécificités. Ainsi, nous pouvons observer que différents langages ont été étudiés : KAON (Stojanovic, 2004), RDF (Luong et Dieng-Kuntz, 2007), OWL (Klein, 2004;Djedidi et Aufaure, 2010), etc. En se basant sur ces langages, plusieurs changements ontologiques ont été définis et différentes classifications de ces changements ont été proposées (Stojanovic, 2004;Klein, 2004). Ainsi, certains travaux se sont intéressés à l\u0027étude des changements élémentaires (Mahfoudh et al., 2013). D\u0027autres ont traité également les changements composés et complexes (Djedidi et Aufaure, 2010;Javed et al., 2013;Liu et al., 2014). Des travaux se sont focalisés sur l\u0027enrichissement d\u0027ontologies (Klein, 2004). D\u0027autres ont étudié aussi le peuplement d\u0027ontologies (Luong et Dieng-Kuntz, 2007;Djedidi et Aufaure, 2010;Mahfoudh et al., 2013). La résolution des inconsistances est encore insuffisamment étudié. En effet, certaines approches ont ignoré cet axe comme ils se sont intéressés à d\u0027autres problématiques, comme par exemple la gestion des versions des ontologies (Hartung et al., 2013). D\u0027autres travaux se sont focalisés plutôt sur l\u0027identification des inconsistances sans les résoudre (Gueffaz et al., 2012). Certains chercheurs se sont intéressés également par la résolution des inconsistances (Djedidi et Aufaure, 2010;Luong et Dieng-Kuntz, 2007;Javed et al., 2013). Cependant, les approches proposées admettant un processus a postériori de traitement des inconsistances qui nécessite l\u0027utilisation d\u0027une ressource externe (tel qu\u0027un raisonneur) afin de vérifier la consistance de l\u0027ontologie évoluée. Afin d\u0027éviter l\u0027utilisation d\u0027un raisonneur externe, Mahfoudh et al. (2013)  -Approche basée sur les patrons de conception.\n-Évaluation de la qualité de l\u0027ontologie évoluée.\n-Approche nécessitant des activités lourdes. (Gueffaz et al., 2012) OWL DL Prototype -Identification des inconsistances en utilisant le checker NuSMV.\n-Approche d\u0027évolution d\u0027ontologies, CLOCk (Change Log Ontology Checker) basée sur le modèle checking.\n-Approche nécessitant la transformation des ontologies OWL au langage NuSMV. (Hartung et al., 2013) OBO \nGrammaires de Graphes Typés\nLes grammaires de graphes, également appelées réécriture de graphes, sont un formalisme mathématique pour représenter et gérer les graphes. Elles permettent la modification de graphes via des règles de réécriture tout en précisant quand et comment faire les changements. Grâce aux concepts et outils qu\u0027elles proposent, les grammaires de graphes sont utilisées dans plusieurs branches de l\u0027informatique, comme par exemple la modélisation des systèmes logiciels et la théorie des langages formels (Ehrig et al., 1996). Elles ont récemment été introduites dans le domaine des ontologies, ce qui a donné naissance à des travaux traitant de la formalisation des ontologies modulaires (d\u0027Aquin et al., 2007), la représentation des graphes RDF (Resource Description Framework) (Braatz et Brandt, 2010), la fusion d\u0027ontologies (Mahfoudh et al., 2014a), etc. Dans ce qui suit, nous dressons un tour d\u0027horizon des définitions de base concernant les fondements théoriques de la réécriture de graphes. Graphe. Un graphe G(N, E) est une structure composée par un ensemble de noeuds (N ), d\u0027arêtes (E) et d\u0027une application s : E ? N × N qui attache les noeuds source/destination à chaque arête. Graphe attribué. Un graphe attribué est un graphe étendu par un ensemble d\u0027attributs A, une fonction d\u0027attribution att : N ? E ? P(A), P représentant l\u0027ensemble des parties, et une fonction d\u0027évaluation val : A ? V . Ainsi, chaque noeud ou arête peut avoir un ensemble d\u0027attributs (P(A)) dont les valeurs seront données par val.\navec N T correspond aux types des noeuds et E T aux types des arêtes. Grammaires de graphes typés. Une grammaire de graphe typé est une structure mathéma-tique définie par T GG \u003d (G, T G, P ) avec :\n-G est un graphe initial, appelé aussi graphe hôte ; -T G est un graphe type précisant le type de l\u0027information représentée dans le graphe hôte (type des noeuds et des arêtes) ; -P un ensemble de règles de réécriture, appelées aussi règles de production ou de transformations de graphes. Une règle de réécriture r est une paire de graphes pattern (LHS, RHS) avec : 1) LHS (Left Hand Side) représente la pré-condition de la règle de réécri-ture et décrit la structure qu\u0027il faut trouver dans un graphe G pour pouvoir appliquer la règle ; 2) RHS (Right Hand Side) représente la post-condition de la règle de réécriture et doit remplacer LHS dans G. Les règles peuvent également avoir des conditions supplémentaires appelées N AC (Negative Application Conditions). Ce sont des graphes pattern définissant des conditions ne devant pas être vérifiées pour que la règle de réécriture puisse être appliquée. La transformation de graphe consiste ainsi à définir comment un graphe G peut être transformé en un nouveau graphe G . Cette transformation peut être réalisée selon deux types d\u0027approches (Rozenberg, 1999) : les approches ensemblistes (Node replacement, Edge replacement, etc.) et les approches algébriques. Dans ce travail, nous utilisons les approches algébriques basées sur le concept de pushout de la théorie des catégories (Ehrig et al., 1973). Pushout. Soient trois objets de la catégorie des graphes :\n. A partir de là, deux variantes sont proposées pour la réécriture des graphes : le Simple pushout SPO (Löwe, 1993) et le Double poushout DPO (Ehrig, 1979). Dans ce travail, seule l\u0027approche SPO a été considérée car elle se voit plus générale et permet l\u0027application des différents changements ontologiques (Mahfoudh et al., 2014b). Ainsi, appliquer une règle de réécriture à un graphe initial G, selon la méthode SPO, revient à : 1) trouver un morphisme (m) permettant d\u0027identifier un sous-graphe de graphe G qui correspond (match) avec la partie LHS (m : LHS ? G) ; 2) appliquer la règle de réécriture sur le sous-graphe en le remplaçant par m(RHS) et supprimant les arêtes suspendues, i.e. les arêtes qui ont une extrémité non liée à un noeud. Ainsi, d\u0027une manière générale, nous avons SP O(G, LHS, RHS) \u003d G .\nFormalisation des changements ontologiques 4.1 Modèle de transformation de graphes\nAfin de représenter les ontologies avec le formalisme de grammaires de graphes, nous considérons une ontologie comme un graphe hôte G possédant une relation de typage avec le graphe type (T G), où T G représente le méta-modèle de l\u0027ontologie. Pour être conforme aux standards, c\u0027est OWL qui a été retenu comme méta-modèle d\u0027ontologies. Ainsi, les types des noeuds considérés sont : N T \u003d {Class(C), P roperty(P ), ObjectP roperty(OP ), DataP roperty(DP ),-Individual(I), DataT ype(D), Restriction(R)}.\nLes types des arêtes correspondent aux axiomes utilisés pour relier les différentes entités :\nLes changements ontologiques sont formalisés par un ensemble de règles de réécriture :\nDans cette définition étendue, CHD correspond à l\u0027ensemble des changements dérivés ajoutés à un changement ontologique pour corriger ses éventuelles inconsistances. La Figure 1 montre une représentation et une application de la règle de réécriture du changement ontologique AddIndividual. Elle permet d\u0027ajouter un individu \"Pascal\" tout en spé-cifiant son type, la classe \"Person\". La règle assure, grâce au N AC, la non redondance de données, i.e. elle empêche l\u0027application du changement dans le cas où l\u0027individu existe déjà dans l\u0027ontologie. \nFormalisation des changements ontologiques\nCette section présente notre formalisation des changements ontologiques composés et complexes par les grammaires de graphes typés. Avant de détailler cette formalisation, une brève introduction des changements élémentaires est indispensable pour mieux comprendre le reste de l\u0027article. Ainsi, les changements élémentaires englobent les changements de renommage, l\u0027ajout et la suppression de certains concepts. Ils n\u0027affectent qu\u0027une seule entité ontologique bien qu\u0027ils dépendent d\u0027autres entités. Le Tableau 2 présente quelques changements élémen-taires adressés dans notre travail et les concepts dont ils sont dépendant. À noter que les N ACs des règles de réécriture sont déduites à partir de ces interdépendances. Un exemple du changement élémentaire AddIndividual est déjà présenté dans la section 4.1. \nChangements ontologiques composés\nLes changements ontologiques composés, appelés aussi composites, affectent une entité ontologique et ses voisins. Ils sont alors formés par plusieurs règles de réécriture : une règle présentant le changement souhaité par l\u0027utilisateur (changement principal) et les autres règles présentant les changements dérivés (CHD) ajoutés pour préserver la consistance de l\u0027ontologie. En effet, l\u0027ordre des règles de réécriture est primordial dans la plupart des changements. Le Tableau 3 présente l\u0027interdépendance entre ces changements organisés dans une matrice de changements. La valeur d\u0027un élément de matrice (i, j) indique que l\u0027application d\u0027un changement relié à une ligne i implique l\u0027application du changement de la colonne j. Ainsi, le changement ontologique RemoveCardinalityRestriction(C, OP ) permet de supprimer une CardinalityRestriction définie sur une classe C et une objectProperty OP . Il est formalisé par deux règles de réécriture. La première représente le changement dérivé RemoveAssertionObjectP roperty qui supprime toutes les assertions définies sur OP . La deuxième règle définit la règle de réécriture principale assurant la suppression de la restriction.\nLe changement ontologique RemoveObjectP roperty(OP ) supprime une objectProperty OP et toutes ses dépendances de l\u0027ontologie. La Figure 2 présente les six règles de réécriture définissant ce changement. Ainsi, les cinq premières règles décrivent les changements déri-vés (CHD) devant être appliqués pour préserver la consistance de l\u0027ontologie. La dernière règle présente la règle de réécriture principale. Ainsi, les restrictions définies sur la propriété OP doivent être supprimées en appliquant les règles suivantes : RemoveAllV aluesRestriction(OP ), RemoveSomeV aluesRestriction(OP ), RemoveHasV alueRestriction(OP ) et RemoveCardinalityRestriction(OP ). Toutes les ObjectP ropertyAssertion qui réfé-rencent l\u0027objectProperty OP doivent également être supprimées. \nChangements ontologiques complexes\nLe Tableau 4 présente l\u0027ensemble des changements complexes abordé dans ce travail et les changements dont ils sont composés.\nComme exemples de changements complexes, nous présentons les changements P ullU pClass, M ergeClass et SplitClass. Ainsi, le changement P ullU pClass(C, C p ) permet de monter une classe C dans sa hiérarchie de classes et l\u0027attacher aux parents de sa super-classe précédente C p . Ceci implique que la classe C n\u0027est plus la subClass de la classe C p et n\u0027infère plus ses propriétés. La Figure 3 présente la règle de réécriture définissant ce changement. Ainsi, le changement dérivé RemoveObjectP ropertyAssertion vérifie si la classe C possède des individus qui partagent une objectP ropertyAssertion sur les propriétés de la classe C p . Dans ce cas, toutes les assertions doivent être supprimées. Le changement RemoveDataP ropertyAssertion supprime toutes les dataP ropertyAssertion définies sur les individus de la classe C et les dataProperties liées à la classe C p .  Le changement M ergeClasses(C 1 , C 2 , C N ew ) fusionne deux classes C 1 et C 2 déjà existantes dans l\u0027ontologie en une nouvelle classe (C N ew ). Il nécessite l\u0027application des règles de réécriture AddClass(C N ew ), RemoveClass(C 1 ) et RemoveClass(C 2 ). Cependant, pour préserver la consistance de l\u0027ontologie, avant de supprimer C 1 et C 2 , toutes leurs propriétés et axiomes doivent être attachés à C N ew . Formellement : 1)\nAddSubClass(C N ew , C j ), 2) répéter le processus pour C 2 , 3) ?C i ? C(O) · C i ? C 1 appliquer AddEquivalentClasses(C i , C N ew ), 4) répéter le processus pour C 2 , etc.\nLe changement SplitClass(C, C N ew1 , C N ew2 ) divise une classe (C) déjà existante dans l\u0027ontologie en deux nouvelles classes C N ew1 et C N ew2 . Il nécessite alors l\u0027application des règles de réécriture AddClass(C N ew1 ), AddClass(C N ew2 ) et RemoveClass(C). Comme le changement M ergeClasses, le changement SplitClass nécessite, avant la suppression de la classe C, d\u0027attacher toutes ses propriétés et axiomes aux classes C N ew1 et C N ew2 .\nDiscussion : Le formalisme des grammaires de graphes offre une représentation simple des changements ontologiques. Le Tableau 5 montre deux exemples de changements AddObject-P roperty et P ullDownClass, représentés à la fois par le formalisme proposé et le travail de (Djedidi et Aufaure, 2010). Dans Djedidi et Aufaure (2010), ces changements sont considé-rés, respectivement, comme composés et complexes. Le premier changement est composé par trois changements élémentaires et le deuxième par deux. De plus, ils nécessitent, comme tous les autres changements ontologiques, l\u0027utilisation du raisonneur Pellet pour identifier d\u0027une manière a posteriori les inconsistances. Dans notre travail, ces changements sont considérés comme élémentaires puisqu\u0027ils ne sont composés que d\u0027une seule règle de réécriture. De plus, pour préserver la consistance de l\u0027ontologie, les inconsistances sont gérées d\u0027une manière a priori grâce à l\u0027utilisation des Negatives Applications Conditions (N AC).\nChangement ontologique (Djedidi et Aufaure, 2010) Formalisme proposé AddObjectP roperty(OP, C1, C2)\nLe changement est composé de trois changements élémen-taires : 1.\nAddObjectP roperty-(OP ), 2. AddDomain(OP, C1) 3. AddRange(OP, C2).\n-Le changement est formalisé par une seule règle de réécriture et évite la non-redondance de données. Le changement est composé par deux changements élémen-taires : 1. AddSubClass(C1, C2) 2.\nRemoveSubClass-(C1, Cp)avec la classe Cp est la super-classe de C1 et C2.\n-Le changement est formalisé par une seule règle de réécriture et évite la contradiction des axiomes. \nConclusion\nNous avons présenté dans cet article une nouvelle formalisation des changements ontologiques composés et complexes basée sur les grammaires de graphes typés et l\u0027approche algé-brique Simple Pushout (SPO) de transformation de graphes. L\u0027utilisation de l\u0027approche SPO offre plusieurs avantages. En particulier, elle permet de définir simplement et formellement les règles de réécriture correspondant aux changements ontologiques. Elle assure le contrôle des transformations de graphes en évitant les incohérences d\u0027une manière a priori. De plus, elle réduit le nombre de changements élémentaires nécessaires pour appliquer les changements complexes et composés. À noter que la formalisation des changements a été implémentée à l\u0027aide de l\u0027outil AGG (Attributed Graph Grammar) et testée sur des ontologies de taille ré-duite. En effet, l\u0027étape la plus coûteuse en temps et en ressources est la reconnaissance du graphe LHS à partir du graphe hôte. Vu que la plupart des changements possèdent des LHS de taille réduite, il s\u0027est avéré que le temps d\u0027exécution est assez limité. À titre d\u0027exemple, pour un graphe d\u0027une ontologie composé de 21 noeuds, l\u0027exécution du changement complexe SplitClass(C, C N ew1 , C N ew2 ) a pris seulement 700 millisecondes (avec un LHS composé de 37 noeuds). Pour mieux évaluer la performance de notre approche, nous travaillons actuellement sur l\u0027évaluation de l\u0027influence de la taille de LHS sur les ontologies de grande taille.\n"
  },
  {
    "id": "288",
    "text": "Introduction\nQue l\u0027on suggère à l\u0027utilisateur d\u0027accéder à une information, de s\u0027inscrire à une newsletter, de commenter un service, ou d\u0027acheter un produit, le Web Usage Mining est indispensable à l\u0027objectif d\u0027adaptabilité de l\u0027offre technologique. La propension d\u0027un utilisateur en ligne à réa-liser une action suggérée dépend en effet de sa réaction face aux modes de sollicitation et ses réactions sont, pour une part déterminante, déclenchées par son expérience en cours. L\u0027expé-rience utilisateur correspond \"aux réponses et aux perceptions d\u0027une personne qui résultent de l\u0027usage ou de l\u0027anticipation de l\u0027usage d\u0027un produit, d\u0027un service ou d\u0027un système\" 1 . Dès lors on comprend aisément l\u0027enjeu communicationnel de l\u0027analyse automatique de l\u0027expérience de l\u0027utilisateur. Le web 3.0 se réfléchit d\u0027ores et déjà dans une logique one to one avec l\u0027individualisation de la communication sur le web comme élément central.\nLes questions ouvertes, par exemple sur la manière d\u0027analyser cette expérience utilisateur, sont nécessairement interdisciplinaires. La première partie de cet article définit une méthode d\u0027analyse sémiotique pour y répondre. C\u0027est ensuite la spécification de cette méthode à travers la proposition de nouveaux descripteurs sémiotiques qui est développée. La dernière partie définit le mode d\u0027apprentissage automatique pertinent pour la détection de la propension d\u0027un individu à réaliser une action suggérée.\nDétermination des données\nContrairement à la majorité des études sur les profils utilisateur, notre recherche porte sur un utilisateur quelconque 2 exposé à n\u0027importe quelle sollicitation, ce qui détermine l\u0027utilisation des « attitudes implicites » comme indicateur de l\u0027expérience. Les \"jugements et attitudes implicites\" sont des indicateurs, issus de récentes études socio-cognitives (cf. Courbet et Fourquet-Courbet (2014)), particulièrement pertinents dans le cadre de notre recherche car définissent des actes non analysables par l\u0027utilisateur et parfois même non perçus, mais révéla-teurs de l\u0027expérience vécue. Nous n\u0027avons donc pas besoin de connaître au préalable l\u0027utilisateur qui ne saurait pas, à titre d\u0027exemple, identifier précisément si et/ou pourquoi il clique sur l\u0027image plutôt que sur le texte pour accéder à l\u0027information désirée.\nL\u0027exploitation des données implicites produites par l\u0027utilisateur permet d\u0027estimer ses attitudes implicites. Les données explicites archivées sont inexistantes pour un utilisateur quelconque et les données explicites liées au déclaratif sont considérées trop intrusives (cf. Oard et Kim (2001)). La sollicitation de l\u0027utilisateur, pour obtenir ces informations, constitue un frein avéré dans le processus de séduction à l\u0027oeuvre dans les techniques de communication.\nL\u0027analyse de ces données a pour objectif de déduire un comportement à partir des interactions de l\u0027utilisateur avec le système. Les interactions sont identifiées par : la durée de lecture (temps passé par l\u0027utilisateur sur un écran, soit le temps passé entre deux actions), le mouvement de souris, le nombre de clics de la souris, la durée de défilement de l\u0027ascenseur, le défilement avec souris, le nombre de clics sur l\u0027ascenseur, le défilement avec les touches du clavier, la sélection du texte (cf. Tchuente (2013)).\nSe pose alors la question de la méthode permettant de détecter des attitudes implicites à partir de l\u0027exploitation de données implicites. Pour y répondre, nous définissons une catégorie sémiotique d\u0027analyse du comportement prenant en considération la portée communicationnelle inconsciente des actions réalisées. Les catégories de comportement, définies dans l\u0027état de l\u0027art (cf. Oard et Kim (2001)), sont constituées d\u0027actants : examiner, référencer, retenir, annoter, créer. Nous cherchons à remplacer ces catégories d\u0027actants par des catégories d\u0027expérience de l\u0027en acte. Notre contribution se situe dans l\u0027utilisation d\u0027une catégorie sémiotique d\u0027analyse du comportement : le style perceptif. Tel que défini par Pignier (2012), le style perceptif est l\u0027expression d\u0027une esthésie, une manière de percevoir le monde, l\u0027autre et les choses, reliée à l\u0027exercice d\u0027une sensibilité. C\u0027est l\u0027expérience de l\u0027utilisateur que nous cherchons à définir, sa façon de communiquer implicitement en interagissant avec le système. Là où la catégorisation classique d\u0027un profil utilisateur s\u0027intéresse aux caractéristiques d\u0027un individu (genre, situation socio-démographique, centres d\u0027intérêt), nous nous intéressons à son hexis numérique, son être impliqué dans l\u0027espace du web et agissant en fonction de lui. Ainsi, le contexte est né-cessairement inclus dans le style perceptif. Qu\u0027il soit acteur adjuvant ou opposant, il influe sur l\u0027expérience de l\u0027utilisateur. Il s\u0027agit de s\u0027approcher du schéma de la communication interpersonnelle qui permet, dans la vie réelle, d\u0027adapter débit de parole, gestualité, tonalité à l\u0027interlocuteur, en fonction de l\u0027observation de ses réactions. La détection du style perceptif poursuit in fine cet objectif d\u0027adaptabilité de la réponse à l\u0027expérience du co-énonciateur à un moment donné.\nAfin de définir le style perceptif, nous proposons une structuration des données implicites de navigation. Le style perceptif se caractérisant par les expériences médiées par les interfaces, 2. L\u0027utilisateur quelconque représente l\u0027utilisateur non identifié versus utilisateur identifié.\nle web contient donc intrinsèquement les informations nécessaires à sa détection. La réflexion se pose au niveau de la production de métadonnées (des descripteurs) caractérisant les données implicites. Le niveau morphosyntaxique de l\u0027analyse sémantique (comme par exemple pour TypWeb dans Beaudouin et al. (2002)) est transféré au niveau sémiotique de l\u0027analyse. En effet, l\u0027exploration qualitative des sites web nécessite l\u0027utilisation de descripteurs contenant les possibilités d\u0027interaction et de contexte.\nNous proposons d\u0027intégrer ici, comme niveau d\u0027analyse, le sème connotatif, qui est défini comme étant un sème connotant l\u0027effet produit par l\u0027interaction entre l\u0027utilisateur et l\u0027élément. À titre d\u0027exemple, à la valeur sémantique de l\u0027élément \"pop-up\", nous associons les valeurs sé-miotiques intrusif et surgissant. Intrusif et surgissant connotent l\u0027effet produit par l\u0027interaction avec \"pop-up\". À la valeur sémantique \"sommaire sous forme de listes à faire défiler\", nous associons la valeur sémiotique cartographique. L\u0027effet cartographique renvoie à l\u0027expérience de la vue d\u0027ensemble. L\u0027ensemble des sèmes ainsi produit forme les descripteurs pour l\u0027annotation des sites web. Le niveau sème connotatif contient l\u0027expérience possible (soit l\u0027effet produit par l\u0027interaction) mais sa capacité à qualifier un style dépend du schéma actantiel (l\u0027action effective de l\u0027utilisateur). Le schéma actantiel de Greimas (1966)  Le niveau sème connotatif est donc pondéré par les actions suivantes sur les éléments : lecture ou visualisation (déterminée elle même par le temps passé), cliquer pour accéder, cliquer pour fermer, défiler, surligner, cliquer pour partager, inscrire, commenter, annoter. Le sème connotatif peut alors soit être positivé ou contrarié. Le sème connotatif intrusif relié à l\u0027apparition d\u0027une pop-up pour l\u0027inscription à une newsletter, sera positivé par l\u0027action \"inscrire\" (par exemple inscrire son mail), mais sera contrarié par l\u0027action \"cliquer pour fermer\". On définit alors la paire de sèmes suivante : intrusif et non intrusif, qui sera niée en dehors des actions contrariantes ou positivantes qui lui sont associées. Il s\u0027agit ensuite d\u0027envisager les structures qui se dessinent au niveau sémiotique. En prenant en considération la chronologie des interactions et leur interopérabilité -simultanéité, succession, opposition, exclusion -et en définissant la dernière action de l\u0027utilisateur comme finalité, le contexte sémiotique est ainsi défini. L\u0027interaction formulée dans la phrase suivante : « L\u0027utilisateur en ligne ferme quasi instantanément la pop-up newsletter puis fait défiler le sommaire dans son intégralité avant d\u0027accéder à la page information » est retranscrite dans le contexte d\u0027une annotation sémiotique par : « Un style perceptif non intrusif successivement cartographique pour aboutir à : accéder à la page désirée ». La création d\u0027une ontologie appliquée à la détection du style perceptif s\u0027avère ainsi nécessaire pour définir les règles et articulations d\u0027une annotation sémiotique. Celle-ci permet ainsi d\u0027exploiter les données implicites, pour faire émerger les attitudes implicites détermi-nantes de l\u0027expérience de l\u0027utilisateur, que nous caractérisons par son style perceptif.\nDescripteurs sémiotiques proposés\nL\u0027étude sémiotique présentée dans la section précédente nous a amené à définir des descripteurs sémiotiques que nous présentons dans ce qui suit. Pour rappel, une session de navigation consiste en une succession de L écrans visités (L étant variable d\u0027une session à une autre), en ne tenant compte que des écrans sur lesquels l\u0027utilisateur a passé un temps supérieur à un certain seuil, ainsi que ceux qu\u0027il ne subit pas par défaut au chargement de la page.\nIl s\u0027agit alors de caractériser le contenu sémiotique de ces écrans, en introduisant des descripteurs qui encodent les informations liées aux styles perceptifs. Comme nous l\u0027avons mentionné dans la section 2, ces informations se trouvent au niveau des éléments qui composent l\u0027écran, et avec lesquels l\u0027utilisateur interagit pendant sa navigation. A noter que cette notion d\u0027interaction est spécifique à chaque type d\u0027éléments : certains ne seront pris en compte dans l\u0027analyse que si l\u0027utilisateur les survole avec la souris, alors que pour d\u0027autres, l\u0027affichage par défaut sur l\u0027écran visité est suffisant.\nNous proposons ainsi d\u0027associer à chaque élément de l\u0027écran un certain nombre de sèmes qui le caractérisent. Un dictionnaire de N sèmes est donc défini (sèmes positifs et négatifs), et une annotation sémiotique des pages du site permet d\u0027attribuer à chaque élément des labels sémiotiques issus de ce dictionnaire. Cette annotation se fait manuellement sur un certain nombre de pages caractéristiques du site, et est ensuite généralisée de manière automatique aux autres pages en attribuant les mêmes labels sémiotiques aux éléments appartenant à une même catégorie, en se basant par exemple sur les attributs HTML. Les descripteurs sémiotiques caractérisant un écran E sont ensuite calculés à partir de ces vecteurs binaires, en mesurant la fréquence de chaque modalité du dictionnaire de sèmes :\nAfin d\u0027éviter que deux écrans ayant un contenu sémiotique similaire et appartenant à des pages sémantiquement éloignées ne soient représentés avec des indicateurs identiques, nous proposons de concaténer le vecteur desc sémiotique (E) avec un descripteur sémantique classique décrivant le contenu de la page à laquelle appartient l\u0027écran.\nPour ce faire, un dictionnaire de M labels sémantiques est ainsi défini, et les différentes pages du site sont annotées avec ces labels. De nombreuses méthodes d\u0027annotation automatique, issues notamment des domaines du Web sémantique et du Web Content Mining, existent dans l\u0027état de l\u0027art (Charrad et al. (2008) par exemple).\nAinsi, chaque écran d\u0027une page donnée sera représenté par un vecteur de description de dimension N × M , composé d\u0027un premier vecteur qui est propre à cet écran, et d\u0027un second qui est commun à tous les écrans d\u0027une même page. Cette représentation permet à la fois de lever la limitation décrite dans le paragraphe précédent, et d\u0027encoder indirectement la suite des pages visitées (comme dans une approche de Web Usage Mining classique) à travers les variations des vecteurs sémantiques à chaque fois que l\u0027utilisateur accède à une nouvelle page.\nTraitement des descripteurs\nNous nous intéressons dans cette section au choix d\u0027un modèle d\u0027apprentissage adapté au traitement des descripteurs introduits dans la section précédente. Nous avons opté pour un modèle neuronal, et ce principalement pour deux raisons : (i) l\u0027aspect temps-réel des applications visées, pour lequel les modèles neuronaux sont particulièrement adaptés de part le fait que la quasi-totalité de la complexité est reportée sur la phase d\u0027apprentissage, et (ii) l\u0027optimalité de ce types de modèles en terme de performances, qui a été démontrée dans de nombreuses études comparatives récentes (parmi lesquelles nous pouvons citer celle de Bengio et Delalleau (2011)).\nVue la nature séquentielle des données traitées (une session de navigation étant représentée par une séquence de vecteurs, de dimension N × M chacun, et de longueur variable), nous avons opté pour un modèle neuronal récurrent. Ce dernier (initialement introduit dans Williams et Zipser (1995) et ayant connu plusieurs évolutions depuis) est analogue à un Perceptron multi-couches classique, mais dans lequel des connexions récurrentes sont rajoutées au niveau des couches cachées (c\u0027est à dire les couches intermédiaires entre l\u0027entrée et la sortie). Ce modèle récurrent est entraîné par une version modifiée de l\u0027algorithme de rétro-propagation du gradient (dans laquelle les connexions récurrentes sont prises en compte), en ciblant les sorties désirées selon l\u0027application visée. Ces sorties peuvent correspondre par exemple à la réponse à une recommandation de produit sur un site de e-commerce, à la réaction à une sollicitation sur un site de collecte de dons, et plus généralement à n\u0027importe quel objectif mesurable lié au comportement de navigation de l\u0027utilisateur.\n"
  },
  {
    "id": "289",
    "text": "Introduction\nLes données massives, appelées communément \"big data\", impactent directement le processus ETL (Extracting-Transforming-Loading) vu que celui-ci est le premier composant du système décisionnel confronté à ces données. Peu de travaux ont traité sur la problématique des données massives dans le processus ETL. Liu et al. (2011) ont proposé une approche parallèle/distribuée appelée ETLMR consistant à améliorer les performances de la phase de transformation (T) et de chargement (L) de l\u0027ETL et ce en adoptant, pour chacune des deux phases, des stratégies de distribution appropriées. Les expérimentations de Misra et al. (2013) ont montré que le paradigme MapReduce est prometteur et que les solutions ETL basées sur des frameworks open source tel que Apache Hadoop sont plus performantes et moins couteuses par rapport aux solutions ETL commercialisées. Contrairement aux travaux de Liu et al. (2011), ceux de Misra et al. (2013) considèrent la phase d\u0027extraction (E) de l\u0027ETL très couteuse ; celle-ci a été traitée dans un environnement parallèle/distribué selon le paradigme MapReduce. (Liu et al., 2012) est une démonstration du prototype ETLMR. Dans (Liu et al., 2014), les auteurs proposent une plateforme CloudETL basée sur Apache Hadoop et Apache Hive où les performances ont été nettement améliorées par rapport à celles d\u0027ETLMR (Liu et al., 2011). Les plateformes ETLMR (Liu et al., 2011) et CloudETL (Liu et al., 2014) sont basées sur du code Python, et par conséquent celles-ci sont destinées aux informaticiens dévelop-peurs de solutions ETL parallèles/distribuées. Dans le but de vulgariser ce type de plateformes et les rendre accessibles aux utilisateurs finaux, nous proposons, dans ce papier, une plateforme baptisée P-ETL (Parallel-ETL) développée sous l\u0027environnement Apache Hadoop\n1\n. Le paramétrage d\u0027un processus se fait, de bout-en-bout, sur une interface unique structurée en trois onglets, chacun concerne une phase du processus (E, T, L). Le même paramétrage peut s\u0027effectuer dans un fichier XML pour un traitement en batch. Nous avons adapté le schéma classique de l\u0027ETL dans l\u0027environnement MapReduce. Ainsi, P-ETL procède en cinq phases : (E)xtracting, (P)artitioning, (T)ransforming, (R)educing et (L)oading ; au lieu d\u0027ETL, on parle plutôt d\u0027EPTRL.\nLes bases de P-ETL\nNous présentons dans cette section les bases et les principes fondamentaux de P-ETL en exposant les techniques de partitionnement supportées, l\u0027adaptation des phases Map et Reduce aux spécificités de l\u0027ETL et nous terminons par l\u0027architecture globale de P-ETL.\nPartitionnement des données\nDans le but de distribuer/paralléliser le processus ETL, les données sources doivent être elles aussi distribuées pour permettre à plusieurs tâches de s\u0027exécuter de façon parallèle où chacune traite sa propre partition de données. P-ETL offre trois types de partitionnement. Afin d\u0027assurer une charge plus ou moins équitable entre les différentes tâches parallèles, le choix du type de partitionnement est important. La présence d\u0027un taux élevé, dans une partition de données, de tuples avec des valeurs creuses implique une charge faible en termes de traitement pour la tâche. En effet, les tuples en question seront rejetés par un filtre tel que NOT NULL.\nSimple : étant donné un volume de données source v, le type de partitionnement simple génère des partitions égales selon l\u0027équation 1 où nb_part étant le nombre de partitions.\nRound Robin (RR) : Avec la technique Round Robin, l\u0027affectation d\u0027un tuple depuis le volume source v vers une partition de données p est basée sur l\u0027équation 2. rang (tuple) étant le rang du tuple dans le volume v et nb_part étant le nombre de partitions.\nRound Robin par Bloc (RRB) : Cette technique est similaire à Round Robin. Dans le but d\u0027accélérer le partitionnement, un bloc de tuples est affecté à la partition, plutôt qu\u0027une affectation tuple par tuple.\nLes mappers et les reducers\nDans la plateforme P-ETL, les primitives map() et reduce() ont été adaptées aux spécificités de l\u0027ETL. Le rôle assigné à un mapper est la normalisation des données (nettoyage, filtrage, conversion, ...). Le mapper traite chaque row dans un tunnel de transformations (T 1 , T 2 ...) où chaque T i est chargé d\u0027une opération particulière telles que le nettoyage, filtrage, projection, conversion et concaténation. La figure 1  \nArchitecture de P-ETL\nLa plateforme P-ETL est organisée en cinq modules : (E)xtracting, (P)artitioning, (T)ransforming, (R)educing et (L)oading (Figure 3). Après extraction (E), les données sources sont chargées dans le système de fichier distribué de Hadoop (HDFS). Ensuite, un partitionnement logique des données (P) s\u0027effectue selon le choix de l\u0027utilisateur final (Simple, RR, RRB). Les partitions des données ainsi générées seront soumises au processus MapReduce. Chaque mapper est en charge de transformer (T) les données de sa partition (nettoyage, filtrage, conversion, ...).\nLes fonctions de fusion et d\u0027agrégation des données sont différées pour être exécutées dans la phase Reduce (R). A ce niveau, les données deviennent pertinentes et peuvent alors être chargées (L) dans l\u0027entrepôt de données.\nFIG. 3 -Architecture de P-ETL.\nParamétrage d\u0027un processus dans P-ETL\nLe paramétrage d\u0027un processus se fait sur une interface unique organisée en trois onglets (Extract, Transform, Load) dédiés au processus lui-même (workflow), plus une partie \"Advanced Parameters\" réservée pour la configuration de l\u0027environnement parallèle/distribué de Apache Hadoop (Figure 4). Pour la configuration du processus, l\u0027utilisateur doit commencer par l\u0027onglet \"Extract\". Les paramètres disponibles sur les deux autres onglets \"Transform\" et \"Load\" dépendent du premier, principalement du format de la source, sa structure et son emplacement. En revanche, la partie \"Advanced Parameters\" peut être utilisée indépendamment des trois onglets.\nFIG. 4 -Interface de configuration P-ETL.\nDans l\u0027onglet \"Extract\", l\u0027utilisateur doit, en premier lieu, localiser les données sources. Le format pivot des données est le fichier csv qui est adopté par toutes les plateformes MapReduce vu sa légèreté (absence de méta-données). Afin d\u0027accélérer le chargement des données sources dans le système HDFS, l\u0027utilisateur pourra activer la compression de celles-ci. Ensuite, l\u0027utilisateur doit choisir un type de partitionnement des données sources (simple, Round Robin, Round Robin by block) ainsi que le nombre de partitions (égal au nombre de mappers). Enfin, il doit paramétrer le mode de lecture du mapreader à partir des partitions de données (Ligne par ligne, nombre de lignes, taille en KO). La figure 4 montre l\u0027onglet \"Transform\" qui fournit à l\u0027utilisateur une liste de fonctions de transformation et d\u0027agrégation. Chaque fonction insérée doit être paramétrée (entrées, conditions, expressions, ...). Le tunnel de transformation ainsi constitué s\u0027exécutera dans l\u0027ordre d\u0027insertion des fonctions. Enfin, l\u0027onglet \"Load\" permet la configuration du chargement des données préparées dans l\u0027entrepôt de données et comporte la destination des données (entrepôt de données, magasin de données, cube de données), la compression des données avant leur chargement dans le système HDFS ainsi que le caractère séparateur du fichier csv cible. Concernant le paramétrage de l\u0027environnement parallèle/distribué dans Apache Hadoop (taille d\u0027un bloc HDFS, nombre de noeuds impliqué dans le processus, taille mémoire réservée à un noeud et à une tâche, compression des résultats des mappers avant de les soumettre aux reducers, ....), celui-ci se fait dans la partie \"Advanced Parameters\".\nExpérimentation\nPour évaluer P-ETL, nous avons installé un cluster de 19 machines, chacune possède un processeur intel-Core TMi3-3220 CPU@3.30 GHZ x 4 processor, 4GO RAM et 500 GO d\u0027espace disque. Le réseau local (LAN) est un Ethernet 100 Mbps. Selon la configuration matérielle présentée ci-dessus, l\u0027environnement Hadoop permet d\u0027affecter, au maximum, deux tâches parallèles à un même noeud. Ainsi, deux tâches parallèles sont équivalentes à un noeud dans ce qui suit.\nDonnées de test :\nNous avons développé un programme qui génère des données synthé-tiques relatives aux renseignements des étudiants. Les expérimentations ont été réalisées sur des jeux de données allant de 244 * 10 6 à 7, 317 * 10 9 tuples. Nous exposons, dans ce qui suit, l\u0027expérimentation réalisée sur un fichier etudiant.csv de 7, 317 * 10 9 lignes où chacune a une taille de 44 octets. Ce fichier contient les attributs suivants : Matricule , Date d\u0027inscription, Cycle (Licence, Master, Doctorat), Spécialité, Bourse et Sport. Le processus ETL paramétré pour l\u0027expérimentation se présente en quatre fonctions. La première tâche est la projection qui consiste à exclure les attributs Bourse et Sport. La deuxième tâche du processus est une restriction qui filtre les tuples et rejette tous ceux présentant une valeur Null dans l\u0027un des attributs : Date d\u0027inscription, Cycle, et Spécialité. La troisième tâche est Year() qui extrait l\u0027année à partir de la date d\u0027inscription. Enfin, la quatrième tâche est une fonction d\u0027agrégation COUNT() qui compte le nombre d\u0027étudiants inscrits durant la même année, dans le même cycle et la même spécialité. Il est à noter que durant l\u0027exécution du processus, lorsque P-ETL rencontre une agrégation, comme COUNT() dans ce cas, celle-ci sera différée pour être exécutée dans la phase Reduce.\nRésultats : Comme le montre le tableau 1, l\u0027augmentation des tâches parallèles améliore de manière significative les performances du processus. Nous remarquons que le gain de temps entre 24 et 30 tâches est très intéressant (50 mn). En revanche, nous constatons une régression du gain entre 30 et 38 tâches (6 mn). Nous pourrons conclure qu\u0027au delà d\u0027un certain seuil en termes de tâches parallèles, l\u0027amélioration des performances des processus sous P-ETL ne devient plus significative. \nConclusion\nLe processus ETL est considéré aujourd\u0027hui comme étant le coeur du système décisionnel puisque toutes les données destinées pour l\u0027analyse transitent par celui-ci. Afin de faire face aux données massives, nous l\u0027avons adapté selon le paradigme MapReduce pour permettre son exécution dans un environnement parallèle et distribué. P-ETL est basé sur une interface de paramétrage conviviale afin de rendre la plateforme accessible aux utilisateurs finaux. Les résultats des expérimentations montrent une meilleure scalabilité de P-ETL face à des volumes de données importants lorsque la taille du cluster augmente.\n"
  },
  {
    "id": "291",
    "text": "Introduction\nDans le cadre de l\u0027édition de manuscrits anciens, le rôle de l\u0027éditeur consiste à reconstruire le plus fidèlement possible le manuscrit original à partir des différentes versions du texte disponible. Pour cela, l\u0027éditeur classe les différentes versions du texte afin d\u0027obtenir un arbre généalogique de cette filiation que l\u0027on nomme stemma codicum (cf. Fig1).\nFIG. 1 -Stemma de De Nuptiis Philologiae et Mercurii établi par Danuta Shanzer.\nLa reconstruction généalogique par un arbre suppose que chaque copiste n\u0027a utilisé qu\u0027un seul manuscrit pour réaliser son exemplaire (filiation unique). Malheureusement, il arrive qu\u0027un exemplaire ait été copié, non à partir d\u0027une seule source, mais sur plusieurs manuscrits existants. On parle alors de contamination ou de corruption.\nNous proposons dans cet article, de représenter une tradition contaminée à l\u0027aide une construction pyramidale basée sur la notion de manuscrits intermédiaires.\nUne des modélisations philologiques utilisées par les éditeurs pour reconstruire le stemma codicum est du à Don Quentin (1926). Elle permet de retrouver, à partir du corpus, des triplets de manuscrits dont l\u0027un est l\u0027intermédiaire des deux autres et que nous nommons T3M. Nous utilisons alors ces triplets T3M pour bâtir une pyramide généalogique.\nLes triplets T3M sont déterminés par un indice qui est nul. L\u0027expérience montre néanmoins que le nombre de triplets T3M obtenus est très faible. Dans des données réelles, la contamination, les erreurs de saisie, etc., empêchent de respecter strictement les conditions de Don Quentin. Nous allons donc être amené à relâcher les conditions strictes de Don Quentin pour créer des triplets T3M-souples où l\u0027on impose à l\u0027indice, non plus d\u0027être nul, mais d\u0027être infé-rieur à un seuil, seuil déterminé par l\u0027éditeur en fonction du corpus.\nPour construire une pyramide à partir des triplets, définissons quatre propriétés : -l\u0027intermédiarité permet à un ensemble de triplets T3M de respecter la transitivité ; -La couverture impose que tous les manuscrits appartiennent au moins à un triplet T3M afin de pouvoir les situer sur la pyramide finale. -La compatibilité consiste pour un ensemble de triplets T3M à être représentable sur une pyramide (cf. Defays (1979)  \nSummary\nIn this paper we present a new codicum stemma visualization method. Don Quentin\u0027s modeling is usec to classify the textual tradition. We supplement the genealogical editor\u0027s information of betweenness triplets obtained directly from the corpus. A pyramid depicting the family codicum stemma is then constructed on the basis of information obtained by the triplets\n"
  },
  {
    "id": "292",
    "text": "Introduction\nL\u0027olfaction, ou la capacité de percevoir des odeurs, est le résultat d\u0027un phénomène complexe : une molécule s\u0027associe à un récepteur de la cavité nasale, et provoque l\u0027émission d\u0027un signal transmis au cerveau qui fait ressentir l\u0027odeur associée [Sezille et Bensafi (2013)-Meierhenrich et al. (2005]. Si les phénomènes qui caractérisent les sens de l\u0027ouïe et de la vue sont bien connus, la perception olfactive n\u0027est, encore aujourd\u0027hui, toujours pas comprise dans sa globalité. Cependant, on dispose de nombreux atlas (comme celui d\u0027Arctander (1969)) qui renseignent les qualités perçues par l\u0027humain pour des milliers de molécules odorantes : des experts senteurs associent à des milliers de molécules odorantes des qualités d\u0027odeurs (fruité, boisé, huileux, etc : un vocabulaire bien défini et consensuel). On dispose également maintenant d\u0027outils capables de calculer des milliers de propriétés physico-chimiques de molécules 1 . Il a alors pu être montré que ces propriétés déterminent la (les) qualité(s) d\u0027une odeur perçue [Khan et al. (2007) -Kaeppler et Mueller (2013)]. Ce lien entre le monde physico-chimique et le monde du percept olfactif a été mis en évidence à l\u0027aide de méthodes d\u0027analyse en composantes principales démontrant, à partir de données, la corrélation existante entre ces deux mondes. Les neuroscientifiques ont donc maintenant besoin de méthodes descriptives afin de comprendre les liens entre propriétés physicochimiques et qualités.\nLa découverte de régularités (ou descriptions) qui distinguent un groupe d\u0027objets selon un label cible (souvent appelé label de classe), est un problème qui a fédéré diverses communautés en intelligence artificielle, fouille de données, apprentissage statistique, etc. En particulier, la découverte supervisée de règles descriptives de type description ?? label est étudiée sous divers formalismes : découverte de sous-groupes, fouille de motifs émergents, ensembles contrastés, hypothèses, etc. (Novak et al. (2009)). Dans tous les cas, nous faisons face à un ensemble d\u0027objets associés à des descriptions (dont l\u0027ensemble forme un ensemble partiellement ordonné), et ces objets sont liés à un ou plusieurs labels de classe.\nDans cet article, on s\u0027intéresse à la découverte de sous-groupes (subgroup discovery), introduite par Klösgen (1996) et Wrobel (1997. Étant donné un ensemble d\u0027objets décrits par un ensemble d\u0027attributs, et chacun associé à un (ou plusieurs) label(s) de classe, un sousgroupe est un sous-ensemble d\u0027objets statistiquement intéressant par sa taille et ses singularités au sein de l\u0027ensemble d\u0027objets initial vis à vis d\u0027un ou plusieurs labels cibles. En fait, il existe deux familles principales de méthodes. La première (Wrobel, 1997) vise à trouver des règles de type description ? label où le conséquent est un unique label. La seconde, la fouille de modèles exceptionnels (exceptional model mining, EMM) introduite par Leman et al. (2008), vise à trouver des sous-groupes dont la répartition d\u0027apparition de tous les labels diffèrent grandement dans le sous-groupe comparé à toute la population, i.e. de la forme description ? {(label 1 , valeur 1 ), ..., (label k , valeur k )} où k est le nombre de labels de l\u0027attribut cible. Dans les deux cas, on veut optimiser une mesure de qualité pour distinguer au mieux le sous-groupe en fonction du label, ou d\u0027une distribution des labels dans le sous-groupe (i.e. le modèle).\nEn olfaction cependant, une molécule est associée à une ou plusieurs qualités d\u0027odeurs : aucune des approches existantes ne permet de se focaliser sur un sous-ensemble de labels de cardinalité arbitraire. Effectivement, ces approches permettent soit de caractériser un seul label de classe par des sous-groupes, soit de trouver des sous-groupes qui caractérisent tous les labels de classes à la fois. Alors, d\u0027une part, un sous-groupe effectue une caractérisation trop locale, trop spécifique et d\u0027autre part la caractérisation est beaucoup trop globale.\nNous cherchons alors à découvrir des sous-groupes comme des règles descriptives de type description ? {label 1 , label 2 , ..., label l } où l \u003c\u003c k. Pour cela, nous proposons une nouvelle méthode appelée ElMM (Exceptional local Model Mining) qui généralise à la fois la méthode de sous-groupes classiques ainsi que EMM. Nous montrerons alors que les sousgroupes extraits sont plus caractéristiques de peu de qualités à la fois, et donc aussi plus faciles à interpréter par l\u0027expert en olfaction.\nLa suite de cet article est organisée comme suit. Tout d\u0027abord, nous introduisons les deux principales méthodes de découverte de sous-groupes en section 2 (subgroup discovery et exceptionnal model mining). Nous montrons alors les limites de ces deux types d\u0027approche avant d\u0027introduire en section 3 notre nouvelle méthode : la découverte de modèles exceptionnels locaux (exceptional local model mining). Un algorithme de découverte est présenté en section 4 et appliqué à des données issues des domaines de la neuroscience et de l\u0027olfaction (section 5). 2) présente ce jeu de données dans le cas où la fonction class n\u0027associe à chaque molécule qu\u0027un seul label de C -mono-qualité-(resp. un sous-ensemble de labels -multi-qualités-).\nUn sous-groupe peut être représenté formellement de manière duale soit en intension soit en extension, c\u0027est-à-dire, soit par une description dans un langage donné mettant en oeuvre des restrictions sur le domaine de valeurs des attributs, soit par l\u0027ensemble d\u0027objets qu\u0027il décrit. Il existe plusieurs langages de description possibles, basés sur différents types de connecteurs logiques (conjonctions, disjonctions, ou encore négations), dont certains sont très expressifs (voir par exemple Galbrun et Kimmig (2014)). Dans la suite nous utiliserons un langage basé uniquement sur des conjonctions.\nDéfinition 2 (Sous-groupe). On note d \u003d 1 , . . . , f |A| la description d\u0027un sous-groupe où chaque f i est une restriction sur le domaine de l\u0027attribut a i ? A (à un sous-ensemble du domaine de a i s\u0027il est nominal, ou à un intervalle s\u0027il est numérique). Chaque restriction peut être assimilée soit à un ensemble (dans le cas d\u0027une restriction sur un attribut nominal), soit à un intervalle dont les bornes appartiennent à Dom(a i ) (dans le cas d\u0027un attribut numérique).\nRelation d\u0027ordre partiel entre les sous-groupes. \nExemple (suite). On a d 1 \u003d W ? 151.28, 23 ? nAT avec pour support l\u0027ensemble des molécules {24, 48, 82, 1633} : la molécule 1 ne vérifie pas la restriction sur l\u0027attribut nAT alors que la molécule 60 ne vérifie pas celle sur M W . Pour plus de lisibilité, lorsque l\u0027on ne précise pas une restriction f i dans une description d cela signifie qu\u0027aucune restriction effective n\u0027est appliquée sur l\u0027attribut a i dans d. La description d 2 \u003d W ? 151.28, 23 ? nAT, 10 ? nC est une spécialisation de d 1 car d 2 comporte les mêmes restrictions que d 1 plus une restriction sur un autre attribut. Réciproquement, d 1 est une généralisation de d 2 .\nÉtant donné un jeu de données, il y a potentiellement 2 |O| sous-groupes, il est donc néces-saire de n\u0027en sélectionner qu\u0027une partie en fonction de leur intérêt. Pour cela, les différentes approches de l\u0027état de l\u0027art utilisent une mesure de qualité qui évalue la singularité du groupe au sein de la population par rapport à une cible, c\u0027est-à-dire l\u0027attribut de classe. La mesure de qualité est choisie en fonction du type de données, mais aussi en fonction de l\u0027attribut de classe et de la finalité de l\u0027application. Il existe deux approches pour la découverte de sous-groupes : l\u0027approche que l\u0027on va définir comme classique (Wrobel, 1997), et l\u0027approche d\u0027Exceptional Model Mining (EMM) introduite par Leman et al. (2008).\nDans la première, chaque objet n\u0027est associé qu\u0027à un et un seul label de l\u0027attribut de classe, c\u0027est-à-dire ?o ? O, class(o) \u003d c avec c ? Dom(C), et la mesure de qualité permet de mettre en évidence la singularité d\u0027un sous-groupe relativement à un seul label de C. Pour un sous-groupe de description d, une mesure généralement utilisée relativement au label l est :\nles proportions d\u0027objets du sous-groupe et du jeu de données entier possédant la classe l. Cette mesure est une généralisation de la mesure WRAcc (? \u003d 1) qui prend en compte à la fois la taille du sous-groupe et aussi sa singularité dont le rapport entre les deux est pondéré par un facteur ?.\nDans le cas d\u0027EMM, un objet est associé à un sous-ensemble de labels de classe, c\u0027est-à-dire, ?o ? O, class(o) ? Dom(C). La mesure de qualité utilisée dans ce cas permet de mettre en évidence la singularité d\u0027un sous-groupe relativement à tous les labels de C à la fois. Une mesure possible est la somme des divergences de Kullback-Leibler pour tous les labels de classe entre les objets du sous-groupe et ceux du jeu de données entier :\nExemple (suite). Avec la description d 1 \u003d W ? 151.28, 23 ? nAT dans la Table 1, en utilisant la mesure de l\u0027équation (1) avec\n(1/2?1/3) \u003d 2/3. Dans la Table 2, en utilisant la mesure de la formule de l\u0027équation (2), on a W KL(d 1 ) \u003d 4/6 × ((2/4 log 2 3/4) + (3/4 log 2 3/2) + (3/4 log 2 3/2)) \u003d 0.45.\nDécouverte de sous-groupes, limites et problème. Étant donnés D(O, A, C, class), minSupp, ? et k l\u0027objectif est de récupérer l\u0027ensemble des k-meilleurs sous-groupes au regard de la mesure de qualité ? choisie où la taille du support du sous-groupe est supérieure ou égale à minSupp. Pour notre domaine d\u0027application de l\u0027olfaction, les approches existantes (décou-verte de sous-groupes classique et EMM) ne permettent pas de répondre à la problématique posée, à savoir la caractérisation de sous-ensemble de qualités d\u0027odeurs. Effectivement, ces approches permettent soit de caractériser un seul label de classe, c\u0027est-à-dire une qualité olfactive, par sous-groupe, soit de trouver des sous-groupes qui caractérisent tous les labels de classes à la fois avec EMM. D\u0027une part un sous-groupe effectue une caractérisation trop locale et spécifique, d\u0027autre part la caractérisation est trop globale. Nous introduisons dans la suite une nouvelle méthode qui généralise ces deux approches en permettant de caractériser par un sous-groupe un sous-ensemble L de taille quelconque de labels de classe.\nDécouverte de modèles exceptionnels locaux : ElMM\nSoit D(O, A, C, class) un jeu de données conforme à la Définition 1, avec Dom(C) \u003d {l 1 , . . . , l k }. Étant donnée une mesure de qualité ?, notre méthode ElMM recherche des sousgroupes de la forme (d, L) où d est la description d\u0027un sous-groupe et L ? Dom(C) est un sous-ensemble de labels de la classe C à caractériser. Cette méthode correspond au cas général de la découverte de sous-groupes. Effectivement, si on fixe pour tous les sous-groupes que L ? Dom(C) on se ramène au cas de la découverte de sous-groupes classique dans lequel un sous-groupe ne caractérise qu\u0027un label de classe. De plus si L \u003d Dom(C) alors on bascule dans le cas d\u0027EMM où chaque sous-groupe doit caractériser tous les labels de classe à la fois. ElMM permet donc de caractériser des sous-ensembles de labels de classe par des sous-groupes appelés sous-groupes locaux. \nLa contrainte (i) permet de ne considérer que les sous-groupes dont le support est supérieur ou égal à un seuil minSupp, évitant ainsi d\u0027obtenir des sous-groupes de trop petite taille qui n\u0027auraient alors aucun intérêt et facilitant l\u0027exploration. La contrainte (ii) permet d\u0027interagir sur le langage de la description en restreignant le nombre maximal de restrictions effectives par description à un seuil maxDesc (|d| est le nombre de restrictions effectives de d). De manière similaire, la contrainte (iii) permet de limiter le nombre de labels à discriminer dans L.\nMesure de qualité. La mesure de qualité utilisée dans EMM dont la formule a été donnée dans l\u0027équation 2 peut être généralisée pour ElMM pour ne considérer qu\u0027un sous-ensemble L ? Dom(C) de labels de classe et non plus l\u0027ensemble complet de labels à la fois :\nCependant, cette mesure de qualité ne correspond pas à l\u0027objectif de notre contexte applicatif car elle ne quantifie pas les labels de L ensemble, c\u0027est-à-dire de manière conjointe, lorsqu\u0027ils sont associés conjointement aux objets. Nous cherchons à caractériser l\u0027ensemble des objets cohérents qui possèdent tous les labels de L, et non pas un sous-ensemble de L. Pour cela, nous nous sommes tournés vers une mesure de qualité usuellement utilisée en classification supervisée : la F 1 -Mesure. Cette mesure nous permet dans notre cas de quantifier la pureté d\u0027un sousgroupe vis à vis des labels à caractériser L, i.e. les objets du support de la description du sousgroupe doivent être le plus possible associés à L (la précision) et les objets associés à L dans D doivent être au maximum inclus dans le support du sous-groupe (le rappel). La F 1 -Mesure se base sur le rappel et la précision d\u0027un sous-groupe vis à vis du sous-ensemble L à caractériser. Pour un sous-groupe local (d, L), on note :\nOn remarque alors que la F 1 -Mesure est comprise entre 0 et 1 puisque la précision et le rappel sont compris aussi entre 0 et 1. Plus la valeur de F 1 (d, L) est proche de 1 plus le sous-groupe (d, L) caractérise spécifiquement le sous-ensemble de labels L.\nExemple. Afin d\u0027illustrer la méthode ElMM, nous reprenons l\u0027exemple de la Table 2. Soit le sous groupe (d, L) avec d 1 \u003d W ? 151.28, 23 ? nAT et L \u003d {M iel, V anillé} le sous-ensemble de labels de classe à caractériser, en appliquant la formule de l\u0027équation 3 afin de calculer la mesure de qualité par la F 1 -Mesure on trouve : \nDécouverte de sous-groupes locaux avec ELMMUT\nDans cette section nous présentons l\u0027algorithme ELMMUT qui répond au problème d\u0027Exceptional local Model Mining (ElMM). Tout d\u0027abord, nous caractérisons l\u0027espace de recherche des sous-groupes. Ensuite, nous décrivons la manière de parcourir cet espace pour produire les sous-groupes en illustrant le pseudo-code de ELMMUT.\nEspace de recherche. L\u0027espace de recherche correspond à l\u0027ensemble de tous les sous-groupes locaux, partiellement ordonnés. Un sous-groupe local\nAinsi, l\u0027espace de recherche correspond à un treillis dans lequel chaque sous-groupe local est un noeud et le lien entre deux noeuds dénote que le noeud de niveau i+1 est une spécialisation du noeud de niveau i par ajout d\u0027une nouvelle classe à caractériser, ou par spécialisation d\u0027une restriction de la description. L\u0027élément le plus général du treillis correspond au sous-groupe local vide que l\u0027on note ( ?) en omettant les f i car aucune restriction n\u0027est effectuée pour tout attribut a i :\nParcours heuristique de l\u0027espace de recherche. L\u0027algorithme ELMMUT effectue un parcours en profondeur de l\u0027arbre de recherche en partant du plus général (le sous-groupe local vide à la racine de l\u0027arbre) vers le plus spécifique. Le principe algorithmique est donné dans l\u0027Algorithme 1. Pour chaque sous-groupe d\u0027un noeud de l\u0027arbre de recherche, ELMMUT essaie de le spécialiser par une extension de description ou de labels tant que la mesure de qualité est améliorée (fonction Spécialiser) (Galbrun et Kimmig, 2014). Cependant, il existe dans le pire des cas |Dom(C)| + (|A| × n(n + 1)/2) possibilités pour spécialiser un sous-groupe, puisque on peut effectuer jusqu\u0027à |Dom(C)| extensions de labels et |A| extensions de description pour lesquelles on peut construire n(n + 1)/2 intervalles possibles (si l\u0027attribut possède n valeurs différentes). Afin de pallier à ce problème d\u0027espace de recherche, nous nous sommes tournés vers une approche heuristique utilisée dans la découverte de sous-groupes et dans la fouille de redescriptions, il s\u0027agit d\u0027une approche de type \"beam-search\" (recherche par faisceau) (Lowerre, 1976). Cette approche permet d\u0027explorer seulement une partie des branches de l\u0027arbre de recherche : à chaque spécialisation, seulement une partie des possibilités (au maximum beamW idth) de spécialisation du sous-groupe va être analysée (cf. ligne 11 de Spécialiser). Optimisation des intervalles à la volée. Pour les attributs numériques, une simple discrétisa-tion en prétraitement n\u0027est pas suffisante. Cette approche est cependant utilisée dans une partie des expérimentations afin de pouvoir se comparer équitablement à l\u0027algorithme de référence pour EMM (DSSD Diverse Subgroup Set Discovery introduit par van Leeuwen et Knobbe (2012)), qui utilise une telle discrétisation. Afin d\u0027obtenir des résultats les meilleurs possibles, le choix des bornes de l\u0027intervalle pour un attribut a ? A doit se faire à la volée pour tenir compte des spécificités d\u0027un sous-groupe particulier. Le choix des bornes de l\u0027intervalle est alors déterminant. Tester toutes les possibilités d\u0027intervalles n\u0027est pas envisageable car cette méthode peut s\u0027avérer beaucoup trop gourmande en ressources (complexité théorique d\u0027ordre n 2 pour n valeurs différentes). Afin de pallier ce problème, nous avons adopté une méthode de discrétisation proche de celle de Fayyad et Irani (1993), introduite dans la découverte de sous-groupes par Grosskreutz et Rüping (2009). La Figure 1 présente la répartition des valeurs prises par les objets du support d\u0027un sous-groupe local (d, L) pour l\u0027attribut a. Pour optimiser la mesure il faut éliminer du support du sous-groupe un maximum d\u0027objets qui ne sont pas associés au sous-ensemble de labels L à caractériser. Soit S \u003d (d, L) un sous-groupe, et a ? A un attribut à partir duquel on veut étendre d, on note {p 1 , . . . , p |a| } l\u0027ensemble ordonné (p 1 \u003c p 2 \u003c · · · \u003c p |a| ) des |a| ? |supp(S)| valeurs différentes prises par l\u0027ensemble des objets de S pour l\u0027attribut a. On dit alors qu\u0027une valeur p i des valeurs prises par a est prometteuse si le nombre d\u0027objets de supp(S) associés à L possédant la valeur p i pour a est supérieur ou égal au nombre d\u0027objets de supp(S) non associés à L possédant la valeur p i . Sinon, on dit qu\u0027elle est non-prometteuse. Ainsi, une valeur p i de a correspond à une borne inférieure potentielle si p i est prometteuse et p i?1 est non-prometteuse. De plus une valeur p i de a correspond à une borne supérieure potentielle si p i est prometteuse et p i+1 est non-prometteuse. Ensuite il suffit de tester tous les intervalles possibles en prenant tous les couples (borne inférieure potentielle, borne supérieure potentielle) et de choisir le meilleur.\nExpérimentations\nJeu de données\nNous disposons d\u0027un atlas Arctander (1969), première base de données olfactive établie, qui sert de référence pour les neuroscientifiques. Il met en oeuvre 1 689 molécules différentes décrites par 1 704 propriétés physicochimiques numériques (leur volume, leur poids, le nombre d\u0027atomes de carbone qu\u0027elles contiennent, etc...) et sont associées à leur(s) qualité(s) olfactive(s) évaluée(s) par des experts. Les possibles discussions quant à l\u0027obtention de cet atlas, et notamment pour les qualités olfactives, ne sont pas abordées dans ce papier comme il s\u0027agit d\u0027un problème traité par les neuroscientifiques en amont. L\u0027atlas étant clairement multi-labels, on associe chaque molécule à un sous-ensemble de qualités olfactives. En moyenne, chaque molécule est associée à 2.88 qualités olfactives.\nA partir de cet atlas, nous avons construit deux jeux de données différents. Dans le premier jeu de données D 1 , on ne considère que 43 attributs (propriétés physicochimiques) de l\u0027atlas Arctender, alors que dans le second jeu de données D 2 on en considère 243. La sélection des 43 attributs de D 1 a été faite sur recommandation de l\u0027expert qui assure que ces attributs doivent être déterminants pour la caractérisation de qualités d\u0027odeurs. Les 243 attributs du jeu de données D 2 ont quant à eux été sélectionnés par une analyse de non-corrélation des attributs.\nRésultats quantitatifs\nTout d\u0027abord afin de juger de la performance de l\u0027approche que nous avons mise en place, considérons l\u0027aspect quantitatif des résultats sur le jeu de données d\u0027olfaction. Nous avons exécuté les expérimentations sur une machine avec 8Go de RAM et un processeur cadencé à 3.10GHz. Les résultats quantitatifs obtenus ont été réalisés sur les deux jeux de données Algorithm 1 ELMMUT.\nEntrée : O, A, C, class, ?, k, beamW idth, minSupp, maxDescr, maxLab Sortie : L\u0027ensemble des sous-groupes locaux R 1: R ? ? 2: for all c ? Dom(C) do 3:\nVérifier et Mettre à jour les contraintes pour ( {c}) 5:\nfor all a ? A do 6:\nf ? Choisir restriction sur a 7:\nVérifier et Mettre à jour les contraintes pour ( {c}) 8:\nAjouter ( {c}) à T emp 9:\nend for 10:\nT emp ? Conserver les k-meilleurs sous-groupes locaux de T emp 11:\nfor all (d, L) ? T emp do 12:\nAjouter Spécialiser(d, L) à R 13:\nend for 14: end for\nAjouter (d, L ? {c}) à T emp 5: end for 6: for all a ? liste des attributs candidats de (d, L) : A Cand do 7:\nf ? Choisir restriction sur a 8:\nVérifier et Mettre à jour les contraintes pour\nAjouter (d ? {f }, L) à T emp 10: end for 11: T emp ? Conserver les beamW idth meilleurs sous-groupes locaux de T emp 12: for all (d, L) ? T emp do 13: Figure 2 présente les différents temps d\u0027exécution de notre approche pour la version de l\u0027algorithme sans la discrétisation à la volée pour les attributs (une discrétisation par effectifs égaux est réalisée a priori pour chaque attribut numérique, comme cela est fait par la méthode EMM). Les trois courbes sont relatives au jeu de données D 1 . On remarque que plus on augmente la taille maximale autorisée pour la description (maxDescr) ou pour le sous-ensemble de labels à caractériser (maxLab), plus le temps d\u0027exécution est long ce qui est tout à fait compréhensible puisque l\u0027algorithme cherche à étendre le plus possible les sousgroupes tant que la mesure de qualité est améliorée. On remarque cependant qu\u0027à partir de maxDescr \u003d 15 le temps d\u0027exécution est sensiblement semblable ce qui signifie que même si la taille maximale autorisée augmente les sous-groupes ont une description dont la taille ne va pas au-delà d\u0027un certain seuil : la mesure ne peut plus être améliorée en les étendant. Ce résultat semble être causé à la fois par le paramètre minSupp et par le jeu de données. Effectivement, plus les descriptions sont étendues plus le support a une taille qui tend à diminuer, et puisque l\u0027algorithme est déterministe, avec ce jeu de données, on ne peut excéder une description de taille 15. De même pour la taille maximale du sous-ensemble de labels à caractériser, à partir de 2 ou 3 le temps d\u0027exécution reste le même, ce qui concorde avec le fait qu\u0027en moyenne une molécule est associée à 2.88 qualités olfactives (au-delà de n \u003e 3 qualités olfactives, le nombre de molécules partageant l\u0027ensemble de ces mêmes n qualités olfactives est trop faible et la contrainte du support minimale n\u0027est pas respectée). La Figure 3 présente l\u0027impact du jeu de données et de la discrétisation à la volée via notre technique. Clairement, le nombre d\u0027attributs est un facteur crucial pour l\u0027algorithme ELMMUT, on observe la présence d\u0027un facteur 10 entre le temps d\u0027exécution sur D 1 avec 43 attributs et celui sur D 2 avec 243. L\u0027utilisation de la discrétisation à la volée ne semble pas passer à l\u0027échelle lorsque l\u0027on augmente la taille des descriptions : à partir d\u0027une valeur de 15 pour maxDescr l\u0027exécution dure plus de 12 heures et a donc été avortée. Nous prévoyons des techniques d\u0027optimisation dans le futur.\nRésultats qualitatifs\nL\u0027interprétation des résultats est un point central dans le cadre de notre application. Les règles descriptives que nous avons mises en place doivent être capables d\u0027informer et d\u0027aiguiller les neuroscientifiques dans leur recherche. Notre approche, ElMM, en ne caractérisant qu\u0027un sous-ensemble de labels de classe permet alors de correspondre au cas pratique à savoir qu\u0027une molécule ne possède en moyenne que 2.88 qualités olfactives. En observant la Figure 4 qui présente la distribution des qualités au sein du jeu de données entier et d\u0027un sous-groupe obtenu par la méthode EMM, on s\u0027aperçoit clairement que l\u0027interprétation d\u0027un tel résultat est très difficile. On constate des différences entre les distributions du sous-groupe et du jeu de données initial mais cette différence est présente sur beaucoup trop de qualités olfactives à la fois et ainsi l\u0027interprétation d\u0027un tel résultat pour la déduction d\u0027une règle descriptive est infaisable pour un neuroscientifique. La Table 3 présente les 5 meilleurs sous-groupes (du point de vue de la mesure F 1 ) obtenus après suppression des motifs redondants (on utilise ici la même méthode que Galbrun et Kimmig (2014)). Ces sous-groupes sont issus de la base de données D 1 lorsque la discrétisation à la volée est activée avec maxDescr \u003d 10, maxLab \u003d 2 et minSupp \u003d 30. Seulement un sous-groupe caractérisant plusieurs labels de classe (Floral et Balsamique) est présent, avec une mesure de 0.33 et un support de 38. Sa description contient 9 restrictions. Des sous-groupes ont aussi des descriptions plus courtes. La taille des supports est variable. De plus, dans le jeu de données D 2 , lorsque la discrétisation à la volée est désactivée et que maxDescr \u003d 15, maxLab \u003d 3 et minSupp \u003d 30, on obtient 74.6% de sous-groupes dont le sous-ensemble de labels est de taille 1, 22.9% de taille 2 et 2.5% de taille 3. \nConclusion\nNous avons présenté la découverte de motifs exceptionnels locaux, une nouvelle méthode de fouille de règles descriptives qui généralise les approches existantes, pour caractériser spé-cifiquement un sous-ensemble de labels de classe. Nous l\u0027avons appliquée au cas concret de l\u0027olfaction afin de mettre en évidence les liens existant entre les propriétés physicochimiques d\u0027une molécule et ses qualités olfactives. Le pouvoir d\u0027interprétation des résultats et l\u0027information qu\u0027ils véhiculent, permettent d\u0027entrevoir une évolution de la connaissance à propos du phénomène complexe qu\u0027est l\u0027olfaction. De nombreuses expérimentations restent à faire et nous envisageons une exploration interactive inspirée par Galbrun et Miettinen (2012).\nRéférences Arctander, S. (1969). Perfume and flavor chemicals :(aroma chemicals), Volume 2. Allured Publishing Corporation.\n"
  }
]