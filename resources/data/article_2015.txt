Introduction
Un système de recherche d'information (SRI) est un module logiciel qui sélectionne, à partir d'une collection de documents, une liste de documents potentiellement pertinents en réponse à une requête utilisateur. Le processus suivi par un SRI est composé de 3 étapes.
Indexation. Cette étape permet de passer d'un document textuel à un document qui peut être utilisé dans la RI. Elle se base sur l'extraction des mots les plus importants des textes. Lors de cette étape, les mots vides tels que le, la, les sont généralement éliminés ; les termes sont ensuite racinisés, c'est-à-dire que des règles de transformation sur les termes sont appliquées afin d'obtenir un radical, limitant les variantes des termes à une forme unique ; enfin une pondération reflète l'importance des différents radicaux obtenus. Dans un cadre non dynamique, l'indexation est réalisée sur l'ensemble des documents, avant toute recherche.
Calcul des scores de pertinence des documents. Lorsqu'une requête est soumise au système, des scores de pertinence sont attribués aux termes qui la composent, en tenant compte de leur présence dans les documents. Ces scores sont ensuite combinés pour calculer le score global de chacun des documents de la collection. Il existe de nombreux modèles de pondéra-tion. La plupart sont basés sur les facteurs T F et IDF . L'expression T F (Term Frequency) correspond à la fréquence du terme dans le document, tandis que l'IDF (Inverse Document Frequency) désigne la fréquence inverse du terme dans le document, inversement proportionnel au nombre de documents qui contiennent le terme.
Reformulation de la requête. Cette étape permet de créer une requête plus adéquate à la RI que celle initialement formulée par l'utilisateur. Le principe de la reformulation automatique est de modifier la requête de l'utilisateur en ajoutant des termes significatifs ou en ré-estimant leurs poids. Dans sa version automatique, il s'agit de considérer les premiers documents restitués comme pertinents et d'ajouter des termes issus de ces documents ; de nouveaux poids sont également estimés et les scores des documents recalculés pour fournir la réponse finale du système. Ce paramètre n'est pas étudié dans le travail présenté dans cet article.
Chacune de ces étapes fait intervenir différents paramètres : par exemple lors de l'indexation, il est possible de choisir entre différents outils de racinisation, lors du calcul des scores de pertinence des documents, différents modèles de pondération peuvent être choisis. Les différents paramètres étudiés dans cet article sont présentés en figure 1.
L'efficacité d'un SRI est évaluée en calculant des mesures de performance comme le rappel, la précision et d'autres mesures associées. Depuis ses débuts, le domaine de la RI est très actif pour fournir de nouvelles propositions correspondant à une évolution de ces trois étapes. Lorsqu'un nouveau modèle de RI est proposé, ses paramètres sont étudiés, mais sans considé-rer les effets croisés. Par exemple dans Ponte et Croft (1998), ce sont les paramètres du modèle lui-même qui sont étudiés sans regarder l'influence du choix de l'algorithme de racinisation. Les modèles d'apprentissage d'ordonnancement (Learning to rank en anglais) considèrent de nombreux paramètres tels que les fréquences TF et IDF, la taille des documents et des caractéristiques comme les scores BM25, LMIR, PageRank des documents (Qin et al., 2010). Ces approches ont pour objectif d'optimiser l'ordonnancement des documents mais ne cherchent pas à connaître l'impact des paramètres. Quelques travaux visent à sélectionner les variables importantes, donc à étudier leur influence (Laporte et al., 2014;Naini et Altingovde, 2014).
Ainsi, généralement, les paramètres sont étudiés de façon indépendante, sans considérer les effets croisés des paramètres. Compte tenu du nombre de paramètres, l'étude des effets croisés est difficile et implique au préalable de collecter des données suffisantes pour le faire. Cet article s'attaque à ce problème. Ainsi, dans cette étude, nous nous appuyons sur un ensemble de données massif (2 millions de configurations) dans lequel les différents paramètres varient.
La littérature du domaine ne s'est que peu intéressée à une analyse de cette nature. Presque tous les articles et les thèses du domaine de la RI rapportent des études montrant la variation de mesures de performance en fonction d'un ou plusieurs paramètres, mais il ne s'agit pas d'une analyse en parallèle de paramètres variés. Quelques travaux se sont cependant intéressés à utiliser les méthodes d'analyse pour étudier les résultats de moteurs de recherche sur un ensemble de requêtes. Banks et al. (1999)   Compaoré et al. (2011) présentent une étude qui a les mêmes objectifs que ceux de ce papier. Cependant, le nombre d'éléments analysés et donc les combinaisons de paramètres est bien moindre. Dans leurs études, les auteurs montrent que les paramètres qui ont le plus d'influence sont différents en fonction que l'on considère les besoins d'information faciles ou difficiles. Bigot et al. (2014) utilisent les résultats d'une analyse pour sélectionner la configuration de système la plus adaptée en fonction de la difficulté du besoin d'information. L'objectif de l'analyse que nous présentons dans le présent papier est d'étudier à grande échelle les caractéristiques des SRI dans le but de déterminer les meilleures combinaisons de paramètres selon certaines mesures de performance. Ce travail a été mené dans le cadre du projet ANR CAAS (Contextual Analysis and Adaptive Search) ANR-10-CORD-001-01.
La suite de cet article est structurée comme suit. La section 2 présente la méthode utilisée pour obtenir les données ainsi que les données elles-mêmes. La section 3 présente l'analyse de la dépendance entre les paramètres et leur influence mutuelle. La section 4 s'attache à déterminer quelles sont les valeurs de paramètres les plus susceptibles de conduire à de bonnes performances du moteur de recherche correspondant. La section 5 conclut cet article.
Variantes de moteurs de recherche et paramétrage
Les données ont été générées via l'interface RunGeneration présentée dans Louédec et Mothe (2013) et qui est une sur-couche à la plateforme Terrier.
Terrier et RunGeneration
La plateforme de RI TERRIER (Ounis et al., 2006) possède de nombreuses possibilités de paramétrage, tant au niveau de l'indexation (indexation par blocs de différentes tailles, choix de la racinisation, etc.) que de la recherche (différents modèles de pondération pour la mise en correspondance entre la requête et les documents, différentes normalisations des poids) et de la reformulation de requêtes. Une fois paramétré, Terrier permet, pour un besoin d'information ou un ensemble de besoins, de retrouver les documents susceptibles de répondre à ce besoin.
L'interface RunGeneration a comme objectif de faciliter le paramétrage d'une chaine de traitement sous Terrier. Une fois les paramètres sélectionnés au travers de l'interface, celle-ci crée le fichier "terrier.properties" indispensable à Terrier et contenant l'ensemble des paramètres. Le second objectif de l'interface RunGeneration est de permettre de lancer plusieurs combinaisons de paramètres simultanément, ce qu'il n'est pas possible de faire en utilisant la plateforme Terrier. Ainsi plusieurs indexations et recherches sont effectuées sur les mêmes données via une seule intervention de l'utilisateur. Celui-ci peut par exemple demander en une
FIG. 1 -Paramètres de la génération des données.
action plusieurs indexations de documents avec des paramètres différents. L'interface a été développée pour fonctionner sur les versions 3.0 et 3.5 de Terrier 1 (Louédec et Mothe, 2013). Ainsi, lors de la génération des données utiles à notre analyse, le principe est le suivant : pour chaque combinaison de paramètres, une liste de documents retrouvés en réponse au besoin d'information est constituée. Cette réponse du système est alors évaluée sur la base de mesures de RI. Ainsi, pour une combinaison de paramètres, nous connaissons la valeur de chacune des caractéristiques correspondant aux paramètres du moteur et aux mesures de performance.
Paramètres utilisés lors de la génération de données
La figure 1 indique les variables utilisées lors de la génération des données ainsi que les modalités de ces variables paramètres. Le nombre de combinaisons obtenues est de 2 263 800. Les variables correspondant à des paramètres du système sont qualitatives.
Collection d'évaluation utilisée
Compte tenu du nombre de combinaisons et des temps nécessaires pour générer les données, dans cette étude, nous n'avons utilisé qu'une seule collection de documents : la collection TREC-8 de la tâche adhoc de TREC 2 . Elle comprend environ 530 000 documents soit 2 Go ; chaque document est composé en moyenne de 532 mots. La collection comprend également 50 besoins d'information et les jugements de pertinence des documents associés à ces besoins. Sur ce jeu de données, nous n'avons pas pris en compte les paramètres de reformulation de requêtes afin de ne pas rendre le nombre de combinaisons possibles trop grand pour être généré. Plutôt nous avons fait l'hypothèse qu'une première analyse permettrait de faire ressortir les paramètres principaux qui eux pourront être combinés avec les paramètres de reformulation. En effet, les principes de reformulation (implantés dans Terrier) s'appuient tous sur l'utilisation des premiers documents retrouvés suite à une première recherche. Aussi, optimiser la précision dans ces premiers documents, optimise à priori la reformulation de requêtes.
Caractéristiques d'évaluation associées
Afin d'évaluer les résultats obtenus nous avons utilisé trec_eval 3 qui calcule plus de 100 mesures de performance telles que bpref , AP et P @5. Cependant, nous avons restreint les variables utilisées à celles qui sont les moins corrélées. Ainsi, nous avons conservé les 6 mesures de performance préconisées dans Baccini et al. (2012). Elles sont résumées dans la table 1. Elles sont toutes quantitatives à valeur continue et leurs valeurs sont comprises entre 0 et 1. 
Distribution des valeurs des variables de mesure de performance
La figure 2 montre la distribution des valeurs des variables correspondant aux mesures d'évaluation de la performance. On note que le minimum 0 est atteint pour chacune des variables ; en revanche le maximum 1 n'est atteint que pour la P @30 et la iprec@recall0. L'ensemble de ces figures montre que les valeurs sont faibles ; la valeur 0 est la plus fréquente, montrant ainsi que beaucoup de configurations échouent dans la RI. Par ailleurs, comme les données ne suivent pas une loi normale, il faudra faire attention aux analyses réa-lisées par la suite pour ne choisir que celles qui s'appliquent à des variables qui ne suivent pas une loi normale. Cependant, les tests et méthodes que nous utilisons dans la suite restent valident car nous travaillons sur un grand jeu de données.
Corrélations entre variables de même type
Nous avons étudié la corrélation d'une part entre les variables correspondant aux paramètres du SRI et d'autre part entre variables d'évaluation des moteurs.
Nous avons analysé le lien entre les paramètres du moteur de RI, pris deux à deux afin de savoir si certaines de ces variables paramètres ont des rôles similaires ou sont liées entre elles. Pour cela, nous avons réalisé le test du chi 2 . Soit l'hypothèse H0 «les deux variables sont indépendantes» contre H1 «les deux variables ont un lien». Nous rejetons l'hypothèse H0 si la p-value est inférieure à 0, 05. Après avoir réalisé le test pour chacune des variables, nous concluons que toutes les variables qualitatives sont indépendantes deux à deux. Elles ont donc chacune leur rôle spécifique.
En revanche, en ce qui concerne les variables quantitatives correspondant aux mesures de performance, nous avons constaté qu'il existe une corrélation. Cette corrélation est plus ou moins importante en fonction des mesures que l'on compare. La figure 5  Ainsi les mesures de performance, déjà réduites à 6 pour plus de 100 au départ sont assez redondantes dans leur capacité à mesurer les performances des systèmes puisque corrélées, même si elles ne mesurent pas le même phénomène. Les paramètres du système en revanche n'étant pas corrélés, cela a un sens de chercher à optimiser chacun de ces paramètres.
Corrélations entre variables paramètres et variables d'évaluation
Afin d'étudier l'effet des paramètres sur les mesures de performance, nous avons effectué une analyse de la variance (ANOVA).
Soit l'hypothèse H0 «Le paramètre n'a pas d'effet sur la mesure de performance» contre H1 «le paramètre a un effet sur la mesure de performance». Nous rejetons H0 si la p-value est < 0, 05.
Nous avons étudié cette corrélation sur chacune des mesures de performance. La table 2  paramètres ont un effet significatif. Ces trois paramètres sans effet pourront donc être fixés dans la génération éventuelle d'autres données. Pour être réellement exhaustif, il faudrait vérifier que ces paramètres n'ont pas d'influence lorsque l'on change de collection, mais compte tenu de leur nature, la probabilité que cela soit le cas est forte.
Effet significatif
Effet non significatif TrecQueryTagsProcess, Topic BlocSize, IgnoreEmptyDocuments RetrievingModel, Stemmer IgnoreLowIdfTerms TAB. 2 -Effets significatifs et non significatifs pour l'ensemble des mesures de performance.
Variables ayant le plus d'influence
Nous avons utilisé la méthode Stepwise qui est une régression linéaire multiple Bendel et Afifi (1977) pour étudier l'influence relative des différentes variables paramètres. Cette mé-thode ajoute les variables les plus significatives du modèle et retire les moins significatives pas à pas. Dans le cadre de notre étude, elle a pour objectif de sélectionner les paramètres qui ont le plus d'influence sur les mesures de performance. C'est une combinaison de la mé-thode Forward et de la méthode Backward. La première méthode part du modèle vide et ajoute les variables les plus significatives du modèle progressivement, tandis que la seconde part du modèle complet et élimine progressivement les variables les moins significatives du modèle.
Après avoir réalisé cette analyse sur les différentes mesures de performance, nous observons que les trois variables supprimées sont BlocsSize, IgnoreEmptyDocuments et IgnoreLowIdf T erms, comme dans le cas de l'étude des corrélations précédente.
Au final cette méthode sélectionne donc les paramètres T opic, T recQueryT agsP rocess, RetrievingM odel et Stemmer. La variable T opic est la plus significative du modèle, suivi de T recQueryT agsP rocess, puis de RetrievingM odel et de Stemmer. Le besoin d'information considéré est le paramètre dont dépend le plus les résultats. Cela est un résultat important concernant la variabilité des résultats. On aurait pu penser que le modèle de recherche utilisé pouvait être le plus important des paramètres. La formulation du besoin d'information est éga-lement importante puisque le paramètre T recQueryT agsP rocess correspond aux parties du besoin d'information pris en compte lors du traitement. Lorsque seul le titre est considéré, il correspond à quelques mots, taille typique des requêtes sur le web. Lorsque les autres champs sont également considérés, il peut s'agir de requêtes plus longues, donnant un contexte précis du besoin d'information. Cette première analyse avait pour objet de déterminer les paramètres du moteur les plus importants ou qui influencent le plus la performance d'une recherche. Dans la section suivante, nous déterminons quelles sont les valeurs de paramètres les plus susceptibles de conduire à de bons résultats.
4 Paramètres des SRI pour des classes de précision L'objectif de cette analyse est d'étudier les valeurs des paramètres du moteur de recherche qui peuvent être associées à des valeurs de précision. Nous nous sommes appuyés dans cette étude sur la AP qui est la mesure consensuelle lorsqu'il s'agit de comparer globalement plusieurs systèmes et qui est utilisée en particulier dans la campagne d'évaluation TREC (trec.nist.gov) (Voorhees, 2007).
Classification mixte
La classification mixte est une méthode de classification qui a pour objectif d'obtenir, à partir des facteurs issus d'une analyse des correspondances multiples (ACM), des classes d'individus les plus cohérentes possibles en constituant les groupes les plus homogènes.
Dans notre cas d'étude, nous utilisons cette méthode afin d'associer à une classe de valeur de AP les paramètres de moteurs. L'idée sous-jacente est de favoriser les combinaisons de paramètres qui sont plutôt associées à des valeurs fortes de AP et d'éviter les combinaisons de paramètres plutôt associées à des valeurs faibles de AP .
Cette étude nécessite d'appliquer une ACM, méthode qui s'applique sur des variables qualitatives. Afin de transformer le paramètre d'évaluation qualitatif considéré (l'AP ) en valeurs qualitatives, nous avons créé des classes de valeurs de AP . Les classes ont été définies de sorte d'avoir des effectifs comparables. Nous noterons dans la suite la classe map1 la classe ayant FIG. 6 -Valeur des paramètres pour les classes de AP.
les valeurs de AP les plus faibles jusqu'à map4 la classe ayant les valeurs de AP les plus fortes. La figure 6 présente les résultats du croisement entre les classes de AP et les variables paramètres selon la méthode de classification mixte.
Nous pouvons observer dans la figure 6 les modalités présentes dans chacune de ces classes. La classe 1 contient des mesures de performance à valeurs faibles, la classe 2 des mesures de performance à valeurs moyennes, la classe 3 des mesures de performance à valeurs très faibles et la classe 4 des mesures de performance à valeurs élevées.
La combinaison de paramètres la plus représentative pour la classe 4, c'est-à-dire pour la classe ayant des mesures de performance à valeurs élevées est :
-IgnoreEmptyDocuments = TRUE -Stemmer = PS -RetrievingModel = LemurTFIDF -TrecQueryTagsProcess = TITLE -IgnoreLowIdfTerms = TRUE -IgnoreEmptyDocuments = TRUE -Stemmer = PS -RetrievingModel = LemurTFIDF -TrecQueryTagsProcess = TITLE -IgnoreLowIdfTerms = TRUE La combinaison de paramètres la plus représentative pour la classe 3, c'est-à-dire pour la classe ayant des mesures de performance à valeurs très faibles (map1) est :
-IgnoreEmptyDocuments = FALSE -Stemmer = Crop -RetrievingModel = DFI0 -TrecQueryTagsProcess = NARR -IgnoreLowIdfTerms = FALSE Cette combinaison de paramètres est donc à éviter, de même, que la combinaison de paramètres la plus représentative pour la classe 1, c'est-à-dire pour la classe ayant des mesures de performance à valeurs faibles (map2).
Conclusions et perspectives
Dans cet article, nous nous sommes intéressés à une analyse massive de résultats de recherche d'information obtenus par un paramétrage du système. Ainsi, de nombreux paramètres ont été analysés, en étudiant les effets croisés de ceux-ci. Nous avons pu distinguer les requêtes en fonction de leur niveau de difficulté et définir les paramètres qui ont le plus d'influence en fonction de ces classes ainsi que leurs valeurs les plus adaptées. Un aspect qui reste à étudier est l'influence de la collection sur les résultats obtenus. En effet, nous nous sommes ici intéressés à une collection unique (TREC8).
Dans la suite de ces travaux, nous allons travailler sur des méthodes sélectives de recherche d'information, c'est à dire des méthodes qui adaptent le traitement en fonction des cas rencontrés. Ainsi, toutes les requêtes ne seront pas traitées de la même façon par le moteur, mais les paramètres du système seront au contraire différents en fonction du type de requêtes.
Le projet CAAS, financé par l'ANR dans le cadre de l'appel Contint 2010, a permis de développer le travail présenté ici. Nous remercions également Anthony Bigot et Sébastien Déjean pour leurs précieux conseils.

Introduction
L'analyse d'opinions est une tâche de fouille de textes qui consiste en l'identification et la classification des textes subjectifs en plusieurs catégories d'opinions (polarités). Dans la dernière décennie, beaucoup de travaux se sont penchés sur cette problématique, en prenant le problème sous différents angles (principalement statistique et/ou linguistique). Cependant, la question de visualisation n'a pas bénéficié de cet intérêt. La plupart des travaux proposent une visualisation basique (e.g., graphiques en secteurs), ce qui est clairement insuffisant dans un contexte de big data où l'utilisateur a d'autant plus besoin d'explorer les données dans l'ensemble, mais aussi dans le détail.
Dans ce travail, nous nous situons dans un contexte de veille sur le Web et nous nous intéressons au problème d'analyse d'opinions dans un contexte de veille. Ainsi, nous proposons une méthode de visualisation d'opinions basée sur l'utilisation de termes clés afin de restituer le maximum d'information à l'utilisateur. Notre méthode est implémentée au sein de la plateforme de veille AMIEI 1 . La section suivante présente la problématique de recherche que nous traitons. La section 3 présente le processus général de veille avec la plateforme AMIEI. La section 4 présente notre approche pour l'analyse d'opinions et la visualisation des résultats. Enfin, la section 5 présente un exemple d'application sur un corpus de tweets politiques.
Contexte et Problématique
L'analyse d'opinions est un domaine de recherche qui se concentre sur l'identification et la classification des opinions dans les données textuelles. Beaucoup de travaux se sont intéressés à l'une ou l'autre de ces problématiques mais la plupart se sont intéressés à la classification d'opinions, i.e., l'association d'un texte à une catégorie d'opinions (e.g., opinion positive vs. négative).
La problématique a été majoritairement approchée sous un angle statistique et/ou linguistique. D'un point de vue statistique, le texte est représenté sur l'espace de descripteurs (e.g., termes) afin qu'il puisse être traité par les outils d'apprentissage statistique, e.g., Pak et Paroubek (2010); Pang et al. (2002). Ces méthodes sont connues pour leur généricité (bon rappel). De l'autre part, les méthodes de linguistique, également appelées méthodes à base de règles, ont été largement déployées pour l'analyse d'opinions, e.g., Kennedy et Inkpen (2006); Wilson et al. (2005). Ces méthodes sont connues pour leur spécificité (bonne précision). Enfin, d'autres travaux ont tenté de mixer la généricité de la statistique et la spécificité de la linguistique afin de proposer des méthodes à la fois robustes et précises (méthodes hybrides), e.g., Dermouche et al. (2013); Kamps et al. (2004); Turney et Littman (2003).
Dans ce travail, nous nous intéressons au problème de visualisation de l'opinion. En effet, la problématique de visualisation n'a pas été suffisamment étudiée dans ce domaine en se contentant de visualiser les proportions de chaque polarité d'opinion sur un graphique. Cette méthode est clairement insuffisante dans le cas où l'on veut savoir davantage sur ses données. Par exemple, dans le domaine industriel, il serait intéressant d'identifier les idées redondantes et les concepts qui sont présents dans une catégorie d'opinion et pas dans une autre. Une telle visualisation a des applications directes dans plusieurs domaines, e.g., la veille stratégique et économique, la CRM, la e-réputation, etc.
Acquisition de l'information
Cette phase permet l'acquisition de l'information selon plusieurs modes : -Un moteur de recherche pour faire des recherches ponctuelles pouvant être capitalisées. -Un automate de collecte pour des opérations récurrentes à des fins de capitalisation. 
Capitalisation et traitement
Partage de l'information
Le partage et la diffusion des informations acquises et validées, ainsi que les résultats de l'analyse se font à travers un portail de consultation, permettant la recherche et le partage des informations organisées par thématique avec une gestion des droits d'accès à partir de profils prédéfinis. Le partage peut également se faire via "Mon espace" ; un module permettant de personnaliser, pour chaque utilisateur, son accès à la plateforme AMI EI.
Pour l'analyse d'opinions, la plateforme AMIEI offre la fonctionnalités suivantes : -Indicateurs classiques de l'opinion globale dans un corpus de documents (distribution des documents sur les classes de polarité). -Visualisation des termes clés pour chaque classe de polarités. Un terme clé doit être fréquent et discriminant vis-à-vis de la classe d'opinion qu'il caractérise. -Evolution des termes clés à travers le temps. -Soit c la classe d'opinion du document d (classe la plus probable). -Evaluer chaque terme w i du document d selon un critère de spécificité (pouvoir discriminatif du terme au regard de la classe d'opinions). Ici, nous choisissons comme critère le gain informationnel (IG). Ensuite, trier les termes w i du document selon ce critère : IG(w m |c) > IG(w n |c) > ... > IG(w p |c). -Les K premiers termes sont ceux qui "expliquent" le mieux cette classification. Nous précisons que les termes discriminants de deux classes différentes sont deux ensembles disjoints. En effet, un terme ne peut être responsable d'affecter un texte qu'à une seule classe.
Méthode et implémentation
Visualisation
La visualisation est une étape clé dans le processus d'analyse d'opinions, d'autant plus dans un contexte de big data. En effet, l'information utile est encore plus enfouie et difficile à retrouver, ce qui nécessite des techniques de visualisation efficaces et adaptées à ce contexte particulier. Dans AMIEI, nous proposons de visualiser l'opinion contenue dans un corpus de FIG. 2 -Visualisation en nuage de termes (extrait).
FIG. 3 -Visualisation en fisheye (extrait).
documents par un "nuage de termes" construit à partir de l'ensemble des termes discriminants responsables de la classification (cf. section 4.1.). Chaque terme discriminant est ainsi repré-senté par une taille proportionnelle à sa fréquence dans le corpus des textes. Nous proposons également une visualisation temporelle du nuage de terms en utilisant la technique de fisheye.
Etude de Cas
Nous réalisons une expérimentation sur un corpus composé de 50000 tweets issus d'une collecte massive réalisée par la plateforme de veille AMIEI dans la soirée du 02 Mai 2012 avec le tag "#ledebat" (400000 tweets collectés). Ces tweets sont relatif au débat télévisé du second tour de l'élection présidentielle française de 2012 ayant opposé F. Hollande et N. Sarkozy. Nous appliquons, comme prétraitement, la suppression de mots outils et de numériques.
Les Figures 2 et 3 représentent la visualisation du résultat d'analyse du corpus Politique. Pour une meilleure lisibilité, seulement une sélection de termes fréquents est représentée ici. La polarité des termes est représentés par une couleur (vert pour le positif et rouge pour le négatif). Ces résultats nous ont permis de cerner les termes et les concepts les plus importants dans chaque catégorie d'opinion. A partir de cette visualisation, nous pouvons tirer plusieurs enseignements dont voici quelques uns : -Le concept de "changement" dans toutes ses variantes (slogan phare de la campagne du candidat F. Hollande) est largement repris par les internautes, et ce de manière positive.

Introduction
Le projet ANR IMAGIWEB 1 consiste à analyser et à suivre l'évolution de l'image (au sens de l'opinion) sur la toile, d'une part des personnages politiques à travers le réseau social Twitter, et d'autre part de l'entreprise EDF vis-à-vis du nucléaire en utilisant des blogs comme données. Ce projet regroupe différents partenaires parmi lesquels un laboratoire de recherche en science politique, des entreprises et des laboratoires de recherche en fouille de données.
Dans un premier temps, les tweets et blogs récoltés sont annotés manuellement pour relater l'opinion qu'ils véhiculent. Par la suite, l'enjeu sera de détecter automatiquement les opinions grâce à des méthodes de fouille d'opinion. Au-delà de la détection des opinions, pour mieux comprendre et analyser le contenu des tweets et des blogs, l'enjeu est aussi de les visualiser et de les explorer. Ainsi, un autre objectif du projet consiste à fournir à l'utilisateur, qu'il soit politologue, sociologue, marketeur ou encore analyste, un outil pour explorer les données (issues de tweets ou de blogs) et pour analyser en ligne l'opinion selon différents points de vue (sujets, temps, ...). L'analyse OLAP (OnLine Analytical Processing) permet de répondre à cet objectif de navigation, d'analyse et de visualisation.
L'OLAP sur des données textuelles correspond à une thématique de recherche récente avec des enjeux scientifiques importants. En effet, si l'OLAP a su montrer tout son potentiel analytique sur des "données classiques", la prise en compte de données textuelles nécessite une adaptation ou une évolution de l'OLAP pour prendre en compte les spécificités de ces données (Ravat et al., 2007;Zhang et al., 2009). Quelques travaux de recherche encore plus récents portent sur l'analyse OLAP de tweets, un cas particulier de données textuelles (Ben Kraiem et al., 2014;Bringay et al., 2011). Dans ce contexte, l'objectif de ce papier est de (1) démontrer l'intérêt de l'analyse OLAP pour ce type de données en se basant sur des cas d'étude réels, (2) relater une implémentation concrète "classique" en utilisant des outils existants.
Pour ce faire, dans la section 2 nous commençons par présenter les deux cas d'étude. Dans la section 3, nous évoquons les aspects de modélisation multidimensionnelle et de navigation. Dans la section 4, nous exposons la mise en oeuvre, avant de conclure dans la section 5.
Deux cas d'étude
Dans le cadre du projet IMAGIWEB, deux cas d'étude sont traités : des tweets à caractère politique et des billets de blogs traitant de l'entreprise EDF et du nucléaire. Pour chacun des cas, un processus d'annotation manuelle concernant l'opinion véhiculée a été mis en place.
Données tweets et besoins d'analyse
Dans le cadre du projet IMAGIWEB, les tweets ont été recueillis grâce à l'API Streaming de Twitter. Ce sont des tweets en français, à caractère politique, portant sur Nicolas Sarkozy et François Hollande, avant et après les élections prési-dentielles de 2012. Les données extraites sont le contenu du tweet, le pseudonyme du twittos, la date du tweet, l'image (à savoir François Hollande ou Nicolas Sarkozy, c'est à dire l'entité sur laquelle porte le tweet), l'URL qui mène vers le tweet.
Une annotation est faite par un annotateur sur un extrait ou un passage d'un tweet. L'annotateur détermine l'opinion contenue dans le passage (avec une polarité allant de -2 pour une opinion très négative à +2 pour une opinion très positive en passant par le zéro si l'opinion est neutre ou par le NULL s'il n'y a pas d'opinion) ainsi que la cible (le sujet sur lequel porte le passage) et la sous-cible. Les cibles et sous-cibles ont été déterminées par les membres du projet. Citons comme exemple de cible "bilan", "compétences", "positionnement". Pour la cible "positionnement", les sous-cibles sont "alliance", "écologie", "économie" et "sociétal". Enfin l'annotateur donne un niveau de confiance dans son annotation. 4073 tweets ont été annotés manuellement, ce qui a donné lieu à 5674 annotations.
Les données tweets constituent le terrain d'analyse des chercheurs en science politique et en sociologie. Les politologues souhaitent pouvoir suivre l'évolution dans le temps des deux images que sont François Hollande et Nicolas Sarkozy à travers Twitter. L'analyse de ces données, à la fois des tweets eux-mêmes et de leurs annotations, constitue un premier enjeu du projet.
Données blogs et besoins d'analyse
Les blogs à analyser concernent tout ce qui touche à EDF et au nucléaire. À partir d'un ensemble de blogs, tous les articles, en français, avec au moins une occurrence du sigle EDF ou des mots "Electricité de France" et de "nucléaire" ont été collectés. Les données contiennent le titre de l'article, l'URL du site web dont provient l'article, la date, le contenu textuel et l'image (sécurité, emploi ou prix). Les données blogs contiennent également le passage annoté (à chaque article correspond un ou plusieurs passages), la cible ("politique", "tarifs" ou encore "risques"), la sous-cible (par exemple "démantèlement/durée de vie" ou "expertise/incident" pour la cible "risques"), la polarité et la confiance. 560 articles ont été annotés manuellement en 3420 annotations (6,1 annotations par article en moyenne).
Par rapport aux besoins, les marketeurs d'EDF souhaitent centrer leur analyse sur les notions de cibles, de polarité. Ils souhaitent également pouvoir naviguer dans les données selon le type de structure (organisme) dont est issu le blog. Cette information peut être portée par l'extension du site web. Par exemple, une organisation à but non lucratif aura généralement un site web avec l'extension ".org" alors qu'une société aura un site web avec une extension ".com". Dans le modèle associé aux tweets (cf. figure 1), deux faits sont observés : ANNOTATION et TWEET. À ces faits sont associées plusieurs dimensions, comme le temps, la cible ou encore l'annotateur. Dans la dimension temps, on retrouve plusieurs niveaux de granularité de l'information avec deux hiérarchies : {jour, semaine, année} et {jour, mois, trimestre, année}.
Le fait TWEET va permettre de compter le nombre de tweets et de RT Confiance donne un indice quant à la confiance accordée à la polarité par l'annotateur. Le modèle permet également de retrouver l'image, la cible, l'annotateur, et bien sûr, le temps. La dimension annotateur rendra possible la comparaison de l'annotation automatique à celle manuelle le moment venu. Les dimensions image, cible et temps sont cruciales pour l'analyse. Une des particularités de ce modèle est de retrouver la polarité et la confiance aussi bien en mesure qu'en dimension. Cela permet de visualiser les données selon différentes manières. La polarité en tant que dimension permet par exemple de visualiser le nombre de fois où la polarité +2 est affectée alors qu'en la plaçant en tant que mesure, elle peut être agrégée avec des fonctions comme la somme ou la moyenne. Le modèle pour les blogs est assez similaire à celui des tweets. On retrouve deux faits Article et Annotation. Les mesures et les fonctions d'agrégat associées sont identiques. On retrouve également plusieurs dimensions en commun, à savoir la polarité, la confiance, la cible et le temps. Toutes ces notions similaires sont en fait celles associées au besoin commun concernant l'analyse de l'opinion. En revanche, notons comme différence que les blogs disposent d'un titre grâce à la dimension Blog et qu'ils sont également porteurs d'informations sur la structure qui héberge l'article (grâce à l'extension du site web) via une dimension Structure.
À partir du modèle multidimensionnel, pour introduire la navigation, la notion de cube OLAP est utilisée. Ainsi, deux cubes ont été créés concernant les données issues de Twitter et il en est de même pour les blogs. La navigation se caractérise par l'application d'opérateurs tels que le Drill Down qui permet d'aller vers un niveau plus détaillé selon la hiérarchie de dimension définie préalablement dans le modèle, en appliquant une fonction d'agrégat sur la mesure qui est observée. Il s'agit par exemple de passer de l'observation de la polarité moyenne par trimestre à l'observation par mois selon la hiérarchie temporelle. L'opérateur inverse s'appelle le Roll Up. Notons également l'existence de l'opérateur Slice & Dice qui permet de sélectionner certaines valeurs pour certains axes d'analyse. Par exemple, dans un cube qui permet d'observer le nombre de tweets par mois et par cible, il serait possible de sélectionner quelques cibles sur lesquelles on souhaite se focaliser.
Mise en oeuvre
Dans le cadre du projet IMAGIWEB, nous avons retenu MySQL comme SGBD en raison de contraintes techniques du projet. Nous avons également choisi de développer notre propre ETL (Extract Transform Load, phase correspondant à l'alimentation des données) car nous souhaitions pouvoir apporter des transformations très particulières en lien avec le contenu textuel (relatives à la fouille de texte) pour la suite du projet. Enfin, nous avons préféré le serveur OLAP Pentaho Mondrian en lui greffant l'interface graphique Saiku pour l'étendue de sa communauté et la prise en main de son environnement. L'implémentation résultante permet de naviguer dans les données en construisant des tableaux de bord très facilement pour l'utilisateur comme nous l'illustrons par la suite sur les données Twitter. Notons qu'il y a un menu sur l'interface qui permet également de représenter les données issues de la navigation sous forme de différents types de graphiques qui sont générés très simplement par l'utilisateur.
Initialement, le politologue peut par exemple observer la polarité moyenne en fonction du temps en trimestre pour les entités Hollande et Sarkozy. Puis, pour observer de façon plus précise, il peut obtenir le détail par mois (ce qui correspond au niveau OLAP à une opération de Drill Down), en se focalisant simplement sur Hollande (réalisant ainsi une opération de Slice), obtenant ainsi les résultats figurant dans la figure 2.
Ainsi, le politologue peut constater une baisse importante de popularité entre le mois de Mai et le mois de Juin (polarité de -0.346 à -0.658). Il peut ensuite détailler les cibles sur lesquelles cette baisse est plus importante, en ajoutant la dimension Cible dans les résultats. Le tableau 1 qui en résulte permet d'observer que l'opinion des Twittos a particulièrement diminué sur ses performances (-0.294 à -1.000) mais aussi sur son positionnement et son projet.
FIG. 2 -Polarité moyenne et nombre de tweets en fonction du temps en mois pour Hollande
L'intérêt pour le chercheur en science politique est ici, sur la base de la navigation, de pouvoir établir des liens entre l'opinion exprimée sur le web et des évènements de la vie politique, d'observer également à quel point le Web est un miroir ou non de l'opinion publique au sens large (comparaison avec les sondages d'opinion classiques).
Conclusion
Dans le cadre du projet IMAGIWEB, l'analyse OLAP était une des pistes à explorer pour visualiser les données. La mise en oeuvre de l'architecture décisionnelle a répondu à de réels 

Introduction
De nos jours, le maintien opérationnel d'un Système d'Information est devenu un des critères essentiels pour toute entreprise, ou personne cherchant à délivrer un service, ou simplement souhaitant communiquer. Le côté déplaisant de l'interconnexion mondiale des Systèmes d'Information réside dans un phénomène appelé "Cybercriminalité". Des personnes, des groupes mal intentionnés ont pour objectif de nuire aux informations d'une entreprise, d'une personne voire d'un Etat. Conséquemment, la détection des intrusions doit permettre de protéger le Système d'Information. L'objectif de cet article est de présenter dans un premier temps l'état de l'art en matière de détection d'intrusions et dans un second temps d'aborder les travaux menés afin de faciliter la visualisation des flux. La première partie de cet article sera consacrée à l'étude de l'existant dans laquelle nous présenterons les différentes approches de détection d'intrusions et leurs limites. Ensuite, nous nous intéresserons à la motivation de nos travaux et nous proposerons une solution. Nous détaillerons par la suite, la première phase de nos travaux ainsi que les résultats et nous terminerons par une conclusion et les perspectives.
Étude de l'existant
Une multitude d'outils (Antivirus, IDS, IPS, HIDS, Firewall) permettent aujourd'hui de mettre en place une sécurité "relative" pour l'ensemble du Système d'Information. Les principaux risques résiduels sont l'absence de constat en temps réel sur le signalement des comportements anormaux et sur l'exploitation des vulnérabilités. Il convient donc de répondre en fournissant des contremesures dans des délais raisonnables.
Les différentes solutions de détection d'intrusions
Les systèmes de détection des intrusions sont divisés selon les 3 familles distinctes :
-NIDS (Network Intrusion Detection System) est une sonde chargée d'analyser l'activité réseau du segment où elle est placée et de signaler les transactions anormales (Bhruyan et al (2011)). -HIDS (Host-Based Intrusion Detection System) est basée sur l'analyse d'un hôte selon les produits utilisés, une HDIS surveille le trafic à destination de l'interface réseau, l'activité système et logiciel, les périphériques amovibles pouvant être connectés. -HYBRIDES, qui rassemble les informations des NIDS et HIDS et produit des alertes aussi bien sur des aspects réseau qu'applicatifs. Il existe aussi une variante appelée "IPS" (Intrusion Prevention System) étant capable d'appliquer une politique de sécurité lors d'une intrusion. Un autre concept nommé CIDN 1 décrit par Fung (2011) offre la possibilité de partager des informations au travers un espace communautaire sur Internet. Les différentes solutions s'appuient sur deux méthodes, la première est fondée sur une comparaison d'une tentative d'intrusion par rapport à une base de signatures. Ce type de système recherche dans les trames réseau un schéma qui correspond à une signature connue via de l'extraction de motifs. Il est possible d'ajouter de nouvelles signatures, c'est à dire de créer une expression régulière qui correspondra par son contenu à une activité malveillante ou abusive. La seconde méthode repose sur des modèles comportementaux appelés "profils". Ils sont utilisés pour détecter les comportements déviant des profils définis. Les anomalies peuvent "signaler" une intrusion ou un nouveau comportement. Dans le second cas, il convient d'ajouter ces nouveaux comportements afin de diminuer les " faux positifs". Le concept de détection des anomalies repose sur une analyse statistique et un apprentissage temporel des comportements. Plusieurs principes de mise en place sont disponibles comme "IDES" 2 (Lunt et al, 1992), ou "EMERALD" 3 (Porras et Neumann, 1997 
Limitation des solutions existantes
Les principales limites des outils présentés dans les chapitres précédents résident dans le fait que l'analyse des événements et journaux systèmes est souvent considérés comme fastidieuse. De plus, ils ne prennent pas encore en compte l'évolution quasi permanente d'un Système d'Information. Par exemple, la sécurité d'un entrepôt de données peut être mise en cause par la non réévaluation du ou des serveurs hébergeant ce dernier. La structuration organisationnelle et l'analyse de risques s'avèrent donc indispensables. 
Motivations et proposition
Réalisation de la première phase
Cette phase constitue un tout en soi dans la mesure où la visualisation des données pour les utilisateurs est un enjeu crucial en terme de prise de décisions sur les problématiques de sécurité. Il s'agit du préambule à la "fouille de données" qui sera effectuée dans les phases suivantes. Un des principaux équipements de sécurité est le "Pare-Feu" , les données brutes envoyées en temps réel par l'ensemble des équipements de filtrage sont traitées selon une extraction de motifs .
Description des données
Le réseau SP1 propose des services à destination de 14 millions de personnes. Les données peuvent être considérées comme sensibles et portent sur une quantité de 9.2 Teraoctets et plusieurs dizaines de millions d'euros par jours. Ces données sont hétérogènes et proviennent de plusieurs sources différentes. Le contenu des variables listées ci-dessous est exporté vers les conteneurs de données. La phase 1 se focalisera uniquement sur l'analyse et la représentation graphique de ces dernières.
-adresse ip source, adresse ip de destination, port de destination, protocole (udp et tcp) -date et heure de la connexion -numéro de la règle du pare feu appliquée, action appliquée par la politique de filtrage.
Le tableau 2 synthétise le volume en nombre de lignes traitées par les équipements de filtrage. 
Scénario de visualisation
La représentation graphique de l'ensemble des flux autorisés selon la période souhaitée relève du problème de vision de grands graphes (voir figure 2), mais il est possible d'extraire des "sous graphes" basés sur du "requêtage" qui visent à sélectionner les modalités de certaines variables (adresses source et de destination ainsi que les services et protocoles). En revanche, l'analyse d'un graphique fondé sur les flux rejetés (même agrégés) comme le montre la figure 3 s'avère simple mais aussi efficace. Une adresse IP tente de se connecter à plusieurs autres adresses sur le port "135". Une recherche de répertoires partagés peut être à l'origine de ce type de comportement. 
Résultat
A l'issue de la phase 1, Le traitement des informations recueillies sur les différents équipe-ments de filtrage permet de visualiser rapidement les tentatives de connexion depuis plusieurs sources vers plusieurs destinations. Ceci rend possible de soulever des interrogations sur cette transaction et de mettre en place une action de surveillance. D'autres options ont été créées afin d'offrir une visualisation des règles de filtrage les plus utilisées. En cas de doute sur une 8. https ://www.perl.org/ 9. Logiciel de visualisation graphique, http ://www.graphviz.org 10. AfterGlow, outil de génération graphique, http ://afterglow.sourceforge.net/ 11. Responsables de la sécurité du système d'information, ingénieurs sécurité, administrateurs réseau adresse Ip, il est possible de lister toutes les activités de cette dernière selon des critères de temps, de destination, de protocoles et de ports utilisés.

Introduction
Dans cet article, nous proposons une nouvelle méthode de classification non supervisée de documents multilingues de corpus comparable bruité afin d'améliorer l'extraction des lexiques de traduction. Nous nous basons sur l'approche de (Rouane et al., 2007) dans la réingénierie des modèles UML et (Mimouni et al., 2012) dans la RI qui ont profité d'un couplage entre l'aspect formel et le relationnel afin de prendre en compte des relations entre les objets d'un même contexte. Nous avons choisi d'effectuer un couplage entre l'Analyse Formelle de Concepts (AFC) et les modèles vectoriels. En effet, l'AFC, appliquée dans un contexte de fouille de textes, permet d'extraire des classes de documents sous formes de CFs. D'un autre côté, les modèles vectoriels basés sur les vecteurs des extensions des CFs extraits, permettent d'aligner les CFs des différentes langues en calculant le degré de similarité des Concepts Fermés monolingues extraits, dans l'objectif de générer des CFs multilingues.
Extraction de Concepts Fermés à partir de corpus comparables
En classification de documents, un Concept Fermé est le couple < T, D >, avec T l'ensemble des termes des documents qui appartiennent à tous les documents D, et D, l'ensemble des documents qui contiennent tous les termes de T . Dans notre contexte de recherche, un Concept Fermé représente une classe de documents regroupés selon un ensemble de termes représentatifs. L'extraction des Concepts Fermés, à partir d'un corpus comparable français-anglais, est précédée par une étape de pré-traitement linguistique du corpus comparable bilingue mais aussi une réorganisation du contenu des documents du corpus en question est né-cessaire. Les concepts en sortie sont de la forme : CF =< {t 1 , t 2 , . . . , t n }, {d 1 , d 2 , . . . , d m } > tel que {t 1 , t 2 , . . . , t n } (ou extension) est l'ensemble des termes qui composent un termset fermé et {d 1 , d 2 , . . . , d m } (ou intension) l'ensemble de documents dans lesquels {t 1 , t 2 , . . . , t n } sont apparus ensemble avec une fréquence supérieure ou égale à minsupp. La sortie est composée de l'ensemble des Concepts Fermés français CF f r et des Concepts Fermés anglais CF en séparément.

?
rabdesselam/fr/ Résumé. Les résultats de toute opération de classification ou de classement d'objets dépendent fortement de la mesure de proximité choisie. L'utilisateur est amené à choisir une mesure parmi les nombreuses mesures de proximité existantes. Or, selon la notion d'équivalence topologique choisie, certaines sont plus ou moins équivalentes. Dans cet article, nous proposons une nouvelle approche de comparaison et de classement de mesures de proximité, dans une structure topologique et dans un objectif de discrimination. Le concept d'équivalence topologique fait appel à la structure de voisinage local. Nous proposons alors de définir l'équivalence topologique entre deux mesures de proximité à travers la structure topologique induite par chaque mesure dans un contexte de discrimination. Nous proposons également un critère pour choisir la "meilleure" mesure adaptée aux données considérées, parmi quelques mesures de proximité les plus utilisées dans le cadre de données quantitatives. Le choix de la "meilleure" mesure de proximité discriminante peut être vérifié a posteriori par une méthode d'apprentissage supervisée de type SVM, analyse discriminante ou encore régression Logistique, appliquée dans un contexte topologique. Le principe de l'approche proposée est illustré à partir d'un exemple de données quantitatives réelles avec huit mesures de proximité classiques de la littérature. Des expérimentations ont permis d'évaluer la performance de cette approche topologique de discrimination en terme de taille et/ou de dimension des données considérées et de sélection de la "meilleur" mesure de proximité discriminante.
Introduction
La comparaison d'objets, de situations ou d'idées sont des tâches essentielles pour évaluer une situation, pour classer des préférences ou encore pour structurer un ensemble d'éléments matériels ou abstraits, etc. En un mot pour comprendre et agir, il faut savoir comparer. Ces comparaisons que le cerveau accomplit naturellement, doivent cependant être explicitées si l'on veut les faire accomplir à une machine. Pour cela, on fait appel aux mesures de proximité. Une mesure de proximité est une fonction qui mesure la ressemblance ou la dissemblance entre deux objets d'un ensemble. Ces mesures de proximité ont des propriétés mathématiques et des axiomes précis. Mais est-ce que ces mesures sont toutes équivalentes ? Peuvent-elles être utilisées dans la pratique de manière indifférenciée ? Produiront-elles les mêmes bases d'apprentissage qui serviront comme entrée pour l'estimation de la classe d'appartenance d'un nouvel objet. Si nous savons que la réponse est non, alors comment pouvoir décider laquelle utiliser ? Certes, le contexte de l'étude ainsi que le type de données considérées peuvent aider à sélectionner quelques mesures de proximités, mais laquelle choisir parmi cette sélection ?
On retrouve cette problèmatique dans le cadre d'une classification supervisée ou d'une discrimination. L'affectation ou le classement d'un objet anonyme à une classe dépend en partie de la base d'apprentissage utilisée. Selon la mesure de proximité choisie, cette base d'apprentissage change et par conséquent le résultat du classement aussi.
On s'intéresse ici au degré d'équivalence topologique de discrimination de ces mesures de proximité. Plusieurs études d'équivalence topologique de mesures de proximité ont été proposées Batagelj et Bren (1992, 1995; Rifqi et al. (2003); Lesot et al. (2009);Zighed et al. (2012) mais pas dans un objectif de discrimination. Cet article met donc l'accent sur la façon de construction la matrice d'adjacence induite par une mesure de proximité, tout en tenant compte des classes d'appartenance des objets, connues a priori, en juxtaposant des matrices d'adjacence intra-groupe et inter-groupes Abdesselam (2014). Un critère de sélection de la "meilleure" mesure est proposé. On vérifie en effet a posteriori qu'elle est bien une bonne mesure discriminante en utilisant la méthode des SVM multi-classes.
TAB. 1 -Quelques mesures de proximité.
Où, p désigne la dimension de l'espace, Cet article est organisé comme suit. Dans la section 2, après avoir rappelé les notions de structure, de graphe et d'équivalence topologique, nous présentons la façon dont a été construite la matrice d'adjacence dans un but de discrimination, le choix de la mesure du degré d'équivalence topologique entre deux mesures de proximité ainsi que le critère de sélection de la "meilleure" mesure discriminante. Un exemple illustratif est commenté en section 3. Une conclusion et quelques perspectives de cette approche sont données en section 4.
Le tableau 1 présente quelques mesures de proximité classiques utilisées pour des données continues, définies sur R p .
Equivalence topologique
L'équivalence topologique repose en fait sur la notion de graphe topologique que l'on dé-signe également sous le nom de graphe de voisinage. L'idée de base est en fait assez simple : deux mesures de proximité sont équivalentes si les graphes topologiques correspondants induits sur l'ensemble des objets restent identiques. Mesurer la ressemblance entre mesures de proximité revient à comparer les graphes de voisinage et à mesurer leur ressemblance. Nous allons tout d'abord définir de manière plus précise ce qu'est un graphe topologique et comment le construire. Nous proposons ensuite une mesure de proximité entre graphes topologiques qui servira par la suite à comparer les mesures de proximité.
Graphe topologique
Sur un ensemble de points x, y, z, . . . de R p , on peut, au moyen d'une mesure de proximité u définir une relation de voisinage V u qui sera une relation binaire sur E × E. Pour simplifier la compréhension mais sans nuire à la généralité du propos, considérons un ensemble d'objets E = {x, y, z, . . .} de n = |E| objets plongés dans R p . Ainsi, pour une mesure de proximité u donnée, nous pouvons construire un graphe de voisinage sur un ensemble d'individus dont les sommets sont les individus et les arêtes sont définis par une propriété de la relation de voisinage.
De nombreuses définitions sont possibles pour construire cette relation binaire de voisinage. On peut, par exemple, choisir l'Arbre de Longueur Minimale (ALM) Kim et Lee (2003), le Graphe de Gabriel (GG) Park et al. (2006), ou encore le Graphe des Voisins Relatifs (GVR) Toussaint (1980), dont tous les couples de points voisins vérifient la propriété suivante :
) V u (x, y) = 0 sinon c'est-à-dire, si les couples de points vérifient ou pas l'inégalité ultratriangulaire (1), condition ultramétrique.
La figure 1 montre, dans R
, un exemple de graphe topologique GVR parfaitement défini par la matrice d'adjacence V u associée, formée de 0 et de 1. Sur le plan géométrique, cela signifie que l'hyper-Lunule (intersection des deux hypersphères centrées sur les deux points) est vide.
Comparaison de mesures de proximité
On dispose de p variables quantitatives explicatives (prédicteurs) {x j ; j = 1, p} et d'une variable qualitative cible à expliquer y, partition de n = ? q k=1 n k individus-objets en q modalités-groupes {G k ; k = 1, q}.
Pour toute mesure de proximité u i donnée, on construit, selon la propriété (1), la matrice d'adjacence binaire globale V ui qui se présente comme une juxtaposition de q matrices d'adjacence symétriques Intra-groupe {V
A noter que la matrice d'adjacence partitionnée globale V u i ainsi construite, n'est pas symétrique. En effet, pour deux objets
• Le premier objectif est de regrouper les différentes mesures de proximité considérées, selon leur similitude topologique pour mieux visualiser leur ressemblance dans un contexte de discrimination.
Pour mesurer le degré d'équivalence topologique de discrimination entre deux mesures de proximité u i et u j , nous proposons de tester si les matrices d'adjacence associées V u i et V u j sont différentes ou pas. Le degré d'équivalence topologique entre deux mesures de proximité est mesuré par la quantité :
0 sinon.
• Le second objectif consiste à établir un critère d'aide à la sélection de la "meilleure" mesure de proximité ; celle qui parmi toutes les mesures considérées, discrimine au mieux les q groupes.
On note, V u * = diag (1 G 1 , . . . , 1 G k , . . . , 1 G q ) la matrice d'adjacence symétrique blocdiagonale de référence "discrimination parfaite" associée à la mesure de proximité inconnue, notée u * , où, 1 n k désigne le vecteur d'ordre n k dont toutes les composantes sont égales à 1 et 1 G k = 1 n k t 1 n k , la matrice carrée d'ordre n k dont tous les éléments sont égaux à 1.
On peut ainsi établir le degré d'équivalence topologique de discrimination S(V u i , V u * ) entre chaque mesure de proximité u i considérée et la mesure de référence u * . Enfin, afin d'évaluer autrement le choix de la "meilleure" mesure de proximité discriminante proposée par cette approche, nous avons appliqué a posteriori une technique de classement par SVM Multiclasses (MSVM) sur la matrice d'adjacence associée à chacune des mesures de proximité considérée y compris à la mesure de référence u * .
Exemple d'application
Pour illustrer notre approche, nous considérons ici un jeu de données bien connu et relativement simple, celui des Iris Fisher (1936); Anderson (1935). Ces données ont été proposées comme données de référence pour l'analyse discriminante et la classification par le statisticien Ronald Aylmer Fisher en 1933. Les données complètes se trouvent notamment dans UCI (2013). Quatre variables (longueur et largeur des sépales et pétales) ont été observées sur 50 fleurs de chacune des 3 espèces d'Iris (Iris Setosa, Iris Virginica, Iris Versicolor).
Comparaison et classement des mesures de proximité
Les principaux résultats de l'approche proposée sont présentés dans les tableaux et le graphique suivants. Ils permettent de visualiser les mesures qui sont proches les unes des autres selon l'objectif de discrimination.
Pour ce jeu de données, le tableau 2 récapitule les similarités entre les 8 mesures de proximité et montre que la mesure u T ch de Tchebychev est la plus proche de la mesure u * de référence.
Une Analyse en Composantes Principales (ACP) suivie d'une Classification Hiérarchique Ascendante (CHA) ont été effectuées à partir de la matrice de similarités entre les 8 mesures de proximité considérées, afin de les partitionner dans des groupes homogènes et de visualiser leurs ressemblances.
L'application d'un algorithme de construction d'une CHA selon le critère de Ward, Ward Jr (1963), permet d'obtenir le dendogramme de la figure 2. Le vecteur similarités S(V ui , V u * ) de 
TAB. 3 -Classement de la mesure de référence.
Au vu des résultats présentés dans le tableau 3 de la partition en 5 classes de mesures de proximité, la mesure de référence inconnue u * , projetée en élément supplémentaire, serait donc plus proche des mesures de la classe 3, c'est-à-dire, de la mesure de Tchebychev u T ch qui serait pour ces données, la "meilleure" mesure de proximité parmi les 8 mesures considé-rées. Ce résultat confirme celui constaté dans le tableau 2, à savoir, une plus grande similarité S(V u T ch , V u * ) = 68.10% de la mesure de Tchebychev avec celle de référence u * .
Les mesures discriminantes selon les MSVM
Cette partie consiste à valider les résultats du choix de la meilleure mesure au vu de la matrice de référence a posteriori en utilisant les MSVM. Nous utilisons ici le modèle M SV M LLW , Lee et al. (2004), considéré comme le plus fondé théoriquement du fait que sa solution donne un classifieur qui converge vers celui de Bayes.
Mesure
Erreur Pour ce faire, nous allons appliquer une des techniques de sélection du modèle consistant à tester plusieurs valeurs du paramètre et à choisir celle qui minimise l'erreur test calculée par validation croisée. Dans cet exemple, nous testons 10 valeurs du paramètres (entre 1 et 100) pour toutes les bases de données. Aprés simulations, la valeur choisie est C = 1.
Les principaux résultats du modèle M SV M LLW , appliqué sur chacune des matrices d'adjacence induites par les mesures de proximité considérées, sont présentés dans le tableau 4.
Le meilleur taux d'erreur est celui donné par les mesures de Tchebechev u T ch et Euclidienne u E , qui est aussi celui enregistré pour la matrice de référence.
L'application du modèle MSVM montre que les mesures de proximité de Tchebechev u T ch et Euclidienne u E sont les plus adaptées pour différencier et séparer au mieux les 3 espèces d'Iris. Ce résultat confirme celui obtenu précédemment, à savoir le choix de la mesure de Tchebychev u T ch comme la mesure plus proche, parmi les huit mesures considérées, de la mesure de référence et donc la plus discriminante.
Expérimentations
Nous avons procédé à des expérimentations sur d'autres jeux de données afin d'essayer d'évaluer l'effet des données, de leur taille et/ou de leur dimension sur les résultats de la classification des mesures de proximité toujours dans un but de discrimination. Est-ce que, par exemple, les mesures de proximité se regroupent différemment selon le jeu de données utilisé ? Selon la taille de l'échantillon et/ou le nombre de variables explicatives considérées dans un même ensemble de données ?
Pour répondre à ces questions, nous avons donc appliqué l'approche proposée sur différents jeux de données, présentés dans le tableau 5, qui proviennent tous du référentiel UCI (2013). L'objectif est de comparer les résultats des classifications des mesures de proximité de toutes ces expérimentations ainsi que la "meilleure" mesure discriminante proposée pour chacun de ces jeux de données.
Etant donné un ensemble de données explicatives X (n,p) à n objets et p variables, et une variable à expliquer ou à discriminer Y q à q modalités-classes.
Pour analyser l'effet du changement de dimension, nous avons considéré le jeu de données "Waveform Database Generator" pour générer 3 échantillons n?4 de taille n = 2000 objets et de dimension p égale respectivement à 40, 20 et à 10 variables.
De même, pour évaluer l'effet du changement de la taille de l'échantillon, nous avons éga-lement généré 3 autres échantillons n?5 de taille n égale respectivement à 3000, 1500 et à 500 objets et de même dimension p égale à 30 variables.
Les principaux résultats de ces expérimentations, à savoir les équivalences topologiques des mesures de proximité discriminantes et l'affectation de la mesure de référence u * dans la classe la plus proche, sont présentés dans le tableau 6.
Pour chacune de ces expérimentations, nous avons retenu une partition en cinq classes de mesures de proximité afin de les comparer et de bien distinguer les mesures de la classe d'appartenance de la mesure de référence, c'est-à-dire les mesures les plus discriminantes.
Les regroupements des mesures de proximité obtenus pour les trois jeux de données n?4 sont pratiquement identiques, il n'y a donc pas vraiment d'effet de la dimension. 
une nouvelle approche d'équivalence entre mesures de proximité dans un contexte de discrimination. Cette approche topologique est basée sur la notion de graphe de voisinage induit par la mesure de proximité. D'un point de vue pratique, dans ce papier, les mesures que nous avons comparées sont toutes construites sur des données quantitatives. Mais ce travail peut parfaitement s'étendre aux données qualitatives en choisissant la bonne structure topologique adaptée.
Nous envisageons d'étendre ce travail à d'autres structures topologiques et d'utiliser un critère de comparaison, autre que les techniques de classification, afin de valider le degré d'équivalence entre deux mesures de proximité. Par exemple, évaluer le degré d'équivalence topologique de discrimination entre deux mesures de proximité en appliquant le test non paramètrique du coefficient de concordance de Kappa, calculé à partir des matrices d'adjacence associées, Abdesselam et Zighed (2011). Cela va permettre de donner une signification statistique du degré de concordance entre les deux matrices de ressemblance et de valider ou pas l'équivalence topologique de discrimination, c'est-à-dire si vraiment elles induisent ou pas la même structure de voisinage sur les groupes d'objets à séparer.
Enfin, les expérimentations menées sur différents jeux de données ont montré qu'il n'y pas du tout d'effet de la dimension et pas vraiment d'effet de la taille de l'échantillon aussi bien sur les regroupement des mesures de proximité que sur le résultat du choix de la meilleure mesure discriminante.
proximity measures in a topological structure and a goal of discrimination. The concept of topological equivalence uses the structure of local neighborhood.
Then we propose to define the topological equivalence between two proximity measures, in the context of discrimination, through the topological structure induced by each measure. We also propose a criterion for choosing the "best" measure adapted to data considered among some of the most used proximity measures for quantitative data. The choice of the "best" discriminating proximity measure can be verified retrospectively by a supervised learning method type SVM, discriminant analysis or Logistic regression applied in a topological context.
The principle of the proposed approach is illustrated using a real quantitative data example with eight conventional proximity measures of literature. Experiments have evaluated the performance of this discriminant topological approach in terms of size and/or dimension of the relevant data and of selecting the "best" discriminant proximity measure.

Introduction
Ce papier propose un nouveau mécanisme d'optimisation pour l'algorithme de classification automatique évidentielle semi-supervisée SECM , qui est le premier à reposer sur des contraintes exprimées sous la forme de données étiquetées. Les algorithmes de classification évidentielle (Masson et Denoeux, 2008) reposent sur le cadre théorique des fonctions de croyance et permettent de représenter tous les types d'affectations partielles grâce au concept de partition crédale qui étend la notion de partition stricte, floue et possibiliste. Ces méthodes évidentielles ont été étendues dans le cadre semi-supervisé (Antoine et al., 2012 pour pouvoir tirer partie de contraintes de type Must-Link (ML) et Cannot-Link (CL) qui spécifient si deux données doivent ou non appartenir à la même classe. La transformation des informations disponibles a priori en ce type de contraintes peut néanmoins induire une perte de connaissance. L'algorithme SECM a été proposé récemment pour tirer partie de données partiellement étiquetées . Cependant, l'algorithme SECM initial repose sur une optimisation stricte qui respecte l'ensemble des contraintes et notamment la positivité des masses de croyances associées à l'affectation d'un point à une classe. Cette contrainte théorique entraîne la formation d'un problème complexe. Nous proposons donc de modifier le mécanisme d'optimisation en relâchant la contrainte de positivité, à l'instar de ce qui est fait dans (Bouchachia et Pedrycz, 2006), et en s'assurant a posteriori de l'optimisation que les masses de croyances sont positives. Nos résultats expérimentaux montrent que notre heuristique ne dégrade pas les performances de l'algorithme SECM et permet de gagner de manière significative en complexité sur nos jeux de tests.
Ce papier est organisé comme suit : la section 2 présente les concepts fondamentaux de la théorie des fonctions de croyance et les principales méthodes de classification automatique sous contraintes. L'algorithme semi-supervisé SECM  est ensuite décrit dans la section 3 et un nouveau schéma d'optimisation est proposé. Enfin, les résultats de l'algorithme sont présentés dans la section 4. Le papier conclut sur l'intérêt de la nouvelle méthode d'optimisation.
2 Travaux existants 2.1 Les fonctions de croyance L'intérêt principal d'un algorithme de classification évidentielle est de pouvoir représenter le doute concernant l'affectation d'un point à un cluster. Pour ce faire, ces méthodes reposent sur la théorie de l'évidence de Dempster-Shafer, également appelée théorie des fonctions de croyance (Shafer, 1976;Smets et Kennes, 1994). Soit ? une variable prenant ses valeurs dans un ensemble fini ? = {? 1 , . . . , ? c } appelé cadre de discernement. La connaissance partielle concernant la valeur de ? peut être représentée par une fonction de masses m, qui est une application de l'ensemble des parties de
Les sous-ensembles A ? ? tels que m(A) > 0 sont appelés les éléments focaux de m. La quantité m(A) s'interprète comme la quantité de croyance allouée à A et qui, faute d'information complémentaire, ne peut être allouée à aucun autre sous-ensemble de A. L'ignorance totale correspond à m(?) = 1 alors qu'une certitude totale se rapporte à l'allocation complète de la masse de croyance sur un unique singleton de ?. Si tous les ensembles focaux de m sont des singletons, alors la fonction de masses de croyances est équivalente à une distribution de probabilités. La quantité m(?) peut être interprétée comme la croyance que la valeur réelle de ? n'appartient pas à ?. Quand m(?) = 0, la fonction de croyance est dite normalisée. La connaissance exprimée par une fonction de croyance peut aussi être représentée par une fonction de plausibilité pl : 2 ? ? [0, 1] définie comme suit :
B?A? =? La quantité pl(A) est interprétée comme le degré maximal de croyance qui peut potentiellement être affecté à l'hypothèse selon laquelle la vraie valeur de ? appartient à A. Quand une décision doit être prise concernant la valeur de ?, il est intéressant de transformer une fonction de masses en probabilité pignistique (Smets et Kennes, 1994) :
??A où |A| dénote la cardinalité de A ? ?. Quand il existe m(?) ? = 0, une étape de normalisation doit précéder la transformation pignistique. La normalisation de Dempster, qui consiste à diviser toutes les masses par 1 ? m(?), est une méthode classique de normalisation.
Algorithme des c-moyennes évidentielles
La version évidentielle des k-moyennes, ECM, est un algorithme de classification automatique qui construit une partition crédale à partir des données. Dans ce formalisme, la connaissance partielle concernant l'appartenance d'un objet x i est représentée par une fonction de croyance m i sur l'ensemble ? des classes possibles. Ainsi, un degré de croyance peut être affecté aux singletons (comme dans les approches floues et possibilistes) mais également à tous les sous-ensembles de ?. Soit {x 1 , . . . x n } un ensemble d'individus dans R p à classer dans un ensemble ? = {? 1 , . . . ? c } de c classes. Pour chaque objet x i , la fonction de croyance m i est calculée en plaçant une grande (resp. petite) quantité de croyance sur le sous-ensemble proche (resp. éloigné) en terme de distance de x i . La distance d ij est une métrique définie entre un objet x i et une représentation dans R p d'un sous-ensemble A j ? ?. Similairement à l'algorithme des c-moyennes floues, chaque classe ? k est représentée par un prototype v k . Pour chaque sous-ensemble A j ? ?, A j ? = ?, un centre v j est calculé comme le barycentre des centres associés aux classes composant A j :
La distance d 2 ij peut être définie comme une distance euclidienne (Masson et Denoeux, 2008). Plus récemment, une variante a été proposée pour prendre en compte une distance de Mahalanobis (Antoine et al., 2012). Similairement aux travaux de (Gustafson et Kessel, 1979), cette distance permet de détecter des clusters ayant différentes formes géométriques, grâce à une matrice de covariance floue S k associée à chaque cluster ? k et qui doit être optimisée.
Ensuite, similairement à ce qui est fait pour les prototypes, pour chaque sous-ensemble de A j qui n'est pas un singleton, une matrice S j est calculée en moyennant les matrices incluses dans
L'algorithme ECM minimise la fonction objectif suivante en fonction des matrices M, V et S précédentes :
Comme m i? correspond à la croyance que x i est un point aberrant, son cas est traité séparément du reste des autres sous-ensembles. Le paramètre ? indique la distance de l'ensemble des objets à l'ensemble vide. Il est intéressant de remarquer à ce niveau qu'une pénalité des sous-ensembles A j ? ? avec une grande cardinalité a été introduite avec la pondération |A j | ? . L'exposant ? permet de contrôler le degré de cette pénalisation.
Tout comme pour les c-moyennes floues, la partition est construite selon un processus itératif qui optimise alternativement les matrices M, V et S. La complexité d'un algorithme évidentiel est linéaire avec le nombre de données mais exponentiel avec le nombre de classes. En conséquence, il est crucial pour ce type de méthodes de minimiser les calculs réalisés dans les phases d'optimisation comme cela est proposé dans cet article.
Algorithmes semi-supervisés
La plupart des méthodes de classification automatique ont été améliorées pour prendre en compte la connaissance experte sous la forme de contraintes soit entre paires de données de type Must-Link (ML) ou Cannot-Link (CL) qui indiquent si deux points doivent ou non appartenir au même cluster, soit sous la forme de données étiquetées (Wagstaff et al., 2001). Citons par exemple des algorithmes de type k-moyennes (Wagstaff et al., 2001;Basu et al., 2002), hiérarchiques (Davidson et Ravi, 2005), basés sur la densité (Ruiz et al., 2010;Lelis et Sander, 2009), des méthodes spectrales (Wang et Davidson, 2010) ainsi que des algorithmes dédiés aux flux de données (Ruiz et al., 2009). D'autres travaux se sont intéressés à l'intégration de contraintes dans l'algorithme des c-moyennes floues (Grira et al., 2006;Pedrycz, 1985;Bensaid et al., 1996;Pedrycz et Waletzky, 1997a). Pour palier les limitations des algorithmes flous en présence de bruit ou de points aberrants, des méthodes possibilistes (Krishnapuram et Keller, 1993;Sen et Davé, 1998) et plus récemment évidentielles ont été proposées (Masson et Denoeux, 2008. Ces dernières ont également été étendues au cas semi-supervisé pour bénéficier des avantages des modèles basés sur les fonctions de croyance dans la prise en compte de la connaissance experte. Les travaux proposés reposent soit sur des contraintes ML et CL (Antoine et al., 2012 soit, plus récemment, sur des données partiellement étiquetées avec l'algorithme SECM . D'un point de vue formel, deux approches ont été proposées dans la littérature pour prendre en compte les contraintes et les étiquettes pendant le processus de classification automatique. En premier lieu, il est possible de modifier le processus des algorithmes de classification, soit durant la phase d'initialisation (Basu et al., 2002), soit pendant la phase de convergence. Dans ce dernier cas, on peut soit imposer un respect strict des contraintes comme dans COP Kmeans (Wagstaff et al., 2001), soit modifier la fonction objectif pour pénaliser les solutions qui ne respectent pas complètement les contraintes (Pedrycz et Waletzky, 1997b). Par exemple, dans (Bouchachia et Pedrycz, 2003), les auteurs décrivent un FCM amélioré dont la fonction objectif introduit un terme de pénalité qui considère à la fois l'appartenance actuelle des points i aux classes k notée u ik , mais également l'appartenance telle qu'elle devrait être à partir des contraintes de l'expert notée˜unotée˜ notée˜u ik comme le montre l'équation (7).
TAB. 2 -Plausibilités calculées à partir de la partition crédale.
où U et V dénotent respectivement la matrice d'appartenance et les coordonnées des centres des clusters. ? est un paramètre de régulation qui permet d'équilibrer l'importance du respect des contraintes dans la fonction objectif.
En second lieu, d'autres méthodes proposent d'adapter la métrique en fonction des contraintes et étiquettes fournies par l'expert comme dans l'algorithme MPC k-means (Bilenko et al., 2004). Par exemple, dans (Bouchachia et Pedrycz, 2006), les auteurs proposent une mé-thode pour adapter une distance de Gustafson-Kessel (Gustafson et Kessel, 1979). Nous proposons dans ce papier de considérer un modèle de contraintes flexibles avec une modification de la fonction objectif qui pénalise les solutions ne respectant pas les données étiquetées.
3 Algorithme SECM 3.1 Formalisation du problème L'idée principale de l'algorithme proposé dans  est d'ajouter un terme de pénalité dans la fonction objectif de ECM afin de prendre en compte un ensemble d'objets étiquetés. La démarche suivie est la même que dans (Bouchachia et Pedrycz, 2003) mais rapportée aux algorithmes évidentiels. L'expression d'un objet étiqueté peut se traduire sous la forme d'une fonction quantifiant la croyance sur l'appartenance de l'objet à une classe. Considérons dans un premier temps une partition crédale connue et définie par le tableau 1. Elle représente la connaissance partielle de l'appartenance de quatre objets à deux classes. Il est alors possible de calculer la plausibilité de chaque objet x i pour chaque classe ? k , comme illustré par le tableau 2. On remarque alors qu'une plausibilité nulle permet de déduire avec certitude qu'un élément n'appartient pas à une classe. Ainsi, l'observation de pl i (? 1 ) = 0 permet de déduire que x 1 , un objet atypique, et x 4 , un objet affecté avec certitude dans la classe ? 2 , ne font pas partie de la classe ? 1 . En revanche, les objets dont la plausibilité pour une classe est élevée ont des chances d'appartenir à cette classe. Ainsi, pl i (? 1 ) = 1 apparaît pour l'objet x 2 , qui appartient à la classe ? 2 avec certitude, et pour l'objet x 3 , qui appartient soit à ? 1 , soit à ? 2 .
Supposons maintenant que l'on ne dispose pas de la partition crédale, mais qu'il existe des contraintes sous formes d'étiquettes. Par exemple, l'objet x i est inclus dans la classe ? k . Il est alors possible d'imposer la contrainte pl i (? k ) = 1. L'effet sera d'exiger :
-une croyance élevée pour les fonctions de masse ayant un sous-ensemble comprenant ? k , donc toutes les fonctions de masses qui ont un degré de croyance plus ou moins fort sur le fait que x i appartienne à ? k , -des valeurs faibles pour toutes les fonctions de masses ayant un sous-ensemble qui n'incluent pas ? k . La contrainte entre un objet x i et la classe ? k est donc respectée pour de nombreuses solutions allant de la certitude totale que x i appartienne à ? k jusqu'à l'incertitude complète de l'affectation de x i entre ? k ou plusieurs autres classes de ?. La contrainte est donc flexible car elle permet si nécessaire de garder un doute quant à l'affectation de l'objet à la classe. Par conséquent, cela limite l'influence négative d'une contrainte bruitée.
Lorsqu'un expert crée des contraintes d'étiquettes, il peut avoir un doute entre plusieurs classes pour un unique objet. Par exemple, l'objet x i appartient à une des classes du sousensemble A j ? ?. Cette information se modélise alors sous la forme d'une contrainte sur la plausibilité de A j : pl i (A j ) = 1. Cela revient à favoriser les fonctions de masses ayant au moins une classe dans A j . Cette contrainte, qui généralise la précédente, permet d'établir un terme de pénalité à ajouter à la fonction objectif de ECM :
La nouvelle fonction objectif est alors la suivante :
sous les contraintes (5)  
Optimisation
L'optimisation du nouveau critère consiste, de la même manière que pour l'algorithme ECM, à minimiser alternativement les matrices M, V et S. Le terme de pénalité J S ne dé-pendant ni de V, ni de S, leur mise à jour est similaire à ECM, et leur formule est présen-tée dans (Masson et Denoeux, 2008). La partition crédale M est au contraire présente dans J S . En fixant ? à 2 alors la minimisation de la fonction objectif par rapport à M devient un problème quadratique à contraintes linéaires. Ce problème peut être résolu par une méthode classique d'optimisation (Ye et Tse, 1989), néanmoins de nombreux auteurs se trouvant dans un contexte similaire proposent d'optimiser directement la fonction objectif sans prendre en compte les contraintes de positivité sur la partition (6), afin de réduire le temps de convergence de l'algorithme.
Afin de résoudre le problème de minimisation contraint par (5), des multiplicateurs de Lagrange ? 1 , . . . ? n sont introduits et le Lagrangien défini : 
A j ??,A j ? =? Annuler les dérivées partielles permet d'obtenir les équations suivantes :
Aj ??,Aj ? =? En utilisant (15) et (16) dans (17), il est possible d'écrire :
Cette équation peut finalement être utilisée dans (15) et (16) pour obtenir la mise à jour des fonctions de masse, ?i = 1, n et ?j/A j ? ?, A j ? = ? : 
A j ??,A j ? =? Comme le numérateur de la première partie de l'équation (19) permet l'obtention de valeurs négatives, la fonction de masse m ij peut être négative. Ces valeurs négatives sont accentuées par l'importance donnée aux contraintes. Si l'utilisateur reste dans un cas normal d'utilisation des contraintes, c'est-à-dire ? ? 0.8 (cf. partie 4.2), les valeurs de m ij < 0 seront proches de 0. Une fonction de réajustement est donc envisageable sans que l'optimisation soit dégradée :
avec 
Expérimentations
Les expériences menées sur plusieurs jeux de données consistent à comparer les résultats obtenus par SECM lorsque la mise à jour des fonctions de masse utilise l'optimisation de (Ye et Tse, 1989), noté SECM-classic, avec SECM et l'optimisation proposée, noté SECM-do.
Données et méthode d'évaluation
Jeux de données : Plusieurs jeux de données issue de l'UCI Machine Learning Repository ont été employés. Le tableau 3 indique leurs caractéristiques ainsi que la métrique utilisée pour les expériences. Il faut noter que LettersIJL correspond au jeu de données Letters modifié comme (Bilenko et al., 2004  Protocole expérimental : Une expérience consiste, pour un certain pourcentage de contraintes, à exécuter 25 fois l'algorithme SECM avec 25 jeux de contraintes différents. Afin d'éviter les optima locaux, chaque exécution teste cinq initialisations aléatoires des centres de gravité et récupère les résultats obtenus par l'initialisation ayant la fonction objectif minimale.
Résultats
Jeux de données réelles : La figure 1 montre l'évolution de l'indice de Rand moyen obtenu avec SECM-classic et SECM-do par rapport au pourcentage de contraintes pour Iris et Wine. Des résultats similaires ont été trouvés avec Ionosphere et LettersIJL. Le coefficient ? est fixé à 0.5. Il est ainsi possible de remarquer (1) que l'ajout progressif de contraintes améliore l'indice de Rand et (2) que l'algorithme SECM-do présente de meilleurs résultats que SECM-classic. Pour ces expériences, nous avons également constaté que les valeurs des fonctions objectif de SECM-do sont plus petites que celles de SECM-classic. Des résultats similaires ont été trouvés pour ? = 0.3 et ? = 0.8. La nouvelle optimisation permet donc d'obtenir un meilleur minimum grâce à sa relaxation des contraintes, ce qui implique de meilleurs résultats de classification. Il faut également noter que pour ? = 0.8, les résultats obtenus prouvent que la fonction de réajustement de SECM-do ne dégrade en rien les solutions.
Pour ces mêmes expériences, le temps CPU a été observé afin de comparer la vitesse d'exé-cution des deux algorithmes. Le tableau 4 présente les résultats obtenus. Il est ainsi aisé de voir que l'algorithme SECM-do est plus rapide que l'algorithme SECM-classic. à une forte valeur. De plus, nous avons choisi de réduire l'incertitude trouvée par la partition finale en fixant ? à une valeur élevée. Ainsi, ECM avec c = 2, ? = 3 et ? 2 = 1000 trouve la partition crédale dure représentée par la figure 2(b). Nous pouvons remarquer que ECM ne permet pas d'isoler correctement l'avion. Dans une seconde expérience, nous introduisons des contraintes sur la partition comme illustré Figure 2(c). Chaque pixel de la première (respectivement seconde) zone est affectée à ? 1 (respectivement ? 2 ). L'algorithme SECM est alors exécuté avec les mêmes paramètres que ECM. La partition crédale résultante est présentée Figure 2(d). Nous pouvons constater que les contraintes ont permis de lever l'indétermination de la plupart des pixels alloués à ?.
Conclusion
Nous avons présenté dans cet article une nouvelle méthode d'optimisation pour l'algorithme de classification automatique intitulé SECM. Ce dernier est une variante de l'algorithme évidentiel ECM prenant en compte des contraintes d'étiquettes. Il repose sur la minimisation d'une fonction objectif avec des contraintes linéaires et non linéaires, ce qui impose l'utilisation de méthodes d'optimisation avancées. De plus, l'utilisation des fonctions de masses liées aux méthodes évidentielles rend la complexité de l'algorithme linéaire par rapport au nombre d'objets et exponentielle par rapport aux nombre de classes. Nous proposons donc de relâ-cher les contraintes de positivité sur les fonctions de masses, c'est-à-dire sur les contraintes non linéaires, afin de réduire l'optimisation de la partition à la méthode des multiplicateurs de Lagrange. Le respect des contraintes de positivité est ensuite vérifié par une méthode de réajustement. Nous avons montré sur un ensemble de jeux de données que cette nouvelle technique permet non seulement d'augmenter la rapidité de SECM mais qu'elle permet également d'améliorer les performances en trouvant de meilleurs minima. Les travaux futurs porteront sur l'étude de nouveaux formalismes plus rapides permettant de conserver la majeure partie de l'expressivité des méthodes évidentielles avec une complexité largement réduite pour permettre de traiter des jeux de données avec un nombre de classes plus important.

Introduction
Les systèmes de recommandation basés sur le contenu suivent généralement un processus en deux étapes : (i) Création d'une représentation du besoin des utilisateurs ainsi que des informations à recommander. (ii) Comparaison des représentations afin d'évaluer la pertinence d'une information pour un utilisateur en fonction de son profil. Notre approche consiste à automatiser l'indexation à l'aide de processus d'inférence sur une ontologie d'indexation intégrant les vocabulaires contrôlés (e.g. thésaurus, nomenclatures, listes) définis par les documentalistes pour modéliser le domaine. Le respect de la vision métier sur le domaine permet une supervision simplifiée pour les documentalistes, garantissant la qualité de l'indexation.
Automatisation du processus d'indexation
La classification multi-label consiste à associer des étiquettes à des items (Tsoumakas et Katakis, 2007). Cet article propose une méthode pour enrichir sémantiquement une ontologie en adoptant des processus d'apprentissage automatique pour indexer et décrire l'indexation de façon à réduire l'écart entre le point de vue des experts et les règles d'indexation. L'approche proposée repose sur les quatre phases suivantes :
Phase 1 : utilisation du travail d'indexation déjà fait par les documentalistes et d'un processus d'analyse de texte pour extraire des mots-clés afin de générer une matrice qui présente la fréquence de chaque mot-clé en fonction de chaque étiquette.
Phase 2 : utilisation de la matrice afin de définir des règles capables de déterminer si un document doit être associé à une étiquette sur la base des mots-clés qu'il contient. Deux seuils de fréquence sont définis, ? et ?. Les mots-clés dont la fréquence est supérieure au seuil ? sont considérés comme des indices fiables. La présence d'un seul de ces mots est considérée comme suffisante pour que le document soit associé à l'étiquette. Le seuil de fréquence infé-rieur est ?. Dans ce cas, nous avons besoin d'une combinaison de ?-termes (dont la fréquence est supérieure à ?) pour prendre la décision d'associer un document avec l'étiquette. Plus d'informations sur les règles d'indexation peuvent être trouvées dans (Werner et al., 2014).
Phase 3 : la classification fournit deux types de résultats. Le premier est la découverte de la classe de subsomption la plus spécifique. Le second est la déduction des classes d'équiva-lence lorsque les contraintes logiques sont équivalentes. D'une part, cela signifie que lorsqu'un document est étiqueté (lors de la phase 4) par une classe qui possède des subsumants, ce document est également marqué par les classes subsumantes. D'autre part, lorsqu'un document est étiqueté avec une classe qui a des classes d'équivalence alors ce document est également étiqueté avec ces classes équivalentes. Ces deux éléments peuvent permettre la classification multi-label. De plus, sachant que les étiquettes peuvent être organisées de façon hiérarchique il peut s'agir d'un processus de classification hiérarchique multi-label (HMC).
Phase 4 : la phase de réalisation consiste à trouver toutes les classes les plus spécifiques des individus. Cette phase est mise en oeuvre par le moteur d'inférence. Les phases 3 et 4 utilisent des raisonneurs comme FaCT ++, HermiT ou Pellet.

Introduction
Un flux de données est une séquence, potentiellement infinie, non-stationnaire (la distribution de probabilité des données peut changer au fil du temps) de données arrivant en continu. Dans le cas d'un flux, l'accès aléatoire aux données n'est pas possible et le stockage de toutes les données arrivant est infaisable. Le clustering de flux de données nécessite un processus capable de partitionner des observations de façon continue avec des restrictions au niveau de la mémoire et du temps. Dans la littérature, de nombreux algorithmes de clustering de flux de données ont été adaptés à partir des algorithmes de clustering traditionnel, par exemple, la méthode DbScan (Cao et al. (2006); Isaksson et al. (2012)) basée sur la densité, la méthode de partitionnement k-means (Ackermann et al. (2012)), ou encore la méthode basée sur le passage de message AP (Affinity Propagation) (Zhang et al. (2008)). Dans cet article, nous proposons le modèle G-Stream, qui permet de découvrir des clusters de formes arbitraires dans un flux de données en constante évolution. Les caractéristiques et les principaux avantages de G-Stream sont décrits ci-dessous : (a) La structure topologique qui est représentée par un graphe dans lequel chaque noeud représente un cluster. Les noeuds (clusters) voisins sont reliés par des arêtes. La taille du graphe est évolutive. (b) L'utilisation d'une fonction d'oubli afin de réduire l'impact des anciennes données dont la pertinence diminue au fil du temps. Les liens entre les noeuds sont également pondérés. (c) Contrairement à de nombreux algorithmes qui utilisent un nombre important de données pour initialiser leur modèle, G-Stream utilise seulement deux noeuds au départ. (d) Toutes les fonctions de G-Stream sont effectuées en-ligne. (e) L'utilisation de la notion de réservoir pour maintenir, de façon temporaire, les données très éloignées des prototypes courants. L'article est organisé comme suit : d'abord, la section 2 décrit plusieurs travaux liés au problème de clustering de flux de données. Ensuite, la section 3 présente notre nouvelle approche de clustering de flux de données, appelée G-Stream. Puis, dans la section 4, nous rapportons une évaluation expérimentale. Enfin, la section 5 conclut cet article et présente nos futurs travaux de recherche.
Travaux similaires
Cette section présente un bref état de l'art qui concerne les problèmes de clustering de flux de données. Nous mettons ainsi en évidence les algorithmes les plus pertinents proposés dans la littérature pour faire face à ce problème. La plupart des algorithmes existants (par exemple, CluStream proposé par (Aggarwal et al. (2003)), DenStream de (Cao et al. (2006)), StreamKM++ de (Ackermann et al. (2012)) divisent le processus de clustering en deux phases : (a) En-ligne, dans cette phase, les données sont résumées, (b) Hors-ligne, dans cette phase, les clusters finaux sont calculés à partir de la quantification fournie par la phase en-ligne. Les deux algorithmes CluStream et DenStream utilisent une extension temporelle du Clustering Feature vector proposée par (Zhang et al. (1996)) (appelée micro-clusters) afin de maintenir des résumés statistiques sur les données ainsi que leur temps d'arrivée, ceci durant la phase en-ligne. En créant deux types de micro-clusters (potentiel et outlier micro-clusters), DenStream surmonte l'un des principaux inconvénients de CluStream, sa sensibilité au bruit. Dans la phase hors-ligne, les micro-clusters trouvés lors de la phase en-ligne sont considérés comme des pseudo-points et seront transmis à une variante de k-means dans l'algorithme CluStream (resp. une variante de DbScan dans l'algorithme DenStream), afin de déterminer les clusters finaux. StreamKM++ est une extension de l'algorithme k-means++ pour le flux de données. Les auteurs de (Isaksson et al. (2012)) ont proposé SOStream, qui est un algorithme de clustering de flux de données, basé sur la densité, inspiré à la fois du principe de l'algorithme DbScan et celui des cartes auto-organisatrices (SOM) de (Kohonen et al. (2001)). L'algorithme E-Stream, qui est proposé par (Udommanetanakit et al. (2007)), classe l'évolution des données en cinq catégories : apparition, disparition, auto-évolution, fusion et division. Il utilise une autre structure de données pour sauvegarder des statistiques sommaires, nommée ?-bin histogramme. (Zhang et al. (2008)) présentent une extension de l'algorithme Affinity Propagation pour le flux de données, appelé StrAP et qui utilise un réservoir pour maintenir d'éventuels outliers. Les auteurs de (Bouguelia et al. (2013)) ont proposé une version incrémentale de l'algorithme GNG de (Fritzke (1994)), appelée AING. où ? 1 > 0, qui est une constante définissant le taux de décroissance du poids au fil du temps. t désigne le temps courant et t 0 est le temps d'arrivée de la donnée. Le poids d'un noeud est calculé à partir des poids des données qui lui sont affectées :
où m est le nombre de données affectées au noeud c au temps courant t. Quand le poids d'un noeud est inférieur à une valeur donnée, alors ce noeud est considéré comme obsolète et sera supprimé (ainsi que ses liens). Gestion des arêtes : la procédure de gestion des arêtes effectue des opérations liées à la mise à jour des arêtes du graphe (les étapes 14-19 de l'algorithme 1). Lors de l'incrémentation de l'âge des arêtes, l'instant de création d'une arête est pris en compte. Contrairement à la fonction d'oubli, l'âge des liens sera renforcé par la fonction exponentielle f 2 (t) = 2 ?2(t?t0) où ? 2 > 0, définit le taux de croissance au temps courant t, t 0 est le temps de création de l'arête. L'étape suivante consiste à ajouter une nouvelle arête reliant les deux noeuds les plus proches. La dernière étape consiste à supprimer chaque lien dépassant un âge maximum.
Gestion du réservoir : l'objectif de l'utilisation d'un réservoir est de maintenir, temporairement, les données éloignées. Comme nous l'avons mentionné précédemment, chaque noeud a un seuil de distance. Les premières données du flux sont affectées aux noeuds les plus proches sans prendre en considération les seuils de distances. Le seuil de distance de chaque noeud est mis-à-jour en prenant la distance maximale du noeud au point le plus éloigné qui lui est affecté. Lorsque le réservoir est plein, ses données sont re-transmises à l'apprentissage. Elles sont placées au début du flux de données, DS, afin de les traiter en premier. Les seuils de distance des noeuds sont mis-à-jour en conséquence.
Évaluation expérimentale
Dans cette section, nous présentons une évaluation expérimentale de l'algorithme GStream. Nous avons comparé notre algorithme avec l'algorithme GNG ainsi qu'avec deux algorithmes pertinents de clustering de flux de données. Nos expériences ont été réali-sées sur la plateforme MATLAB en utilisant des données réelles et synthétiques. Les bases de données réelles, Shuttle (43500x9) et KddCup1 (49402x34), ont été prises à partir du répertoire UCI. Les bases DS1 (9153x2) et DS2 (5458x2) sont générées à l'aide du programme disponible sur http://impca.curtin.edu.au/local/software/ synthetic-data-sets.tar.bz2. Comme nous l'avons expliqué dans la section 3, les 
Conclusion
Dans ce papier, nous avons proposé, G-Stream, une méthode efficace pour le clustering topologique en-ligne de flux de données évolutives. Dans G-Stream, les noeuds ainsi que les arêtes composant la structure topologique sont pondérés. A partir de deux noeuds, G-Stream compare les données arrivant aux prototypes courants ; il sauvegarde celles très éloignées dans un réservoir ; il apprend les seuils de distance automatiquement ainsi que plusieurs noeuds sont créés à la fois. L'évaluation expérimentale sur des bases de données réelles et synthétiques a démontré l'efficacité de G-Stream à découvrir des clusters de formes arbitraires. Les résultats obtenus sont prometteurs. Nous envisageons à l'avenir d'appliquer le principe des fenêtres adaptatives, de rendre notre algorithme le plus autonome possible et de le développer en Spark.

Introduction
Le Web Sémantique a été lancé en 2001 par le W3C 1 pour promouvoir le partage et la créa-tion de données structurées sur le Web en proposant des recommandations pour la description de données (RDF), d'ontologies (RDFS, OWL), et des méthodes et outils associés (SPARQL, ...) pour gérer les connaissances. Actuellement le Web Sémantique correspond à des centaines de bases RDF communautaires (e.g. DBPedia Notre contribution est de fournir une méthode originale d'évaluation de mises à jour, inspirée du raisonnement par cas, utilisant exclusivement les données de la base RDF mise à jour (i.e. ne nécessitant pas l'utilisation d'une ontologie ou de méta-données). Par cette méthode, une mise à jour candidate est évaluée positivement si ses modifications dans la base RDF rendent -selon certains critères -la partie cible mise à jour dans la base plus structurellement similaire à d'autres parties de la base. Notre méthode d'évaluation de la cohérence peut être décomposée en 3 étapes : (i) extraction des contextes de la mise à jour depuis la base, (ii) ré-cupération des parties de la base similaires à la mise à jour et à ses contextes et (iii) évaluation par similarité de la cohérence des données de la mise à jour par rapport à la base.
En Section 2 nous définissons une mise à jour RDF et ses contextes (première étape de notre approche). En Section 3 nous détaillons notre méthode d'évaluation de mise à jour en définissant quelles sous-parties de la base sont prises en compte lors de l'évaluation d'une mise à jour (Section 3.1), et enfin comment évaluer la cohérence d'une mise à jour RDF par rapport à une base (Section 3.2).
Mise à jour RDF
Nous introduisons ici quelques définitions pour formaliser les notions de mises à jour RDF et de contextes associés dans une base RDF.
Nous rappelons quelques vocabulaires du Web Sémantique et introduisons quelques termes utilisés dans la suite de l'article. Ainsi, nous considérerons comme synonymes les termes document RDF, base RDF et ensemble de triplets RDF ; pour un document RDF D nous noterons R D l'ensemble des ressources -sujet, prédicat ou objet -des triplets de D ; nous appellerons une ressource noeud une ressource étant soit sujet, soit objet d'un triplet ; pour un document RDF D, nous noterons N D l'ensemble des ressources noeud de D ; nous appellerons document RDF connexe un document RDF dans lequel il y a un chemin connectant chaque ressource noeud du document à une autre, en d'autres termes si le graphe RDF représentant le document est un graphe connexe ; nous appellerons degré d'une ressource noeud le nombre de triplets la contenant dans une base RDF, en d'autres termes son degré dans le graphe RDF.
Notons aussi que implicitement nous désignons toujours les données d'une base RDF sans (avant) que les modifications d'une mise à jour ne lui soient appliquées. Enfin, toute mise à jour d'une base RDF peut être vue en tant que combinaison de deux sections : une section d'ajout qui contient ce que la mise à jour ajoute à la base et une section de suppression qui contient ce que la mise à jour supprime dans la base.
Définition 1 (Mise à jour RDF). Une mise à jour RDF u d'une base RDF B est un couple d'ensembles de triplets RDF (A, R) tels que :
Une mise à jour RDF qui ajoute des informations à une base doit apporter de nouveaux éléments liés à des données déjà existantes. Une mise à jour qui supprime des informations peut uniquement supprimer des données déjà existantes dans la base. Les sections d'ajout et de suppression ne contiennent pas de triplets en commun (l'ordre d'application de la suppression ou de l'ajout dans la base n'a pas d'importance). Une mise à jour contient nécessairement une section d'ajout et forme un document connexe (autrement il s'agit de 2 mises à jour distinctes).
De la Définition 1 nous pouvons classer les mises à jour RDF en deux catégories : les mises à jour d'ajout, définies par A = ? et R = ?, et les mises à jour de modification, définies par A = ? et R = ?. Le cas des suppressions « pure » est discuté en conclusion.
Pour comparer les données d'une mise à jour à une base RDF selon notre évaluation, nous utilisons le voisinage dans la base de toutes les ressources de la mise à jour. Ainsi les contextes d'une mise à jour sont obtenus grâce aux voisinages des sections d'ajout et de suppression.
Définition 2 (Contextes de mise à jour RDF). Soient un ensemble de triplets RDF B, une mise à jour RDF u = (A, R) candidate à B et n ? N un rang de voisinage. Soit la fonction voisinage n B (r) : N B ? B retournant tous les triplets de B connectés à r par un chemin de longueur égale ou inférieure à n, appelée fonction de voisinage de r.
Les contextes d'une mise à jour u candidate à B sont les deux ensembles de triplets RDF I u et F u définis par :
-I u appelé le contexte initial de u dans B tel que
Le contexte initial représente l'état initial de la partie de la base autour des ressources de la mise à jour candidate, le contexte final représente l'état théorique de la base si la mise à jour était appliquée. 3 Évaluer la cohérence par mesure de la similarité
Nous considérons qu'une mise à jour est cohérente avec une base si on peut trouver suffisamment de sous-parties de la base suffisamment similaires avec les contextes de la mise à jour. Nous procédons en 3 étapes : (i) recherche dans la base de sous-parties structurellement comparables aux contextes de la mise à jour, (ii) quantification de la similarité entre chaque sous-partie et les contextes de la base, (iii) conclusion sur la cohérence de la mise à jour.
Contexte initial I u1 .
Contexte final F u1 .
FIG. 2 -Contextes de u 1 (En pointillés : voisinage dans la base des ressources de u 1 )
Trouver des références dans la base
Deux ensembles de triplets RDF peuvent être structurellement comparés si leurs ressources noeuds sont liées de façon similaire, incluant (au moins) une ressource commune.
Définition 3 (Ensembles de triplets RDF comparables). Soit deux ensembles de triplets RDF connexes G et H.
G est comparable à H s'il existe une fonction de transformation f :
Deux ensembles comparables contiennent au moins une ressource commune. De plus, en théorie des graphes, on dira qu'un ensemble de triplets RDF est comparable à un autre si il est homomorphique à une partie d'un autre, sans considérer l'orientation des arcs. Comparer une mise à jour à une base entière signifie comparer structurellement les contextes initial et final de la mise à jour à chaque sous-partie de la base comparable à la mise à jour. Nous appelons références ces sous-parties de la base dépendantes de la mise à jour.
DBPedia, en tant que références pour u 1 en Fig. 1 (Élé-ments en commun avec u 1 en traits épais orange).
De la base DBPedia, deux références pour u 1 peuvent être extraites, notées D 1 et D 2 et représentées en Fig. 3. D 1 et D 2 contiennent plusieurs ressources en commun avec u 1 . D 1 suit le même modèle que u 1 avec une boisson liée à une ville liée à une personne, alors que D 2 concerne une boisson liée à une entreprise liée à une personne.
Évaluer la cohérence d'une mise à jour
Dans notre approche, si les modifications d'un mise à jour rendent la partie ciblée de la base plus similaire à d'autres parties (existantes) de la base alors nous évaluons positivement cette mise à jour.
Nous proposons d'évaluer la similarité de la mise à jour par rapport à chacune de ses réfé-rences à l'aide d'une mesure de la similarité structurelle entre deux graphes. Dans l'évaluation en Définition 5, nous supposons l'usage d'une mesure donnant un score de similarité dans R + tel que plus le score est élevé, plus la similarité est grande (un score de 0 signifie aucune similarité). Plusieurs mesures sont utilisables telles que la distance d'édition entre deux graphes, le coefficient de Jaccard, etc. . On note similarity la mesure de similarité entre deux graphes RDF avec similarity(u, La valeur de similarité seule n'importe pas dans notre évaluation, seul le signe de la diffé-rence entre l'état final et initial indique si la mise à jour apporte des informations similaires à ce qui est déjà connu.
Exemple 4. Dans cet exemple, nous choisissons d'utiliser une mesure de similarité en considérant dans chaque ensemble de triplets l'ensemble des ressources et l'ensemble des couples de ressources (sujet, relation) et (relation, objet) où le score est calculé simplement -pédago-giquement -avec similarity = +1 pour chaque ressource commune et +2 pour chaque couple de ressources communes. La différence de similarité entre la référence D 1 et les contextes de u 1 est positive (similarity(F u1 , D 1 ) ? similarity(I u1 , D 1 ) = 12 ? 11) et celle entre D 2 et les contextes de u 1 est nulle (similarity(F u1 , D 2 ) ? similarity(I u1 , D 2 ) = 9 ? 9), ainsi, avec un nombre minimum de références de 1, on a eval(u 1 , B, 2, 1) = true.
La mise à jour u 1 est donc cohérente avec la base DBPedia : les modifications de la mise à jour créent des données structurellement similaires à des parties de la base. Cette mise à jour peut être appliquée à la base.
Conclusion
Dans cet article nous proposons une approche d'intégration, ou de mise à jour, de données dans des bases RDF par une évaluation de la cohérence des mises à jour en fonction de leur

Introduction
Networks are studied in numerous contexts such as biology, sociology, online social networks, marketing, etc. Graphs are mathematical representations of networks, where the entities are called nodes and the connections are called edges. Very large graphs are difficult to analyse and it is often beneficial to divide them in smaller homogeneous components easier to handle. The process of decomposing a network has received different names : graph clustering (in data analysis), modularization, community structure identification. The clusters can be called communities or modules ; in this paper we use those words as synonyms.
Assessing the quality of a graph partition requires a modularization criterion. This function will be optimized to find the best partition. Various modularization criteria have been formulated in the past to address different practical applications. Those criteria differ in the definition given to the notion of community or cluster.
To understand the differences between the optimal partitions obtained by each criterion we show how to represent them using the same basic formalism. In this paper we use the Mathematical Relational Analysis (MRA) to express six linear modularization criteria. Linear criteria are easy to handle, for instance, the Louvain method can be adapted to linear quality functions (see Campigotto et al. (2014)). The six criteria studied are : the Newman-Girvan modularity, the Zahn-Condorcet criterion, the Owsi´nskiOwsi´nski-Zadrozny criterion, the Deviation to Uniformity, the Deviation to Indetermination index and the Balanced Modularity (details in section 3). The relational representation allows to understand the properties of those modularization criteria. It allows to easily identify the criteria suffering from a resolution limit, first discussed by Fortunato et Barthelemy (2006). We will complete this theoretical study by some experiments on real and synthetic networks, demonstrating the effectiveness of our classification. This paper is organized as follows : Section 2 presents the Mathematical Relational Analysis approach, we introduce the property of balance for linear criteria and its relation to the property of resolution limit. In Section 3, we present the six linear modularization criteria in the relational formalism. Next, Section 4 presents some experiments on real and artificial graphs to confirm the theoretical properties found previously.
Relational Analysis approach
There is a strong link between the Mathematical Relational Analysis 2 and graph theory : a graph is a mathematical structure that represents binary relations between objects belonging to the same set. Therefore, a non-oriented and non-weighted graph G = (V, E), with N = |V | nodes and M = |E| edges, is a binary symmetric relation on its set of nodes V represented by its adjacency matrix A as follows :
We denote the degree d i of node i the number of edges incident to i. It can be calculated by summing up the terms of the row (or column) i of the adjacency matrix :
N 2 the density of edges of the whole graph.
Partitioning a graph implies defining an equivalence relation on the set of nodes V , that means a symmetric, reflexive and transitive relation. Mathematically, an equivalence relation is represented by a square matrix X of order N = |V |, whose entries are defined as follows :
2. For more details about Relational Analysis theory see Marcotorchino et Michaud (1979) and Marcotorchino (1984).
Modularizing a graph implies to find X as close as possible to A. A modularization criterion F (X) is a function which measures either a similarity or a distance between A and X. Therefore, the problem of modularization can be written as a function to optimize F (X) where the unknown X is subject to the constraints of an equivalence relation 3 .
We define as well ¯ X and ¯ A as the inverse relation of X and A respectively. Their entries are defined as ¯ x ii = 1 ? x ii and ¯ a ii = 1 ? a ii respectively. In the following we denote ? the optimal number of clusters, that means the number of clusters of the partition X which maximizes the criterion F (X).
Linear balanced criteria
Every linear criterion is an affine function of X, therefore in relational notation it can be written as :
where the function ?(a ii ) depends only on the original data (for instance the adjacency matrix). In the following we will use K to denote any constant depending only on the original data.
Definition 1 (Property of linear balance) A linear criterion is balanced if it can be written in the following general form :
where ?(.) and ¯ ?(.) are non negative functions depending only on the original data and
3. In fact, the problem of modularization can be written in the general form :
subject to the constraints of an equivalence relation :
x ii ? {0, 1} Binary
The exact solving of this 0 ? 1 linear program due to the size of the constraints is impractical for big networks. So, heuristic approaches are the only reasonable way to proceed.
By replacing ¯
x by its definition 1 ? x ii , equation (4) can be rewritten as follows :
From this expression we can deduce the importance of the property of balance for linear criteria. If the criterion is a function to maximize, the presence and/or absence of the terms ? ii and ¯ ? ii has the following impact on the optimal solution :
the solution that maximizes F (X) is the partition where all nodes are clustered together in a single cluster, so ? = 1 and
) and
then the optimal solution that maximizes F (X) is the partition where all nodes are separated, so ? = N and
In other words, the optimization of a linear criterion who does not verify the property of balance will either cluster all the nodes in a single cluster or isolate each node in its own cluster, therefore forcing the user to fix the number of clusters in advance.
We can deduce from the previous paragraphs that the values taken by the functions ? and ¯ ? create a sort of balance between the fact of generating as many clusters as possible, ? = N , and the fact generating only one cluster, ? = 1.
In the following we will call the quantity N i=1 N i =1 ?(a ii )x ii the term of positive agreements and the quantity N
ii the term of negative agreements.
Different levels of balance
We define two levels of balance for all linear balanced criterion :
Definition 2 (Property of local balance) A balanced linear criterion whose functions ? ii and ¯ ? ii satisfy
where K L is a constant depending only upon the pair (i, i ) (therefore not depending on global properties of the graph) has the property of local balance.
Some remarks about definition 2 : -Since K L depends only on properties of the pair (i, i
) , that is local properties, we call this property local balance.
-When we talk about global properties we refer to the total number of nodes, the total number of edges or other properties describing the global structure of the graph. -In the particular case of local balance where
), that is ? ii and ¯ ? ii sum up to a constant, we have the following situation : whereas ? ii increases ¯ ? ii decreases and vice versa.
Let us consider the special case where ?(a ii ) = a ii , the general term of the adjacency matrix. A null model is a graph with the same total number of edges and nodes and where the edges are randomly distributed. Let us denote the general term of the adjacency matrix of this random graph ¯ ?(a ii ). A criterion based on a null model considers that a random graph does not have community structure. The goal of such a criterion is to maximize the deviation between the real graph, represented by ?(a ii ) and the null model version of this graph, represented by ¯ ?(a ii ) as shown in equation (5).
That implies
. This constraint implies that ¯ ? ii depends upon the total number of edges M . Consequently, the decision of clustering together two sub-graphs depends on a characteristic of the whole network and the criterion is not scale invariant because it depends on a global property of the graph.
The definition of null model for linear criteria can be generalized as follows :
Definition 3 (Criterion based on a null model) A balanced linear criterion whose functions ? ii and ¯ ? ii satisfy the following conditions :
where g(K G ) is a function depending on global properties of the graph K G is a criterion based on a null model. K G can be for example the total number of edges or nodes. We can deduce from definitions 2 and 3 that a linear criterion can not be local balanced and based on a null model at the same time.
In the particular case where ¯ ? decreases if the size of the network increases, it becomes negligible for large graphs. As explained previously, if this term tends to zero, the optimization of the criterion will tend to put together the nodes more easily. For instance, a single edge between two sub-graphs would be interpreted by the criterion as a sign of a strong correlation between the two clusters, and optimizing the criterion would lead to the merge of the two clusters. Such a criterion is said to have a resolution limit.
The resolution limit was introduced by Fortunato et Barthelemy (2006), where the authors studied the resolution limit of the modularity of Newman-Girvan. They demonstrated that modularity optimization may fail to identify modules smaller than a scale which depends on global characteristics of the graph even weakly interconnected complete graphs, which represent the best identifiable communities, would be merged by this kind of optimization criteria if the network is sufficiently large. According to Kumpula et al. (2007) the resolution limit is present in any modularization criterion based on global optimization of intra-cluster edges and extracommunity links and on a comparison to any null model.
In section 4 we will show how criteria having a resolution limit fail to identify certain groups of densely connected nodes.
Modularization criteria in relational notation
Graph clustering criteria depend strongly on the meaning given to the notion of community. In this section, we describe six linear modularization criteria and their relational coding in Table 1. We assume that the graphs we want to modularize are scale-free, that means that their degree distribution follows a power law.
1. The Zahn-Condorcet criterion (1785, 1964) : C.T. Zahn (see Zahn (1964)) was the first author who studied the problem of finding an equivalence relation X, which best approximates a given symmetric relation A in the sense of minimizing the distance of the symmetric difference. However the criterion defined by Zahn corresponds to the dual Condorcet's criterion (see Condorcet (1785)) introduced in Relational Consensus and whose relational coding is given in Marcotorchino et Michaud (1979). This criterion requires that every node in each cluster be connected to at least as half as the total nodes inside the cluster. Consequently, for each cluster the fraction of within cluster edges is at least 50% (see Conde-Céspedes (2013) for the demonstration).
2. The Owsí nski-Zadro? zny criterion (1986) (see Owsi´nskiOwsi´nski et Zadro? zny (1986)) it is a generalization of Condorcet's function. It has a parameter ?, which allows, according to the context, to define the minimal percentage of required within-cluster edges : ?. For ? = 0.5 this criterion is equivalent to Condorcet's criterion. The parameter ? defines the balance between the positive agreements term and the negative agreements term. For each cluster the density of edges is at least ?% (see Conde-Céspedes (2013)).
3. The Newman-Girvan criterion (2004) (see Newman et Girvan (2004)) : It is the best known modularization criterion, called sometimes simply modularity. It relies upon a null model. Its definition involves a comparison of the number of within-cluster edges in the real network and the expected number of such edges in a random graph where edges are distributed following the independence structure (a network without regard to community structure). In fact, the modularity measures the deviation to independence. As mention in the previous section, this criterion, based on a null model and it has a resolution limit (see Fortunato et Barthelemy (2006)). In fact, as the network becomes larger M ?? ?, the term ¯ ? ii = ai.a .i 2M tends to zero for since the degree distribution follows a power law.
4. The Deviation to Uniformity (2013) This criterion maximizes the deviation to the uniformity structure, it was proposed in Conde-Céspedes (2013). It compares the number of within-cluster edges in the real graph and the expected number of such edges in a random graph (the null model) where edges are uniformly distributed, thus all the nodes have the same degree equal to the average degree of the graph. This criterion is based on a null model and it has a resolution limit. indeed ? ?? 0 as N ?? ?.
The Deviation to Indetermination (2013)
Analogously to Newman-Girvan function, this criterion compares the number of within-cluster edges in the real network and the expected number of such edges in a random graph where edges are distributed following the indetermination structure 4 (a graph without regard to community structure), introduced in Marcotorchino (2013) and . The Deviation to Indetermination is based on a null model, therefore it has a resolution limit.
The Balanced modularity (2013) This criterion, introduced in Conde-Céspedes et
Marcotorchino (2013), was constructed by adding to the Newman-Girvan modularity a term taking into account the absence of edges ¯ A. Whereas Newman-Girvan modularity compares the actual value of a ii to its equivalent in the case of a random graph ai.a .i 2M , the new term compares the value of ¯ a ii to its version in case of a random graph
. It is based on a null model and it has a resolution limit.
where 4. There exists a duality between the independence structure and the indetermination structure (see Marcotorchino (1984), Marcotorchino (1985) and Ah-Pine et Marcotorchino (2007)).
The six linear criteria of Table 1 verify the property of balance, so it is not necessary to fix in advance the number of clusters, more specifically : From Tables 1 and 2 one can easily deduce that for the criteria having a resolution limit the quantity ¯ ? ii decreases when the size of the graph becomes larger.
Tests with real and artificial networks
We modularized six real networks of different sizes : Jazz (Gleiser et Danon (2003)), Internet (Hoerdt et Magoni (2003)), Web nd.edu (Albert et al. (1999)), Amazon (Yang et Leskovec (2012) 5 ) and Youtube (Mislove et al. (2007)). We ran a generic version of Louvain Algorithm (see Campigotto et al. (2014) and Blondel et al. (2008)) until achievement of a stable value of each criterion. The number of clusters obtained for each network is shown in Table 3. Table 3 shows that the Zahn-Condorcet and Owsi´nskiOwsi´nski-Zadro? zny criteria generate many more clusters than the other criteria having a resolution limit, for which the number of clusters is rather comparable. Moreover, this difference increases with the network size. Notice that the number of clusters for the Owsi´nskiOwsi´nski-Zadro? zny criterion decreases with ?, that is the minimal required fraction of within-cluster edges, so the criterion becomes more flexible.
Only ground-truth overlapping communities are defined on these previuos real networks. This fact makes difficult to judge the quality of the obtained partitions. That si why we generated five benchmark LFR graphs (see Lancichinetti et al. (2008)) of different sizes 1000, 5000, 10000, 100000 and 500000. The input parameters are the same as those considered in Lancichinetti et Fortunato (2009). The average degree is 20, the maximum degree 50, the exponent 5. the data was taken from http://snap.stanford.edu/data/com-Amazon.html. of the degree distribution is -2 and that of the community size distribution is -1. In order to test the existence of resolution limit we chose small communities sizes, ranging from 10 to 50 nodes, and a low mixing parameter, 0.10. So, the communities are clearly defined. Figure 1 shows the average number of clusters for 100 runs of the generic Louvain algorithm. Network size: N FIGURE 1 -Average number of cluster for artificial LFR graphs (logarithmic scale). Figure 1 shows clearly the difference between the behaviour of those criteria having a resolution limit (NG, DU, DI and BM) and the behaviour of criteria locally defined (ZC and OZ). As the size of the network increases the four criteria suffering from resolution-limit detect fewer clusters than those predefined. The number of clusters is rather comparable for these four functions, one reason can be the fact that the term of negative agreements tends to zero when the network gets bigger. Conversely, the criteria locally defined identified more clusters than those predefined, specially ZC. The criterion which best approaches the real number of clusters is OZ with ? = 0.2. Figure 2 shows the average Normalized Mutual Information for the partitions in Figure 1. Figure 2 shows that the average NMI decreases with the network size for criteria having a resolution limit. The criterion with the highest NMI is OZ with ? = 0.2 which guarantees an within-cluster density of 20%.
Number of clusters
Conclusions
We presented six linear modularization criteria in relational notation, Zahn-Condorcet, Owsi´nskiOwsi´nski-Zadro? zny, the Newman-Girvan modularity, the Deviation to Uniformity index, the Deviation to Indetermination index and the Balanced-Modularity. This notation allowed us to easily identify the criteria suffering from a resolution limit. We found that the first two criteria had a local definition whereas the others, based on a null model, had a resolution limit. These findings were confirmed by modularizing real and artificial graphs using a generic version of the Louvain algorithm. We compared the number of clusters found by the six criteria and the Normalized Mutual information for artificial graphs. The results showed that those criteria ba-

Introduction
En apprentissage automatique, la précision et le rappel sont des mesures classiques pour évaluer les résultats et la performance des algorithmes utilisés. Ces mesures sont essentiellement utilisées en apprentissage supervisé (Sokolova et al., 2006), en classification simple (Jain, 2010) et croisée Hanczar et Nadif (2013) et en recherche d'information (Manning et al., 2008). Dans ce dernier cas, la performance de l'algorithme employé est évaluée à partir de la similarité entre l'ensemble de documents retrouvés et l'ensemble des documents cibles. Cette similarité se base sur la précision et le rappel. De la même manière en classification simple (resp. croisée), les algorithmes identifient des groupes (resp. biclusters) d'éléments qui sont comparés à des groupes (resp. biclusters) de référence. En apprentissage supervisé, l'évaluation d'un classeur se fait en comparant les classes prédites avec les vraies classes sur un ensemble de test. On mesure la similarité entre les classes prédites et les vraies classes en calculant leur préci-sion et rappel. Cependant cette approche ne tient pas compte du taux de vrais négatifs. Pour ces raisons, on préfère dans certains cas utiliser le couple sensibilité-spécificité que le couple précision-rappel dans ce contexte. La précision et le rappel sont donc deux mesures très utilisées dans les procédures d'évaluations de nombreux domaines. Il est extrêmement fréquent de combiner ces deux valeurs afin de construire des indices de performance tel que la F-mesure ou l'indice de Jaccard (Albatineh et Niewiadomska-Bugaj, 2011).
Par défaut les indices de performance donnent la même importance à la précision et au rappel. Or dans de nombreux cas, on peut vouloir privilégier l'un par rapport à l'autre. Par exemple, en génomique des groupes de gènes ayant des profils d'expression similaires sont identifiés en utilisant des méthodes de classification. Ces groupes sont comparés à des classifications de gènes issues de bases de connaissance afin d'estimer leur pertinence biologique (Datta et Datta, 2006). L'objectif de ces analyses est de capturer le plus d'information biologique dans les groupes de gènes, on veut donc privilégier le rappel par rapport à la précision dans ce contexte. Certains indices de performance ont une variante introduisant un paramètre permettant de contrôler le compromis précision-rappel comme c'est le cas de la F-mesure qui est une généralisation de l'indice de Dice. Pour d'autre mesures, le contrôle du compromis précision-rappel est plus difficile, comme c'est le cas de l'indice de Jaccard. Dans cet article nous analysons les différents indices de performance en fonction du compromis précision-rappel. Nous proposons également un nouvel outil d'analyse qu'est l'espace de compromis qui présente de nombreux avantages par rapport à l'espace précision-rappel.
Dans la section 2, nous présentons les différents indices de performance étudiés ainsi que leurs variantes sensibles au compromis. Dans la section 3, nous rappelons les propriétés de l'espace précision-rappel. Nous analysons le comportement des différents indices dans cet espace. Dans la section 4, nous définissons l'espace de compromis et nous montrons comment représenter les performances par les courbes de compromis. Dans la section 5, nous montrons les avantages à travailler dans l'espace de compromis en particulier pour la sélection de modèles et la comparaison d'algorithmes. Nous illustrons ces propriétés avec un exemple dans le contexte du biclustering. Dans la section 6, nous exposons nos conclusions et perspectives.
2 Indices basés sur le couple précision et rappel
Définitions
Soit D un ensemble des données contenant N éléments. Nous appelons groupe cible le sous-ensemble T ? D que nous recherchons. Un algorithme dont l'objectif est de retrouver le groupe cible produit un groupe X. Pour mesurer la qualité de ce groupe X, un indice de performance est utilisé afin d'évaluer la similitude entre T et X. Ces indices de performances sont généralement basés sur deux valeurs : la précision et le rappel. La précision représente la proportion de X qui recouvre T quant au rappel il exprime la proportion de T retrouvé par X. Ces deux indices prennent les formes suivantes :
Les principaux indices de performances utilisés sont une combinaison de la précision et du rappel. Dans cet article nous étudierons les quatre plus populaires : l'indice de Kulczynski, Fmesure, Folke et Jaccard. Ces travaux pourront être facilement étendus à d'autres indices. Par défaut chacun de ces indices donne la même importance à la précision et au rappel. Cependant on peut construire des versions pondérées permettant de privilégier la précision par rapport au rappel ou inversement.
L'indice de Kulczynski
L'indice de Kulczynski est la moyenne arithmétique de la précision et du rappel.
Une version pondérée introduit le paramètre R ? [0, +?] qui permet de contrôler le compromis entre la précision et le rappel. Plus R est grand, plus le rappel est important, le point d'équilibre est atteint pour R = 1. Nous réécrivons cet indice en effectuant le changement de variable suivant : ? = R R+1 , ? ? [0, 1] contrôle désormais le compromis et le point d'équilibre est atteint pour ? = 0.5.
La F-mesure
La F-mesure, appelée aussi indice de Dice, est le rapport entre l'intersection et la somme des tailles du groupe X et du groupe cible T . C'est aussi la moyenne harmonique entre la précision et la rappel. 
L'indice de Folke
L'indice de Folke correspond à la moyenne géométrique de la précision et du rappel.
Il est possible de pondérer le moyenne géométrique en introduisant un paramètre ? ? [0, 1]. Plus ? est grand plus le rappel est important et le point d'équilibre est atteint pour ? = 0.5.
L'indice de Jaccard
L'indice de Jaccard est le rapport entre l'intersection et l'union du groupe X et le groupe cible T .
Il n'est pas facile de définir une version pondérée de l'indice de Jaccard à cause de la pré-sence du terme pre.rec au dénominateur. Nous voulons un indice pondéré ayant les proprié-tés suivantes : I Jac (T, X, ?) ? [0, 1] ; I Jac (T, T, ?) = 1 ; I Jac (T, X, 0.5) = I Jac (T, X) ; I Jac (T, X, 0) = pre ; I Jac (T, X, 1) = rec. Pour cela nous proposons l'indice suivant :
3 L'espace précision-rappel L'espace précision-rappel, illustré dans la figure 1, est un espace à deux dimensions dans lequel les abscisses et ordonnées représentent respectivement le rappel et la précision (Buckland et Gey, 1994). Une performance est représentée par un point dans cet espace (le point blanc par exemple). Le principe de l'espace précision-rappel est proche de celui de l'espace ROC qui représente le taux de vrais positifs en fonction du taux de faux positifs (Fawcett, 2006). Plusieurs relations ont d'ailleurs été identifiées entre ces deux espaces (Davis et Goadrich, 2006). Un point dans l'espace précision-rappel représente tous les groupes de taille |X| = |T | rec pre ayant une intersection avec le groupe cible de |T ? X| = |T |rec. Le point (1,1) (point noir), maximisant la précision et le rappel, représente le groupe idéal et dans ce cas il y a une parfaite correspondance avec le groupe cible (X = T ). Le point (1, |D| rec puisqu'on a |D| ? |X|. Tous les groupes dont la performance se situe sur la droite pre = |T | |D| rec sont ceux dont |T ? X| est minimale. Cette droite représente tous les groupes dont |T ? X| est nulle. La plupart des algorithmes a un paramètre permettant de contrôler la taille du résultat X. Pour chaque taille de X on obtient des valeurs de précision et rappel différentes. La performance d'un algorithme peut donc être représentée par un ensemble de points et approximée par une courbe dans l'espace précision-rappel. Dans la figure 1, on donne un exemple de courbe précision-rappel. On peut tirer plusieurs informations sur les performances de ces différents groupes même sans se référer à un indice en particulier. Si un point domine un autre, c-à-d si sa précision et son rappel sont supérieurs, alors on peut conclure qu'il aura une meilleure performance quelque soit l'indice utilisé. Les points noirs représentent les points dominants de la courbe, il ne sont dominés par aucun autre point et représentent les performances des meilleurs groupes. Il n'y a pas de rapport de domination entre ces types de points, il est nécessaire d'utiliser un indice pour les comparer.
Le comportement des différents indices de performances peut se visualiser en dessinant leur iso-ligne dans l'espace précision-rappel. Une iso-ligne est un ensemble de points dans l'espace précision-rappel ayant tous la même valeur d'indice (Flach, 2003;Hanczar et Nadif, 2013). La figure 2 montre les iso-lignes des indices de Kulczynski, F-mesure, Folke et Jaccard. Les lignes en gras représentent les iso-lignes lorsque ? = 0.5. Pour les quatre indices, nous observons que les iso-lignes ont une symétrie autour de l'axe pre = rec, ceci signifie que la précision et le rappel ont la même importance. Par contre les différents indices ne considèrent pas la différence entre précision et rappel de la même façon. Cette différence n'est pas prise en compte dans l'indice de Kulczynski, alors que les autres indices la pénalisent. L'indice de Folke pénalise moins que la F-mesure et l'indice de Jaccard. Ces deux derniers sont équivalents car ils sont compatibles,
Dans la figure 2, les lignes en pointillées représentent les iso-lignes pour ? = 0.2 et les lignes pleines ? = 0.8. La modification de la valeur de ? déforme les iso-lignes, ce qui permet de donner plus d'importance à la précision ou au rappel. A noter que pour pre = rec les indices de Kulczynski, F-mesure et Folke retournent la même valeur quelque soit ?. L'indice de Jaccard a un comportement différent, il pénalise le fait que ? s'approche de 0.5. Dans la L'espace de compromis, que nous proposons, offre un nouvel outil de visualisation des performances des résultats ou des algorithmes en fonction du compromis précision-rappel. Il y a certaines similitudes avec les "cost curves" utilisées en apprentissage supervisé (Drummond et Holte, 2006). L'espace de compromis représente en abscisse ? et en ordonnée l'indice de performance. La performance d'un groupe X est représentée dans cet espace par une courbe f (?). On a une correspondance entre les points de l'espace précision-rappel et les courbes Cette dernière courbe définit le domaine d'application des indices de performances pour un problème donné, illustré dans la figure 3 par les zones blanches. Un point situé dans l'une des zones grises, signifie que le groupe correspondant à de moins bonnes performances que le groupe maximal et peut donc être considéré comme non informatif. On constate que le domaine d'application de l'indice de Kulczynski est beaucoup plus petit que celui des autres indices. Cela est dû au fait que cet indice ne pénalise pas la différence entre précision et rappel. La ligne en pointillé représente le groupe contenant un unique élément appartenant au groupe cible. Le groupe parfait est représenté par la droite f (?) = 1. A l'inverse les groupes ayant une intersection nulle sont représentés par la droite f (?) = 0. Les groupes aléatoires sont représentés par les courbes partant du point (0, |T | |D| ).
Courbe optimale de compromis
Comme nous l'avons illustré dans la figure 1, la performance d'un algorithme peut être représentée par une courbe dans l'espace précision-rappel. A chaque point de cette courbe correspond une courbe dans l'espace de compromis. On peut représenter la courbe précision-rappel par un ensemble de courbes dans l'espace de compromis. La figure 4 donne la représentation de la courbe précision-rappel de la figure 1 dans l'espace de compromis pour les différents indices de performance. On s'intéressera particulièrement à l'enveloppe supérieure de cet ensemble de courbes, représentée en gras dans la figure 4 que nous appellerons courbe optimale de compromis. Cette dernière représente les meilleurs performances de l'algorithme pour tous les compromis. On s'aperçoit que les courbes formant l'enveloppe supérieure correspondent tous à des points dominants de la courbe précision-rappel. Les points dominés ont toujours leur courbe en dessous de la courbe optimale de compromis. Dans le cas de l'indice de Kulczynski, les courbes formant l'enveloppe supérieure correspondent aux points de l'enveloppe convexe de la courbe précision-rappel. Ces courbes de compromis permettent d'analyser les résultats bien plus facilement que les courbes précision-rappel. 5 Application des courbes de compromis 5.1 Sélection de modèles L'utilisation de l'espace de compromis permet d'identifier très facilement le résultat optimal pour un compromis donné. Ceci est illustré dans la figure 5 à travers un problème de classification croisée. Nous avons généré une matrice de données aléatoires dans laquelle un bicluster a été introduit, ce dernier suit un modèle additif selon la définition de Madeira et Oliveira (2004). Nous utilisons l'algorithme CC (Cheng & Church) pour retrouver ce bicluster (Cheng et Church, 2000). La similarité entre le bicluster retourné par l'algorithme et le bicluster recherché est alors calculée par les différents indices de performance. Cet algorithme dispose d'un paramètre permettant de contrôler la taille du bicluster retourné, nous pouvons donc représenter les performances de cet algorithme par une courbe précision-rappel (figure 5). A partir de cette courbe il n'est pas facile de déterminer le meilleur bicluster pour un compromis de précision-rappel donné. Même en ajoutant les iso-lignes au graphique, la comparaison des différents biclusters n'est pas intuitive. Dans la figure 5 est représentée la courbe optimale de compromis pour la F-mesure. A partir de cette courbe on peut instantanément identifier le meilleur bicluster pour un compromis donné. On a aussi une décomposition de la valeur de ? en une série d'intervalles qui sont délimités sur le graphique par les lignes verticales pointillées, pour lesquels le meilleur bicluster est donné. Sur notre exemple on constate qu'il y a sept intervalles, nous nous intéresserons donc qu'aux sept biclusters correspondants, identifiés sur la figure par leur taille. Pour le dernier intervalle (? > 0.74) le meilleur bicluster est la matrice entière, la courbe optimale de compromis est confondue avec la courbe du bicluster maximal. Notons qu'il n'est pas possible d'identifier visuellement ces biclusters dans l'espace précision-rappel car ils ne correspondent ni à l'ensemble des points dominants ni à l'enveloppe convexe de la courbe précision-rappel (sauf dans le cas de l'indice de Kulczynski). Il est également très facile de travailler avec des contraintes sur la précision ou le rappel dans l'espace de compromis. Nous rappelons que la précision et le rappel se lisent à l'extrémité de chaque courbe de compromis. Lorsqu'on demande une précision minimale pre min , il suffit de considérer unique les courbes de compromis qui partent au-dessus du seuil minimum c-à-d f (0) > pre min . De même avec un rappel minimum rec min , on ne conserve que les courbes qui arrivent au-dessus du seuil de rappel c-à-d f (1) > rec min .
Comparaison d'algorithmes
L'espace de compris simplifie également grandement la comparaison des algorithmes. Nous reprenons l'exemple de classification croisée précédent dans lequel un autre algorithme, ISA (Bergmann et al., 2003), est testé et comparé à CC. Les performances de ce nouvel algorithme sont représentées dans l'espace précision-rappel et l'espace de compromis par la courbe grise dans la figure 6. Dans l'espace précision-rappel les deux courbes se croisent plusieurs fois, aucun des deux algorithmes n'est donc absolument meilleur que l'autre. Il est difficile de voir dans quelles conditions CC est meilleur que ISA et inversement. Dans l'espace de compromis on visualise immédiatement quel est le meilleur algorithme pour chaque valeur de compromis. Pour ? < 0.28 CC est meilleur que ISA, pour 0.28 < ? < 0.83, ISA est meilleur, pour ? > 0.83 les deux algorithmes retournent un bicluster contenant toute la matrice de donnée et ont donc des performances identiques. La distance entre les deux courbes permet de FIG. 5 -Identification des meilleurs biclusters dans l'espace précision-rappel et l'espace de compromis. A gauche, la courbe précision-rappel. A droite, la courbe optimale de compromis.
visualiser la différence de qualité entre les deux algorithmes. Dans l'espace précision-rappel les courbes des deux algorithmes se croisent trois fois, laissant penser qu'il y a deux intervalles de ? pour lesquelles CC est meilleur (de même pour ISA). Les courbes optimales de compromis montrent que l'identité du meilleur algorithme ne change qu'une fois, en ? = 0.28. Dans l'espace précision-rappel, CC a une meilleurez précision que ISA ; 14 fois sur 20 ce qui laisse penser que CC est plus souvent meilleur que ISA. L'espace de compromis nous montre qu'au contraire l'intervalle [0, 0.28] pour lequel CC est meilleur est deux fois plus petit que celui de ISA [0.28,0.83]. Cet exemple illustre bien la facilité de la comparaison d'algorithmes dans l'espace de compromis.
FIG. 6 -Identification du meilleur algorithme. A gauche, les courbes précision-rappel. A droite, les courbes optimales de compromis.

Introduction
Les requêtes skyline sont importantes dans les applications qui nécessitent la localisation des réponses selon plusieurs critères. Ayant un ensemble de points dans un espace vectoriel de d dimensions, un algorithme traitant ce type de requêtes doit retourner l'ensemble des points de S dits non dominés. Il est meilleur pour ce type d'algorithmes de fonctionner progressivement Kossmann et al. (2002) car les utilisateurs sont souvent impatients de recevoir des réponses. La relation de dominance se définit comme suit Börzsonyi et al. (2001) : Soit S un ensemble de données de d dimensions (d critères) sur lequel va porter l'opérateur skyline. Soit D l'ensemble de toutes les dimensions D = {d 1 , , d d }. Soient p et q deux points de S. La relation de dominance (?) suivant D est pour 1 ? i, j ? d ,p domine q ?? {?d i ? D, p(i) ? q(i)} et {?d j ? D, p(j) < q(j)}. Lorsque aucun point ne domine l'autre on dit qu'ils sont non dominés ou concurrents. L'opérateur skyline renvoie l'ensemble des points concurrents, suivant toutes les dimensions D : SkyD(S) = {p ? S/ ? S : q ? p} Dans ce papier, nous présentons une solution analytique pour déduire l'ensemble de points candidats afin d'éviter de parcourir l'ensemble S entièrement. Nous donnons un nouveau théo-rème pour l'élimination des points non candidats. Notre méthode est basée sur le tri Tan et al. (2001) et à la différence avec ce travail, où les tests entre les points balayent tout l'ensemble S, nous donnons des théorèmes pour la déduction des points les plus évidents et qui constituent les premières solutions à présenter. Nous montrerons que la combinaison de DC Divide-andConquer avec notre méthode fournit des résultats meilleurs que lorsqu'il est appliqué tout seul. Le reste de ce papier se présente comme suit. La section 2 présente les travaux liés à cette problématique. La section 3 donne notre approche. La section 4 présente les résultats des expérimentations. La conclusion et les travaux futurs sont donnés dans la section 5. Börzsonyi et al. (2001) était le premier travail ayant adapté l'optimisation au sens de Pareto dans les bases de données. Intuitivement, le calcul du skyline consiste à comparer chaque point p avec tous les autres et si aucun point ne le domine alors p est un point skyline. L'algorithme BNL Börzsonyi et al. (2001) utilise cette technique directe. Il met en mémoire une liste candidate et teste à chaque fois si un nouveau point p domine un ou plusieurs points déjà insérés. Si c'est le cas, il est inséré et l'ensemble des points dominé est écarté sinon il passe au point suivant. Cette méthode peut être utilisée facilement et ne requiert pas de prétraitement, sauf qu'elle est gourmande en mémoire et en temps de calcul. DC Börzsonyi et al. (2001) divise l'entrée en plusieurs partitions et détermine le skyline de chaque partition. Par la suite, les skyline sont fusionnés et les points dominés sont écartés. Cette méthode est meilleure que BNL mais souffre des multiples duplications lors de la fusion. Notons que ni BNL ni DC ne fonctionnent en on-line. L'algorithme Bitmap Tan et al. (2001) consiste à encoder dans des vecteurs bitmap toutes les informations de chaque point selon le nombre de points distincts sur chaque axe. La comparaison des vecteurs bitmap se fait par la suite. Bitmap est progressive mais nécessite beaucoup d'opérations et de codage en commençant par la détermination des points distincts dans chaque axe car il y aura beaucoup de tests dupliqués. Index Tan et al. (2001) consiste à trier les données sur chacun des d axes dans un ordre croissant. Afin de déter-miner le skyline, les points sont testés de façon circulaire. Le problème est que la récupération des coordonnées des points peut prendre du temps. Cette méthode est bien adaptée pour les applications on-line, elle retourne aussitôt les premiers points, sauf que les auteurs ne déduise pas les skyline induit par le tri. NN Kossmann et al. (2002) est un algorithme qui utilise les RTrees pour indexer les données. Il partitionne l'espace selon chaque axe selon le point le proche voisin de l'origine. NN est progressif et est efficace dans un espace à deux dimensions, mais il souffre du problème de duplications des éliminations pour 3 dimensions et plus. A partir de 4 dimensions, il devient difficile de l'appliquer. Les auteurs proposent différentes techniques pour remédier à ces problèmes. Branch and Bound Skyline Papadias et al. (2003) exploite les R-Tree, la méthode de Branch and Bound et NN afin de calculer, en on-line, le skyline. Son plus grand problème est qu'il souffre de requêtes redondantes. Yuan et al. (2005) proposent Skycube. Il calculent les skyline fils de toutes les combinaisons possibles des points dans le treillis. Lorsqu'ils passent au niveau supérieur ou inférieur du treillis ils fusionnent ces fils.
Travaux liés
La méthode DCRD
Notre méthode DCRD pour Divide-and-Conquer for Reduced Data est une méthode analytique qui détermine l'espace candidat en se basant sur le tri. L'utilisation directe de l'espace Pareto est simple pour un espace à 2 dimensions, mais au-delà de 3 dimensions, il faut ajouter des méthodes efficaces pour calculer cet espace. Nous donnons un nouveau théorème qui permet de déduire cet espace. Dans Index, les tests de dominance se font entre tous les points sans l'exploitation de la concurrence induite par le tri. Par exemple, il est impossible qu'un point A, ayant la valeur minimale unique sur un axe X, soit dominé par un autre point. Ce qui nous mène à donner le théorème 1. Preuve. Suite à la discussion précédente, sur l'axe i, il est impossible qu'un autre point puisse dominer le point p, puisqu'aucune valeur sur cet axe ne sera inférieure à celle de p. Si pour tous les autres axes, le point q domine p, il sera impossible qu'il le domine sur i, d'où p est soit concurrent avec q soit le domine. Définition du conflit entre les points ayant des valeurs minimales sur le même axe. Il est fréquent que deux points ou plus aient la même valeur minimale sur un axe. Ainsi, il faut résoudre ce conflit de dominance avant de passer au calcul du skyline définitif. Ainsi, nous donnons le théorème 2 suivant : Théorème 2 : Existence de plusieurs points minimaux sur un axe i Soient p et q deux points de S tel qu'il existe un axe i avec p Preuve. Ceci revient à résoudre le conflit entre p et q dans les autres sous-espaces. Le test montrera soit la dominance soit la concurrence entre eux sur les autres axes.
Phase 1 : Tri des données et déduction des premiers points skyline
trier les valeurs de S par ordre croissant ; -Extraire l'ensemble Sky des premiers points skyline en utilisant les théorèmes 1 et 2 ;
Phase 2 : Réduction de l'espace de données
Cette phase consiste à limiter l'espace de données en filtrant l'ensemble de données selon deux points appelés M in sys et M ax sys , autrement dit, on détermine l'espace de dominance dans lequel se trouvent tous les points candidats. M in sys est le même que le point idéal de Pareto. Le point M ax sys est un point virtuel qui permet de délimiter cet espace. Il sert à éli-miner un espace important non utile. En deux dimensions, il se confond au point nadir mais, à plus de dimensions, ils sont différents. Le point nadir est un point pour lequel la fonction à optimiser est maximale ? et ce n'est pas notre cas car M ax sys est dominé. D'une manière analytique, nous déterminons les points M in sys et M ax sys à partir des coordonnées des points skyline déduits de la phase précédente : M in sys et M ax sys possède chacun d composantes. Chaque composante M in sys [i] (resp. M ax sys [i]) de M in sys (resp. M ax sys ) est la valeur minimale(resp. maximale) de toutes les composantes des points de Sky. Calcul de l'espace des candidats. Dans cette étape, l'ensemble des données candidats est ré-duit à un hypercube. Pour l'obtenir, nous énonçons et appliquons le théorème3 suivant :  
. p ne peut dominer q sur l'axe d. Donc, il y a au moins un point m appartenant au skyline actuel qui domine p.
Phase 3 : Calcul du skyline final
A l'issue de la phase 2, l'espace déduit contient les points candidats. Il ne reste que de les comparer entre eux et éliminer les points dominés, nous appliquons ainsi DC.
Expérimentations
Les expérimentations ont été réalisées dans une machine dotée d'Intel Core i5 2,50 GHz, et de 4 Go de RAM, sous Windows 7, 64 bits. Le programme est écrit sous Java. MySQL 5.1.41 est utilisé comme système gestion des bases de données. Nous avons utilisé les mêmes bases de données synthétiques de Börzsonyi et al. (2001)  Kossmann et al. (2002). Il s'agit de trois types de données : corrélées, anti-corrélées et indépen-dantes. Nous avons calculé le temps nécessaire en secondes pour retourner le skyline en tenant compte de la dimensionalité et la cardinalité.
Comparaison entre DC et DCRD dans le type anti-corrélé. En fixant la dimension à 5 et en variant la cardinalité de 10000 à 100000 points. La figure 1 montre que pour ce type de données, DCRD a rendu les résultats dans des temps meilleurs puisqu'il y a eu des éliminations de points inutiles. Pour 10000 points et en variant la dimension de 1 à 10, nous remarquons sur la figure 6 que DCRD a commencé à rendre les réponses rapidement à partir de d=6. On déduit ainsi que le Comparaison entre DC et DCRD dans le type corrélé Ce type de données est intéressant et facile à manier ; même un algorithme naïf pourrait présenter de bonnes performances. En comparant ces deux méthodes sur la cardinalité et en fixant le nombre d'attributs à 5, nous remarquons sur la figure 3 que DCRD a dépassé de loin DC. Ceci est dû au nombre important de points qui ont été éliminés. La faiblesse de DC est qu'il exécute des tests de dominance sur tout l'ensemble de données d'une façon aveugle. De même, en comparant DC et DCRD selon la dimension, nous avons fixé la cardinalité à 10000 et nous avons varié le nombre d'attributs. Puisque le nombre de points éliminé est important, la figure 4 montre que DCRD a été très rapide alors que DC a consommé plus de temps. Ceci est toujours le cas puisque DC ne fait aucun traitement préalable et exécute des partitions sur tout l'ensemble d'entrée. Ceci a un effet sur les temps de réponse.

Introduction
L'explosion d'internet, couplée à l'effet de la mondialisation, a pour résultat d'interconnecter les personnes, les entreprises, les états. Le côté déplaisant de cette interconnexion mondiale des Systèmes d'Information réside dans un phénomène appelé "Cybercriminalité". Des personnes, des groupes mal intentionnés ont pour objectif de nuire dans un but pécuniaire ou pour une "cause", aux informations d'une entreprise, d'une personne voire d'un Etat. Il n'est pas rare que des faits de "cyber-attaques" soient relatés dans les médias envers des grandes socié-tés comme "Google","Visa","Sony", "Apple". La sécurité d'un Système d'Information se doit d'être présente afin de garantir la confidentialité, l'intégrité, la disponibilité de l'information. De ce fait, il existe une multitude d'équipements de sécurité qui permettent de détecter les comportements anormaux. Un des principaux équipements de sécurité est le "Pare-Feu" 1 ou plus communément appelé "Firewall". Il a pour mission comme le décrit Al-Shaer et Hamed (2003) de filtrer , selon une politique fondée sur les flux autorisés à pénétrer dans un réseau selon leurs sources, leurs destinations et les services souhaités (navigation internet, transfert de fichiers, etc... ). Par son positionnement, il donne une visibilité totale de l'ensemble des flux. Cet équipement offre aussi la possibilité "d'historiser" vers des journaux les flux ayant été autorisés ou interdits. L'exploitation et l'analyse des journaux d'événements liés aux équi-pements de sécurité sont devenues primordiales pour la maîtrise des flux et la détection des intrusions ainsi que pour la vérification du bien fondé de la politique de filtrage mise en place (Golnabi et al, 2006). Dans ce contexte, les constructeurs d'équipements de filtrage mettent à disposition des logiciels permettant d'analyser les flux. Ces derniers nécessitent un accès et une connaissance dudit équipement. La détection des anomalies et des comportements anormaux est conséquemment réservée à ces seuls utilisateurs. La problématique de la représentation des événements de sécurité est tellement répandue que plusieurs outils ont même été regroupés au . Le principe est de modéliser un système de "monitoring" et de visualisation des données réseau en temps réel permettant de détecter rapidement les tentatives d'intrusions.
Composition du projet "D113"
Le projet "D113" est composé de quatre phases qui s'inscrivent dans le cadre d'un travail de thèse en sécurité s'appuyant sur des données issues des différents équipements et outils de sécurité. Les différentes phases du projet se déclinent selon la liste suivante.
- Notre démonstration de logiciel portera uniquement sur la phase 1 qui est le préambule à la "fouille de données" qui sera effectuée dans les phases suivantes. La première phase constitue un tout en soi dans la mesure où la visualisation des données pour les utilisateurs est un enjeu crucial en termes de prise de décisions sur les problématiques de sécurité. Ces trois sites sont opérationnels, c'est à dire que les données traitées et analysées dans les sections suivantes correspondent à des données de production. Pour des raisons de confidentialité les adresses IP ont été anonymisées. Le réseau SP1 est doté de son propre conteneur de données qui est alimenté par les événements envoyés en temps réel par le "Firewall". Les réseaux SAB1 et SQ1 mutualisent un même conteneur. Les données brutes envoyées par l'ensemble des équipements filtrants sont traitées selon une extraction de motifs.
Description des données
Le contenu des variables listées ci-dessous sont exportées vers des conteneurs de données. 
Conclusions et perspectives
A l'issue de la phase 1, l'ensemble des événements liés au filtrage est exporté en temps réel vers des conteneurs de données. Dans un souci de performance et compte-tenu de l'importance

Introduction
Les travaux de cette dernière décennie dans le domaine de la découverte de connaissances, comme ceux notamment de ) et de (Tiwari et al. (2010)), témoignent du vif intérêt pour le problème de l'extraction des ensembles d'items fré-quents, des motifs séquentiels, des motifs structurels (dans les données de type arbres, graphes ou treillis), et la recherche de méthodes efficaces pour extraire ces motifs. Dans cet article, nous nous intéressons à la notion de proportion analogique, essentiellement étudiée dans le domaine de l'intelligence artificielle, pour extraire de nouveaux types de motifs dans les bases de données. Les proportions analogiques relient quatre objets A, B, C, D du même type dans une assertion de la forme « A est à B ce que C est à D ». Ils permettent d'exprimer l'identité (ou la proximité) des rapports existant entre deux paires d'éléments. Des exemples typiques de cette notion en langage naturel sont : « le veau est à la vache ce que le poulain est à la jument », « l'aurochs est au boeuf ce que le mammouth est à l'éléphant ». Ces relations permettent d'exprimer que ce qui distingue A de B est comparable à ce qui distingue C de D. Les exemples ci-dessus montrent la diversité (et la potentielle complexité) des sémantiques possibles du connecteur « est à » intervenant dans une proportion analogique. Dans le premier exemple, ce connecteur représente une relation de filiation tandis que dans la seconde, il exprime une évolution possible. Le connecteur « ce que » de la proportion représente généralement l'identité ou la similarité. Quand les éléments A, B, C et D sont des valeurs numériques, la relation peut être définie en utilisant les proportions mathéma-tiques classiques, comme la proportion géométrique : A/B = C/D (e.g., 1/3 = 2/6) ou la proportion arithmétique : A ? B = C ? D (e.g., 5 ? 3 = 9 ? 7). Quand les objets A et B, resp. C et D, représentent les mêmes entités à différents moments ou états de leur vie (par exemple, A et B décrivent le même endroit à deux moments différents), la proportion analogique peut exprimer des évolutions similaires. De manière générale, les proportions analogiques permettent de trouver des parallèles entre quatre événements ou situations.
Nous cherchons à exploiter la notion de proportion analogique dans le contexte des bases de données relationnelles afin d'extraire des combinaisons de quatre n-uplets liés par une telle relation. Notre objectif est de découvrir des parallèles entre des paires de n-uplets, i.e., des paires d'éléments qui sont dans les mêmes rapports. Ces parallèles ne reflètent pas forcément une relation de proximité (A est aussi proche de B que C est proche de D), mais plutôt une transformation semblable (On passe de A à B comme on passe de C à D). Ces parallèles sont d'une importance majeure puisqu'il permettent de modéliser des règles d'évolution reproductible dans les systèmes écolo-giques (les états de deux littoraux qui évoluent dans les mêmes directions : apparition et disparition des mêmes espèces, évolution d'une pollution d'une région à une autre), des mouvements sociétaux (extension d'une crise géopolitique ou comparaison avec des successions d'événements passés), ou des déplacements parallèles d'objets.
Les contributions de cet article sont les suivantes. Nous proposons une méthode pour identifier les proportions analogiques dans les bases de données. À cette fin, nous suivons une approche vectorielle pour définir la notion de proportion analogique adaptée au modèle relationnel. Puis nous montrons qu'il est possible de ramener le problème d'énumération de toutes les combinaisons de quatre n-uplets liés par une relation d'analogie, à un problème de clustering moyennant un prétraitement et l'utilisation d'une métrique. Ceci permet de rassembler des paires d'éléments qui définissent des vecteurs égaux ou presque. Nous analysons ensuite les résultats de notre approche appliquée à un jeu de données réelles.
Notre article est organisé comme suit. En section 2, nous introduisons la notion de proportion analogique et nous proposons une définition graduelle de celle-ci adaptée au contexte des bases de données. La section 3 présente notre approche de découverte des proportions analogiques et l'algorithme qui en découle, tandis que la section 4 détaille les expérimentations effectuées. Enfin, nous présentons les travaux relatifs à notre proposition (Section 5) puis nous concluons (Section 6).
Proportions analogiques et modélisation
Les proportions analogiques
Cette section s'appuie sur les références (Miclet et Prade (2009)) et (Lepage (2012)). Une proportion analogique est une assertion de la forme « A est à B ce que C est à D», notée par la suite Un algorithme naïf pour énumérer les proportions analogiques issues d'un ensemble d'objets de cardinalité n, a une complexité temporelle en n 4 . En utilisant un point de vue vectoriel de la notion de proportion analogique, les objets A, B, C et D désignent des points d'un espace à n dimensions. S'ils forment une relation d'analogie alors ces points forment un parallèlogramme. Par exemple, la figure 1 montre la relation de proportion analogique existant entre les points A(1, 2), B(4, 4), C(3, 1), D(6, 3) représentés dans un repère orthonormé.
Ainsi quatre objets A, B, C et D sont en proportion analogique si et seulement si
La relation de proportion analogique liant A, B, C, et D peut être alors symbolisée par le vecteur ? ? ? AB ( ou ? ? ? CD). Dans ce cas particulier (la conformité est la relation d'identité), il est possible de définir un algorithme dont la complexité temporelle est en n 2 : celui-ci calcule tous les vecteurs existant entre les paires de n-uplets et rassemblent toutes les paires de nuplets définissant des vecteurs égaux en une classe d'équivalence (Lepage (2012)). Une classe d'équivalence, représentée par un vecteur, rassemble ainsi des paires de points qui, prises deux à deux, sont en proportion analogique « selon ce vecteur ». Il est alors aisé de générer l'ensemble de toutes les relations d'analogie à partir de chacune des classes d'équivalence.  
Supposons que
Modéliser les proportions analogiques selon une approche géométrique
La modélisation des proportions analogiques dans le contexte des bases de données relationnelles est influencée par les propriétés du modèle relationnel. Soit un ensemble d'attributs {A 1 , . . . , A m }, un schéma de relation est défini comme un sous-ensemble d'attributs S = {A i1 , . . . , A in }. Une relation définie en termes d'un schéma de relation S est un sous-ensemble fini du produit cartésien des domaines de chacun des attributs de S. Chaque élément d'une relation est appelé n-uplet qui peut être représenté par 1. La propriété de permutation des moyens permet d'éviter de calculer à la fois
un point décrit par n dimensions. Une base de données est un ensemble fini de relations. D'autres contraintes additionnelles comme les dépendences fonctionnelles et les dépendances d'inclusion permettent de restreindre le contenu des relations. Il serait intéressant d'en tenir compte dans la recherche des proportions analogiques mais nous nous limiterons ici au cas de la recherche de proportions existantes entre quatre points. Ainsi les n-uplets A, B, C, et D d'une relation sont considérés comme des points à n dimensions, et sont dénotés comme suit :
Comme dit précédemment, A, B, C, et D sont liés par une relation de proportion analogique si et seulement si
L'égalité est difficile à obtenir quand on considère des jeux de données réels. Il convient alors de rendre cette définition plus flexible, notamment en donnant une vision plus graduelle de la relation de proportion analogique. Deux vecteurs ne doivent plus être égaux mais presque égaux ce qui revient à mesurer dans quelle mesure || ? ? ? AB ? ? ? ? CD|| est proche de 0. On cherche alors à évaluer la « distorsion » entre les deux vecteurs. Pour permettre la commensurabilité des dimensions lorsque les attributs portent sur des domaines différents, les valeurs des vecteurs sont normalisées afin qu'elles appartiennent à l'intervalle [0, 1]. Pour cela, chaque valeur v du domaine actif d'un attribut peut être remplacée par la valeur suivante :
v ? min att max att ? min att où min att et max att désignent respectivement la valeur minimale et la valeur maximale du domaine actif de l'attribut.
Plusieurs stratégies peuvent être utilisées pour mesurer à quel point l'expression || ? ? ? AB ? ? ? ? CD|| est proche de || ? ? 0 ||. Différentes normes peuvent être utilisées comme la norme de Minkowsky (norme p), qui donnera la longueur du vecteur correctif permettant de passer de ? ? ? AB à ? ? ? CD, ou, la norme infinie qui donnera la coordonnée maximale de ce vecteur correctif.
Définition 1 : Distorsion analogique fondée sur une norme infinie Soit A, B, C, et D quatre n-uplets de n dimensions.
La norme infinie retourne la plus grande différence de dimension entre les deux vecteurs. Les deux vecteurs sont d'autant plus égaux que le changement maximal sur une dimension est proche de zéro.
Définition 2 : Distorsion analogique fondée sur la norme p Soit A, B, C, et D quatre n-uplets de n dimensions.
Dans ce cas, la définition met l'accent sur la longueur du vecteur correctif permettant de passer de
Dans tous les cas (Définition 1 ou Définition 2), la relation de proportion analogique est d'autant plus vraie que la distorsion est proche de 0.
Proposition : Les deux définitions de distorsion vérifient les propriétés fondamentales des proportions analogiques. En effet, les propriétés suivantes sont vérifiées :
puisque les deux définitions reposent sur une valeur absolue des différences entre chaque coordonnée.
-la permutation des moyens : la relation Dist(
Dans la suite, nous utiliserons la norme infinie qui est la plus drastique et possède un meilleur pouvoir de discrimination dans la mesure où elle évite tout effet de compromis entre les composantes des vecteurs.
Découvrir les proportions analogiques
Notre objectif est de découvrir toutes les proportions analogiques présentes dans un ensemble de données et si possible de dégager des tendances, i.e., les différents vecteurs représentatifs des proportions analogiques découvertes. Une approche naïve pourrait consister à énumérer tous les vecteurs et à calculer la distorsion entre chaque paire de vecteurs, puis à ne garder que les paires de vecteurs dont la valeur de distorsion ne dé-passe pas un certain seuil. Cependant, une telle approche poserait la question du choix du seuil, très dépendant des données. Il nous semble par ailleurs préférable d'utiliser une technique permettant de fournir une vue synthétique des motifs découverts. Une approche de type clustering semble tout à fait appropriée dans ce contexte. Un argument supplémentaire en faveur d'une telle approche est lié à l'objectif final que nous nous sommes fixé, à savoir l'extension des langages d'interrogation de bases de données avec des requêtes analogiques, i.e., des requêtes visant à découvrir des proportions analogiques existant dans un ensemble de données. En effet, l'identification de classes d'équivalence regroupant les paires de points représentant des vecteurs (presque) égaux permettrait la définition d'index, utiles pour optimiser l'évaluation de telles requêtes. Le problème ici étudié, qui consiste à regrouper des vecteurs à n dimensions égaux ou presque, se ramène à un problème de clustering classique dès lors que l'on dispose d'une métrique. Or les définitions des distorsions analogiques (Dist( ? ? u , ? ? v )) satisfont les conditions qui caractérisent les métriques, soit :
-l'identité des indiscernables : Dist( ? ? u , ? ? v ) = 0 ssi ? ? u = ? ? v , -la propriété de symétrie et -l'inégalité triangulaire. Différentes approches de clustering sont donc utilisables, comme les k-means et l'approche hiérarchique (Xu et al. (2005)). Notre objectif étant de tester l'approche et de la valider sur des jeux de données réels, puis d'identifier les relations découvertes, nous avons choisi de reprendre un algorithme de clustering hiérarchique (Xu et al. (2005)) dont les étapes sont énoncées dans l'Algorithme 1. Cet algorithme requiert une étape préalable de construction des vecteurs à partir des points de la relation. 
Data
Expérimentations
Dans cette section, nous illustrons notre approche avec des données électorales afin de découvrir des parallèles entre les résultats des votes de différentes régions, puis des évolutions des résultats de votes d'une année à l'autre. Pour cela, nous avons exploité les jeux de données ouverts décrivant les résultats des élections présidentielles en    Figure 2).
Les expérimentations visaient à montrer que le cadre proposé permet de mettre en évidence des parallèles existant dans les données, ce que montrent nos premiers résul-tats. Vu la nature de la relation de proportion analogique, l'approche peut évidemment s'appliquer à bien d'autres domaines, comme la recherche de trajectoires parallèles d'objets mobiles, moyennant une adaptation, ou dans les domaines environnemental et sociétal, pour découvrir des évolutions analogues.
5 État de l'art L'originalité de l'approche présentée ici tient à la nature même du type de régularité que l'on cherche à découvrir dans les données. La majeure partie des travaux en fouille de données visent à découvrir des caractéristiques fréquentes (vues comme des valeurs d'attribut ou des séquences de valeurs d'attribut lorsqu'un aspect temporel est pris en compte) dans un ensemble de données. Avec les proportions analogiques, qui sont des relations quaternaires, nous cherchons à vérifier l'existence de "parallèles" entre des couples d'objets d'une collection. Une proportion analogique, lorsqu'elle met en parallèle les situations de deux éléments à deux moments différents, peut être vue comme une sorte de règle d'évolution. Dans un tel contexte, la recherche de proportions analogiques peut constituer une alternative aux approches de la littérature visant à -extraire des règles d'évolution dans des graphes (Berlingerio et al. (2009)), ou à -découvrir des trajectoires parallèles d'objets en mouvement (Vlachos et al. (2002); Chen et al. (2005); Lee et al. (2007); Li et al. (2013)), ou encore à -classer des séquences d'événements (Studer et al. (2010); Guigourès et al. (2014); Lin et al. (2003); Malinowski et al. (2013); Zhou et al. (2013)).
Quoi qu'il en soit, l'approche proposée fournit un cadre plus général que celui dédié spécifiquement à l'extraction de règles d'évolution dans les données. En effet, la notion de proportion analogique n'implique pas l'existence d'une dimension temporelle et peut servir à décrire des parallèles de nature très variée entre deux paires d'objets.

Introduction
Actuellement, la recherche et la détection de similitudes s'effectuent en deux phases : une première phase de recherche de sources candidates, suivie d'une seconde de comparaison de ces sources possibles avec le document que l'on suspecte d'être un plagiat. La phase de collecte est de plus en plus optimale grâce à l'amélioration de l'efficacité des moteurs de recherche en local et sur le Web. C'est à la seconde phase que cet article s'intéresse. Une fois qu'une source candidate est trouvée, elle doit être comparée avec le document sur lequel pèse les soupçons. À l'heure actuelle, la plupart des logiciels anti-plagiat, une fois une liste de sources candidates constituée, se contentent de comparer mot à mot le document analysé avec chaque source possible. Cette technique permet seulement de détecter les similitudes de types « copier/coller ». Bien que cette approche ait prouvé son efficacité et suffise la plupart du temps, en France près d'un étudiant sur deux a déjà eu recours au « copier/coller » (Gibney, 2006), une énorme faille persiste. En effet, le fait de reformuler ou tout simplement de paraphraser un texte, en utilisant des synonymes par exemple, rend la plupart des techniques actuelles caduques. Certains articles (Callison-Burch et al., 2008;Bannard et Callison-Burch, 2005) se sont tout de même intéressés à la détection de reformulations paraphrastiques avec des approches d'alignement. Malgré le fait que ces approches soient plus robustes à l'ajout et à la suppression de mots ainsi qu'à l'utilisation de synonymes, elles restent toutefois inefficaces face aux reformulations non paraphrastiques comme le passage de la forme active à la forme passive. L'approche proposée consiste à comparer les deux textes, phrase par phrase, et non plus mot à mot et à rechercher si une phrase de l'un des textes comporte le même sens qu'une phrase dans l'autre texte. Ceci repose sur l'hypothèse que lorsqu'on paraphrase ou reformule un texte, on garde le sens de celui-ci et ainsi on garde les mots-clés principaux, porteurs du plus de sens de chaque phrase. Après avoir défini quelques notions et présenté l'état de l'art, nous décrirons d'abord comment segmenter le texte en unités de sens, pour ensuite procéder sur chacune de ces unités à l'extraction des mots porteurs de sens, afin de rechercher des concordances de mots de même concept dans un autre texte. Enfin, nous testerons trois algorithmes utilisant notre approche et nous déterminerons le meilleur seuil pour chacun d'entre eux. Le seuil est le nombre minimum de concepts identiques dans deux phrases permettant d'affirmer que l'une est la reformulation de l'autre. Pour finir, nous présenterons l'évaluation de notre approche en comparant la méthode retenue aux méthodes classiques de détection des paraphrases par alignement.
2 La comparaison au-delà du « copier/coller »
La notion de comparaison
La « comparaison de deux documents » est un terme assez vague. Pour comparer correctement deux documents, il faut repérer leurs points communs (leurs similitudes) et leurs diffé-rences. Les similitudes étant plus simples à détecter, il est de convention de chercher à repérer celles-ci en premier lieu et d'en déduire ensuite les différences, représentées alors par le reste du document. Cependant, la plupart des comparaisons textuelles se limitent au « copier/coller », or ce ne sont pas les seules similitudes pouvant être recensées dans un texte.
La notion de similitudes
Bien que l'on puisse avoir au sein d'un document des tableaux, images, graphiques ou tout autre type de données, cet article traite seulement des similitudes d'ordre textuel. On distingue plusieurs types de similitudes allant de la ressemblance jusqu'à l'identité même (SimacLejeune, 2013b). Les ressemblances sont les types de similitudes les plus difficiles à repérer et sont pour cause le point faible des logiciels anti-plagiat actuels. Dans notre cas, on distingue trois types majeurs de similitudes textuelles, de la plus simple à détecter à la plus complexe :
-la copie, qui consiste à copier mot à mot tout ou partie d'un texte dans un autre. Pour exemple, considérons la phrase suivante présente dans un texte : « En cinquante ans, grâce à des efforts considérables dans la recherche et l'élaboration de la fusion, la performance des plasmas a été multipliée par 10'000. » Elle sera recopiée à l'identique dans un autre texte ; -la paraphrase, aussi appelée reformulation paraphrastique, qui consiste à reprendre une phrase d'un texte pour la détailler ou l'expliciter. Elle conserve donc l'ordre des éléments évoqués, autorisant simplement le changement de vocabulaire, l'ajout, la suppression et la substitution de mots. Toujours en considérant la phrase de l'exemple précédent, une paraphrase possible serait : « En une cinquantaine d'années, grâce à un immense effort de recherche, la performance des plasmas produits par les machines de fusion a été multipliée par 10000. » On remarque la conservation des concepts, mais aussi la substitution ou la suppression de certains d'entre eux ; -la reformulation, qui autorise elle toutes modifications textuelles à condition que le sens de la phrase soit conservé. Cela donne souvent lieu à un changement d'ordre des concepts. La reformulation de la phrase exemple serait : « La performance des plasmas produits par les machines de fusion a été multipliée par 10,000 grâce à un immense effort de la recherche bien que cela ait pris une cinquantaine d'années. »
La notion de concept
Un concept est une idée, un sens représenté par un mot ou un groupe de mots. Les reformulations et paraphrases exploitent les propriétés paradigmatiques des mots (leur capacité à se substituer mutuellement) et entraînent ainsi des changements de vocabulaire mais elles conservent les concepts et les idées exprimées (Duclaye, 2003). Il est alors, dans le cadre de la détection de similitudes, plus judicieux de représenter un mot par un concept plutôt que par son identité ou sa définition. Par exemple, il est plus judicieux de représenter un mot par un tableau de tous les mots par lesquels il peut être substitué (un tableau de ses synonymes, lui compris) plutôt que seulement par lui-même.
État de l'art
Lorsque les processus anti-plagiat comparent deux documents, ils recherchent les éléments de l'un également présents dans l'autre. Ils tentent de détecter des similitudes, toutes informations communes laissant penser qu'un plagiat a pu avoir lieu. La comparaison mot à mot est certes efficace pour trouver les zones de « copier/coller » mais les plagiaires ne se contentent plus de copier des éléments depuis une source, ils essaient à présent de camoufler leurs emprunts d'idées derrière des modifications syntaxiques. Les recherches de Barron-Cedeño et al. (2013) se concentrant sur la détection de paraphrases appliquée dans le cadre de la détection du plagiat démontrent que le phénomène de paraphrasage nuit aux systèmes anti-plagiat et rend la détection de similitudes plus difficile. Il faut donc tenter de détecter les paraphrases et les reformulations par des moyens différents, car bien que souvent associés ces deux termes représentent des opérations textuelles bien distinctes. Toutefois, les travaux linguistiques ayant portés sur leur définition, s'accordent sur le fait que ce sont des opérations de modifications de texte, certes bien différentes, mais qui conservent toutes deux le sens (Harris, 1957;Martin, 1976;Duclaye, 2003).
Des recherches (Gülich et Kotschi, 1983;Eshkol-Taravella et Grabar, 2014) se sont attardées à chercher des marqueurs de reformulations afin de mieux les repérer par la suite et d'étudier leur fonctionnement et leur construction. D'autres recherches se sont cantonnées à étudier les limites de la détection des paraphrases (Vila et al., 2011) en estimant au contraire qu'il n'existait pas de caractérisation complète sur le plan linguistique et computationnelle de la paraphrase.
Face à ces difficultés, des chercheurs se sont concentrés sur des approches alternatives ne permettant pas de détecter concrètement des reformulations mais de tout de même déterminer qu'un texte en contient :
-les approches stylométriques (Iyer et Singh, 2005) qui suggèrent qu'en analysant des statistiques de fréquences de mots ou bien d'autres caractéristiques d'un texte on peut en reconnaître l'auteur, et ainsi, si un passage du document ne possède pas les mêmes caractéristiques que le reste du document, on peut en déduire que ce passage aura été emprunté à un autre auteur (Oberreuter et Velásquez, 2013;van Halteren, 2004;Jardino et al., 2007) ; -les approches de calcul de distances (Simac-Lejeune, 2013a) qui propose de calculer une distance « sémantique » entre deux textes après avoir extrait les mots clefs de chaque texte, exposant ainsi l'emprunt probable de l'un dans l'autre.
En dehors de ces approches, la majorité des travaux portent sur la détection des reformulations paraphrastiques, comme les recherches de Eshkol-Taravella et Grabar (2014) portant sur leur détection dans des corpus oraux. Les approches les plus répandues sont les méthodes par alignement (Callison-Burch et al., 2008;Bannard et Callison-Burch, 2005). Servant la plupart du temps dans un contexte bi-linguale (alignement d'un texte et de sa traduction), elles consistent à aligner deux textes par leurs mots ou groupes de mots en communs et ainsi de repérer les mots ou groupes de mots différents mais équivalents. Certaines recherches (Shen et al., 2006), visant à produire des paraphrases, se sont également avérées intéressantes. En effet, étudiant la possibilité de générer automatiquement des paraphrases, un processus d'assemblage puis de désassemblage s'est dégagé, remettant ainsi sur le devant de la scène les méthodes d'alignement. Proche de ces méthodes avec alignement, on peut citer le travail de Fenoglio et al. (2007) traitant de la comparaison de versions de documents textuels à la façon des serveurs de versions. Il met en lumière les transformations élémentaires (déplacements, insertions, suppressions et remplacements de blocs de caractères), identifiées depuis longtemps par les spécialistes de la génétique textuelle (de Biasi, 2000;Grésillon, 1994) comme éléments fondateurs d'une paraphrase.
Toutefois, le cadre théorique le plus souvent adopté est la théorie linguistique Sens-Texte (Kahane, 2003) élaborée dans les années 1960 par Mel'? cuk, notamment son système de paraphrasage (Žolkovskij et Mel'? cuk, 1967;Mel'? cuk, 1992;Mili´cevi´cMili´cevi´Mili´cevi´c, 2007) comme dans le travail de Mili´cevi´cMili´cevi´Mili´cevi´c (2010). Ce dernier met également en avant des approches sémantiques qui permettent de s'approcher d'une détection de reformulations. La plupart des règles sémantiques de paraphrasage trouvées jusqu'ici mettent en jeu un découpage du texte en proposition et des liens communicatifs et rhétoriques entre celles-ci (Danlos, 2006), coïncidant ainsi, dans la plupart des cas, à la définition d'une reformulation qui se contente d'être une paraphrase avec changement d'ordre des propositions.
La reformulation non paraphrastique étant bien plus complexe à détecter que sa voisine la paraphrase, les études se concentrant uniquement sur elle se font plus rares. Mais dès lors qu'on sait que la reformulation conserve également le sens du texte (Harris, 1957;Martin, 1976;Duclaye, 2003) et que le mécanisme de paraphrase le plus utilisé est le changement de lexique (Barron-Cedeño et al., 2013), on peut envisager d'appliquer plus ou moins les mêmes approches sémantiques que pour la paraphrase ou bien même, des approches plus naïves de recherche de correspondances de concepts.
3 Notre approche
Segmentation
Dans un premier temps, l'idée est de segmenter le document que l'on suspecte être un plagiat. Plusieurs algorithmes de segmentation ont été évalués :
-la segmentation par nombre de blocs : on découpe le document en un certain nombre de blocs de même taille (de même nombre de mots), peu importe la taille finale de chaque bloc ; -la segmentation par taille de blocs : on découpe le document par blocs d'une certaine taille (un certain nombre de mots), peu importe le nombre de blocs créés ; -la segmentation par pourcentage que représente un bloc sur l'ensemble du document (e.g. une segmentation comme celle-ci avec en paramètre un pourcentage de 1% pour un bloc reviendrait à une segmentation en 100 blocs, chaque bloc représentant 1%) ; -la segmentation par granularité (Simac-Lejeune, 2013b) : il s'agit d'une segmentation hiérarchique, on découpe le texte en nb blocs de même taille, puis on redécoupe chaque bloc ainsi obtenu en nb blocs de même taille, et ainsi de suite sur une profondeur limite définie. Ceci permettant d'affiner l'analyse niveau par niveau ; -la segmentation par paragraphe : chaque segment représentant un paragraphe ; -la segmentation par phrase : chaque segment représentant une phrase du document ; -la segmentation par proposition : chaque segment représentant une unité minimale de sens, les délimiteurs étant la ponctuation de fin de phrase mais aussi les virgules, les conjonctions de coordination et divers mots de liaison ou de causalité. Chaque algorithme a fait l'étude, via de nombreux tests et corrections, à l'optimisation de ses paramètres afin de mettre l'accent sur la rapidité du processus de découpage mais aussi sur la pertinence des métadonnées extraites dans chaque segment. Il est important que chaque segment conserve un sens afin d'être potentiellement sujet à une reformulation. Une segmentation en unité de sens a donc été choisie. Un découpage par phrase ou par proposition est à privilé-gier car une segmentation à faible granularité, comme celle par paragraphe, donne lieu à des segments trop volumineux pour l'étape d'extraction qui suivra. Au contraire, une segmentation à trop grand niveau de granularité pourrait, en plus d'entraîner un temps d'exécution plus important (plus de segments à traiter), occasionner une perte d'informations dans sa globalité (aucune liaison entre les concepts extraits). C'est pourquoi la segmentation qui a été retenue est une fusion du découpage par phrase et du découpage par taille de blocs : une segmentation par phrase mais d'une taille minimale (en mots). On conserve ainsi une unité de sens (une ou plusieurs phrases) tout en gardant une taille suffisamment importante pour pouvoir obtenir une pertinence raisonnable des métadonnées extraites mais suffisamment petite pour être considé-rée comme indépendante et donc éventuellement reformulée. Après divers tests, le seuil a été fixé à 15 mots, taille moyenne des phrases dans la langue française. Avec un seuil si faible, c'est l'une des méthodes de segmentation évaluée les plus chronophages mais pour notre étude elle garantit un rapport taille/pertinence optimal.
Extraction de mots clefs (mots porteurs de sens)
La seconde étape du processus est une étape d'extraction des mots porteurs de sens de chaque segment c'est-à-dire des mots représentant les concepts que le plagiaire a été obligé de réutiliser s'il voulait conserver le sens de la phrase, même s'il a pu les remplacer par des synonymes. Pour déterminer les mots porteurs de sens d'un texte, l'étiqueteur morphosyntaxique TreeTagger (Schmid, 1994) a été utilisé. Il détermine la classe lexicale, le "Part Of Speech" de chaque unité lexicale (token) du texte. De façon plus commune, on peut dire que pour chaque mot ou élément du texte, TreeTagger détermine s'il s'agit d'un nom, d'un verbe, d'un adjectif, d'une ponctuation, etc. L'étiquetage morphosyntaxique permet d'identifier les mots clefs d'un texte par leur classe lexicale. Plutôt que de discriminer les mots vides (stop words) par leur taille, ceci pouvant générer des erreurs (e.g. un mot de moins de trois lettres n'est pas pertinent, un contre exemple est le mot « as » qui peut être important, et le mot « mais » qui est simplement une conjonction), on les discriminera par leur "Part Of Speech".
Dans notre cas, les mots pertinents à conserver sont un peu plus riches sémantiquement que les mots clefs habituels. On ne conserve pas seulement les noms communs et propres, il est important de garder aussi les adjectifs, les verbes et également les adverbes, en réalité tout mot porteur de sens au sein d'une phrase. On néglige donc les méthodes les plus courantes pour extraire des mots clefs, les méthodes fréquentielles (Lee et Baik, 2004) qui consiste pour chaque mot du texte à calculer sa fréquence d'apparition dans le texte. C'est pour cela que le terme de mots clefs est ici un abus de langage et que nous allons préférer le terme de mots porteurs de sens d'une phrase. Tout mot porteur de sens d'une phrase doit être conservé, peu importe son nombre d'occurrences dans le texte.
Un filtre de mots vides a été ajouté à la sortie de TreeTagger afin d'être certain de la pertinence des mots porteurs de sens extraits. Ainsi en couplant les deux techniques, l'efficacité de l'étiquetage est passée d'environ 96% à quasiment 100%.
Considérons la phrase suivante : « Ce peu de masse disparue crée une grande quantité d'énergie comme le démontre la fameuse formule d'Einstein E=mc2. » Ses mots porteurs de sens extraits seraient « peu, masse, disparue, crée, grande, quantité, énergie, démontre, fameuse, formule, Einstein, E=mc2 ».
Thésaurus -chargement d'un dictionnaire de synonymes
Parallèlement à cela, un dictionnaire de synonymes est chargé. Pour chaque mot, on a donc accès à un tableau contenant tous les mots de la langue par lesquels il peut être substitué. L'efficacité de notre approche dépendant en grande partie du contenu de cette ressource, il est important de faire la différence entre des synonymes et des mots de substitution possibles. Par exemple, pour le mot « père », « papa » serait un synonyme alors que « parent » serait un mot de substitution envisageable. Autre exemple, le mot « île » a pour synonyme « îlot », « archipel » ou bien encore « atoll » mais aucunement les mots « tâche » ou « pâté » qui eux se trouvent pourtant dans notre tableau et peuvent servir de mot de substitution. En effet, on peut très bien imaginer dans un poème une phrase telle que « cette tâche au milieu de l'océan » faisant référence à un îlot. En règle générale, « îlot » et « tâche » ne sont pas synonymes mais ici, ils représentent le même concept.
Dès lors, un concept est un mot porteur de sens ainsi que tous ses mots de substitution possibles contenus dans son tableau.
Le tableau 1 représente une partie des mots de substitution correspondant aux mots porteurs de sens extraits sur la phrase exemple lors de l'étape précédente. On remarque, comme dans l'exemple de l'îlot cité précédemment, que le terme « énergie » laisse place à « assiduité » qui n'a strictement rien à voir avec le contexte de notre phrase mais qui dans un autre contexte aurait très bien pu être un synonyme envisageable. 
Correspondance
La dernière étape de notre approche consiste à comparer chaque phrase d'une source candidate avec les mots porteurs de sens de chaque segment du texte en cours d'analyse ainsi qu'avec leurs mots de substitution possibles contenus dans le tableau défini précédemment. On appellera seuil de correspondance le nombre de concepts communs à partir duquel on peut estimer qu'une phrase est la reformulation d'une autre. S'il y a plus de concepts pertinents communs entre deux phrases que le seuil de correspondance défini, c'est sans doute que l'une est une reformulation de l'autre.
Plusieurs algorithmes mettant en oeuvre cette méthode ont été développés, certains plus robustes que d'autres face aux changements de genre, de nombre, de casse typographique ou bien d'ordre des mots (e.g. phrase passée de la forme active à la forme passive et vice versa). L'efficacité de la détection dépend de l'algorithme choisi, du seuil de correspondance défini, et du nombre et de la pertinence des « synonymes » disponibles dans le dictionnaire chargé.
Nous proposons trois algorithmes, trois implémentations différentes de l'approche décrite précédemment.
-un premier (tableau 2 -A) qui compare la présence des concepts dans l'ordre et tels qu'ils sont présents dans les phrases. Il ne supporte donc ni le changement de casse typographique, ni la dérivation et la flexion ; -un second (tableau 2 -B) qui compare également la présence des concepts dans l'ordre des phrases mais en comparant leurs lemmes en minuscules, il supporte donc le changement de casse typographique, la dérivation et le changement de genre et de nombre ;
-un troisième (tableau 2 -C), plus naïf, qui reprend le principe du précédent, en comparant cette fois la présence des concepts dans les deux phrases sans prendre l'ordre en compte. Il est ainsi robuste aux reformulations non paraphrastiques de type mise à la forme passive. Le tableau 2 résume les différentes variations de la langue supportées par chaque algorithme. Considérons maintenant la phrase : « La célèbre équation d'Einstein E = mc 2 exprime le phénomène suivant : une importante quantité d'énergie est apparue et un peu de la masse a disparu. » ainsi que sa reformulation : « Ce peu de masse disparue crée une grande quantité d'énergie comme le démontre la fameuse formule d'Albert Einstein E=mc2. » Si on opère la comparaison de type C sur ces deux phrases, on retrouve bien, malgré le changement de vocabulaire et d'ordre des mots, la correspondance de nos concepts, ici en gras.
A noter l'importance du seuil de correspondance, il y a dans cette exemple 11 concepts identiques, avec un seuil de correspondance de 11 ou moins, la phrase est reconnue comme reformulation, alors qu'avec un seuil de correspondance supérieur ce ne sera plus le cas.
Évaluation et tests 4.1 La base de tests et protocole
La base de tests est composée de 150 textes, représentant chacun un passage d'un document, allant de plus de 100 mots pour le plus petit à environ 9000 mots pour le plus grand. Cela représente 400 comparaisons de textes deux à deux. Afin de tester correctement les performances des algorithmes évalués, aussi bien des paraphrases que des reformulations plus complexes sont présentes dans le corpus, ainsi que des textes « pièges » traitant du même sujet et donc employant le même vocabulaire mais n'étant pas pour autant des reformulations d'un autre texte du corpus.
Ci-dessous la répartition des types de textes présents dans le corpus : -10 différents chapitres tirés d'un même roman ; -20 chapitres de la bible (deux traductions différentes pour 10 chapitres) ; -25 textes de Wikipédia (différentes versions à différentes dates de 10 articles) ; -35 extraits de travaux d'élèves (avec leurs sources provenant du Web) ; Les extraits de travaux d'élèves proviennent pour la plupart de rapports et mémoires scientifiques ou économiques. L'intégralité des textes sont en français.
Résultats
Dans un premier temps, on compare les trois méthodes décrites précédemment, leur efficacité et leur temps moyen d'exécution respectifs étant différents selon le seuil utilisé, on détermine d'abord le seuil optimal pour chacune d'entre elles. Le tableau 3 représente le rapport précision/rappel des trois algorithmes allant du seuil 1 à 7. Un seuillage de 4 semble mieux convenir aux algorithmes A et B, tandis qu'un seuillage de 5 semble idéal pour l'algorithme C. Prenant en compte la F-mesure et privilégiant le rappel plutôt que la précision, l'algorithme C se montre être le plus efficace sur la base de tests. Le tableau 4 représente le temps d'exécution de la procédure (segmentation, extraction de mots porteurs de sens, chargement du thésaurus et comparaison) des trois algorithmes en utilisant leur meilleur seuillage en fonction du nombre moyen de mots contenus dans les deux textes à comparer (moyenne du nombre de mots des deux textes). La méthode C s'avère être la plus rapide (200 secondes en moyenne pour un texte d'environ 4000 mots contre 250 pour la méthode A et 212 secondes pour la méthode B) en plus d'avoir un meilleur rapport préci-sion/rappel, respectivement 0.745 et 0.807, car malgré le fait qu'elle soit utilisée avec un seuil plus grand (5 contre 4 pour les deux autres implémentations) et qu'elle fasse donc forcément un plus long parcours, elle ne vérifie pas l'ordre des mots et néglige donc des permutations et suppressions de tableau. On remarque néanmoins une précision générale assez basse due aux faux positifs générés par les propriétés paradigmatiques des mots contenus dans le thésaurus.
Le tableau 5 compare la méthode retenue (l'algorithme C avec un seuil de 5) avec une mé-thode d'alignement basée sur la méthode de Bannard et Callison-Burch (2005) mais appliquée sur un corpus mono-lingue. Ces deux approches possèdent des performances similaires face à la détection de « copier/coller », environ 84% d'efficacité, en revanche notre méthode montre de biens meilleurs résultats sur la détection des reformulations non paraphrastiques (un rappel de 0.80 contre 0.24 pour une méthode avec alignement). TAB. 5 -Evaluation de notre méthode par rapport à une méthode à alignement en fonction des types de similitudes à détecter.
Conclusions
La méthode retenue montre des résultats similaires aux méthodes avec alignement sur la détection de copies exactes et de paraphrases et se montre beaucoup plus robuste face aux reformulations. Néanmoins, sa précision est fortement impactée par le thesaurus utilisé, qui peut engendrer des faux positifs pour les raisons évoquées dans la partie 3.2 Extraction de mots clefs, et la segmentation, qui peut être faussée par du texte enrichi (tableau, liste à puces). Nous conviendrons également que cette technique est plutôt coûteuse en temps et en ressources (chargement du thesaurus en mémoire).
Un seuil adaptatif évoluant en fonction de la taille des phrases pourra également être mis en place dans de futurs travaux. Pour des phrases standards comportant entre 8 et 15 mots, il sera préférable de fixer le seuil à 5, en revanche si la phrase excède la vingtaine de mots, il faudra définir le seuil entre 10 et 12 mots communs.
Au final, cette approche reste naïve et gourmande aussi bien en temps qu'en ressources, néanmoins elle permet de détecter des reformulations jusque là impossibles à détecter avec des méthodes conventionnelles à alignement et constitue donc une alternative intéressante. Elle est à privilégier pour la détection de reformulations non paraphrastiques.

Introduction
La plupart des logiciels anti-plagiat se concentrent sur une détection extrinsèque de plagiat, c'est-à-dire sur le fait de trouver des similitudes entre un document et un corpus de sources probables. Or ce système est inutile si le document ayant été plagié ne se trouve pas dans le corpus fouillé. Néanmoins, il existe un autre type de détection, la détection intrinsèque qui exploite des données extraites de l'intérieur même du document. La détection d'auteurs par étude du style d'écriture du document est la forme de détection de plagiat intrinsèque la plus répandue. Cette approche diverge selon les travaux car elle soulève plusieurs problèmes, allant du découpage du texte de façon pertinente, au choix et à la collecte des données stylistiques à surveiller, en passant par la manière de découper et de classer les différents passages du document par auteur. C'est sur ce dernier point que l'article va essentiellement se concentrer.
La détection d'auteur 2.1 La notion de stylométrie
La stylométrie ou l'étude stylométrique d'un texte est une analyse à mi chemin entre une analyse linguistique et statistique. Elle exploite des variables stylométriques, qui sont des caractéristiques linguistiques du texte, afin d'établir des statistiques sur le document étudié. Effectuer l'analyse stylométrique d'un document consiste à surveiller les variations du style d'écriture du document en surveillant l'évolution des variables stylométriques au sein de celuici afin d'en détecter les irrégularités et ainsi pouvoir déterminer si certains passages, appelés phases stylistiques, sortent de la norme par rapport à la majorité du texte.
État de l'art
Dès le XIX e siècle, Mendenhall (1887) suggère qu'en analysant des caractéristiques internes d'un texte on peut en reconnaître l'auteur. Depuis, les techniques d'études stylomé-triques de document ont fait d'importantes avancées et de nombreuses recherches (Stein et Eissen, 2007;Layton et al., 2013;Jayapal et Goswami, 2013) appliquent cette découverte à la détection de plagiat. Certaines de ces recherches se concentrent sur l'extraction et la surveillance des données stylométriques les plus pertinentes. Stein et Eissen (2007) ainsi que Zamani et al. (2014)    (Cheng, 1995), un algorithme multidimensionnel des k-moyennes non paramétrique.
Segmentation
Dans un premier temps, l'idée est de segmenter le document. Il est important que chaque segment conserve un sens afin d'être autonome et donc d'être potentiellement écrit par une personne différente. Une segmentation en unité de sens est donc à privilégier. S'appuyant sur le travail de Zechner et al. (2009), c'est une segmentation pseudo sémantique qui a été retenue : un découpage par phrase d'une taille minimale (en mots). Le seuil a été fixé à 15 mots, taille moyenne des phrases dans la langue française.
Extraction de la stylométrie
La seconde étape du processus consiste à extraire la stylométrie de chaque segment. Pour ce faire, il faut au préalable détecter la langue de chaque segment au moyen d'un module im-plémentant la technique de catégorisation de texte à base de n-grammes de Cavnar et Trenkle (1994). Ensuite, l'étiqueteur morphosyntaxique TreeTagger (Schmid, 1994)  
Construction des courbes
Une fois la segmentation et les calculs stylométriques opérés, on obtient donc plusieurs valeurs par segment (i.e. une valeur par variable stylométrique). Une suite de valeurs brutes sans cohérence n'étant pas exploitable, on représente la stylométrie du document sous la forme de courbes, avec en abscisse, la position des segments (la ligne de vie du document) et en ordonnée, les valeurs des variables stylométriques observées. Ceci a pour avantage, en plus de permettre une représentation visuelle, de faciliter la comparaison et la manipulation des valeurs entre elles, les algorithmes de manipulation de courbes étant courant.
Il est possible que le style d'un même auteur varie énormément au fil d'un même texte. La fatigue ou la maturité lors de longs écrits peuvent entraîner du bruit ou des variations brusques. On convient alors qu'un lissage est nécessaire. C'est le lissage par la moyenne glissante sans pondération (Chou, 1975) qui a été utilisé dans cet article.
Regroupement
Il reste à déterminer les phases stylistiques de façon automatique. Un algorithme d'apprentissage automatique non supervisé (i.e. sans intervention humaine) est idéal dans ce cas de figure qui s'apparente au clustering car il faut déterminer à quel auteur (i.e. à quel cluster) chaque donnée s'apparente. Sachant que le nombre d'auteurs et donc de clusters n'est pas connu à l'avance, c'est le Mean Shift multidimensionnel (Cheng, 1995) qui se dégage. En effet, cet algorithme permet de clustériser un ensemble de points sans connaître à l'avance le nombre k de clusters. L'idée dans notre cas est de déterminer le nombre k à partir d'un seuil. On dé-finit alors empiriquement un nombre k de départ assez grand, admettons 10 et un seuil, entre 2% et 15% en fonction de la moyenne de la variable stylométrique observée (seuil adaptatif). Tant qu'il existe deux clusters voisins avec une différence de stylométrie inférieure au seuil, on relance un KMeans avec k = k ? 1. Une fois toutes nos phases identifiées et k définitif, s'il existe deux clusters (non voisins cette fois-ci étant donné que les voisins ont déjà été réunifiés) avec une différence de stylométrie inférieure au seuil, on en déduit qu'ils sont du même auteur.
On prend en considération plusieurs variables stylométriques en même temps, tout comme le fait van Halteren (2004). L'idée est de surveiller plusieurs variables stylométriques afin qu'elles se « complètent » mutuellement. On augmente ainsi le taux de certitude de l'existence d'une zone par le fait qu'une zone est définie comme telle si la majorité des courbes fléchissent de telle façon à la dessiner. De plus, la zone de flexion retenue est maintenant désignée par la moyenne des zones de flexion de toutes les courbes surveillées, ceci réduisant considérable-ment l'erreur d'approximation et rendant plus sûr notre prise de décision. Pour exemple, sur la FIG. 1 -Mean Shift sur plusieurs variables stylométrique.
figure 1 chaque zone de flexion des courbes est représentée par une ligne verticale pointillée de la même couleur que la courbe dont elle dépend. Les lignes pointillées noires plus épaisses représentent les découpages retenus (les moyennes des trois flexions des trois courbes).
Pour faciliter l'observation des écarts et des flexions, les courbes sur cette figure ont été normalisées (mises à la même échelle), leurs valeurs stylométriques sont donc faussées.
Évaluation et tests 4.1 La base de tests et protocole
La base de tests est composée de 500 textes contenant en moyenne 7 000 mots. Les textes sont constitués d'un (l'intégralité du texte) à cinq passages, chaque passage étant potentiellement écrit par un auteur différent. Un texte peut contenir plusieurs passages écrits par un même auteur. On recense en totalité dans la base, une dizaine d'auteurs différents. La langue prédomi-nante au sein des textes est le français, cependant pour tester l'adaptabilité et le plurilinguisme du système de nombreux passages sont en anglais ou en italien. Afin de tester correctement la procédure évaluée, des passages traitant du même sujet et donc employant le même vocabulaire ont été utilisés dans le but de tromper la stylométrie extraite. De plus, l'intégralité des textes est annotée, de telle sorte à savoir précisément de quel mot à quel mot les textes sont écrits par un auteur ou par un autre.
Résultats
Notre procédure présente une précision de 0.89 et un rappel de 0.34. Il est néanmoins important d'étudier plus en détails les limites de cette procédure et de nuancer un rappel si faible. La figure 2 est un diagramme à bulles représentant les performances du découpage stylomé-trique. L'axe des abscisses représente la taille en segment de la phase stylistique concernée et celui des ordonnées l'écart moyen de la variable stylométrique observé entre cette phase et ses voisines, son unité est notée us pour unité stylométrique. Les bulles représentent les différents 
Conclusions
Notre approche montre des résultats exploitables lorsque les phases stylométriques à identifier ne sont pas trop importantes (n'excèdent pas 190 segments soit environ 4000 mots) et lorsque la différence de stylométrie est suffisamment grande (supérieur à 0.20 us). En revanche dans tous autres cas, les limites de notre approche se font ressentir. Pour palier ces problèmes, un seuil adaptatif pourra être défini en fonction du type de variable stylométrique surveillée. De plus, avec du recul, nous convenons qu'un Mean Shift n'était sans doute pas la meilleure option de clustering. Dans la suite de nos travaux nous implémenterons d'autres classifieurs (hiérarchique, DBSCAN, etc.).
Pour conclure, bien que perfectible, cette approche permet de détecter différents styles d'écriture au sein d'un même texte et notre contribution malgré ses limites permet bien de regrouper automatiquement les phases stylistiques par auteur.

Introduction
Le risque chimique ou alimentaire se manifeste lorsque les produits chimiques sont dangereux pour la santé et consommation humaine ou animale, et pour l'environnement. Si certains produits et substances sont maintenant clairement identifiés comme dangereux (e.g. l'amiante, l'arsenic, le plomb), nos connaissances actuelles sur d'autres substances sont moins complètes. Nous nous intéressons en particulier au risque alimentaire (e.g. l'arsenic, les nitrates, la listeria, la dioxine) et au risque chimique (e.g. le bisphénol A, les phtalates). Ces substances entrent souvent dans la composition de produits courants et peuvent avoir l'effet nuisible sur l'organisme humain. Le contrôle sur la commercialisation de ces substances est effectué par des organismes sanitaires dédiés, comme EFSA (European Food Safety Authority) ou ANSES (Agence nationale de sécurité sanitaire de l'alimentation, de l'environnement et du travail). Les experts se retrouvent face à une littérature scientifique abondante et doivent l'étudier pour avoir une base solide pour la prise de décisions. L'objectif de notre travail consiste à proposer une aide automatique pour l'analyse de la littérature scientifique afin de détecter les phrases indicatives du risque induit par ces substances. Nous abordons cette tâche comme une problématique de catégorisation : les phrases des textes doivent être catégorisées dans les classes du risque. Nous présentons les données (section 2) et approches utilisées (sections 3 et 4). Nous discutons ensuite les résultats obtenus et concluons avec les pistes pour les travaux futurs (section 5).
Notre objectif est de catégoriser les phrases des corpus dans les classes de risque. L'é-valuation est effectuée par rapport aux données de référence. Une liste de mots vides et des ressources linguistiques sont aussi utilisées. Le travail est effectué avec le matériel en anglais.
Corpus. Les corpus proviennent de la littérature scientifique, qui est le matériel typique utilisé par les experts. Le corpus du risque chimique (80 000 occ.) contient le rapport sur le bisphénol A (EFSA Panel, 2010). Le corpus du risque alimentaire (>240 000 occ.) a été constitué à partir de 115 documents officiels publiés entre 2000 et 2010 sur une dizaine de substances, comme l'arsenic, la dioxine ou le nitrate (Blanchemanche et al., 2013). Trois sections (introduction, conclusion et résumé) sont traitées car elles comportent les résultats principaux.
Classifications du risque. Les classifications du risque (alimentaire (Blanchemanche et al., 2013) et chimique (Maxim et van der Sluijs, 2014)) sont structurées hiérarchiquement et décrivent différents aspects révélateurs de la nocivité des substances chimiques :
-significativité des résultats (The Panel concluded that the current NOAEL for BPA (5 mg/kg b.w./day) would be sufficiently low to exclude any concern for this effect) ; -hypothèse scientifique (Despite this lack of evidence, the possibility of poultry and egg consumption as an exposure route to HPAIV remains a concern to food safety experts). Le risque est présent lorsque la nocivité des substances est apparente dans la littérature scientifique, ou lorsque les expériences présentées montrent des imprécisions et incertitudes.
Ressources linguistiques. Des ressources linguistiques sont utilisées avec l'approche par apprentissage supervisé pour enrichir l'annotation. Nous supposons que ces différentes expressions, souvent liées à la notion d'incertitude, sont indicatrices de la notion du risque chimique :
-l'incertitude (e.g. possible, should, may, usually) indique des doutes existant au sujet des résultats obtenus expérimentalement, leur interprétation, etc. ; -la négation (e.g. no, neither, lack, absent, missing) indique que de tels résultats n'ont pas été observés, que l'étude ne respecte pas les normes, etc. ; -les limitations (e.g. only, shortcoming, insufficient) indiquent des limites, comme la taille insuffisante de l'échantillon traité, le faible nombre de tests ou de doses testées, etc. ; -l'approximation (e.g., approximately, commonly, estimated) indique d'autres insuffisances liées aux valeurs imprécises de substances, d'échantillons, de doses, etc. Avec la recherche d'information, nous utilisons des ressources pour l'extension de requêtes :
-101 805 paires de synonymes provenant de la langue générale (Fellbaum, 1998) et spé-cialisée (Grabar et Hamon, 2010), -des clusters de mots générés avec des méthodes distributionnelles à partir des corpus (Brown et al., 1992;Liang, 2005). Données de référence. Les données de référence sont obtenues grâce à l'annotation par des spécialistes en évaluation du risque. Un expert a annoté 425 phrases couvrant 55 classes du risque chimique. Plusieurs experts ont participé dans l'annotation du corpus du risque alimentaire et fournissent des données de référence pour 657 phrases monoclasses couvrant 27 classes et 389 phrases multiclasses, pour un total de 1 046 phrases annotées. Plusieurs des classes contiennent très peu de phrases annotées et nous gardons celles qui fournissent un nombre suffisant d'exemples (minimum de 10 pour le risque alimentaire, 5 pour le risque chimique).
Liste de mots vides. La liste de mots vides contient 176 mots (e.g., & about again all almost and any by do to etc). Cette liste contient essentiellement des mots grammaticaux.
3 Approche par apprentissage supervisé Méthode. Nous utilisons différents algorithmes de la plateforme Weka (Witten et Frank, 2005) avec le paramétrage par défaut. Les phrases sont l'unité de travail. Nous visons la dé-tection de phrases liées au risque : (1) de manière générale G pour détecter les phrases relatives au risque ; (2) de manière précise D pour associer ces phrases aux classes de risque. Les descripteurs sont fournis par l'annotation sémantique et linguistique : forms (les formes de mots comme elles apparaissent dans le corpus), lemmas (mots lemmatisés), lf (combinaison de formes et de lemmes), tag (les étiquettes morpho-syntaxiques des formes (e.g. noms, verbes, adjectifs)), lft (combinaison de formes, lemmes et étiquettes morpho-syntaxiques), stag (étiquettes sémantiques de mots (e.g. incertitude, négation, limitations)), all (combinaison de tous les descripteurs). Les descripteurs sont pondérés de trois manières : freq (fréquence brute des descripteurs), norm (fréquence normalisée par la taille du corpus), tfidf (pondération tfidf (Salton et Buckley, 1987)). Nous effectuons une validation croisée. Les mesures d'évaluation sont la précision, le rappel et la F-mesure (moyenne harmonique de la précision et du rappel). Résultats. Les résultats présentés sont obtenus avec J48 (Quinlan, 1993). Avec l'expéri-ence G, les performances avec le risque alimentaire (autour de 0,8) sont meilleures que celles du risque chimique (0,61-0,64). Les performances sont assez stables avec les différents descripteurs et pondérations. L'exploitation de formes, d'étiquettes sémantiques et les différentes combinaisons de descripteurs donnent des résultats légèrement supérieurs. Bien que très simplistes, les étiquettes morpho-syntaxiques (e.g. noms, verbes, adjectifs) sont assez efficaces sur les deux corpus. Les étiquettes sémantiques seules (stag) sont parmi les plus efficaces pour détecter le risque chimique, mais montrent une F-mesure assez faible pour le risque alimentaire. À la figure 1, nous présentons l'expérience D avec les descripteurs lft (formes, lemmes et étiquettes morpho-syntaxiques). Les résultats sont élevés avec les classes du risque alimentaire et deux classes du risque chimique (Facteur d'incertitude et Hypothèses scientifiques). Le tfidf donne de meilleurs résultats dans la plupart des cas, mais la pondération norm est aussi compétitive. Les descripteurs lft fournissent de meilleurs résultats que les autres descripteurs. Les résultats sont meilleurs avec le risque alimentaire, où il existe plus de données d'apprentissage.
Approche de recherche d'information
Méthode. Nous considérons les libellés des classes comme les requêtes et les phrases des corpus comme les réponses potentielles à ces requêtes. Nous exploitons le système de recherche d'information Indri (Strohman et al., 2005), qui utilise un modèle probabiliste basé sur le champ aléatoire de Markov et offre plusieurs fonctionnalités, comme par exemple :
-la racinisation (Porter (Porter, 1980) et de Krovetz (Krovetz, 1993)) réduit un mot à sa racine (e.g., suppression de pluriels et de chaînes finales comme -ment et -ique) ; -le et booléen (band) permet de combiner plusieurs mots clés ; -les fenêtres ordonnées ou non ordonnées permettent de spécifier l'ordre des mots clés ; -la pondération (tfidf (Salton et Buckley, 1987) et okapi (Robertson et al., 1998)) permet de relativiser le poids des mots-clés ; -la pondération des synonymes (wsyn) permet d'indiquer l'importance des mots clés. Pour l'expansion des requêtes, nous retenons les mots supplémentaires des ressources linguistiques (synonymes et clusters) si ces mots montrent au moins 0,3 % de précision. L'évaluation est effectuée avec plusieurs mesures : précision, rappel, F-mesure et MAP (Mean Average Precision), cette dernière prenant en compte l'ordre des réponses. Pour la baseline, les mots des libellés de classes sont utilisés, sans la racinisation ni l'expansion de requêtes. Résultats. Le tableau 1 indique la MAP et la F-mesure de différentes expériences : baseline, utilisation de raciniseurs, pondération des mots clés et des clusters. Nous obtenons de meilleurs résultats avec les libellés du risque chimique, car ils sont plus explicites. La F-mesure est en général plus élevée que la MAP. Les résultats sont améliorés avec la racinisation, la pondération tfidf et okapi, et les clusters. Plusieurs autres expériences n'ont pas été concluantes (e.g. exploitation des définitions, pondération des synonymes, fenêtres ordonnées des mots clés des requêtes, et booléen). Krovetz, la pondération et les clusters fournissent les meilleurs ré-sultats (figure 2). Les raciniseurs améliorent le rappel et donc les performances globales, tandis que l'utilisation de la pondération des mots clés (okapi ou tfidf) améliore surtout les valeurs de la MAP : les phrases retournées sont alors les mêmes, mais leur ordre devient plus correct. Les ressources linguistiques supplémentaires sont favorables pour certaines classes. Elles permettent surtout d'améliorer le rappel.
Discussion et Conclusion
L'apprentissage supervisé est plus performant que la recherche d'information, tandis que cette dernière, étant moins supervisée, permet de traiter un plus grand nombre de classes. La recherche d'information permet aussi de varier plus facilement les paramètres selon que l'on voudrait privilégier la précision ou le rappel. La pondération montre toujours un effet favorable. Dans une expérience similaire avec le risque alimentaire, des résultats comparables aux nôtres sont obtenus (Blanchemanche et al., 2013). Notons que nous avons aussi testé une approche non supervisée à base de règles, qui montre des résultats très faibles : rappel quasinul pour une précision entre 0,5 et 0,6. Il existe plusieurs possibilités pour combiner les deux approches testées : combinaison des sorties pour augmenter le rappel ; le vote des approches pour améliorer la précision ; l'utilisation des noeuds décisionnels des modèles d'apprentissage supervisé pour l'extension de requêtes ; l'exploitation des sorties de recherche d'information et du système à base de règles par l'apprentissage supervisé.
En conclusion, nous utilisons l'apprentissage supervisé et la recherche d'information pour détecter des phrases relatives au risque induit par les substances chimiques. Nous abordons la tâche comme une problématique de catégorisation : les phrases des textes doivent être caté-gorisées dans les classes de risque. Deux corpus et deux classifications du risque sont utilisés. Les résultats par apprentissage automatique sont les plus performants. Les résultats indiquent aussi que l'expression de l'incertitude linguistique (e.g., likely, should, assume) est associée avec la notion du risque chimique. Dans les travaux futurs, nous allons tester d'autres paramètres pour améliorer les performances des approches testées et nous allons combiner les résultats de ces approches de différentes manières. Ces résultats peuvent être utilisés par les experts travaillant sur la gestion du risque pour la prise de décisions et évalués par eux.
Remerciements. Ce travail est soutenu par le projet PNRPE DICO-Risk.
Références
Blanchemanche, S., A. Rona-Tas, A. Duroy, et C. Martin (2013). Empirical ontology of scientific uncertainty : Expression of uncertainty in food risk analysis. In Society for Social Studies of Science, pp. 1-27.

Introduction
Ces dernières années, nous assistons à la sémantisation des données statiques et dynamiques (flux de données). Toutefois, vu la spécificité de ces derniers ni les technologies du web sémantique ni celles des Systèmes de Gestions de Flux de Données (SGFD) ne peuvent les traiter. Pour ce faire, les chercheurs proposent aujourd'hui de nouveaux systèmes tels que C-SPARQL (Barbieri et al., 2010), CQELS (Phuoc) et SPARQL Stream (Calbimonte et al.). Lorsque le débit du flux en entrée de ces systèmes dépasse les seuils supportés, deux solutions existent : 1-Allouer au système autant de ressources que nécessaires (Hoeksema et Kotoulas, 2011)  manière, nous préservons le niveau sémantique de l'information et protégeons la cohérence des données du graphe en mémoire.
Conclusion
Nous avons proposé dans cet article une approche orientée graphe pour la réduction de charge des systèmes de traitement de flux de données sémantiques. Notre approche, permet d'améliorer la qualité des résultats des requêtes des systèmes de traitement de flux de données sémantiques, en protégeant la sémantique et la cohérence des données de ces flux, contrairement à une application naïve de l'approche orientée triplet RDF utilisée jusqu'à présent.

Introduction à Sélection basée sur le Degré de Pertinence
Les librairies digitales sont actuellement très répandues. Elles renferment des quantités d'informations énormes et nécessitent des mécanismes efficaces d'indexation et de manipulation. Les moteurs de recherche du type général ne peuvent pas les indexer car ils exigent que l'information qu'ils manipulent soit composée d'entités indépendantes. Dans le besoin de traiter rapidement et efficacement les requêtes, des méthodes basées des approches diffé-rentes ont été inventées. On rencontre alors, des méthodes se basant sur les réseaux bayésiens comme CORI Callan et al. (1995), d'autres méthodes qui se basent sur les statistiques TF*IDF. Il existe aussi des méthodes qui se basent sur le modèle de langage et la pseudo-pertinence. Ces méthodes utilisent des résultats déjà obtenus pour de réponses futures. Puisque le modèle centralisé souffre du problème de passage à l'échelle, certaines méthodes ont été mises pour tourner sur les systèmes pair-à-pair. La méthode CORI a été une source d'inspiration et a été utilisée comme moyen de classification dans beaucoup de travaux. Cette méthode fonctionne sur un système bayésien pour localiser des réponses probables aux utilisateurs. La fonction de score donnée dépend de certains paramètres obtenus à partir d'expérimentations sur des datasets. Ce paramétrage fait que CORI est devenue instable. Ces paramètres doivent être réajustés pour chaque nouvelle collection. Afin de réduire le nombre de collections interrogées, Abbaci et al. (2002) présente la méthode CS. Celle-ci définit ndoc le nombre de documents à retourner et tient compte uniquement des deux premiers termes lors de l'évaluation des requêtes longues. Bien que l'objectif de réduction de flux est atteint, CS produit des faux positifs et faux négatifs importants à cause des restrictions imposées. Soit un système distribué où un serveur appelé courtier est lié à un ensemble de serveurs. Le courtier détient un index Terme/Serveur qui indique pour chaque terme t i la liste des serveurs qui le manipulent. Chaque serveur S i est responsable d'une collection de documents c i et manipule un index Terme/Documents. Cet index définit pour chaque terme t i la liste des documents où il figure. Par cette définition, le courtier sélectionne de façon déterministe le sous-ensemble de serveurs pertinents. Ces index permettent de réduire la charge du système. Un document est jugé pertinent s'il partage au moins un terme avec la requête. Plus un document partage de termes avec la requête plus son degré de pertinence s'élève, induisant ainsi que le score d'une Sélection basée sur la pertinence
collection est proportionnel aux nombre de documents pertinents qu'elle contient. Sur cette définition, pour une requête q, le score d'une collection c i se calcule selon la fonction SDP suivante :
T F ti est la fréquence du terme t dans la collection c i . L'expérimentation des trois méthodes sur le dataset Reuters21578, sur un système distribué. La figure Fig. 1 présente la comparaison entre les trois méthodes en fonction du recall. Nous avons réalisé des expéri-mentations intensives en faisant varier le Top-k. Nous remarquons que les valeurs pour cette métrique sont plus grandes dans SDP que dans les autres méthodes. CS (CS2 pour ndoc=2, CS3 pour ndoc=3, CS5 pour ndoc=5) a retourné un recall plus faible. C'est certainement à cause du ndoc qui influence la recherche. Avec ndoc=2 et 3 ; le recall n'atteint pas 1 c-à-dire il existe des documents pertinents et rares où le système n'arrive pas à les sélectionner. CORI c'est placé au-dessus de CS.

Introduction
Avec les avancées technologiques en terme d'acquisition des données scientifiques (images satellitaires, capteurs, etc.), les scientifiques s'intéressent de plus en plus à des applications importantes en terme de surveillance et suivi de l'environnement. Les données collectées sont généralement hétérogènes, multiéchelles, spatiales et temporelles (série temporelle d'images satellites, aériennes, modèles numériques de terrain, nature du sol ...) et sont destinées à comprendre et prédire des phénomènes résultant de processus complexes et d'origine pluridisciplinaire (données climatiques, géologiques, ...). L'explosion de cette information spatiale, temporelle et des systèmes d'informations géographiques nécessitent l'investissement dans des méthodes d'extraction de connaissances et nous nous intéressons à celles qui reposent sur la détection de motifs locaux comme, par exemple, la découverte de motifs séquentiels (Agrawal et Srikant, 1995;Mannila et al., 1997;Masseglia et al., 1998) ou de motifs plus complexes comme des sous-graphes Inokuchi et al. (2000) ou des sous-arbres Zaki (2002). Nos besoins concernent l'étude spatiale et temporelle des évolutions d'objets et de leurs interactions. Les objets peuvent être caractérisés par plusieurs attributs et leurs évolutions que l'on appelle parfois dynamique se décrivent par les évolutions des attributs, par leur emplacement géographique, leur existence (apparition/disparition) et leur structure topologique (fusion/division). Pour certaines applications, nous pouvons transformer la base de données spatio-temporelles dans une base de données transactionnelles Hai et al. (2012) ou dans une base de séquences pour les analyser. Cependant, ces transformations peuvent s'avérer très fastidieuse et les résultats peuvent être difficilement interprétables. Des domaines de motifs plus sophistiqués et applicables à l'étude de phénomènes spatio-temporels ont donc été proposés. Ainsi, plusieurs travaux se sont intéressés à l'extraction de motifs dans des graphes étiquetés Inokuchi et al. (2000). Quelques travaux ont été menés dernièrement sur des graphes attribués (Fukuzaki et al., 2010;Miyoshi et al., 2009;Desmier et al., 2013). Les difficultés dans la fouille de graphes attribués résident dans l'explosion combinatoire de l'exploration de l'espace de recherche. En effet, cette espace de recherche porte à la fois sur les combinaisons de graphes et les combinaisons d'attributs. Dans un travail présenté dans (Sanhes et al. (2013a,b)), Sanhes et al. ont proposé de travailler à la modélisation de données spatio-temporelles dans des DAG attribués, autrement dit un unique graphe orienté acyclique attribué (a-DAG) (cf. Figure 1) : les sommets sont des objets spatiaux caractérisés par un ensemble d'attributs ou caractéristiques et les arcs dénotent la proximité spatio-temporelle entre ces objets (par exemple le voisinage spatial entre deux objets de deux pas de temps consécutifs). Le but est de trouver les transitions ou cheminements de caractéristiques pouvant montrer une tendance attendue ou surprenante, expliquer un phénomène particulier, ce qui revient à chercher dans un a-DAG les chemins fréquents d'attributs. On trouve quelques travaux s'attaquant à la fouille de graphes orientés FIGURE 1: Exemple de a-DAG construit sur des objets représentés dans des images temporelles acycliques mais étiquetés (et non pas attribués) tels que Chen et al. (2004);Termier et al. (2007). Ces méthodes recherchent des sous-graphes dans un ensemble de graphes, et de plus les sommets sont plutôt labélisés ou considérés comme labélisés et non attribués. Dans notre cas, on est en présence d'un seul graphe orienté acyclique et attribué (a-DAG) ce qui pose des problèmes très différents. Le domaine de motif proposé pour la première fois dans Sanhes et al. (2013b) est appelé domaine des chemins pondérés dans un a-DAG. Lorsque nous avons voulu travailler à une implémentation efficace de l'algorithme d'extraction de ce type de motif, le seul à notre connaissance qui calcule des motifs dans des DAG attribués, nous avons étudié de près ses propriétés et nous avons découvert qu'il était juste mais incomplet. Nous présentons ici un nouvel algorithme permettant de réaliser l'extraction de tels motifs de façon complète. Non seulement nous proposons une correction du premier algorithme mais aussi nous étudions des optimisations nécessaires au passage à l'échelle en introduisant des structures de données complémentaires comme un graphe de motifs. Nous montrons que la performance de l'extraction est améliorée de plusieurs ordres de magnitude sur des jeux de données artificiels et nous l'appliquons aussi à des données réelles pour motiver qualitativement l'usage des chemins pondérés.
Dans la section suivante, nous présenterons les concepts et définitions nécessaires à la compréhension de l'algorithme. En Section 3, nous prouvons l'incomplétude de l'algorithme existant. Nous proposons une solution de complétude et une optimisation basée sur une structure de graphe de motifs en Section 4. Nous montrerons les performances de l'algorithme complet et optimisé sur des jeux de données artificielles en Section 5. Et enfin, nous conclurons en Section 6. 
Un chemin P est une séquence d'itemsets P = P 1 2 · · · n tel qu'il existe un chemin O = v 1 2 · · · n dans le graphe où P i est inclus dans l'ensemble des items de v i (notion à différentier de la définition classique d'un chemin dans un graphe). On dit que alors que O est une occurrence du chemin P et l'ensemble des occurrences de P est noté occu G (P ).
Par exemple dans le graphe de la figure 2 les occurrences du chemin de taille 3 ah sont 2 3 6 , 2 3 8 , 2 4 7 , 2 5 7 , et 5 7 8 . Un chemin pondéré P est un chemin où un poids est associé à chaque arc P i i+1 constituant P . Ce poids correspond au nombre d'occurrences distinctes de P i i+1 dans le graphe. Pour l'exemple précédent, le chemin ah et ses occurrences permettent de construire le motif pondéré : ah 4 cd 5 i. En effet, le nombre d'occurrences de ah dans occur G (P ) st 4, et le nombre d'occurrences de cd dans occur G (P ) est 5. Une telle représentation permet de voir que l'itemset ah apparaît 4 fois avant l'apparition du chemin cd et que l'itemset i apparaît 5 fois après l'apparition du chemin ah Dorénavant, ? G (P i i+1 ) désignera le poids de l'arc entre les itemsets
Relation d'inclusion L'opérateur d'inclusion sur un couple de chemins pondérés est défini de la manière suivante :
Autrement dit, P est inclus dans P' s'il existe une sous-séquence Q de P' tel que les itemsets de P sont inclus un à un dans ceux de Q avec les mêmes poids au niveau des arcs. On dit que P est un sur-chemin ou super-chemin pondéré de P . A partir de la mesure proposée par Bringmann et Nijssen (2008), nous définissons le support d'un chemin pondéré P, noté ?(P ), comme étant le poids minimal de ses arcs.
Chemin pondéré condensé Un chemin pondéré P est un condensé s'il n'admet aucun surchemin pondéré. Pour simplifier, nous appellerons motif un chemin pondéré et les occurrences d'un motif seront tout simplement des chemins.
Cette méthode permet bien l'extraction des motifs condensés de manière juste mais ne les génère pas tous. En effet, toutes les graines condensées forment bien des motifs condensés mais un motif condensé de taille supérieure à 2 peut contenir des graines non condensées. Effectivement, il existe des motifs condensés au sens de l'inclusion qui peuvent être formés par certains motifs de taille 2 qui ne sont pas générés par la première étape.
Pour illustrer l'incomplétude de l'algorithme, nous montrons un contre-exemple sur le graphe de la figure 3. Dans ce a-DAG, les graines générées par la première étape de l'algorithme ne permettent pas de construire le motif condensé a 1 bc 1 de.
:a 2 :a
Condensés représentés dans le a-DAG :
• chemins de tailles 2 : a de où ? ne vaut plus 2 car toutes les occurrences de P ne sont pas utilisées : 2 4 n'est pas relié à de. Dans ce cas on parle d'extension avec perte d'occurrences. Par conséquent il faut déterminer les occurrences utilisées et mettre à jour les différents poids (?) ainsi que les itemsets du motif. En réalité avec la séparation des 2 étapes, l'information structurelle est perdue pendant le parcours en profondeur.
Algorithme complet et optimisé
Dans ce paragraphe, nous proposerons une solution permettant de corriger la complétude et nous proposerons par la même occasion une version optimisée utilisant une structure de graphe permettant de stocker les motifs que l'on appellera graphe de motifs.
... ?n I n un motif. Nous pouvons étendre P sans perte d'occurrences s'il existe un itemset I n+1 fermé fréquent dans l'ensemble des sommets accessibles par V n (nous appellerons I n+1 un fermé fréquent local à V n ), tels que V n+1 supporte I n+1 et qu'il existe au moins un arc de chaque sommet de V n vers V n+1 , c'est-à-dire que toutes les occurrences de P sont conservées. De manière analogue, nous pouvons étendre P avec perte d'occurrences lorsque toutes les occurrences de P ne sont pas utilisées lors de l'extension. Dans ce cas nous obtenons un motif P = I 1 ? 1
... 
Stratégie de l'algorithme complet
À partir des notions introduites précédemment, nous pouvons présenter la stratégie géné-rale de l'algorithme complet (cf. algorithme 1). Cet algorithme est basé sur un parcours en profondeur de l'espace de recherche pour étendre les motifs condensés.
En partant de l'ensemble des sommets du graphe, l'algorithme effectue un parcours en profondeur dans l'espace de recherche pour étendre le motif condensé P initialisé à ?. La ligne 1 exprime le cas d'arrêt de l'algorithme : l'ensemble des sommets destinations V P est vide. Le parcours en profondeur se fait aux lignes 2 et 3. L'extension de P se fait avec l'itemset Y fermé fréquent par rapport aux sommets de V P (ligne 4). Nous notons P le motif ainsi étendu. Lorsqu'il s'agit d'une extension avec perte d'occurrences, il est nécessaire de mettre à jour les occurrences du motif (ligne 5). Ce nouveau motif P est potentiellement un motif ou un sous-motif condensé. Nous supprimons les motifs qui sont inclus dans P , et insérons P dans C l'ensemble des motifs condensés (lignes 7 et 8). Puis nous continuons le parcours (ligne 9).
La complexité de l'algorithme ne permet pas le passage à l'échelle à cause de nombreux tests coûteux d'inclusion de motifs pour vérifier sa maximalité. Nous proposons ci-dessous une implémentation optimisée basée sur une structure de graphe pour stocker les motifs. Cette nouvelle structure va permettre d'éviter les tests trop coûteux de comparaison entre motifs.
Algorithme 1 : DepthFirstMining
Entrées : P motif courant (? à l'appel initial) V P ens. des sommets destinations de P (V P = V G à l'appel initial) C ens. des motifs condensés (? à l'appel initial) Mettre à jour les occurrences de P 6
Supprimer dans C les motifs Q inclus dans P 7
Insérer P dans C.  
Implémentation optimisée
Nous allons nous servir de la maximalité des chemins pondérés condensés recherchés pour optimiser l'algorithme qui se traduit par la maximalité des itemsets (itemsets fermés) du chemin et sa taille. Les tests d'inclusions de l'algorithme se font sur les itemsets et sur les chemins. Pour éviter ces tests nous allons définir une structure de graphe permettant le stockage des motifs trouvés au fur et à mesure du parcours de l'espace de recherche. Cette structure est appelée graphe de motifs. • V m ? P(V ) est l'ensemble des sommets • E m ? P(V ) × P(V ) est l'ensemble des arcs • ? m : la fonction d'attributs définie par : V m ?? P(I) V ?? X où X représente l'itemset maximal caractérisant les sommets de V . Réciproquement, on associe à un itemset X l'ensemble des sommets noté V X supportant l'itemset X. Un sommet du graphe G m est identifié par un ensemble de sommets du graphe G. Un motif condensé est alors un chemin c = V 1 n de G m de longueur maximale (cf. figure 5), i.e. V 1 est une source (pas d'arc incident) et V n est un puits (pas d'arc sortant).
Procédure cherCondRec(X : itemset, V X : ens. de sommets, min_sup : entier) sommets_a_remonter : var. globale contenant les sommets à backtracker
BdT (E + (V X )) la base de données transactionnelles construites à partir de ...
?n?1 I n , c'est une séquence d'itemsets et chaque itemsets I i du motif P représente un sommet dans le graphe des motifs qui n'est autre que V Ii ensemble des sommets du graphe a-DAG contenant l'itemsets I. Le motif P est alors identifié de manière unique par la séquence V I1 In dans le graphe des motifs. Au moment de la construction du graphe des motifs et à l'insertion d'un nouveau sommet V i dans le graphe des motifs, il suffit de vérifier s'il est déjà présent dans le graphe des motifs alors il a déjà été parcouru sinon il est inséré et le parcours de l'espace de recherche continue. L'algorithme final se déroule en 2 grandes étapes suivantes : -Recherche des motifs condensés par un parcours en profondeur de l'espace de recherche (procédure 2). Les motifs sont stockés dans le graphe des motifs. Pendant la recherche, les sommets pour lesquels il y a eu extension avec perte d'occurrences sont marqués pour être traités par la phase de backtracking.
-Phase de backtracking sur les sommets marqués (pour lesquels il y a eu extension avec perte d'occurrences) pour mettre à jour les occurrences des motifs (cf. procédure 3). La première étape fait appel à la procédure cherCondRec qui effectue un parcours en profondeur de l'espace de recherche. Cette procédure étend récursivement les motifs condensés au fur et à mesure de leur construction dans G m . A une étape de la construction du motif P , soit V X le sommet à étendre dans le graphe des motifs G m supportant l'itemset X, on calcule V X utile ensemble de tous les sommets ayant au moins un arc sortant de V X . On calcule tous les items accessibles par X, on obtient une base de données transactionnelles dont les transactions sont les arcs sortants et les items sont les items accessibles par X (ligne 3 et 4). Pour chaque itemset maximal Y dans cette base transactionnelle, on va étendre V X par V Y . Deux cas se présentent :
• V X = V X utile : tous les sommets de V X sont utilisés pour l'extension, il y a extension sans perte. Il suffit alors de créer le sommet V Y et le relier à V X par un arc dans G m .
• V X = V X utile : l'extension est réalisé avec perte d'occurrences, on duplique le motif P en remplaçant le sommet V X par V X utile (à marquer pour être traiter dans la phase de Backtracking). On insère un nouveau sommet V Y et on crée un arc entre
Procédure backtrackRec(V X utile : sommet du graphe des motifs, V toBacktrack : sommet du graphe des motifs, min_sup : entier)
Soit V i utile ensemble des sommets de V i ayant au moins un arc sortant vers V X utile .
4
Insérer V i utile ? > V X utile dans le graphe des motifs G m 5 // Lors de l'insertion, mise à jour des attributs. backtrackRec(V i utile , V i )// backtracking vers le haut 6 L'étape de backtracking décrite par la procédure récursive backtrackRec retraite chaque sommet marqué en visitant la branche du motif dans le sens inverse pour mettre à jour les occurrences des motifs et les informations tels que les poids et les itemsets. La figure 6 montre le déroulement de l'algorithme sur l'exemple du graphe de la figure 5. Les arcs du graphe de motifs sont en bleu. Les arcs en rouge définissent le cas d'extension avec perte d'occurrences, les arcs en vert montrent la phase de backtracking avec la mise à jour des poids et des itemsets.
Expérimentations et résultats
Nous avons appliqué la méthode sur des jeux de données artificielles pour montrer la performance que nous avons comparé avec la méthode incomplète. Pour montrer l'intérêt de ce nouveau domaine de motifs, nous avons utilisé un jeu de données réelles pour le problème de suivi du phénomène de l'érosion.
Dans un premier temps, nous avons créé artificiellement trois jeux de données afin d'observer l'impact de la taille des a-DAG sur les performances notés « V20K, E60K » pour un a-DAG   données artificiel ressemble plus aux jeux de données tirés d'une application spatio-temporelle.

Introduction
La quantité d'information dans le Web a augmenté ces dix dernières années. Ce phénomène a favorisé la progression de la recherche dans le domaine des systèmes de recommandation. Les systèmes de recommandation consistent en un filtrage de l'information dans le but de ne présenter aux utilisateurs que les éléments qui sont susceptibles de l'intéresser, quel que soit le domaine. Les éléments à recommander sont également appelés items et peuvent être de différents types : des produits, services, informations, etc. Les systèmes de recommandation se doivent de sélectionner les informations les plus intéressantes en fonction du but recherché, tout en conciliant nouveauté, surprise et pertinence. Un système de recommandation se base sur des caractéristiques de références acquises de manière automatisée selon plusieurs méthodes différentes. Les caractéristiques de références peuvent provenir de : -L'item (l'objet à recommander) lui-même, on parle alors « d'approche basée sur le contenu » (ou content-based approach) Balabanovi´cBalabanovi´c et Shoham (1997). Le filtrage basé sur le contenu calcule la similarité entre les objets afin de trouver l'objet le plus semblable aux goûts de l'utilisateur. Dans ce cas, l'utilisateur se voit recommander des items similaires à ceux qu'il a préférés dans le passé. -L'utilisateur et l'environnement social, on parle alors « d'approche de filtrage collaboratif » (ou collaborative filtering). Le principe du filtrage collaboratif Breese et al. (1998)   Koren et al. (2009). Pour remplir leurs fonctions, les technologies de recommandation font aujourd'hui face à des défis scientifiques majeurs. Comment intégrer l'hétérogénéité des sources d'information pour modéliser les préférences, comment prendre en compte le contexte, comment traiter efficacement ces masses d'information, quels types d'interfaces faut-il considérer ? Par ailleurs, les deux approches citées précédemment présentent des inconvénients principalement liés au démarrage à froid et à la montée en charge du système d'où la nécessité de mettre en place des algorithmes performants et robustes. Ceci est l'objectif de cette étude en vue d'amélio-rer la qualité des systèmes de recommandation en introduisant de la sémantique aux données et en distribuant les traitements afin de minimiser les temps de calcul. La sémantique est ici représentée par une ontologie du domaine (domaines des films pour les expérimentations).
Architecture globale
Afin de fournir une généricité dans le domaine d'application, un passage à l'échelle et une recommandation précise, nous proposons un système à trois couches : une couche de pré-analyse, une couche sémantique et une couche de recommandation.
Le module de pré-analyse met en oeuvre un filtre de comptage afin d'étudier en profondeur l'intérêt implicite des utilisateurs : le filtre permet de compter le nombre de fois qu'une valeur d'un attribut figure parmi les items évalués par les utilisateurs, pour cela nous utilisons un filtre de Bloom. Un filtre de Bloom Broder et Mitzenmacher (2004) est un tableau de bits qui permet de tester d'une manière rapide l'appartenance d'un élément à un certain ensemble. Le FBC, décrit dans Broder et Mitzenmacher (2004) est une extension du filtre de Bloom standard qui fournit la possibilité de supprimer des éléments du filtre. Le vecteur de bits y est remplacé par un tableau d'entiers, où chaque position est utilisée comme compteur. L'insertion d'un élément est réalisée en incrémentant de 1 les entiers aux positions renvoyées par les fonctions de hachage. Le retrait est réalisé en décrémentant de 1 ces entiers. La question d'appartenance d'un élément au filtre est traitée en regardant si tous les entiers aux positions renvoyées par les k fonctions de hachage sont strictement positifs. Nous proposons de se baser sur l'ontologie du domaine afin d'extraire les attributs des items. Ensuite, nous utilisons les filtres de bloom avec compteur afin de stocker l'intérêt implicite des utilisateurs dans les attributs des éléments. Ceci se fait en suivant les étapes : (1) Pour chaque utilisateur, nous créons un filtre de bloom avec compteur vide, (2) pour chaque élément noté par cet utilisateur, nous extrayons ses attributs et enfin (3) nous insérons ces attributs dans le filtre . Ainsi, le filtre contient tous les attributs des items précédemment notés par l'utilisateur.
Le module sémantique exploite l'ensemble des données ainsi que l'ontologie dans le but de définir les relations entre les utilisateurs, les items et les attributs. Ceci se traduit par la transformation sémantique des notes des utilisateurs. Nous nous intéressons tout d'abord au nombre d'occurrence des attributs qui ont été notés par un utilisateur. Nous appelons cette occurrence « la fréquence d'apparition » ou « coïncidence » : cette valeur correspond au nombre de fois que les valeurs des attributs se répètent dans les items notés par l'utilisateur. Cette valeur est extraite à partir des filtres de bloom avec compteurs.
La deuxième étape consiste à calculer la valeur sémantique (SV) en se basant sur la fré-quence d'apparition. L'équation utilisée est la suivante (1).
Avec F le nombre total des attributs, N u le nombre total des items notés par l'utilisateur "u". C j est la fréquence d'apparition de l'attribut j dans l'ensemble des items qui ont été notés par l'utilisateur et W j étant le poids calculé à partir de la phase de la sélection des attributs par une analyse des composantes principales.
E[r u, * ] est la moyenne des notes de l'utilisateur et r u,i est la valeur du rating initial donnée à l'item "i". L'utilisation de N permet de normaliser l'équation sémantique. Cette équation a l'intérêt de pouvoir prendre en compte des valeurs positives et/ou négatives comme note.
L'équation sémantique peut être appliquée à deux niveaux dans la recommandation. D'une part, nous pouvons appliquer cette équation à toutes les notes disponibles dans la base de données initiale, ce qui permet de mieux expliquer l'intérêt des utilisateurs pour les caractéristiques définissant les items notés (ajouter du sens à la note). D'autre part, nous pouvons faire le choix d'appliquer l'équation sémantique à la sortie de la recommandation. Supposons que le module de recommandation renvoie un résultat des top K items (les K items les plus pertinents) pour un utilisateur donné, avec une estimation de la note pour ces top K. Ces notes seront transformées en une note sémantique suivant l'équation (1) et les items proposés seront réordonnés en conséquence en top K', K' pouvant être inférieur ou égal à K.
Enfin, le module de recommandation utilise une technique de filtrage collaboratif basée sur une méthode de factorisation de la matrice pour générer des recommandations précises. Nous avons fait le choix d'utiliser la factorisation de matrice car cette technique a montré son efficacité comme méthode de filtrage collaboratif pour la recommandation Koren et Bell (2011 Nous avons testé le module sémantique au travers des deux approches : -Application du module sémantique au jeu de données (Semantic dataset). Dans cette approche, l'application du module sémantique (que nous appelons "sémantisation") porte sur les données en entrée du système de recommandation. Il s'agit donc de traiter l'ensemble du jeu de données, avant son analyse par le système de recommandation et le filtrage collaboratif. Cette approche nous permet de retrouver des objets non pris en compte a priori. Cependant, puisque le module sémantique doit analyser tout le jeu de données, le temps de calcul peut être grandement augmenté par rapport à une analyse non sémantique. -Application du module sémantique au résultat de la recommandation Top K (semantic top K). Dans cette première approche, nous cherchons à "sémantiser" les données en sortie du système de recommandation. Le système de recommandation fournit classiquement une liste de K objets, ordonnés par ordre décroissant de préférence. La sé-mantisation réordonne ces objets, et fournit une nouvelle liste plus pertinente. Cette approche est extrêmement légère, et permet d'améliorer la recommandation sans (trop de) perte de temps. De plus, avec cette approche, les paramètres de pondérations peuvent être personnalisés par l'utilisateur plutôt que de considérer l'ensemble du jeu de données. Néanmoins, une analyse a priori des données de l'utilisateur est nécessaire avant de personnaliser les paramètres de pondérations. Dans la section suivante, nous présentons comment ces deux approches modifient les recommandations, et la qualité des résultats obtenus.
F-Mesure
Cette métrique est généralement utilisée dans l'évaluation des systèmes de recommandations. Cette métrique n'évalue pas la qualité de la prédiction des notes, mais la pertinence des items qui sont proposés aux utilisateurs. La F-mesure est une façon courante de combiner le rappel et la précision dans une seule métrique afin de faciliter la comparaison. le rappel étant la probabilité qu'un item choisi soit pertinent et la précision calcule la probabilité qu'un item pertinent soit choisi. tel que nous pouvons le constater dans la figure 1, nos approches donnent
FIG. 2 -ILS in "genre" attribute.
de meilleurs résultats que la technique de matrice de factorisation (SVD++) sans sémantique. l'amélioration est plus prononcée pour le cas de la" sémantisation" du jeu de données.
ILS
ILS (Intra-List Similarity), appelée également ILD (Intra-List Diversity) mesure la diversité/similarité entre les items dans la liste des top-k présentée à l'utilisateur. Un bon système de recommandation doit trouver l'équilibre entre ces deux concepts diversité et similarité. En effet, des items trop diversifiés peut provoquer une confusion chez l'utilisateur, alors que recommander toujours les mêmes items peut ennuyer celui-ci. Le figure 2 représente cette mesure en se concentrant sur le attribut genres des films dans le top-k. Des valeurs élevées correspondent à une grande similarité. Ainsi, nous pouvons constater que notre approche permet de retourner des items plus similaires dans le top-k. Ceci est du au fait que nous prenons en compte l'intérêt pour les attributs afin d'identifier les items susceptibles d'intéresser l'utilisateur.
Conclusion
Nous avons proposé un système de recommandation qui repose sur deux concepts : relations sémantiques entre les données manipulées et un filtrage collaboratif basé sur la factorisation des matrices. Dans le but d'améliorer la pertinence des recommandations, nous avons étudié en profondeur l'intérêt implicite des utilisateurs pour les attributs des items. pour cela, nous appliquons une équation sémantique permettant de modifier les notes initiales des utilisateurs pour refléter leur intérêt pour les items.
Notre système de recommandation opère en plusieurs étapes : Nous comptons le nombre de fois qu'un attribut figure dans les items notés par les utilisateurs. Pour cela, nous nous sommes appuyés sur l'utilisation d'un filtre de bloom avec compteur. Ensuite, après passage par le module sémantique (transformation des notes des utilisateurs en appliquant l'équation sémantique) , les recommandations sont générées en utilisant une technique de matrice de factorisation (SVD++).
L'approche proposée dans ce papier a montré un intérêt pour la recommandation de films en utilisant le jeu de données MovieLens combiné à une ontologie de films, peuplé par les données de IMDB. Nous avons fait le choix de travailler avec ce jeu de données car il est disponible et public et c est celui qui est le plus utilisé dans les expérimentations autour des systèmes de recommandation. Toutefois, nous avons conçu notre approche indépendamment du jeu de données que nous avions utilisé. Notre approche peut être utilisée comme une boite noire nécessitant de se connecter à une base de données des ratings disponibles, mais aussi à l'ontologie du domaine de l'application. Nous envisageons de tester notre approche sur des jeux de données provenant d'autres applications telles que la recommandation nutritionnelle ou de tourisme. Nous envisageons également d'intégrer dans notre approche la prise en compte des notes négatives (attributs non aimés par l'utilisateur).

Introduction
Traditionnellement, les données hydrométriques se présentent sous la forme de séries temporelles, représentant des mesures effectuées régulièrement par des stations : ces mesures peuvent concerner différents aspects comme les hauteurs et les débits de l'eau dans les cours d'eau, les quantités de précipitations, etc. Comme ces mesures sont souvent prélevées par un réseau distribué de capteurs, le problème des données manquantes est inévitable. Allant d'une simple valeur manquante à une longue plage de valeurs manquantes, les lacunes peuvent avoir des causes multiples : dysfonctionnement des capteurs, maintenance des stations de mesure, erreurs humaines, etc. (Harvey et al. (2010)).
Le réseau hydrométrique au Luxembourg fournit un bon cas d'utilisation. Il est constitué de différentes stations hydrométriques permettant de mesurer notamment les débits des cours d'eau. Les mesures sont ensuite fréquemment utilisées dans les modèles numériques de prévi-sion hydrologique ou pour calculer des statistiques sur les écoulements (e.g. temps de retour des crues ou des sécheresses).
En conséquence, lorsque certaines séries de mesures présentent beaucoup de lacunes (par exemple : les données de la station de HallerBach au Luxembourg, Figure 1), cela pose de nombreux problèmes et il est nécessaire d'apporter un soin très particulier à combler ces lacunes avec une bonne précision.
Afin de combler ces lacunes, les méthodes classiques d'analyse de données ont été appliquées dans le domaine hydrologique (Salas (1980)), et des travaux récents tentent de fournir des solutions toujours plus efficaces (Harvey et al. (2010); Mwale et al. (2012)  Kotsiantis (2013)). -Les réseaux de neurones artificiels ont récemment été utilisés, notamment via des perceptrons (Tfwala et al. (2013)) ou des cartes auto-adaptatives (Mwale et al. (2012)). -L'algorithme espérance-maximisation (EM) est souvent utilisé pour reconstruire des données manquantes (Van Hulse et Khoshgoftaar (2008)). -Enfin, différentes techniques de prédiction de séries temporelles peuvent être utilisées suivant les caractéristiques des séries (ARMA, ARIMA, etc.). Habituellement, les experts en hydrologie se servent de divers scripts pour corriger les données (R, MATLAB). Ainsi est-il important pour eux de pouvoir disposer d'un outil interactif pour à la fois intégrer les diverses sources de données, visualiser les séries temporelles, et enfin choisir le mode d'estimation de valeurs manquantes le plus adapté.
2 gapIT : un outil pour estimer les données manquantes 2.1 Technologie gapIT est une application développée en JAVA et basée sur le logiciel d'analyse et de traitement de données Cadral (Pinheiro et al. (2014) 
Inspection et caractérisation des valeurs manquantes
Les valeurs manquantes ne se corrigent pas de la même manière selon le contexte (taille des trous, saison durant laquelle les valeurs sont manquantes, probabilité qu'une crue soit en cours, etc.) (Gyau-Boakye et Schultz (1994)  Premièrement, la phase de sélection des stations de référence est critique, car elle permet à l'expert de choisir les séries temporelles qui serviront à compléter les séries temporelles lacunaires. Pour ce faire, l'outil propose plusieurs approches complémentaires :
-la sélection des stations les plus proches géographiquement ; -la sélection des stations se trouvant sur le même cours d'eau (en amont et/ou en aval) ; -la sélection des stations ayant les séries temporelles les plus similaires sur la période concernée ; la similarité est calculée en utilisant la déformation temporelle dynamique (Dynamic Time Warping) (Berndt et Clifford (1994)). Deuxièmement, l'outil propose diverses méthodes d'estimation : interpolation, régressions multiples, arbres de régressions, réseaux de neurones (perceptron multi-couches), algorithme des plus proches voisins, etc.
Troisièmement, l'utilisateur peut ensuite appliquer via le logiciel la méthode d'estimation sélectionnée en se basant sur les séries temporelles choisies pour évaluer les valeurs man-quantes. Or, il est important de déterminer la précision des estimations produites. Pour ce faire, des trous fictifs sont créés dans la fenêtre de temps proche du trou réel à remplir (par exemple : un trou avant, un trou après, élargissement du trou en cours d'examen ou à un endroit choisi par l'utilisateur). Ensuite, les mesures suivantes sont calculées : l'erreur absolue moyenne (MAE), la racine carrée de l'erreur quadratique moyenne (RMSE) et le coefficient Nash-Sutcliffe car c'est un indicateur très commun en hydrologie (Nash et Sutcliffe (1970)).
Pour finir, l'outil est capable de calculer automatiquement la configuration optimale (stations de référence et algorithme). Dans ce cas, l'utilisateur garde la possibilité de modifier la configuration à son gré de manière à obtenir un nouveau résultat plus proche de ses attentes.
3 Exemple : les débits des cours d'eau au Luxembourg gapIT a été utilisé pour estimer les débits d'écoulement manquants pour des stations sélec-tionnées au Luxembourg, sur la période allant du 1er janvier 2007 au 31 décembre 2013. Le jeu de données utilisé correspond à des mesures effectuées toutes les quinze minutes dans 24 stations. Afin de tester l'efficacité de l'outil, un ensemble de trous fictifs a été créé pour ces stations. Pour obtenir un ensemble représentatif, les trous générés sont de différentes tailles, se situent durant différentes saisons, etc. Ensuite, pour chacun de ces trous fictifs, toutes les techniques d'estimation proposées par l'outil ont été testées, et pour chacun des cas, les taux d'erreur ont été mesurés (MAE, RMSE, Nash-Sutcliffe).
Ainsi, nous avons constaté que les réseaux de neurones et les arbres de régression permettent d'obtenir les taux d'erreur les plus faibles. De plus, si l'on considère le meilleur ré-sultat concernant chaque trou, alors on voit que les taux d'erreur sont globalement très faibles. Cela signifie que pour ces cas, une estimation correcte est possible en utilisant les données pré-sentes (Figure 4). En revanche, dans un certain nombre de cas, les meilleurs taux d'erreurs sont élevés. Après analyse, il s'avère que pour les stations concernées, il n'existe pas suffisamment de stations assez proches, similaires ou dépendantes afin de créer une estimation assez précise. 

Contexte
La recrudescence des documents textuels disponibles sur le web incite de plus en plus travaux à l'exploitation de ces données de manières automatiques. Pour faire interagir ces données entre elles de manière efficace, il faut développer des moyens basés non seulement sur la ressemblance syntaxique mais également sur la correspondance sémantique.
GEOLSemantics est une entreprise qui propose une solution logicielle de traitement linguistique basée sur une analyse linguistique profonde. Le but est d'extraire automatiquement, d'un ensemble de textes, des connaissances structurées, localisées dans le temps et l'espace. Pour représenter ces connaissances, nous avons opté pour les technologies du web sémantique. Nous représentons nos extractions sous forme de triplets RDF et exploitons une ontologie pour apporter de la cohérence. Cette approche permet de relier les résultats de nos extractions aux connaissances du Linked Open Data, tels que Dbpedia et Geonames.
Lors de l'analyse linguistique, il arrive que l'information traitée contienne des imperfections. Dans notre travail, nous intéressons à l'incertitude. Notre première contribution porte sur une catégorisation de l'incertitude lors des différentes phases d'extraction. Notre seconde contribution se situe au niveau de la représentation de l'incertitude dans le graphe RDF.
2 Acquisition de l'information avec incertitude L'analyse des textes comporte plusieurs étapes distinctes allant du simple découpage du texte en mots à la représentation de son contenu. Parmi ces étapes, nous retrouvons : (i) l'analyse syntaxique, il s'agit de la mise en évidence des structures d'agencement des catégories grammaticales, afin d'en découvrir les relations formelles ou fonctionnelles. (ii) l'analyse sé-mantique, l'objectif principal de cette analyse est de déterminer le sens des mots des phrases. (iii) l'extraction de connaissances permet de mettre en évidence des entités nommées et des relations relatives à un concept particulier. Grâce à des déclencheurs qui indiquent qu'une relation relative à un concept peut être présente et extraite. Un déclencheur correspond généra-lement à un concept présent dans l'ontologie, ce qui permet de guider la règle d'extraction par la suite. (iv) la mise en cohérence permet de consolider les connaissances extraites notamment le regroupement des entités nommées, la résolution des dates relatives. Cette étape peut être Gestion de l'incertitude. suivi par un enrichissement à partir des données du Linked Open Data. Cependant, la fiabilité de l'information est très souvent remise en cause. Notre démarche est de considérer le cycle de vie de la connaissance depuis son acquisition jusqu'à son stockage dans la base de connaissances pour cela, nous identifions trois catégories : Pré-extraction de la connaissance : il s'agira lors de cette étape de considérer les modalités de publication de l'information à savoir : la date et le lieu de publication, la fiabilité accordée à la source, qu'il s'agisse de l'auteur ou de l'organisme de publication... Pendant l'extraction de la connaissance : l'incertitude pourra concerner aussi bien l'information véhiculée que la règle d'extraction à appliquer. Post-extraction de la connaissance : l'incertitude peut intervenir au niveau des règles de mise en cohérence ou bien au niveau du choix de la base de référence.
Le formalisme de représentation de connaissances choisie est le RDF tout en nous basons sur une ontologie développée pour prendre en compte les concepts relatifs à un domaine particulier. Notre approche consiste à considérer l'incertitude comme une connaissance à part entière telle que le décrit l'ontologie suivante. La classe Uncertainty, nous permet de modéliser l'incertitude. Elle est décrite par trois proprié-tés : weight : une propriété littérale pour quantifier l'incertitude identifiée, hasUncertainProp : une propriété objet qui servira d'intermédiaire entre le domaine initial de la propriété et la propriété en question, isUncertain : propriété objet qui aura pour co-domain le top-concept, cela veut dire que tout concept de l'ontologie pourra être visé par une incertitude. Cette ontologie est indépendante de tout domaine d'application. Dès lors, elle peut être ajoutée à toute autre ontologie voulant prendre en compte l'incertitude.
Conclusion et perspectives
Dans cet article nous nous intéressons au traitement de l'information incertaine dans le cadre d'une extraction de connaissances à partir de texte. Le traitement repose sur les technologies du web sémantique pour permettre de faire le lien avec les données du Linked Open Data. Notre démarche consiste à identifier les différentes situations où une incertitude remettant en cause la validité de l'information peut subsister. Nous proposons une ontologie pour modéliser l'information incertaine et la représenter au format RDF. Nous travaillons actuellement sur développement d'un ensemble de patterns pouvant faciliter l'interrogation du graphe RDF prenant en compte notre représentation de l'incertitude. Nous prévoyons par la suite de développer un raisonneur basé sur le formalisme des logiques possibilistes afin de permettre l'inférence sur les données incertaines.
Summary
The knowledge representation area needs some methods that allow to detect and handle uncertainty. Indeed, a lot of text hold information whose the veracity can be called into question. These information should be managed efficiently in order to represent the knowledge in an explicit way. As first step, we have identified the different forms of uncertainty during a knowledge extraction process, then we have introduce an RDF representation for these kind of knowledge based on an ontologie that we developped for this issue.

Introduction
L'utilisation d'ontologies s'est montrée très efficace dans bien des domaines. La taille et la dynamique des domaines considérés demande toutefois l'exploitation combinée de plusieurs ontologies, d'où la nécessité d'établir des correspondances sémantiques, ou mappings (Euzenat et Shvaiko, 2007), entre ontologies. Ainsi, la qualité des résultats produits par les systèmes utilisant des ontologies dépend de la validité des mappings entre ontologies, ce qui oblige des experts du domaine à réviser leur définition lorsque les ontologies évoluent. Si cette maintenance peut être effectuée manuellement sur de petits ensembles de mappings, une approche plus automatique est nécessaire lorsqu'ils sont volumineux, comme dans le domaine médical. L'existence de mappings erronés est souvent dûe à l'évolution des ontologies, les erreurs d'alignement mises à part (Dos . Il est alors fondamental de comprendre l'évolution des ontologies pour pouvoir agir sur les mappings afin de garantir leur validité. Ce faisant, nous avons proposé un ensemble de patrons de changement permettant de caractériser l'évolution des concepts d'une ontologie en analysant les changements dans la définition des concepts . Nous avons également observé sur des jeux de données réelles le comportement des mappings dans le temps, ce qui nous a permis d'identifier un ensemble d'actions d'adaptation pouvant s'appliquer aux mappings pour les faire évoluer (Dos . L'objet de cet article formalise, sous forme d'heuristiques, le lien entre les patrons de changement et les actions d'adaptation pour faire évoluer les mappings lorsque les ontologies liées évoluent. Aprés avoir introduit les concepts de notre approche (Section 2), en particulier les patrons de changement et les actions d'adaptation, nous présentons les heuristiques proposées (Section 3) et le cadre expérimental emprunté au domaine biomédical pour les évaluer (Section 4) et les discuter par rapport à l'existant (Section 5) avant de conclure (Section 6).
Préliminaires
Nous présentons ici les notions et notations utilisées pour définir notre approche. Soit O t , semT ype) relie deux concepts c s ? C X et c t ? C Y par la relation sémantique semT ype, telle que semT ype ? {?, <, >, ?} (cf. tableau 1). 
TAB. 1 -Notations pour la formalisation des heuristiques
Patrons de changement caractérisant l'évolution d'ontologies
Nos travaux ont montré que l'évolution des ontologies rend nécessaire d'adapter les mappings. Nous pensons qu'une compréhension précise de cette évolution va nous renseigner sur la façon d'adapter les mappings au cours du temps. Pour caractériser l'évolution des ontologies, nous avons proposé un ensemble de patrons de changement . Contrairement à ceux de la littérature (Djedidi et Aufaure, 2009), (Javed et al., 2013), (Gröner et al., 2010), nos patrons considèrent les changements syntaxiques et sémantiques au niveau des attributs des concepts. Ce choix a été motivé par les résultats expérimentaux obtenus montrant que la définition des mappings repose sur certaines valeurs d'attributs (Dos .
Les Une Copie Totale caractérise le changement à travers lequel une valeur d'attribut devient également la valeur d'un attribut d'un autre concept. Par exemple, un attribut a 1 d'un concept c 1 a pour valeur "portal systemic encephalopathy" au temps j. Au temps j + 1, a 1 a la même valeur, mais un attribut a 2 d'un concept c 2 aura également cette valeur.
Une Copie Partielle est une copie d'une partie de la valeur d'un attribut. Un attribut a 1 d'un concept c 1 a la valeur "familial hyperchylomicromenia" au temps j. Au temps j + 1, a 1 garde cette valeur, mais un attribut a 2 aura "familial chylomicromenia" comme nouvelle valeur.
Un Transfert Total correspond au transfert de la totalité de la valeur d'un attribut à un autre attribut. Contrairement au cas TC, la valeur originale de l'attribut n'est pas conservée.
Un Transfert Partiel définit le transfert d'un partie de la valeur d'un attribut. Par exemple, un attribut a 1 peut valoir "eye swelling" au temps j. Au temps j + 1, cette valeur sera supprimée partiellement de a 1 mais un attribut a 2 vaudra "head swelling" (i.e., le terme "swelling" est déplacé de a 1 vers a 2 entre j et j + 1).
Les Patrons de changement sémantiques (SCP) s'intéressent à l'évolution de la séman-tique de la valeur des attributs au cours du temps. Nos observations ont montré qu'à travers leurs évolutions successives, les concepts pouvaient devenir plus généraux, plus spécifiques ou rester équivalents, modifiant ainsi la relation sémantique des mappings. Les 4 SCP que nous proposons sont : Equivalent (EQV), Plus Spécifique (MSP), Moins Spécifique (LSP) et Recouvrement Partiel (PTM).
Equivalent stipule que les changements syntaxiques au niveau de la valeur de l'attribut ne modifient pas sa sémantique. Par exemple, la valeur d'un attribut peut passer de "Diabetes type 1" à "Diabetes type I" sans en affecter le sens.
Plus Spécifique identifie un changement rendant un concept plus spécifique que sa nouvelle version. Le changement menant de "kappa light chain disease" à "kappa chain disease" rend le premier plus spécifique du fait de la suppression du qualificatif "light". Moins Spécifique décrit l'effet inverse.
Recouvrement Partiel identifie un changement au niveau de la sémantique ne pouvant être caractérisé par les autres SCP. Considérons la valeur originale "focal atelectasis" et son évolu-tion "helical atelectasis". Ces deux valeurs font toutes deux référence à la notion de "atelectasis", mais ne peuvent être déclarées ni équivalentes, ni plus ou moins spécifiques. ModSemTypeM(m st , semT ype) consiste en la modification de semT ype (la relation sé-mantique) à cause des modifications sur les concepts sources (c s ) et/ou cibles (c t ).
Les actions d'adaptation de mappings
NoAction(m st ) est appliquée lorsque les modifications sur les concepts sources et/ou cibles n'entraînent pas de changements sur la sémantique des mappings.
Heuristiques d'adaptation des mappings
Nous présentons dans cette section des heuristiques indiquant sous quelles conditions adapter les mappings afin qu'ils restent valides dans le temps. Ces heuristiques ont été établies expérimentalement à partir de l'observation de l'évolution des mappings entre des jeux de données réelles et de l'analyse de l'impact des patrons de changements sur leur évolution. Des heuristiques ont été définies pour chaque type d'adaptation.
Heuristiques pour les mappings de type Move et Derive
Soit Cand l'ensemble des concepts dits candidats regroupant les concepts du contexte de c s en j + 1 pour lesquels il existe un changement de type LCP entre un de leurs attributs et un attribut de c s . Soit la fonction topA(c s , c t , n) retournant les n attributs expliquant le mieux le mapping entre c s et c t . Soit la fonction SLCP (a 1 , a 2 ) retournant VRAI s'il existe un changement de type LCP entre a 1 et a 2 et FAUX sinon. Soit la fonction SCP (a 1 , a 2 ) retournant les changements de type SCP entre a 1 et a 2 ou ? si aucun changement de type SCP n'a été identifié.
MoveM. Expérimentalement, nous avons observé que l'adaptation d'un mapping de type MoveM correspondait à l'existence de changements de type LCP entre attributs. Plus précisé-ment, nous avons observé que lorsque l'adaptation du mapping est de type MoveM, il n'existe qu'un seul attribut de c s au temps j expliquant le mapping avec un changement de type LCP avec un attribut du contexte de c s au temps j + 1. De ce fait, nous appliquons l'adaptation de type MoveM lorsqu'il n'existe qu'un seul concept candidat avec un changement de type LCP  Figure 1). Intuitivement, cela signifie que le mapping suit l'évolution des attributs qui l'expliquent.
Soit les concepts '128829008' "Acute myeloid leukemia, 11q23 abnormalities (morphologic abnormality)" et 'C6924' "Acute Myeloid Leukemia with 11q23 MLL Abnormalities" issus de SNOMED CT et de NCI. Deux changements de type LCP se sont produits lors de l'évolu-tion de 'C6924'. Un transfert total s'est produit sur l'attribut qui expliquait le mieux le mapping ("Acute Myeloid Leukemia with 11q23 Abnormalities"). Une copie totale a eu lieu pour un autre attribut qui expliquait aussi le mapping. Ces deux changements concernent le même concept candidat 'C82403' "Acute Myeloid Leukemia with t 9 11 p22 q23 MLLT3-MLL" dans la nouvelle version de l'ontologie, mais deux attributs différents expliquant le mapping. Suite à l'évolution de 'C6924', le mapping relie dorénavant les concepts '128829008' et 'C82403'.
DeriveM. De façon similaire à l'action MoveM, DeriveM est appliquée lorsque plusieurs changements entre attributs de type LCP sont reconnus suite à une évolution. Le mapping original est préservé et c s existe dans la nouvelle version de l'ontologie. Le fait que plusieurs changements de type LCP concernent les attributs pertinents de c s et qu'il existe donc plusieurs concepts candidats conduit à la création de nouveaux mappings entre ces candidats et c t (cf. Figure 2). Formellement, nous définissons cette heuristique de la façon suivante :
En guise d'illustration, considérons le mapping 'plus spécifique que (<) entre '41452004' -"Uterus acollis (disorder)" dans SNOMED CT et '752.3' -"Other anomalies of uterus" dans ICD-9-CM. L'analyse de l'évolution de 752.3 permet d'observer plusieurs changements de type LCP concernant des concepts candidats différents et également plusieurs changements de type SCP. Par example, il y a une copie totale (TC) de l'attribut "Other anomalies of uterus" expliquant le mapping vers un attribut de '752.33' qui fait apparaître une équivalence. En plus, l'attribut "Bicornuate uterus" est totalement transféré (TT) dans '752.34' faisant également apparaître une relation d'équivalence. L'action d'adaptation à réaliser, selon notre heuristique, est DeriveM en l'appliquant aux deux concepts candidats concernés par les changements de type LCP et en gardant la même relation sémantique que la relation du mapping original.
3.2 Heuristiques associées à la modification de relation sémantique L'application de l'action ModSTR dépend des changements de types SCP trouvés entre attributs. Nous avons identifié deux scénarios différents. Le premier concerne la modification de la relation du mapping original m 0 st alors que c s ne change pas (cf. Equation 3). Le second scénario concerne la modification de la relation du mapping original suite à un MoveM ou DeriveM (cf. Equation 4). Dans le premier cas, la nouvelle relation sémantique lie le concept source (en terme de contenu) au temps j + 1 et le concept cible alors que dans le second cas, c s est remplacé par un concept candidat (un concept appartenant au contexte de c s ). Le type de la relation sémantique après évolution est obtenu en combinant le type de la relation du mapping original semT ype et le type d'un changement de type SCP détecté entre attributs. Soit la fonction getSemType qui fournit la relation sémantique obtenue en combinant le type d'un mapping original semType (x) avec des relations identifiées par des patrons de type SCP, SCPs (y). Par example, si la relation sémantique entre c 
Voici les heuristiques associées au premier et deuxième scénario :
st ? {?, <, >, ?}, semT
ct ? {?, <, >, ?}, semT , nous sélectionnons les concepts candidats au temps j + 1 à partir du contexte de c s au temps j. Les expérimentations à partir desquelles cette heuristique a été proposée ont montré que la similarité entre les attributs expliquant un mapping et les attributs des concepts du contexte de c s après évolution était très faible lorsque le mapping était supprimé (Dos . Ceci a conduit à introduire une condition portant sur la similarité dans l'heuristique proposée.
No action. L'heuristique pour NoAction considère que des patrons de type LCP et SCP ne sont pas observables (cf. Equation 6). Elle s'applique dans des cas où les changements portant sur un concept lié par un mapping n'ont pas d'effet sur les attributs expliquant ce mapping, ou lorsque la similarité avec de nouveaux attributs du contexte est faible (Dos .
Evaluation
Cette section porte sur l'évaluation des heuristiques proposées. Nous étudions si les changements identifés dans une ontologie conduisent à des adaptations correctes (i.e. MAAs) des mappings affectés. Cette évaluation a été réalisée sur plusieurs versions d'ontologies biomédi-cales (SNOMED CT et ICD-9-CM) et plusieurs versions de mappings associés.
Protocole d'expérimentation
4. Deux types de mesure, représentées respectivement par and sont proposées pour évaluer de manière rigoureuse les actions proposées (cf. Résultats obtenus dans le tableau 2). Cette distinction est bien appropriée à l'évaluation des actions MoveM, DeriveM and ModSTR. Le symbole exprime la précision, le rappel et la F-mesure lorsque les actions proposées sont correctes par rapport aux actions attendues, en plus du concept candidat ou de la relation sémantique. Le symbole correspond aux cas où seul le type d'action est correct, le concept candidat ou la relation sont erronés (par exemple, MoveM ou DeriveM est proposé mais pas avec le bon concept candidat à j+1). La mesure est plus contrainte, elle pourrait conduire à des valeurs plus faibles. Ainsi, si nous observons un MoveM avec un concept c obs du contexte et si notre mécanisme propose un MoveM avec le même concept c obs , nous choisissons de mesurer la précison, le rappel et la F-mesure de (action d'adaptation, concept lié et relation sémantique exacts). En revanche, si seule l'action MoveM est correcte, nous faisons le calcul correspondant à Nous procédons de la même façon pour ModSTR, en considérant en plus de l'action d'adaptation du mapping, le type de la relation sémantique.
5. Enfin, nous effectuons un calcul global qui calcule la précision, le rappel et la F-mesure pour chaque jeu de données, indépendamment des types d'action d'adaptation.
Résultats et discussion
Les résultats de l'adaptation des mappings basés sur nos heuristiques sont présentés dans la Mapping Move. Pour ce type d'action, les résultats varient selon que l'évolution concerne ICD-9-CM ou SNOMED CT uniquement pour la précision. Ainsi, nos heuristiques peuvent générer des adaptations plus ou moins correctes selon les jeux de données. Les différences de résultats entre et étant très faibles, on remarque que, la plupart du temps, lorsque notre système propose un MoveM, c'est avec un bon concept candidat du contexte.
Mapping Derive. Les résultats obtenus sont plus élevés que ceux de Move pour SNOMED CT. L'action DeriveM est complexe car l'action propose plusieurs DeriveM par mapping. Aucune action DeriveM n'a été observée pour ICD-9-CM. Là encore, les conditions d'application des heuristiques sont différentes selon les jeux de données. L'explication peut provenir du processus de maintenance, de matching ou de la granularité des ontologies alignées.
Modification de la relation sémantique. L'évaluation de cette heuristique a été difficile car les mappings dont nous disposions contenaient peu de relations différentes (la plupart était des équivalences). Néanmoins, les résultats montrent que les nouvelles relations proposées quand une action de type ModSemTypeM est détectée, sont pertinentes (surtout pour SNOMED CT). Ce type d'adaptation est généralement combiné avec un MoveM ou un DeriveM ce qui oblige à choisir la bonne action avant de changer la relation. Parfois, les changements dans l'ontologie ne sont pas l'unique raison pour changer la relation. Cela influe négativement sur nos résultats. Enfin, considérer la nouvelle version des mappings comme référence peut aussi poser problème. C'est un ensemble de mappings qui a subi des évolutions mais ce n'est pas à proprement parler un jeu de données de référence. Une relation sémantique proposée peut être considérée comme fausse par notre processus d'évaluation alors qu'elle peut être correcte d'un point de vue sémantique. L'intervention d'un expert serait nécessaire pour y remédier.
Mapping Remove. La précision et le rappel sont relativement bons. Ils ne varient que légèrement selon l'ontologie. La F-mesure minimale est de 0.54. La prise en compte du retrait de l'attribut expliquant le mieux le mapping dans l'heuristique semble être une bonne décision.
Application d'aucune action. Ce type d'adaptation couvre le plus grand nombre de cas que ce soit en valeur absolue (5892 attendus dans SNOMED CT et 3139 dans ICD-9-CM) ou relative. Les résultats montrent une grande efficacité des heuristiques. On constate que, bien que les concepts source ou cible évoluent, si les attributs expliquant les mappings qui les concernent restent inchangés, ces mappings n'évolueront pas non plus. Les très bons ré-sultats obtenus avec l'application de cette heuristique permettent aux experts de se concentrer sur l'étude d'une toute petite partie des mappings (en comparaison à l'ensemble initial), ces mappings étant plus difficiles à adapter et pouvant nécessiter une intervention humaine.
Résultats globaux. Nous avons analysé les résultats en combinant tous les types d'adaptation pour avoir une idée de la qualité du processus général d'adaptation. La F-mesure est élevée (un min de 0.85 dans ICD-9-CM). L'action d'adaptation NoAction influe beaucoup sur ces résultats. Néanmoins les résultats sont globalement acceptables et prometteurs même si certaines actions sont difficiles à appliquer. Ceci montre que les conditions d'application des heuristiques que nous proposons sont adaptées. L'amélioration des résultats liés à certaines actions nécessiterait de rechercher quels autres éléments influencent les types d'adaptation et d'étudier comment les prendre en compte dans les heuristiques.
Etat de l'art
Nous distinguons trois principaux types d'approches pour la maintenance des mappings due à l'évolution d'ontologies. La première consiste à identifier et réparer les mappings invalides (Meilicke et al., 2008)  (Ivanova et Lambrix, 2013). Ces approches effectuent des raisonnements logiques pour identifier les mappings produisant des incohérences logiques. Ce mécanisme ne s'applique que sur des ontologies formelles, qui n'existent pas toujours.
Les travaux s'inscrivant dans la deuxième catégorie reposent sur des techniques de ré-alignement total ou partiel d'ontologies. Si le premier type d'approche ne considère aucune information provenant de l'évolution, le second recoupe l'ensemble des concepts modifiés au niveau des ontologies avec ceux impliqués dans des mappings pour ne réaligner que ce sousensemble de concepts (Khattak et al., 2012). Ces approches sont coûteuses en temps de calcul et de validation lorsque les ontologies sont volumineuses (Shvaiko et Euzenat, 2013).
La troisième catégorie, inspirée du monde des bases de données (Velegrakis et al., 2003), fait référence à des approches qui profitent au maximum des informations provenant de l'évo-lution d'ontologies pour éviter de les réaligner totalement. Tang et Tang (2010) ont proposé une méthode visant à trouver l'impact minimal de la propagation des changements au niveau d'une ontologie. Martins et Silva (2009) suggèrent d'adapter les mappings en procédant de la même façon que lors de la modification des concepts de l'ontologie qui évolue. Cependant, dans leur approche, les mappings ne sont adaptés que lorsque les concepts sont supprimés. L'originalité de nos travaux, par rapport à cet état de l'art, réside dans : (i) l'importance donnée aux modifications des ontologies sous-jacentes, (ii) à leur caractérisation et (iii) à la prise en compte du changement au niveau de la relation sémantique des mappings, ce dernier point étant souvent négligé dans les approches existantes qui ne considèrent qu'un seul type de relation (Dos Reis et al., 2012).
Conclusion
Dans cet article, nous avons proposé un ensemble d'heuristiques guidant l'adaptation des mappings entre ontologies. Ces dernières formalisent le lien existant entre les changements identifiés au niveau des éléments ontologiques et les actions d'adaptation à appliquer sur les composants des mappings pour préserver leur validité. L'approche décrite a été validée expé-rimentalement sur des données réelles du monde biomédical. Le manque de données de ré-férence nécessite l'implication d'experts du domaine mais le travail réalisé permet de réduire considérablement le temps nécessaire aux experts pour valider les adaptations proposées.

Introduction
Qui n'a pas dit, un jour, en écoutant la radio : "Mais ça ressemble à Supertramp ou à une musique de Chopin ou à une musique baroque" ? Sur la base d'un court morceau écouté on peut en effet identifier directement l'auteur ou le placer dans une catégorie même si on ne connait pas forcément le morceau. Si c'est un chanteur, on le reconnait facilement au timbre de sa voix, pour un morceau de musique classique, l'interprétation peut varier et on détecte plutôt la ligne musicale. Pour les documents textuels, ce problème d'authentification d'auteur présumé est récurrent, et la fouille de texte peut s'avérer très utile. Ainsi, par exemple pour authentifier une élégie de Shakespeare en 1995 1 des techniques telles que le comptage exclusif des mots et la prise en compte de mots rares ont été employés avec succès (Foster (1996)). Le champ littéraire n'est cependant pas le seul concerné. Le problème d'authentification d'un auteur apparait aussi dans bien d'autres applications, comme dans le domaine juridique par exemple pour l'authentification d'un testament ou dans le cadre des investigations anticriminelles ou antiterroristes pour identifier la provenance d'une demande de rançon ou de posts émis sur des forums de discussion du Dark Web (Abbasi et Chen (2005)). Le marketing peut également être intéressé par le profiling des auteurs des blogs ou des commentaires sur le Web.
Dans le cas de textes écrits, on peut plus généralement distinguer trois variétés de problèmes liés à la détermination d'un auteur inconnu :
-l'extraction de profil (Author Profiling) : il s'agit d'indiquer à partir d'un texte des éléments du profil de son auteur comme, par exemple, la tranche d'âge et le genre ( Rangel et al. (2013)) ou l'appartenance à une catégorie particulière comme celle des criminels potentiels ) -la reconnaissance de l'auteur (Author Verification ou Author Recognition) : il s'agit de vérifier parmi une liste des auteurs possibles lequel est le bon -l'identification d'un auteur : il s'agit de décider si un texte donné a été écrit par l'auteur d'un autre groupe de documents Ainsi, dans ces trois problèmes, on s'interroge sur l'auteur d'un document écrit, et dans ces trois cas pour répondre, il est nécessaire de représenter de façon appropriée le document à explorer et de pouvoir le comparer avec d'autres. Toutefois, nous pensons qu'il est illusoire de rechercher une empreinte d'un auteur sur un texte qu'on pourrait comparer avec des empreintes extraites d'autres textes et qui serait unique au même titre qu'une empreinte digitale. Nous pensons qu'il faut utiliser divers espaces de représentation pour les textes à analyser selon la langue d'origine ou encore le genre ou la qualité du document. Dans cet article, où nous nous intéressons plus spécifiquement à des problèmes d'identification d'auteurs à partir de documents rédigés dans diverses langues et de différents types (textes littéraires courts ou longs, articles de presse ou publications, blogs) nous avons exploré différents modes de représentation des documents. Nous avons ensuite proposé de formaliser l'identification d'auteurs comme un problème de classement que nous avons résolu de trois façons : à l'aide d'un algorithme original de comptage de similarité (DCM), puis avec deux autres méthodes qui exploitent ce comptage, par une technique de vote (DCM-voting), par apprentissage automatique (DCM-classifier).
Notre article est organisé de la manière suivante : après la section 2 consacrée aux travaux relatifs à l'identification d'auteurs, nous définissons plus formellement le problème dans la section 3, puis nous décrivons les trois méthodes proposées pour le résoudre dans la section 4. La section 5 présentera les résultats des expériences réalisées afin d'évaluer l'intérêt de ces approches et de les comparer à celles de l'état de l'art. Des conclusions seront présentées dans la dernière section.
Etat de l'art
L'identification d'auteur peut être définie comme un problème de classification de textes : Etant donné un ensemble, grand ou réduit à un seul élément, de documents d'un même auteur, il faut déterminer si un nouveau document a été écrit par le même auteur que les autres ".
Il s'agit donc d'un problème de classement supervisé binaire dont la réponse attendue est binaire ("oui" ou "non") ou une probabilité d'appartenance à l'ensemble de documents fournis. Toutefois, une des spécificités de ce problème de classement est que seuls des exemples d'une des deux classes sont donnés : les documents rédigés par l'auteur, mais la seconde classe n'est pas explicitée. De plus, parfois le nombre d'exemples positifs est réduit à un seul document, ce qui rend la tâche particulièrement difficile.
Pour pallier l'absence d'exemples négatifs, on peut essayer d'en produire. C'est la voie explorée par différents auteurs parmi lesquels figurent (Seidman (2013)) qui construisent une classe d'"imposteurs" choisis aléatoirement sur la base des dix mots les plus fréquents figurant dans les documents disponibles pour remplir la classe du "non". D'autres auteurs, comme Zhang et al. (2014) et Halvani et al. (2013), transforment ce problème de classification à deux classes en un problème avec plusieurs classes, soit en rajoutant des classes extérieures, soit en transformant la classes initiale en plusieurs. Les même auteurs (Halvani et al. (2013)) augmentent la taille de la classe des documents connus quand celle-ci est réduite à un seul document. Ainsi, ces approches permettent de revenir à un problème classique de classement supervisé mais, lors de la construction des exemples négatifs, elles sont confrontées au risque de choisir des documents trop proches ou trop éloignés des documents déjà fournis.
Outre la question des données disponibles pour résoudre le problème, l'identification d'auteurs est ensuite confrontée à deux autres questions classiques en fouille de texte : comment représenter les documents et, une fois l'espace de représentation choisi, quelles méthodes appliquer pour résoudre le problème de classement ?
Comme nous l'avons déjà remarqué, l'identification d'auteurs peut être réalisée à partir de documents très différents : méls (de Vel et al. (2001); Chaurasia et Kumar (2010)), programmes, parties des oeuvres littéraires ou parties des documents de la vie de tous les jours, texte plat (Zhang et al. (2014)), extraits de chat (Inches et Crestani (2013)) ou séquences de commandes Unix (Szymanski et Zhang (2004)). Le choix des caractéristiques examinées, on parle des caractéristiques stylométriques, dépend du type de document, parfois de la langue et aussi de la qualité du texte initial. Les caractéristiques dites "spécifiques" aux applications portent plutôt sur le comptage des tabulations et autres séparateurs ou l'analyse des caracté-ristiques spécifiques, telles que la position des parenthèses et des crochets fermants pour les programmes, et des lignes vides pour les méls. Les caractéristiques sémantiques sont prises en compte plutôt pour des textes issus du web (forum et chat), comme, par exemple, l'usage des abréviations ou des mots démonstratifs fréquents ("well") ou des transcriptions concentrées des expressions orales ("sse u"). On peut également considérer des caractéristiques syntaxiques comme des fautes d'orthographe ou les abréviations. Si on se place dans un cadre générique (authentification d'auteur dans diverses langues et dans divers genres), on utilise plutôt des caractéristiques de type caractère ou mot, ou suites de caractères ou de mots (n-grams) (Chaurasia et Kumar (2010); Szymanski et Zhang (2004)). On peut aussi avoir recours à un étiqueteur lexical et syntaxique mais son usage augmente considérablement le temps de traitement (Juola et Stamatos (2013)) et les résultats vont dépendre de sa qualité (Vilariño et al. (2013)). Lorsque le choix de ces caractéristiques est fait, les documents peuvent être transformés en vecteurs en utilisant, le plus souvent, tf-idf comme pondération ou uniquement la fréquence. Ensuite, selon la représentation du texte adoptée, on peut comparer les documents à l'aide de fonctions "classiques" de similarité telles que le cosinus, la corrélation, moins souvent des mesures de compression de données comme la Fast Compression Distance (Cerra et al. (2014)) ou la Common N-Gram dissimilarity (Layton et al. (2013)).
Pour ce qui concerne la résolution du problème de classement lui-même, on peut appliquer des méthodes "classiques" telles que les k plus proches voisins (k-NN) (Zhang et al. (2014); Ghaeini (2013); Halvani et al. (2013)) ou les SVM (Vilariño et al. (2013)). Certains auteurs (Dam (2013); Layton et al. (2013) ;Jankowska et Milios (2013)) proposent des méthodes basées sur le choix d'un seuil ou d'un vote et des formules de calcul de l'éloignement entre le document d'auteur inconnu et les autres. Les différences entre ces approches résident dans la phase de prétraitement, dans l'extraction des caractéristiques et dans le choix du seuil et de la fonction de dissimilarité.
Par rapport aux travaux antérieurs, notre contribution se situe dans un cadre plus large avec l'objectif de proposer une méthodologie générique, applicable à des collections très différentes tant par le genre des documents que par le langage. Ceci nécessite la mise en place d'une approche permettant de choisir automatiquement la représentation textuelle la mieux adaptée pour un corpus donné.
Définition du problème et représentation des documents
Le problème d'identification d'auteur peut être défini de la façon suivante. Etant donné un corpus composé de documents d'un même type (mel, blog, roman, code, etc.) écrits dans un même langage (anglais, français, Java, etc.) on dispose pour chaque problème p d'un ou plusieurs documents A p du corpus qui ont été rédigés par un même auteur et d'un document u p dont l'auteur est inconnu. L'objectif est de déterminer si u p a été écrit ou non par le même auteur que les documents de A p . Si on dispose d'un échantillon d'apprentissage, autrement dit d'un ensemble de problèmes P tel que pour chaque problème p ? P on sait si le document inconnu u p a été rédigé ou non par le même auteur que les documents associés A p , alors on peut formaliser le problème comme un problème de classement supervisé binaire et le résoudre à l'aide de méthodes d'apprentissage automatique. La difficulté consiste alors à déterminer d'une part un ou des espaces de représentation des documents appropriés et d'autre part à construire à partir de ces représentations des facteurs descriptifs des documents inconnus permettant de prédire efficacement si chaque document u p a été ou non produit par le même auteur que les documents de A p qui lui sont associés. Parmi les modèles les plus connus et les plus utilisés pour représenter des documents figure le modèle tf-idf introduit par Salton et al. (1975). Un document d est représenté par un vecteur (w 1 , . . . , w j , . . . , w |T | ) tel que le poids w j du terme t j dans d correspond au produit de la fréquence tf j du terme t j dans d par le pouvoir discriminant idf (j) de t j . Ce modèle est très efficace notamment pour identifier des termes (caractères, mots ou séquences de mots ou caractères correspondant à des n-grams) qui sont fréquents dans un document et rares dans les autres. Mais, comme nous l'avons souligné en introduction, d'autres caractéristiques peuvent être prises en compte pour représenter les documents. De plus, nous pensons qu'il n'existe pas un modèle de représentation universel adapté à tous les documents mais que le choix de cet espace de représentation doit dépendre du type de documents et du langage.
Ceci nous a conduit à considérer d'autres espaces de représentation indiqués dans le tableau 1. Outre le modèle tf-idf défini à partir des mots, avec élimination des mots outils à l'aide d'un dictionnaire (R5) ou en considérant leur fréquence (R4), des suites de mots ou de caractères (R1, R2, R3), nous avons introduit trois autres modèles de représentation (R6, R7 et R8) visant à caractériser le style d'écriture du document. Dans le modèle R6, la moyenne et l'écart type du nombre de mots par phrase sont associés au document. Le modèle R7 attribue à chaque document une mesure de diversité du vocabulaire définie comme le nombre de mots différents employés divisé par le nombre total d'occurrences de mots (i.e. la longueur du document). Le modèle R8 correspond au modèle de Salton dans lequel on considère les caractères de ponctuation au lieu des termes (mot ou caractère n-grams). Enfin, le modèle R678 correspond à la concaténation des trois modèles précédents : chaque document est représenté par un vecteur indiquant la moyenne par phrase des caractères de ponctuation " :" , " ;" , ",", la moyenne et l'écart type du nombre de mots par phrase et la diversité du vocabulaire. Ponctuation nombre moyen de signe de ponctuation par phrase caractères pris en compte : "," " ;" " :" "(" ")" " !" " ?" R678 Concaténation R6 + R7 + R8 TAB. 1: La liste de espaces de représentation considérés
DCM, DCM-voting et DCM-classifier
Ayant choisi un des espaces de représentation, on peut comparer les documents deux à deux à l'aide de mesures de similarité comme le cosinus et le coefficient de corrélation ou avec la distance euclidienne et appliquer une des trois méthodes (DCM, DCM-voting, DCMClassifier) que nous avons proposées pour résoudre le problème d'identification d'auteur. La première méthode DCM permet de traiter directement le problème d'identification p ? P , en considérant uniquement les similarités entre les vecteurs décrivant les documents suivant un des espaces. Les deux autres méthodes, basées sur DCM, permettent de combiner diffé-rentes représentations des documents, par une méthode de vote dans le cas de DCM-voting, à l'aide d'une méthode d'apprentissage supervisée nécessitant la construction d'attributs prédic-tifs pour DCM-classifier. Ces différentes méthodes sont décrites dans les sections suivantes.
Méthodes de comptage de similarités : DCM et DCM-voting
Etant donné un problème p ? P défini par un ensemble A p de documents rédigés par un même auteur et un document u p dont l'auteur est inconnu, représentés dans un même espace, et un seuil de décision ? , la méthode DCM, décrite par l'algorithme suivant, fournit en sortie la valeur True si l'auteur de u p est le même que celui des documents de A p ou la valeur False dans le cas contraire. Cette méthode exploite les similarités (ou distances) entre tous les documents disponibles. Elle consiste à assigner le document u p au même auteur que les documents de A p si la plupart d'entre eux sont plus proches de u p qu'ils ne le sont des autres documents de A p . Plus précisément la plus grande similarité de chaque document d x ? A p aux autres documents de A p est calculée puis comparée à la similarité de d x à u p . Si la première est inférieure à la seconde, un compteur est incrémenté. Après examen de tous les documents de A p (fin de la boucle for extérieure), il comptabilise la proportion de documents de A p qui sont les plus proches de u p que des autres de A p et si cette proportion est supérieure au seuil fixé ?, alors l'auteur du document inconnu u p est le même que celui des autres documents. Cette méthode présente l'avantage d'être simple et rapide à mettre en oeuvre et elle permet de traiter un problème d'identification p ? P indépendamment des autres. En revanche, elle n'exploite qu'un seul mode de représentation des documents.
Pour pallier ce défaut, on peut avoir recours à une méthode de vote DCM-voting consistant simplement à appliquer la méthode DCM en considérant plusieurs espaces de représentation des documents, de préférence en nombre impair, puis à affecter le document inconnu à la classe majoritairement retournée par les différentes exécutions. Cependant, comme nous l'avons souligné précédemment, tous les espaces de représentation ne sont pas équivalents et il serait souhaitable de pouvoir ajuster leur poids dans la décision finale ; ce qui est difficile à faire en pratique même pour un expert ; de même que le choix du seuil ?. Pour toutes ces raisons, nous proposons une autre méthode plus générale permettant d'exploiter simultanément plusieurs modes de représentation des documents et d'ajuster automatiquement, par apprentissage automatique, leur importance dans l'identification de la classe des documents inconnus.
La méthode DCM-classifier
Dans le cadre de l'apprentissage supervisé, on suppose que pour un sous-ensemble P A de problèmes de P , on sait en fait si les documents inconnus u p ont ou non été produits par le même auteur que les documents qui lui sont associés i.e. on dispose en plus de la classe class(u p ) des documents inconnus à savoir même auteur ou auteur différent. Ce sousensemble P A est décomposé en un échantillon d'apprentissage P a utilisé pour construire un modèle de décision et un échantillon test employé pour l'évaluer. La phase d'apprentissage permet de mettre en relation des facteurs descriptifs (ou attributs) des documents avec leur classe de façon à pouvoir ensuite identifier l'auteur d'un nouveau document dont la classe est inconnue uniquement à partir de ces facteurs descriptifs. Il est clair que la qualité du modèle dépend largement du pouvoir prédictif de ces facteurs que nous proposons de définir de la façon suivante.
Pour chaque espace de représentation R v , v ? {1, .., V }, chaque document u p est décrit par deux attributs count v (u p ) et mean v (u p ) respectivement définis à l'aide d'une mesure de similarité s par :
Un dernier attribut T OT count (u p ), basé sur tous les espaces de représentation est égale-ment calculé afin d'avoir une description plus synthétique. Il est défini par :
Ainsi lors de l'apprentissage, on considère les documents u p de chaque problème p de P a décrit par ces attributs descriptifs prédictifs et par leur classe réelle (
Compte tenu du caractère numérique de ces attributs descriptifs, plusieurs méthodes d'apprentissage supervisé peuvent alors être employées ( SVM, etc). Dans le cadre des expérimentations, nous avons privilégié les arbres de décision qui ont l'avantage d'intégrer une phase de sélection des attributs en fonction de leur pouvoir prédictif ; ce qui permet de favoriser selon la famille de problèmes considérés tel ou tel espace de représentation. De plus, ils permettent aussi d'ajuster automatiquement les paramètres du modèle. Les résultats obtenus sur chaque corpus des collections ev2013, app2014 et ev2014 ont été évalués à l'aide des indicateurs habituels de précision, de rappel et avec la mesure F 1.
Expérimentations et résultats
Le taux d'erreur indiqué par la mesure F 1 étant très synthétique, pour comparer les mé-thodes, on a également utilisé l'indicateur de performance AU C qui mesure l'aire de la courbe ROC (Davis et Goadrich (2006)).
Pour la collection ev2014, seuls les indicateurs de performances calculés par la plateforme du challenge pour chaque corpus sont disponibles : AU C, l'indicateur c@1, le produit des deux indicateurs et le temps d'exécution. L'indicateur c@1 permet de donner plus d'importance à une réponse correcte par rapport à l'absence de décision (i.e. une probabilité d'appartenance à la classe de 0.5). Cet indicateur est défini par :
où n est la taille du corpus, n c est le nombre de réponses correctes, n u le nombre de problèmes laissés sans décision.
Résultats sur la collection 2013
Pour la méthode DCM, nous avons utilisé la représentation R1 (caractère 8-grams) qui avait donné les meilleurs résultats sur la collection d'apprentissage de 2013 et fixé ? à |Ap| 2 alors que pour DCM-voting, nous avons privilégié les espaces de représentation R1, R2, R3, R4 et R678 (cf. Tableau 1). Pour les quatre espaces ayant trait aux mots ou aux n-grams caractères, les documents sont représentés sous format vectoriel avec la pondération tf ? idf .
La table 3 présente les résultats produits par les trois méthodes sur la collection eval13 ainsi que les résultats des gagnants de la compétition par corpus puis sur l'ensemble de la collection. La faible précision de DCM-classifier pour le corpus espagnol peut s'expliquer par le manque de problèmes pour cette langue (seulement quatre) dans le corpus d'apprentissage rendant difficile la construction d'un modèle performant. Les trois méthodes produisent des résultats satisfaisants cependant, DCM et DCM-voting sont limités aux problèmes contenant au moins deux textes connus. Si on compare les résultats obtenus par nos méthodes avec ceux des gagnants de la compétition par corpus, alors il n'y a que sur le corpus grec que la méthode DCM-classifier l'emporte avec un score de 85%. Par contre, sur l'ensemble de la collection, DCM-voting comme DCM-classifier obtiennent des résultats meilleurs ou équivalents à ceux du gagnant pour tous les critères d'évaluation (F1, précision et rappel) . TAB. 4: Résultats de la 10-cross validation de DCM-classifier sur app2014
Résultats sur les collections 2014
La collection 2014 contient un nombre plus élevé de problèmes et de types de document que la collection de 2013 et elle s'avère plus difficile à traiter puisque plus de la moitié des problèmes ont un seul document connu (|A| < 2) ; ce qui rend les méthodes DCM et DCMvoting inadaptées et inefficaces. Pour cette raison, seule DCM-classifier a été évaluée, d'abord sur la collection d'apprentissage en utilisant la technique de 10-validation croisée qui consiste à séparer le corpus à traiter en deux groupes, un pour entrainer le modèle et l'autre pour le tester, puis sur la collection d'évaluation dans le cadre du challenge.
Les résultats obtenus par validation croisée sur l'ensemble d'apprentissage sont présentés dans la table 4. Ils confirment les performances de la méthode DCM-classifier.
Le tableau 5 contient les résultats officiels obtenus lors de la compétition PAN14 in Author Identification, Stamatatos et al. (2014). Ils permettent de comparer notre méthode à celles des autres participants. DCM-classifier nous a permis d'être classé en deuxième position lors de la compétition. Elle fournit de bons résultats en un temps relativement court. Il convient de noter que les temps de traitement affichés par le gagnant de la compétition sont en moyenne supé-rieurs à trois heures alors que ceux de DCM-classifier sont de l'ordre de quelques secondes.
Un des avantages de la méthode DCM-classifier, basée sur les arbres de décision, est de mettre en évidence les caractéristiques qui permettent le mieux d'identifier l'auteur d'un do- 1/6 1% GR T OTcount 1/6 8% SP TAB. 6: Classement des attributs cument selon le type de corpus considéré. En effet, les documents sont décrits par des attributs calculés sur différents espaces de représentation mais l'apprentissage intègre une phase de sé-lection de ceux qui sont les plus discriminants. Ainsi, on peut en déduire pour chaque corpus l'importance de chaque attribut.
Les figures 1 et 2 présentent les différents espaces de représentation utilisés pour deux corpus de langues différentes, on voit que ces espaces sont très différents et que les poids rattachés le sont aussi. La table 6 indique de manière synthétique la liste des attributs les plus utilisés sur l'ensemble des corpus de la collection d'évaluation 2014. Ce résultat confirme l'intérêt de combiner plusieurs espaces de représentation pour résoudre le problème d'identification d'auteurs.

Introduction
Les systèmes de recommandation (SR), visent à recommander à des utilisateurs des ressources pertinentes pour eux. Le filtrage collaboratif (FC) (Resnick et al., 1994) est une des approches les plus populaires de la recommandation.
Bien que la qualité des recommandations fournies par le FC soit considérée comme satisfaisante en moyenne (Castagnos et al., 2013), certains utilisateurs ne reçoivent pas de recommandations de qualité. Le manque de données sur ces utilisateurs est une des raisons possibles expliquant cette mauvaise qualité. Ce problème est appelé démarrage à froid (Schein et al., 2001). Parmi les autres raisons évoquées dans l'état de l'art se trouve la trop grande différence des préférences de ces utilisateurs, par rapport à celles des autres (Haydar et al., 2012). C'est sur cette raison que nous nous focalisons dans cet article. En effet, le filtrage collaboratif suppose une cohérence entre les préférences des utilisateurs ; ces utilisateurs ne respectant pas ce critère, il semble normal qu'ils se voient proposer des recommandations de mauvaise qualité. Ces utilisateurs peuvent aussi être considérés comme des données aberrantes, ou outliers. Nous choisissons de les appeler des utilisateurs atypiques.
Notre objectif ici est d'identifier ces utilisateurs atypiques. Nous proposons, dans ce travail préliminaire, plusieurs mesures permettant de les identifier, en exploitant uniquement leurs préférences.
La section 2 se focalise sur les systèmes de recommandation et l'atypisme. Dans la section 3, nous proposons des mesures d'identification des utilisateurs atypiques. Ensuite, nous pré-sentons les expérimentations menées pour valider ces mesures et nous concluons notre travail.
État de l'art
La recommandation sociale, ou filtrage collaboratif (FC), exploite les préférences d'utilisateurs (en général des notes sur des ressources) pour estimer des préférences inconnues. L'approche à base de mémoire, et notamment les k plus proches voisins (knn), exploite les similarités de préférences entre utilisateurs. Bien que simple à mettre en oeuvre, intégrant dynamiquement les nouvelles préférences et fournissant des recommandations de qualité, cette approche ne passe pas à l'échelle. L'approche à base de modèle souffre moins du problème de passage à l'échelle. La technique de factorisation de matrices (Hu et al., 2008), la plus répan-due, forme un sous-espace de caractéristiques latentes, dans lequel utilisateurs et ressources sont représentés, qui permet d'estimer les préférences inconnues.
Dans la littérature, les utilisateurs que nous appelons atypiques sont nommés déviants, anormaux, etc. (Del Prete et Capra, 2010) et la définition qui en est faite varie légèrement. Les travaux dédiés à leur identification sont peu nombreux. La mesure d'anormalité (Del Prete et Capra, 2010;Haydar et al., 2012) aussi appelée coefficient de déviance, déviance, etc., est la plus utilisée pour les identifier. Elle représente la propension qu'a un utilisateur à noter différemment des autres. Elle exploite l'écart entre les notes d'un utilisateur sur des ressources et la note moyenne sur ces ressources (équation (1)).
où n u,r est la note que l'utilisateur u a donné à la ressource r, n r la note moyenne sur r, R u l'ensemble des ressources notées par u et u leur nombre. Les utilisateurs dont l'anormalité est très élevée sont considérés comme atypiques. Bien que peu complexe, cette mesure ne tient pas compte du comportement propre à chaque utilisateur et les ressources sur lesquelles les utilisateurs ne sont pas unanimes vont injustement augmenter l'anormalité. (Bellogín et al., 2011) définit un indicateur de clarté qui identifie les utilisateurs ambigus (instables) dans leur notation, basé sur la mesure de l'entropie. Il a l'inconvénient d'identifier comme instables des utilisateurs dont les préférences évoluent ou dont les préférences diffèrent en fonction des domaines des ressources. Cette mesure ne nous paraît pas adéquate car l'approche sociale peut leur proposer des recommandations de bonne qualité. (Bellogín et al., 2011;Haydar et al., 2012;Griffith et al., 2012) identifient un lien entre l'erreur commise sur chaque utilisateur et ses caractéristiques (nombre de notes, de voisins, etc.). (Haydar et al., 2012), forme des clusters d'utilisateurs et identifie un cluster d'atypiques : des utilisateurs avec une forte erreur et un fort indice d'anormalité. Cependant, nous pensons que les atypiques ne sont pas toujours similaires entre eux (ce qui en ferait d'ailleurs des utilisateurs non atypiques au sens de la recommandation sociale), le clustering échouera probablement à former des clusters d'atypiques. C'est dans ce sens que va le travail présenté dans (Ghazanfar et Prugel-Bennett, 2011), qui clusterise des utilisateurs et propose de considérer comme atypiques les utilisateurs qui ne sont proches du centre d'aucun des clusters formés.
L'identification d'utilisateurs atypiques peut être associée à l'identification de données aberrantes (outliers) : un outlier est une donnée qui dévie tellement des autres données que cela laisse penser qu'elle a été générée par un mécanisme différent. Les méthodes statistiques et le clustering sont également très utilisées dans le domaine de la détection d'outliers (Aggarwal, 2013).
Nouvelles mesures d'identification d'utilisateurs atypiques
Partant des travaux de la littérature, nous proposons de nouvelles mesures permettant l'identification d'utilisateurs atypiques.
CorrKMax -Nous pensons que l'approche knn échoue sur les utilisateurs n'ayant pas suffisamment d'utilisateurs similaires. CorrKM ax représente la similarité moyenne qu'a un utilisateur u avec ses k utilisateurs les plus similaires (équation (2)).
où CorrP earson(u, v) est la corrélation de Pearson entre u et v. V (u) représente les k utilisateurs les plus similaires à u. Nous pensons que les utilisateurs associés à une faible valeur de CorrKM ax(u) recevront des recommandations de mauvaise qualité.
Anormalité CR (Anormalité avec Controverse sur les Ressources) -Cette mesure repose sur la mesure d'anormalité de l'état de l'art. Elle suppose que l'écart sur une ressource controversée n'a pas le même sens qu'un même écart sur une ressource consensuelle, ce qui n'est pas considéré par la mesure d'anormalité de l'état de l'art. Nous proposons de pondérer les notes par le degré de controverse de la ressource, fonction de l'écart-type de ses notes. L'Anormalite CR d'un utilisateur u est présentée dans l'équation (3).
où contr(r) est la controverse associée à une ressource r. Cet indice est basé sur l'écart-type normalisé des notes sur r. Où contr(r) = 1 ? ?r??min ?max??min , avec ? r est l'écart-type des notes de r. ? min et ? max sont respectivement le plus petit et le plus grand écart-type de notes possibles parmi les ressources.
Le calcul d'Anormalité CR est d'une complexité comparable à celle de l'anormalité de l'état de l'art. Elle peut donc être calculée fréquemment et ainsi prendre en compte les nouvelles préférences des utilisateurs.
Anormalite CRU (Anormalité avec Controverse sur les Ressources et profil Utilisateur) -Ni Anormalité ni Anormalité CR ne tiennent compte du comportement général de l'utilisateur auquel elles s'appliquent. Un utilisateur sévère peut être identifié comme atypique alors qu'il n'est atypique que dans sa manière de noter, et non dans ses préférences et qu'il recevra probablement des recommandations de qualité. Pour éviter ce biais, nous proposons de centrer les notes de chaque utilisateur par rapport à sa moyenne de notes. L'Anormalité d'un utilisateur u, notée Anormalité CRU (u) est calculée selon l'équation (4) :
où n Cr représente la moyenne des notes centrées des utilisateurs sur r, contr C (r) est l'indice de controverse associé à r, calculé à partir de l'écart-type des notes sur la ressource, centrées par rapport aux utilisateurs. Le calcul de Anormalité CRU (u) est certes plus coûteux en temps que Anormalité CR (u), mais devrait permettre une identification plus précise des utilisateurs atypiques. Notons que ces deux dernières mesures sont indépendantes de l'approche de recommandation utilisées (knn ou factorisation de matrices), à l'opposé de la mesure CorrKM ax.
Expérimentations
Dans cette section, nous évaluons les mesures d'identification des utilisateurs atypiques que nous proposons, en comparaison avec celles de l'état de l'art.
Nous utilisons le corpus de données de l'état de l'art MovieLens, composé de 100 000 notes (de 1 à 5) de 943 utilisateurs sur 1 682 films (ressources). Une division du corpus en 2 sous-corpus de 80% (pour l'apprentissage) et 20% (pour le test) est effectuée. L'état de l'art souligne que les utilisateurs pour lesquels le système manque de données (démarrage à froid) reçoivent de mauvaises recommandations. De façon à ne pas biaiser notre évaluation, nous écartons du corpus les utilisateurs associés à du démarrage à froid : ceux avec moins de 20 notes dans le corpus d'apprentissage (Schickel-Zuber et Faltings, 2006). Le corpus est alors réduit à 821 utilisateurs. La mesure Anormalité de l'état de l'art présente une corrélation de 0,453 avec la RMSE. Cette corrélation significative confirme le lien existant entre l'anormalité d'un utilisateur et l'erreur commise par une approche knn : plus un individu est anormal (atypique), plus l'erreur commise sera élevée. A l'opposé, moins il est anormal, moins l'erreur sera élevée.
Corrélations entre les mesures et l'erreur de recommandation
Anormalité CR augmente la corrélation de 11% (0,504). La controverse associée aux ressources permet donc d'améliorer l'estimation de la qualité des recommandations fournies aux utilisateurs. Anormalité CRU prend également en compte le profil de l'utilisateur. Une corréla-tion de 0,546 est obtenue (amélioration supplémentaire de 8%, et donc de 20% par rapport à l'état de l'art). La prise en compte des particularités de notation des utilisateurs permet donc d'améliorer l'estimation de la qualité des prédictions fournies aux utilisateurs.
La faible corrélation de la mesure CorrKMax (-0,22) indique que, contrairement à notre intuition, la qualité des voisins d'un utilisateur n'est pas corrélée à la qualité des recommandations dans le cadre d'une approche de recommandation knn.
Erreur en prédiction sur les utilisateurs atypiques
Une corrélation représente le lien entre deux variables sur un ensemble d'observations. Cependant, il est possible qu'un lien existe sur une seule partie des observations, ce qui ne sera pas reflété par la corrélation. Ici, nous nous intéressons uniquement aux utilisateurs qualifiés d'atypiques, c'est-à-dire à ceux ayant des valeurs d'anormalité les plus extrêmes. Par consé-quent, dans la suite des expérimentations, nous nous intéressons uniquement à la répartition FIG. 1 -Répartition de la RMSE des utilisateurs atypiques avec l'approche knn des erreurs observées sur les utilisateurs considérés comme atypiques. Pour représenter la ré-partition de ces erreurs, nous exploitons les quartiles et la médiane de ces erreurs, sur les 4 mesures d'anormalité. Plus les erreurs sont élevées, plus nous pouvons considérer que la mesure est de qualité. Nous comparons ces répartitions à celle associée à l'ensemble total des utilisateurs, dénommé Complet (Figure 1).
Nous utilisons un pourcentage d'utilisateurs atypiques fixe, car les mesures n'ont pas des valeurs d'anormalité comparables. Nous avons fixé exprimentalement ce seuil à 6% des utilisateurs, cela correspond à environ 50 utilisateurs parmi les 821 utilisateurs.
L'erreur médiane sur l'ensemble des utilisateurs (Complet) est de 0, 82. Celle de Anormalité est de 1, 26, ce qui correspond à une augmentation de l'erreur de plus de 50%, elle est d'ailleurs équivalente au troisième quartile de l'ensemble complet. Cependant, 25% des utilisateurs qualifiés d'atypiques ont une RMSE plus faible que la RMSE médiane sur l'ensemble des utilisateurs. Par conséquent, Anormalité semble sélectionner un grand nombre d'utilisateurs dont les recommandations sont de bonne qualité. Anormalité CR et Anormalité CRU pré-sentent de meilleurs résultats qu'Anormalité. Anormalité CRU est la plus performante : plus de 75% des utilisateurs sélectionnés ont une RMSE supérieure à 1,25 : 75% des utilisateurs Anormaux CRU font partie des 25% de l'ensemble complet des utilisateurs à recevoir les moins bonnes recommandations. Enfin, environ 50% des utilisateurs sélectionnés avec CorrKM ax reçoivent de bonnes recommandations, ce qui confirme les premières conclusions obtenues avec la corrélation.
Nous pouvons conclure que Anormalité CRU identifie de façon fiable les utilisateurs atypiques : ceux recevant des recommandations de mauvaise qualité avec une approche knn.
Conclusion et perspectives
Notre objectif dans ce premier travail était d'identifier, en recommandation sociale, les utilisateurs qui reçoivent des recommandations de mauvaise qualité, avant que des recommandations ne leur soient proposées. Nous avons fait l'hypothèse que ces utilisateurs avaient des préférences différentes des autres utilisateurs : des utilisateurs atypiques. Nous avons proposé plusieurs mesures exploitant la similarité de préférence avec les autres utilisateurs, l'écart des notes par rapport aux autres, le consensus de notation sur les ressources et le profil de nota-tion des utilisateurs. Nous avons montré que, sur un corpus de l'état de l'art, ces informations permettaient de prédire fiablement la mauvaise qualité des recommandations faites à un utilisateur. Nous pouvons donc conclure que les utilisateurs présentant des préférences différentes des autres utilisateurs reçoivent effectivement des recommandations de mauvaise qualité. Ces mesures peuvent donc être utilisées pour anticiper une mauvaise recommandation et la suite de ce travail portera naturellement sur la prise en compte des préférences atypiques des utilisateurs pour leur fournir des recommandations de qualité.

Introduction
Depuis les débuts du TALN, la compréhension de texte fait l'objet d'un suivi particulier de plusieurs recherches. C'est en faveur du développement rapide de la tâche d'extraction d'information que la tâche de REN s'est manifestée. Elle consiste à rechercher les expressions réfé-rentielles (Ehrmann (2008)), qui recouvrent classiquement les noms désignant des personnes, des lieux, des organisations, des expressions temporelles et celles numériques, mais peuvent aussi se rapporter à des notions plus techniques comme les maladies. Dès la campagne MUC-6, la tâche de REN s'est ainsi polarisée sur trois types d'entités (Grishman et Sundheim (1996)), à savoir : ENAMEX (personnes, organisations et lieux), TIMEX (expressions temporelles), NUMEX (expressions numériques). Cette première définition a été étendue dans la campagne CoNLL (Tjong Kim Sang et De Meulder, 2003) où 4 classes ont été normalisées : personnes, organisations, lieux, Divers. Dans les campagnes d'évaluation ESTER2 (Galliano et al., 2009) 8 catégories ont été normalisées à savoir personnes, fonctions,organisations, lieux, productions humaines, dates, montants et événements.
Travaux Connexes
Auparavant, la visibilité de la langue amazighe au Maroc était quasiment nulle. Récem-ment, et grâce aux revendications qui se sont faites à l'aide de l'IRCAM 1 , elle a été soumise à un processus de codification et de standardisation. Face à l'augmentation vertigineuse des informations en langue amazighe, disponibles librement sur le Web, plusieurs recherches ont été entamées dans ce sens. Il y en a celles qui se concentrent sur la reconnaissance optique des caractères (OCR) (Es-Saady et al., 2012) et celles qui se focalisent sur le TALN que nous pouvons classer en deux grandes catégories : (1) ressources informatiques, y compris des études sur la construction des corpus amazighe (Boulaknadel et Ataa Allah, 2011) et (2) les outils du TAL qui ont été réalisés comme le concordancier (Boulaknadel et Ataa Allah, 2010), l'analyseur morphologique (Nejme et al., 2013a,b) et (Ataa Allah et Boulaknadel, 2010). Quant au domaine de la REN, il a acquis un certain intérêt à travers les travaux réalisés de Talha et Boulaknadel (Talha et al., 2014b,a;Boulaknadel et al., 2014).
Aperçu général de notre approche
On distingue traditionnellement trois grandes approches : Approches symboliques qui reposent sur l'utilisation de grammaire formelle construite par la main. Approches statistiques qui permettent d'apprendre, des modèles d'analyse de textes sur de large corpus annoté auparavant, et ensuite établir automatiquement une base de connaissances à l'aide de plusieurs modèles numériques comme le CRF, SVM, etc. Au-delà de ces deux approches, il existe une autre qualifiée d'hybride qui représente un arrangement entre ses antécédents. Dans notre contribution, nous proposons un système fondé sur une approche symbolique, vu la non disponibilité d'un large corpus, où le repérage s'effectue en se basant sur un ensemble de gazetteers et de règles qu'on a construit manuellement tout en exploitant le principe de transducteurs à états finis disponibles sous GATE. 
Architecture du système
Notre système de repérage d'entités nommées permet l'identification des bornes des EN, ainsi que leur catégorisation dans des classes prédéfinies. Son architecture, détaillée sur la figure 1, comporte 3 modules qui effectuent un traitement séquentiel immédiat des données : 
Prétraitement Morphologique
Manipuler des textes écrits en langue amazighe nécessite une analyse préliminaire qui consiste à : La suppression des espaces supplémentaires existants entre les mots et l'élimi-nation de tous les mots non-amazighes figurant dans le corpus. Notre analyse comprend deux phases :
-La segmentation du texte amazighe en des phrases.
-L'identification des entités linguistiques de base « tokenisation ». Ces deux phases citées au dessus sont implémentées en utilisant, respectivement, les modules de GATE : le « Sentence Splitter » et le « Tokeniser ».   
Constitution des gazetteers
Évaluation
Discussion
Les entités nommées qui n'ont pas été identifiées, correspondent soit à des entités qui ne font pas partie de nos ressources, soit à des entités qui font partie de nos ressources, mais sont ambiguës. Certaines entités sont cependant ambiguës pour cause d'homographie, ou encore le cas d'entités poly-référentielles, une même entité nommée peut convenir à plusieurs classes. La prépondérance des entités mal classées implique un manque d'information que ce soit au niveau du contexte syntaxique ou de la présence des indices externes, qui, en plus de déter-mination des mots d'arrêt qui permettent de décider ou s'arrêter, augmente les probabilités d'erreurs de délimitation. Une analyse approfondie a conduit aux constats suivants :
-Enrichir nos gazetteers (Personne, Organisation, Localisation, DATE, NUM).
-Effectuer un traitement syntaxique supplémentaire afin de mieux saisir la structure syntaxique des phrases amazighes avant d'effectuer le repérage des entités nommées. -Étendre le nombre de règles linguistiques pour chaque classe d'entité nommée.
Conclusions et perspectives
Dans cet article nous avons proposé un système de repérage des entités nommées amazighes à base de règle. L'évaluation du système montre que les résultats obtenus sont assez encourageants et nous invitent à explorer de nouveaux modes de repérage d'entités nommées, afin de tirer le meilleur parti de notre approche et affiner le repérage des entités nommées amazighes.
Références
Ataa Allah, F. et S. Boulaknadel (2010). Light morphology processing for amazighe language.
In proceeding of the Workshop on Language Resources and Human Language Technology for Semitic Languages, Volume 17.

Introduction
Lorsque l'on cherche à comparer deux documents, on recherche tout élément présent dans l'un qui est également présent dans l'autre, ces éléments sont dénommés "similitudes". Les plus évidentes à voir à l'oeil humain sont les similitudes exactes, les parties copiées d'un document directement dans l'autre. Cependant, reproduire informatiquement cette capacité humaine est une opération délicate. Ce procédé est souvent gourmand en temps, car passant par une comparaison mot à mot afin d'identifier les séquences de mots identiques dans les deux textes. De ce fait, des méthodes beaucoup moins gourmandes ont vu le jour. Basées sur un système de n-grammes, elles extraient des séquences de n mots se suivant d'un texte et en cherche la présence dans l'autre. C'est dans l'optique de proposer une alternative à ces méthodes que nous allons décrire dans cet article une nouvelle approche de construction de séquences communes.
Après avoir présenté l'état de l'art, nous décrirons dans un premier temps le processus d'intersection des deux textes, ensuite, la phase de construction des plus longues séquences communes et pour finir, nous présenterons l'évaluation de notre approche en la comparant aux méthodes naïves de comparaison mot à mot et à la méthode classique n-grammes.
2 Le « copier/coller » 2.1 Le phénomène « copier/coller » Le « copier/coller » touche particulièrement les étudiants, en Europe, 34,5% (Guibert et Michaut, 2011) d'entre eux auraient déjà recopié tout ou partie d'un document pour le présen-ter comme travail personnel. Cette fréquence rejoint celle de travaux américains (Park, 2003) estimant à environ 30% la proportion d'étudiants ayant produit un travail reprenant des phrases d'Internet sans en citer la source. Une étude européenne (Gibney, 2006) révèle que près d'un étudiant français sur deux (46%) a déjà fait usage du plagiat pendant son cursus, contre environ un tiers des étudiants anglais et 10% des étudiants allemands. Ces résultats, qui paraissent déjà impressionnants, pourraient pourtant encore être sous-évalués. En effet, toujours selon la même étude, 40% des étudiants ne comprennent pas ce que signifie réellement le plagiat et n'assimilent pas le « copier/coller » à de la tricherie. La recherche de « copier/coller » entre deux textes joue donc un rôle essentiel dans la prévention du plagiat et la protection du droit d'auteur.
État de l'art
Les « copier/coller » sont en théorie les similitudes textuelles les plus facilement repé-rables et identifiables. En effet, la détection de celles-ci équivaut à comparer l'égalité entre deux textes. Pour effectuer cette recherche automatiquement on est obligé de procéder à une comparaison mot à mot. Cette opération, étant beaucoup trop chronophage pour être intégrée dans des solutions à but commercial ou hébergées en ligne, comme des services anti-plagiat, des techniques alternatives ont dû être mises au point.
Les méthodes les plus efficaces restent les méthodes classiques dites n-grammes (Torrejón et Ramos, 2013), qui consistent à construire puis comparer à partir de textes, des séquences de n éléments pouvant être des syllabes, des mots, des entités nommées, etc. La recherche de Barron-Cedeño et Rosso (2009) prouve qu'en prenant des "n-words" (séquence de n mots se suivant) de petites tailles, deux ou trois par exemple, les résultats sont bien meilleurs qu'en utilisant des longues séquences avec un n important. Sur le même principe mais plus originale, on peut citer la méthode de Stamatatos (2009), utilisant des n-grammes mais lors d'une détection intrinsèque, c'est-à-dire sans utilisation de document externe, on ne cherche pas des similitudes avec d'autres documents mais on étudie l'intérieur même du document analysé pour y repérer des irrégularités, des zones suspectes. Les n-grammes les plus pertinents ne sont pas toujours des séquences de mots, comme en atteste le travail de Shrestha et Solorio (2013), des n-grammes de mots vides (stop words) et d'entités nommées sont également utilisés pour dé-tecter des parties de textes similaires entre deux documents. Toutefois, les méthodes les plus répandues sont les méthodes "fingerprint", créant une empreinte du document pour la comparer avec celle d'autres documents. La plupart de ces méthodes (Kent et Salim, 2010) utilisent également des n-grammes pour construire l'empreinte des documents.
Les méthodes "fingerprint" divisent la plupart du temps le document en grammes de longueur n, ainsi les empreintes de deux documents peuvent être comparées et les points (i.e. grammes) concordants, identifiés comme étant des passages identiques dans les textes. Certaines méthodes de "fingerprint" (Stein et Eissen, 2006, 2007Lyon et al., 2001) vont audelà de la recherche de similitudes exactes et introduisent la notion de « similarités proches » pouvant ainsi détecter les paraphrases. Toujours dans cette optique, des recherches plus ré-centes (Simac-Lejeune, 2013; Kong et al., 2013) ne se contentent pas de comparer des mots ou groupes de mots d'un document à un autre mais tentent d'établir une corrélation « sémantique » entre deux documents par une approche utilisant des mots-clefs.
3 Notre approche 3.1 Intersection de deux textes L'idée de cette première étape est d'effectuer une intersection de deux textes, afin d'obtenir un tableau des mots présents dans les deux textes tout en conservant la position qu'ils ont dans l'un des deux. La procédure utilisée durant cette étude est la suivante :
1. passage en minuscule des deux textes à comparer ; 2. transformation en tableaux de ces deux phrases en segmentant sur les espaces et les caractères de ponctuation (lemmatisation) (chaque tableau représente une phrase et chaque cellule d'un tableau contient un lemme de la phrase à laquelle il correspond) ; 3. intersection des deux tableaux créés en conservant les offsets (positions) des mots du premier tableau et donc de la première phrase.
Construction de séquences maximales communes
La seconde et dernière étape consiste à construire, à partir du tableau obtenu à l'étape précédente, les séquences d'un minimum de n mots se suivant dans le premier texte, se suivant donc dans le tableau et étant également présentes dans le second texte. Le seuil n est le nombre de mots se suivant à partir duquel on peut déterminer qu'une séquence est la copie d'une autre et qu'elle n'est pas due au hasard. Nous pourrions dès lors nous poser la question : à partir de combien de mots se suivant une séquence peut être considérée comme réellement copiée ? En effet, il existe des séquences de trois mots ou plus, suffisamment fréquentes dans la langue, pour fausser la comparaison, comme les séquences « il était une fois » ou « nulle par ailleurs ». Cependant, les résultats des travaux de Barron-Cedeño et Rosso (2009) démontrent que sur de larges textes, il est tout aussi efficace de fixer un n petit, à deux ou trois par exemple.
La procédure de construction des séquences est la suivante :
1. on déplace une fenêtre de glissement de n éléments dans le tableau en fonction du seuil n choisi afin de constituer des "n-words" se suivant donc forcément dans le premier texte ; 2. pour chaque "n-word" constitué, on vérifie son existence dans le second texte ; 3. tant qu'une correspondance est trouvée et que la séquence existe bien dans les deux textes, on construit la séquence de taille n + 1 en y concaténant le mot suivant du tableau ; 4. dès que la séquence ne s'y trouve plus, on récupère la séquence maximale commune (la séquence essayée précédemment avant que le test échoue) et on recommence depuis l'étape 1 en déplaçant la fenêtre de glissement sur le mot suivant et en reprenant le n initial.
Évaluation et tests 4.1 La base de tests et protocole
La base de tests est composée de 200 textes, allant de 100 mots à environ 20 000 mots (avec une moyenne de 1500 mots), représentant 500 comparaisons de textes deux à deux annotés manuellement afin de savoir quel passage est réellement la copie d'un autre. Pour tester correctement les performances des algorithmes évalués, la base comporte aussi bien des passages entièrement copiés que des paraphrases ou des reformulations plus complexes, ainsi que des textes « pièges » traitant du même sujet et donc employant le même vocabulaire mais n'étant pas pour autant un « copier/coller » ou une reformulation quelconque d'un autre texte présent dans le corpus. L'intégralité de ces textes est en français. Ci-dessous la répartition des comparaisons :
-120 comparaisons effectuées afin de détecter des textes entièrement « copier/coller » de façon exact ; -80 comparaisons afin de détecter des textes entièrement paraphrasés ou reformulés ; -200 comparaisons entre des textes ne comportant que quelques passages rigoureusement identique (copier/coller) ; -100 comparaisons entre deux textes ne comportant que quelques passages « similaires » (paraphrases ou reformulations de phrases et/ou paragraphes). Ces comparaisons sont réparties entre des travaux d'élèves (mémoires financiers et scientifiques) avec leurs sources, différentes versions à différentes dates d'un même article de Wikipédia et des extraits du corpus de la PAN-CLEF 2014 en matière d'alignement de textes.
Résultats
Les résultats obtenus sur le corpus de test, par la méthode naïve de comparaison mot à mot, la méthode classique des n-grammes et notre méthode, sont représentées dans le tableau 1. La méthode n-grammes évaluée est celle décrite dans l'article de Barron  (2009) disant que prendre un n de petite taille augmente l'efficacité de détection, sachant que prendre des bigrams favorise le rappel, tandis que prendre un n supérieur favorise la précision. Ce phénomène s'explique par le fait que prendre un petit n forme des séquences courtes, on ne manque ainsi aucune correspondance mais on favorise les faux positifs, baissant alors la précision. En revanche prendre un n plus important construit des séquences plus longues, réduisant ainsi la correspondance de chaînes et donc le rappel mais augmentant le taux de certitude des concordances et donc la précision. Cet article ne pose pas la question d'optimisation de la détection en fonction du n choisi, on fixe donc n = 2 pour la suite de notre évaluation.
On constate dans le tableau 1 que notre méthode donne de meilleur résultat que celle des n-grammes (0.76 de précision contre 0.72 et 1 de rappel contre 0.78 avec n = 2 pour les deux méthodes). Toutefois, on peut voir dans le tableau 2 qu'en moyenne elle est 15% moins rapide et 30% plus coûteuse en mémoire que la méthode n-grammes (en allouant 6. 
Conclusions
Notre approche montre donc des résultats supérieurs aux méthodes n-grammes classiques, dans le sens où elle recherche une séquence de taille minimale n et agrandit si possible la séquence trouvée afin d'obtenir une séquence maximale commune. En revanche, elle est tout aussi dépendante du nombre n choisi que les méthodes n-grammes. Son temps d'exécution et son usage de la mémoire restent supérieurs à ceux des méthodes n-grammes bien que nettement inférieurs à ceux de la méthode mot à mot.
Pour des travaux futurs, nous envisageons de confronter nos résultats à des méthodes ngrammes plus sophistiquées comme celles décrites dans l'article de Shrestha et Solorio (2013).
Pour conclure, bien que moins rapide, notre méthode montre une précision équivalente aux méthodes n-grammes tout en proposant un rappel nettement supérieur.
Summary
Plagiarism detection most commonly use the most naive phase of similarities search, the detection of copy and paste. In this paper, we propose an alternative method to the standard verbatim comparison approach. The idea is to carry out an intersection of two texts to get a table of common words and to keep only the maximum sequences of consecutive words in one of the texts which also exists in the other. We show that this method is faster and less expensive in memory that commonly used scan texts methods. The goal is to detect identical passages between two texts faster than verbatim comparison methods, while operating more efficient than the n-grams.

Introduction
Le BiClustering consiste à réaliser un clustering simultanément sur les observations et les variables. Govaert et al ont introduit une adaptation de l'algorithme k-means au biclustering nommée "Croeuc" qui permet de découvrir tous les biclusters en même temps. Dans Labiod et Nadif (2011), les auteurs ont proposés une approche de factorisation CUNMTF, qui généra-lise le concept de la NMF Lee et Seung (1999). D'autres modèles probabiliste de biclustering sont proposés dans Govaert et Nadif (2008). Le Biclustering a de nombreuses applications et devient un challenge de plus en plus important avec l'augmentation des volumes de données. Cependant les bons algorithmes de clustering sont encore extrêmement utiles, il est donc néces-saire de les adapter aux nouvelles architectures massivement distribuées utilisant le paradigme MapReduce.
Le paradigme MapReduce Dean et Ghemawat ( Dans ce papier nous proposons une nouvelle approche globale de biclustering basé sur les cartes auto-organisatrices et le calcul distribué. Le modèle de biclustering BiTM (Biclustering using Topological Maps) a deja donnée lieu à une publication dans Chaibi et al. (2014). Dans ce papier nous proposons une adaptation de ces travaux a l'architecture MapReduce avec une implémentation de BiTM sous Spark, une technologie open source de calcul distribué incluant plusieurs paradigmes de programmation. Les principales problématiques abordées dans ce papier sont la minimisation de la fonction et la taille des données en entrée et en sortie des fonctions primitive (Map et Reduce) d'un algorithme de biclustering topologique.

Introduction
Les tweets sont des messages courts ne dépassant pas 140 caractères. Cette contrainte impose l'utilisation d'un vocabulaire particulier pour les rédiger et donc elle rend indispensable de connaitre leurs contextes pour les comprendre. Pour ces raisons, nous allons nous concentrer sur la tâche de contextualisation des tweets attribuée à INEX2014 1 . Les participants devaient fournir un contexte, pour permettre aux lecteurs de bien comprendre le tweet en utilisant un système de recherche d'information SRI et système de résumé automatique SRA. Dans cet article, nous proposons une nouvelle approche de contextualisation de tweets basée sur les règles d'association inter-termes.
Cet article est organisé comme suit : Dans la section 2, nous détaillons notre nouvelle approche. la section 3 sera consacrée aux différentes expériences menées , finalement nous conclurons dans la section 4. 
Approche proposée
Conclusion
Dans cet article, nous avons décrit une nouvelle approche de contextualisation de tweet basée sur règles d'association inter-termes. Les résultats ont confirmé que la synergie entre les règles d'association entre termes et l'expansion de tweets est fructueuse. Dans un travail en cours, nous proposons d'ajouter une phase de désambiguïsation pour réduire le bruit dans nos résultats. 
Summary
Tweets are short messages that do not exceed 140 characters. Since they must be written respecting this limitation, a particular vocabulary is used. To make them understandable to a reader, it is therefore necessary to know their context. In this paper, we describe our approach for the tweet contextualization. This approach allows the extension of the tweet's vocabulary by a set of thematically related words using mining association rules between terms.

Introduction
L'objectif des systèmes de recommandation est de prédire les choix et les préférences individuelles en fonction des comportements et des préférences observées. Le filtrage collaboratif est la technique la plus utilisée par les systèmes de recommandation. Il consiste à comparer les données d'un utilisateur avec des données similaires d'autres utilisateurs, basée sur les habitudes d'achat et de navigation (Goldberg et al., 1992). Il permet aux commerçants de fournir des recommendations aux clients pour de futurs achats. Dans la suite, les données sont représentées par une matrice U de taille (n × p) où chaque ligne représente un utilisateur, les colonnes représentent des items, et chaque cellule (u ij ) de U est la note attribuée par un utilisateur i pour un item j. Les notes (u ij ) peuvent être binaires, ou réelles et dans ce cas U est appelée matrice réelle de notations. La matrice U peut être obtenue de manière explicite (en gardant les évaluations fournies par les utilisateurs pour des articles donnés) ou de manière implicite (en considérant qu'un utilisateur préfère implicitement acheter ou pas les éléments présentés sur des pages Web visitées).
Dans le filtrage collaboratif (désormais désigné par FC), plusieurs approches sont utilisées. Les techniques de FC actuelles telles que celles basées sur la corrélation entre utilisateurs (Bobadilla et al., 2013) ou sur la factorisation matricielle (Koren, 2009;Sarwar et al., 2000;Delporte et al., 2014) sont couramment utilisées, mais nécessitent un temps de calcul très coûteux et ne peuvent être déployées en ligne. Dans ce contexte, la classification croisée ou co-clustering, qui consiste à regrouper simultanément les utilisateurs et les items, est une bonne solution. Elle est particulièrement appropriée dans les systèmes de de recommendation. Il est ainsi, par exemple, intéressant de disposer de groupes d'utilisateurs appréciant un groupe de films. Dans (George et Merugu, 2005), les auteurs ont proposé une approche de FC basé sur un algorithme de classification croisée pondérée (COCLUST) qui implique le regroupement simultané des utilisateurs et des articles. Malheureusement, dans cette approche la prise en compte des données manquantes n'est pas appropriée, conduisant ainsi à une faible qualité de recommandation. Nous proposons donc de faire un meilleur usage de cet algorithme par une prise en compte plus efficace des données manquantes. D'autre part, en exploitant le potentiel des résultats de la classification croisée, nous développons un outil interactif de visualisation et d'interprétation simultanée des groupes d'utilisateurs et des groupes d'items.
Le reste du papier est organisé comme suit. La section 2 présente le système de FC basé sur la classification croisée (COCLUST). Les sections 3 et 4 fournissent des détails sur nos approches de gestion des notes manquantes et de visualisation. La section 5 démontre l'efficacité des approches proposées sur des données réelles. Enfin, la section 6 conclut et présente les directions pour des recherches futures.
Notation. Soit U la matrice des notes, une classification croisée en K × L co-clusters (blocs ou sous-matrices résultant d'une classification croisée) par COCLUST conduit à une partition de l'ensemble des utilisateurs en K classes et une partition de l'ensemble des items en L classes. Notons Z = (z ik ) la matrice de classification binaire de taille (n × K) dé-finie par z ik = 1 si l'utilisateur i appartient à la k i` eme classe et 0 sinon. De la même manière notons W = (w j ) la matrice de classification binaire de taille (p × L) définie par w j = 1 si l'item j appartient à la i` eme classe et 0 sinon. Par commodité, nous utiliserons également z = (z 1 , . . . , z n ) avec z i ? {1, . . . , K} (respectivement w = (w 1 , . . . , w p ) avec w j ? {1, . . . , L} ; le vecteur des étiquettes des items). Enfin, nous utiliserons les indices i, j, k et pour désigner implicitement les lignes (utilisateurs), les colonnes (items), les classes en ligne (classes d'utilisateurs) et les classes en colonnes (classes des items) respectivement.
2 Classification croisée par COCLUST Partant de l'algorithme weighted Bregman co-clustering (Banerjee et al., 2004), les auteurs dans (George et Merugu, 2005) ont proposé de s'attaquer au problème de recommandation moyennant une reconstitution des données observées suivant une classification croisée donnée. Plus précisément, à partir de la réorganisation en co-clusters obtenus par l'algorithme COCLUST, les auteurs proposent une matrice d'approximation de U sparse par une matricêmatricê U = (ˆ u ij ) non sparse où chaque cellule est définie de la manière suivante :
avec u k , u k. , u .. , u i , u j sont respectivement les moyennes calculées sur l'ensemble des valeurs observées dans le co-cluster (k, dans la classe des utilisateurs k, dans la classe des items, pour chaque utilisateur et pour chaque item. Notons donc que dans cette formulationû formulationˆformulationû ij dépend de i, j, k et Sachant que les partitions Z and W sont inconnues, le critère à minimiser par COCLUST est le suivant :
où M = (m ij ) est une matrice binaire de taille (n × p) où m ij = 1 si u ij est observé et m ij = 0 si u ij est manquant. Une solution (optimum local) de ce problème peut être obtenue par une minimisation alternée ; sachant Z puis sachant W (Banerjee et al., 2004) jusqu'à la convergence (Algorithm 1). A la convergence la prédiction est obtenue en utilisant (1).
Algorithm 1 Training based on Co-clustering.
. , u i and u j ; ( ? k, i and j) 2. Update Z :
Comme indiqué précedemment les estimations au cours des itérations de COCLUST sont basées uniquement sur les données observées. Malheureusement et étant donné que le taux des données manquantes est très élevé (la sparsité de certaines matrices peut être de l'ordre de 99 %), les prédictions sont biaisées impliquant des qualités de recommandation discutables. D'autre part, George et Merugu (2005) ont proposé de remplacer la moyenne d'un co-cluster vide (qui ne contient aucune note observée) par la moyenne globale. Cette stratégie peut fortement perturber la qualité de la classification croisée, et de plus elle ne garantit pas la convergence de COCLUST, comme le montre figure 1. Pour plus d'explications, nous avons rapporté  
Gestion des notes manquantes dans le FC
Pour surmonter le problème des données manquantes dans le FC, deux approches sont principalement utilisées. La première consiste à travailler uniquement sur les valeurs observées, et la deuxième consiste à utiliser les procédures d'imputation. L'imputation par la moyenne est la plus couramment utilisée, elle consiste à remplacer les notes manquantes d'un item/utilisateur par la moyenne de ses notes observées.
Ces approches peuvent être efficaces, si peu de valeurs sont manquantes, et que le mé-canismes des données manquantes est Missing completely at random ou Missing at random (Little et Rubin, 2002). Malheureusement le taux de notes manquantes dans le filtrage collaboratif est très élevé, ce qui rend ces approches inefficaces dans ce contexte. En effet, elles peuvent conduire à des estimations fortement biaisées, ce qui impacte négativement la qualité des recommandations. Pour illustrer ce propos, nous avons rapporté dans figure 3 un exemple d'une matrice utilisateur-item, avant (figure 3a) et après l'imputation par les moyennes des items (figure 3b). Si nous voulons ordonner les items en fonction des préférences des utilisateurs, l'ordre le plus fiable serait : i1, i4, i2, i3 (tels que i1 est l'élément le plus apprécié). En revanche si nous utilisons la matrice après imputation pour trier ces éléments de la même manière, nous obtiendrons l'ordre suivant : i2, i4, i1, i3 qui est absurde, puisque i1 arrive seulement en troisième position et i2 arrive en première position. Cela est dû aux estimations fortement biaisées des moyennes des utilisateurs i2 et i4. Dans ce qui suit, nous allons présenter une nouvelle méthode d'imputation basée sur la version en ligne de l'algorithme kmeans sphé-rique (OSPK-means) (Zhong, 2005). Notre approche repose sur les deux étapes principales, 1) Partitionner l'ensemble des utilisateurs en k classes, en utilisant l'algorithme OSPK-means et en tenant compte des valeurs manquantes, 2) Estimer les notes manquantes, en se basant sur les résultats de la classification. Et enfin remplacer celles-ci dans la matrice U. Ci-dessous, nous décrivons de manière détaillée les différentes étapes de notre approche :
Etape de Classification
Dans le but de partitionner l'ensemble des utilisateurs en k groupes, nous proposons les procédures suivantes : Initialisation : Dans la version initiale de OSPK-means, l'initialisation se fait par un tirage aléatoire de K centres initiaux parmi l'ensemble des utilisateurs. Cependant cette stratégie n'est pas efficace dans notre cas. En effet la probabilité de choisir un utilisateur avec très peu de notes observées, comme un centre de gravité initial est élevée. D'autre part, sélectionner les centres initiaux uniquement parmi l'ensemble des utilisateurs ayant noté beaucoup d'items, permettrait seulement la détection de certains groupes. Afin de surmonter ces difficultés, nous proposons la procédure d'initialisation suivante :
1. Générer une partition aléatoire des utilisateurs en k classes.
2. Estimer les centres initiaux comme suit : soit µ kj la j i` eme composante du centre k alors :
; sinon où S est un seuil proportionnel à la taille de la classe k, et peut être défini par l'utilisateur. Intuitivement, cette stratégie permet d'estimer la j i` eme composante du centre de la k i` eme classe à partir des données disponibles, mais seulement s'il y a suffisamment de notes observées pour dans cette classe. En revanche, quand peu de valeurs sont observées pour une composante j l'estimation de celle-ci est pénalisée, en divisant par le seuil S. Etape de mise à jour : Lorsque l'utilisateur choisi dans l'étape d'affectation ne dispose pas de suffisamment de notes-observées, l'assignation de celui-ci n'est pas fiable. Par conséquent le centre correspondant ne doit pas être déplacé dans le sens de cet utilisateur. Pour résoudre ce problème, nous introduisons une fonction binaire (h(u) ? {0, 1}) qui annule la mise à jour dans ce cas. L'Algorithme 2 fournit plus de détails sur cette étape de classification.
Algorithm 2 Classification.
Input : n normalized users u i ( i = 1) in R p , K : number of user clusters, ? : learning rate, B : number of batch iterations ; Output : K Centroids µ k in R p , and z = (z 1 , . . . , z n ) ; Steps : 1. Random initialization of the partition z ; 2. Estimation of initial centroids :
; otherwize. for b = 1 to B do for i = 1 to n do 3. Assignment : for each user u i , compute z i :
z i +?h(ui)ui ; t = t + 1 ; end for end for
Estimation des notes manquantes
Dans cette étape les notes manquantes sont estimées, en se basant sur les résultats de la classification. Cependant, une pondération des notes s'avère encore nécessaire. Nous avons choisi d'accorder plus d'importance aux utilisateurs representant le mieux leur classe d'appartenance en pondérant par cos(u i , µ k ) tout en atténuant l'effet des utilisateurs qui ne sont pas en accord avec la préférence globale pour un item au sein de leur classe d'appartenance à l'aide de p(u ij ). Soit u a un utilisateur actif, k = z a , la note pour pour un item j prend la forme suivante :
où r med est la note médiane (r med = 3 if u ij ? {1, 2, 3, 4, 5}, p(u ij ? r med ) est la probabilité qu'un item j soit apprécié au sein d'un groupe k, tel que :
Dans la section suivante, nous proposons d'exploiter les résultats de classification croisée, dans le but de fournir aux utilisateurs une représentation interactive basée sur des graphes bipartis. Cette dernière permet non seulement de faciliter l'interprétation des résultats, mais aussi de donner un sens aux préférences des utilisateurs dans le contexte du filtrage collaboratif.
Visualisation des résultats de la classification croisée
Il y a très peu de travaux qui se sont intéressés à l'aspect visualisation dans le contexte des systèmes de recommandation. En effet ces systèmes sont souvent évalués pour leur capacité à faire de bonnes recommandations, mais leur fonctionnement reste abstrait pour les utilisateurs. Parmi les quelques travaux de visualisation on peut citer la méthode de visualisation des données du FC (Mei et Shelton, 2006) qui consiste à représenter les utilisateurs à côté des items qu'ils aiment, sur le même espace euclidien. On peut aussi citer PeerChooser (Smyth et al., 2008) qui est un système de FC interactif qui permet de visualiser sous forme de graphe les interactions entre un utilisateur actif et son voisinage, tout en offrant la possibilité de modifier ce dérnier. Il existe aussi des travaux qui proposent de visualiser sur un plan à deux dimensions la liste d'items à recommander pour un utilisateur actif, en utilisant les techniques classiques telles que l'ACP, MDS, SOM.
Dans ce travail nous proposons une nouvelle approche de visualisation dans le contexte des systèmes de recommandations. Contrairement aux autres méthodes citées ci-dessus, notre approche est globale, c'est à dire qu'elle ne se focalise pas uniquement sur l'utilisateur actif. Elle exploite la dualité inhérente de la classification croisée pour mieux mettre en évidence les affinités entre certains types de groupes d'utilisateurs et certains types de produits. Plus pré-cisément, nous proposons de représenter les relations de préférences entre des groupes d'utilisateurs et groupes d'items, au moyen des graphes bipartis. Notre approche peut être décrite comme suit : 1) Classifier la matrice utilisateur-item en K classes d'utilisateurs et L classes d'items. Dans nos expérimentations nous avons utilisé COCLUST, après la gestion des données manquantes, présentée dans la section précédente, 2) Construire une matrice résumant les résultats de la classification croisée, dans laquelle chaque groupe de de lignes et chaque groupe de colonnes est représenté par les utilisateurs et les items les plus populaires (qui ont le plus de votes) respectivement, 3) Calculer la relation de préférence entre chaque groupe d'utilisateurs et chaque groupe d'items, à l'aide de la formule (4), 4) Construire le graphe biparti, étape décrite en détail dans la partie expérimentale.
Soit U = (u ij ) la matrice résumée de l'étape 2, avec n utilisateurs et p items. Et soit E = (e ij ) une matrice binaire de n × p, tel que e ij = 1 si l'utilisateur i aime l'item j et e ij = 0 sinon. Alors la corrélation entre la k 
Intuitivement la corrélation (de préférence) 4, entre un groupe d'utilisateurs k et un certain groupe d'items représente la proportion des items populaires dans la i` eme classe ayant été appréciée par les utilisateurs les plus populaires de la classe k. La section suivante présente les résultats expérimentaux démontrant l'efficacité des approches proposées.
Algorithm 3 Bipartite procedure.
Input : U, K and L ; Output : C : correlation matrix between clusters ; Steps : 1. Compute (Z, W) into K row clusters and L column clusters ; 2. Compute U with the relevant users and items. for k = 1 to K do for l = 1 to L do 4. Compute C = (c k ) the correlation matrix between clusters, by using (4) end for end for 5. Build the bipartite graph
Résultats expérimentaux
Dans nos expériences, nous avons choisi les deux jeux de données de MovieLens 1 (ML-100K et ML-1M) qui sont beaucoup utilisés dans le domaine. L'échantillon ML-1M est constitué de 6040 utilisateurs, 3952 films, et de 1 million de notes observées. L'ensemble ML-100K contient 100,000 notes fournies par 943 utilisateurs pour 1664 films. La proportion des notes observées dans ce dernier est seulement de 6, 4%. Les évaluations des utilisateurs (u ij ) appartiennent à l'intervalle : [1; 5], et les notes manquantes sont codées par : NA. Les données MovieLens fournissent également certaines informations démographiques sur les utilisateurs, telles que : le sexe, l'âge, la profession, code postal ; et des informations de base sur les films tels que : le titre, le genre, la date de sortie, etc. A noter qu'un film peut être de plusieurs genres à la fois. Nous proposons dans la suite de réaliser la comparaison des courbes ROC et de la F-measure, des systèmes de FC suivants : COCLUST, le FC incrémental basé sur la décompo-sition en valeurs singulières SVDCF (Sarwar et al., 2002), et COCLUST++ (COCLUST après la gestion des valeurs manquantes). Ces comparaisons sont réalisées sous recommenderlab (Hahsler, 2011), que nous avons combiné avec le langage C pour implémenter les différentes méthodes ci-dessus. Les courbes de figure. 4a sont construites en faisant varier le nombre d'items à recommander de 1 à 40. Les deux figures 4a et 4b montrent une amélioration significative des performances de COCLUST, grâce à la gestion des données manquantes que nous proposons. On remarque aussi, une faible qualité des recommandations pour SVDCF, qui est due à une gestion des données manquantes inappropriée. En effet dans cette dernière approche (Sarwar et al., 2002), les notes manquantes sont remplacées par les moyennes des items dont les estimations sont fortement biaisées. En d'autres termes cette imputation favorise les items avec très peu de notes observées, comme illustré dans la section 3 (figure 3). La figure 4d montre que même avec l'étape d'imputation COCLUST++ reste plus rapide que SVDCF.
En ce qui concerne les possibilités de visualisation exploitant la classification croisée, la figure 5 montre un exemple de graphe biparti, qui est construit comme suit 1) Classification de l'ensemble ML-100k, en 6 classes utilisateurs et 8 classes d'items, en utilisant COCLUST++, 2) Calculer les corrélations entre les groupes d'utilisateurs et d'items, via la formule (4), 3) Construire le graphe biparti où les rectangles de gauche représentent des groupes d'utilisateurs, tandis que ceux de droite des groupes d'items. Seuls les liens qui correspondent à de fortes corrélations sont représentés. Pour chaque groupe d'utilisateurs, les deux professions les plus populaires sont présentées, de même les deux genres les plus populaires dans chaque classe  
Conclusion
Dans ce papier nous avons proposé une meilleure exploitation du potentiel de la classification croisée dans les systèmes de FC. Pour ce faire, nous avons développé une nouvelle stratégie pour une gestion efficace des données manquantes. Nous avons ensuite proposé une nouvelle approche interactive basée sur des graphes bipartis, permettant d'interpréter et de comprendre les résultats de la classification croisée dans le contexte du FC. Les résultats expé-rimentaux montrent une amélioration importante des performances de la classification croisée dans le FC, grâce à une meilleure gestion des notes manquantes. Nous avons aussi montré, comment les représentations interactives basées sur des graphes bipartis peuvent aider les uti-

Introduction
Dans le contexte d'une fouille exploratoire, le recours à des techniques de réduction de dimensionnalité permet classiquement de contourner la difficulté de représenter des résultats de clustering réalisés sur des données à haute dimensionnalité (HD). Les étiquettes de clusters, associées par exemple à des couleurs catégorielles, peuvent alors être appliquées aux points d'un nuage 2D ou 3D.
Les techniques de réduction de dimensionnalité sont susceptibles d'introduire des artéfacts de déchirement et de recollement (Aupetit, 2007). Les algorithmes de clustering ne sont pas sujets à ces artéfacts, mais peuvent mener à des résultats sous-optimaux, ou avoir été mal paramétrés. L'objet de cet article est de proposer un outil interactif de fouille visuelle combinant le meilleur de ces deux approches. Il utilise une projection 2D obtenue par t-SNE, une technique de réduction de dimensionnalité non-supervisée (van der Maaten et Hinton, 2008). Nous ne proposons pas un algorithme de clustering per se, mais plutôt une manière itérative d'amélio-rer conjointement un clustering initial calculé de manière non-supervisée dans un espace HD, et une représentation 2D associée.
Une présentation générale de notre outil est proposée en section 2. Les clusters sont amendés grâce à des techniques de diffusion d'étiquettes, présentées en section 3. Réciproquement, l'adaptation de la projection 2D aux clusters est évoquée dans la section 4. Les exemples donnés en section 5 et tout au long de cet article utilisent le jeu de données COIL-20 (Nene et al., FIG. 1 -Diagramme résumant la logique de l'outil. 1996). Il contient 1440 images, réparties en 10 classes. Les images sont décrites par les intensités de leurs 1024 pixels sur une échelle de gris.
Présentation de l'outil
La logique de l'outil est résumée dans la figure 1. Il est paramétré par un jeu de données HD (i.e., > 3), et par un clustering non-supervisé réalisé dans l'espace HD.
Une projection 2D initiale est calculée de manière non-supervisée par l'algorithme itératif t-SNE. Elle est matérialisée par un nuage de points, dont les colorations sont associées aux étiquettes de clusters via un ensemble de couleurs catégorielles. À partir d'une projection et d'un clustering donnés, l'utilisateur peut déclencher les actions suivantes :
-Sélection d'une restriction : optionnellement, l'utilisateur peut limiter le rayon de son action à un ensemble de clusters sélectionnés directement en cliquant la légende (voir Figure 2e). -Sélection de pivots : en cliquant sur un des points dans le nuage (voir Figure 2d), l'utilisateur définit un pivot pour la diffusion d'une nouvelle étiquette de cluster. Un inspecteur interactif est à sa disposition pour associer une sémantique à chaque pivot (voir Figure 2c). -Diffusion d'étiquettes : les étiquettes des pivots sélectionnés sont diffusées en utilisant la proximité 2D entre éléments. -Modification des dissimilarités : la répartition en clusters peut être utilisée pour influencer les dissimilarités sous-jacentes à l'algorithme t-SNE. L'utilisateur peut paramétrer le niveau de cet impact. Les actions de l'utilisateur modifient la visualisation et les clusters, dont le nouvel état peut servir d'entrée à une nouvelle itération du diagramme en figure 1. Des itérations sont effectuées jusqu'à ce que l'utilisateur soit satisfait du résultat. fastidieuses, nous proposons une diffusion semi-automatique basée sur la sélection de pivots par l'utilisateur. La diffusion peut être calculée selon deux algorithmes issus de la littérature : -Propagation probabiliste : les étiquettes peuvent métaphoriquement sauter de manière probabiliste depuis les pivots associés, puis d'élément en élément. Ce processus converge, et le résultat peut être obtenu sous une forme analytique impliquant de simples produits de matrices (Zhu et Ghahramani, 2002). -Coupes de l'arbre de couverture minimal : l'arbre de couverture minimal d'un graphe peut être calculé grâce à l'algorithme de Kruskal (Kruskal, 1956). Des coupes dans cet arbre isolent des composantes connexes du graphe.
Diffusion d'étiquettes
Ces opérations sont réalisées relativement à la distribution visuelle des éléments ; les distances 2D entre éléments dans la projection sont donc utilisées dans les algorithmes.
L'utilisateur commence par sélectionner un ou plusieurs pivots dans la restriction en cours (qui peut englober tous les éléments si aucun cluster n'a été sélectionné dans la légende). Ces derniers peuvent être vus comme des prototypes de clusters, existants et à redécouper, ou à créer. Selon ses préférences, l'utilisateur peut alors les propager exhaustivement, ou paramé-trer interactivement la coupe de l'arbre de couverture minimal pour isoler des composantes connexes. Dans ce dernier cas, il peut aussi choisir de regrouper les composantes sans pivot dans un cluster résiduel, ou laisser leur étiquette telle qu'avant l'interaction.
Modification des dissimilarités
Plutôt que d'utiliser la distribution 2D des éléments pour modifier les clusters comme dans la section 3, l'utilisateur peut utiliser la répartition en clusters pour modifier la projection 2D, de manière par exemple à renforcer la séparation des clusters.
Considérons le graphe complet entre les éléments dans la restriction en cours, pondéré par les dissimilarités dans l'espace HD. Nous voulons utiliser l'information portée par les clusters pour amender les dissimilarités HD entre éléments. Pour préserver la structure interne des clusters, nous proposons de restreindre la modification au sous-graphe multipartite induit par les clusters. La fonction cumulative normalisée de la distribution Beta est alors appliquée aux poids d'arêtes associés :
Les bornes a et b permettent d'adapter la transformation aux valeurs de dissimilarité, e.g., à la valeur maximale observée dans la restriction, ou à la cohésion interne des clusters. Les changements trop disruptifs sont ainsi évités. L'utilisateur peut alors paramétrer interactivement un rapprochement (respectivement un éloignement) des clusters en augmentant le paramètre ? (respectivement ?).
L'algorithme t-SNE se base sur des dissimilarités HD entre les éléments pour estimer leurs positions dans le nuage de points. Classiquement, ces dissimilarités sont initialisées par une distance Euclidienne dans l'espace HD. Dans l'outil, les dissimilarités peuvent être modifiées dynamiquement.
L'algorithme t-SNE peut être interprété comme une variante d'algorithme force et ressort. La métaphore physique suivie par cette classe d'algorithmes convertit des changements discontinus des forces en présence en mouvements continus. Ainsi, la discontinuité obtenue à l'application de l'équation (1) est convertie en mouvements continus, facilitant leur suivi par un utilisateur. Après une modification de dissimilarités, ce dernier peut alors suivre le changement progressif induit par son action.
Les dissimilarités HD demeurent latentes à la projection 2D, et ne peuvent pas être observées directement dans la visualisation. Pour pallier cette limitation, nous avons incorporé l'outil ProxiViz (Heulot et al., 2012), qui permet de mapper interactivement les dissimilarités sur le diagramme de Voronoï du nuage de points (voir Figure 3). L'utilisateur dispose ainsi d'une information plus complète avant de procéder à ses modifications. Ceci peut par exemple permettre de prendre en compte d'éventuels artéfacts de projection (Aupetit, 2007).
Exemples
Au cours de ses manipulations avec l'outil, l'utilisateur est confronté à la situation de la figure 4a. Le cluster vert, identifié par l'algorithme de clustering dans l'espace HD, est éclaté en 3 composantes dans la visualisation. L'utilisateur souhaite les regrouper, en respectant le voisinage des composantes dans la projection.
Il commence par vérifier la pertinence d'un tel regroupement en utilisant l'outil ProxiViz. Les composantes du cluster vert sont bruitées par les clusters violet et orange. Il commence FIG. 3 -Le survol du nuage de points déclenche l'outil ProxiViz. Les dissimilarités HD par rapport au point de la cellule survolée sont mappées sur une échelle de gris, et colorent les cellules de Voronoï des éléments respectifs. Les étiquettes de clusters sont rappelées en colorant le contour des points, ainsi que la cellule du point survolé.
FIG. 4 -a)
Le cluster vert est réparti en 3 composantes, bruité par les clusters violet et orange. Un pivot est sélectionné pour chaque composante. b) L'exécution d'itérations de t-SNE sur la restriction en cours ne réunit que partiellement le cluster. c) Après mise à jour des dissimilarités, le cluster est effectivement regroupé. donc par définir une restriction à ces 3 clusters, et sélectionne des pivots pour isoler les composantes (voir Section 3). Les composantes sont ensuite réunies simplement en éditant la légende interactive.
En déclenchant t-SNE sur la restriction en cours, le cluster est partiellement réuni (voir Figure 4b). L'utilisateur influence leur rapprochement en mettant à jour les dissimilarités (voir Section 4 et Figure 4c).
Conclusion
Dans cet article, nous avons exposé notre outil de clustering visuel et interactif. Les techniques de diffusion d'étiquettes et de modification des dissimilarités permettent d'enrichir mutuellement une projection 2D et un clustering itérativement mis à jour. La métaphore physique suivie par le nuage de points et les moyens de contrôle offerts par l'interface permettent à l'utilisateur de suivre le changement progressif induit par ses actions. Nous avons illustré l'intérêt de l'approche au travers d'exemples.

Introduction
La découverte de motifs locaux introduite par Agrawal et Srikant (1994) consiste à extraire des informations pertinentes décrivant une portion des données. Evaluer et garantir la qualité des motifs extraits demeure une problématique très ouverte malgré le nombre important de propositions (Giacometti et al., 2013). Chacune de ces propositions repose explicitement ou implicitement sur une mesure d'intérêt dont la qualité dépend de la complexité du modèle sous-jacent et de son ajustement aux données. Le modèle repose en général sur des fondements statistiques dont la complexité et la compréhension sont bien connues. A l'inverse, l'ajustement aux données reste une notion difficile à appréhender. Pourtant, c'est probablement cette notion qui distingue la fouille de données des statistiques traditionnelles. L'ajustement aux données est souvent connoté négativement et synonyme de sur-apprentissage par rapport aux données. De notre point de vue, l'ajustement aux données n'est pas un biais d'apprentissage mais un moyen pour lever certaines hypothèses sur le modèle en les remplaçant par des mesures sur les données. Nous proposons d'étudier l'ajustement aux données à travers les interrelations entre motifs lors de l'évaluation d'une mesure d'intérêt ou d'une contrainte d'extraction.
La qualité d'une mesure repose sur sa capacité à isoler un motif singulier qui dévie des autres motifs communs. Pour cette raison, une mesure se doit de mettre en relation le motif évalué avec d'autres motifs, dits motifs liés. Par exemple, la confiance de la règle d'association X ? Y met en relation la fréquence de X ? Y (motif évalué) par rapport à la fréquence de X (motif lié). La qualité de la règle augmente avec la fréquence de X ? Y tandis qu'elle diminue si la fréquence de X augmente (lorsque les autres fréquences restent constantes). Ces variations de la confiance sont en partie conformes aux deux axiomes formulés par PiatetskyShapiro (1991). De manière intéressante, ces axiomes permettent d'étudier formellement le comportement des mesures d'intérêt dédiées à l'évaluation des règles d'association. Dans cet article, nous proposons de généraliser ce principe en introduisant la notion de motif lié pour s'attaquer à l'évaluation de n'importe quelle méthode de découverte de motifs.
L'objectif de ce travail est de formaliser la qualité et la sémantique des mesures d'intérêt et contraintes en analysant les interrelations entre les motifs nécessaires à l'évaluation de chaque motif extrait. Ce travail s'inscrit dans la lignée des travaux sur l'analyse des propriétés formelles vérifiées par les mesures d'intérêt Piatetsky-Shapiro (1991); Tan et al. (2004); Geng et Hamilton (2006); Lenca et al. (2008); Hämäläinen et al. (2010) en les étendant aux contraintes d'extraction. Nous formaliserons l'interrelation entre motifs en introduisant l'ensemble de motifs liés. Cet ensemble regroupe tous les motifs susceptibles d'impacter l'évaluation d'un motif donné. Nous distinguerons les motifs liés positivement qui permettent d'accroître la mesure d'intérêt de ceux qui la font décroître, i.e., les motifs liés négativement. Nous formulerons alors trois axiomes que devraient satisfaire une mesure d'intérêt ou contrainte. Chacun de ces axiomes impose des contraintes topologiques que doivent respecter les deux ensembles de motifs liés. Enfin, nous proposerons des critères d'analyse de la complexité et de la sémantique d'une mesure d'intérêt ou contrainte. Nous introduirons finalement la complexité en évaluation qui repose sur la cardinalité de l'ensemble des motifs liés.
Travaux relatifs
A notre connaissance, très peu de travaux se sont intéressés à l'interrelation entre les motifs lors de l'évaluation d'une mesure d'intérêt ou d'une contrainte. De manière plus générale, l'évaluation de la qualité des méthodes de découverte de motifs est une tâche ardue et peu étudiée.
Protocoles expérimentaux
La plupart des méthodes d'évaluation ou d'analyse de la qualité concernant la découverte de motifs repose sur des protocoles expérimentaux où l'objectif est de vérifier la conformité du résultat avec un étalon-or. Dans un contexte supervisé, il est possible d'exploiter directement la variable cible comme référence. Ensuite, le cadre précision/rappel, l'analyse ROC (Fawcett, 2006), la validation croisée (Kohavi, 1995), etc sont utilisés pour évaluer l'écart entre les motifs extraits et la référence. L'évaluation de la qualité des méthodes de découverte de motifs dans un contexte non-supervisé s'avère bien plus difficile comme le rappellent de Lin et Chalupsky (2004). En effet, la validation des motifs extraits ne peut pas s'appuyer sur un étalon-or explicite. Plusieurs stratégies sont alors mises en oeuvre pour obtenir un succédané de cet étalon-or.
Premièrement, il est possible de s'appuyer sur la connaissance d'experts d'un domaine (Carvalho et al., 2005). Avec un cas d'utilisation, des experts du domaine sont sollicités pour juger la justesse des motifs découverts et ainsi, évaluer la méthode d'extraction. La stratégie de redécouverte teste si un processus parvient à retrouver les connaissances bien établies dans un certain domaine. Deuxièmement, il est parfois possible de construire l'étalon-or (Gupta et al., 2008;Zimmermann, 2013). Par exemple, Gupta et al. (2008) propose un protocole d'évaluation quantitative des algorithmes d'extraction de motifs approximés fréquents : (1) l'extraction des motifs fréquents dans un jeu de données classique, (2) l'ajout de bruit dans ce jeu de données, (3) l'extraction des motifs approximés fréquents et enfin, (4) la comparaison des véritables motifs fréquents avec les motifs approximés fréquents. Ici, l'étape 1 explicite l'étalon-or. Enfin, l'hypothèse nulle peut parfois être construite expérimentalement. Gionis et al. (2007)  Ces protocoles expérimentaux sont clairement utiles, mais néanmoins ils souffrent de plusieurs limites. Tous ces protocoles expérimentaux reposent sur l'évaluation de la collection de motifs extraits. Ils ne peuvent donc fournir qu'un résultat a posteriori, i.e., après l'implémen-tation d'un prototype et de son application sur un jeu de données. L'évaluation est forcément dépendante du jeu de données considéré et les résultats sont difficilement généralisables à n'importe quel jeu de données, de n'importe quel domaine.
Outils formels
Plusieurs outils formels ont été proposés dans la littérature pour analyser qualitativement les méthodes de découverte de motifs. Premièrement, la taille des représentations condensées est souvent utilisée comme une mesure objective d'évaluation de leur intérêt (Calders et al., 2004). Par exemple, une représentation condensée fondée sur les motifs fermés est toujours plus compacte qu'une représentation condensée fondée sur les motifs libres. Les motifs fermés sont donc jugés comme plus intéressants. Cependant, les représentations condensées les plus compactes ne sont pas forcément les plus utilisées. Par exemple, les itemsets non-dérivables (NDI) sont rarement utilisés malgré leur taux de compression impressionnant. La sémantique complexe des NDI expliquerait cette impopularité pour certaines personnes.  (2010) les ont étendus aux motifs ensemblistes. A notre connaissance, de tels axiomes n'ont jamais été appliqués à des contraintes ou des algorithmes de construction de modèles. De plus, ils se sont essentiellement concentrés sur les mesures dédiées à la recherche de corrélations. Comment généraliser ces axiomes à n'importe quelle mesure d'intérêt ou contrainte ?
Enfin, à notre connaissance seuls deux travaux ont étudié l'interrelation entre les motifs lors de l'évaluation d'une mesure d'intérêt. Crémilleux et Soulet (2008) ont défini informellement la notion de contrainte globale. Il s'agit de contraintes dont l'évaluation met en correspondance plusieurs motifs. Giacometti et al. (2011) ont ensuite formalisé cette notion de contrainte globale en utilisant une algèbre relationnelle étendue spécifiquement pour la découverte de motifs. Notre cadre propose une définition formelle plus générale et plus précise, mais surtout permet de mieux analyser l'interrelation entre les motifs lors de l'évaluation. En particulier, nous ré-pondrons aux deux questions énoncées ci-avant.
3 Formalisation de l'interrelation entre motifs Fu et al., 2000) Bouncer and Picker Algorithme de sélection avec différentes heuristiques (Bringmann et Zimmermann, 2009) TAB. 1 -Définition de plusieurs méthodes de découverte de motifs fondées sur la fréquence
Méthode de découverte de motifs
Dans cet article, nous modélisons une méthode de découverte de motifs par une mesure
-le langage L correspond à l'ensemble des parties de I (i.e., L = 2 I ) 1 et -? est l'ensemble de tous les jeux de données possibles sachant qu'un jeu de données est un multi-ensemble de L.
Si le jeu de données visé est clair, M (X, D) est simplement noté M (X). Sans perte de géné-ralité, nous considérons que l'intérêt d'un motif X augmente avec M (X) i.e., si le motif X est plus intéressant que
Notre modélisation d'une méthode de découverte de motifs par une mesure d'intérêt M est suffisamment souple pour englober les principaux outils de la littérature :
-Evaluation par mesure d'intérêt : l'évaluation par mesure d'intérêt consiste à affecter un score ou un rang à chaque motif reflétant sa qualité (e.g., la all-confidence proposée par Omiecinski (2003)). -Extraction sous contraintes : l'extraction sous contraintes consiste à extraire tous les motifs satisfaisant un prédicat de sélection qui détermine la pertinence d'un motif. Par exemple, il est courant d'utiliser un seuil minimal sur une mesure d'intérêt pour filtrer les motifs (e.g., la contrainte de support minimal (Agrawal et Srikant, 1994) 
Notion de motifs liés
La qualité d'une méthode de découverte de motifs repose sur sa capacité à isoler un motif singulier qui dévie des autres motifs communs. Pour cette raison, cette méthode doit mettre en relation le motif évalué avec d'autres motifs, dits liés. Pour analyser cette interrelation entre motifs, nous proposons de déterminer son ensemble de motifs liés en identifiant l'impact de chacun de ces motifs sur le motif évalué. L'impact d'un motif Y sur le motif évalué X peut se mesurer en observant s'il existe deux jeux de données D et D quasi-équivalents où la seule variation de Y modifie l'évaluation de X. Typiquement, la all-confidence d'un itemset X introduite par Omiecinski (2003) correspond à la plus petite confiance des règles d'association Y ? Z incluses dans X. Il est bien connu que la all-confidence peut être réécrite comme le ratio entre la fréquence de X et la fréquence maximale de ses items : f req(X)/ max i?X f req({i}). Dans ce cas, quand X est évalué, les motifs liés de X sont luimême et tous ses items. En effet, augmenter la fréquence d'un item peut faire décroître la allconfidence de X. A l'inverse, augmenter la fréquence de X augmente aussi sa all-confidence. Cet exemple conduit à deux observations d'importance :
1. Il y a deux catégories de motifs liés : ceux qui peuvent améliorer la qualité du motif évalué (ici, X) et ceux qui peuvent la détériorer (ici, les items qui constituent X).
2. Les motifs liés impactent la mesure d'intérêt qu'on analyse (ici, la all-confidence) via une autre mesure d'intérêt élémentaire (ici, la fréquence).
Suivant ces deux observations, nous définissions formellement la notion de motif lié en nous appuyant sur la définition d'équivalence avec exception :  Illustrons la définition 2 avec la all-confidence. Comme la all-confidence ne peut croître qu'avec la fréquence de X, all-conf + f req (X) est égal à {X}. Nous verrons qu'il est courant voire souhaitable que le motif évalué soit aussi un motif lié. Pour l'ensemble des motifs liés négativement, nous obtenons que all-conf ? f req (X) = {{i}|i ? X} car seuls l'augmentation de la fréquence d'un item de X peut faire diminuer all-conf (X). Le tableau 2 donne d'autres exemples d'ensembles de motifs liés. Une force de la notion de motifs liés est de bien identifier les motifs « réellement » impliqués dans l'évaluation. Par exemple, la définition des motifs libres donnée dans le tableau 1 implique tous les sous-ensembles de X (avec le ?Y ? X). Pourtant, seuls les sous-ensembles directs sont des motifs liés.
Dans le tableau 2, les mesures, contraintes, algorithme de construction de modèles choisis reposent exclusivement sur la fréquence (comme mesure m qui implique les motifs liés). Il est à noter que les définitions de motifs libres/fermés auraient pu être présentées avec d'autres mesures élémentaires (e.g., fréquence disjonctive ou fonction d'agrégat). Les bordures néga-tive et positive pourraient être analysées avec d'autres mesures suivant la contrainte monotone ou anti-monotone considérée (Mannila et Toivonen, 1997). De même, la notion de top-k motif est pertinente avec d'autres mesures d'intérêt que la fréquence. En changeant la mesure élé-mentaire m introduite dans la définition 2, l'analyse des interrelations entre motifs se ferait de manière analogue.
où k = |X| et n = |I| ; singletons correspond à {{i}|i ? X} ; sous-ensembles directs correspond à {X \ {i}|i ? X} ; sur-ensembles directs correspond à {X ? {i}|i ? I \ X} ; sous-ensembles correspond à 2 X \ {X} ; treillis correspond à L \ {X}.
TAB. 2 -Analyse des méthodes suivant leurs ensembles de motifs liés
Axiomes de qualité
En s'inspirant de ce qui a été fait pour les mesures d'intérêt dédiées aux règles d'association, cette section énonce trois axiomes que devraient satisfaire une mesure ou une contrainte idéale. Le tableau 2 illustre la satisfaction ou non des axiomes par les différentes mesures et contraintes.
Réflexivité
Revenons sur l'exemple de la all-confidence. Nous avons constaté que l'augmentation de la fréquence de certains motifs avait un impact sur la all-confidence de X. Donc la fréquence est une mesure élémentaire d'importance pour la all-confidence. Par ailleurs, il est souvent considéré que l'intérêt d'un motif augmente avec sa fréquence. Il paraît donc naturel qu'augmenter la fréquence de X augmente également la all-confidence de X. Plus généralement, si l'intérêt d'un motif X augmente avec la mesure élémentaire m, l'intérêt de ce motif X selon la mesure d'intérêt M devrait également augmenter lorsque m(X) croît. Comme all-conf + f req (X) = {X}, la all-confidence est bien une mesure réflexive par rapport à la fréquence. A l'inverse, l'extraction des motifs libres n'est pas réflexive par rapport à la fréquence puisque f ree + f req (X) = {X \ {i}|i ? X} n'inclut pas X. Tandis qu'un motif est jugé plus intéressant quand sa fréquence augmente, il a moins de chance d'être libre (car sa fréquence sera plus proche de celle de ses sous-ensembles). Par conséquent, la contrainte de liberté pourrait même être qualifiée d'irréflexive par rapport à la fréquence (i.e., X ? f ree ? f req (X)). Nous reviendrons dessus dans la sous-section suivante. L'axiome 1 est une généralisation de plusieurs propositions de la littérature où la mesure m est restreinte au support. Pour les règles d'associations, Piatetsky-Shapiro (1991)  
Exclusivité
Pour être facilement compréhensible par l'utilisateur final, le comportement d'une mesure M doit toujours rester le même vis-à-vis de chaque motif lié. En d'autres termes, un motif lié ne devrait pas permettre d'augmenter la mesure M dans certains cas, et de la diminuer dans d'autres.
Axiome 2 (Exclusive) Une mesure d'intérêt M est exclusive par rapport à m ssi aucun motif est à la fois lié positivement et négativement à un motif donné pour m :
Cet axiome est largement vérifié par les méthodes de la littérature comme le montre le tableau 2 (colonne A2). Par exemple, comme all-conf + f req (X) ? all-conf ? f req (X) = ?, la allconfidence est exclusive par rapport à la fréquence. A l'inverse, l'extraction des motifs libres fréquents n'est pas exclusive puisque X appartient à la fois à f ree&f req + f req (X) à cause de la contrainte de fréquence et à f ree&f req ? f req (X) à cause de la contrainte de liberté. Le non-respect de l'axiome 2 complexifie la lecture d'une méthode d'extraction puisqu'il devient nécessaire de se reporter au jeu de données ou à d'autres motifs pour comprendre les motifs extraits. Par exemple, lors de l'extraction des motifs libres fréquents, un motif X peut ne pas être extrait soit si sa fréquence est trop basse, soit si sa fréquence est trop élevée. A l'inverse, pour les motifs fermés fréquents, un motif n'est pas extrait si sa fréquence est trop faible (aussi bien si le motif est non-fréquent ou non-fermé). Nous estimons donc que la violation de l'axiome 2 pourrait expliquer en partie l'échec des motifs NDI même s'ils constituent une représentation condensée extrêmement compacte.
La combinaison des axiomes 1 et 2 (notée A1+2 dans le tableau 2) implique naturellement que X ? M ? m (X). Si une mesure M viole cette propriété, M est dite irréflexive selon m. L'extraction des motifs libres et celle de la bordure négative des motifs fréquents sont irréflexives.
Exhaustivité
La pertinence d'un motif est d'autant plus forte que son intérêt dépend de la variation de nombreux autres motifs. Pour cette raison, tous les motifs du langage L devraient avoir un impact sur la mesure M . En d'autres termes, l'ensemble des motifs liés devrait idéalement être égal à la totalité du langage L.
Axiome 3 (Exhaustive) Une mesure d'intérêt M est exhaustive par rapport à m ssi tous les motifs du langage sont liés à tout motif pour m :
Cet axiome exprime que l'interrelation entre les motifs lors de l'évaluation d'une mesure M devrait concerner tous les motifs. Chaque variation du jeu de données mesurable à travers m(X) devrait avoir une incidence sur l'évaluation de M . Bien sûr, la plupart des méthodes d'extraction (mesures ou contraintes de la littérature) ne satisfont pas cet axiome. Cependant, nous pensons que cet axiome donne la direction à suivre et nous revenons longuement dans la section suivante sur la forme de l'ensemble de motifs liés.
Complexité et sémantique
En pratique, l'axiome 3 est peu vérifié. Néanmoins, les méthodes d'extraction proposées tendent plus ou moins à le satisfaire. Cette section propose d'étudier deux grandes caractéris-tiques de l'ensemble des motifs liés à savoir sa taille et sa forme.
Complexité en évaluation
Suivant l'axiome 3, nous affirmons que la pertinence d'un motif pour une mesure élémen-taire m est encore plus forte lorsque sa pertinence dépend de la variation de la pertinence de nombreux autres motifs selon m. Par conséquent, l'intérêt d'une mesure M (au sens de sa globalité) selon m se mesure avec la taille de l'ensemble de motifs liés : De manière similaire, on peut déterminer que la complexité en évalua-tion de la productivité est exponentielle par rapport à la taille du motif évalué puisque tous les sous-ensembles sont impliqués dans l'évaluation de cette contrainte. Suivant la complexité en évaluation, on dira donc que la productivité est plus intéressante (car plus globale) que la all-confidence car les interrelations sont plus nombreuses.
A notre connaissance, la complexité en évaluation est le premier indicateur pour mesurer l'interrelation entre les motifs lors de l'évaluation d'une mesure d'intérêt. Cette complexité permet de comparer plusieurs mesures d'intérêt entre elles. La colonne |M ± f req (X)| du tableau 2 indique la complexité en évaluation des mesures et contraintes définies dans le tableau 1. Il se dégage clairement 3 grandes classes correspondant à 3 complexités en évaluation : constant, linéaire et exponentiel.
Bien que le tableau 1 ne comporte qu'un échantillon restreint de la découverte de motifs, la complexité en évaluation des méthodes d'extraction de motifs semble avoir augmenté durant ces deux dernières décennies. Au-delà de l'intérêt des motifs extraits, nous pensons que la complexité en évaluation reflète aussi la difficulté algorithmique à les extraire. Ainsi, l'amé-lioration des techniques d'extraction pourrait expliquer cette augmentation de la qualité des motifs extraits.
TAB. 3 -Liens entre l'ensemble des motifs liés, la sémantique et la complexité
Sémantique de l'ensemble de motifs liés
Contrainte globale Le tableau 3 schématise les principales topologies observées notamment au sein du tableau 2 en les organisant en trois grandes classes de complexité évoquées dans la section précédente. Ces classes font écho à la notion de contrainte globale introduite par Crémilleux et Soulet (2008) puis définie formellement par Giacometti et al. (2011). Il s'agit des prédicats de sélection dont la complexité est au moins linéaire :
Propriété 1 (Contrainte globale) Une contrainte q : L ? {1, 0} est globale ssi il existe une mesure élémentaire m telle que la complexité en évaluation de q selon m est au moins linéaire. Propriété 2 (Webb et Vreeken (2013); Hämäläinen et al. (2010)) Une mesure d'intérêt M se comportant bien doit vérifier ?X ? L : 2 X \ {X} ? M ? f req (X) Non-redondance Toutes les méthodes de découverte de motifs visant à réduire les redondances exploitent les sous-et/ou sur-ensembles du motif évalué. La majorité des représenta-tions condensées s'appuient exclusivement sur les sous-ensembles ou sur-ensembles directs. Modèle Nous avons constaté que tous les algorithmes de construction de modèles ont leurs motifs liés qui couvrent l'intégralité du treillis comme c'est le cas pour Bouncer and Picker (Bringmann et Zimmermann, 2009). Les modèles sont souvent vus comme une amélioration des représentations condensées. La complexité en évaluation confirme que les modèles sont plus intéressants que les représentations condensées. La complexité des motifs top-k fréquents se rapproche de celle des modèles sans toutefois l'atteindre.
2. Dans ce contexte, « un bon comportement » signifie que les corrélations doivent être évaluées plus favorablement que les non-corrélations.
Conclusion
Cet article a introduit la notion de motifs liés qui nous semble centrale pour analyser l'interrelation des motifs pour les méthodes de découverte de motifs. Une force de notre approche est son large spectre d'application qui va au-delà des mesures d'intérêt pour traiter aussi bien l'extraction sous contraintes que la construction de modèles. Pour la première fois, des axiomes de qualité concernent la problématique de la non-redondance. L'introduction de la complexité en évaluation permet de dépasser le stade qualitatif pour mieux comparer plusieurs méthodes.
Plusieurs axes de progression subsistent au sein de notre cadre. La définition actuelle des motifs liés repose sur une notion d'équivalence entre jeux de données où seule une mesure élé-mentaire m est impliquée dans l'évaluation de M ; comment tenir compte qu'une autre mesure m peut potentiellement impacter M en parallèle ? Une réflexion sur la définition de mesure élémentaire et des interrelations entre mesures élémentaires est nécessaire pour répondre à cette question. Par ailleurs, d'autres axiomes importants sur les mesures d'intérêts mériteraient d'être étendus en s'appuyant sur la notion de motifs liés. Bien que l'incidence de la mesure élémentaire m dans l'évaluation de M soit cruciale sur la sémantique des motifs liés, nous n'avons pas encore étudié les implications des propriétés de m sur celles de M .

Introduction et contexte
Le problème de la prédiction de liens tel qu'il est formulé dans l'article de Liben-Nowell et al. (2007) peut être compris comme une tâche de classification binaire. Des outils classiques d'apprentissage tels que les arbres de classification, les SVM ou les réseaux de neurones ont été utilisés pour le résoudre sur des réseaux biologiques et de collaboration (Pujari et al. (2012)). Cependant, ces méthodes ne permettent pas à l'utilisateur de faire varier le nombre de prédic-tions selon ses besoins. Pour ce faire, il est possible de calculer un score pour chaque paire de noeuds, corrélé à la probabilité d'existence d'un lien entre ces noeuds, on obtient alors un classement, et l'utilisateur effectue la prédiction en sélectionnant les T paires les mieux classées. Le score peut être basé sur la structure connue du réseau, mais également sur d'autres sources d'information : par exemple les attributs des noeuds, la dynamique des contacts ou la localisation géographique (Scellato et al. (2011)). Pour combiner les informations capturées par différents scores, on utilise des méthodologies d'apprentissage de classements. Parmi les méthodes à disposition, certaines sont non-supervisées et peuvent être vues comme des mé-thodes de consensus, telles que celles décrites dans Dwork et al. (2001). Il existe également des méthodes supervisées : une solution consiste à se ramener à un problème de classification en effectuant une transformation deux-à-deux, plutôt que de considérer des éléments à ordonner, on examine des couples d'éléments dont on cherche à dire lequel doit être classé audessus de l'autre (Herbrich et al. (1999)). Malheureusement, cette méthode n'est pas adaptée à de grands réseaux où le nombre d'éléments à classer est élevé. Plus généralement, la plupart des méthodes d'apprentissage de classements ont été créées pour des tâches de recherche d'information, où l'on souhaite une précision élevée sur un petit nombre d'éléments (e.g., Burges et al. (2011)). Nous souhaitons ici au contraire pouvoir fixer le nombre de prédictions, quitte à perdre en précision, pour prédire des liens non-observés dans de grands réseaux sociaux, et nous définissons dans ce but une méthodologie simple mais efficace d'apprentissage supervisé. 2 Classements non-supervisés Scores de classement. Le but de ce travail est de montrer comment notre méthode permet de combiner des informations issues de différentes sources, nous ne présentons ici que quelques caractéristiques structurelles permettant d'associer à chaque paire de noeuds un score à partir duquel est construit un classement. On trouve dans la littérature beaucoup d'indices, nous en avons choisi quelques classiques et proposons de les généraliser à des réseaux pondérés. Certaines de ces caractéristiques sont dites locales car elles ne considèrent que des paires de noeuds à distance au plus 2. On appelle N (i) l'ensemble des voisins du sommet i : -nombre de voisins communs (CN) :
Données
D'autres caractéristiques sont dites globales car elles sont calculées sur l'ensemble de la structure du réseau et permettent de classer des paires de noeuds distantes :
-indice de Katz (Katz) : calculé à l'aide du nombre de chemins de longueur l de i à j (au sens d'un multigraphe pour un réseau pondéré), noté ? ij (l), selon l'expression : p.w(k, k )/W (k) et revenant en i avec une probabilité 1 ? p, l'indice est la probabilité pour que ce marcheur soit en j dans l'état stationnaire de la marche.
-attachement préférentiel (PA), basé sur l'observation dans les réseaux sociaux que les noeuds de fort degré tendent à créer plus de nouveaux liens : s PAw (i, j) = W (i).W (j) 2 . Enfin nous utilisons une méthode pour agréger des classements dans le but d'améliorer la qualité de la prédiction. Il s'agit de la méthode de Borda, initialement définie pour obtenir un consensus dans un système de vote. On affecte à chaque paire un score correspondant à la somme sur l'ensemble des classements des nombres de paires moins bien classées, soit :
Résultats. Certaines métriques usuelles d'évaluation des problèmes de classification, comme la courbe ROC, ne sont pas adaptées au problème, en raison de la forte asymétrie des classes. En effet, les graphes étant peu denses, on s'attend à ce que le taux de faux positif soit élevé dans une large gamme de prédictions et rende l'observation de la courbe ROC peu instructive. Comme nous souhaitons pouvoir ajuster le nombre de prédictions pour trouver un bon compromis entre précision (Pr) et rappel (Rc), nous visualisons les résultats à l'aide du F-score et des courbes Pr-Rc.
Nous traçons sur la Figure 1 les résultats obtenus sur le graphe d'apprentissage G learn pour prédire les liens A 2 ? A 2 en utilisant certains des scores définis précédemment (pas la totalité pour une question de lisibilité). On voit que l'allure des courbes de F-score varient significativement d'un indice à un autre, ce qui indique que chaque score permet de détecter des liens différents, ce dont nous chercherons à tirer parti dans RankMerging. Comme on peut s'y attendre pour une méthode de consensus, la méthode de Borda améliore les performances de la classification, en particulier la précision sur la prédiction des paires les mieux classées. Étant donnée la difficulté de la tâche, la précision est peu élevée en moyenne : lorsque Rc est plus grand que 0.06, Pr est inférieur à 0.3 pour toutes les méthodes. Nous n'utilisons que des scores structurels, rendant improbable la prédiction de liens entre paires de sommets éloignées dans le graphe, le rappel est donc limité à des valeurs relativement faibles, car augmenter le nombre de prédictions diminuerait drastiquement la précision.
Méthode RankMerging
Les différentes méthodes de classements précédemment citées ne sont pas sensibles aux mêmes types d'information structurelle. Nous décrivons une méthode générique d'apprentissage supervisé qui permet d'agréger les classements en tirant partie de la complémentarité des sources d'information. Celle-ci ne nécessite pas qu'une paire soit bien classée selon chaque critère, comme pour une règle de consensus, mais qu'elle soit bien classée selon au moins un. La procédure est dénommée RankMerging, une implémentation et un guide utilisateur sont à disposition sur http://lioneltabourier.fr/program.html.
2. Katz et RWR sont calculés à l'aide de sommes infinies, que nous approximons à l'aide des quatre premiers termes afin de réduire le coût du calcul. La forte asymétrie des classes tend à diminuer les performances de l'indice PA, nous limitons la prédiction avec cet indice aux paires de sommets distantes d'au plus 3.
3. Pour que cette méthode ne biaise pas en faveur des prédicteurs qui classent un grand nombre d'éléments, on considère qu'une paire classée dans r? mais pas dans r ? est aussi classée dans r ? , avec un rang équivalent à toutes les autres paires non-classées et en-dessous de toute paire classée. Pour plus de détails, voir Dwork et al. (2001). L'idée centrale est de déterminer à chaque pas de l'algorithme le classement qui prédit le nombre le plus élevé de tp dans les prochaines étapes. Dans ce but nous définissons une fenêtre W i pour chaque classement : c'est l'ensemble des g liens non-prédits classés à partir du rang ? i dans r i 4 . On appelle qualité ? i le nombre de tp dans W i . À chaque étape, le classement r i dont la fenêtre a la qualité la plus élevée est sélectionné (en cas d'égalité, on choisit aléatoirement), et on ajoute alors la paire classée au rang ? i de r i au classement de sortie de la phase d'apprentissage (r L M ). On met à jour ? i et le curseur de fin de la fenêtre, de manière à ce que chaque W i contienne toujours exactement g paires, puis on met à jour les qualités ? i . Au cours du processus, on enregistre à chaque pas les valeurs des curseurs ? 1 ...? ? , qui indiquent la contribution de chaque classement au classement agrégé. Cette procédure est itérée jusqu'à ce que le classement agrégé r L M contienne le nombre de paires T fixé par l'utilisateur. Cet algorithme présente l'intérêt majeur de ne parcourir qu'une seule fois chaque classement, soit une complexité en O(?N ) pour ? classements de taille N (on note que N ? T ).
La phase de test consiste à agréger les classements obtenus sur le réseau G test en utilisant les paramètres ? 1 ...? ? appris durant la première phase. L'implémentation pratique est simple : à chaque étape on regarde quel classement a été choisi à l'étape correspondante de l'apprentissage, et on sélectionne la paire la mieux classée du classement correspondant pour le graphe de test. Si celle-ci n'est pas déjà dans le classement agrégé de la phase de test (r Protocole expérimental et résultats. Nous évaluons les performances de RankMerging en les comparant à des techniques existantes. En premier lieu, nous confrontons les résultats à la méthode non-supervisée de Borda, vue précédemment. Nous comparons aussi à des méthodes de classification supervisées, même si celles-ci ne sont pas conçues pour varier le nombre T de prédictions possibles 6 . Nous utilisons les implémentations proposées dans la boîte à outils Python scikit learn (scikit-learn.org) des méthodes suivantes : les k-plus proches voisins (NN), les arbres de classification (CT) et AdaBoost (AB), en faisant varier les paramètres de manière à obtenir plusieurs points dans l'espace Pr-Rc 7 . Suivant la description de la méthode faite précédemment, nous mesurons les ? i obtenus sur G learn pour découvrir les liens A 2 ? A 2 , puis nous les utilisons pour agréger les classements obtenus sur G test pour prédire les liens B ? B, en utilisant le facteur d'échelle f ? 1.5. La sélection des caractéristiques pertinentes n'est pas problématique ici. En effet, l'utilisateur peut agréger autant de classements qu'il le souhaite car, d'une part, l'addition d'un nouveau classement a un faible coût computationnel, et d'autre part, la méthode est conçue pour qu'un classement n'apportant pas d'information nouvelle soit simplement ignoré pendant l'agréga-tion. Le paramètre g de l'algorithme est ici fixé par une simple extrapolation : on mesure la valeur de g qui permet de maximiser la qualité de l'agrégation sur G learn , et on emploie la même pour l'agrégation sur G test (ici g = 200). Sur la Figure 2, nous traçons le F-score et la courbe précision-rappel obtenus pour RankMerging (g = 200), en agrégeant les classements des indices suivants : AA w , CN w , CN , Jacc w , Katz w (? = 0.01), PA w , RWR w (p = 0.8) et la méthode de Borda appliquée à ces sept indices. RankMerging permet d'améliorer les prédictions : la mesure de l'aire sous la courbe Pr-Rc indique une amélioration de 8.3% par rapport à la méthode de Borda 8 . On pouvait s'y attendre, dans la mesure où RankMerging est une méthode supervisée et utilise l'information contenue dans le consensus de Borda. En fait, la méthode est conçue pour que n'importe quel classement non-supervisé puisse être agrégé sans perte de performance. Nous vérifions cela en pratique en retirant un à un les classements et en constatant que l'amélioration ne fait que décroître, et ce quel que soit l'ordre dans lequel on retire les différents classements 9 . En ce qui concerne les méthodes de classifications supervisées, nous pouvons constater que leurs performances sont élevées uniquement pour un faible nombre de prédictions (inférieur à 2000), mais ces méthodes n'étant pas conçues pour faire varier le nombre de prédictions, elles produisent de très faibles performances en dehors de leur domaine optimal.

Introduction
Le problème de prédiction de séquences est un problème important en fouille de données, défini de la façon suivante. Soit un alphabet Z = {e 1 , e 2 , ..., e m } contenant un ensemble d'éléments (symboles). Une séquence est une suite d'éléments totalement ordonnée s = 1 , i 2 , ...i n où i k ? Z (1 ? k ? n). Un modèle de prédiction M est un modèle entraîné avec un ensemble de séquences d'entraînement. Une fois entraîné, le modèle peut être utilisé pour effectuer des prédictions. Une prédiction consiste, à prédire le prochain élément i n+1 d'une séquence 1 , i 2 , ...i n en utilisant le modèle M . La prédiction de séquences a des applications importantes dans une multitude de domaines tels que le préchargement de pages Web (Deshpande et Karypis, 2004;Padmanabhan et Mogul, 1996), la recommandation de produits de consommation, la prévision météorologique et la prédiction des tendances du marché boursier.
Un grand nombre de modèles de prédictions ont été proposés pour la prédiction de sé-quences. Un des modèle les plus connus est PPM (Prediction by Partial Matching) (Cleary et Witten, 1984). Ce modèle, basé sur la propriété de Markov, a engendré une multitude d'approches dérivées telles que Dependancy Graph (DG) (Padmanabhan et Mogul, 1996), All-korder-Markov (Pitkow et Pirolli, 1999) et Transition Directed Acyclic Graph (TDAG) (Laird et Saul, 1994). Bien que des propositions ont été faites pour réduire la complexité temporelle et spatiale de ces modèles (Begleiter et al., 2004), l'exactitude de leurs prédictions a subi peu d'amélioration. D'autre part, un certain nombre d'algorithmes de compression ont été adaptés pour la prédiction de séquences tels que LZ78 (Ziv et Lempel, 1978) et Active Lezi (Gopalratnam et Cook, 2007). De plus, des algorithmes d'apprentissage machine comme les réseaux de neurones et la découverte de règles d'association séquentielles ont été employés pour faire de la prédiction de séquences (Fournier-Viger et al., 2012;Sun et Giles, 2001). Néanmoins, ces modèles souffrent de limites importantes. Premièrement, la plupart d'entre eux partent de l'hypothèse Markovienne qu'un événement ne dépend que de son prédécesseur. Or, ce n'est pas le cas pour de nombreuses applications, ce qui nuit à l'exactitude des prédictions. Deuxièmement, tous ces modèles sont construits avec perte d'information par rapport aux séquences d'entraî-nement. Donc, ils n'utilisent pas toute l'information disponible dans les séquences d'entraîne-ment pour effectuer les prédictions.
Pour pallier ces limites, un modèle nommé Compact Prediction Tree (CPT) (Gueniche et al., 2013) a été récemment proposé. Il utilise une structure en arbre pour compresser les sé-quences d'entraînement sans perte ou avec une perte minime d'information. De plus, il emploie un algorithme de prédiction conçu pour tenir compte du bruit et de plusieurs événements anté-rieurs lors d'une prédiction plutôt que seulement le dernier. Il a été montré que ce modèle peut obtenir des prédictions jusqu'à 12 % plus exactes que PPM, DG et All-K-order-markov sur des jeux de données provenant de divers domaines, ce qui constitue un gain important. Néanmoins, une limite de CPT est sa complexité temporelle et spatiale élevée. Dans cet article, nous pallions ces problèmes en proposant trois stratégies pour réduire la taille et le temps de prédiction de CPT. De plus, nous présentons une comparaison expérimentale avec davantage de modèles de prédiction de la littérature : All-K-order Markov, DG, Lz78, PPM et TDAG. Les résultats expérimentaux sur 7 jeux de données réels montrent que le modèle résultant nommé CPT+ est jusqu'à 98 fois plus compact et est jusqu'à 4.5 fois plus rapide que CPT. De plus, CPT+ conserve une exactitude très élevée par rapport aux autres approches de la littérature.
Le reste de cet article est organisé de la façon suivante. La section 2 décrit brièvement le modèle CPT. Les sections 3 et 4 proposent respectivement de nouvelles stratégies pour réduire la taille du modèle CPT et ses temps de prédiction. La section 5 présente l'évaluation expérimentale avec plusieurs jeux de données et les principaux modèles de prédictions de la littérature. Finalement, la section 6 est dédiée à la conclusion et aux travaux futurs.
Le processus d'entraînement
Le processus d'entraînement génère trois structures distinctes à partir des séquences d'entraînement : (1) un Arbre de Prédiction (AP), (2) un Dictionnaire de Séquences (DS) et (3) un Index Inversé (II). Pendant l'entraînement, les séquences sont considérées les unes après les autres pour construire incrémentalement ces trois structures. À titre d'exemple, la figure 1 illustre la création des structures de CPT par insertion successive des séquences
(1) Insertion de ?í µí±¨, í µí±©, í µí±ª? . Chacun des noeuds de l'arbre représente un élément et chacune des séquences d'entraînement est représentée par un chemin partant de la racine de l'arbre et se terminant par un noeud interne de l'arbre ou une feuille. La construction de cet arbre a une basse complexité. Insérer une séquence de m éléments demande de parcourir/créer au plus m noeuds. La construction complète de l'arbre est O(n) où n est le nombre de séquences à insérer. Tout comme un arbre préfixe, cet arbre est une représentation compacte des séquences d'entraînement, car les séquences partageant un préfixe commun partagent un chemin dans l'arbre. Par exemple, à la figure 1, les séquences s 1 , s 2 et s 3 partagent le même chemin correspondant au préfixe B Dans le pire cas, le gain spatial offert par cette compression est nul, mais en pratique, tout dépendant de la densité et de la similarité des séquences du jeu de données utilisé, l'arbre peut offrir une réduction spatiale très importante allant jusqu'à 98% (Gueniche et al., 2013).
Le Dictionnaire de Séquences est une structure qui permet d'extraire chacune des sé-quences d'entraînement de l'arbre de prédiction. Lors de la construction du modèle CPT, un identifiant unique est assigné à chaque séquence. Il est égal à 1 pour la première séquence insérée (dénoté s 1 ) et est incrémenté d'un pour chaque séquence subséquente (s 2 , s 3 , ...). Le dictionnaire de séquences associe chaque identifiant de séquence s a à un pointeur vers un noeud de l'arbre. Ce noeud représente le dernier élément de la séquence s a dans l'arbre. Grâce à cette structure, il est possible de parcourir chaque séquence d'entraînement dans l'arbre de prédiction du dernier au premier élément.
L'Index Inversé permet d'identifier rapidement dans quelles séquences apparaît un ensemble d'éléments d'une séquence à prédire. L'index inversé contient un vecteur de bits v e pour chaque élément e de l'alphabet Z présent dans les séquences d'entraînement. Le k-ième bit d'un vecteur de bit v e prend la valeur 1 si l'élément e apparaît dans la séquence s k , sinon il prend la valeur 0. Par exemple, à la figure 1, le vecteur de bit de l'élément C après l'insertion des séquences s 1 , s 2 , s 3 , s 4 et s 5 est 10110, car C apparaît dans les séquences s 1 , s 3 et s 4 . L'index inversé est utilisé pour déterminer rapidement les séquences d'entraînement contenant un ensemble d'éléments d'une séquence à prédire. Cela est réalisé en faisant l'intersection des vecteurs de bits des éléments. Par exemple, déterminer l'ensemble des séquences contenant les éléments A et C est réalisé par l'opération 11101 ? 10110, donnant le résultat 10000, autrement dit {s 1 }. Grâce à l'index inversé, cette tâche est très rapide ; O(i) où i est le nombre d'éléments dans l'ensemble.
Le processus de prédiction
Le processus de prédiction de CPT utilise les trois structures décrites précédemment. Soit une séquence s = 1 , i 2 , ...i n de n éléments et y, un nombre entier représentant le nombre d'éléments de s à considérer pour faire une prédiction. Le suffixe de taille y de s dénoté P y (s) est défini comme étant P y (s) = n?x+1 , i n?x+2 ...i n La prédiction du prochain élément de s est effectuée de la façon suivante : CPT identifie tout d'abord les séquences similaires à P y (s), c.à.d. qui contiennent les derniers y éléments de P y (s) dans n'importe quel ordre et positions. Puis, pour chaque séquence similaire, CPT considère son conséquent. Le consé-quent d'une séquence u est la sous-séquence débutant après le dernier élément en commun avec P y (s) jusqu'à la fin de u. Chaque élément e dans un de ces conséquents est ensuite stocké dans une structure nommé Table de Compte (TC) avec son nombre d'occurrences (ce nombre est une estimation de la probabilité P (e|P y (s))). L'élément ayant le plus grand nombre d'occurrences est l'élément prédit par CPT. La mesure de similarité utilisée pour déterminer les séquences similaires est de nature stricte, mais est relâchée dynamiquement par le processus de prédiction, pour deux raisons. Premièrement, avec une mesure de similarité trop stricte, une séquence à prédire peut n'être similaire à aucune séquence d'entraînement, et donc aucune prédiction n'est possible. Deuxièmement, une mesure de similarité trop stricte ne permet pas de considérer qu'une séquence peut-être partiellement similaire à une autre. Or, dans les applications réelles, il y a souvent des éléments présents dans les séquences qui sont du bruit. Pour relâcher la mesure de similarité, CPT suppose qu'un ou plusieurs éléments présents dans le suffixe de la séquence à prédire sont du bruit et qu'ils peuvent être ignorés lors du calcul de similarité. Le calcul de similarité pour un suffixe P y (s) est fait par niveau, où à chaque niveau k = 1, 2, ..., |P y (s)| ? 1 toutes les sous-séquences de taille |P y (s)| ? k de P y (s) sont générées. Chacune des sous-séquence u est utilisée pour trouver les séquences similaires dans l'ensemble de séquences d'entraînement et pour mettre à jour la TC. Ce relâchement de la mesure de similarité se poursuit pour la séquence à prédire d'un niveau à l'autre tant que TC n'a pas été mise à jour un nombre minimum de fois.
Stratégies de compression de l'arbre de prédiction
Bien que CPT offre des prédictions plus exactes que les principaux modèles de prédiction de la littérature selon une étude antérieure (Gueniche et al., 2013), une limite importante de CPT est sa complexité spatiale. Il a été montré que la taille des structures de CPT est inférieure à All-k-order Markov, mais demeure nettement supérieures à d'autres modèles comme DG et PPM. L'arbre de prédiction étant la structure la plus imposante de CPT, nous proposons ci-après deux stratégies pour réduire sa taille.
Stratégie 1 : Compressions des Chaînes Fréquentes (CCF). Certaines répétitions peuvent être identifiées dans les séquences d'entraînement. Dépendamment du jeu de données, ces répé-titions peuvent être nombreuses et fréquentes. La compression des chaînes fréquentes consiste à identifier les sous-chaînes fréquentes d'éléments apparaissant dans les séquences d'entraîne-ment, puis à remplacer les sous-chaînes fréquentes par des éléments individuels.
Soit une séquence s = 1 , i 2 , ..., i n Une séquence c = m+1 , j m+2 , ..., j m+k est une sous-chaine de s, dénoté c s, si et seulement si 1 ? m ? m + k ? n. Pour un ensemble de séquences d'entraînement S, une sous-chaîne d est fréquente si |{t|t ? S ? d t}| > minsup pour un seuil minsup fixé par l'utilisateur.
La compression des chaînes fréquentes est effectuée pendant la phase d'entraînement de CPT en trois étapes : (1) identifier les chaînes fréquentes dans l'ensemble des séquences d'entraînement, (2) créer un nouvel élément dans l'alphabet Z pour représenter chaque sous-chaîne fréquente et (3) remplacer les sous-chaînes fréquentes par l'élément correspondant lors de la construction de l'arbre de prédiction de CPT. L'identification de séquences fréquentes dans un ensemble de séquences est un problème populaire en fouille de données, pour lequel un grand nombre d'algorithmes ont été proposés. Pour cette tâche, nous avons adapté un des algorithmes les plus performants nommé PrefixSpan (Pei et al., 2001), afin de ne découvrir que les séquences fréquentes d'éléments consé-cutifs (sous-chaînes). De plus, nous avons ajouté la contrainte que les sous-chaînes fréquentes doivent respecter des contraintes de longueur minimale minSize et maximale maxSize (deux paramètres).
Les sous-chaînes fréquentes identifiées sont stockées dans une nouvelle structure nommée Dictionnaire des chaînes fréquentes (DCF). Cette structure associe un nouvel élément non présent dans l'alphabet Z (dans les séquences d'entraînement) à chaque sous-chaîne fréquente. Le DCF permet de rapidement convertir une sous-chaîne en son élément correspondant et viceversa. Lors de l'insertion des séquences d'entraînement dans l'arbre de prédiction, le DCF est utilisé pour remplacer chaque sous-chaîne par son élément correspondant.
À titre d'exemple, l'illustration (1) de la figure 2 affiche la compression de l'arbre de pré-diction de l'illustration (5) de la figure 1 par la stratégie CCF. La sous chaîne fréquence B a été remplacée par un nouveau symbole x, réduisant le nombre de noeuds de l'arbre de pré-diction.
La stratégie de compression de séquences CCF a un effet seulement sur l'arbre de pré-diction où son nombre de noeuds et sa hauteur tendent à diminuer grandement. La stratégie CCF est transparente pour le processus de prédiction de CPT. En effet, lors de l'extraction de séquences similaires, les branches de l'arbre de prédiction sélectionnées sont décompressées à la volée par DCF. L'identification et le remplacement de branches simples sont faits en un seul parcours de l'arbre de prédiction. L'index inversé et le dictionnaire de séquences n'étant pas influencés par cette approche, le seul changement au processus de prédiction est la décompression dynamique des branches simples lorsque nécessaire. La complexité de ce remplacement est de O(n * (1 ? t)) où s est le nombre de séquence et t le taux de recouvrement de l'arbre, ce dernier est défini comme le "ratio" de noeuds qui partagent plusieurs séquences par le nombre total de noeuds dans l'arbre.
Stratégie de réduction des temps de prédiction
Stratégie 3 : Prédiction avec réduction du Bruit Amélioré (PBA). Tel qu'expliqué pré-cédemment, pour prédire le prochain élément s n + 1 d'une séquence s = 1 , i 2 , ..., i n CPT utilise le suffixe de taille y de s dénoté P y (s) (les y derniers éléments de s), où y est un paramètre propre à chaque jeu de donnée. CPT prédit le prochain élément de s en parcourant les séquences similaires à son suffixe P y (s). La recherche de séquences similaires est rapide (O(y)). Toutefois, le mécanisme de réduction du bruit lors des prédictions (décrit à la section 2) ne l'est pas, car il requiert de considérer non seulement P y (s) pour une prédiction, mais aussi toutes les sous-séquences de P y (s) de taille t > k. Plus y et k sont grands, plus le nombre de sous-séquences à considérer l'est aussi, et donc le temps de prédiction. Lors d'une tâche de prédiction, certains éléments dans une séquence à prédire peuvent être considérés comme du bruit si leur simple présence affecte de façon négative le résultat de la prédiction. La stratégie PBA se base sur l'hypothèse que le bruit observé dans une séquence est constitué des éléments ayant une faible fréquence, où la fréquence d'un élément est le nombre de séquences d'entraînement contenant l'élément. Pour cette raison, PBA enlève seulement les éléments qui ont une faible fréquence pendant la phase de prédiction. Puisque la définition du bruit de CPT+ est plus restrictive que celle de CPT, un moins grand nombre de sous-séquences sont considé-rées. Cette réduction à un impact positif et tangible sur les temps de calculs tel que présentés dans notre évaluation expérimentale (section 5). Le pseudo-code illustrant la stratégie PBA est présenté ci-après (Algorithme 1). L'algorithme prend en paramètres le préfixe P y (s) à prédire, les autres structures de CPT, un taux de bruit et un nombre minimum de mise à jour à faire à la TC pour faire une prédiction. Le taux de bruit représente le pourcentage d'éléments dans une séquence qui doivent être considérés comme du bruit ; un taux de bruit de 0 indique que les séquence n'ont pas de bruit alors qu'un taux de bruit de 0.4 signifie que 40% des éléments d'une séquence pourrait être du bruit. PBA est récursive de nature et considère un nombre minimal de sous-séquences dérivées de P y (s) pour faire une prédiction. Le bruit est d'abord retiré de chaque sous-séquence. Puis la TC est mise à jour. Lorsque le nombre minimal de mise à jour est atteint, une prédiction est faite comme dans CPT en utilisant la TC. La stratégie PBA est une généralisation de la stratégie de réduction du bruit utilisée par CPT. En effet, selon les paramètres utilisés, il est possible de reproduire le fonctionnement original de CPT. Les trois contributions principales apportées par PBA sont l'imposition d'un nombre minimal de mise à jour de la TC pour faire une prédiction, la définition du bruit basé sur la fréquence d'un élément et la réduction relative du bruit par rapport à la longueur de la séquence.
Algorithme 1 : L'algorithme de prédiction avec PBA input : PS : le suffixe P s, CPT : les structures de CPT, TB : le taux de bruit output : Seq : un ou plusieurs éléments prédits file.ajouter(PS); while nombreMiseAjour < minNombreMiseAJourTC ? file.nonVide() do suffixe = file.prochain(); elementsBruit = selectionnerElementsMoinsFrequents(TB); foreach elementBruit ? elementsBruit do suffixeSansBruit = copierSuffixeSansBruit(suffixe, elementBruit); if suf f ixeSansBruit.length > 1 then file.ajouter(suffixeSansBruit); end mettreAJourCountTable(CPT.countTable, suffixeSansBruit); nombreMiseAjour++; end retourne faireUnePrediction(CPT.countTable); end
Évaluation expérimentale
Nous avons effectué une série d'expériences pour comparer la performance de CPT+, CPT et les principaux modèles de prédiction de la littérature All-K-order Markov, DG, Lz78, PPM et TDAG. Pour implémenter CPT+, nous avons obtenu et modifié le code source proposé dans l'article original de CPT (Gueniche et al., 2013). Pour permettre la reproduction des expériences, le code source des modèles et jeux de données sont fournis à l'adresse http: //goo.gl/LE4uYO. Tous les modèles sont implémentés en Java 8. Les expériences ont été réalisées sur une machine dotée d'un processeur deux coeurs Intel i5 de 4ème génération avec 8 Go de mémoire vive et un SSD en SATA 600. Tous les modèles de prédiction utilisés ont été configurés empiriquement pour tenter de donner des valeurs optimales à chacun de leurs paramètres. PPM et LZ78 n'ont pas de paramètres, DG et AKOM ont respectivement une fenêtre de 4 et un ordre de 5, finalement, par soucis d'espace, TDAG à une hauteur maximale de 6. CPT à 4 paramètres et CPT+ en a 8, leurs valeurs sont elles aussi déterminées via une exploration expérimentale de l'espace de valeurs possibles. Ces valeurs sont accessible dans les fichiers sources du projet. Les paramètres propres à l'expérience se limitent à la longueur minimale et maximale des séquences utilisées, la taille du suffixe à considérer pour une séquence à prédire et la quantité d'éléments à prédire pour chacune des séquences.
Des jeux de données ayant des caractéristiques variées ont été utilisés (cf. Table 1) : sé-quences courtes/longues, séquences denses/éparses, petit/grands alphabets et divers types de données. Les jeux de données BMS, Kosarak, MSNBC et FIFA consistent en des séquences de pages Web visitées par des utilisateurs sur un site Web. Dans ce scénario, les modèles de pré-diction sont appliqués pour prédire la prochaine page Web que visitera chaque utilisateur. Le jeu de données SIGN est un ensemble de phrases exprimées en langage des signes, transcrites à partir de vidéos. Bible Word et Bible Char sont deux jeux de données qui proviennent de la Bible, livre religieux, le premier est l'ensemble des phrases découpées en mots et le second est l'ensemble des phrases découpées en caractères.
Pour l'évaluation des prédictions des modèles, une prédiction est soit un succès, un échec, ou une abstention (si un modèle ne peut effectuer une prédiction). Deux mesures sont utilisées. La couverture est le nombre d'abstentions divisé par le nombre de séquences à prédire. L'exactitude (alias précision) le nombre de succès divisé par le nombre de séquences à prédire.
Nom
Nombre Expérience 1 : comparaisons des optimisations. Dans cette première expérience, nous avons tout d'abord évalué les améliorations spatiales présentées à la section 3 en terme de taux de compression et de temps de calcul à l'entraînement. Les autres mesures de performance telles que le temps de prédiction, la couverture et l'exactitude ne sont pas affectées par la compression de l'arbre de prédiction. Pour un arbre de prédiction A avec s noeuds avant compression et s2 noeuds après compression, le taux de compression tc a de A est défini comme tc = 1 ? (s2/s), et est compris entre 0.0 et 1.0 non inclusivement. Plus la valeur est haute, plus la compression est importante. Les deux stratégies de compression sont évaluées d'abord individuellement (dénotées CCF et CBS) puis en conjonction (dénoté CPT+). Toute compression permet d'obtenir un gain spatial au prix d'un coût temporel. La figure 3 présente cette relation pour chacune des stratégies de compression. Les résultats présentés à la figure 3 montrent que le taux de compression de l'arbre varie selon le jeu de données de 58.90% à 98.65%. CCF offre un taux de compression moyen de 48.55% avec un faible écart type de 6.7%. alors que CBS à un taux de compression moyen de 77.87% avec un écart type beaucoup plus prononcé de 15.9%. L'efficacité de CBS est dépen-dante au jeu de données ; dans le cas de MSNBC, qui est le jeu de données le moins affecté par les stratégies de compression, la faible cardinalité de son alphabet permet à MSNBC d'être naturellement compressé grâce au fort recouvrement des branches de son arbre de prédiction. En effet, MSNBC ne possède que 17 éléments uniques et même si la taille moyenne des sé-quences ressemble à celle des autres jeux de données, la taille de son arbre avant compression est très petite. Le jeu de données où les stratégies de compression CCF et CBS sont les plus effectives est SIGN. SIGN a un très faible nombre de séquences, mais chacune d'elle est très longue (en moyenne 93 éléments). Ces caractéristiques font en sorte que son arbre de prédic-tion a un faible taux de recouvrement et donc une importante partie de ses noeuds n'ont qu'un seul fils ; ce qui rend ce jeu de données un candidat idéal pour la stratégie CBS. CBS offre un taux de compression de 98.60 % pour SIGN.
La figure 3 présente également les temps d'entraînement engendrés par les deux stratégies de compression de CPT, CBS et CCF. La mesure utilisée est un facteur multiplicatif du temps d'entraînement. Par exemple, un facteur de x pour CBS signifie que CBS a eu une phase d'entraînement x fois plus longue. Pour tous les jeux de données à l'exception de SIGN, CBS est plus rapide que CCF. Il est intéressant d'observer que le temps pris par la combinaison des deux stratégies de compression n'est pas simplement une addition de leur coût d'entraînement.
CBS et CCF sont appliqués indépendamment à CPT et pourtant l'utilisation de CBS réduit les temps de calcul de CCF grâce à une diminution du nombre de branches qui ont besoin d'être compressées.
Nous avons également évalué le gain en temps de prédiction et l'exactitude (précision) obtenue en appliquant la stratégie PBA. La figure 4 (gauche) illustre les temps de prédiction de CPT+ (avec CBA), et ceux de CPT. Les gains temporels sont importants pour la plupart des jeux de données notamment pour SIGN et MSNBC où les temps d'entraînement sont jusqu'à 4.5 fois moindres. Pour les jeux de données Bible Word et FIFA, les temps de prédiction sont plus élevés pour obtenir un gain en exactitude comme le montre la figure 4 (droite). L'effet de CBA sur l'exactitude des prédictions est positif pour tous les jeux de données sauf MSNBC. Cette amélioration s'élève jusqu'à 5.47% dans le cas de Bible Word. CBA se montre donc une stratégie effective pour à la fois réduire les temps de prédiction et augmenter l'exactitude des prédictions. Expérience 2 : Mise à l'échelle. Nous avons également comparé la complexité spatiale de CPT+ (avec ses deux stratégies de compression) avec celle de CPT et All-K-order Markov, DG, Lz78, PPM et TDAG, en termes de mise à l'échelle par rapport au nombre de séquences. Les deux seuls jeux de données utilisés sont FIFA et Kosarak à cause de leur grand nombre de séquences (573,060 et 638,811 respectivement). L'accroissement du nombre de séquences dans cette expérience est quadratique et s'arrête à 128,000 séquences dû aux énormes temps de calcul requis pour réaliser chaque expérience. La figure 5 présente les résultats. Le taux de compression de CPT+ tend à baisser très légèrement avec l'accroissement du nombre de sé-quences, ce phénomène est causé par un recouvrement de plus en plus important des branches dans l'arbre de prédiction ; car la taille de l'alphabet étant constante, plus de séquences sont utilisées et plus de branches s'unifient. Les modèles DG et PPM ont une croissance linéaire, car ils sont basés sur la taille de l'alphabet et indirectement sur le nombre de séquences d'entraînement. Les autres modèles ont tous une croissance beaucoup plus importante que DG et PPM, notamment TDAG et LZ78.
Expérience 3 : Comparaison avec les autres modèles de prédiction Dans l'expérience 1, nous avons comparé l'exactitude des prédictions de CPT+ avec celle de CPT afin d'évaluer la contribution de la stratégie PBA. Dans cette expérience, nous effectuons une comparaison de l'exactitude celle des autres principaux modèles de prédiction de la littérature All-K-order Markov, DG, Lz78, PPM et TDAG, sur les mêmes jeux de données. Il est à noter que nous ajoutons dans cette comparaison deux modèles de prédictions (Lz78 et TDAG) qui n'ont pas été utilisés dans l'article original proposant CPT. 
Conclusion
Dans cet article, nous avons présenté trois stratégies pour réduire la taille et le temps de prédiction de CPT, nommées CCF (Compression des Chaînes Fréquences), CBS (Compression des Branches Simples) et PBA (Préduction avec réduction du Bruit Améliorée). Les résultats expérimentaux sur 7 jeux de données réels ont montré que le modèle résultant nommé CPT+ est jusqu'à 98 fois plus compact que CPT, et que cette compression demeure lorsque le nombre de séquence augmente. En termes de temps d'exécution, CPT+ s'est montré jusqu'à 4.5 fois plus rapide que CPT. Finalement, CPT+ s'est montré comme étant le modèle offrant les pré-dictions généralement les plus exactes dans une comparaison avec les principaux modèles de la littérature CPT, All-K-order Markov, DG, Lz78, PPM et TDAG.
Comme travaux futurs, nous adapterons CPT+ pour la prédiction de séquences dans le contexte d'un flux infini de séquences. CPT+, de par sa nature incrémentale, pourrait être adapté à ce problème.

Introduction
Ce travail s'inscrit dans le cadre de l'apprentissage supervisé et plus précisément des systèmes de classification à base de règles floues (Ishibuchi et al. (1992). Ces systèmes ont la spécificité d'être facilement interprétables grâce à l'utilisation de termes linguistiques.
Dans ces systèmes, les règles floues peuvent être fournies par un expert humain. Comme l'acquisition des connaissances humaines est une tâche complexe, plusieurs travaux se sont consacrés à l'automatisation de la construction des règles à partir des données numériques (Ishibuchi et al., 1992), (Dehzangi et al., 2007). Cette construction comprend deux phases : une partition floue de l'espace des entrées puis la construction d'une règle floue pour chaque sous-espace flou issu de cette partition. Dans ces systèmes, un nombre élevé d'attributs conduit à une explosion du nombre de règles générées, ce qui entraîne une dégradation de la compréhensibilité des systèmes, et affecte le temps de réponse nécessaire aussi bien à la phase d'apprentissage qu'à la phase de classification.
De ce fait, l'optimisation du nombre de règles floues ainsi que du nombre d'antécédents paraît comme une clé pour améliorer les systèmes de classification à base de règles floues. Dans ce cadre, plusieurs approches ont été proposées dans la littérature. On peut citer l'approche de sélection des règles pertinentes par algorithme génétique (Ishibuchi et al., 1995) ou par le concept d'oubli . Une autre approche consiste à réduire le nombre d'attributs par une sélection des attributs les plus significatifs (Lee et al., 2001).
Dans ce papier, nous nous intéressons à la technique de regroupement des attributs dans les prémisses des règles. Dans cette approche, initialement introduite dans un cadre non flou par Borgi (1999), les attributs prédictifs sont regroupés en blocs, les attributs de chaque bloc sont traités séparément et apparaissent ensemble dans une même prémisse. Une première extension de ce travail dans un cadre flou, pour la génération de règles dans les systèmes d'infé-rence floue, a été réalisée par Soua et al. (2012). Cette approche de regroupement des attributs, nommée SIFCO, présente l'avantage de décomposer le problème d'apprentissage en des sousproblèmes de complexité inférieure, et de réduire ainsi le nombre de règles générées. De plus, cette approche permet d'obtenir des règles plus intelligibles car de taille réduite.
Le regroupement d'attributs dans (Borgi, 1999) et (Soua et al., 2012) se fait par recherche de corrélation linéaire : les attributs linéairement corrélés sont regroupés et traités séparément. Dans cet article, nous proposons une méthode qui se base sur le concept des règles d'association (RA) introduit par Agrawal et al. (1993). Les RA vont nous permettre de déterminer les attributs "liés" ou "associés" qui seront regroupés dans les mêmes règles.
L'article est organisé comme suit : dans la partie 2, nous présentons les systèmes de classification à base de règles floues. Nous décrivons, dans la partie 3, le principe de regroupement des attributs comme présenté dans (Borgi, 1999) et (Soua et al., 2012). Notre approche de regroupement des attributs par RA est décrite dans la partie 4 et les résultats des tests expéri-mentaux sont présentés dans la partie 5. Nous concluons l'article en présentant les principales perspectives de ce travail.
Apprentissage à base de règles floues
On se place dans le cadre des problèmes de classification supervisée dont le but est d'affecter une classe à un objet décrit par des variables descriptives (des attributs). Nous nous intéressons au système de classification floue proposé dans (Ishibuchi et al., 1992). Afin de simplifier les notations, nous désignons ce système par l'acronyme SIF. Deux phases sont à distinguer dans ce système : la phase d'apprentissage dans laquelle on construit le modèle de classement à partir des données d'apprentissage, et la phase de classification qui sert à associer une classe à un objet inconnu en utilisant ce modèle.
Phase d'apprentissage
La méthode de génération des règles floues que nous adoptons correspond à l'utilisation d'une grille floue simple, proposée par Ishibuchi et al. (1992). Pour illustrer cette approche, nous supposons, par souci de clarté, que notre problème d'apprentissage est un problème bidimensionnel (2 attributs : X 1 et X 2 ). Les m exemples d'apprentissage considérés sont notés E p = (X p1 , X p2 ) ; p = 1, 2, . . . , m , ils appartiennent chacun à l'une des C classes : y 1 , y 2 , . . . , y C . Il est à noter que dans les SIF les attributs considérés sont numériques ; chacun des attributs X 1 et X 2 est partitionné en k sous-ensembles flous {A 1 , A 2 , . . . , A k } où chaque sous-ensemble A i est défini par une fonction d'appartenance triangulaire symétrique. Un exemple de grille floue simple est présenté dans Fig 
-y k ij est la conclusion de la règle, elle correspond à l'une des C classes -CF k ij est le degré de certitude de la règle, il traduit sa validité.
La conclusion et le degré de certitude de chaque règle sont déterminés comme suit :
1. Pour chaque classe y t , calculer la somme des compatibilités des exemples d'apprentissage appartenant à cette classe, par rapport à la prémisse de la règle :
Trouver la classe y a qui a la plus grande valeur de compatibilité
3. Déterminer le degré de certitude CF
Dans les travaux portant sur la construction de règles de classification floues, les attributs ne sont pas nécessairement partitionnés en un même nombre de sous-ensembles flous. Plusieurs types de grilles floues ont été étudiés comme par exemple la grille floue rectangulaire .
Phase de classification
Dans cette phase, le système décide, à partir de la base de règles générées, notée S R , de la classe y a à associer à un individu E = (X 1 , X
2 ) de classe inconnue. 1. Pour chaque classe y t ; t = 1, 2, . . . , C, calculer ? yt par :
2. Trouver la classe y a qui maximise ? yt :
FIG. 1 -Grille floue simple.
Regroupement d'attributs
L'approche de regroupement d'attributs se base sur le concept des ensembles d'apprentissage artificiel, qui repose sur la combinaison des décisions de plusieurs apprenants pour améliorer l'exécution du système global (Valentini et Masulli, 2002). L'idée de ce concept est de répartir l'information -qui peut correspondre aux exemples d'apprentissage, aux attributs descriptifs ou encore aux classes -entre plusieurs apprenants, chaque apprenant réalise la phase d'apprentissage sur l'information qui lui a été fournie, et les opinions "individuelles" des différents apprenants sont ensuite combinées pour atteindre une décision finale. Dans notre cas, l'information à répartir correspond aux attributs descriptifs : chaque classifieur utilise un sous-ensemble des attributs initiaux et construit une base de règles locale, puis les différentes bases locales obtenues sont combinées pour former le modèle final (voir Fig. 2).
Cette approche, vérifiée expérimentalement dans (Soua et al., 2012) et (Borgi, 1999), permet de garantir une réduction conséquente du nombre de règles sans trop altérer les taux de bonnes classifications. Pour un problème de n attributs et k sous ensembles flous pour chaque attribut, le nombre de règles générés par les SIF, noté N R SIF , vaut k n . Lorsqu'on découpe le problème d'apprentissage en g sous-problèmes et on applique sur chacun d'eux la même démarche de génération de règles que les SIF, on obtient un nombre de règles N R regrp égal à :
où n i est le nombre d'attributs liés dans le i ème groupe g i . Il a été démontré dans (Soua et al., 2012) que si les groupes d'attributs, issus de l'approche de regroupement, forment une partition de l'ensemble des attributs de départ (
Par conséquent :
FIG. 2 -Approche de génération de règles par regroupement des attributs (Soua et al., 2012).
Approche proposée : regroupement des attributs par RA
Notre contribution réside au niveau de la méthode de regroupement d'attributs ; nous proposons une nouvelle méthode n'utilisant pas la recherche de corrélation linéaire, mais qui se base sur le concept des règles d'association (Agrawal et al., 1993). Les algorithmes d'extraction des RA déterminent les associations intéressantes entre les attributs en analysant leurs apparitions simultanées dans les enregistrements de la base de données. Cette méthode peut être très intéressante pour les bases de données pour lesquelles il n'existe aucune relation de type corrélation linéaire entre les attributs.
Généralement, les algorithmes d'extraction des RA déterminent les associations entre des variables de type booléen. Comme les données traitées dans les SIF sont quantitatives, il est nécessaire de commencer par les transformer en des valeurs booléennes, puis d'appliquer le concept des RA sur ces valeurs. Ensuite, à partir des associations trouvées entre ces valeurs booléennes, nous déduisons les associations entre les attributs de départ. Enfin et dans le but de garantir une réduction du nombre de règles, nous nous proposons de filtrer les groupes d'attributs associés de manière à obtenir une partition de l'ensemble des attributs de départ. Nous décrivons dans ce qui suit ces différentes étapes.
Génération des itemsets fréquents : liaisons locales entre attributs
L'existence d'une liaison entre deux variables dépend de la réponse à la question : est-ce que la connaissance des valeurs de l'une permet de prédire les valeurs de l'autre ? Le concept des RA répond à cette question en associant les valeurs qui apparaissent souvent ensemble dans les transactions de la base de données considérée.
Les RA ont été introduites par Agrawal et al. (1993)  -la génération des itemsets fréquents (tous les itemsets ayant un support supérieur à un seuil prédéfini minSupp). -la génération des règles d'association à partir de ces itemsets fréquents ; une RA doit avoir une confiance supérieure à un seuil prédéfini par l'utilisateur minConf .
Dans ce travail, nous nous intéressons au premier sous-problème et nous cherchons à déter-miner les groupes d'attributs liés. Pour déterminer les itemsets fréquents, plusieurs algorithmes ont été proposés (Agrawal et al., 1993)  (Agrawal et Srikant, 1994)  (Savasere et al., 1995). L'algorithme Apriori proposé dans (Agrawal et Srikant, 1994) est le plus connu et il est largement utilisé mais il ne traite que des données booléennes. Dans les problèmes courants, la majorité des données sont quantitatives et qualitatives et nécessitent des algorithmes applicables à ce type de données. Une extension de Apriori a été proposée par Srikant et Agrawal (1996) ; ils ont proposé de faire une correspondance entre des variables quantitatives ou qualitatives et des variables booléennes par le codage disjonctif complet. Pour une variable qualitative, chaque catégorie correspond à un élément booléen. Pour une variable quantitative, on discrétise l'attribut en des intervalles, puis on fait correspondre une variable booléenne à chaque intervalle.
Dans notre cas, les attributs étant continus, nous recourons au codage disjonctif complet des attributs. Le partitionnement des attributs se fait par une discrétisation régulière à intervalles égaux. Nous obtenons donc des intervalles que nous assimilons à des valeurs booléennes. Nous appliquons ensuite l'algorithme Apriori sur ces intervalles et obtenons ainsi des itemsets fré-quents ou des groupes d'intervalles liés.
Détermination des attributs liés : liaisons globales entre attributs
Dans l'étape précédente, nous avons déterminé les groupes d'intervalles liés. Notre but étant de faire un regroupement des attributs et non pas de leurs intervalles, on se propose de développer une procédure qui permet de déterminer la liaison entre un groupe d'attributs à partir des liaisons trouvées entre leurs intervalles.
Nous définissons pour cela une grille d'association qui représente les associations entre les valeurs (intervalles) d'un groupe d'attributs. Chaque axe de la grille concerne un attribut. La Fig. 3 présente 3 exemples de grille avec deux attributs X 1 et X 2 ; X 1 est décomposé en 6 valeurs (val de valeurs des attributs X 1 et X 2 . Quand deux valeurs forment un itemset fréquent, la case correspondante à leur intersection est grisée : on appelle cette case une région liée.
La liaison entre deux valeurs de deux attributs n'entraîne pas forcément la liaison entre les deux attributs puisque d'une part, ces attributs peuvent avoir très peu de régions liées (exemple (2) de la Fig. 3) et d'autre part, le nombre de données dans ces régions peut être très faible par rapport au nombre total de données (exemple (3) de la Fig. 3).
Dire que, plus le nombre de régions liées est grand, plus l'association entre les attributs est forte, n'est pas toujours suffisant. En effet, le principe de RA détermine si une région est liée en analysant son support, et ce dernier reflète la densité de données, c.à.d. la fréquence d'apparition des données dans la région. Ainsi, une seule région liée peut entraîner une association plus significative que plusieurs régions liées si cette unique région a une densité plus importante que la densité totale de l'ensemble des autres régions liées. Nous proposons donc de prendre en compte aussi bien le nombre de régions liées que leurs densités. Pour cela, nous commençons par définir le poids d'une région, appelé aussi coefficient de pondération. Ce poids caractérise la densité de données dans cette région, ce qui revient à son support.
Pour les valeurs respectives val Nous nous inspirons ensuite du principe des RA généralisées où une taxonomie (Fig. 4) existe entre les variables. D'après (Srikant et Agrawal, 1995), les associations trouvées à un niveau donné peuvent remonter au niveau supérieur en sommant leurs supports, à condition qu'il n'y ait pas de recouvrement. Avec l'exemple de la Fig. 4, si les itemsets (Veste, Botte) et (Veste, Espadrille) sont extraits, alors il n'est pas possible de les généraliser à l'itemset de niveau supérieur (Vêtements, Chaussure) en sommant leurs supports, car Veste, Botte et Espadrille peuvent figurer dans une même transaction. Dans notre cas, les variables quantitatives sont partitionnées en des valeurs sous forme d'intervalles ; la présence de deux valeurs d'un seul attribut n'est donc pas possible dans le même enregistrement. En formant une taxonomie entre un attribut et ses intervalles (Fig. 5), et comme il n'y a pas de recouvrement entre les FIG. 4 -Exemple de taxonomie pris de Srikant et Agrawal (1995).
FIG. 5 -Taxonomie entre un attribut et ses valeurs.
intervalles, on peut calculer le degré d'association des attributs comme la somme des supports de leurs valeurs (intervalles) liées. En utilisant ce principe, et en ne comptabilisant que les régions liées, nous définissons un degré d'association entre deux attributs ou plus, par la somme des coefficients de pondération de leurs régions liées. Donc, pour deux attributs X 1 et X 2 , le degré d'association ? s'écrit : Dans le cas général d'un ensemble d'attributs X = {X n1 , X n2 , . . . , X n l }, le degré d'association de ces l attributs est :
-r i1i2...i l X est la région formée par les intervalles val
. . , k l sont respectivement les tailles des partitions de X n1 , X n2 , . . . , X n l .
Le degré ? est compris entre 0 et 1, on peut alors définir un seuil d'association ? min au delà duquel on considère que les attributs de l'ensemble X sont liés.
Choix des groupes d'attributs associés
La procédure présentée dans 4.1 et 4.2 est basée sur le principe de l'algorithme Apriori ; elle fournit donc tous les groupes d'attributs associés de différentes tailles. Il est à noter que ces groupes ne constituent pas forcément une partition de l'ensemble des attributs de départ : on peut avoir des relations d'inclusion entre deux groupes d'attributs, ou une intersection non vide. Afin de garantir une réduction du nombre de règles générées, nous nous proposons de sélectionner un ensemble de groupe d'attributs de manière à former une partition de l'ensemble des attributs de départ (voir partie 3). La sélection se base sur les deux critères suivants : 
Expérimentation
Notre système, baptisé SIFRA, utilise l'approche de regroupement des attributs dans le cadre des SIF (Ishibuchi et al., 1992) comme cela est fait dans SIFCO (Soua et al., 2012) mais avec une nouvelle méthode de regroupement des attributs, celle que nous avons proposée et qui se base sur le concept des règles d'association. Après avoir déterminé les groupes d'attributs associés, nous utilisons la démarche proposée par Ishibuchi et al. (1992) pour générer les règles floues (partie 2.1), pour chaque groupe d'attributs. La classification d'un objet inconnu se fait par la méthode de classification de Ishibuchi et al. (1992) (partie 2.2).
Nous avons testé notre système SIFRA sur des bases de données qui diffèrent par le nombre d'attributs, le nombre d'exemples et le nombre de classes (Tab. 1). Pour évaluer la capacité de généralisation de notre méthode, nous avons adopté la technique de validation croisée d'ordre 10 ( Kohavi, 1995). Dans le tableau 2, nous présentons le taux de bonne classification suivi entre parenthèses du nombre de règles générées. Les meilleurs taux de classification sont présentés en gras. Le terme "imp" fait référence à l'impossibilité de générer les règles floues à cause du nombre de règles très élevé (supérieur à 10 5 ). Pour la phase de regroupement des attributs, nous avons utilisé une discrétisation à intervalles égaux et avons fixé le nombre d'intervalles à 3. D'autres tailles de discrétisation ainsi que d'autres méthodes de discrétisation pourront être étudiées dans de prochains travaux. Au niveau de la phase d'apprentissage, nous avons utilisé une partition floue homogène et une partition floue supervisée. Pour la partition homogène, nous avons testé plusieurs valeurs de la taille de partition k. Comme dans SIFCO, la valeur de k qui permet d'obtenir le meilleur taux de bonne classification dépend fortement des données de la base. Pour la partition floue supervisée, nous avons adopté la méthode MDLP de Fayyad et Irani (1993 Nous présentons dans Tab. 2 une comparaison de notre méthode SIFRA avec les deux méthodes SIF et SIFCO. Chacune des 3 méthodes possède des paramètres d'entrée à définir, à savoir la taille de la partition foue k, le seuil et la méthode de corrélation pour SIFCO, les seuils minSupp et ? min pour SIFRA. Pour comparer la performance des 3 méthodes et pour simplifier la lecture des résultats, nous présentons dans Tab. 2, pour chaque méthode, le meilleur taux de bonne classification obtenu en faisant varier ses paramètres d'entrée. D'après Tab. 2, il est clair que notre approche SIFRA fournit des taux de bonne classification très satisfaisants comparée à la méthode SIF, et des taux similaires ou meilleurs comparée à SIFCO. Comparée aux SIF, notre approche permet d'améliorer la performance de classification et de diminuer notablement le nombre de règles (en particulier avec les bases Wine, Vehicle et Sonar pour lesquelles la génération des règles avec SIF est impossible vu l'explosion de leur nombre). Comparée à SIFCO, notre méthode donne le meilleur taux de classification pour la base Iris avec un nombre de règles plus élevé mais qui reste faible (33). Pour Lupus, Wine et Sonar, les mêmes taux ont été obtenus par SIFRA et SIFCO. Concernant la base Vehicle, notre approche améliore considérablement le taux de bonne classification (67.73% contre 54.97% avec SIFCO) mais avec un nombre de règles plus important. Ce différentiel du nombre de règles s'explique par le fait que les groupes d'attributs liés détectés par SIFRA (basé sur les RA) contiennent plus d'attributs que les groupes détectés par SIFCO (basé sur une recherche de corrélation linéaire) (équation 7). Avec ces données, les liaisons entre attributs déterminées par notre approche semblent donc être plus pertinentes que celles trouvées avec SIFCO. 6 Conclusion

introduction
Les méthodes à noyau utilisées en analyse exploratoire des données (K-PCA, K-LDA, K-CCA, etc.) ou pour traiter des tâches de classification ou de régression (machines à support vectoriel, SVM) nécessitent, dans leurs fondements, l'usage de noyaux définis (positifs ou né-gatifs). Pourtant, bon nombre d'études relativement récentes en fouille de données temporelles présentent des résultats produits par de telles méthodes exploitant des noyaux temporellement élastiques (NTE) non définis Haasdonk (2005)  Zhang et al. (2010) ou régularisés par des mé-thodes spectrales Narita et al. (2007). L'émergence de nouvelles méthodes de régularisation pour NTE offre aujourd'hui des alternatives à l'exploitation des noyaux élastiques non définis que nous nous proposons d'évaluer de manière comparative sur des jeux de données simples mais potentiellement explicites. D'une manière générale, les procédures de régulation ont été développées pour approximer des noyaux non définis par des noyaux définis (ou semi-définis). Les premières approches appliquent directement des transformations spectrales aux matrices de Gram issues des noyaux non définis. Ces méthodes Wu et al. (2005)  Chen et al. (2009) consistent à i) changer le signe des valeurs propres négatives ou décaler ces valeurs propres en utilisant la valeur de décalage minimal nécessaire pour rendre le spectre des valeurs propres TAB. 1 -Liste des noyaux analysés positif, et ii) reconstruire la matrice de Gram issue du noyau avec les vecteurs propres d'origine afin de produire une matrice semi-définie positive. D'autres approches sont basées sur la recherche de la matrice de corrélation (matrice symétrique positive semi-définie ayant une diagonale unitaire) la plus proche de la matrice de Gram issue du noyau non défini, la proximité étant prise au sens d'une norme (norme de Frobenius pondérée) Higham (2002).
Cependant ces procédures de convexification sont difficiles à interpréter géométriquement Graepel et al. (1998) et l'effet attendu du noyau d'origine non défini peut être, selon les études, soit perdu ou pour le moins atténué par ces méthodes agissant directement sur le spectre matriciel, soit encore minime voir négatif comparativement à l'exploitation directe de la matrice non régularisée Chen et Ye (2008). Dans le contexte de l'alignement de séquences ou de séries temporelles, des approches de régularisation plus directes pour les NTE consistent à remplacer les opérateurs min ou max par un opérateur de sommation ( ) dans les équations récursives qui définissent les distances élastiques. Il en résulte qu'au lieu de ne considérer que le meilleur chemin d'alignement possible entre deux séries temporelles, le noyau régularisé effectue la somme des couts (ou gains) de tous les chemins d'alignement possibles avec un mécanisme de pondération qui cherche à favoriser les bons alignements et à pénaliser les mauvais alignements. Ces principes ont été appliqués avec succès par Saigo et al. (2004) pour la mesure (non définie) de Smith et Waterman (1981) très utilisée pour la comparaison de séquences symboliques, et plus récemment pour la speudo distance Dynamic Time Warping (DTW, Velichko et Zagoruyko (1970), Sakoe et Chiba (1971)) Cuturi et al. (2007), .
Nous développons dans cet article, sur la base d'une analyse en composantes principales à noyau (K-PCA), une étude expérimentale permettant d'évaluer les noyaux listés en table 1 sur de tâches de classification (supervisée et non-supervisée) de séries temporelles dans des sous espaces de dimension réduite. En nous limitant à des ensembles de séries temporelles de taille fixe, nous proposons ainsi de comparer expérimentalement au travers d'une analyse K-PCA un noyau Gaussien construit à partir de la distance Euclidienne (noyau défini positif, non temporellement élastique, ce noyau servant de base de référence), un noyau Gaussien construit à partir de la pseudo distance DTW (noyau non défini en général, mais élastique), une version régularisée du noyau précédent basée sur la recherche de la matrice de corrélation la plus proche Higham (2002), et enfin le noyau DTW régularisé suivant la méthode proposée par , K DT W , et une version normalisée, K Faisant suite aux travaux de Cuturi et al. (2007), la technique de régularisation développée dans  s'attache à transformer les équations récursives définissant la DTW (Dynamic Time Warping) de manière à produire une mesure de similarité notée K DT W constituant un noyau défini positif, c'est-à-dire s'apparentant à un produit scalaire dans un espace de Hilbert à noyau reproduisant. K DT W se distingue de l'approche proposée par Cuturi et al. en prenant la forme d'un noyau de convolution tel que défini par Haussler (1999) tout en imposant une condition sur les coûts locaux d'alignement moins restrictive. Pour rappel, un noyau défini sur R est est une fonction continue symétrique K :
Definition
La définition récursive du noyau K DT W est la suivante : • ? ? R + est un paramètre d'ajustement qui permet de pondérer les contributions locales, i.e. les distances entre les positions localement alignées, et
Ainsi, fondamentalement, l'opérateur min (ou max) est remplacé par un opérateur de sommation et une fonction de corridor symétrique (la fonction h dans l'équation récursive cidessus) est introduite pour, éventuellement, limiter la sommation et et donc la complexité algorithmique. Enfin, un nouveau terme récursif nécessaire à la régularisation (K xx ) est ajouté, de telle sorte que la preuve de la propriété de positivité du noyau peut être comprise comme une conséquence directe du théorème de convolution d ' Haussler (1999).
Normalisation
Le noyau K DT W effectue la somme sur l'ensemble des chemins d'alignement possibles des produits des termes locaux d'alignement e
Pour les séries temporelles de grandes tailles, ces produits deviennent infimes et K DT W incalculable lorsque ? est trop faible. Ainsi, le domaine de variation de K DT W s'amenuise en convergeant vers 0 lorsque ? tend vers 0 sauf lorsque l'on compare deux séries temporelles identiques (la matrice de Gram correspondante souffre ainsi d'une dominance diagonale). Comme proposé dans , une manière de palier ce problème consiste à considérer le noyau normalisé : . Si l'on oublie la constante de proportionnalité, cela revient simplement à élever le noyau
, ce qui montre que˜Kque˜ que˜K DT W est lui aussi défini positif (Berg et al. (1984), Proposition 2.7). Une correction de dominance diagonale similaire (sous-polynomial, i.e. t < 1) a initialement été proposée dans Schölkopf et al. (2002). L'effet de ce type de normalisation, sur le jeu de données SwedishLeaf (c.f. Table 2) est illustré en figure 1. La distribution des valeurs de la matrice de Gram associée au noyau non normalisé K DT W (évalué avec ? = 1) présente une très forte accumulation autour des valeurs très faibles (pour ? = 1, K DT W M in = 2.1e ? 77 et K DT W M ax = 1.9e ? 06 sur le jeu de données testé), tandis que la distribution des valeurs de la matrice de Gram associée au noyau normalisé˜Knormalisé˜ normalisé˜K DT W = K t DT W est plus diffuse. Les valeurs du noyau sont par ailleurs bornées :  
Complexité algorithmique
La définition récursive précédente permet de montrer que la complexité algorithmique liée au calcul du noyau K DT W est O(n 2 ), où n est la longueur des deux séries temporelles mises en correspondance, et lorsqu'aucun corridor n'est spécifié. Cette complexité est ramenée à O(c.n) quand un corridor symétrique de taille c est exploité par le biais de la fonction symmétrique h.
L'analyse en composantes principales non-linéaire, encore appelée ACP à noyau ou Kernel-PCA, Schölkopf et al. (1998) peut être vue comme une généralisation de l'ACP classique : elle permet d'engendrer une réduction de dimensionnalité non linéaire du point de vue de l'espace de représentation initial des données. Le principe consiste à projeter, par le biais d'une fonction non-linéaire ?(.), les données initiales dans un espace en général de plus haute dimension de sorte que l'image de la variété (non linéaire) contenant les données initiales devienne plus facilement linéairement séparable dans le nouvel espace, appelé espace des caractéristiques. Il suffit alors d'effectuer une ACP classique dans cet espace linéaire pour obtenir une réduction de dimensionalité non linéaire dans l'espace des données initiales. Si l'on exploite un noyau K(., .) défini positif, celui-ci induit de manière implicite une fonction nonlinéaire dite de mapping ?(.) telle que ?x, y, K(x, y) =< ?(x)
T , ?(y) >. Cette fonction ?(.) n'a pas besoin d'être connue explicitement (on évoque ici l'astuce du noyau).
Algorithm 1 ACP non linéaire 1: Choix du noyau (défini positif) k 2: Construction de la matrice de Gram à partir des données :
..,m 3: Centrage de la matrice de Gram (on retire la moyenne des données projetées dans l'espace des caractéristiques) : 
L'algorithme 1 présente succinctement les étapes de l'ACP non-linéaire, qui, à partir du choix d'un noyau défini positif, extrait les valeurs et vecteurs propres de la matrice de Gram centrée associée, puis projette toute donnée (initiale ou de test) dans un espace des caracté-ristiques de dimension réduite (d). Il est clair que l'ACP non-linéaire nécessite que le noyau utilisé soit défini positif.  Keogh et al. (2006). Ils sont de tailles modestes pour permettre (éventuellement) une visualisation la plus explicite possible en faible dimension. Le nombre de catégories varie de 2 à 50 et la longueurs des séries varie de 60 à 463.
Pour les 13 jeux de données listés en Table 2, et les 5 noyaux listés en Table 1, une ACP non linéaire est pratiquée puis les données sont projetées dans l'espace des caracté-ristiques obtenu en faisant varier le nombre de vecteurs propres, c'est à dire la dimension de l'espace réduit. Par exemple, en figure 2 les projections des séries temporelles du jeu de données Gun_Point sont présentées dans un espace des caractéristiques de dimension 3 pour les noyaux Gaussien-Euclidien, Gaussien-DTW régularisé par matrice de corrélation la plus proche (MCPP), K DT W et K t DT W . La valeur du paramètre t, exposant du noyau K t DT W , est estimé directement à partir des données d'apprentissage en évaluant les valeurs extrêmes prises par le noyau K DT W non normalisé. A titre d'exemple, on considère le jeu de données Gun_Point, pour lequel a matrice de Gram évaluée sur le noyau Gaussien DTW n'est pas dé-finie. Comme le montre la figure 2, les projections dans le sous-espace des caractéristiques de dimension 3 sont très proches pour le noyau Gaussien DTW et sa version régularisée par matrice de corrélation la plus proche. La séparation des classes est, sur cet exemple, bien meilleure pour le noyau KDT W et sa version normalisée KDT W t . A l'issue de l'ACP non linéaire, nous proposons une expérience de classification supervisée basée sur la règle du plus proche voisin (1-PPV) et une expérience de clustering basée sur l'algorithme des K-moyennes (K correspondant ici au nombre effectif de catégories du jeux de données). Pour chaque jeux de données et pour chaque noyaux testés la classification supervisée et non supervisée sont effectuées dans le sous espace des caractéristiques défini par K-PCA en faisant varier la dimension du sous-espace de 1 à 20 (pour dix dimensions, cela correspond à une réduction dimensionnelle variant de 83% à 98% pour les jeux de données testés).
La qualité de la classification est ensuite évaluée à partir du taux d'erreur de classification estimé à partir d'une validation croisée en 5 sous-échantillons. La précision et l'information mutuelle normalisée (IM N ) sont utilisées pour évaluer la qualité d'une classification non supervisée sur des données La précision est définie comme la fraction des individus correctement étiquetés, étant donné une correspondance 1-vers-1 entre les vraies classes et les clusters dé-couverts. Si p dénote une permutation quelconque des indices des clusters {˜c{˜c i } proposés (ou des vraies classes {c j }), la précision est alors définie par : L'information mutuelle normalisée, IM N , entre la vraie classification C et celle prédite˜Cprédite˜ prédite˜C est définie par :  Par ailleurs, le noyau Gaussien-DTW et sa variante régularisée à partir de la matrice de corré-lation la plus proche conduisent à des résultats très similaires et sensiblement moins bons comparativement aux noyaux KDT W et KDT W t . La régularisation par matrice de corrélation la plus proche ne semble donc pas apporter de bénéfice significatif en terme de classification supervisée ou non supervisée sur ces jeux de données par rapport au noyau DTW non défini. Il obtientles taux d'erreur de classification les plus faibles sur 10 des 13 jeux de données (CBF est mieux classé par le noyau Gaussien-DTW, ECG200 par le noyau Gaussien-Euclidien et Lighting2 par le noyau Gaussien-DTW régularisé par matrice de corrélation la plus proche). Pour la classification non supervisée, K t DT W obtient également les meilleurs résultats d'après les mesures Précision et IMN pour 8 des 13 jeux de données. Pour cette tâche, K DT W est meilleur sur les jeux de données yoga et Gun_Point, tandis que le noyau Gaussien-Euclidien se distingue sur ECG200 et les noyaux Gaussien-DTW sur Lighting2. Sur les jeux de données pour lesquels K t DT W n'arrive pas en tête, ce noyau se positionne entre le noyau Gaussien-Euclidien et les noyaux Gaussien-DTW. Contrairement à la régularisation par matrice de corrélation la plus proche, le principe de régularisation mise en oeuvre dans K DT W et K t DT W modifie en profondeur la nature même de la fonction de similarité sous-jacente à la mesure DTW en apportant en général une meilleure capacité à séparer ou regrouper les séries temporelles dans des espaces de dimension réduite.
Résultats et analyse
Conclusion
Nous avons évalué expérimentalement la capacité de quelques noyaux (élastiques, nonélastiques, définis, non-définis) à classer des séries temporelles de manière supervisée ou non dans des espaces de caractéristiques à dimension réduite obtenus par ACP non linéaire. Les résultats présentés montrent que les approches récentes de régularisation de noyaux élastiques offrent des alternatives bien meilleures que les principes classiques de régularisation basés sur des approches spectrales portant directement sur les valeurs propres des matrices de Gram construites à partir des noyaux non définis. Le noyau DTW régularisé K DT W exploité dans cet article dans sa version normalisé K t DT W offre un bon compromis entre les noyaux définis non-élastiques (tel que le noyau Gaussien-Euclidien) et les noyaux non définis élastiques (tel que le noyau Gaussien-DTW). Non seulement celui-ci conserve une caractéristique d'élasti-cité temporelle tout en étant défini positif, mais il se marie également bien avec les approches à noyau tel que l'ACP non linéaire. Sa capacité à proposer des espaces de caractéristiques discriminantes en dimension réduite en font un outil en général efficace pour l'analyse exploratoire d'ensembles de séries temporelles. Ces résultats confirment et complètent ainsi ceux présentés par  et  dans le cadre d'une classification supervisée par machine à support vectoriel en apportant un éclairage sur la normalisation de ce type de noyau.

Introduction
À cause de l'explosion du nombre d'informations stockées et partagées sur Internet, et l'introduction de nouvelles technologies pour capturer ces données, l'analyse des données incertaines est devenue essentielle dans de nombreuses applications pour la prise de décision. Pour gérer et traiter l'incertitude des données, plusieurs modèles ont été proposés, ce qui a donné naissance à différents types de bases de données imparfaites. Nous pouvons citer les plus connues : les bases de données probabilistes présentées par Dalvi et Suciu (2007); Aggarwal et Yu (2009), les bases de données possibilistes introduites par Bosc et Pivert (2010) et les bases de données évidentielles basées sur la théorie de Dempster-Shafer proposées par Bell et al. (1996). L'utilisation des bases de données évidentielles offre plusieurs avantages à savoir : (i) Elles permettent de modéliser l'incertitude et l'imprécision des données ; et (ii) cela représente une généralisation des deux modèles ; probabiliste et possibiliste à la fois. Dans cet article, nous nous nous intéressons aux requêtes Skyline sur des données incertaines où l'incertitude est modélisée par la théorie de l'évidence, ce qui constitue un travail pionnier.
Le reste cet article est organisé comme suit. La section 2 contient un rappel sur l'opérateur Skyline, les notions de base de la théorie de l'évidence et les bases de données évidentielles. Dans la section 3, nous définissons formellement la relation de dominance et modélisons le Skyline évidentiel. Nos expérimentations sont données dans la section 4. Enfin, la section 5 conclut l'article.
Notions de base
Dans cette section, nous présentons d'abord les requêtes Skyline sur des données classiques Borzsonyi et al. (2001). Ensuite, nous présentons les notions de base de la théorie de l'évidence et les bases de données évidentielles.
Les requêtes Skyline
Pour simplifier, nous supposons que la valeur la plus élevée, est la plus préférée.  
Définition 1 (Relation de Dominance) Étant donnés deux objets
La théorie de l'évidence
La théorie de l'évidence a été introduite par Shafer (1976) dont le but est d'évaluer subjectivement la vérité d'une proposition. Cette théorie, aussi connue sous le nom de "théorie des fonctions de croyance" est une généralisation de la théorie bayésienne Dempster (1968). Elle représente un ensemble d'hypothèses désigné par le cadre de discernement.  
Les bases de données évidentielles (BDE)
Les BDE permettent de représenter les données manquantes, incertaines ou imprécises. 
Les requêtes Skyline en présence de données évidentielles
Dans cette section, nous introduisons la relation de dominance entre les objets dont l'incertitude est modélisée par la théorie des fonctions de croyance, par la suite, nous présentons la définition du Skyline évidentiel. Étant donné un ensemble d'objets O = {O 1 , O 2 , . . . , O n } défini sur un ensemble d'attributs A = {a 1 , a 2 , . . . , a d }, avec o i .a k représente l'ensemble des éléments focaux de l'objet o i et l'attribut a k ; par exemple 1 , o 1 .wl = { 16, 18}, 0.1 20}, 0.9 et o 1 .r = { 0.5 100}, 0.5 Le degré de croyance qu'une valeur incertaine de l'objet o i définie sur l'attribut a k est préférée ou égale à une autre valeur de l'objet o j , est donné par Bell et al. (1996) :
Dans notre exemple, nous avons bel(o 1 .wl ? o 3 .wl) = 0.3 · (0.1 + 0.9) + 0.7 · (0.1 + 0.9) = 1, et bel(o 1 .r ? o 3 .r) = 0.7 · 0.7 + 0.3 · 0.7 = 0.7.
Étant donnés deux objets
Le Tableau 2 présente les degrés de croyance que chaque objet en ligne, domine un autre objet en colonne.
Les objets o i et o j comparés sont supposés différents, ce qui fait que la relation ? suffit pour exprimer que o i domine o j (au sens de (2)). Remarquons que cette définition se réduit à la 
Par exemple, o 1 0.9-domine o 2 et o 4 . Mais, il ne 0.9-domine pas o 4 car bel(o 1 ? o 4 ) = 0.3 < 0.9. Intuitivement, un objet est dans le Skyline s'il n'est pas dominé par rapport à un certain seuil b. 
Définition 7 (b-dominant skyline) Le skyline de O, désigné par b-Sky O , comprend les objets dans O qui ne sont pas b-dominés par aucun autre objet, i.e., b-Sky
Théorème 1 Étant donnés deux seuils de croyance
Preuve 1 Supposons qu'il existe un objet
Le théorème 1 indique que la taille de b-dominant skyline est plus petite que celle de b ? -dominant skyline si b < b ? . Les utilisateurs ont donc la possibilité de contrôler la taille des objets que contient le Skyline évidentiel en faisant varier le seuil de croyance. obtenus. Dans chaque expérimentation., nous varions un seul paramètre, tandis que les autres paramètres prennent leurs valeurs par défaut. Le Tableau 3 montre ces paramètres et leurs symboles ; les valeurs par défaut sont en gras. La figure 1.a montre que la taille du skyline évidentiel augmente avec l'augmentation de n. Dans la figure 1.b, on montre que la taille du skyline évidentiel augmente aussi de façon significative avec l'augmentation de d. 
Étude Expérimentale
Conclusion
Dans cet article, nous avons abordé le problème des requêtes skyline dans le cadre des bases de données évidentielles et nous avons introduit un nouveau type de skyline. Notre étude expérimentale a démontré la faisabilité et la flexibilité du skyline évidentiel. Comme perspective, nous envisageons de développer des techniques de classement des objets retournés par l'opérateur skyline évidentiel.

Introduction
L'analyse en composantes principales (ACP, Jolliffe (1986)) est une des méthodes, si ce n'est la méthode, d'analyse exploratoire les plus couramment utilisées. Elle a été ré-interprétée sous un formalisme probabiliste par Tipping et Bishop (1999), montrant que les composantes principales pouvaient être estimées par maximum de vraisemblance dans le cadre d'un modèle à variables latentes. Avec l'avénement des données de grande dimension, la problématique consistant à sélectionner un petit nombre de variables d'intérêt parmi l'ensemble des variables disponibles est devenue primordiale. Un des soucis majeurs de l'ACP dans cette optique est que les composantes principales sont définies comme une combinaison linéaire de l'ensemble des variables initiales. Des versions parcimonieuses de l'ACP (Zou et al., 2004) ainsi que de sa version probabiliste (Guan et Dy, 2009) ont été proposées récemment. La version parcimonieuse de Zou et al. (2004) repose sur l'ajout d'une pénalisation de type 1 au problème des moindres carrés, qui nécessite le choix du coefficient de pénalisation de façon heuristique. Dans Guan et Dy (2009), une version sparse bayésienne de l'ACP probabiliste est proposée. Nous proposons dans ce travail une alternative fréquentiste utilisant un algorithme EM pour l'inférence. La procédure d'estimation obtenue à l'avantage d'être particulièrement simple, et ne nécessite pas le choix de loi a priori. Elle offre en outre la possibilité de considérer le problème du choix de la pénalité comme un problème de choix de modèles.
Analyse en composantes principales probabiliste
Soit y un vecteur aléatoire observé de dimension p, et x un vecteur aléatoire latent (non observé), de dimension d, relié à y par l'équation suivante :
où W est une matrice p × d, µ est le vecteur moyenne (supposé nul dans la suite, µ = 0), et ? N (0, ? 2 I). Conditionnellement au vecteur x, la distribution des vecteurs observés est :
En supposant x ? N (0, I), la distribution marginale du vecteur observé est :
Ce modèle est un modèle de type "factor analysis" (Bartholomew et al., 2011), popularisé par Tipping et Bishop (1999) sous le nom d'analyse en composantes principales probabiliste (Probabilistic Principal Component Analysis (PPCA)). En effet, une estimation par maximum de vraisemblance des paramètres du modèle à l'aide d'un algorithme EM (Dempster et al., 1977), considérant le vecteur latent x comme manquant, conduit à estimer les colonnes de W par les vecteurs propres de la matrice de covariance empirique, vecteurs qui ne sont rien d'autres que les axes principaux classiques. Soit y 1 , . . . , y n un échantillon i.i.d de vecteurs observés. L'algorithme EM consiste à maximiser de façon itérative la log-vraisemblance complétée par les données non observées x 1 , . . . , x n :
3 Une version parcimonieuse de l'analyse en composantes principales probabiliste
Dans ce travail, nous considérons une version parcimonieuse de l'analyse en composantes principales probabiliste. L'objectif est d'obtenir des axes principaux déterminés uniquement grâce à un nombre restreint de variables initiales, et ainsi faciliter leur interprétation. De plus, comme nous le verrons par la suite, l'approche probabiliste de l'ACP permet de sélectionner le paramètre de pénalité par des méthodes classiques de sélection de modèles.
Dans l'optique d'introduire de la parcimonie au sein de axes principaux, nous considérons une pénalité 1 sur les colonnes de la matrice W. La vraisemblance complétée à maximiser est alors la suivante :
où w = (w 1 , . . . , w pp ) t est la colonne de W et ? > 0 est le paramètre de pénalisation. L'algorithme EM est un algorithme itératif qui alterne deux étapes (E et M), décrites ci-après.
La q-ème itération de l'étape E consiste à calculer l'espérance de pen c sous la loi p(x|y, ? (q) ), où ? (q) = (W (q) , ? 2(q) ) est la valeur courante de l'estimation des paramètres du modèles :
est une constante indépendante des paramètres du modèle et où
De sorte à faciliter la maximisation, nous considérons l'approximation de la norme 1 par la forme quadratique suivante (Fan et Li, 2001) :
j w j . La maximisation de Q(?, ? (q) ) en fonction de W n'ayant pas de solution analytique, nous utilisons une approche alternative consistant à maximiser Q(?, ? (q) ) en fonction de chaque éléments de la matrice W successivement. On obtient alors, en dérivant Q(?, ? (q) ) par rapport à w j et en égalant à 0 :
Cette dérivation élément par élément ne conduit pas nécessairement au maximum de Q(?, ? (q) ), mais suffit pour faire augmenter la log-vraisemblance à chaque étape de l'algorithme. On obtient alors un algorithme GEM (Generalized EM) qui conserve les mêmes propriétés de convergence qu'un algorithme EM classique. L'estimateur de la variance résiduelle est quant à lui :
et s'avère être identique à celui de la version non sparse de l'analyse en composantes principales probabiliste (Tipping et Bishop, 1999).
4 Sélection de ? par l'heuristique de pente Dans Zou et al. (2004), le choix de la pénalité ? est réalisé de manière heuristique en se basant sur l'éboulis des valeurs propres. L'idée de la stratégie que nous proposons est d'estimer le modèle sur une grille de valeurs de ?, et d'utiliser un outil de sélection de modèles pour choisir le meilleur modèle. Les outils classiques de sélection de modèles sont par exemple les critères AIC (Akaike, 1974) et BIC (Schwarz, 1978), qui pénalisent la log-vraisemblancêvraisemblancê ?) de la façon suivante :
, où ? est le nombre de paramètres libres du modèles et n le nombre d'observations. La valeur de ? dépend directement de la valeur de ? puisqu'elle est égale au nombre d'éléments non nuls dans W plus un (pour la variance résiduelle). Même si ces critères sont largement utilisés et asymptotiquement consistants, ils sont aussi connus pour être plus efficaces sur simulations que sur données réelles.
Pour surmonter ce problème, Birgé et Massart (2007) ont récemment proposé une mé-thode dirigée par les données pour calibrer la pénalité des critères pénalisés, connue sous le nom d'heuristique de pente. L'heuristique de pente a été proposé initialement dans un cadre d'un modèle de régression gaussien homoscédastique, mais a ensuite été étendue à d'autres situations. Birgé et Massart (2007) ont démontré qu'il existait une pénalité minimale et que de considérer une pénalité égale au double de la pénalité minimale permettait d'approcher le modèle oracle en terme de risque. La pénalité minimale est en pratique estimée par la pente de la partie linéaire de la log-vraisemblance pen c ( ˆ ?) exprimée en fonction de la complexité du modèle. Le critère associé est alors défini par :
oùˆsoùˆ oùˆs est l'estimation de la pente de la partie linéaire de pen c ( ˆ ?). Une revue détaillée et des conseils d'implémentation sont donnés dans Baudry et al. (2012).
Illustrations numériques
Nous choisissons pour illustrer notre méthodologie un jeu de données classique issu de l'UCI machine learning repository : le jeux de données USPS. Le jeu original contient 7291 images représentant des chiffres manuscrits de 0 à 9. Chaque chiffre est une image en niveaux de gris de taille 16 × 16, représentée par un vecteur de dimension 256. Pour cette expérience, nous avons extrait un sous ensemble de 1756 images correspondant aux chiffres 3, 5 et 8. Nous réalisons sur ces données une ACP ainsi que l'ACP parcimonieuse que nous proposons. Pour cette dernière, nous fixons le nombre maximum d'itérations de l'algorithme EM à 500 et le seuil de convergence à 10
, et nous considérons une grille de valeurs de ? de 0 à 150, avec un pas de 1. La méthode de l'heuristique de pente (figure 1) conduit à choisir ? = 126. Dans un but illustratif, nous discutons ici les résultats concernant les deux premières composantes principales. La figure 2 représente la projection des 1756 images dans le premier plan principal de l'ACP ainsi que les deux premières composantes principales, tandis que la figure 3 propose la même représentation pour l'ACP parcimonieuse. Nous pouvons noter que les deux méthodes définissent un premier plan principal relativement discriminant vis-à-vis des trois types d'images. Tout l'intérêt de l'ACP parcimonieuse est que les composantes principales   

Introduction
Avec la récente explosion du nombre d'images et de données satellites disponibles, la conception de systèmes capables d'interpréter automatiquement de telles données est devenue un domaine florissant. En effet, les satellites modernes sont capables d'acquérir des images à très hautes résolutions (THR) avec une définition de plus en plus élevée sur un large domaine spectral. Or, les algorithmes capables de traiter un tel volume de données en un temps raisonnable sont pour le moment assez rares.
La segmentation de telles données d'imageries peut se faire en utilisant des algorithmes basés sur les champs de Markov, (Roth et Black, 2011). Les champs de Markov reposent sur la notion de voisinage pour modéliser les dépendances qui peuvent exister entre des données telles que des pixels adjacents, ou des super-pixels adjacents (groupes de pixels).
Dans ces modèle, on considère S = {s 1 , ..., s N }, s i ? 1..K un ensemble de variables aléatoires représentant les états (labels) des données. Ces états sont supposés liés dans l'espaces par des relations de voisinages et émettent des observations X = {x 1 , ..., x N } où les x i sont des vecteurs contenant les attributs de chaque donnée (RGB pour les modèles les plus simples). L'objectif est alors de déterminer la configuration idéale de S, c'est à dire de trouver les valeurs des s i afin d'obtenir une segmentation optimale.
Une des méthodes possibles pour résoudre ce problème est l'utilisation du couple ICM-EM, (Zhang et al., 2001). Le choix de l'algorithme ICM (Besag, 1986) vient du fait que cet algorithme a une complexité plus faible que celles des algorithmes plus récents utilisés pour les champs de Markov, ce qui est un atout non-négligeables pour traiter le volume important des données d'une image à très haute résolution. De plus, la plupart des données issues de telles images sont déjà pré-traitées et l'ICM est donc suffisant pour les segmenter. Il est également important de préciser qu'à ce jour, l'algorithme ICM est le seul à avoir été adapté pour pouvoir optimiser le modèle d'énergie contenant des informations sémantiques utiles dans le cadre des image satellites que nous avons proposé dans de précédent travaux (Sublime et al., 2014). L'objectif de l'algorithme ICM est d'optimiser itérativement une fonction d'énergie locale dérivant du logarithme de P (x|s, ? s )P (s). Bien que cet algorithme ait montré son efficacité pour résoudre ce problème, il a cependant plusieurs défauts tels que son critère d'arrêt basé sur l'évolution de l'énergie global et l'absence de garantie de convergence. En effet, cet algorithme essaye d'optimiser une fonction non-convexe, et il a été montré qu'après un processus d'optimisation relativement rapide de forme parabolique, l'algorithme ne se stabilisait pas toujours et pouvait même se mettre à diverger entrainant ainsi une détérioration des résultats (Zhang, 1989). Une des difficultés récurrente de la segmentation d'image dans le cadre non-supervisé est qu'il est difficile d'évaluer la qualité des résultats. Dans le cas de l'ICM, le critère d'arrêt repose sur l'énergie globale liée aux champs de Markov (somme des énergies locales) et sur l'hypothèse que cette énergie va se stabiliser. Le problème d'une telle approche est précisément que cette énergie globale ne se stabilise pas toujours, l'algorithme n'ayant pas de garantie de convergence, et que même dans le cas où elle se stabilise cette stabilisation n'intervient pas né-cessairement au moment où les résultats de la segmentation sont les meilleurs. De plus, en se basant sur l'énergie globale avec un nombre de données élevé, il y a un risque non négligeable d'"overflow" ou d'arrondi de cette énergie.
Dans cette article, nous proposons un nouveau critère d'arrêt pouvant être facilement calculé et qui repose sur un modèle d'énergie adapté aux images satellites à très haute résolution.
Algorithme proposé
On note V x le voisinage d'une observation x, et A = {a i,j } K×K la matrice de voisinage de l'image, où chaque a i,j est la probabilité de passer de l'état i à l'état j entre deux données voisines. A partir de ces notations nous utilisons comme modèle d'énergie locale le modèle défini lors de nos précédents travaux (Sublime et al., 2014) :
La fonction d'énergie décrite précédemment utilise une énergie locale dérivant de la loi normale dans laquelle µ s est la valeur moyenne associée à l'état s et ? s sa matrice de covariance. Le dernier membre de cette énergie qui décrit l'énergie de voisinage est une fonction positive calculée à partir des éléments de la matrice A. Le facteur ? x,v représente le poids de v en tant que voisin de la donnée x selon la proportion de frontière qu'il occupe. Ce modèle repose sur l'idée que des données voisines ayant des clusters différents ne sont pas nécessaire-ment incompatibles, et peut être vu comme une version relaxée du graph-cuts (Boykov et al., 2001).
La matrice de voisinage A de ce modèle d'énergie contient des informations sémantiques telles que les affinités des différents clusters ou leur compacité sur l'image. En effet, la diagonale de la matrice A contient la probabilité pour chaque cluster d'avoir un voisinage plus ou moins composé d'éléments du même cluster. Cette diagonale peut par conséquent être considérée comme un indicateur de compacité des clusters. Les éléments non-diagonaux quant à eux fournissent des informations sur les affinités des clusters.
Étant donné que l'objectif principal de l'utilisation des champs de Markov en segmentation d'image est d'obtenir des zones homogènes, nous avons décidé d'utiliser cette information de compacité contenue dans notre matrice de voisinage et d'en faire le nouveau critère d'arrêt de notre algorithme C-ICM : "Compactness-based Iterated Conditional Modes".
Il est en effet raisonnable de supposer qu'il faut arrêter l'algorithme lorsque la compacité des clusters cesse d'augmenter. Aussi, nous utilisons les variations de la trace de la matrice A comme nouveau critère d'arrêt. Ce critère présente plusieurs avantages : Les calculs sont faciles et plus rapides que ceux pour obtenir l'énergie globale, ce critère permet également de repérer les cas où un cluster commence à en absorber d'autres, ce qui arrive assez souvent avec l'ICM. Un tel cas de figure provoquerait rapidement la convergence de la trace de A vers 1. Dans l'algorithme (1), nous montrons comment nous avons adapté le framework EM-ICM pour notre critère d'arrêt : Comme on peut le voir sur la Figure (2), le critère d'arrêt basé sur l'énergie aurait conduit l'algorithme à s'arrêter après la 8ème itération. Or, sur la Figure (1) l'état de fusion des zones homogènes n'est pas encore assez avancé après l'itération 8 : La mer et une partie des bâtiments sont encore très fragmentés. Le critère de compacité aurait de son côté amené l'algorithme à s'arrêter après l'itération 43 ce qui aurait conduit à des zones nettement plus homogènes : routes, eau, gros bâtiments, etc.
On notera également que les deux critères se stabilisent définitivement après l'itération 47, itération après laquelle l'image n'évolue presque plus. Sur l'image prise après la 50ème itéra-tion de l'algorithme, on constate même le début assez marqué d'un phénomène de détérioration avec certains clusters qui ont commencé à déborder sur d'autres.
De cette première expérience, nous pouvons tirer les conclusions suivantes : Tout d'abord, elle confirme la difficulté évoquée dans notre introduction de trouver un critère d'arrêt idéal. En effet, le critère basé sur l'énergie aurait ici arrêté la segmentation trop tôt et l'énergie rebondit deux fois avant d'atteindre sa stabilisation finale. Ensuite, on voit que notre critère basé sur la compacité semble plus stable : il n'y a pas de rebond. Enfin, on notera que lorsque les deux critères semblent finalement se mettre d'accord pour arrêter la segmentation (itération 43 pour la compacité, et itération 47 pour la stabilisation définitive de l'énergie), on s'aperçoit que nous sommes déjà dangereusement proche de la zone à partir de laquelle la segmentation commence à se détériorer.
Données THR Strasbourg
Notre seconde expérience a été effectuée sur un jeu de données construit à partir d'une image satellite à très haute résolution de la ville de Strasbourg (Rougier et Puissant, 2014). Ce jeu de données pré-traitées utilise le modèle des super-pixels (agglomérats de pixels) avec des voisinages irréguliers : chaque super-pixel a entre 1 à 15 voisins. L'image est représentée sous forme de 187.058 super-pixels ayant chacun 27 attributs radio-métriques et géométriques. On constate à l'issue de cette expérience que notre indice de compacité tombe d'accord avec l'indice de qualité de clustering pour déterminer quand arrêter l'algorithme. On notera tout de même qu'il y a peu de différence en terme de qualité de résultats entre le moment d'arrêt décidé par le critère de compacité, et celui décidé par le critère classique d'énergie. Cependant, sur une image satellite à très haute résolution de cette taille, 2 itérations supplé-mentaires coûtent plusieurs minutes de calcul pour avoir dans le cas de cette expérience un résultat légèrement moins bon. Notre critère d'arrêt semble donc être ici un choix plus judicieux pour décider d'arrêter l'algorithme au bon moment et économiser du temps de calcul.
Conclusion
Dans cet article, nous avons proposé une amélioration de l'algorithme ICM pour la segmentation des images satellites à très haute résolution. Notre algorithme C-ICM introduit un nouveau critère d'arrêt basé sur un modèle d'énergie spécifique permettant d'avoir des informations sur les relations entre les différents clusters. Notre critère repose ainsi sur la compacité et l'homogénéité des clusters dans la segmentation plutôt que sur le traditionnel critère d'énergie globale. Nos expériences préliminaires ont montré des résultats intéressants qui pourraient mener à une amélioration globale de l'efficacité et à une vitesse accrue du traitement des

Introduction
Le modèle des graphes conceptuels (Sowa, 1984;Chein et Mugnier, 2009) permet de repré-senter des connaissances sous la forme d'un graphe étiqueté. Le modèle des graphes conceptuels utilise une représentation graphique visuelle des connaissances afin de faciliter la compré-hension pour les utilisateurs. La méthode d'interrogation du modèle est basée sur l'opération principale des graphes conceptuels, un homomorphisme de graphes appelé la projection : cette opération permet de déterminer si les connaissances exprimées dans un graphe conceptuel appelé graphe requête peuvent être déduites de celles exprimées dans la base de connaissances, représentée par un graphe conceptuel appelé graphe fait.
Les objectifs de ce modèle sont proches d'une partie de ceux des langages du Web sé-mantique tels que RDF, RDF-Schema ou OWL (Manola et al., 2004;Brickley et Guha, 2004;McGuinness et al., 2004) qui sont généralement interrogés en utilisant SPARQL (Garlik et al., 2013), une recommandation officielle du W3C disponible dans plusieurs outils. SPARQL offre plus de flexibilité par rapport aux graphes conceptuels dans l'interrogation d'une base de connaissances. D'une part, SPARQL permet d'exprimer une disjonction entre plusieurs parties d'une requête (SPARQL utilise le mot « union ») et d'identifier des parties comme obligatoires ou optionnelles. D'autre part, SPARQL permet d'interroger la base grâce à quatre types de requêtes : l'interrogation, la sélection, la description et la construction. La requête d'interrogation permet de savoir si la connaissance représentée par la requête est présente dans la base. La requête de sélection permet de trouver et extraire de la base des connaissances identifiées dans la requête comme importantes. La requête de description permet d'obtenir des informations sur des connaissances de la requête. La requête de construction permet de déduire de nouvelles connaissances à partir de celles contenues dans la base.
Cet article propose de combiner la simplicité de la représentation visuelle des graphes conceptuels avec la puissance du modèle d'interrogation du Web sémantique pour améliorer l'expression des requêtes des graphes conceptuels. La contribution de cet article est triple. D'abord, le graphe d'interrogation est introduit. La notion de graphe d'interrogation permet d'exprimer des conditions de disjonction -un « ou » entre deux de ses sous-graphes -et des conditions d'option -un sous-graphe est préféré mais non-nécessaire -dans un graphe conceptuel. Ensuite, le graphe d'interrogation nous permet de définir un langage d'interrogation pour les graphes conceptuels formé de quatre types de requêtes : requête d'interrogation, requête de sélection, requête de description et requête de construction. Une requête d'interrogation permet de savoir si le graphe de la requête est déductible du graphe fait. Une requête de sélection permet de trouver et extraire du graphe fait des sommets identifiés comme étant importants dans le graphe requête. Une requête de description permet d'obtenir des informations complémentaires du graphe fait liées à un sommet particulier du graphe requête. Une requête de construction permet de déduire de nouvelles connaissances en utilisant les connaissances extraites du graphe fait. Enfin, l'opération basique de calcul utilisée pour interroger et obtenir des réponses est introduite : la projection d'un graphe d'interrogation dans un graphe fait. Cette projection est définie en utilisant la projection classique d'un graphe conceptuel dans un graphe fait.
Aucun langage générique d'interrogation pour les graphes conceptuels n'a encore été proposé, mais différentes idées ont été mises en avant. De même que la requête de sélection, la possibilité d'identifier certains sommets dans un graphe requête pour facilement exploiter le résultat d'une projection a déjà été proposé dans Sowa (1984). Notre approche est différente de Sowa (1984) dans lequel les sommets sont simplement marqués puisque nous proposons de les nommer pour pouvoir facilement les identifier dans le résultat de la projection. Dans la communauté de SPARQL, Corby et Faron-Zucker (2007) propose une implémentation de la recherche de motifs de graphes de SPARQL en utilisant la projection classique des graphes conceptuels. Notre approche, à l'inverse, est de transposer les idées de SPARQL dans le modèle des graphes conceptuels.
L'article est organisé comme suit. La section 2 rappelle les bases du modèle des graphes conceptuels et de la projection. La section 3 présente le modèle des graphes d'interrogation. La section 4 présente la projection d'un graphe d'interrogation dans un graphe conceptuel fait ainsi que la requête d'interrogation, La section 5 présente la requête de sélection, La section 6 présente la requête de description, La section 7 présente la requête de construction. Enfin, la section 8 présente notre implémentation et donne quelques éléments pour comparer notre langage d'interrogation des graphes conceptuels avec SPARQL.
Modèle des graphes conceptuels
Le modèle des graphes conceptuels (Chein et Mugnier, 2009)  
Homme Femme aPourPère(Humain, Homme)
, ?) avec T C partiellement représenté par l'arbre de gauche, T R partiellement représenté par l'arbre de droite, et la signature ? de chaque relation est précisée à coté de chaque type de relation.
Exemple. La figure 1 montre un extrait du vocabulaire utilisé dans les exemples suivants.
Un graphe conceptuel est un multigraphe biparti défini sur un vocabulaire. Un des ensembles de sommets est appelé l'ensemble des sommets concepts, et l'autre ensemble est appelé l'ensemble des sommets relations, représentant les liens entre les concepts. Chaque sommet est étiqueté. Un sommet relation est étiqueté par un type de relations et un sommet concept est étiqueté par un couple formé d'un type de concepts et d'un marqueur. Si un concept c est le i-ème argument d'une relation r, alors il y a une arête entre c et r étiquetée par i, et le type de concepts de c doit respecter les contraintes de la signature de r. Un sommet concept individuel est référencé par un marqueur individuel de I. Un sommet concept générique est ré-férencé par le marqueur générique * . Nous considérons les graphes sous forme normale : deux sommets concepts différents ne peuvent pas être étiquetés par le même marqueur individuel.
Un graphe conceptuel défini sur V est un quadruplet G = (C, R, E, l) qui satisfait les conditions suivantes :
-(C, R, E) est un multigraphe fini, non-orienté et biparti. C est l'ensemble des sommets concepts, R est l'ensemble des sommets relations, E est l'ensemble des arêtes 
La projection est l'opération d'interrogation du modèle des graphes conceptuels. Soient un graphe requête G r et un graphe fait G f définis sur le même vocabulaire, G r se projette dans G f si les informations représentées par G r se déduisent de celles représentées par G f .
Les sommets de G f qui correspondent aux sommets de G r sont appelés les images des sommets de G r par ?.
Un bloc est défini comme un ensemble de sommets du graphe associé à un type de bloc. Tous les sommets concepts de l'ensemble doivent avoir leurs sommets relations voisins dans l'ensemble : ceci permet à un sommet concept du bloc d'être caractérisé par les relations auxquelles il est lié. Différents types de blocs permettent d'identifier la condition d'un bloc : l'Option (le bloc est dit optionnel), la Disjonction (le bloc est dit de disjonction) et le Standard (le bloc est dit standard).
Un bloc optionnel permet d'exprimer que la partie du graphe qu'il contient est facultative : on recherche dans le graphe fait un graphe avec la partie optionnelle, mais si elle n'est pas trouvée, un graphe privé de la partie optionnelle conviendra. Un bloc de disjonction est composé de blocs fils et exprime une disjonction entre ses blocs fils : on recherche dans le graphe fait un graphe avec seulement un des fils.
Un bloc du graphe G est un couple b = (S, T ) où -S ? C ? R est l'ensemble des sommets du bloc tel que :
?c ? S ? C, ?r ? R, si rc ? E alors r ? S -T ? {Standard, Option, Disjonction} est le type du bloc. Une arborescence est utilisée afin de structurer les blocs d'un graphe requête. Les blocs sont les sommets de l'arborescence. Les notions de bloc racine, bloc père et blocs fils sont définies grâce au vocabulaire lié aux arborescences. La structuration est hiérarchique, elle impose que la relation père-fils entre blocs vérifie l'inclusion des sommets du bloc fils dans ceux du bloc père et que si deux blocs ont un sommet en commun, alors un de ces blocs est un ancêtre de l'autre.
Standard Disjonction
Définition 5. Soit un graphe conceptuel G = (C, R, E, l).
Une structuration H de G est une arborescence (B, A, r) où la racine r est le bloc (C ? R, Standard), B étant l'ensemble des blocs de G qui forme l'ensemble des sommets de l'arborescence, et A l'ensemble des arcs. La structuration est telle que : H) est un couple où G est un graphe conceptuel et H est une structuration de G. 
Projection et requête d'interrogation
Les graphes d'interrogation sont utilisés comme base de construction pour chacun des quatre types de requêtes.
Un graphe d'interrogation peut être développé en un ensemble de graphes conceptuels en suivant les conditions d'option et de disjonction. Cet ensemble de graphes conceptuels, appelé l'ensemble des graphes développés du graphe d'interrogation, représente l'ensemble des graphes conceptuels dont on cherche à savoir s'ils sont déductibles du graphe fait. Cet ensemble permet de définir une projection d'un graphe d'interrogation dans un graphe fait comme étant une projection d'un graphe développé dans le graphe fait. Pour obtenir un graphe développé à partir d'un graphe d'interrogation, des opérations sont appliquées sur le graphe d'interrogation pour aboutir à un graphe ne contenant que des blocs standards. Les blocs optionnels seront retirés ou leur type sera changé en Standard. Le type des blocs de disjonction sera changé en Standard et seulement un et un seul de ses fils sera conservé.
Soit b ? B, un bloc optionnel. Le graphe G i dans lequel le type de b est changé en Standard est un développement
Le graphe G i dans lequel le type de b est changé en Standard et dans lequel tous les fils de b sauf un sont retirés est un développement de G i .
L'ensemble des graphes développés GD de G i est l'ensemble de tous les graphes conceptuels associés aux graphes d'interrogation ne contenant que des blocs standards obtenus grâce à une suite d'applications de l'opération de développement à partir de G i . Exemple. Les graphes conceptuels de la figure 4 forment l'ensemble des graphes développés
Pour définir la projection d'un graphe d'interrogation dans un graphe fait, on construit les projections des graphes développés dans le graphe fait. La projection d'un graphe développé est généralement une projection du graphe d'interrogation. L'exception est due à la sémantique de l'option, en effet, la partie optionnelle d'un graphe doit être présente dans la réponse si elle est présente dans le graphe fait : dans ce cas les projections qui ne contiendraient pas le bloc optionnel ne sont pas des projections du graphe d'interrogation. Notons que ces dernières projections sont prolongées 2 par la projection contenant les informations optionnelles. On définit donc l'ensemble des projections d'un graphe d'interrogation dans un graphe conceptuel comme l'ensemble des projections des graphes développés du graphe d'interrogation privé des projections prolongées.
L'ensemble des projections du graphe d'interrogation G i dans G f est : 
Requête de sélection
Dans une requête d'interrogation, l'image de certains sommets peut être plus importante que l'image des autres sommets de la requête. Cet article propose un type de requête dans lequel on peut identifier les sommets jugés importants et dont on veut retenir les images par la projection du graphe d'interrogation : la requête de sélection.
Une requête de sélection est donc un couple composé d'un graphe d'interrogation et d'une fonction de nommage qui associe à des noms à sélectionner, des sommets du graphe.
Définition 11. Soit un ensemble de noms N .
Une requête de sélection est un couple
Une réponse à une requête de sélection est une fonction qui associe à chaque nom de l'ensemble des noms à sélectionner de la requête, un sommet du graphe fait. Si le graphe d'interrogation se projette dans le graphe fait, les noms à sélectionner seront associés avec l'image dans le graphe fait, si elle existe, du sommet du graphe d'interrogation auxquels ils sont liés. Un sommet peut ne pas avoir d'image par la projection, dans ce cas, le nom n'est associé à aucun sommet. 
Une réponse à la requête de sélection R s est une fonction partielle de N dans C f ? R f associant à chaque élément n de N le sommet ?(select(n)) de G f s'il existe.
Exemple. La figure 7 montre une requête de sélection R s = [G i , select], où la fonction de nommage select, définie sur N = {?prénom, ?surnom}, est représentée sur G i . La requête permet de connaître le prénom, et s'il existe le surnom, de tous les humains du graphe fait. Les deux réponses à l'exécution de R s dans G f (figure 5) sont présentées en partie droite de la figure 7.
Requête de description
La requête de sélection permet d'obtenir les images des sommets qui intéressent l'utilisateur, la requête de description permet, elle, d'obtenir non seulement les images des sommets qui intéressent l'utilisateur, mais aussi leur description. La description d'un sommet est formée par les sommets voisins qui apportent des informations sur ce sommet. Une requête de description est un couple composé d'un graphe d'interrogation et d'une fonction de nommage qui à un nom de l'ensemble des noms à décrire, associe un sommet du graphe. Étant donné que seuls les concepts peuvent être décrits, l'ensemble à décrire ne doit contenir que des noms se référant à des sommets concepts.
Définition 13. Soit un ensemble de noms N .
Une requête de description est un couple
Une réponse à une requête de description est un sous-graphe du graphe fait dans lequel figurent les images des concepts à décrire, les relations qui sont liées à ces concepts par une arête étiquetée par « 1 », et les concepts liés à ces relations. Seules les relations reliées par une arête étiquetée par « 1 » sont sélectionnées puisque se sont les relations dont le concept à décrire est le sujet. 
Une réponse à la requête de description R d est un sous-graphe
Exemple. La figure 8 montre une requête de description R d = [G i , desc], où la fonction de nommage desc, définie sur N = {?père}, est représentée sur G i . La requête permet de décrire tous les humains qui sont pères. Les trois réponses à l'exécution de R d dans G f (figure 5) sont présentées en partie droite de la figure 8 : H2 et H3 sont pères, mais H2 est père à la fois de H1 et de F1.
Requête de construction
Une requête de construction est composée de deux graphes : un graphe d'interrogation, appelé graphe condition, et un graphe conceptuel, appelé graphe modèle,. Le principe d'une requête de construction est d'une part d'extraire des informations du graphe fait à l'aide du graphe condition, qui doit se projeter dans le graphe fait, et d'autre part d'exploiter ces informations afin de construire un nouveau graphe conceptuel, appelé graphe réponse, à partir du graphe modèle. Une requête de construction peut être vue comme une règle (Salvat, 1998) dont la partie condition est définie par le graphe condition et la partie conclusion est obtenue par le graphe modèle. Pour lier les sommets du graphe condition à ceux du graphe modèle, on utilise une fonction lien qui associe à un sommet du graphe condition, un sommet du graphe modèle. Un sommet du graphe condition qui est lié à un sommet du graphe modèle doit avoir la même étiquette que ce dernier. Notons qu'en utilisant l'extension des types conjonctifs (Chein et Mugnier, 2004), il serait possible que deux sommets liés ne possèdent pas la même étiquette.
est un graphe conceptuel appelé graphe modèle, tels que G et G m sont définis sur le même vocabulaire, lien est une fonction partielle définie comme suit : lien :
Une réponse à une requête de construction est un graphe réponse construit sur le modèle du graphe modèle comme suit. Les sommets du graphe modèle liés aux sommets du graphe condition sont étiquetés par les images des sommets du graphe condition dans le graphe fait. Les sommets qui ne sont pas liés dans le graphe modèle gardent leurs étiquettes. Il se peut que des sommets du graphe condition ne possèdent pas d'image par la projection, les sommets liés au graphe modèle sont alors retirés du graphe réponse, ainsi que toutes les relations éventuel-lement connectées à ces sommets. Ce cas se présente lorsqu'un sommet du graphe modèle est lié à un sommet du graphe condition qui appartient à un bloc option, ou de disjonction.
Définition 16. Soient un graphe conceptuel G f = (C f , R f , E f , l f ), une requête de construction R c = [G i , G m , lien] avec G i = (G, H) où G = (C, R, E, l) et avec G m = (C m , R m , E m , l m ), une projection ? de G i dans G f , et l'ensemble S ? des sommets de G qui ont une image dans G f par ?.
Une réponse à la requête de construction R c est un graphe conceptuel appelé graphe ré-ponse G r = (C r , R r , E r , l r ), tel que : -C r = C m \ {c = lien(c ) ? C m |c ? C, c / ? S ? } -R r = (R m \ {r = lien(r ) ? R m |r ? R, r / ? S ? }) \ {r ? R m |rc ? E m , c / ? C r } -E r = {nn ? E m |n, n ? C r ? R r } -l r est définie de la façon suivante ?s ? C r ? R r ? E r :
si ?s ? C ? R tel que s = lien(s ), l r (s) = l f (?(s )) sinon, l r (s) = l m (s)
Exemple. La figure 9 montre la requête R c = [G i , G m , lien] où la fonction lien est représentée directement sur le graphe via les pointillés. Cette requête peut s'apparenter à une règle qui dit que si un humain a pour parent un autre humain, alors ce dernier est un parent du premier, mais si cet humain n'a pas de parent, personne n'est son parent. Les quatre réponses à l'exécution de R c dans G f (figure 5) sont présentées en partie droite de la figure 9.
8 Conclusion

Introduction
Les réseaux sociaux sont l'objet d'une recherche intense depuis plusieurs années (Carrington et al., 2005;Newman et al., 2006;Scott et Carrington, 2011). Leur étude donne lieu à différentes questions concernant leur évolution, qu'il s'agisse d'analyser comment les interactions se sont mises en place, ou alors de comprendre l'état du système qu'ils décrivent. Parmi ces interrogations, l'étude des phénomènes de propagation dans les réseaux a suscité un intérêt soutenu au sein de la communauté, multipliant les domaines d'applications, allant de la sociologie (Granovetter, 1978;Macy, 1991) à l'épidémiologie (Hethcote, 2000;Dodds et Watts, 2005;Bertuzzo et al., 2010) en passant par la publicité virale et le placement de produits (Domingos et Richardson, 2001;Chen et al., 2010).
Nous nous intéressons dans cet article à l'étude de la propagation dans les réseaux sociaux. Notre objectif est de proposer une méthodologie permettant de comparer des modèles préexistants et documentés de propagation. Le grand nombre et les différentes variations de ces derniers offrent un assortiment de solutions compliquant le choix d'un modèle particulier. Afin de faciliter cette tâche, il convient de pouvoir comparer effectivement les modèles et non seulement les résultats finaux obtenus suite à leur application.
Cette ambition rejoint Kempe et al. (2003) qui proposent une généralisation de différents types de modèles de propagation. Ces résultats permettent de voir les modèles dans un cadre entièrement mathématique où chacun des algorithmes devient une solution à un problème d'optimisation commun. A l'inverse, nous adoptons une perspective résolument algorithmique dont l'objectif est de venir en appui à une approche exploratoire.
Il n'existe à notre connaissance pas de formalisme unifiant toutes les approches permettant d'effectuer une comparaison des modèles, de leur formulation, leur complexité ou leurs performances. La première contribution de cet article est donc de proposer un tel cadre unificateur basé sur un formalisme solide : la réécriture de graphes.
La propagation est généralement vue comme un phénomène global au réseau bien qu'elle émerge en réalité de la somme d'une multitude d'évènements y agissant localement. La plupart des modèles consistent donc en un ensemble de règles décrivant les situations dans lesquelles une entité peut influencer ses voisins. Bien que chacun de ces évènements soit décrit localement et succinctement, l'application répétée de transformations locales permet de faire émer-ger le comportement du modèle au niveau global. Dans ce formalisme, un modèle correspond alors à un ensemble de règles de transformation couplé à une stratégie qui régule et ordonne l'application de ces mêmes règles.
Les modèles auxquels nous nous intéressons par la suite considèrent un réseau social dont la topologie est fixée. Les règles décrivent alors comment évoluent les états des sommets du ré-seau. Dans leurs travaux, Kejžar et al. (2008) se sont intéressés, de façon similaire, à l'évolution du caractère topologique d'un réseau social. Partant d'un réseau pré-existant, les auteurs proposent une série de règles permettant de modifier les connexions entre les acteurs du réseau, autorisant ainsi la création ou suppression de liens. Leur travail rejoint donc notre approche consistant à exploiter la réécriture comme mécanisme pour exprimer leurs modèles d'évo-lution des réseaux. Cependant, leur article est davantage orienté vers l'analyse des résultats asymptotiques probabilistes sur l'évolution de la taille et la densité des réseaux ainsi produits. L'intérêt de notre approche basée sur une description commune des modèles tient égale-ment à la possibilité d'étudier et de comparer ceux-ci de manière expérimentale. La plupart des travaux s'intéressent à l'objectif atteint au terme d'une propagation (couverture du réseau, vitesse de propagation, etc.), en revanche, il est plus difficile d'établir des résultats expliquant comment se déroule la propagation et pourquoi cet objectif est atteint. L'utilisation d'un formalisme commun nous permet, au contraire, la réalisation de ce type d'investigation.
De plus, cette méthodologie prend un sens particulier lorsque l'étude des modèles se fait de manière visuelle et interactive. En manipulant le modèle (en lançant des simulations, en isolant une règle, etc.), l'utilisateur est à même de développer une connaissance du modèle ainsi que de mesurer et suivre son comportement au fil du déroulement. Pour ces raisons, nous présentons une plate-forme de visualisation analytique -exploitant une version étendue de PORGY (Pinaud et al., 2012) (voir Fig. 1) -pour, simultanément, construire les réseaux et règles de réécriture, simuler la propagation selon différentes stratégies (i.e. les modèles) et comparer les traces d'exécution de ces dernières à l'aide de divers critères.
L'article introduit d'abord la terminologie propre aux modèles de propagation des réseaux et décrit deux modèles particuliers (section 2). Ces modèles sont ensuite exprimés à l'aide de règles de réécriture illustrant ainsi le pouvoir d'expression et l'utilisabilité du formalisme (section 3). Nous montrons enfin comment la plate-forme de visualisation peut être utilisée pour étudier les modèles et exhiber leurs différences (section 4).
FIG. 1: Interface de PORGY :
(1) le réseau social sur lequel on applique la propagation ; (2) édition d'une règle ; (3) portion de l'arbre de dérivation, conservant une trace complète des calculs réalisés (le graphe (1) représente un des sommet de celui-ci) ; (4) courbe montrant l'évolution du nombre de sommets actifs ; (5) autre représentation de l'arbre de dérivation ; (6) éditeur de stratégies.
Modélisation de la propagation dans les réseaux sociaux
Un réseau social (Brandes et Wagner, 2003)   (2010)) construits au fil du déroulement de la propagation et qui peuvent être utilisés pour mesurer l'influence d'un utilisateur sur ses voisins ou représenter sa tolérance à la réalisation d'une action (plus un utilisateur est sollicité, plus il sera enclin à s'activer ou inversement).
Face à cette diversité, nous nous limitons dans la suite de l'article à illustrer la faisabilité de notre approche sur deux modèles représentatifs : un modèle à cascades indépendantes (IC, Kempe et al. (2003)) utilisé comme base pour de nombreux cas, et un modèle à seuils linéaires (LT, Goyal et al. (2010)) qui exploite un principe d'activation non probabiliste contrairement au modèle précédent :
Le modèle à cascades indépendantes IC. Ce modèle comporte de nombreuses variations (e.g. Gomez-Rodriguez et al. (2010); Watts (2002)) permettant, par exemple, la propagation d'opinions divergentes dans un même réseau (Chen et al., 2011). Nous le décrivons sous une forme basique, telle que proposée par Kempe et al. (2003).
Soit un sous-ensemble de sommets A 0 ? V activés au temps t = 0 et les probabilités p v,w , définies pour toutes paires de sommets voisins {v, w} pour représenter l'influence de v ? A 0 sur w / ? A 0 . Une série de nouveaux ensembles de sommets activés A t+1 est calculé à partir de A t . Pour chacun des sommets dans A t , on visite les voisins w ? N (v) qui n'ont pas déjà été activés (mais qui peuvent déjà avoir été visités) ; en d'autres mots, w ? N (v) \ ? t i=0 A i . Un sommet w peut alors devenir actif avec une probabilité p v,w , auquel cas il est ajouté à A t+1 . L'algorithme s'arrête lorsque A t+k est vide (pour k ? 0).
Modèle à seuil linéaire LT. Ce modèle suit un déroulement différent du précédent. Il le rejoint cependant en ce qu'un sommet ne change plus d'état dès lors qu'il est activé. On suppose donné, soit aléatoirement, soit appris selon un historique d'actions connues, les probabilités p v,w . Chaque sommet w ? V est aussi équipé d'un seuil ? w . Soit S w l'ensemble des voisins du sommet w qui sont activés. On détermine l'ensemble A t+1 en calculant pour chaque sommet w non encore activé la valeur d'influence jointe p w (S) = 1 ? v?Sw (1 ? p v,w ). Le sommet w devient ainsi actif dès que l'influence de ses voisins excède son seuil d'activation, c'est à dire lorsque p w (S) ? ? w . 
Réécriture de graphes
Les éléments de base du calcul de la réécriture sont des sommets du graphe, additionnellement équipés de ports, auxquels les arêtes vont se connecter. Plus généralement, les sommets, ports et arêtes vont avoir des propriétés associées à une valeur (par exemple la probabilité p v,w ou le nom donné à un port) qui permettront de les distinguer entre eux, une combinaison spécifique des ces propriétés pouvant être identifiée comme un état.
Une règle de réécriture consiste en un couple L ? R où L et R sont eux-mêmes des graphes (souvent petits). L et R sont respectivement appelés les membres gauche et droit de la règle. L'application d'une règle sur un graphe G se fait en localisant dans G un sousgraphe H isomorphe à L et en le "remplaçant" par R. La notion d'isomorphisme doit toutefois être étendue pour tenir compte des états des sommets, des ports et des arêtes. La règle doit également, le cas échéant, préciser comment traiter les arêtes incidentes aux ports de H qui ne sont pas mentionnées dans L. De plus, une règle peut aussi calculer les valeurs de plusieurs propriétés de R en fonction de celles de H.
Des exemples de règles sont donnés figure 2. La règle 2a concerne une paire de sommets, l'un dans l'état activé (vert), l'autre étant non activé (rouge), connectés par une arête allant du port In du sommet activé vers le port Out du sommet non activé. La règle maintient la connexion entre les sommets et modifie l'état du sommet rouge, le faisant passer dans l'état violet signifiant que le sommet a été visité (ses propriétés ont été lues et il est possible de tenter de l'activer). La règle 2b ne concerne qu'un seul sommet ; son application est donc potentiellement réalisable sur tout sommet du graphe à condition qu'il soit dans le même état (ici visité, symbolisé par la couleur violette). La règle consiste simplement à modifier l'état du sommet, le faisant passer de l'état violet à l'état vert (activé).
Le défi consiste la plupart du temps à savoir prédire le comportement de la réécriture ré-pétée de règles sur un graphe. En effet, l'exécution des règles n'est pas déterministe puisque leur ordre d'application n'est, a priori, pas précisé, mais aussi parce qu'elles peuvent être appliquées sur de multiples instances H du membre gauche L de la règle. Il devient alors intéressant de savoir si le calcul de la réécriture converge et s'il est confluent.
Dans cet optique, il peut également être tentant de chercher à conditionner l'ordre d'application des règles afin de guider le comportement du calcul. A cette fin, il est possible de définir une stratégie d'application des règles. Une stratégie permet de choisir un ensemble des règles à appliquer, préciser leur ordre, le nombre de répétitions, et l'endroit où celles-ci peuvent être ap-pliquées. Pour plus de détails sur le langage de stratégies utilisé par la plate-forme PORGY, sa formalisation et ses propriétés en tant que langage formel, le lecteur pourra consulter l'article de Fernandez et al. (2014).
Traduction des modèles de propagation
Le premier défi qui se pose à nous est de pouvoir donner, pour chacun des modèles présen-tés dans la section 2, un ensemble de règles et une stratégie d'application de ces dernières qui permet d'émuler le fonctionnement du modèle.
Notre démarche de traduction d'un modèle de propagation quelconque en une série de règles de réécriture et leur stratégie d'application est aisément généralisable. Celle-ci a pu être appliquée à tous les modèles rencontrés dans la bibliographie étudiée. Pour la clarté de la discussion concernant les étapes à suivre, nous présentons uniquement la traduction du modèle de propagation à cascades indépendantes (section 2). Ce dernier illustre tout à fait les opérations à réaliser et toute procédure de traduction d'autres modèles suit un déroulement similaire.
Le motif (membre gauche d'une règle) principal à rechercher pour faire évoluer la propagation consiste à identifier un couple de sommets voisins dont l'un est activé et l'autre ne l'est pas. Il est ainsi nécessaire de conserver pour chaque sommet son état actuel. De manière similaire, chaque arête devra préserver les probabilités d'influence p v,w et p w,v que ses extrémités v et w pourront imposer l'une sur l'autre lorsque l'un des sommets s'activera.
La stratégie employée consiste, pour chaque sommet non actif, à calculer puis stocker l'influence de ses voisins actifs en ne conservant que la valeur pour le sommet qui a l'influence la plus forte. Le sommet non actif passe alors dans l'état visité (règle de la figure 2a). Le parcours du voisinage est, de cette manière, contrôlé par la stratégie tandis que les actions à effectuer sur le réseau sont contrôlées par les règles. La procédure décrite ci-dessus forme une stratégie qui sera répétée tant qu'un sommet actif peut influencer un de ses voisins (il reste une arête non marquée qui permet d'appliquer la règle 2a). Chaque application de règle entraîne l'ajout d'un sommet sur l'arbre de dérivation et d'une arête (de couleur violette) pour montrer la succession des opérations. Les points de départ et d'arrivée d'une stratégie (enchaînement de plusieurs règles) sont eux représentés par une arête verte (Fig. 1). Cet arbre peut rapidement atteindre une taille conséquente, rendant la lisibilité difficile. Nous pouvons néanmoins le filtrer et ne conserver que les arêtes vertes et les sommets correspondants (Fig. 4).
Visualisation analytique et comparaison des modèles
Nous détaillons dans cette partie comment la plate-forme de visualisation PORGY (Pinaud et al., 2012) est utilisée pour comparer deux applications des modèles de propagation présentés au début de cet article. Nous avons utilisé le modèle de Wang et al. (2006) pour générer un réseau social aléatoire de 300 sommets. Le réseau obtenu a 597 arêtes. Les conditions de départ sont identiques pour les deux modèles : même ensemble initial de sommets activés et même distribution de probabilités d'influence entre les sommets. Notre objectif n'est pas de montrer que tel modèle de propagation est meilleur que tel autre (ceci nécessiterait de nombreuses simulations pour calculer les résultats moyens sur les modèles probabilistes) mais plutôt de comprendre comment les modèles fonctionnent 1 . Les applications successives de la stratégie décrite précédemment permettent aux sommets actifs de transmettre l'information ou l'action représentant le sujet de la propagation à leur voisins. Chaque exécution de règle va créer un état intermédiaire du graphe d'origine qui sera conservé dans la trace de la propagation (passage des différents sommets de non visité à visité puis potentiellement actif ). Cet historique va pouvoir être exploité pour étudier et comparer le graphe à un instant donné ou pour reconstituer et suivre le chemin emprunté par le processus d'activation des sommets.
Un arbre de dérivation (voir fig.1) est ainsi créé et maintenu pour fournir toutes ces informations. En visualisant des états successifs, nous pouvons observer cette progression. La figure 3 présente quelques vignettes d'une vue de type Small-Multiples, qui est une partie du graphe analysé, et montre l'évolution de l'état de ses sommets. Les différents temps t représentent les applications successives des stratégies (un sommet non visité à t peut donc se retrouver activé à t + 1). L'arbre de dérivation nous permet immédiatement de montrer quel est le modèle qui nécessite le moins d'étapes de calcul ou le moins de lancements de stratégie avant d'arriver à terme car sa branche est la plus courte (figure 4). Nous avons ainsi une première approximation de la complexité en temps des algorithmes.
Lier la profondeur de l'arbre (donc le temps) avec d'autres mesures nous permet de considérer l'évolution de différents paramètres tout au long de la propagation. Nous pouvons, par exemple, aborder la notion de vitesse de propagation, valeur indiquant l'évolution du nombre de sommets actifs en fonction du temps. La figure 4 présente l'évolution de cette valeur pour une exécution des modèles à cascades indépendantes (partie droite, courbe du haut) et à seuils linéaires (courbe du bas). Les courbes présentées n'ont pas les mêmes échelles puisqu'elles sont calculées indépendamment pour chacun des modèles. Malgré ceci, nous pouvons observer que le modèle à cascades va parvenir à activer environ 80% des sommets contre seulement 18% pour celui à seuils (pour un même nombre d'étapes de réécritures), démontrant le fort impact sur les performances des modèles, dans un premier temps, des valeurs utilisées pour l'initialisation des probabilités d'influence, et dans un second temps, du choix de l'ensemble de sommets initialement activés. Nous avons utilisé une loupe (fonctionnalité de PORGY) sur le haut de chaque axe pour rendre les valeurs lisibles. Nous pouvons aussi noter qu'après la première application de la stratégie, le nombre de sommets activés est très proche pour chacun des modèles, les différences apparaissant et se confirmant par la suite. Il peut aussi être intéressant de voir l'état du graphe quand le nombre de sommets activés atteint un seuil. Puisque les différentes vues sont liées, la sélection d'un sommet/arête (en bleu sur la figure 4) de l'arbre de dérivation entraîne sa sélection dans la courbe et vice-versa. De manière similaire, la sélection d'un sommet lors d'une étape de la propagation sera immédia-tement répercutée à l'ensemble des étapes contenant ce même élément, rendant la sélection visible même sur les sommets de l'arbre de dérivation. D'après la méthodologie employée par l'application PORGY, tant qu'un élément n'est pas modifié par une règle, il n'est jamais changé. En conséquence, la sélection d'un sommet d'intérêt dans l'un des graphes intermé-diaires représentant le réseau permet de savoir directement quand cet élément a changé d'état, surtout si l'on travaille sur la version complète de l'arbre de dérivation (montrant le détail des applications de règles).
Finalement, le type de mesure évoluant selon le temps peut être généralisé à d'autres propriétés des modèles de propagation. La notion de sommet visité, introduite précédemment, peut présenter un intérêt, dans le cas où un message doit seulement être vu et non nécessai-rement redistribué ou propagé par les utilisateurs. Cette vitesse de connaissance du contenu de la propagation est observable de manière similaire à la vitesse de propagation. De plus, en considérant ces deux mesures, nous pouvons en proposer une troisième exprimant l'efficacité d'une propagation, calculable grâce au rapport entre le nombre de sommets activés à un instant t et ceux visités/influencés au moment précédent t ? 1.
Conclusion et travaux futurs
Nous avons présenté un formalisme basé sur la réécriture de graphes vu comme un langage commun à l'expression de tous les modèles de propagation sur réseaux. Lorsque la propagation n'entraîne pas de modifications de la topologie du graphe, le modèle consiste en un ensemble réduit de règles gérant les transitions d'état des sommets du graphe. Le stockage des états des sommets, les règles et le langage de stratégie qui pilote leur application facilite la gestion du caractère probabiliste des modèles.
Nous envisageons d'étendre notre étude à un panel plus large de modèles de propagation afin de démontrer le caractère "universel" de notre approche. Cela exige aussi de pouvoir multiplier les simulations sur des réseaux de tailles conséquentes. Cet aspect pose un défi en raison de la complexité liée à la recherche de motifs correspondant aux membres gauches des règles -d'abord parce que nous faisons face à un problème NP-Complet (isomorphisme de sous-graphes), mais aussi à cause de l'explosion combinatoire qu'elle engendre et qui doit être gérée à l'aide du langage de stratégie (d'application des règles).
La formulation des modèles à l'aide de réécritures offre une possibilité nouvelle qui permettra de combiner propagation dans le réseau et évolution de la topologie du réseau sur lequel la propagation a lieu. Là encore, un langage de stratégie facilitera la gestion de l'application simultanée ou alternée de ces deux types de transformations. Il n'existe pas, à notre connaissance, de tels modèles. Nous espérons ainsi pouvoir proposer des modèles réalistes d'évolution de réseaux, dont le réalisme tiendrait à la fois aux caractères structurels des réseaux produits, mais aussi à leur qualité en terme de circulation de l'information.
can only be made at the cost of describing models based on a common formalism and independant from them. We propose to use graph rewriting to formally describe the propagation mechanisms as local transformation rules applied according to a strategy. This approach makes complete sense when supported by a visual analytics framework dedicated to graph rewriting. The paper first presents several models and illustrates them through selected simulations. We then show how our visual analytics framework allows to interactively manipulate models, and underline their differences based on measures computed on simulation traces.

Introduction
L'évolution d'ontologie est un sujet posé avec l'apparition des méthodologies de construction d'ontologies. Il s'est avéré indispensable de penser à maintenir et faire évoluer les ontologies, après leur construction, afin d'assurer leur réutilisation et leur continuité. Ce besoin s'est rapidement développé avec la prolifération des ontologies et leur large utilisation. À titre d'exemple, depuis Janvier 2010, 59 versions de l'ontologie Gene Ontology 1 (une des plus fameuses ontologies) ont été publiées à raison d'une version par mois. Ainsi, afin de définir et gérer le processus d'évolution, plusieurs méthodologies ont été proposées dans la littérature (Klein, 2004;Stojanovic, 2004;Djedidi et Aufaure, 2010;Khattak et al., 2013). Les premiers travaux ont pensé à définir ce qu'est une évolution d'ontologie. D'où la définition proposée par (Stojanovic, 2004) : "l'évolution d'ontologie est l'adaptation, dans le temps, d'une ontologie aux besoins de changement et la propagation cohérente des changements aux artefacts dépen-dants". Cette définition a ouvert le débat sur la signification des changements ontologiques, leurs formalisations et leurs types (Klein, 2004;Stojanovic, 2004). Ainsi, un changement ontologique est une modification d'une ou plusieurs entités ontologiques (classe, propriété, axiome, individus, etc.). Il peut viser la modification de la structure de l'ontologie (ex. ajout de classe, ajout de propriété) et on parle dans ce cas de l'enrichissement d'ontologie. Il peut viser égale-ment l'ajout d'individus et on parle alors du peuplement d'ontologie. Les changements ontologiques sont souvent de trois types (Stojanovic, 2004) : 1) les changements élémentaires qui représentent une opération primitive et non décomposable qui affecte une seule entité ontologique (ex. renommer une classe) ; 2) les changements composés (composites) qui affectent une entité ontologique et ses voisins (ex. suppression d'une classe) ; 3) les changements complexes qui expriment un enchainement de plusieurs changements élémentaires et/ou composés (ex. fusion de classes). En effet, les changements composés et complexes sont des changements utiles et demandés par l'utilisateur. Ils englobent plusieurs modifications en une seule opéra-tion, ce qui lui permet d'adapter son ontologie d'une manière plus facile sans se perdre dans les détails des changements élémentaires (Klein et Noy, 2003). Cependant, la définition et la formalisation de ces changements sont des tâches non triviales comme leur application peut affecter la cohérence de l'ontologie. Deux types de cohérence sont généralement distingués dans la littérature : 1) la cohérence conceptuelle qui se réfère aux règles structurelles et contraintes du langage de représentation de l'ontologie (ex. inexistence de concepts isolés) ; 2) la cohé-rence sémantique qui se réfère à la cohérence logique de l'ontologie dans le sens où elle ne doit pas comporter des contradictions logiques (ex. ne pas avoir des relations contradictoires entre deux concepts). En effet, la préservation de la consistance de l'ontologie et la résolution des incohérences résultantes de l'application des changements ontologiques sont encore des problématiques insuffisamment étudiées. Ainsi, nous proposons dans cet article une nouvelle formalisation des changements ontologiques composés et complexes permettant : 1) d'éviter les inconsistances d'une manière a priori en utilisant les concepts des grammaires de graphes typés ; 2) de réduire le nombre de changements élémentaires constituant les changements composés/complexes. Les changements étudiés traitent à la fois le niveau structurel de l'ontologie (l'enrichissement de l'ontologie) et aussi le niveau assertionnel (le peuplement d'ontologie).
Le reste de l'article sera organisé comme suit : la section 2 présente un tour d'horizon sur les principales approches d'évolution d'ontologie. La section 3 introduit les concepts de base des grammaires de graphes. La section 4 propose une nouvelle formalisation des changements ontologiques composés et complexes. Enfin, une conclusion synthétise le travail présenté et donne les perspectives envisagées.
État de l'art
De nombreuses approches ont été proposées dans la littérature pour définir et implémenter le processus d'évolution d'ontologies. Le Tableau 1 présente certaines approches tout en préci-sant les langages utilisés, l'implémentation, la gestion des inconsistances et leurs spécificités. Ainsi, nous pouvons observer que différents langages ont été étudiés : KAON (Stojanovic, 2004), RDF (Luong et Dieng-Kuntz, 2007), OWL (Klein, 2004;Djedidi et Aufaure, 2010), etc. En se basant sur ces langages, plusieurs changements ontologiques ont été définis et différentes classifications de ces changements ont été proposées (Stojanovic, 2004;Klein, 2004). Ainsi, certains travaux se sont intéressés à l'étude des changements élémentaires (Mahfoudh et al., 2013). D'autres ont traité également les changements composés et complexes (Djedidi et Aufaure, 2010;Javed et al., 2013;Liu et al., 2014). Des travaux se sont focalisés sur l'enrichissement d'ontologies (Klein, 2004). D'autres ont étudié aussi le peuplement d'ontologies (Luong et Dieng-Kuntz, 2007;Djedidi et Aufaure, 2010;Mahfoudh et al., 2013). La résolution des inconsistances est encore insuffisamment étudié. En effet, certaines approches ont ignoré cet axe comme ils se sont intéressés à d'autres problématiques, comme par exemple la gestion des versions des ontologies (Hartung et al., 2013). D'autres travaux se sont focalisés plutôt sur l'identification des inconsistances sans les résoudre (Gueffaz et al., 2012). Certains chercheurs se sont intéressés également par la résolution des inconsistances (Djedidi et Aufaure, 2010;Luong et Dieng-Kuntz, 2007;Javed et al., 2013). Cependant, les approches proposées admettant un processus a postériori de traitement des inconsistances qui nécessite l'utilisation d'une ressource externe (tel qu'un raisonneur) afin de vérifier la consistance de l'ontologie évoluée. Afin d'éviter l'utilisation d'un raisonneur externe, Mahfoudh et al. (2013)  -Approche basée sur les patrons de conception.
-Évaluation de la qualité de l'ontologie évoluée.
-Approche nécessitant des activités lourdes. (Gueffaz et al., 2012) OWL DL Prototype -Identification des inconsistances en utilisant le checker NuSMV.
-Approche d'évolution d'ontologies, CLOCk (Change Log Ontology Checker) basée sur le modèle checking.
-Approche nécessitant la transformation des ontologies OWL au langage NuSMV. (Hartung et al., 2013) OBO 
Grammaires de Graphes Typés
Les grammaires de graphes, également appelées réécriture de graphes, sont un formalisme mathématique pour représenter et gérer les graphes. Elles permettent la modification de graphes via des règles de réécriture tout en précisant quand et comment faire les changements. Grâce aux concepts et outils qu'elles proposent, les grammaires de graphes sont utilisées dans plusieurs branches de l'informatique, comme par exemple la modélisation des systèmes logiciels et la théorie des langages formels (Ehrig et al., 1996). Elles ont récemment été introduites dans le domaine des ontologies, ce qui a donné naissance à des travaux traitant de la formalisation des ontologies modulaires (d'Aquin et al., 2007), la représentation des graphes RDF (Resource Description Framework) (Braatz et Brandt, 2010), la fusion d'ontologies (Mahfoudh et al., 2014a), etc. Dans ce qui suit, nous dressons un tour d'horizon des définitions de base concernant les fondements théoriques de la réécriture de graphes. Graphe. Un graphe G(N, E) est une structure composée par un ensemble de noeuds (N ), d'arêtes (E) et d'une application s : E ? N × N qui attache les noeuds source/destination à chaque arête. Graphe attribué. Un graphe attribué est un graphe étendu par un ensemble d'attributs A, une fonction d'attribution att : N ? E ? P(A), P représentant l'ensemble des parties, et une fonction d'évaluation val : A ? V . Ainsi, chaque noeud ou arête peut avoir un ensemble d'attributs (P(A)) dont les valeurs seront données par val.
avec N T correspond aux types des noeuds et E T aux types des arêtes. Grammaires de graphes typés. Une grammaire de graphe typé est une structure mathéma-tique définie par T GG = (G, T G, P ) avec :
-G est un graphe initial, appelé aussi graphe hôte ; -T G est un graphe type précisant le type de l'information représentée dans le graphe hôte (type des noeuds et des arêtes) ; -P un ensemble de règles de réécriture, appelées aussi règles de production ou de transformations de graphes. Une règle de réécriture r est une paire de graphes pattern (LHS, RHS) avec : 1) LHS (Left Hand Side) représente la pré-condition de la règle de réécri-ture et décrit la structure qu'il faut trouver dans un graphe G pour pouvoir appliquer la règle ; 2) RHS (Right Hand Side) représente la post-condition de la règle de réécriture et doit remplacer LHS dans G. Les règles peuvent également avoir des conditions supplémentaires appelées N AC (Negative Application Conditions). Ce sont des graphes pattern définissant des conditions ne devant pas être vérifiées pour que la règle de réécriture puisse être appliquée. La transformation de graphe consiste ainsi à définir comment un graphe G peut être transformé en un nouveau graphe G . Cette transformation peut être réalisée selon deux types d'approches (Rozenberg, 1999) : les approches ensemblistes (Node replacement, Edge replacement, etc.) et les approches algébriques. Dans ce travail, nous utilisons les approches algébriques basées sur le concept de pushout de la théorie des catégories (Ehrig et al., 1973). Pushout. Soient trois objets de la catégorie des graphes :
. A partir de là, deux variantes sont proposées pour la réécriture des graphes : le Simple pushout SPO (Löwe, 1993) et le Double poushout DPO (Ehrig, 1979). Dans ce travail, seule l'approche SPO a été considérée car elle se voit plus générale et permet l'application des différents changements ontologiques (Mahfoudh et al., 2014b). Ainsi, appliquer une règle de réécriture à un graphe initial G, selon la méthode SPO, revient à : 1) trouver un morphisme (m) permettant d'identifier un sous-graphe de graphe G qui correspond (match) avec la partie LHS (m : LHS ? G) ; 2) appliquer la règle de réécriture sur le sous-graphe en le remplaçant par m(RHS) et supprimant les arêtes suspendues, i.e. les arêtes qui ont une extrémité non liée à un noeud. Ainsi, d'une manière générale, nous avons SP O(G, LHS, RHS) = G .
Formalisation des changements ontologiques 4.1 Modèle de transformation de graphes
Afin de représenter les ontologies avec le formalisme de grammaires de graphes, nous considérons une ontologie comme un graphe hôte G possédant une relation de typage avec le graphe type (T G), où T G représente le méta-modèle de l'ontologie. Pour être conforme aux standards, c'est OWL qui a été retenu comme méta-modèle d'ontologies. Ainsi, les types des noeuds considérés sont : N T = {Class(C), P roperty(P ), ObjectP roperty(OP ), DataP roperty(DP ),-Individual(I), DataT ype(D), Restriction(R)}.
Les types des arêtes correspondent aux axiomes utilisés pour relier les différentes entités :
Les changements ontologiques sont formalisés par un ensemble de règles de réécriture :
Dans cette définition étendue, CHD correspond à l'ensemble des changements dérivés ajoutés à un changement ontologique pour corriger ses éventuelles inconsistances. La Figure 1 montre une représentation et une application de la règle de réécriture du changement ontologique AddIndividual. Elle permet d'ajouter un individu "Pascal" tout en spé-cifiant son type, la classe "Person". La règle assure, grâce au N AC, la non redondance de données, i.e. elle empêche l'application du changement dans le cas où l'individu existe déjà dans l'ontologie. 
Formalisation des changements ontologiques
Cette section présente notre formalisation des changements ontologiques composés et complexes par les grammaires de graphes typés. Avant de détailler cette formalisation, une brève introduction des changements élémentaires est indispensable pour mieux comprendre le reste de l'article. Ainsi, les changements élémentaires englobent les changements de renommage, l'ajout et la suppression de certains concepts. Ils n'affectent qu'une seule entité ontologique bien qu'ils dépendent d'autres entités. Le Tableau 2 présente quelques changements élémen-taires adressés dans notre travail et les concepts dont ils sont dépendant. À noter que les N ACs des règles de réécriture sont déduites à partir de ces interdépendances. Un exemple du changement élémentaire AddIndividual est déjà présenté dans la section 4.1. 
Changements ontologiques composés
Les changements ontologiques composés, appelés aussi composites, affectent une entité ontologique et ses voisins. Ils sont alors formés par plusieurs règles de réécriture : une règle présentant le changement souhaité par l'utilisateur (changement principal) et les autres règles présentant les changements dérivés (CHD) ajoutés pour préserver la consistance de l'ontologie. En effet, l'ordre des règles de réécriture est primordial dans la plupart des changements. Le Tableau 3 présente l'interdépendance entre ces changements organisés dans une matrice de changements. La valeur d'un élément de matrice (i, j) indique que l'application d'un changement relié à une ligne i implique l'application du changement de la colonne j. Ainsi, le changement ontologique RemoveCardinalityRestriction(C, OP ) permet de supprimer une CardinalityRestriction définie sur une classe C et une objectProperty OP . Il est formalisé par deux règles de réécriture. La première représente le changement dérivé RemoveAssertionObjectP roperty qui supprime toutes les assertions définies sur OP . La deuxième règle définit la règle de réécriture principale assurant la suppression de la restriction.
Le changement ontologique RemoveObjectP roperty(OP ) supprime une objectProperty OP et toutes ses dépendances de l'ontologie. La Figure 2 présente les six règles de réécriture définissant ce changement. Ainsi, les cinq premières règles décrivent les changements déri-vés (CHD) devant être appliqués pour préserver la consistance de l'ontologie. La dernière règle présente la règle de réécriture principale. Ainsi, les restrictions définies sur la propriété OP doivent être supprimées en appliquant les règles suivantes : RemoveAllV aluesRestriction(OP ), RemoveSomeV aluesRestriction(OP ), RemoveHasV alueRestriction(OP ) et RemoveCardinalityRestriction(OP ). Toutes les ObjectP ropertyAssertion qui réfé-rencent l'objectProperty OP doivent également être supprimées. 
Changements ontologiques complexes
Le Tableau 4 présente l'ensemble des changements complexes abordé dans ce travail et les changements dont ils sont composés.
Comme exemples de changements complexes, nous présentons les changements P ullU pClass, M ergeClass et SplitClass. Ainsi, le changement P ullU pClass(C, C p ) permet de monter une classe C dans sa hiérarchie de classes et l'attacher aux parents de sa super-classe précédente C p . Ceci implique que la classe C n'est plus la subClass de la classe C p et n'infère plus ses propriétés. La Figure 3 présente la règle de réécriture définissant ce changement. Ainsi, le changement dérivé RemoveObjectP ropertyAssertion vérifie si la classe C possède des individus qui partagent une objectP ropertyAssertion sur les propriétés de la classe C p . Dans ce cas, toutes les assertions doivent être supprimées. Le changement RemoveDataP ropertyAssertion supprime toutes les dataP ropertyAssertion définies sur les individus de la classe C et les dataProperties liées à la classe C p .  Le changement M ergeClasses(C 1 , C 2 , C N ew ) fusionne deux classes C 1 et C 2 déjà existantes dans l'ontologie en une nouvelle classe (C N ew ). Il nécessite l'application des règles de réécriture AddClass(C N ew ), RemoveClass(C 1 ) et RemoveClass(C 2 ). Cependant, pour préserver la consistance de l'ontologie, avant de supprimer C 1 et C 2 , toutes leurs propriétés et axiomes doivent être attachés à C N ew . Formellement : 1)
AddSubClass(C N ew , C j ), 2) répéter le processus pour C 2 , 3) ?C i ? C(O) · C i ? C 1 appliquer AddEquivalentClasses(C i , C N ew ), 4) répéter le processus pour C 2 , etc.
Le changement SplitClass(C, C N ew1 , C N ew2 ) divise une classe (C) déjà existante dans l'ontologie en deux nouvelles classes C N ew1 et C N ew2 . Il nécessite alors l'application des règles de réécriture AddClass(C N ew1 ), AddClass(C N ew2 ) et RemoveClass(C). Comme le changement M ergeClasses, le changement SplitClass nécessite, avant la suppression de la classe C, d'attacher toutes ses propriétés et axiomes aux classes C N ew1 et C N ew2 .
Discussion : Le formalisme des grammaires de graphes offre une représentation simple des changements ontologiques. Le Tableau 5 montre deux exemples de changements AddObject-P roperty et P ullDownClass, représentés à la fois par le formalisme proposé et le travail de (Djedidi et Aufaure, 2010). Dans Djedidi et Aufaure (2010), ces changements sont considé-rés, respectivement, comme composés et complexes. Le premier changement est composé par trois changements élémentaires et le deuxième par deux. De plus, ils nécessitent, comme tous les autres changements ontologiques, l'utilisation du raisonneur Pellet pour identifier d'une manière a posteriori les inconsistances. Dans notre travail, ces changements sont considérés comme élémentaires puisqu'ils ne sont composés que d'une seule règle de réécriture. De plus, pour préserver la consistance de l'ontologie, les inconsistances sont gérées d'une manière a priori grâce à l'utilisation des Negatives Applications Conditions (N AC).
Changement ontologique (Djedidi et Aufaure, 2010) Formalisme proposé AddObjectP roperty(OP, C1, C2)
Le changement est composé de trois changements élémen-taires : 1.
AddObjectP roperty-(OP ), 2. AddDomain(OP, C1) 3. AddRange(OP, C2).
-Le changement est formalisé par une seule règle de réécriture et évite la non-redondance de données. Le changement est composé par deux changements élémen-taires : 1. AddSubClass(C1, C2) 2.
RemoveSubClass-(C1, Cp)avec la classe Cp est la super-classe de C1 et C2.
-Le changement est formalisé par une seule règle de réécriture et évite la contradiction des axiomes. 
Conclusion
Nous avons présenté dans cet article une nouvelle formalisation des changements ontologiques composés et complexes basée sur les grammaires de graphes typés et l'approche algé-brique Simple Pushout (SPO) de transformation de graphes. L'utilisation de l'approche SPO offre plusieurs avantages. En particulier, elle permet de définir simplement et formellement les règles de réécriture correspondant aux changements ontologiques. Elle assure le contrôle des transformations de graphes en évitant les incohérences d'une manière a priori. De plus, elle réduit le nombre de changements élémentaires nécessaires pour appliquer les changements complexes et composés. À noter que la formalisation des changements a été implémentée à l'aide de l'outil AGG (Attributed Graph Grammar) et testée sur des ontologies de taille ré-duite. En effet, l'étape la plus coûteuse en temps et en ressources est la reconnaissance du graphe LHS à partir du graphe hôte. Vu que la plupart des changements possèdent des LHS de taille réduite, il s'est avéré que le temps d'exécution est assez limité. À titre d'exemple, pour un graphe d'une ontologie composé de 21 noeuds, l'exécution du changement complexe SplitClass(C, C N ew1 , C N ew2 ) a pris seulement 700 millisecondes (avec un LHS composé de 37 noeuds). Pour mieux évaluer la performance de notre approche, nous travaillons actuellement sur l'évaluation de l'influence de la taille de LHS sur les ontologies de grande taille.

Introduction
Que l'on suggère à l'utilisateur d'accéder à une information, de s'inscrire à une newsletter, de commenter un service, ou d'acheter un produit, le Web Usage Mining est indispensable à l'objectif d'adaptabilité de l'offre technologique. La propension d'un utilisateur en ligne à réa-liser une action suggérée dépend en effet de sa réaction face aux modes de sollicitation et ses réactions sont, pour une part déterminante, déclenchées par son expérience en cours. L'expé-rience utilisateur correspond "aux réponses et aux perceptions d'une personne qui résultent de l'usage ou de l'anticipation de l'usage d'un produit, d'un service ou d'un système" 1 . Dès lors on comprend aisément l'enjeu communicationnel de l'analyse automatique de l'expérience de l'utilisateur. Le web 3.0 se réfléchit d'ores et déjà dans une logique one to one avec l'individualisation de la communication sur le web comme élément central.
Les questions ouvertes, par exemple sur la manière d'analyser cette expérience utilisateur, sont nécessairement interdisciplinaires. La première partie de cet article définit une méthode d'analyse sémiotique pour y répondre. C'est ensuite la spécification de cette méthode à travers la proposition de nouveaux descripteurs sémiotiques qui est développée. La dernière partie définit le mode d'apprentissage automatique pertinent pour la détection de la propension d'un individu à réaliser une action suggérée.
Détermination des données
Contrairement à la majorité des études sur les profils utilisateur, notre recherche porte sur un utilisateur quelconque 2 exposé à n'importe quelle sollicitation, ce qui détermine l'utilisation des « attitudes implicites » comme indicateur de l'expérience. Les "jugements et attitudes implicites" sont des indicateurs, issus de récentes études socio-cognitives (cf. Courbet et Fourquet-Courbet (2014)), particulièrement pertinents dans le cadre de notre recherche car définissent des actes non analysables par l'utilisateur et parfois même non perçus, mais révéla-teurs de l'expérience vécue. Nous n'avons donc pas besoin de connaître au préalable l'utilisateur qui ne saurait pas, à titre d'exemple, identifier précisément si et/ou pourquoi il clique sur l'image plutôt que sur le texte pour accéder à l'information désirée.
L'exploitation des données implicites produites par l'utilisateur permet d'estimer ses attitudes implicites. Les données explicites archivées sont inexistantes pour un utilisateur quelconque et les données explicites liées au déclaratif sont considérées trop intrusives (cf. Oard et Kim (2001)). La sollicitation de l'utilisateur, pour obtenir ces informations, constitue un frein avéré dans le processus de séduction à l'oeuvre dans les techniques de communication.
L'analyse de ces données a pour objectif de déduire un comportement à partir des interactions de l'utilisateur avec le système. Les interactions sont identifiées par : la durée de lecture (temps passé par l'utilisateur sur un écran, soit le temps passé entre deux actions), le mouvement de souris, le nombre de clics de la souris, la durée de défilement de l'ascenseur, le défilement avec souris, le nombre de clics sur l'ascenseur, le défilement avec les touches du clavier, la sélection du texte (cf. Tchuente (2013)).
Se pose alors la question de la méthode permettant de détecter des attitudes implicites à partir de l'exploitation de données implicites. Pour y répondre, nous définissons une catégorie sémiotique d'analyse du comportement prenant en considération la portée communicationnelle inconsciente des actions réalisées. Les catégories de comportement, définies dans l'état de l'art (cf. Oard et Kim (2001)), sont constituées d'actants : examiner, référencer, retenir, annoter, créer. Nous cherchons à remplacer ces catégories d'actants par des catégories d'expérience de l'en acte. Notre contribution se situe dans l'utilisation d'une catégorie sémiotique d'analyse du comportement : le style perceptif. Tel que défini par Pignier (2012), le style perceptif est l'expression d'une esthésie, une manière de percevoir le monde, l'autre et les choses, reliée à l'exercice d'une sensibilité. C'est l'expérience de l'utilisateur que nous cherchons à définir, sa façon de communiquer implicitement en interagissant avec le système. Là où la catégorisation classique d'un profil utilisateur s'intéresse aux caractéristiques d'un individu (genre, situation socio-démographique, centres d'intérêt), nous nous intéressons à son hexis numérique, son être impliqué dans l'espace du web et agissant en fonction de lui. Ainsi, le contexte est né-cessairement inclus dans le style perceptif. Qu'il soit acteur adjuvant ou opposant, il influe sur l'expérience de l'utilisateur. Il s'agit de s'approcher du schéma de la communication interpersonnelle qui permet, dans la vie réelle, d'adapter débit de parole, gestualité, tonalité à l'interlocuteur, en fonction de l'observation de ses réactions. La détection du style perceptif poursuit in fine cet objectif d'adaptabilité de la réponse à l'expérience du co-énonciateur à un moment donné.
Afin de définir le style perceptif, nous proposons une structuration des données implicites de navigation. Le style perceptif se caractérisant par les expériences médiées par les interfaces, 2. L'utilisateur quelconque représente l'utilisateur non identifié versus utilisateur identifié.
le web contient donc intrinsèquement les informations nécessaires à sa détection. La réflexion se pose au niveau de la production de métadonnées (des descripteurs) caractérisant les données implicites. Le niveau morphosyntaxique de l'analyse sémantique (comme par exemple pour TypWeb dans Beaudouin et al. (2002)) est transféré au niveau sémiotique de l'analyse. En effet, l'exploration qualitative des sites web nécessite l'utilisation de descripteurs contenant les possibilités d'interaction et de contexte.
Nous proposons d'intégrer ici, comme niveau d'analyse, le sème connotatif, qui est défini comme étant un sème connotant l'effet produit par l'interaction entre l'utilisateur et l'élément. À titre d'exemple, à la valeur sémantique de l'élément "pop-up", nous associons les valeurs sé-miotiques intrusif et surgissant. Intrusif et surgissant connotent l'effet produit par l'interaction avec "pop-up". À la valeur sémantique "sommaire sous forme de listes à faire défiler", nous associons la valeur sémiotique cartographique. L'effet cartographique renvoie à l'expérience de la vue d'ensemble. L'ensemble des sèmes ainsi produit forme les descripteurs pour l'annotation des sites web. Le niveau sème connotatif contient l'expérience possible (soit l'effet produit par l'interaction) mais sa capacité à qualifier un style dépend du schéma actantiel (l'action effective de l'utilisateur). Le schéma actantiel de Greimas (1966)  Le niveau sème connotatif est donc pondéré par les actions suivantes sur les éléments : lecture ou visualisation (déterminée elle même par le temps passé), cliquer pour accéder, cliquer pour fermer, défiler, surligner, cliquer pour partager, inscrire, commenter, annoter. Le sème connotatif peut alors soit être positivé ou contrarié. Le sème connotatif intrusif relié à l'apparition d'une pop-up pour l'inscription à une newsletter, sera positivé par l'action "inscrire" (par exemple inscrire son mail), mais sera contrarié par l'action "cliquer pour fermer". On définit alors la paire de sèmes suivante : intrusif et non intrusif, qui sera niée en dehors des actions contrariantes ou positivantes qui lui sont associées. Il s'agit ensuite d'envisager les structures qui se dessinent au niveau sémiotique. En prenant en considération la chronologie des interactions et leur interopérabilité -simultanéité, succession, opposition, exclusion -et en définissant la dernière action de l'utilisateur comme finalité, le contexte sémiotique est ainsi défini. L'interaction formulée dans la phrase suivante : « L'utilisateur en ligne ferme quasi instantanément la pop-up newsletter puis fait défiler le sommaire dans son intégralité avant d'accéder à la page information » est retranscrite dans le contexte d'une annotation sémiotique par : « Un style perceptif non intrusif successivement cartographique pour aboutir à : accéder à la page désirée ». La création d'une ontologie appliquée à la détection du style perceptif s'avère ainsi nécessaire pour définir les règles et articulations d'une annotation sémiotique. Celle-ci permet ainsi d'exploiter les données implicites, pour faire émerger les attitudes implicites détermi-nantes de l'expérience de l'utilisateur, que nous caractérisons par son style perceptif.
Descripteurs sémiotiques proposés
L'étude sémiotique présentée dans la section précédente nous a amené à définir des descripteurs sémiotiques que nous présentons dans ce qui suit. Pour rappel, une session de navigation consiste en une succession de L écrans visités (L étant variable d'une session à une autre), en ne tenant compte que des écrans sur lesquels l'utilisateur a passé un temps supérieur à un certain seuil, ainsi que ceux qu'il ne subit pas par défaut au chargement de la page.
Il s'agit alors de caractériser le contenu sémiotique de ces écrans, en introduisant des descripteurs qui encodent les informations liées aux styles perceptifs. Comme nous l'avons mentionné dans la section 2, ces informations se trouvent au niveau des éléments qui composent l'écran, et avec lesquels l'utilisateur interagit pendant sa navigation. A noter que cette notion d'interaction est spécifique à chaque type d'éléments : certains ne seront pris en compte dans l'analyse que si l'utilisateur les survole avec la souris, alors que pour d'autres, l'affichage par défaut sur l'écran visité est suffisant.
Nous proposons ainsi d'associer à chaque élément de l'écran un certain nombre de sèmes qui le caractérisent. Un dictionnaire de N sèmes est donc défini (sèmes positifs et négatifs), et une annotation sémiotique des pages du site permet d'attribuer à chaque élément des labels sémiotiques issus de ce dictionnaire. Cette annotation se fait manuellement sur un certain nombre de pages caractéristiques du site, et est ensuite généralisée de manière automatique aux autres pages en attribuant les mêmes labels sémiotiques aux éléments appartenant à une même catégorie, en se basant par exemple sur les attributs HTML. Les descripteurs sémiotiques caractérisant un écran E sont ensuite calculés à partir de ces vecteurs binaires, en mesurant la fréquence de chaque modalité du dictionnaire de sèmes :
Afin d'éviter que deux écrans ayant un contenu sémiotique similaire et appartenant à des pages sémantiquement éloignées ne soient représentés avec des indicateurs identiques, nous proposons de concaténer le vecteur desc sémiotique (E) avec un descripteur sémantique classique décrivant le contenu de la page à laquelle appartient l'écran.
Pour ce faire, un dictionnaire de M labels sémantiques est ainsi défini, et les différentes pages du site sont annotées avec ces labels. De nombreuses méthodes d'annotation automatique, issues notamment des domaines du Web sémantique et du Web Content Mining, existent dans l'état de l'art (Charrad et al. (2008) par exemple).
Ainsi, chaque écran d'une page donnée sera représenté par un vecteur de description de dimension N × M , composé d'un premier vecteur qui est propre à cet écran, et d'un second qui est commun à tous les écrans d'une même page. Cette représentation permet à la fois de lever la limitation décrite dans le paragraphe précédent, et d'encoder indirectement la suite des pages visitées (comme dans une approche de Web Usage Mining classique) à travers les variations des vecteurs sémantiques à chaque fois que l'utilisateur accède à une nouvelle page.
Traitement des descripteurs
Nous nous intéressons dans cette section au choix d'un modèle d'apprentissage adapté au traitement des descripteurs introduits dans la section précédente. Nous avons opté pour un modèle neuronal, et ce principalement pour deux raisons : (i) l'aspect temps-réel des applications visées, pour lequel les modèles neuronaux sont particulièrement adaptés de part le fait que la quasi-totalité de la complexité est reportée sur la phase d'apprentissage, et (ii) l'optimalité de ce types de modèles en terme de performances, qui a été démontrée dans de nombreuses études comparatives récentes (parmi lesquelles nous pouvons citer celle de Bengio et Delalleau (2011)).
Vue la nature séquentielle des données traitées (une session de navigation étant représentée par une séquence de vecteurs, de dimension N × M chacun, et de longueur variable), nous avons opté pour un modèle neuronal récurrent. Ce dernier (initialement introduit dans Williams et Zipser (1995) et ayant connu plusieurs évolutions depuis) est analogue à un Perceptron multi-couches classique, mais dans lequel des connexions récurrentes sont rajoutées au niveau des couches cachées (c'est à dire les couches intermédiaires entre l'entrée et la sortie). Ce modèle récurrent est entraîné par une version modifiée de l'algorithme de rétro-propagation du gradient (dans laquelle les connexions récurrentes sont prises en compte), en ciblant les sorties désirées selon l'application visée. Ces sorties peuvent correspondre par exemple à la réponse à une recommandation de produit sur un site de e-commerce, à la réaction à une sollicitation sur un site de collecte de dons, et plus généralement à n'importe quel objectif mesurable lié au comportement de navigation de l'utilisateur.

Introduction
Les données massives, appelées communément "big data", impactent directement le processus ETL (Extracting-Transforming-Loading) vu que celui-ci est le premier composant du système décisionnel confronté à ces données. Peu de travaux ont traité sur la problématique des données massives dans le processus ETL. Liu et al. (2011) ont proposé une approche parallèle/distribuée appelée ETLMR consistant à améliorer les performances de la phase de transformation (T) et de chargement (L) de l'ETL et ce en adoptant, pour chacune des deux phases, des stratégies de distribution appropriées. Les expérimentations de Misra et al. (2013) ont montré que le paradigme MapReduce est prometteur et que les solutions ETL basées sur des frameworks open source tel que Apache Hadoop sont plus performantes et moins couteuses par rapport aux solutions ETL commercialisées. Contrairement aux travaux de Liu et al. (2011), ceux de Misra et al. (2013) considèrent la phase d'extraction (E) de l'ETL très couteuse ; celle-ci a été traitée dans un environnement parallèle/distribué selon le paradigme MapReduce. (Liu et al., 2012) est une démonstration du prototype ETLMR. Dans (Liu et al., 2014), les auteurs proposent une plateforme CloudETL basée sur Apache Hadoop et Apache Hive où les performances ont été nettement améliorées par rapport à celles d'ETLMR (Liu et al., 2011). Les plateformes ETLMR (Liu et al., 2011) et CloudETL (Liu et al., 2014) sont basées sur du code Python, et par conséquent celles-ci sont destinées aux informaticiens dévelop-peurs de solutions ETL parallèles/distribuées. Dans le but de vulgariser ce type de plateformes et les rendre accessibles aux utilisateurs finaux, nous proposons, dans ce papier, une plateforme baptisée P-ETL (Parallel-ETL) développée sous l'environnement Apache Hadoop
1
. Le paramétrage d'un processus se fait, de bout-en-bout, sur une interface unique structurée en trois onglets, chacun concerne une phase du processus (E, T, L). Le même paramétrage peut s'effectuer dans un fichier XML pour un traitement en batch. Nous avons adapté le schéma classique de l'ETL dans l'environnement MapReduce. Ainsi, P-ETL procède en cinq phases : (E)xtracting, (P)artitioning, (T)ransforming, (R)educing et (L)oading ; au lieu d'ETL, on parle plutôt d'EPTRL.
Les bases de P-ETL
Nous présentons dans cette section les bases et les principes fondamentaux de P-ETL en exposant les techniques de partitionnement supportées, l'adaptation des phases Map et Reduce aux spécificités de l'ETL et nous terminons par l'architecture globale de P-ETL.
Partitionnement des données
Dans le but de distribuer/paralléliser le processus ETL, les données sources doivent être elles aussi distribuées pour permettre à plusieurs tâches de s'exécuter de façon parallèle où chacune traite sa propre partition de données. P-ETL offre trois types de partitionnement. Afin d'assurer une charge plus ou moins équitable entre les différentes tâches parallèles, le choix du type de partitionnement est important. La présence d'un taux élevé, dans une partition de données, de tuples avec des valeurs creuses implique une charge faible en termes de traitement pour la tâche. En effet, les tuples en question seront rejetés par un filtre tel que NOT NULL.
Simple : étant donné un volume de données source v, le type de partitionnement simple génère des partitions égales selon l'équation 1 où nb_part étant le nombre de partitions.
Round Robin (RR) : Avec la technique Round Robin, l'affectation d'un tuple depuis le volume source v vers une partition de données p est basée sur l'équation 2. rang (tuple) étant le rang du tuple dans le volume v et nb_part étant le nombre de partitions.
Round Robin par Bloc (RRB) : Cette technique est similaire à Round Robin. Dans le but d'accélérer le partitionnement, un bloc de tuples est affecté à la partition, plutôt qu'une affectation tuple par tuple.
Les mappers et les reducers
Dans la plateforme P-ETL, les primitives map() et reduce() ont été adaptées aux spécificités de l'ETL. Le rôle assigné à un mapper est la normalisation des données (nettoyage, filtrage, conversion, ...). Le mapper traite chaque row dans un tunnel de transformations (T 1 , T 2 ...) où chaque T i est chargé d'une opération particulière telles que le nettoyage, filtrage, projection, conversion et concaténation. La figure 1  
Architecture de P-ETL
La plateforme P-ETL est organisée en cinq modules : (E)xtracting, (P)artitioning, (T)ransforming, (R)educing et (L)oading (Figure 3). Après extraction (E), les données sources sont chargées dans le système de fichier distribué de Hadoop (HDFS). Ensuite, un partitionnement logique des données (P) s'effectue selon le choix de l'utilisateur final (Simple, RR, RRB). Les partitions des données ainsi générées seront soumises au processus MapReduce. Chaque mapper est en charge de transformer (T) les données de sa partition (nettoyage, filtrage, conversion, ...).
Les fonctions de fusion et d'agrégation des données sont différées pour être exécutées dans la phase Reduce (R). A ce niveau, les données deviennent pertinentes et peuvent alors être chargées (L) dans l'entrepôt de données.
FIG. 3 -Architecture de P-ETL.
Paramétrage d'un processus dans P-ETL
Le paramétrage d'un processus se fait sur une interface unique organisée en trois onglets (Extract, Transform, Load) dédiés au processus lui-même (workflow), plus une partie "Advanced Parameters" réservée pour la configuration de l'environnement parallèle/distribué de Apache Hadoop (Figure 4). Pour la configuration du processus, l'utilisateur doit commencer par l'onglet "Extract". Les paramètres disponibles sur les deux autres onglets "Transform" et "Load" dépendent du premier, principalement du format de la source, sa structure et son emplacement. En revanche, la partie "Advanced Parameters" peut être utilisée indépendamment des trois onglets.
FIG. 4 -Interface de configuration P-ETL.
Dans l'onglet "Extract", l'utilisateur doit, en premier lieu, localiser les données sources. Le format pivot des données est le fichier csv qui est adopté par toutes les plateformes MapReduce vu sa légèreté (absence de méta-données). Afin d'accélérer le chargement des données sources dans le système HDFS, l'utilisateur pourra activer la compression de celles-ci. Ensuite, l'utilisateur doit choisir un type de partitionnement des données sources (simple, Round Robin, Round Robin by block) ainsi que le nombre de partitions (égal au nombre de mappers). Enfin, il doit paramétrer le mode de lecture du mapreader à partir des partitions de données (Ligne par ligne, nombre de lignes, taille en KO). La figure 4 montre l'onglet "Transform" qui fournit à l'utilisateur une liste de fonctions de transformation et d'agrégation. Chaque fonction insérée doit être paramétrée (entrées, conditions, expressions, ...). Le tunnel de transformation ainsi constitué s'exécutera dans l'ordre d'insertion des fonctions. Enfin, l'onglet "Load" permet la configuration du chargement des données préparées dans l'entrepôt de données et comporte la destination des données (entrepôt de données, magasin de données, cube de données), la compression des données avant leur chargement dans le système HDFS ainsi que le caractère séparateur du fichier csv cible. Concernant le paramétrage de l'environnement parallèle/distribué dans Apache Hadoop (taille d'un bloc HDFS, nombre de noeuds impliqué dans le processus, taille mémoire réservée à un noeud et à une tâche, compression des résultats des mappers avant de les soumettre aux reducers, ....), celui-ci se fait dans la partie "Advanced Parameters".
Expérimentation
Pour évaluer P-ETL, nous avons installé un cluster de 19 machines, chacune possède un processeur intel-Core TMi3-3220 CPU@3.30 GHZ x 4 processor, 4GO RAM et 500 GO d'espace disque. Le réseau local (LAN) est un Ethernet 100 Mbps. Selon la configuration matérielle présentée ci-dessus, l'environnement Hadoop permet d'affecter, au maximum, deux tâches parallèles à un même noeud. Ainsi, deux tâches parallèles sont équivalentes à un noeud dans ce qui suit.
Données de test :
Nous avons développé un programme qui génère des données synthé-tiques relatives aux renseignements des étudiants. Les expérimentations ont été réalisées sur des jeux de données allant de 244 * 10 6 à 7, 317 * 10 9 tuples. Nous exposons, dans ce qui suit, l'expérimentation réalisée sur un fichier etudiant.csv de 7, 317 * 10 9 lignes où chacune a une taille de 44 octets. Ce fichier contient les attributs suivants : Matricule , Date d'inscription, Cycle (Licence, Master, Doctorat), Spécialité, Bourse et Sport. Le processus ETL paramétré pour l'expérimentation se présente en quatre fonctions. La première tâche est la projection qui consiste à exclure les attributs Bourse et Sport. La deuxième tâche du processus est une restriction qui filtre les tuples et rejette tous ceux présentant une valeur Null dans l'un des attributs : Date d'inscription, Cycle, et Spécialité. La troisième tâche est Year() qui extrait l'année à partir de la date d'inscription. Enfin, la quatrième tâche est une fonction d'agrégation COUNT() qui compte le nombre d'étudiants inscrits durant la même année, dans le même cycle et la même spécialité. Il est à noter que durant l'exécution du processus, lorsque P-ETL rencontre une agrégation, comme COUNT() dans ce cas, celle-ci sera différée pour être exécutée dans la phase Reduce.
Résultats : Comme le montre le tableau 1, l'augmentation des tâches parallèles améliore de manière significative les performances du processus. Nous remarquons que le gain de temps entre 24 et 30 tâches est très intéressant (50 mn). En revanche, nous constatons une régression du gain entre 30 et 38 tâches (6 mn). Nous pourrons conclure qu'au delà d'un certain seuil en termes de tâches parallèles, l'amélioration des performances des processus sous P-ETL ne devient plus significative. 
Conclusion
Le processus ETL est considéré aujourd'hui comme étant le coeur du système décisionnel puisque toutes les données destinées pour l'analyse transitent par celui-ci. Afin de faire face aux données massives, nous l'avons adapté selon le paradigme MapReduce pour permettre son exécution dans un environnement parallèle et distribué. P-ETL est basé sur une interface de paramétrage conviviale afin de rendre la plateforme accessible aux utilisateurs finaux. Les résultats des expérimentations montrent une meilleure scalabilité de P-ETL face à des volumes de données importants lorsque la taille du cluster augmente.

Introduction
Dans le cadre de l'édition de manuscrits anciens, le rôle de l'éditeur consiste à reconstruire le plus fidèlement possible le manuscrit original à partir des différentes versions du texte disponible. Pour cela, l'éditeur classe les différentes versions du texte afin d'obtenir un arbre généalogique de cette filiation que l'on nomme stemma codicum (cf. Fig1).
FIG. 1 -Stemma de De Nuptiis Philologiae et Mercurii établi par Danuta Shanzer.
La reconstruction généalogique par un arbre suppose que chaque copiste n'a utilisé qu'un seul manuscrit pour réaliser son exemplaire (filiation unique). Malheureusement, il arrive qu'un exemplaire ait été copié, non à partir d'une seule source, mais sur plusieurs manuscrits existants. On parle alors de contamination ou de corruption.
Nous proposons dans cet article, de représenter une tradition contaminée à l'aide une construction pyramidale basée sur la notion de manuscrits intermédiaires.
Une des modélisations philologiques utilisées par les éditeurs pour reconstruire le stemma codicum est du à Don Quentin (1926). Elle permet de retrouver, à partir du corpus, des triplets de manuscrits dont l'un est l'intermédiaire des deux autres et que nous nommons T3M. Nous utilisons alors ces triplets T3M pour bâtir une pyramide généalogique.
Les triplets T3M sont déterminés par un indice qui est nul. L'expérience montre néanmoins que le nombre de triplets T3M obtenus est très faible. Dans des données réelles, la contamination, les erreurs de saisie, etc., empêchent de respecter strictement les conditions de Don Quentin. Nous allons donc être amené à relâcher les conditions strictes de Don Quentin pour créer des triplets T3M-souples où l'on impose à l'indice, non plus d'être nul, mais d'être infé-rieur à un seuil, seuil déterminé par l'éditeur en fonction du corpus.
Pour construire une pyramide à partir des triplets, définissons quatre propriétés : -l'intermédiarité permet à un ensemble de triplets T3M de respecter la transitivité ; -La couverture impose que tous les manuscrits appartiennent au moins à un triplet T3M afin de pouvoir les situer sur la pyramide finale. -La compatibilité consiste pour un ensemble de triplets T3M à être représentable sur une pyramide (cf. Defays (1979)  
Summary
In this paper we present a new codicum stemma visualization method. Don Quentin's modeling is usec to classify the textual tradition. We supplement the genealogical editor's information of betweenness triplets obtained directly from the corpus. A pyramid depicting the family codicum stemma is then constructed on the basis of information obtained by the triplets

Introduction
L'olfaction, ou la capacité de percevoir des odeurs, est le résultat d'un phénomène complexe : une molécule s'associe à un récepteur de la cavité nasale, et provoque l'émission d'un signal transmis au cerveau qui fait ressentir l'odeur associée [Sezille et Bensafi (2013)-Meierhenrich et al. (2005]. Si les phénomènes qui caractérisent les sens de l'ouïe et de la vue sont bien connus, la perception olfactive n'est, encore aujourd'hui, toujours pas comprise dans sa globalité. Cependant, on dispose de nombreux atlas (comme celui d'Arctander (1969)) qui renseignent les qualités perçues par l'humain pour des milliers de molécules odorantes : des experts senteurs associent à des milliers de molécules odorantes des qualités d'odeurs (fruité, boisé, huileux, etc : un vocabulaire bien défini et consensuel). On dispose également maintenant d'outils capables de calculer des milliers de propriétés physico-chimiques de molécules 1 . Il a alors pu être montré que ces propriétés déterminent la (les) qualité(s) d'une odeur perçue [Khan et al. (2007) -Kaeppler et Mueller (2013)]. Ce lien entre le monde physico-chimique et le monde du percept olfactif a été mis en évidence à l'aide de méthodes d'analyse en composantes principales démontrant, à partir de données, la corrélation existante entre ces deux mondes. Les neuroscientifiques ont donc maintenant besoin de méthodes descriptives afin de comprendre les liens entre propriétés physicochimiques et qualités.
La découverte de régularités (ou descriptions) qui distinguent un groupe d'objets selon un label cible (souvent appelé label de classe), est un problème qui a fédéré diverses communautés en intelligence artificielle, fouille de données, apprentissage statistique, etc. En particulier, la découverte supervisée de règles descriptives de type description ?? label est étudiée sous divers formalismes : découverte de sous-groupes, fouille de motifs émergents, ensembles contrastés, hypothèses, etc. (Novak et al. (2009)). Dans tous les cas, nous faisons face à un ensemble d'objets associés à des descriptions (dont l'ensemble forme un ensemble partiellement ordonné), et ces objets sont liés à un ou plusieurs labels de classe.
Dans cet article, on s'intéresse à la découverte de sous-groupes (subgroup discovery), introduite par Klösgen (1996) et Wrobel (1997. Étant donné un ensemble d'objets décrits par un ensemble d'attributs, et chacun associé à un (ou plusieurs) label(s) de classe, un sousgroupe est un sous-ensemble d'objets statistiquement intéressant par sa taille et ses singularités au sein de l'ensemble d'objets initial vis à vis d'un ou plusieurs labels cibles. En fait, il existe deux familles principales de méthodes. La première (Wrobel, 1997) vise à trouver des règles de type description ? label où le conséquent est un unique label. La seconde, la fouille de modèles exceptionnels (exceptional model mining, EMM) introduite par Leman et al. (2008), vise à trouver des sous-groupes dont la répartition d'apparition de tous les labels diffèrent grandement dans le sous-groupe comparé à toute la population, i.e. de la forme description ? {(label 1 , valeur 1 ), ..., (label k , valeur k )} où k est le nombre de labels de l'attribut cible. Dans les deux cas, on veut optimiser une mesure de qualité pour distinguer au mieux le sous-groupe en fonction du label, ou d'une distribution des labels dans le sous-groupe (i.e. le modèle).
En olfaction cependant, une molécule est associée à une ou plusieurs qualités d'odeurs : aucune des approches existantes ne permet de se focaliser sur un sous-ensemble de labels de cardinalité arbitraire. Effectivement, ces approches permettent soit de caractériser un seul label de classe par des sous-groupes, soit de trouver des sous-groupes qui caractérisent tous les labels de classes à la fois. Alors, d'une part, un sous-groupe effectue une caractérisation trop locale, trop spécifique et d'autre part la caractérisation est beaucoup trop globale.
Nous cherchons alors à découvrir des sous-groupes comme des règles descriptives de type description ? {label 1 , label 2 , ..., label l } où l << k. Pour cela, nous proposons une nouvelle méthode appelée ElMM (Exceptional local Model Mining) qui généralise à la fois la méthode de sous-groupes classiques ainsi que EMM. Nous montrerons alors que les sousgroupes extraits sont plus caractéristiques de peu de qualités à la fois, et donc aussi plus faciles à interpréter par l'expert en olfaction.
La suite de cet article est organisée comme suit. Tout d'abord, nous introduisons les deux principales méthodes de découverte de sous-groupes en section 2 (subgroup discovery et exceptionnal model mining). Nous montrons alors les limites de ces deux types d'approche avant d'introduire en section 3 notre nouvelle méthode : la découverte de modèles exceptionnels locaux (exceptional local model mining). Un algorithme de découverte est présenté en section 4 et appliqué à des données issues des domaines de la neuroscience et de l'olfaction (section 5). 2) présente ce jeu de données dans le cas où la fonction class n'associe à chaque molécule qu'un seul label de C -mono-qualité-(resp. un sous-ensemble de labels -multi-qualités-).
Un sous-groupe peut être représenté formellement de manière duale soit en intension soit en extension, c'est-à-dire, soit par une description dans un langage donné mettant en oeuvre des restrictions sur le domaine de valeurs des attributs, soit par l'ensemble d'objets qu'il décrit. Il existe plusieurs langages de description possibles, basés sur différents types de connecteurs logiques (conjonctions, disjonctions, ou encore négations), dont certains sont très expressifs (voir par exemple Galbrun et Kimmig (2014)). Dans la suite nous utiliserons un langage basé uniquement sur des conjonctions.
Définition 2 (Sous-groupe). On note d = 1 , . . . , f |A| la description d'un sous-groupe où chaque f i est une restriction sur le domaine de l'attribut a i ? A (à un sous-ensemble du domaine de a i s'il est nominal, ou à un intervalle s'il est numérique). Chaque restriction peut être assimilée soit à un ensemble (dans le cas d'une restriction sur un attribut nominal), soit à un intervalle dont les bornes appartiennent à Dom(a i ) (dans le cas d'un attribut numérique).
Relation d'ordre partiel entre les sous-groupes. 
Exemple (suite). On a d 1 = W ? 151.28, 23 ? nAT avec pour support l'ensemble des molécules {24, 48, 82, 1633} : la molécule 1 ne vérifie pas la restriction sur l'attribut nAT alors que la molécule 60 ne vérifie pas celle sur M W . Pour plus de lisibilité, lorsque l'on ne précise pas une restriction f i dans une description d cela signifie qu'aucune restriction effective n'est appliquée sur l'attribut a i dans d. La description d 2 = W ? 151.28, 23 ? nAT, 10 ? nC est une spécialisation de d 1 car d 2 comporte les mêmes restrictions que d 1 plus une restriction sur un autre attribut. Réciproquement, d 1 est une généralisation de d 2 .
Étant donné un jeu de données, il y a potentiellement 2 |O| sous-groupes, il est donc néces-saire de n'en sélectionner qu'une partie en fonction de leur intérêt. Pour cela, les différentes approches de l'état de l'art utilisent une mesure de qualité qui évalue la singularité du groupe au sein de la population par rapport à une cible, c'est-à-dire l'attribut de classe. La mesure de qualité est choisie en fonction du type de données, mais aussi en fonction de l'attribut de classe et de la finalité de l'application. Il existe deux approches pour la découverte de sous-groupes : l'approche que l'on va définir comme classique (Wrobel, 1997), et l'approche d'Exceptional Model Mining (EMM) introduite par Leman et al. (2008).
Dans la première, chaque objet n'est associé qu'à un et un seul label de l'attribut de classe, c'est-à-dire ?o ? O, class(o) = c avec c ? Dom(C), et la mesure de qualité permet de mettre en évidence la singularité d'un sous-groupe relativement à un seul label de C. Pour un sous-groupe de description d, une mesure généralement utilisée relativement au label l est :
les proportions d'objets du sous-groupe et du jeu de données entier possédant la classe l. Cette mesure est une généralisation de la mesure WRAcc (? = 1) qui prend en compte à la fois la taille du sous-groupe et aussi sa singularité dont le rapport entre les deux est pondéré par un facteur ?.
Dans le cas d'EMM, un objet est associé à un sous-ensemble de labels de classe, c'est-à-dire, ?o ? O, class(o) ? Dom(C). La mesure de qualité utilisée dans ce cas permet de mettre en évidence la singularité d'un sous-groupe relativement à tous les labels de C à la fois. Une mesure possible est la somme des divergences de Kullback-Leibler pour tous les labels de classe entre les objets du sous-groupe et ceux du jeu de données entier :
Exemple (suite). Avec la description d 1 = W ? 151.28, 23 ? nAT dans la Table 1, en utilisant la mesure de l'équation (1) avec
(1/2?1/3) = 2/3. Dans la Table 2, en utilisant la mesure de la formule de l'équation (2), on a W KL(d 1 ) = 4/6 × ((2/4 log 2 3/4) + (3/4 log 2 3/2) + (3/4 log 2 3/2)) = 0.45.
Découverte de sous-groupes, limites et problème. Étant donnés D(O, A, C, class), minSupp, ? et k l'objectif est de récupérer l'ensemble des k-meilleurs sous-groupes au regard de la mesure de qualité ? choisie où la taille du support du sous-groupe est supérieure ou égale à minSupp. Pour notre domaine d'application de l'olfaction, les approches existantes (décou-verte de sous-groupes classique et EMM) ne permettent pas de répondre à la problématique posée, à savoir la caractérisation de sous-ensemble de qualités d'odeurs. Effectivement, ces approches permettent soit de caractériser un seul label de classe, c'est-à-dire une qualité olfactive, par sous-groupe, soit de trouver des sous-groupes qui caractérisent tous les labels de classes à la fois avec EMM. D'une part un sous-groupe effectue une caractérisation trop locale et spécifique, d'autre part la caractérisation est trop globale. Nous introduisons dans la suite une nouvelle méthode qui généralise ces deux approches en permettant de caractériser par un sous-groupe un sous-ensemble L de taille quelconque de labels de classe.
Découverte de modèles exceptionnels locaux : ElMM
Soit D(O, A, C, class) un jeu de données conforme à la Définition 1, avec Dom(C) = {l 1 , . . . , l k }. Étant donnée une mesure de qualité ?, notre méthode ElMM recherche des sousgroupes de la forme (d, L) où d est la description d'un sous-groupe et L ? Dom(C) est un sous-ensemble de labels de la classe C à caractériser. Cette méthode correspond au cas général de la découverte de sous-groupes. Effectivement, si on fixe pour tous les sous-groupes que L ? Dom(C) on se ramène au cas de la découverte de sous-groupes classique dans lequel un sous-groupe ne caractérise qu'un label de classe. De plus si L = Dom(C) alors on bascule dans le cas d'EMM où chaque sous-groupe doit caractériser tous les labels de classe à la fois. ElMM permet donc de caractériser des sous-ensembles de labels de classe par des sous-groupes appelés sous-groupes locaux. 
La contrainte (i) permet de ne considérer que les sous-groupes dont le support est supérieur ou égal à un seuil minSupp, évitant ainsi d'obtenir des sous-groupes de trop petite taille qui n'auraient alors aucun intérêt et facilitant l'exploration. La contrainte (ii) permet d'interagir sur le langage de la description en restreignant le nombre maximal de restrictions effectives par description à un seuil maxDesc (|d| est le nombre de restrictions effectives de d). De manière similaire, la contrainte (iii) permet de limiter le nombre de labels à discriminer dans L.
Mesure de qualité. La mesure de qualité utilisée dans EMM dont la formule a été donnée dans l'équation 2 peut être généralisée pour ElMM pour ne considérer qu'un sous-ensemble L ? Dom(C) de labels de classe et non plus l'ensemble complet de labels à la fois :
Cependant, cette mesure de qualité ne correspond pas à l'objectif de notre contexte applicatif car elle ne quantifie pas les labels de L ensemble, c'est-à-dire de manière conjointe, lorsqu'ils sont associés conjointement aux objets. Nous cherchons à caractériser l'ensemble des objets cohérents qui possèdent tous les labels de L, et non pas un sous-ensemble de L. Pour cela, nous nous sommes tournés vers une mesure de qualité usuellement utilisée en classification supervisée : la F 1 -Mesure. Cette mesure nous permet dans notre cas de quantifier la pureté d'un sousgroupe vis à vis des labels à caractériser L, i.e. les objets du support de la description du sousgroupe doivent être le plus possible associés à L (la précision) et les objets associés à L dans D doivent être au maximum inclus dans le support du sous-groupe (le rappel). La F 1 -Mesure se base sur le rappel et la précision d'un sous-groupe vis à vis du sous-ensemble L à caractériser. Pour un sous-groupe local (d, L), on note :
On remarque alors que la F 1 -Mesure est comprise entre 0 et 1 puisque la précision et le rappel sont compris aussi entre 0 et 1. Plus la valeur de F 1 (d, L) est proche de 1 plus le sous-groupe (d, L) caractérise spécifiquement le sous-ensemble de labels L.
Exemple. Afin d'illustrer la méthode ElMM, nous reprenons l'exemple de la Table 2. Soit le sous groupe (d, L) avec d 1 = W ? 151.28, 23 ? nAT et L = {M iel, V anillé} le sous-ensemble de labels de classe à caractériser, en appliquant la formule de l'équation 3 afin de calculer la mesure de qualité par la F 1 -Mesure on trouve : 
Découverte de sous-groupes locaux avec ELMMUT
Dans cette section nous présentons l'algorithme ELMMUT qui répond au problème d'Exceptional local Model Mining (ElMM). Tout d'abord, nous caractérisons l'espace de recherche des sous-groupes. Ensuite, nous décrivons la manière de parcourir cet espace pour produire les sous-groupes en illustrant le pseudo-code de ELMMUT.
Espace de recherche. L'espace de recherche correspond à l'ensemble de tous les sous-groupes locaux, partiellement ordonnés. Un sous-groupe local
Ainsi, l'espace de recherche correspond à un treillis dans lequel chaque sous-groupe local est un noeud et le lien entre deux noeuds dénote que le noeud de niveau i+1 est une spécialisation du noeud de niveau i par ajout d'une nouvelle classe à caractériser, ou par spécialisation d'une restriction de la description. L'élément le plus général du treillis correspond au sous-groupe local vide que l'on note ( ?) en omettant les f i car aucune restriction n'est effectuée pour tout attribut a i :
Parcours heuristique de l'espace de recherche. L'algorithme ELMMUT effectue un parcours en profondeur de l'arbre de recherche en partant du plus général (le sous-groupe local vide à la racine de l'arbre) vers le plus spécifique. Le principe algorithmique est donné dans l'Algorithme 1. Pour chaque sous-groupe d'un noeud de l'arbre de recherche, ELMMUT essaie de le spécialiser par une extension de description ou de labels tant que la mesure de qualité est améliorée (fonction Spécialiser) (Galbrun et Kimmig, 2014). Cependant, il existe dans le pire des cas |Dom(C)| + (|A| × n(n + 1)/2) possibilités pour spécialiser un sous-groupe, puisque on peut effectuer jusqu'à |Dom(C)| extensions de labels et |A| extensions de description pour lesquelles on peut construire n(n + 1)/2 intervalles possibles (si l'attribut possède n valeurs différentes). Afin de pallier à ce problème d'espace de recherche, nous nous sommes tournés vers une approche heuristique utilisée dans la découverte de sous-groupes et dans la fouille de redescriptions, il s'agit d'une approche de type "beam-search" (recherche par faisceau) (Lowerre, 1976). Cette approche permet d'explorer seulement une partie des branches de l'arbre de recherche : à chaque spécialisation, seulement une partie des possibilités (au maximum beamW idth) de spécialisation du sous-groupe va être analysée (cf. ligne 11 de Spécialiser). Optimisation des intervalles à la volée. Pour les attributs numériques, une simple discrétisa-tion en prétraitement n'est pas suffisante. Cette approche est cependant utilisée dans une partie des expérimentations afin de pouvoir se comparer équitablement à l'algorithme de référence pour EMM (DSSD Diverse Subgroup Set Discovery introduit par van Leeuwen et Knobbe (2012)), qui utilise une telle discrétisation. Afin d'obtenir des résultats les meilleurs possibles, le choix des bornes de l'intervalle pour un attribut a ? A doit se faire à la volée pour tenir compte des spécificités d'un sous-groupe particulier. Le choix des bornes de l'intervalle est alors déterminant. Tester toutes les possibilités d'intervalles n'est pas envisageable car cette méthode peut s'avérer beaucoup trop gourmande en ressources (complexité théorique d'ordre n 2 pour n valeurs différentes). Afin de pallier ce problème, nous avons adopté une méthode de discrétisation proche de celle de Fayyad et Irani (1993), introduite dans la découverte de sous-groupes par Grosskreutz et Rüping (2009). La Figure 1 présente la répartition des valeurs prises par les objets du support d'un sous-groupe local (d, L) pour l'attribut a. Pour optimiser la mesure il faut éliminer du support du sous-groupe un maximum d'objets qui ne sont pas associés au sous-ensemble de labels L à caractériser. Soit S = (d, L) un sous-groupe, et a ? A un attribut à partir duquel on veut étendre d, on note {p 1 , . . . , p |a| } l'ensemble ordonné (p 1 < p 2 < · · · < p |a| ) des |a| ? |supp(S)| valeurs différentes prises par l'ensemble des objets de S pour l'attribut a. On dit alors qu'une valeur p i des valeurs prises par a est prometteuse si le nombre d'objets de supp(S) associés à L possédant la valeur p i pour a est supérieur ou égal au nombre d'objets de supp(S) non associés à L possédant la valeur p i . Sinon, on dit qu'elle est non-prometteuse. Ainsi, une valeur p i de a correspond à une borne inférieure potentielle si p i est prometteuse et p i?1 est non-prometteuse. De plus une valeur p i de a correspond à une borne supérieure potentielle si p i est prometteuse et p i+1 est non-prometteuse. Ensuite il suffit de tester tous les intervalles possibles en prenant tous les couples (borne inférieure potentielle, borne supérieure potentielle) et de choisir le meilleur.
Expérimentations
Jeu de données
Nous disposons d'un atlas Arctander (1969), première base de données olfactive établie, qui sert de référence pour les neuroscientifiques. Il met en oeuvre 1 689 molécules différentes décrites par 1 704 propriétés physicochimiques numériques (leur volume, leur poids, le nombre d'atomes de carbone qu'elles contiennent, etc...) et sont associées à leur(s) qualité(s) olfactive(s) évaluée(s) par des experts. Les possibles discussions quant à l'obtention de cet atlas, et notamment pour les qualités olfactives, ne sont pas abordées dans ce papier comme il s'agit d'un problème traité par les neuroscientifiques en amont. L'atlas étant clairement multi-labels, on associe chaque molécule à un sous-ensemble de qualités olfactives. En moyenne, chaque molécule est associée à 2.88 qualités olfactives.
A partir de cet atlas, nous avons construit deux jeux de données différents. Dans le premier jeu de données D 1 , on ne considère que 43 attributs (propriétés physicochimiques) de l'atlas Arctender, alors que dans le second jeu de données D 2 on en considère 243. La sélection des 43 attributs de D 1 a été faite sur recommandation de l'expert qui assure que ces attributs doivent être déterminants pour la caractérisation de qualités d'odeurs. Les 243 attributs du jeu de données D 2 ont quant à eux été sélectionnés par une analyse de non-corrélation des attributs.
Résultats quantitatifs
Tout d'abord afin de juger de la performance de l'approche que nous avons mise en place, considérons l'aspect quantitatif des résultats sur le jeu de données d'olfaction. Nous avons exécuté les expérimentations sur une machine avec 8Go de RAM et un processeur cadencé à 3.10GHz. Les résultats quantitatifs obtenus ont été réalisés sur les deux jeux de données Algorithm 1 ELMMUT.
Entrée : O, A, C, class, ?, k, beamW idth, minSupp, maxDescr, maxLab Sortie : L'ensemble des sous-groupes locaux R 1: R ? ? 2: for all c ? Dom(C) do 3:
Vérifier et Mettre à jour les contraintes pour ( {c}) 5:
for all a ? A do 6:
f ? Choisir restriction sur a 7:
Vérifier et Mettre à jour les contraintes pour ( {c}) 8:
Ajouter ( {c}) à T emp 9:
end for 10:
T emp ? Conserver les k-meilleurs sous-groupes locaux de T emp 11:
for all (d, L) ? T emp do 12:
Ajouter Spécialiser(d, L) à R 13:
end for 14: end for
Ajouter (d, L ? {c}) à T emp 5: end for 6: for all a ? liste des attributs candidats de (d, L) : A Cand do 7:
f ? Choisir restriction sur a 8:
Vérifier et Mettre à jour les contraintes pour
Ajouter (d ? {f }, L) à T emp 10: end for 11: T emp ? Conserver les beamW idth meilleurs sous-groupes locaux de T emp 12: for all (d, L) ? T emp do 13: Figure 2 présente les différents temps d'exécution de notre approche pour la version de l'algorithme sans la discrétisation à la volée pour les attributs (une discrétisation par effectifs égaux est réalisée a priori pour chaque attribut numérique, comme cela est fait par la méthode EMM). Les trois courbes sont relatives au jeu de données D 1 . On remarque que plus on augmente la taille maximale autorisée pour la description (maxDescr) ou pour le sous-ensemble de labels à caractériser (maxLab), plus le temps d'exécution est long ce qui est tout à fait compréhensible puisque l'algorithme cherche à étendre le plus possible les sousgroupes tant que la mesure de qualité est améliorée. On remarque cependant qu'à partir de maxDescr = 15 le temps d'exécution est sensiblement semblable ce qui signifie que même si la taille maximale autorisée augmente les sous-groupes ont une description dont la taille ne va pas au-delà d'un certain seuil : la mesure ne peut plus être améliorée en les étendant. Ce résultat semble être causé à la fois par le paramètre minSupp et par le jeu de données. Effectivement, plus les descriptions sont étendues plus le support a une taille qui tend à diminuer, et puisque l'algorithme est déterministe, avec ce jeu de données, on ne peut excéder une description de taille 15. De même pour la taille maximale du sous-ensemble de labels à caractériser, à partir de 2 ou 3 le temps d'exécution reste le même, ce qui concorde avec le fait qu'en moyenne une molécule est associée à 2.88 qualités olfactives (au-delà de n > 3 qualités olfactives, le nombre de molécules partageant l'ensemble de ces mêmes n qualités olfactives est trop faible et la contrainte du support minimale n'est pas respectée). La Figure 3 présente l'impact du jeu de données et de la discrétisation à la volée via notre technique. Clairement, le nombre d'attributs est un facteur crucial pour l'algorithme ELMMUT, on observe la présence d'un facteur 10 entre le temps d'exécution sur D 1 avec 43 attributs et celui sur D 2 avec 243. L'utilisation de la discrétisation à la volée ne semble pas passer à l'échelle lorsque l'on augmente la taille des descriptions : à partir d'une valeur de 15 pour maxDescr l'exécution dure plus de 12 heures et a donc été avortée. Nous prévoyons des techniques d'optimisation dans le futur.
Résultats qualitatifs
L'interprétation des résultats est un point central dans le cadre de notre application. Les règles descriptives que nous avons mises en place doivent être capables d'informer et d'aiguiller les neuroscientifiques dans leur recherche. Notre approche, ElMM, en ne caractérisant qu'un sous-ensemble de labels de classe permet alors de correspondre au cas pratique à savoir qu'une molécule ne possède en moyenne que 2.88 qualités olfactives. En observant la Figure 4 qui présente la distribution des qualités au sein du jeu de données entier et d'un sous-groupe obtenu par la méthode EMM, on s'aperçoit clairement que l'interprétation d'un tel résultat est très difficile. On constate des différences entre les distributions du sous-groupe et du jeu de données initial mais cette différence est présente sur beaucoup trop de qualités olfactives à la fois et ainsi l'interprétation d'un tel résultat pour la déduction d'une règle descriptive est infaisable pour un neuroscientifique. La Table 3 présente les 5 meilleurs sous-groupes (du point de vue de la mesure F 1 ) obtenus après suppression des motifs redondants (on utilise ici la même méthode que Galbrun et Kimmig (2014)). Ces sous-groupes sont issus de la base de données D 1 lorsque la discrétisation à la volée est activée avec maxDescr = 10, maxLab = 2 et minSupp = 30. Seulement un sous-groupe caractérisant plusieurs labels de classe (Floral et Balsamique) est présent, avec une mesure de 0.33 et un support de 38. Sa description contient 9 restrictions. Des sous-groupes ont aussi des descriptions plus courtes. La taille des supports est variable. De plus, dans le jeu de données D 2 , lorsque la discrétisation à la volée est désactivée et que maxDescr = 15, maxLab = 3 et minSupp = 30, on obtient 74.6% de sous-groupes dont le sous-ensemble de labels est de taille 1, 22.9% de taille 2 et 2.5% de taille 3. 
Conclusion
Nous avons présenté la découverte de motifs exceptionnels locaux, une nouvelle méthode de fouille de règles descriptives qui généralise les approches existantes, pour caractériser spé-cifiquement un sous-ensemble de labels de classe. Nous l'avons appliquée au cas concret de l'olfaction afin de mettre en évidence les liens existant entre les propriétés physicochimiques d'une molécule et ses qualités olfactives. Le pouvoir d'interprétation des résultats et l'information qu'ils véhiculent, permettent d'entrevoir une évolution de la connaissance à propos du phénomène complexe qu'est l'olfaction. De nombreuses expérimentations restent à faire et nous envisageons une exploration interactive inspirée par Galbrun et Miettinen (2012).
Références Arctander, S. (1969). Perfume and flavor chemicals :(aroma chemicals), Volume 2. Allured Publishing Corporation.

