[
  {
    "id": "230",
    "text": "Introduction\nUn système de recherche d\u0027information (SRI) est un module logiciel qui sélectionne, à partir d\u0027une collection de documents, une liste de documents potentiellement pertinents en réponse à une requête utilisateur. Le processus suivi par un SRI est composé de 3 étapes.\nIndexation. Cette étape permet de passer d\u0027un document textuel à un document qui peut être utilisé dans la RI. Elle se base sur l\u0027extraction des mots les plus importants des textes. Lors de cette étape, les mots vides tels que le, la, les sont généralement éliminés ; les termes sont ensuite racinisés, c\u0027est-à-dire que des règles de transformation sur les termes sont appliquées afin d\u0027obtenir un radical, limitant les variantes des termes à une forme unique ; enfin une pondération reflète l\u0027importance des différents radicaux obtenus. Dans un cadre non dynamique, l\u0027indexation est réalisée sur l\u0027ensemble des documents, avant toute recherche.\nCalcul des scores de pertinence des documents. Lorsqu\u0027une requête est soumise au système, des scores de pertinence sont attribués aux termes qui la composent, en tenant compte de leur présence dans les documents. Ces scores sont ensuite combinés pour calculer le score global de chacun des documents de la collection. Il existe de nombreux modèles de pondéra-tion. La plupart sont basés sur les facteurs T F et IDF . L\u0027expression T F (Term Frequency) correspond à la fréquence du terme dans le document, tandis que l\u0027IDF (Inverse Document Frequency) désigne la fréquence inverse du terme dans le document, inversement proportionnel au nombre de documents qui contiennent le terme.\nReformulation de la requête. Cette étape permet de créer une requête plus adéquate à la RI que celle initialement formulée par l\u0027utilisateur. Le principe de la reformulation automatique est de modifier la requête de l\u0027utilisateur en ajoutant des termes significatifs ou en ré-estimant leurs poids. Dans sa version automatique, il s\u0027agit de considérer les premiers documents restitués comme pertinents et d\u0027ajouter des termes issus de ces documents ; de nouveaux poids sont également estimés et les scores des documents recalculés pour fournir la réponse finale du système. Ce paramètre n\u0027est pas étudié dans le travail présenté dans cet article.\nChacune de ces étapes fait intervenir différents paramètres : par exemple lors de l\u0027indexation, il est possible de choisir entre différents outils de racinisation, lors du calcul des scores de pertinence des documents, différents modèles de pondération peuvent être choisis. Les différents paramètres étudiés dans cet article sont présentés en figure 1.\nL\u0027efficacité d\u0027un SRI est évaluée en calculant des mesures de performance comme le rappel, la précision et d\u0027autres mesures associées. Depuis ses débuts, le domaine de la RI est très actif pour fournir de nouvelles propositions correspondant à une évolution de ces trois étapes. Lorsqu\u0027un nouveau modèle de RI est proposé, ses paramètres sont étudiés, mais sans considé-rer les effets croisés. Par exemple dans Ponte et Croft (1998), ce sont les paramètres du modèle lui-même qui sont étudiés sans regarder l\u0027influence du choix de l\u0027algorithme de racinisation. Les modèles d\u0027apprentissage d\u0027ordonnancement (Learning to rank en anglais) considèrent de nombreux paramètres tels que les fréquences TF et IDF, la taille des documents et des caractéristiques comme les scores BM25, LMIR, PageRank des documents (Qin et al., 2010). Ces approches ont pour objectif d\u0027optimiser l\u0027ordonnancement des documents mais ne cherchent pas à connaître l\u0027impact des paramètres. Quelques travaux visent à sélectionner les variables importantes, donc à étudier leur influence (Laporte et al., 2014;Naini et Altingovde, 2014).\nAinsi, généralement, les paramètres sont étudiés de façon indépendante, sans considérer les effets croisés des paramètres. Compte tenu du nombre de paramètres, l\u0027étude des effets croisés est difficile et implique au préalable de collecter des données suffisantes pour le faire. Cet article s\u0027attaque à ce problème. Ainsi, dans cette étude, nous nous appuyons sur un ensemble de données massif (2 millions de configurations) dans lequel les différents paramètres varient.\nLa littérature du domaine ne s\u0027est que peu intéressée à une analyse de cette nature. Presque tous les articles et les thèses du domaine de la RI rapportent des études montrant la variation de mesures de performance en fonction d\u0027un ou plusieurs paramètres, mais il ne s\u0027agit pas d\u0027une analyse en parallèle de paramètres variés. Quelques travaux se sont cependant intéressés à utiliser les méthodes d\u0027analyse pour étudier les résultats de moteurs de recherche sur un ensemble de requêtes. Banks et al. (1999)   Compaoré et al. (2011) présentent une étude qui a les mêmes objectifs que ceux de ce papier. Cependant, le nombre d\u0027éléments analysés et donc les combinaisons de paramètres est bien moindre. Dans leurs études, les auteurs montrent que les paramètres qui ont le plus d\u0027influence sont différents en fonction que l\u0027on considère les besoins d\u0027information faciles ou difficiles. Bigot et al. (2014) utilisent les résultats d\u0027une analyse pour sélectionner la configuration de système la plus adaptée en fonction de la difficulté du besoin d\u0027information. L\u0027objectif de l\u0027analyse que nous présentons dans le présent papier est d\u0027étudier à grande échelle les caractéristiques des SRI dans le but de déterminer les meilleures combinaisons de paramètres selon certaines mesures de performance. Ce travail a été mené dans le cadre du projet ANR CAAS (Contextual Analysis and Adaptive Search) ANR-10-CORD-001-01.\nLa suite de cet article est structurée comme suit. La section 2 présente la méthode utilisée pour obtenir les données ainsi que les données elles-mêmes. La section 3 présente l\u0027analyse de la dépendance entre les paramètres et leur influence mutuelle. La section 4 s\u0027attache à déterminer quelles sont les valeurs de paramètres les plus susceptibles de conduire à de bonnes performances du moteur de recherche correspondant. La section 5 conclut cet article.\nVariantes de moteurs de recherche et paramétrage\nLes données ont été générées via l\u0027interface RunGeneration présentée dans Louédec et Mothe (2013) et qui est une sur-couche à la plateforme Terrier.\nTerrier et RunGeneration\nLa plateforme de RI TERRIER (Ounis et al., 2006) possède de nombreuses possibilités de paramétrage, tant au niveau de l\u0027indexation (indexation par blocs de différentes tailles, choix de la racinisation, etc.) que de la recherche (différents modèles de pondération pour la mise en correspondance entre la requête et les documents, différentes normalisations des poids) et de la reformulation de requêtes. Une fois paramétré, Terrier permet, pour un besoin d\u0027information ou un ensemble de besoins, de retrouver les documents susceptibles de répondre à ce besoin.\nL\u0027interface RunGeneration a comme objectif de faciliter le paramétrage d\u0027une chaine de traitement sous Terrier. Une fois les paramètres sélectionnés au travers de l\u0027interface, celle-ci crée le fichier \"terrier.properties\" indispensable à Terrier et contenant l\u0027ensemble des paramètres. Le second objectif de l\u0027interface RunGeneration est de permettre de lancer plusieurs combinaisons de paramètres simultanément, ce qu\u0027il n\u0027est pas possible de faire en utilisant la plateforme Terrier. Ainsi plusieurs indexations et recherches sont effectuées sur les mêmes données via une seule intervention de l\u0027utilisateur. Celui-ci peut par exemple demander en une\nFIG. 1 -Paramètres de la génération des données.\naction plusieurs indexations de documents avec des paramètres différents. L\u0027interface a été développée pour fonctionner sur les versions 3.0 et 3.5 de Terrier 1 (Louédec et Mothe, 2013). Ainsi, lors de la génération des données utiles à notre analyse, le principe est le suivant : pour chaque combinaison de paramètres, une liste de documents retrouvés en réponse au besoin d\u0027information est constituée. Cette réponse du système est alors évaluée sur la base de mesures de RI. Ainsi, pour une combinaison de paramètres, nous connaissons la valeur de chacune des caractéristiques correspondant aux paramètres du moteur et aux mesures de performance.\nParamètres utilisés lors de la génération de données\nLa figure 1 indique les variables utilisées lors de la génération des données ainsi que les modalités de ces variables paramètres. Le nombre de combinaisons obtenues est de 2 263 800. Les variables correspondant à des paramètres du système sont qualitatives.\nCollection d\u0027évaluation utilisée\nCompte tenu du nombre de combinaisons et des temps nécessaires pour générer les données, dans cette étude, nous n\u0027avons utilisé qu\u0027une seule collection de documents : la collection TREC-8 de la tâche adhoc de TREC 2 . Elle comprend environ 530 000 documents soit 2 Go ; chaque document est composé en moyenne de 532 mots. La collection comprend également 50 besoins d\u0027information et les jugements de pertinence des documents associés à ces besoins. Sur ce jeu de données, nous n\u0027avons pas pris en compte les paramètres de reformulation de requêtes afin de ne pas rendre le nombre de combinaisons possibles trop grand pour être généré. Plutôt nous avons fait l\u0027hypothèse qu\u0027une première analyse permettrait de faire ressortir les paramètres principaux qui eux pourront être combinés avec les paramètres de reformulation. En effet, les principes de reformulation (implantés dans Terrier) s\u0027appuient tous sur l\u0027utilisation des premiers documents retrouvés suite à une première recherche. Aussi, optimiser la précision dans ces premiers documents, optimise à priori la reformulation de requêtes.\nCaractéristiques d\u0027évaluation associées\nAfin d\u0027évaluer les résultats obtenus nous avons utilisé trec_eval 3 qui calcule plus de 100 mesures de performance telles que bpref , AP et P @5. Cependant, nous avons restreint les variables utilisées à celles qui sont les moins corrélées. Ainsi, nous avons conservé les 6 mesures de performance préconisées dans Baccini et al. (2012). Elles sont résumées dans la table 1. Elles sont toutes quantitatives à valeur continue et leurs valeurs sont comprises entre 0 et 1. \nDistribution des valeurs des variables de mesure de performance\nLa figure 2 montre la distribution des valeurs des variables correspondant aux mesures d\u0027évaluation de la performance. On note que le minimum 0 est atteint pour chacune des variables ; en revanche le maximum 1 n\u0027est atteint que pour la P @30 et la iprec@recall0. L\u0027ensemble de ces figures montre que les valeurs sont faibles ; la valeur 0 est la plus fréquente, montrant ainsi que beaucoup de configurations échouent dans la RI. Par ailleurs, comme les données ne suivent pas une loi normale, il faudra faire attention aux analyses réa-lisées par la suite pour ne choisir que celles qui s\u0027appliquent à des variables qui ne suivent pas une loi normale. Cependant, les tests et méthodes que nous utilisons dans la suite restent valident car nous travaillons sur un grand jeu de données.\nCorrélations entre variables de même type\nNous avons étudié la corrélation d\u0027une part entre les variables correspondant aux paramètres du SRI et d\u0027autre part entre variables d\u0027évaluation des moteurs.\nNous avons analysé le lien entre les paramètres du moteur de RI, pris deux à deux afin de savoir si certaines de ces variables paramètres ont des rôles similaires ou sont liées entre elles. Pour cela, nous avons réalisé le test du chi 2 . Soit l\u0027hypothèse H0 «les deux variables sont indépendantes» contre H1 «les deux variables ont un lien». Nous rejetons l\u0027hypothèse H0 si la p-value est inférieure à 0, 05. Après avoir réalisé le test pour chacune des variables, nous concluons que toutes les variables qualitatives sont indépendantes deux à deux. Elles ont donc chacune leur rôle spécifique.\nEn revanche, en ce qui concerne les variables quantitatives correspondant aux mesures de performance, nous avons constaté qu\u0027il existe une corrélation. Cette corrélation est plus ou moins importante en fonction des mesures que l\u0027on compare. La figure 5  Ainsi les mesures de performance, déjà réduites à 6 pour plus de 100 au départ sont assez redondantes dans leur capacité à mesurer les performances des systèmes puisque corrélées, même si elles ne mesurent pas le même phénomène. Les paramètres du système en revanche n\u0027étant pas corrélés, cela a un sens de chercher à optimiser chacun de ces paramètres.\nCorrélations entre variables paramètres et variables d\u0027évaluation\nAfin d\u0027étudier l\u0027effet des paramètres sur les mesures de performance, nous avons effectué une analyse de la variance (ANOVA).\nSoit l\u0027hypothèse H0 «Le paramètre n\u0027a pas d\u0027effet sur la mesure de performance» contre H1 «le paramètre a un effet sur la mesure de performance». Nous rejetons H0 si la p-value est \u003c 0, 05.\nNous avons étudié cette corrélation sur chacune des mesures de performance. La table 2  paramètres ont un effet significatif. Ces trois paramètres sans effet pourront donc être fixés dans la génération éventuelle d\u0027autres données. Pour être réellement exhaustif, il faudrait vérifier que ces paramètres n\u0027ont pas d\u0027influence lorsque l\u0027on change de collection, mais compte tenu de leur nature, la probabilité que cela soit le cas est forte.\nEffet significatif\nEffet non significatif TrecQueryTagsProcess, Topic BlocSize, IgnoreEmptyDocuments RetrievingModel, Stemmer IgnoreLowIdfTerms TAB. 2 -Effets significatifs et non significatifs pour l\u0027ensemble des mesures de performance.\nVariables ayant le plus d\u0027influence\nNous avons utilisé la méthode Stepwise qui est une régression linéaire multiple Bendel et Afifi (1977) pour étudier l\u0027influence relative des différentes variables paramètres. Cette mé-thode ajoute les variables les plus significatives du modèle et retire les moins significatives pas à pas. Dans le cadre de notre étude, elle a pour objectif de sélectionner les paramètres qui ont le plus d\u0027influence sur les mesures de performance. C\u0027est une combinaison de la mé-thode Forward et de la méthode Backward. La première méthode part du modèle vide et ajoute les variables les plus significatives du modèle progressivement, tandis que la seconde part du modèle complet et élimine progressivement les variables les moins significatives du modèle.\nAprès avoir réalisé cette analyse sur les différentes mesures de performance, nous observons que les trois variables supprimées sont BlocsSize, IgnoreEmptyDocuments et IgnoreLowIdf T erms, comme dans le cas de l\u0027étude des corrélations précédente.\nAu final cette méthode sélectionne donc les paramètres T opic, T recQueryT agsP rocess, RetrievingM odel et Stemmer. La variable T opic est la plus significative du modèle, suivi de T recQueryT agsP rocess, puis de RetrievingM odel et de Stemmer. Le besoin d\u0027information considéré est le paramètre dont dépend le plus les résultats. Cela est un résultat important concernant la variabilité des résultats. On aurait pu penser que le modèle de recherche utilisé pouvait être le plus important des paramètres. La formulation du besoin d\u0027information est éga-lement importante puisque le paramètre T recQueryT agsP rocess correspond aux parties du besoin d\u0027information pris en compte lors du traitement. Lorsque seul le titre est considéré, il correspond à quelques mots, taille typique des requêtes sur le web. Lorsque les autres champs sont également considérés, il peut s\u0027agir de requêtes plus longues, donnant un contexte précis du besoin d\u0027information. Cette première analyse avait pour objet de déterminer les paramètres du moteur les plus importants ou qui influencent le plus la performance d\u0027une recherche. Dans la section suivante, nous déterminons quelles sont les valeurs de paramètres les plus susceptibles de conduire à de bons résultats.\n4 Paramètres des SRI pour des classes de précision L\u0027objectif de cette analyse est d\u0027étudier les valeurs des paramètres du moteur de recherche qui peuvent être associées à des valeurs de précision. Nous nous sommes appuyés dans cette étude sur la AP qui est la mesure consensuelle lorsqu\u0027il s\u0027agit de comparer globalement plusieurs systèmes et qui est utilisée en particulier dans la campagne d\u0027évaluation TREC (trec.nist.gov) (Voorhees, 2007).\nClassification mixte\nLa classification mixte est une méthode de classification qui a pour objectif d\u0027obtenir, à partir des facteurs issus d\u0027une analyse des correspondances multiples (ACM), des classes d\u0027individus les plus cohérentes possibles en constituant les groupes les plus homogènes.\nDans notre cas d\u0027étude, nous utilisons cette méthode afin d\u0027associer à une classe de valeur de AP les paramètres de moteurs. L\u0027idée sous-jacente est de favoriser les combinaisons de paramètres qui sont plutôt associées à des valeurs fortes de AP et d\u0027éviter les combinaisons de paramètres plutôt associées à des valeurs faibles de AP .\nCette étude nécessite d\u0027appliquer une ACM, méthode qui s\u0027applique sur des variables qualitatives. Afin de transformer le paramètre d\u0027évaluation qualitatif considéré (l\u0027AP ) en valeurs qualitatives, nous avons créé des classes de valeurs de AP . Les classes ont été définies de sorte d\u0027avoir des effectifs comparables. Nous noterons dans la suite la classe map1 la classe ayant FIG. 6 -Valeur des paramètres pour les classes de AP.\nles valeurs de AP les plus faibles jusqu\u0027à map4 la classe ayant les valeurs de AP les plus fortes. La figure 6 présente les résultats du croisement entre les classes de AP et les variables paramètres selon la méthode de classification mixte.\nNous pouvons observer dans la figure 6 les modalités présentes dans chacune de ces classes. La classe 1 contient des mesures de performance à valeurs faibles, la classe 2 des mesures de performance à valeurs moyennes, la classe 3 des mesures de performance à valeurs très faibles et la classe 4 des mesures de performance à valeurs élevées.\nLa combinaison de paramètres la plus représentative pour la classe 4, c\u0027est-à-dire pour la classe ayant des mesures de performance à valeurs élevées est :\n-IgnoreEmptyDocuments \u003d TRUE -Stemmer \u003d PS -RetrievingModel \u003d LemurTFIDF -TrecQueryTagsProcess \u003d TITLE -IgnoreLowIdfTerms \u003d TRUE -IgnoreEmptyDocuments \u003d TRUE -Stemmer \u003d PS -RetrievingModel \u003d LemurTFIDF -TrecQueryTagsProcess \u003d TITLE -IgnoreLowIdfTerms \u003d TRUE La combinaison de paramètres la plus représentative pour la classe 3, c\u0027est-à-dire pour la classe ayant des mesures de performance à valeurs très faibles (map1) est :\n-IgnoreEmptyDocuments \u003d FALSE -Stemmer \u003d Crop -RetrievingModel \u003d DFI0 -TrecQueryTagsProcess \u003d NARR -IgnoreLowIdfTerms \u003d FALSE Cette combinaison de paramètres est donc à éviter, de même, que la combinaison de paramètres la plus représentative pour la classe 1, c\u0027est-à-dire pour la classe ayant des mesures de performance à valeurs faibles (map2).\nConclusions et perspectives\nDans cet article, nous nous sommes intéressés à une analyse massive de résultats de recherche d\u0027information obtenus par un paramétrage du système. Ainsi, de nombreux paramètres ont été analysés, en étudiant les effets croisés de ceux-ci. Nous avons pu distinguer les requêtes en fonction de leur niveau de difficulté et définir les paramètres qui ont le plus d\u0027influence en fonction de ces classes ainsi que leurs valeurs les plus adaptées. Un aspect qui reste à étudier est l\u0027influence de la collection sur les résultats obtenus. En effet, nous nous sommes ici intéressés à une collection unique (TREC8).\nDans la suite de ces travaux, nous allons travailler sur des méthodes sélectives de recherche d\u0027information, c\u0027est à dire des méthodes qui adaptent le traitement en fonction des cas rencontrés. Ainsi, toutes les requêtes ne seront pas traitées de la même façon par le moteur, mais les paramètres du système seront au contraire différents en fonction du type de requêtes.\nLe projet CAAS, financé par l\u0027ANR dans le cadre de l\u0027appel Contint 2010, a permis de développer le travail présenté ici. Nous remercions également Anthony Bigot et Sébastien Déjean pour leurs précieux conseils.\n"
  },
  {
    "id": "231",
    "text": "Introduction\nL\u0027analyse d\u0027opinions est une tâche de fouille de textes qui consiste en l\u0027identification et la classification des textes subjectifs en plusieurs catégories d\u0027opinions (polarités). Dans la dernière décennie, beaucoup de travaux se sont penchés sur cette problématique, en prenant le problème sous différents angles (principalement statistique et/ou linguistique). Cependant, la question de visualisation n\u0027a pas bénéficié de cet intérêt. La plupart des travaux proposent une visualisation basique (e.g., graphiques en secteurs), ce qui est clairement insuffisant dans un contexte de big data où l\u0027utilisateur a d\u0027autant plus besoin d\u0027explorer les données dans l\u0027ensemble, mais aussi dans le détail.\nDans ce travail, nous nous situons dans un contexte de veille sur le Web et nous nous intéressons au problème d\u0027analyse d\u0027opinions dans un contexte de veille. Ainsi, nous proposons une méthode de visualisation d\u0027opinions basée sur l\u0027utilisation de termes clés afin de restituer le maximum d\u0027information à l\u0027utilisateur. Notre méthode est implémentée au sein de la plateforme de veille AMIEI 1 . La section suivante présente la problématique de recherche que nous traitons. La section 3 présente le processus général de veille avec la plateforme AMIEI. La section 4 présente notre approche pour l\u0027analyse d\u0027opinions et la visualisation des résultats. Enfin, la section 5 présente un exemple d\u0027application sur un corpus de tweets politiques.\nContexte et Problématique\nL\u0027analyse d\u0027opinions est un domaine de recherche qui se concentre sur l\u0027identification et la classification des opinions dans les données textuelles. Beaucoup de travaux se sont intéressés à l\u0027une ou l\u0027autre de ces problématiques mais la plupart se sont intéressés à la classification d\u0027opinions, i.e., l\u0027association d\u0027un texte à une catégorie d\u0027opinions (e.g., opinion positive vs. négative).\nLa problématique a été majoritairement approchée sous un angle statistique et/ou linguistique. D\u0027un point de vue statistique, le texte est représenté sur l\u0027espace de descripteurs (e.g., termes) afin qu\u0027il puisse être traité par les outils d\u0027apprentissage statistique, e.g., Pak et Paroubek (2010); Pang et al. (2002). Ces méthodes sont connues pour leur généricité (bon rappel). De l\u0027autre part, les méthodes de linguistique, également appelées méthodes à base de règles, ont été largement déployées pour l\u0027analyse d\u0027opinions, e.g., Kennedy et Inkpen (2006); Wilson et al. (2005). Ces méthodes sont connues pour leur spécificité (bonne précision). Enfin, d\u0027autres travaux ont tenté de mixer la généricité de la statistique et la spécificité de la linguistique afin de proposer des méthodes à la fois robustes et précises (méthodes hybrides), e.g., Dermouche et al. (2013); Kamps et al. (2004); Turney et Littman (2003).\nDans ce travail, nous nous intéressons au problème de visualisation de l\u0027opinion. En effet, la problématique de visualisation n\u0027a pas été suffisamment étudiée dans ce domaine en se contentant de visualiser les proportions de chaque polarité d\u0027opinion sur un graphique. Cette méthode est clairement insuffisante dans le cas où l\u0027on veut savoir davantage sur ses données. Par exemple, dans le domaine industriel, il serait intéressant d\u0027identifier les idées redondantes et les concepts qui sont présents dans une catégorie d\u0027opinion et pas dans une autre. Une telle visualisation a des applications directes dans plusieurs domaines, e.g., la veille stratégique et économique, la CRM, la e-réputation, etc.\nAcquisition de l\u0027information\nCette phase permet l\u0027acquisition de l\u0027information selon plusieurs modes : -Un moteur de recherche pour faire des recherches ponctuelles pouvant être capitalisées. -Un automate de collecte pour des opérations récurrentes à des fins de capitalisation. \nCapitalisation et traitement\nPartage de l\u0027information\nLe partage et la diffusion des informations acquises et validées, ainsi que les résultats de l\u0027analyse se font à travers un portail de consultation, permettant la recherche et le partage des informations organisées par thématique avec une gestion des droits d\u0027accès à partir de profils prédéfinis. Le partage peut également se faire via \"Mon espace\" ; un module permettant de personnaliser, pour chaque utilisateur, son accès à la plateforme AMI EI.\nPour l\u0027analyse d\u0027opinions, la plateforme AMIEI offre la fonctionnalités suivantes : -Indicateurs classiques de l\u0027opinion globale dans un corpus de documents (distribution des documents sur les classes de polarité). -Visualisation des termes clés pour chaque classe de polarités. Un terme clé doit être fréquent et discriminant vis-à-vis de la classe d\u0027opinion qu\u0027il caractérise. -Evolution des termes clés à travers le temps. -Soit c la classe d\u0027opinion du document d (classe la plus probable). -Evaluer chaque terme w i du document d selon un critère de spécificité (pouvoir discriminatif du terme au regard de la classe d\u0027opinions). Ici, nous choisissons comme critère le gain informationnel (IG). Ensuite, trier les termes w i du document selon ce critère : IG(w m |c) \u003e IG(w n |c) \u003e ... \u003e IG(w p |c). -Les K premiers termes sont ceux qui \"expliquent\" le mieux cette classification. Nous précisons que les termes discriminants de deux classes différentes sont deux ensembles disjoints. En effet, un terme ne peut être responsable d\u0027affecter un texte qu\u0027à une seule classe.\nMéthode et implémentation\nVisualisation\nLa visualisation est une étape clé dans le processus d\u0027analyse d\u0027opinions, d\u0027autant plus dans un contexte de big data. En effet, l\u0027information utile est encore plus enfouie et difficile à retrouver, ce qui nécessite des techniques de visualisation efficaces et adaptées à ce contexte particulier. Dans AMIEI, nous proposons de visualiser l\u0027opinion contenue dans un corpus de FIG. 2 -Visualisation en nuage de termes (extrait).\nFIG. 3 -Visualisation en fisheye (extrait).\ndocuments par un \"nuage de termes\" construit à partir de l\u0027ensemble des termes discriminants responsables de la classification (cf. section 4.1.). Chaque terme discriminant est ainsi repré-senté par une taille proportionnelle à sa fréquence dans le corpus des textes. Nous proposons également une visualisation temporelle du nuage de terms en utilisant la technique de fisheye.\nEtude de Cas\nNous réalisons une expérimentation sur un corpus composé de 50000 tweets issus d\u0027une collecte massive réalisée par la plateforme de veille AMIEI dans la soirée du 02 Mai 2012 avec le tag \"#ledebat\" (400000 tweets collectés). Ces tweets sont relatif au débat télévisé du second tour de l\u0027élection présidentielle française de 2012 ayant opposé F. Hollande et N. Sarkozy. Nous appliquons, comme prétraitement, la suppression de mots outils et de numériques.\nLes Figures 2 et 3 représentent la visualisation du résultat d\u0027analyse du corpus Politique. Pour une meilleure lisibilité, seulement une sélection de termes fréquents est représentée ici. La polarité des termes est représentés par une couleur (vert pour le positif et rouge pour le négatif). Ces résultats nous ont permis de cerner les termes et les concepts les plus importants dans chaque catégorie d\u0027opinion. A partir de cette visualisation, nous pouvons tirer plusieurs enseignements dont voici quelques uns : -Le concept de \"changement\" dans toutes ses variantes (slogan phare de la campagne du candidat F. Hollande) est largement repris par les internautes, et ce de manière positive.\n"
  },
  {
    "id": "232",
    "text": "Introduction\nLe projet ANR IMAGIWEB 1 consiste à analyser et à suivre l\u0027évolution de l\u0027image (au sens de l\u0027opinion) sur la toile, d\u0027une part des personnages politiques à travers le réseau social Twitter, et d\u0027autre part de l\u0027entreprise EDF vis-à-vis du nucléaire en utilisant des blogs comme données. Ce projet regroupe différents partenaires parmi lesquels un laboratoire de recherche en science politique, des entreprises et des laboratoires de recherche en fouille de données.\nDans un premier temps, les tweets et blogs récoltés sont annotés manuellement pour relater l\u0027opinion qu\u0027ils véhiculent. Par la suite, l\u0027enjeu sera de détecter automatiquement les opinions grâce à des méthodes de fouille d\u0027opinion. Au-delà de la détection des opinions, pour mieux comprendre et analyser le contenu des tweets et des blogs, l\u0027enjeu est aussi de les visualiser et de les explorer. Ainsi, un autre objectif du projet consiste à fournir à l\u0027utilisateur, qu\u0027il soit politologue, sociologue, marketeur ou encore analyste, un outil pour explorer les données (issues de tweets ou de blogs) et pour analyser en ligne l\u0027opinion selon différents points de vue (sujets, temps, ...). L\u0027analyse OLAP (OnLine Analytical Processing) permet de répondre à cet objectif de navigation, d\u0027analyse et de visualisation.\nL\u0027OLAP sur des données textuelles correspond à une thématique de recherche récente avec des enjeux scientifiques importants. En effet, si l\u0027OLAP a su montrer tout son potentiel analytique sur des \"données classiques\", la prise en compte de données textuelles nécessite une adaptation ou une évolution de l\u0027OLAP pour prendre en compte les spécificités de ces données (Ravat et al., 2007;Zhang et al., 2009). Quelques travaux de recherche encore plus récents portent sur l\u0027analyse OLAP de tweets, un cas particulier de données textuelles (Ben Kraiem et al., 2014;Bringay et al., 2011). Dans ce contexte, l\u0027objectif de ce papier est de (1) démontrer l\u0027intérêt de l\u0027analyse OLAP pour ce type de données en se basant sur des cas d\u0027étude réels, (2) relater une implémentation concrète \"classique\" en utilisant des outils existants.\nPour ce faire, dans la section 2 nous commençons par présenter les deux cas d\u0027étude. Dans la section 3, nous évoquons les aspects de modélisation multidimensionnelle et de navigation. Dans la section 4, nous exposons la mise en oeuvre, avant de conclure dans la section 5.\nDeux cas d\u0027étude\nDans le cadre du projet IMAGIWEB, deux cas d\u0027étude sont traités : des tweets à caractère politique et des billets de blogs traitant de l\u0027entreprise EDF et du nucléaire. Pour chacun des cas, un processus d\u0027annotation manuelle concernant l\u0027opinion véhiculée a été mis en place.\nDonnées tweets et besoins d\u0027analyse\nDans le cadre du projet IMAGIWEB, les tweets ont été recueillis grâce à l\u0027API Streaming de Twitter. Ce sont des tweets en français, à caractère politique, portant sur Nicolas Sarkozy et François Hollande, avant et après les élections prési-dentielles de 2012. Les données extraites sont le contenu du tweet, le pseudonyme du twittos, la date du tweet, l\u0027image (à savoir François Hollande ou Nicolas Sarkozy, c\u0027est à dire l\u0027entité sur laquelle porte le tweet), l\u0027URL qui mène vers le tweet.\nUne annotation est faite par un annotateur sur un extrait ou un passage d\u0027un tweet. L\u0027annotateur détermine l\u0027opinion contenue dans le passage (avec une polarité allant de -2 pour une opinion très négative à +2 pour une opinion très positive en passant par le zéro si l\u0027opinion est neutre ou par le NULL s\u0027il n\u0027y a pas d\u0027opinion) ainsi que la cible (le sujet sur lequel porte le passage) et la sous-cible. Les cibles et sous-cibles ont été déterminées par les membres du projet. Citons comme exemple de cible \"bilan\", \"compétences\", \"positionnement\". Pour la cible \"positionnement\", les sous-cibles sont \"alliance\", \"écologie\", \"économie\" et \"sociétal\". Enfin l\u0027annotateur donne un niveau de confiance dans son annotation. 4073 tweets ont été annotés manuellement, ce qui a donné lieu à 5674 annotations.\nLes données tweets constituent le terrain d\u0027analyse des chercheurs en science politique et en sociologie. Les politologues souhaitent pouvoir suivre l\u0027évolution dans le temps des deux images que sont François Hollande et Nicolas Sarkozy à travers Twitter. L\u0027analyse de ces données, à la fois des tweets eux-mêmes et de leurs annotations, constitue un premier enjeu du projet.\nDonnées blogs et besoins d\u0027analyse\nLes blogs à analyser concernent tout ce qui touche à EDF et au nucléaire. À partir d\u0027un ensemble de blogs, tous les articles, en français, avec au moins une occurrence du sigle EDF ou des mots \"Electricité de France\" et de \"nucléaire\" ont été collectés. Les données contiennent le titre de l\u0027article, l\u0027URL du site web dont provient l\u0027article, la date, le contenu textuel et l\u0027image (sécurité, emploi ou prix). Les données blogs contiennent également le passage annoté (à chaque article correspond un ou plusieurs passages), la cible (\"politique\", \"tarifs\" ou encore \"risques\"), la sous-cible (par exemple \"démantèlement/durée de vie\" ou \"expertise/incident\" pour la cible \"risques\"), la polarité et la confiance. 560 articles ont été annotés manuellement en 3420 annotations (6,1 annotations par article en moyenne).\nPar rapport aux besoins, les marketeurs d\u0027EDF souhaitent centrer leur analyse sur les notions de cibles, de polarité. Ils souhaitent également pouvoir naviguer dans les données selon le type de structure (organisme) dont est issu le blog. Cette information peut être portée par l\u0027extension du site web. Par exemple, une organisation à but non lucratif aura généralement un site web avec l\u0027extension \".org\" alors qu\u0027une société aura un site web avec une extension \".com\". Dans le modèle associé aux tweets (cf. figure 1), deux faits sont observés : ANNOTATION et TWEET. À ces faits sont associées plusieurs dimensions, comme le temps, la cible ou encore l\u0027annotateur. Dans la dimension temps, on retrouve plusieurs niveaux de granularité de l\u0027information avec deux hiérarchies : {jour, semaine, année} et {jour, mois, trimestre, année}.\nLe fait TWEET va permettre de compter le nombre de tweets et de RT Confiance donne un indice quant à la confiance accordée à la polarité par l\u0027annotateur. Le modèle permet également de retrouver l\u0027image, la cible, l\u0027annotateur, et bien sûr, le temps. La dimension annotateur rendra possible la comparaison de l\u0027annotation automatique à celle manuelle le moment venu. Les dimensions image, cible et temps sont cruciales pour l\u0027analyse. Une des particularités de ce modèle est de retrouver la polarité et la confiance aussi bien en mesure qu\u0027en dimension. Cela permet de visualiser les données selon différentes manières. La polarité en tant que dimension permet par exemple de visualiser le nombre de fois où la polarité +2 est affectée alors qu\u0027en la plaçant en tant que mesure, elle peut être agrégée avec des fonctions comme la somme ou la moyenne. Le modèle pour les blogs est assez similaire à celui des tweets. On retrouve deux faits Article et Annotation. Les mesures et les fonctions d\u0027agrégat associées sont identiques. On retrouve également plusieurs dimensions en commun, à savoir la polarité, la confiance, la cible et le temps. Toutes ces notions similaires sont en fait celles associées au besoin commun concernant l\u0027analyse de l\u0027opinion. En revanche, notons comme différence que les blogs disposent d\u0027un titre grâce à la dimension Blog et qu\u0027ils sont également porteurs d\u0027informations sur la structure qui héberge l\u0027article (grâce à l\u0027extension du site web) via une dimension Structure.\nÀ partir du modèle multidimensionnel, pour introduire la navigation, la notion de cube OLAP est utilisée. Ainsi, deux cubes ont été créés concernant les données issues de Twitter et il en est de même pour les blogs. La navigation se caractérise par l\u0027application d\u0027opérateurs tels que le Drill Down qui permet d\u0027aller vers un niveau plus détaillé selon la hiérarchie de dimension définie préalablement dans le modèle, en appliquant une fonction d\u0027agrégat sur la mesure qui est observée. Il s\u0027agit par exemple de passer de l\u0027observation de la polarité moyenne par trimestre à l\u0027observation par mois selon la hiérarchie temporelle. L\u0027opérateur inverse s\u0027appelle le Roll Up. Notons également l\u0027existence de l\u0027opérateur Slice \u0026 Dice qui permet de sélectionner certaines valeurs pour certains axes d\u0027analyse. Par exemple, dans un cube qui permet d\u0027observer le nombre de tweets par mois et par cible, il serait possible de sélectionner quelques cibles sur lesquelles on souhaite se focaliser.\nMise en oeuvre\nDans le cadre du projet IMAGIWEB, nous avons retenu MySQL comme SGBD en raison de contraintes techniques du projet. Nous avons également choisi de développer notre propre ETL (Extract Transform Load, phase correspondant à l\u0027alimentation des données) car nous souhaitions pouvoir apporter des transformations très particulières en lien avec le contenu textuel (relatives à la fouille de texte) pour la suite du projet. Enfin, nous avons préféré le serveur OLAP Pentaho Mondrian en lui greffant l\u0027interface graphique Saiku pour l\u0027étendue de sa communauté et la prise en main de son environnement. L\u0027implémentation résultante permet de naviguer dans les données en construisant des tableaux de bord très facilement pour l\u0027utilisateur comme nous l\u0027illustrons par la suite sur les données Twitter. Notons qu\u0027il y a un menu sur l\u0027interface qui permet également de représenter les données issues de la navigation sous forme de différents types de graphiques qui sont générés très simplement par l\u0027utilisateur.\nInitialement, le politologue peut par exemple observer la polarité moyenne en fonction du temps en trimestre pour les entités Hollande et Sarkozy. Puis, pour observer de façon plus précise, il peut obtenir le détail par mois (ce qui correspond au niveau OLAP à une opération de Drill Down), en se focalisant simplement sur Hollande (réalisant ainsi une opération de Slice), obtenant ainsi les résultats figurant dans la figure 2.\nAinsi, le politologue peut constater une baisse importante de popularité entre le mois de Mai et le mois de Juin (polarité de -0.346 à -0.658). Il peut ensuite détailler les cibles sur lesquelles cette baisse est plus importante, en ajoutant la dimension Cible dans les résultats. Le tableau 1 qui en résulte permet d\u0027observer que l\u0027opinion des Twittos a particulièrement diminué sur ses performances (-0.294 à -1.000) mais aussi sur son positionnement et son projet.\nFIG. 2 -Polarité moyenne et nombre de tweets en fonction du temps en mois pour Hollande\nL\u0027intérêt pour le chercheur en science politique est ici, sur la base de la navigation, de pouvoir établir des liens entre l\u0027opinion exprimée sur le web et des évènements de la vie politique, d\u0027observer également à quel point le Web est un miroir ou non de l\u0027opinion publique au sens large (comparaison avec les sondages d\u0027opinion classiques).\nConclusion\nDans le cadre du projet IMAGIWEB, l\u0027analyse OLAP était une des pistes à explorer pour visualiser les données. La mise en oeuvre de l\u0027architecture décisionnelle a répondu à de réels \n"
  },
  {
    "id": "233",
    "text": "Introduction\nDe nos jours, le maintien opérationnel d\u0027un Système d\u0027Information est devenu un des critères essentiels pour toute entreprise, ou personne cherchant à délivrer un service, ou simplement souhaitant communiquer. Le côté déplaisant de l\u0027interconnexion mondiale des Systèmes d\u0027Information réside dans un phénomène appelé \"Cybercriminalité\". Des personnes, des groupes mal intentionnés ont pour objectif de nuire aux informations d\u0027une entreprise, d\u0027une personne voire d\u0027un Etat. Conséquemment, la détection des intrusions doit permettre de protéger le Système d\u0027Information. L\u0027objectif de cet article est de présenter dans un premier temps l\u0027état de l\u0027art en matière de détection d\u0027intrusions et dans un second temps d\u0027aborder les travaux menés afin de faciliter la visualisation des flux. La première partie de cet article sera consacrée à l\u0027étude de l\u0027existant dans laquelle nous présenterons les différentes approches de détection d\u0027intrusions et leurs limites. Ensuite, nous nous intéresserons à la motivation de nos travaux et nous proposerons une solution. Nous détaillerons par la suite, la première phase de nos travaux ainsi que les résultats et nous terminerons par une conclusion et les perspectives.\nÉtude de l\u0027existant\nUne multitude d\u0027outils (Antivirus, IDS, IPS, HIDS, Firewall) permettent aujourd\u0027hui de mettre en place une sécurité \"relative\" pour l\u0027ensemble du Système d\u0027Information. Les principaux risques résiduels sont l\u0027absence de constat en temps réel sur le signalement des comportements anormaux et sur l\u0027exploitation des vulnérabilités. Il convient donc de répondre en fournissant des contremesures dans des délais raisonnables.\nLes différentes solutions de détection d\u0027intrusions\nLes systèmes de détection des intrusions sont divisés selon les 3 familles distinctes :\n-NIDS (Network Intrusion Detection System) est une sonde chargée d\u0027analyser l\u0027activité réseau du segment où elle est placée et de signaler les transactions anormales (Bhruyan et al (2011)). -HIDS (Host-Based Intrusion Detection System) est basée sur l\u0027analyse d\u0027un hôte selon les produits utilisés, une HDIS surveille le trafic à destination de l\u0027interface réseau, l\u0027activité système et logiciel, les périphériques amovibles pouvant être connectés. -HYBRIDES, qui rassemble les informations des NIDS et HIDS et produit des alertes aussi bien sur des aspects réseau qu\u0027applicatifs. Il existe aussi une variante appelée \"IPS\" (Intrusion Prevention System) étant capable d\u0027appliquer une politique de sécurité lors d\u0027une intrusion. Un autre concept nommé CIDN 1 décrit par Fung (2011) offre la possibilité de partager des informations au travers un espace communautaire sur Internet. Les différentes solutions s\u0027appuient sur deux méthodes, la première est fondée sur une comparaison d\u0027une tentative d\u0027intrusion par rapport à une base de signatures. Ce type de système recherche dans les trames réseau un schéma qui correspond à une signature connue via de l\u0027extraction de motifs. Il est possible d\u0027ajouter de nouvelles signatures, c\u0027est à dire de créer une expression régulière qui correspondra par son contenu à une activité malveillante ou abusive. La seconde méthode repose sur des modèles comportementaux appelés \"profils\". Ils sont utilisés pour détecter les comportements déviant des profils définis. Les anomalies peuvent \"signaler\" une intrusion ou un nouveau comportement. Dans le second cas, il convient d\u0027ajouter ces nouveaux comportements afin de diminuer les \" faux positifs\". Le concept de détection des anomalies repose sur une analyse statistique et un apprentissage temporel des comportements. Plusieurs principes de mise en place sont disponibles comme \"IDES\" 2 (Lunt et al, 1992), ou \"EMERALD\" 3 (Porras et Neumann, 1997 \nLimitation des solutions existantes\nLes principales limites des outils présentés dans les chapitres précédents résident dans le fait que l\u0027analyse des événements et journaux systèmes est souvent considérés comme fastidieuse. De plus, ils ne prennent pas encore en compte l\u0027évolution quasi permanente d\u0027un Système d\u0027Information. Par exemple, la sécurité d\u0027un entrepôt de données peut être mise en cause par la non réévaluation du ou des serveurs hébergeant ce dernier. La structuration organisationnelle et l\u0027analyse de risques s\u0027avèrent donc indispensables. \nMotivations et proposition\nRéalisation de la première phase\nCette phase constitue un tout en soi dans la mesure où la visualisation des données pour les utilisateurs est un enjeu crucial en terme de prise de décisions sur les problématiques de sécurité. Il s\u0027agit du préambule à la \"fouille de données\" qui sera effectuée dans les phases suivantes. Un des principaux équipements de sécurité est le \"Pare-Feu\" , les données brutes envoyées en temps réel par l\u0027ensemble des équipements de filtrage sont traitées selon une extraction de motifs .\nDescription des données\nLe réseau SP1 propose des services à destination de 14 millions de personnes. Les données peuvent être considérées comme sensibles et portent sur une quantité de 9.2 Teraoctets et plusieurs dizaines de millions d\u0027euros par jours. Ces données sont hétérogènes et proviennent de plusieurs sources différentes. Le contenu des variables listées ci-dessous est exporté vers les conteneurs de données. La phase 1 se focalisera uniquement sur l\u0027analyse et la représentation graphique de ces dernières.\n-adresse ip source, adresse ip de destination, port de destination, protocole (udp et tcp) -date et heure de la connexion -numéro de la règle du pare feu appliquée, action appliquée par la politique de filtrage.\nLe tableau 2 synthétise le volume en nombre de lignes traitées par les équipements de filtrage. \nScénario de visualisation\nLa représentation graphique de l\u0027ensemble des flux autorisés selon la période souhaitée relève du problème de vision de grands graphes (voir figure 2), mais il est possible d\u0027extraire des \"sous graphes\" basés sur du \"requêtage\" qui visent à sélectionner les modalités de certaines variables (adresses source et de destination ainsi que les services et protocoles). En revanche, l\u0027analyse d\u0027un graphique fondé sur les flux rejetés (même agrégés) comme le montre la figure 3 s\u0027avère simple mais aussi efficace. Une adresse IP tente de se connecter à plusieurs autres adresses sur le port \"135\". Une recherche de répertoires partagés peut être à l\u0027origine de ce type de comportement. \nRésultat\nA l\u0027issue de la phase 1, Le traitement des informations recueillies sur les différents équipe-ments de filtrage permet de visualiser rapidement les tentatives de connexion depuis plusieurs sources vers plusieurs destinations. Ceci rend possible de soulever des interrogations sur cette transaction et de mettre en place une action de surveillance. D\u0027autres options ont été créées afin d\u0027offrir une visualisation des règles de filtrage les plus utilisées. En cas de doute sur une 8. https ://www.perl.org/ 9. Logiciel de visualisation graphique, http ://www.graphviz.org 10. AfterGlow, outil de génération graphique, http ://afterglow.sourceforge.net/ 11. Responsables de la sécurité du système d\u0027information, ingénieurs sécurité, administrateurs réseau adresse Ip, il est possible de lister toutes les activités de cette dernière selon des critères de temps, de destination, de protocoles et de ports utilisés.\n"
  },
  {
    "id": "234",
    "text": "Introduction\nDans cet article, nous proposons une nouvelle méthode de classification non supervisée de documents multilingues de corpus comparable bruité afin d\u0027améliorer l\u0027extraction des lexiques de traduction. Nous nous basons sur l\u0027approche de (Rouane et al., 2007) dans la réingénierie des modèles UML et (Mimouni et al., 2012) dans la RI qui ont profité d\u0027un couplage entre l\u0027aspect formel et le relationnel afin de prendre en compte des relations entre les objets d\u0027un même contexte. Nous avons choisi d\u0027effectuer un couplage entre l\u0027Analyse Formelle de Concepts (AFC) et les modèles vectoriels. En effet, l\u0027AFC, appliquée dans un contexte de fouille de textes, permet d\u0027extraire des classes de documents sous formes de CFs. D\u0027un autre côté, les modèles vectoriels basés sur les vecteurs des extensions des CFs extraits, permettent d\u0027aligner les CFs des différentes langues en calculant le degré de similarité des Concepts Fermés monolingues extraits, dans l\u0027objectif de générer des CFs multilingues.\nExtraction de Concepts Fermés à partir de corpus comparables\nEn classification de documents, un Concept Fermé est le couple \u003c T, D \u003e, avec T l\u0027ensemble des termes des documents qui appartiennent à tous les documents D, et D, l\u0027ensemble des documents qui contiennent tous les termes de T . Dans notre contexte de recherche, un Concept Fermé représente une classe de documents regroupés selon un ensemble de termes représentatifs. L\u0027extraction des Concepts Fermés, à partir d\u0027un corpus comparable français-anglais, est précédée par une étape de pré-traitement linguistique du corpus comparable bilingue mais aussi une réorganisation du contenu des documents du corpus en question est né-cessaire. Les concepts en sortie sont de la forme : CF \u003d\u003c {t 1 , t 2 , . . . , t n }, {d 1 , d 2 , . . . , d m } \u003e tel que {t 1 , t 2 , . . . , t n } (ou extension) est l\u0027ensemble des termes qui composent un termset fermé et {d 1 , d 2 , . . . , d m } (ou intension) l\u0027ensemble de documents dans lesquels {t 1 , t 2 , . . . , t n } sont apparus ensemble avec une fréquence supérieure ou égale à minsupp. La sortie est composée de l\u0027ensemble des Concepts Fermés français CF f r et des Concepts Fermés anglais CF en séparément.\n"
  },
  {
    "id": "239",
    "text": "?\nrabdesselam/fr/ Résumé. Les résultats de toute opération de classification ou de classement d\u0027objets dépendent fortement de la mesure de proximité choisie. L\u0027utilisateur est amené à choisir une mesure parmi les nombreuses mesures de proximité existantes. Or, selon la notion d\u0027équivalence topologique choisie, certaines sont plus ou moins équivalentes. Dans cet article, nous proposons une nouvelle approche de comparaison et de classement de mesures de proximité, dans une structure topologique et dans un objectif de discrimination. Le concept d\u0027équivalence topologique fait appel à la structure de voisinage local. Nous proposons alors de définir l\u0027équivalence topologique entre deux mesures de proximité à travers la structure topologique induite par chaque mesure dans un contexte de discrimination. Nous proposons également un critère pour choisir la \"meilleure\" mesure adaptée aux données considérées, parmi quelques mesures de proximité les plus utilisées dans le cadre de données quantitatives. Le choix de la \"meilleure\" mesure de proximité discriminante peut être vérifié a posteriori par une méthode d\u0027apprentissage supervisée de type SVM, analyse discriminante ou encore régression Logistique, appliquée dans un contexte topologique. Le principe de l\u0027approche proposée est illustré à partir d\u0027un exemple de données quantitatives réelles avec huit mesures de proximité classiques de la littérature. Des expérimentations ont permis d\u0027évaluer la performance de cette approche topologique de discrimination en terme de taille et/ou de dimension des données considérées et de sélection de la \"meilleur\" mesure de proximité discriminante.\nIntroduction\nLa comparaison d\u0027objets, de situations ou d\u0027idées sont des tâches essentielles pour évaluer une situation, pour classer des préférences ou encore pour structurer un ensemble d\u0027éléments matériels ou abstraits, etc. En un mot pour comprendre et agir, il faut savoir comparer. Ces comparaisons que le cerveau accomplit naturellement, doivent cependant être explicitées si l\u0027on veut les faire accomplir à une machine. Pour cela, on fait appel aux mesures de proximité. Une mesure de proximité est une fonction qui mesure la ressemblance ou la dissemblance entre deux objets d\u0027un ensemble. Ces mesures de proximité ont des propriétés mathématiques et des axiomes précis. Mais est-ce que ces mesures sont toutes équivalentes ? Peuvent-elles être utilisées dans la pratique de manière indifférenciée ? Produiront-elles les mêmes bases d\u0027apprentissage qui serviront comme entrée pour l\u0027estimation de la classe d\u0027appartenance d\u0027un nouvel objet. Si nous savons que la réponse est non, alors comment pouvoir décider laquelle utiliser ? Certes, le contexte de l\u0027étude ainsi que le type de données considérées peuvent aider à sélectionner quelques mesures de proximités, mais laquelle choisir parmi cette sélection ?\nOn retrouve cette problèmatique dans le cadre d\u0027une classification supervisée ou d\u0027une discrimination. L\u0027affectation ou le classement d\u0027un objet anonyme à une classe dépend en partie de la base d\u0027apprentissage utilisée. Selon la mesure de proximité choisie, cette base d\u0027apprentissage change et par conséquent le résultat du classement aussi.\nOn s\u0027intéresse ici au degré d\u0027équivalence topologique de discrimination de ces mesures de proximité. Plusieurs études d\u0027équivalence topologique de mesures de proximité ont été proposées Batagelj et Bren (1992, 1995; Rifqi et al. (2003); Lesot et al. (2009);Zighed et al. (2012) mais pas dans un objectif de discrimination. Cet article met donc l\u0027accent sur la façon de construction la matrice d\u0027adjacence induite par une mesure de proximité, tout en tenant compte des classes d\u0027appartenance des objets, connues a priori, en juxtaposant des matrices d\u0027adjacence intra-groupe et inter-groupes Abdesselam (2014). Un critère de sélection de la \"meilleure\" mesure est proposé. On vérifie en effet a posteriori qu\u0027elle est bien une bonne mesure discriminante en utilisant la méthode des SVM multi-classes.\nTAB. 1 -Quelques mesures de proximité.\nOù, p désigne la dimension de l\u0027espace, Cet article est organisé comme suit. Dans la section 2, après avoir rappelé les notions de structure, de graphe et d\u0027équivalence topologique, nous présentons la façon dont a été construite la matrice d\u0027adjacence dans un but de discrimination, le choix de la mesure du degré d\u0027équivalence topologique entre deux mesures de proximité ainsi que le critère de sélection de la \"meilleure\" mesure discriminante. Un exemple illustratif est commenté en section 3. Une conclusion et quelques perspectives de cette approche sont données en section 4.\nLe tableau 1 présente quelques mesures de proximité classiques utilisées pour des données continues, définies sur R p .\nEquivalence topologique\nL\u0027équivalence topologique repose en fait sur la notion de graphe topologique que l\u0027on dé-signe également sous le nom de graphe de voisinage. L\u0027idée de base est en fait assez simple : deux mesures de proximité sont équivalentes si les graphes topologiques correspondants induits sur l\u0027ensemble des objets restent identiques. Mesurer la ressemblance entre mesures de proximité revient à comparer les graphes de voisinage et à mesurer leur ressemblance. Nous allons tout d\u0027abord définir de manière plus précise ce qu\u0027est un graphe topologique et comment le construire. Nous proposons ensuite une mesure de proximité entre graphes topologiques qui servira par la suite à comparer les mesures de proximité.\nGraphe topologique\nSur un ensemble de points x, y, z, . . . de R p , on peut, au moyen d\u0027une mesure de proximité u définir une relation de voisinage V u qui sera une relation binaire sur E × E. Pour simplifier la compréhension mais sans nuire à la généralité du propos, considérons un ensemble d\u0027objets E \u003d {x, y, z, . . .} de n \u003d |E| objets plongés dans R p . Ainsi, pour une mesure de proximité u donnée, nous pouvons construire un graphe de voisinage sur un ensemble d\u0027individus dont les sommets sont les individus et les arêtes sont définis par une propriété de la relation de voisinage.\nDe nombreuses définitions sont possibles pour construire cette relation binaire de voisinage. On peut, par exemple, choisir l\u0027Arbre de Longueur Minimale (ALM) Kim et Lee (2003), le Graphe de Gabriel (GG) Park et al. (2006), ou encore le Graphe des Voisins Relatifs (GVR) Toussaint (1980), dont tous les couples de points voisins vérifient la propriété suivante :\n) V u (x, y) \u003d 0 sinon c\u0027est-à-dire, si les couples de points vérifient ou pas l\u0027inégalité ultratriangulaire (1), condition ultramétrique.\nLa figure 1 montre, dans R\n, un exemple de graphe topologique GVR parfaitement défini par la matrice d\u0027adjacence V u associée, formée de 0 et de 1. Sur le plan géométrique, cela signifie que l\u0027hyper-Lunule (intersection des deux hypersphères centrées sur les deux points) est vide.\nComparaison de mesures de proximité\nOn dispose de p variables quantitatives explicatives (prédicteurs) {x j ; j \u003d 1, p} et d\u0027une variable qualitative cible à expliquer y, partition de n \u003d ? q k\u003d1 n k individus-objets en q modalités-groupes {G k ; k \u003d 1, q}.\nPour toute mesure de proximité u i donnée, on construit, selon la propriété (1), la matrice d\u0027adjacence binaire globale V ui qui se présente comme une juxtaposition de q matrices d\u0027adjacence symétriques Intra-groupe {V\nA noter que la matrice d\u0027adjacence partitionnée globale V u i ainsi construite, n\u0027est pas symétrique. En effet, pour deux objets\n• Le premier objectif est de regrouper les différentes mesures de proximité considérées, selon leur similitude topologique pour mieux visualiser leur ressemblance dans un contexte de discrimination.\nPour mesurer le degré d\u0027équivalence topologique de discrimination entre deux mesures de proximité u i et u j , nous proposons de tester si les matrices d\u0027adjacence associées V u i et V u j sont différentes ou pas. Le degré d\u0027équivalence topologique entre deux mesures de proximité est mesuré par la quantité :\n0 sinon.\n• Le second objectif consiste à établir un critère d\u0027aide à la sélection de la \"meilleure\" mesure de proximité ; celle qui parmi toutes les mesures considérées, discrimine au mieux les q groupes.\nOn note, V u * \u003d diag (1 G 1 , . . . , 1 G k , . . . , 1 G q ) la matrice d\u0027adjacence symétrique blocdiagonale de référence \"discrimination parfaite\" associée à la mesure de proximité inconnue, notée u * , où, 1 n k désigne le vecteur d\u0027ordre n k dont toutes les composantes sont égales à 1 et 1 G k \u003d 1 n k t 1 n k , la matrice carrée d\u0027ordre n k dont tous les éléments sont égaux à 1.\nOn peut ainsi établir le degré d\u0027équivalence topologique de discrimination S(V u i , V u * ) entre chaque mesure de proximité u i considérée et la mesure de référence u * . Enfin, afin d\u0027évaluer autrement le choix de la \"meilleure\" mesure de proximité discriminante proposée par cette approche, nous avons appliqué a posteriori une technique de classement par SVM Multiclasses (MSVM) sur la matrice d\u0027adjacence associée à chacune des mesures de proximité considérée y compris à la mesure de référence u * .\nExemple d\u0027application\nPour illustrer notre approche, nous considérons ici un jeu de données bien connu et relativement simple, celui des Iris Fisher (1936); Anderson (1935). Ces données ont été proposées comme données de référence pour l\u0027analyse discriminante et la classification par le statisticien Ronald Aylmer Fisher en 1933. Les données complètes se trouvent notamment dans UCI (2013). Quatre variables (longueur et largeur des sépales et pétales) ont été observées sur 50 fleurs de chacune des 3 espèces d\u0027Iris (Iris Setosa, Iris Virginica, Iris Versicolor).\nComparaison et classement des mesures de proximité\nLes principaux résultats de l\u0027approche proposée sont présentés dans les tableaux et le graphique suivants. Ils permettent de visualiser les mesures qui sont proches les unes des autres selon l\u0027objectif de discrimination.\nPour ce jeu de données, le tableau 2 récapitule les similarités entre les 8 mesures de proximité et montre que la mesure u T ch de Tchebychev est la plus proche de la mesure u * de référence.\nUne Analyse en Composantes Principales (ACP) suivie d\u0027une Classification Hiérarchique Ascendante (CHA) ont été effectuées à partir de la matrice de similarités entre les 8 mesures de proximité considérées, afin de les partitionner dans des groupes homogènes et de visualiser leurs ressemblances.\nL\u0027application d\u0027un algorithme de construction d\u0027une CHA selon le critère de Ward, Ward Jr (1963), permet d\u0027obtenir le dendogramme de la figure 2. Le vecteur similarités S(V ui , V u * ) de \nTAB. 3 -Classement de la mesure de référence.\nAu vu des résultats présentés dans le tableau 3 de la partition en 5 classes de mesures de proximité, la mesure de référence inconnue u * , projetée en élément supplémentaire, serait donc plus proche des mesures de la classe 3, c\u0027est-à-dire, de la mesure de Tchebychev u T ch qui serait pour ces données, la \"meilleure\" mesure de proximité parmi les 8 mesures considé-rées. Ce résultat confirme celui constaté dans le tableau 2, à savoir, une plus grande similarité S(V u T ch , V u * ) \u003d 68.10% de la mesure de Tchebychev avec celle de référence u * .\nLes mesures discriminantes selon les MSVM\nCette partie consiste à valider les résultats du choix de la meilleure mesure au vu de la matrice de référence a posteriori en utilisant les MSVM. Nous utilisons ici le modèle M SV M LLW , Lee et al. (2004), considéré comme le plus fondé théoriquement du fait que sa solution donne un classifieur qui converge vers celui de Bayes.\nMesure\nErreur Pour ce faire, nous allons appliquer une des techniques de sélection du modèle consistant à tester plusieurs valeurs du paramètre et à choisir celle qui minimise l\u0027erreur test calculée par validation croisée. Dans cet exemple, nous testons 10 valeurs du paramètres (entre 1 et 100) pour toutes les bases de données. Aprés simulations, la valeur choisie est C \u003d 1.\nLes principaux résultats du modèle M SV M LLW , appliqué sur chacune des matrices d\u0027adjacence induites par les mesures de proximité considérées, sont présentés dans le tableau 4.\nLe meilleur taux d\u0027erreur est celui donné par les mesures de Tchebechev u T ch et Euclidienne u E , qui est aussi celui enregistré pour la matrice de référence.\nL\u0027application du modèle MSVM montre que les mesures de proximité de Tchebechev u T ch et Euclidienne u E sont les plus adaptées pour différencier et séparer au mieux les 3 espèces d\u0027Iris. Ce résultat confirme celui obtenu précédemment, à savoir le choix de la mesure de Tchebychev u T ch comme la mesure plus proche, parmi les huit mesures considérées, de la mesure de référence et donc la plus discriminante.\nExpérimentations\nNous avons procédé à des expérimentations sur d\u0027autres jeux de données afin d\u0027essayer d\u0027évaluer l\u0027effet des données, de leur taille et/ou de leur dimension sur les résultats de la classification des mesures de proximité toujours dans un but de discrimination. Est-ce que, par exemple, les mesures de proximité se regroupent différemment selon le jeu de données utilisé ? Selon la taille de l\u0027échantillon et/ou le nombre de variables explicatives considérées dans un même ensemble de données ?\nPour répondre à ces questions, nous avons donc appliqué l\u0027approche proposée sur différents jeux de données, présentés dans le tableau 5, qui proviennent tous du référentiel UCI (2013). L\u0027objectif est de comparer les résultats des classifications des mesures de proximité de toutes ces expérimentations ainsi que la \"meilleure\" mesure discriminante proposée pour chacun de ces jeux de données.\nEtant donné un ensemble de données explicatives X (n,p) à n objets et p variables, et une variable à expliquer ou à discriminer Y q à q modalités-classes.\nPour analyser l\u0027effet du changement de dimension, nous avons considéré le jeu de données \"Waveform Database Generator\" pour générer 3 échantillons n?4 de taille n \u003d 2000 objets et de dimension p égale respectivement à 40, 20 et à 10 variables.\nDe même, pour évaluer l\u0027effet du changement de la taille de l\u0027échantillon, nous avons éga-lement généré 3 autres échantillons n?5 de taille n égale respectivement à 3000, 1500 et à 500 objets et de même dimension p égale à 30 variables.\nLes principaux résultats de ces expérimentations, à savoir les équivalences topologiques des mesures de proximité discriminantes et l\u0027affectation de la mesure de référence u * dans la classe la plus proche, sont présentés dans le tableau 6.\nPour chacune de ces expérimentations, nous avons retenu une partition en cinq classes de mesures de proximité afin de les comparer et de bien distinguer les mesures de la classe d\u0027appartenance de la mesure de référence, c\u0027est-à-dire les mesures les plus discriminantes.\nLes regroupements des mesures de proximité obtenus pour les trois jeux de données n?4 sont pratiquement identiques, il n\u0027y a donc pas vraiment d\u0027effet de la dimension. \nune nouvelle approche d\u0027équivalence entre mesures de proximité dans un contexte de discrimination. Cette approche topologique est basée sur la notion de graphe de voisinage induit par la mesure de proximité. D\u0027un point de vue pratique, dans ce papier, les mesures que nous avons comparées sont toutes construites sur des données quantitatives. Mais ce travail peut parfaitement s\u0027étendre aux données qualitatives en choisissant la bonne structure topologique adaptée.\nNous envisageons d\u0027étendre ce travail à d\u0027autres structures topologiques et d\u0027utiliser un critère de comparaison, autre que les techniques de classification, afin de valider le degré d\u0027équivalence entre deux mesures de proximité. Par exemple, évaluer le degré d\u0027équivalence topologique de discrimination entre deux mesures de proximité en appliquant le test non paramètrique du coefficient de concordance de Kappa, calculé à partir des matrices d\u0027adjacence associées, Abdesselam et Zighed (2011). Cela va permettre de donner une signification statistique du degré de concordance entre les deux matrices de ressemblance et de valider ou pas l\u0027équivalence topologique de discrimination, c\u0027est-à-dire si vraiment elles induisent ou pas la même structure de voisinage sur les groupes d\u0027objets à séparer.\nEnfin, les expérimentations menées sur différents jeux de données ont montré qu\u0027il n\u0027y pas du tout d\u0027effet de la dimension et pas vraiment d\u0027effet de la taille de l\u0027échantillon aussi bien sur les regroupement des mesures de proximité que sur le résultat du choix de la meilleure mesure discriminante.\nproximity measures in a topological structure and a goal of discrimination. The concept of topological equivalence uses the structure of local neighborhood.\nThen we propose to define the topological equivalence between two proximity measures, in the context of discrimination, through the topological structure induced by each measure. We also propose a criterion for choosing the \"best\" measure adapted to data considered among some of the most used proximity measures for quantitative data. The choice of the \"best\" discriminating proximity measure can be verified retrospectively by a supervised learning method type SVM, discriminant analysis or Logistic regression applied in a topological context.\nThe principle of the proposed approach is illustrated using a real quantitative data example with eight conventional proximity measures of literature. Experiments have evaluated the performance of this discriminant topological approach in terms of size and/or dimension of the relevant data and of selecting the \"best\" discriminant proximity measure.\n"
  },
  {
    "id": "240",
    "text": "Introduction\nCe papier propose un nouveau mécanisme d\u0027optimisation pour l\u0027algorithme de classification automatique évidentielle semi-supervisée SECM , qui est le premier à reposer sur des contraintes exprimées sous la forme de données étiquetées. Les algorithmes de classification évidentielle (Masson et Denoeux, 2008) reposent sur le cadre théorique des fonctions de croyance et permettent de représenter tous les types d\u0027affectations partielles grâce au concept de partition crédale qui étend la notion de partition stricte, floue et possibiliste. Ces méthodes évidentielles ont été étendues dans le cadre semi-supervisé (Antoine et al., 2012 pour pouvoir tirer partie de contraintes de type Must-Link (ML) et Cannot-Link (CL) qui spécifient si deux données doivent ou non appartenir à la même classe. La transformation des informations disponibles a priori en ce type de contraintes peut néanmoins induire une perte de connaissance. L\u0027algorithme SECM a été proposé récemment pour tirer partie de données partiellement étiquetées . Cependant, l\u0027algorithme SECM initial repose sur une optimisation stricte qui respecte l\u0027ensemble des contraintes et notamment la positivité des masses de croyances associées à l\u0027affectation d\u0027un point à une classe. Cette contrainte théorique entraîne la formation d\u0027un problème complexe. Nous proposons donc de modifier le mécanisme d\u0027optimisation en relâchant la contrainte de positivité, à l\u0027instar de ce qui est fait dans (Bouchachia et Pedrycz, 2006), et en s\u0027assurant a posteriori de l\u0027optimisation que les masses de croyances sont positives. Nos résultats expérimentaux montrent que notre heuristique ne dégrade pas les performances de l\u0027algorithme SECM et permet de gagner de manière significative en complexité sur nos jeux de tests.\nCe papier est organisé comme suit : la section 2 présente les concepts fondamentaux de la théorie des fonctions de croyance et les principales méthodes de classification automatique sous contraintes. L\u0027algorithme semi-supervisé SECM  est ensuite décrit dans la section 3 et un nouveau schéma d\u0027optimisation est proposé. Enfin, les résultats de l\u0027algorithme sont présentés dans la section 4. Le papier conclut sur l\u0027intérêt de la nouvelle méthode d\u0027optimisation.\n2 Travaux existants 2.1 Les fonctions de croyance L\u0027intérêt principal d\u0027un algorithme de classification évidentielle est de pouvoir représenter le doute concernant l\u0027affectation d\u0027un point à un cluster. Pour ce faire, ces méthodes reposent sur la théorie de l\u0027évidence de Dempster-Shafer, également appelée théorie des fonctions de croyance (Shafer, 1976;Smets et Kennes, 1994). Soit ? une variable prenant ses valeurs dans un ensemble fini ? \u003d {? 1 , . . . , ? c } appelé cadre de discernement. La connaissance partielle concernant la valeur de ? peut être représentée par une fonction de masses m, qui est une application de l\u0027ensemble des parties de\nLes sous-ensembles A ? ? tels que m(A) \u003e 0 sont appelés les éléments focaux de m. La quantité m(A) s\u0027interprète comme la quantité de croyance allouée à A et qui, faute d\u0027information complémentaire, ne peut être allouée à aucun autre sous-ensemble de A. L\u0027ignorance totale correspond à m(?) \u003d 1 alors qu\u0027une certitude totale se rapporte à l\u0027allocation complète de la masse de croyance sur un unique singleton de ?. Si tous les ensembles focaux de m sont des singletons, alors la fonction de masses de croyances est équivalente à une distribution de probabilités. La quantité m(?) peut être interprétée comme la croyance que la valeur réelle de ? n\u0027appartient pas à ?. Quand m(?) \u003d 0, la fonction de croyance est dite normalisée. La connaissance exprimée par une fonction de croyance peut aussi être représentée par une fonction de plausibilité pl : 2 ? ? [0, 1] définie comme suit :\nB?A? \u003d? La quantité pl(A) est interprétée comme le degré maximal de croyance qui peut potentiellement être affecté à l\u0027hypothèse selon laquelle la vraie valeur de ? appartient à A. Quand une décision doit être prise concernant la valeur de ?, il est intéressant de transformer une fonction de masses en probabilité pignistique (Smets et Kennes, 1994) :\n??A où |A| dénote la cardinalité de A ? ?. Quand il existe m(?) ? \u003d 0, une étape de normalisation doit précéder la transformation pignistique. La normalisation de Dempster, qui consiste à diviser toutes les masses par 1 ? m(?), est une méthode classique de normalisation.\nAlgorithme des c-moyennes évidentielles\nLa version évidentielle des k-moyennes, ECM, est un algorithme de classification automatique qui construit une partition crédale à partir des données. Dans ce formalisme, la connaissance partielle concernant l\u0027appartenance d\u0027un objet x i est représentée par une fonction de croyance m i sur l\u0027ensemble ? des classes possibles. Ainsi, un degré de croyance peut être affecté aux singletons (comme dans les approches floues et possibilistes) mais également à tous les sous-ensembles de ?. Soit {x 1 , . . . x n } un ensemble d\u0027individus dans R p à classer dans un ensemble ? \u003d {? 1 , . . . ? c } de c classes. Pour chaque objet x i , la fonction de croyance m i est calculée en plaçant une grande (resp. petite) quantité de croyance sur le sous-ensemble proche (resp. éloigné) en terme de distance de x i . La distance d ij est une métrique définie entre un objet x i et une représentation dans R p d\u0027un sous-ensemble A j ? ?. Similairement à l\u0027algorithme des c-moyennes floues, chaque classe ? k est représentée par un prototype v k . Pour chaque sous-ensemble A j ? ?, A j ? \u003d ?, un centre v j est calculé comme le barycentre des centres associés aux classes composant A j :\nLa distance d 2 ij peut être définie comme une distance euclidienne (Masson et Denoeux, 2008). Plus récemment, une variante a été proposée pour prendre en compte une distance de Mahalanobis (Antoine et al., 2012). Similairement aux travaux de (Gustafson et Kessel, 1979), cette distance permet de détecter des clusters ayant différentes formes géométriques, grâce à une matrice de covariance floue S k associée à chaque cluster ? k et qui doit être optimisée.\nEnsuite, similairement à ce qui est fait pour les prototypes, pour chaque sous-ensemble de A j qui n\u0027est pas un singleton, une matrice S j est calculée en moyennant les matrices incluses dans\nL\u0027algorithme ECM minimise la fonction objectif suivante en fonction des matrices M, V et S précédentes :\nComme m i? correspond à la croyance que x i est un point aberrant, son cas est traité séparément du reste des autres sous-ensembles. Le paramètre ? indique la distance de l\u0027ensemble des objets à l\u0027ensemble vide. Il est intéressant de remarquer à ce niveau qu\u0027une pénalité des sous-ensembles A j ? ? avec une grande cardinalité a été introduite avec la pondération |A j | ? . L\u0027exposant ? permet de contrôler le degré de cette pénalisation.\nTout comme pour les c-moyennes floues, la partition est construite selon un processus itératif qui optimise alternativement les matrices M, V et S. La complexité d\u0027un algorithme évidentiel est linéaire avec le nombre de données mais exponentiel avec le nombre de classes. En conséquence, il est crucial pour ce type de méthodes de minimiser les calculs réalisés dans les phases d\u0027optimisation comme cela est proposé dans cet article.\nAlgorithmes semi-supervisés\nLa plupart des méthodes de classification automatique ont été améliorées pour prendre en compte la connaissance experte sous la forme de contraintes soit entre paires de données de type Must-Link (ML) ou Cannot-Link (CL) qui indiquent si deux points doivent ou non appartenir au même cluster, soit sous la forme de données étiquetées (Wagstaff et al., 2001). Citons par exemple des algorithmes de type k-moyennes (Wagstaff et al., 2001;Basu et al., 2002), hiérarchiques (Davidson et Ravi, 2005), basés sur la densité (Ruiz et al., 2010;Lelis et Sander, 2009), des méthodes spectrales (Wang et Davidson, 2010) ainsi que des algorithmes dédiés aux flux de données (Ruiz et al., 2009). D\u0027autres travaux se sont intéressés à l\u0027intégration de contraintes dans l\u0027algorithme des c-moyennes floues (Grira et al., 2006;Pedrycz, 1985;Bensaid et al., 1996;Pedrycz et Waletzky, 1997a). Pour palier les limitations des algorithmes flous en présence de bruit ou de points aberrants, des méthodes possibilistes (Krishnapuram et Keller, 1993;Sen et Davé, 1998) et plus récemment évidentielles ont été proposées (Masson et Denoeux, 2008. Ces dernières ont également été étendues au cas semi-supervisé pour bénéficier des avantages des modèles basés sur les fonctions de croyance dans la prise en compte de la connaissance experte. Les travaux proposés reposent soit sur des contraintes ML et CL (Antoine et al., 2012 soit, plus récemment, sur des données partiellement étiquetées avec l\u0027algorithme SECM . D\u0027un point de vue formel, deux approches ont été proposées dans la littérature pour prendre en compte les contraintes et les étiquettes pendant le processus de classification automatique. En premier lieu, il est possible de modifier le processus des algorithmes de classification, soit durant la phase d\u0027initialisation (Basu et al., 2002), soit pendant la phase de convergence. Dans ce dernier cas, on peut soit imposer un respect strict des contraintes comme dans COP Kmeans (Wagstaff et al., 2001), soit modifier la fonction objectif pour pénaliser les solutions qui ne respectent pas complètement les contraintes (Pedrycz et Waletzky, 1997b). Par exemple, dans (Bouchachia et Pedrycz, 2003), les auteurs décrivent un FCM amélioré dont la fonction objectif introduit un terme de pénalité qui considère à la fois l\u0027appartenance actuelle des points i aux classes k notée u ik , mais également l\u0027appartenance telle qu\u0027elle devrait être à partir des contraintes de l\u0027expert notée˜unotée˜ notée˜u ik comme le montre l\u0027équation (7).\nTAB. 2 -Plausibilités calculées à partir de la partition crédale.\noù U et V dénotent respectivement la matrice d\u0027appartenance et les coordonnées des centres des clusters. ? est un paramètre de régulation qui permet d\u0027équilibrer l\u0027importance du respect des contraintes dans la fonction objectif.\nEn second lieu, d\u0027autres méthodes proposent d\u0027adapter la métrique en fonction des contraintes et étiquettes fournies par l\u0027expert comme dans l\u0027algorithme MPC k-means (Bilenko et al., 2004). Par exemple, dans (Bouchachia et Pedrycz, 2006), les auteurs proposent une mé-thode pour adapter une distance de Gustafson-Kessel (Gustafson et Kessel, 1979). Nous proposons dans ce papier de considérer un modèle de contraintes flexibles avec une modification de la fonction objectif qui pénalise les solutions ne respectant pas les données étiquetées.\n3 Algorithme SECM 3.1 Formalisation du problème L\u0027idée principale de l\u0027algorithme proposé dans  est d\u0027ajouter un terme de pénalité dans la fonction objectif de ECM afin de prendre en compte un ensemble d\u0027objets étiquetés. La démarche suivie est la même que dans (Bouchachia et Pedrycz, 2003) mais rapportée aux algorithmes évidentiels. L\u0027expression d\u0027un objet étiqueté peut se traduire sous la forme d\u0027une fonction quantifiant la croyance sur l\u0027appartenance de l\u0027objet à une classe. Considérons dans un premier temps une partition crédale connue et définie par le tableau 1. Elle représente la connaissance partielle de l\u0027appartenance de quatre objets à deux classes. Il est alors possible de calculer la plausibilité de chaque objet x i pour chaque classe ? k , comme illustré par le tableau 2. On remarque alors qu\u0027une plausibilité nulle permet de déduire avec certitude qu\u0027un élément n\u0027appartient pas à une classe. Ainsi, l\u0027observation de pl i (? 1 ) \u003d 0 permet de déduire que x 1 , un objet atypique, et x 4 , un objet affecté avec certitude dans la classe ? 2 , ne font pas partie de la classe ? 1 . En revanche, les objets dont la plausibilité pour une classe est élevée ont des chances d\u0027appartenir à cette classe. Ainsi, pl i (? 1 ) \u003d 1 apparaît pour l\u0027objet x 2 , qui appartient à la classe ? 2 avec certitude, et pour l\u0027objet x 3 , qui appartient soit à ? 1 , soit à ? 2 .\nSupposons maintenant que l\u0027on ne dispose pas de la partition crédale, mais qu\u0027il existe des contraintes sous formes d\u0027étiquettes. Par exemple, l\u0027objet x i est inclus dans la classe ? k . Il est alors possible d\u0027imposer la contrainte pl i (? k ) \u003d 1. L\u0027effet sera d\u0027exiger :\n-une croyance élevée pour les fonctions de masse ayant un sous-ensemble comprenant ? k , donc toutes les fonctions de masses qui ont un degré de croyance plus ou moins fort sur le fait que x i appartienne à ? k , -des valeurs faibles pour toutes les fonctions de masses ayant un sous-ensemble qui n\u0027incluent pas ? k . La contrainte entre un objet x i et la classe ? k est donc respectée pour de nombreuses solutions allant de la certitude totale que x i appartienne à ? k jusqu\u0027à l\u0027incertitude complète de l\u0027affectation de x i entre ? k ou plusieurs autres classes de ?. La contrainte est donc flexible car elle permet si nécessaire de garder un doute quant à l\u0027affectation de l\u0027objet à la classe. Par conséquent, cela limite l\u0027influence négative d\u0027une contrainte bruitée.\nLorsqu\u0027un expert crée des contraintes d\u0027étiquettes, il peut avoir un doute entre plusieurs classes pour un unique objet. Par exemple, l\u0027objet x i appartient à une des classes du sousensemble A j ? ?. Cette information se modélise alors sous la forme d\u0027une contrainte sur la plausibilité de A j : pl i (A j ) \u003d 1. Cela revient à favoriser les fonctions de masses ayant au moins une classe dans A j . Cette contrainte, qui généralise la précédente, permet d\u0027établir un terme de pénalité à ajouter à la fonction objectif de ECM :\nLa nouvelle fonction objectif est alors la suivante :\nsous les contraintes (5)  \nOptimisation\nL\u0027optimisation du nouveau critère consiste, de la même manière que pour l\u0027algorithme ECM, à minimiser alternativement les matrices M, V et S. Le terme de pénalité J S ne dé-pendant ni de V, ni de S, leur mise à jour est similaire à ECM, et leur formule est présen-tée dans (Masson et Denoeux, 2008). La partition crédale M est au contraire présente dans J S . En fixant ? à 2 alors la minimisation de la fonction objectif par rapport à M devient un problème quadratique à contraintes linéaires. Ce problème peut être résolu par une méthode classique d\u0027optimisation (Ye et Tse, 1989), néanmoins de nombreux auteurs se trouvant dans un contexte similaire proposent d\u0027optimiser directement la fonction objectif sans prendre en compte les contraintes de positivité sur la partition (6), afin de réduire le temps de convergence de l\u0027algorithme.\nAfin de résoudre le problème de minimisation contraint par (5), des multiplicateurs de Lagrange ? 1 , . . . ? n sont introduits et le Lagrangien défini : \nA j ??,A j ? \u003d? Annuler les dérivées partielles permet d\u0027obtenir les équations suivantes :\nAj ??,Aj ? \u003d? En utilisant (15) et (16) dans (17), il est possible d\u0027écrire :\nCette équation peut finalement être utilisée dans (15) et (16) pour obtenir la mise à jour des fonctions de masse, ?i \u003d 1, n et ?j/A j ? ?, A j ? \u003d ? : \nA j ??,A j ? \u003d? Comme le numérateur de la première partie de l\u0027équation (19) permet l\u0027obtention de valeurs négatives, la fonction de masse m ij peut être négative. Ces valeurs négatives sont accentuées par l\u0027importance donnée aux contraintes. Si l\u0027utilisateur reste dans un cas normal d\u0027utilisation des contraintes, c\u0027est-à-dire ? ? 0.8 (cf. partie 4.2), les valeurs de m ij \u003c 0 seront proches de 0. Une fonction de réajustement est donc envisageable sans que l\u0027optimisation soit dégradée :\navec \nExpérimentations\nLes expériences menées sur plusieurs jeux de données consistent à comparer les résultats obtenus par SECM lorsque la mise à jour des fonctions de masse utilise l\u0027optimisation de (Ye et Tse, 1989), noté SECM-classic, avec SECM et l\u0027optimisation proposée, noté SECM-do.\nDonnées et méthode d\u0027évaluation\nJeux de données : Plusieurs jeux de données issue de l\u0027UCI Machine Learning Repository ont été employés. Le tableau 3 indique leurs caractéristiques ainsi que la métrique utilisée pour les expériences. Il faut noter que LettersIJL correspond au jeu de données Letters modifié comme (Bilenko et al., 2004  Protocole expérimental : Une expérience consiste, pour un certain pourcentage de contraintes, à exécuter 25 fois l\u0027algorithme SECM avec 25 jeux de contraintes différents. Afin d\u0027éviter les optima locaux, chaque exécution teste cinq initialisations aléatoires des centres de gravité et récupère les résultats obtenus par l\u0027initialisation ayant la fonction objectif minimale.\nRésultats\nJeux de données réelles : La figure 1 montre l\u0027évolution de l\u0027indice de Rand moyen obtenu avec SECM-classic et SECM-do par rapport au pourcentage de contraintes pour Iris et Wine. Des résultats similaires ont été trouvés avec Ionosphere et LettersIJL. Le coefficient ? est fixé à 0.5. Il est ainsi possible de remarquer (1) que l\u0027ajout progressif de contraintes améliore l\u0027indice de Rand et (2) que l\u0027algorithme SECM-do présente de meilleurs résultats que SECM-classic. Pour ces expériences, nous avons également constaté que les valeurs des fonctions objectif de SECM-do sont plus petites que celles de SECM-classic. Des résultats similaires ont été trouvés pour ? \u003d 0.3 et ? \u003d 0.8. La nouvelle optimisation permet donc d\u0027obtenir un meilleur minimum grâce à sa relaxation des contraintes, ce qui implique de meilleurs résultats de classification. Il faut également noter que pour ? \u003d 0.8, les résultats obtenus prouvent que la fonction de réajustement de SECM-do ne dégrade en rien les solutions.\nPour ces mêmes expériences, le temps CPU a été observé afin de comparer la vitesse d\u0027exé-cution des deux algorithmes. Le tableau 4 présente les résultats obtenus. Il est ainsi aisé de voir que l\u0027algorithme SECM-do est plus rapide que l\u0027algorithme SECM-classic. à une forte valeur. De plus, nous avons choisi de réduire l\u0027incertitude trouvée par la partition finale en fixant ? à une valeur élevée. Ainsi, ECM avec c \u003d 2, ? \u003d 3 et ? 2 \u003d 1000 trouve la partition crédale dure représentée par la figure 2(b). Nous pouvons remarquer que ECM ne permet pas d\u0027isoler correctement l\u0027avion. Dans une seconde expérience, nous introduisons des contraintes sur la partition comme illustré Figure 2(c). Chaque pixel de la première (respectivement seconde) zone est affectée à ? 1 (respectivement ? 2 ). L\u0027algorithme SECM est alors exécuté avec les mêmes paramètres que ECM. La partition crédale résultante est présentée Figure 2(d). Nous pouvons constater que les contraintes ont permis de lever l\u0027indétermination de la plupart des pixels alloués à ?.\nConclusion\nNous avons présenté dans cet article une nouvelle méthode d\u0027optimisation pour l\u0027algorithme de classification automatique intitulé SECM. Ce dernier est une variante de l\u0027algorithme évidentiel ECM prenant en compte des contraintes d\u0027étiquettes. Il repose sur la minimisation d\u0027une fonction objectif avec des contraintes linéaires et non linéaires, ce qui impose l\u0027utilisation de méthodes d\u0027optimisation avancées. De plus, l\u0027utilisation des fonctions de masses liées aux méthodes évidentielles rend la complexité de l\u0027algorithme linéaire par rapport au nombre d\u0027objets et exponentielle par rapport aux nombre de classes. Nous proposons donc de relâ-cher les contraintes de positivité sur les fonctions de masses, c\u0027est-à-dire sur les contraintes non linéaires, afin de réduire l\u0027optimisation de la partition à la méthode des multiplicateurs de Lagrange. Le respect des contraintes de positivité est ensuite vérifié par une méthode de réajustement. Nous avons montré sur un ensemble de jeux de données que cette nouvelle technique permet non seulement d\u0027augmenter la rapidité de SECM mais qu\u0027elle permet également d\u0027améliorer les performances en trouvant de meilleurs minima. Les travaux futurs porteront sur l\u0027étude de nouveaux formalismes plus rapides permettant de conserver la majeure partie de l\u0027expressivité des méthodes évidentielles avec une complexité largement réduite pour permettre de traiter des jeux de données avec un nombre de classes plus important.\n"
  },
  {
    "id": "241",
    "text": "Introduction\nLes systèmes de recommandation basés sur le contenu suivent généralement un processus en deux étapes : (i) Création d\u0027une représentation du besoin des utilisateurs ainsi que des informations à recommander. (ii) Comparaison des représentations afin d\u0027évaluer la pertinence d\u0027une information pour un utilisateur en fonction de son profil. Notre approche consiste à automatiser l\u0027indexation à l\u0027aide de processus d\u0027inférence sur une ontologie d\u0027indexation intégrant les vocabulaires contrôlés (e.g. thésaurus, nomenclatures, listes) définis par les documentalistes pour modéliser le domaine. Le respect de la vision métier sur le domaine permet une supervision simplifiée pour les documentalistes, garantissant la qualité de l\u0027indexation.\nAutomatisation du processus d\u0027indexation\nLa classification multi-label consiste à associer des étiquettes à des items (Tsoumakas et Katakis, 2007). Cet article propose une méthode pour enrichir sémantiquement une ontologie en adoptant des processus d\u0027apprentissage automatique pour indexer et décrire l\u0027indexation de façon à réduire l\u0027écart entre le point de vue des experts et les règles d\u0027indexation. L\u0027approche proposée repose sur les quatre phases suivantes :\nPhase 1 : utilisation du travail d\u0027indexation déjà fait par les documentalistes et d\u0027un processus d\u0027analyse de texte pour extraire des mots-clés afin de générer une matrice qui présente la fréquence de chaque mot-clé en fonction de chaque étiquette.\nPhase 2 : utilisation de la matrice afin de définir des règles capables de déterminer si un document doit être associé à une étiquette sur la base des mots-clés qu\u0027il contient. Deux seuils de fréquence sont définis, ? et ?. Les mots-clés dont la fréquence est supérieure au seuil ? sont considérés comme des indices fiables. La présence d\u0027un seul de ces mots est considérée comme suffisante pour que le document soit associé à l\u0027étiquette. Le seuil de fréquence infé-rieur est ?. Dans ce cas, nous avons besoin d\u0027une combinaison de ?-termes (dont la fréquence est supérieure à ?) pour prendre la décision d\u0027associer un document avec l\u0027étiquette. Plus d\u0027informations sur les règles d\u0027indexation peuvent être trouvées dans (Werner et al., 2014).\nPhase 3 : la classification fournit deux types de résultats. Le premier est la découverte de la classe de subsomption la plus spécifique. Le second est la déduction des classes d\u0027équiva-lence lorsque les contraintes logiques sont équivalentes. D\u0027une part, cela signifie que lorsqu\u0027un document est étiqueté (lors de la phase 4) par une classe qui possède des subsumants, ce document est également marqué par les classes subsumantes. D\u0027autre part, lorsqu\u0027un document est étiqueté avec une classe qui a des classes d\u0027équivalence alors ce document est également étiqueté avec ces classes équivalentes. Ces deux éléments peuvent permettre la classification multi-label. De plus, sachant que les étiquettes peuvent être organisées de façon hiérarchique il peut s\u0027agir d\u0027un processus de classification hiérarchique multi-label (HMC).\nPhase 4 : la phase de réalisation consiste à trouver toutes les classes les plus spécifiques des individus. Cette phase est mise en oeuvre par le moteur d\u0027inférence. Les phases 3 et 4 utilisent des raisonneurs comme FaCT ++, HermiT ou Pellet.\n"
  },
  {
    "id": "242",
    "text": "Introduction\nUn flux de données est une séquence, potentiellement infinie, non-stationnaire (la distribution de probabilité des données peut changer au fil du temps) de données arrivant en continu. Dans le cas d\u0027un flux, l\u0027accès aléatoire aux données n\u0027est pas possible et le stockage de toutes les données arrivant est infaisable. Le clustering de flux de données nécessite un processus capable de partitionner des observations de façon continue avec des restrictions au niveau de la mémoire et du temps. Dans la littérature, de nombreux algorithmes de clustering de flux de données ont été adaptés à partir des algorithmes de clustering traditionnel, par exemple, la méthode DbScan (Cao et al. (2006); Isaksson et al. (2012)) basée sur la densité, la méthode de partitionnement k-means (Ackermann et al. (2012)), ou encore la méthode basée sur le passage de message AP (Affinity Propagation) (Zhang et al. (2008)). Dans cet article, nous proposons le modèle G-Stream, qui permet de découvrir des clusters de formes arbitraires dans un flux de données en constante évolution. Les caractéristiques et les principaux avantages de G-Stream sont décrits ci-dessous : (a) La structure topologique qui est représentée par un graphe dans lequel chaque noeud représente un cluster. Les noeuds (clusters) voisins sont reliés par des arêtes. La taille du graphe est évolutive. (b) L\u0027utilisation d\u0027une fonction d\u0027oubli afin de réduire l\u0027impact des anciennes données dont la pertinence diminue au fil du temps. Les liens entre les noeuds sont également pondérés. (c) Contrairement à de nombreux algorithmes qui utilisent un nombre important de données pour initialiser leur modèle, G-Stream utilise seulement deux noeuds au départ. (d) Toutes les fonctions de G-Stream sont effectuées en-ligne. (e) L\u0027utilisation de la notion de réservoir pour maintenir, de façon temporaire, les données très éloignées des prototypes courants. L\u0027article est organisé comme suit : d\u0027abord, la section 2 décrit plusieurs travaux liés au problème de clustering de flux de données. Ensuite, la section 3 présente notre nouvelle approche de clustering de flux de données, appelée G-Stream. Puis, dans la section 4, nous rapportons une évaluation expérimentale. Enfin, la section 5 conclut cet article et présente nos futurs travaux de recherche.\nTravaux similaires\nCette section présente un bref état de l\u0027art qui concerne les problèmes de clustering de flux de données. Nous mettons ainsi en évidence les algorithmes les plus pertinents proposés dans la littérature pour faire face à ce problème. La plupart des algorithmes existants (par exemple, CluStream proposé par (Aggarwal et al. (2003)), DenStream de (Cao et al. (2006)), StreamKM++ de (Ackermann et al. (2012)) divisent le processus de clustering en deux phases : (a) En-ligne, dans cette phase, les données sont résumées, (b) Hors-ligne, dans cette phase, les clusters finaux sont calculés à partir de la quantification fournie par la phase en-ligne. Les deux algorithmes CluStream et DenStream utilisent une extension temporelle du Clustering Feature vector proposée par (Zhang et al. (1996)) (appelée micro-clusters) afin de maintenir des résumés statistiques sur les données ainsi que leur temps d\u0027arrivée, ceci durant la phase en-ligne. En créant deux types de micro-clusters (potentiel et outlier micro-clusters), DenStream surmonte l\u0027un des principaux inconvénients de CluStream, sa sensibilité au bruit. Dans la phase hors-ligne, les micro-clusters trouvés lors de la phase en-ligne sont considérés comme des pseudo-points et seront transmis à une variante de k-means dans l\u0027algorithme CluStream (resp. une variante de DbScan dans l\u0027algorithme DenStream), afin de déterminer les clusters finaux. StreamKM++ est une extension de l\u0027algorithme k-means++ pour le flux de données. Les auteurs de (Isaksson et al. (2012)) ont proposé SOStream, qui est un algorithme de clustering de flux de données, basé sur la densité, inspiré à la fois du principe de l\u0027algorithme DbScan et celui des cartes auto-organisatrices (SOM) de (Kohonen et al. (2001)). L\u0027algorithme E-Stream, qui est proposé par (Udommanetanakit et al. (2007)), classe l\u0027évolution des données en cinq catégories : apparition, disparition, auto-évolution, fusion et division. Il utilise une autre structure de données pour sauvegarder des statistiques sommaires, nommée ?-bin histogramme. (Zhang et al. (2008)) présentent une extension de l\u0027algorithme Affinity Propagation pour le flux de données, appelé StrAP et qui utilise un réservoir pour maintenir d\u0027éventuels outliers. Les auteurs de (Bouguelia et al. (2013)) ont proposé une version incrémentale de l\u0027algorithme GNG de (Fritzke (1994)), appelée AING. où ? 1 \u003e 0, qui est une constante définissant le taux de décroissance du poids au fil du temps. t désigne le temps courant et t 0 est le temps d\u0027arrivée de la donnée. Le poids d\u0027un noeud est calculé à partir des poids des données qui lui sont affectées :\noù m est le nombre de données affectées au noeud c au temps courant t. Quand le poids d\u0027un noeud est inférieur à une valeur donnée, alors ce noeud est considéré comme obsolète et sera supprimé (ainsi que ses liens). Gestion des arêtes : la procédure de gestion des arêtes effectue des opérations liées à la mise à jour des arêtes du graphe (les étapes 14-19 de l\u0027algorithme 1). Lors de l\u0027incrémentation de l\u0027âge des arêtes, l\u0027instant de création d\u0027une arête est pris en compte. Contrairement à la fonction d\u0027oubli, l\u0027âge des liens sera renforcé par la fonction exponentielle f 2 (t) \u003d 2 ?2(t?t0) où ? 2 \u003e 0, définit le taux de croissance au temps courant t, t 0 est le temps de création de l\u0027arête. L\u0027étape suivante consiste à ajouter une nouvelle arête reliant les deux noeuds les plus proches. La dernière étape consiste à supprimer chaque lien dépassant un âge maximum.\nGestion du réservoir : l\u0027objectif de l\u0027utilisation d\u0027un réservoir est de maintenir, temporairement, les données éloignées. Comme nous l\u0027avons mentionné précédemment, chaque noeud a un seuil de distance. Les premières données du flux sont affectées aux noeuds les plus proches sans prendre en considération les seuils de distances. Le seuil de distance de chaque noeud est mis-à-jour en prenant la distance maximale du noeud au point le plus éloigné qui lui est affecté. Lorsque le réservoir est plein, ses données sont re-transmises à l\u0027apprentissage. Elles sont placées au début du flux de données, DS, afin de les traiter en premier. Les seuils de distance des noeuds sont mis-à-jour en conséquence.\nÉvaluation expérimentale\nDans cette section, nous présentons une évaluation expérimentale de l\u0027algorithme GStream. Nous avons comparé notre algorithme avec l\u0027algorithme GNG ainsi qu\u0027avec deux algorithmes pertinents de clustering de flux de données. Nos expériences ont été réali-sées sur la plateforme MATLAB en utilisant des données réelles et synthétiques. Les bases de données réelles, Shuttle (43500x9) et KddCup1 (49402x34), ont été prises à partir du répertoire UCI. Les bases DS1 (9153x2) et DS2 (5458x2) sont générées à l\u0027aide du programme disponible sur http://impca.curtin.edu.au/local/software/ synthetic-data-sets.tar.bz2. Comme nous l\u0027avons expliqué dans la section 3, les \nConclusion\nDans ce papier, nous avons proposé, G-Stream, une méthode efficace pour le clustering topologique en-ligne de flux de données évolutives. Dans G-Stream, les noeuds ainsi que les arêtes composant la structure topologique sont pondérés. A partir de deux noeuds, G-Stream compare les données arrivant aux prototypes courants ; il sauvegarde celles très éloignées dans un réservoir ; il apprend les seuils de distance automatiquement ainsi que plusieurs noeuds sont créés à la fois. L\u0027évaluation expérimentale sur des bases de données réelles et synthétiques a démontré l\u0027efficacité de G-Stream à découvrir des clusters de formes arbitraires. Les résultats obtenus sont prometteurs. Nous envisageons à l\u0027avenir d\u0027appliquer le principe des fenêtres adaptatives, de rendre notre algorithme le plus autonome possible et de le développer en Spark.\n"
  },
  {
    "id": "243",
    "text": "Introduction\nLe Web Sémantique a été lancé en 2001 par le W3C 1 pour promouvoir le partage et la créa-tion de données structurées sur le Web en proposant des recommandations pour la description de données (RDF), d\u0027ontologies (RDFS, OWL), et des méthodes et outils associés (SPARQL, ...) pour gérer les connaissances. Actuellement le Web Sémantique correspond à des centaines de bases RDF communautaires (e.g. DBPedia Notre contribution est de fournir une méthode originale d\u0027évaluation de mises à jour, inspirée du raisonnement par cas, utilisant exclusivement les données de la base RDF mise à jour (i.e. ne nécessitant pas l\u0027utilisation d\u0027une ontologie ou de méta-données). Par cette méthode, une mise à jour candidate est évaluée positivement si ses modifications dans la base RDF rendent -selon certains critères -la partie cible mise à jour dans la base plus structurellement similaire à d\u0027autres parties de la base. Notre méthode d\u0027évaluation de la cohérence peut être décomposée en 3 étapes : (i) extraction des contextes de la mise à jour depuis la base, (ii) ré-cupération des parties de la base similaires à la mise à jour et à ses contextes et (iii) évaluation par similarité de la cohérence des données de la mise à jour par rapport à la base.\nEn Section 2 nous définissons une mise à jour RDF et ses contextes (première étape de notre approche). En Section 3 nous détaillons notre méthode d\u0027évaluation de mise à jour en définissant quelles sous-parties de la base sont prises en compte lors de l\u0027évaluation d\u0027une mise à jour (Section 3.1), et enfin comment évaluer la cohérence d\u0027une mise à jour RDF par rapport à une base (Section 3.2).\nMise à jour RDF\nNous introduisons ici quelques définitions pour formaliser les notions de mises à jour RDF et de contextes associés dans une base RDF.\nNous rappelons quelques vocabulaires du Web Sémantique et introduisons quelques termes utilisés dans la suite de l\u0027article. Ainsi, nous considérerons comme synonymes les termes document RDF, base RDF et ensemble de triplets RDF ; pour un document RDF D nous noterons R D l\u0027ensemble des ressources -sujet, prédicat ou objet -des triplets de D ; nous appellerons une ressource noeud une ressource étant soit sujet, soit objet d\u0027un triplet ; pour un document RDF D, nous noterons N D l\u0027ensemble des ressources noeud de D ; nous appellerons document RDF connexe un document RDF dans lequel il y a un chemin connectant chaque ressource noeud du document à une autre, en d\u0027autres termes si le graphe RDF représentant le document est un graphe connexe ; nous appellerons degré d\u0027une ressource noeud le nombre de triplets la contenant dans une base RDF, en d\u0027autres termes son degré dans le graphe RDF.\nNotons aussi que implicitement nous désignons toujours les données d\u0027une base RDF sans (avant) que les modifications d\u0027une mise à jour ne lui soient appliquées. Enfin, toute mise à jour d\u0027une base RDF peut être vue en tant que combinaison de deux sections : une section d\u0027ajout qui contient ce que la mise à jour ajoute à la base et une section de suppression qui contient ce que la mise à jour supprime dans la base.\nDéfinition 1 (Mise à jour RDF). Une mise à jour RDF u d\u0027une base RDF B est un couple d\u0027ensembles de triplets RDF (A, R) tels que :\nUne mise à jour RDF qui ajoute des informations à une base doit apporter de nouveaux éléments liés à des données déjà existantes. Une mise à jour qui supprime des informations peut uniquement supprimer des données déjà existantes dans la base. Les sections d\u0027ajout et de suppression ne contiennent pas de triplets en commun (l\u0027ordre d\u0027application de la suppression ou de l\u0027ajout dans la base n\u0027a pas d\u0027importance). Une mise à jour contient nécessairement une section d\u0027ajout et forme un document connexe (autrement il s\u0027agit de 2 mises à jour distinctes).\nDe la Définition 1 nous pouvons classer les mises à jour RDF en deux catégories : les mises à jour d\u0027ajout, définies par A \u003d ? et R \u003d ?, et les mises à jour de modification, définies par A \u003d ? et R \u003d ?. Le cas des suppressions « pure » est discuté en conclusion.\nPour comparer les données d\u0027une mise à jour à une base RDF selon notre évaluation, nous utilisons le voisinage dans la base de toutes les ressources de la mise à jour. Ainsi les contextes d\u0027une mise à jour sont obtenus grâce aux voisinages des sections d\u0027ajout et de suppression.\nDéfinition 2 (Contextes de mise à jour RDF). Soient un ensemble de triplets RDF B, une mise à jour RDF u \u003d (A, R) candidate à B et n ? N un rang de voisinage. Soit la fonction voisinage n B (r) : N B ? B retournant tous les triplets de B connectés à r par un chemin de longueur égale ou inférieure à n, appelée fonction de voisinage de r.\nLes contextes d\u0027une mise à jour u candidate à B sont les deux ensembles de triplets RDF I u et F u définis par :\n-I u appelé le contexte initial de u dans B tel que\nLe contexte initial représente l\u0027état initial de la partie de la base autour des ressources de la mise à jour candidate, le contexte final représente l\u0027état théorique de la base si la mise à jour était appliquée. 3 Évaluer la cohérence par mesure de la similarité\nNous considérons qu\u0027une mise à jour est cohérente avec une base si on peut trouver suffisamment de sous-parties de la base suffisamment similaires avec les contextes de la mise à jour. Nous procédons en 3 étapes : (i) recherche dans la base de sous-parties structurellement comparables aux contextes de la mise à jour, (ii) quantification de la similarité entre chaque sous-partie et les contextes de la base, (iii) conclusion sur la cohérence de la mise à jour.\nContexte initial I u1 .\nContexte final F u1 .\nFIG. 2 -Contextes de u 1 (En pointillés : voisinage dans la base des ressources de u 1 )\nTrouver des références dans la base\nDeux ensembles de triplets RDF peuvent être structurellement comparés si leurs ressources noeuds sont liées de façon similaire, incluant (au moins) une ressource commune.\nDéfinition 3 (Ensembles de triplets RDF comparables). Soit deux ensembles de triplets RDF connexes G et H.\nG est comparable à H s\u0027il existe une fonction de transformation f :\nDeux ensembles comparables contiennent au moins une ressource commune. De plus, en théorie des graphes, on dira qu\u0027un ensemble de triplets RDF est comparable à un autre si il est homomorphique à une partie d\u0027un autre, sans considérer l\u0027orientation des arcs. Comparer une mise à jour à une base entière signifie comparer structurellement les contextes initial et final de la mise à jour à chaque sous-partie de la base comparable à la mise à jour. Nous appelons références ces sous-parties de la base dépendantes de la mise à jour.\nDBPedia, en tant que références pour u 1 en Fig. 1 (Élé-ments en commun avec u 1 en traits épais orange).\nDe la base DBPedia, deux références pour u 1 peuvent être extraites, notées D 1 et D 2 et représentées en Fig. 3. D 1 et D 2 contiennent plusieurs ressources en commun avec u 1 . D 1 suit le même modèle que u 1 avec une boisson liée à une ville liée à une personne, alors que D 2 concerne une boisson liée à une entreprise liée à une personne.\nÉvaluer la cohérence d\u0027une mise à jour\nDans notre approche, si les modifications d\u0027un mise à jour rendent la partie ciblée de la base plus similaire à d\u0027autres parties (existantes) de la base alors nous évaluons positivement cette mise à jour.\nNous proposons d\u0027évaluer la similarité de la mise à jour par rapport à chacune de ses réfé-rences à l\u0027aide d\u0027une mesure de la similarité structurelle entre deux graphes. Dans l\u0027évaluation en Définition 5, nous supposons l\u0027usage d\u0027une mesure donnant un score de similarité dans R + tel que plus le score est élevé, plus la similarité est grande (un score de 0 signifie aucune similarité). Plusieurs mesures sont utilisables telles que la distance d\u0027édition entre deux graphes, le coefficient de Jaccard, etc. . On note similarity la mesure de similarité entre deux graphes RDF avec similarity(u, La valeur de similarité seule n\u0027importe pas dans notre évaluation, seul le signe de la diffé-rence entre l\u0027état final et initial indique si la mise à jour apporte des informations similaires à ce qui est déjà connu.\nExemple 4. Dans cet exemple, nous choisissons d\u0027utiliser une mesure de similarité en considérant dans chaque ensemble de triplets l\u0027ensemble des ressources et l\u0027ensemble des couples de ressources (sujet, relation) et (relation, objet) où le score est calculé simplement -pédago-giquement -avec similarity \u003d +1 pour chaque ressource commune et +2 pour chaque couple de ressources communes. La différence de similarité entre la référence D 1 et les contextes de u 1 est positive (similarity(F u1 , D 1 ) ? similarity(I u1 , D 1 ) \u003d 12 ? 11) et celle entre D 2 et les contextes de u 1 est nulle (similarity(F u1 , D 2 ) ? similarity(I u1 , D 2 ) \u003d 9 ? 9), ainsi, avec un nombre minimum de références de 1, on a eval(u 1 , B, 2, 1) \u003d true.\nLa mise à jour u 1 est donc cohérente avec la base DBPedia : les modifications de la mise à jour créent des données structurellement similaires à des parties de la base. Cette mise à jour peut être appliquée à la base.\nConclusion\nDans cet article nous proposons une approche d\u0027intégration, ou de mise à jour, de données dans des bases RDF par une évaluation de la cohérence des mises à jour en fonction de leur\n"
  },
  {
    "id": "244",
    "text": "Introduction\nNetworks are studied in numerous contexts such as biology, sociology, online social networks, marketing, etc. Graphs are mathematical representations of networks, where the entities are called nodes and the connections are called edges. Very large graphs are difficult to analyse and it is often beneficial to divide them in smaller homogeneous components easier to handle. The process of decomposing a network has received different names : graph clustering (in data analysis), modularization, community structure identification. The clusters can be called communities or modules ; in this paper we use those words as synonyms.\nAssessing the quality of a graph partition requires a modularization criterion. This function will be optimized to find the best partition. Various modularization criteria have been formulated in the past to address different practical applications. Those criteria differ in the definition given to the notion of community or cluster.\nTo understand the differences between the optimal partitions obtained by each criterion we show how to represent them using the same basic formalism. In this paper we use the Mathematical Relational Analysis (MRA) to express six linear modularization criteria. Linear criteria are easy to handle, for instance, the Louvain method can be adapted to linear quality functions (see Campigotto et al. (2014)). The six criteria studied are : the Newman-Girvan modularity, the Zahn-Condorcet criterion, the Owsi´nskiOwsi´nski-Zadrozny criterion, the Deviation to Uniformity, the Deviation to Indetermination index and the Balanced Modularity (details in section 3). The relational representation allows to understand the properties of those modularization criteria. It allows to easily identify the criteria suffering from a resolution limit, first discussed by Fortunato et Barthelemy (2006). We will complete this theoretical study by some experiments on real and synthetic networks, demonstrating the effectiveness of our classification. This paper is organized as follows : Section 2 presents the Mathematical Relational Analysis approach, we introduce the property of balance for linear criteria and its relation to the property of resolution limit. In Section 3, we present the six linear modularization criteria in the relational formalism. Next, Section 4 presents some experiments on real and artificial graphs to confirm the theoretical properties found previously.\nRelational Analysis approach\nThere is a strong link between the Mathematical Relational Analysis 2 and graph theory : a graph is a mathematical structure that represents binary relations between objects belonging to the same set. Therefore, a non-oriented and non-weighted graph G \u003d (V, E), with N \u003d |V | nodes and M \u003d |E| edges, is a binary symmetric relation on its set of nodes V represented by its adjacency matrix A as follows :\nWe denote the degree d i of node i the number of edges incident to i. It can be calculated by summing up the terms of the row (or column) i of the adjacency matrix :\nN 2 the density of edges of the whole graph.\nPartitioning a graph implies defining an equivalence relation on the set of nodes V , that means a symmetric, reflexive and transitive relation. Mathematically, an equivalence relation is represented by a square matrix X of order N \u003d |V |, whose entries are defined as follows :\n2. For more details about Relational Analysis theory see Marcotorchino et Michaud (1979) and Marcotorchino (1984).\nModularizing a graph implies to find X as close as possible to A. A modularization criterion F (X) is a function which measures either a similarity or a distance between A and X. Therefore, the problem of modularization can be written as a function to optimize F (X) where the unknown X is subject to the constraints of an equivalence relation 3 .\nWe define as well ¯ X and ¯ A as the inverse relation of X and A respectively. Their entries are defined as ¯ x ii \u003d 1 ? x ii and ¯ a ii \u003d 1 ? a ii respectively. In the following we denote ? the optimal number of clusters, that means the number of clusters of the partition X which maximizes the criterion F (X).\nLinear balanced criteria\nEvery linear criterion is an affine function of X, therefore in relational notation it can be written as :\nwhere the function ?(a ii ) depends only on the original data (for instance the adjacency matrix). In the following we will use K to denote any constant depending only on the original data.\nDefinition 1 (Property of linear balance) A linear criterion is balanced if it can be written in the following general form :\nwhere ?(.) and ¯ ?(.) are non negative functions depending only on the original data and\n3. In fact, the problem of modularization can be written in the general form :\nsubject to the constraints of an equivalence relation :\nx ii ? {0, 1} Binary\nThe exact solving of this 0 ? 1 linear program due to the size of the constraints is impractical for big networks. So, heuristic approaches are the only reasonable way to proceed.\nBy replacing ¯\nx by its definition 1 ? x ii , equation (4) can be rewritten as follows :\nFrom this expression we can deduce the importance of the property of balance for linear criteria. If the criterion is a function to maximize, the presence and/or absence of the terms ? ii and ¯ ? ii has the following impact on the optimal solution :\nthe solution that maximizes F (X) is the partition where all nodes are clustered together in a single cluster, so ? \u003d 1 and\n) and\nthen the optimal solution that maximizes F (X) is the partition where all nodes are separated, so ? \u003d N and\nIn other words, the optimization of a linear criterion who does not verify the property of balance will either cluster all the nodes in a single cluster or isolate each node in its own cluster, therefore forcing the user to fix the number of clusters in advance.\nWe can deduce from the previous paragraphs that the values taken by the functions ? and ¯ ? create a sort of balance between the fact of generating as many clusters as possible, ? \u003d N , and the fact generating only one cluster, ? \u003d 1.\nIn the following we will call the quantity N i\u003d1 N i \u003d1 ?(a ii )x ii the term of positive agreements and the quantity N\nii the term of negative agreements.\nDifferent levels of balance\nWe define two levels of balance for all linear balanced criterion :\nDefinition 2 (Property of local balance) A balanced linear criterion whose functions ? ii and ¯ ? ii satisfy\nwhere K L is a constant depending only upon the pair (i, i ) (therefore not depending on global properties of the graph) has the property of local balance.\nSome remarks about definition 2 : -Since K L depends only on properties of the pair (i, i\n) , that is local properties, we call this property local balance.\n-When we talk about global properties we refer to the total number of nodes, the total number of edges or other properties describing the global structure of the graph. -In the particular case of local balance where\n), that is ? ii and ¯ ? ii sum up to a constant, we have the following situation : whereas ? ii increases ¯ ? ii decreases and vice versa.\nLet us consider the special case where ?(a ii ) \u003d a ii , the general term of the adjacency matrix. A null model is a graph with the same total number of edges and nodes and where the edges are randomly distributed. Let us denote the general term of the adjacency matrix of this random graph ¯ ?(a ii ). A criterion based on a null model considers that a random graph does not have community structure. The goal of such a criterion is to maximize the deviation between the real graph, represented by ?(a ii ) and the null model version of this graph, represented by ¯ ?(a ii ) as shown in equation (5).\nThat implies\n. This constraint implies that ¯ ? ii depends upon the total number of edges M . Consequently, the decision of clustering together two sub-graphs depends on a characteristic of the whole network and the criterion is not scale invariant because it depends on a global property of the graph.\nThe definition of null model for linear criteria can be generalized as follows :\nDefinition 3 (Criterion based on a null model) A balanced linear criterion whose functions ? ii and ¯ ? ii satisfy the following conditions :\nwhere g(K G ) is a function depending on global properties of the graph K G is a criterion based on a null model. K G can be for example the total number of edges or nodes. We can deduce from definitions 2 and 3 that a linear criterion can not be local balanced and based on a null model at the same time.\nIn the particular case where ¯ ? decreases if the size of the network increases, it becomes negligible for large graphs. As explained previously, if this term tends to zero, the optimization of the criterion will tend to put together the nodes more easily. For instance, a single edge between two sub-graphs would be interpreted by the criterion as a sign of a strong correlation between the two clusters, and optimizing the criterion would lead to the merge of the two clusters. Such a criterion is said to have a resolution limit.\nThe resolution limit was introduced by Fortunato et Barthelemy (2006), where the authors studied the resolution limit of the modularity of Newman-Girvan. They demonstrated that modularity optimization may fail to identify modules smaller than a scale which depends on global characteristics of the graph even weakly interconnected complete graphs, which represent the best identifiable communities, would be merged by this kind of optimization criteria if the network is sufficiently large. According to Kumpula et al. (2007) the resolution limit is present in any modularization criterion based on global optimization of intra-cluster edges and extracommunity links and on a comparison to any null model.\nIn section 4 we will show how criteria having a resolution limit fail to identify certain groups of densely connected nodes.\nModularization criteria in relational notation\nGraph clustering criteria depend strongly on the meaning given to the notion of community. In this section, we describe six linear modularization criteria and their relational coding in Table 1. We assume that the graphs we want to modularize are scale-free, that means that their degree distribution follows a power law.\n1. The Zahn-Condorcet criterion (1785, 1964) : C.T. Zahn (see Zahn (1964)) was the first author who studied the problem of finding an equivalence relation X, which best approximates a given symmetric relation A in the sense of minimizing the distance of the symmetric difference. However the criterion defined by Zahn corresponds to the dual Condorcet\u0027s criterion (see Condorcet (1785)) introduced in Relational Consensus and whose relational coding is given in Marcotorchino et Michaud (1979). This criterion requires that every node in each cluster be connected to at least as half as the total nodes inside the cluster. Consequently, for each cluster the fraction of within cluster edges is at least 50% (see Conde-Céspedes (2013) for the demonstration).\n2. The Owsí nski-Zadro? zny criterion (1986) (see Owsi´nskiOwsi´nski et Zadro? zny (1986)) it is a generalization of Condorcet\u0027s function. It has a parameter ?, which allows, according to the context, to define the minimal percentage of required within-cluster edges : ?. For ? \u003d 0.5 this criterion is equivalent to Condorcet\u0027s criterion. The parameter ? defines the balance between the positive agreements term and the negative agreements term. For each cluster the density of edges is at least ?% (see Conde-Céspedes (2013)).\n3. The Newman-Girvan criterion (2004) (see Newman et Girvan (2004)) : It is the best known modularization criterion, called sometimes simply modularity. It relies upon a null model. Its definition involves a comparison of the number of within-cluster edges in the real network and the expected number of such edges in a random graph where edges are distributed following the independence structure (a network without regard to community structure). In fact, the modularity measures the deviation to independence. As mention in the previous section, this criterion, based on a null model and it has a resolution limit (see Fortunato et Barthelemy (2006)). In fact, as the network becomes larger M ?? ?, the term ¯ ? ii \u003d ai.a .i 2M tends to zero for since the degree distribution follows a power law.\n4. The Deviation to Uniformity (2013) This criterion maximizes the deviation to the uniformity structure, it was proposed in Conde-Céspedes (2013). It compares the number of within-cluster edges in the real graph and the expected number of such edges in a random graph (the null model) where edges are uniformly distributed, thus all the nodes have the same degree equal to the average degree of the graph. This criterion is based on a null model and it has a resolution limit. indeed ? ?? 0 as N ?? ?.\nThe Deviation to Indetermination (2013)\nAnalogously to Newman-Girvan function, this criterion compares the number of within-cluster edges in the real network and the expected number of such edges in a random graph where edges are distributed following the indetermination structure 4 (a graph without regard to community structure), introduced in Marcotorchino (2013) and . The Deviation to Indetermination is based on a null model, therefore it has a resolution limit.\nThe Balanced modularity (2013) This criterion, introduced in Conde-Céspedes et\nMarcotorchino (2013), was constructed by adding to the Newman-Girvan modularity a term taking into account the absence of edges ¯ A. Whereas Newman-Girvan modularity compares the actual value of a ii to its equivalent in the case of a random graph ai.a .i 2M , the new term compares the value of ¯ a ii to its version in case of a random graph\n. It is based on a null model and it has a resolution limit.\nwhere 4. There exists a duality between the independence structure and the indetermination structure (see Marcotorchino (1984), Marcotorchino (1985) and Ah-Pine et Marcotorchino (2007)).\nThe six linear criteria of Table 1 verify the property of balance, so it is not necessary to fix in advance the number of clusters, more specifically : From Tables 1 and 2 one can easily deduce that for the criteria having a resolution limit the quantity ¯ ? ii decreases when the size of the graph becomes larger.\nTests with real and artificial networks\nWe modularized six real networks of different sizes : Jazz (Gleiser et Danon (2003)), Internet (Hoerdt et Magoni (2003)), Web nd.edu (Albert et al. (1999)), Amazon (Yang et Leskovec (2012) 5 ) and Youtube (Mislove et al. (2007)). We ran a generic version of Louvain Algorithm (see Campigotto et al. (2014) and Blondel et al. (2008)) until achievement of a stable value of each criterion. The number of clusters obtained for each network is shown in Table 3. Table 3 shows that the Zahn-Condorcet and Owsi´nskiOwsi´nski-Zadro? zny criteria generate many more clusters than the other criteria having a resolution limit, for which the number of clusters is rather comparable. Moreover, this difference increases with the network size. Notice that the number of clusters for the Owsi´nskiOwsi´nski-Zadro? zny criterion decreases with ?, that is the minimal required fraction of within-cluster edges, so the criterion becomes more flexible.\nOnly ground-truth overlapping communities are defined on these previuos real networks. This fact makes difficult to judge the quality of the obtained partitions. That si why we generated five benchmark LFR graphs (see Lancichinetti et al. (2008)) of different sizes 1000, 5000, 10000, 100000 and 500000. The input parameters are the same as those considered in Lancichinetti et Fortunato (2009). The average degree is 20, the maximum degree 50, the exponent 5. the data was taken from http://snap.stanford.edu/data/com-Amazon.html. of the degree distribution is -2 and that of the community size distribution is -1. In order to test the existence of resolution limit we chose small communities sizes, ranging from 10 to 50 nodes, and a low mixing parameter, 0.10. So, the communities are clearly defined. Figure 1 shows the average number of clusters for 100 runs of the generic Louvain algorithm. Network size: N FIGURE 1 -Average number of cluster for artificial LFR graphs (logarithmic scale). Figure 1 shows clearly the difference between the behaviour of those criteria having a resolution limit (NG, DU, DI and BM) and the behaviour of criteria locally defined (ZC and OZ). As the size of the network increases the four criteria suffering from resolution-limit detect fewer clusters than those predefined. The number of clusters is rather comparable for these four functions, one reason can be the fact that the term of negative agreements tends to zero when the network gets bigger. Conversely, the criteria locally defined identified more clusters than those predefined, specially ZC. The criterion which best approaches the real number of clusters is OZ with ? \u003d 0.2. Figure 2 shows the average Normalized Mutual Information for the partitions in Figure 1. Figure 2 shows that the average NMI decreases with the network size for criteria having a resolution limit. The criterion with the highest NMI is OZ with ? \u003d 0.2 which guarantees an within-cluster density of 20%.\nNumber of clusters\nConclusions\nWe presented six linear modularization criteria in relational notation, Zahn-Condorcet, Owsi´nskiOwsi´nski-Zadro? zny, the Newman-Girvan modularity, the Deviation to Uniformity index, the Deviation to Indetermination index and the Balanced-Modularity. This notation allowed us to easily identify the criteria suffering from a resolution limit. We found that the first two criteria had a local definition whereas the others, based on a null model, had a resolution limit. These findings were confirmed by modularizing real and artificial graphs using a generic version of the Louvain algorithm. We compared the number of clusters found by the six criteria and the Normalized Mutual information for artificial graphs. The results showed that those criteria ba-\n"
  },
  {
    "id": "245",
    "text": "Introduction\nEn apprentissage automatique, la précision et le rappel sont des mesures classiques pour évaluer les résultats et la performance des algorithmes utilisés. Ces mesures sont essentiellement utilisées en apprentissage supervisé (Sokolova et al., 2006), en classification simple (Jain, 2010) et croisée Hanczar et Nadif (2013) et en recherche d\u0027information (Manning et al., 2008). Dans ce dernier cas, la performance de l\u0027algorithme employé est évaluée à partir de la similarité entre l\u0027ensemble de documents retrouvés et l\u0027ensemble des documents cibles. Cette similarité se base sur la précision et le rappel. De la même manière en classification simple (resp. croisée), les algorithmes identifient des groupes (resp. biclusters) d\u0027éléments qui sont comparés à des groupes (resp. biclusters) de référence. En apprentissage supervisé, l\u0027évaluation d\u0027un classeur se fait en comparant les classes prédites avec les vraies classes sur un ensemble de test. On mesure la similarité entre les classes prédites et les vraies classes en calculant leur préci-sion et rappel. Cependant cette approche ne tient pas compte du taux de vrais négatifs. Pour ces raisons, on préfère dans certains cas utiliser le couple sensibilité-spécificité que le couple précision-rappel dans ce contexte. La précision et le rappel sont donc deux mesures très utilisées dans les procédures d\u0027évaluations de nombreux domaines. Il est extrêmement fréquent de combiner ces deux valeurs afin de construire des indices de performance tel que la F-mesure ou l\u0027indice de Jaccard (Albatineh et Niewiadomska-Bugaj, 2011).\nPar défaut les indices de performance donnent la même importance à la précision et au rappel. Or dans de nombreux cas, on peut vouloir privilégier l\u0027un par rapport à l\u0027autre. Par exemple, en génomique des groupes de gènes ayant des profils d\u0027expression similaires sont identifiés en utilisant des méthodes de classification. Ces groupes sont comparés à des classifications de gènes issues de bases de connaissance afin d\u0027estimer leur pertinence biologique (Datta et Datta, 2006). L\u0027objectif de ces analyses est de capturer le plus d\u0027information biologique dans les groupes de gènes, on veut donc privilégier le rappel par rapport à la précision dans ce contexte. Certains indices de performance ont une variante introduisant un paramètre permettant de contrôler le compromis précision-rappel comme c\u0027est le cas de la F-mesure qui est une généralisation de l\u0027indice de Dice. Pour d\u0027autre mesures, le contrôle du compromis précision-rappel est plus difficile, comme c\u0027est le cas de l\u0027indice de Jaccard. Dans cet article nous analysons les différents indices de performance en fonction du compromis précision-rappel. Nous proposons également un nouvel outil d\u0027analyse qu\u0027est l\u0027espace de compromis qui présente de nombreux avantages par rapport à l\u0027espace précision-rappel.\nDans la section 2, nous présentons les différents indices de performance étudiés ainsi que leurs variantes sensibles au compromis. Dans la section 3, nous rappelons les propriétés de l\u0027espace précision-rappel. Nous analysons le comportement des différents indices dans cet espace. Dans la section 4, nous définissons l\u0027espace de compromis et nous montrons comment représenter les performances par les courbes de compromis. Dans la section 5, nous montrons les avantages à travailler dans l\u0027espace de compromis en particulier pour la sélection de modèles et la comparaison d\u0027algorithmes. Nous illustrons ces propriétés avec un exemple dans le contexte du biclustering. Dans la section 6, nous exposons nos conclusions et perspectives.\n2 Indices basés sur le couple précision et rappel\nDéfinitions\nSoit D un ensemble des données contenant N éléments. Nous appelons groupe cible le sous-ensemble T ? D que nous recherchons. Un algorithme dont l\u0027objectif est de retrouver le groupe cible produit un groupe X. Pour mesurer la qualité de ce groupe X, un indice de performance est utilisé afin d\u0027évaluer la similitude entre T et X. Ces indices de performances sont généralement basés sur deux valeurs : la précision et le rappel. La précision représente la proportion de X qui recouvre T quant au rappel il exprime la proportion de T retrouvé par X. Ces deux indices prennent les formes suivantes :\nLes principaux indices de performances utilisés sont une combinaison de la précision et du rappel. Dans cet article nous étudierons les quatre plus populaires : l\u0027indice de Kulczynski, Fmesure, Folke et Jaccard. Ces travaux pourront être facilement étendus à d\u0027autres indices. Par défaut chacun de ces indices donne la même importance à la précision et au rappel. Cependant on peut construire des versions pondérées permettant de privilégier la précision par rapport au rappel ou inversement.\nL\u0027indice de Kulczynski\nL\u0027indice de Kulczynski est la moyenne arithmétique de la précision et du rappel.\nUne version pondérée introduit le paramètre R ? [0, +?] qui permet de contrôler le compromis entre la précision et le rappel. Plus R est grand, plus le rappel est important, le point d\u0027équilibre est atteint pour R \u003d 1. Nous réécrivons cet indice en effectuant le changement de variable suivant : ? \u003d R R+1 , ? ? [0, 1] contrôle désormais le compromis et le point d\u0027équilibre est atteint pour ? \u003d 0.5.\nLa F-mesure\nLa F-mesure, appelée aussi indice de Dice, est le rapport entre l\u0027intersection et la somme des tailles du groupe X et du groupe cible T . C\u0027est aussi la moyenne harmonique entre la précision et la rappel. \nL\u0027indice de Folke\nL\u0027indice de Folke correspond à la moyenne géométrique de la précision et du rappel.\nIl est possible de pondérer le moyenne géométrique en introduisant un paramètre ? ? [0, 1]. Plus ? est grand plus le rappel est important et le point d\u0027équilibre est atteint pour ? \u003d 0.5.\nL\u0027indice de Jaccard\nL\u0027indice de Jaccard est le rapport entre l\u0027intersection et l\u0027union du groupe X et le groupe cible T .\nIl n\u0027est pas facile de définir une version pondérée de l\u0027indice de Jaccard à cause de la pré-sence du terme pre.rec au dénominateur. Nous voulons un indice pondéré ayant les proprié-tés suivantes : I Jac (T, X, ?) ? [0, 1] ; I Jac (T, T, ?) \u003d 1 ; I Jac (T, X, 0.5) \u003d I Jac (T, X) ; I Jac (T, X, 0) \u003d pre ; I Jac (T, X, 1) \u003d rec. Pour cela nous proposons l\u0027indice suivant :\n3 L\u0027espace précision-rappel L\u0027espace précision-rappel, illustré dans la figure 1, est un espace à deux dimensions dans lequel les abscisses et ordonnées représentent respectivement le rappel et la précision (Buckland et Gey, 1994). Une performance est représentée par un point dans cet espace (le point blanc par exemple). Le principe de l\u0027espace précision-rappel est proche de celui de l\u0027espace ROC qui représente le taux de vrais positifs en fonction du taux de faux positifs (Fawcett, 2006). Plusieurs relations ont d\u0027ailleurs été identifiées entre ces deux espaces (Davis et Goadrich, 2006). Un point dans l\u0027espace précision-rappel représente tous les groupes de taille |X| \u003d |T | rec pre ayant une intersection avec le groupe cible de |T ? X| \u003d |T |rec. Le point (1,1) (point noir), maximisant la précision et le rappel, représente le groupe idéal et dans ce cas il y a une parfaite correspondance avec le groupe cible (X \u003d T ). Le point (1, |D| rec puisqu\u0027on a |D| ? |X|. Tous les groupes dont la performance se situe sur la droite pre \u003d |T | |D| rec sont ceux dont |T ? X| est minimale. Cette droite représente tous les groupes dont |T ? X| est nulle. La plupart des algorithmes a un paramètre permettant de contrôler la taille du résultat X. Pour chaque taille de X on obtient des valeurs de précision et rappel différentes. La performance d\u0027un algorithme peut donc être représentée par un ensemble de points et approximée par une courbe dans l\u0027espace précision-rappel. Dans la figure 1, on donne un exemple de courbe précision-rappel. On peut tirer plusieurs informations sur les performances de ces différents groupes même sans se référer à un indice en particulier. Si un point domine un autre, c-à-d si sa précision et son rappel sont supérieurs, alors on peut conclure qu\u0027il aura une meilleure performance quelque soit l\u0027indice utilisé. Les points noirs représentent les points dominants de la courbe, il ne sont dominés par aucun autre point et représentent les performances des meilleurs groupes. Il n\u0027y a pas de rapport de domination entre ces types de points, il est nécessaire d\u0027utiliser un indice pour les comparer.\nLe comportement des différents indices de performances peut se visualiser en dessinant leur iso-ligne dans l\u0027espace précision-rappel. Une iso-ligne est un ensemble de points dans l\u0027espace précision-rappel ayant tous la même valeur d\u0027indice (Flach, 2003;Hanczar et Nadif, 2013). La figure 2 montre les iso-lignes des indices de Kulczynski, F-mesure, Folke et Jaccard. Les lignes en gras représentent les iso-lignes lorsque ? \u003d 0.5. Pour les quatre indices, nous observons que les iso-lignes ont une symétrie autour de l\u0027axe pre \u003d rec, ceci signifie que la précision et le rappel ont la même importance. Par contre les différents indices ne considèrent pas la différence entre précision et rappel de la même façon. Cette différence n\u0027est pas prise en compte dans l\u0027indice de Kulczynski, alors que les autres indices la pénalisent. L\u0027indice de Folke pénalise moins que la F-mesure et l\u0027indice de Jaccard. Ces deux derniers sont équivalents car ils sont compatibles,\nDans la figure 2, les lignes en pointillées représentent les iso-lignes pour ? \u003d 0.2 et les lignes pleines ? \u003d 0.8. La modification de la valeur de ? déforme les iso-lignes, ce qui permet de donner plus d\u0027importance à la précision ou au rappel. A noter que pour pre \u003d rec les indices de Kulczynski, F-mesure et Folke retournent la même valeur quelque soit ?. L\u0027indice de Jaccard a un comportement différent, il pénalise le fait que ? s\u0027approche de 0.5. Dans la L\u0027espace de compromis, que nous proposons, offre un nouvel outil de visualisation des performances des résultats ou des algorithmes en fonction du compromis précision-rappel. Il y a certaines similitudes avec les \"cost curves\" utilisées en apprentissage supervisé (Drummond et Holte, 2006). L\u0027espace de compromis représente en abscisse ? et en ordonnée l\u0027indice de performance. La performance d\u0027un groupe X est représentée dans cet espace par une courbe f (?). On a une correspondance entre les points de l\u0027espace précision-rappel et les courbes Cette dernière courbe définit le domaine d\u0027application des indices de performances pour un problème donné, illustré dans la figure 3 par les zones blanches. Un point situé dans l\u0027une des zones grises, signifie que le groupe correspondant à de moins bonnes performances que le groupe maximal et peut donc être considéré comme non informatif. On constate que le domaine d\u0027application de l\u0027indice de Kulczynski est beaucoup plus petit que celui des autres indices. Cela est dû au fait que cet indice ne pénalise pas la différence entre précision et rappel. La ligne en pointillé représente le groupe contenant un unique élément appartenant au groupe cible. Le groupe parfait est représenté par la droite f (?) \u003d 1. A l\u0027inverse les groupes ayant une intersection nulle sont représentés par la droite f (?) \u003d 0. Les groupes aléatoires sont représentés par les courbes partant du point (0, |T | |D| ).\nCourbe optimale de compromis\nComme nous l\u0027avons illustré dans la figure 1, la performance d\u0027un algorithme peut être représentée par une courbe dans l\u0027espace précision-rappel. A chaque point de cette courbe correspond une courbe dans l\u0027espace de compromis. On peut représenter la courbe précision-rappel par un ensemble de courbes dans l\u0027espace de compromis. La figure 4 donne la représentation de la courbe précision-rappel de la figure 1 dans l\u0027espace de compromis pour les différents indices de performance. On s\u0027intéressera particulièrement à l\u0027enveloppe supérieure de cet ensemble de courbes, représentée en gras dans la figure 4 que nous appellerons courbe optimale de compromis. Cette dernière représente les meilleurs performances de l\u0027algorithme pour tous les compromis. On s\u0027aperçoit que les courbes formant l\u0027enveloppe supérieure correspondent tous à des points dominants de la courbe précision-rappel. Les points dominés ont toujours leur courbe en dessous de la courbe optimale de compromis. Dans le cas de l\u0027indice de Kulczynski, les courbes formant l\u0027enveloppe supérieure correspondent aux points de l\u0027enveloppe convexe de la courbe précision-rappel. Ces courbes de compromis permettent d\u0027analyser les résultats bien plus facilement que les courbes précision-rappel. 5 Application des courbes de compromis 5.1 Sélection de modèles L\u0027utilisation de l\u0027espace de compromis permet d\u0027identifier très facilement le résultat optimal pour un compromis donné. Ceci est illustré dans la figure 5 à travers un problème de classification croisée. Nous avons généré une matrice de données aléatoires dans laquelle un bicluster a été introduit, ce dernier suit un modèle additif selon la définition de Madeira et Oliveira (2004). Nous utilisons l\u0027algorithme CC (Cheng \u0026 Church) pour retrouver ce bicluster (Cheng et Church, 2000). La similarité entre le bicluster retourné par l\u0027algorithme et le bicluster recherché est alors calculée par les différents indices de performance. Cet algorithme dispose d\u0027un paramètre permettant de contrôler la taille du bicluster retourné, nous pouvons donc représenter les performances de cet algorithme par une courbe précision-rappel (figure 5). A partir de cette courbe il n\u0027est pas facile de déterminer le meilleur bicluster pour un compromis de précision-rappel donné. Même en ajoutant les iso-lignes au graphique, la comparaison des différents biclusters n\u0027est pas intuitive. Dans la figure 5 est représentée la courbe optimale de compromis pour la F-mesure. A partir de cette courbe on peut instantanément identifier le meilleur bicluster pour un compromis donné. On a aussi une décomposition de la valeur de ? en une série d\u0027intervalles qui sont délimités sur le graphique par les lignes verticales pointillées, pour lesquels le meilleur bicluster est donné. Sur notre exemple on constate qu\u0027il y a sept intervalles, nous nous intéresserons donc qu\u0027aux sept biclusters correspondants, identifiés sur la figure par leur taille. Pour le dernier intervalle (? \u003e 0.74) le meilleur bicluster est la matrice entière, la courbe optimale de compromis est confondue avec la courbe du bicluster maximal. Notons qu\u0027il n\u0027est pas possible d\u0027identifier visuellement ces biclusters dans l\u0027espace précision-rappel car ils ne correspondent ni à l\u0027ensemble des points dominants ni à l\u0027enveloppe convexe de la courbe précision-rappel (sauf dans le cas de l\u0027indice de Kulczynski). Il est également très facile de travailler avec des contraintes sur la précision ou le rappel dans l\u0027espace de compromis. Nous rappelons que la précision et le rappel se lisent à l\u0027extrémité de chaque courbe de compromis. Lorsqu\u0027on demande une précision minimale pre min , il suffit de considérer unique les courbes de compromis qui partent au-dessus du seuil minimum c-à-d f (0) \u003e pre min . De même avec un rappel minimum rec min , on ne conserve que les courbes qui arrivent au-dessus du seuil de rappel c-à-d f (1) \u003e rec min .\nComparaison d\u0027algorithmes\nL\u0027espace de compris simplifie également grandement la comparaison des algorithmes. Nous reprenons l\u0027exemple de classification croisée précédent dans lequel un autre algorithme, ISA (Bergmann et al., 2003), est testé et comparé à CC. Les performances de ce nouvel algorithme sont représentées dans l\u0027espace précision-rappel et l\u0027espace de compromis par la courbe grise dans la figure 6. Dans l\u0027espace précision-rappel les deux courbes se croisent plusieurs fois, aucun des deux algorithmes n\u0027est donc absolument meilleur que l\u0027autre. Il est difficile de voir dans quelles conditions CC est meilleur que ISA et inversement. Dans l\u0027espace de compromis on visualise immédiatement quel est le meilleur algorithme pour chaque valeur de compromis. Pour ? \u003c 0.28 CC est meilleur que ISA, pour 0.28 \u003c ? \u003c 0.83, ISA est meilleur, pour ? \u003e 0.83 les deux algorithmes retournent un bicluster contenant toute la matrice de donnée et ont donc des performances identiques. La distance entre les deux courbes permet de FIG. 5 -Identification des meilleurs biclusters dans l\u0027espace précision-rappel et l\u0027espace de compromis. A gauche, la courbe précision-rappel. A droite, la courbe optimale de compromis.\nvisualiser la différence de qualité entre les deux algorithmes. Dans l\u0027espace précision-rappel les courbes des deux algorithmes se croisent trois fois, laissant penser qu\u0027il y a deux intervalles de ? pour lesquelles CC est meilleur (de même pour ISA). Les courbes optimales de compromis montrent que l\u0027identité du meilleur algorithme ne change qu\u0027une fois, en ? \u003d 0.28. Dans l\u0027espace précision-rappel, CC a une meilleurez précision que ISA ; 14 fois sur 20 ce qui laisse penser que CC est plus souvent meilleur que ISA. L\u0027espace de compromis nous montre qu\u0027au contraire l\u0027intervalle [0, 0.28] pour lequel CC est meilleur est deux fois plus petit que celui de ISA [0.28,0.83]. Cet exemple illustre bien la facilité de la comparaison d\u0027algorithmes dans l\u0027espace de compromis.\nFIG. 6 -Identification du meilleur algorithme. A gauche, les courbes précision-rappel. A droite, les courbes optimales de compromis.\n"
  },
  {
    "id": "246",
    "text": "Introduction\nLes requêtes skyline sont importantes dans les applications qui nécessitent la localisation des réponses selon plusieurs critères. Ayant un ensemble de points dans un espace vectoriel de d dimensions, un algorithme traitant ce type de requêtes doit retourner l\u0027ensemble des points de S dits non dominés. Il est meilleur pour ce type d\u0027algorithmes de fonctionner progressivement Kossmann et al. (2002) car les utilisateurs sont souvent impatients de recevoir des réponses. La relation de dominance se définit comme suit Börzsonyi et al. (2001) : Soit S un ensemble de données de d dimensions (d critères) sur lequel va porter l\u0027opérateur skyline. Soit D l\u0027ensemble de toutes les dimensions D \u003d {d 1 , , d d }. Soient p et q deux points de S. La relation de dominance (?) suivant D est pour 1 ? i, j ? d ,p domine q ?? {?d i ? D, p(i) ? q(i)} et {?d j ? D, p(j) \u003c q(j)}. Lorsque aucun point ne domine l\u0027autre on dit qu\u0027ils sont non dominés ou concurrents. L\u0027opérateur skyline renvoie l\u0027ensemble des points concurrents, suivant toutes les dimensions D : SkyD(S) \u003d {p ? S/ ? S : q ? p} Dans ce papier, nous présentons une solution analytique pour déduire l\u0027ensemble de points candidats afin d\u0027éviter de parcourir l\u0027ensemble S entièrement. Nous donnons un nouveau théo-rème pour l\u0027élimination des points non candidats. Notre méthode est basée sur le tri Tan et al. (2001) et à la différence avec ce travail, où les tests entre les points balayent tout l\u0027ensemble S, nous donnons des théorèmes pour la déduction des points les plus évidents et qui constituent les premières solutions à présenter. Nous montrerons que la combinaison de DC Divide-andConquer avec notre méthode fournit des résultats meilleurs que lorsqu\u0027il est appliqué tout seul. Le reste de ce papier se présente comme suit. La section 2 présente les travaux liés à cette problématique. La section 3 donne notre approche. La section 4 présente les résultats des expérimentations. La conclusion et les travaux futurs sont donnés dans la section 5. Börzsonyi et al. (2001) était le premier travail ayant adapté l\u0027optimisation au sens de Pareto dans les bases de données. Intuitivement, le calcul du skyline consiste à comparer chaque point p avec tous les autres et si aucun point ne le domine alors p est un point skyline. L\u0027algorithme BNL Börzsonyi et al. (2001) utilise cette technique directe. Il met en mémoire une liste candidate et teste à chaque fois si un nouveau point p domine un ou plusieurs points déjà insérés. Si c\u0027est le cas, il est inséré et l\u0027ensemble des points dominé est écarté sinon il passe au point suivant. Cette méthode peut être utilisée facilement et ne requiert pas de prétraitement, sauf qu\u0027elle est gourmande en mémoire et en temps de calcul. DC Börzsonyi et al. (2001) divise l\u0027entrée en plusieurs partitions et détermine le skyline de chaque partition. Par la suite, les skyline sont fusionnés et les points dominés sont écartés. Cette méthode est meilleure que BNL mais souffre des multiples duplications lors de la fusion. Notons que ni BNL ni DC ne fonctionnent en on-line. L\u0027algorithme Bitmap Tan et al. (2001) consiste à encoder dans des vecteurs bitmap toutes les informations de chaque point selon le nombre de points distincts sur chaque axe. La comparaison des vecteurs bitmap se fait par la suite. Bitmap est progressive mais nécessite beaucoup d\u0027opérations et de codage en commençant par la détermination des points distincts dans chaque axe car il y aura beaucoup de tests dupliqués. Index Tan et al. (2001) consiste à trier les données sur chacun des d axes dans un ordre croissant. Afin de déter-miner le skyline, les points sont testés de façon circulaire. Le problème est que la récupération des coordonnées des points peut prendre du temps. Cette méthode est bien adaptée pour les applications on-line, elle retourne aussitôt les premiers points, sauf que les auteurs ne déduise pas les skyline induit par le tri. NN Kossmann et al. (2002) est un algorithme qui utilise les RTrees pour indexer les données. Il partitionne l\u0027espace selon chaque axe selon le point le proche voisin de l\u0027origine. NN est progressif et est efficace dans un espace à deux dimensions, mais il souffre du problème de duplications des éliminations pour 3 dimensions et plus. A partir de 4 dimensions, il devient difficile de l\u0027appliquer. Les auteurs proposent différentes techniques pour remédier à ces problèmes. Branch and Bound Skyline Papadias et al. (2003) exploite les R-Tree, la méthode de Branch and Bound et NN afin de calculer, en on-line, le skyline. Son plus grand problème est qu\u0027il souffre de requêtes redondantes. Yuan et al. (2005) proposent Skycube. Il calculent les skyline fils de toutes les combinaisons possibles des points dans le treillis. Lorsqu\u0027ils passent au niveau supérieur ou inférieur du treillis ils fusionnent ces fils.\nTravaux liés\nLa méthode DCRD\nNotre méthode DCRD pour Divide-and-Conquer for Reduced Data est une méthode analytique qui détermine l\u0027espace candidat en se basant sur le tri. L\u0027utilisation directe de l\u0027espace Pareto est simple pour un espace à 2 dimensions, mais au-delà de 3 dimensions, il faut ajouter des méthodes efficaces pour calculer cet espace. Nous donnons un nouveau théorème qui permet de déduire cet espace. Dans Index, les tests de dominance se font entre tous les points sans l\u0027exploitation de la concurrence induite par le tri. Par exemple, il est impossible qu\u0027un point A, ayant la valeur minimale unique sur un axe X, soit dominé par un autre point. Ce qui nous mène à donner le théorème 1. Preuve. Suite à la discussion précédente, sur l\u0027axe i, il est impossible qu\u0027un autre point puisse dominer le point p, puisqu\u0027aucune valeur sur cet axe ne sera inférieure à celle de p. Si pour tous les autres axes, le point q domine p, il sera impossible qu\u0027il le domine sur i, d\u0027où p est soit concurrent avec q soit le domine. Définition du conflit entre les points ayant des valeurs minimales sur le même axe. Il est fréquent que deux points ou plus aient la même valeur minimale sur un axe. Ainsi, il faut résoudre ce conflit de dominance avant de passer au calcul du skyline définitif. Ainsi, nous donnons le théorème 2 suivant : Théorème 2 : Existence de plusieurs points minimaux sur un axe i Soient p et q deux points de S tel qu\u0027il existe un axe i avec p Preuve. Ceci revient à résoudre le conflit entre p et q dans les autres sous-espaces. Le test montrera soit la dominance soit la concurrence entre eux sur les autres axes.\nPhase 1 : Tri des données et déduction des premiers points skyline\ntrier les valeurs de S par ordre croissant ; -Extraire l\u0027ensemble Sky des premiers points skyline en utilisant les théorèmes 1 et 2 ;\nPhase 2 : Réduction de l\u0027espace de données\nCette phase consiste à limiter l\u0027espace de données en filtrant l\u0027ensemble de données selon deux points appelés M in sys et M ax sys , autrement dit, on détermine l\u0027espace de dominance dans lequel se trouvent tous les points candidats. M in sys est le même que le point idéal de Pareto. Le point M ax sys est un point virtuel qui permet de délimiter cet espace. Il sert à éli-miner un espace important non utile. En deux dimensions, il se confond au point nadir mais, à plus de dimensions, ils sont différents. Le point nadir est un point pour lequel la fonction à optimiser est maximale ? et ce n\u0027est pas notre cas car M ax sys est dominé. D\u0027une manière analytique, nous déterminons les points M in sys et M ax sys à partir des coordonnées des points skyline déduits de la phase précédente : M in sys et M ax sys possède chacun d composantes. Chaque composante M in sys [i] (resp. M ax sys [i]) de M in sys (resp. M ax sys ) est la valeur minimale(resp. maximale) de toutes les composantes des points de Sky. Calcul de l\u0027espace des candidats. Dans cette étape, l\u0027ensemble des données candidats est ré-duit à un hypercube. Pour l\u0027obtenir, nous énonçons et appliquons le théorème3 suivant :  \n. p ne peut dominer q sur l\u0027axe d. Donc, il y a au moins un point m appartenant au skyline actuel qui domine p.\nPhase 3 : Calcul du skyline final\nA l\u0027issue de la phase 2, l\u0027espace déduit contient les points candidats. Il ne reste que de les comparer entre eux et éliminer les points dominés, nous appliquons ainsi DC.\nExpérimentations\nLes expérimentations ont été réalisées dans une machine dotée d\u0027Intel Core i5 2,50 GHz, et de 4 Go de RAM, sous Windows 7, 64 bits. Le programme est écrit sous Java. MySQL 5.1.41 est utilisé comme système gestion des bases de données. Nous avons utilisé les mêmes bases de données synthétiques de Börzsonyi et al. (2001)  Kossmann et al. (2002). Il s\u0027agit de trois types de données : corrélées, anti-corrélées et indépen-dantes. Nous avons calculé le temps nécessaire en secondes pour retourner le skyline en tenant compte de la dimensionalité et la cardinalité.\nComparaison entre DC et DCRD dans le type anti-corrélé. En fixant la dimension à 5 et en variant la cardinalité de 10000 à 100000 points. La figure 1 montre que pour ce type de données, DCRD a rendu les résultats dans des temps meilleurs puisqu\u0027il y a eu des éliminations de points inutiles. Pour 10000 points et en variant la dimension de 1 à 10, nous remarquons sur la figure 6 que DCRD a commencé à rendre les réponses rapidement à partir de d\u003d6. On déduit ainsi que le Comparaison entre DC et DCRD dans le type corrélé Ce type de données est intéressant et facile à manier ; même un algorithme naïf pourrait présenter de bonnes performances. En comparant ces deux méthodes sur la cardinalité et en fixant le nombre d\u0027attributs à 5, nous remarquons sur la figure 3 que DCRD a dépassé de loin DC. Ceci est dû au nombre important de points qui ont été éliminés. La faiblesse de DC est qu\u0027il exécute des tests de dominance sur tout l\u0027ensemble de données d\u0027une façon aveugle. De même, en comparant DC et DCRD selon la dimension, nous avons fixé la cardinalité à 10000 et nous avons varié le nombre d\u0027attributs. Puisque le nombre de points éliminé est important, la figure 4 montre que DCRD a été très rapide alors que DC a consommé plus de temps. Ceci est toujours le cas puisque DC ne fait aucun traitement préalable et exécute des partitions sur tout l\u0027ensemble d\u0027entrée. Ceci a un effet sur les temps de réponse.\n"
  },
  {
    "id": "247",
    "text": "Introduction\nL\u0027explosion d\u0027internet, couplée à l\u0027effet de la mondialisation, a pour résultat d\u0027interconnecter les personnes, les entreprises, les états. Le côté déplaisant de cette interconnexion mondiale des Systèmes d\u0027Information réside dans un phénomène appelé \"Cybercriminalité\". Des personnes, des groupes mal intentionnés ont pour objectif de nuire dans un but pécuniaire ou pour une \"cause\", aux informations d\u0027une entreprise, d\u0027une personne voire d\u0027un Etat. Il n\u0027est pas rare que des faits de \"cyber-attaques\" soient relatés dans les médias envers des grandes socié-tés comme \"Google\",\"Visa\",\"Sony\", \"Apple\". La sécurité d\u0027un Système d\u0027Information se doit d\u0027être présente afin de garantir la confidentialité, l\u0027intégrité, la disponibilité de l\u0027information. De ce fait, il existe une multitude d\u0027équipements de sécurité qui permettent de détecter les comportements anormaux. Un des principaux équipements de sécurité est le \"Pare-Feu\" 1 ou plus communément appelé \"Firewall\". Il a pour mission comme le décrit Al-Shaer et Hamed (2003) de filtrer , selon une politique fondée sur les flux autorisés à pénétrer dans un réseau selon leurs sources, leurs destinations et les services souhaités (navigation internet, transfert de fichiers, etc... ). Par son positionnement, il donne une visibilité totale de l\u0027ensemble des flux. Cet équipement offre aussi la possibilité \"d\u0027historiser\" vers des journaux les flux ayant été autorisés ou interdits. L\u0027exploitation et l\u0027analyse des journaux d\u0027événements liés aux équi-pements de sécurité sont devenues primordiales pour la maîtrise des flux et la détection des intrusions ainsi que pour la vérification du bien fondé de la politique de filtrage mise en place (Golnabi et al, 2006). Dans ce contexte, les constructeurs d\u0027équipements de filtrage mettent à disposition des logiciels permettant d\u0027analyser les flux. Ces derniers nécessitent un accès et une connaissance dudit équipement. La détection des anomalies et des comportements anormaux est conséquemment réservée à ces seuls utilisateurs. La problématique de la représentation des événements de sécurité est tellement répandue que plusieurs outils ont même été regroupés au . Le principe est de modéliser un système de \"monitoring\" et de visualisation des données réseau en temps réel permettant de détecter rapidement les tentatives d\u0027intrusions.\nComposition du projet \"D113\"\nLe projet \"D113\" est composé de quatre phases qui s\u0027inscrivent dans le cadre d\u0027un travail de thèse en sécurité s\u0027appuyant sur des données issues des différents équipements et outils de sécurité. Les différentes phases du projet se déclinent selon la liste suivante.\n- Notre démonstration de logiciel portera uniquement sur la phase 1 qui est le préambule à la \"fouille de données\" qui sera effectuée dans les phases suivantes. La première phase constitue un tout en soi dans la mesure où la visualisation des données pour les utilisateurs est un enjeu crucial en termes de prise de décisions sur les problématiques de sécurité. Ces trois sites sont opérationnels, c\u0027est à dire que les données traitées et analysées dans les sections suivantes correspondent à des données de production. Pour des raisons de confidentialité les adresses IP ont été anonymisées. Le réseau SP1 est doté de son propre conteneur de données qui est alimenté par les événements envoyés en temps réel par le \"Firewall\". Les réseaux SAB1 et SQ1 mutualisent un même conteneur. Les données brutes envoyées par l\u0027ensemble des équipements filtrants sont traitées selon une extraction de motifs.\nDescription des données\nLe contenu des variables listées ci-dessous sont exportées vers des conteneurs de données. \nConclusions et perspectives\nA l\u0027issue de la phase 1, l\u0027ensemble des événements liés au filtrage est exporté en temps réel vers des conteneurs de données. Dans un souci de performance et compte-tenu de l\u0027importance\n"
  },
  {
    "id": "248",
    "text": "Introduction\nLes travaux de cette dernière décennie dans le domaine de la découverte de connaissances, comme ceux notamment de ) et de (Tiwari et al. (2010)), témoignent du vif intérêt pour le problème de l\u0027extraction des ensembles d\u0027items fré-quents, des motifs séquentiels, des motifs structurels (dans les données de type arbres, graphes ou treillis), et la recherche de méthodes efficaces pour extraire ces motifs. Dans cet article, nous nous intéressons à la notion de proportion analogique, essentiellement étudiée dans le domaine de l\u0027intelligence artificielle, pour extraire de nouveaux types de motifs dans les bases de données. Les proportions analogiques relient quatre objets A, B, C, D du même type dans une assertion de la forme « A est à B ce que C est à D ». Ils permettent d\u0027exprimer l\u0027identité (ou la proximité) des rapports existant entre deux paires d\u0027éléments. Des exemples typiques de cette notion en langage naturel sont : « le veau est à la vache ce que le poulain est à la jument », « l\u0027aurochs est au boeuf ce que le mammouth est à l\u0027éléphant ». Ces relations permettent d\u0027exprimer que ce qui distingue A de B est comparable à ce qui distingue C de D. Les exemples ci-dessus montrent la diversité (et la potentielle complexité) des sémantiques possibles du connecteur « est à » intervenant dans une proportion analogique. Dans le premier exemple, ce connecteur représente une relation de filiation tandis que dans la seconde, il exprime une évolution possible. Le connecteur « ce que » de la proportion représente généralement l\u0027identité ou la similarité. Quand les éléments A, B, C et D sont des valeurs numériques, la relation peut être définie en utilisant les proportions mathéma-tiques classiques, comme la proportion géométrique : A/B \u003d C/D (e.g., 1/3 \u003d 2/6) ou la proportion arithmétique : A ? B \u003d C ? D (e.g., 5 ? 3 \u003d 9 ? 7). Quand les objets A et B, resp. C et D, représentent les mêmes entités à différents moments ou états de leur vie (par exemple, A et B décrivent le même endroit à deux moments différents), la proportion analogique peut exprimer des évolutions similaires. De manière générale, les proportions analogiques permettent de trouver des parallèles entre quatre événements ou situations.\nNous cherchons à exploiter la notion de proportion analogique dans le contexte des bases de données relationnelles afin d\u0027extraire des combinaisons de quatre n-uplets liés par une telle relation. Notre objectif est de découvrir des parallèles entre des paires de n-uplets, i.e., des paires d\u0027éléments qui sont dans les mêmes rapports. Ces parallèles ne reflètent pas forcément une relation de proximité (A est aussi proche de B que C est proche de D), mais plutôt une transformation semblable (On passe de A à B comme on passe de C à D). Ces parallèles sont d\u0027une importance majeure puisqu\u0027il permettent de modéliser des règles d\u0027évolution reproductible dans les systèmes écolo-giques (les états de deux littoraux qui évoluent dans les mêmes directions : apparition et disparition des mêmes espèces, évolution d\u0027une pollution d\u0027une région à une autre), des mouvements sociétaux (extension d\u0027une crise géopolitique ou comparaison avec des successions d\u0027événements passés), ou des déplacements parallèles d\u0027objets.\nLes contributions de cet article sont les suivantes. Nous proposons une méthode pour identifier les proportions analogiques dans les bases de données. À cette fin, nous suivons une approche vectorielle pour définir la notion de proportion analogique adaptée au modèle relationnel. Puis nous montrons qu\u0027il est possible de ramener le problème d\u0027énumération de toutes les combinaisons de quatre n-uplets liés par une relation d\u0027analogie, à un problème de clustering moyennant un prétraitement et l\u0027utilisation d\u0027une métrique. Ceci permet de rassembler des paires d\u0027éléments qui définissent des vecteurs égaux ou presque. Nous analysons ensuite les résultats de notre approche appliquée à un jeu de données réelles.\nNotre article est organisé comme suit. En section 2, nous introduisons la notion de proportion analogique et nous proposons une définition graduelle de celle-ci adaptée au contexte des bases de données. La section 3 présente notre approche de découverte des proportions analogiques et l\u0027algorithme qui en découle, tandis que la section 4 détaille les expérimentations effectuées. Enfin, nous présentons les travaux relatifs à notre proposition (Section 5) puis nous concluons (Section 6).\nProportions analogiques et modélisation\nLes proportions analogiques\nCette section s\u0027appuie sur les références (Miclet et Prade (2009)) et (Lepage (2012)). Une proportion analogique est une assertion de la forme « A est à B ce que C est à D», notée par la suite Un algorithme naïf pour énumérer les proportions analogiques issues d\u0027un ensemble d\u0027objets de cardinalité n, a une complexité temporelle en n 4 . En utilisant un point de vue vectoriel de la notion de proportion analogique, les objets A, B, C et D désignent des points d\u0027un espace à n dimensions. S\u0027ils forment une relation d\u0027analogie alors ces points forment un parallèlogramme. Par exemple, la figure 1 montre la relation de proportion analogique existant entre les points A(1, 2), B(4, 4), C(3, 1), D(6, 3) représentés dans un repère orthonormé.\nAinsi quatre objets A, B, C et D sont en proportion analogique si et seulement si\nLa relation de proportion analogique liant A, B, C, et D peut être alors symbolisée par le vecteur ? ? ? AB ( ou ? ? ? CD). Dans ce cas particulier (la conformité est la relation d\u0027identité), il est possible de définir un algorithme dont la complexité temporelle est en n 2 : celui-ci calcule tous les vecteurs existant entre les paires de n-uplets et rassemblent toutes les paires de nuplets définissant des vecteurs égaux en une classe d\u0027équivalence (Lepage (2012)). Une classe d\u0027équivalence, représentée par un vecteur, rassemble ainsi des paires de points qui, prises deux à deux, sont en proportion analogique « selon ce vecteur ». Il est alors aisé de générer l\u0027ensemble de toutes les relations d\u0027analogie à partir de chacune des classes d\u0027équivalence.  \nSupposons que\nModéliser les proportions analogiques selon une approche géométrique\nLa modélisation des proportions analogiques dans le contexte des bases de données relationnelles est influencée par les propriétés du modèle relationnel. Soit un ensemble d\u0027attributs {A 1 , . . . , A m }, un schéma de relation est défini comme un sous-ensemble d\u0027attributs S \u003d {A i1 , . . . , A in }. Une relation définie en termes d\u0027un schéma de relation S est un sous-ensemble fini du produit cartésien des domaines de chacun des attributs de S. Chaque élément d\u0027une relation est appelé n-uplet qui peut être représenté par 1. La propriété de permutation des moyens permet d\u0027éviter de calculer à la fois\nun point décrit par n dimensions. Une base de données est un ensemble fini de relations. D\u0027autres contraintes additionnelles comme les dépendences fonctionnelles et les dépendances d\u0027inclusion permettent de restreindre le contenu des relations. Il serait intéressant d\u0027en tenir compte dans la recherche des proportions analogiques mais nous nous limiterons ici au cas de la recherche de proportions existantes entre quatre points. Ainsi les n-uplets A, B, C, et D d\u0027une relation sont considérés comme des points à n dimensions, et sont dénotés comme suit :\nComme dit précédemment, A, B, C, et D sont liés par une relation de proportion analogique si et seulement si\nL\u0027égalité est difficile à obtenir quand on considère des jeux de données réels. Il convient alors de rendre cette définition plus flexible, notamment en donnant une vision plus graduelle de la relation de proportion analogique. Deux vecteurs ne doivent plus être égaux mais presque égaux ce qui revient à mesurer dans quelle mesure || ? ? ? AB ? ? ? ? CD|| est proche de 0. On cherche alors à évaluer la « distorsion » entre les deux vecteurs. Pour permettre la commensurabilité des dimensions lorsque les attributs portent sur des domaines différents, les valeurs des vecteurs sont normalisées afin qu\u0027elles appartiennent à l\u0027intervalle [0, 1]. Pour cela, chaque valeur v du domaine actif d\u0027un attribut peut être remplacée par la valeur suivante :\nv ? min att max att ? min att où min att et max att désignent respectivement la valeur minimale et la valeur maximale du domaine actif de l\u0027attribut.\nPlusieurs stratégies peuvent être utilisées pour mesurer à quel point l\u0027expression || ? ? ? AB ? ? ? ? CD|| est proche de || ? ? 0 ||. Différentes normes peuvent être utilisées comme la norme de Minkowsky (norme p), qui donnera la longueur du vecteur correctif permettant de passer de ? ? ? AB à ? ? ? CD, ou, la norme infinie qui donnera la coordonnée maximale de ce vecteur correctif.\nDéfinition 1 : Distorsion analogique fondée sur une norme infinie Soit A, B, C, et D quatre n-uplets de n dimensions.\nLa norme infinie retourne la plus grande différence de dimension entre les deux vecteurs. Les deux vecteurs sont d\u0027autant plus égaux que le changement maximal sur une dimension est proche de zéro.\nDéfinition 2 : Distorsion analogique fondée sur la norme p Soit A, B, C, et D quatre n-uplets de n dimensions.\nDans ce cas, la définition met l\u0027accent sur la longueur du vecteur correctif permettant de passer de\nDans tous les cas (Définition 1 ou Définition 2), la relation de proportion analogique est d\u0027autant plus vraie que la distorsion est proche de 0.\nProposition : Les deux définitions de distorsion vérifient les propriétés fondamentales des proportions analogiques. En effet, les propriétés suivantes sont vérifiées :\npuisque les deux définitions reposent sur une valeur absolue des différences entre chaque coordonnée.\n-la permutation des moyens : la relation Dist(\nDans la suite, nous utiliserons la norme infinie qui est la plus drastique et possède un meilleur pouvoir de discrimination dans la mesure où elle évite tout effet de compromis entre les composantes des vecteurs.\nDécouvrir les proportions analogiques\nNotre objectif est de découvrir toutes les proportions analogiques présentes dans un ensemble de données et si possible de dégager des tendances, i.e., les différents vecteurs représentatifs des proportions analogiques découvertes. Une approche naïve pourrait consister à énumérer tous les vecteurs et à calculer la distorsion entre chaque paire de vecteurs, puis à ne garder que les paires de vecteurs dont la valeur de distorsion ne dé-passe pas un certain seuil. Cependant, une telle approche poserait la question du choix du seuil, très dépendant des données. Il nous semble par ailleurs préférable d\u0027utiliser une technique permettant de fournir une vue synthétique des motifs découverts. Une approche de type clustering semble tout à fait appropriée dans ce contexte. Un argument supplémentaire en faveur d\u0027une telle approche est lié à l\u0027objectif final que nous nous sommes fixé, à savoir l\u0027extension des langages d\u0027interrogation de bases de données avec des requêtes analogiques, i.e., des requêtes visant à découvrir des proportions analogiques existant dans un ensemble de données. En effet, l\u0027identification de classes d\u0027équivalence regroupant les paires de points représentant des vecteurs (presque) égaux permettrait la définition d\u0027index, utiles pour optimiser l\u0027évaluation de telles requêtes. Le problème ici étudié, qui consiste à regrouper des vecteurs à n dimensions égaux ou presque, se ramène à un problème de clustering classique dès lors que l\u0027on dispose d\u0027une métrique. Or les définitions des distorsions analogiques (Dist( ? ? u , ? ? v )) satisfont les conditions qui caractérisent les métriques, soit :\n-l\u0027identité des indiscernables : Dist( ? ? u , ? ? v ) \u003d 0 ssi ? ? u \u003d ? ? v , -la propriété de symétrie et -l\u0027inégalité triangulaire. Différentes approches de clustering sont donc utilisables, comme les k-means et l\u0027approche hiérarchique (Xu et al. (2005)). Notre objectif étant de tester l\u0027approche et de la valider sur des jeux de données réels, puis d\u0027identifier les relations découvertes, nous avons choisi de reprendre un algorithme de clustering hiérarchique (Xu et al. (2005)) dont les étapes sont énoncées dans l\u0027Algorithme 1. Cet algorithme requiert une étape préalable de construction des vecteurs à partir des points de la relation. \nData\nExpérimentations\nDans cette section, nous illustrons notre approche avec des données électorales afin de découvrir des parallèles entre les résultats des votes de différentes régions, puis des évolutions des résultats de votes d\u0027une année à l\u0027autre. Pour cela, nous avons exploité les jeux de données ouverts décrivant les résultats des élections présidentielles en    Figure 2).\nLes expérimentations visaient à montrer que le cadre proposé permet de mettre en évidence des parallèles existant dans les données, ce que montrent nos premiers résul-tats. Vu la nature de la relation de proportion analogique, l\u0027approche peut évidemment s\u0027appliquer à bien d\u0027autres domaines, comme la recherche de trajectoires parallèles d\u0027objets mobiles, moyennant une adaptation, ou dans les domaines environnemental et sociétal, pour découvrir des évolutions analogues.\n5 État de l\u0027art L\u0027originalité de l\u0027approche présentée ici tient à la nature même du type de régularité que l\u0027on cherche à découvrir dans les données. La majeure partie des travaux en fouille de données visent à découvrir des caractéristiques fréquentes (vues comme des valeurs d\u0027attribut ou des séquences de valeurs d\u0027attribut lorsqu\u0027un aspect temporel est pris en compte) dans un ensemble de données. Avec les proportions analogiques, qui sont des relations quaternaires, nous cherchons à vérifier l\u0027existence de \"parallèles\" entre des couples d\u0027objets d\u0027une collection. Une proportion analogique, lorsqu\u0027elle met en parallèle les situations de deux éléments à deux moments différents, peut être vue comme une sorte de règle d\u0027évolution. Dans un tel contexte, la recherche de proportions analogiques peut constituer une alternative aux approches de la littérature visant à -extraire des règles d\u0027évolution dans des graphes (Berlingerio et al. (2009)), ou à -découvrir des trajectoires parallèles d\u0027objets en mouvement (Vlachos et al. (2002); Chen et al. (2005); Lee et al. (2007); Li et al. (2013)), ou encore à -classer des séquences d\u0027événements (Studer et al. (2010); Guigourès et al. (2014); Lin et al. (2003); Malinowski et al. (2013); Zhou et al. (2013)).\nQuoi qu\u0027il en soit, l\u0027approche proposée fournit un cadre plus général que celui dédié spécifiquement à l\u0027extraction de règles d\u0027évolution dans les données. En effet, la notion de proportion analogique n\u0027implique pas l\u0027existence d\u0027une dimension temporelle et peut servir à décrire des parallèles de nature très variée entre deux paires d\u0027objets.\n"
  },
  {
    "id": "249",
    "text": "Introduction\nActuellement, la recherche et la détection de similitudes s\u0027effectuent en deux phases : une première phase de recherche de sources candidates, suivie d\u0027une seconde de comparaison de ces sources possibles avec le document que l\u0027on suspecte d\u0027être un plagiat. La phase de collecte est de plus en plus optimale grâce à l\u0027amélioration de l\u0027efficacité des moteurs de recherche en local et sur le Web. C\u0027est à la seconde phase que cet article s\u0027intéresse. Une fois qu\u0027une source candidate est trouvée, elle doit être comparée avec le document sur lequel pèse les soupçons. À l\u0027heure actuelle, la plupart des logiciels anti-plagiat, une fois une liste de sources candidates constituée, se contentent de comparer mot à mot le document analysé avec chaque source possible. Cette technique permet seulement de détecter les similitudes de types « copier/coller ». Bien que cette approche ait prouvé son efficacité et suffise la plupart du temps, en France près d\u0027un étudiant sur deux a déjà eu recours au « copier/coller » (Gibney, 2006), une énorme faille persiste. En effet, le fait de reformuler ou tout simplement de paraphraser un texte, en utilisant des synonymes par exemple, rend la plupart des techniques actuelles caduques. Certains articles (Callison-Burch et al., 2008;Bannard et Callison-Burch, 2005) se sont tout de même intéressés à la détection de reformulations paraphrastiques avec des approches d\u0027alignement. Malgré le fait que ces approches soient plus robustes à l\u0027ajout et à la suppression de mots ainsi qu\u0027à l\u0027utilisation de synonymes, elles restent toutefois inefficaces face aux reformulations non paraphrastiques comme le passage de la forme active à la forme passive. L\u0027approche proposée consiste à comparer les deux textes, phrase par phrase, et non plus mot à mot et à rechercher si une phrase de l\u0027un des textes comporte le même sens qu\u0027une phrase dans l\u0027autre texte. Ceci repose sur l\u0027hypothèse que lorsqu\u0027on paraphrase ou reformule un texte, on garde le sens de celui-ci et ainsi on garde les mots-clés principaux, porteurs du plus de sens de chaque phrase. Après avoir défini quelques notions et présenté l\u0027état de l\u0027art, nous décrirons d\u0027abord comment segmenter le texte en unités de sens, pour ensuite procéder sur chacune de ces unités à l\u0027extraction des mots porteurs de sens, afin de rechercher des concordances de mots de même concept dans un autre texte. Enfin, nous testerons trois algorithmes utilisant notre approche et nous déterminerons le meilleur seuil pour chacun d\u0027entre eux. Le seuil est le nombre minimum de concepts identiques dans deux phrases permettant d\u0027affirmer que l\u0027une est la reformulation de l\u0027autre. Pour finir, nous présenterons l\u0027évaluation de notre approche en comparant la méthode retenue aux méthodes classiques de détection des paraphrases par alignement.\n2 La comparaison au-delà du « copier/coller »\nLa notion de comparaison\nLa « comparaison de deux documents » est un terme assez vague. Pour comparer correctement deux documents, il faut repérer leurs points communs (leurs similitudes) et leurs diffé-rences. Les similitudes étant plus simples à détecter, il est de convention de chercher à repérer celles-ci en premier lieu et d\u0027en déduire ensuite les différences, représentées alors par le reste du document. Cependant, la plupart des comparaisons textuelles se limitent au « copier/coller », or ce ne sont pas les seules similitudes pouvant être recensées dans un texte.\nLa notion de similitudes\nBien que l\u0027on puisse avoir au sein d\u0027un document des tableaux, images, graphiques ou tout autre type de données, cet article traite seulement des similitudes d\u0027ordre textuel. On distingue plusieurs types de similitudes allant de la ressemblance jusqu\u0027à l\u0027identité même (SimacLejeune, 2013b). Les ressemblances sont les types de similitudes les plus difficiles à repérer et sont pour cause le point faible des logiciels anti-plagiat actuels. Dans notre cas, on distingue trois types majeurs de similitudes textuelles, de la plus simple à détecter à la plus complexe :\n-la copie, qui consiste à copier mot à mot tout ou partie d\u0027un texte dans un autre. Pour exemple, considérons la phrase suivante présente dans un texte : « En cinquante ans, grâce à des efforts considérables dans la recherche et l\u0027élaboration de la fusion, la performance des plasmas a été multipliée par 10\u0027000. » Elle sera recopiée à l\u0027identique dans un autre texte ; -la paraphrase, aussi appelée reformulation paraphrastique, qui consiste à reprendre une phrase d\u0027un texte pour la détailler ou l\u0027expliciter. Elle conserve donc l\u0027ordre des éléments évoqués, autorisant simplement le changement de vocabulaire, l\u0027ajout, la suppression et la substitution de mots. Toujours en considérant la phrase de l\u0027exemple précédent, une paraphrase possible serait : « En une cinquantaine d\u0027années, grâce à un immense effort de recherche, la performance des plasmas produits par les machines de fusion a été multipliée par 10000. » On remarque la conservation des concepts, mais aussi la substitution ou la suppression de certains d\u0027entre eux ; -la reformulation, qui autorise elle toutes modifications textuelles à condition que le sens de la phrase soit conservé. Cela donne souvent lieu à un changement d\u0027ordre des concepts. La reformulation de la phrase exemple serait : « La performance des plasmas produits par les machines de fusion a été multipliée par 10,000 grâce à un immense effort de la recherche bien que cela ait pris une cinquantaine d\u0027années. »\nLa notion de concept\nUn concept est une idée, un sens représenté par un mot ou un groupe de mots. Les reformulations et paraphrases exploitent les propriétés paradigmatiques des mots (leur capacité à se substituer mutuellement) et entraînent ainsi des changements de vocabulaire mais elles conservent les concepts et les idées exprimées (Duclaye, 2003). Il est alors, dans le cadre de la détection de similitudes, plus judicieux de représenter un mot par un concept plutôt que par son identité ou sa définition. Par exemple, il est plus judicieux de représenter un mot par un tableau de tous les mots par lesquels il peut être substitué (un tableau de ses synonymes, lui compris) plutôt que seulement par lui-même.\nÉtat de l\u0027art\nLorsque les processus anti-plagiat comparent deux documents, ils recherchent les éléments de l\u0027un également présents dans l\u0027autre. Ils tentent de détecter des similitudes, toutes informations communes laissant penser qu\u0027un plagiat a pu avoir lieu. La comparaison mot à mot est certes efficace pour trouver les zones de « copier/coller » mais les plagiaires ne se contentent plus de copier des éléments depuis une source, ils essaient à présent de camoufler leurs emprunts d\u0027idées derrière des modifications syntaxiques. Les recherches de Barron-Cedeño et al. (2013) se concentrant sur la détection de paraphrases appliquée dans le cadre de la détection du plagiat démontrent que le phénomène de paraphrasage nuit aux systèmes anti-plagiat et rend la détection de similitudes plus difficile. Il faut donc tenter de détecter les paraphrases et les reformulations par des moyens différents, car bien que souvent associés ces deux termes représentent des opérations textuelles bien distinctes. Toutefois, les travaux linguistiques ayant portés sur leur définition, s\u0027accordent sur le fait que ce sont des opérations de modifications de texte, certes bien différentes, mais qui conservent toutes deux le sens (Harris, 1957;Martin, 1976;Duclaye, 2003).\nDes recherches (Gülich et Kotschi, 1983;Eshkol-Taravella et Grabar, 2014) se sont attardées à chercher des marqueurs de reformulations afin de mieux les repérer par la suite et d\u0027étudier leur fonctionnement et leur construction. D\u0027autres recherches se sont cantonnées à étudier les limites de la détection des paraphrases (Vila et al., 2011) en estimant au contraire qu\u0027il n\u0027existait pas de caractérisation complète sur le plan linguistique et computationnelle de la paraphrase.\nFace à ces difficultés, des chercheurs se sont concentrés sur des approches alternatives ne permettant pas de détecter concrètement des reformulations mais de tout de même déterminer qu\u0027un texte en contient :\n-les approches stylométriques (Iyer et Singh, 2005) qui suggèrent qu\u0027en analysant des statistiques de fréquences de mots ou bien d\u0027autres caractéristiques d\u0027un texte on peut en reconnaître l\u0027auteur, et ainsi, si un passage du document ne possède pas les mêmes caractéristiques que le reste du document, on peut en déduire que ce passage aura été emprunté à un autre auteur (Oberreuter et Velásquez, 2013;van Halteren, 2004;Jardino et al., 2007) ; -les approches de calcul de distances (Simac-Lejeune, 2013a) qui propose de calculer une distance « sémantique » entre deux textes après avoir extrait les mots clefs de chaque texte, exposant ainsi l\u0027emprunt probable de l\u0027un dans l\u0027autre.\nEn dehors de ces approches, la majorité des travaux portent sur la détection des reformulations paraphrastiques, comme les recherches de Eshkol-Taravella et Grabar (2014) portant sur leur détection dans des corpus oraux. Les approches les plus répandues sont les méthodes par alignement (Callison-Burch et al., 2008;Bannard et Callison-Burch, 2005). Servant la plupart du temps dans un contexte bi-linguale (alignement d\u0027un texte et de sa traduction), elles consistent à aligner deux textes par leurs mots ou groupes de mots en communs et ainsi de repérer les mots ou groupes de mots différents mais équivalents. Certaines recherches (Shen et al., 2006), visant à produire des paraphrases, se sont également avérées intéressantes. En effet, étudiant la possibilité de générer automatiquement des paraphrases, un processus d\u0027assemblage puis de désassemblage s\u0027est dégagé, remettant ainsi sur le devant de la scène les méthodes d\u0027alignement. Proche de ces méthodes avec alignement, on peut citer le travail de Fenoglio et al. (2007) traitant de la comparaison de versions de documents textuels à la façon des serveurs de versions. Il met en lumière les transformations élémentaires (déplacements, insertions, suppressions et remplacements de blocs de caractères), identifiées depuis longtemps par les spécialistes de la génétique textuelle (de Biasi, 2000;Grésillon, 1994) comme éléments fondateurs d\u0027une paraphrase.\nToutefois, le cadre théorique le plus souvent adopté est la théorie linguistique Sens-Texte (Kahane, 2003) élaborée dans les années 1960 par Mel\u0027? cuk, notamment son système de paraphrasage (Žolkovskij et Mel\u0027? cuk, 1967;Mel\u0027? cuk, 1992;Mili´cevi´cMili´cevi´Mili´cevi´c, 2007) comme dans le travail de Mili´cevi´cMili´cevi´Mili´cevi´c (2010). Ce dernier met également en avant des approches sémantiques qui permettent de s\u0027approcher d\u0027une détection de reformulations. La plupart des règles sémantiques de paraphrasage trouvées jusqu\u0027ici mettent en jeu un découpage du texte en proposition et des liens communicatifs et rhétoriques entre celles-ci (Danlos, 2006), coïncidant ainsi, dans la plupart des cas, à la définition d\u0027une reformulation qui se contente d\u0027être une paraphrase avec changement d\u0027ordre des propositions.\nLa reformulation non paraphrastique étant bien plus complexe à détecter que sa voisine la paraphrase, les études se concentrant uniquement sur elle se font plus rares. Mais dès lors qu\u0027on sait que la reformulation conserve également le sens du texte (Harris, 1957;Martin, 1976;Duclaye, 2003) et que le mécanisme de paraphrase le plus utilisé est le changement de lexique (Barron-Cedeño et al., 2013), on peut envisager d\u0027appliquer plus ou moins les mêmes approches sémantiques que pour la paraphrase ou bien même, des approches plus naïves de recherche de correspondances de concepts.\n3 Notre approche\nSegmentation\nDans un premier temps, l\u0027idée est de segmenter le document que l\u0027on suspecte être un plagiat. Plusieurs algorithmes de segmentation ont été évalués :\n-la segmentation par nombre de blocs : on découpe le document en un certain nombre de blocs de même taille (de même nombre de mots), peu importe la taille finale de chaque bloc ; -la segmentation par taille de blocs : on découpe le document par blocs d\u0027une certaine taille (un certain nombre de mots), peu importe le nombre de blocs créés ; -la segmentation par pourcentage que représente un bloc sur l\u0027ensemble du document (e.g. une segmentation comme celle-ci avec en paramètre un pourcentage de 1% pour un bloc reviendrait à une segmentation en 100 blocs, chaque bloc représentant 1%) ; -la segmentation par granularité (Simac-Lejeune, 2013b) : il s\u0027agit d\u0027une segmentation hiérarchique, on découpe le texte en nb blocs de même taille, puis on redécoupe chaque bloc ainsi obtenu en nb blocs de même taille, et ainsi de suite sur une profondeur limite définie. Ceci permettant d\u0027affiner l\u0027analyse niveau par niveau ; -la segmentation par paragraphe : chaque segment représentant un paragraphe ; -la segmentation par phrase : chaque segment représentant une phrase du document ; -la segmentation par proposition : chaque segment représentant une unité minimale de sens, les délimiteurs étant la ponctuation de fin de phrase mais aussi les virgules, les conjonctions de coordination et divers mots de liaison ou de causalité. Chaque algorithme a fait l\u0027étude, via de nombreux tests et corrections, à l\u0027optimisation de ses paramètres afin de mettre l\u0027accent sur la rapidité du processus de découpage mais aussi sur la pertinence des métadonnées extraites dans chaque segment. Il est important que chaque segment conserve un sens afin d\u0027être potentiellement sujet à une reformulation. Une segmentation en unité de sens a donc été choisie. Un découpage par phrase ou par proposition est à privilé-gier car une segmentation à faible granularité, comme celle par paragraphe, donne lieu à des segments trop volumineux pour l\u0027étape d\u0027extraction qui suivra. Au contraire, une segmentation à trop grand niveau de granularité pourrait, en plus d\u0027entraîner un temps d\u0027exécution plus important (plus de segments à traiter), occasionner une perte d\u0027informations dans sa globalité (aucune liaison entre les concepts extraits). C\u0027est pourquoi la segmentation qui a été retenue est une fusion du découpage par phrase et du découpage par taille de blocs : une segmentation par phrase mais d\u0027une taille minimale (en mots). On conserve ainsi une unité de sens (une ou plusieurs phrases) tout en gardant une taille suffisamment importante pour pouvoir obtenir une pertinence raisonnable des métadonnées extraites mais suffisamment petite pour être considé-rée comme indépendante et donc éventuellement reformulée. Après divers tests, le seuil a été fixé à 15 mots, taille moyenne des phrases dans la langue française. Avec un seuil si faible, c\u0027est l\u0027une des méthodes de segmentation évaluée les plus chronophages mais pour notre étude elle garantit un rapport taille/pertinence optimal.\nExtraction de mots clefs (mots porteurs de sens)\nLa seconde étape du processus est une étape d\u0027extraction des mots porteurs de sens de chaque segment c\u0027est-à-dire des mots représentant les concepts que le plagiaire a été obligé de réutiliser s\u0027il voulait conserver le sens de la phrase, même s\u0027il a pu les remplacer par des synonymes. Pour déterminer les mots porteurs de sens d\u0027un texte, l\u0027étiqueteur morphosyntaxique TreeTagger (Schmid, 1994) a été utilisé. Il détermine la classe lexicale, le \"Part Of Speech\" de chaque unité lexicale (token) du texte. De façon plus commune, on peut dire que pour chaque mot ou élément du texte, TreeTagger détermine s\u0027il s\u0027agit d\u0027un nom, d\u0027un verbe, d\u0027un adjectif, d\u0027une ponctuation, etc. L\u0027étiquetage morphosyntaxique permet d\u0027identifier les mots clefs d\u0027un texte par leur classe lexicale. Plutôt que de discriminer les mots vides (stop words) par leur taille, ceci pouvant générer des erreurs (e.g. un mot de moins de trois lettres n\u0027est pas pertinent, un contre exemple est le mot « as » qui peut être important, et le mot « mais » qui est simplement une conjonction), on les discriminera par leur \"Part Of Speech\".\nDans notre cas, les mots pertinents à conserver sont un peu plus riches sémantiquement que les mots clefs habituels. On ne conserve pas seulement les noms communs et propres, il est important de garder aussi les adjectifs, les verbes et également les adverbes, en réalité tout mot porteur de sens au sein d\u0027une phrase. On néglige donc les méthodes les plus courantes pour extraire des mots clefs, les méthodes fréquentielles (Lee et Baik, 2004) qui consiste pour chaque mot du texte à calculer sa fréquence d\u0027apparition dans le texte. C\u0027est pour cela que le terme de mots clefs est ici un abus de langage et que nous allons préférer le terme de mots porteurs de sens d\u0027une phrase. Tout mot porteur de sens d\u0027une phrase doit être conservé, peu importe son nombre d\u0027occurrences dans le texte.\nUn filtre de mots vides a été ajouté à la sortie de TreeTagger afin d\u0027être certain de la pertinence des mots porteurs de sens extraits. Ainsi en couplant les deux techniques, l\u0027efficacité de l\u0027étiquetage est passée d\u0027environ 96% à quasiment 100%.\nConsidérons la phrase suivante : « Ce peu de masse disparue crée une grande quantité d\u0027énergie comme le démontre la fameuse formule d\u0027Einstein E\u003dmc2. » Ses mots porteurs de sens extraits seraient « peu, masse, disparue, crée, grande, quantité, énergie, démontre, fameuse, formule, Einstein, E\u003dmc2 ».\nThésaurus -chargement d\u0027un dictionnaire de synonymes\nParallèlement à cela, un dictionnaire de synonymes est chargé. Pour chaque mot, on a donc accès à un tableau contenant tous les mots de la langue par lesquels il peut être substitué. L\u0027efficacité de notre approche dépendant en grande partie du contenu de cette ressource, il est important de faire la différence entre des synonymes et des mots de substitution possibles. Par exemple, pour le mot « père », « papa » serait un synonyme alors que « parent » serait un mot de substitution envisageable. Autre exemple, le mot « île » a pour synonyme « îlot », « archipel » ou bien encore « atoll » mais aucunement les mots « tâche » ou « pâté » qui eux se trouvent pourtant dans notre tableau et peuvent servir de mot de substitution. En effet, on peut très bien imaginer dans un poème une phrase telle que « cette tâche au milieu de l\u0027océan » faisant référence à un îlot. En règle générale, « îlot » et « tâche » ne sont pas synonymes mais ici, ils représentent le même concept.\nDès lors, un concept est un mot porteur de sens ainsi que tous ses mots de substitution possibles contenus dans son tableau.\nLe tableau 1 représente une partie des mots de substitution correspondant aux mots porteurs de sens extraits sur la phrase exemple lors de l\u0027étape précédente. On remarque, comme dans l\u0027exemple de l\u0027îlot cité précédemment, que le terme « énergie » laisse place à « assiduité » qui n\u0027a strictement rien à voir avec le contexte de notre phrase mais qui dans un autre contexte aurait très bien pu être un synonyme envisageable. \nCorrespondance\nLa dernière étape de notre approche consiste à comparer chaque phrase d\u0027une source candidate avec les mots porteurs de sens de chaque segment du texte en cours d\u0027analyse ainsi qu\u0027avec leurs mots de substitution possibles contenus dans le tableau défini précédemment. On appellera seuil de correspondance le nombre de concepts communs à partir duquel on peut estimer qu\u0027une phrase est la reformulation d\u0027une autre. S\u0027il y a plus de concepts pertinents communs entre deux phrases que le seuil de correspondance défini, c\u0027est sans doute que l\u0027une est une reformulation de l\u0027autre.\nPlusieurs algorithmes mettant en oeuvre cette méthode ont été développés, certains plus robustes que d\u0027autres face aux changements de genre, de nombre, de casse typographique ou bien d\u0027ordre des mots (e.g. phrase passée de la forme active à la forme passive et vice versa). L\u0027efficacité de la détection dépend de l\u0027algorithme choisi, du seuil de correspondance défini, et du nombre et de la pertinence des « synonymes » disponibles dans le dictionnaire chargé.\nNous proposons trois algorithmes, trois implémentations différentes de l\u0027approche décrite précédemment.\n-un premier (tableau 2 -A) qui compare la présence des concepts dans l\u0027ordre et tels qu\u0027ils sont présents dans les phrases. Il ne supporte donc ni le changement de casse typographique, ni la dérivation et la flexion ; -un second (tableau 2 -B) qui compare également la présence des concepts dans l\u0027ordre des phrases mais en comparant leurs lemmes en minuscules, il supporte donc le changement de casse typographique, la dérivation et le changement de genre et de nombre ;\n-un troisième (tableau 2 -C), plus naïf, qui reprend le principe du précédent, en comparant cette fois la présence des concepts dans les deux phrases sans prendre l\u0027ordre en compte. Il est ainsi robuste aux reformulations non paraphrastiques de type mise à la forme passive. Le tableau 2 résume les différentes variations de la langue supportées par chaque algorithme. Considérons maintenant la phrase : « La célèbre équation d\u0027Einstein E \u003d mc 2 exprime le phénomène suivant : une importante quantité d\u0027énergie est apparue et un peu de la masse a disparu. » ainsi que sa reformulation : « Ce peu de masse disparue crée une grande quantité d\u0027énergie comme le démontre la fameuse formule d\u0027Albert Einstein E\u003dmc2. » Si on opère la comparaison de type C sur ces deux phrases, on retrouve bien, malgré le changement de vocabulaire et d\u0027ordre des mots, la correspondance de nos concepts, ici en gras.\nA noter l\u0027importance du seuil de correspondance, il y a dans cette exemple 11 concepts identiques, avec un seuil de correspondance de 11 ou moins, la phrase est reconnue comme reformulation, alors qu\u0027avec un seuil de correspondance supérieur ce ne sera plus le cas.\nÉvaluation et tests 4.1 La base de tests et protocole\nLa base de tests est composée de 150 textes, représentant chacun un passage d\u0027un document, allant de plus de 100 mots pour le plus petit à environ 9000 mots pour le plus grand. Cela représente 400 comparaisons de textes deux à deux. Afin de tester correctement les performances des algorithmes évalués, aussi bien des paraphrases que des reformulations plus complexes sont présentes dans le corpus, ainsi que des textes « pièges » traitant du même sujet et donc employant le même vocabulaire mais n\u0027étant pas pour autant des reformulations d\u0027un autre texte du corpus.\nCi-dessous la répartition des types de textes présents dans le corpus : -10 différents chapitres tirés d\u0027un même roman ; -20 chapitres de la bible (deux traductions différentes pour 10 chapitres) ; -25 textes de Wikipédia (différentes versions à différentes dates de 10 articles) ; -35 extraits de travaux d\u0027élèves (avec leurs sources provenant du Web) ; Les extraits de travaux d\u0027élèves proviennent pour la plupart de rapports et mémoires scientifiques ou économiques. L\u0027intégralité des textes sont en français.\nRésultats\nDans un premier temps, on compare les trois méthodes décrites précédemment, leur efficacité et leur temps moyen d\u0027exécution respectifs étant différents selon le seuil utilisé, on détermine d\u0027abord le seuil optimal pour chacune d\u0027entre elles. Le tableau 3 représente le rapport précision/rappel des trois algorithmes allant du seuil 1 à 7. Un seuillage de 4 semble mieux convenir aux algorithmes A et B, tandis qu\u0027un seuillage de 5 semble idéal pour l\u0027algorithme C. Prenant en compte la F-mesure et privilégiant le rappel plutôt que la précision, l\u0027algorithme C se montre être le plus efficace sur la base de tests. Le tableau 4 représente le temps d\u0027exécution de la procédure (segmentation, extraction de mots porteurs de sens, chargement du thésaurus et comparaison) des trois algorithmes en utilisant leur meilleur seuillage en fonction du nombre moyen de mots contenus dans les deux textes à comparer (moyenne du nombre de mots des deux textes). La méthode C s\u0027avère être la plus rapide (200 secondes en moyenne pour un texte d\u0027environ 4000 mots contre 250 pour la méthode A et 212 secondes pour la méthode B) en plus d\u0027avoir un meilleur rapport préci-sion/rappel, respectivement 0.745 et 0.807, car malgré le fait qu\u0027elle soit utilisée avec un seuil plus grand (5 contre 4 pour les deux autres implémentations) et qu\u0027elle fasse donc forcément un plus long parcours, elle ne vérifie pas l\u0027ordre des mots et néglige donc des permutations et suppressions de tableau. On remarque néanmoins une précision générale assez basse due aux faux positifs générés par les propriétés paradigmatiques des mots contenus dans le thésaurus.\nLe tableau 5 compare la méthode retenue (l\u0027algorithme C avec un seuil de 5) avec une mé-thode d\u0027alignement basée sur la méthode de Bannard et Callison-Burch (2005) mais appliquée sur un corpus mono-lingue. Ces deux approches possèdent des performances similaires face à la détection de « copier/coller », environ 84% d\u0027efficacité, en revanche notre méthode montre de biens meilleurs résultats sur la détection des reformulations non paraphrastiques (un rappel de 0.80 contre 0.24 pour une méthode avec alignement). TAB. 5 -Evaluation de notre méthode par rapport à une méthode à alignement en fonction des types de similitudes à détecter.\nConclusions\nLa méthode retenue montre des résultats similaires aux méthodes avec alignement sur la détection de copies exactes et de paraphrases et se montre beaucoup plus robuste face aux reformulations. Néanmoins, sa précision est fortement impactée par le thesaurus utilisé, qui peut engendrer des faux positifs pour les raisons évoquées dans la partie 3.2 Extraction de mots clefs, et la segmentation, qui peut être faussée par du texte enrichi (tableau, liste à puces). Nous conviendrons également que cette technique est plutôt coûteuse en temps et en ressources (chargement du thesaurus en mémoire).\nUn seuil adaptatif évoluant en fonction de la taille des phrases pourra également être mis en place dans de futurs travaux. Pour des phrases standards comportant entre 8 et 15 mots, il sera préférable de fixer le seuil à 5, en revanche si la phrase excède la vingtaine de mots, il faudra définir le seuil entre 10 et 12 mots communs.\nAu final, cette approche reste naïve et gourmande aussi bien en temps qu\u0027en ressources, néanmoins elle permet de détecter des reformulations jusque là impossibles à détecter avec des méthodes conventionnelles à alignement et constitue donc une alternative intéressante. Elle est à privilégier pour la détection de reformulations non paraphrastiques.\n"
  },
  {
    "id": "250",
    "text": "Introduction\nLa plupart des logiciels anti-plagiat se concentrent sur une détection extrinsèque de plagiat, c\u0027est-à-dire sur le fait de trouver des similitudes entre un document et un corpus de sources probables. Or ce système est inutile si le document ayant été plagié ne se trouve pas dans le corpus fouillé. Néanmoins, il existe un autre type de détection, la détection intrinsèque qui exploite des données extraites de l\u0027intérieur même du document. La détection d\u0027auteurs par étude du style d\u0027écriture du document est la forme de détection de plagiat intrinsèque la plus répandue. Cette approche diverge selon les travaux car elle soulève plusieurs problèmes, allant du découpage du texte de façon pertinente, au choix et à la collecte des données stylistiques à surveiller, en passant par la manière de découper et de classer les différents passages du document par auteur. C\u0027est sur ce dernier point que l\u0027article va essentiellement se concentrer.\nLa détection d\u0027auteur 2.1 La notion de stylométrie\nLa stylométrie ou l\u0027étude stylométrique d\u0027un texte est une analyse à mi chemin entre une analyse linguistique et statistique. Elle exploite des variables stylométriques, qui sont des caractéristiques linguistiques du texte, afin d\u0027établir des statistiques sur le document étudié. Effectuer l\u0027analyse stylométrique d\u0027un document consiste à surveiller les variations du style d\u0027écriture du document en surveillant l\u0027évolution des variables stylométriques au sein de celuici afin d\u0027en détecter les irrégularités et ainsi pouvoir déterminer si certains passages, appelés phases stylistiques, sortent de la norme par rapport à la majorité du texte.\nÉtat de l\u0027art\nDès le XIX e siècle, Mendenhall (1887) suggère qu\u0027en analysant des caractéristiques internes d\u0027un texte on peut en reconnaître l\u0027auteur. Depuis, les techniques d\u0027études stylomé-triques de document ont fait d\u0027importantes avancées et de nombreuses recherches (Stein et Eissen, 2007;Layton et al., 2013;Jayapal et Goswami, 2013) appliquent cette découverte à la détection de plagiat. Certaines de ces recherches se concentrent sur l\u0027extraction et la surveillance des données stylométriques les plus pertinentes. Stein et Eissen (2007) ainsi que Zamani et al. (2014)    (Cheng, 1995), un algorithme multidimensionnel des k-moyennes non paramétrique.\nSegmentation\nDans un premier temps, l\u0027idée est de segmenter le document. Il est important que chaque segment conserve un sens afin d\u0027être autonome et donc d\u0027être potentiellement écrit par une personne différente. Une segmentation en unité de sens est donc à privilégier. S\u0027appuyant sur le travail de Zechner et al. (2009), c\u0027est une segmentation pseudo sémantique qui a été retenue : un découpage par phrase d\u0027une taille minimale (en mots). Le seuil a été fixé à 15 mots, taille moyenne des phrases dans la langue française.\nExtraction de la stylométrie\nLa seconde étape du processus consiste à extraire la stylométrie de chaque segment. Pour ce faire, il faut au préalable détecter la langue de chaque segment au moyen d\u0027un module im-plémentant la technique de catégorisation de texte à base de n-grammes de Cavnar et Trenkle (1994). Ensuite, l\u0027étiqueteur morphosyntaxique TreeTagger (Schmid, 1994)  \nConstruction des courbes\nUne fois la segmentation et les calculs stylométriques opérés, on obtient donc plusieurs valeurs par segment (i.e. une valeur par variable stylométrique). Une suite de valeurs brutes sans cohérence n\u0027étant pas exploitable, on représente la stylométrie du document sous la forme de courbes, avec en abscisse, la position des segments (la ligne de vie du document) et en ordonnée, les valeurs des variables stylométriques observées. Ceci a pour avantage, en plus de permettre une représentation visuelle, de faciliter la comparaison et la manipulation des valeurs entre elles, les algorithmes de manipulation de courbes étant courant.\nIl est possible que le style d\u0027un même auteur varie énormément au fil d\u0027un même texte. La fatigue ou la maturité lors de longs écrits peuvent entraîner du bruit ou des variations brusques. On convient alors qu\u0027un lissage est nécessaire. C\u0027est le lissage par la moyenne glissante sans pondération (Chou, 1975) qui a été utilisé dans cet article.\nRegroupement\nIl reste à déterminer les phases stylistiques de façon automatique. Un algorithme d\u0027apprentissage automatique non supervisé (i.e. sans intervention humaine) est idéal dans ce cas de figure qui s\u0027apparente au clustering car il faut déterminer à quel auteur (i.e. à quel cluster) chaque donnée s\u0027apparente. Sachant que le nombre d\u0027auteurs et donc de clusters n\u0027est pas connu à l\u0027avance, c\u0027est le Mean Shift multidimensionnel (Cheng, 1995) qui se dégage. En effet, cet algorithme permet de clustériser un ensemble de points sans connaître à l\u0027avance le nombre k de clusters. L\u0027idée dans notre cas est de déterminer le nombre k à partir d\u0027un seuil. On dé-finit alors empiriquement un nombre k de départ assez grand, admettons 10 et un seuil, entre 2% et 15% en fonction de la moyenne de la variable stylométrique observée (seuil adaptatif). Tant qu\u0027il existe deux clusters voisins avec une différence de stylométrie inférieure au seuil, on relance un KMeans avec k \u003d k ? 1. Une fois toutes nos phases identifiées et k définitif, s\u0027il existe deux clusters (non voisins cette fois-ci étant donné que les voisins ont déjà été réunifiés) avec une différence de stylométrie inférieure au seuil, on en déduit qu\u0027ils sont du même auteur.\nOn prend en considération plusieurs variables stylométriques en même temps, tout comme le fait van Halteren (2004). L\u0027idée est de surveiller plusieurs variables stylométriques afin qu\u0027elles se « complètent » mutuellement. On augmente ainsi le taux de certitude de l\u0027existence d\u0027une zone par le fait qu\u0027une zone est définie comme telle si la majorité des courbes fléchissent de telle façon à la dessiner. De plus, la zone de flexion retenue est maintenant désignée par la moyenne des zones de flexion de toutes les courbes surveillées, ceci réduisant considérable-ment l\u0027erreur d\u0027approximation et rendant plus sûr notre prise de décision. Pour exemple, sur la FIG. 1 -Mean Shift sur plusieurs variables stylométrique.\nfigure 1 chaque zone de flexion des courbes est représentée par une ligne verticale pointillée de la même couleur que la courbe dont elle dépend. Les lignes pointillées noires plus épaisses représentent les découpages retenus (les moyennes des trois flexions des trois courbes).\nPour faciliter l\u0027observation des écarts et des flexions, les courbes sur cette figure ont été normalisées (mises à la même échelle), leurs valeurs stylométriques sont donc faussées.\nÉvaluation et tests 4.1 La base de tests et protocole\nLa base de tests est composée de 500 textes contenant en moyenne 7 000 mots. Les textes sont constitués d\u0027un (l\u0027intégralité du texte) à cinq passages, chaque passage étant potentiellement écrit par un auteur différent. Un texte peut contenir plusieurs passages écrits par un même auteur. On recense en totalité dans la base, une dizaine d\u0027auteurs différents. La langue prédomi-nante au sein des textes est le français, cependant pour tester l\u0027adaptabilité et le plurilinguisme du système de nombreux passages sont en anglais ou en italien. Afin de tester correctement la procédure évaluée, des passages traitant du même sujet et donc employant le même vocabulaire ont été utilisés dans le but de tromper la stylométrie extraite. De plus, l\u0027intégralité des textes est annotée, de telle sorte à savoir précisément de quel mot à quel mot les textes sont écrits par un auteur ou par un autre.\nRésultats\nNotre procédure présente une précision de 0.89 et un rappel de 0.34. Il est néanmoins important d\u0027étudier plus en détails les limites de cette procédure et de nuancer un rappel si faible. La figure 2 est un diagramme à bulles représentant les performances du découpage stylomé-trique. L\u0027axe des abscisses représente la taille en segment de la phase stylistique concernée et celui des ordonnées l\u0027écart moyen de la variable stylométrique observé entre cette phase et ses voisines, son unité est notée us pour unité stylométrique. Les bulles représentent les différents \nConclusions\nNotre approche montre des résultats exploitables lorsque les phases stylométriques à identifier ne sont pas trop importantes (n\u0027excèdent pas 190 segments soit environ 4000 mots) et lorsque la différence de stylométrie est suffisamment grande (supérieur à 0.20 us). En revanche dans tous autres cas, les limites de notre approche se font ressentir. Pour palier ces problèmes, un seuil adaptatif pourra être défini en fonction du type de variable stylométrique surveillée. De plus, avec du recul, nous convenons qu\u0027un Mean Shift n\u0027était sans doute pas la meilleure option de clustering. Dans la suite de nos travaux nous implémenterons d\u0027autres classifieurs (hiérarchique, DBSCAN, etc.).\nPour conclure, bien que perfectible, cette approche permet de détecter différents styles d\u0027écriture au sein d\u0027un même texte et notre contribution malgré ses limites permet bien de regrouper automatiquement les phases stylistiques par auteur.\n"
  },
  {
    "id": "251",
    "text": "Introduction\nLe risque chimique ou alimentaire se manifeste lorsque les produits chimiques sont dangereux pour la santé et consommation humaine ou animale, et pour l\u0027environnement. Si certains produits et substances sont maintenant clairement identifiés comme dangereux (e.g. l\u0027amiante, l\u0027arsenic, le plomb), nos connaissances actuelles sur d\u0027autres substances sont moins complètes. Nous nous intéressons en particulier au risque alimentaire (e.g. l\u0027arsenic, les nitrates, la listeria, la dioxine) et au risque chimique (e.g. le bisphénol A, les phtalates). Ces substances entrent souvent dans la composition de produits courants et peuvent avoir l\u0027effet nuisible sur l\u0027organisme humain. Le contrôle sur la commercialisation de ces substances est effectué par des organismes sanitaires dédiés, comme EFSA (European Food Safety Authority) ou ANSES (Agence nationale de sécurité sanitaire de l\u0027alimentation, de l\u0027environnement et du travail). Les experts se retrouvent face à une littérature scientifique abondante et doivent l\u0027étudier pour avoir une base solide pour la prise de décisions. L\u0027objectif de notre travail consiste à proposer une aide automatique pour l\u0027analyse de la littérature scientifique afin de détecter les phrases indicatives du risque induit par ces substances. Nous abordons cette tâche comme une problématique de catégorisation : les phrases des textes doivent être catégorisées dans les classes du risque. Nous présentons les données (section 2) et approches utilisées (sections 3 et 4). Nous discutons ensuite les résultats obtenus et concluons avec les pistes pour les travaux futurs (section 5).\nNotre objectif est de catégoriser les phrases des corpus dans les classes de risque. L\u0027é-valuation est effectuée par rapport aux données de référence. Une liste de mots vides et des ressources linguistiques sont aussi utilisées. Le travail est effectué avec le matériel en anglais.\nCorpus. Les corpus proviennent de la littérature scientifique, qui est le matériel typique utilisé par les experts. Le corpus du risque chimique (80 000 occ.) contient le rapport sur le bisphénol A (EFSA Panel, 2010). Le corpus du risque alimentaire (\u003e240 000 occ.) a été constitué à partir de 115 documents officiels publiés entre 2000 et 2010 sur une dizaine de substances, comme l\u0027arsenic, la dioxine ou le nitrate (Blanchemanche et al., 2013). Trois sections (introduction, conclusion et résumé) sont traitées car elles comportent les résultats principaux.\nClassifications du risque. Les classifications du risque (alimentaire (Blanchemanche et al., 2013) et chimique (Maxim et van der Sluijs, 2014)) sont structurées hiérarchiquement et décrivent différents aspects révélateurs de la nocivité des substances chimiques :\n-significativité des résultats (The Panel concluded that the current NOAEL for BPA (5 mg/kg b.w./day) would be sufficiently low to exclude any concern for this effect) ; -hypothèse scientifique (Despite this lack of evidence, the possibility of poultry and egg consumption as an exposure route to HPAIV remains a concern to food safety experts). Le risque est présent lorsque la nocivité des substances est apparente dans la littérature scientifique, ou lorsque les expériences présentées montrent des imprécisions et incertitudes.\nRessources linguistiques. Des ressources linguistiques sont utilisées avec l\u0027approche par apprentissage supervisé pour enrichir l\u0027annotation. Nous supposons que ces différentes expressions, souvent liées à la notion d\u0027incertitude, sont indicatrices de la notion du risque chimique :\n-l\u0027incertitude (e.g. possible, should, may, usually) indique des doutes existant au sujet des résultats obtenus expérimentalement, leur interprétation, etc. ; -la négation (e.g. no, neither, lack, absent, missing) indique que de tels résultats n\u0027ont pas été observés, que l\u0027étude ne respecte pas les normes, etc. ; -les limitations (e.g. only, shortcoming, insufficient) indiquent des limites, comme la taille insuffisante de l\u0027échantillon traité, le faible nombre de tests ou de doses testées, etc. ; -l\u0027approximation (e.g., approximately, commonly, estimated) indique d\u0027autres insuffisances liées aux valeurs imprécises de substances, d\u0027échantillons, de doses, etc. Avec la recherche d\u0027information, nous utilisons des ressources pour l\u0027extension de requêtes :\n-101 805 paires de synonymes provenant de la langue générale (Fellbaum, 1998) et spé-cialisée (Grabar et Hamon, 2010), -des clusters de mots générés avec des méthodes distributionnelles à partir des corpus (Brown et al., 1992;Liang, 2005). Données de référence. Les données de référence sont obtenues grâce à l\u0027annotation par des spécialistes en évaluation du risque. Un expert a annoté 425 phrases couvrant 55 classes du risque chimique. Plusieurs experts ont participé dans l\u0027annotation du corpus du risque alimentaire et fournissent des données de référence pour 657 phrases monoclasses couvrant 27 classes et 389 phrases multiclasses, pour un total de 1 046 phrases annotées. Plusieurs des classes contiennent très peu de phrases annotées et nous gardons celles qui fournissent un nombre suffisant d\u0027exemples (minimum de 10 pour le risque alimentaire, 5 pour le risque chimique).\nListe de mots vides. La liste de mots vides contient 176 mots (e.g., \u0026 about again all almost and any by do to etc). Cette liste contient essentiellement des mots grammaticaux.\n3 Approche par apprentissage supervisé Méthode. Nous utilisons différents algorithmes de la plateforme Weka (Witten et Frank, 2005) avec le paramétrage par défaut. Les phrases sont l\u0027unité de travail. Nous visons la dé-tection de phrases liées au risque : (1) de manière générale G pour détecter les phrases relatives au risque ; (2) de manière précise D pour associer ces phrases aux classes de risque. Les descripteurs sont fournis par l\u0027annotation sémantique et linguistique : forms (les formes de mots comme elles apparaissent dans le corpus), lemmas (mots lemmatisés), lf (combinaison de formes et de lemmes), tag (les étiquettes morpho-syntaxiques des formes (e.g. noms, verbes, adjectifs)), lft (combinaison de formes, lemmes et étiquettes morpho-syntaxiques), stag (étiquettes sémantiques de mots (e.g. incertitude, négation, limitations)), all (combinaison de tous les descripteurs). Les descripteurs sont pondérés de trois manières : freq (fréquence brute des descripteurs), norm (fréquence normalisée par la taille du corpus), tfidf (pondération tfidf (Salton et Buckley, 1987)). Nous effectuons une validation croisée. Les mesures d\u0027évaluation sont la précision, le rappel et la F-mesure (moyenne harmonique de la précision et du rappel). Résultats. Les résultats présentés sont obtenus avec J48 (Quinlan, 1993). Avec l\u0027expéri-ence G, les performances avec le risque alimentaire (autour de 0,8) sont meilleures que celles du risque chimique (0,61-0,64). Les performances sont assez stables avec les différents descripteurs et pondérations. L\u0027exploitation de formes, d\u0027étiquettes sémantiques et les différentes combinaisons de descripteurs donnent des résultats légèrement supérieurs. Bien que très simplistes, les étiquettes morpho-syntaxiques (e.g. noms, verbes, adjectifs) sont assez efficaces sur les deux corpus. Les étiquettes sémantiques seules (stag) sont parmi les plus efficaces pour détecter le risque chimique, mais montrent une F-mesure assez faible pour le risque alimentaire. À la figure 1, nous présentons l\u0027expérience D avec les descripteurs lft (formes, lemmes et étiquettes morpho-syntaxiques). Les résultats sont élevés avec les classes du risque alimentaire et deux classes du risque chimique (Facteur d\u0027incertitude et Hypothèses scientifiques). Le tfidf donne de meilleurs résultats dans la plupart des cas, mais la pondération norm est aussi compétitive. Les descripteurs lft fournissent de meilleurs résultats que les autres descripteurs. Les résultats sont meilleurs avec le risque alimentaire, où il existe plus de données d\u0027apprentissage.\nApproche de recherche d\u0027information\nMéthode. Nous considérons les libellés des classes comme les requêtes et les phrases des corpus comme les réponses potentielles à ces requêtes. Nous exploitons le système de recherche d\u0027information Indri (Strohman et al., 2005), qui utilise un modèle probabiliste basé sur le champ aléatoire de Markov et offre plusieurs fonctionnalités, comme par exemple :\n-la racinisation (Porter (Porter, 1980) et de Krovetz (Krovetz, 1993)) réduit un mot à sa racine (e.g., suppression de pluriels et de chaînes finales comme -ment et -ique) ; -le et booléen (band) permet de combiner plusieurs mots clés ; -les fenêtres ordonnées ou non ordonnées permettent de spécifier l\u0027ordre des mots clés ; -la pondération (tfidf (Salton et Buckley, 1987) et okapi (Robertson et al., 1998)) permet de relativiser le poids des mots-clés ; -la pondération des synonymes (wsyn) permet d\u0027indiquer l\u0027importance des mots clés. Pour l\u0027expansion des requêtes, nous retenons les mots supplémentaires des ressources linguistiques (synonymes et clusters) si ces mots montrent au moins 0,3 % de précision. L\u0027évaluation est effectuée avec plusieurs mesures : précision, rappel, F-mesure et MAP (Mean Average Precision), cette dernière prenant en compte l\u0027ordre des réponses. Pour la baseline, les mots des libellés de classes sont utilisés, sans la racinisation ni l\u0027expansion de requêtes. Résultats. Le tableau 1 indique la MAP et la F-mesure de différentes expériences : baseline, utilisation de raciniseurs, pondération des mots clés et des clusters. Nous obtenons de meilleurs résultats avec les libellés du risque chimique, car ils sont plus explicites. La F-mesure est en général plus élevée que la MAP. Les résultats sont améliorés avec la racinisation, la pondération tfidf et okapi, et les clusters. Plusieurs autres expériences n\u0027ont pas été concluantes (e.g. exploitation des définitions, pondération des synonymes, fenêtres ordonnées des mots clés des requêtes, et booléen). Krovetz, la pondération et les clusters fournissent les meilleurs ré-sultats (figure 2). Les raciniseurs améliorent le rappel et donc les performances globales, tandis que l\u0027utilisation de la pondération des mots clés (okapi ou tfidf) améliore surtout les valeurs de la MAP : les phrases retournées sont alors les mêmes, mais leur ordre devient plus correct. Les ressources linguistiques supplémentaires sont favorables pour certaines classes. Elles permettent surtout d\u0027améliorer le rappel.\nDiscussion et Conclusion\nL\u0027apprentissage supervisé est plus performant que la recherche d\u0027information, tandis que cette dernière, étant moins supervisée, permet de traiter un plus grand nombre de classes. La recherche d\u0027information permet aussi de varier plus facilement les paramètres selon que l\u0027on voudrait privilégier la précision ou le rappel. La pondération montre toujours un effet favorable. Dans une expérience similaire avec le risque alimentaire, des résultats comparables aux nôtres sont obtenus (Blanchemanche et al., 2013). Notons que nous avons aussi testé une approche non supervisée à base de règles, qui montre des résultats très faibles : rappel quasinul pour une précision entre 0,5 et 0,6. Il existe plusieurs possibilités pour combiner les deux approches testées : combinaison des sorties pour augmenter le rappel ; le vote des approches pour améliorer la précision ; l\u0027utilisation des noeuds décisionnels des modèles d\u0027apprentissage supervisé pour l\u0027extension de requêtes ; l\u0027exploitation des sorties de recherche d\u0027information et du système à base de règles par l\u0027apprentissage supervisé.\nEn conclusion, nous utilisons l\u0027apprentissage supervisé et la recherche d\u0027information pour détecter des phrases relatives au risque induit par les substances chimiques. Nous abordons la tâche comme une problématique de catégorisation : les phrases des textes doivent être caté-gorisées dans les classes de risque. Deux corpus et deux classifications du risque sont utilisés. Les résultats par apprentissage automatique sont les plus performants. Les résultats indiquent aussi que l\u0027expression de l\u0027incertitude linguistique (e.g., likely, should, assume) est associée avec la notion du risque chimique. Dans les travaux futurs, nous allons tester d\u0027autres paramètres pour améliorer les performances des approches testées et nous allons combiner les résultats de ces approches de différentes manières. Ces résultats peuvent être utilisés par les experts travaillant sur la gestion du risque pour la prise de décisions et évalués par eux.\nRemerciements. Ce travail est soutenu par le projet PNRPE DICO-Risk.\nRéférences\nBlanchemanche, S., A. Rona-Tas, A. Duroy, et C. Martin (2013). Empirical ontology of scientific uncertainty : Expression of uncertainty in food risk analysis. In Society for Social Studies of Science, pp. 1-27.\n"
  },
  {
    "id": "252",
    "text": "Introduction\nCes dernières années, nous assistons à la sémantisation des données statiques et dynamiques (flux de données). Toutefois, vu la spécificité de ces derniers ni les technologies du web sémantique ni celles des Systèmes de Gestions de Flux de Données (SGFD) ne peuvent les traiter. Pour ce faire, les chercheurs proposent aujourd\u0027hui de nouveaux systèmes tels que C-SPARQL (Barbieri et al., 2010), CQELS (Phuoc) et SPARQL Stream (Calbimonte et al.). Lorsque le débit du flux en entrée de ces systèmes dépasse les seuils supportés, deux solutions existent : 1-Allouer au système autant de ressources que nécessaires (Hoeksema et Kotoulas, 2011)  manière, nous préservons le niveau sémantique de l\u0027information et protégeons la cohérence des données du graphe en mémoire.\nConclusion\nNous avons proposé dans cet article une approche orientée graphe pour la réduction de charge des systèmes de traitement de flux de données sémantiques. Notre approche, permet d\u0027améliorer la qualité des résultats des requêtes des systèmes de traitement de flux de données sémantiques, en protégeant la sémantique et la cohérence des données de ces flux, contrairement à une application naïve de l\u0027approche orientée triplet RDF utilisée jusqu\u0027à présent.\n"
  },
  {
    "id": "253",
    "text": "Introduction à Sélection basée sur le Degré de Pertinence\nLes librairies digitales sont actuellement très répandues. Elles renferment des quantités d\u0027informations énormes et nécessitent des mécanismes efficaces d\u0027indexation et de manipulation. Les moteurs de recherche du type général ne peuvent pas les indexer car ils exigent que l\u0027information qu\u0027ils manipulent soit composée d\u0027entités indépendantes. Dans le besoin de traiter rapidement et efficacement les requêtes, des méthodes basées des approches diffé-rentes ont été inventées. On rencontre alors, des méthodes se basant sur les réseaux bayésiens comme CORI Callan et al. (1995), d\u0027autres méthodes qui se basent sur les statistiques TF*IDF. Il existe aussi des méthodes qui se basent sur le modèle de langage et la pseudo-pertinence. Ces méthodes utilisent des résultats déjà obtenus pour de réponses futures. Puisque le modèle centralisé souffre du problème de passage à l\u0027échelle, certaines méthodes ont été mises pour tourner sur les systèmes pair-à-pair. La méthode CORI a été une source d\u0027inspiration et a été utilisée comme moyen de classification dans beaucoup de travaux. Cette méthode fonctionne sur un système bayésien pour localiser des réponses probables aux utilisateurs. La fonction de score donnée dépend de certains paramètres obtenus à partir d\u0027expérimentations sur des datasets. Ce paramétrage fait que CORI est devenue instable. Ces paramètres doivent être réajustés pour chaque nouvelle collection. Afin de réduire le nombre de collections interrogées, Abbaci et al. (2002) présente la méthode CS. Celle-ci définit ndoc le nombre de documents à retourner et tient compte uniquement des deux premiers termes lors de l\u0027évaluation des requêtes longues. Bien que l\u0027objectif de réduction de flux est atteint, CS produit des faux positifs et faux négatifs importants à cause des restrictions imposées. Soit un système distribué où un serveur appelé courtier est lié à un ensemble de serveurs. Le courtier détient un index Terme/Serveur qui indique pour chaque terme t i la liste des serveurs qui le manipulent. Chaque serveur S i est responsable d\u0027une collection de documents c i et manipule un index Terme/Documents. Cet index définit pour chaque terme t i la liste des documents où il figure. Par cette définition, le courtier sélectionne de façon déterministe le sous-ensemble de serveurs pertinents. Ces index permettent de réduire la charge du système. Un document est jugé pertinent s\u0027il partage au moins un terme avec la requête. Plus un document partage de termes avec la requête plus son degré de pertinence s\u0027élève, induisant ainsi que le score d\u0027une Sélection basée sur la pertinence\ncollection est proportionnel aux nombre de documents pertinents qu\u0027elle contient. Sur cette définition, pour une requête q, le score d\u0027une collection c i se calcule selon la fonction SDP suivante :\nT F ti est la fréquence du terme t dans la collection c i . L\u0027expérimentation des trois méthodes sur le dataset Reuters21578, sur un système distribué. La figure Fig. 1 présente la comparaison entre les trois méthodes en fonction du recall. Nous avons réalisé des expéri-mentations intensives en faisant varier le Top-k. Nous remarquons que les valeurs pour cette métrique sont plus grandes dans SDP que dans les autres méthodes. CS (CS2 pour ndoc\u003d2, CS3 pour ndoc\u003d3, CS5 pour ndoc\u003d5) a retourné un recall plus faible. C\u0027est certainement à cause du ndoc qui influence la recherche. Avec ndoc\u003d2 et 3 ; le recall n\u0027atteint pas 1 c-à-dire il existe des documents pertinents et rares où le système n\u0027arrive pas à les sélectionner. CORI c\u0027est placé au-dessus de CS.\n"
  },
  {
    "id": "254",
    "text": "Introduction\nAvec les avancées technologiques en terme d\u0027acquisition des données scientifiques (images satellitaires, capteurs, etc.), les scientifiques s\u0027intéressent de plus en plus à des applications importantes en terme de surveillance et suivi de l\u0027environnement. Les données collectées sont généralement hétérogènes, multiéchelles, spatiales et temporelles (série temporelle d\u0027images satellites, aériennes, modèles numériques de terrain, nature du sol ...) et sont destinées à comprendre et prédire des phénomènes résultant de processus complexes et d\u0027origine pluridisciplinaire (données climatiques, géologiques, ...). L\u0027explosion de cette information spatiale, temporelle et des systèmes d\u0027informations géographiques nécessitent l\u0027investissement dans des méthodes d\u0027extraction de connaissances et nous nous intéressons à celles qui reposent sur la détection de motifs locaux comme, par exemple, la découverte de motifs séquentiels (Agrawal et Srikant, 1995;Mannila et al., 1997;Masseglia et al., 1998) ou de motifs plus complexes comme des sous-graphes Inokuchi et al. (2000) ou des sous-arbres Zaki (2002). Nos besoins concernent l\u0027étude spatiale et temporelle des évolutions d\u0027objets et de leurs interactions. Les objets peuvent être caractérisés par plusieurs attributs et leurs évolutions que l\u0027on appelle parfois dynamique se décrivent par les évolutions des attributs, par leur emplacement géographique, leur existence (apparition/disparition) et leur structure topologique (fusion/division). Pour certaines applications, nous pouvons transformer la base de données spatio-temporelles dans une base de données transactionnelles Hai et al. (2012) ou dans une base de séquences pour les analyser. Cependant, ces transformations peuvent s\u0027avérer très fastidieuse et les résultats peuvent être difficilement interprétables. Des domaines de motifs plus sophistiqués et applicables à l\u0027étude de phénomènes spatio-temporels ont donc été proposés. Ainsi, plusieurs travaux se sont intéressés à l\u0027extraction de motifs dans des graphes étiquetés Inokuchi et al. (2000). Quelques travaux ont été menés dernièrement sur des graphes attribués (Fukuzaki et al., 2010;Miyoshi et al., 2009;Desmier et al., 2013). Les difficultés dans la fouille de graphes attribués résident dans l\u0027explosion combinatoire de l\u0027exploration de l\u0027espace de recherche. En effet, cette espace de recherche porte à la fois sur les combinaisons de graphes et les combinaisons d\u0027attributs. Dans un travail présenté dans (Sanhes et al. (2013a,b)), Sanhes et al. ont proposé de travailler à la modélisation de données spatio-temporelles dans des DAG attribués, autrement dit un unique graphe orienté acyclique attribué (a-DAG) (cf. Figure 1) : les sommets sont des objets spatiaux caractérisés par un ensemble d\u0027attributs ou caractéristiques et les arcs dénotent la proximité spatio-temporelle entre ces objets (par exemple le voisinage spatial entre deux objets de deux pas de temps consécutifs). Le but est de trouver les transitions ou cheminements de caractéristiques pouvant montrer une tendance attendue ou surprenante, expliquer un phénomène particulier, ce qui revient à chercher dans un a-DAG les chemins fréquents d\u0027attributs. On trouve quelques travaux s\u0027attaquant à la fouille de graphes orientés FIGURE 1: Exemple de a-DAG construit sur des objets représentés dans des images temporelles acycliques mais étiquetés (et non pas attribués) tels que Chen et al. (2004);Termier et al. (2007). Ces méthodes recherchent des sous-graphes dans un ensemble de graphes, et de plus les sommets sont plutôt labélisés ou considérés comme labélisés et non attribués. Dans notre cas, on est en présence d\u0027un seul graphe orienté acyclique et attribué (a-DAG) ce qui pose des problèmes très différents. Le domaine de motif proposé pour la première fois dans Sanhes et al. (2013b) est appelé domaine des chemins pondérés dans un a-DAG. Lorsque nous avons voulu travailler à une implémentation efficace de l\u0027algorithme d\u0027extraction de ce type de motif, le seul à notre connaissance qui calcule des motifs dans des DAG attribués, nous avons étudié de près ses propriétés et nous avons découvert qu\u0027il était juste mais incomplet. Nous présentons ici un nouvel algorithme permettant de réaliser l\u0027extraction de tels motifs de façon complète. Non seulement nous proposons une correction du premier algorithme mais aussi nous étudions des optimisations nécessaires au passage à l\u0027échelle en introduisant des structures de données complémentaires comme un graphe de motifs. Nous montrons que la performance de l\u0027extraction est améliorée de plusieurs ordres de magnitude sur des jeux de données artificiels et nous l\u0027appliquons aussi à des données réelles pour motiver qualitativement l\u0027usage des chemins pondérés.\nDans la section suivante, nous présenterons les concepts et définitions nécessaires à la compréhension de l\u0027algorithme. En Section 3, nous prouvons l\u0027incomplétude de l\u0027algorithme existant. Nous proposons une solution de complétude et une optimisation basée sur une structure de graphe de motifs en Section 4. Nous montrerons les performances de l\u0027algorithme complet et optimisé sur des jeux de données artificielles en Section 5. Et enfin, nous conclurons en Section 6. \nUn chemin P est une séquence d\u0027itemsets P \u003d P 1 2 · · · n tel qu\u0027il existe un chemin O \u003d v 1 2 · · · n dans le graphe où P i est inclus dans l\u0027ensemble des items de v i (notion à différentier de la définition classique d\u0027un chemin dans un graphe). On dit que alors que O est une occurrence du chemin P et l\u0027ensemble des occurrences de P est noté occu G (P ).\nPar exemple dans le graphe de la figure 2 les occurrences du chemin de taille 3 ah sont 2 3 6 , 2 3 8 , 2 4 7 , 2 5 7 , et 5 7 8 . Un chemin pondéré P est un chemin où un poids est associé à chaque arc P i i+1 constituant P . Ce poids correspond au nombre d\u0027occurrences distinctes de P i i+1 dans le graphe. Pour l\u0027exemple précédent, le chemin ah et ses occurrences permettent de construire le motif pondéré : ah 4 cd 5 i. En effet, le nombre d\u0027occurrences de ah dans occur G (P ) st 4, et le nombre d\u0027occurrences de cd dans occur G (P ) est 5. Une telle représentation permet de voir que l\u0027itemset ah apparaît 4 fois avant l\u0027apparition du chemin cd et que l\u0027itemset i apparaît 5 fois après l\u0027apparition du chemin ah Dorénavant, ? G (P i i+1 ) désignera le poids de l\u0027arc entre les itemsets\nRelation d\u0027inclusion L\u0027opérateur d\u0027inclusion sur un couple de chemins pondérés est défini de la manière suivante :\nAutrement dit, P est inclus dans P\u0027 s\u0027il existe une sous-séquence Q de P\u0027 tel que les itemsets de P sont inclus un à un dans ceux de Q avec les mêmes poids au niveau des arcs. On dit que P est un sur-chemin ou super-chemin pondéré de P . A partir de la mesure proposée par Bringmann et Nijssen (2008), nous définissons le support d\u0027un chemin pondéré P, noté ?(P ), comme étant le poids minimal de ses arcs.\nChemin pondéré condensé Un chemin pondéré P est un condensé s\u0027il n\u0027admet aucun surchemin pondéré. Pour simplifier, nous appellerons motif un chemin pondéré et les occurrences d\u0027un motif seront tout simplement des chemins.\nCette méthode permet bien l\u0027extraction des motifs condensés de manière juste mais ne les génère pas tous. En effet, toutes les graines condensées forment bien des motifs condensés mais un motif condensé de taille supérieure à 2 peut contenir des graines non condensées. Effectivement, il existe des motifs condensés au sens de l\u0027inclusion qui peuvent être formés par certains motifs de taille 2 qui ne sont pas générés par la première étape.\nPour illustrer l\u0027incomplétude de l\u0027algorithme, nous montrons un contre-exemple sur le graphe de la figure 3. Dans ce a-DAG, les graines générées par la première étape de l\u0027algorithme ne permettent pas de construire le motif condensé a 1 bc 1 de.\n:a 2 :a\nCondensés représentés dans le a-DAG :\n• chemins de tailles 2 : a de où ? ne vaut plus 2 car toutes les occurrences de P ne sont pas utilisées : 2 4 n\u0027est pas relié à de. Dans ce cas on parle d\u0027extension avec perte d\u0027occurrences. Par conséquent il faut déterminer les occurrences utilisées et mettre à jour les différents poids (?) ainsi que les itemsets du motif. En réalité avec la séparation des 2 étapes, l\u0027information structurelle est perdue pendant le parcours en profondeur.\nAlgorithme complet et optimisé\nDans ce paragraphe, nous proposerons une solution permettant de corriger la complétude et nous proposerons par la même occasion une version optimisée utilisant une structure de graphe permettant de stocker les motifs que l\u0027on appellera graphe de motifs.\n... ?n I n un motif. Nous pouvons étendre P sans perte d\u0027occurrences s\u0027il existe un itemset I n+1 fermé fréquent dans l\u0027ensemble des sommets accessibles par V n (nous appellerons I n+1 un fermé fréquent local à V n ), tels que V n+1 supporte I n+1 et qu\u0027il existe au moins un arc de chaque sommet de V n vers V n+1 , c\u0027est-à-dire que toutes les occurrences de P sont conservées. De manière analogue, nous pouvons étendre P avec perte d\u0027occurrences lorsque toutes les occurrences de P ne sont pas utilisées lors de l\u0027extension. Dans ce cas nous obtenons un motif P \u003d I 1 ? 1\n... \nStratégie de l\u0027algorithme complet\nÀ partir des notions introduites précédemment, nous pouvons présenter la stratégie géné-rale de l\u0027algorithme complet (cf. algorithme 1). Cet algorithme est basé sur un parcours en profondeur de l\u0027espace de recherche pour étendre les motifs condensés.\nEn partant de l\u0027ensemble des sommets du graphe, l\u0027algorithme effectue un parcours en profondeur dans l\u0027espace de recherche pour étendre le motif condensé P initialisé à ?. La ligne 1 exprime le cas d\u0027arrêt de l\u0027algorithme : l\u0027ensemble des sommets destinations V P est vide. Le parcours en profondeur se fait aux lignes 2 et 3. L\u0027extension de P se fait avec l\u0027itemset Y fermé fréquent par rapport aux sommets de V P (ligne 4). Nous notons P le motif ainsi étendu. Lorsqu\u0027il s\u0027agit d\u0027une extension avec perte d\u0027occurrences, il est nécessaire de mettre à jour les occurrences du motif (ligne 5). Ce nouveau motif P est potentiellement un motif ou un sous-motif condensé. Nous supprimons les motifs qui sont inclus dans P , et insérons P dans C l\u0027ensemble des motifs condensés (lignes 7 et 8). Puis nous continuons le parcours (ligne 9).\nLa complexité de l\u0027algorithme ne permet pas le passage à l\u0027échelle à cause de nombreux tests coûteux d\u0027inclusion de motifs pour vérifier sa maximalité. Nous proposons ci-dessous une implémentation optimisée basée sur une structure de graphe pour stocker les motifs. Cette nouvelle structure va permettre d\u0027éviter les tests trop coûteux de comparaison entre motifs.\nAlgorithme 1 : DepthFirstMining\nEntrées : P motif courant (? à l\u0027appel initial) V P ens. des sommets destinations de P (V P \u003d V G à l\u0027appel initial) C ens. des motifs condensés (? à l\u0027appel initial) Mettre à jour les occurrences de P 6\nSupprimer dans C les motifs Q inclus dans P 7\nInsérer P dans C.  \nImplémentation optimisée\nNous allons nous servir de la maximalité des chemins pondérés condensés recherchés pour optimiser l\u0027algorithme qui se traduit par la maximalité des itemsets (itemsets fermés) du chemin et sa taille. Les tests d\u0027inclusions de l\u0027algorithme se font sur les itemsets et sur les chemins. Pour éviter ces tests nous allons définir une structure de graphe permettant le stockage des motifs trouvés au fur et à mesure du parcours de l\u0027espace de recherche. Cette structure est appelée graphe de motifs. • V m ? P(V ) est l\u0027ensemble des sommets • E m ? P(V ) × P(V ) est l\u0027ensemble des arcs • ? m : la fonction d\u0027attributs définie par : V m ?? P(I) V ?? X où X représente l\u0027itemset maximal caractérisant les sommets de V . Réciproquement, on associe à un itemset X l\u0027ensemble des sommets noté V X supportant l\u0027itemset X. Un sommet du graphe G m est identifié par un ensemble de sommets du graphe G. Un motif condensé est alors un chemin c \u003d V 1 n de G m de longueur maximale (cf. figure 5), i.e. V 1 est une source (pas d\u0027arc incident) et V n est un puits (pas d\u0027arc sortant).\nProcédure cherCondRec(X : itemset, V X : ens. de sommets, min_sup : entier) sommets_a_remonter : var. globale contenant les sommets à backtracker\nBdT (E + (V X )) la base de données transactionnelles construites à partir de ...\n?n?1 I n , c\u0027est une séquence d\u0027itemsets et chaque itemsets I i du motif P représente un sommet dans le graphe des motifs qui n\u0027est autre que V Ii ensemble des sommets du graphe a-DAG contenant l\u0027itemsets I. Le motif P est alors identifié de manière unique par la séquence V I1 In dans le graphe des motifs. Au moment de la construction du graphe des motifs et à l\u0027insertion d\u0027un nouveau sommet V i dans le graphe des motifs, il suffit de vérifier s\u0027il est déjà présent dans le graphe des motifs alors il a déjà été parcouru sinon il est inséré et le parcours de l\u0027espace de recherche continue. L\u0027algorithme final se déroule en 2 grandes étapes suivantes : -Recherche des motifs condensés par un parcours en profondeur de l\u0027espace de recherche (procédure 2). Les motifs sont stockés dans le graphe des motifs. Pendant la recherche, les sommets pour lesquels il y a eu extension avec perte d\u0027occurrences sont marqués pour être traités par la phase de backtracking.\n-Phase de backtracking sur les sommets marqués (pour lesquels il y a eu extension avec perte d\u0027occurrences) pour mettre à jour les occurrences des motifs (cf. procédure 3). La première étape fait appel à la procédure cherCondRec qui effectue un parcours en profondeur de l\u0027espace de recherche. Cette procédure étend récursivement les motifs condensés au fur et à mesure de leur construction dans G m . A une étape de la construction du motif P , soit V X le sommet à étendre dans le graphe des motifs G m supportant l\u0027itemset X, on calcule V X utile ensemble de tous les sommets ayant au moins un arc sortant de V X . On calcule tous les items accessibles par X, on obtient une base de données transactionnelles dont les transactions sont les arcs sortants et les items sont les items accessibles par X (ligne 3 et 4). Pour chaque itemset maximal Y dans cette base transactionnelle, on va étendre V X par V Y . Deux cas se présentent :\n• V X \u003d V X utile : tous les sommets de V X sont utilisés pour l\u0027extension, il y a extension sans perte. Il suffit alors de créer le sommet V Y et le relier à V X par un arc dans G m .\n• V X \u003d V X utile : l\u0027extension est réalisé avec perte d\u0027occurrences, on duplique le motif P en remplaçant le sommet V X par V X utile (à marquer pour être traiter dans la phase de Backtracking). On insère un nouveau sommet V Y et on crée un arc entre\nProcédure backtrackRec(V X utile : sommet du graphe des motifs, V toBacktrack : sommet du graphe des motifs, min_sup : entier)\nSoit V i utile ensemble des sommets de V i ayant au moins un arc sortant vers V X utile .\n4\nInsérer V i utile ? \u003e V X utile dans le graphe des motifs G m 5 // Lors de l\u0027insertion, mise à jour des attributs. backtrackRec(V i utile , V i )// backtracking vers le haut 6 L\u0027étape de backtracking décrite par la procédure récursive backtrackRec retraite chaque sommet marqué en visitant la branche du motif dans le sens inverse pour mettre à jour les occurrences des motifs et les informations tels que les poids et les itemsets. La figure 6 montre le déroulement de l\u0027algorithme sur l\u0027exemple du graphe de la figure 5. Les arcs du graphe de motifs sont en bleu. Les arcs en rouge définissent le cas d\u0027extension avec perte d\u0027occurrences, les arcs en vert montrent la phase de backtracking avec la mise à jour des poids et des itemsets.\nExpérimentations et résultats\nNous avons appliqué la méthode sur des jeux de données artificielles pour montrer la performance que nous avons comparé avec la méthode incomplète. Pour montrer l\u0027intérêt de ce nouveau domaine de motifs, nous avons utilisé un jeu de données réelles pour le problème de suivi du phénomène de l\u0027érosion.\nDans un premier temps, nous avons créé artificiellement trois jeux de données afin d\u0027observer l\u0027impact de la taille des a-DAG sur les performances notés « V20K, E60K » pour un a-DAG   données artificiel ressemble plus aux jeux de données tirés d\u0027une application spatio-temporelle.\n"
  },
  {
    "id": "255",
    "text": "Introduction\nLa quantité d\u0027information dans le Web a augmenté ces dix dernières années. Ce phénomène a favorisé la progression de la recherche dans le domaine des systèmes de recommandation. Les systèmes de recommandation consistent en un filtrage de l\u0027information dans le but de ne présenter aux utilisateurs que les éléments qui sont susceptibles de l\u0027intéresser, quel que soit le domaine. Les éléments à recommander sont également appelés items et peuvent être de différents types : des produits, services, informations, etc. Les systèmes de recommandation se doivent de sélectionner les informations les plus intéressantes en fonction du but recherché, tout en conciliant nouveauté, surprise et pertinence. Un système de recommandation se base sur des caractéristiques de références acquises de manière automatisée selon plusieurs méthodes différentes. Les caractéristiques de références peuvent provenir de : -L\u0027item (l\u0027objet à recommander) lui-même, on parle alors « d\u0027approche basée sur le contenu » (ou content-based approach) Balabanovi´cBalabanovi´c et Shoham (1997). Le filtrage basé sur le contenu calcule la similarité entre les objets afin de trouver l\u0027objet le plus semblable aux goûts de l\u0027utilisateur. Dans ce cas, l\u0027utilisateur se voit recommander des items similaires à ceux qu\u0027il a préférés dans le passé. -L\u0027utilisateur et l\u0027environnement social, on parle alors « d\u0027approche de filtrage collaboratif » (ou collaborative filtering). Le principe du filtrage collaboratif Breese et al. (1998)   Koren et al. (2009). Pour remplir leurs fonctions, les technologies de recommandation font aujourd\u0027hui face à des défis scientifiques majeurs. Comment intégrer l\u0027hétérogénéité des sources d\u0027information pour modéliser les préférences, comment prendre en compte le contexte, comment traiter efficacement ces masses d\u0027information, quels types d\u0027interfaces faut-il considérer ? Par ailleurs, les deux approches citées précédemment présentent des inconvénients principalement liés au démarrage à froid et à la montée en charge du système d\u0027où la nécessité de mettre en place des algorithmes performants et robustes. Ceci est l\u0027objectif de cette étude en vue d\u0027amélio-rer la qualité des systèmes de recommandation en introduisant de la sémantique aux données et en distribuant les traitements afin de minimiser les temps de calcul. La sémantique est ici représentée par une ontologie du domaine (domaines des films pour les expérimentations).\nArchitecture globale\nAfin de fournir une généricité dans le domaine d\u0027application, un passage à l\u0027échelle et une recommandation précise, nous proposons un système à trois couches : une couche de pré-analyse, une couche sémantique et une couche de recommandation.\nLe module de pré-analyse met en oeuvre un filtre de comptage afin d\u0027étudier en profondeur l\u0027intérêt implicite des utilisateurs : le filtre permet de compter le nombre de fois qu\u0027une valeur d\u0027un attribut figure parmi les items évalués par les utilisateurs, pour cela nous utilisons un filtre de Bloom. Un filtre de Bloom Broder et Mitzenmacher (2004) est un tableau de bits qui permet de tester d\u0027une manière rapide l\u0027appartenance d\u0027un élément à un certain ensemble. Le FBC, décrit dans Broder et Mitzenmacher (2004) est une extension du filtre de Bloom standard qui fournit la possibilité de supprimer des éléments du filtre. Le vecteur de bits y est remplacé par un tableau d\u0027entiers, où chaque position est utilisée comme compteur. L\u0027insertion d\u0027un élément est réalisée en incrémentant de 1 les entiers aux positions renvoyées par les fonctions de hachage. Le retrait est réalisé en décrémentant de 1 ces entiers. La question d\u0027appartenance d\u0027un élément au filtre est traitée en regardant si tous les entiers aux positions renvoyées par les k fonctions de hachage sont strictement positifs. Nous proposons de se baser sur l\u0027ontologie du domaine afin d\u0027extraire les attributs des items. Ensuite, nous utilisons les filtres de bloom avec compteur afin de stocker l\u0027intérêt implicite des utilisateurs dans les attributs des éléments. Ceci se fait en suivant les étapes : (1) Pour chaque utilisateur, nous créons un filtre de bloom avec compteur vide, (2) pour chaque élément noté par cet utilisateur, nous extrayons ses attributs et enfin (3) nous insérons ces attributs dans le filtre . Ainsi, le filtre contient tous les attributs des items précédemment notés par l\u0027utilisateur.\nLe module sémantique exploite l\u0027ensemble des données ainsi que l\u0027ontologie dans le but de définir les relations entre les utilisateurs, les items et les attributs. Ceci se traduit par la transformation sémantique des notes des utilisateurs. Nous nous intéressons tout d\u0027abord au nombre d\u0027occurrence des attributs qui ont été notés par un utilisateur. Nous appelons cette occurrence « la fréquence d\u0027apparition » ou « coïncidence » : cette valeur correspond au nombre de fois que les valeurs des attributs se répètent dans les items notés par l\u0027utilisateur. Cette valeur est extraite à partir des filtres de bloom avec compteurs.\nLa deuxième étape consiste à calculer la valeur sémantique (SV) en se basant sur la fré-quence d\u0027apparition. L\u0027équation utilisée est la suivante (1).\nAvec F le nombre total des attributs, N u le nombre total des items notés par l\u0027utilisateur \"u\". C j est la fréquence d\u0027apparition de l\u0027attribut j dans l\u0027ensemble des items qui ont été notés par l\u0027utilisateur et W j étant le poids calculé à partir de la phase de la sélection des attributs par une analyse des composantes principales.\nE[r u, * ] est la moyenne des notes de l\u0027utilisateur et r u,i est la valeur du rating initial donnée à l\u0027item \"i\". L\u0027utilisation de N permet de normaliser l\u0027équation sémantique. Cette équation a l\u0027intérêt de pouvoir prendre en compte des valeurs positives et/ou négatives comme note.\nL\u0027équation sémantique peut être appliquée à deux niveaux dans la recommandation. D\u0027une part, nous pouvons appliquer cette équation à toutes les notes disponibles dans la base de données initiale, ce qui permet de mieux expliquer l\u0027intérêt des utilisateurs pour les caractéristiques définissant les items notés (ajouter du sens à la note). D\u0027autre part, nous pouvons faire le choix d\u0027appliquer l\u0027équation sémantique à la sortie de la recommandation. Supposons que le module de recommandation renvoie un résultat des top K items (les K items les plus pertinents) pour un utilisateur donné, avec une estimation de la note pour ces top K. Ces notes seront transformées en une note sémantique suivant l\u0027équation (1) et les items proposés seront réordonnés en conséquence en top K\u0027, K\u0027 pouvant être inférieur ou égal à K.\nEnfin, le module de recommandation utilise une technique de filtrage collaboratif basée sur une méthode de factorisation de la matrice pour générer des recommandations précises. Nous avons fait le choix d\u0027utiliser la factorisation de matrice car cette technique a montré son efficacité comme méthode de filtrage collaboratif pour la recommandation Koren et Bell (2011 Nous avons testé le module sémantique au travers des deux approches : -Application du module sémantique au jeu de données (Semantic dataset). Dans cette approche, l\u0027application du module sémantique (que nous appelons \"sémantisation\") porte sur les données en entrée du système de recommandation. Il s\u0027agit donc de traiter l\u0027ensemble du jeu de données, avant son analyse par le système de recommandation et le filtrage collaboratif. Cette approche nous permet de retrouver des objets non pris en compte a priori. Cependant, puisque le module sémantique doit analyser tout le jeu de données, le temps de calcul peut être grandement augmenté par rapport à une analyse non sémantique. -Application du module sémantique au résultat de la recommandation Top K (semantic top K). Dans cette première approche, nous cherchons à \"sémantiser\" les données en sortie du système de recommandation. Le système de recommandation fournit classiquement une liste de K objets, ordonnés par ordre décroissant de préférence. La sé-mantisation réordonne ces objets, et fournit une nouvelle liste plus pertinente. Cette approche est extrêmement légère, et permet d\u0027améliorer la recommandation sans (trop de) perte de temps. De plus, avec cette approche, les paramètres de pondérations peuvent être personnalisés par l\u0027utilisateur plutôt que de considérer l\u0027ensemble du jeu de données. Néanmoins, une analyse a priori des données de l\u0027utilisateur est nécessaire avant de personnaliser les paramètres de pondérations. Dans la section suivante, nous présentons comment ces deux approches modifient les recommandations, et la qualité des résultats obtenus.\nF-Mesure\nCette métrique est généralement utilisée dans l\u0027évaluation des systèmes de recommandations. Cette métrique n\u0027évalue pas la qualité de la prédiction des notes, mais la pertinence des items qui sont proposés aux utilisateurs. La F-mesure est une façon courante de combiner le rappel et la précision dans une seule métrique afin de faciliter la comparaison. le rappel étant la probabilité qu\u0027un item choisi soit pertinent et la précision calcule la probabilité qu\u0027un item pertinent soit choisi. tel que nous pouvons le constater dans la figure 1, nos approches donnent\nFIG. 2 -ILS in \"genre\" attribute.\nde meilleurs résultats que la technique de matrice de factorisation (SVD++) sans sémantique. l\u0027amélioration est plus prononcée pour le cas de la\" sémantisation\" du jeu de données.\nILS\nILS (Intra-List Similarity), appelée également ILD (Intra-List Diversity) mesure la diversité/similarité entre les items dans la liste des top-k présentée à l\u0027utilisateur. Un bon système de recommandation doit trouver l\u0027équilibre entre ces deux concepts diversité et similarité. En effet, des items trop diversifiés peut provoquer une confusion chez l\u0027utilisateur, alors que recommander toujours les mêmes items peut ennuyer celui-ci. Le figure 2 représente cette mesure en se concentrant sur le attribut genres des films dans le top-k. Des valeurs élevées correspondent à une grande similarité. Ainsi, nous pouvons constater que notre approche permet de retourner des items plus similaires dans le top-k. Ceci est du au fait que nous prenons en compte l\u0027intérêt pour les attributs afin d\u0027identifier les items susceptibles d\u0027intéresser l\u0027utilisateur.\nConclusion\nNous avons proposé un système de recommandation qui repose sur deux concepts : relations sémantiques entre les données manipulées et un filtrage collaboratif basé sur la factorisation des matrices. Dans le but d\u0027améliorer la pertinence des recommandations, nous avons étudié en profondeur l\u0027intérêt implicite des utilisateurs pour les attributs des items. pour cela, nous appliquons une équation sémantique permettant de modifier les notes initiales des utilisateurs pour refléter leur intérêt pour les items.\nNotre système de recommandation opère en plusieurs étapes : Nous comptons le nombre de fois qu\u0027un attribut figure dans les items notés par les utilisateurs. Pour cela, nous nous sommes appuyés sur l\u0027utilisation d\u0027un filtre de bloom avec compteur. Ensuite, après passage par le module sémantique (transformation des notes des utilisateurs en appliquant l\u0027équation sémantique) , les recommandations sont générées en utilisant une technique de matrice de factorisation (SVD++).\nL\u0027approche proposée dans ce papier a montré un intérêt pour la recommandation de films en utilisant le jeu de données MovieLens combiné à une ontologie de films, peuplé par les données de IMDB. Nous avons fait le choix de travailler avec ce jeu de données car il est disponible et public et c est celui qui est le plus utilisé dans les expérimentations autour des systèmes de recommandation. Toutefois, nous avons conçu notre approche indépendamment du jeu de données que nous avions utilisé. Notre approche peut être utilisée comme une boite noire nécessitant de se connecter à une base de données des ratings disponibles, mais aussi à l\u0027ontologie du domaine de l\u0027application. Nous envisageons de tester notre approche sur des jeux de données provenant d\u0027autres applications telles que la recommandation nutritionnelle ou de tourisme. Nous envisageons également d\u0027intégrer dans notre approche la prise en compte des notes négatives (attributs non aimés par l\u0027utilisateur).\n"
  },
  {
    "id": "257",
    "text": "Introduction\nTraditionnellement, les données hydrométriques se présentent sous la forme de séries temporelles, représentant des mesures effectuées régulièrement par des stations : ces mesures peuvent concerner différents aspects comme les hauteurs et les débits de l\u0027eau dans les cours d\u0027eau, les quantités de précipitations, etc. Comme ces mesures sont souvent prélevées par un réseau distribué de capteurs, le problème des données manquantes est inévitable. Allant d\u0027une simple valeur manquante à une longue plage de valeurs manquantes, les lacunes peuvent avoir des causes multiples : dysfonctionnement des capteurs, maintenance des stations de mesure, erreurs humaines, etc. (Harvey et al. (2010)).\nLe réseau hydrométrique au Luxembourg fournit un bon cas d\u0027utilisation. Il est constitué de différentes stations hydrométriques permettant de mesurer notamment les débits des cours d\u0027eau. Les mesures sont ensuite fréquemment utilisées dans les modèles numériques de prévi-sion hydrologique ou pour calculer des statistiques sur les écoulements (e.g. temps de retour des crues ou des sécheresses).\nEn conséquence, lorsque certaines séries de mesures présentent beaucoup de lacunes (par exemple : les données de la station de HallerBach au Luxembourg, Figure 1), cela pose de nombreux problèmes et il est nécessaire d\u0027apporter un soin très particulier à combler ces lacunes avec une bonne précision.\nAfin de combler ces lacunes, les méthodes classiques d\u0027analyse de données ont été appliquées dans le domaine hydrologique (Salas (1980)), et des travaux récents tentent de fournir des solutions toujours plus efficaces (Harvey et al. (2010); Mwale et al. (2012)  Kotsiantis (2013)). -Les réseaux de neurones artificiels ont récemment été utilisés, notamment via des perceptrons (Tfwala et al. (2013)) ou des cartes auto-adaptatives (Mwale et al. (2012)). -L\u0027algorithme espérance-maximisation (EM) est souvent utilisé pour reconstruire des données manquantes (Van Hulse et Khoshgoftaar (2008)). -Enfin, différentes techniques de prédiction de séries temporelles peuvent être utilisées suivant les caractéristiques des séries (ARMA, ARIMA, etc.). Habituellement, les experts en hydrologie se servent de divers scripts pour corriger les données (R, MATLAB). Ainsi est-il important pour eux de pouvoir disposer d\u0027un outil interactif pour à la fois intégrer les diverses sources de données, visualiser les séries temporelles, et enfin choisir le mode d\u0027estimation de valeurs manquantes le plus adapté.\n2 gapIT : un outil pour estimer les données manquantes 2.1 Technologie gapIT est une application développée en JAVA et basée sur le logiciel d\u0027analyse et de traitement de données Cadral (Pinheiro et al. (2014) \nInspection et caractérisation des valeurs manquantes\nLes valeurs manquantes ne se corrigent pas de la même manière selon le contexte (taille des trous, saison durant laquelle les valeurs sont manquantes, probabilité qu\u0027une crue soit en cours, etc.) (Gyau-Boakye et Schultz (1994)  Premièrement, la phase de sélection des stations de référence est critique, car elle permet à l\u0027expert de choisir les séries temporelles qui serviront à compléter les séries temporelles lacunaires. Pour ce faire, l\u0027outil propose plusieurs approches complémentaires :\n-la sélection des stations les plus proches géographiquement ; -la sélection des stations se trouvant sur le même cours d\u0027eau (en amont et/ou en aval) ; -la sélection des stations ayant les séries temporelles les plus similaires sur la période concernée ; la similarité est calculée en utilisant la déformation temporelle dynamique (Dynamic Time Warping) (Berndt et Clifford (1994)). Deuxièmement, l\u0027outil propose diverses méthodes d\u0027estimation : interpolation, régressions multiples, arbres de régressions, réseaux de neurones (perceptron multi-couches), algorithme des plus proches voisins, etc.\nTroisièmement, l\u0027utilisateur peut ensuite appliquer via le logiciel la méthode d\u0027estimation sélectionnée en se basant sur les séries temporelles choisies pour évaluer les valeurs man-quantes. Or, il est important de déterminer la précision des estimations produites. Pour ce faire, des trous fictifs sont créés dans la fenêtre de temps proche du trou réel à remplir (par exemple : un trou avant, un trou après, élargissement du trou en cours d\u0027examen ou à un endroit choisi par l\u0027utilisateur). Ensuite, les mesures suivantes sont calculées : l\u0027erreur absolue moyenne (MAE), la racine carrée de l\u0027erreur quadratique moyenne (RMSE) et le coefficient Nash-Sutcliffe car c\u0027est un indicateur très commun en hydrologie (Nash et Sutcliffe (1970)).\nPour finir, l\u0027outil est capable de calculer automatiquement la configuration optimale (stations de référence et algorithme). Dans ce cas, l\u0027utilisateur garde la possibilité de modifier la configuration à son gré de manière à obtenir un nouveau résultat plus proche de ses attentes.\n3 Exemple : les débits des cours d\u0027eau au Luxembourg gapIT a été utilisé pour estimer les débits d\u0027écoulement manquants pour des stations sélec-tionnées au Luxembourg, sur la période allant du 1er janvier 2007 au 31 décembre 2013. Le jeu de données utilisé correspond à des mesures effectuées toutes les quinze minutes dans 24 stations. Afin de tester l\u0027efficacité de l\u0027outil, un ensemble de trous fictifs a été créé pour ces stations. Pour obtenir un ensemble représentatif, les trous générés sont de différentes tailles, se situent durant différentes saisons, etc. Ensuite, pour chacun de ces trous fictifs, toutes les techniques d\u0027estimation proposées par l\u0027outil ont été testées, et pour chacun des cas, les taux d\u0027erreur ont été mesurés (MAE, RMSE, Nash-Sutcliffe).\nAinsi, nous avons constaté que les réseaux de neurones et les arbres de régression permettent d\u0027obtenir les taux d\u0027erreur les plus faibles. De plus, si l\u0027on considère le meilleur ré-sultat concernant chaque trou, alors on voit que les taux d\u0027erreur sont globalement très faibles. Cela signifie que pour ces cas, une estimation correcte est possible en utilisant les données pré-sentes (Figure 4). En revanche, dans un certain nombre de cas, les meilleurs taux d\u0027erreurs sont élevés. Après analyse, il s\u0027avère que pour les stations concernées, il n\u0027existe pas suffisamment de stations assez proches, similaires ou dépendantes afin de créer une estimation assez précise. \n"
  },
  {
    "id": "258",
    "text": "Contexte\nLa recrudescence des documents textuels disponibles sur le web incite de plus en plus travaux à l\u0027exploitation de ces données de manières automatiques. Pour faire interagir ces données entre elles de manière efficace, il faut développer des moyens basés non seulement sur la ressemblance syntaxique mais également sur la correspondance sémantique.\nGEOLSemantics est une entreprise qui propose une solution logicielle de traitement linguistique basée sur une analyse linguistique profonde. Le but est d\u0027extraire automatiquement, d\u0027un ensemble de textes, des connaissances structurées, localisées dans le temps et l\u0027espace. Pour représenter ces connaissances, nous avons opté pour les technologies du web sémantique. Nous représentons nos extractions sous forme de triplets RDF et exploitons une ontologie pour apporter de la cohérence. Cette approche permet de relier les résultats de nos extractions aux connaissances du Linked Open Data, tels que Dbpedia et Geonames.\nLors de l\u0027analyse linguistique, il arrive que l\u0027information traitée contienne des imperfections. Dans notre travail, nous intéressons à l\u0027incertitude. Notre première contribution porte sur une catégorisation de l\u0027incertitude lors des différentes phases d\u0027extraction. Notre seconde contribution se situe au niveau de la représentation de l\u0027incertitude dans le graphe RDF.\n2 Acquisition de l\u0027information avec incertitude L\u0027analyse des textes comporte plusieurs étapes distinctes allant du simple découpage du texte en mots à la représentation de son contenu. Parmi ces étapes, nous retrouvons : (i) l\u0027analyse syntaxique, il s\u0027agit de la mise en évidence des structures d\u0027agencement des catégories grammaticales, afin d\u0027en découvrir les relations formelles ou fonctionnelles. (ii) l\u0027analyse sé-mantique, l\u0027objectif principal de cette analyse est de déterminer le sens des mots des phrases. (iii) l\u0027extraction de connaissances permet de mettre en évidence des entités nommées et des relations relatives à un concept particulier. Grâce à des déclencheurs qui indiquent qu\u0027une relation relative à un concept peut être présente et extraite. Un déclencheur correspond généra-lement à un concept présent dans l\u0027ontologie, ce qui permet de guider la règle d\u0027extraction par la suite. (iv) la mise en cohérence permet de consolider les connaissances extraites notamment le regroupement des entités nommées, la résolution des dates relatives. Cette étape peut être Gestion de l\u0027incertitude. suivi par un enrichissement à partir des données du Linked Open Data. Cependant, la fiabilité de l\u0027information est très souvent remise en cause. Notre démarche est de considérer le cycle de vie de la connaissance depuis son acquisition jusqu\u0027à son stockage dans la base de connaissances pour cela, nous identifions trois catégories : Pré-extraction de la connaissance : il s\u0027agira lors de cette étape de considérer les modalités de publication de l\u0027information à savoir : la date et le lieu de publication, la fiabilité accordée à la source, qu\u0027il s\u0027agisse de l\u0027auteur ou de l\u0027organisme de publication... Pendant l\u0027extraction de la connaissance : l\u0027incertitude pourra concerner aussi bien l\u0027information véhiculée que la règle d\u0027extraction à appliquer. Post-extraction de la connaissance : l\u0027incertitude peut intervenir au niveau des règles de mise en cohérence ou bien au niveau du choix de la base de référence.\nLe formalisme de représentation de connaissances choisie est le RDF tout en nous basons sur une ontologie développée pour prendre en compte les concepts relatifs à un domaine particulier. Notre approche consiste à considérer l\u0027incertitude comme une connaissance à part entière telle que le décrit l\u0027ontologie suivante. La classe Uncertainty, nous permet de modéliser l\u0027incertitude. Elle est décrite par trois proprié-tés : weight : une propriété littérale pour quantifier l\u0027incertitude identifiée, hasUncertainProp : une propriété objet qui servira d\u0027intermédiaire entre le domaine initial de la propriété et la propriété en question, isUncertain : propriété objet qui aura pour co-domain le top-concept, cela veut dire que tout concept de l\u0027ontologie pourra être visé par une incertitude. Cette ontologie est indépendante de tout domaine d\u0027application. Dès lors, elle peut être ajoutée à toute autre ontologie voulant prendre en compte l\u0027incertitude.\nConclusion et perspectives\nDans cet article nous nous intéressons au traitement de l\u0027information incertaine dans le cadre d\u0027une extraction de connaissances à partir de texte. Le traitement repose sur les technologies du web sémantique pour permettre de faire le lien avec les données du Linked Open Data. Notre démarche consiste à identifier les différentes situations où une incertitude remettant en cause la validité de l\u0027information peut subsister. Nous proposons une ontologie pour modéliser l\u0027information incertaine et la représenter au format RDF. Nous travaillons actuellement sur développement d\u0027un ensemble de patterns pouvant faciliter l\u0027interrogation du graphe RDF prenant en compte notre représentation de l\u0027incertitude. Nous prévoyons par la suite de développer un raisonneur basé sur le formalisme des logiques possibilistes afin de permettre l\u0027inférence sur les données incertaines.\nSummary\nThe knowledge representation area needs some methods that allow to detect and handle uncertainty. Indeed, a lot of text hold information whose the veracity can be called into question. These information should be managed efficiently in order to represent the knowledge in an explicit way. As first step, we have identified the different forms of uncertainty during a knowledge extraction process, then we have introduce an RDF representation for these kind of knowledge based on an ontologie that we developped for this issue.\n"
  },
  {
    "id": "259",
    "text": "Introduction\nL\u0027utilisation d\u0027ontologies s\u0027est montrée très efficace dans bien des domaines. La taille et la dynamique des domaines considérés demande toutefois l\u0027exploitation combinée de plusieurs ontologies, d\u0027où la nécessité d\u0027établir des correspondances sémantiques, ou mappings (Euzenat et Shvaiko, 2007), entre ontologies. Ainsi, la qualité des résultats produits par les systèmes utilisant des ontologies dépend de la validité des mappings entre ontologies, ce qui oblige des experts du domaine à réviser leur définition lorsque les ontologies évoluent. Si cette maintenance peut être effectuée manuellement sur de petits ensembles de mappings, une approche plus automatique est nécessaire lorsqu\u0027ils sont volumineux, comme dans le domaine médical. L\u0027existence de mappings erronés est souvent dûe à l\u0027évolution des ontologies, les erreurs d\u0027alignement mises à part (Dos . Il est alors fondamental de comprendre l\u0027évolution des ontologies pour pouvoir agir sur les mappings afin de garantir leur validité. Ce faisant, nous avons proposé un ensemble de patrons de changement permettant de caractériser l\u0027évolution des concepts d\u0027une ontologie en analysant les changements dans la définition des concepts . Nous avons également observé sur des jeux de données réelles le comportement des mappings dans le temps, ce qui nous a permis d\u0027identifier un ensemble d\u0027actions d\u0027adaptation pouvant s\u0027appliquer aux mappings pour les faire évoluer (Dos . L\u0027objet de cet article formalise, sous forme d\u0027heuristiques, le lien entre les patrons de changement et les actions d\u0027adaptation pour faire évoluer les mappings lorsque les ontologies liées évoluent. Aprés avoir introduit les concepts de notre approche (Section 2), en particulier les patrons de changement et les actions d\u0027adaptation, nous présentons les heuristiques proposées (Section 3) et le cadre expérimental emprunté au domaine biomédical pour les évaluer (Section 4) et les discuter par rapport à l\u0027existant (Section 5) avant de conclure (Section 6).\nPréliminaires\nNous présentons ici les notions et notations utilisées pour définir notre approche. Soit O t , semT ype) relie deux concepts c s ? C X et c t ? C Y par la relation sémantique semT ype, telle que semT ype ? {?, \u003c, \u003e, ?} (cf. tableau 1). \nTAB. 1 -Notations pour la formalisation des heuristiques\nPatrons de changement caractérisant l\u0027évolution d\u0027ontologies\nNos travaux ont montré que l\u0027évolution des ontologies rend nécessaire d\u0027adapter les mappings. Nous pensons qu\u0027une compréhension précise de cette évolution va nous renseigner sur la façon d\u0027adapter les mappings au cours du temps. Pour caractériser l\u0027évolution des ontologies, nous avons proposé un ensemble de patrons de changement . Contrairement à ceux de la littérature (Djedidi et Aufaure, 2009), (Javed et al., 2013), (Gröner et al., 2010), nos patrons considèrent les changements syntaxiques et sémantiques au niveau des attributs des concepts. Ce choix a été motivé par les résultats expérimentaux obtenus montrant que la définition des mappings repose sur certaines valeurs d\u0027attributs (Dos .\nLes Une Copie Totale caractérise le changement à travers lequel une valeur d\u0027attribut devient également la valeur d\u0027un attribut d\u0027un autre concept. Par exemple, un attribut a 1 d\u0027un concept c 1 a pour valeur \"portal systemic encephalopathy\" au temps j. Au temps j + 1, a 1 a la même valeur, mais un attribut a 2 d\u0027un concept c 2 aura également cette valeur.\nUne Copie Partielle est une copie d\u0027une partie de la valeur d\u0027un attribut. Un attribut a 1 d\u0027un concept c 1 a la valeur \"familial hyperchylomicromenia\" au temps j. Au temps j + 1, a 1 garde cette valeur, mais un attribut a 2 aura \"familial chylomicromenia\" comme nouvelle valeur.\nUn Transfert Total correspond au transfert de la totalité de la valeur d\u0027un attribut à un autre attribut. Contrairement au cas TC, la valeur originale de l\u0027attribut n\u0027est pas conservée.\nUn Transfert Partiel définit le transfert d\u0027un partie de la valeur d\u0027un attribut. Par exemple, un attribut a 1 peut valoir \"eye swelling\" au temps j. Au temps j + 1, cette valeur sera supprimée partiellement de a 1 mais un attribut a 2 vaudra \"head swelling\" (i.e., le terme \"swelling\" est déplacé de a 1 vers a 2 entre j et j + 1).\nLes Patrons de changement sémantiques (SCP) s\u0027intéressent à l\u0027évolution de la séman-tique de la valeur des attributs au cours du temps. Nos observations ont montré qu\u0027à travers leurs évolutions successives, les concepts pouvaient devenir plus généraux, plus spécifiques ou rester équivalents, modifiant ainsi la relation sémantique des mappings. Les 4 SCP que nous proposons sont : Equivalent (EQV), Plus Spécifique (MSP), Moins Spécifique (LSP) et Recouvrement Partiel (PTM).\nEquivalent stipule que les changements syntaxiques au niveau de la valeur de l\u0027attribut ne modifient pas sa sémantique. Par exemple, la valeur d\u0027un attribut peut passer de \"Diabetes type 1\" à \"Diabetes type I\" sans en affecter le sens.\nPlus Spécifique identifie un changement rendant un concept plus spécifique que sa nouvelle version. Le changement menant de \"kappa light chain disease\" à \"kappa chain disease\" rend le premier plus spécifique du fait de la suppression du qualificatif \"light\". Moins Spécifique décrit l\u0027effet inverse.\nRecouvrement Partiel identifie un changement au niveau de la sémantique ne pouvant être caractérisé par les autres SCP. Considérons la valeur originale \"focal atelectasis\" et son évolu-tion \"helical atelectasis\". Ces deux valeurs font toutes deux référence à la notion de \"atelectasis\", mais ne peuvent être déclarées ni équivalentes, ni plus ou moins spécifiques. ModSemTypeM(m st , semT ype) consiste en la modification de semT ype (la relation sé-mantique) à cause des modifications sur les concepts sources (c s ) et/ou cibles (c t ).\nLes actions d\u0027adaptation de mappings\nNoAction(m st ) est appliquée lorsque les modifications sur les concepts sources et/ou cibles n\u0027entraînent pas de changements sur la sémantique des mappings.\nHeuristiques d\u0027adaptation des mappings\nNous présentons dans cette section des heuristiques indiquant sous quelles conditions adapter les mappings afin qu\u0027ils restent valides dans le temps. Ces heuristiques ont été établies expérimentalement à partir de l\u0027observation de l\u0027évolution des mappings entre des jeux de données réelles et de l\u0027analyse de l\u0027impact des patrons de changements sur leur évolution. Des heuristiques ont été définies pour chaque type d\u0027adaptation.\nHeuristiques pour les mappings de type Move et Derive\nSoit Cand l\u0027ensemble des concepts dits candidats regroupant les concepts du contexte de c s en j + 1 pour lesquels il existe un changement de type LCP entre un de leurs attributs et un attribut de c s . Soit la fonction topA(c s , c t , n) retournant les n attributs expliquant le mieux le mapping entre c s et c t . Soit la fonction SLCP (a 1 , a 2 ) retournant VRAI s\u0027il existe un changement de type LCP entre a 1 et a 2 et FAUX sinon. Soit la fonction SCP (a 1 , a 2 ) retournant les changements de type SCP entre a 1 et a 2 ou ? si aucun changement de type SCP n\u0027a été identifié.\nMoveM. Expérimentalement, nous avons observé que l\u0027adaptation d\u0027un mapping de type MoveM correspondait à l\u0027existence de changements de type LCP entre attributs. Plus précisé-ment, nous avons observé que lorsque l\u0027adaptation du mapping est de type MoveM, il n\u0027existe qu\u0027un seul attribut de c s au temps j expliquant le mapping avec un changement de type LCP avec un attribut du contexte de c s au temps j + 1. De ce fait, nous appliquons l\u0027adaptation de type MoveM lorsqu\u0027il n\u0027existe qu\u0027un seul concept candidat avec un changement de type LCP  Figure 1). Intuitivement, cela signifie que le mapping suit l\u0027évolution des attributs qui l\u0027expliquent.\nSoit les concepts \u0027128829008\u0027 \"Acute myeloid leukemia, 11q23 abnormalities (morphologic abnormality)\" et \u0027C6924\u0027 \"Acute Myeloid Leukemia with 11q23 MLL Abnormalities\" issus de SNOMED CT et de NCI. Deux changements de type LCP se sont produits lors de l\u0027évolu-tion de \u0027C6924\u0027. Un transfert total s\u0027est produit sur l\u0027attribut qui expliquait le mieux le mapping (\"Acute Myeloid Leukemia with 11q23 Abnormalities\"). Une copie totale a eu lieu pour un autre attribut qui expliquait aussi le mapping. Ces deux changements concernent le même concept candidat \u0027C82403\u0027 \"Acute Myeloid Leukemia with t 9 11 p22 q23 MLLT3-MLL\" dans la nouvelle version de l\u0027ontologie, mais deux attributs différents expliquant le mapping. Suite à l\u0027évolution de \u0027C6924\u0027, le mapping relie dorénavant les concepts \u0027128829008\u0027 et \u0027C82403\u0027.\nDeriveM. De façon similaire à l\u0027action MoveM, DeriveM est appliquée lorsque plusieurs changements entre attributs de type LCP sont reconnus suite à une évolution. Le mapping original est préservé et c s existe dans la nouvelle version de l\u0027ontologie. Le fait que plusieurs changements de type LCP concernent les attributs pertinents de c s et qu\u0027il existe donc plusieurs concepts candidats conduit à la création de nouveaux mappings entre ces candidats et c t (cf. Figure 2). Formellement, nous définissons cette heuristique de la façon suivante :\nEn guise d\u0027illustration, considérons le mapping \u0027plus spécifique que (\u003c) entre \u002741452004\u0027 -\"Uterus acollis (disorder)\" dans SNOMED CT et \u0027752.3\u0027 -\"Other anomalies of uterus\" dans ICD-9-CM. L\u0027analyse de l\u0027évolution de 752.3 permet d\u0027observer plusieurs changements de type LCP concernant des concepts candidats différents et également plusieurs changements de type SCP. Par example, il y a une copie totale (TC) de l\u0027attribut \"Other anomalies of uterus\" expliquant le mapping vers un attribut de \u0027752.33\u0027 qui fait apparaître une équivalence. En plus, l\u0027attribut \"Bicornuate uterus\" est totalement transféré (TT) dans \u0027752.34\u0027 faisant également apparaître une relation d\u0027équivalence. L\u0027action d\u0027adaptation à réaliser, selon notre heuristique, est DeriveM en l\u0027appliquant aux deux concepts candidats concernés par les changements de type LCP et en gardant la même relation sémantique que la relation du mapping original.\n3.2 Heuristiques associées à la modification de relation sémantique L\u0027application de l\u0027action ModSTR dépend des changements de types SCP trouvés entre attributs. Nous avons identifié deux scénarios différents. Le premier concerne la modification de la relation du mapping original m 0 st alors que c s ne change pas (cf. Equation 3). Le second scénario concerne la modification de la relation du mapping original suite à un MoveM ou DeriveM (cf. Equation 4). Dans le premier cas, la nouvelle relation sémantique lie le concept source (en terme de contenu) au temps j + 1 et le concept cible alors que dans le second cas, c s est remplacé par un concept candidat (un concept appartenant au contexte de c s ). Le type de la relation sémantique après évolution est obtenu en combinant le type de la relation du mapping original semT ype et le type d\u0027un changement de type SCP détecté entre attributs. Soit la fonction getSemType qui fournit la relation sémantique obtenue en combinant le type d\u0027un mapping original semType (x) avec des relations identifiées par des patrons de type SCP, SCPs (y). Par example, si la relation sémantique entre c \nVoici les heuristiques associées au premier et deuxième scénario :\nst ? {?, \u003c, \u003e, ?}, semT\nct ? {?, \u003c, \u003e, ?}, semT , nous sélectionnons les concepts candidats au temps j + 1 à partir du contexte de c s au temps j. Les expérimentations à partir desquelles cette heuristique a été proposée ont montré que la similarité entre les attributs expliquant un mapping et les attributs des concepts du contexte de c s après évolution était très faible lorsque le mapping était supprimé (Dos . Ceci a conduit à introduire une condition portant sur la similarité dans l\u0027heuristique proposée.\nNo action. L\u0027heuristique pour NoAction considère que des patrons de type LCP et SCP ne sont pas observables (cf. Equation 6). Elle s\u0027applique dans des cas où les changements portant sur un concept lié par un mapping n\u0027ont pas d\u0027effet sur les attributs expliquant ce mapping, ou lorsque la similarité avec de nouveaux attributs du contexte est faible (Dos .\nEvaluation\nCette section porte sur l\u0027évaluation des heuristiques proposées. Nous étudions si les changements identifés dans une ontologie conduisent à des adaptations correctes (i.e. MAAs) des mappings affectés. Cette évaluation a été réalisée sur plusieurs versions d\u0027ontologies biomédi-cales (SNOMED CT et ICD-9-CM) et plusieurs versions de mappings associés.\nProtocole d\u0027expérimentation\n4. Deux types de mesure, représentées respectivement par and sont proposées pour évaluer de manière rigoureuse les actions proposées (cf. Résultats obtenus dans le tableau 2). Cette distinction est bien appropriée à l\u0027évaluation des actions MoveM, DeriveM and ModSTR. Le symbole exprime la précision, le rappel et la F-mesure lorsque les actions proposées sont correctes par rapport aux actions attendues, en plus du concept candidat ou de la relation sémantique. Le symbole correspond aux cas où seul le type d\u0027action est correct, le concept candidat ou la relation sont erronés (par exemple, MoveM ou DeriveM est proposé mais pas avec le bon concept candidat à j+1). La mesure est plus contrainte, elle pourrait conduire à des valeurs plus faibles. Ainsi, si nous observons un MoveM avec un concept c obs du contexte et si notre mécanisme propose un MoveM avec le même concept c obs , nous choisissons de mesurer la précison, le rappel et la F-mesure de (action d\u0027adaptation, concept lié et relation sémantique exacts). En revanche, si seule l\u0027action MoveM est correcte, nous faisons le calcul correspondant à Nous procédons de la même façon pour ModSTR, en considérant en plus de l\u0027action d\u0027adaptation du mapping, le type de la relation sémantique.\n5. Enfin, nous effectuons un calcul global qui calcule la précision, le rappel et la F-mesure pour chaque jeu de données, indépendamment des types d\u0027action d\u0027adaptation.\nRésultats et discussion\nLes résultats de l\u0027adaptation des mappings basés sur nos heuristiques sont présentés dans la Mapping Move. Pour ce type d\u0027action, les résultats varient selon que l\u0027évolution concerne ICD-9-CM ou SNOMED CT uniquement pour la précision. Ainsi, nos heuristiques peuvent générer des adaptations plus ou moins correctes selon les jeux de données. Les différences de résultats entre et étant très faibles, on remarque que, la plupart du temps, lorsque notre système propose un MoveM, c\u0027est avec un bon concept candidat du contexte.\nMapping Derive. Les résultats obtenus sont plus élevés que ceux de Move pour SNOMED CT. L\u0027action DeriveM est complexe car l\u0027action propose plusieurs DeriveM par mapping. Aucune action DeriveM n\u0027a été observée pour ICD-9-CM. Là encore, les conditions d\u0027application des heuristiques sont différentes selon les jeux de données. L\u0027explication peut provenir du processus de maintenance, de matching ou de la granularité des ontologies alignées.\nModification de la relation sémantique. L\u0027évaluation de cette heuristique a été difficile car les mappings dont nous disposions contenaient peu de relations différentes (la plupart était des équivalences). Néanmoins, les résultats montrent que les nouvelles relations proposées quand une action de type ModSemTypeM est détectée, sont pertinentes (surtout pour SNOMED CT). Ce type d\u0027adaptation est généralement combiné avec un MoveM ou un DeriveM ce qui oblige à choisir la bonne action avant de changer la relation. Parfois, les changements dans l\u0027ontologie ne sont pas l\u0027unique raison pour changer la relation. Cela influe négativement sur nos résultats. Enfin, considérer la nouvelle version des mappings comme référence peut aussi poser problème. C\u0027est un ensemble de mappings qui a subi des évolutions mais ce n\u0027est pas à proprement parler un jeu de données de référence. Une relation sémantique proposée peut être considérée comme fausse par notre processus d\u0027évaluation alors qu\u0027elle peut être correcte d\u0027un point de vue sémantique. L\u0027intervention d\u0027un expert serait nécessaire pour y remédier.\nMapping Remove. La précision et le rappel sont relativement bons. Ils ne varient que légèrement selon l\u0027ontologie. La F-mesure minimale est de 0.54. La prise en compte du retrait de l\u0027attribut expliquant le mieux le mapping dans l\u0027heuristique semble être une bonne décision.\nApplication d\u0027aucune action. Ce type d\u0027adaptation couvre le plus grand nombre de cas que ce soit en valeur absolue (5892 attendus dans SNOMED CT et 3139 dans ICD-9-CM) ou relative. Les résultats montrent une grande efficacité des heuristiques. On constate que, bien que les concepts source ou cible évoluent, si les attributs expliquant les mappings qui les concernent restent inchangés, ces mappings n\u0027évolueront pas non plus. Les très bons ré-sultats obtenus avec l\u0027application de cette heuristique permettent aux experts de se concentrer sur l\u0027étude d\u0027une toute petite partie des mappings (en comparaison à l\u0027ensemble initial), ces mappings étant plus difficiles à adapter et pouvant nécessiter une intervention humaine.\nRésultats globaux. Nous avons analysé les résultats en combinant tous les types d\u0027adaptation pour avoir une idée de la qualité du processus général d\u0027adaptation. La F-mesure est élevée (un min de 0.85 dans ICD-9-CM). L\u0027action d\u0027adaptation NoAction influe beaucoup sur ces résultats. Néanmoins les résultats sont globalement acceptables et prometteurs même si certaines actions sont difficiles à appliquer. Ceci montre que les conditions d\u0027application des heuristiques que nous proposons sont adaptées. L\u0027amélioration des résultats liés à certaines actions nécessiterait de rechercher quels autres éléments influencent les types d\u0027adaptation et d\u0027étudier comment les prendre en compte dans les heuristiques.\nEtat de l\u0027art\nNous distinguons trois principaux types d\u0027approches pour la maintenance des mappings due à l\u0027évolution d\u0027ontologies. La première consiste à identifier et réparer les mappings invalides (Meilicke et al., 2008)  (Ivanova et Lambrix, 2013). Ces approches effectuent des raisonnements logiques pour identifier les mappings produisant des incohérences logiques. Ce mécanisme ne s\u0027applique que sur des ontologies formelles, qui n\u0027existent pas toujours.\nLes travaux s\u0027inscrivant dans la deuxième catégorie reposent sur des techniques de ré-alignement total ou partiel d\u0027ontologies. Si le premier type d\u0027approche ne considère aucune information provenant de l\u0027évolution, le second recoupe l\u0027ensemble des concepts modifiés au niveau des ontologies avec ceux impliqués dans des mappings pour ne réaligner que ce sousensemble de concepts (Khattak et al., 2012). Ces approches sont coûteuses en temps de calcul et de validation lorsque les ontologies sont volumineuses (Shvaiko et Euzenat, 2013).\nLa troisième catégorie, inspirée du monde des bases de données (Velegrakis et al., 2003), fait référence à des approches qui profitent au maximum des informations provenant de l\u0027évo-lution d\u0027ontologies pour éviter de les réaligner totalement. Tang et Tang (2010) ont proposé une méthode visant à trouver l\u0027impact minimal de la propagation des changements au niveau d\u0027une ontologie. Martins et Silva (2009) suggèrent d\u0027adapter les mappings en procédant de la même façon que lors de la modification des concepts de l\u0027ontologie qui évolue. Cependant, dans leur approche, les mappings ne sont adaptés que lorsque les concepts sont supprimés. L\u0027originalité de nos travaux, par rapport à cet état de l\u0027art, réside dans : (i) l\u0027importance donnée aux modifications des ontologies sous-jacentes, (ii) à leur caractérisation et (iii) à la prise en compte du changement au niveau de la relation sémantique des mappings, ce dernier point étant souvent négligé dans les approches existantes qui ne considèrent qu\u0027un seul type de relation (Dos Reis et al., 2012).\nConclusion\nDans cet article, nous avons proposé un ensemble d\u0027heuristiques guidant l\u0027adaptation des mappings entre ontologies. Ces dernières formalisent le lien existant entre les changements identifiés au niveau des éléments ontologiques et les actions d\u0027adaptation à appliquer sur les composants des mappings pour préserver leur validité. L\u0027approche décrite a été validée expé-rimentalement sur des données réelles du monde biomédical. Le manque de données de ré-férence nécessite l\u0027implication d\u0027experts du domaine mais le travail réalisé permet de réduire considérablement le temps nécessaire aux experts pour valider les adaptations proposées.\n"
  },
  {
    "id": "260",
    "text": "Introduction\nQui n\u0027a pas dit, un jour, en écoutant la radio : \"Mais ça ressemble à Supertramp ou à une musique de Chopin ou à une musique baroque\" ? Sur la base d\u0027un court morceau écouté on peut en effet identifier directement l\u0027auteur ou le placer dans une catégorie même si on ne connait pas forcément le morceau. Si c\u0027est un chanteur, on le reconnait facilement au timbre de sa voix, pour un morceau de musique classique, l\u0027interprétation peut varier et on détecte plutôt la ligne musicale. Pour les documents textuels, ce problème d\u0027authentification d\u0027auteur présumé est récurrent, et la fouille de texte peut s\u0027avérer très utile. Ainsi, par exemple pour authentifier une élégie de Shakespeare en 1995 1 des techniques telles que le comptage exclusif des mots et la prise en compte de mots rares ont été employés avec succès (Foster (1996)). Le champ littéraire n\u0027est cependant pas le seul concerné. Le problème d\u0027authentification d\u0027un auteur apparait aussi dans bien d\u0027autres applications, comme dans le domaine juridique par exemple pour l\u0027authentification d\u0027un testament ou dans le cadre des investigations anticriminelles ou antiterroristes pour identifier la provenance d\u0027une demande de rançon ou de posts émis sur des forums de discussion du Dark Web (Abbasi et Chen (2005)). Le marketing peut également être intéressé par le profiling des auteurs des blogs ou des commentaires sur le Web.\nDans le cas de textes écrits, on peut plus généralement distinguer trois variétés de problèmes liés à la détermination d\u0027un auteur inconnu :\n-l\u0027extraction de profil (Author Profiling) : il s\u0027agit d\u0027indiquer à partir d\u0027un texte des éléments du profil de son auteur comme, par exemple, la tranche d\u0027âge et le genre ( Rangel et al. (2013)) ou l\u0027appartenance à une catégorie particulière comme celle des criminels potentiels ) -la reconnaissance de l\u0027auteur (Author Verification ou Author Recognition) : il s\u0027agit de vérifier parmi une liste des auteurs possibles lequel est le bon -l\u0027identification d\u0027un auteur : il s\u0027agit de décider si un texte donné a été écrit par l\u0027auteur d\u0027un autre groupe de documents Ainsi, dans ces trois problèmes, on s\u0027interroge sur l\u0027auteur d\u0027un document écrit, et dans ces trois cas pour répondre, il est nécessaire de représenter de façon appropriée le document à explorer et de pouvoir le comparer avec d\u0027autres. Toutefois, nous pensons qu\u0027il est illusoire de rechercher une empreinte d\u0027un auteur sur un texte qu\u0027on pourrait comparer avec des empreintes extraites d\u0027autres textes et qui serait unique au même titre qu\u0027une empreinte digitale. Nous pensons qu\u0027il faut utiliser divers espaces de représentation pour les textes à analyser selon la langue d\u0027origine ou encore le genre ou la qualité du document. Dans cet article, où nous nous intéressons plus spécifiquement à des problèmes d\u0027identification d\u0027auteurs à partir de documents rédigés dans diverses langues et de différents types (textes littéraires courts ou longs, articles de presse ou publications, blogs) nous avons exploré différents modes de représentation des documents. Nous avons ensuite proposé de formaliser l\u0027identification d\u0027auteurs comme un problème de classement que nous avons résolu de trois façons : à l\u0027aide d\u0027un algorithme original de comptage de similarité (DCM), puis avec deux autres méthodes qui exploitent ce comptage, par une technique de vote (DCM-voting), par apprentissage automatique (DCM-classifier).\nNotre article est organisé de la manière suivante : après la section 2 consacrée aux travaux relatifs à l\u0027identification d\u0027auteurs, nous définissons plus formellement le problème dans la section 3, puis nous décrivons les trois méthodes proposées pour le résoudre dans la section 4. La section 5 présentera les résultats des expériences réalisées afin d\u0027évaluer l\u0027intérêt de ces approches et de les comparer à celles de l\u0027état de l\u0027art. Des conclusions seront présentées dans la dernière section.\nEtat de l\u0027art\nL\u0027identification d\u0027auteur peut être définie comme un problème de classification de textes : Etant donné un ensemble, grand ou réduit à un seul élément, de documents d\u0027un même auteur, il faut déterminer si un nouveau document a été écrit par le même auteur que les autres \".\nIl s\u0027agit donc d\u0027un problème de classement supervisé binaire dont la réponse attendue est binaire (\"oui\" ou \"non\") ou une probabilité d\u0027appartenance à l\u0027ensemble de documents fournis. Toutefois, une des spécificités de ce problème de classement est que seuls des exemples d\u0027une des deux classes sont donnés : les documents rédigés par l\u0027auteur, mais la seconde classe n\u0027est pas explicitée. De plus, parfois le nombre d\u0027exemples positifs est réduit à un seul document, ce qui rend la tâche particulièrement difficile.\nPour pallier l\u0027absence d\u0027exemples négatifs, on peut essayer d\u0027en produire. C\u0027est la voie explorée par différents auteurs parmi lesquels figurent (Seidman (2013)) qui construisent une classe d\u0027\"imposteurs\" choisis aléatoirement sur la base des dix mots les plus fréquents figurant dans les documents disponibles pour remplir la classe du \"non\". D\u0027autres auteurs, comme Zhang et al. (2014) et Halvani et al. (2013), transforment ce problème de classification à deux classes en un problème avec plusieurs classes, soit en rajoutant des classes extérieures, soit en transformant la classes initiale en plusieurs. Les même auteurs (Halvani et al. (2013)) augmentent la taille de la classe des documents connus quand celle-ci est réduite à un seul document. Ainsi, ces approches permettent de revenir à un problème classique de classement supervisé mais, lors de la construction des exemples négatifs, elles sont confrontées au risque de choisir des documents trop proches ou trop éloignés des documents déjà fournis.\nOutre la question des données disponibles pour résoudre le problème, l\u0027identification d\u0027auteurs est ensuite confrontée à deux autres questions classiques en fouille de texte : comment représenter les documents et, une fois l\u0027espace de représentation choisi, quelles méthodes appliquer pour résoudre le problème de classement ?\nComme nous l\u0027avons déjà remarqué, l\u0027identification d\u0027auteurs peut être réalisée à partir de documents très différents : méls (de Vel et al. (2001); Chaurasia et Kumar (2010)), programmes, parties des oeuvres littéraires ou parties des documents de la vie de tous les jours, texte plat (Zhang et al. (2014)), extraits de chat (Inches et Crestani (2013)) ou séquences de commandes Unix (Szymanski et Zhang (2004)). Le choix des caractéristiques examinées, on parle des caractéristiques stylométriques, dépend du type de document, parfois de la langue et aussi de la qualité du texte initial. Les caractéristiques dites \"spécifiques\" aux applications portent plutôt sur le comptage des tabulations et autres séparateurs ou l\u0027analyse des caracté-ristiques spécifiques, telles que la position des parenthèses et des crochets fermants pour les programmes, et des lignes vides pour les méls. Les caractéristiques sémantiques sont prises en compte plutôt pour des textes issus du web (forum et chat), comme, par exemple, l\u0027usage des abréviations ou des mots démonstratifs fréquents (\"well\") ou des transcriptions concentrées des expressions orales (\"sse u\"). On peut également considérer des caractéristiques syntaxiques comme des fautes d\u0027orthographe ou les abréviations. Si on se place dans un cadre générique (authentification d\u0027auteur dans diverses langues et dans divers genres), on utilise plutôt des caractéristiques de type caractère ou mot, ou suites de caractères ou de mots (n-grams) (Chaurasia et Kumar (2010); Szymanski et Zhang (2004)). On peut aussi avoir recours à un étiqueteur lexical et syntaxique mais son usage augmente considérablement le temps de traitement (Juola et Stamatos (2013)) et les résultats vont dépendre de sa qualité (Vilariño et al. (2013)). Lorsque le choix de ces caractéristiques est fait, les documents peuvent être transformés en vecteurs en utilisant, le plus souvent, tf-idf comme pondération ou uniquement la fréquence. Ensuite, selon la représentation du texte adoptée, on peut comparer les documents à l\u0027aide de fonctions \"classiques\" de similarité telles que le cosinus, la corrélation, moins souvent des mesures de compression de données comme la Fast Compression Distance (Cerra et al. (2014)) ou la Common N-Gram dissimilarity (Layton et al. (2013)).\nPour ce qui concerne la résolution du problème de classement lui-même, on peut appliquer des méthodes \"classiques\" telles que les k plus proches voisins (k-NN) (Zhang et al. (2014); Ghaeini (2013); Halvani et al. (2013)) ou les SVM (Vilariño et al. (2013)). Certains auteurs (Dam (2013); Layton et al. (2013) ;Jankowska et Milios (2013)) proposent des méthodes basées sur le choix d\u0027un seuil ou d\u0027un vote et des formules de calcul de l\u0027éloignement entre le document d\u0027auteur inconnu et les autres. Les différences entre ces approches résident dans la phase de prétraitement, dans l\u0027extraction des caractéristiques et dans le choix du seuil et de la fonction de dissimilarité.\nPar rapport aux travaux antérieurs, notre contribution se situe dans un cadre plus large avec l\u0027objectif de proposer une méthodologie générique, applicable à des collections très différentes tant par le genre des documents que par le langage. Ceci nécessite la mise en place d\u0027une approche permettant de choisir automatiquement la représentation textuelle la mieux adaptée pour un corpus donné.\nDéfinition du problème et représentation des documents\nLe problème d\u0027identification d\u0027auteur peut être défini de la façon suivante. Etant donné un corpus composé de documents d\u0027un même type (mel, blog, roman, code, etc.) écrits dans un même langage (anglais, français, Java, etc.) on dispose pour chaque problème p d\u0027un ou plusieurs documents A p du corpus qui ont été rédigés par un même auteur et d\u0027un document u p dont l\u0027auteur est inconnu. L\u0027objectif est de déterminer si u p a été écrit ou non par le même auteur que les documents de A p . Si on dispose d\u0027un échantillon d\u0027apprentissage, autrement dit d\u0027un ensemble de problèmes P tel que pour chaque problème p ? P on sait si le document inconnu u p a été rédigé ou non par le même auteur que les documents associés A p , alors on peut formaliser le problème comme un problème de classement supervisé binaire et le résoudre à l\u0027aide de méthodes d\u0027apprentissage automatique. La difficulté consiste alors à déterminer d\u0027une part un ou des espaces de représentation des documents appropriés et d\u0027autre part à construire à partir de ces représentations des facteurs descriptifs des documents inconnus permettant de prédire efficacement si chaque document u p a été ou non produit par le même auteur que les documents de A p qui lui sont associés. Parmi les modèles les plus connus et les plus utilisés pour représenter des documents figure le modèle tf-idf introduit par Salton et al. (1975). Un document d est représenté par un vecteur (w 1 , . . . , w j , . . . , w |T | ) tel que le poids w j du terme t j dans d correspond au produit de la fréquence tf j du terme t j dans d par le pouvoir discriminant idf (j) de t j . Ce modèle est très efficace notamment pour identifier des termes (caractères, mots ou séquences de mots ou caractères correspondant à des n-grams) qui sont fréquents dans un document et rares dans les autres. Mais, comme nous l\u0027avons souligné en introduction, d\u0027autres caractéristiques peuvent être prises en compte pour représenter les documents. De plus, nous pensons qu\u0027il n\u0027existe pas un modèle de représentation universel adapté à tous les documents mais que le choix de cet espace de représentation doit dépendre du type de documents et du langage.\nCeci nous a conduit à considérer d\u0027autres espaces de représentation indiqués dans le tableau 1. Outre le modèle tf-idf défini à partir des mots, avec élimination des mots outils à l\u0027aide d\u0027un dictionnaire (R5) ou en considérant leur fréquence (R4), des suites de mots ou de caractères (R1, R2, R3), nous avons introduit trois autres modèles de représentation (R6, R7 et R8) visant à caractériser le style d\u0027écriture du document. Dans le modèle R6, la moyenne et l\u0027écart type du nombre de mots par phrase sont associés au document. Le modèle R7 attribue à chaque document une mesure de diversité du vocabulaire définie comme le nombre de mots différents employés divisé par le nombre total d\u0027occurrences de mots (i.e. la longueur du document). Le modèle R8 correspond au modèle de Salton dans lequel on considère les caractères de ponctuation au lieu des termes (mot ou caractère n-grams). Enfin, le modèle R678 correspond à la concaténation des trois modèles précédents : chaque document est représenté par un vecteur indiquant la moyenne par phrase des caractères de ponctuation \" :\" , \" ;\" , \",\", la moyenne et l\u0027écart type du nombre de mots par phrase et la diversité du vocabulaire. Ponctuation nombre moyen de signe de ponctuation par phrase caractères pris en compte : \",\" \" ;\" \" :\" \"(\" \")\" \" !\" \" ?\" R678 Concaténation R6 + R7 + R8 TAB. 1: La liste de espaces de représentation considérés\nDCM, DCM-voting et DCM-classifier\nAyant choisi un des espaces de représentation, on peut comparer les documents deux à deux à l\u0027aide de mesures de similarité comme le cosinus et le coefficient de corrélation ou avec la distance euclidienne et appliquer une des trois méthodes (DCM, DCM-voting, DCMClassifier) que nous avons proposées pour résoudre le problème d\u0027identification d\u0027auteur. La première méthode DCM permet de traiter directement le problème d\u0027identification p ? P , en considérant uniquement les similarités entre les vecteurs décrivant les documents suivant un des espaces. Les deux autres méthodes, basées sur DCM, permettent de combiner diffé-rentes représentations des documents, par une méthode de vote dans le cas de DCM-voting, à l\u0027aide d\u0027une méthode d\u0027apprentissage supervisée nécessitant la construction d\u0027attributs prédic-tifs pour DCM-classifier. Ces différentes méthodes sont décrites dans les sections suivantes.\nMéthodes de comptage de similarités : DCM et DCM-voting\nEtant donné un problème p ? P défini par un ensemble A p de documents rédigés par un même auteur et un document u p dont l\u0027auteur est inconnu, représentés dans un même espace, et un seuil de décision ? , la méthode DCM, décrite par l\u0027algorithme suivant, fournit en sortie la valeur True si l\u0027auteur de u p est le même que celui des documents de A p ou la valeur False dans le cas contraire. Cette méthode exploite les similarités (ou distances) entre tous les documents disponibles. Elle consiste à assigner le document u p au même auteur que les documents de A p si la plupart d\u0027entre eux sont plus proches de u p qu\u0027ils ne le sont des autres documents de A p . Plus précisément la plus grande similarité de chaque document d x ? A p aux autres documents de A p est calculée puis comparée à la similarité de d x à u p . Si la première est inférieure à la seconde, un compteur est incrémenté. Après examen de tous les documents de A p (fin de la boucle for extérieure), il comptabilise la proportion de documents de A p qui sont les plus proches de u p que des autres de A p et si cette proportion est supérieure au seuil fixé ?, alors l\u0027auteur du document inconnu u p est le même que celui des autres documents. Cette méthode présente l\u0027avantage d\u0027être simple et rapide à mettre en oeuvre et elle permet de traiter un problème d\u0027identification p ? P indépendamment des autres. En revanche, elle n\u0027exploite qu\u0027un seul mode de représentation des documents.\nPour pallier ce défaut, on peut avoir recours à une méthode de vote DCM-voting consistant simplement à appliquer la méthode DCM en considérant plusieurs espaces de représentation des documents, de préférence en nombre impair, puis à affecter le document inconnu à la classe majoritairement retournée par les différentes exécutions. Cependant, comme nous l\u0027avons souligné précédemment, tous les espaces de représentation ne sont pas équivalents et il serait souhaitable de pouvoir ajuster leur poids dans la décision finale ; ce qui est difficile à faire en pratique même pour un expert ; de même que le choix du seuil ?. Pour toutes ces raisons, nous proposons une autre méthode plus générale permettant d\u0027exploiter simultanément plusieurs modes de représentation des documents et d\u0027ajuster automatiquement, par apprentissage automatique, leur importance dans l\u0027identification de la classe des documents inconnus.\nLa méthode DCM-classifier\nDans le cadre de l\u0027apprentissage supervisé, on suppose que pour un sous-ensemble P A de problèmes de P , on sait en fait si les documents inconnus u p ont ou non été produits par le même auteur que les documents qui lui sont associés i.e. on dispose en plus de la classe class(u p ) des documents inconnus à savoir même auteur ou auteur différent. Ce sousensemble P A est décomposé en un échantillon d\u0027apprentissage P a utilisé pour construire un modèle de décision et un échantillon test employé pour l\u0027évaluer. La phase d\u0027apprentissage permet de mettre en relation des facteurs descriptifs (ou attributs) des documents avec leur classe de façon à pouvoir ensuite identifier l\u0027auteur d\u0027un nouveau document dont la classe est inconnue uniquement à partir de ces facteurs descriptifs. Il est clair que la qualité du modèle dépend largement du pouvoir prédictif de ces facteurs que nous proposons de définir de la façon suivante.\nPour chaque espace de représentation R v , v ? {1, .., V }, chaque document u p est décrit par deux attributs count v (u p ) et mean v (u p ) respectivement définis à l\u0027aide d\u0027une mesure de similarité s par :\nUn dernier attribut T OT count (u p ), basé sur tous les espaces de représentation est égale-ment calculé afin d\u0027avoir une description plus synthétique. Il est défini par :\nAinsi lors de l\u0027apprentissage, on considère les documents u p de chaque problème p de P a décrit par ces attributs descriptifs prédictifs et par leur classe réelle (\nCompte tenu du caractère numérique de ces attributs descriptifs, plusieurs méthodes d\u0027apprentissage supervisé peuvent alors être employées ( SVM, etc). Dans le cadre des expérimentations, nous avons privilégié les arbres de décision qui ont l\u0027avantage d\u0027intégrer une phase de sélection des attributs en fonction de leur pouvoir prédictif ; ce qui permet de favoriser selon la famille de problèmes considérés tel ou tel espace de représentation. De plus, ils permettent aussi d\u0027ajuster automatiquement les paramètres du modèle. Les résultats obtenus sur chaque corpus des collections ev2013, app2014 et ev2014 ont été évalués à l\u0027aide des indicateurs habituels de précision, de rappel et avec la mesure F 1.\nExpérimentations et résultats\nLe taux d\u0027erreur indiqué par la mesure F 1 étant très synthétique, pour comparer les mé-thodes, on a également utilisé l\u0027indicateur de performance AU C qui mesure l\u0027aire de la courbe ROC (Davis et Goadrich (2006)).\nPour la collection ev2014, seuls les indicateurs de performances calculés par la plateforme du challenge pour chaque corpus sont disponibles : AU C, l\u0027indicateur c@1, le produit des deux indicateurs et le temps d\u0027exécution. L\u0027indicateur c@1 permet de donner plus d\u0027importance à une réponse correcte par rapport à l\u0027absence de décision (i.e. une probabilité d\u0027appartenance à la classe de 0.5). Cet indicateur est défini par :\noù n est la taille du corpus, n c est le nombre de réponses correctes, n u le nombre de problèmes laissés sans décision.\nRésultats sur la collection 2013\nPour la méthode DCM, nous avons utilisé la représentation R1 (caractère 8-grams) qui avait donné les meilleurs résultats sur la collection d\u0027apprentissage de 2013 et fixé ? à |Ap| 2 alors que pour DCM-voting, nous avons privilégié les espaces de représentation R1, R2, R3, R4 et R678 (cf. Tableau 1). Pour les quatre espaces ayant trait aux mots ou aux n-grams caractères, les documents sont représentés sous format vectoriel avec la pondération tf ? idf .\nLa table 3 présente les résultats produits par les trois méthodes sur la collection eval13 ainsi que les résultats des gagnants de la compétition par corpus puis sur l\u0027ensemble de la collection. La faible précision de DCM-classifier pour le corpus espagnol peut s\u0027expliquer par le manque de problèmes pour cette langue (seulement quatre) dans le corpus d\u0027apprentissage rendant difficile la construction d\u0027un modèle performant. Les trois méthodes produisent des résultats satisfaisants cependant, DCM et DCM-voting sont limités aux problèmes contenant au moins deux textes connus. Si on compare les résultats obtenus par nos méthodes avec ceux des gagnants de la compétition par corpus, alors il n\u0027y a que sur le corpus grec que la méthode DCM-classifier l\u0027emporte avec un score de 85%. Par contre, sur l\u0027ensemble de la collection, DCM-voting comme DCM-classifier obtiennent des résultats meilleurs ou équivalents à ceux du gagnant pour tous les critères d\u0027évaluation (F1, précision et rappel) . TAB. 4: Résultats de la 10-cross validation de DCM-classifier sur app2014\nRésultats sur les collections 2014\nLa collection 2014 contient un nombre plus élevé de problèmes et de types de document que la collection de 2013 et elle s\u0027avère plus difficile à traiter puisque plus de la moitié des problèmes ont un seul document connu (|A| \u003c 2) ; ce qui rend les méthodes DCM et DCMvoting inadaptées et inefficaces. Pour cette raison, seule DCM-classifier a été évaluée, d\u0027abord sur la collection d\u0027apprentissage en utilisant la technique de 10-validation croisée qui consiste à séparer le corpus à traiter en deux groupes, un pour entrainer le modèle et l\u0027autre pour le tester, puis sur la collection d\u0027évaluation dans le cadre du challenge.\nLes résultats obtenus par validation croisée sur l\u0027ensemble d\u0027apprentissage sont présentés dans la table 4. Ils confirment les performances de la méthode DCM-classifier.\nLe tableau 5 contient les résultats officiels obtenus lors de la compétition PAN14 in Author Identification, Stamatatos et al. (2014). Ils permettent de comparer notre méthode à celles des autres participants. DCM-classifier nous a permis d\u0027être classé en deuxième position lors de la compétition. Elle fournit de bons résultats en un temps relativement court. Il convient de noter que les temps de traitement affichés par le gagnant de la compétition sont en moyenne supé-rieurs à trois heures alors que ceux de DCM-classifier sont de l\u0027ordre de quelques secondes.\nUn des avantages de la méthode DCM-classifier, basée sur les arbres de décision, est de mettre en évidence les caractéristiques qui permettent le mieux d\u0027identifier l\u0027auteur d\u0027un do- 1/6 1% GR T OTcount 1/6 8% SP TAB. 6: Classement des attributs cument selon le type de corpus considéré. En effet, les documents sont décrits par des attributs calculés sur différents espaces de représentation mais l\u0027apprentissage intègre une phase de sé-lection de ceux qui sont les plus discriminants. Ainsi, on peut en déduire pour chaque corpus l\u0027importance de chaque attribut.\nLes figures 1 et 2 présentent les différents espaces de représentation utilisés pour deux corpus de langues différentes, on voit que ces espaces sont très différents et que les poids rattachés le sont aussi. La table 6 indique de manière synthétique la liste des attributs les plus utilisés sur l\u0027ensemble des corpus de la collection d\u0027évaluation 2014. Ce résultat confirme l\u0027intérêt de combiner plusieurs espaces de représentation pour résoudre le problème d\u0027identification d\u0027auteurs.\n"
  },
  {
    "id": "261",
    "text": "Introduction\nLes systèmes de recommandation (SR), visent à recommander à des utilisateurs des ressources pertinentes pour eux. Le filtrage collaboratif (FC) (Resnick et al., 1994) est une des approches les plus populaires de la recommandation.\nBien que la qualité des recommandations fournies par le FC soit considérée comme satisfaisante en moyenne (Castagnos et al., 2013), certains utilisateurs ne reçoivent pas de recommandations de qualité. Le manque de données sur ces utilisateurs est une des raisons possibles expliquant cette mauvaise qualité. Ce problème est appelé démarrage à froid (Schein et al., 2001). Parmi les autres raisons évoquées dans l\u0027état de l\u0027art se trouve la trop grande différence des préférences de ces utilisateurs, par rapport à celles des autres (Haydar et al., 2012). C\u0027est sur cette raison que nous nous focalisons dans cet article. En effet, le filtrage collaboratif suppose une cohérence entre les préférences des utilisateurs ; ces utilisateurs ne respectant pas ce critère, il semble normal qu\u0027ils se voient proposer des recommandations de mauvaise qualité. Ces utilisateurs peuvent aussi être considérés comme des données aberrantes, ou outliers. Nous choisissons de les appeler des utilisateurs atypiques.\nNotre objectif ici est d\u0027identifier ces utilisateurs atypiques. Nous proposons, dans ce travail préliminaire, plusieurs mesures permettant de les identifier, en exploitant uniquement leurs préférences.\nLa section 2 se focalise sur les systèmes de recommandation et l\u0027atypisme. Dans la section 3, nous proposons des mesures d\u0027identification des utilisateurs atypiques. Ensuite, nous pré-sentons les expérimentations menées pour valider ces mesures et nous concluons notre travail.\nÉtat de l\u0027art\nLa recommandation sociale, ou filtrage collaboratif (FC), exploite les préférences d\u0027utilisateurs (en général des notes sur des ressources) pour estimer des préférences inconnues. L\u0027approche à base de mémoire, et notamment les k plus proches voisins (knn), exploite les similarités de préférences entre utilisateurs. Bien que simple à mettre en oeuvre, intégrant dynamiquement les nouvelles préférences et fournissant des recommandations de qualité, cette approche ne passe pas à l\u0027échelle. L\u0027approche à base de modèle souffre moins du problème de passage à l\u0027échelle. La technique de factorisation de matrices (Hu et al., 2008), la plus répan-due, forme un sous-espace de caractéristiques latentes, dans lequel utilisateurs et ressources sont représentés, qui permet d\u0027estimer les préférences inconnues.\nDans la littérature, les utilisateurs que nous appelons atypiques sont nommés déviants, anormaux, etc. (Del Prete et Capra, 2010) et la définition qui en est faite varie légèrement. Les travaux dédiés à leur identification sont peu nombreux. La mesure d\u0027anormalité (Del Prete et Capra, 2010;Haydar et al., 2012) aussi appelée coefficient de déviance, déviance, etc., est la plus utilisée pour les identifier. Elle représente la propension qu\u0027a un utilisateur à noter différemment des autres. Elle exploite l\u0027écart entre les notes d\u0027un utilisateur sur des ressources et la note moyenne sur ces ressources (équation (1)).\noù n u,r est la note que l\u0027utilisateur u a donné à la ressource r, n r la note moyenne sur r, R u l\u0027ensemble des ressources notées par u et u leur nombre. Les utilisateurs dont l\u0027anormalité est très élevée sont considérés comme atypiques. Bien que peu complexe, cette mesure ne tient pas compte du comportement propre à chaque utilisateur et les ressources sur lesquelles les utilisateurs ne sont pas unanimes vont injustement augmenter l\u0027anormalité. (Bellogín et al., 2011) définit un indicateur de clarté qui identifie les utilisateurs ambigus (instables) dans leur notation, basé sur la mesure de l\u0027entropie. Il a l\u0027inconvénient d\u0027identifier comme instables des utilisateurs dont les préférences évoluent ou dont les préférences diffèrent en fonction des domaines des ressources. Cette mesure ne nous paraît pas adéquate car l\u0027approche sociale peut leur proposer des recommandations de bonne qualité. (Bellogín et al., 2011;Haydar et al., 2012;Griffith et al., 2012) identifient un lien entre l\u0027erreur commise sur chaque utilisateur et ses caractéristiques (nombre de notes, de voisins, etc.). (Haydar et al., 2012), forme des clusters d\u0027utilisateurs et identifie un cluster d\u0027atypiques : des utilisateurs avec une forte erreur et un fort indice d\u0027anormalité. Cependant, nous pensons que les atypiques ne sont pas toujours similaires entre eux (ce qui en ferait d\u0027ailleurs des utilisateurs non atypiques au sens de la recommandation sociale), le clustering échouera probablement à former des clusters d\u0027atypiques. C\u0027est dans ce sens que va le travail présenté dans (Ghazanfar et Prugel-Bennett, 2011), qui clusterise des utilisateurs et propose de considérer comme atypiques les utilisateurs qui ne sont proches du centre d\u0027aucun des clusters formés.\nL\u0027identification d\u0027utilisateurs atypiques peut être associée à l\u0027identification de données aberrantes (outliers) : un outlier est une donnée qui dévie tellement des autres données que cela laisse penser qu\u0027elle a été générée par un mécanisme différent. Les méthodes statistiques et le clustering sont également très utilisées dans le domaine de la détection d\u0027outliers (Aggarwal, 2013).\nNouvelles mesures d\u0027identification d\u0027utilisateurs atypiques\nPartant des travaux de la littérature, nous proposons de nouvelles mesures permettant l\u0027identification d\u0027utilisateurs atypiques.\nCorrKMax -Nous pensons que l\u0027approche knn échoue sur les utilisateurs n\u0027ayant pas suffisamment d\u0027utilisateurs similaires. CorrKM ax représente la similarité moyenne qu\u0027a un utilisateur u avec ses k utilisateurs les plus similaires (équation (2)).\noù CorrP earson(u, v) est la corrélation de Pearson entre u et v. V (u) représente les k utilisateurs les plus similaires à u. Nous pensons que les utilisateurs associés à une faible valeur de CorrKM ax(u) recevront des recommandations de mauvaise qualité.\nAnormalité CR (Anormalité avec Controverse sur les Ressources) -Cette mesure repose sur la mesure d\u0027anormalité de l\u0027état de l\u0027art. Elle suppose que l\u0027écart sur une ressource controversée n\u0027a pas le même sens qu\u0027un même écart sur une ressource consensuelle, ce qui n\u0027est pas considéré par la mesure d\u0027anormalité de l\u0027état de l\u0027art. Nous proposons de pondérer les notes par le degré de controverse de la ressource, fonction de l\u0027écart-type de ses notes. L\u0027Anormalite CR d\u0027un utilisateur u est présentée dans l\u0027équation (3).\noù contr(r) est la controverse associée à une ressource r. Cet indice est basé sur l\u0027écart-type normalisé des notes sur r. Où contr(r) \u003d 1 ? ?r??min ?max??min , avec ? r est l\u0027écart-type des notes de r. ? min et ? max sont respectivement le plus petit et le plus grand écart-type de notes possibles parmi les ressources.\nLe calcul d\u0027Anormalité CR est d\u0027une complexité comparable à celle de l\u0027anormalité de l\u0027état de l\u0027art. Elle peut donc être calculée fréquemment et ainsi prendre en compte les nouvelles préférences des utilisateurs.\nAnormalite CRU (Anormalité avec Controverse sur les Ressources et profil Utilisateur) -Ni Anormalité ni Anormalité CR ne tiennent compte du comportement général de l\u0027utilisateur auquel elles s\u0027appliquent. Un utilisateur sévère peut être identifié comme atypique alors qu\u0027il n\u0027est atypique que dans sa manière de noter, et non dans ses préférences et qu\u0027il recevra probablement des recommandations de qualité. Pour éviter ce biais, nous proposons de centrer les notes de chaque utilisateur par rapport à sa moyenne de notes. L\u0027Anormalité d\u0027un utilisateur u, notée Anormalité CRU (u) est calculée selon l\u0027équation (4) :\noù n Cr représente la moyenne des notes centrées des utilisateurs sur r, contr C (r) est l\u0027indice de controverse associé à r, calculé à partir de l\u0027écart-type des notes sur la ressource, centrées par rapport aux utilisateurs. Le calcul de Anormalité CRU (u) est certes plus coûteux en temps que Anormalité CR (u), mais devrait permettre une identification plus précise des utilisateurs atypiques. Notons que ces deux dernières mesures sont indépendantes de l\u0027approche de recommandation utilisées (knn ou factorisation de matrices), à l\u0027opposé de la mesure CorrKM ax.\nExpérimentations\nDans cette section, nous évaluons les mesures d\u0027identification des utilisateurs atypiques que nous proposons, en comparaison avec celles de l\u0027état de l\u0027art.\nNous utilisons le corpus de données de l\u0027état de l\u0027art MovieLens, composé de 100 000 notes (de 1 à 5) de 943 utilisateurs sur 1 682 films (ressources). Une division du corpus en 2 sous-corpus de 80% (pour l\u0027apprentissage) et 20% (pour le test) est effectuée. L\u0027état de l\u0027art souligne que les utilisateurs pour lesquels le système manque de données (démarrage à froid) reçoivent de mauvaises recommandations. De façon à ne pas biaiser notre évaluation, nous écartons du corpus les utilisateurs associés à du démarrage à froid : ceux avec moins de 20 notes dans le corpus d\u0027apprentissage (Schickel-Zuber et Faltings, 2006). Le corpus est alors réduit à 821 utilisateurs. La mesure Anormalité de l\u0027état de l\u0027art présente une corrélation de 0,453 avec la RMSE. Cette corrélation significative confirme le lien existant entre l\u0027anormalité d\u0027un utilisateur et l\u0027erreur commise par une approche knn : plus un individu est anormal (atypique), plus l\u0027erreur commise sera élevée. A l\u0027opposé, moins il est anormal, moins l\u0027erreur sera élevée.\nCorrélations entre les mesures et l\u0027erreur de recommandation\nAnormalité CR augmente la corrélation de 11% (0,504). La controverse associée aux ressources permet donc d\u0027améliorer l\u0027estimation de la qualité des recommandations fournies aux utilisateurs. Anormalité CRU prend également en compte le profil de l\u0027utilisateur. Une corréla-tion de 0,546 est obtenue (amélioration supplémentaire de 8%, et donc de 20% par rapport à l\u0027état de l\u0027art). La prise en compte des particularités de notation des utilisateurs permet donc d\u0027améliorer l\u0027estimation de la qualité des prédictions fournies aux utilisateurs.\nLa faible corrélation de la mesure CorrKMax (-0,22) indique que, contrairement à notre intuition, la qualité des voisins d\u0027un utilisateur n\u0027est pas corrélée à la qualité des recommandations dans le cadre d\u0027une approche de recommandation knn.\nErreur en prédiction sur les utilisateurs atypiques\nUne corrélation représente le lien entre deux variables sur un ensemble d\u0027observations. Cependant, il est possible qu\u0027un lien existe sur une seule partie des observations, ce qui ne sera pas reflété par la corrélation. Ici, nous nous intéressons uniquement aux utilisateurs qualifiés d\u0027atypiques, c\u0027est-à-dire à ceux ayant des valeurs d\u0027anormalité les plus extrêmes. Par consé-quent, dans la suite des expérimentations, nous nous intéressons uniquement à la répartition FIG. 1 -Répartition de la RMSE des utilisateurs atypiques avec l\u0027approche knn des erreurs observées sur les utilisateurs considérés comme atypiques. Pour représenter la ré-partition de ces erreurs, nous exploitons les quartiles et la médiane de ces erreurs, sur les 4 mesures d\u0027anormalité. Plus les erreurs sont élevées, plus nous pouvons considérer que la mesure est de qualité. Nous comparons ces répartitions à celle associée à l\u0027ensemble total des utilisateurs, dénommé Complet (Figure 1).\nNous utilisons un pourcentage d\u0027utilisateurs atypiques fixe, car les mesures n\u0027ont pas des valeurs d\u0027anormalité comparables. Nous avons fixé exprimentalement ce seuil à 6% des utilisateurs, cela correspond à environ 50 utilisateurs parmi les 821 utilisateurs.\nL\u0027erreur médiane sur l\u0027ensemble des utilisateurs (Complet) est de 0, 82. Celle de Anormalité est de 1, 26, ce qui correspond à une augmentation de l\u0027erreur de plus de 50%, elle est d\u0027ailleurs équivalente au troisième quartile de l\u0027ensemble complet. Cependant, 25% des utilisateurs qualifiés d\u0027atypiques ont une RMSE plus faible que la RMSE médiane sur l\u0027ensemble des utilisateurs. Par conséquent, Anormalité semble sélectionner un grand nombre d\u0027utilisateurs dont les recommandations sont de bonne qualité. Anormalité CR et Anormalité CRU pré-sentent de meilleurs résultats qu\u0027Anormalité. Anormalité CRU est la plus performante : plus de 75% des utilisateurs sélectionnés ont une RMSE supérieure à 1,25 : 75% des utilisateurs Anormaux CRU font partie des 25% de l\u0027ensemble complet des utilisateurs à recevoir les moins bonnes recommandations. Enfin, environ 50% des utilisateurs sélectionnés avec CorrKM ax reçoivent de bonnes recommandations, ce qui confirme les premières conclusions obtenues avec la corrélation.\nNous pouvons conclure que Anormalité CRU identifie de façon fiable les utilisateurs atypiques : ceux recevant des recommandations de mauvaise qualité avec une approche knn.\nConclusion et perspectives\nNotre objectif dans ce premier travail était d\u0027identifier, en recommandation sociale, les utilisateurs qui reçoivent des recommandations de mauvaise qualité, avant que des recommandations ne leur soient proposées. Nous avons fait l\u0027hypothèse que ces utilisateurs avaient des préférences différentes des autres utilisateurs : des utilisateurs atypiques. Nous avons proposé plusieurs mesures exploitant la similarité de préférence avec les autres utilisateurs, l\u0027écart des notes par rapport aux autres, le consensus de notation sur les ressources et le profil de nota-tion des utilisateurs. Nous avons montré que, sur un corpus de l\u0027état de l\u0027art, ces informations permettaient de prédire fiablement la mauvaise qualité des recommandations faites à un utilisateur. Nous pouvons donc conclure que les utilisateurs présentant des préférences différentes des autres utilisateurs reçoivent effectivement des recommandations de mauvaise qualité. Ces mesures peuvent donc être utilisées pour anticiper une mauvaise recommandation et la suite de ce travail portera naturellement sur la prise en compte des préférences atypiques des utilisateurs pour leur fournir des recommandations de qualité.\n"
  },
  {
    "id": "262",
    "text": "Introduction\nDepuis les débuts du TALN, la compréhension de texte fait l\u0027objet d\u0027un suivi particulier de plusieurs recherches. C\u0027est en faveur du développement rapide de la tâche d\u0027extraction d\u0027information que la tâche de REN s\u0027est manifestée. Elle consiste à rechercher les expressions réfé-rentielles (Ehrmann (2008)), qui recouvrent classiquement les noms désignant des personnes, des lieux, des organisations, des expressions temporelles et celles numériques, mais peuvent aussi se rapporter à des notions plus techniques comme les maladies. Dès la campagne MUC-6, la tâche de REN s\u0027est ainsi polarisée sur trois types d\u0027entités (Grishman et Sundheim (1996)), à savoir : ENAMEX (personnes, organisations et lieux), TIMEX (expressions temporelles), NUMEX (expressions numériques). Cette première définition a été étendue dans la campagne CoNLL (Tjong Kim Sang et De Meulder, 2003) où 4 classes ont été normalisées : personnes, organisations, lieux, Divers. Dans les campagnes d\u0027évaluation ESTER2 (Galliano et al., 2009) 8 catégories ont été normalisées à savoir personnes, fonctions,organisations, lieux, productions humaines, dates, montants et événements.\nTravaux Connexes\nAuparavant, la visibilité de la langue amazighe au Maroc était quasiment nulle. Récem-ment, et grâce aux revendications qui se sont faites à l\u0027aide de l\u0027IRCAM 1 , elle a été soumise à un processus de codification et de standardisation. Face à l\u0027augmentation vertigineuse des informations en langue amazighe, disponibles librement sur le Web, plusieurs recherches ont été entamées dans ce sens. Il y en a celles qui se concentrent sur la reconnaissance optique des caractères (OCR) (Es-Saady et al., 2012) et celles qui se focalisent sur le TALN que nous pouvons classer en deux grandes catégories : (1) ressources informatiques, y compris des études sur la construction des corpus amazighe (Boulaknadel et Ataa Allah, 2011) et (2) les outils du TAL qui ont été réalisés comme le concordancier (Boulaknadel et Ataa Allah, 2010), l\u0027analyseur morphologique (Nejme et al., 2013a,b) et (Ataa Allah et Boulaknadel, 2010). Quant au domaine de la REN, il a acquis un certain intérêt à travers les travaux réalisés de Talha et Boulaknadel (Talha et al., 2014b,a;Boulaknadel et al., 2014).\nAperçu général de notre approche\nOn distingue traditionnellement trois grandes approches : Approches symboliques qui reposent sur l\u0027utilisation de grammaire formelle construite par la main. Approches statistiques qui permettent d\u0027apprendre, des modèles d\u0027analyse de textes sur de large corpus annoté auparavant, et ensuite établir automatiquement une base de connaissances à l\u0027aide de plusieurs modèles numériques comme le CRF, SVM, etc. Au-delà de ces deux approches, il existe une autre qualifiée d\u0027hybride qui représente un arrangement entre ses antécédents. Dans notre contribution, nous proposons un système fondé sur une approche symbolique, vu la non disponibilité d\u0027un large corpus, où le repérage s\u0027effectue en se basant sur un ensemble de gazetteers et de règles qu\u0027on a construit manuellement tout en exploitant le principe de transducteurs à états finis disponibles sous GATE. \nArchitecture du système\nNotre système de repérage d\u0027entités nommées permet l\u0027identification des bornes des EN, ainsi que leur catégorisation dans des classes prédéfinies. Son architecture, détaillée sur la figure 1, comporte 3 modules qui effectuent un traitement séquentiel immédiat des données : \nPrétraitement Morphologique\nManipuler des textes écrits en langue amazighe nécessite une analyse préliminaire qui consiste à : La suppression des espaces supplémentaires existants entre les mots et l\u0027élimi-nation de tous les mots non-amazighes figurant dans le corpus. Notre analyse comprend deux phases :\n-La segmentation du texte amazighe en des phrases.\n-L\u0027identification des entités linguistiques de base « tokenisation ». Ces deux phases citées au dessus sont implémentées en utilisant, respectivement, les modules de GATE : le « Sentence Splitter » et le « Tokeniser ».   \nConstitution des gazetteers\nÉvaluation\nDiscussion\nLes entités nommées qui n\u0027ont pas été identifiées, correspondent soit à des entités qui ne font pas partie de nos ressources, soit à des entités qui font partie de nos ressources, mais sont ambiguës. Certaines entités sont cependant ambiguës pour cause d\u0027homographie, ou encore le cas d\u0027entités poly-référentielles, une même entité nommée peut convenir à plusieurs classes. La prépondérance des entités mal classées implique un manque d\u0027information que ce soit au niveau du contexte syntaxique ou de la présence des indices externes, qui, en plus de déter-mination des mots d\u0027arrêt qui permettent de décider ou s\u0027arrêter, augmente les probabilités d\u0027erreurs de délimitation. Une analyse approfondie a conduit aux constats suivants :\n-Enrichir nos gazetteers (Personne, Organisation, Localisation, DATE, NUM).\n-Effectuer un traitement syntaxique supplémentaire afin de mieux saisir la structure syntaxique des phrases amazighes avant d\u0027effectuer le repérage des entités nommées. -Étendre le nombre de règles linguistiques pour chaque classe d\u0027entité nommée.\nConclusions et perspectives\nDans cet article nous avons proposé un système de repérage des entités nommées amazighes à base de règle. L\u0027évaluation du système montre que les résultats obtenus sont assez encourageants et nous invitent à explorer de nouveaux modes de repérage d\u0027entités nommées, afin de tirer le meilleur parti de notre approche et affiner le repérage des entités nommées amazighes.\nRéférences\nAtaa Allah, F. et S. Boulaknadel (2010). Light morphology processing for amazighe language.\nIn proceeding of the Workshop on Language Resources and Human Language Technology for Semitic Languages, Volume 17.\n"
  },
  {
    "id": "267",
    "text": "Introduction\nLorsque l\u0027on cherche à comparer deux documents, on recherche tout élément présent dans l\u0027un qui est également présent dans l\u0027autre, ces éléments sont dénommés \"similitudes\". Les plus évidentes à voir à l\u0027oeil humain sont les similitudes exactes, les parties copiées d\u0027un document directement dans l\u0027autre. Cependant, reproduire informatiquement cette capacité humaine est une opération délicate. Ce procédé est souvent gourmand en temps, car passant par une comparaison mot à mot afin d\u0027identifier les séquences de mots identiques dans les deux textes. De ce fait, des méthodes beaucoup moins gourmandes ont vu le jour. Basées sur un système de n-grammes, elles extraient des séquences de n mots se suivant d\u0027un texte et en cherche la présence dans l\u0027autre. C\u0027est dans l\u0027optique de proposer une alternative à ces méthodes que nous allons décrire dans cet article une nouvelle approche de construction de séquences communes.\nAprès avoir présenté l\u0027état de l\u0027art, nous décrirons dans un premier temps le processus d\u0027intersection des deux textes, ensuite, la phase de construction des plus longues séquences communes et pour finir, nous présenterons l\u0027évaluation de notre approche en la comparant aux méthodes naïves de comparaison mot à mot et à la méthode classique n-grammes.\n2 Le « copier/coller » 2.1 Le phénomène « copier/coller » Le « copier/coller » touche particulièrement les étudiants, en Europe, 34,5% (Guibert et Michaut, 2011) d\u0027entre eux auraient déjà recopié tout ou partie d\u0027un document pour le présen-ter comme travail personnel. Cette fréquence rejoint celle de travaux américains (Park, 2003) estimant à environ 30% la proportion d\u0027étudiants ayant produit un travail reprenant des phrases d\u0027Internet sans en citer la source. Une étude européenne (Gibney, 2006) révèle que près d\u0027un étudiant français sur deux (46%) a déjà fait usage du plagiat pendant son cursus, contre environ un tiers des étudiants anglais et 10% des étudiants allemands. Ces résultats, qui paraissent déjà impressionnants, pourraient pourtant encore être sous-évalués. En effet, toujours selon la même étude, 40% des étudiants ne comprennent pas ce que signifie réellement le plagiat et n\u0027assimilent pas le « copier/coller » à de la tricherie. La recherche de « copier/coller » entre deux textes joue donc un rôle essentiel dans la prévention du plagiat et la protection du droit d\u0027auteur.\nÉtat de l\u0027art\nLes « copier/coller » sont en théorie les similitudes textuelles les plus facilement repé-rables et identifiables. En effet, la détection de celles-ci équivaut à comparer l\u0027égalité entre deux textes. Pour effectuer cette recherche automatiquement on est obligé de procéder à une comparaison mot à mot. Cette opération, étant beaucoup trop chronophage pour être intégrée dans des solutions à but commercial ou hébergées en ligne, comme des services anti-plagiat, des techniques alternatives ont dû être mises au point.\nLes méthodes les plus efficaces restent les méthodes classiques dites n-grammes (Torrejón et Ramos, 2013), qui consistent à construire puis comparer à partir de textes, des séquences de n éléments pouvant être des syllabes, des mots, des entités nommées, etc. La recherche de Barron-Cedeño et Rosso (2009) prouve qu\u0027en prenant des \"n-words\" (séquence de n mots se suivant) de petites tailles, deux ou trois par exemple, les résultats sont bien meilleurs qu\u0027en utilisant des longues séquences avec un n important. Sur le même principe mais plus originale, on peut citer la méthode de Stamatatos (2009), utilisant des n-grammes mais lors d\u0027une détection intrinsèque, c\u0027est-à-dire sans utilisation de document externe, on ne cherche pas des similitudes avec d\u0027autres documents mais on étudie l\u0027intérieur même du document analysé pour y repérer des irrégularités, des zones suspectes. Les n-grammes les plus pertinents ne sont pas toujours des séquences de mots, comme en atteste le travail de Shrestha et Solorio (2013), des n-grammes de mots vides (stop words) et d\u0027entités nommées sont également utilisés pour dé-tecter des parties de textes similaires entre deux documents. Toutefois, les méthodes les plus répandues sont les méthodes \"fingerprint\", créant une empreinte du document pour la comparer avec celle d\u0027autres documents. La plupart de ces méthodes (Kent et Salim, 2010) utilisent également des n-grammes pour construire l\u0027empreinte des documents.\nLes méthodes \"fingerprint\" divisent la plupart du temps le document en grammes de longueur n, ainsi les empreintes de deux documents peuvent être comparées et les points (i.e. grammes) concordants, identifiés comme étant des passages identiques dans les textes. Certaines méthodes de \"fingerprint\" (Stein et Eissen, 2006, 2007Lyon et al., 2001) vont audelà de la recherche de similitudes exactes et introduisent la notion de « similarités proches » pouvant ainsi détecter les paraphrases. Toujours dans cette optique, des recherches plus ré-centes (Simac-Lejeune, 2013; Kong et al., 2013) ne se contentent pas de comparer des mots ou groupes de mots d\u0027un document à un autre mais tentent d\u0027établir une corrélation « sémantique » entre deux documents par une approche utilisant des mots-clefs.\n3 Notre approche 3.1 Intersection de deux textes L\u0027idée de cette première étape est d\u0027effectuer une intersection de deux textes, afin d\u0027obtenir un tableau des mots présents dans les deux textes tout en conservant la position qu\u0027ils ont dans l\u0027un des deux. La procédure utilisée durant cette étude est la suivante :\n1. passage en minuscule des deux textes à comparer ; 2. transformation en tableaux de ces deux phrases en segmentant sur les espaces et les caractères de ponctuation (lemmatisation) (chaque tableau représente une phrase et chaque cellule d\u0027un tableau contient un lemme de la phrase à laquelle il correspond) ; 3. intersection des deux tableaux créés en conservant les offsets (positions) des mots du premier tableau et donc de la première phrase.\nConstruction de séquences maximales communes\nLa seconde et dernière étape consiste à construire, à partir du tableau obtenu à l\u0027étape précédente, les séquences d\u0027un minimum de n mots se suivant dans le premier texte, se suivant donc dans le tableau et étant également présentes dans le second texte. Le seuil n est le nombre de mots se suivant à partir duquel on peut déterminer qu\u0027une séquence est la copie d\u0027une autre et qu\u0027elle n\u0027est pas due au hasard. Nous pourrions dès lors nous poser la question : à partir de combien de mots se suivant une séquence peut être considérée comme réellement copiée ? En effet, il existe des séquences de trois mots ou plus, suffisamment fréquentes dans la langue, pour fausser la comparaison, comme les séquences « il était une fois » ou « nulle par ailleurs ». Cependant, les résultats des travaux de Barron-Cedeño et Rosso (2009) démontrent que sur de larges textes, il est tout aussi efficace de fixer un n petit, à deux ou trois par exemple.\nLa procédure de construction des séquences est la suivante :\n1. on déplace une fenêtre de glissement de n éléments dans le tableau en fonction du seuil n choisi afin de constituer des \"n-words\" se suivant donc forcément dans le premier texte ; 2. pour chaque \"n-word\" constitué, on vérifie son existence dans le second texte ; 3. tant qu\u0027une correspondance est trouvée et que la séquence existe bien dans les deux textes, on construit la séquence de taille n + 1 en y concaténant le mot suivant du tableau ; 4. dès que la séquence ne s\u0027y trouve plus, on récupère la séquence maximale commune (la séquence essayée précédemment avant que le test échoue) et on recommence depuis l\u0027étape 1 en déplaçant la fenêtre de glissement sur le mot suivant et en reprenant le n initial.\nÉvaluation et tests 4.1 La base de tests et protocole\nLa base de tests est composée de 200 textes, allant de 100 mots à environ 20 000 mots (avec une moyenne de 1500 mots), représentant 500 comparaisons de textes deux à deux annotés manuellement afin de savoir quel passage est réellement la copie d\u0027un autre. Pour tester correctement les performances des algorithmes évalués, la base comporte aussi bien des passages entièrement copiés que des paraphrases ou des reformulations plus complexes, ainsi que des textes « pièges » traitant du même sujet et donc employant le même vocabulaire mais n\u0027étant pas pour autant un « copier/coller » ou une reformulation quelconque d\u0027un autre texte présent dans le corpus. L\u0027intégralité de ces textes est en français. Ci-dessous la répartition des comparaisons :\n-120 comparaisons effectuées afin de détecter des textes entièrement « copier/coller » de façon exact ; -80 comparaisons afin de détecter des textes entièrement paraphrasés ou reformulés ; -200 comparaisons entre des textes ne comportant que quelques passages rigoureusement identique (copier/coller) ; -100 comparaisons entre deux textes ne comportant que quelques passages « similaires » (paraphrases ou reformulations de phrases et/ou paragraphes). Ces comparaisons sont réparties entre des travaux d\u0027élèves (mémoires financiers et scientifiques) avec leurs sources, différentes versions à différentes dates d\u0027un même article de Wikipédia et des extraits du corpus de la PAN-CLEF 2014 en matière d\u0027alignement de textes.\nRésultats\nLes résultats obtenus sur le corpus de test, par la méthode naïve de comparaison mot à mot, la méthode classique des n-grammes et notre méthode, sont représentées dans le tableau 1. La méthode n-grammes évaluée est celle décrite dans l\u0027article de Barron  (2009) disant que prendre un n de petite taille augmente l\u0027efficacité de détection, sachant que prendre des bigrams favorise le rappel, tandis que prendre un n supérieur favorise la précision. Ce phénomène s\u0027explique par le fait que prendre un petit n forme des séquences courtes, on ne manque ainsi aucune correspondance mais on favorise les faux positifs, baissant alors la précision. En revanche prendre un n plus important construit des séquences plus longues, réduisant ainsi la correspondance de chaînes et donc le rappel mais augmentant le taux de certitude des concordances et donc la précision. Cet article ne pose pas la question d\u0027optimisation de la détection en fonction du n choisi, on fixe donc n \u003d 2 pour la suite de notre évaluation.\nOn constate dans le tableau 1 que notre méthode donne de meilleur résultat que celle des n-grammes (0.76 de précision contre 0.72 et 1 de rappel contre 0.78 avec n \u003d 2 pour les deux méthodes). Toutefois, on peut voir dans le tableau 2 qu\u0027en moyenne elle est 15% moins rapide et 30% plus coûteuse en mémoire que la méthode n-grammes (en allouant 6. \nConclusions\nNotre approche montre donc des résultats supérieurs aux méthodes n-grammes classiques, dans le sens où elle recherche une séquence de taille minimale n et agrandit si possible la séquence trouvée afin d\u0027obtenir une séquence maximale commune. En revanche, elle est tout aussi dépendante du nombre n choisi que les méthodes n-grammes. Son temps d\u0027exécution et son usage de la mémoire restent supérieurs à ceux des méthodes n-grammes bien que nettement inférieurs à ceux de la méthode mot à mot.\nPour des travaux futurs, nous envisageons de confronter nos résultats à des méthodes ngrammes plus sophistiquées comme celles décrites dans l\u0027article de Shrestha et Solorio (2013).\nPour conclure, bien que moins rapide, notre méthode montre une précision équivalente aux méthodes n-grammes tout en proposant un rappel nettement supérieur.\nSummary\nPlagiarism detection most commonly use the most naive phase of similarities search, the detection of copy and paste. In this paper, we propose an alternative method to the standard verbatim comparison approach. The idea is to carry out an intersection of two texts to get a table of common words and to keep only the maximum sequences of consecutive words in one of the texts which also exists in the other. We show that this method is faster and less expensive in memory that commonly used scan texts methods. The goal is to detect identical passages between two texts faster than verbatim comparison methods, while operating more efficient than the n-grams.\n"
  },
  {
    "id": "269",
    "text": "Introduction\nLe BiClustering consiste à réaliser un clustering simultanément sur les observations et les variables. Govaert et al ont introduit une adaptation de l\u0027algorithme k-means au biclustering nommée \"Croeuc\" qui permet de découvrir tous les biclusters en même temps. Dans Labiod et Nadif (2011), les auteurs ont proposés une approche de factorisation CUNMTF, qui généra-lise le concept de la NMF Lee et Seung (1999). D\u0027autres modèles probabiliste de biclustering sont proposés dans Govaert et Nadif (2008). Le Biclustering a de nombreuses applications et devient un challenge de plus en plus important avec l\u0027augmentation des volumes de données. Cependant les bons algorithmes de clustering sont encore extrêmement utiles, il est donc néces-saire de les adapter aux nouvelles architectures massivement distribuées utilisant le paradigme MapReduce.\nLe paradigme MapReduce Dean et Ghemawat ( Dans ce papier nous proposons une nouvelle approche globale de biclustering basé sur les cartes auto-organisatrices et le calcul distribué. Le modèle de biclustering BiTM (Biclustering using Topological Maps) a deja donnée lieu à une publication dans Chaibi et al. (2014). Dans ce papier nous proposons une adaptation de ces travaux a l\u0027architecture MapReduce avec une implémentation de BiTM sous Spark, une technologie open source de calcul distribué incluant plusieurs paradigmes de programmation. Les principales problématiques abordées dans ce papier sont la minimisation de la fonction et la taille des données en entrée et en sortie des fonctions primitive (Map et Reduce) d\u0027un algorithme de biclustering topologique.\n"
  },
  {
    "id": "270",
    "text": "Introduction\nLes tweets sont des messages courts ne dépassant pas 140 caractères. Cette contrainte impose l\u0027utilisation d\u0027un vocabulaire particulier pour les rédiger et donc elle rend indispensable de connaitre leurs contextes pour les comprendre. Pour ces raisons, nous allons nous concentrer sur la tâche de contextualisation des tweets attribuée à INEX2014 1 . Les participants devaient fournir un contexte, pour permettre aux lecteurs de bien comprendre le tweet en utilisant un système de recherche d\u0027information SRI et système de résumé automatique SRA. Dans cet article, nous proposons une nouvelle approche de contextualisation de tweets basée sur les règles d\u0027association inter-termes.\nCet article est organisé comme suit : Dans la section 2, nous détaillons notre nouvelle approche. la section 3 sera consacrée aux différentes expériences menées , finalement nous conclurons dans la section 4. \nApproche proposée\nConclusion\nDans cet article, nous avons décrit une nouvelle approche de contextualisation de tweet basée sur règles d\u0027association inter-termes. Les résultats ont confirmé que la synergie entre les règles d\u0027association entre termes et l\u0027expansion de tweets est fructueuse. Dans un travail en cours, nous proposons d\u0027ajouter une phase de désambiguïsation pour réduire le bruit dans nos résultats. \nSummary\nTweets are short messages that do not exceed 140 characters. Since they must be written respecting this limitation, a particular vocabulary is used. To make them understandable to a reader, it is therefore necessary to know their context. In this paper, we describe our approach for the tweet contextualization. This approach allows the extension of the tweet\u0027s vocabulary by a set of thematically related words using mining association rules between terms.\n"
  },
  {
    "id": "271",
    "text": "Introduction\nL\u0027objectif des systèmes de recommandation est de prédire les choix et les préférences individuelles en fonction des comportements et des préférences observées. Le filtrage collaboratif est la technique la plus utilisée par les systèmes de recommandation. Il consiste à comparer les données d\u0027un utilisateur avec des données similaires d\u0027autres utilisateurs, basée sur les habitudes d\u0027achat et de navigation (Goldberg et al., 1992). Il permet aux commerçants de fournir des recommendations aux clients pour de futurs achats. Dans la suite, les données sont représentées par une matrice U de taille (n × p) où chaque ligne représente un utilisateur, les colonnes représentent des items, et chaque cellule (u ij ) de U est la note attribuée par un utilisateur i pour un item j. Les notes (u ij ) peuvent être binaires, ou réelles et dans ce cas U est appelée matrice réelle de notations. La matrice U peut être obtenue de manière explicite (en gardant les évaluations fournies par les utilisateurs pour des articles donnés) ou de manière implicite (en considérant qu\u0027un utilisateur préfère implicitement acheter ou pas les éléments présentés sur des pages Web visitées).\nDans le filtrage collaboratif (désormais désigné par FC), plusieurs approches sont utilisées. Les techniques de FC actuelles telles que celles basées sur la corrélation entre utilisateurs (Bobadilla et al., 2013) ou sur la factorisation matricielle (Koren, 2009;Sarwar et al., 2000;Delporte et al., 2014) sont couramment utilisées, mais nécessitent un temps de calcul très coûteux et ne peuvent être déployées en ligne. Dans ce contexte, la classification croisée ou co-clustering, qui consiste à regrouper simultanément les utilisateurs et les items, est une bonne solution. Elle est particulièrement appropriée dans les systèmes de de recommendation. Il est ainsi, par exemple, intéressant de disposer de groupes d\u0027utilisateurs appréciant un groupe de films. Dans (George et Merugu, 2005), les auteurs ont proposé une approche de FC basé sur un algorithme de classification croisée pondérée (COCLUST) qui implique le regroupement simultané des utilisateurs et des articles. Malheureusement, dans cette approche la prise en compte des données manquantes n\u0027est pas appropriée, conduisant ainsi à une faible qualité de recommandation. Nous proposons donc de faire un meilleur usage de cet algorithme par une prise en compte plus efficace des données manquantes. D\u0027autre part, en exploitant le potentiel des résultats de la classification croisée, nous développons un outil interactif de visualisation et d\u0027interprétation simultanée des groupes d\u0027utilisateurs et des groupes d\u0027items.\nLe reste du papier est organisé comme suit. La section 2 présente le système de FC basé sur la classification croisée (COCLUST). Les sections 3 et 4 fournissent des détails sur nos approches de gestion des notes manquantes et de visualisation. La section 5 démontre l\u0027efficacité des approches proposées sur des données réelles. Enfin, la section 6 conclut et présente les directions pour des recherches futures.\nNotation. Soit U la matrice des notes, une classification croisée en K × L co-clusters (blocs ou sous-matrices résultant d\u0027une classification croisée) par COCLUST conduit à une partition de l\u0027ensemble des utilisateurs en K classes et une partition de l\u0027ensemble des items en L classes. Notons Z \u003d (z ik ) la matrice de classification binaire de taille (n × K) dé-finie par z ik \u003d 1 si l\u0027utilisateur i appartient à la k i` eme classe et 0 sinon. De la même manière notons W \u003d (w j ) la matrice de classification binaire de taille (p × L) définie par w j \u003d 1 si l\u0027item j appartient à la i` eme classe et 0 sinon. Par commodité, nous utiliserons également z \u003d (z 1 , . . . , z n ) avec z i ? {1, . . . , K} (respectivement w \u003d (w 1 , . . . , w p ) avec w j ? {1, . . . , L} ; le vecteur des étiquettes des items). Enfin, nous utiliserons les indices i, j, k et pour désigner implicitement les lignes (utilisateurs), les colonnes (items), les classes en ligne (classes d\u0027utilisateurs) et les classes en colonnes (classes des items) respectivement.\n2 Classification croisée par COCLUST Partant de l\u0027algorithme weighted Bregman co-clustering (Banerjee et al., 2004), les auteurs dans (George et Merugu, 2005) ont proposé de s\u0027attaquer au problème de recommandation moyennant une reconstitution des données observées suivant une classification croisée donnée. Plus précisément, à partir de la réorganisation en co-clusters obtenus par l\u0027algorithme COCLUST, les auteurs proposent une matrice d\u0027approximation de U sparse par une matricêmatricê U \u003d (ˆ u ij ) non sparse où chaque cellule est définie de la manière suivante :\navec u k , u k. , u .. , u i , u j sont respectivement les moyennes calculées sur l\u0027ensemble des valeurs observées dans le co-cluster (k, dans la classe des utilisateurs k, dans la classe des items, pour chaque utilisateur et pour chaque item. Notons donc que dans cette formulationû formulationˆformulationû ij dépend de i, j, k et Sachant que les partitions Z and W sont inconnues, le critère à minimiser par COCLUST est le suivant :\noù M \u003d (m ij ) est une matrice binaire de taille (n × p) où m ij \u003d 1 si u ij est observé et m ij \u003d 0 si u ij est manquant. Une solution (optimum local) de ce problème peut être obtenue par une minimisation alternée ; sachant Z puis sachant W (Banerjee et al., 2004) jusqu\u0027à la convergence (Algorithm 1). A la convergence la prédiction est obtenue en utilisant (1).\nAlgorithm 1 Training based on Co-clustering.\n. , u i and u j ; ( ? k, i and j) 2. Update Z :\nComme indiqué précedemment les estimations au cours des itérations de COCLUST sont basées uniquement sur les données observées. Malheureusement et étant donné que le taux des données manquantes est très élevé (la sparsité de certaines matrices peut être de l\u0027ordre de 99 %), les prédictions sont biaisées impliquant des qualités de recommandation discutables. D\u0027autre part, George et Merugu (2005) ont proposé de remplacer la moyenne d\u0027un co-cluster vide (qui ne contient aucune note observée) par la moyenne globale. Cette stratégie peut fortement perturber la qualité de la classification croisée, et de plus elle ne garantit pas la convergence de COCLUST, comme le montre figure 1. Pour plus d\u0027explications, nous avons rapporté  \nGestion des notes manquantes dans le FC\nPour surmonter le problème des données manquantes dans le FC, deux approches sont principalement utilisées. La première consiste à travailler uniquement sur les valeurs observées, et la deuxième consiste à utiliser les procédures d\u0027imputation. L\u0027imputation par la moyenne est la plus couramment utilisée, elle consiste à remplacer les notes manquantes d\u0027un item/utilisateur par la moyenne de ses notes observées.\nCes approches peuvent être efficaces, si peu de valeurs sont manquantes, et que le mé-canismes des données manquantes est Missing completely at random ou Missing at random (Little et Rubin, 2002). Malheureusement le taux de notes manquantes dans le filtrage collaboratif est très élevé, ce qui rend ces approches inefficaces dans ce contexte. En effet, elles peuvent conduire à des estimations fortement biaisées, ce qui impacte négativement la qualité des recommandations. Pour illustrer ce propos, nous avons rapporté dans figure 3 un exemple d\u0027une matrice utilisateur-item, avant (figure 3a) et après l\u0027imputation par les moyennes des items (figure 3b). Si nous voulons ordonner les items en fonction des préférences des utilisateurs, l\u0027ordre le plus fiable serait : i1, i4, i2, i3 (tels que i1 est l\u0027élément le plus apprécié). En revanche si nous utilisons la matrice après imputation pour trier ces éléments de la même manière, nous obtiendrons l\u0027ordre suivant : i2, i4, i1, i3 qui est absurde, puisque i1 arrive seulement en troisième position et i2 arrive en première position. Cela est dû aux estimations fortement biaisées des moyennes des utilisateurs i2 et i4. Dans ce qui suit, nous allons présenter une nouvelle méthode d\u0027imputation basée sur la version en ligne de l\u0027algorithme kmeans sphé-rique (OSPK-means) (Zhong, 2005). Notre approche repose sur les deux étapes principales, 1) Partitionner l\u0027ensemble des utilisateurs en k classes, en utilisant l\u0027algorithme OSPK-means et en tenant compte des valeurs manquantes, 2) Estimer les notes manquantes, en se basant sur les résultats de la classification. Et enfin remplacer celles-ci dans la matrice U. Ci-dessous, nous décrivons de manière détaillée les différentes étapes de notre approche :\nEtape de Classification\nDans le but de partitionner l\u0027ensemble des utilisateurs en k groupes, nous proposons les procédures suivantes : Initialisation : Dans la version initiale de OSPK-means, l\u0027initialisation se fait par un tirage aléatoire de K centres initiaux parmi l\u0027ensemble des utilisateurs. Cependant cette stratégie n\u0027est pas efficace dans notre cas. En effet la probabilité de choisir un utilisateur avec très peu de notes observées, comme un centre de gravité initial est élevée. D\u0027autre part, sélectionner les centres initiaux uniquement parmi l\u0027ensemble des utilisateurs ayant noté beaucoup d\u0027items, permettrait seulement la détection de certains groupes. Afin de surmonter ces difficultés, nous proposons la procédure d\u0027initialisation suivante :\n1. Générer une partition aléatoire des utilisateurs en k classes.\n2. Estimer les centres initiaux comme suit : soit µ kj la j i` eme composante du centre k alors :\n; sinon où S est un seuil proportionnel à la taille de la classe k, et peut être défini par l\u0027utilisateur. Intuitivement, cette stratégie permet d\u0027estimer la j i` eme composante du centre de la k i` eme classe à partir des données disponibles, mais seulement s\u0027il y a suffisamment de notes observées pour dans cette classe. En revanche, quand peu de valeurs sont observées pour une composante j l\u0027estimation de celle-ci est pénalisée, en divisant par le seuil S. Etape de mise à jour : Lorsque l\u0027utilisateur choisi dans l\u0027étape d\u0027affectation ne dispose pas de suffisamment de notes-observées, l\u0027assignation de celui-ci n\u0027est pas fiable. Par conséquent le centre correspondant ne doit pas être déplacé dans le sens de cet utilisateur. Pour résoudre ce problème, nous introduisons une fonction binaire (h(u) ? {0, 1}) qui annule la mise à jour dans ce cas. L\u0027Algorithme 2 fournit plus de détails sur cette étape de classification.\nAlgorithm 2 Classification.\nInput : n normalized users u i ( i \u003d 1) in R p , K : number of user clusters, ? : learning rate, B : number of batch iterations ; Output : K Centroids µ k in R p , and z \u003d (z 1 , . . . , z n ) ; Steps : 1. Random initialization of the partition z ; 2. Estimation of initial centroids :\n; otherwize. for b \u003d 1 to B do for i \u003d 1 to n do 3. Assignment : for each user u i , compute z i :\nz i +?h(ui)ui ; t \u003d t + 1 ; end for end for\nEstimation des notes manquantes\nDans cette étape les notes manquantes sont estimées, en se basant sur les résultats de la classification. Cependant, une pondération des notes s\u0027avère encore nécessaire. Nous avons choisi d\u0027accorder plus d\u0027importance aux utilisateurs representant le mieux leur classe d\u0027appartenance en pondérant par cos(u i , µ k ) tout en atténuant l\u0027effet des utilisateurs qui ne sont pas en accord avec la préférence globale pour un item au sein de leur classe d\u0027appartenance à l\u0027aide de p(u ij ). Soit u a un utilisateur actif, k \u003d z a , la note pour pour un item j prend la forme suivante :\noù r med est la note médiane (r med \u003d 3 if u ij ? {1, 2, 3, 4, 5}, p(u ij ? r med ) est la probabilité qu\u0027un item j soit apprécié au sein d\u0027un groupe k, tel que :\nDans la section suivante, nous proposons d\u0027exploiter les résultats de classification croisée, dans le but de fournir aux utilisateurs une représentation interactive basée sur des graphes bipartis. Cette dernière permet non seulement de faciliter l\u0027interprétation des résultats, mais aussi de donner un sens aux préférences des utilisateurs dans le contexte du filtrage collaboratif.\nVisualisation des résultats de la classification croisée\nIl y a très peu de travaux qui se sont intéressés à l\u0027aspect visualisation dans le contexte des systèmes de recommandation. En effet ces systèmes sont souvent évalués pour leur capacité à faire de bonnes recommandations, mais leur fonctionnement reste abstrait pour les utilisateurs. Parmi les quelques travaux de visualisation on peut citer la méthode de visualisation des données du FC (Mei et Shelton, 2006) qui consiste à représenter les utilisateurs à côté des items qu\u0027ils aiment, sur le même espace euclidien. On peut aussi citer PeerChooser (Smyth et al., 2008) qui est un système de FC interactif qui permet de visualiser sous forme de graphe les interactions entre un utilisateur actif et son voisinage, tout en offrant la possibilité de modifier ce dérnier. Il existe aussi des travaux qui proposent de visualiser sur un plan à deux dimensions la liste d\u0027items à recommander pour un utilisateur actif, en utilisant les techniques classiques telles que l\u0027ACP, MDS, SOM.\nDans ce travail nous proposons une nouvelle approche de visualisation dans le contexte des systèmes de recommandations. Contrairement aux autres méthodes citées ci-dessus, notre approche est globale, c\u0027est à dire qu\u0027elle ne se focalise pas uniquement sur l\u0027utilisateur actif. Elle exploite la dualité inhérente de la classification croisée pour mieux mettre en évidence les affinités entre certains types de groupes d\u0027utilisateurs et certains types de produits. Plus pré-cisément, nous proposons de représenter les relations de préférences entre des groupes d\u0027utilisateurs et groupes d\u0027items, au moyen des graphes bipartis. Notre approche peut être décrite comme suit : 1) Classifier la matrice utilisateur-item en K classes d\u0027utilisateurs et L classes d\u0027items. Dans nos expérimentations nous avons utilisé COCLUST, après la gestion des données manquantes, présentée dans la section précédente, 2) Construire une matrice résumant les résultats de la classification croisée, dans laquelle chaque groupe de de lignes et chaque groupe de colonnes est représenté par les utilisateurs et les items les plus populaires (qui ont le plus de votes) respectivement, 3) Calculer la relation de préférence entre chaque groupe d\u0027utilisateurs et chaque groupe d\u0027items, à l\u0027aide de la formule (4), 4) Construire le graphe biparti, étape décrite en détail dans la partie expérimentale.\nSoit U \u003d (u ij ) la matrice résumée de l\u0027étape 2, avec n utilisateurs et p items. Et soit E \u003d (e ij ) une matrice binaire de n × p, tel que e ij \u003d 1 si l\u0027utilisateur i aime l\u0027item j et e ij \u003d 0 sinon. Alors la corrélation entre la k \nIntuitivement la corrélation (de préférence) 4, entre un groupe d\u0027utilisateurs k et un certain groupe d\u0027items représente la proportion des items populaires dans la i` eme classe ayant été appréciée par les utilisateurs les plus populaires de la classe k. La section suivante présente les résultats expérimentaux démontrant l\u0027efficacité des approches proposées.\nAlgorithm 3 Bipartite procedure.\nInput : U, K and L ; Output : C : correlation matrix between clusters ; Steps : 1. Compute (Z, W) into K row clusters and L column clusters ; 2. Compute U with the relevant users and items. for k \u003d 1 to K do for l \u003d 1 to L do 4. Compute C \u003d (c k ) the correlation matrix between clusters, by using (4) end for end for 5. Build the bipartite graph\nRésultats expérimentaux\nDans nos expériences, nous avons choisi les deux jeux de données de MovieLens 1 (ML-100K et ML-1M) qui sont beaucoup utilisés dans le domaine. L\u0027échantillon ML-1M est constitué de 6040 utilisateurs, 3952 films, et de 1 million de notes observées. L\u0027ensemble ML-100K contient 100,000 notes fournies par 943 utilisateurs pour 1664 films. La proportion des notes observées dans ce dernier est seulement de 6, 4%. Les évaluations des utilisateurs (u ij ) appartiennent à l\u0027intervalle : [1; 5], et les notes manquantes sont codées par : NA. Les données MovieLens fournissent également certaines informations démographiques sur les utilisateurs, telles que : le sexe, l\u0027âge, la profession, code postal ; et des informations de base sur les films tels que : le titre, le genre, la date de sortie, etc. A noter qu\u0027un film peut être de plusieurs genres à la fois. Nous proposons dans la suite de réaliser la comparaison des courbes ROC et de la F-measure, des systèmes de FC suivants : COCLUST, le FC incrémental basé sur la décompo-sition en valeurs singulières SVDCF (Sarwar et al., 2002), et COCLUST++ (COCLUST après la gestion des valeurs manquantes). Ces comparaisons sont réalisées sous recommenderlab (Hahsler, 2011), que nous avons combiné avec le langage C pour implémenter les différentes méthodes ci-dessus. Les courbes de figure. 4a sont construites en faisant varier le nombre d\u0027items à recommander de 1 à 40. Les deux figures 4a et 4b montrent une amélioration significative des performances de COCLUST, grâce à la gestion des données manquantes que nous proposons. On remarque aussi, une faible qualité des recommandations pour SVDCF, qui est due à une gestion des données manquantes inappropriée. En effet dans cette dernière approche (Sarwar et al., 2002), les notes manquantes sont remplacées par les moyennes des items dont les estimations sont fortement biaisées. En d\u0027autres termes cette imputation favorise les items avec très peu de notes observées, comme illustré dans la section 3 (figure 3). La figure 4d montre que même avec l\u0027étape d\u0027imputation COCLUST++ reste plus rapide que SVDCF.\nEn ce qui concerne les possibilités de visualisation exploitant la classification croisée, la figure 5 montre un exemple de graphe biparti, qui est construit comme suit 1) Classification de l\u0027ensemble ML-100k, en 6 classes utilisateurs et 8 classes d\u0027items, en utilisant COCLUST++, 2) Calculer les corrélations entre les groupes d\u0027utilisateurs et d\u0027items, via la formule (4), 3) Construire le graphe biparti où les rectangles de gauche représentent des groupes d\u0027utilisateurs, tandis que ceux de droite des groupes d\u0027items. Seuls les liens qui correspondent à de fortes corrélations sont représentés. Pour chaque groupe d\u0027utilisateurs, les deux professions les plus populaires sont présentées, de même les deux genres les plus populaires dans chaque classe  \nConclusion\nDans ce papier nous avons proposé une meilleure exploitation du potentiel de la classification croisée dans les systèmes de FC. Pour ce faire, nous avons développé une nouvelle stratégie pour une gestion efficace des données manquantes. Nous avons ensuite proposé une nouvelle approche interactive basée sur des graphes bipartis, permettant d\u0027interpréter et de comprendre les résultats de la classification croisée dans le contexte du FC. Les résultats expé-rimentaux montrent une amélioration importante des performances de la classification croisée dans le FC, grâce à une meilleure gestion des notes manquantes. Nous avons aussi montré, comment les représentations interactives basées sur des graphes bipartis peuvent aider les uti-\n"
  },
  {
    "id": "272",
    "text": "Introduction\nDans le contexte d\u0027une fouille exploratoire, le recours à des techniques de réduction de dimensionnalité permet classiquement de contourner la difficulté de représenter des résultats de clustering réalisés sur des données à haute dimensionnalité (HD). Les étiquettes de clusters, associées par exemple à des couleurs catégorielles, peuvent alors être appliquées aux points d\u0027un nuage 2D ou 3D.\nLes techniques de réduction de dimensionnalité sont susceptibles d\u0027introduire des artéfacts de déchirement et de recollement (Aupetit, 2007). Les algorithmes de clustering ne sont pas sujets à ces artéfacts, mais peuvent mener à des résultats sous-optimaux, ou avoir été mal paramétrés. L\u0027objet de cet article est de proposer un outil interactif de fouille visuelle combinant le meilleur de ces deux approches. Il utilise une projection 2D obtenue par t-SNE, une technique de réduction de dimensionnalité non-supervisée (van der Maaten et Hinton, 2008). Nous ne proposons pas un algorithme de clustering per se, mais plutôt une manière itérative d\u0027amélio-rer conjointement un clustering initial calculé de manière non-supervisée dans un espace HD, et une représentation 2D associée.\nUne présentation générale de notre outil est proposée en section 2. Les clusters sont amendés grâce à des techniques de diffusion d\u0027étiquettes, présentées en section 3. Réciproquement, l\u0027adaptation de la projection 2D aux clusters est évoquée dans la section 4. Les exemples donnés en section 5 et tout au long de cet article utilisent le jeu de données COIL-20 (Nene et al., FIG. 1 -Diagramme résumant la logique de l\u0027outil. 1996). Il contient 1440 images, réparties en 10 classes. Les images sont décrites par les intensités de leurs 1024 pixels sur une échelle de gris.\nPrésentation de l\u0027outil\nLa logique de l\u0027outil est résumée dans la figure 1. Il est paramétré par un jeu de données HD (i.e., \u003e 3), et par un clustering non-supervisé réalisé dans l\u0027espace HD.\nUne projection 2D initiale est calculée de manière non-supervisée par l\u0027algorithme itératif t-SNE. Elle est matérialisée par un nuage de points, dont les colorations sont associées aux étiquettes de clusters via un ensemble de couleurs catégorielles. À partir d\u0027une projection et d\u0027un clustering donnés, l\u0027utilisateur peut déclencher les actions suivantes :\n-Sélection d\u0027une restriction : optionnellement, l\u0027utilisateur peut limiter le rayon de son action à un ensemble de clusters sélectionnés directement en cliquant la légende (voir Figure 2e). -Sélection de pivots : en cliquant sur un des points dans le nuage (voir Figure 2d), l\u0027utilisateur définit un pivot pour la diffusion d\u0027une nouvelle étiquette de cluster. Un inspecteur interactif est à sa disposition pour associer une sémantique à chaque pivot (voir Figure 2c). -Diffusion d\u0027étiquettes : les étiquettes des pivots sélectionnés sont diffusées en utilisant la proximité 2D entre éléments. -Modification des dissimilarités : la répartition en clusters peut être utilisée pour influencer les dissimilarités sous-jacentes à l\u0027algorithme t-SNE. L\u0027utilisateur peut paramétrer le niveau de cet impact. Les actions de l\u0027utilisateur modifient la visualisation et les clusters, dont le nouvel état peut servir d\u0027entrée à une nouvelle itération du diagramme en figure 1. Des itérations sont effectuées jusqu\u0027à ce que l\u0027utilisateur soit satisfait du résultat. fastidieuses, nous proposons une diffusion semi-automatique basée sur la sélection de pivots par l\u0027utilisateur. La diffusion peut être calculée selon deux algorithmes issus de la littérature : -Propagation probabiliste : les étiquettes peuvent métaphoriquement sauter de manière probabiliste depuis les pivots associés, puis d\u0027élément en élément. Ce processus converge, et le résultat peut être obtenu sous une forme analytique impliquant de simples produits de matrices (Zhu et Ghahramani, 2002). -Coupes de l\u0027arbre de couverture minimal : l\u0027arbre de couverture minimal d\u0027un graphe peut être calculé grâce à l\u0027algorithme de Kruskal (Kruskal, 1956). Des coupes dans cet arbre isolent des composantes connexes du graphe.\nDiffusion d\u0027étiquettes\nCes opérations sont réalisées relativement à la distribution visuelle des éléments ; les distances 2D entre éléments dans la projection sont donc utilisées dans les algorithmes.\nL\u0027utilisateur commence par sélectionner un ou plusieurs pivots dans la restriction en cours (qui peut englober tous les éléments si aucun cluster n\u0027a été sélectionné dans la légende). Ces derniers peuvent être vus comme des prototypes de clusters, existants et à redécouper, ou à créer. Selon ses préférences, l\u0027utilisateur peut alors les propager exhaustivement, ou paramé-trer interactivement la coupe de l\u0027arbre de couverture minimal pour isoler des composantes connexes. Dans ce dernier cas, il peut aussi choisir de regrouper les composantes sans pivot dans un cluster résiduel, ou laisser leur étiquette telle qu\u0027avant l\u0027interaction.\nModification des dissimilarités\nPlutôt que d\u0027utiliser la distribution 2D des éléments pour modifier les clusters comme dans la section 3, l\u0027utilisateur peut utiliser la répartition en clusters pour modifier la projection 2D, de manière par exemple à renforcer la séparation des clusters.\nConsidérons le graphe complet entre les éléments dans la restriction en cours, pondéré par les dissimilarités dans l\u0027espace HD. Nous voulons utiliser l\u0027information portée par les clusters pour amender les dissimilarités HD entre éléments. Pour préserver la structure interne des clusters, nous proposons de restreindre la modification au sous-graphe multipartite induit par les clusters. La fonction cumulative normalisée de la distribution Beta est alors appliquée aux poids d\u0027arêtes associés :\nLes bornes a et b permettent d\u0027adapter la transformation aux valeurs de dissimilarité, e.g., à la valeur maximale observée dans la restriction, ou à la cohésion interne des clusters. Les changements trop disruptifs sont ainsi évités. L\u0027utilisateur peut alors paramétrer interactivement un rapprochement (respectivement un éloignement) des clusters en augmentant le paramètre ? (respectivement ?).\nL\u0027algorithme t-SNE se base sur des dissimilarités HD entre les éléments pour estimer leurs positions dans le nuage de points. Classiquement, ces dissimilarités sont initialisées par une distance Euclidienne dans l\u0027espace HD. Dans l\u0027outil, les dissimilarités peuvent être modifiées dynamiquement.\nL\u0027algorithme t-SNE peut être interprété comme une variante d\u0027algorithme force et ressort. La métaphore physique suivie par cette classe d\u0027algorithmes convertit des changements discontinus des forces en présence en mouvements continus. Ainsi, la discontinuité obtenue à l\u0027application de l\u0027équation (1) est convertie en mouvements continus, facilitant leur suivi par un utilisateur. Après une modification de dissimilarités, ce dernier peut alors suivre le changement progressif induit par son action.\nLes dissimilarités HD demeurent latentes à la projection 2D, et ne peuvent pas être observées directement dans la visualisation. Pour pallier cette limitation, nous avons incorporé l\u0027outil ProxiViz (Heulot et al., 2012), qui permet de mapper interactivement les dissimilarités sur le diagramme de Voronoï du nuage de points (voir Figure 3). L\u0027utilisateur dispose ainsi d\u0027une information plus complète avant de procéder à ses modifications. Ceci peut par exemple permettre de prendre en compte d\u0027éventuels artéfacts de projection (Aupetit, 2007).\nExemples\nAu cours de ses manipulations avec l\u0027outil, l\u0027utilisateur est confronté à la situation de la figure 4a. Le cluster vert, identifié par l\u0027algorithme de clustering dans l\u0027espace HD, est éclaté en 3 composantes dans la visualisation. L\u0027utilisateur souhaite les regrouper, en respectant le voisinage des composantes dans la projection.\nIl commence par vérifier la pertinence d\u0027un tel regroupement en utilisant l\u0027outil ProxiViz. Les composantes du cluster vert sont bruitées par les clusters violet et orange. Il commence FIG. 3 -Le survol du nuage de points déclenche l\u0027outil ProxiViz. Les dissimilarités HD par rapport au point de la cellule survolée sont mappées sur une échelle de gris, et colorent les cellules de Voronoï des éléments respectifs. Les étiquettes de clusters sont rappelées en colorant le contour des points, ainsi que la cellule du point survolé.\nFIG. 4 -a)\nLe cluster vert est réparti en 3 composantes, bruité par les clusters violet et orange. Un pivot est sélectionné pour chaque composante. b) L\u0027exécution d\u0027itérations de t-SNE sur la restriction en cours ne réunit que partiellement le cluster. c) Après mise à jour des dissimilarités, le cluster est effectivement regroupé. donc par définir une restriction à ces 3 clusters, et sélectionne des pivots pour isoler les composantes (voir Section 3). Les composantes sont ensuite réunies simplement en éditant la légende interactive.\nEn déclenchant t-SNE sur la restriction en cours, le cluster est partiellement réuni (voir Figure 4b). L\u0027utilisateur influence leur rapprochement en mettant à jour les dissimilarités (voir Section 4 et Figure 4c).\nConclusion\nDans cet article, nous avons exposé notre outil de clustering visuel et interactif. Les techniques de diffusion d\u0027étiquettes et de modification des dissimilarités permettent d\u0027enrichir mutuellement une projection 2D et un clustering itérativement mis à jour. La métaphore physique suivie par le nuage de points et les moyens de contrôle offerts par l\u0027interface permettent à l\u0027utilisateur de suivre le changement progressif induit par ses actions. Nous avons illustré l\u0027intérêt de l\u0027approche au travers d\u0027exemples.\n"
  },
  {
    "id": "273",
    "text": "Introduction\nLa découverte de motifs locaux introduite par Agrawal et Srikant (1994) consiste à extraire des informations pertinentes décrivant une portion des données. Evaluer et garantir la qualité des motifs extraits demeure une problématique très ouverte malgré le nombre important de propositions (Giacometti et al., 2013). Chacune de ces propositions repose explicitement ou implicitement sur une mesure d\u0027intérêt dont la qualité dépend de la complexité du modèle sous-jacent et de son ajustement aux données. Le modèle repose en général sur des fondements statistiques dont la complexité et la compréhension sont bien connues. A l\u0027inverse, l\u0027ajustement aux données reste une notion difficile à appréhender. Pourtant, c\u0027est probablement cette notion qui distingue la fouille de données des statistiques traditionnelles. L\u0027ajustement aux données est souvent connoté négativement et synonyme de sur-apprentissage par rapport aux données. De notre point de vue, l\u0027ajustement aux données n\u0027est pas un biais d\u0027apprentissage mais un moyen pour lever certaines hypothèses sur le modèle en les remplaçant par des mesures sur les données. Nous proposons d\u0027étudier l\u0027ajustement aux données à travers les interrelations entre motifs lors de l\u0027évaluation d\u0027une mesure d\u0027intérêt ou d\u0027une contrainte d\u0027extraction.\nLa qualité d\u0027une mesure repose sur sa capacité à isoler un motif singulier qui dévie des autres motifs communs. Pour cette raison, une mesure se doit de mettre en relation le motif évalué avec d\u0027autres motifs, dits motifs liés. Par exemple, la confiance de la règle d\u0027association X ? Y met en relation la fréquence de X ? Y (motif évalué) par rapport à la fréquence de X (motif lié). La qualité de la règle augmente avec la fréquence de X ? Y tandis qu\u0027elle diminue si la fréquence de X augmente (lorsque les autres fréquences restent constantes). Ces variations de la confiance sont en partie conformes aux deux axiomes formulés par PiatetskyShapiro (1991). De manière intéressante, ces axiomes permettent d\u0027étudier formellement le comportement des mesures d\u0027intérêt dédiées à l\u0027évaluation des règles d\u0027association. Dans cet article, nous proposons de généraliser ce principe en introduisant la notion de motif lié pour s\u0027attaquer à l\u0027évaluation de n\u0027importe quelle méthode de découverte de motifs.\nL\u0027objectif de ce travail est de formaliser la qualité et la sémantique des mesures d\u0027intérêt et contraintes en analysant les interrelations entre les motifs nécessaires à l\u0027évaluation de chaque motif extrait. Ce travail s\u0027inscrit dans la lignée des travaux sur l\u0027analyse des propriétés formelles vérifiées par les mesures d\u0027intérêt Piatetsky-Shapiro (1991); Tan et al. (2004); Geng et Hamilton (2006); Lenca et al. (2008); Hämäläinen et al. (2010) en les étendant aux contraintes d\u0027extraction. Nous formaliserons l\u0027interrelation entre motifs en introduisant l\u0027ensemble de motifs liés. Cet ensemble regroupe tous les motifs susceptibles d\u0027impacter l\u0027évaluation d\u0027un motif donné. Nous distinguerons les motifs liés positivement qui permettent d\u0027accroître la mesure d\u0027intérêt de ceux qui la font décroître, i.e., les motifs liés négativement. Nous formulerons alors trois axiomes que devraient satisfaire une mesure d\u0027intérêt ou contrainte. Chacun de ces axiomes impose des contraintes topologiques que doivent respecter les deux ensembles de motifs liés. Enfin, nous proposerons des critères d\u0027analyse de la complexité et de la sémantique d\u0027une mesure d\u0027intérêt ou contrainte. Nous introduirons finalement la complexité en évaluation qui repose sur la cardinalité de l\u0027ensemble des motifs liés.\nTravaux relatifs\nA notre connaissance, très peu de travaux se sont intéressés à l\u0027interrelation entre les motifs lors de l\u0027évaluation d\u0027une mesure d\u0027intérêt ou d\u0027une contrainte. De manière plus générale, l\u0027évaluation de la qualité des méthodes de découverte de motifs est une tâche ardue et peu étudiée.\nProtocoles expérimentaux\nLa plupart des méthodes d\u0027évaluation ou d\u0027analyse de la qualité concernant la découverte de motifs repose sur des protocoles expérimentaux où l\u0027objectif est de vérifier la conformité du résultat avec un étalon-or. Dans un contexte supervisé, il est possible d\u0027exploiter directement la variable cible comme référence. Ensuite, le cadre précision/rappel, l\u0027analyse ROC (Fawcett, 2006), la validation croisée (Kohavi, 1995), etc sont utilisés pour évaluer l\u0027écart entre les motifs extraits et la référence. L\u0027évaluation de la qualité des méthodes de découverte de motifs dans un contexte non-supervisé s\u0027avère bien plus difficile comme le rappellent de Lin et Chalupsky (2004). En effet, la validation des motifs extraits ne peut pas s\u0027appuyer sur un étalon-or explicite. Plusieurs stratégies sont alors mises en oeuvre pour obtenir un succédané de cet étalon-or.\nPremièrement, il est possible de s\u0027appuyer sur la connaissance d\u0027experts d\u0027un domaine (Carvalho et al., 2005). Avec un cas d\u0027utilisation, des experts du domaine sont sollicités pour juger la justesse des motifs découverts et ainsi, évaluer la méthode d\u0027extraction. La stratégie de redécouverte teste si un processus parvient à retrouver les connaissances bien établies dans un certain domaine. Deuxièmement, il est parfois possible de construire l\u0027étalon-or (Gupta et al., 2008;Zimmermann, 2013). Par exemple, Gupta et al. (2008) propose un protocole d\u0027évaluation quantitative des algorithmes d\u0027extraction de motifs approximés fréquents : (1) l\u0027extraction des motifs fréquents dans un jeu de données classique, (2) l\u0027ajout de bruit dans ce jeu de données, (3) l\u0027extraction des motifs approximés fréquents et enfin, (4) la comparaison des véritables motifs fréquents avec les motifs approximés fréquents. Ici, l\u0027étape 1 explicite l\u0027étalon-or. Enfin, l\u0027hypothèse nulle peut parfois être construite expérimentalement. Gionis et al. (2007)  Ces protocoles expérimentaux sont clairement utiles, mais néanmoins ils souffrent de plusieurs limites. Tous ces protocoles expérimentaux reposent sur l\u0027évaluation de la collection de motifs extraits. Ils ne peuvent donc fournir qu\u0027un résultat a posteriori, i.e., après l\u0027implémen-tation d\u0027un prototype et de son application sur un jeu de données. L\u0027évaluation est forcément dépendante du jeu de données considéré et les résultats sont difficilement généralisables à n\u0027importe quel jeu de données, de n\u0027importe quel domaine.\nOutils formels\nPlusieurs outils formels ont été proposés dans la littérature pour analyser qualitativement les méthodes de découverte de motifs. Premièrement, la taille des représentations condensées est souvent utilisée comme une mesure objective d\u0027évaluation de leur intérêt (Calders et al., 2004). Par exemple, une représentation condensée fondée sur les motifs fermés est toujours plus compacte qu\u0027une représentation condensée fondée sur les motifs libres. Les motifs fermés sont donc jugés comme plus intéressants. Cependant, les représentations condensées les plus compactes ne sont pas forcément les plus utilisées. Par exemple, les itemsets non-dérivables (NDI) sont rarement utilisés malgré leur taux de compression impressionnant. La sémantique complexe des NDI expliquerait cette impopularité pour certaines personnes.  (2010) les ont étendus aux motifs ensemblistes. A notre connaissance, de tels axiomes n\u0027ont jamais été appliqués à des contraintes ou des algorithmes de construction de modèles. De plus, ils se sont essentiellement concentrés sur les mesures dédiées à la recherche de corrélations. Comment généraliser ces axiomes à n\u0027importe quelle mesure d\u0027intérêt ou contrainte ?\nEnfin, à notre connaissance seuls deux travaux ont étudié l\u0027interrelation entre les motifs lors de l\u0027évaluation d\u0027une mesure d\u0027intérêt. Crémilleux et Soulet (2008) ont défini informellement la notion de contrainte globale. Il s\u0027agit de contraintes dont l\u0027évaluation met en correspondance plusieurs motifs. Giacometti et al. (2011) ont ensuite formalisé cette notion de contrainte globale en utilisant une algèbre relationnelle étendue spécifiquement pour la découverte de motifs. Notre cadre propose une définition formelle plus générale et plus précise, mais surtout permet de mieux analyser l\u0027interrelation entre les motifs lors de l\u0027évaluation. En particulier, nous ré-pondrons aux deux questions énoncées ci-avant.\n3 Formalisation de l\u0027interrelation entre motifs Fu et al., 2000) Bouncer and Picker Algorithme de sélection avec différentes heuristiques (Bringmann et Zimmermann, 2009) TAB. 1 -Définition de plusieurs méthodes de découverte de motifs fondées sur la fréquence\nMéthode de découverte de motifs\nDans cet article, nous modélisons une méthode de découverte de motifs par une mesure\n-le langage L correspond à l\u0027ensemble des parties de I (i.e., L \u003d 2 I ) 1 et -? est l\u0027ensemble de tous les jeux de données possibles sachant qu\u0027un jeu de données est un multi-ensemble de L.\nSi le jeu de données visé est clair, M (X, D) est simplement noté M (X). Sans perte de géné-ralité, nous considérons que l\u0027intérêt d\u0027un motif X augmente avec M (X) i.e., si le motif X est plus intéressant que\nNotre modélisation d\u0027une méthode de découverte de motifs par une mesure d\u0027intérêt M est suffisamment souple pour englober les principaux outils de la littérature :\n-Evaluation par mesure d\u0027intérêt : l\u0027évaluation par mesure d\u0027intérêt consiste à affecter un score ou un rang à chaque motif reflétant sa qualité (e.g., la all-confidence proposée par Omiecinski (2003)). -Extraction sous contraintes : l\u0027extraction sous contraintes consiste à extraire tous les motifs satisfaisant un prédicat de sélection qui détermine la pertinence d\u0027un motif. Par exemple, il est courant d\u0027utiliser un seuil minimal sur une mesure d\u0027intérêt pour filtrer les motifs (e.g., la contrainte de support minimal (Agrawal et Srikant, 1994) \nNotion de motifs liés\nLa qualité d\u0027une méthode de découverte de motifs repose sur sa capacité à isoler un motif singulier qui dévie des autres motifs communs. Pour cette raison, cette méthode doit mettre en relation le motif évalué avec d\u0027autres motifs, dits liés. Pour analyser cette interrelation entre motifs, nous proposons de déterminer son ensemble de motifs liés en identifiant l\u0027impact de chacun de ces motifs sur le motif évalué. L\u0027impact d\u0027un motif Y sur le motif évalué X peut se mesurer en observant s\u0027il existe deux jeux de données D et D quasi-équivalents où la seule variation de Y modifie l\u0027évaluation de X. Typiquement, la all-confidence d\u0027un itemset X introduite par Omiecinski (2003) correspond à la plus petite confiance des règles d\u0027association Y ? Z incluses dans X. Il est bien connu que la all-confidence peut être réécrite comme le ratio entre la fréquence de X et la fréquence maximale de ses items : f req(X)/ max i?X f req({i}). Dans ce cas, quand X est évalué, les motifs liés de X sont luimême et tous ses items. En effet, augmenter la fréquence d\u0027un item peut faire décroître la allconfidence de X. A l\u0027inverse, augmenter la fréquence de X augmente aussi sa all-confidence. Cet exemple conduit à deux observations d\u0027importance :\n1. Il y a deux catégories de motifs liés : ceux qui peuvent améliorer la qualité du motif évalué (ici, X) et ceux qui peuvent la détériorer (ici, les items qui constituent X).\n2. Les motifs liés impactent la mesure d\u0027intérêt qu\u0027on analyse (ici, la all-confidence) via une autre mesure d\u0027intérêt élémentaire (ici, la fréquence).\nSuivant ces deux observations, nous définissions formellement la notion de motif lié en nous appuyant sur la définition d\u0027équivalence avec exception :  Illustrons la définition 2 avec la all-confidence. Comme la all-confidence ne peut croître qu\u0027avec la fréquence de X, all-conf + f req (X) est égal à {X}. Nous verrons qu\u0027il est courant voire souhaitable que le motif évalué soit aussi un motif lié. Pour l\u0027ensemble des motifs liés négativement, nous obtenons que all-conf ? f req (X) \u003d {{i}|i ? X} car seuls l\u0027augmentation de la fréquence d\u0027un item de X peut faire diminuer all-conf (X). Le tableau 2 donne d\u0027autres exemples d\u0027ensembles de motifs liés. Une force de la notion de motifs liés est de bien identifier les motifs « réellement » impliqués dans l\u0027évaluation. Par exemple, la définition des motifs libres donnée dans le tableau 1 implique tous les sous-ensembles de X (avec le ?Y ? X). Pourtant, seuls les sous-ensembles directs sont des motifs liés.\nDans le tableau 2, les mesures, contraintes, algorithme de construction de modèles choisis reposent exclusivement sur la fréquence (comme mesure m qui implique les motifs liés). Il est à noter que les définitions de motifs libres/fermés auraient pu être présentées avec d\u0027autres mesures élémentaires (e.g., fréquence disjonctive ou fonction d\u0027agrégat). Les bordures néga-tive et positive pourraient être analysées avec d\u0027autres mesures suivant la contrainte monotone ou anti-monotone considérée (Mannila et Toivonen, 1997). De même, la notion de top-k motif est pertinente avec d\u0027autres mesures d\u0027intérêt que la fréquence. En changeant la mesure élé-mentaire m introduite dans la définition 2, l\u0027analyse des interrelations entre motifs se ferait de manière analogue.\noù k \u003d |X| et n \u003d |I| ; singletons correspond à {{i}|i ? X} ; sous-ensembles directs correspond à {X \\ {i}|i ? X} ; sur-ensembles directs correspond à {X ? {i}|i ? I \\ X} ; sous-ensembles correspond à 2 X \\ {X} ; treillis correspond à L \\ {X}.\nTAB. 2 -Analyse des méthodes suivant leurs ensembles de motifs liés\nAxiomes de qualité\nEn s\u0027inspirant de ce qui a été fait pour les mesures d\u0027intérêt dédiées aux règles d\u0027association, cette section énonce trois axiomes que devraient satisfaire une mesure ou une contrainte idéale. Le tableau 2 illustre la satisfaction ou non des axiomes par les différentes mesures et contraintes.\nRéflexivité\nRevenons sur l\u0027exemple de la all-confidence. Nous avons constaté que l\u0027augmentation de la fréquence de certains motifs avait un impact sur la all-confidence de X. Donc la fréquence est une mesure élémentaire d\u0027importance pour la all-confidence. Par ailleurs, il est souvent considéré que l\u0027intérêt d\u0027un motif augmente avec sa fréquence. Il paraît donc naturel qu\u0027augmenter la fréquence de X augmente également la all-confidence de X. Plus généralement, si l\u0027intérêt d\u0027un motif X augmente avec la mesure élémentaire m, l\u0027intérêt de ce motif X selon la mesure d\u0027intérêt M devrait également augmenter lorsque m(X) croît. Comme all-conf + f req (X) \u003d {X}, la all-confidence est bien une mesure réflexive par rapport à la fréquence. A l\u0027inverse, l\u0027extraction des motifs libres n\u0027est pas réflexive par rapport à la fréquence puisque f ree + f req (X) \u003d {X \\ {i}|i ? X} n\u0027inclut pas X. Tandis qu\u0027un motif est jugé plus intéressant quand sa fréquence augmente, il a moins de chance d\u0027être libre (car sa fréquence sera plus proche de celle de ses sous-ensembles). Par conséquent, la contrainte de liberté pourrait même être qualifiée d\u0027irréflexive par rapport à la fréquence (i.e., X ? f ree ? f req (X)). Nous reviendrons dessus dans la sous-section suivante. L\u0027axiome 1 est une généralisation de plusieurs propositions de la littérature où la mesure m est restreinte au support. Pour les règles d\u0027associations, Piatetsky-Shapiro (1991)  \nExclusivité\nPour être facilement compréhensible par l\u0027utilisateur final, le comportement d\u0027une mesure M doit toujours rester le même vis-à-vis de chaque motif lié. En d\u0027autres termes, un motif lié ne devrait pas permettre d\u0027augmenter la mesure M dans certains cas, et de la diminuer dans d\u0027autres.\nAxiome 2 (Exclusive) Une mesure d\u0027intérêt M est exclusive par rapport à m ssi aucun motif est à la fois lié positivement et négativement à un motif donné pour m :\nCet axiome est largement vérifié par les méthodes de la littérature comme le montre le tableau 2 (colonne A2). Par exemple, comme all-conf + f req (X) ? all-conf ? f req (X) \u003d ?, la allconfidence est exclusive par rapport à la fréquence. A l\u0027inverse, l\u0027extraction des motifs libres fréquents n\u0027est pas exclusive puisque X appartient à la fois à f ree\u0026f req + f req (X) à cause de la contrainte de fréquence et à f ree\u0026f req ? f req (X) à cause de la contrainte de liberté. Le non-respect de l\u0027axiome 2 complexifie la lecture d\u0027une méthode d\u0027extraction puisqu\u0027il devient nécessaire de se reporter au jeu de données ou à d\u0027autres motifs pour comprendre les motifs extraits. Par exemple, lors de l\u0027extraction des motifs libres fréquents, un motif X peut ne pas être extrait soit si sa fréquence est trop basse, soit si sa fréquence est trop élevée. A l\u0027inverse, pour les motifs fermés fréquents, un motif n\u0027est pas extrait si sa fréquence est trop faible (aussi bien si le motif est non-fréquent ou non-fermé). Nous estimons donc que la violation de l\u0027axiome 2 pourrait expliquer en partie l\u0027échec des motifs NDI même s\u0027ils constituent une représentation condensée extrêmement compacte.\nLa combinaison des axiomes 1 et 2 (notée A1+2 dans le tableau 2) implique naturellement que X ? M ? m (X). Si une mesure M viole cette propriété, M est dite irréflexive selon m. L\u0027extraction des motifs libres et celle de la bordure négative des motifs fréquents sont irréflexives.\nExhaustivité\nLa pertinence d\u0027un motif est d\u0027autant plus forte que son intérêt dépend de la variation de nombreux autres motifs. Pour cette raison, tous les motifs du langage L devraient avoir un impact sur la mesure M . En d\u0027autres termes, l\u0027ensemble des motifs liés devrait idéalement être égal à la totalité du langage L.\nAxiome 3 (Exhaustive) Une mesure d\u0027intérêt M est exhaustive par rapport à m ssi tous les motifs du langage sont liés à tout motif pour m :\nCet axiome exprime que l\u0027interrelation entre les motifs lors de l\u0027évaluation d\u0027une mesure M devrait concerner tous les motifs. Chaque variation du jeu de données mesurable à travers m(X) devrait avoir une incidence sur l\u0027évaluation de M . Bien sûr, la plupart des méthodes d\u0027extraction (mesures ou contraintes de la littérature) ne satisfont pas cet axiome. Cependant, nous pensons que cet axiome donne la direction à suivre et nous revenons longuement dans la section suivante sur la forme de l\u0027ensemble de motifs liés.\nComplexité et sémantique\nEn pratique, l\u0027axiome 3 est peu vérifié. Néanmoins, les méthodes d\u0027extraction proposées tendent plus ou moins à le satisfaire. Cette section propose d\u0027étudier deux grandes caractéris-tiques de l\u0027ensemble des motifs liés à savoir sa taille et sa forme.\nComplexité en évaluation\nSuivant l\u0027axiome 3, nous affirmons que la pertinence d\u0027un motif pour une mesure élémen-taire m est encore plus forte lorsque sa pertinence dépend de la variation de la pertinence de nombreux autres motifs selon m. Par conséquent, l\u0027intérêt d\u0027une mesure M (au sens de sa globalité) selon m se mesure avec la taille de l\u0027ensemble de motifs liés : De manière similaire, on peut déterminer que la complexité en évalua-tion de la productivité est exponentielle par rapport à la taille du motif évalué puisque tous les sous-ensembles sont impliqués dans l\u0027évaluation de cette contrainte. Suivant la complexité en évaluation, on dira donc que la productivité est plus intéressante (car plus globale) que la all-confidence car les interrelations sont plus nombreuses.\nA notre connaissance, la complexité en évaluation est le premier indicateur pour mesurer l\u0027interrelation entre les motifs lors de l\u0027évaluation d\u0027une mesure d\u0027intérêt. Cette complexité permet de comparer plusieurs mesures d\u0027intérêt entre elles. La colonne |M ± f req (X)| du tableau 2 indique la complexité en évaluation des mesures et contraintes définies dans le tableau 1. Il se dégage clairement 3 grandes classes correspondant à 3 complexités en évaluation : constant, linéaire et exponentiel.\nBien que le tableau 1 ne comporte qu\u0027un échantillon restreint de la découverte de motifs, la complexité en évaluation des méthodes d\u0027extraction de motifs semble avoir augmenté durant ces deux dernières décennies. Au-delà de l\u0027intérêt des motifs extraits, nous pensons que la complexité en évaluation reflète aussi la difficulté algorithmique à les extraire. Ainsi, l\u0027amé-lioration des techniques d\u0027extraction pourrait expliquer cette augmentation de la qualité des motifs extraits.\nTAB. 3 -Liens entre l\u0027ensemble des motifs liés, la sémantique et la complexité\nSémantique de l\u0027ensemble de motifs liés\nContrainte globale Le tableau 3 schématise les principales topologies observées notamment au sein du tableau 2 en les organisant en trois grandes classes de complexité évoquées dans la section précédente. Ces classes font écho à la notion de contrainte globale introduite par Crémilleux et Soulet (2008) puis définie formellement par Giacometti et al. (2011). Il s\u0027agit des prédicats de sélection dont la complexité est au moins linéaire :\nPropriété 1 (Contrainte globale) Une contrainte q : L ? {1, 0} est globale ssi il existe une mesure élémentaire m telle que la complexité en évaluation de q selon m est au moins linéaire. Propriété 2 (Webb et Vreeken (2013); Hämäläinen et al. (2010)) Une mesure d\u0027intérêt M se comportant bien doit vérifier ?X ? L : 2 X \\ {X} ? M ? f req (X) Non-redondance Toutes les méthodes de découverte de motifs visant à réduire les redondances exploitent les sous-et/ou sur-ensembles du motif évalué. La majorité des représenta-tions condensées s\u0027appuient exclusivement sur les sous-ensembles ou sur-ensembles directs. Modèle Nous avons constaté que tous les algorithmes de construction de modèles ont leurs motifs liés qui couvrent l\u0027intégralité du treillis comme c\u0027est le cas pour Bouncer and Picker (Bringmann et Zimmermann, 2009). Les modèles sont souvent vus comme une amélioration des représentations condensées. La complexité en évaluation confirme que les modèles sont plus intéressants que les représentations condensées. La complexité des motifs top-k fréquents se rapproche de celle des modèles sans toutefois l\u0027atteindre.\n2. Dans ce contexte, « un bon comportement » signifie que les corrélations doivent être évaluées plus favorablement que les non-corrélations.\nConclusion\nCet article a introduit la notion de motifs liés qui nous semble centrale pour analyser l\u0027interrelation des motifs pour les méthodes de découverte de motifs. Une force de notre approche est son large spectre d\u0027application qui va au-delà des mesures d\u0027intérêt pour traiter aussi bien l\u0027extraction sous contraintes que la construction de modèles. Pour la première fois, des axiomes de qualité concernent la problématique de la non-redondance. L\u0027introduction de la complexité en évaluation permet de dépasser le stade qualitatif pour mieux comparer plusieurs méthodes.\nPlusieurs axes de progression subsistent au sein de notre cadre. La définition actuelle des motifs liés repose sur une notion d\u0027équivalence entre jeux de données où seule une mesure élé-mentaire m est impliquée dans l\u0027évaluation de M ; comment tenir compte qu\u0027une autre mesure m peut potentiellement impacter M en parallèle ? Une réflexion sur la définition de mesure élémentaire et des interrelations entre mesures élémentaires est nécessaire pour répondre à cette question. Par ailleurs, d\u0027autres axiomes importants sur les mesures d\u0027intérêts mériteraient d\u0027être étendus en s\u0027appuyant sur la notion de motifs liés. Bien que l\u0027incidence de la mesure élémentaire m dans l\u0027évaluation de M soit cruciale sur la sémantique des motifs liés, nous n\u0027avons pas encore étudié les implications des propriétés de m sur celles de M .\n"
  },
  {
    "id": "274",
    "text": "Introduction et contexte\nLe problème de la prédiction de liens tel qu\u0027il est formulé dans l\u0027article de Liben-Nowell et al. (2007) peut être compris comme une tâche de classification binaire. Des outils classiques d\u0027apprentissage tels que les arbres de classification, les SVM ou les réseaux de neurones ont été utilisés pour le résoudre sur des réseaux biologiques et de collaboration (Pujari et al. (2012)). Cependant, ces méthodes ne permettent pas à l\u0027utilisateur de faire varier le nombre de prédic-tions selon ses besoins. Pour ce faire, il est possible de calculer un score pour chaque paire de noeuds, corrélé à la probabilité d\u0027existence d\u0027un lien entre ces noeuds, on obtient alors un classement, et l\u0027utilisateur effectue la prédiction en sélectionnant les T paires les mieux classées. Le score peut être basé sur la structure connue du réseau, mais également sur d\u0027autres sources d\u0027information : par exemple les attributs des noeuds, la dynamique des contacts ou la localisation géographique (Scellato et al. (2011)). Pour combiner les informations capturées par différents scores, on utilise des méthodologies d\u0027apprentissage de classements. Parmi les méthodes à disposition, certaines sont non-supervisées et peuvent être vues comme des mé-thodes de consensus, telles que celles décrites dans Dwork et al. (2001). Il existe également des méthodes supervisées : une solution consiste à se ramener à un problème de classification en effectuant une transformation deux-à-deux, plutôt que de considérer des éléments à ordonner, on examine des couples d\u0027éléments dont on cherche à dire lequel doit être classé audessus de l\u0027autre (Herbrich et al. (1999)). Malheureusement, cette méthode n\u0027est pas adaptée à de grands réseaux où le nombre d\u0027éléments à classer est élevé. Plus généralement, la plupart des méthodes d\u0027apprentissage de classements ont été créées pour des tâches de recherche d\u0027information, où l\u0027on souhaite une précision élevée sur un petit nombre d\u0027éléments (e.g., Burges et al. (2011)). Nous souhaitons ici au contraire pouvoir fixer le nombre de prédictions, quitte à perdre en précision, pour prédire des liens non-observés dans de grands réseaux sociaux, et nous définissons dans ce but une méthodologie simple mais efficace d\u0027apprentissage supervisé. 2 Classements non-supervisés Scores de classement. Le but de ce travail est de montrer comment notre méthode permet de combiner des informations issues de différentes sources, nous ne présentons ici que quelques caractéristiques structurelles permettant d\u0027associer à chaque paire de noeuds un score à partir duquel est construit un classement. On trouve dans la littérature beaucoup d\u0027indices, nous en avons choisi quelques classiques et proposons de les généraliser à des réseaux pondérés. Certaines de ces caractéristiques sont dites locales car elles ne considèrent que des paires de noeuds à distance au plus 2. On appelle N (i) l\u0027ensemble des voisins du sommet i : -nombre de voisins communs (CN) :\nDonnées\nD\u0027autres caractéristiques sont dites globales car elles sont calculées sur l\u0027ensemble de la structure du réseau et permettent de classer des paires de noeuds distantes :\n-indice de Katz (Katz) : calculé à l\u0027aide du nombre de chemins de longueur l de i à j (au sens d\u0027un multigraphe pour un réseau pondéré), noté ? ij (l), selon l\u0027expression : p.w(k, k )/W (k) et revenant en i avec une probabilité 1 ? p, l\u0027indice est la probabilité pour que ce marcheur soit en j dans l\u0027état stationnaire de la marche.\n-attachement préférentiel (PA), basé sur l\u0027observation dans les réseaux sociaux que les noeuds de fort degré tendent à créer plus de nouveaux liens : s PAw (i, j) \u003d W (i).W (j) 2 . Enfin nous utilisons une méthode pour agréger des classements dans le but d\u0027améliorer la qualité de la prédiction. Il s\u0027agit de la méthode de Borda, initialement définie pour obtenir un consensus dans un système de vote. On affecte à chaque paire un score correspondant à la somme sur l\u0027ensemble des classements des nombres de paires moins bien classées, soit :\nRésultats. Certaines métriques usuelles d\u0027évaluation des problèmes de classification, comme la courbe ROC, ne sont pas adaptées au problème, en raison de la forte asymétrie des classes. En effet, les graphes étant peu denses, on s\u0027attend à ce que le taux de faux positif soit élevé dans une large gamme de prédictions et rende l\u0027observation de la courbe ROC peu instructive. Comme nous souhaitons pouvoir ajuster le nombre de prédictions pour trouver un bon compromis entre précision (Pr) et rappel (Rc), nous visualisons les résultats à l\u0027aide du F-score et des courbes Pr-Rc.\nNous traçons sur la Figure 1 les résultats obtenus sur le graphe d\u0027apprentissage G learn pour prédire les liens A 2 ? A 2 en utilisant certains des scores définis précédemment (pas la totalité pour une question de lisibilité). On voit que l\u0027allure des courbes de F-score varient significativement d\u0027un indice à un autre, ce qui indique que chaque score permet de détecter des liens différents, ce dont nous chercherons à tirer parti dans RankMerging. Comme on peut s\u0027y attendre pour une méthode de consensus, la méthode de Borda améliore les performances de la classification, en particulier la précision sur la prédiction des paires les mieux classées. Étant donnée la difficulté de la tâche, la précision est peu élevée en moyenne : lorsque Rc est plus grand que 0.06, Pr est inférieur à 0.3 pour toutes les méthodes. Nous n\u0027utilisons que des scores structurels, rendant improbable la prédiction de liens entre paires de sommets éloignées dans le graphe, le rappel est donc limité à des valeurs relativement faibles, car augmenter le nombre de prédictions diminuerait drastiquement la précision.\nMéthode RankMerging\nLes différentes méthodes de classements précédemment citées ne sont pas sensibles aux mêmes types d\u0027information structurelle. Nous décrivons une méthode générique d\u0027apprentissage supervisé qui permet d\u0027agréger les classements en tirant partie de la complémentarité des sources d\u0027information. Celle-ci ne nécessite pas qu\u0027une paire soit bien classée selon chaque critère, comme pour une règle de consensus, mais qu\u0027elle soit bien classée selon au moins un. La procédure est dénommée RankMerging, une implémentation et un guide utilisateur sont à disposition sur http://lioneltabourier.fr/program.html.\n2. Katz et RWR sont calculés à l\u0027aide de sommes infinies, que nous approximons à l\u0027aide des quatre premiers termes afin de réduire le coût du calcul. La forte asymétrie des classes tend à diminuer les performances de l\u0027indice PA, nous limitons la prédiction avec cet indice aux paires de sommets distantes d\u0027au plus 3.\n3. Pour que cette méthode ne biaise pas en faveur des prédicteurs qui classent un grand nombre d\u0027éléments, on considère qu\u0027une paire classée dans r? mais pas dans r ? est aussi classée dans r ? , avec un rang équivalent à toutes les autres paires non-classées et en-dessous de toute paire classée. Pour plus de détails, voir Dwork et al. (2001). L\u0027idée centrale est de déterminer à chaque pas de l\u0027algorithme le classement qui prédit le nombre le plus élevé de tp dans les prochaines étapes. Dans ce but nous définissons une fenêtre W i pour chaque classement : c\u0027est l\u0027ensemble des g liens non-prédits classés à partir du rang ? i dans r i 4 . On appelle qualité ? i le nombre de tp dans W i . À chaque étape, le classement r i dont la fenêtre a la qualité la plus élevée est sélectionné (en cas d\u0027égalité, on choisit aléatoirement), et on ajoute alors la paire classée au rang ? i de r i au classement de sortie de la phase d\u0027apprentissage (r L M ). On met à jour ? i et le curseur de fin de la fenêtre, de manière à ce que chaque W i contienne toujours exactement g paires, puis on met à jour les qualités ? i . Au cours du processus, on enregistre à chaque pas les valeurs des curseurs ? 1 ...? ? , qui indiquent la contribution de chaque classement au classement agrégé. Cette procédure est itérée jusqu\u0027à ce que le classement agrégé r L M contienne le nombre de paires T fixé par l\u0027utilisateur. Cet algorithme présente l\u0027intérêt majeur de ne parcourir qu\u0027une seule fois chaque classement, soit une complexité en O(?N ) pour ? classements de taille N (on note que N ? T ).\nLa phase de test consiste à agréger les classements obtenus sur le réseau G test en utilisant les paramètres ? 1 ...? ? appris durant la première phase. L\u0027implémentation pratique est simple : à chaque étape on regarde quel classement a été choisi à l\u0027étape correspondante de l\u0027apprentissage, et on sélectionne la paire la mieux classée du classement correspondant pour le graphe de test. Si celle-ci n\u0027est pas déjà dans le classement agrégé de la phase de test (r Protocole expérimental et résultats. Nous évaluons les performances de RankMerging en les comparant à des techniques existantes. En premier lieu, nous confrontons les résultats à la méthode non-supervisée de Borda, vue précédemment. Nous comparons aussi à des méthodes de classification supervisées, même si celles-ci ne sont pas conçues pour varier le nombre T de prédictions possibles 6 . Nous utilisons les implémentations proposées dans la boîte à outils Python scikit learn (scikit-learn.org) des méthodes suivantes : les k-plus proches voisins (NN), les arbres de classification (CT) et AdaBoost (AB), en faisant varier les paramètres de manière à obtenir plusieurs points dans l\u0027espace Pr-Rc 7 . Suivant la description de la méthode faite précédemment, nous mesurons les ? i obtenus sur G learn pour découvrir les liens A 2 ? A 2 , puis nous les utilisons pour agréger les classements obtenus sur G test pour prédire les liens B ? B, en utilisant le facteur d\u0027échelle f ? 1.5. La sélection des caractéristiques pertinentes n\u0027est pas problématique ici. En effet, l\u0027utilisateur peut agréger autant de classements qu\u0027il le souhaite car, d\u0027une part, l\u0027addition d\u0027un nouveau classement a un faible coût computationnel, et d\u0027autre part, la méthode est conçue pour qu\u0027un classement n\u0027apportant pas d\u0027information nouvelle soit simplement ignoré pendant l\u0027agréga-tion. Le paramètre g de l\u0027algorithme est ici fixé par une simple extrapolation : on mesure la valeur de g qui permet de maximiser la qualité de l\u0027agrégation sur G learn , et on emploie la même pour l\u0027agrégation sur G test (ici g \u003d 200). Sur la Figure 2, nous traçons le F-score et la courbe précision-rappel obtenus pour RankMerging (g \u003d 200), en agrégeant les classements des indices suivants : AA w , CN w , CN , Jacc w , Katz w (? \u003d 0.01), PA w , RWR w (p \u003d 0.8) et la méthode de Borda appliquée à ces sept indices. RankMerging permet d\u0027améliorer les prédictions : la mesure de l\u0027aire sous la courbe Pr-Rc indique une amélioration de 8.3% par rapport à la méthode de Borda 8 . On pouvait s\u0027y attendre, dans la mesure où RankMerging est une méthode supervisée et utilise l\u0027information contenue dans le consensus de Borda. En fait, la méthode est conçue pour que n\u0027importe quel classement non-supervisé puisse être agrégé sans perte de performance. Nous vérifions cela en pratique en retirant un à un les classements et en constatant que l\u0027amélioration ne fait que décroître, et ce quel que soit l\u0027ordre dans lequel on retire les différents classements 9 . En ce qui concerne les méthodes de classifications supervisées, nous pouvons constater que leurs performances sont élevées uniquement pour un faible nombre de prédictions (inférieur à 2000), mais ces méthodes n\u0027étant pas conçues pour faire varier le nombre de prédictions, elles produisent de très faibles performances en dehors de leur domaine optimal.\n"
  },
  {
    "id": "275",
    "text": "Introduction\nLe problème de prédiction de séquences est un problème important en fouille de données, défini de la façon suivante. Soit un alphabet Z \u003d {e 1 , e 2 , ..., e m } contenant un ensemble d\u0027éléments (symboles). Une séquence est une suite d\u0027éléments totalement ordonnée s \u003d 1 , i 2 , ...i n où i k ? Z (1 ? k ? n). Un modèle de prédiction M est un modèle entraîné avec un ensemble de séquences d\u0027entraînement. Une fois entraîné, le modèle peut être utilisé pour effectuer des prédictions. Une prédiction consiste, à prédire le prochain élément i n+1 d\u0027une séquence 1 , i 2 , ...i n en utilisant le modèle M . La prédiction de séquences a des applications importantes dans une multitude de domaines tels que le préchargement de pages Web (Deshpande et Karypis, 2004;Padmanabhan et Mogul, 1996), la recommandation de produits de consommation, la prévision météorologique et la prédiction des tendances du marché boursier.\nUn grand nombre de modèles de prédictions ont été proposés pour la prédiction de sé-quences. Un des modèle les plus connus est PPM (Prediction by Partial Matching) (Cleary et Witten, 1984). Ce modèle, basé sur la propriété de Markov, a engendré une multitude d\u0027approches dérivées telles que Dependancy Graph (DG) (Padmanabhan et Mogul, 1996), All-korder-Markov (Pitkow et Pirolli, 1999) et Transition Directed Acyclic Graph (TDAG) (Laird et Saul, 1994). Bien que des propositions ont été faites pour réduire la complexité temporelle et spatiale de ces modèles (Begleiter et al., 2004), l\u0027exactitude de leurs prédictions a subi peu d\u0027amélioration. D\u0027autre part, un certain nombre d\u0027algorithmes de compression ont été adaptés pour la prédiction de séquences tels que LZ78 (Ziv et Lempel, 1978) et Active Lezi (Gopalratnam et Cook, 2007). De plus, des algorithmes d\u0027apprentissage machine comme les réseaux de neurones et la découverte de règles d\u0027association séquentielles ont été employés pour faire de la prédiction de séquences (Fournier-Viger et al., 2012;Sun et Giles, 2001). Néanmoins, ces modèles souffrent de limites importantes. Premièrement, la plupart d\u0027entre eux partent de l\u0027hypothèse Markovienne qu\u0027un événement ne dépend que de son prédécesseur. Or, ce n\u0027est pas le cas pour de nombreuses applications, ce qui nuit à l\u0027exactitude des prédictions. Deuxièmement, tous ces modèles sont construits avec perte d\u0027information par rapport aux séquences d\u0027entraî-nement. Donc, ils n\u0027utilisent pas toute l\u0027information disponible dans les séquences d\u0027entraîne-ment pour effectuer les prédictions.\nPour pallier ces limites, un modèle nommé Compact Prediction Tree (CPT) (Gueniche et al., 2013) a été récemment proposé. Il utilise une structure en arbre pour compresser les sé-quences d\u0027entraînement sans perte ou avec une perte minime d\u0027information. De plus, il emploie un algorithme de prédiction conçu pour tenir compte du bruit et de plusieurs événements anté-rieurs lors d\u0027une prédiction plutôt que seulement le dernier. Il a été montré que ce modèle peut obtenir des prédictions jusqu\u0027à 12 % plus exactes que PPM, DG et All-K-order-markov sur des jeux de données provenant de divers domaines, ce qui constitue un gain important. Néanmoins, une limite de CPT est sa complexité temporelle et spatiale élevée. Dans cet article, nous pallions ces problèmes en proposant trois stratégies pour réduire la taille et le temps de prédiction de CPT. De plus, nous présentons une comparaison expérimentale avec davantage de modèles de prédiction de la littérature : All-K-order Markov, DG, Lz78, PPM et TDAG. Les résultats expérimentaux sur 7 jeux de données réels montrent que le modèle résultant nommé CPT+ est jusqu\u0027à 98 fois plus compact et est jusqu\u0027à 4.5 fois plus rapide que CPT. De plus, CPT+ conserve une exactitude très élevée par rapport aux autres approches de la littérature.\nLe reste de cet article est organisé de la façon suivante. La section 2 décrit brièvement le modèle CPT. Les sections 3 et 4 proposent respectivement de nouvelles stratégies pour réduire la taille du modèle CPT et ses temps de prédiction. La section 5 présente l\u0027évaluation expérimentale avec plusieurs jeux de données et les principaux modèles de prédictions de la littérature. Finalement, la section 6 est dédiée à la conclusion et aux travaux futurs.\nLe processus d\u0027entraînement\nLe processus d\u0027entraînement génère trois structures distinctes à partir des séquences d\u0027entraînement : (1) un Arbre de Prédiction (AP), (2) un Dictionnaire de Séquences (DS) et (3) un Index Inversé (II). Pendant l\u0027entraînement, les séquences sont considérées les unes après les autres pour construire incrémentalement ces trois structures. À titre d\u0027exemple, la figure 1 illustre la création des structures de CPT par insertion successive des séquences\n(1) Insertion de ?í µí±¨, í µí±©, í µí±ª? . Chacun des noeuds de l\u0027arbre représente un élément et chacune des séquences d\u0027entraînement est représentée par un chemin partant de la racine de l\u0027arbre et se terminant par un noeud interne de l\u0027arbre ou une feuille. La construction de cet arbre a une basse complexité. Insérer une séquence de m éléments demande de parcourir/créer au plus m noeuds. La construction complète de l\u0027arbre est O(n) où n est le nombre de séquences à insérer. Tout comme un arbre préfixe, cet arbre est une représentation compacte des séquences d\u0027entraînement, car les séquences partageant un préfixe commun partagent un chemin dans l\u0027arbre. Par exemple, à la figure 1, les séquences s 1 , s 2 et s 3 partagent le même chemin correspondant au préfixe B Dans le pire cas, le gain spatial offert par cette compression est nul, mais en pratique, tout dépendant de la densité et de la similarité des séquences du jeu de données utilisé, l\u0027arbre peut offrir une réduction spatiale très importante allant jusqu\u0027à 98% (Gueniche et al., 2013).\nLe Dictionnaire de Séquences est une structure qui permet d\u0027extraire chacune des sé-quences d\u0027entraînement de l\u0027arbre de prédiction. Lors de la construction du modèle CPT, un identifiant unique est assigné à chaque séquence. Il est égal à 1 pour la première séquence insérée (dénoté s 1 ) et est incrémenté d\u0027un pour chaque séquence subséquente (s 2 , s 3 , ...). Le dictionnaire de séquences associe chaque identifiant de séquence s a à un pointeur vers un noeud de l\u0027arbre. Ce noeud représente le dernier élément de la séquence s a dans l\u0027arbre. Grâce à cette structure, il est possible de parcourir chaque séquence d\u0027entraînement dans l\u0027arbre de prédiction du dernier au premier élément.\nL\u0027Index Inversé permet d\u0027identifier rapidement dans quelles séquences apparaît un ensemble d\u0027éléments d\u0027une séquence à prédire. L\u0027index inversé contient un vecteur de bits v e pour chaque élément e de l\u0027alphabet Z présent dans les séquences d\u0027entraînement. Le k-ième bit d\u0027un vecteur de bit v e prend la valeur 1 si l\u0027élément e apparaît dans la séquence s k , sinon il prend la valeur 0. Par exemple, à la figure 1, le vecteur de bit de l\u0027élément C après l\u0027insertion des séquences s 1 , s 2 , s 3 , s 4 et s 5 est 10110, car C apparaît dans les séquences s 1 , s 3 et s 4 . L\u0027index inversé est utilisé pour déterminer rapidement les séquences d\u0027entraînement contenant un ensemble d\u0027éléments d\u0027une séquence à prédire. Cela est réalisé en faisant l\u0027intersection des vecteurs de bits des éléments. Par exemple, déterminer l\u0027ensemble des séquences contenant les éléments A et C est réalisé par l\u0027opération 11101 ? 10110, donnant le résultat 10000, autrement dit {s 1 }. Grâce à l\u0027index inversé, cette tâche est très rapide ; O(i) où i est le nombre d\u0027éléments dans l\u0027ensemble.\nLe processus de prédiction\nLe processus de prédiction de CPT utilise les trois structures décrites précédemment. Soit une séquence s \u003d 1 , i 2 , ...i n de n éléments et y, un nombre entier représentant le nombre d\u0027éléments de s à considérer pour faire une prédiction. Le suffixe de taille y de s dénoté P y (s) est défini comme étant P y (s) \u003d n?x+1 , i n?x+2 ...i n La prédiction du prochain élément de s est effectuée de la façon suivante : CPT identifie tout d\u0027abord les séquences similaires à P y (s), c.à.d. qui contiennent les derniers y éléments de P y (s) dans n\u0027importe quel ordre et positions. Puis, pour chaque séquence similaire, CPT considère son conséquent. Le consé-quent d\u0027une séquence u est la sous-séquence débutant après le dernier élément en commun avec P y (s) jusqu\u0027à la fin de u. Chaque élément e dans un de ces conséquents est ensuite stocké dans une structure nommé Table de Compte (TC) avec son nombre d\u0027occurrences (ce nombre est une estimation de la probabilité P (e|P y (s))). L\u0027élément ayant le plus grand nombre d\u0027occurrences est l\u0027élément prédit par CPT. La mesure de similarité utilisée pour déterminer les séquences similaires est de nature stricte, mais est relâchée dynamiquement par le processus de prédiction, pour deux raisons. Premièrement, avec une mesure de similarité trop stricte, une séquence à prédire peut n\u0027être similaire à aucune séquence d\u0027entraînement, et donc aucune prédiction n\u0027est possible. Deuxièmement, une mesure de similarité trop stricte ne permet pas de considérer qu\u0027une séquence peut-être partiellement similaire à une autre. Or, dans les applications réelles, il y a souvent des éléments présents dans les séquences qui sont du bruit. Pour relâcher la mesure de similarité, CPT suppose qu\u0027un ou plusieurs éléments présents dans le suffixe de la séquence à prédire sont du bruit et qu\u0027ils peuvent être ignorés lors du calcul de similarité. Le calcul de similarité pour un suffixe P y (s) est fait par niveau, où à chaque niveau k \u003d 1, 2, ..., |P y (s)| ? 1 toutes les sous-séquences de taille |P y (s)| ? k de P y (s) sont générées. Chacune des sous-séquence u est utilisée pour trouver les séquences similaires dans l\u0027ensemble de séquences d\u0027entraînement et pour mettre à jour la TC. Ce relâchement de la mesure de similarité se poursuit pour la séquence à prédire d\u0027un niveau à l\u0027autre tant que TC n\u0027a pas été mise à jour un nombre minimum de fois.\nStratégies de compression de l\u0027arbre de prédiction\nBien que CPT offre des prédictions plus exactes que les principaux modèles de prédiction de la littérature selon une étude antérieure (Gueniche et al., 2013), une limite importante de CPT est sa complexité spatiale. Il a été montré que la taille des structures de CPT est inférieure à All-k-order Markov, mais demeure nettement supérieures à d\u0027autres modèles comme DG et PPM. L\u0027arbre de prédiction étant la structure la plus imposante de CPT, nous proposons ci-après deux stratégies pour réduire sa taille.\nStratégie 1 : Compressions des Chaînes Fréquentes (CCF). Certaines répétitions peuvent être identifiées dans les séquences d\u0027entraînement. Dépendamment du jeu de données, ces répé-titions peuvent être nombreuses et fréquentes. La compression des chaînes fréquentes consiste à identifier les sous-chaînes fréquentes d\u0027éléments apparaissant dans les séquences d\u0027entraîne-ment, puis à remplacer les sous-chaînes fréquentes par des éléments individuels.\nSoit une séquence s \u003d 1 , i 2 , ..., i n Une séquence c \u003d m+1 , j m+2 , ..., j m+k est une sous-chaine de s, dénoté c s, si et seulement si 1 ? m ? m + k ? n. Pour un ensemble de séquences d\u0027entraînement S, une sous-chaîne d est fréquente si |{t|t ? S ? d t}| \u003e minsup pour un seuil minsup fixé par l\u0027utilisateur.\nLa compression des chaînes fréquentes est effectuée pendant la phase d\u0027entraînement de CPT en trois étapes : (1) identifier les chaînes fréquentes dans l\u0027ensemble des séquences d\u0027entraînement, (2) créer un nouvel élément dans l\u0027alphabet Z pour représenter chaque sous-chaîne fréquente et (3) remplacer les sous-chaînes fréquentes par l\u0027élément correspondant lors de la construction de l\u0027arbre de prédiction de CPT. L\u0027identification de séquences fréquentes dans un ensemble de séquences est un problème populaire en fouille de données, pour lequel un grand nombre d\u0027algorithmes ont été proposés. Pour cette tâche, nous avons adapté un des algorithmes les plus performants nommé PrefixSpan (Pei et al., 2001), afin de ne découvrir que les séquences fréquentes d\u0027éléments consé-cutifs (sous-chaînes). De plus, nous avons ajouté la contrainte que les sous-chaînes fréquentes doivent respecter des contraintes de longueur minimale minSize et maximale maxSize (deux paramètres).\nLes sous-chaînes fréquentes identifiées sont stockées dans une nouvelle structure nommée Dictionnaire des chaînes fréquentes (DCF). Cette structure associe un nouvel élément non présent dans l\u0027alphabet Z (dans les séquences d\u0027entraînement) à chaque sous-chaîne fréquente. Le DCF permet de rapidement convertir une sous-chaîne en son élément correspondant et viceversa. Lors de l\u0027insertion des séquences d\u0027entraînement dans l\u0027arbre de prédiction, le DCF est utilisé pour remplacer chaque sous-chaîne par son élément correspondant.\nÀ titre d\u0027exemple, l\u0027illustration (1) de la figure 2 affiche la compression de l\u0027arbre de pré-diction de l\u0027illustration (5) de la figure 1 par la stratégie CCF. La sous chaîne fréquence B a été remplacée par un nouveau symbole x, réduisant le nombre de noeuds de l\u0027arbre de pré-diction.\nLa stratégie de compression de séquences CCF a un effet seulement sur l\u0027arbre de pré-diction où son nombre de noeuds et sa hauteur tendent à diminuer grandement. La stratégie CCF est transparente pour le processus de prédiction de CPT. En effet, lors de l\u0027extraction de séquences similaires, les branches de l\u0027arbre de prédiction sélectionnées sont décompressées à la volée par DCF. L\u0027identification et le remplacement de branches simples sont faits en un seul parcours de l\u0027arbre de prédiction. L\u0027index inversé et le dictionnaire de séquences n\u0027étant pas influencés par cette approche, le seul changement au processus de prédiction est la décompression dynamique des branches simples lorsque nécessaire. La complexité de ce remplacement est de O(n * (1 ? t)) où s est le nombre de séquence et t le taux de recouvrement de l\u0027arbre, ce dernier est défini comme le \"ratio\" de noeuds qui partagent plusieurs séquences par le nombre total de noeuds dans l\u0027arbre.\nStratégie de réduction des temps de prédiction\nStratégie 3 : Prédiction avec réduction du Bruit Amélioré (PBA). Tel qu\u0027expliqué pré-cédemment, pour prédire le prochain élément s n + 1 d\u0027une séquence s \u003d 1 , i 2 , ..., i n CPT utilise le suffixe de taille y de s dénoté P y (s) (les y derniers éléments de s), où y est un paramètre propre à chaque jeu de donnée. CPT prédit le prochain élément de s en parcourant les séquences similaires à son suffixe P y (s). La recherche de séquences similaires est rapide (O(y)). Toutefois, le mécanisme de réduction du bruit lors des prédictions (décrit à la section 2) ne l\u0027est pas, car il requiert de considérer non seulement P y (s) pour une prédiction, mais aussi toutes les sous-séquences de P y (s) de taille t \u003e k. Plus y et k sont grands, plus le nombre de sous-séquences à considérer l\u0027est aussi, et donc le temps de prédiction. Lors d\u0027une tâche de prédiction, certains éléments dans une séquence à prédire peuvent être considérés comme du bruit si leur simple présence affecte de façon négative le résultat de la prédiction. La stratégie PBA se base sur l\u0027hypothèse que le bruit observé dans une séquence est constitué des éléments ayant une faible fréquence, où la fréquence d\u0027un élément est le nombre de séquences d\u0027entraînement contenant l\u0027élément. Pour cette raison, PBA enlève seulement les éléments qui ont une faible fréquence pendant la phase de prédiction. Puisque la définition du bruit de CPT+ est plus restrictive que celle de CPT, un moins grand nombre de sous-séquences sont considé-rées. Cette réduction à un impact positif et tangible sur les temps de calculs tel que présentés dans notre évaluation expérimentale (section 5). Le pseudo-code illustrant la stratégie PBA est présenté ci-après (Algorithme 1). L\u0027algorithme prend en paramètres le préfixe P y (s) à prédire, les autres structures de CPT, un taux de bruit et un nombre minimum de mise à jour à faire à la TC pour faire une prédiction. Le taux de bruit représente le pourcentage d\u0027éléments dans une séquence qui doivent être considérés comme du bruit ; un taux de bruit de 0 indique que les séquence n\u0027ont pas de bruit alors qu\u0027un taux de bruit de 0.4 signifie que 40% des éléments d\u0027une séquence pourrait être du bruit. PBA est récursive de nature et considère un nombre minimal de sous-séquences dérivées de P y (s) pour faire une prédiction. Le bruit est d\u0027abord retiré de chaque sous-séquence. Puis la TC est mise à jour. Lorsque le nombre minimal de mise à jour est atteint, une prédiction est faite comme dans CPT en utilisant la TC. La stratégie PBA est une généralisation de la stratégie de réduction du bruit utilisée par CPT. En effet, selon les paramètres utilisés, il est possible de reproduire le fonctionnement original de CPT. Les trois contributions principales apportées par PBA sont l\u0027imposition d\u0027un nombre minimal de mise à jour de la TC pour faire une prédiction, la définition du bruit basé sur la fréquence d\u0027un élément et la réduction relative du bruit par rapport à la longueur de la séquence.\nAlgorithme 1 : L\u0027algorithme de prédiction avec PBA input : PS : le suffixe P s, CPT : les structures de CPT, TB : le taux de bruit output : Seq : un ou plusieurs éléments prédits file.ajouter(PS); while nombreMiseAjour \u003c minNombreMiseAJourTC ? file.nonVide() do suffixe \u003d file.prochain(); elementsBruit \u003d selectionnerElementsMoinsFrequents(TB); foreach elementBruit ? elementsBruit do suffixeSansBruit \u003d copierSuffixeSansBruit(suffixe, elementBruit); if suf f ixeSansBruit.length \u003e 1 then file.ajouter(suffixeSansBruit); end mettreAJourCountTable(CPT.countTable, suffixeSansBruit); nombreMiseAjour++; end retourne faireUnePrediction(CPT.countTable); end\nÉvaluation expérimentale\nNous avons effectué une série d\u0027expériences pour comparer la performance de CPT+, CPT et les principaux modèles de prédiction de la littérature All-K-order Markov, DG, Lz78, PPM et TDAG. Pour implémenter CPT+, nous avons obtenu et modifié le code source proposé dans l\u0027article original de CPT (Gueniche et al., 2013). Pour permettre la reproduction des expériences, le code source des modèles et jeux de données sont fournis à l\u0027adresse http: //goo.gl/LE4uYO. Tous les modèles sont implémentés en Java 8. Les expériences ont été réalisées sur une machine dotée d\u0027un processeur deux coeurs Intel i5 de 4ème génération avec 8 Go de mémoire vive et un SSD en SATA 600. Tous les modèles de prédiction utilisés ont été configurés empiriquement pour tenter de donner des valeurs optimales à chacun de leurs paramètres. PPM et LZ78 n\u0027ont pas de paramètres, DG et AKOM ont respectivement une fenêtre de 4 et un ordre de 5, finalement, par soucis d\u0027espace, TDAG à une hauteur maximale de 6. CPT à 4 paramètres et CPT+ en a 8, leurs valeurs sont elles aussi déterminées via une exploration expérimentale de l\u0027espace de valeurs possibles. Ces valeurs sont accessible dans les fichiers sources du projet. Les paramètres propres à l\u0027expérience se limitent à la longueur minimale et maximale des séquences utilisées, la taille du suffixe à considérer pour une séquence à prédire et la quantité d\u0027éléments à prédire pour chacune des séquences.\nDes jeux de données ayant des caractéristiques variées ont été utilisés (cf. Table 1) : sé-quences courtes/longues, séquences denses/éparses, petit/grands alphabets et divers types de données. Les jeux de données BMS, Kosarak, MSNBC et FIFA consistent en des séquences de pages Web visitées par des utilisateurs sur un site Web. Dans ce scénario, les modèles de pré-diction sont appliqués pour prédire la prochaine page Web que visitera chaque utilisateur. Le jeu de données SIGN est un ensemble de phrases exprimées en langage des signes, transcrites à partir de vidéos. Bible Word et Bible Char sont deux jeux de données qui proviennent de la Bible, livre religieux, le premier est l\u0027ensemble des phrases découpées en mots et le second est l\u0027ensemble des phrases découpées en caractères.\nPour l\u0027évaluation des prédictions des modèles, une prédiction est soit un succès, un échec, ou une abstention (si un modèle ne peut effectuer une prédiction). Deux mesures sont utilisées. La couverture est le nombre d\u0027abstentions divisé par le nombre de séquences à prédire. L\u0027exactitude (alias précision) le nombre de succès divisé par le nombre de séquences à prédire.\nNom\nNombre Expérience 1 : comparaisons des optimisations. Dans cette première expérience, nous avons tout d\u0027abord évalué les améliorations spatiales présentées à la section 3 en terme de taux de compression et de temps de calcul à l\u0027entraînement. Les autres mesures de performance telles que le temps de prédiction, la couverture et l\u0027exactitude ne sont pas affectées par la compression de l\u0027arbre de prédiction. Pour un arbre de prédiction A avec s noeuds avant compression et s2 noeuds après compression, le taux de compression tc a de A est défini comme tc \u003d 1 ? (s2/s), et est compris entre 0.0 et 1.0 non inclusivement. Plus la valeur est haute, plus la compression est importante. Les deux stratégies de compression sont évaluées d\u0027abord individuellement (dénotées CCF et CBS) puis en conjonction (dénoté CPT+). Toute compression permet d\u0027obtenir un gain spatial au prix d\u0027un coût temporel. La figure 3 présente cette relation pour chacune des stratégies de compression. Les résultats présentés à la figure 3 montrent que le taux de compression de l\u0027arbre varie selon le jeu de données de 58.90% à 98.65%. CCF offre un taux de compression moyen de 48.55% avec un faible écart type de 6.7%. alors que CBS à un taux de compression moyen de 77.87% avec un écart type beaucoup plus prononcé de 15.9%. L\u0027efficacité de CBS est dépen-dante au jeu de données ; dans le cas de MSNBC, qui est le jeu de données le moins affecté par les stratégies de compression, la faible cardinalité de son alphabet permet à MSNBC d\u0027être naturellement compressé grâce au fort recouvrement des branches de son arbre de prédiction. En effet, MSNBC ne possède que 17 éléments uniques et même si la taille moyenne des sé-quences ressemble à celle des autres jeux de données, la taille de son arbre avant compression est très petite. Le jeu de données où les stratégies de compression CCF et CBS sont les plus effectives est SIGN. SIGN a un très faible nombre de séquences, mais chacune d\u0027elle est très longue (en moyenne 93 éléments). Ces caractéristiques font en sorte que son arbre de prédic-tion a un faible taux de recouvrement et donc une importante partie de ses noeuds n\u0027ont qu\u0027un seul fils ; ce qui rend ce jeu de données un candidat idéal pour la stratégie CBS. CBS offre un taux de compression de 98.60 % pour SIGN.\nLa figure 3 présente également les temps d\u0027entraînement engendrés par les deux stratégies de compression de CPT, CBS et CCF. La mesure utilisée est un facteur multiplicatif du temps d\u0027entraînement. Par exemple, un facteur de x pour CBS signifie que CBS a eu une phase d\u0027entraînement x fois plus longue. Pour tous les jeux de données à l\u0027exception de SIGN, CBS est plus rapide que CCF. Il est intéressant d\u0027observer que le temps pris par la combinaison des deux stratégies de compression n\u0027est pas simplement une addition de leur coût d\u0027entraînement.\nCBS et CCF sont appliqués indépendamment à CPT et pourtant l\u0027utilisation de CBS réduit les temps de calcul de CCF grâce à une diminution du nombre de branches qui ont besoin d\u0027être compressées.\nNous avons également évalué le gain en temps de prédiction et l\u0027exactitude (précision) obtenue en appliquant la stratégie PBA. La figure 4 (gauche) illustre les temps de prédiction de CPT+ (avec CBA), et ceux de CPT. Les gains temporels sont importants pour la plupart des jeux de données notamment pour SIGN et MSNBC où les temps d\u0027entraînement sont jusqu\u0027à 4.5 fois moindres. Pour les jeux de données Bible Word et FIFA, les temps de prédiction sont plus élevés pour obtenir un gain en exactitude comme le montre la figure 4 (droite). L\u0027effet de CBA sur l\u0027exactitude des prédictions est positif pour tous les jeux de données sauf MSNBC. Cette amélioration s\u0027élève jusqu\u0027à 5.47% dans le cas de Bible Word. CBA se montre donc une stratégie effective pour à la fois réduire les temps de prédiction et augmenter l\u0027exactitude des prédictions. Expérience 2 : Mise à l\u0027échelle. Nous avons également comparé la complexité spatiale de CPT+ (avec ses deux stratégies de compression) avec celle de CPT et All-K-order Markov, DG, Lz78, PPM et TDAG, en termes de mise à l\u0027échelle par rapport au nombre de séquences. Les deux seuls jeux de données utilisés sont FIFA et Kosarak à cause de leur grand nombre de séquences (573,060 et 638,811 respectivement). L\u0027accroissement du nombre de séquences dans cette expérience est quadratique et s\u0027arrête à 128,000 séquences dû aux énormes temps de calcul requis pour réaliser chaque expérience. La figure 5 présente les résultats. Le taux de compression de CPT+ tend à baisser très légèrement avec l\u0027accroissement du nombre de sé-quences, ce phénomène est causé par un recouvrement de plus en plus important des branches dans l\u0027arbre de prédiction ; car la taille de l\u0027alphabet étant constante, plus de séquences sont utilisées et plus de branches s\u0027unifient. Les modèles DG et PPM ont une croissance linéaire, car ils sont basés sur la taille de l\u0027alphabet et indirectement sur le nombre de séquences d\u0027entraînement. Les autres modèles ont tous une croissance beaucoup plus importante que DG et PPM, notamment TDAG et LZ78.\nExpérience 3 : Comparaison avec les autres modèles de prédiction Dans l\u0027expérience 1, nous avons comparé l\u0027exactitude des prédictions de CPT+ avec celle de CPT afin d\u0027évaluer la contribution de la stratégie PBA. Dans cette expérience, nous effectuons une comparaison de l\u0027exactitude celle des autres principaux modèles de prédiction de la littérature All-K-order Markov, DG, Lz78, PPM et TDAG, sur les mêmes jeux de données. Il est à noter que nous ajoutons dans cette comparaison deux modèles de prédictions (Lz78 et TDAG) qui n\u0027ont pas été utilisés dans l\u0027article original proposant CPT. \nConclusion\nDans cet article, nous avons présenté trois stratégies pour réduire la taille et le temps de prédiction de CPT, nommées CCF (Compression des Chaînes Fréquences), CBS (Compression des Branches Simples) et PBA (Préduction avec réduction du Bruit Améliorée). Les résultats expérimentaux sur 7 jeux de données réels ont montré que le modèle résultant nommé CPT+ est jusqu\u0027à 98 fois plus compact que CPT, et que cette compression demeure lorsque le nombre de séquence augmente. En termes de temps d\u0027exécution, CPT+ s\u0027est montré jusqu\u0027à 4.5 fois plus rapide que CPT. Finalement, CPT+ s\u0027est montré comme étant le modèle offrant les pré-dictions généralement les plus exactes dans une comparaison avec les principaux modèles de la littérature CPT, All-K-order Markov, DG, Lz78, PPM et TDAG.\nComme travaux futurs, nous adapterons CPT+ pour la prédiction de séquences dans le contexte d\u0027un flux infini de séquences. CPT+, de par sa nature incrémentale, pourrait être adapté à ce problème.\n"
  },
  {
    "id": "276",
    "text": "Introduction\nCe travail s\u0027inscrit dans le cadre de l\u0027apprentissage supervisé et plus précisément des systèmes de classification à base de règles floues (Ishibuchi et al. (1992). Ces systèmes ont la spécificité d\u0027être facilement interprétables grâce à l\u0027utilisation de termes linguistiques.\nDans ces systèmes, les règles floues peuvent être fournies par un expert humain. Comme l\u0027acquisition des connaissances humaines est une tâche complexe, plusieurs travaux se sont consacrés à l\u0027automatisation de la construction des règles à partir des données numériques (Ishibuchi et al., 1992), (Dehzangi et al., 2007). Cette construction comprend deux phases : une partition floue de l\u0027espace des entrées puis la construction d\u0027une règle floue pour chaque sous-espace flou issu de cette partition. Dans ces systèmes, un nombre élevé d\u0027attributs conduit à une explosion du nombre de règles générées, ce qui entraîne une dégradation de la compréhensibilité des systèmes, et affecte le temps de réponse nécessaire aussi bien à la phase d\u0027apprentissage qu\u0027à la phase de classification.\nDe ce fait, l\u0027optimisation du nombre de règles floues ainsi que du nombre d\u0027antécédents paraît comme une clé pour améliorer les systèmes de classification à base de règles floues. Dans ce cadre, plusieurs approches ont été proposées dans la littérature. On peut citer l\u0027approche de sélection des règles pertinentes par algorithme génétique (Ishibuchi et al., 1995) ou par le concept d\u0027oubli . Une autre approche consiste à réduire le nombre d\u0027attributs par une sélection des attributs les plus significatifs (Lee et al., 2001).\nDans ce papier, nous nous intéressons à la technique de regroupement des attributs dans les prémisses des règles. Dans cette approche, initialement introduite dans un cadre non flou par Borgi (1999), les attributs prédictifs sont regroupés en blocs, les attributs de chaque bloc sont traités séparément et apparaissent ensemble dans une même prémisse. Une première extension de ce travail dans un cadre flou, pour la génération de règles dans les systèmes d\u0027infé-rence floue, a été réalisée par Soua et al. (2012). Cette approche de regroupement des attributs, nommée SIFCO, présente l\u0027avantage de décomposer le problème d\u0027apprentissage en des sousproblèmes de complexité inférieure, et de réduire ainsi le nombre de règles générées. De plus, cette approche permet d\u0027obtenir des règles plus intelligibles car de taille réduite.\nLe regroupement d\u0027attributs dans (Borgi, 1999) et (Soua et al., 2012) se fait par recherche de corrélation linéaire : les attributs linéairement corrélés sont regroupés et traités séparément. Dans cet article, nous proposons une méthode qui se base sur le concept des règles d\u0027association (RA) introduit par Agrawal et al. (1993). Les RA vont nous permettre de déterminer les attributs \"liés\" ou \"associés\" qui seront regroupés dans les mêmes règles.\nL\u0027article est organisé comme suit : dans la partie 2, nous présentons les systèmes de classification à base de règles floues. Nous décrivons, dans la partie 3, le principe de regroupement des attributs comme présenté dans (Borgi, 1999) et (Soua et al., 2012). Notre approche de regroupement des attributs par RA est décrite dans la partie 4 et les résultats des tests expéri-mentaux sont présentés dans la partie 5. Nous concluons l\u0027article en présentant les principales perspectives de ce travail.\nApprentissage à base de règles floues\nOn se place dans le cadre des problèmes de classification supervisée dont le but est d\u0027affecter une classe à un objet décrit par des variables descriptives (des attributs). Nous nous intéressons au système de classification floue proposé dans (Ishibuchi et al., 1992). Afin de simplifier les notations, nous désignons ce système par l\u0027acronyme SIF. Deux phases sont à distinguer dans ce système : la phase d\u0027apprentissage dans laquelle on construit le modèle de classement à partir des données d\u0027apprentissage, et la phase de classification qui sert à associer une classe à un objet inconnu en utilisant ce modèle.\nPhase d\u0027apprentissage\nLa méthode de génération des règles floues que nous adoptons correspond à l\u0027utilisation d\u0027une grille floue simple, proposée par Ishibuchi et al. (1992). Pour illustrer cette approche, nous supposons, par souci de clarté, que notre problème d\u0027apprentissage est un problème bidimensionnel (2 attributs : X 1 et X 2 ). Les m exemples d\u0027apprentissage considérés sont notés E p \u003d (X p1 , X p2 ) ; p \u003d 1, 2, . . . , m , ils appartiennent chacun à l\u0027une des C classes : y 1 , y 2 , . . . , y C . Il est à noter que dans les SIF les attributs considérés sont numériques ; chacun des attributs X 1 et X 2 est partitionné en k sous-ensembles flous {A 1 , A 2 , . . . , A k } où chaque sous-ensemble A i est défini par une fonction d\u0027appartenance triangulaire symétrique. Un exemple de grille floue simple est présenté dans Fig \n-y k ij est la conclusion de la règle, elle correspond à l\u0027une des C classes -CF k ij est le degré de certitude de la règle, il traduit sa validité.\nLa conclusion et le degré de certitude de chaque règle sont déterminés comme suit :\n1. Pour chaque classe y t , calculer la somme des compatibilités des exemples d\u0027apprentissage appartenant à cette classe, par rapport à la prémisse de la règle :\nTrouver la classe y a qui a la plus grande valeur de compatibilité\n3. Déterminer le degré de certitude CF\nDans les travaux portant sur la construction de règles de classification floues, les attributs ne sont pas nécessairement partitionnés en un même nombre de sous-ensembles flous. Plusieurs types de grilles floues ont été étudiés comme par exemple la grille floue rectangulaire .\nPhase de classification\nDans cette phase, le système décide, à partir de la base de règles générées, notée S R , de la classe y a à associer à un individu E \u003d (X 1 , X\n2 ) de classe inconnue. 1. Pour chaque classe y t ; t \u003d 1, 2, . . . , C, calculer ? yt par :\n2. Trouver la classe y a qui maximise ? yt :\nFIG. 1 -Grille floue simple.\nRegroupement d\u0027attributs\nL\u0027approche de regroupement d\u0027attributs se base sur le concept des ensembles d\u0027apprentissage artificiel, qui repose sur la combinaison des décisions de plusieurs apprenants pour améliorer l\u0027exécution du système global (Valentini et Masulli, 2002). L\u0027idée de ce concept est de répartir l\u0027information -qui peut correspondre aux exemples d\u0027apprentissage, aux attributs descriptifs ou encore aux classes -entre plusieurs apprenants, chaque apprenant réalise la phase d\u0027apprentissage sur l\u0027information qui lui a été fournie, et les opinions \"individuelles\" des différents apprenants sont ensuite combinées pour atteindre une décision finale. Dans notre cas, l\u0027information à répartir correspond aux attributs descriptifs : chaque classifieur utilise un sous-ensemble des attributs initiaux et construit une base de règles locale, puis les différentes bases locales obtenues sont combinées pour former le modèle final (voir Fig. 2).\nCette approche, vérifiée expérimentalement dans (Soua et al., 2012) et (Borgi, 1999), permet de garantir une réduction conséquente du nombre de règles sans trop altérer les taux de bonnes classifications. Pour un problème de n attributs et k sous ensembles flous pour chaque attribut, le nombre de règles générés par les SIF, noté N R SIF , vaut k n . Lorsqu\u0027on découpe le problème d\u0027apprentissage en g sous-problèmes et on applique sur chacun d\u0027eux la même démarche de génération de règles que les SIF, on obtient un nombre de règles N R regrp égal à :\noù n i est le nombre d\u0027attributs liés dans le i ème groupe g i . Il a été démontré dans (Soua et al., 2012) que si les groupes d\u0027attributs, issus de l\u0027approche de regroupement, forment une partition de l\u0027ensemble des attributs de départ (\nPar conséquent :\nFIG. 2 -Approche de génération de règles par regroupement des attributs (Soua et al., 2012).\nApproche proposée : regroupement des attributs par RA\nNotre contribution réside au niveau de la méthode de regroupement d\u0027attributs ; nous proposons une nouvelle méthode n\u0027utilisant pas la recherche de corrélation linéaire, mais qui se base sur le concept des règles d\u0027association (Agrawal et al., 1993). Les algorithmes d\u0027extraction des RA déterminent les associations intéressantes entre les attributs en analysant leurs apparitions simultanées dans les enregistrements de la base de données. Cette méthode peut être très intéressante pour les bases de données pour lesquelles il n\u0027existe aucune relation de type corrélation linéaire entre les attributs.\nGénéralement, les algorithmes d\u0027extraction des RA déterminent les associations entre des variables de type booléen. Comme les données traitées dans les SIF sont quantitatives, il est nécessaire de commencer par les transformer en des valeurs booléennes, puis d\u0027appliquer le concept des RA sur ces valeurs. Ensuite, à partir des associations trouvées entre ces valeurs booléennes, nous déduisons les associations entre les attributs de départ. Enfin et dans le but de garantir une réduction du nombre de règles, nous nous proposons de filtrer les groupes d\u0027attributs associés de manière à obtenir une partition de l\u0027ensemble des attributs de départ. Nous décrivons dans ce qui suit ces différentes étapes.\nGénération des itemsets fréquents : liaisons locales entre attributs\nL\u0027existence d\u0027une liaison entre deux variables dépend de la réponse à la question : est-ce que la connaissance des valeurs de l\u0027une permet de prédire les valeurs de l\u0027autre ? Le concept des RA répond à cette question en associant les valeurs qui apparaissent souvent ensemble dans les transactions de la base de données considérée.\nLes RA ont été introduites par Agrawal et al. (1993)  -la génération des itemsets fréquents (tous les itemsets ayant un support supérieur à un seuil prédéfini minSupp). -la génération des règles d\u0027association à partir de ces itemsets fréquents ; une RA doit avoir une confiance supérieure à un seuil prédéfini par l\u0027utilisateur minConf .\nDans ce travail, nous nous intéressons au premier sous-problème et nous cherchons à déter-miner les groupes d\u0027attributs liés. Pour déterminer les itemsets fréquents, plusieurs algorithmes ont été proposés (Agrawal et al., 1993)  (Agrawal et Srikant, 1994)  (Savasere et al., 1995). L\u0027algorithme Apriori proposé dans (Agrawal et Srikant, 1994) est le plus connu et il est largement utilisé mais il ne traite que des données booléennes. Dans les problèmes courants, la majorité des données sont quantitatives et qualitatives et nécessitent des algorithmes applicables à ce type de données. Une extension de Apriori a été proposée par Srikant et Agrawal (1996) ; ils ont proposé de faire une correspondance entre des variables quantitatives ou qualitatives et des variables booléennes par le codage disjonctif complet. Pour une variable qualitative, chaque catégorie correspond à un élément booléen. Pour une variable quantitative, on discrétise l\u0027attribut en des intervalles, puis on fait correspondre une variable booléenne à chaque intervalle.\nDans notre cas, les attributs étant continus, nous recourons au codage disjonctif complet des attributs. Le partitionnement des attributs se fait par une discrétisation régulière à intervalles égaux. Nous obtenons donc des intervalles que nous assimilons à des valeurs booléennes. Nous appliquons ensuite l\u0027algorithme Apriori sur ces intervalles et obtenons ainsi des itemsets fré-quents ou des groupes d\u0027intervalles liés.\nDétermination des attributs liés : liaisons globales entre attributs\nDans l\u0027étape précédente, nous avons déterminé les groupes d\u0027intervalles liés. Notre but étant de faire un regroupement des attributs et non pas de leurs intervalles, on se propose de développer une procédure qui permet de déterminer la liaison entre un groupe d\u0027attributs à partir des liaisons trouvées entre leurs intervalles.\nNous définissons pour cela une grille d\u0027association qui représente les associations entre les valeurs (intervalles) d\u0027un groupe d\u0027attributs. Chaque axe de la grille concerne un attribut. La Fig. 3 présente 3 exemples de grille avec deux attributs X 1 et X 2 ; X 1 est décomposé en 6 valeurs (val de valeurs des attributs X 1 et X 2 . Quand deux valeurs forment un itemset fréquent, la case correspondante à leur intersection est grisée : on appelle cette case une région liée.\nLa liaison entre deux valeurs de deux attributs n\u0027entraîne pas forcément la liaison entre les deux attributs puisque d\u0027une part, ces attributs peuvent avoir très peu de régions liées (exemple (2) de la Fig. 3) et d\u0027autre part, le nombre de données dans ces régions peut être très faible par rapport au nombre total de données (exemple (3) de la Fig. 3).\nDire que, plus le nombre de régions liées est grand, plus l\u0027association entre les attributs est forte, n\u0027est pas toujours suffisant. En effet, le principe de RA détermine si une région est liée en analysant son support, et ce dernier reflète la densité de données, c.à.d. la fréquence d\u0027apparition des données dans la région. Ainsi, une seule région liée peut entraîner une association plus significative que plusieurs régions liées si cette unique région a une densité plus importante que la densité totale de l\u0027ensemble des autres régions liées. Nous proposons donc de prendre en compte aussi bien le nombre de régions liées que leurs densités. Pour cela, nous commençons par définir le poids d\u0027une région, appelé aussi coefficient de pondération. Ce poids caractérise la densité de données dans cette région, ce qui revient à son support.\nPour les valeurs respectives val Nous nous inspirons ensuite du principe des RA généralisées où une taxonomie (Fig. 4) existe entre les variables. D\u0027après (Srikant et Agrawal, 1995), les associations trouvées à un niveau donné peuvent remonter au niveau supérieur en sommant leurs supports, à condition qu\u0027il n\u0027y ait pas de recouvrement. Avec l\u0027exemple de la Fig. 4, si les itemsets (Veste, Botte) et (Veste, Espadrille) sont extraits, alors il n\u0027est pas possible de les généraliser à l\u0027itemset de niveau supérieur (Vêtements, Chaussure) en sommant leurs supports, car Veste, Botte et Espadrille peuvent figurer dans une même transaction. Dans notre cas, les variables quantitatives sont partitionnées en des valeurs sous forme d\u0027intervalles ; la présence de deux valeurs d\u0027un seul attribut n\u0027est donc pas possible dans le même enregistrement. En formant une taxonomie entre un attribut et ses intervalles (Fig. 5), et comme il n\u0027y a pas de recouvrement entre les FIG. 4 -Exemple de taxonomie pris de Srikant et Agrawal (1995).\nFIG. 5 -Taxonomie entre un attribut et ses valeurs.\nintervalles, on peut calculer le degré d\u0027association des attributs comme la somme des supports de leurs valeurs (intervalles) liées. En utilisant ce principe, et en ne comptabilisant que les régions liées, nous définissons un degré d\u0027association entre deux attributs ou plus, par la somme des coefficients de pondération de leurs régions liées. Donc, pour deux attributs X 1 et X 2 , le degré d\u0027association ? s\u0027écrit : Dans le cas général d\u0027un ensemble d\u0027attributs X \u003d {X n1 , X n2 , . . . , X n l }, le degré d\u0027association de ces l attributs est :\n-r i1i2...i l X est la région formée par les intervalles val\n. . , k l sont respectivement les tailles des partitions de X n1 , X n2 , . . . , X n l .\nLe degré ? est compris entre 0 et 1, on peut alors définir un seuil d\u0027association ? min au delà duquel on considère que les attributs de l\u0027ensemble X sont liés.\nChoix des groupes d\u0027attributs associés\nLa procédure présentée dans 4.1 et 4.2 est basée sur le principe de l\u0027algorithme Apriori ; elle fournit donc tous les groupes d\u0027attributs associés de différentes tailles. Il est à noter que ces groupes ne constituent pas forcément une partition de l\u0027ensemble des attributs de départ : on peut avoir des relations d\u0027inclusion entre deux groupes d\u0027attributs, ou une intersection non vide. Afin de garantir une réduction du nombre de règles générées, nous nous proposons de sélectionner un ensemble de groupe d\u0027attributs de manière à former une partition de l\u0027ensemble des attributs de départ (voir partie 3). La sélection se base sur les deux critères suivants : \nExpérimentation\nNotre système, baptisé SIFRA, utilise l\u0027approche de regroupement des attributs dans le cadre des SIF (Ishibuchi et al., 1992) comme cela est fait dans SIFCO (Soua et al., 2012) mais avec une nouvelle méthode de regroupement des attributs, celle que nous avons proposée et qui se base sur le concept des règles d\u0027association. Après avoir déterminé les groupes d\u0027attributs associés, nous utilisons la démarche proposée par Ishibuchi et al. (1992) pour générer les règles floues (partie 2.1), pour chaque groupe d\u0027attributs. La classification d\u0027un objet inconnu se fait par la méthode de classification de Ishibuchi et al. (1992) (partie 2.2).\nNous avons testé notre système SIFRA sur des bases de données qui diffèrent par le nombre d\u0027attributs, le nombre d\u0027exemples et le nombre de classes (Tab. 1). Pour évaluer la capacité de généralisation de notre méthode, nous avons adopté la technique de validation croisée d\u0027ordre 10 ( Kohavi, 1995). Dans le tableau 2, nous présentons le taux de bonne classification suivi entre parenthèses du nombre de règles générées. Les meilleurs taux de classification sont présentés en gras. Le terme \"imp\" fait référence à l\u0027impossibilité de générer les règles floues à cause du nombre de règles très élevé (supérieur à 10 5 ). Pour la phase de regroupement des attributs, nous avons utilisé une discrétisation à intervalles égaux et avons fixé le nombre d\u0027intervalles à 3. D\u0027autres tailles de discrétisation ainsi que d\u0027autres méthodes de discrétisation pourront être étudiées dans de prochains travaux. Au niveau de la phase d\u0027apprentissage, nous avons utilisé une partition floue homogène et une partition floue supervisée. Pour la partition homogène, nous avons testé plusieurs valeurs de la taille de partition k. Comme dans SIFCO, la valeur de k qui permet d\u0027obtenir le meilleur taux de bonne classification dépend fortement des données de la base. Pour la partition floue supervisée, nous avons adopté la méthode MDLP de Fayyad et Irani (1993 Nous présentons dans Tab. 2 une comparaison de notre méthode SIFRA avec les deux méthodes SIF et SIFCO. Chacune des 3 méthodes possède des paramètres d\u0027entrée à définir, à savoir la taille de la partition foue k, le seuil et la méthode de corrélation pour SIFCO, les seuils minSupp et ? min pour SIFRA. Pour comparer la performance des 3 méthodes et pour simplifier la lecture des résultats, nous présentons dans Tab. 2, pour chaque méthode, le meilleur taux de bonne classification obtenu en faisant varier ses paramètres d\u0027entrée. D\u0027après Tab. 2, il est clair que notre approche SIFRA fournit des taux de bonne classification très satisfaisants comparée à la méthode SIF, et des taux similaires ou meilleurs comparée à SIFCO. Comparée aux SIF, notre approche permet d\u0027améliorer la performance de classification et de diminuer notablement le nombre de règles (en particulier avec les bases Wine, Vehicle et Sonar pour lesquelles la génération des règles avec SIF est impossible vu l\u0027explosion de leur nombre). Comparée à SIFCO, notre méthode donne le meilleur taux de classification pour la base Iris avec un nombre de règles plus élevé mais qui reste faible (33). Pour Lupus, Wine et Sonar, les mêmes taux ont été obtenus par SIFRA et SIFCO. Concernant la base Vehicle, notre approche améliore considérablement le taux de bonne classification (67.73% contre 54.97% avec SIFCO) mais avec un nombre de règles plus important. Ce différentiel du nombre de règles s\u0027explique par le fait que les groupes d\u0027attributs liés détectés par SIFRA (basé sur les RA) contiennent plus d\u0027attributs que les groupes détectés par SIFCO (basé sur une recherche de corrélation linéaire) (équation 7). Avec ces données, les liaisons entre attributs déterminées par notre approche semblent donc être plus pertinentes que celles trouvées avec SIFCO. 6 Conclusion\n"
  },
  {
    "id": "277",
    "text": "introduction\nLes méthodes à noyau utilisées en analyse exploratoire des données (K-PCA, K-LDA, K-CCA, etc.) ou pour traiter des tâches de classification ou de régression (machines à support vectoriel, SVM) nécessitent, dans leurs fondements, l\u0027usage de noyaux définis (positifs ou né-gatifs). Pourtant, bon nombre d\u0027études relativement récentes en fouille de données temporelles présentent des résultats produits par de telles méthodes exploitant des noyaux temporellement élastiques (NTE) non définis Haasdonk (2005)  Zhang et al. (2010) ou régularisés par des mé-thodes spectrales Narita et al. (2007). L\u0027émergence de nouvelles méthodes de régularisation pour NTE offre aujourd\u0027hui des alternatives à l\u0027exploitation des noyaux élastiques non définis que nous nous proposons d\u0027évaluer de manière comparative sur des jeux de données simples mais potentiellement explicites. D\u0027une manière générale, les procédures de régulation ont été développées pour approximer des noyaux non définis par des noyaux définis (ou semi-définis). Les premières approches appliquent directement des transformations spectrales aux matrices de Gram issues des noyaux non définis. Ces méthodes Wu et al. (2005)  Chen et al. (2009) consistent à i) changer le signe des valeurs propres négatives ou décaler ces valeurs propres en utilisant la valeur de décalage minimal nécessaire pour rendre le spectre des valeurs propres TAB. 1 -Liste des noyaux analysés positif, et ii) reconstruire la matrice de Gram issue du noyau avec les vecteurs propres d\u0027origine afin de produire une matrice semi-définie positive. D\u0027autres approches sont basées sur la recherche de la matrice de corrélation (matrice symétrique positive semi-définie ayant une diagonale unitaire) la plus proche de la matrice de Gram issue du noyau non défini, la proximité étant prise au sens d\u0027une norme (norme de Frobenius pondérée) Higham (2002).\nCependant ces procédures de convexification sont difficiles à interpréter géométriquement Graepel et al. (1998) et l\u0027effet attendu du noyau d\u0027origine non défini peut être, selon les études, soit perdu ou pour le moins atténué par ces méthodes agissant directement sur le spectre matriciel, soit encore minime voir négatif comparativement à l\u0027exploitation directe de la matrice non régularisée Chen et Ye (2008). Dans le contexte de l\u0027alignement de séquences ou de séries temporelles, des approches de régularisation plus directes pour les NTE consistent à remplacer les opérateurs min ou max par un opérateur de sommation ( ) dans les équations récursives qui définissent les distances élastiques. Il en résulte qu\u0027au lieu de ne considérer que le meilleur chemin d\u0027alignement possible entre deux séries temporelles, le noyau régularisé effectue la somme des couts (ou gains) de tous les chemins d\u0027alignement possibles avec un mécanisme de pondération qui cherche à favoriser les bons alignements et à pénaliser les mauvais alignements. Ces principes ont été appliqués avec succès par Saigo et al. (2004) pour la mesure (non définie) de Smith et Waterman (1981) très utilisée pour la comparaison de séquences symboliques, et plus récemment pour la speudo distance Dynamic Time Warping (DTW, Velichko et Zagoruyko (1970), Sakoe et Chiba (1971)) Cuturi et al. (2007), .\nNous développons dans cet article, sur la base d\u0027une analyse en composantes principales à noyau (K-PCA), une étude expérimentale permettant d\u0027évaluer les noyaux listés en table 1 sur de tâches de classification (supervisée et non-supervisée) de séries temporelles dans des sous espaces de dimension réduite. En nous limitant à des ensembles de séries temporelles de taille fixe, nous proposons ainsi de comparer expérimentalement au travers d\u0027une analyse K-PCA un noyau Gaussien construit à partir de la distance Euclidienne (noyau défini positif, non temporellement élastique, ce noyau servant de base de référence), un noyau Gaussien construit à partir de la pseudo distance DTW (noyau non défini en général, mais élastique), une version régularisée du noyau précédent basée sur la recherche de la matrice de corrélation la plus proche Higham (2002), et enfin le noyau DTW régularisé suivant la méthode proposée par , K DT W , et une version normalisée, K Faisant suite aux travaux de Cuturi et al. (2007), la technique de régularisation développée dans  s\u0027attache à transformer les équations récursives définissant la DTW (Dynamic Time Warping) de manière à produire une mesure de similarité notée K DT W constituant un noyau défini positif, c\u0027est-à-dire s\u0027apparentant à un produit scalaire dans un espace de Hilbert à noyau reproduisant. K DT W se distingue de l\u0027approche proposée par Cuturi et al. en prenant la forme d\u0027un noyau de convolution tel que défini par Haussler (1999) tout en imposant une condition sur les coûts locaux d\u0027alignement moins restrictive. Pour rappel, un noyau défini sur R est est une fonction continue symétrique K :\nDefinition\nLa définition récursive du noyau K DT W est la suivante : • ? ? R + est un paramètre d\u0027ajustement qui permet de pondérer les contributions locales, i.e. les distances entre les positions localement alignées, et\nAinsi, fondamentalement, l\u0027opérateur min (ou max) est remplacé par un opérateur de sommation et une fonction de corridor symétrique (la fonction h dans l\u0027équation récursive cidessus) est introduite pour, éventuellement, limiter la sommation et et donc la complexité algorithmique. Enfin, un nouveau terme récursif nécessaire à la régularisation (K xx ) est ajouté, de telle sorte que la preuve de la propriété de positivité du noyau peut être comprise comme une conséquence directe du théorème de convolution d \u0027 Haussler (1999).\nNormalisation\nLe noyau K DT W effectue la somme sur l\u0027ensemble des chemins d\u0027alignement possibles des produits des termes locaux d\u0027alignement e\nPour les séries temporelles de grandes tailles, ces produits deviennent infimes et K DT W incalculable lorsque ? est trop faible. Ainsi, le domaine de variation de K DT W s\u0027amenuise en convergeant vers 0 lorsque ? tend vers 0 sauf lorsque l\u0027on compare deux séries temporelles identiques (la matrice de Gram correspondante souffre ainsi d\u0027une dominance diagonale). Comme proposé dans , une manière de palier ce problème consiste à considérer le noyau normalisé : . Si l\u0027on oublie la constante de proportionnalité, cela revient simplement à élever le noyau\n, ce qui montre que˜Kque˜ que˜K DT W est lui aussi défini positif (Berg et al. (1984), Proposition 2.7). Une correction de dominance diagonale similaire (sous-polynomial, i.e. t \u003c 1) a initialement été proposée dans Schölkopf et al. (2002). L\u0027effet de ce type de normalisation, sur le jeu de données SwedishLeaf (c.f. Table 2) est illustré en figure 1. La distribution des valeurs de la matrice de Gram associée au noyau non normalisé K DT W (évalué avec ? \u003d 1) présente une très forte accumulation autour des valeurs très faibles (pour ? \u003d 1, K DT W M in \u003d 2.1e ? 77 et K DT W M ax \u003d 1.9e ? 06 sur le jeu de données testé), tandis que la distribution des valeurs de la matrice de Gram associée au noyau normalisé˜Knormalisé˜ normalisé˜K DT W \u003d K t DT W est plus diffuse. Les valeurs du noyau sont par ailleurs bornées :  \nComplexité algorithmique\nLa définition récursive précédente permet de montrer que la complexité algorithmique liée au calcul du noyau K DT W est O(n 2 ), où n est la longueur des deux séries temporelles mises en correspondance, et lorsqu\u0027aucun corridor n\u0027est spécifié. Cette complexité est ramenée à O(c.n) quand un corridor symétrique de taille c est exploité par le biais de la fonction symmétrique h.\nL\u0027analyse en composantes principales non-linéaire, encore appelée ACP à noyau ou Kernel-PCA, Schölkopf et al. (1998) peut être vue comme une généralisation de l\u0027ACP classique : elle permet d\u0027engendrer une réduction de dimensionnalité non linéaire du point de vue de l\u0027espace de représentation initial des données. Le principe consiste à projeter, par le biais d\u0027une fonction non-linéaire ?(.), les données initiales dans un espace en général de plus haute dimension de sorte que l\u0027image de la variété (non linéaire) contenant les données initiales devienne plus facilement linéairement séparable dans le nouvel espace, appelé espace des caractéristiques. Il suffit alors d\u0027effectuer une ACP classique dans cet espace linéaire pour obtenir une réduction de dimensionalité non linéaire dans l\u0027espace des données initiales. Si l\u0027on exploite un noyau K(., .) défini positif, celui-ci induit de manière implicite une fonction nonlinéaire dite de mapping ?(.) telle que ?x, y, K(x, y) \u003d\u003c ?(x)\nT , ?(y) \u003e. Cette fonction ?(.) n\u0027a pas besoin d\u0027être connue explicitement (on évoque ici l\u0027astuce du noyau).\nAlgorithm 1 ACP non linéaire 1: Choix du noyau (défini positif) k 2: Construction de la matrice de Gram à partir des données :\n..,m 3: Centrage de la matrice de Gram (on retire la moyenne des données projetées dans l\u0027espace des caractéristiques) : \nL\u0027algorithme 1 présente succinctement les étapes de l\u0027ACP non-linéaire, qui, à partir du choix d\u0027un noyau défini positif, extrait les valeurs et vecteurs propres de la matrice de Gram centrée associée, puis projette toute donnée (initiale ou de test) dans un espace des caracté-ristiques de dimension réduite (d). Il est clair que l\u0027ACP non-linéaire nécessite que le noyau utilisé soit défini positif.  Keogh et al. (2006). Ils sont de tailles modestes pour permettre (éventuellement) une visualisation la plus explicite possible en faible dimension. Le nombre de catégories varie de 2 à 50 et la longueurs des séries varie de 60 à 463.\nPour les 13 jeux de données listés en Table 2, et les 5 noyaux listés en Table 1, une ACP non linéaire est pratiquée puis les données sont projetées dans l\u0027espace des caracté-ristiques obtenu en faisant varier le nombre de vecteurs propres, c\u0027est à dire la dimension de l\u0027espace réduit. Par exemple, en figure 2 les projections des séries temporelles du jeu de données Gun_Point sont présentées dans un espace des caractéristiques de dimension 3 pour les noyaux Gaussien-Euclidien, Gaussien-DTW régularisé par matrice de corrélation la plus proche (MCPP), K DT W et K t DT W . La valeur du paramètre t, exposant du noyau K t DT W , est estimé directement à partir des données d\u0027apprentissage en évaluant les valeurs extrêmes prises par le noyau K DT W non normalisé. A titre d\u0027exemple, on considère le jeu de données Gun_Point, pour lequel a matrice de Gram évaluée sur le noyau Gaussien DTW n\u0027est pas dé-finie. Comme le montre la figure 2, les projections dans le sous-espace des caractéristiques de dimension 3 sont très proches pour le noyau Gaussien DTW et sa version régularisée par matrice de corrélation la plus proche. La séparation des classes est, sur cet exemple, bien meilleure pour le noyau KDT W et sa version normalisée KDT W t . A l\u0027issue de l\u0027ACP non linéaire, nous proposons une expérience de classification supervisée basée sur la règle du plus proche voisin (1-PPV) et une expérience de clustering basée sur l\u0027algorithme des K-moyennes (K correspondant ici au nombre effectif de catégories du jeux de données). Pour chaque jeux de données et pour chaque noyaux testés la classification supervisée et non supervisée sont effectuées dans le sous espace des caractéristiques défini par K-PCA en faisant varier la dimension du sous-espace de 1 à 20 (pour dix dimensions, cela correspond à une réduction dimensionnelle variant de 83% à 98% pour les jeux de données testés).\nLa qualité de la classification est ensuite évaluée à partir du taux d\u0027erreur de classification estimé à partir d\u0027une validation croisée en 5 sous-échantillons. La précision et l\u0027information mutuelle normalisée (IM N ) sont utilisées pour évaluer la qualité d\u0027une classification non supervisée sur des données La précision est définie comme la fraction des individus correctement étiquetés, étant donné une correspondance 1-vers-1 entre les vraies classes et les clusters dé-couverts. Si p dénote une permutation quelconque des indices des clusters {˜c{˜c i } proposés (ou des vraies classes {c j }), la précision est alors définie par : L\u0027information mutuelle normalisée, IM N , entre la vraie classification C et celle prédite˜Cprédite˜ prédite˜C est définie par :  Par ailleurs, le noyau Gaussien-DTW et sa variante régularisée à partir de la matrice de corré-lation la plus proche conduisent à des résultats très similaires et sensiblement moins bons comparativement aux noyaux KDT W et KDT W t . La régularisation par matrice de corrélation la plus proche ne semble donc pas apporter de bénéfice significatif en terme de classification supervisée ou non supervisée sur ces jeux de données par rapport au noyau DTW non défini. Il obtientles taux d\u0027erreur de classification les plus faibles sur 10 des 13 jeux de données (CBF est mieux classé par le noyau Gaussien-DTW, ECG200 par le noyau Gaussien-Euclidien et Lighting2 par le noyau Gaussien-DTW régularisé par matrice de corrélation la plus proche). Pour la classification non supervisée, K t DT W obtient également les meilleurs résultats d\u0027après les mesures Précision et IMN pour 8 des 13 jeux de données. Pour cette tâche, K DT W est meilleur sur les jeux de données yoga et Gun_Point, tandis que le noyau Gaussien-Euclidien se distingue sur ECG200 et les noyaux Gaussien-DTW sur Lighting2. Sur les jeux de données pour lesquels K t DT W n\u0027arrive pas en tête, ce noyau se positionne entre le noyau Gaussien-Euclidien et les noyaux Gaussien-DTW. Contrairement à la régularisation par matrice de corrélation la plus proche, le principe de régularisation mise en oeuvre dans K DT W et K t DT W modifie en profondeur la nature même de la fonction de similarité sous-jacente à la mesure DTW en apportant en général une meilleure capacité à séparer ou regrouper les séries temporelles dans des espaces de dimension réduite.\nRésultats et analyse\nConclusion\nNous avons évalué expérimentalement la capacité de quelques noyaux (élastiques, nonélastiques, définis, non-définis) à classer des séries temporelles de manière supervisée ou non dans des espaces de caractéristiques à dimension réduite obtenus par ACP non linéaire. Les résultats présentés montrent que les approches récentes de régularisation de noyaux élastiques offrent des alternatives bien meilleures que les principes classiques de régularisation basés sur des approches spectrales portant directement sur les valeurs propres des matrices de Gram construites à partir des noyaux non définis. Le noyau DTW régularisé K DT W exploité dans cet article dans sa version normalisé K t DT W offre un bon compromis entre les noyaux définis non-élastiques (tel que le noyau Gaussien-Euclidien) et les noyaux non définis élastiques (tel que le noyau Gaussien-DTW). Non seulement celui-ci conserve une caractéristique d\u0027élasti-cité temporelle tout en étant défini positif, mais il se marie également bien avec les approches à noyau tel que l\u0027ACP non linéaire. Sa capacité à proposer des espaces de caractéristiques discriminantes en dimension réduite en font un outil en général efficace pour l\u0027analyse exploratoire d\u0027ensembles de séries temporelles. Ces résultats confirment et complètent ainsi ceux présentés par  et  dans le cadre d\u0027une classification supervisée par machine à support vectoriel en apportant un éclairage sur la normalisation de ce type de noyau.\n"
  },
  {
    "id": "278",
    "text": "Introduction\nÀ cause de l\u0027explosion du nombre d\u0027informations stockées et partagées sur Internet, et l\u0027introduction de nouvelles technologies pour capturer ces données, l\u0027analyse des données incertaines est devenue essentielle dans de nombreuses applications pour la prise de décision. Pour gérer et traiter l\u0027incertitude des données, plusieurs modèles ont été proposés, ce qui a donné naissance à différents types de bases de données imparfaites. Nous pouvons citer les plus connues : les bases de données probabilistes présentées par Dalvi et Suciu (2007); Aggarwal et Yu (2009), les bases de données possibilistes introduites par Bosc et Pivert (2010) et les bases de données évidentielles basées sur la théorie de Dempster-Shafer proposées par Bell et al. (1996). L\u0027utilisation des bases de données évidentielles offre plusieurs avantages à savoir : (i) Elles permettent de modéliser l\u0027incertitude et l\u0027imprécision des données ; et (ii) cela représente une généralisation des deux modèles ; probabiliste et possibiliste à la fois. Dans cet article, nous nous nous intéressons aux requêtes Skyline sur des données incertaines où l\u0027incertitude est modélisée par la théorie de l\u0027évidence, ce qui constitue un travail pionnier.\nLe reste cet article est organisé comme suit. La section 2 contient un rappel sur l\u0027opérateur Skyline, les notions de base de la théorie de l\u0027évidence et les bases de données évidentielles. Dans la section 3, nous définissons formellement la relation de dominance et modélisons le Skyline évidentiel. Nos expérimentations sont données dans la section 4. Enfin, la section 5 conclut l\u0027article.\nNotions de base\nDans cette section, nous présentons d\u0027abord les requêtes Skyline sur des données classiques Borzsonyi et al. (2001). Ensuite, nous présentons les notions de base de la théorie de l\u0027évidence et les bases de données évidentielles.\nLes requêtes Skyline\nPour simplifier, nous supposons que la valeur la plus élevée, est la plus préférée.  \nDéfinition 1 (Relation de Dominance) Étant donnés deux objets\nLa théorie de l\u0027évidence\nLa théorie de l\u0027évidence a été introduite par Shafer (1976) dont le but est d\u0027évaluer subjectivement la vérité d\u0027une proposition. Cette théorie, aussi connue sous le nom de \"théorie des fonctions de croyance\" est une généralisation de la théorie bayésienne Dempster (1968). Elle représente un ensemble d\u0027hypothèses désigné par le cadre de discernement.  \nLes bases de données évidentielles (BDE)\nLes BDE permettent de représenter les données manquantes, incertaines ou imprécises. \nLes requêtes Skyline en présence de données évidentielles\nDans cette section, nous introduisons la relation de dominance entre les objets dont l\u0027incertitude est modélisée par la théorie des fonctions de croyance, par la suite, nous présentons la définition du Skyline évidentiel. Étant donné un ensemble d\u0027objets O \u003d {O 1 , O 2 , . . . , O n } défini sur un ensemble d\u0027attributs A \u003d {a 1 , a 2 , . . . , a d }, avec o i .a k représente l\u0027ensemble des éléments focaux de l\u0027objet o i et l\u0027attribut a k ; par exemple 1 , o 1 .wl \u003d { 16, 18}, 0.1 20}, 0.9 et o 1 .r \u003d { 0.5 100}, 0.5 Le degré de croyance qu\u0027une valeur incertaine de l\u0027objet o i définie sur l\u0027attribut a k est préférée ou égale à une autre valeur de l\u0027objet o j , est donné par Bell et al. (1996) :\nDans notre exemple, nous avons bel(o 1 .wl ? o 3 .wl) \u003d 0.3 · (0.1 + 0.9) + 0.7 · (0.1 + 0.9) \u003d 1, et bel(o 1 .r ? o 3 .r) \u003d 0.7 · 0.7 + 0.3 · 0.7 \u003d 0.7.\nÉtant donnés deux objets\nLe Tableau 2 présente les degrés de croyance que chaque objet en ligne, domine un autre objet en colonne.\nLes objets o i et o j comparés sont supposés différents, ce qui fait que la relation ? suffit pour exprimer que o i domine o j (au sens de (2)). Remarquons que cette définition se réduit à la \nPar exemple, o 1 0.9-domine o 2 et o 4 . Mais, il ne 0.9-domine pas o 4 car bel(o 1 ? o 4 ) \u003d 0.3 \u003c 0.9. Intuitivement, un objet est dans le Skyline s\u0027il n\u0027est pas dominé par rapport à un certain seuil b. \nDéfinition 7 (b-dominant skyline) Le skyline de O, désigné par b-Sky O , comprend les objets dans O qui ne sont pas b-dominés par aucun autre objet, i.e., b-Sky\nThéorème 1 Étant donnés deux seuils de croyance\nPreuve 1 Supposons qu\u0027il existe un objet\nLe théorème 1 indique que la taille de b-dominant skyline est plus petite que celle de b ? -dominant skyline si b \u003c b ? . Les utilisateurs ont donc la possibilité de contrôler la taille des objets que contient le Skyline évidentiel en faisant varier le seuil de croyance. obtenus. Dans chaque expérimentation., nous varions un seul paramètre, tandis que les autres paramètres prennent leurs valeurs par défaut. Le Tableau 3 montre ces paramètres et leurs symboles ; les valeurs par défaut sont en gras. La figure 1.a montre que la taille du skyline évidentiel augmente avec l\u0027augmentation de n. Dans la figure 1.b, on montre que la taille du skyline évidentiel augmente aussi de façon significative avec l\u0027augmentation de d. \nÉtude Expérimentale\nConclusion\nDans cet article, nous avons abordé le problème des requêtes skyline dans le cadre des bases de données évidentielles et nous avons introduit un nouveau type de skyline. Notre étude expérimentale a démontré la faisabilité et la flexibilité du skyline évidentiel. Comme perspective, nous envisageons de développer des techniques de classement des objets retournés par l\u0027opérateur skyline évidentiel.\n"
  },
  {
    "id": "282",
    "text": "Introduction\nL\u0027analyse en composantes principales (ACP, Jolliffe (1986)) est une des méthodes, si ce n\u0027est la méthode, d\u0027analyse exploratoire les plus couramment utilisées. Elle a été ré-interprétée sous un formalisme probabiliste par Tipping et Bishop (1999), montrant que les composantes principales pouvaient être estimées par maximum de vraisemblance dans le cadre d\u0027un modèle à variables latentes. Avec l\u0027avénement des données de grande dimension, la problématique consistant à sélectionner un petit nombre de variables d\u0027intérêt parmi l\u0027ensemble des variables disponibles est devenue primordiale. Un des soucis majeurs de l\u0027ACP dans cette optique est que les composantes principales sont définies comme une combinaison linéaire de l\u0027ensemble des variables initiales. Des versions parcimonieuses de l\u0027ACP (Zou et al., 2004) ainsi que de sa version probabiliste (Guan et Dy, 2009) ont été proposées récemment. La version parcimonieuse de Zou et al. (2004) repose sur l\u0027ajout d\u0027une pénalisation de type 1 au problème des moindres carrés, qui nécessite le choix du coefficient de pénalisation de façon heuristique. Dans Guan et Dy (2009), une version sparse bayésienne de l\u0027ACP probabiliste est proposée. Nous proposons dans ce travail une alternative fréquentiste utilisant un algorithme EM pour l\u0027inférence. La procédure d\u0027estimation obtenue à l\u0027avantage d\u0027être particulièrement simple, et ne nécessite pas le choix de loi a priori. Elle offre en outre la possibilité de considérer le problème du choix de la pénalité comme un problème de choix de modèles.\nAnalyse en composantes principales probabiliste\nSoit y un vecteur aléatoire observé de dimension p, et x un vecteur aléatoire latent (non observé), de dimension d, relié à y par l\u0027équation suivante :\noù W est une matrice p × d, µ est le vecteur moyenne (supposé nul dans la suite, µ \u003d 0), et ? N (0, ? 2 I). Conditionnellement au vecteur x, la distribution des vecteurs observés est :\nEn supposant x ? N (0, I), la distribution marginale du vecteur observé est :\nCe modèle est un modèle de type \"factor analysis\" (Bartholomew et al., 2011), popularisé par Tipping et Bishop (1999) sous le nom d\u0027analyse en composantes principales probabiliste (Probabilistic Principal Component Analysis (PPCA)). En effet, une estimation par maximum de vraisemblance des paramètres du modèle à l\u0027aide d\u0027un algorithme EM (Dempster et al., 1977), considérant le vecteur latent x comme manquant, conduit à estimer les colonnes de W par les vecteurs propres de la matrice de covariance empirique, vecteurs qui ne sont rien d\u0027autres que les axes principaux classiques. Soit y 1 , . . . , y n un échantillon i.i.d de vecteurs observés. L\u0027algorithme EM consiste à maximiser de façon itérative la log-vraisemblance complétée par les données non observées x 1 , . . . , x n :\n3 Une version parcimonieuse de l\u0027analyse en composantes principales probabiliste\nDans ce travail, nous considérons une version parcimonieuse de l\u0027analyse en composantes principales probabiliste. L\u0027objectif est d\u0027obtenir des axes principaux déterminés uniquement grâce à un nombre restreint de variables initiales, et ainsi faciliter leur interprétation. De plus, comme nous le verrons par la suite, l\u0027approche probabiliste de l\u0027ACP permet de sélectionner le paramètre de pénalité par des méthodes classiques de sélection de modèles.\nDans l\u0027optique d\u0027introduire de la parcimonie au sein de axes principaux, nous considérons une pénalité 1 sur les colonnes de la matrice W. La vraisemblance complétée à maximiser est alors la suivante :\noù w \u003d (w 1 , . . . , w pp ) t est la colonne de W et ? \u003e 0 est le paramètre de pénalisation. L\u0027algorithme EM est un algorithme itératif qui alterne deux étapes (E et M), décrites ci-après.\nLa q-ème itération de l\u0027étape E consiste à calculer l\u0027espérance de pen c sous la loi p(x|y, ? (q) ), où ? (q) \u003d (W (q) , ? 2(q) ) est la valeur courante de l\u0027estimation des paramètres du modèles :\nest une constante indépendante des paramètres du modèle et où\nDe sorte à faciliter la maximisation, nous considérons l\u0027approximation de la norme 1 par la forme quadratique suivante (Fan et Li, 2001) :\nj w j . La maximisation de Q(?, ? (q) ) en fonction de W n\u0027ayant pas de solution analytique, nous utilisons une approche alternative consistant à maximiser Q(?, ? (q) ) en fonction de chaque éléments de la matrice W successivement. On obtient alors, en dérivant Q(?, ? (q) ) par rapport à w j et en égalant à 0 :\nCette dérivation élément par élément ne conduit pas nécessairement au maximum de Q(?, ? (q) ), mais suffit pour faire augmenter la log-vraisemblance à chaque étape de l\u0027algorithme. On obtient alors un algorithme GEM (Generalized EM) qui conserve les mêmes propriétés de convergence qu\u0027un algorithme EM classique. L\u0027estimateur de la variance résiduelle est quant à lui :\net s\u0027avère être identique à celui de la version non sparse de l\u0027analyse en composantes principales probabiliste (Tipping et Bishop, 1999).\n4 Sélection de ? par l\u0027heuristique de pente Dans Zou et al. (2004), le choix de la pénalité ? est réalisé de manière heuristique en se basant sur l\u0027éboulis des valeurs propres. L\u0027idée de la stratégie que nous proposons est d\u0027estimer le modèle sur une grille de valeurs de ?, et d\u0027utiliser un outil de sélection de modèles pour choisir le meilleur modèle. Les outils classiques de sélection de modèles sont par exemple les critères AIC (Akaike, 1974) et BIC (Schwarz, 1978), qui pénalisent la log-vraisemblancêvraisemblancê ?) de la façon suivante :\n, où ? est le nombre de paramètres libres du modèles et n le nombre d\u0027observations. La valeur de ? dépend directement de la valeur de ? puisqu\u0027elle est égale au nombre d\u0027éléments non nuls dans W plus un (pour la variance résiduelle). Même si ces critères sont largement utilisés et asymptotiquement consistants, ils sont aussi connus pour être plus efficaces sur simulations que sur données réelles.\nPour surmonter ce problème, Birgé et Massart (2007) ont récemment proposé une mé-thode dirigée par les données pour calibrer la pénalité des critères pénalisés, connue sous le nom d\u0027heuristique de pente. L\u0027heuristique de pente a été proposé initialement dans un cadre d\u0027un modèle de régression gaussien homoscédastique, mais a ensuite été étendue à d\u0027autres situations. Birgé et Massart (2007) ont démontré qu\u0027il existait une pénalité minimale et que de considérer une pénalité égale au double de la pénalité minimale permettait d\u0027approcher le modèle oracle en terme de risque. La pénalité minimale est en pratique estimée par la pente de la partie linéaire de la log-vraisemblance pen c ( ˆ ?) exprimée en fonction de la complexité du modèle. Le critère associé est alors défini par :\noùˆsoùˆ oùˆs est l\u0027estimation de la pente de la partie linéaire de pen c ( ˆ ?). Une revue détaillée et des conseils d\u0027implémentation sont donnés dans Baudry et al. (2012).\nIllustrations numériques\nNous choisissons pour illustrer notre méthodologie un jeu de données classique issu de l\u0027UCI machine learning repository : le jeux de données USPS. Le jeu original contient 7291 images représentant des chiffres manuscrits de 0 à 9. Chaque chiffre est une image en niveaux de gris de taille 16 × 16, représentée par un vecteur de dimension 256. Pour cette expérience, nous avons extrait un sous ensemble de 1756 images correspondant aux chiffres 3, 5 et 8. Nous réalisons sur ces données une ACP ainsi que l\u0027ACP parcimonieuse que nous proposons. Pour cette dernière, nous fixons le nombre maximum d\u0027itérations de l\u0027algorithme EM à 500 et le seuil de convergence à 10\n, et nous considérons une grille de valeurs de ? de 0 à 150, avec un pas de 1. La méthode de l\u0027heuristique de pente (figure 1) conduit à choisir ? \u003d 126. Dans un but illustratif, nous discutons ici les résultats concernant les deux premières composantes principales. La figure 2 représente la projection des 1756 images dans le premier plan principal de l\u0027ACP ainsi que les deux premières composantes principales, tandis que la figure 3 propose la même représentation pour l\u0027ACP parcimonieuse. Nous pouvons noter que les deux méthodes définissent un premier plan principal relativement discriminant vis-à-vis des trois types d\u0027images. Tout l\u0027intérêt de l\u0027ACP parcimonieuse est que les composantes principales   \n"
  },
  {
    "id": "283",
    "text": "Introduction\nAvec la récente explosion du nombre d\u0027images et de données satellites disponibles, la conception de systèmes capables d\u0027interpréter automatiquement de telles données est devenue un domaine florissant. En effet, les satellites modernes sont capables d\u0027acquérir des images à très hautes résolutions (THR) avec une définition de plus en plus élevée sur un large domaine spectral. Or, les algorithmes capables de traiter un tel volume de données en un temps raisonnable sont pour le moment assez rares.\nLa segmentation de telles données d\u0027imageries peut se faire en utilisant des algorithmes basés sur les champs de Markov, (Roth et Black, 2011). Les champs de Markov reposent sur la notion de voisinage pour modéliser les dépendances qui peuvent exister entre des données telles que des pixels adjacents, ou des super-pixels adjacents (groupes de pixels).\nDans ces modèle, on considère S \u003d {s 1 , ..., s N }, s i ? 1..K un ensemble de variables aléatoires représentant les états (labels) des données. Ces états sont supposés liés dans l\u0027espaces par des relations de voisinages et émettent des observations X \u003d {x 1 , ..., x N } où les x i sont des vecteurs contenant les attributs de chaque donnée (RGB pour les modèles les plus simples). L\u0027objectif est alors de déterminer la configuration idéale de S, c\u0027est à dire de trouver les valeurs des s i afin d\u0027obtenir une segmentation optimale.\nUne des méthodes possibles pour résoudre ce problème est l\u0027utilisation du couple ICM-EM, (Zhang et al., 2001). Le choix de l\u0027algorithme ICM (Besag, 1986) vient du fait que cet algorithme a une complexité plus faible que celles des algorithmes plus récents utilisés pour les champs de Markov, ce qui est un atout non-négligeables pour traiter le volume important des données d\u0027une image à très haute résolution. De plus, la plupart des données issues de telles images sont déjà pré-traitées et l\u0027ICM est donc suffisant pour les segmenter. Il est également important de préciser qu\u0027à ce jour, l\u0027algorithme ICM est le seul à avoir été adapté pour pouvoir optimiser le modèle d\u0027énergie contenant des informations sémantiques utiles dans le cadre des image satellites que nous avons proposé dans de précédent travaux (Sublime et al., 2014). L\u0027objectif de l\u0027algorithme ICM est d\u0027optimiser itérativement une fonction d\u0027énergie locale dérivant du logarithme de P (x|s, ? s )P (s). Bien que cet algorithme ait montré son efficacité pour résoudre ce problème, il a cependant plusieurs défauts tels que son critère d\u0027arrêt basé sur l\u0027évolution de l\u0027énergie global et l\u0027absence de garantie de convergence. En effet, cet algorithme essaye d\u0027optimiser une fonction non-convexe, et il a été montré qu\u0027après un processus d\u0027optimisation relativement rapide de forme parabolique, l\u0027algorithme ne se stabilisait pas toujours et pouvait même se mettre à diverger entrainant ainsi une détérioration des résultats (Zhang, 1989). Une des difficultés récurrente de la segmentation d\u0027image dans le cadre non-supervisé est qu\u0027il est difficile d\u0027évaluer la qualité des résultats. Dans le cas de l\u0027ICM, le critère d\u0027arrêt repose sur l\u0027énergie globale liée aux champs de Markov (somme des énergies locales) et sur l\u0027hypothèse que cette énergie va se stabiliser. Le problème d\u0027une telle approche est précisément que cette énergie globale ne se stabilise pas toujours, l\u0027algorithme n\u0027ayant pas de garantie de convergence, et que même dans le cas où elle se stabilise cette stabilisation n\u0027intervient pas né-cessairement au moment où les résultats de la segmentation sont les meilleurs. De plus, en se basant sur l\u0027énergie globale avec un nombre de données élevé, il y a un risque non négligeable d\u0027\"overflow\" ou d\u0027arrondi de cette énergie.\nDans cette article, nous proposons un nouveau critère d\u0027arrêt pouvant être facilement calculé et qui repose sur un modèle d\u0027énergie adapté aux images satellites à très haute résolution.\nAlgorithme proposé\nOn note V x le voisinage d\u0027une observation x, et A \u003d {a i,j } K×K la matrice de voisinage de l\u0027image, où chaque a i,j est la probabilité de passer de l\u0027état i à l\u0027état j entre deux données voisines. A partir de ces notations nous utilisons comme modèle d\u0027énergie locale le modèle défini lors de nos précédents travaux (Sublime et al., 2014) :\nLa fonction d\u0027énergie décrite précédemment utilise une énergie locale dérivant de la loi normale dans laquelle µ s est la valeur moyenne associée à l\u0027état s et ? s sa matrice de covariance. Le dernier membre de cette énergie qui décrit l\u0027énergie de voisinage est une fonction positive calculée à partir des éléments de la matrice A. Le facteur ? x,v représente le poids de v en tant que voisin de la donnée x selon la proportion de frontière qu\u0027il occupe. Ce modèle repose sur l\u0027idée que des données voisines ayant des clusters différents ne sont pas nécessaire-ment incompatibles, et peut être vu comme une version relaxée du graph-cuts (Boykov et al., 2001).\nLa matrice de voisinage A de ce modèle d\u0027énergie contient des informations sémantiques telles que les affinités des différents clusters ou leur compacité sur l\u0027image. En effet, la diagonale de la matrice A contient la probabilité pour chaque cluster d\u0027avoir un voisinage plus ou moins composé d\u0027éléments du même cluster. Cette diagonale peut par conséquent être considérée comme un indicateur de compacité des clusters. Les éléments non-diagonaux quant à eux fournissent des informations sur les affinités des clusters.\nÉtant donné que l\u0027objectif principal de l\u0027utilisation des champs de Markov en segmentation d\u0027image est d\u0027obtenir des zones homogènes, nous avons décidé d\u0027utiliser cette information de compacité contenue dans notre matrice de voisinage et d\u0027en faire le nouveau critère d\u0027arrêt de notre algorithme C-ICM : \"Compactness-based Iterated Conditional Modes\".\nIl est en effet raisonnable de supposer qu\u0027il faut arrêter l\u0027algorithme lorsque la compacité des clusters cesse d\u0027augmenter. Aussi, nous utilisons les variations de la trace de la matrice A comme nouveau critère d\u0027arrêt. Ce critère présente plusieurs avantages : Les calculs sont faciles et plus rapides que ceux pour obtenir l\u0027énergie globale, ce critère permet également de repérer les cas où un cluster commence à en absorber d\u0027autres, ce qui arrive assez souvent avec l\u0027ICM. Un tel cas de figure provoquerait rapidement la convergence de la trace de A vers 1. Dans l\u0027algorithme (1), nous montrons comment nous avons adapté le framework EM-ICM pour notre critère d\u0027arrêt : Comme on peut le voir sur la Figure (2), le critère d\u0027arrêt basé sur l\u0027énergie aurait conduit l\u0027algorithme à s\u0027arrêter après la 8ème itération. Or, sur la Figure (1) l\u0027état de fusion des zones homogènes n\u0027est pas encore assez avancé après l\u0027itération 8 : La mer et une partie des bâtiments sont encore très fragmentés. Le critère de compacité aurait de son côté amené l\u0027algorithme à s\u0027arrêter après l\u0027itération 43 ce qui aurait conduit à des zones nettement plus homogènes : routes, eau, gros bâtiments, etc.\nOn notera également que les deux critères se stabilisent définitivement après l\u0027itération 47, itération après laquelle l\u0027image n\u0027évolue presque plus. Sur l\u0027image prise après la 50ème itéra-tion de l\u0027algorithme, on constate même le début assez marqué d\u0027un phénomène de détérioration avec certains clusters qui ont commencé à déborder sur d\u0027autres.\nDe cette première expérience, nous pouvons tirer les conclusions suivantes : Tout d\u0027abord, elle confirme la difficulté évoquée dans notre introduction de trouver un critère d\u0027arrêt idéal. En effet, le critère basé sur l\u0027énergie aurait ici arrêté la segmentation trop tôt et l\u0027énergie rebondit deux fois avant d\u0027atteindre sa stabilisation finale. Ensuite, on voit que notre critère basé sur la compacité semble plus stable : il n\u0027y a pas de rebond. Enfin, on notera que lorsque les deux critères semblent finalement se mettre d\u0027accord pour arrêter la segmentation (itération 43 pour la compacité, et itération 47 pour la stabilisation définitive de l\u0027énergie), on s\u0027aperçoit que nous sommes déjà dangereusement proche de la zone à partir de laquelle la segmentation commence à se détériorer.\nDonnées THR Strasbourg\nNotre seconde expérience a été effectuée sur un jeu de données construit à partir d\u0027une image satellite à très haute résolution de la ville de Strasbourg (Rougier et Puissant, 2014). Ce jeu de données pré-traitées utilise le modèle des super-pixels (agglomérats de pixels) avec des voisinages irréguliers : chaque super-pixel a entre 1 à 15 voisins. L\u0027image est représentée sous forme de 187.058 super-pixels ayant chacun 27 attributs radio-métriques et géométriques. On constate à l\u0027issue de cette expérience que notre indice de compacité tombe d\u0027accord avec l\u0027indice de qualité de clustering pour déterminer quand arrêter l\u0027algorithme. On notera tout de même qu\u0027il y a peu de différence en terme de qualité de résultats entre le moment d\u0027arrêt décidé par le critère de compacité, et celui décidé par le critère classique d\u0027énergie. Cependant, sur une image satellite à très haute résolution de cette taille, 2 itérations supplé-mentaires coûtent plusieurs minutes de calcul pour avoir dans le cas de cette expérience un résultat légèrement moins bon. Notre critère d\u0027arrêt semble donc être ici un choix plus judicieux pour décider d\u0027arrêter l\u0027algorithme au bon moment et économiser du temps de calcul.\nConclusion\nDans cet article, nous avons proposé une amélioration de l\u0027algorithme ICM pour la segmentation des images satellites à très haute résolution. Notre algorithme C-ICM introduit un nouveau critère d\u0027arrêt basé sur un modèle d\u0027énergie spécifique permettant d\u0027avoir des informations sur les relations entre les différents clusters. Notre critère repose ainsi sur la compacité et l\u0027homogénéité des clusters dans la segmentation plutôt que sur le traditionnel critère d\u0027énergie globale. Nos expériences préliminaires ont montré des résultats intéressants qui pourraient mener à une amélioration globale de l\u0027efficacité et à une vitesse accrue du traitement des\n"
  },
  {
    "id": "284",
    "text": "Introduction\nLe modèle des graphes conceptuels (Sowa, 1984;Chein et Mugnier, 2009) permet de repré-senter des connaissances sous la forme d\u0027un graphe étiqueté. Le modèle des graphes conceptuels utilise une représentation graphique visuelle des connaissances afin de faciliter la compré-hension pour les utilisateurs. La méthode d\u0027interrogation du modèle est basée sur l\u0027opération principale des graphes conceptuels, un homomorphisme de graphes appelé la projection : cette opération permet de déterminer si les connaissances exprimées dans un graphe conceptuel appelé graphe requête peuvent être déduites de celles exprimées dans la base de connaissances, représentée par un graphe conceptuel appelé graphe fait.\nLes objectifs de ce modèle sont proches d\u0027une partie de ceux des langages du Web sé-mantique tels que RDF, RDF-Schema ou OWL (Manola et al., 2004;Brickley et Guha, 2004;McGuinness et al., 2004) qui sont généralement interrogés en utilisant SPARQL (Garlik et al., 2013), une recommandation officielle du W3C disponible dans plusieurs outils. SPARQL offre plus de flexibilité par rapport aux graphes conceptuels dans l\u0027interrogation d\u0027une base de connaissances. D\u0027une part, SPARQL permet d\u0027exprimer une disjonction entre plusieurs parties d\u0027une requête (SPARQL utilise le mot « union ») et d\u0027identifier des parties comme obligatoires ou optionnelles. D\u0027autre part, SPARQL permet d\u0027interroger la base grâce à quatre types de requêtes : l\u0027interrogation, la sélection, la description et la construction. La requête d\u0027interrogation permet de savoir si la connaissance représentée par la requête est présente dans la base. La requête de sélection permet de trouver et extraire de la base des connaissances identifiées dans la requête comme importantes. La requête de description permet d\u0027obtenir des informations sur des connaissances de la requête. La requête de construction permet de déduire de nouvelles connaissances à partir de celles contenues dans la base.\nCet article propose de combiner la simplicité de la représentation visuelle des graphes conceptuels avec la puissance du modèle d\u0027interrogation du Web sémantique pour améliorer l\u0027expression des requêtes des graphes conceptuels. La contribution de cet article est triple. D\u0027abord, le graphe d\u0027interrogation est introduit. La notion de graphe d\u0027interrogation permet d\u0027exprimer des conditions de disjonction -un « ou » entre deux de ses sous-graphes -et des conditions d\u0027option -un sous-graphe est préféré mais non-nécessaire -dans un graphe conceptuel. Ensuite, le graphe d\u0027interrogation nous permet de définir un langage d\u0027interrogation pour les graphes conceptuels formé de quatre types de requêtes : requête d\u0027interrogation, requête de sélection, requête de description et requête de construction. Une requête d\u0027interrogation permet de savoir si le graphe de la requête est déductible du graphe fait. Une requête de sélection permet de trouver et extraire du graphe fait des sommets identifiés comme étant importants dans le graphe requête. Une requête de description permet d\u0027obtenir des informations complémentaires du graphe fait liées à un sommet particulier du graphe requête. Une requête de construction permet de déduire de nouvelles connaissances en utilisant les connaissances extraites du graphe fait. Enfin, l\u0027opération basique de calcul utilisée pour interroger et obtenir des réponses est introduite : la projection d\u0027un graphe d\u0027interrogation dans un graphe fait. Cette projection est définie en utilisant la projection classique d\u0027un graphe conceptuel dans un graphe fait.\nAucun langage générique d\u0027interrogation pour les graphes conceptuels n\u0027a encore été proposé, mais différentes idées ont été mises en avant. De même que la requête de sélection, la possibilité d\u0027identifier certains sommets dans un graphe requête pour facilement exploiter le résultat d\u0027une projection a déjà été proposé dans Sowa (1984). Notre approche est différente de Sowa (1984) dans lequel les sommets sont simplement marqués puisque nous proposons de les nommer pour pouvoir facilement les identifier dans le résultat de la projection. Dans la communauté de SPARQL, Corby et Faron-Zucker (2007) propose une implémentation de la recherche de motifs de graphes de SPARQL en utilisant la projection classique des graphes conceptuels. Notre approche, à l\u0027inverse, est de transposer les idées de SPARQL dans le modèle des graphes conceptuels.\nL\u0027article est organisé comme suit. La section 2 rappelle les bases du modèle des graphes conceptuels et de la projection. La section 3 présente le modèle des graphes d\u0027interrogation. La section 4 présente la projection d\u0027un graphe d\u0027interrogation dans un graphe conceptuel fait ainsi que la requête d\u0027interrogation, La section 5 présente la requête de sélection, La section 6 présente la requête de description, La section 7 présente la requête de construction. Enfin, la section 8 présente notre implémentation et donne quelques éléments pour comparer notre langage d\u0027interrogation des graphes conceptuels avec SPARQL.\nModèle des graphes conceptuels\nLe modèle des graphes conceptuels (Chein et Mugnier, 2009)  \nHomme Femme aPourPère(Humain, Homme)\n, ?) avec T C partiellement représenté par l\u0027arbre de gauche, T R partiellement représenté par l\u0027arbre de droite, et la signature ? de chaque relation est précisée à coté de chaque type de relation.\nExemple. La figure 1 montre un extrait du vocabulaire utilisé dans les exemples suivants.\nUn graphe conceptuel est un multigraphe biparti défini sur un vocabulaire. Un des ensembles de sommets est appelé l\u0027ensemble des sommets concepts, et l\u0027autre ensemble est appelé l\u0027ensemble des sommets relations, représentant les liens entre les concepts. Chaque sommet est étiqueté. Un sommet relation est étiqueté par un type de relations et un sommet concept est étiqueté par un couple formé d\u0027un type de concepts et d\u0027un marqueur. Si un concept c est le i-ème argument d\u0027une relation r, alors il y a une arête entre c et r étiquetée par i, et le type de concepts de c doit respecter les contraintes de la signature de r. Un sommet concept individuel est référencé par un marqueur individuel de I. Un sommet concept générique est ré-férencé par le marqueur générique * . Nous considérons les graphes sous forme normale : deux sommets concepts différents ne peuvent pas être étiquetés par le même marqueur individuel.\nUn graphe conceptuel défini sur V est un quadruplet G \u003d (C, R, E, l) qui satisfait les conditions suivantes :\n-(C, R, E) est un multigraphe fini, non-orienté et biparti. C est l\u0027ensemble des sommets concepts, R est l\u0027ensemble des sommets relations, E est l\u0027ensemble des arêtes \nLa projection est l\u0027opération d\u0027interrogation du modèle des graphes conceptuels. Soient un graphe requête G r et un graphe fait G f définis sur le même vocabulaire, G r se projette dans G f si les informations représentées par G r se déduisent de celles représentées par G f .\nLes sommets de G f qui correspondent aux sommets de G r sont appelés les images des sommets de G r par ?.\nUn bloc est défini comme un ensemble de sommets du graphe associé à un type de bloc. Tous les sommets concepts de l\u0027ensemble doivent avoir leurs sommets relations voisins dans l\u0027ensemble : ceci permet à un sommet concept du bloc d\u0027être caractérisé par les relations auxquelles il est lié. Différents types de blocs permettent d\u0027identifier la condition d\u0027un bloc : l\u0027Option (le bloc est dit optionnel), la Disjonction (le bloc est dit de disjonction) et le Standard (le bloc est dit standard).\nUn bloc optionnel permet d\u0027exprimer que la partie du graphe qu\u0027il contient est facultative : on recherche dans le graphe fait un graphe avec la partie optionnelle, mais si elle n\u0027est pas trouvée, un graphe privé de la partie optionnelle conviendra. Un bloc de disjonction est composé de blocs fils et exprime une disjonction entre ses blocs fils : on recherche dans le graphe fait un graphe avec seulement un des fils.\nUn bloc du graphe G est un couple b \u003d (S, T ) où -S ? C ? R est l\u0027ensemble des sommets du bloc tel que :\n?c ? S ? C, ?r ? R, si rc ? E alors r ? S -T ? {Standard, Option, Disjonction} est le type du bloc. Une arborescence est utilisée afin de structurer les blocs d\u0027un graphe requête. Les blocs sont les sommets de l\u0027arborescence. Les notions de bloc racine, bloc père et blocs fils sont définies grâce au vocabulaire lié aux arborescences. La structuration est hiérarchique, elle impose que la relation père-fils entre blocs vérifie l\u0027inclusion des sommets du bloc fils dans ceux du bloc père et que si deux blocs ont un sommet en commun, alors un de ces blocs est un ancêtre de l\u0027autre.\nStandard Disjonction\nDéfinition 5. Soit un graphe conceptuel G \u003d (C, R, E, l).\nUne structuration H de G est une arborescence (B, A, r) où la racine r est le bloc (C ? R, Standard), B étant l\u0027ensemble des blocs de G qui forme l\u0027ensemble des sommets de l\u0027arborescence, et A l\u0027ensemble des arcs. La structuration est telle que : H) est un couple où G est un graphe conceptuel et H est une structuration de G. \nProjection et requête d\u0027interrogation\nLes graphes d\u0027interrogation sont utilisés comme base de construction pour chacun des quatre types de requêtes.\nUn graphe d\u0027interrogation peut être développé en un ensemble de graphes conceptuels en suivant les conditions d\u0027option et de disjonction. Cet ensemble de graphes conceptuels, appelé l\u0027ensemble des graphes développés du graphe d\u0027interrogation, représente l\u0027ensemble des graphes conceptuels dont on cherche à savoir s\u0027ils sont déductibles du graphe fait. Cet ensemble permet de définir une projection d\u0027un graphe d\u0027interrogation dans un graphe fait comme étant une projection d\u0027un graphe développé dans le graphe fait. Pour obtenir un graphe développé à partir d\u0027un graphe d\u0027interrogation, des opérations sont appliquées sur le graphe d\u0027interrogation pour aboutir à un graphe ne contenant que des blocs standards. Les blocs optionnels seront retirés ou leur type sera changé en Standard. Le type des blocs de disjonction sera changé en Standard et seulement un et un seul de ses fils sera conservé.\nSoit b ? B, un bloc optionnel. Le graphe G i dans lequel le type de b est changé en Standard est un développement\nLe graphe G i dans lequel le type de b est changé en Standard et dans lequel tous les fils de b sauf un sont retirés est un développement de G i .\nL\u0027ensemble des graphes développés GD de G i est l\u0027ensemble de tous les graphes conceptuels associés aux graphes d\u0027interrogation ne contenant que des blocs standards obtenus grâce à une suite d\u0027applications de l\u0027opération de développement à partir de G i . Exemple. Les graphes conceptuels de la figure 4 forment l\u0027ensemble des graphes développés\nPour définir la projection d\u0027un graphe d\u0027interrogation dans un graphe fait, on construit les projections des graphes développés dans le graphe fait. La projection d\u0027un graphe développé est généralement une projection du graphe d\u0027interrogation. L\u0027exception est due à la sémantique de l\u0027option, en effet, la partie optionnelle d\u0027un graphe doit être présente dans la réponse si elle est présente dans le graphe fait : dans ce cas les projections qui ne contiendraient pas le bloc optionnel ne sont pas des projections du graphe d\u0027interrogation. Notons que ces dernières projections sont prolongées 2 par la projection contenant les informations optionnelles. On définit donc l\u0027ensemble des projections d\u0027un graphe d\u0027interrogation dans un graphe conceptuel comme l\u0027ensemble des projections des graphes développés du graphe d\u0027interrogation privé des projections prolongées.\nL\u0027ensemble des projections du graphe d\u0027interrogation G i dans G f est : \nRequête de sélection\nDans une requête d\u0027interrogation, l\u0027image de certains sommets peut être plus importante que l\u0027image des autres sommets de la requête. Cet article propose un type de requête dans lequel on peut identifier les sommets jugés importants et dont on veut retenir les images par la projection du graphe d\u0027interrogation : la requête de sélection.\nUne requête de sélection est donc un couple composé d\u0027un graphe d\u0027interrogation et d\u0027une fonction de nommage qui associe à des noms à sélectionner, des sommets du graphe.\nDéfinition 11. Soit un ensemble de noms N .\nUne requête de sélection est un couple\nUne réponse à une requête de sélection est une fonction qui associe à chaque nom de l\u0027ensemble des noms à sélectionner de la requête, un sommet du graphe fait. Si le graphe d\u0027interrogation se projette dans le graphe fait, les noms à sélectionner seront associés avec l\u0027image dans le graphe fait, si elle existe, du sommet du graphe d\u0027interrogation auxquels ils sont liés. Un sommet peut ne pas avoir d\u0027image par la projection, dans ce cas, le nom n\u0027est associé à aucun sommet. \nUne réponse à la requête de sélection R s est une fonction partielle de N dans C f ? R f associant à chaque élément n de N le sommet ?(select(n)) de G f s\u0027il existe.\nExemple. La figure 7 montre une requête de sélection R s \u003d [G i , select], où la fonction de nommage select, définie sur N \u003d {?prénom, ?surnom}, est représentée sur G i . La requête permet de connaître le prénom, et s\u0027il existe le surnom, de tous les humains du graphe fait. Les deux réponses à l\u0027exécution de R s dans G f (figure 5) sont présentées en partie droite de la figure 7.\nRequête de description\nLa requête de sélection permet d\u0027obtenir les images des sommets qui intéressent l\u0027utilisateur, la requête de description permet, elle, d\u0027obtenir non seulement les images des sommets qui intéressent l\u0027utilisateur, mais aussi leur description. La description d\u0027un sommet est formée par les sommets voisins qui apportent des informations sur ce sommet. Une requête de description est un couple composé d\u0027un graphe d\u0027interrogation et d\u0027une fonction de nommage qui à un nom de l\u0027ensemble des noms à décrire, associe un sommet du graphe. Étant donné que seuls les concepts peuvent être décrits, l\u0027ensemble à décrire ne doit contenir que des noms se référant à des sommets concepts.\nDéfinition 13. Soit un ensemble de noms N .\nUne requête de description est un couple\nUne réponse à une requête de description est un sous-graphe du graphe fait dans lequel figurent les images des concepts à décrire, les relations qui sont liées à ces concepts par une arête étiquetée par « 1 », et les concepts liés à ces relations. Seules les relations reliées par une arête étiquetée par « 1 » sont sélectionnées puisque se sont les relations dont le concept à décrire est le sujet. \nUne réponse à la requête de description R d est un sous-graphe\nExemple. La figure 8 montre une requête de description R d \u003d [G i , desc], où la fonction de nommage desc, définie sur N \u003d {?père}, est représentée sur G i . La requête permet de décrire tous les humains qui sont pères. Les trois réponses à l\u0027exécution de R d dans G f (figure 5) sont présentées en partie droite de la figure 8 : H2 et H3 sont pères, mais H2 est père à la fois de H1 et de F1.\nRequête de construction\nUne requête de construction est composée de deux graphes : un graphe d\u0027interrogation, appelé graphe condition, et un graphe conceptuel, appelé graphe modèle,. Le principe d\u0027une requête de construction est d\u0027une part d\u0027extraire des informations du graphe fait à l\u0027aide du graphe condition, qui doit se projeter dans le graphe fait, et d\u0027autre part d\u0027exploiter ces informations afin de construire un nouveau graphe conceptuel, appelé graphe réponse, à partir du graphe modèle. Une requête de construction peut être vue comme une règle (Salvat, 1998) dont la partie condition est définie par le graphe condition et la partie conclusion est obtenue par le graphe modèle. Pour lier les sommets du graphe condition à ceux du graphe modèle, on utilise une fonction lien qui associe à un sommet du graphe condition, un sommet du graphe modèle. Un sommet du graphe condition qui est lié à un sommet du graphe modèle doit avoir la même étiquette que ce dernier. Notons qu\u0027en utilisant l\u0027extension des types conjonctifs (Chein et Mugnier, 2004), il serait possible que deux sommets liés ne possèdent pas la même étiquette.\nest un graphe conceptuel appelé graphe modèle, tels que G et G m sont définis sur le même vocabulaire, lien est une fonction partielle définie comme suit : lien :\nUne réponse à une requête de construction est un graphe réponse construit sur le modèle du graphe modèle comme suit. Les sommets du graphe modèle liés aux sommets du graphe condition sont étiquetés par les images des sommets du graphe condition dans le graphe fait. Les sommets qui ne sont pas liés dans le graphe modèle gardent leurs étiquettes. Il se peut que des sommets du graphe condition ne possèdent pas d\u0027image par la projection, les sommets liés au graphe modèle sont alors retirés du graphe réponse, ainsi que toutes les relations éventuel-lement connectées à ces sommets. Ce cas se présente lorsqu\u0027un sommet du graphe modèle est lié à un sommet du graphe condition qui appartient à un bloc option, ou de disjonction.\nDéfinition 16. Soient un graphe conceptuel G f \u003d (C f , R f , E f , l f ), une requête de construction R c \u003d [G i , G m , lien] avec G i \u003d (G, H) où G \u003d (C, R, E, l) et avec G m \u003d (C m , R m , E m , l m ), une projection ? de G i dans G f , et l\u0027ensemble S ? des sommets de G qui ont une image dans G f par ?.\nUne réponse à la requête de construction R c est un graphe conceptuel appelé graphe ré-ponse G r \u003d (C r , R r , E r , l r ), tel que : -C r \u003d C m \\ {c \u003d lien(c ) ? C m |c ? C, c / ? S ? } -R r \u003d (R m \\ {r \u003d lien(r ) ? R m |r ? R, r / ? S ? }) \\ {r ? R m |rc ? E m , c / ? C r } -E r \u003d {nn ? E m |n, n ? C r ? R r } -l r est définie de la façon suivante ?s ? C r ? R r ? E r :\nsi ?s ? C ? R tel que s \u003d lien(s ), l r (s) \u003d l f (?(s )) sinon, l r (s) \u003d l m (s)\nExemple. La figure 9 montre la requête R c \u003d [G i , G m , lien] où la fonction lien est représentée directement sur le graphe via les pointillés. Cette requête peut s\u0027apparenter à une règle qui dit que si un humain a pour parent un autre humain, alors ce dernier est un parent du premier, mais si cet humain n\u0027a pas de parent, personne n\u0027est son parent. Les quatre réponses à l\u0027exécution de R c dans G f (figure 5) sont présentées en partie droite de la figure 9.\n8 Conclusion\n"
  },
  {
    "id": "286",
    "text": "Introduction\nLes réseaux sociaux sont l\u0027objet d\u0027une recherche intense depuis plusieurs années (Carrington et al., 2005;Newman et al., 2006;Scott et Carrington, 2011). Leur étude donne lieu à différentes questions concernant leur évolution, qu\u0027il s\u0027agisse d\u0027analyser comment les interactions se sont mises en place, ou alors de comprendre l\u0027état du système qu\u0027ils décrivent. Parmi ces interrogations, l\u0027étude des phénomènes de propagation dans les réseaux a suscité un intérêt soutenu au sein de la communauté, multipliant les domaines d\u0027applications, allant de la sociologie (Granovetter, 1978;Macy, 1991) à l\u0027épidémiologie (Hethcote, 2000;Dodds et Watts, 2005;Bertuzzo et al., 2010) en passant par la publicité virale et le placement de produits (Domingos et Richardson, 2001;Chen et al., 2010).\nNous nous intéressons dans cet article à l\u0027étude de la propagation dans les réseaux sociaux. Notre objectif est de proposer une méthodologie permettant de comparer des modèles préexistants et documentés de propagation. Le grand nombre et les différentes variations de ces derniers offrent un assortiment de solutions compliquant le choix d\u0027un modèle particulier. Afin de faciliter cette tâche, il convient de pouvoir comparer effectivement les modèles et non seulement les résultats finaux obtenus suite à leur application.\nCette ambition rejoint Kempe et al. (2003) qui proposent une généralisation de différents types de modèles de propagation. Ces résultats permettent de voir les modèles dans un cadre entièrement mathématique où chacun des algorithmes devient une solution à un problème d\u0027optimisation commun. A l\u0027inverse, nous adoptons une perspective résolument algorithmique dont l\u0027objectif est de venir en appui à une approche exploratoire.\nIl n\u0027existe à notre connaissance pas de formalisme unifiant toutes les approches permettant d\u0027effectuer une comparaison des modèles, de leur formulation, leur complexité ou leurs performances. La première contribution de cet article est donc de proposer un tel cadre unificateur basé sur un formalisme solide : la réécriture de graphes.\nLa propagation est généralement vue comme un phénomène global au réseau bien qu\u0027elle émerge en réalité de la somme d\u0027une multitude d\u0027évènements y agissant localement. La plupart des modèles consistent donc en un ensemble de règles décrivant les situations dans lesquelles une entité peut influencer ses voisins. Bien que chacun de ces évènements soit décrit localement et succinctement, l\u0027application répétée de transformations locales permet de faire émer-ger le comportement du modèle au niveau global. Dans ce formalisme, un modèle correspond alors à un ensemble de règles de transformation couplé à une stratégie qui régule et ordonne l\u0027application de ces mêmes règles.\nLes modèles auxquels nous nous intéressons par la suite considèrent un réseau social dont la topologie est fixée. Les règles décrivent alors comment évoluent les états des sommets du ré-seau. Dans leurs travaux, Kejžar et al. (2008) se sont intéressés, de façon similaire, à l\u0027évolution du caractère topologique d\u0027un réseau social. Partant d\u0027un réseau pré-existant, les auteurs proposent une série de règles permettant de modifier les connexions entre les acteurs du réseau, autorisant ainsi la création ou suppression de liens. Leur travail rejoint donc notre approche consistant à exploiter la réécriture comme mécanisme pour exprimer leurs modèles d\u0027évo-lution des réseaux. Cependant, leur article est davantage orienté vers l\u0027analyse des résultats asymptotiques probabilistes sur l\u0027évolution de la taille et la densité des réseaux ainsi produits. L\u0027intérêt de notre approche basée sur une description commune des modèles tient égale-ment à la possibilité d\u0027étudier et de comparer ceux-ci de manière expérimentale. La plupart des travaux s\u0027intéressent à l\u0027objectif atteint au terme d\u0027une propagation (couverture du réseau, vitesse de propagation, etc.), en revanche, il est plus difficile d\u0027établir des résultats expliquant comment se déroule la propagation et pourquoi cet objectif est atteint. L\u0027utilisation d\u0027un formalisme commun nous permet, au contraire, la réalisation de ce type d\u0027investigation.\nDe plus, cette méthodologie prend un sens particulier lorsque l\u0027étude des modèles se fait de manière visuelle et interactive. En manipulant le modèle (en lançant des simulations, en isolant une règle, etc.), l\u0027utilisateur est à même de développer une connaissance du modèle ainsi que de mesurer et suivre son comportement au fil du déroulement. Pour ces raisons, nous présentons une plate-forme de visualisation analytique -exploitant une version étendue de PORGY (Pinaud et al., 2012) (voir Fig. 1) -pour, simultanément, construire les réseaux et règles de réécriture, simuler la propagation selon différentes stratégies (i.e. les modèles) et comparer les traces d\u0027exécution de ces dernières à l\u0027aide de divers critères.\nL\u0027article introduit d\u0027abord la terminologie propre aux modèles de propagation des réseaux et décrit deux modèles particuliers (section 2). Ces modèles sont ensuite exprimés à l\u0027aide de règles de réécriture illustrant ainsi le pouvoir d\u0027expression et l\u0027utilisabilité du formalisme (section 3). Nous montrons enfin comment la plate-forme de visualisation peut être utilisée pour étudier les modèles et exhiber leurs différences (section 4).\nFIG. 1: Interface de PORGY :\n(1) le réseau social sur lequel on applique la propagation ; (2) édition d\u0027une règle ; (3) portion de l\u0027arbre de dérivation, conservant une trace complète des calculs réalisés (le graphe (1) représente un des sommet de celui-ci) ; (4) courbe montrant l\u0027évolution du nombre de sommets actifs ; (5) autre représentation de l\u0027arbre de dérivation ; (6) éditeur de stratégies.\nModélisation de la propagation dans les réseaux sociaux\nUn réseau social (Brandes et Wagner, 2003)   (2010)) construits au fil du déroulement de la propagation et qui peuvent être utilisés pour mesurer l\u0027influence d\u0027un utilisateur sur ses voisins ou représenter sa tolérance à la réalisation d\u0027une action (plus un utilisateur est sollicité, plus il sera enclin à s\u0027activer ou inversement).\nFace à cette diversité, nous nous limitons dans la suite de l\u0027article à illustrer la faisabilité de notre approche sur deux modèles représentatifs : un modèle à cascades indépendantes (IC, Kempe et al. (2003)) utilisé comme base pour de nombreux cas, et un modèle à seuils linéaires (LT, Goyal et al. (2010)) qui exploite un principe d\u0027activation non probabiliste contrairement au modèle précédent :\nLe modèle à cascades indépendantes IC. Ce modèle comporte de nombreuses variations (e.g. Gomez-Rodriguez et al. (2010); Watts (2002)) permettant, par exemple, la propagation d\u0027opinions divergentes dans un même réseau (Chen et al., 2011). Nous le décrivons sous une forme basique, telle que proposée par Kempe et al. (2003).\nSoit un sous-ensemble de sommets A 0 ? V activés au temps t \u003d 0 et les probabilités p v,w , définies pour toutes paires de sommets voisins {v, w} pour représenter l\u0027influence de v ? A 0 sur w / ? A 0 . Une série de nouveaux ensembles de sommets activés A t+1 est calculé à partir de A t . Pour chacun des sommets dans A t , on visite les voisins w ? N (v) qui n\u0027ont pas déjà été activés (mais qui peuvent déjà avoir été visités) ; en d\u0027autres mots, w ? N (v) \\ ? t i\u003d0 A i . Un sommet w peut alors devenir actif avec une probabilité p v,w , auquel cas il est ajouté à A t+1 . L\u0027algorithme s\u0027arrête lorsque A t+k est vide (pour k ? 0).\nModèle à seuil linéaire LT. Ce modèle suit un déroulement différent du précédent. Il le rejoint cependant en ce qu\u0027un sommet ne change plus d\u0027état dès lors qu\u0027il est activé. On suppose donné, soit aléatoirement, soit appris selon un historique d\u0027actions connues, les probabilités p v,w . Chaque sommet w ? V est aussi équipé d\u0027un seuil ? w . Soit S w l\u0027ensemble des voisins du sommet w qui sont activés. On détermine l\u0027ensemble A t+1 en calculant pour chaque sommet w non encore activé la valeur d\u0027influence jointe p w (S) \u003d 1 ? v?Sw (1 ? p v,w ). Le sommet w devient ainsi actif dès que l\u0027influence de ses voisins excède son seuil d\u0027activation, c\u0027est à dire lorsque p w (S) ? ? w . \nRéécriture de graphes\nLes éléments de base du calcul de la réécriture sont des sommets du graphe, additionnellement équipés de ports, auxquels les arêtes vont se connecter. Plus généralement, les sommets, ports et arêtes vont avoir des propriétés associées à une valeur (par exemple la probabilité p v,w ou le nom donné à un port) qui permettront de les distinguer entre eux, une combinaison spécifique des ces propriétés pouvant être identifiée comme un état.\nUne règle de réécriture consiste en un couple L ? R où L et R sont eux-mêmes des graphes (souvent petits). L et R sont respectivement appelés les membres gauche et droit de la règle. L\u0027application d\u0027une règle sur un graphe G se fait en localisant dans G un sousgraphe H isomorphe à L et en le \"remplaçant\" par R. La notion d\u0027isomorphisme doit toutefois être étendue pour tenir compte des états des sommets, des ports et des arêtes. La règle doit également, le cas échéant, préciser comment traiter les arêtes incidentes aux ports de H qui ne sont pas mentionnées dans L. De plus, une règle peut aussi calculer les valeurs de plusieurs propriétés de R en fonction de celles de H.\nDes exemples de règles sont donnés figure 2. La règle 2a concerne une paire de sommets, l\u0027un dans l\u0027état activé (vert), l\u0027autre étant non activé (rouge), connectés par une arête allant du port In du sommet activé vers le port Out du sommet non activé. La règle maintient la connexion entre les sommets et modifie l\u0027état du sommet rouge, le faisant passer dans l\u0027état violet signifiant que le sommet a été visité (ses propriétés ont été lues et il est possible de tenter de l\u0027activer). La règle 2b ne concerne qu\u0027un seul sommet ; son application est donc potentiellement réalisable sur tout sommet du graphe à condition qu\u0027il soit dans le même état (ici visité, symbolisé par la couleur violette). La règle consiste simplement à modifier l\u0027état du sommet, le faisant passer de l\u0027état violet à l\u0027état vert (activé).\nLe défi consiste la plupart du temps à savoir prédire le comportement de la réécriture ré-pétée de règles sur un graphe. En effet, l\u0027exécution des règles n\u0027est pas déterministe puisque leur ordre d\u0027application n\u0027est, a priori, pas précisé, mais aussi parce qu\u0027elles peuvent être appliquées sur de multiples instances H du membre gauche L de la règle. Il devient alors intéressant de savoir si le calcul de la réécriture converge et s\u0027il est confluent.\nDans cet optique, il peut également être tentant de chercher à conditionner l\u0027ordre d\u0027application des règles afin de guider le comportement du calcul. A cette fin, il est possible de définir une stratégie d\u0027application des règles. Une stratégie permet de choisir un ensemble des règles à appliquer, préciser leur ordre, le nombre de répétitions, et l\u0027endroit où celles-ci peuvent être ap-pliquées. Pour plus de détails sur le langage de stratégies utilisé par la plate-forme PORGY, sa formalisation et ses propriétés en tant que langage formel, le lecteur pourra consulter l\u0027article de Fernandez et al. (2014).\nTraduction des modèles de propagation\nLe premier défi qui se pose à nous est de pouvoir donner, pour chacun des modèles présen-tés dans la section 2, un ensemble de règles et une stratégie d\u0027application de ces dernières qui permet d\u0027émuler le fonctionnement du modèle.\nNotre démarche de traduction d\u0027un modèle de propagation quelconque en une série de règles de réécriture et leur stratégie d\u0027application est aisément généralisable. Celle-ci a pu être appliquée à tous les modèles rencontrés dans la bibliographie étudiée. Pour la clarté de la discussion concernant les étapes à suivre, nous présentons uniquement la traduction du modèle de propagation à cascades indépendantes (section 2). Ce dernier illustre tout à fait les opérations à réaliser et toute procédure de traduction d\u0027autres modèles suit un déroulement similaire.\nLe motif (membre gauche d\u0027une règle) principal à rechercher pour faire évoluer la propagation consiste à identifier un couple de sommets voisins dont l\u0027un est activé et l\u0027autre ne l\u0027est pas. Il est ainsi nécessaire de conserver pour chaque sommet son état actuel. De manière similaire, chaque arête devra préserver les probabilités d\u0027influence p v,w et p w,v que ses extrémités v et w pourront imposer l\u0027une sur l\u0027autre lorsque l\u0027un des sommets s\u0027activera.\nLa stratégie employée consiste, pour chaque sommet non actif, à calculer puis stocker l\u0027influence de ses voisins actifs en ne conservant que la valeur pour le sommet qui a l\u0027influence la plus forte. Le sommet non actif passe alors dans l\u0027état visité (règle de la figure 2a). Le parcours du voisinage est, de cette manière, contrôlé par la stratégie tandis que les actions à effectuer sur le réseau sont contrôlées par les règles. La procédure décrite ci-dessus forme une stratégie qui sera répétée tant qu\u0027un sommet actif peut influencer un de ses voisins (il reste une arête non marquée qui permet d\u0027appliquer la règle 2a). Chaque application de règle entraîne l\u0027ajout d\u0027un sommet sur l\u0027arbre de dérivation et d\u0027une arête (de couleur violette) pour montrer la succession des opérations. Les points de départ et d\u0027arrivée d\u0027une stratégie (enchaînement de plusieurs règles) sont eux représentés par une arête verte (Fig. 1). Cet arbre peut rapidement atteindre une taille conséquente, rendant la lisibilité difficile. Nous pouvons néanmoins le filtrer et ne conserver que les arêtes vertes et les sommets correspondants (Fig. 4).\nVisualisation analytique et comparaison des modèles\nNous détaillons dans cette partie comment la plate-forme de visualisation PORGY (Pinaud et al., 2012) est utilisée pour comparer deux applications des modèles de propagation présentés au début de cet article. Nous avons utilisé le modèle de Wang et al. (2006) pour générer un réseau social aléatoire de 300 sommets. Le réseau obtenu a 597 arêtes. Les conditions de départ sont identiques pour les deux modèles : même ensemble initial de sommets activés et même distribution de probabilités d\u0027influence entre les sommets. Notre objectif n\u0027est pas de montrer que tel modèle de propagation est meilleur que tel autre (ceci nécessiterait de nombreuses simulations pour calculer les résultats moyens sur les modèles probabilistes) mais plutôt de comprendre comment les modèles fonctionnent 1 . Les applications successives de la stratégie décrite précédemment permettent aux sommets actifs de transmettre l\u0027information ou l\u0027action représentant le sujet de la propagation à leur voisins. Chaque exécution de règle va créer un état intermédiaire du graphe d\u0027origine qui sera conservé dans la trace de la propagation (passage des différents sommets de non visité à visité puis potentiellement actif ). Cet historique va pouvoir être exploité pour étudier et comparer le graphe à un instant donné ou pour reconstituer et suivre le chemin emprunté par le processus d\u0027activation des sommets.\nUn arbre de dérivation (voir fig.1) est ainsi créé et maintenu pour fournir toutes ces informations. En visualisant des états successifs, nous pouvons observer cette progression. La figure 3 présente quelques vignettes d\u0027une vue de type Small-Multiples, qui est une partie du graphe analysé, et montre l\u0027évolution de l\u0027état de ses sommets. Les différents temps t représentent les applications successives des stratégies (un sommet non visité à t peut donc se retrouver activé à t + 1). L\u0027arbre de dérivation nous permet immédiatement de montrer quel est le modèle qui nécessite le moins d\u0027étapes de calcul ou le moins de lancements de stratégie avant d\u0027arriver à terme car sa branche est la plus courte (figure 4). Nous avons ainsi une première approximation de la complexité en temps des algorithmes.\nLier la profondeur de l\u0027arbre (donc le temps) avec d\u0027autres mesures nous permet de considérer l\u0027évolution de différents paramètres tout au long de la propagation. Nous pouvons, par exemple, aborder la notion de vitesse de propagation, valeur indiquant l\u0027évolution du nombre de sommets actifs en fonction du temps. La figure 4 présente l\u0027évolution de cette valeur pour une exécution des modèles à cascades indépendantes (partie droite, courbe du haut) et à seuils linéaires (courbe du bas). Les courbes présentées n\u0027ont pas les mêmes échelles puisqu\u0027elles sont calculées indépendamment pour chacun des modèles. Malgré ceci, nous pouvons observer que le modèle à cascades va parvenir à activer environ 80% des sommets contre seulement 18% pour celui à seuils (pour un même nombre d\u0027étapes de réécritures), démontrant le fort impact sur les performances des modèles, dans un premier temps, des valeurs utilisées pour l\u0027initialisation des probabilités d\u0027influence, et dans un second temps, du choix de l\u0027ensemble de sommets initialement activés. Nous avons utilisé une loupe (fonctionnalité de PORGY) sur le haut de chaque axe pour rendre les valeurs lisibles. Nous pouvons aussi noter qu\u0027après la première application de la stratégie, le nombre de sommets activés est très proche pour chacun des modèles, les différences apparaissant et se confirmant par la suite. Il peut aussi être intéressant de voir l\u0027état du graphe quand le nombre de sommets activés atteint un seuil. Puisque les différentes vues sont liées, la sélection d\u0027un sommet/arête (en bleu sur la figure 4) de l\u0027arbre de dérivation entraîne sa sélection dans la courbe et vice-versa. De manière similaire, la sélection d\u0027un sommet lors d\u0027une étape de la propagation sera immédia-tement répercutée à l\u0027ensemble des étapes contenant ce même élément, rendant la sélection visible même sur les sommets de l\u0027arbre de dérivation. D\u0027après la méthodologie employée par l\u0027application PORGY, tant qu\u0027un élément n\u0027est pas modifié par une règle, il n\u0027est jamais changé. En conséquence, la sélection d\u0027un sommet d\u0027intérêt dans l\u0027un des graphes intermé-diaires représentant le réseau permet de savoir directement quand cet élément a changé d\u0027état, surtout si l\u0027on travaille sur la version complète de l\u0027arbre de dérivation (montrant le détail des applications de règles).\nFinalement, le type de mesure évoluant selon le temps peut être généralisé à d\u0027autres propriétés des modèles de propagation. La notion de sommet visité, introduite précédemment, peut présenter un intérêt, dans le cas où un message doit seulement être vu et non nécessai-rement redistribué ou propagé par les utilisateurs. Cette vitesse de connaissance du contenu de la propagation est observable de manière similaire à la vitesse de propagation. De plus, en considérant ces deux mesures, nous pouvons en proposer une troisième exprimant l\u0027efficacité d\u0027une propagation, calculable grâce au rapport entre le nombre de sommets activés à un instant t et ceux visités/influencés au moment précédent t ? 1.\nConclusion et travaux futurs\nNous avons présenté un formalisme basé sur la réécriture de graphes vu comme un langage commun à l\u0027expression de tous les modèles de propagation sur réseaux. Lorsque la propagation n\u0027entraîne pas de modifications de la topologie du graphe, le modèle consiste en un ensemble réduit de règles gérant les transitions d\u0027état des sommets du graphe. Le stockage des états des sommets, les règles et le langage de stratégie qui pilote leur application facilite la gestion du caractère probabiliste des modèles.\nNous envisageons d\u0027étendre notre étude à un panel plus large de modèles de propagation afin de démontrer le caractère \"universel\" de notre approche. Cela exige aussi de pouvoir multiplier les simulations sur des réseaux de tailles conséquentes. Cet aspect pose un défi en raison de la complexité liée à la recherche de motifs correspondant aux membres gauches des règles -d\u0027abord parce que nous faisons face à un problème NP-Complet (isomorphisme de sous-graphes), mais aussi à cause de l\u0027explosion combinatoire qu\u0027elle engendre et qui doit être gérée à l\u0027aide du langage de stratégie (d\u0027application des règles).\nLa formulation des modèles à l\u0027aide de réécritures offre une possibilité nouvelle qui permettra de combiner propagation dans le réseau et évolution de la topologie du réseau sur lequel la propagation a lieu. Là encore, un langage de stratégie facilitera la gestion de l\u0027application simultanée ou alternée de ces deux types de transformations. Il n\u0027existe pas, à notre connaissance, de tels modèles. Nous espérons ainsi pouvoir proposer des modèles réalistes d\u0027évolution de réseaux, dont le réalisme tiendrait à la fois aux caractères structurels des réseaux produits, mais aussi à leur qualité en terme de circulation de l\u0027information.\ncan only be made at the cost of describing models based on a common formalism and independant from them. We propose to use graph rewriting to formally describe the propagation mechanisms as local transformation rules applied according to a strategy. This approach makes complete sense when supported by a visual analytics framework dedicated to graph rewriting. The paper first presents several models and illustrates them through selected simulations. We then show how our visual analytics framework allows to interactively manipulate models, and underline their differences based on measures computed on simulation traces.\n"
  },
  {
    "id": "287",
    "text": "Introduction\nL\u0027évolution d\u0027ontologie est un sujet posé avec l\u0027apparition des méthodologies de construction d\u0027ontologies. Il s\u0027est avéré indispensable de penser à maintenir et faire évoluer les ontologies, après leur construction, afin d\u0027assurer leur réutilisation et leur continuité. Ce besoin s\u0027est rapidement développé avec la prolifération des ontologies et leur large utilisation. À titre d\u0027exemple, depuis Janvier 2010, 59 versions de l\u0027ontologie Gene Ontology 1 (une des plus fameuses ontologies) ont été publiées à raison d\u0027une version par mois. Ainsi, afin de définir et gérer le processus d\u0027évolution, plusieurs méthodologies ont été proposées dans la littérature (Klein, 2004;Stojanovic, 2004;Djedidi et Aufaure, 2010;Khattak et al., 2013). Les premiers travaux ont pensé à définir ce qu\u0027est une évolution d\u0027ontologie. D\u0027où la définition proposée par (Stojanovic, 2004) : \"l\u0027évolution d\u0027ontologie est l\u0027adaptation, dans le temps, d\u0027une ontologie aux besoins de changement et la propagation cohérente des changements aux artefacts dépen-dants\". Cette définition a ouvert le débat sur la signification des changements ontologiques, leurs formalisations et leurs types (Klein, 2004;Stojanovic, 2004). Ainsi, un changement ontologique est une modification d\u0027une ou plusieurs entités ontologiques (classe, propriété, axiome, individus, etc.). Il peut viser la modification de la structure de l\u0027ontologie (ex. ajout de classe, ajout de propriété) et on parle dans ce cas de l\u0027enrichissement d\u0027ontologie. Il peut viser égale-ment l\u0027ajout d\u0027individus et on parle alors du peuplement d\u0027ontologie. Les changements ontologiques sont souvent de trois types (Stojanovic, 2004) : 1) les changements élémentaires qui représentent une opération primitive et non décomposable qui affecte une seule entité ontologique (ex. renommer une classe) ; 2) les changements composés (composites) qui affectent une entité ontologique et ses voisins (ex. suppression d\u0027une classe) ; 3) les changements complexes qui expriment un enchainement de plusieurs changements élémentaires et/ou composés (ex. fusion de classes). En effet, les changements composés et complexes sont des changements utiles et demandés par l\u0027utilisateur. Ils englobent plusieurs modifications en une seule opéra-tion, ce qui lui permet d\u0027adapter son ontologie d\u0027une manière plus facile sans se perdre dans les détails des changements élémentaires (Klein et Noy, 2003). Cependant, la définition et la formalisation de ces changements sont des tâches non triviales comme leur application peut affecter la cohérence de l\u0027ontologie. Deux types de cohérence sont généralement distingués dans la littérature : 1) la cohérence conceptuelle qui se réfère aux règles structurelles et contraintes du langage de représentation de l\u0027ontologie (ex. inexistence de concepts isolés) ; 2) la cohé-rence sémantique qui se réfère à la cohérence logique de l\u0027ontologie dans le sens où elle ne doit pas comporter des contradictions logiques (ex. ne pas avoir des relations contradictoires entre deux concepts). En effet, la préservation de la consistance de l\u0027ontologie et la résolution des incohérences résultantes de l\u0027application des changements ontologiques sont encore des problématiques insuffisamment étudiées. Ainsi, nous proposons dans cet article une nouvelle formalisation des changements ontologiques composés et complexes permettant : 1) d\u0027éviter les inconsistances d\u0027une manière a priori en utilisant les concepts des grammaires de graphes typés ; 2) de réduire le nombre de changements élémentaires constituant les changements composés/complexes. Les changements étudiés traitent à la fois le niveau structurel de l\u0027ontologie (l\u0027enrichissement de l\u0027ontologie) et aussi le niveau assertionnel (le peuplement d\u0027ontologie).\nLe reste de l\u0027article sera organisé comme suit : la section 2 présente un tour d\u0027horizon sur les principales approches d\u0027évolution d\u0027ontologie. La section 3 introduit les concepts de base des grammaires de graphes. La section 4 propose une nouvelle formalisation des changements ontologiques composés et complexes. Enfin, une conclusion synthétise le travail présenté et donne les perspectives envisagées.\nÉtat de l\u0027art\nDe nombreuses approches ont été proposées dans la littérature pour définir et implémenter le processus d\u0027évolution d\u0027ontologies. Le Tableau 1 présente certaines approches tout en préci-sant les langages utilisés, l\u0027implémentation, la gestion des inconsistances et leurs spécificités. Ainsi, nous pouvons observer que différents langages ont été étudiés : KAON (Stojanovic, 2004), RDF (Luong et Dieng-Kuntz, 2007), OWL (Klein, 2004;Djedidi et Aufaure, 2010), etc. En se basant sur ces langages, plusieurs changements ontologiques ont été définis et différentes classifications de ces changements ont été proposées (Stojanovic, 2004;Klein, 2004). Ainsi, certains travaux se sont intéressés à l\u0027étude des changements élémentaires (Mahfoudh et al., 2013). D\u0027autres ont traité également les changements composés et complexes (Djedidi et Aufaure, 2010;Javed et al., 2013;Liu et al., 2014). Des travaux se sont focalisés sur l\u0027enrichissement d\u0027ontologies (Klein, 2004). D\u0027autres ont étudié aussi le peuplement d\u0027ontologies (Luong et Dieng-Kuntz, 2007;Djedidi et Aufaure, 2010;Mahfoudh et al., 2013). La résolution des inconsistances est encore insuffisamment étudié. En effet, certaines approches ont ignoré cet axe comme ils se sont intéressés à d\u0027autres problématiques, comme par exemple la gestion des versions des ontologies (Hartung et al., 2013). D\u0027autres travaux se sont focalisés plutôt sur l\u0027identification des inconsistances sans les résoudre (Gueffaz et al., 2012). Certains chercheurs se sont intéressés également par la résolution des inconsistances (Djedidi et Aufaure, 2010;Luong et Dieng-Kuntz, 2007;Javed et al., 2013). Cependant, les approches proposées admettant un processus a postériori de traitement des inconsistances qui nécessite l\u0027utilisation d\u0027une ressource externe (tel qu\u0027un raisonneur) afin de vérifier la consistance de l\u0027ontologie évoluée. Afin d\u0027éviter l\u0027utilisation d\u0027un raisonneur externe, Mahfoudh et al. (2013)  -Approche basée sur les patrons de conception.\n-Évaluation de la qualité de l\u0027ontologie évoluée.\n-Approche nécessitant des activités lourdes. (Gueffaz et al., 2012) OWL DL Prototype -Identification des inconsistances en utilisant le checker NuSMV.\n-Approche d\u0027évolution d\u0027ontologies, CLOCk (Change Log Ontology Checker) basée sur le modèle checking.\n-Approche nécessitant la transformation des ontologies OWL au langage NuSMV. (Hartung et al., 2013) OBO \nGrammaires de Graphes Typés\nLes grammaires de graphes, également appelées réécriture de graphes, sont un formalisme mathématique pour représenter et gérer les graphes. Elles permettent la modification de graphes via des règles de réécriture tout en précisant quand et comment faire les changements. Grâce aux concepts et outils qu\u0027elles proposent, les grammaires de graphes sont utilisées dans plusieurs branches de l\u0027informatique, comme par exemple la modélisation des systèmes logiciels et la théorie des langages formels (Ehrig et al., 1996). Elles ont récemment été introduites dans le domaine des ontologies, ce qui a donné naissance à des travaux traitant de la formalisation des ontologies modulaires (d\u0027Aquin et al., 2007), la représentation des graphes RDF (Resource Description Framework) (Braatz et Brandt, 2010), la fusion d\u0027ontologies (Mahfoudh et al., 2014a), etc. Dans ce qui suit, nous dressons un tour d\u0027horizon des définitions de base concernant les fondements théoriques de la réécriture de graphes. Graphe. Un graphe G(N, E) est une structure composée par un ensemble de noeuds (N ), d\u0027arêtes (E) et d\u0027une application s : E ? N × N qui attache les noeuds source/destination à chaque arête. Graphe attribué. Un graphe attribué est un graphe étendu par un ensemble d\u0027attributs A, une fonction d\u0027attribution att : N ? E ? P(A), P représentant l\u0027ensemble des parties, et une fonction d\u0027évaluation val : A ? V . Ainsi, chaque noeud ou arête peut avoir un ensemble d\u0027attributs (P(A)) dont les valeurs seront données par val.\navec N T correspond aux types des noeuds et E T aux types des arêtes. Grammaires de graphes typés. Une grammaire de graphe typé est une structure mathéma-tique définie par T GG \u003d (G, T G, P ) avec :\n-G est un graphe initial, appelé aussi graphe hôte ; -T G est un graphe type précisant le type de l\u0027information représentée dans le graphe hôte (type des noeuds et des arêtes) ; -P un ensemble de règles de réécriture, appelées aussi règles de production ou de transformations de graphes. Une règle de réécriture r est une paire de graphes pattern (LHS, RHS) avec : 1) LHS (Left Hand Side) représente la pré-condition de la règle de réécri-ture et décrit la structure qu\u0027il faut trouver dans un graphe G pour pouvoir appliquer la règle ; 2) RHS (Right Hand Side) représente la post-condition de la règle de réécriture et doit remplacer LHS dans G. Les règles peuvent également avoir des conditions supplémentaires appelées N AC (Negative Application Conditions). Ce sont des graphes pattern définissant des conditions ne devant pas être vérifiées pour que la règle de réécriture puisse être appliquée. La transformation de graphe consiste ainsi à définir comment un graphe G peut être transformé en un nouveau graphe G . Cette transformation peut être réalisée selon deux types d\u0027approches (Rozenberg, 1999) : les approches ensemblistes (Node replacement, Edge replacement, etc.) et les approches algébriques. Dans ce travail, nous utilisons les approches algébriques basées sur le concept de pushout de la théorie des catégories (Ehrig et al., 1973). Pushout. Soient trois objets de la catégorie des graphes :\n. A partir de là, deux variantes sont proposées pour la réécriture des graphes : le Simple pushout SPO (Löwe, 1993) et le Double poushout DPO (Ehrig, 1979). Dans ce travail, seule l\u0027approche SPO a été considérée car elle se voit plus générale et permet l\u0027application des différents changements ontologiques (Mahfoudh et al., 2014b). Ainsi, appliquer une règle de réécriture à un graphe initial G, selon la méthode SPO, revient à : 1) trouver un morphisme (m) permettant d\u0027identifier un sous-graphe de graphe G qui correspond (match) avec la partie LHS (m : LHS ? G) ; 2) appliquer la règle de réécriture sur le sous-graphe en le remplaçant par m(RHS) et supprimant les arêtes suspendues, i.e. les arêtes qui ont une extrémité non liée à un noeud. Ainsi, d\u0027une manière générale, nous avons SP O(G, LHS, RHS) \u003d G .\nFormalisation des changements ontologiques 4.1 Modèle de transformation de graphes\nAfin de représenter les ontologies avec le formalisme de grammaires de graphes, nous considérons une ontologie comme un graphe hôte G possédant une relation de typage avec le graphe type (T G), où T G représente le méta-modèle de l\u0027ontologie. Pour être conforme aux standards, c\u0027est OWL qui a été retenu comme méta-modèle d\u0027ontologies. Ainsi, les types des noeuds considérés sont : N T \u003d {Class(C), P roperty(P ), ObjectP roperty(OP ), DataP roperty(DP ),-Individual(I), DataT ype(D), Restriction(R)}.\nLes types des arêtes correspondent aux axiomes utilisés pour relier les différentes entités :\nLes changements ontologiques sont formalisés par un ensemble de règles de réécriture :\nDans cette définition étendue, CHD correspond à l\u0027ensemble des changements dérivés ajoutés à un changement ontologique pour corriger ses éventuelles inconsistances. La Figure 1 montre une représentation et une application de la règle de réécriture du changement ontologique AddIndividual. Elle permet d\u0027ajouter un individu \"Pascal\" tout en spé-cifiant son type, la classe \"Person\". La règle assure, grâce au N AC, la non redondance de données, i.e. elle empêche l\u0027application du changement dans le cas où l\u0027individu existe déjà dans l\u0027ontologie. \nFormalisation des changements ontologiques\nCette section présente notre formalisation des changements ontologiques composés et complexes par les grammaires de graphes typés. Avant de détailler cette formalisation, une brève introduction des changements élémentaires est indispensable pour mieux comprendre le reste de l\u0027article. Ainsi, les changements élémentaires englobent les changements de renommage, l\u0027ajout et la suppression de certains concepts. Ils n\u0027affectent qu\u0027une seule entité ontologique bien qu\u0027ils dépendent d\u0027autres entités. Le Tableau 2 présente quelques changements élémen-taires adressés dans notre travail et les concepts dont ils sont dépendant. À noter que les N ACs des règles de réécriture sont déduites à partir de ces interdépendances. Un exemple du changement élémentaire AddIndividual est déjà présenté dans la section 4.1. \nChangements ontologiques composés\nLes changements ontologiques composés, appelés aussi composites, affectent une entité ontologique et ses voisins. Ils sont alors formés par plusieurs règles de réécriture : une règle présentant le changement souhaité par l\u0027utilisateur (changement principal) et les autres règles présentant les changements dérivés (CHD) ajoutés pour préserver la consistance de l\u0027ontologie. En effet, l\u0027ordre des règles de réécriture est primordial dans la plupart des changements. Le Tableau 3 présente l\u0027interdépendance entre ces changements organisés dans une matrice de changements. La valeur d\u0027un élément de matrice (i, j) indique que l\u0027application d\u0027un changement relié à une ligne i implique l\u0027application du changement de la colonne j. Ainsi, le changement ontologique RemoveCardinalityRestriction(C, OP ) permet de supprimer une CardinalityRestriction définie sur une classe C et une objectProperty OP . Il est formalisé par deux règles de réécriture. La première représente le changement dérivé RemoveAssertionObjectP roperty qui supprime toutes les assertions définies sur OP . La deuxième règle définit la règle de réécriture principale assurant la suppression de la restriction.\nLe changement ontologique RemoveObjectP roperty(OP ) supprime une objectProperty OP et toutes ses dépendances de l\u0027ontologie. La Figure 2 présente les six règles de réécriture définissant ce changement. Ainsi, les cinq premières règles décrivent les changements déri-vés (CHD) devant être appliqués pour préserver la consistance de l\u0027ontologie. La dernière règle présente la règle de réécriture principale. Ainsi, les restrictions définies sur la propriété OP doivent être supprimées en appliquant les règles suivantes : RemoveAllV aluesRestriction(OP ), RemoveSomeV aluesRestriction(OP ), RemoveHasV alueRestriction(OP ) et RemoveCardinalityRestriction(OP ). Toutes les ObjectP ropertyAssertion qui réfé-rencent l\u0027objectProperty OP doivent également être supprimées. \nChangements ontologiques complexes\nLe Tableau 4 présente l\u0027ensemble des changements complexes abordé dans ce travail et les changements dont ils sont composés.\nComme exemples de changements complexes, nous présentons les changements P ullU pClass, M ergeClass et SplitClass. Ainsi, le changement P ullU pClass(C, C p ) permet de monter une classe C dans sa hiérarchie de classes et l\u0027attacher aux parents de sa super-classe précédente C p . Ceci implique que la classe C n\u0027est plus la subClass de la classe C p et n\u0027infère plus ses propriétés. La Figure 3 présente la règle de réécriture définissant ce changement. Ainsi, le changement dérivé RemoveObjectP ropertyAssertion vérifie si la classe C possède des individus qui partagent une objectP ropertyAssertion sur les propriétés de la classe C p . Dans ce cas, toutes les assertions doivent être supprimées. Le changement RemoveDataP ropertyAssertion supprime toutes les dataP ropertyAssertion définies sur les individus de la classe C et les dataProperties liées à la classe C p .  Le changement M ergeClasses(C 1 , C 2 , C N ew ) fusionne deux classes C 1 et C 2 déjà existantes dans l\u0027ontologie en une nouvelle classe (C N ew ). Il nécessite l\u0027application des règles de réécriture AddClass(C N ew ), RemoveClass(C 1 ) et RemoveClass(C 2 ). Cependant, pour préserver la consistance de l\u0027ontologie, avant de supprimer C 1 et C 2 , toutes leurs propriétés et axiomes doivent être attachés à C N ew . Formellement : 1)\nAddSubClass(C N ew , C j ), 2) répéter le processus pour C 2 , 3) ?C i ? C(O) · C i ? C 1 appliquer AddEquivalentClasses(C i , C N ew ), 4) répéter le processus pour C 2 , etc.\nLe changement SplitClass(C, C N ew1 , C N ew2 ) divise une classe (C) déjà existante dans l\u0027ontologie en deux nouvelles classes C N ew1 et C N ew2 . Il nécessite alors l\u0027application des règles de réécriture AddClass(C N ew1 ), AddClass(C N ew2 ) et RemoveClass(C). Comme le changement M ergeClasses, le changement SplitClass nécessite, avant la suppression de la classe C, d\u0027attacher toutes ses propriétés et axiomes aux classes C N ew1 et C N ew2 .\nDiscussion : Le formalisme des grammaires de graphes offre une représentation simple des changements ontologiques. Le Tableau 5 montre deux exemples de changements AddObject-P roperty et P ullDownClass, représentés à la fois par le formalisme proposé et le travail de (Djedidi et Aufaure, 2010). Dans Djedidi et Aufaure (2010), ces changements sont considé-rés, respectivement, comme composés et complexes. Le premier changement est composé par trois changements élémentaires et le deuxième par deux. De plus, ils nécessitent, comme tous les autres changements ontologiques, l\u0027utilisation du raisonneur Pellet pour identifier d\u0027une manière a posteriori les inconsistances. Dans notre travail, ces changements sont considérés comme élémentaires puisqu\u0027ils ne sont composés que d\u0027une seule règle de réécriture. De plus, pour préserver la consistance de l\u0027ontologie, les inconsistances sont gérées d\u0027une manière a priori grâce à l\u0027utilisation des Negatives Applications Conditions (N AC).\nChangement ontologique (Djedidi et Aufaure, 2010) Formalisme proposé AddObjectP roperty(OP, C1, C2)\nLe changement est composé de trois changements élémen-taires : 1.\nAddObjectP roperty-(OP ), 2. AddDomain(OP, C1) 3. AddRange(OP, C2).\n-Le changement est formalisé par une seule règle de réécriture et évite la non-redondance de données. Le changement est composé par deux changements élémen-taires : 1. AddSubClass(C1, C2) 2.\nRemoveSubClass-(C1, Cp)avec la classe Cp est la super-classe de C1 et C2.\n-Le changement est formalisé par une seule règle de réécriture et évite la contradiction des axiomes. \nConclusion\nNous avons présenté dans cet article une nouvelle formalisation des changements ontologiques composés et complexes basée sur les grammaires de graphes typés et l\u0027approche algé-brique Simple Pushout (SPO) de transformation de graphes. L\u0027utilisation de l\u0027approche SPO offre plusieurs avantages. En particulier, elle permet de définir simplement et formellement les règles de réécriture correspondant aux changements ontologiques. Elle assure le contrôle des transformations de graphes en évitant les incohérences d\u0027une manière a priori. De plus, elle réduit le nombre de changements élémentaires nécessaires pour appliquer les changements complexes et composés. À noter que la formalisation des changements a été implémentée à l\u0027aide de l\u0027outil AGG (Attributed Graph Grammar) et testée sur des ontologies de taille ré-duite. En effet, l\u0027étape la plus coûteuse en temps et en ressources est la reconnaissance du graphe LHS à partir du graphe hôte. Vu que la plupart des changements possèdent des LHS de taille réduite, il s\u0027est avéré que le temps d\u0027exécution est assez limité. À titre d\u0027exemple, pour un graphe d\u0027une ontologie composé de 21 noeuds, l\u0027exécution du changement complexe SplitClass(C, C N ew1 , C N ew2 ) a pris seulement 700 millisecondes (avec un LHS composé de 37 noeuds). Pour mieux évaluer la performance de notre approche, nous travaillons actuellement sur l\u0027évaluation de l\u0027influence de la taille de LHS sur les ontologies de grande taille.\n"
  },
  {
    "id": "288",
    "text": "Introduction\nQue l\u0027on suggère à l\u0027utilisateur d\u0027accéder à une information, de s\u0027inscrire à une newsletter, de commenter un service, ou d\u0027acheter un produit, le Web Usage Mining est indispensable à l\u0027objectif d\u0027adaptabilité de l\u0027offre technologique. La propension d\u0027un utilisateur en ligne à réa-liser une action suggérée dépend en effet de sa réaction face aux modes de sollicitation et ses réactions sont, pour une part déterminante, déclenchées par son expérience en cours. L\u0027expé-rience utilisateur correspond \"aux réponses et aux perceptions d\u0027une personne qui résultent de l\u0027usage ou de l\u0027anticipation de l\u0027usage d\u0027un produit, d\u0027un service ou d\u0027un système\" 1 . Dès lors on comprend aisément l\u0027enjeu communicationnel de l\u0027analyse automatique de l\u0027expérience de l\u0027utilisateur. Le web 3.0 se réfléchit d\u0027ores et déjà dans une logique one to one avec l\u0027individualisation de la communication sur le web comme élément central.\nLes questions ouvertes, par exemple sur la manière d\u0027analyser cette expérience utilisateur, sont nécessairement interdisciplinaires. La première partie de cet article définit une méthode d\u0027analyse sémiotique pour y répondre. C\u0027est ensuite la spécification de cette méthode à travers la proposition de nouveaux descripteurs sémiotiques qui est développée. La dernière partie définit le mode d\u0027apprentissage automatique pertinent pour la détection de la propension d\u0027un individu à réaliser une action suggérée.\nDétermination des données\nContrairement à la majorité des études sur les profils utilisateur, notre recherche porte sur un utilisateur quelconque 2 exposé à n\u0027importe quelle sollicitation, ce qui détermine l\u0027utilisation des « attitudes implicites » comme indicateur de l\u0027expérience. Les \"jugements et attitudes implicites\" sont des indicateurs, issus de récentes études socio-cognitives (cf. Courbet et Fourquet-Courbet (2014)), particulièrement pertinents dans le cadre de notre recherche car définissent des actes non analysables par l\u0027utilisateur et parfois même non perçus, mais révéla-teurs de l\u0027expérience vécue. Nous n\u0027avons donc pas besoin de connaître au préalable l\u0027utilisateur qui ne saurait pas, à titre d\u0027exemple, identifier précisément si et/ou pourquoi il clique sur l\u0027image plutôt que sur le texte pour accéder à l\u0027information désirée.\nL\u0027exploitation des données implicites produites par l\u0027utilisateur permet d\u0027estimer ses attitudes implicites. Les données explicites archivées sont inexistantes pour un utilisateur quelconque et les données explicites liées au déclaratif sont considérées trop intrusives (cf. Oard et Kim (2001)). La sollicitation de l\u0027utilisateur, pour obtenir ces informations, constitue un frein avéré dans le processus de séduction à l\u0027oeuvre dans les techniques de communication.\nL\u0027analyse de ces données a pour objectif de déduire un comportement à partir des interactions de l\u0027utilisateur avec le système. Les interactions sont identifiées par : la durée de lecture (temps passé par l\u0027utilisateur sur un écran, soit le temps passé entre deux actions), le mouvement de souris, le nombre de clics de la souris, la durée de défilement de l\u0027ascenseur, le défilement avec souris, le nombre de clics sur l\u0027ascenseur, le défilement avec les touches du clavier, la sélection du texte (cf. Tchuente (2013)).\nSe pose alors la question de la méthode permettant de détecter des attitudes implicites à partir de l\u0027exploitation de données implicites. Pour y répondre, nous définissons une catégorie sémiotique d\u0027analyse du comportement prenant en considération la portée communicationnelle inconsciente des actions réalisées. Les catégories de comportement, définies dans l\u0027état de l\u0027art (cf. Oard et Kim (2001)), sont constituées d\u0027actants : examiner, référencer, retenir, annoter, créer. Nous cherchons à remplacer ces catégories d\u0027actants par des catégories d\u0027expérience de l\u0027en acte. Notre contribution se situe dans l\u0027utilisation d\u0027une catégorie sémiotique d\u0027analyse du comportement : le style perceptif. Tel que défini par Pignier (2012), le style perceptif est l\u0027expression d\u0027une esthésie, une manière de percevoir le monde, l\u0027autre et les choses, reliée à l\u0027exercice d\u0027une sensibilité. C\u0027est l\u0027expérience de l\u0027utilisateur que nous cherchons à définir, sa façon de communiquer implicitement en interagissant avec le système. Là où la catégorisation classique d\u0027un profil utilisateur s\u0027intéresse aux caractéristiques d\u0027un individu (genre, situation socio-démographique, centres d\u0027intérêt), nous nous intéressons à son hexis numérique, son être impliqué dans l\u0027espace du web et agissant en fonction de lui. Ainsi, le contexte est né-cessairement inclus dans le style perceptif. Qu\u0027il soit acteur adjuvant ou opposant, il influe sur l\u0027expérience de l\u0027utilisateur. Il s\u0027agit de s\u0027approcher du schéma de la communication interpersonnelle qui permet, dans la vie réelle, d\u0027adapter débit de parole, gestualité, tonalité à l\u0027interlocuteur, en fonction de l\u0027observation de ses réactions. La détection du style perceptif poursuit in fine cet objectif d\u0027adaptabilité de la réponse à l\u0027expérience du co-énonciateur à un moment donné.\nAfin de définir le style perceptif, nous proposons une structuration des données implicites de navigation. Le style perceptif se caractérisant par les expériences médiées par les interfaces, 2. L\u0027utilisateur quelconque représente l\u0027utilisateur non identifié versus utilisateur identifié.\nle web contient donc intrinsèquement les informations nécessaires à sa détection. La réflexion se pose au niveau de la production de métadonnées (des descripteurs) caractérisant les données implicites. Le niveau morphosyntaxique de l\u0027analyse sémantique (comme par exemple pour TypWeb dans Beaudouin et al. (2002)) est transféré au niveau sémiotique de l\u0027analyse. En effet, l\u0027exploration qualitative des sites web nécessite l\u0027utilisation de descripteurs contenant les possibilités d\u0027interaction et de contexte.\nNous proposons d\u0027intégrer ici, comme niveau d\u0027analyse, le sème connotatif, qui est défini comme étant un sème connotant l\u0027effet produit par l\u0027interaction entre l\u0027utilisateur et l\u0027élément. À titre d\u0027exemple, à la valeur sémantique de l\u0027élément \"pop-up\", nous associons les valeurs sé-miotiques intrusif et surgissant. Intrusif et surgissant connotent l\u0027effet produit par l\u0027interaction avec \"pop-up\". À la valeur sémantique \"sommaire sous forme de listes à faire défiler\", nous associons la valeur sémiotique cartographique. L\u0027effet cartographique renvoie à l\u0027expérience de la vue d\u0027ensemble. L\u0027ensemble des sèmes ainsi produit forme les descripteurs pour l\u0027annotation des sites web. Le niveau sème connotatif contient l\u0027expérience possible (soit l\u0027effet produit par l\u0027interaction) mais sa capacité à qualifier un style dépend du schéma actantiel (l\u0027action effective de l\u0027utilisateur). Le schéma actantiel de Greimas (1966)  Le niveau sème connotatif est donc pondéré par les actions suivantes sur les éléments : lecture ou visualisation (déterminée elle même par le temps passé), cliquer pour accéder, cliquer pour fermer, défiler, surligner, cliquer pour partager, inscrire, commenter, annoter. Le sème connotatif peut alors soit être positivé ou contrarié. Le sème connotatif intrusif relié à l\u0027apparition d\u0027une pop-up pour l\u0027inscription à une newsletter, sera positivé par l\u0027action \"inscrire\" (par exemple inscrire son mail), mais sera contrarié par l\u0027action \"cliquer pour fermer\". On définit alors la paire de sèmes suivante : intrusif et non intrusif, qui sera niée en dehors des actions contrariantes ou positivantes qui lui sont associées. Il s\u0027agit ensuite d\u0027envisager les structures qui se dessinent au niveau sémiotique. En prenant en considération la chronologie des interactions et leur interopérabilité -simultanéité, succession, opposition, exclusion -et en définissant la dernière action de l\u0027utilisateur comme finalité, le contexte sémiotique est ainsi défini. L\u0027interaction formulée dans la phrase suivante : « L\u0027utilisateur en ligne ferme quasi instantanément la pop-up newsletter puis fait défiler le sommaire dans son intégralité avant d\u0027accéder à la page information » est retranscrite dans le contexte d\u0027une annotation sémiotique par : « Un style perceptif non intrusif successivement cartographique pour aboutir à : accéder à la page désirée ». La création d\u0027une ontologie appliquée à la détection du style perceptif s\u0027avère ainsi nécessaire pour définir les règles et articulations d\u0027une annotation sémiotique. Celle-ci permet ainsi d\u0027exploiter les données implicites, pour faire émerger les attitudes implicites détermi-nantes de l\u0027expérience de l\u0027utilisateur, que nous caractérisons par son style perceptif.\nDescripteurs sémiotiques proposés\nL\u0027étude sémiotique présentée dans la section précédente nous a amené à définir des descripteurs sémiotiques que nous présentons dans ce qui suit. Pour rappel, une session de navigation consiste en une succession de L écrans visités (L étant variable d\u0027une session à une autre), en ne tenant compte que des écrans sur lesquels l\u0027utilisateur a passé un temps supérieur à un certain seuil, ainsi que ceux qu\u0027il ne subit pas par défaut au chargement de la page.\nIl s\u0027agit alors de caractériser le contenu sémiotique de ces écrans, en introduisant des descripteurs qui encodent les informations liées aux styles perceptifs. Comme nous l\u0027avons mentionné dans la section 2, ces informations se trouvent au niveau des éléments qui composent l\u0027écran, et avec lesquels l\u0027utilisateur interagit pendant sa navigation. A noter que cette notion d\u0027interaction est spécifique à chaque type d\u0027éléments : certains ne seront pris en compte dans l\u0027analyse que si l\u0027utilisateur les survole avec la souris, alors que pour d\u0027autres, l\u0027affichage par défaut sur l\u0027écran visité est suffisant.\nNous proposons ainsi d\u0027associer à chaque élément de l\u0027écran un certain nombre de sèmes qui le caractérisent. Un dictionnaire de N sèmes est donc défini (sèmes positifs et négatifs), et une annotation sémiotique des pages du site permet d\u0027attribuer à chaque élément des labels sémiotiques issus de ce dictionnaire. Cette annotation se fait manuellement sur un certain nombre de pages caractéristiques du site, et est ensuite généralisée de manière automatique aux autres pages en attribuant les mêmes labels sémiotiques aux éléments appartenant à une même catégorie, en se basant par exemple sur les attributs HTML. Les descripteurs sémiotiques caractérisant un écran E sont ensuite calculés à partir de ces vecteurs binaires, en mesurant la fréquence de chaque modalité du dictionnaire de sèmes :\nAfin d\u0027éviter que deux écrans ayant un contenu sémiotique similaire et appartenant à des pages sémantiquement éloignées ne soient représentés avec des indicateurs identiques, nous proposons de concaténer le vecteur desc sémiotique (E) avec un descripteur sémantique classique décrivant le contenu de la page à laquelle appartient l\u0027écran.\nPour ce faire, un dictionnaire de M labels sémantiques est ainsi défini, et les différentes pages du site sont annotées avec ces labels. De nombreuses méthodes d\u0027annotation automatique, issues notamment des domaines du Web sémantique et du Web Content Mining, existent dans l\u0027état de l\u0027art (Charrad et al. (2008) par exemple).\nAinsi, chaque écran d\u0027une page donnée sera représenté par un vecteur de description de dimension N × M , composé d\u0027un premier vecteur qui est propre à cet écran, et d\u0027un second qui est commun à tous les écrans d\u0027une même page. Cette représentation permet à la fois de lever la limitation décrite dans le paragraphe précédent, et d\u0027encoder indirectement la suite des pages visitées (comme dans une approche de Web Usage Mining classique) à travers les variations des vecteurs sémantiques à chaque fois que l\u0027utilisateur accède à une nouvelle page.\nTraitement des descripteurs\nNous nous intéressons dans cette section au choix d\u0027un modèle d\u0027apprentissage adapté au traitement des descripteurs introduits dans la section précédente. Nous avons opté pour un modèle neuronal, et ce principalement pour deux raisons : (i) l\u0027aspect temps-réel des applications visées, pour lequel les modèles neuronaux sont particulièrement adaptés de part le fait que la quasi-totalité de la complexité est reportée sur la phase d\u0027apprentissage, et (ii) l\u0027optimalité de ce types de modèles en terme de performances, qui a été démontrée dans de nombreuses études comparatives récentes (parmi lesquelles nous pouvons citer celle de Bengio et Delalleau (2011)).\nVue la nature séquentielle des données traitées (une session de navigation étant représentée par une séquence de vecteurs, de dimension N × M chacun, et de longueur variable), nous avons opté pour un modèle neuronal récurrent. Ce dernier (initialement introduit dans Williams et Zipser (1995) et ayant connu plusieurs évolutions depuis) est analogue à un Perceptron multi-couches classique, mais dans lequel des connexions récurrentes sont rajoutées au niveau des couches cachées (c\u0027est à dire les couches intermédiaires entre l\u0027entrée et la sortie). Ce modèle récurrent est entraîné par une version modifiée de l\u0027algorithme de rétro-propagation du gradient (dans laquelle les connexions récurrentes sont prises en compte), en ciblant les sorties désirées selon l\u0027application visée. Ces sorties peuvent correspondre par exemple à la réponse à une recommandation de produit sur un site de e-commerce, à la réaction à une sollicitation sur un site de collecte de dons, et plus généralement à n\u0027importe quel objectif mesurable lié au comportement de navigation de l\u0027utilisateur.\n"
  },
  {
    "id": "289",
    "text": "Introduction\nLes données massives, appelées communément \"big data\", impactent directement le processus ETL (Extracting-Transforming-Loading) vu que celui-ci est le premier composant du système décisionnel confronté à ces données. Peu de travaux ont traité sur la problématique des données massives dans le processus ETL. Liu et al. (2011) ont proposé une approche parallèle/distribuée appelée ETLMR consistant à améliorer les performances de la phase de transformation (T) et de chargement (L) de l\u0027ETL et ce en adoptant, pour chacune des deux phases, des stratégies de distribution appropriées. Les expérimentations de Misra et al. (2013) ont montré que le paradigme MapReduce est prometteur et que les solutions ETL basées sur des frameworks open source tel que Apache Hadoop sont plus performantes et moins couteuses par rapport aux solutions ETL commercialisées. Contrairement aux travaux de Liu et al. (2011), ceux de Misra et al. (2013) considèrent la phase d\u0027extraction (E) de l\u0027ETL très couteuse ; celle-ci a été traitée dans un environnement parallèle/distribué selon le paradigme MapReduce. (Liu et al., 2012) est une démonstration du prototype ETLMR. Dans (Liu et al., 2014), les auteurs proposent une plateforme CloudETL basée sur Apache Hadoop et Apache Hive où les performances ont été nettement améliorées par rapport à celles d\u0027ETLMR (Liu et al., 2011). Les plateformes ETLMR (Liu et al., 2011) et CloudETL (Liu et al., 2014) sont basées sur du code Python, et par conséquent celles-ci sont destinées aux informaticiens dévelop-peurs de solutions ETL parallèles/distribuées. Dans le but de vulgariser ce type de plateformes et les rendre accessibles aux utilisateurs finaux, nous proposons, dans ce papier, une plateforme baptisée P-ETL (Parallel-ETL) développée sous l\u0027environnement Apache Hadoop\n1\n. Le paramétrage d\u0027un processus se fait, de bout-en-bout, sur une interface unique structurée en trois onglets, chacun concerne une phase du processus (E, T, L). Le même paramétrage peut s\u0027effectuer dans un fichier XML pour un traitement en batch. Nous avons adapté le schéma classique de l\u0027ETL dans l\u0027environnement MapReduce. Ainsi, P-ETL procède en cinq phases : (E)xtracting, (P)artitioning, (T)ransforming, (R)educing et (L)oading ; au lieu d\u0027ETL, on parle plutôt d\u0027EPTRL.\nLes bases de P-ETL\nNous présentons dans cette section les bases et les principes fondamentaux de P-ETL en exposant les techniques de partitionnement supportées, l\u0027adaptation des phases Map et Reduce aux spécificités de l\u0027ETL et nous terminons par l\u0027architecture globale de P-ETL.\nPartitionnement des données\nDans le but de distribuer/paralléliser le processus ETL, les données sources doivent être elles aussi distribuées pour permettre à plusieurs tâches de s\u0027exécuter de façon parallèle où chacune traite sa propre partition de données. P-ETL offre trois types de partitionnement. Afin d\u0027assurer une charge plus ou moins équitable entre les différentes tâches parallèles, le choix du type de partitionnement est important. La présence d\u0027un taux élevé, dans une partition de données, de tuples avec des valeurs creuses implique une charge faible en termes de traitement pour la tâche. En effet, les tuples en question seront rejetés par un filtre tel que NOT NULL.\nSimple : étant donné un volume de données source v, le type de partitionnement simple génère des partitions égales selon l\u0027équation 1 où nb_part étant le nombre de partitions.\nRound Robin (RR) : Avec la technique Round Robin, l\u0027affectation d\u0027un tuple depuis le volume source v vers une partition de données p est basée sur l\u0027équation 2. rang (tuple) étant le rang du tuple dans le volume v et nb_part étant le nombre de partitions.\nRound Robin par Bloc (RRB) : Cette technique est similaire à Round Robin. Dans le but d\u0027accélérer le partitionnement, un bloc de tuples est affecté à la partition, plutôt qu\u0027une affectation tuple par tuple.\nLes mappers et les reducers\nDans la plateforme P-ETL, les primitives map() et reduce() ont été adaptées aux spécificités de l\u0027ETL. Le rôle assigné à un mapper est la normalisation des données (nettoyage, filtrage, conversion, ...). Le mapper traite chaque row dans un tunnel de transformations (T 1 , T 2 ...) où chaque T i est chargé d\u0027une opération particulière telles que le nettoyage, filtrage, projection, conversion et concaténation. La figure 1  \nArchitecture de P-ETL\nLa plateforme P-ETL est organisée en cinq modules : (E)xtracting, (P)artitioning, (T)ransforming, (R)educing et (L)oading (Figure 3). Après extraction (E), les données sources sont chargées dans le système de fichier distribué de Hadoop (HDFS). Ensuite, un partitionnement logique des données (P) s\u0027effectue selon le choix de l\u0027utilisateur final (Simple, RR, RRB). Les partitions des données ainsi générées seront soumises au processus MapReduce. Chaque mapper est en charge de transformer (T) les données de sa partition (nettoyage, filtrage, conversion, ...).\nLes fonctions de fusion et d\u0027agrégation des données sont différées pour être exécutées dans la phase Reduce (R). A ce niveau, les données deviennent pertinentes et peuvent alors être chargées (L) dans l\u0027entrepôt de données.\nFIG. 3 -Architecture de P-ETL.\nParamétrage d\u0027un processus dans P-ETL\nLe paramétrage d\u0027un processus se fait sur une interface unique organisée en trois onglets (Extract, Transform, Load) dédiés au processus lui-même (workflow), plus une partie \"Advanced Parameters\" réservée pour la configuration de l\u0027environnement parallèle/distribué de Apache Hadoop (Figure 4). Pour la configuration du processus, l\u0027utilisateur doit commencer par l\u0027onglet \"Extract\". Les paramètres disponibles sur les deux autres onglets \"Transform\" et \"Load\" dépendent du premier, principalement du format de la source, sa structure et son emplacement. En revanche, la partie \"Advanced Parameters\" peut être utilisée indépendamment des trois onglets.\nFIG. 4 -Interface de configuration P-ETL.\nDans l\u0027onglet \"Extract\", l\u0027utilisateur doit, en premier lieu, localiser les données sources. Le format pivot des données est le fichier csv qui est adopté par toutes les plateformes MapReduce vu sa légèreté (absence de méta-données). Afin d\u0027accélérer le chargement des données sources dans le système HDFS, l\u0027utilisateur pourra activer la compression de celles-ci. Ensuite, l\u0027utilisateur doit choisir un type de partitionnement des données sources (simple, Round Robin, Round Robin by block) ainsi que le nombre de partitions (égal au nombre de mappers). Enfin, il doit paramétrer le mode de lecture du mapreader à partir des partitions de données (Ligne par ligne, nombre de lignes, taille en KO). La figure 4 montre l\u0027onglet \"Transform\" qui fournit à l\u0027utilisateur une liste de fonctions de transformation et d\u0027agrégation. Chaque fonction insérée doit être paramétrée (entrées, conditions, expressions, ...). Le tunnel de transformation ainsi constitué s\u0027exécutera dans l\u0027ordre d\u0027insertion des fonctions. Enfin, l\u0027onglet \"Load\" permet la configuration du chargement des données préparées dans l\u0027entrepôt de données et comporte la destination des données (entrepôt de données, magasin de données, cube de données), la compression des données avant leur chargement dans le système HDFS ainsi que le caractère séparateur du fichier csv cible. Concernant le paramétrage de l\u0027environnement parallèle/distribué dans Apache Hadoop (taille d\u0027un bloc HDFS, nombre de noeuds impliqué dans le processus, taille mémoire réservée à un noeud et à une tâche, compression des résultats des mappers avant de les soumettre aux reducers, ....), celui-ci se fait dans la partie \"Advanced Parameters\".\nExpérimentation\nPour évaluer P-ETL, nous avons installé un cluster de 19 machines, chacune possède un processeur intel-Core TMi3-3220 CPU@3.30 GHZ x 4 processor, 4GO RAM et 500 GO d\u0027espace disque. Le réseau local (LAN) est un Ethernet 100 Mbps. Selon la configuration matérielle présentée ci-dessus, l\u0027environnement Hadoop permet d\u0027affecter, au maximum, deux tâches parallèles à un même noeud. Ainsi, deux tâches parallèles sont équivalentes à un noeud dans ce qui suit.\nDonnées de test :\nNous avons développé un programme qui génère des données synthé-tiques relatives aux renseignements des étudiants. Les expérimentations ont été réalisées sur des jeux de données allant de 244 * 10 6 à 7, 317 * 10 9 tuples. Nous exposons, dans ce qui suit, l\u0027expérimentation réalisée sur un fichier etudiant.csv de 7, 317 * 10 9 lignes où chacune a une taille de 44 octets. Ce fichier contient les attributs suivants : Matricule , Date d\u0027inscription, Cycle (Licence, Master, Doctorat), Spécialité, Bourse et Sport. Le processus ETL paramétré pour l\u0027expérimentation se présente en quatre fonctions. La première tâche est la projection qui consiste à exclure les attributs Bourse et Sport. La deuxième tâche du processus est une restriction qui filtre les tuples et rejette tous ceux présentant une valeur Null dans l\u0027un des attributs : Date d\u0027inscription, Cycle, et Spécialité. La troisième tâche est Year() qui extrait l\u0027année à partir de la date d\u0027inscription. Enfin, la quatrième tâche est une fonction d\u0027agrégation COUNT() qui compte le nombre d\u0027étudiants inscrits durant la même année, dans le même cycle et la même spécialité. Il est à noter que durant l\u0027exécution du processus, lorsque P-ETL rencontre une agrégation, comme COUNT() dans ce cas, celle-ci sera différée pour être exécutée dans la phase Reduce.\nRésultats : Comme le montre le tableau 1, l\u0027augmentation des tâches parallèles améliore de manière significative les performances du processus. Nous remarquons que le gain de temps entre 24 et 30 tâches est très intéressant (50 mn). En revanche, nous constatons une régression du gain entre 30 et 38 tâches (6 mn). Nous pourrons conclure qu\u0027au delà d\u0027un certain seuil en termes de tâches parallèles, l\u0027amélioration des performances des processus sous P-ETL ne devient plus significative. \nConclusion\nLe processus ETL est considéré aujourd\u0027hui comme étant le coeur du système décisionnel puisque toutes les données destinées pour l\u0027analyse transitent par celui-ci. Afin de faire face aux données massives, nous l\u0027avons adapté selon le paradigme MapReduce pour permettre son exécution dans un environnement parallèle et distribué. P-ETL est basé sur une interface de paramétrage conviviale afin de rendre la plateforme accessible aux utilisateurs finaux. Les résultats des expérimentations montrent une meilleure scalabilité de P-ETL face à des volumes de données importants lorsque la taille du cluster augmente.\n"
  },
  {
    "id": "291",
    "text": "Introduction\nDans le cadre de l\u0027édition de manuscrits anciens, le rôle de l\u0027éditeur consiste à reconstruire le plus fidèlement possible le manuscrit original à partir des différentes versions du texte disponible. Pour cela, l\u0027éditeur classe les différentes versions du texte afin d\u0027obtenir un arbre généalogique de cette filiation que l\u0027on nomme stemma codicum (cf. Fig1).\nFIG. 1 -Stemma de De Nuptiis Philologiae et Mercurii établi par Danuta Shanzer.\nLa reconstruction généalogique par un arbre suppose que chaque copiste n\u0027a utilisé qu\u0027un seul manuscrit pour réaliser son exemplaire (filiation unique). Malheureusement, il arrive qu\u0027un exemplaire ait été copié, non à partir d\u0027une seule source, mais sur plusieurs manuscrits existants. On parle alors de contamination ou de corruption.\nNous proposons dans cet article, de représenter une tradition contaminée à l\u0027aide une construction pyramidale basée sur la notion de manuscrits intermédiaires.\nUne des modélisations philologiques utilisées par les éditeurs pour reconstruire le stemma codicum est du à Don Quentin (1926). Elle permet de retrouver, à partir du corpus, des triplets de manuscrits dont l\u0027un est l\u0027intermédiaire des deux autres et que nous nommons T3M. Nous utilisons alors ces triplets T3M pour bâtir une pyramide généalogique.\nLes triplets T3M sont déterminés par un indice qui est nul. L\u0027expérience montre néanmoins que le nombre de triplets T3M obtenus est très faible. Dans des données réelles, la contamination, les erreurs de saisie, etc., empêchent de respecter strictement les conditions de Don Quentin. Nous allons donc être amené à relâcher les conditions strictes de Don Quentin pour créer des triplets T3M-souples où l\u0027on impose à l\u0027indice, non plus d\u0027être nul, mais d\u0027être infé-rieur à un seuil, seuil déterminé par l\u0027éditeur en fonction du corpus.\nPour construire une pyramide à partir des triplets, définissons quatre propriétés : -l\u0027intermédiarité permet à un ensemble de triplets T3M de respecter la transitivité ; -La couverture impose que tous les manuscrits appartiennent au moins à un triplet T3M afin de pouvoir les situer sur la pyramide finale. -La compatibilité consiste pour un ensemble de triplets T3M à être représentable sur une pyramide (cf. Defays (1979)  \nSummary\nIn this paper we present a new codicum stemma visualization method. Don Quentin\u0027s modeling is usec to classify the textual tradition. We supplement the genealogical editor\u0027s information of betweenness triplets obtained directly from the corpus. A pyramid depicting the family codicum stemma is then constructed on the basis of information obtained by the triplets\n"
  },
  {
    "id": "292",
    "text": "Introduction\nL\u0027olfaction, ou la capacité de percevoir des odeurs, est le résultat d\u0027un phénomène complexe : une molécule s\u0027associe à un récepteur de la cavité nasale, et provoque l\u0027émission d\u0027un signal transmis au cerveau qui fait ressentir l\u0027odeur associée [Sezille et Bensafi (2013)-Meierhenrich et al. (2005]. Si les phénomènes qui caractérisent les sens de l\u0027ouïe et de la vue sont bien connus, la perception olfactive n\u0027est, encore aujourd\u0027hui, toujours pas comprise dans sa globalité. Cependant, on dispose de nombreux atlas (comme celui d\u0027Arctander (1969)) qui renseignent les qualités perçues par l\u0027humain pour des milliers de molécules odorantes : des experts senteurs associent à des milliers de molécules odorantes des qualités d\u0027odeurs (fruité, boisé, huileux, etc : un vocabulaire bien défini et consensuel). On dispose également maintenant d\u0027outils capables de calculer des milliers de propriétés physico-chimiques de molécules 1 . Il a alors pu être montré que ces propriétés déterminent la (les) qualité(s) d\u0027une odeur perçue [Khan et al. (2007) -Kaeppler et Mueller (2013)]. Ce lien entre le monde physico-chimique et le monde du percept olfactif a été mis en évidence à l\u0027aide de méthodes d\u0027analyse en composantes principales démontrant, à partir de données, la corrélation existante entre ces deux mondes. Les neuroscientifiques ont donc maintenant besoin de méthodes descriptives afin de comprendre les liens entre propriétés physicochimiques et qualités.\nLa découverte de régularités (ou descriptions) qui distinguent un groupe d\u0027objets selon un label cible (souvent appelé label de classe), est un problème qui a fédéré diverses communautés en intelligence artificielle, fouille de données, apprentissage statistique, etc. En particulier, la découverte supervisée de règles descriptives de type description ?? label est étudiée sous divers formalismes : découverte de sous-groupes, fouille de motifs émergents, ensembles contrastés, hypothèses, etc. (Novak et al. (2009)). Dans tous les cas, nous faisons face à un ensemble d\u0027objets associés à des descriptions (dont l\u0027ensemble forme un ensemble partiellement ordonné), et ces objets sont liés à un ou plusieurs labels de classe.\nDans cet article, on s\u0027intéresse à la découverte de sous-groupes (subgroup discovery), introduite par Klösgen (1996) et Wrobel (1997. Étant donné un ensemble d\u0027objets décrits par un ensemble d\u0027attributs, et chacun associé à un (ou plusieurs) label(s) de classe, un sousgroupe est un sous-ensemble d\u0027objets statistiquement intéressant par sa taille et ses singularités au sein de l\u0027ensemble d\u0027objets initial vis à vis d\u0027un ou plusieurs labels cibles. En fait, il existe deux familles principales de méthodes. La première (Wrobel, 1997) vise à trouver des règles de type description ? label où le conséquent est un unique label. La seconde, la fouille de modèles exceptionnels (exceptional model mining, EMM) introduite par Leman et al. (2008), vise à trouver des sous-groupes dont la répartition d\u0027apparition de tous les labels diffèrent grandement dans le sous-groupe comparé à toute la population, i.e. de la forme description ? {(label 1 , valeur 1 ), ..., (label k , valeur k )} où k est le nombre de labels de l\u0027attribut cible. Dans les deux cas, on veut optimiser une mesure de qualité pour distinguer au mieux le sous-groupe en fonction du label, ou d\u0027une distribution des labels dans le sous-groupe (i.e. le modèle).\nEn olfaction cependant, une molécule est associée à une ou plusieurs qualités d\u0027odeurs : aucune des approches existantes ne permet de se focaliser sur un sous-ensemble de labels de cardinalité arbitraire. Effectivement, ces approches permettent soit de caractériser un seul label de classe par des sous-groupes, soit de trouver des sous-groupes qui caractérisent tous les labels de classes à la fois. Alors, d\u0027une part, un sous-groupe effectue une caractérisation trop locale, trop spécifique et d\u0027autre part la caractérisation est beaucoup trop globale.\nNous cherchons alors à découvrir des sous-groupes comme des règles descriptives de type description ? {label 1 , label 2 , ..., label l } où l \u003c\u003c k. Pour cela, nous proposons une nouvelle méthode appelée ElMM (Exceptional local Model Mining) qui généralise à la fois la méthode de sous-groupes classiques ainsi que EMM. Nous montrerons alors que les sousgroupes extraits sont plus caractéristiques de peu de qualités à la fois, et donc aussi plus faciles à interpréter par l\u0027expert en olfaction.\nLa suite de cet article est organisée comme suit. Tout d\u0027abord, nous introduisons les deux principales méthodes de découverte de sous-groupes en section 2 (subgroup discovery et exceptionnal model mining). Nous montrons alors les limites de ces deux types d\u0027approche avant d\u0027introduire en section 3 notre nouvelle méthode : la découverte de modèles exceptionnels locaux (exceptional local model mining). Un algorithme de découverte est présenté en section 4 et appliqué à des données issues des domaines de la neuroscience et de l\u0027olfaction (section 5). 2) présente ce jeu de données dans le cas où la fonction class n\u0027associe à chaque molécule qu\u0027un seul label de C -mono-qualité-(resp. un sous-ensemble de labels -multi-qualités-).\nUn sous-groupe peut être représenté formellement de manière duale soit en intension soit en extension, c\u0027est-à-dire, soit par une description dans un langage donné mettant en oeuvre des restrictions sur le domaine de valeurs des attributs, soit par l\u0027ensemble d\u0027objets qu\u0027il décrit. Il existe plusieurs langages de description possibles, basés sur différents types de connecteurs logiques (conjonctions, disjonctions, ou encore négations), dont certains sont très expressifs (voir par exemple Galbrun et Kimmig (2014)). Dans la suite nous utiliserons un langage basé uniquement sur des conjonctions.\nDéfinition 2 (Sous-groupe). On note d \u003d 1 , . . . , f |A| la description d\u0027un sous-groupe où chaque f i est une restriction sur le domaine de l\u0027attribut a i ? A (à un sous-ensemble du domaine de a i s\u0027il est nominal, ou à un intervalle s\u0027il est numérique). Chaque restriction peut être assimilée soit à un ensemble (dans le cas d\u0027une restriction sur un attribut nominal), soit à un intervalle dont les bornes appartiennent à Dom(a i ) (dans le cas d\u0027un attribut numérique).\nRelation d\u0027ordre partiel entre les sous-groupes. \nExemple (suite). On a d 1 \u003d W ? 151.28, 23 ? nAT avec pour support l\u0027ensemble des molécules {24, 48, 82, 1633} : la molécule 1 ne vérifie pas la restriction sur l\u0027attribut nAT alors que la molécule 60 ne vérifie pas celle sur M W . Pour plus de lisibilité, lorsque l\u0027on ne précise pas une restriction f i dans une description d cela signifie qu\u0027aucune restriction effective n\u0027est appliquée sur l\u0027attribut a i dans d. La description d 2 \u003d W ? 151.28, 23 ? nAT, 10 ? nC est une spécialisation de d 1 car d 2 comporte les mêmes restrictions que d 1 plus une restriction sur un autre attribut. Réciproquement, d 1 est une généralisation de d 2 .\nÉtant donné un jeu de données, il y a potentiellement 2 |O| sous-groupes, il est donc néces-saire de n\u0027en sélectionner qu\u0027une partie en fonction de leur intérêt. Pour cela, les différentes approches de l\u0027état de l\u0027art utilisent une mesure de qualité qui évalue la singularité du groupe au sein de la population par rapport à une cible, c\u0027est-à-dire l\u0027attribut de classe. La mesure de qualité est choisie en fonction du type de données, mais aussi en fonction de l\u0027attribut de classe et de la finalité de l\u0027application. Il existe deux approches pour la découverte de sous-groupes : l\u0027approche que l\u0027on va définir comme classique (Wrobel, 1997), et l\u0027approche d\u0027Exceptional Model Mining (EMM) introduite par Leman et al. (2008).\nDans la première, chaque objet n\u0027est associé qu\u0027à un et un seul label de l\u0027attribut de classe, c\u0027est-à-dire ?o ? O, class(o) \u003d c avec c ? Dom(C), et la mesure de qualité permet de mettre en évidence la singularité d\u0027un sous-groupe relativement à un seul label de C. Pour un sous-groupe de description d, une mesure généralement utilisée relativement au label l est :\nles proportions d\u0027objets du sous-groupe et du jeu de données entier possédant la classe l. Cette mesure est une généralisation de la mesure WRAcc (? \u003d 1) qui prend en compte à la fois la taille du sous-groupe et aussi sa singularité dont le rapport entre les deux est pondéré par un facteur ?.\nDans le cas d\u0027EMM, un objet est associé à un sous-ensemble de labels de classe, c\u0027est-à-dire, ?o ? O, class(o) ? Dom(C). La mesure de qualité utilisée dans ce cas permet de mettre en évidence la singularité d\u0027un sous-groupe relativement à tous les labels de C à la fois. Une mesure possible est la somme des divergences de Kullback-Leibler pour tous les labels de classe entre les objets du sous-groupe et ceux du jeu de données entier :\nExemple (suite). Avec la description d 1 \u003d W ? 151.28, 23 ? nAT dans la Table 1, en utilisant la mesure de l\u0027équation (1) avec\n(1/2?1/3) \u003d 2/3. Dans la Table 2, en utilisant la mesure de la formule de l\u0027équation (2), on a W KL(d 1 ) \u003d 4/6 × ((2/4 log 2 3/4) + (3/4 log 2 3/2) + (3/4 log 2 3/2)) \u003d 0.45.\nDécouverte de sous-groupes, limites et problème. Étant donnés D(O, A, C, class), minSupp, ? et k l\u0027objectif est de récupérer l\u0027ensemble des k-meilleurs sous-groupes au regard de la mesure de qualité ? choisie où la taille du support du sous-groupe est supérieure ou égale à minSupp. Pour notre domaine d\u0027application de l\u0027olfaction, les approches existantes (décou-verte de sous-groupes classique et EMM) ne permettent pas de répondre à la problématique posée, à savoir la caractérisation de sous-ensemble de qualités d\u0027odeurs. Effectivement, ces approches permettent soit de caractériser un seul label de classe, c\u0027est-à-dire une qualité olfactive, par sous-groupe, soit de trouver des sous-groupes qui caractérisent tous les labels de classes à la fois avec EMM. D\u0027une part un sous-groupe effectue une caractérisation trop locale et spécifique, d\u0027autre part la caractérisation est trop globale. Nous introduisons dans la suite une nouvelle méthode qui généralise ces deux approches en permettant de caractériser par un sous-groupe un sous-ensemble L de taille quelconque de labels de classe.\nDécouverte de modèles exceptionnels locaux : ElMM\nSoit D(O, A, C, class) un jeu de données conforme à la Définition 1, avec Dom(C) \u003d {l 1 , . . . , l k }. Étant donnée une mesure de qualité ?, notre méthode ElMM recherche des sousgroupes de la forme (d, L) où d est la description d\u0027un sous-groupe et L ? Dom(C) est un sous-ensemble de labels de la classe C à caractériser. Cette méthode correspond au cas général de la découverte de sous-groupes. Effectivement, si on fixe pour tous les sous-groupes que L ? Dom(C) on se ramène au cas de la découverte de sous-groupes classique dans lequel un sous-groupe ne caractérise qu\u0027un label de classe. De plus si L \u003d Dom(C) alors on bascule dans le cas d\u0027EMM où chaque sous-groupe doit caractériser tous les labels de classe à la fois. ElMM permet donc de caractériser des sous-ensembles de labels de classe par des sous-groupes appelés sous-groupes locaux. \nLa contrainte (i) permet de ne considérer que les sous-groupes dont le support est supérieur ou égal à un seuil minSupp, évitant ainsi d\u0027obtenir des sous-groupes de trop petite taille qui n\u0027auraient alors aucun intérêt et facilitant l\u0027exploration. La contrainte (ii) permet d\u0027interagir sur le langage de la description en restreignant le nombre maximal de restrictions effectives par description à un seuil maxDesc (|d| est le nombre de restrictions effectives de d). De manière similaire, la contrainte (iii) permet de limiter le nombre de labels à discriminer dans L.\nMesure de qualité. La mesure de qualité utilisée dans EMM dont la formule a été donnée dans l\u0027équation 2 peut être généralisée pour ElMM pour ne considérer qu\u0027un sous-ensemble L ? Dom(C) de labels de classe et non plus l\u0027ensemble complet de labels à la fois :\nCependant, cette mesure de qualité ne correspond pas à l\u0027objectif de notre contexte applicatif car elle ne quantifie pas les labels de L ensemble, c\u0027est-à-dire de manière conjointe, lorsqu\u0027ils sont associés conjointement aux objets. Nous cherchons à caractériser l\u0027ensemble des objets cohérents qui possèdent tous les labels de L, et non pas un sous-ensemble de L. Pour cela, nous nous sommes tournés vers une mesure de qualité usuellement utilisée en classification supervisée : la F 1 -Mesure. Cette mesure nous permet dans notre cas de quantifier la pureté d\u0027un sousgroupe vis à vis des labels à caractériser L, i.e. les objets du support de la description du sousgroupe doivent être le plus possible associés à L (la précision) et les objets associés à L dans D doivent être au maximum inclus dans le support du sous-groupe (le rappel). La F 1 -Mesure se base sur le rappel et la précision d\u0027un sous-groupe vis à vis du sous-ensemble L à caractériser. Pour un sous-groupe local (d, L), on note :\nOn remarque alors que la F 1 -Mesure est comprise entre 0 et 1 puisque la précision et le rappel sont compris aussi entre 0 et 1. Plus la valeur de F 1 (d, L) est proche de 1 plus le sous-groupe (d, L) caractérise spécifiquement le sous-ensemble de labels L.\nExemple. Afin d\u0027illustrer la méthode ElMM, nous reprenons l\u0027exemple de la Table 2. Soit le sous groupe (d, L) avec d 1 \u003d W ? 151.28, 23 ? nAT et L \u003d {M iel, V anillé} le sous-ensemble de labels de classe à caractériser, en appliquant la formule de l\u0027équation 3 afin de calculer la mesure de qualité par la F 1 -Mesure on trouve : \nDécouverte de sous-groupes locaux avec ELMMUT\nDans cette section nous présentons l\u0027algorithme ELMMUT qui répond au problème d\u0027Exceptional local Model Mining (ElMM). Tout d\u0027abord, nous caractérisons l\u0027espace de recherche des sous-groupes. Ensuite, nous décrivons la manière de parcourir cet espace pour produire les sous-groupes en illustrant le pseudo-code de ELMMUT.\nEspace de recherche. L\u0027espace de recherche correspond à l\u0027ensemble de tous les sous-groupes locaux, partiellement ordonnés. Un sous-groupe local\nAinsi, l\u0027espace de recherche correspond à un treillis dans lequel chaque sous-groupe local est un noeud et le lien entre deux noeuds dénote que le noeud de niveau i+1 est une spécialisation du noeud de niveau i par ajout d\u0027une nouvelle classe à caractériser, ou par spécialisation d\u0027une restriction de la description. L\u0027élément le plus général du treillis correspond au sous-groupe local vide que l\u0027on note ( ?) en omettant les f i car aucune restriction n\u0027est effectuée pour tout attribut a i :\nParcours heuristique de l\u0027espace de recherche. L\u0027algorithme ELMMUT effectue un parcours en profondeur de l\u0027arbre de recherche en partant du plus général (le sous-groupe local vide à la racine de l\u0027arbre) vers le plus spécifique. Le principe algorithmique est donné dans l\u0027Algorithme 1. Pour chaque sous-groupe d\u0027un noeud de l\u0027arbre de recherche, ELMMUT essaie de le spécialiser par une extension de description ou de labels tant que la mesure de qualité est améliorée (fonction Spécialiser) (Galbrun et Kimmig, 2014). Cependant, il existe dans le pire des cas |Dom(C)| + (|A| × n(n + 1)/2) possibilités pour spécialiser un sous-groupe, puisque on peut effectuer jusqu\u0027à |Dom(C)| extensions de labels et |A| extensions de description pour lesquelles on peut construire n(n + 1)/2 intervalles possibles (si l\u0027attribut possède n valeurs différentes). Afin de pallier à ce problème d\u0027espace de recherche, nous nous sommes tournés vers une approche heuristique utilisée dans la découverte de sous-groupes et dans la fouille de redescriptions, il s\u0027agit d\u0027une approche de type \"beam-search\" (recherche par faisceau) (Lowerre, 1976). Cette approche permet d\u0027explorer seulement une partie des branches de l\u0027arbre de recherche : à chaque spécialisation, seulement une partie des possibilités (au maximum beamW idth) de spécialisation du sous-groupe va être analysée (cf. ligne 11 de Spécialiser). Optimisation des intervalles à la volée. Pour les attributs numériques, une simple discrétisa-tion en prétraitement n\u0027est pas suffisante. Cette approche est cependant utilisée dans une partie des expérimentations afin de pouvoir se comparer équitablement à l\u0027algorithme de référence pour EMM (DSSD Diverse Subgroup Set Discovery introduit par van Leeuwen et Knobbe (2012)), qui utilise une telle discrétisation. Afin d\u0027obtenir des résultats les meilleurs possibles, le choix des bornes de l\u0027intervalle pour un attribut a ? A doit se faire à la volée pour tenir compte des spécificités d\u0027un sous-groupe particulier. Le choix des bornes de l\u0027intervalle est alors déterminant. Tester toutes les possibilités d\u0027intervalles n\u0027est pas envisageable car cette méthode peut s\u0027avérer beaucoup trop gourmande en ressources (complexité théorique d\u0027ordre n 2 pour n valeurs différentes). Afin de pallier ce problème, nous avons adopté une méthode de discrétisation proche de celle de Fayyad et Irani (1993), introduite dans la découverte de sous-groupes par Grosskreutz et Rüping (2009). La Figure 1 présente la répartition des valeurs prises par les objets du support d\u0027un sous-groupe local (d, L) pour l\u0027attribut a. Pour optimiser la mesure il faut éliminer du support du sous-groupe un maximum d\u0027objets qui ne sont pas associés au sous-ensemble de labels L à caractériser. Soit S \u003d (d, L) un sous-groupe, et a ? A un attribut à partir duquel on veut étendre d, on note {p 1 , . . . , p |a| } l\u0027ensemble ordonné (p 1 \u003c p 2 \u003c · · · \u003c p |a| ) des |a| ? |supp(S)| valeurs différentes prises par l\u0027ensemble des objets de S pour l\u0027attribut a. On dit alors qu\u0027une valeur p i des valeurs prises par a est prometteuse si le nombre d\u0027objets de supp(S) associés à L possédant la valeur p i pour a est supérieur ou égal au nombre d\u0027objets de supp(S) non associés à L possédant la valeur p i . Sinon, on dit qu\u0027elle est non-prometteuse. Ainsi, une valeur p i de a correspond à une borne inférieure potentielle si p i est prometteuse et p i?1 est non-prometteuse. De plus une valeur p i de a correspond à une borne supérieure potentielle si p i est prometteuse et p i+1 est non-prometteuse. Ensuite il suffit de tester tous les intervalles possibles en prenant tous les couples (borne inférieure potentielle, borne supérieure potentielle) et de choisir le meilleur.\nExpérimentations\nJeu de données\nNous disposons d\u0027un atlas Arctander (1969), première base de données olfactive établie, qui sert de référence pour les neuroscientifiques. Il met en oeuvre 1 689 molécules différentes décrites par 1 704 propriétés physicochimiques numériques (leur volume, leur poids, le nombre d\u0027atomes de carbone qu\u0027elles contiennent, etc...) et sont associées à leur(s) qualité(s) olfactive(s) évaluée(s) par des experts. Les possibles discussions quant à l\u0027obtention de cet atlas, et notamment pour les qualités olfactives, ne sont pas abordées dans ce papier comme il s\u0027agit d\u0027un problème traité par les neuroscientifiques en amont. L\u0027atlas étant clairement multi-labels, on associe chaque molécule à un sous-ensemble de qualités olfactives. En moyenne, chaque molécule est associée à 2.88 qualités olfactives.\nA partir de cet atlas, nous avons construit deux jeux de données différents. Dans le premier jeu de données D 1 , on ne considère que 43 attributs (propriétés physicochimiques) de l\u0027atlas Arctender, alors que dans le second jeu de données D 2 on en considère 243. La sélection des 43 attributs de D 1 a été faite sur recommandation de l\u0027expert qui assure que ces attributs doivent être déterminants pour la caractérisation de qualités d\u0027odeurs. Les 243 attributs du jeu de données D 2 ont quant à eux été sélectionnés par une analyse de non-corrélation des attributs.\nRésultats quantitatifs\nTout d\u0027abord afin de juger de la performance de l\u0027approche que nous avons mise en place, considérons l\u0027aspect quantitatif des résultats sur le jeu de données d\u0027olfaction. Nous avons exécuté les expérimentations sur une machine avec 8Go de RAM et un processeur cadencé à 3.10GHz. Les résultats quantitatifs obtenus ont été réalisés sur les deux jeux de données Algorithm 1 ELMMUT.\nEntrée : O, A, C, class, ?, k, beamW idth, minSupp, maxDescr, maxLab Sortie : L\u0027ensemble des sous-groupes locaux R 1: R ? ? 2: for all c ? Dom(C) do 3:\nVérifier et Mettre à jour les contraintes pour ( {c}) 5:\nfor all a ? A do 6:\nf ? Choisir restriction sur a 7:\nVérifier et Mettre à jour les contraintes pour ( {c}) 8:\nAjouter ( {c}) à T emp 9:\nend for 10:\nT emp ? Conserver les k-meilleurs sous-groupes locaux de T emp 11:\nfor all (d, L) ? T emp do 12:\nAjouter Spécialiser(d, L) à R 13:\nend for 14: end for\nAjouter (d, L ? {c}) à T emp 5: end for 6: for all a ? liste des attributs candidats de (d, L) : A Cand do 7:\nf ? Choisir restriction sur a 8:\nVérifier et Mettre à jour les contraintes pour\nAjouter (d ? {f }, L) à T emp 10: end for 11: T emp ? Conserver les beamW idth meilleurs sous-groupes locaux de T emp 12: for all (d, L) ? T emp do 13: Figure 2 présente les différents temps d\u0027exécution de notre approche pour la version de l\u0027algorithme sans la discrétisation à la volée pour les attributs (une discrétisation par effectifs égaux est réalisée a priori pour chaque attribut numérique, comme cela est fait par la méthode EMM). Les trois courbes sont relatives au jeu de données D 1 . On remarque que plus on augmente la taille maximale autorisée pour la description (maxDescr) ou pour le sous-ensemble de labels à caractériser (maxLab), plus le temps d\u0027exécution est long ce qui est tout à fait compréhensible puisque l\u0027algorithme cherche à étendre le plus possible les sousgroupes tant que la mesure de qualité est améliorée. On remarque cependant qu\u0027à partir de maxDescr \u003d 15 le temps d\u0027exécution est sensiblement semblable ce qui signifie que même si la taille maximale autorisée augmente les sous-groupes ont une description dont la taille ne va pas au-delà d\u0027un certain seuil : la mesure ne peut plus être améliorée en les étendant. Ce résultat semble être causé à la fois par le paramètre minSupp et par le jeu de données. Effectivement, plus les descriptions sont étendues plus le support a une taille qui tend à diminuer, et puisque l\u0027algorithme est déterministe, avec ce jeu de données, on ne peut excéder une description de taille 15. De même pour la taille maximale du sous-ensemble de labels à caractériser, à partir de 2 ou 3 le temps d\u0027exécution reste le même, ce qui concorde avec le fait qu\u0027en moyenne une molécule est associée à 2.88 qualités olfactives (au-delà de n \u003e 3 qualités olfactives, le nombre de molécules partageant l\u0027ensemble de ces mêmes n qualités olfactives est trop faible et la contrainte du support minimale n\u0027est pas respectée). La Figure 3 présente l\u0027impact du jeu de données et de la discrétisation à la volée via notre technique. Clairement, le nombre d\u0027attributs est un facteur crucial pour l\u0027algorithme ELMMUT, on observe la présence d\u0027un facteur 10 entre le temps d\u0027exécution sur D 1 avec 43 attributs et celui sur D 2 avec 243. L\u0027utilisation de la discrétisation à la volée ne semble pas passer à l\u0027échelle lorsque l\u0027on augmente la taille des descriptions : à partir d\u0027une valeur de 15 pour maxDescr l\u0027exécution dure plus de 12 heures et a donc été avortée. Nous prévoyons des techniques d\u0027optimisation dans le futur.\nRésultats qualitatifs\nL\u0027interprétation des résultats est un point central dans le cadre de notre application. Les règles descriptives que nous avons mises en place doivent être capables d\u0027informer et d\u0027aiguiller les neuroscientifiques dans leur recherche. Notre approche, ElMM, en ne caractérisant qu\u0027un sous-ensemble de labels de classe permet alors de correspondre au cas pratique à savoir qu\u0027une molécule ne possède en moyenne que 2.88 qualités olfactives. En observant la Figure 4 qui présente la distribution des qualités au sein du jeu de données entier et d\u0027un sous-groupe obtenu par la méthode EMM, on s\u0027aperçoit clairement que l\u0027interprétation d\u0027un tel résultat est très difficile. On constate des différences entre les distributions du sous-groupe et du jeu de données initial mais cette différence est présente sur beaucoup trop de qualités olfactives à la fois et ainsi l\u0027interprétation d\u0027un tel résultat pour la déduction d\u0027une règle descriptive est infaisable pour un neuroscientifique. La Table 3 présente les 5 meilleurs sous-groupes (du point de vue de la mesure F 1 ) obtenus après suppression des motifs redondants (on utilise ici la même méthode que Galbrun et Kimmig (2014)). Ces sous-groupes sont issus de la base de données D 1 lorsque la discrétisation à la volée est activée avec maxDescr \u003d 10, maxLab \u003d 2 et minSupp \u003d 30. Seulement un sous-groupe caractérisant plusieurs labels de classe (Floral et Balsamique) est présent, avec une mesure de 0.33 et un support de 38. Sa description contient 9 restrictions. Des sous-groupes ont aussi des descriptions plus courtes. La taille des supports est variable. De plus, dans le jeu de données D 2 , lorsque la discrétisation à la volée est désactivée et que maxDescr \u003d 15, maxLab \u003d 3 et minSupp \u003d 30, on obtient 74.6% de sous-groupes dont le sous-ensemble de labels est de taille 1, 22.9% de taille 2 et 2.5% de taille 3. \nConclusion\nNous avons présenté la découverte de motifs exceptionnels locaux, une nouvelle méthode de fouille de règles descriptives qui généralise les approches existantes, pour caractériser spé-cifiquement un sous-ensemble de labels de classe. Nous l\u0027avons appliquée au cas concret de l\u0027olfaction afin de mettre en évidence les liens existant entre les propriétés physicochimiques d\u0027une molécule et ses qualités olfactives. Le pouvoir d\u0027interprétation des résultats et l\u0027information qu\u0027ils véhiculent, permettent d\u0027entrevoir une évolution de la connaissance à propos du phénomène complexe qu\u0027est l\u0027olfaction. De nombreuses expérimentations restent à faire et nous envisageons une exploration interactive inspirée par Galbrun et Miettinen (2012).\nRéférences Arctander, S. (1969). Perfume and flavor chemicals :(aroma chemicals), Volume 2. Allured Publishing Corporation.\n"
  },
  {
    "id": "295",
    "text": "Introduction\nLa fouille de séries temporelles (FST) a récemment focalisé l\u0027attention des chercheurs en fouille de données en raison de l\u0027augmentation de la disponibilité de données comportant une dimension temporelle. Les algorithmes de FST tels que la classification/regroupement des sé-ries temporelles, l\u0027extraction de motifs ou la recherche de similarités nécessitent une mesure de distance entre séries temporelles. Le calcul de ces distances repose principalement sur la classique distance euclidienne ou l\u0027alignement temporel dynamique (DTW -Dynamic Time Warping) qui peuvent conduire à des temps de calcul trop importants pour s\u0027attaquer à de longues séries ou à des bases de séries volumineuses. Aussi, de nombreuses représentations approchées des séries temporelles ont émergé au cours de la dernière décennie. La représenta-tion symbolique est une technique pour approximer les séries temporelles. L\u0027algorithme SAX proposé par Lin et al. (2003) est un des plus utilisés pour la symbolisation. C\u0027est une technique très simple, permettant de symboliser par segments les séries temporelles sans nécessiter d\u0027information a priori. Lin et al. (2003) ont montré que SAX possède de bonnes performances pour la FST. Elle ne permet néanmoins pas de prendre en compte les informations de tendance dans les segments de séries temporelles. Plusieurs extensions de la représentation SAX ont été proposées pour pallier ce manque (Esmael et al. (2012); Lkhagva et al. (2006);Zalewski et al. (2012)). Cependant l\u0027information sur la pente dans ces travaux est soit très simplifiée, soit ne tient pas compte du fait que la distribution des valeurs de pente dépend de la taille des segments. De plus, ces méthodes augmentent significativement la taille de la représenation symbolique associée et les résultats n\u0027en tiennent pas toujours compte.\nNous avons proposé dans Malinowski et al. (2013), une nouvelle représentation symbolique 1d-SAX pour les séries temporelles, basée sur la quantification de la régression linéaire des segments de la série. Il est montré que cette nouvelle représentation approche plus préci-sément les données d\u0027origine que la représentation SAX pour une même quantité de symboles disponibles, et qu\u0027elle permet d\u0027améliorer les performances en termes de recherche efficace de plus proche voisins dans des bases de série. Dans cet article, nous rappelons les grands principes de 1d-SAX, et montrons dans la dernière section que cette technique peut-être utilisée avantageusement (en termes de compromis complexité/stockage/performance) pour de la classification de séries temporelles d\u0027images satellites.\n2. Calculer la régression linéaire de la série temporelle sur chaque segment, 3. Quantifier les couples (moyenne, pente) de la régression linéaire en un symbole choisi dans un alphabet de taille N .\nDans l\u0027étape 2, l\u0027algorithme calcule la régression linéaire de chaque segment produit dans l\u0027étape 1, puis cette régression est quantifiée dans un alphabet fini (étape 3). La régression linéaire est calculée selon l\u0027estimation des moindres carrés :\n. La régression linéaire de V sur T est la fonction l(x) \u003d sx + b qui minimise la distance euclidienne entre l et V sur T . Elle est entièrement déterminée par les deux valeurs s et b. s représente la pente de l et b la valeur de l pour x \u003d 0 :\noù T et V représentent respectivement la moyenne des valeurs de V et de T . Dans ce qui suit, nous avons choisi de représenter une régression linéaire de V sur les instants T par la valeur de la pente s, et la valeur moyenne du segment a. a est défini par l\u0027équation a \u003d s × (t 1 + t L )/2 + b. À l\u0027issue de l\u0027étape 2, la série temporelle est représentée par des couples (s, a) pour chaque segment résultant de la segmentation de la série. Il faut ensuite quantifier ces couples dans un alphabet de N symboles. À cette fin, les deux valeurs d\u0027un couple sont quantifiées séparément puis combinées en un symbole. Avec la même hypothèse de gaussiannité de la série temporelle, les propriétés statistiques de la régression linéaire garantissent que les distributions des valeurs de moyenne et des valeurs de pente sont gaussiennes de moyenne 0, de variance 1 pour les valeurs de moyenne et de variance ? 2 L , une fonction décrois-sante de L, pour les valeurs de pente. Selon ces propriétés, la quantification de la moyenne et de la pente peut se faire de la même manière que pour SAX. Les valeurs des moyennes sont quantifiées sur N a niveaux selon les (N a ? 1) quantiles de la distribution gaussienne N (0, 1), tandis que les valeurs des pentes sont quantifiées sur N s niveaux selon les (N s ? 1) quantiles de la distribution gaussienne N 0, ? 2 L\n. Le choix du paramètre ? 2 L est important. À partir de l\u0027analyse de l\u0027impact de ? L sur de nombreuses séries temporelles de distribution gaussienne, ? 2 L \u003d 0.03/L semble un bon compromis. Pour une même nombre de niveaux N \u003d N a × N s , la représentation 1d-SAX permet différentes configurations, suivant le nombre de niveaux affectés respectivement à la moyenne et à la pente. Par exemple, une représentation symbolique sur 64 niveaux peut être répartie en 32 pour la moyenne et 2 pour la pente ou à 16 pour la moyenne et 4 pour la pente, etc.\nInterrogation asymétrique d\u0027une base de séries\nNous avons appliqué cette nouvelle représentation des séries temporelles au problème de la recherche du plus proche voisin (1-PPV). L\u0027objectif de cette application est le suivant. Étant donné une base de données D contenant #D séries temporelles et une série requête q, nous souhaitons trouver les séries temporelles de D les plus proches de q. Nous supposons dans la suite de cet article que la série requête q et toutes les séries temporelles de la base de données sont de même longueur. La méthode naïve consiste à calculer la distance entre la requête et toutes les séries de la base de données et à retourner la série la plus similaire à q. Le nombre de distances à calculer est donc #D. Nous pouvons tirer parti de la représentation symbolique afin d\u0027accélérer la recherche dans de grandes bases de données de séries temporelles. La représentation SAX a par exemple été utilisée pour indexer et interroger des bases contenant des téraoctets de séries temporelles (Shieh et Keogh (2008)). Nous définissons dans cette section un schéma d\u0027interrogation asymétrique pour la recherche approchée de séries temporelles dans une base de données. Le terme asymétrique signifie que les requêtes ne sont pas quantifiées pour éviter d\u0027avoir une double erreur de quantification (quand les requêtes et les séries de la base de données sont quantifiées). Jégou et al. (2011) ont montré que l\u0027interrogation asymétrique améliore la précision de l\u0027approximation pour la recherche de vecteurs. Nous proposons une méthode basée sur cette même idée pour la recherche de séries temporelles dans une base de données. Nous supposons que D contient les séries temporelles, ainsi que leur représentation symbolique (1d-SAX) pour un ensemble de paramètres donnés (L, N a , N s ). Cet ensemble de paramètres définit complètement les N \u003d N a × N s symboles s 1 , . . . , s N utilisés pour quantifier la série. L\u0027algorithme de recherche du 1-PPV de la requête q est le suivant :\n1. Découper q en segments de longueur L : q \u003d q 1 , . . . , q w 2. Calculer les distances euclidiennes entre chaque segment de q et les symboles s j , 1 ? j ? N . Ces valeurs sont stockées dans une table\n. ED représente la distance euclidienne.\nPour toute série\nobtenue par simple accès à la table A et sommation sur w éléments.\nUne fois ces étapes effectuées, les distances entre q et toutes les séries temporelles de D sont disponibles pour identifier les plus proches voisins de q. Le nombre ? q d\u0027opérations arithmétiques élémentaires nécessaires pour satisfaire une requête est :? q \u003d (3L ? 1) × w × N + (w ? 1) × #D, où la partie gauche est le coût de l\u0027étape 2 ci-dessus et la partie droite celui de l\u0027étape 3. Dans le cas de la méthode naïve le nombre d\u0027opérations élémentaires est (3Lw ? 1) × #D. Le coût du calcul dans le schéma de recherche approchée est plus faible, en particulier pour de grandes bases de données où N #D. La figure 1 compare les taux de classifications correctes obtenus en gardant les séries temporelles telles quelles (et en utilisant la distance euclidienne pour les comparer), et en les symbolisant par les techniques SAX et 1d-SAX. L\u0027axe des abscisses représente le paramètre k de la classification par k plus proches voisins. On peut constater que les taux de classification correctes obtenus par la méthode 1d-SAX sont globalement supérieurs à ceux obtenus avec SAX. De plus, ils sont assez proches de ceux obtenus avec la distance euclidienne (méthode beaucoup plus coûteuse en temps et en espace de stockage des images).\nExpérimentations et évaluations\nConclusion\nDans cet article, nous avons proposé une nouvelle représentation symbolique des séries temporelles. Cette représentation est basée sur la quantification de la régression linéaire des segments constituant la série. Les symboles prennent en compte des informations sur la moyenne et la pente des segments de la série temporelle. Dans toutes nos expérimentations, nous\n"
  },
  {
    "id": "296",
    "text": "Introduction\nLa représentation sac-de-mots des documents (abrégée ici en BoW, Bag-of-Words) est très largement utilisée en recherche d\u0027information (RI) et en traitement automatique des langues (TAL). Elle permet d\u0027associer à un texte un descripteur unique basé sur l\u0027ensemble des motsformes qu\u0027il contient. Cependant, cette représentation est parfois trop grossière pour certaines tâches. Plusieurs représentations alternatives ont été imaginées selon les cas et les informations disponibles. Les similarités entre objets complexes (graphes, arbres...) ont été extensivement étudiés (Bunke, 2000, inter alia), mais sont rarement utilisées en RI à cause de leur coût calculatoire. C\u0027est pourquoi, dans beaucoup de cas, les travaux gardent une structure de données identique à celle des sacs-de-mots (même si ce sont des morphèmes, des n-grammes ou des syntagmes et non plus des mots qui sont manipulés). Dans cet article, nous nous intéressons à une extension simple de la représentation classique en sac-de-mots dans laquelle un objet est décrit par un multiensemble de sac-de-mots. Cette représentation en sac-de-sacs-de-mots (BoBoW, Bag-of-Bags-of-Words garde certaines propriétés calculatoires des BoW, mais nécessite de savoir comment agréger les résultats obtenus entre sacs-de-sacs. Dans son travail séminal en RI, Wilkinson (1994) l\u0027utilise pour comparer une requête aux différentes portions d\u0027un document et combiner les résultats, soit sur les similarités soit sur les rangs. Mais les quelques fonctions d\u0027agrégation testées obtiennent des résultats inférieurs à un système vectoriel classique. En revanche, cette représentation a été utilisée avec succès dans des cadres particuliers en TAL (Ebadat et al., 2012) et en image (Kondor et Jebara, 2003). Elle est aussi à rapprocher des travaux sur la recherche d\u0027information structurée (Luk et al., 2002)  \nLa comparaison de deux documents nécessite le calcul de toutes les combinaisons de similarités entre les vecteurs des deux documents, comme illustré en figure 1.\nIl faut noter que la représentation en sacs-de-sacs implique une plus grande complexité mé-moire et calculatoire. Il faut en effet stocker plusieurs sous-documents pour un document ; ces sous-documents contiennent certes moins de mots, mais la somme de leur empreinte mémoire est supérieure à celle du document considéré dans son ensemble. Cette consommation supplé-mentaire dépend en pratique du nombre moyen de sacs par documents et de la façon dont ils sont stockés. En ce qui concerne le temps de calcul, la complexité est aussi plus grande. Au moment de la recherche, le calcul exhaustif du score d\u0027un document est en O(n * m * d) avec n le nombre de sacs de la requête, m celui du document et d la complexité du calcul de ?.\nPropriétés des fonctions d\u0027agrégation pour le modèle vectoriel\nLes mesures similarités entre deux sacs-de-sacs-de-mots agrègent les résultats de la similarité mineure ? pour toutes les combinaisons possibles vecteur à vecteur. Deux façons simples pour ce faire ont été proposées (Haussler, 1999) :\nPlus récemment, une mesure plus générale a été proposée (Gosselin et al., 2007) : \nBeaucoup de façons d\u0027agréger les similarités mineures peuvent être imaginées. Il convient cependant de s\u0027interroger sur les propriétés attendues de ces agrégations. Nous en listons ici quelques unes qui nous semblent essentielles ou d\u0027autres souhaitables pour que la représen-tation en sacs-de-sacs garde une sémantique correcte. En RI, cette sémantique doit assurer l\u0027ordonnancement des documents par proximité avec la requête. Les modèles usuels traduisent cette proximité selon une réponse graduelle, de manière à induire un ordre total entre les documents. L\u0027agrégation des similarités mineures se doit donc de conserver au mieux cette propriété. Pour simplifier les notations, nous considérons l\u0027agrégation comme une fonction, notée Aggreg, prenant en paramètre les similarité mineures notées a, b, c....\nAssociativité Cette propriété traduit le fait que le résultat d\u0027une agrégation soit considérée de même nature qu\u0027une similarité mineure et puisse être à son tour utilisée pour une agrégation. Cela nous permet de généraliser les propriétés ci-dessous, définies sur des relations binaires, à des fonctions n-aires : Aggreg(a, b, c) \u003d Aggreg(a, Aggreg(b, c)) \u003d Aggreg (Aggreg(a, b), c). Ainsi, on peut définir la fonction PowerScalar avec deux arguments :\nComme nous l\u0027avons dit, aucun ordre n\u0027est à considérer pour les sacs-desacs ; on souhaite donc avoir une fonction commutative : Aggreg(a, b) \u003d Aggreg(b, a)\nMonotonie croissante. Une autre propriété essentielle est la monotonie de l\u0027agrégation (croissante si la similarité mineure l\u0027est) en fonction des similarités mineures :\nCertaines autres propriétés ne sont pas nécessaires pour que la métrique résultante ait bien le comportement attendu, mais peuvent être recherchées pour espérer de bons résultats.\nÉlément neutre. Pour ne pas favoriser les documents contenant beaucoup de sacs sans liens avec la requête, il est souhaitable que la mesure d\u0027agrégation ait un élément neutre qui soit le minimum de la similarité mineure. C\u0027est le cas avec les fonctions proposées en section 2.2 dont l\u0027élément neutre est 0, avec une similarité mineure basée sur le produit scalaire : Aggreg(a, 0) \u003d a Continuité. La continuité de la fonction d\u0027agrégation en fonction de toutes ses variables (similarités mineures) n\u0027est pas non plus une condition nécessaire pour la sémantique de l\u0027agré-gation. Cependant, une telle continuité permet évidemment un comportement plus facilement interprétable et prévisible.   (Hull, 1993).\nDans ces expériences et les suivantes rapportées dans cet article, les textes que nous traitons doivent être découpés en sous-documents, chacun étant représenté par un sac. Selon le formatage disponible (textes organisés en paragraphes ou non, etc.) et l\u0027application visée, plusieurs options peuvent être explorées. Dans le cadre de nos évaluations, nous adoptons un découpage en phrase (détectées via les marques de ponctuations. Les requêtes sont également représentées en sacs de sacs : un sous-document correspond ici aussi à une phrase, s\u0027il y en a, ou au plus à un champ (titre, corps...).\nAdaptations des pondérations\nSelon le modèle adopté, il peut être nécessaire d\u0027adapter les pondérations utilisées. La plupart des fonctions de pondérations de RI font intervenir des valeurs calculées sur le document (IDF, longueur du document DL...). Le passage à une unité plus petite pose la question du calcul de ces valeurs. Nous rapportons ci-dessous une des expériences menées pour tester deux stratégies : dans un cas, les variables problématiques (IDF, DL...) sont calculées classiquement sur le document, et dans l\u0027autre cas, sur le sous-document (le DL est alors la longueur du sous-document considéré, l\u0027IDF est la fréquence document inverse dans l\u0027ensemble des sousdocuments de la collection). Pour ces expériences préliminaires, nous utilisons la collection INIST, nous fixons Sum-Sum comme mesure d\u0027agrégation. La similarité mineure est le modèle Okapi-BM25 (Robertson et al., 1998), classiquement utilisé en RI, avec les constantes fixées à leur valeur par défaut : k 1 \u003d 2, k 3 \u003d 1000 et b \u003d 0.75.\nLes résultats sont donnés dans le tableau 1, avec la représentation sac- \nRésultats\nNous évaluons les systèmes de RI BoBoW avec les paramètres décrits dans les soussections précédentes selon les fonctions d\u0027agrégation présentées en section 2.2. Le tableau 2 présente les résultats obtenus respectivement en utilisant un modèle Okapi-BM25 (avec les paramètres IDF et DL calculés à l\u0027échelle du document). Nous faisons apparaître les différences par rapport à la référence ; celles non statistiquement significatives sont en italiques.\nOn remarque que la représentation en sacs-de-sacs de mots obtient dans la plupart des cas des performances au moins équivalentes à celles du sac-de-mots classique, alors même que nous n\u0027en avons optimisé aucun des paramètres. Cela montre le potentiel intéressant de ce type de représentations. À ce titre, les bons résultats de l\u0027agrégation PowerScalar recoupent les constatations faites dans d\u0027autres contextes (Ebadat et al., 2012;Gosselin et al., 2007). À l\u0027inverse, le fait que Max-Max fonctionne moins bien que les autres agrégations, comme dans les tentatives de (Wilkinson, 1994), est dû au caractère non archimédien de cette fonction (la pertinence repose sur la proximité entre un seul sous-document du document et de la requête).\nConclusion\nL\u0027objectif de cet article était d\u0027étudier les conditions nécessaires à l\u0027utilisation des repré-sentations en sac-de-sacs-de-mots, en s\u0027intéressant notamment aux fonctions d\u0027agrégation au coeur de cette approche. Nous avons mis en lumière quelques unes des propriétés souhaitables de ces fonctions, que nous avons illustrées dans un cadre vectoriel classique. Les expérimenta-tions menées ont mis en exergue l\u0027importance du choix de la fonction, en fonction notamment de son comportement seuillant ou non. Bien que notre but n\u0027était pas d\u0027optimiser un système\n"
  },
  {
    "id": "297",
    "text": "Introduction\nLes ontologies sont nées d\u0027un besoin de standardisation des vocabulaires ressenti dans de nombreux domaines et connaissent, depuis quelques années, un succès qui a en partie été porté par l\u0027explosion du web de données (cf. Heath et Bizer (2011)), dont la promesse est le partage à grande échelle de données ouvertes et liées. Le web d\u0027aujourd\u0027hui est donc non seulement une source inépuisable d\u0027informations, mais il est devenu, à travers le web de données, un vecteur de développement et de diffusion incontournable pour les ingénieurs de la connaissance et les fournisseurs de données.\nParticiper au web de données suppose d\u0027être capable de s\u0027interconnecter avec les données et ontologies déjà présentes et disponibles. Publier une nouvelle ontologie nécessite donc au préalable de la relier avec les \"bonnes\" ontologies publiées sur le web de données comportant des concepts similaires dans des domaines similaires. Relier deux ontologies consiste en fait à les aligner, i.e. trouver des correspondances entre les entités (concepts, propriétés ou instances) des deux ontologies. L\u0027alignement d\u0027ontologies est un domaine en plein essor qui a donné lieu à de nombreux travaux de recherche. Nous pouvons notamment citer Shvaiko et Euzenat (2013), Bernstein et al. (2011), Rahm (2011) ou encore Euzenat et Shvaiko (2007). Ces travaux ont permis de définir des formalismes et des outils, qui sont régulièrement évalués dans le cadre de la campagne d\u0027évaluation de l\u0027OAEI Nous proposons dans cet article une méthode d\u0027alignement d\u0027une ontologie source avec des ontologies cibles déjà publiées sur le web de données et liées entre elles. Ces ontologies peuvent en fait être des thésaurus, des ontologies ou encore des ressources termino-ontologique qui peuvent être exprimées dans différents langages de représentation. Notre méthode repose sur un principe de raffinage d\u0027alignements qui exploite des ontologies liées sur le web de données. Cette méthode prend en entrée un ensemble d\u0027alignements qui peuvent être générés en utilisant différentes approches d\u0027alignement.\nNous proposons d\u0027illustrer la méthode proposée par un retour d\u0027expérience sur l\u0027alignement d\u0027une ontologie dans le domaine des sciences du vivant et de l\u0027environnement. Dans ce domaine, plusieurs thésaurus ont été créés au niveau international et publiés sur le web de données pour faire face au besoin de standardisation des vocabulaires. Les deux plus importants sont actuellement AGROVOC 2 et NALT 3 . AGROVOC a été créé dans les années 1980 par la FAO (Food and Agriculture Organization of the United Nations) comme un thésaurus structuré multilingue pour les domaines de l\u0027agriculture, de la sylviculture, de la pêche, de l\u0027alimentation et de domaines apparentés (comme l\u0027environnement). Il est actuellement disponible en 19 langues, avec une moyenne d\u0027environ 40 000 termes dans chaque langue (cf. Caracciolo et al. (2012)  Caracciolo et al. (2012)). Dans ce papier, nous nous intéressons à l\u0027alignement d\u0027une ressource termino-ontologique naRyQ (n-ary Relations between Quantitative experimental data) non encore publiée (cf. (Buche et al., 2013)) avec AGROVOC et NALT. naRyQ contient environ 1 100 concepts structurés en plusieurs sous-domaines, tels que les produits alimentaires, les microorganismes et les emballages.\nNous présentons, dans la section 2, notre méthode d\u0027alignement d\u0027une ontologie avec des ontologies liées et, dans la section 3, les résultats de notre expérimentation dans le domaine des sciences du vivant et de l\u0027environnement. Enfin, nous concluons dans la section 4. \nAlignement des variantes des ontologies\nLe processus d\u0027alignement d\u0027ontologies prend en entrée deux ontologies et produit en sortie un ensemble de correspondances entre les entités de ces ontologies. Selon Euzenat et Shvaiko (2007), ce processus est défini comme suit :  \nOn remarquera que le nombre total de processus d\u0027alignement lancés pour obtenir l\u0027alignement de l\u0027ontologie source O s avec chacune des deux ontologies cibles O \nDeuxième étape : raffinage des correspondances trouvées\nLa deuxième étape consiste à raffiner les ensembles d\u0027ensembles de correspondances\n. Ces deux ensembles d\u0027ensembles qui résultent de la concaténation des résultats de plusieurs processus d\u0027alignement lancés sur plusieurs variantes d\u0027ontologies permettent d\u0027obtenir de nombreuses correspondances (qui laisse présager une bonne couverture), mais aussi beaucoup de bruit, i.e. des mauvaises correspondances, qu\u0027il convient de réduire.\nNous proposons deux méthodes de raffinage pour améliorer la qualité des correspondances trouvées lors de la première étape : la première méthode permet de supprimer des correspondances considérées comme ambiguës et donc potentiellement éronnées (section 2.2.1) ; la deuxième méthode permet d\u0027identifier les correspondances considérées comme potentiellement correctes (section 2.2.2).\nSuppression des correspondances ambiguës\nNous distinguons trois types d\u0027ambiguïté entre correspondances. Le premier type d\u0027ambiguïté concerne les correspondances obtenues à partir d\u0027une même méthode d\u0027alignement lancée sur différentes variantes des ontologies source et cible, correspondances qui ont la même entité source, la même entité cible et la même relation. Nous proposons de lever les ambiguïtés de type 1 en ne conservant que la correspondance ayant le degré de confiance le plus élevé. \nsont ambiguës selon le type 1 si :\nL\u0027ensemble des ensembles de correspondances non ambigüs selon le type 1 est :\nLe deuxième type d\u0027ambiguïté identifié entre des correspondances correspond au cas où une entité d\u0027une ontologie source O s est alignée, par la relation d\u0027équivalence, avec deux entités distinctes d\u0027une ontologie cible O c . Nous proposons dans ce cas de ne conserver que la correspondance la plus pertinente, i.e. celle qui a a priori le degré de confiance le plus élevé. Cependant lorsque ces correspondances n\u0027ont pas été générées par la même méthode d\u0027alignement, leur degré de confiance ne sont pas comparables. Nous proposons de calculer une mesure de similarité sim, indépendante des méthodes d\u0027alignement utilisées, pour les deux correspondances à comparer, qui peut, par exemple, s\u0027appuyer sur les mesures de similarité syntaxiques usuelles implémentées dans l\u0027Alignment API (David et al., 2011). \nc . L\u0027ensemble des ensembles de correspondances non ambiguës de type 2 est :\nLe troisième type d\u0027ambiguïté identifié entre des correspondances correspond au cas où deux entités distinctes d\u0027une ontologie source O s sont alignées, par la même relation, avec une même entité d\u0027une ontologie cible O c . Nous proposons dans ce cas de ne conserver que la correspondance la plus pertinente, i.e. celle avec la mesure de similarité sim la plus élevée. \nIdentification des correspondances potentiellement correctes\nLorsque des redondances apparaissent entre des correspondances qui ont été générées à partir d\u0027au moins deux méthodes d\u0027alignement distinctes, nous faisons l\u0027hypothèse que ces correspondances peuvent être considérées comme ayant plus de \"chance\" d\u0027être bonnes. Nous les conserverons dans un ensemble distinct, noté recT C Os?Oc pour ensemble de recouvrement, afin de les présenter à l\u0027utilisateur comme des correspondances potentiellement correctes.   \nNous considérons dans la section 3 que l\u0027ensemble des correspondances potentiellement correctes entre une ontologie source O s et une ontologie cible O c est l\u0027ensemble obtenu par l\u0027union des deux ensembles de recouvrement définis ci-dessus, noté U * ,2 * ,3 * recT Os?Oc \u003d C Os?Oc ? LOD C Os?Oc , dans lequel ont été supprimées les ambiguïtés de type 1, 2 et 3.\nExpérimentation\nNous illustrons dans cette section la méthode présentée dans la section 2 pour aligner une ontologie source naRyQ , présentée dans la section 3.1, avec chacune des deux ontologies cibles, AGROVOC et NALT. L\u0027alignement de l\u0027ontologie naRyQ avec AGROVOC est noté naRyQ ? AGROVOC et l\u0027alignement de l\u0027ontologie naRyQ avec NALT est noté naRyQ ? NALT.\nLOD pour Linked Open Data en anglais\n3.1 L\u0027ontologie source naRyQ L\u0027ontologie naRyQ (n-ary Relations between Quantitative experimental data) a été définie pour représenter des relations n-aires entre des données quantitatives expérimentales (Buche et al., 2013). Les spécificités de cette ontologie sont les suivantes : (i) c\u0027est une ressource termino-ontologique qui est un modèle hybride ; (ii) les labels sont disponibles en français et en anglais ; (iii) elle est représentée en OWL DL et SKOS ; (iv) la composante conceptuelle contient environ 1 100 concepts structurés en plusieurs sous-domaines, les plus importants en effectif étant les produits alimentaires (? 460 concepts), les microorganismes (? 180 concepts) et les emballages (? 150 concepts).\nProduction des alignements de références\nPour évaluer la qualité des alignements produits et pour comparer entre eux les résultats des processus d\u0027alignement, nous utilisons les mesures de précision et de rappel adaptées à l\u0027alignement d\u0027ontologies (Euzenat et Shvaiko, 2007). Ces mesures s\u0027appuient sur une comparaison avec un alignement de référence, R. La production d\u0027un alignement complet n\u0027étant pas envisageable car elle demanderait de nombreuses personnes, du temps et une expertise pointue, nous avons construit deux alignements, partiels, notés R AGROVOC pour l\u0027alignement naRyQ ? AGROVOC, et R NALT pour l\u0027alignement naRyQ ? NALT. Dans la suite, nous n\u0027étu-dierons que la relation d\u0027équivalence ?.\nPour chaque ontologie et pour tout concept, nous avons extrait ses labels (e.g. skos :prefLabel, skos :altLabel, rdfs :label, rdfs :comment) en anglais ou en français ainsi que des éléments de structure (e.g. skos :broader, rdfs :subClassOf). Un premier alignement a été produit en utilisant SMOA (A String Metric for Ontology Alignment) (Stoilos et al., 2005), une similarité syntaxique destinée à l\u0027alignement d\u0027ontologie. Cet alignement a permis de générer 1 453 correspondances, qui ont ensuite été validées par deux experts en double aveugle et ont été réconciliées, i.e. les experts se sont a posteriori mis d\u0027accord. Cette validation a été réalisée en quatre heures, en utilisant un outil de visualisation spécifiquement développé pour cette tâche.\nAfin d\u0027améliorer les premiers alignements produits R AGROVOC et R NALT , nous les avons enrichis en exploitant les résultats d\u0027alignement obtenus avec la méthode proposée dans ce papier. Les alignements ainsi enrichis, notés R  (David, 2007). Quelque soit l\u0027outil utilisé, nous avons, dans la suite, fait l\u0027hypothèse qu\u0027une correspondance est jugée acceptable si elle est obtenue avec un degré de confiance supérieur ou égal à 0.5, seuil qui a été obtenu empiriquement suite à de nombreux tests.\nNous avons retenus les variantes d\u0027ontologies suivantes, en ne retenant pour AGROVOC que les labels en français et en anglais et pour NALT que les labels en anglais :\nRemarque 3.1 La variante naRyQ OW L a été obtenue en gardant la structure de la composante conceptuelle et en transformant les skos :prefLabel et skos :altLabel en rdfs :label. La variante naRyQ SKOS a été obtenue en gardant les labels de la composante terminologique et en transformant la hiérarchie conceptuelle en éléments de hiérarchie SKOS.   \nÉvaluation de naRyQ ? NALT\nLe tableau 3 présente l\u0027évaluation des différents ensembles de correspondances produits durant la phase de raffinage par rapport à l\u0027ensemble d\u0027alignement de référence partiel, R \nDiscussion\nComme nous pouvons l\u0027observer dans les tableaux 2 et 3 et comme nous pouvions nous y attendre, (1) l\u0027augmentation de l\u0027ensemble d\u0027alignements permet d\u0027améliorer les valeurs de rappel (au détriment de la précision) pour la plupart des ensembles produits (meilleur rappel avec agr C * ), et, (2) en combinant les différentes méthodes de raffinage, nous obtenons de meilleurs résultats en termes de précision (ensemble U * ,2 * ,3 * ). En comparant ces résultats avec les meilleurs scores obtenus par les outils d\u0027alignement (cf. tableau 1), dans le cas de AGROVOC, notre approche obtient des performances similaires en termes de F-mesure, tandis que dans le cas de NALT une augmentation de la F-mesure est observée. De plus, notre approche surpasse les meilleurs résultats individuels en rappel pour AGROVOC et en précision pour NALT. Ce qui représente, dans l\u0027ensemble, des résultats très encourageants.\nNotre approche d\u0027exploitation des alignements existants sur le web de données pour raffiner des résultats d\u0027alignement est une piste prometteuse. D\u0027autres travaux exploitent également le web de données pour aider à la tâche d\u0027alignement. Pernelle et Sais (2011) proposent une approche qui combine à la fois la découverte de liens entre les données du web de données et l\u0027alignement entre les concepts de deux ontologies. Parundekar et al. (2012) exploitent les liens entre différents sources de données du web de donnés pour aligner deux ontologies.\nLa plupart des outils d\u0027alignement utilisent des stratégies pour combiner différentes mé-thodes d\u0027alignement de base (i.e., lexicale, structurale, etc.) au sein d\u0027un processus d\u0027alignement et pour filtrer leurs résultats (seuil, agrégation pondérée, règles, etc.) (cf. Euzenat et Shvaiko (2007)). Nous nous intéressons ici à la fois au raffinage des ensembles d\u0027alignements produits par différents outils d\u0027alignements et à la discrimination de l\u0027ensemble de correspondances trouvées. Dans le premier cas, nous avons identifié trois types d\u0027ambiguïté à résoudre pour raffiner l\u0027ensemble de correspondances en supprimant un certain nombre. Dans le deuxième cas, nous proposons deux méthodes originales pour discriminer l\u0027ensemble des correspondances. La première méthode consiste à considérer la redondance entre des correspondances trouvées dans au moins deux processus d\u0027alignement issus de méthodes distinctes comme gage de validité (i.e. une correspondance apparaissant dans le résultat d\u0027au moins deux processus d\u0027alignement est susceptible d\u0027être correcte). La deuxième méthode consiste à exploiter les alignements définis sur le web de données pour appuyer la validité de certaines correspondances (i.e. les correspondances permettant d\u0027aligner une même entité à deux entités distinctes mais liées sur le web de données sont susceptibles d\u0027être correctes). D\u0027autres travaux (Mochol et Jentzsch, 2008;Steyskal et Polleres, 2013) ont, comme nous, eu l\u0027idée d\u0027utiliser des outils existants et de combiner leurs résultats pour aligner des ontologies. Tandis que Mochol et Jentzsch (2008) utilisent un ensemble de règles pour sélectionner le meilleur outil, Steyskal et Polleres (2013) proposent une méthode itérative basée sur le vote où, à chaque tour, les correspondances acceptées par la majorité des outils sont considérées comme valides. Cependant, ces travaux n\u0027exploitent pas des alignements sur le web de données.\nUn autre point original de notre approche réside dans l\u0027exploitation de différentes variantes qui permet de prendre en compte les spécificités des ontologies à lier qui peuvent être des ontologies, des thésaurus, des RTO et qui peuvent être exprimées dans différents langages de représentation avec différents niveaux d\u0027expressivité. Cela nous donne la possibilité de couvrir un panel large et hétérogène de ressources.\nConclusion et persectives\nNous avons proposé une méthode originale d\u0027alignement d\u0027ontologies qui permet de relever l\u0027un des défis des alignements d\u0027ontologies cité dans Shvaiko et Euzenat (2013) : « Matching with background knowledge ». Notre méthode permet, d\u0027une part, d\u0027obtenir de nombreuses correspondances en utilisant et combinant des méthodes existantes pour aligner des ontologies, des thésaurus et des RTO exprimés dans différents langages. Elle permet, d\u0027autre part, de discriminer les correspondances obtenues en en supprimant certaines ambiguës, et,\n"
  },
  {
    "id": "298",
    "text": "Introduction\nLa numérisation de documents administratifs est un enjeu économique et écologique prioritaire dans le contexte sociétal actuel. Le défi n\u0027est plus la numérisation du document, mais l\u0027extraction des informations qu\u0027ils contiennent. Cet article présente une nouvelle approche d\u0027annotation automatique de documents administratifs (certificat d\u0027assurance, acte de naissance, etc.) qui utilise le logo contenu dans les documents comme élément d\u0027apprentissage. Le logo est un élément graphique riche de sens (Duthil et al., 2013) auquel il est possible de rattacher de multiples informations (secteur d\u0027activité, etc.). L\u0027objectif ne se limite donc pas à la reconnaissance d\u0027un logo dans un document (élément graphique) mais également aux aspects sémantiques connexes. Cet article est organisé de la manière suivante : la section 2 présente un état de l\u0027art des méthodes existantes de classification et d\u0027extraction de logo pour l\u0027annotation. La section 3 présente notre approche d\u0027annotation automatique de documents administratifs. La section 4 est consacrée aux expériences et la section 5 conclue cet article et donne quelques perspectives.\nÉtat de l\u0027art\nLa détection et l\u0027extraction de symboles ou de logos est un sujet de recherche actif de ces deux dernières décennies, comme l\u0027atteste les très nombreuses publications réalisées dans les conférences ICDAR ou GREC depuis 1995. Ces approches ont tout d\u0027abord cherché à exploiter des images binaires, pour ensuite évoluer vers des images de documents en couleurs (Ahmed, 2008;Nourbakhsh et al., 2011). Nous proposons un bref résumé des méthodes dédiées aux logos ci-dessous.\nEn 2008, Zeggari (Ahmed, 2008) développé un algorithme d\u0027extraction de logo reposant sur deux propriétés principales des logos : leur compacité spatiale et leur uniformité colorimétrique. Tout d\u0027abord, le contenu de l\u0027image est simplifié et transformé à partir d\u0027opérateur morphologiques pour uniformiser les logos appartenant à une même classe. Puis, la densité spatiale et chromatique des régions composant chaque logo sont calculés.\nUne approche intéressante, proposée en 2012 par Sahbi et al (Sahbi et al., 2012), vise à définir un noyau sur la \"similarité liée au contexte\", qui intègre le contexte spatial de caracté-ristiques locales. Les points d\u0027intérêts sont détectés à l\u0027aide du détecteur SIFT, et le contexte est décrit à l\u0027aide d\u0027un shape-context. Une fonction mesurant la similarité est alors définie en s\u0027appuyant sur trois critères principaux : la fidélité, le contexte et l\u0027entropie du terme, pour rechercher des points d\u0027intérêts similaires selon ces critères.\nLa plupart de ces méthodes présentent des restrictions particulières,soit elles reposent sur des heuristiques à priori, ou alors sont très coûteuses en temps de calcul. De plus, hormis la dernière approche citée, toutes les autres reposent sur des critères bas-niveau (descripteurs radiométriques) sans intégrer le contexte ni conceptualiser le contenu d\u0027une image. L\u0027objectif de cet article est de proposer une méthode applicable à la fois sur les images en couleurs et en noir et blanc, avec des documents de bonne qualité ou bruités, et capable d\u0027intégrer un apprentissage incrémental (apprentissage de nouveau logo à la volée). Enfin, nous proposons une méthode qui combine des traitements d\u0027images et des techniques de fouille de texte afin d\u0027annoter un document par un ensemble de mots clés caractéristiques des différents concepts contenus dans le documents.\nL\u0027approche\nNotre approche d\u0027annotation de documents administratifs est composée de quatre étapes. La première étape consiste à extraire automatiquement les zones saillantesdu document afin d\u0027extraire et d\u0027identifier le(s) logo(s) contenu(s) dans le document. La seconde étape permet la construction automatique d\u0027un corpus d\u0027apprentissage en utilisant les vignette identifiées précédemment. La troisième étape sert d\u0027apprentissage du vocabulaire relatif au logo. La dernière étape correspond à l\u0027annotation du document par le lexique de descripteurs appris lors de l\u0027étape précédente.\nExtraction de logo\nUn logo est une région particulière d\u0027un document qui peut-être définie par les trois proprié-tés suivantes (Duthil et al., 2013) : Région visuellement saillante par rapport à son contexte, récurrent visuellement (même image dans différent contextes), élément graphique créé par l\u0027Homme pour l\u0027Homme. L\u0027objectif de cette première étape est donc de rechercher et d\u0027extraire des zones saillantes dans un document, zones qui seront analysées par la suite pour identifier les logos potentiellement présents. Pour réaliser cette étape, nous nous sommes appuyés sur l\u0027approche proposée par (Perreira Da Silva et Courboulay, 2012) pour l\u0027analyse visuel du document. Ce modèle hybride permet d\u0027étudier l\u0027évolution temporelle du focus attentionel. Le système visuel est inspiré des modèles d\u0027attention de Itti et al. (1998) et Frintrop (2005. La scène visuelle est décomposée en différentes caractéristiques selon une approche multiréso-lution et ces caractéristiques sont calculées à partir de filtres numériques. Le système génère, pour chacune des caractéristiques prises en compte (intensité, couleur et orientation), un certain nombre de cartes représentant les éléments les plus saillants. À partir de chacune des vignettes identifiées par le processus de saillance visuelle, un processus d\u0027identification des logos est effectué. Chaque vignette fait l\u0027objet d\u0027une requête sous forme d\u0027image sur un moteur de recherche web (Google Images) pour identifier le nom du logo. Si la vignette n\u0027est pas reconnue, elle ne sera plus considérée dans la suite du processus (fouille de texte), sinon, le nom du logo est conservé et il sera ensuite utilisé lors de la phase d\u0027apprentissage des descripteurs sous forme de \"mot germe\"(c.f. section 3.2.2).\nFouille de texte\nLe processus de fouille de texte est composé des étapes 2 et 3. Cette section présente la mé-thode de construction automatique du corpus d\u0027apprentissage et d\u0027apprentissage automatique des descripteurs. L\u0027approche Synopsis proposée par Duthil et al. ((Duthil et al., 2012)) entre dans ce cadre. D\u0027une part, Synopsis nous permet de construire automatiquement un corpus d\u0027apprentissage à partir de \"mots germes\", et d\u0027autre part, l\u0027approche nous permet un apprentissage automatique des descripteurs.\nConstruction du corpus d\u0027apprentissage\nLa construction du corpus d\u0027apprentissage a pour objectif d\u0027obtenir des documents web qui ont un contenu similaire à la vignette requête. À chaque vignette saillante est associé un corpus de documents. Plus formellement, à chaque vignette q, q variant de 1 à k, k étant le nombre de vignettes identifiées, un corpus Doc q de n documents est associé tel que Doc q \u003d doc q n , n \u003d 1 . . . n q .\nApprentissage des descripteurs\nL\u0027objectif de l\u0027apprentissage est de construire un lexique de descripteurs textuels (mots) décrivant sémantiquement chacun des logos. À chaque logo est associé un lexique L q . Pour faire le lien entre les éléments graphiques contenus dans le document et les différents concepts auxquels ils font référence, nous utilisons le corpus d\u0027apprentissage constitué à l\u0027étape 2. L\u0027approche Synospis est principalement basée sur deux éléments clés : la notion de fenêtre et la notion de classe/anti-classe. La fenêtre permet d\u0027effectuer un apprentissage des descripteurs tout en assurant leur cohérence sémantique avec le mot germe (Duthil et al., 2012). La notion de classe/anti-classe permet de filtrer le bruit web. Les noms communs et les noms propres sont les deux classes grammaticales apprises car elles sont reconnues comme porteuses de sens.\nPlus formellement une fenêtre de taille sz centrée sur un mot germe g pour un document doc est définie par F (g, sz, doc) \u003d {m ? doc/d Le principe général est de calculer la représentativité (Duthil et al., 2012) d\u0027un mot M dans chacune des deux classes (fréquence d\u0027apparition normalisée ?(M ) dans la classe (mots présents dans les fenêtres) et dans l\u0027anti-classe ?(M ) (mots en dehors des fenêtres). La repré-sentativité d\u0027un mot M dans chacune des classes est défini tel que :\nF (?, sz, doc))| À partir de la représentativité d\u0027un descripteur dans chacune des classes, il devient possible de déterminer la proximité sémantique du descripteur M considéré en appliquant une formule de discrimination f tel que cela est proposé dans (Duthil et al., 2012). Un score Sc(M ) est alors attribué à chaque descripteur. Chaque descripteur constitue une entrée du lexique L q propre au logo q considéré. Le score d\u0027un descripteur est calculé tel que :\nAnnotation des documents\nL\u0027étape d\u0027annotation consiste à rattacher à un document l\u0027ensemble des concepts qu\u0027il contient (Lexique de mots précédemment construits). Annoter sémantiquement un document au format image revient à rechercher, et à identifier, les logos qu\u0027il contient afin de lui rattacher les lexiques associés (sémantique). Annoter un document à partir de son contenu textuel (résul-tat d\u0027OCR) consiste à identifier les concepts contenus dans le document. L\u0027approche Synopsis permet d\u0027identifier (segmentation), à partir d\u0027un lexique, les zones du document qui traitent du concept (logo) considéré. L\u0027approche utilise une fenêtre glissante (Duthil et al., 2012) centrée sur les noms communs pour identifier les segments de textes pertinents. Cette méthode nous permet également de connaître l\u0027intensité ( (Duthil et al., 2012)) du discours. À chaque document est associé un fichier xml qui contient un ensemble d\u0027informations sémantiques : lexiques associés (identifiant du lexique correspondant au nom du logo), intensité du discours et l\u0027importance du discours pour chacun des concepts (lexiques) qui ont été rattachés.\nExpérimentations\nDans cette section nous évaluons la qualité de l\u0027apprentissage sur un corpus de 1766 documents administratifs dans un contexte de classification. Ce corpus est composé de 4 classes de documents : acte de mariage (A-M), certificat d\u0027assurance (C-A), relevé d\u0027identité bancaire (RIB) et certificat de naissance (C-N). Le tableau 1 montre la répartition de ces quatre classes (Nombre de logos différents dans la classe et nombre de document de la classe). Cette base confidentielle provient d\u0027un des leaders mondiaux de la dématérialisation documentaire. Chaque document contient un des 196 logos identifiés dans le corpus. Les documents sont scannés en 200 dpi noir et blanc. Nous utilisons les indicateurs classiques de mesure pour éva-luer la classification : Précision, Rappel. La Précision est calculée en considérant les erreurs d\u0027identification du logo : le système identifie un logo qui n\u0027est pas le bon, le lexique associé ne correspond donc pas au logo à identifier. Le Rappel est calculé en utilisant le nombre de logos correctement identifié par le système. Nous utilisons durant tout le processus comme moteur de recherche web Google Images. Nos résultats sont résumés dans le tableau 2.\nLes résultats montre la pertinence du système. Les différences de résultats entre chacune des classes s\u0027expliquent par la qualité des documents. Cependant, les résultats sont remarquables, nous obtenons un Rappel de 80,6 et une précision de 100 toutes classes confondues. La Précision (100) met en évidence la robustesse de l\u0027approche. \n"
  },
  {
    "id": "299",
    "text": "Introduction\nLe monde des musées aujourd\u0027hui connaît un engouement sans précédent qui a vu plus de 31 millions de visiteurs se précipiter en 2012 dans les musées nationaux français (NuitDesMusées, 2013). Par ailleurs, et depuis plus d\u0027une dizaine d\u0027années, les ministères et les services publiques des différents pays accordent de plus en plus d\u0027importance à l\u0027ouverture et à la réuti-lisation de leurs données collectées ou produites au niveau de leurs différents établissements. Or, avec le temps ces données s\u0027accumulent et deviennent très difficiles à stocker, à traiter et à analyser. D\u0027où la nécessité de s\u0027orienter vers de nouvelles solutions et paradigmes de programmation afin de faire face à ces difficultés. Nous proposons dans ce travail un algorithme basé sur le paradigme MapReduce (Dean et Ghemawat, 2008) qui permet, à partir des données ouvertes du Ministère français de la communication et de la culture, de déterminer le degré d\u0027accessibilité des personnes handicapées (tout type d\u0027handicape) aux musées nationaux français. L\u0027algorithme définit, à partir de ces données brutes et hétérogènes collectées, une note pour chaque musée selon son degré d\u0027accessibilité, et retourne un classement final des musées par commune et par région de France. Notre algorithme de classement présente plusieurs avantages notamment en terme de rapidité de traitement de grandes quantités de données puisqu\u0027il ne s\u0027exécute pas de manière classique (mono-poste) mais distribué sur un cluster d\u0027ordinateurs. Nous avons implémenté notre algorithme sur une plateforme Hadoop multi-noeuds.\nJeu de données\nL\u0027algorithme que nous présentons dans ce papier est appliqué aux données publiques 1 ré-coltées à partir de la grande manifestation annuelle : \"La nuit européenne des musées\". Elles concernent plus de 3000 musées en Europe dont plus de 1200 établissements français labellisés \"Musées de France\" par le Ministère de la communication et de la culture. Ces données sont fournies sous le format de fichier .csv, elles regroupent des informations (plus de 48 attributs avec la dernière mise à jour) sur chaque établissement tel que : Le nom du musée, adresse, commune, etc. Mais aussi des données sur différents types d\u0027accessibilités aux personnes handicapés offerts tel que : accès-handicape-moteur, accès-handicape-visuel, accès-handicape-auditif, accès-handicape-intellectuel et acces-handicap-langueDesSignes. Ces dernières sont des données binaires, le but est de traiter ces données là afin d\u0027extraire une note d\u0027accessibilité pour chaque musée, et au final agréger les résultats obtenus géographiquement par ville et par région de France. \nSummary\nBased on official data from the French Ministry of Communication and Culture, we propose in this paper a parallel algorithm as a solution to extract and process these data sets in order to define a ranking of national museums and galleries according to the accessibility degrees for people with disabilities.\n"
  },
  {
    "id": "300",
    "text": ", Christine Froidevaux\n(1), (3) (1) LRI, CNRS UMR 8623, Université Paris-Sud, 91405 Orsay Résumé. Les fonctions biologiques dans la cellule mettent en jeu des interactions 3D entre protéines et ARN. Les avancées des techniques exérimentales restent insuffisantes pour de nombreuse applications. Il faut alors pouvoir pré-dire in silico les interactions protéine-ARN. Dans ce contexte, nos travaux sont focalisés sur la construction de fonctions de score permettant d\u0027ordonner les solutions générées par le programme d\u0027amarrage protéine-ARN RosettaDock. La méthodologie d\u0027évaluation utilisée par RosettaDock impose de trouver une fonction de score s\u0027exprimant comme une combinaison linéaire de mesures physicochimiques. Avec une approche d\u0027apprentissage supervisé par algorithme géné-tique, nous avons appris différentes fonctions de score en imposant des contraintes sur la nature des poids recherchés. Les résultats obtenus montrent l\u0027importance de la signification des poids à apprendre et de l\u0027espace de recherche associé.\nIntroduction\nLa plupart des mécanismes cellulaires mettent en jeu des complexes protéine-ARN. La compréhension de leurs fonctions dans un but thérapeutique ne peut se faire que par une connaissance fine des mécanismes moléculaires. Même si plus d\u0027un millier de structures 3D de complexes protéine-ARN sont disponibles dans la Protein Data Bank 1 , base de données de référence des structures 3D, la résolution expérimentale reste longue et coûteuse, parfois même impossible. Les travaux présentés dans cet article sont focalisés sur l\u0027amélioration d\u0027une des approches de référence dans le domaine de la prédiction de l\u0027amarrage (docking) de structures 3D in silico : RosettaDock (Gray et al. (2003)). L\u0027objectif de ces approches est de modéliser la protéine et l\u0027ARN et d\u0027en prédire les assemblages 3D les plus probables. De nombreuses méthodes, dont RosettaDock, fonctionnent en deux phases imbricables l\u0027une dans l\u0027autre : (1) génération d\u0027un large ensemble de candidats 2 et (2) évaluation de ces candidats pour ne retenir que les plus plausibles. La \"qualité\" des candidats est évaluée avec une fonction de score dédiée à la problématique de l\u0027amarrage (Lensink et Wodak (2010)). La complexité des objets étudiés rend quasiment impossible l\u0027obtention d\u0027un candidat en tous points identique à la solution obtenue expérimentalement. Afin de déterminer pour chaque candidat s\u0027il est acceptable, la mesure RMSD (Root Mean Square Deviation) est calculée entre ce candidat et la solution. Tous les candidats ayant un RMSD ? 5 Å sont considérés comme des solutions acceptables (des presque natifs), les autres candidats étant appelés leurres.\nLes travaux présentés dans cet article concernent la construction d\u0027une fonction de score permettant de trier les candidats générés. Il a déjà été montré que les techniques d\u0027apprentissage se prêtaient bien à ce genre de problème Bernauer et al. (2007); Bourquard et al. (2011) . Nous avons choisi d\u0027adapter l\u0027outil RosettaDock pour l\u0027amarrage protéine-ARN en créant une fonction de score spécifique. Les performances obtenues par RosettaDock, dans le cadre de la compétition internationale d\u0027amarrage CAPRI 3 , font de cet outil un logiciel très performant pour l\u0027amarrage protéine-protéine.\nLa problématique de l\u0027amarrage protéine-ARN est assez récente dans CAPRI (Lensink et Wodak (2010); Pons et al. (2010)) et il n\u0027existe pas encore de consensus sur la nature des fonctions de score à utiliser. Dans sa version actuelle, RosettaDock ne dispose pas d\u0027une fonction de score dédiée. Les fonctions de score de RosettaDock sont de la forme suivante : f (X) \u003d i w i x i où X représente le candidat à évaluer, w i les poids de chaque attribut et x i les attributs physico-chimiques. Les attributs sont tels que des poids à valeurs positives sont biologiquement interprétables mais la restriction des poids aux valeurs positives impose une contrainte forte sur l\u0027espace de recherche des poids optimaux. Nous avons donc relâché cette contrainte en autorisant les poids à évoluer dans les trois intervalles suivants :\n. La recherche de ces poids est effectuée par un algorithme génétique présenté dans la section 3.\nDonnées utilisées\nPour ce travail, nous avons utilisé un jeu de données de 120 complexes binaires (une protéine et un ARN) de référence issus de la PRIDB 3 Protocole expérimental L\u0027utilisation de RosettaDock impose que les fonctions de score recherchées sont des combinaisons linéaires des attributs physico-chimiques présentés dans la section 2. Plusieurs formes d\u0027apprentissage ont été testées pour optimiser les poids des différents attributs : régression linéaire, régression logistique et SVM. Les meilleurs résultats ont été obtenus avec une approche par régression logistique dont les poids sont optimisés par l\u0027algorithme génétique (RO-GER (Sebag et al. (2003)) adapté à la régression logistique). La fonction à optimiser est l\u0027aire sous la courbe ROC (ROC-AUC). 100 000 itérations avec µ \u003d 10 (nombre de parents) et ? \u003d 80 (nombre d\u0027enfants) sont effectuées.\nAfin d\u0027évaluer les performances de notre approche, nous avons mis en place une variante du cadre classique d\u0027évaluation Leave-one-out. Au niveau de la phase d\u0027apprentissage, nous pouvons parfaitement mélanger les candidats issus de plusieurs couples protéine-ARN diffé-rents car nous cherchons à apprendre des poids valides pour tous les couples protéine-ARN. Par contre, pour tester l\u0027efficacité d\u0027une fonction de score, il est impératif de travailler sur un ensemble de candidats issus d\u0027un seul et unique couple protéine-ARN. Nous avons donc mis en place une validation de type Leave-one-pdb-out, où pdb se réfère à la structure native issue de la PRIDB.\nSachant que le jeu de données de référence contient 120 structures natives, nous avons effectué 120 apprentissages à partir de 119 × 10 000 candidats. Pour ne pas biaiser la phase d\u0027apprentissage nous avons échantillonné les jeux d\u0027apprentissage qui contiennent 30 presque natifs et 30 leurres par complexe, soit 3 570 candidats de chaque classe par jeu. Puis, chaque fonction apprise a été évaluée sur les 10 000 candidats associés à la structure native écartée pour le test.\nLes résultats obtenus pour les trois fonctions de scores apprises, avec des contraintes différentes sur l\u0027espace de recherche des poids, sont présentés sous deux angles : l\u0027analyse \"classique\" des performances en ROC-AUC et l\u0027analyse plus \"biologique\" des résultats en nous focalisant sur le gain de performance par rapport au cadre d\u0027évaluation CAPRI.\nRésultats\n5\nNous présentons les résultats obtenus pour les quatre fonctions de score suivantes : POS, la fonction de score à poids dans [0 ; 1] ; NEG à poids dans [?1 ; 0] ; ALL dans [?1 ; 1] et ROS, la fonction par défaut de RosettaDock. La figure 1 montre les résultats obtenus pour les 4 fonctions de score évaluées. On remarque que ROS ne permet pas de trier correctement des candidats (AU C \u003c 0, 5) alors que POS le permet. C\u0027est la fonction de score la plus performante (AU C \u003d 0, 80 ± 0, 02). POS présente aussi une courbe ROC de forte pente à l\u0027origine, indiquant que l\u0027enrichissement en presque natifs dans les premiers résultats est importante. Les autres fonctions de scores ont des AUC nettement inférieures. La connaissance de l\u0027interprétation physico-chimique des descripteurs qui incite à ne chercher que des poids positifs fait augmenter drastiquement la performance. Avec ALL, les minimima locaux de l\u0027intervalle [?1 ; 0] sont si importants qu\u0027ils restreignent les solutions à ce dernier intervalle, ne permettant pas d\u0027obtenir les meilleures solutions dans [0 ; 1]. Le score d\u0027enrichissement défini par Tsai et al. (2003) et noté SE, représente la proportion des candidats se trouvant à la fois dans les 10 % premiers candidats en score et en RMSD. C\u0027est un critère d\u0027évaluation courant pour les expériences à grande échelle de prédiction de structures 3D. Sa valeur varie de de 0 à 10 avec 10 pour une fonction de score extrayant parfaitement les 10 % meilleurs candidats en RMSD, notés top10% RM SD et 1 pour une fonction de score triant aléatoirement. On considère habituellement qu\u0027un score supérieur à 5 est preuve de performances très intéressantes dans ce domaine.\nOn observe des scores d\u0027enrichissement supérieurs à 6 pour 27 structures natives avec POS, aucune avec les autres fonctions. Ils sont aussi supérieurs à 4 pour 54 structures natives avec POS et pour 4 avec les autres fonctions. Cela confirme la performance plus importante de POS. De façon similaire, lorsque la performance est dégradée, elle l\u0027est moins avec POS qu\u0027avec les autres. En effet, il y a 6 structures natives qui ont un score d\u0027enrichissement inférieur à 1 avec POS, alors qu\u0027il y en a 69 pour les autres fonctions. La comparaison des scores d\u0027enrichissement montre qu\u0027il n\u0027y a pas de structure native pour laquelle POS a un score d\u0027enrichissement inférieur de plus de 1 aux deux autres fonctions de score dédiées. Il y en a 6 pour lesquelles la fonction de score par défaut est meilleure de plus de 1. L\u0027évaluation des fonctions de score sur le critère du score d\u0027enrichissement corrobore les résultats obtenus en AUC et permet de parvenir à la même conclusion.\nLe score d\u0027enrichissement représente donc bien la capacité à obtenir des structures plausibles en premier. Cela est particulièrement visible sur les structures 3D. La figure 2 présente un exemple caractéristique. Les résultats obtenus grâce à POS sont de très bonne qualité et très resserrés dans l\u0027espace par rapport aux leurres. L\u0027épitope (zone d\u0027interaction) est bien (mieux) caractérisé pour les deux partenaires après sélection des meilleurs candidats.\nPour avoir un critère d´évaluation proche de celui des expérimentalistes, on utilise le nombre de presque natifs du topN . C\u0027est le nombre de presque natifs obtenus dans les N premiers candidats après tri. Pour nous placer dans l\u0027objectif CAPRI, N est fixé à 10 candidats.\nDans 92 cas sur 120, POS prédit au moins un presque natif de plus que la valeur attendue sous l\u0027hypothèse d\u0027une distribution uniforme des candidats. Par contre, les fonctions apprises échouent en proposant une structure de moins dans 11 cas, contrairement aux 21   \nConclusion et perspectives\nDans cette étude, nous avons montré qu\u0027une technique d\u0027apprentissage classique, la régres-sion logistique, réalisée à l\u0027aide d\u0027un algorithme génétique pour l\u0027apprentissage des poids, se prête bien à un problème d\u0027optimisation de poids pour les fonctions de score pour l\u0027amarrage. Nous avons aussi mis en évidence que la connaissance des données physico-chimiques qui impose a priori des contraintes sur l\u0027intervalle de recherche des poids, est ici essentielle car l\u0027intervalle de recherche ne peut être déterminé de façon automatique. Par rapport aux trois autres, la fonction de score à poids positifs permet d\u0027obtenir de très bonnes performances. La nature chimique fine des interactions pour chaque exemple pour lequel les performances de la fonction de score sont dégradées devra encore être analysée. Il semble aussi clair, étant donnés les résultats de cette étude et au vu des précédentes études réalisées sur ce même type de données, qu\u0027une fonction de score linéaire des paramètres, bien que conforme au modèle physico-chimique sous-jacent, ne permet pas une classification optimale. Une approche par filtres collaboratifs pourrait, par exemple, donner de bien meilleurs résultats. En se plaçant dans un contexte réaliste d\u0027évaluation CAPRI, la comparaison entre les structures natives du nombre attendu de presque natifs dans le top10 a montré que la nature des données ne permet pas de travailler sur des jeux de données équilibrés. Utiliser le critère ROC-AUC plutôt que le top10 pour définir la fonction à optimiser a permis d\u0027obtenir des résultats comparables entre structures natives malgré la différence d\u0027équilibre des jeux de données entre structures natives. Pour conclure, ces premiers travaux nous permettent d\u0027envisager d\u0027intégrer la fonction\n"
  },
  {
    "id": "301",
    "text": "Introduction\nDu fait de l\u0027accroissement continu des capacités de stockage, la capture et le traitement des données ont profondément évolué durant ces dernières décennies. Il est désormais courant de traiter des données comprenant un très grand nombre de variables et les volumes considérés sont tels qu\u0027il n\u0027est plus forcément envisageable de pouvoir les charger intégralement : on se tourne alors vers leur traitement en ligne durant lequel on ne voit les données qu\u0027une seule fois. Dans ce contexte, on considère le problème de classification supervisée où Y est une variable catégorielle à prédire prenant J modalités C 1 , . . . , C J et X \u003d (X 1 , . . . , X K ) l\u0027ensemble des K variables explicatives, numériques ou catégorielles. On s\u0027intéresse à la famille des prédicteurs de type Bayésien naïf. L\u0027hypothèse d\u0027indépendance des variables explicatives conditionnellement à la variable cible rend les modèles directement calculables à partir des estimations conditionnelles univariées de chaque variable explicative. Pour une instance n, la probabilité de prédire la classe cible C conditionnellement aux valeurs des variables explicatives se calcule alors selon la formule :\nOn considère ici qu\u0027une estimation des probabilités a priori P (Y \u003d C j ) et des probabilités conditionnelles p(x k |C j ) sont disponibles. Dans notre cas, ces probabilités seront estimées dans les expérimentations par discrétisation ou groupage univarié MODL (cf. ). Ces probabilités univariées étant connues, un prédicteur Bayésien naïf pondéré est décrit entièrement par son vecteur de poids des variables W \u003d (w 1 , w 2 , . . . , w K ). Au sein de cette famille de prédicteurs, on peut distinguer : -les prédicteurs avec des poids à valeur booléenne. En parcourant l\u0027ensemble des combinaisons possibles de valeurs pour le vecteur de poids, on peut calculer le prédicteur MAP à savoir le prédicteur qui maximise la vraisemblance des données d\u0027apprentissage. Cependant, quand le nombre de variables est élevé un tel parcours exhaustif devient impossible et l\u0027on doit se résoudre à un parcours sous-optimal de l\u0027espace {0, 1}\nK . -les prédicteurs avec des poids continus dans [0, 1] K . De tels prédicteurs peuvent être obtenus par moyennage de modèles du type précédent avec une pondération proportionnelle à la probabilité a posteriori du modèle (Hoeting et al., 1999) ou à leur taux de compression . Cependant, pour des bases comprenant un très grand nombre de variables, on observe que les modèles obtenus par moyennage conservent un très grand nombre de variables ce qui rend les modèles à la fois plus coûteux à calculer et à déployer et moins interprétables. Dans cet article on s\u0027intéresse à l\u0027estimation directe du vecteur des poids par optimisation de la log vraisemblance régularisée dans [0, 1] K . L\u0027attente principale est d\u0027obtenir via cette approche des modèles robustes comprenant moins de variables, à performances équivalentes. Des travaux préliminaires (Guigourès et Boullé, 2011) ont montré l\u0027intérêt d\u0027une telle estimation directe des poids. La suite de l\u0027article est organisée de la façon suivante : la régularisation parcimonieuse proposée est présentée en section 2 et la mise en place d\u0027un algorithme de gradient en ligne, anytime et à budget limité pour l\u0027optimisation du critère régularisé en section 3. Plusieurs expérimentations sont présentées en section 4 avant un bilan et la présentation de nos perspectives pour ces travaux.\nConstruction d\u0027un critère régularisé\nÉtant donné un jeu de données D N \u003d (x n , y n ) N n\u003d1 , on cherche à minimiser sa logvraisemblance négative qui s\u0027écrit :\nVu comme un problème classique d\u0027optimisation, la régularisation de la log vraisemblance est opérée par l\u0027ajout d\u0027un terme de régularisation (ou terme d\u0027a priori) qui exprime les contraintes que nous souhaiterions imposer au vecteur de poids W . Le critère régularisé est donc un critère de la forme :\nn\u003d1 où ll désigne la log vraisemblance, f la fonction de régularisation et ? le poids de la régulari-sation. Plusieurs objectifs ont guidé notre choix pour la fonction de régularisation : \nCe coefficient est supposé connu en amont de l\u0027optimisation. Si aucune connaissance n\u0027est disponible, ce coefficient est fixé à 1. Il peut être utilisé pour intégrer des préfé-rences \"métier\" entre les variables. Dans notre cas, on y décrit le coût de préparation de la variable, i.e. le coût de discrétisation pour une variable numérique et le coût de groupage pour une variable catégorielle décrits respectivement équations (2.4), resp. (2.7) de . 3. Sa cohérence avec le critère régularisé du prédicteur MODL Bayésien naïf avec sé-lection binaire des variables . Pour que les deux critères coïncident pour ? \u003d 1 et w k à valeurs binaires, on utilise finalement le terme de régularisation :\n3 Algorithme d\u0027optimisation : descente de gradient par mini-lots avec perturbation à voisinage variable\nqui sont toutes des quantités constantes dans ce problème d\u0027optimisation. Le critère régularisé à minimiser s\u0027écrit alors :\nOn se donne la contrainte que w soit à valeurs dans \nCalcul de ? t+1 ; Calcul de la valeur du critère sur l\u0027historique de taille N : CR D t,N (w t+1 ) ; if amélioration du critère then mémorisation de la meilleure valeur : w * \u003d w t+1 ; else incrémentation du compteur des dégradations successives ; end end Algorithme 1 : Algorithme de descente de gradient projeté par mini-lots (DGML) dérivée partielle :\nLe gradient ?CR D N (w t ) est le vecteur de ces dérivées partielles pour ? \u003d 1, . . . , K. On s\u0027est intéressé à sa minimisation par un algorithme de type descente de gradient projeté (Bertsekas, 1976) c\u0027est à dire un algorithme de type descente de gradient pour lequel, à chaque itération, le vecteur w obtenu est projeté sur [0, 1] K . Plusieurs objectifs ont guidé notre choix d\u0027algorithme :\n1. algorithme en ligne : que la structure de l\u0027algorithme soit adaptée au traitement d\u0027un flux de données et qu\u0027il ne nécessite donc pas le traitement de la base dans son intégralité ; 2. algorithme anytime : que l\u0027algorithme soit interruptible et qu\u0027il soit en mesure de retourner la meilleure optimisation étant donné un temps de calcul budgété au préalable. Dans l\u0027algorithme de gradient projeté de type batch, on procède itérativement en mettant à jour le vecteur de poids à chaque itération t selon le gradient calculé sur toutes les instances pondéré par un pas. Si on note w t le vecteur de poids obtenu à l\u0027itération t, la mise à jour à l\u0027itération t + 1 s\u0027effectue selon l\u0027équation :  . Cette approche batch suppose que l\u0027on dispose de l\u0027intégralité de la base pour être en mesure de débuter l\u0027optimisation. Dans sa variante stochastique, la mise à jour se fait en intégrant le gradient calculé sur une seule instance. La descente de gradient peut alors se révéler chaotique si la variance du gradient d\u0027une instance à l\u0027autre est élevée. Souhaitant adopter une approche en ligne nous avons retenu une variante à la croisée de ces deux approches (batch et stochastique) à savoir l\u0027approche par mini-lots (Dekel et al., 2012) qui consiste à orienter la descente dans le sens des gradients calculés sur des paquets successifs de données de taille que nous noterons L. Afin que les chemins de descente soient comparables lorsque la taille des lots varie, le gradient utilisé est rapporté à la taille L des mini-lots. L\u0027algorithme de descente de gradient par mini-lots adopté est résumé dans l\u0027algorithme 1.\nLa valeur optimale du pas ? t a fait l\u0027objet de nombreuses recherches conduisant à des algorithmes plus ou moins coûteux en temps de calcul. Nous avons opté pour la méthode Rprop (Riedmiller et Braun, 1993) : le calcul du pas est spécifique à chaque composante du vecteur c\u0027est à dire que ? est un vecteur de pas de dimension K et que chaque composante de ce vecteur est multiplié par un facteur plus grand, resp. plus petit que 1, si la dérivée partielle change, resp. ne change pas, de signe d\u0027une itération à l\u0027autre. En terme de complexité algorithmique, chaque itération nécessite l\u0027évaluation du critère sur un échantillon de taille N soit une complexité en O(K * N ). Pour L \u003d N , on retrouve l\u0027algorithme batch classique et pour L \u003d 1, le gradient stochastique.\nEtant donné la non-convexité du critère à optimiser, il présente en général de nombreux minima locaux vers lesquels une telle descente de gradient peut converger. Il est alors courant de lancer plusieurs descentes de gradient avec des initialisations aléatoires distinctes (approche multi-start) en espérant que l\u0027un de ces chemins de descente conduise au minimum global du critère. Afin de rendre l\u0027optimisation efficace et de ne pas perdre de temps de calcul au début de chacune de ces descentes, il est également possible de perturber la solution obtenue au bout d\u0027un certain nombre d\u0027itérations afin de \"sortir\" de l\u0027éventuelle cuvette contenant un minimum local. En rendant variable la taille du voisinage dans lequel on perturbe la solution, on se donne les moyens de sortir des minima locaux (approche Variable Neighborhood Search (Hansen et Mladenovic, 2001)). Cette approche notée DGML-VNS est décrite dans l\u0027algorithme 2. On peut remarquer que, pour un voisinage recouvrant intégralement [0, 1] K , l\u0027algorithme DGML-VNS revient à une structure multi-start avec initialisation aléatoire. D\u0027autre part, précisons que le tirage aléatoire peut conduire à une valeur non nulle pour un poids mis à zéro à l\u0027issue du start précédent. Une variable peut donc ré-apparaître en cours de lecture du flux.\nL\u0027algorithme DGML-VNS est anytime dans le sens où une estimation du minimum du critère est disponible à la fin de la première descente de gradient et qu\u0027elle est ensuite améliorée en fonction du budget disponible en temps de calcul mais interruptible à tout moment. Sa complexité totale est en O(T * K * N ) où T est le nombre total d\u0027itérations autorisées. Ce paramétrage par T permet de limiter le budget maximal utilisé par l\u0027algorithme.\nExpérimentations\nLes premières expérimentations ont pour objectif d\u0027évaluer la qualité de l\u0027optimisation obtenue avec l\u0027algorithme DGML-VNS en fonction de la taille L des mini-lots présentés et du nombre total d\u0027itérations autorisées T . Pour étudier la qualité intrinsèque de l\u0027optimisation indépendamment des performances statistiques du prédicteur associé, nous avons pour cela fixé le poids ? de la régularisation à 0 ce qui revient à optimiser directement la vraisemblance non régularisé. La seconde partie des expérimentations traite des performances statistiques du classifieur obtenu par optimisation du critère régularisé (? \u003d 0). Pour l\u0027ensemble des expérimentations les paramètres suivants de l\u0027algorithme DGML sont fixés aux valeurs suivantes :\navec une multiplication par 0.5, resp. 1.2, en cas de changement, resp. non changement, de signe entre deux gradient successifs -Max \u003d 100 le nombre maximal d\u0027itérations (i.e. le nombre de mini-lots présentés). On a vérifié que ce nombre n\u0027avait jamais été atteint pour les 36 bases testées. -Tol \u003d 5 le nombre de dégradations successives autorisées. On considère qu\u0027il y a amélioration du critère pour une amélioration d\u0027au moins \u003d 10  Toutes les expérimentations ont été menées en 10-fold-cross-validation sur les 36 bases de l\u0027UCI décrites dans le tableau 1. Dans la présentation des résultats \u0027SNB\u0027 désigne la performance d\u0027un classifieur de Bayes moyenné à l\u0027aide du taux de compression . \nExpérimentations sur la qualité de l\u0027optimisation\nFIG. 1 -Taux de compression moyen en Train et en Test pour 36 bases de l\u0027UCI\nTout d\u0027abord, nous avons étudié les performances de l\u0027algorithme DGML, c\u0027est à dire de l\u0027algorithme de descente de gradient projeté sans post-optimisation, en fonction de la taille des mini-lots L. Nous avons choisi comme indicateur de la qualité de l\u0027optimisation le taux q q q q q q q q q q q q q q q q q 1600 q 1200 q Critère q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q L\u003dbatch L\u003d1000 L\u003d100 0 20 40 60 80 400 600 800 Itération FIG. 2 -Chemins de convergence du critère selon la taille des mini-lots (DGML) au cours des itérations pour la base Phoneme de compression qui mesure le logarithme négatif de la vraisemblance du modèle, normalisé par l\u0027entropie de Shannon. Plus ce taux est proche de 1, plus la vraisemblance du modèle est élevée. Pour des modèles moins performants que le modèle aléatoire, le taux de compression est négatif. La valeur du taux sur les données d\u0027apprentissage est donc un bon indicateur de la qualité de l\u0027optimisation obtenue étant donné que le critère non régularisé est réduit à la log vraisemblance négative. La figure 1 présente le taux de compression obtenu en Train et en Test et moyenné sur les 36 bases de l\u0027UCI pour différentes tailles de mini-lots L \u003d 100, 1000, N . Dans le dernier cas, le choix L \u003d N revient à un algorithme de type batch. Les taux de compression obtenus en Train et en Test pour le SNB MODL  servent ici de référence. Les résultats obtenus indiquent que, plus la taille des mini-lots est petite, plus la qualité de l\u0027optimisation se dégrade. D\u0027autre part, les résultats obtenus pour L \u003d 1000 et L \u003d N sont très proches. Le taux de compression en Train est significativement meilleur en batch que pour L \u003d 1000 pour 8 des 36 bases. La figure 2 présente à titre illustratif la série des valeurs prises par le critère au cours de l\u0027optimisation selon la valeur de L \u003d 100, 1000, N pour la base Phoneme. Sur l\u0027ensemble des 36 bases la convergence est plus rapide mais plus chaotique lorsque la taille des mini-lots diminue.\nNous avons ensuite comparé la qualité de l\u0027optimisation pour l\u0027algorithme DGML sans post-optimisation d\u0027une part et pour un algorithme DGML post-optimisé d\u0027autre part. Plusieurs types de post-optimisations ont été testées : par multi-start (DGML-MS) ou par perturbation aléatoire à voisinage variable (DGML-VNS). Pour avoir une complexité du même ordre de grandeur que celle de l\u0027algorithme de pré-traitement univarié MODL, à savoir O(K * N * log (K * N )), on a fixé le nombre total d\u0027itérations autorisées T proportionnel à log (K * N ). Plus précisément, on a choisi T \u003d log (K * N ) * 2 PostOptiLevel où PostOptiLevel est un entier qui permet de régler le niveau de post-optimisation souhaité. Pour chacun de ces deux types de post-optimisation, on a étudié l\u0027influence de la valeur du niveau d\u0027optimisation OptiLevel \u003d 3, 4, 5. Dans la mesure où l\u0027algorithme post-optimisé mémorise au fur et à mesure la meilleure solution rencontrée, la post-optimisation ne peut qu\u0027améliorer le taux de compression en Train. On a donc dans un premier temps mesuré si cette amélioration était significative ou pas. Pour une post-optimisation de type MS, le taux de compression en Train est amélioré de manière significative pour 7, 16, 18 des 36 bases avec un niveau d\u0027optimisation respectivement de 3, 4, 5. Pour une post-optimisation de type VNS, le taux de compression en Train est amélioré de manière significative pour 18, 19, 23 des 36 bases avec un niveau d\u0027optimisation respectivement de 3, 4, 5. La post-optimisation de type VNS semble donc préférable à la post-optimisation MS : l\u0027exploration guidée par un voisinage de taille variable à partir du meilleur minima rencontré permet une recherche plus fructueuse des autres minimas qu\u0027une exploration purement aléatoire. La figure 3 permet d\u0027illustrer ce phénomène de \"gaspillage\" d\u0027itérations en mode MS au début q q q q q q q q q q q q q q q q q q q q q q q q q q q q q MS VNS q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q de chaque nouveau lancement pour la base. Les expérimentations présentées dans cette section ont mis en évidence : l\u0027effet de la taille des mini-lots sur la qualité de l\u0027optimisation. Plus cette taille est élevée meilleure est la qualité de l\u0027optimisation ; l\u0027intérêt de la post-optimisation VNS comparé notamment à une postoptimisation MS. Nous retenons donc pour la suite des expérimentations un algorithme de type DGML-VNS avec une taille de mini-lots fixée à L \u003d 1000 et un niveau d\u0027optimisation PostOptiLevel \u003d 5.\nPerformances du classifieur régularisé\nNous présentons les performances du classifieur en fonction du paramétrage pour le poids de la régularisation ? et l\u0027exposant p de la fonction |w k | p . Trois valeurs ont été testées pour ? \u003d 0.01, 0.1, 0.5 et pour p \u003d 0.5, 1, 2. Les performances en terme d\u0027AUC des neuf prédicteurs régularisés associés à ces valeurs sont présentées figure 4 avec pour référence les performances du prédicteur non régularisé obtenu pour ? \u003d 0 et du prédicteur SNB.\nPour la valeur la plus élevée du poids de régularisation, à savoir ? \u003d 0.5 (en violet sur la figure), les performances en terme d\u0027AUC sont dégradées par rapport aux performances obtenues sans régularisation (en rond rouge sur la figure) quelle que soit la valeur de p. En re- D\u0027autre part, pour étudier la parcimonie des prédicteurs obtenus, la figure 5 présente le nombre de variables retenues et la somme de leur poids. On voit tout d\u0027abord que plus p est faible plus le nombre de poids non nuls est faible également. La régularisation quadratique est celle qui conduit aux prédicteurs les moins parcimonieux. Parmi la régularisation en valeur absolue (p \u003d 1) et celle en racine carrée (p \u003d 0.5), c\u0027est la seconde qui permet une réduction la plus importante du nombre de variables retenues. Concernant la somme des poids des variables retenues, tous les types de régularisation permettent de réduire en moyenne la somme des poids. D\u0027autre part, à poids ? donné, la régularisation quadratique a un impact moins important sur la réduction de la somme des poids que les deux autres régularisations dont les performances sont très proches pour cet indicateur. En prenant en compte ces deux aspects de performance statistique et de parcimonie, le compromis p \u003d 1 et ? \u003d 0.1 semble le plus favorable. Sans dégrader les performances du prédicteur non régularisé, il permet de réduire de façon importante le nombre de variables sélectionnées ce qui rend le prédicteur plus interprétable et moins complexe à déployer.\nConclusion\nNous avons proposé une régularisation parcimonieuse de la log vraisemblance d\u0027un classifieur Bayésien naïf pondéré. Nous avons décrit et expérimenté un algorithme de descente de gradient intégrant en ligne des mini-lots de données et optimisant les poids de ce classifieur par exploration plus ou moins poussée de la solution optimale courante en fonction d\u0027un budget donné. Les expérimentations menées ont permis de montrer l\u0027intérêt de l\u0027introduction de ces \n"
  },
  {
    "id": "302",
    "text": "Introduction et état de l\u0027art\nUne structure de dépendances d\u0027une phrase traduit une hiérarchie syntaxique des mots, et permet d\u0027en inférer une sémantique. Les applications liées aux structures de dépendance sont multiples, on peut citer entre autres la modélisation de langages, la reconnaissance d\u0027implications textuelles, les moteurs de question/réponse, l\u0027extraction d\u0027information, l\u0027induction d\u0027ontologies lexicales et la traduction automatique.\nUne structure de dépendances d\u0027une phrase (cf. Fig. 1 à gauche) est un arbre dont les noeuds sont les mots, ou tokens, de la phrase. Un des mots est désigné comme la racine de l\u0027arbre (en général un verbe), à laquelle sont attachés des sous-arbres couvrant des portions contigües de la phrase. Un arbre de dépendances est constitué de relations orientées entre un mot syntaxiquement plus fort (tête) et un mot plus faible (dépendant). Le modèle de dépendances est un compromis intéressant entre l\u0027analyse syntaxique classique complète et une représentation \"sac de mots\".\nLes modèles d\u0027apprentissage supervisé de dépendances exigent un nombre important d\u0027exemples annotés à la main. Ce travail, très long et fastidieux, demande une expertise de linguiste essentielle et doit être remanié profondément à chaque nouveau type de texte analysé. La quantité de textes annotés en dépendances est faible comparée à la variété des types de textes disponibles sur le Web. Nous proposons une approche non supervisée ne demandant qu\u0027une connaissance très superficielle de la langue et du type de texte. Nous nous plaçons dans le cadre de l\u0027Apprentissage Non Supervisé de Dépendances (ANSD). Le corpus journalistique américain Penn Treebank propose une version \"dépendances\" du corpus, donnant les structures de dépendances des phrases. Klein et Manning (2004) furent les premiers à obtenir par ANSD, sur les phrases de moins de 10 mots de ce corpus, des résultats meilleurs que le simple attachement de chaque mot à son voisin de droite, ou que des arbres aléatoires. Ils ont nommé leur modèle Dependency Model of Valence (DMV). L\u0027entrée de cet algorithme est constituée des étiquettes grammaticales des mots des phrases. C\u0027est un modèle génératif dans lequel la racine de la phrase est générée et ensuite, chaque nouvelle tête génère récursivement ses dépendants gauches et droits. La catégorie du dépendant est déterminée en fonction de la tête et de la direction (gauche ou droite). L\u0027apprentissage des probabilités du modèle, dont celles des types de dépendance favorisées, est effectué par une procédure classique de maximisation de l\u0027espérance (EM), basée sur des probabilités a priori calibrées à la main suivant des critères linguistiques.\nCe modèle est riche et intéressant, mais l\u0027initialisation des paramètres du modèle est un problème complexe et essentiel. Elle demande à la fois de l\u0027innovation technique de la part d\u0027un expert en apprentissage et des connaissances linguistiques poussées de la part d\u0027un expert dans la construction syntaxique de la langue étudiée.\nApprentissage de grammaire hors contexte\nNous proposons une démarche incluant un apprentissage de grammaire hors contexte probabiliste (PCFG) par l\u0027algorithme Inside-Outside (Lari et Young, 1990), puis une analyse des phrases par une version probabiliste de CYK (Jurafsky et Martin, 2009)  Inside-Outside est un algorithme génératif, que l\u0027on peut considérer comme une extension des modèles de Markov cachés (HMM) permettant d\u0027apprendre des grammaires hors contexte probabilistes. Alors que les HMM apprennent les probabilités des règles de dérivation par des calculs sur les sous-séquences précédant et suivant une position t, Inside-Outside se base sur des calculs de sous-séquences à l\u0027intérieur et à l\u0027extérieur de deux positions t 1 et t 2 .\nCYK probabiliste est un algorithme d\u0027analyse qui choisit parmi toutes les analyses autorisées par les règles de la grammaire, celle qui est la plus probable.\nLe formalisme. L\u0027originalité de notre méthode réside dans le choix d\u0027une grammaire simple permettant d\u0027exprimer des dépendances entre les mots d\u0027une phrase. Par exemple, dans la phrase \"il a obtenu ce qu\u0027il souhaitait\", \"obtenu\" est un dominant dont dépendent à gauche \"il\" et \"a\" et à droite \"ce\", dominant lui-même \"qu\u0027il souhaitait\". L\u0027arbre de dépendances de référence est représenté en Figure 1 (gauche). Notre modèle associe à chaque mot (représenté par son étiquette grammaticale) sa qualité de dominant ou dominé par rapport à ses voisins. Pour analyser une phrase, le modèle combine ensuite, par un jeu de symboles, les groupes de mots jusqu\u0027à ce que chaque mot trouve sa place dans l\u0027arbre de dépendances (cf. -Un non terminal en majuscule va dominer le non terminal en minuscule auquel il est associé dans une règle de dérivation. Par exemple G ? G d exprime qu\u0027un dominant gauche peut se décomposer en un dominant gauche et un dominé droit. -Un non terminal gauche g (resp. G) est associé par la gauche à D (resp. d). nt ? G d ou nt ? g D. Le sens que l\u0027on donne aux non terminaux interdit de nombreuses règles, et permet de limiter la taille de la grammaire tout en gardant une signification de grammaire de dépendances latéralisée (la position relative, gauche ou droite, des mots importe). Les règles de Chomsky du premier type (nt ? nt nt) sont des règles exprimant la construction interne des phrases, nous parlons de règles de structure. Les règles de Chomsky de second type (nt ? terminal) sont celles par lesquelles on transmet l\u0027information qu\u0027une catégorie peut (ou non) dominer ses voisins de gauche ou de droite. Par exemple, nous interdirons systématiquement pour le français les règles nt ? DET pour tout nt \u003d g car un déterminant est toujours dominé par un nom dont il placé à la gauche.\nLes variantes. En fonction de la structure de la langue considérée, les règles de structure de la phrase peuvent ne pas correspondre à la forme intrinsèque des phrases. La grammaire présentée est nommée 4bin car elle contient, en plus du symbole de départ, 4 non terminaux (D, G, d et g) et les règles de structure de la phrase sont écrites de façon binaire, suivant la forme normale de Chomsky.\nLa signification de ces quatre non terminaux témoigne d\u0027une différenciation essentielle entre les rôles des catégories grammaticales qui domineraient à gauche ou à droite. Pour le français, dans de nombreux cas cette différenciation est pertinente. On peut cependant rencontrer des situations où cette différenciation n\u0027est pas pertinente. Par exemple, deux adjectifs qualifiant le même nom, l\u0027un à sa gauche, l\u0027autre à sa droite, sont dominés par le nom. Nous avons donc envisagé une version 3bin ne comportant que trois non terminaux (en plus de S). Nous avons conservé dans cette variante la latéralisation des dominés g et d, mais n\u0027avons gardé qu\u0027un non terminal dominant N non latéralisé. \nTAB. 1 -Différentes variantes de la grammaire formelle DGdg (MAJ désigne les non terminaux majuscules (G ou D)).\nLe choix de ces deux formes (4bin et 3bin) implique une vision \"binaire\" des découpages de la phrase en groupes de mots, imposée par la forme normale de Chomsky. Nous pouvons cependant nous affranchir de cette contrainte par la traduction des règles ternaires en règles binaires et envisager ainsi une structure ternaire (4ter) suggérée par les phrases du type : sujet (à gauche), verbe (au centre) et complément (à droite).\nDans la continuité de cette idée, en conservant les rôles latéralisés des dominants D et G, mais en permettant aussi une domination centrale non latéralisée, nous avons introduit un nouveau symbole N (pour neutre) dans les variantes 5ter et 5ter+ qui se distinguent par le fait que l\u0027on interdit (5ter) ou que l\u0027on autorise (5ter+) l\u0027utilisation récursive du symbole N , conduisant à des structures plus complexes dans le dernier cas. Le Tableau 1 résume les différentes variantes proposées.\nLe calibrage. Tous les modèles d\u0027ANSD sont calibrés en fonction de la langue du corpus. Ce calibrage consiste à ne sélectionner que les règles du type nt ? terminal qui ont linguistiquement un sens. Par exemple, en français, un déterminant dépendra toujours d\u0027un nom situé à sa droite ; c\u0027est pourquoi parmi les règles nt ? DET seule la règle g ? DET sera conservée. Dans les expérimentations à suivre, le calibrage a été réalisé en observant pour chaque langue quelques arbres donnés comme référence dans les treebanks de dépendances.\nExpérimentations et résultats\nLe French Treebank (Abeillé et Barrier, 2004) donne la structure en constituants (groupes nominaux, verbaux. . .) ainsi que les fonctions syntaxiques (sujet, objet. . .) de nombreuses phrases issues d\u0027articles du journal Le Monde. Depuis 2009, ce treebank a été converti en arbres de dépendances (Candito et al., 2010). Nous avons comparé les arbres appris par notre modèle à ceux donnés en référence dans le treebank de Candito et al. (2010). Pour la comparaison, nous avons utilisé le score UAS (Unlabeled Attachment Score) qui calcule, pour un ensemble de phrases, le nombre de dépendances correctes (sans les ponctuations).\nLes scores sont différents en fonction des variantes de la grammaire DGdg. Nous obtenons pour 3bin : 29.8%, pour 4bin : 32.9%, pour 4ter : 42.1%, pour 5ter : 37.4% et 5ter+ : 42.2 % ; à titre de comparaison nous obtenons un score de 14.2% pour des arbres générés aléatoirement. On observe que les deux variantes (4ter et 5ter+), qui autorisent des règles ternaires récursives, avec un groupe de mots central dominant et deux groupes latéraux dominés, engendrent des scores pratiquement identiques, nettement supérieurs à ceux obtenus pour les autres variantes. Cela suggèrerait que la structure sous-jacente de ces phrases journalistiques, assez sophistiquées, est mieux capturée par un modèle plus complexe. À notre connaissance, nous sommes les premiers à traiter cette tâche d\u0027ANSD pour le français.\nCelle-ci ayant été largement abordée pour l\u0027anglais, puis dans d\u0027autres langues à partir de la conférence CONLL 2006 (Buchholz et Marsi, 2006), nous avons confronté notre modèle à la référence DMV. Le Tableau 2 résume les résultats obtenus, ainsi que la variante ayant engendré le meilleur score. Les résultats pouvent différer beaucoup d\u0027une variante à une autre, celle-ci doit être judicieusement choisie en fonction de la langue et du type de texte. TAB. 2 -Les meilleurs scores UAS obtenus comparés aux références soft-EM données dans Spitkovsky et al. (2011). Les treebank de dépendances des différentes langues proviennent : pour l\u0027Anglais : Marcus et al. (1993), pour le français : Candito et al. (2010); Abeillé et Barrier (2004), les autres langues étant celles de CONLL 2006, Buchholz et Marsi (2006.\nDiscussion et conclusion\nLes temps d\u0027apprentissage dépendent fortement du volume des données et faiblement du nombre de catégories. Le petit nombre de règles de structures de la grammaire permet cependant un apprentissage raisonnable en temps, voire très rapide sur de petits corpus. Une fois la grammaire apprise, l\u0027analyse est quasiment instantanée (quelques secondes pour des milliers de phrases). Ceci argue de la souplesse de notre modèle et de la rapidité de sa mise en oeuvre. Celui-ci est donc portable et performant, relativement à DMV. D\u0027autres tests ont révélé que les résultats peuvent encore être améliorés en considérant des catégories grammaticales plus fines (morpho-syntaxe). Le temps d\u0027apprentissage s\u0027en ressent nécessairement.\nPour améliorer encore notre modèle, nous envisageons d\u0027y intégrer des informations lexicales pour que deux séquences de catégories identiques puissent, en fonction du vocabulaire, être interprétées en arbres de dépendances différents.\n"
  },
  {
    "id": "303",
    "text": "Introduction\nSelon Klein (2001), la fusion d\u0027ontologies est \"la création d\u0027une nouvelle ontologie à partir de deux ou plusieurs ontologies existantes avec des parties qui se chevauchent\". La création de la nouvelle ontologie est généralement une tâche difficile et requiert un cadre formel capable de contrôler les différentes étapes de construction. Cet article propose ainsi de lever ce verrou scientifique par l\u0027utilisation des grammaires de graphes typés (T GG) basées sur les approches algébriques. Les T GG sont un formalisme mathématique de représentation et de manipulation des graphes. Elles sont composées d\u0027un graphe type (T G), un graphe hôte (G) et d\u0027un ensemble de règles de production (P ) appelées aussi règles de réécriture. Ces règles sont définies par une paire de graphes : 1) LHS (Left Hand Side) représente les pré-conditions de la règle ; 2) RHS (Right Hand Side) représente les post-conditions et doit remplacer LHS dans G. Les règles peuvent également avoir des conditions supplémentaires appelées N AC (Negative Application Conditions). La transformation de graphe consiste ainsi à définir comment un graphe G peut être transformé en un nouveau graphe G . Pour cela, il doit exister un morphisme qui remplace LHS par RHS pour obtenir G . Différentes approches ont été proposées pour appliquer ce remplacement. Dans ce travail, nous utilisons l\u0027approche algébrique Simple pushout SPO (Löwe, 1993). Ainsi, appliquer une règle de réécriture à un graphe G, selon la méthode SPO, revient à : 1) trouver le LHS dans G ; 2) supprimer de\nDans notre travail (Mahfoudh et al., 2013), nous avons utilisé les T GG pour la formalisation et l\u0027implémentation des changements ontologiques. Elles nous ont permis, grâce à leurs \nLes ontologies et les grammaires de graphes typés\nEn adoptant le formalisme de grammaire de graphes typés sur les ontologies, on obtient : -G est le graphe hôte qui représente une ontologie (voir l\u0027exemple de la Figure 1). -T G est le graphe type qui représente le méta-modèle de l\u0027ontologie. Dans le cadre de cet article, c\u0027est le méta-modèle d\u0027OWL qui a été retenu. Ainsi, les types des noeuds sont les classes (C), individus (I), propriétés (P \u003d DP ? OP avec DP sont les datatype properties et OP sont les object properties). Les types des arêtes sont les axiomes (A). Les restrictions (R) nécessitent à la fois les noeuds et les arêtes. -P sont les règles de réécriture correspondantes aux changements ontologiques (ex.\nAddClass, RemoveP roperty). Un changement ontologique CH \u003d (N ame, N ACs, LHS, RHS, CHDs) avec : 1) N ame précise le nom du changement ; 2) N ACs défi-nissent les conditions qui ne doivent pas être satisfaites pour pouvoir appliquer le changement ontologique ; 3) LHS représente les pré-conditions du changement ; 4) RHS définit les post-conditions du changement ; 5) CHDs sont les changements dérivés, c\u0027est à dire les règles de réécriture additionnelles qui sont attachées au CH pour corriger ses éventuelles inconsistances. Plus de détails et exemples des changements ontologiques sont présentés dans (Mahfoudh et al., 2013). Nous décrivons dans ce qui suit notre approche de fusion des ontologies. A noter que l\u0027approche a été implémentée en se basant sur l\u0027API AGG (Algebraic Graph Grammar).\nRecherche de similarité\nPour identifier la similarité entre les ontologies, nous nous sommes basés sur la distance de Levenshtein pour détecter les correspondances syntaxiques et l\u0027ontologie linguistique WordNet pour identifier les correspondances sémantiques. Le processus de recherche de similarité prend ainsi deux ontologies et génère l\u0027ensemble de correspondances suivantes : 1) CN est l\u0027ensemble des noeuds communs ; 2) EN est l\u0027ensemble des noeuds équivalents ; 3) SN est l\u0027ensemble des noeuds partageant une relation sémantique. Les relations de subsomption (IsaN ) sont identifiées manuellement. En considérant l\u0027exemple des ontologies O 1 et O 2 , on aura : 1) CN \u003d {\"Automobile\", \"F iat, \"BM W \"} ; 2) EN \u003d {(\"has_owner\", \"hasOwner\")} ; 3) SN \u003d {(\"P erson\", \"Individual\")}. 4) IsaN \u003d {(\"German_Car\", \"European_Car\"), (\"Italian_Car\", \"European_Car\"), (\"M ercedes\", \"German_Car\")}.\nFusion des ontologies\nLe processus de fusion des ontologies avec l\u0027approche SPO passe par trois étapes. La première vise à minimiser la différence entre les deux ontologies. Elle remplace ainsi les entités de l\u0027ontologie O 1 par leurs équivalents de O 2 et ceci en appliquant la règle de réécriture RenameN ode (N i , N j ) avec N i est un noeud de O 1 et N j est un noeud de O 2 . On aura alors pour ce SPO : 1) le graphe hôte est l\u0027ontologie O 1 ; 2) le LHS est le graphe constitué de l\u0027ensemble des noeuds {N i ? EN } ; 3) le RHS est le graphe formé par {N j ? EN }. L\u0027étape 2 consiste à créer l\u0027ontologie (CO) qui est le sous-graphe commun entre les deux ontologies. Elle est composée par les noeuds communs (CN ) et les arêtes qu\u0027ils partagent. La dernière étape fusionne les ontologies avec la règle de réécriture M ergeGraph(CO, O 2 ). Cette règle a comme graphe hôte l\u0027ontologie O des axiomes est contrôlé par les N ACs pour ne pas altérer la consistance de l\u0027ontologie. Ainsi la règle AddSubClass (C 1 , C 2 ) est définie par :\n-NACs : 1) C 1 C 2 , condition pour éviter la redondance ; 2) C 2 C 1 , la relation de subsomption ne peut pas être symétrique ; 3) C 1 ¬C 2 , les classes qui partagent une relation de subsomption ne peuvent pas être disjointes ; 4)\n, s\u0027il existe dans l\u0027ontologie une classe C i qui est à la fois la subClass de la classe C 2 et la superClass de C 1 , alors, C 1 est déjà la subClass de C 2 et ce lien de subsomption ne doit pas être rajouté ; 5)\n, les classes qui partagent une relation de subsomption ne peuvent pas avoir des subClasses disjoints. -LHS : {C 1 , C 2 }, les classes doivent exister dans l\u0027ontologie.\n-RHS : (C 1 C 2 ), l\u0027axiome doit être ajouté à l\u0027ontologie.\n-CHD : ?.\nConclusion et futurs travaux\nNous avons présenté dans cet article une approche formelle de fusion d\u0027ontologies basée sur les grammaires de graphes typés. L\u0027approche proposée a été implémentée avec l\u0027outil AGG qui permet de réaliser les SPOs requis pour la fusion. Comme perspectives de ce travail, nous envisageons l\u0027étude de différents conflits qui peuvent affecter le résultat de fusion et la manière de les résoudre.\n"
  },
  {
    "id": "304",
    "text": "Introduction\nCes dernières années, plusieurs travaux se sont intéressés à la fouille de graphes pour modéliser des phénomènes réels. Récemment, on trouve des travaux sur les graphes dynamiques (Pei et al. (2005); Borgwardt et al. (2006); Bilgin et Yener (2006); Robardet (2009) ;Rossi et al. (2013)) qui ont permis la modélisation de l\u0027évolution d\u0027objets au cours du temps ainsi que leurs relations. Dans ce type de méthodes, on analyse essentiellement les évolutions structurelles. Par exemple dans (Robardet (2009)), les auteurs étudient l\u0027évolution de sous-graphes en considérant des opérations tels que le découpage, le regroupement, la suppression ou la création de quasi-cliques. De plus en plus de travaux de fouille de graphes s\u0027orientent vers les graphes attribués (e.g., Moser et al. (2009) ;Silva et al. (2012); Mougel et al. (2012a)) qui sont des graphes dynamiques dont les sommets sont décrits par des attributs. Ces derniers ont permis l\u0027étude de plusieurs domaines d\u0027applications notamment les réseaux biologiques (Fukuzaki et al. (2010); Mougel et al. (2012b)). Mais à notre connaissance peu de travaux traitent la fouille de graphe dynamique dont le nombre de sommets évolue dans le temps.\nDans cet article, nous proposons une approche permettant d\u0027étudier les graphes attribués dynamiques et dont le nombre de sommets varient dans le temps. Dans ce type d\u0027approche on considère une séquence temporelle de graphes attribués dont le nombre de sommets peut varier dans le temps. Un domaine d\u0027application pour lequel une telle approche est particulièrement intéressante est l\u0027analyse d\u0027objets dans une séquence d\u0027images satellites à différentes résolu-tions. En effet, il est très difficile de constituer une longue séquence temporelle d\u0027images à la même résolution notamment des images à très haute résolution qui sont coûteuses en acquisition.\nPour traiter ce problème, nous proposons un nouveau modèle de données basé sur une séquence de graphes attribuées 1 nommé Graph Attribué Multi-résolution (GAM). Un GAM est une séquence de graphes tels que les sommets entre deux temps consécutifs peuvent être reliés par des arêtes temporelles, les arêtes reliant des sommets à un temps donnée sont appelées arêtes structurelles. A partir de ce modèle, nous proposons de rechercher des motifs nommés Graphes Multi-résolutions Homogènes (GMH). Un GMH est formé par une collection de sous-graphes connexes et homogènes nommés Graphe Connexe Homogène (GCH). La propriété d\u0027homogénéité assure que le graphe est formé par des sommets partageant des propriétés similaires. La contrainte structurelle de connexité permet de trouver des graphes sans forme définie a priori. Dans le cadre de l\u0027analyse d\u0027images, ces deux conditions sont basées sur les hypothèses suivantes : (1) un GCH représente un segment de l\u0027image correspondant à un objet réel, (2) un objet segmenté dans une image est généralement décrit par des pixels ayant des propriétés similaires (e.g., rouge, vert, bleu) et (3) la forme des objets segmentés n\u0027est pas connue à l\u0027avance mais reste connexe.\n2 Contexte et définition des motifs\nLa fonction AtbV associe un ensemble de valeurs d\u0027attributs à chaque sommet.\nLe nombre de pas de temps dans G (i.e., |V|) est noté T G . L\u0027ensemble de tous les sommets de G est noté V G , i.e., V G \u003d ? i V i . Le domaine de tous les attributs de la collection A t ? A est noté D t , i.e., D t \u003d ? A?At Dom(A). Le sous-graphe de G induit par l\u0027ensemble de sommets\nNous définissons une fonction Hmg associant l\u0027ensemble des valeurs d\u0027attributs partagés par un ensemble de sommets et la fonction inverse V ert. Etant donné un ensemble de sommets V et un ensemble de valeurs d\u0027attributs A,\nNous proposons une nouvelle classe de motifs nommée Graphes Homogènes Multi résolu-tions (GMH). Un GMH est formé par une collection de sous-graphes connexes et homogènes nommés Graphe Connexe Homogène (GCH).\nseuils définis par l\u0027utilisateur. Un ensemble de sommets H tel que H ? V t ? V est un Graphe Connexe Homogène (GCH) si et seulement si (1) tous les sommets de H partagent au moins h · |A t | valeurs d\u0027attributs en commun avec les autres sommets de H, i.e., |Hmg(H)| \u003e\u003d h·|A t | ; (3) il existe un chemin passant par des arêtes structurelles entre chaque paires de sommets de H ; (2) la collection contient au moins |H| / |V t | ? s sommets ; et (4) il n\u0027existe pas de sommets v ? V G n\u0027appartenant pas à H tel que H ? {v} vérifie les conditions précédentes.\nLa première condition assure l\u0027homogénéité de l\u0027ensemble des sommets par rapport aux attributs. La condition (2) s\u0027intéresse à la structure du graphe. Dans le contexte de l\u0027analyse d\u0027images, la contrainte de connexité permet de trouver des régions de forme arbitraire. La condition (3) permet de filtrer les petits graphes qui peuvent ne pas être intéressant pour les experts. Enfin, la condition (4) assure la maximalité des GCH par rapport à l\u0027inclusion.\nUn GCH décrit une partie d\u0027un GAM pour un temps donné. Afin d\u0027étudier l\u0027évolution des GCH nous proposons de grouper les GCH connectés entre des pas de temps consécutifs.\n|?(H1)| ? ? , avec ?(H) l\u0027ensemble des sommets connectés par des arêtes temporelles par des sommets de H. I.e.,\n. . , H n est une chaîne connectée temporellement si et seulement si ?i ? {1, . . . , n ? 1}, H i est connecté temporellement à H i+1 .\nDéfinition 2.4 (Graphe Multi-résolution Homogènes) Une collection P de GCH est un Graphe Multi-résolution Homogène (GMH) ssi. (1) pour chaque paire H 1 , H 2 ? P , il existe un GCH H ? P et deux chaînes connectées temporellement S 1 et S 2 formées uniquement par des GCH de P tel que (2) pour tout GCH H, si H n\u0027appartient pas à P alors il n\u0027existe pas de GCH de P connecté temporellement à H.\nLa condition (1) assure que chaque paire de GCH d\u0027un GMH est connectée de manière transitive par la relation de connectivité temporelle. La condition (2) assure la maximalité du motif dans le sens qu\u0027il n\u0027existe pas de GCH n\u0027appartenant pas au motif et pouvant lui être ajouté sans violer la condition (1). D\u0027après cette propriété pour calculer la collection des GCH à un temps t un algorithme naîf peut énumérer tous les ensembles non vides de valeurs d\u0027attributs X ? D t . Si |X| / |D t | ? h alors la collection des composantes connexes dans le graphe G[V ert(X)] satisfaisant la condition de taille minimale est formée uniquement par des GCH. Cependant cette approche nécessite l\u0027énumération de 2 |Dt| ? 1 ensembles de valeurs d\u0027attributs pour chaque temps. Les propriétés suivantes permettent de réduire l\u0027espace de recherche.\nMéthode d\u0027extraction\nPropriété 3.1 Soit G un GAM et V ? V t ? V un ensemble de sommets. Uniquement les sommets de V appartenant à une composante connexe de G[V ] ayant au moins s×|V t | sommets peuvent former un GCH. Description des algorithmes. L\u0027algorithme principal se déroule en deux parties. La première partie calcule la collection des GCH notée H t pour chaque temps t ? {1, . . . , T G } en appelant l\u0027algorithme 1. La deuxième partie de cet algorithme réalise l\u0027extension temporelle. Pour chaque temps t ? {1, . . . , T G ? 1}, les GCH de H t qui n\u0027ont pas été précédemment traités sont utilisés pour construire un nouveau GMH noté P . Si un GCH H n\u0027a pas été précédemment énuméré, la fonction ?(H) renvoie ?, sinon le motif auquel il appartient. Enfin, P est étendu en utilisant les GCH au temps consécutif en appelant l\u0027algorithme 2 .\nL\u0027algorithme 1 calcule la collection des GCH pour un temps donné. Le test effectué à la ligne 1 vérifie si l\u0027ensemble de sommets actuellement énuméré est homogène. Si c\u0027est le cas, la collection de GCH est mise à jour aux lignes 2 ou 3. Le test de la ligne 2 permet de traiter le cas particulier où tous les sommets à un temps donné sont homogènes. Lors des appels récusrifs, l\u0027ensemble de sommets V est nécessairement connexe donc la collection peut directement être mise à jour (ligne 3). D\u0027après la condition de maximalité, si l\u0027ensemble des sommets est homogène, l\u0027énumération de la branche peut s\u0027arréter, sinon l\u0027énumération continue aux lignes 5 à 10. Le filtrage effectué à la ligne 5 retire de l\u0027ensemble des valeurs attributs énumérées A cand les valeurs d\u0027attributs partagées par tous les sommets de V cand . L\u0027énuméra-tion des valeurs d\u0027attributs restantes est effectuée ensuite, ainsi que le calcul des composantes connexes correspondantes (ligne 8, fonction CC). Pour chaque composante connexe satisfaisant la condition de taille minimale, l\u0027algorithme est appelé de manière recursive.\nSélectionner un élément a de A cand et le retirer\nL\u0027algorithme 2 effectue le regroupement temporelle des GCH précédement extraits. étant donné un GCH H, la première ligne de l\u0027algorithme calcule les sommets V suiv connectés à H par des arêtes temporelles. La ligne 2 énumère pour chaque GCH H du temps consécutif ceux qui sont connectés temporellement à H. Le motif P en cours de construction est ensuite mis à jour avec H (ligne 3). Si H appartenait déjà à un motif, les deux motifs sont regroupés (lignes 5 et 6), sinon, un appel récursif à l\u0027algorithme est effectué à partir de H (ligne 8).\nFIG. 1 -Exemple de motif. Les zones coloriées correspondent aux GCH.\nRésultats expérimentaux\nLe jeu de données a été construit à partir d\u0027images satellites à différentes résolutions (30 mètres et 10 mètres le pixel). Huit images ont été utilisées de la même région. Nous avons utilisés six attributs (discrétisés) dont trois correspondent à la radiomètre rouge, vert et proche infrarouge. Les trois autres propriétés sont des indices calculés à partir des attributs précé-dents : l\u0027indice de rougeur, l\u0027indice de brillance, et l\u0027indice normalisé de végétation (NDVI). Dans le graphe, un sommet correspond à un pixel et les arêtes structurelles au 4 voisinages d\u0027un pixel (i.e., sans considérer les diagonales). Une arête temporelle connecte deux sommets correspondant aux pixels situés dans la même zone entre des temps consécutifs. Le GAM construit contient 4 graphes avec 10 890 000 de sommets et 4 graphes avec 1 210 000 de sommets.\nDans ce jeu de données, nous recherchons des motifs homogènes sur la moitié des attributs (i.e., h \u003d 0.5) et ayant une taille relativement petite, correspondant à une superficie d\u0027environ 0.45 km 2 (i.e., s \u003d 2 × 10\n?6\n). Plus précisément les GCH regroupés partagent au moins 80 % de leurs sommets avec un autre GCH du motif (i.e., ? \u003d 0.8). En utilisant ces seuils, 24 motifs ont été extraits en 5 minutes. Parmi ces motifs, nous présentons sur la figure 1 un motif représentant une zone qui évolue de la même manière correspondant à l\u0027activité minière. De manière intéressante, on peut noter que regrouper les motifs avec la contrainte de connectivité temporelle permet de bien couvrir la zone.\n"
  },
  {
    "id": "307",
    "text": "Introduction\nLe stockage et la distribution à travers les média numériques, et plus particulièrement internet, de données visuelles atteignent des proportions gigantesques. Ceci est accéléré par la banalisation des outils de capture et d\u0027éditions de données numériques telles que la vidéo et l\u0027audio. Cette masse de données de différentes modalités représente une information capitale pour les étapes d\u0027identification d\u0027événements et de décision. Il est donc nécessaire de développer des solutions automatiques pour analyser ces contenus numériques. Une des problématiques qui suscitent beaucoup d\u0027intérêt depuis quelques années est la détection et la reconnaissance des actions humaines dans les séquences vidéo. On appelle action tout évènement caractérisé par des mouvements ou de comportements anormaux que l\u0027on rencontre par exemple dans les flux de vidéo surveillance Bouttefroy et al. (2010). La détection d\u0027actions trouve de nombreuses applications telles que l\u0027indexation, la vidéo surveillance Hu et al. (2004) ou le résumé de vidéos Zhou et al. (2008), pour ne citer que quelques-unes. Dans le contexte de la reconnaissance de l\u0027action, la représentation des descripteurs vidéo au moyen d\u0027un dictionnaire de mots visuels, est un domaine de recherche très actif, Willamowski et al. (2004). L\u0027idée de base d\u0027un DMV est de grouper un ensemble d\u0027objets, par exemple des descripteurs visuels, en groupes de sorte que les objets de même type soient dans un même groupe (cluster). Ré-cemment, l\u0027algorithme des K-moyennes a été largement utilisé pour construire les DMV en raison de ses hautes performances et de sa simplicité. Chaque vidéo est ensuite représentée par une distribution de mots visuels. Ces distributions servent de paramètres d\u0027entrée dans le processus d\u0027apprentissage à l\u0027issue duquel une classification des actions est obtenue. Cependant, dans une telle approche, la difficulté réside souvent dans la recherche de liens plausibles entre ces entités perceptuelles et l\u0027interprétation de la scène dans le contexte considéré. Il est donc important de trouver le moyen de définir des descripteurs plus riches en information et surtout corrélés aux actions que l\u0027on souhaite identifier et classer.\nAfin d\u0027extraire les descripteurs de vidéo, Mojarrad et al. (2008), ont utilisé des descripteurs relatifs à des régions du corps humain, moyennant quelques hypothèses souvent difficiles à satisfaire. Afin, d\u0027éviter ces problèmes, Dollár et al. (2005) ont choisi d\u0027extraire des descripteurs locaux en détectant les cuboïdes locaux, cette méthode produit des mots visuels basés sur la quantification en suivant le même principe que le DMV proposé par Csurka et al. (2004). Dans la même veine, Laptev et Lindeberg (2006) ont proposé le descripteur STIP (Points d\u0027intérêts spatio-temporels) pour détecter les cuboïdes. Néanmoins, les limites des méthodes mentionnées ci-dessus concernent non seulement la difficulté de trouver la taille optimale du \"cuboïde\", mais aussi le temps de calcul élevé. Pour surmonter ces problèmes, nous proposons un descripteur spatio-temporel basé sur le descripteur local SURF proposé dans Bay et al. (2006). Ce descripteur est ensuite étendu à un espace spatio-temporel 3D. Nous montrons expérimentalement l\u0027efficacité de cette contribution pour la détection des actions humaines dans la base réaliste \"UCF sport\" proposée par Rodriguez et al. (2008).\nApproche proposée pour la reconnaissance d\u0027actions\nLes points d\u0027intérêt ST-SURF sont localisés à l\u0027aide du détecteur fast-hessien proposé par Beaudet (1978) ensuite extraits à partir de l\u0027intégralité des vidéos de la base d\u0027aprentissage. Les ST-SURF extraits sont regroupés en utilisant l\u0027algorithme de clustering des K-moyennes. Les clips vidéo sont représentés sous forme d\u0027histogrammes de distributions de mots visuels. Enfin, l\u0027étape d\u0027apprentissage est réalisée à l\u0027aide d\u0027une machine à vecteurs de supports (SVM).\nExtention du descripteur SURF dans le domaine temporel\nL\u0027extension du descripteur SURF dans le domaine temporel est effectuée en estimant le flot optique proposé par Sun et al. (2010). Ces deniers ont montré que les algorithmes, de calcul du flot optique, fondés sur une étape de filtrage médian permettent d\u0027obtenir un flot optique stable sur un voisinage important, Sun et al. (2010).\nDans cet article, un point d\u0027intérêt IP \u003d (x, y, t) est défini par sa position (x, y) à un instant t . Dans la trame (t + n). Si cet IP effectue un déplacement u suivant la direction x et v suivant celle de y. IP devient, IP (t + n) \u003d (x + u, y + v, t + n). Dans toutes nos expériences, sauf mention du contraire, nous supposons qu\u0027en raison de la segmentation de la vidéo en ensemble de trames (ETr), selon la méthode de Megrhi et al. (2013), les trajectoires des vecteurs de mouvement sont stables et parallèles. Les points d\u0027intérêts tels que u \u003d v \u003d 0 seront négligés. L\u0027ensemble des trames forment un volume dans l\u0027espace. Ce volume est un parallélipipède. Ainsi, tout au long des trames du parallélépipède, la direction 3D (u, v, n) représente la direction du mouvement de IP . Le vecteur de mouvement est calculé pour chaque IP . Notre contribution consiste en l\u0027utilisation de l\u0027orientation du mouvement et de sa position afin de caractériser le mouvement, au lieu d\u0027utiliser le vecteur de direction (u, v, n) généré par le calcul du flot optique. Nous supposons que le vecteur de mouvement dans l\u0027espace 3D peut être défini comme l\u0027intersection de deux plans perpendiculaires respectivement au plan (x, t) et le plan (t, y). Pour extraire l\u0027orientation du IP , nous projetons le vecteur de mouvement sur les plans (x, t) et (t, y) de l\u0027ETr pour définir un angle pour chaque projection, le premier angle ? x entre le flot optique et le plan (t, x), l\u0027angle ? y entre le plan (t, y) et le vecteur mouvement. \navec t max , x max et y max sont les dimensions du Parallélipipède, t max varie en fonction du nombre de trames segmentées. Dans ce qui suit, D x et D y décrivent les distances de déplace-ment d\u0027un point d\u0027intérêt donné. La figure 1, illustre la projection du centre du parallélipipède sur les plans (t, x) et (t, y).\nFIG. 1 -La projection du vecteur mouvement sur les plans adjacents.\nExtraction du ST-SURF\nLa génération du nouveau descripteur ST-SURF est réalisée par la fusion par concaténa-tion du descripteur local SURF, de dimension 64-D et les 4 paramètres décrivant la position et l\u0027orientation de chaque SURF. Le ST-SURF est donc un vecteur de 68-D. Les ST-SURF générés sont quantifiés en mots visuels en utilisant l\u0027algorithme des k-moyennes. Chaque sé-quence vidéo est alors représentée par l\u0027histogramme de fréquence des mots visuels. Les histogrammes des occurrences de mots visuels qui en résultent sont utilisés comme entrées du classifieur SVM.\nRésultats expérimentaux\nLe tableau 1, comporte les Meilleures Moyennes de Précisions (MMP) pour les différents ensembles détecteurs/descripteurs de l\u0027état de l\u0027art ainsi que la Précision Moyenne reportée en utilisant le détecteur Hessien (PMH). Hu et al. (2004) ont obtenu une précision de 77,4% en utilisant le descripteur HOG et 82,6% en utilisant le HOF. En effet les descripteurs de mouvement local, donnés par les histogrammes des flots optiques(HOF), caractérisent mieux l\u0027action que les histogrammes du gradient orienté (HOG) qui décrivent l\u0027apparence locale. L\u0027utilisation de la combinaison HOG/HOF n\u0027améliore pas la précision de la reconnaissance d\u0027action. En effet, un taux de 81,6% a été reporté par Hu et al. (2004), ceci s\u0027explique par le fait que les HOG sont moins précis pour caractériser l\u0027information temporelle. L\u0027extension du HOG dans le domaine temporel a permis à Wang et al. (2009) d\u0027atteindre 85% en utilisant HOG3D/Gabor. L\u0027orientation spatiale de ce descripteur décrit les informations de l\u0027apparence et l\u0027orientation temporelle extraite renseigne sur la la vitesse du mouvement. La précision du ST-SURF reste en dessous des résultats réalisés par Laptev et al. avec 85 % en utilisant la combinaison HOG3D/Gabor. Ainsi, nous adoptons l\u0027hypothèse que celà pourrait être dû à la génération de différents DMV et l\u0027utilisation de différents détecteurs de points d\u0027intérêt. En utilisant la combinaison détecteur Hessien/SURF, Le ST-SURF que nous proposons donne les meilleurs résultats PMH et atteint 80,7 % de précision surpassant tous les descripteurs locaux spatio-temporels de l\u0027état de l\u0027art. En effet, ce descripteur est une combinaison de l\u0027information spatiale, donnée par le SURF, et l\u0027information temporelle dérivée du flot optique. TAB. 1 -Précision moyenne pour différentes combinaisons de détecteurs/descripteurs appliquées sur la base UCF sport.\nLa matrice de confusion relative à la base \"UCF sport\" est donnée dans le tableau 2. Nous notons que le descripteur proposé donne des résultats satisfaisants dans des vidéos réalistes. Nous soulignons que les précisions les plus faibles sont obtenues par les actions « skate » et « ride », car les mouvements de ces actions sont horizontaux. Le résultat s\u0027améliore au fur et à mesure que les actions contiennent des mouvements verticaux comme « walk », « kick » et « lift » qui présentent des mouvements de rotation importants. Les précisions entre « dive » et « swing » sont trop proches ceci est dû à la ressemblance entre ces deux actions. L\u0027ensemble de nos résultats prouvent que notre méthode est équivalente à l\u0027état de l\u0027art, et montre des performances satisfaisantes sinon meilleures que d\u0027autres méthodes utilisant la même configuration.\nTAB. 2 -Matrice de confusion de la reconnaissance d\u0027actions de la base UCF en utilisant le ST-SURF.\nConclusion\nDans cet article, nous avons proposé un nouveau descripteur spatio-temporel basé sur l\u0027extension du descripteur local SURF vers le domaine temporel. L\u0027extraction du descripteur consiste à détecter des IPs et les projeter dans un espace 3D basé sur une exploitation originale de l\u0027orientation du flot optique et de sa position. Les descripteurs extraits sont intégrés dans un DMV, pour finalement être classés en neuf actions réalistes de la base \"UCF sport\". En outre, le ST-SURF proposé démontre des performances de reconnaissance prometteuses sur cette base avec une précision d\u0027environ 80,7 %. En effet, le reparamétrage du flot-optique a permis de décrire l\u0027orientation de la trajectoire de la région d\u0027intérêt ainsi que sa position dans un volume spatio-temporel. L\u0027exploitation de l\u0027information relative à l\u0027orientaion garantit l\u0027invariance par rotation du ST-SURF. La position de la région d\u0027intérêt est extraite afin d\u0027améliorer et optimiser la classification pour plus de précision. Ainsi, la classification sera plus robuste aux décalages de pixels qui peuvent aboutir à plus de mots visuels. Ainsi, en utilisant la position de la région d\u0027intérêt à partir du flot optique (au lieu des coordonnées du point d\u0027intérêts) nous obtenons un DMV plus compact. Enfin, les résultats obtenus démontrent la viabilité de notre approche et prouve que nous sommes déjà équivalents aux performances données par l\u0027état de l\u0027art. Nous imaginons de nombreuses perspectives pour l\u0027avenir, la plus importante est d\u0027appliquer la même méthode sur des vidéos contenant des actions plus complexes. Nous prévoyons également d\u0027améliorer notre ST-SURF et envisageons de le combiner avec d\u0027autres descripteurs de bas niveau et de différentes modalités.\nRéférences\nBay, H., T. Tuytelaars, et L. Van Gool (2006 \n"
  },
  {
    "id": "308",
    "text": "Introduction\nLes sources de production d\u0027énergie autonomes intermittentes, de type photovoltaïque, connaissent un développement important dans les îles subtropicales. Un projet a été mis en place pour améliorer la capacité à prédire la production d\u0027énergie d\u0027une installation photovoltaïque grâce à un réseau de capteurs intelligents. Les données disponibles concernent 956 journées, du 2008-12-21 au 2012-03-21, sur lesquelles ont été mesurés les cumuls horaires du rayonnement solaire journalier de 9H jusqu\u0027à 17h. Nous présentons deux stratégies (Bessafi et al., 2013) de classification des jours selon leurs rayonnements solaires puis une méthode de prédiction du flux solaire basée sur les résultats des classifications précédentes\nClassification\nLe rayonnement solaire peut-être décomposé en trois flux : le flux global F Global , diffus F Dif f us et direct F Direct \u003d F Global ? F Dif f us . Nous définissons l\u0027indice de fraction directe noté k b \u003d F Direct /F Global pour représenter le rayonnement solaire journalier. Lorsque cet indice est proche de 1, le flux direct est proche du flux global et on est en présence d\u0027une journée ensoleillée ; inversement, lorsque l\u0027indice est proche de 0, la journée est nuageuse (Figure 1). On note I l\u0027ensemble de n journées, T l\u0027ensemble de p heures et K le nombre de classes. Dans la suite, les indices i, t décriront respectivement I, T et k \u003d 1 . . . K. Dans la première approche, une journée d i par le vecteur des indices k b horaires\nLa première démarche pour classer l\u0027ensemble des journées combine trois méthodes éprou-vées d\u0027analyse des données : -l\u0027Analyse en Composantes Principales pour réduire la dimension des données. Afin de trouver un nombre optimal de classes pour la partition, -la Classification Hiérarchique Ascendante de Ward (CAH) est appliquée sur un ensemble pertinent de composantes principales. -la qualité de la partition obtenue par la CAH est ensuite améliorée en appliquant la mé-thode k-means. La librairie FactoMineR (Lê et al., 2008) qui implémente cette stratégie dans le logiciel R a été utilisée. Dans la seconde approche, une journée d i est caractérisée par trois composantes\n?t 2 ) t l\u0027accélération. Pour classifier les journées, un indice global de dissimilarité est défini sur les paires des journées La plus récente méthode pour la détermination de poids optimaux est la méthode CARD (Clustering and Aggregation of Relational Data) de (Frigui et al., 2007) qui introduit une estimation des pondérations pour chaque matrice des dissimilarités. Nous proposons une nouvelle méthode (De Carvalho et al., 2012) qui détermine simultanément un ensemble de pondérations optimales et une classification des objets décrits par plusieurs matrices de dissimilarités.\nSoient P \u003d (C 1 , . . . , C K ) une partition de E, un ensemble de matrices de dissimilarités D t définies sur E et une matrice ? \u003d (? kt ) k,t où ? kt est la pondération associée à la dissimilarité\nLe problème de classification s\u0027énonce comme la recherche du triplet optimal (P\nL\u0027algorithme proposé comporte trois étapes.\n-Étape 1 : construction de la meilleure partition en K Classes -Étape 2 : calcul de la meilleure matrice de pondération -Étape 3 : recherche le meilleur vecteur de K prototypes L\u0027algorithme démarre avec un vecteur de prototypes tiré au hasard et toutes les pondérations égales à 1 et alterne ces trois étapes jusqu\u0027à la convergence. L\u0027application de la première stratégie classique de classification sur la composante position a déterminé une partition P 1 a 5 classes des journées. Pour étudier l\u0027influence des composants vélocité et accélération, nous appliquons la seconde stratégie qui calcule les pondérations ? t,k pour chaque classe et pour chaque dissimilarité D t et détermine la partition P 2 . Les courbes des moyennes des classes des partitions P 1 et P 2 sont similaires (figure 2). La similitude des deux partitions est confirmée par l\u0027analyse du tableau 1 de confusion entre \nPrédiction\nL\u0027objectif du projet est de proposer des outils de prédiction, à travers un site web, du rayonnement solaire horaire dans une journée. L\u0027intérêt de ces prédictions est de permettre d\u0027anticiper un pic ou une chute de la production d\u0027électricité pour l\u0027heure à venir. Dans la suite, F désignera un des indicateurs de flux F Global , F Dif f us , K b . Notons F (i, t) le flux à prévoir à l\u0027heure t d\u0027une journée d i et F (i, t) la valeur estimée. Le problème de prédiction se formule comme la recherche d\u0027une fonction f telle que F (i, t) \u003d f (F (i, 1 : (t ? 1))). La classification précédente a mis en évidence l\u0027existence de plusieurs régimes de flux solaire journalier (figure 2). Elle suggère de rechercher des modèles de prévision horaire par classe qui seraient plus appropriés qu\u0027un modèle unique.\nSi P \u003d {C k } k est une partition des jours, le modèle de prévision locale s\u0027écrit :\n. La mise en oeuvre de cette approche nécessite le choix d\u0027une partition P , d\u0027une fonction d\u0027affectation 1 C k d\u0027une observation à une classe et d\u0027un modèle de prévision par classe f k . Cart-Regression (Breiman et al., 1984) est un exemple de méthode qui adopte cette approche locale. Son modèle de prévision est\n. La moyenne du flux F (t) dans la classe C k est l\u0027estimation de la valeur du flux pour les journées de cette classe. Des partitions homogènes relatives à la variable à prédire F (t) sont déterminées de manière récursive et des arbres de décision calculés sur les variables prédictives permettent d\u0027affecter une journée aux classes de ces partitions.\nLa méthode de prévision globale (Regr-Globale) que nous avons utilisée s\u0027appuie sur un modèle linéaire simple :\n. Plusieurs types de coefficients ont été essayés, comme a(t) \u003d 0, b(t) \u003d\nMoyF (t)\nMoyF (t?1) ainsi que d\u0027autres statistiques la médiane, le troisième quartile. La régression linéaire simple a été retenue car elle a donné le meilleur score de prévision sur les ensembles de test. Ce modèle de régression linéaire simple a été aussi choisi pour l\u0027approche locale. Ce choix a été motivé par la propriété physique de la persistance du flux horaire, la contrainte du projet d\u0027avoir un système de prédiction efficace en ligne et les études préalables sur la sélection de variables discriminantes pour la régression. Le modèle de prévision de la régression locale (Regr-Locale) proposée s\u0027écrit\n, b(t, k) sont les coefficients estimés pour la classe C k d\u0027une partition P de l\u0027échantillon d\u0027apprentissage des jours.\nPour cette méthode, une partition unique des jours caractérisés par ses flux horaires est dé-terminée par la première méthodologie de classification précédente. La qualité d\u0027une méthode de prédiction est mesurée par le rapport des normes Le tableau 2(a) donne les valeurs de ? EQM des flux relatifs à des partitions calculées sur des échantillons d\u0027apprentissage pour des nombres de classes différents. De manière logique, on constate que plus le nombre de classes K augmente plus la qualité de la prédiction s\u0027amé-liore (l\u0027indice ? EQM décroit). Pour K \u003d n, on a ? EQM \u003d 0 car à un individu correspond une classe, la prédiction est parfaite mais ne présente pas d\u0027intérêt (sur-apprentissage). Sur les ensembles de test, la classe d\u0027appartenance d\u0027une journée est inconnue. A l\u0027heure t, une journée est affectée à la classe la plus proche selon sa distance aux centres de gravité des classes calculée à partir de F (1 : t ? 1). Le modèle de régression de la classe d\u0027affectation est ensuite choisi pour l\u0027estimation de F (t).\nLe tableau 2(b) donne les scores moyens, sur les ensembles de test, des méthodes CartRegression, Regr-Globale et Regr-Locale. On constate que la qualité moyenne de prévision de\n"
  },
  {
    "id": "309",
    "text": "Introduction\nPour les données hors-ligne, des méthodes d\u0027extractions de connaissances performantes et éprouvées depuis plusieurs années existent. Différents types de classifieurs ont été proposés : plus proches voisins, bayésien naïf, SVM, arbre de décision, système à base de règles... Mais avec l\u0027apparition de nouvelles applications comme les réseaux sociaux, la publicité en-ligne, les données du Web... la quantité de données et leurs disponibilités ont changé. Les données auparavant facilement disponibles et pouvant tenir en mémoire (données hors-ligne) sont devenus massives et visibles une seule fois (flux de données). La plupart des classifieurs, prévus pour fonctionner hors-ligne, ne peuvent généralement pas s\u0027appliquer directement sur un flux de données.\nDepuis les années 2000, l\u0027extraction de connaissances sur flux de données est devenue un sujet de recherche à part entière. De nombreux travaux traitant cette nouvelle problématique ont été proposés (Salperwyck et Lemaire, 2011;Gama, 2010). Parmi les solutions aux problèmes de l\u0027apprentissage en-ligne sur flux de données, les algorithmes d\u0027apprentissage incrémentaux sont l\u0027une des techniques les plus utilisées. Ces algorithmes sont capables de mettre à jour leur modèle à partir d\u0027un seul nouvel exemple. Cependant la plupart d\u0027entre eux, bien qu\u0027étant incrémentaux, ne sont pas capables de traiter des flux de données car leur complexité n\u0027est pas linéaire.\nDans cet article, on s\u0027intéresse plus particulièrement à l\u0027un des classifieurs les plus utilisés dans l\u0027état de l\u0027art pour réaliser une classification supervisée en ligne : le classifieur naïf de Bayes. Nous modifions ce classifieur de manière à réaliser un apprentissage en ligne pour flux de données. Ce classifieur ne nécessite en entrée que des probabilités conditionnelles P (X i |C) (où X i représente une variable explicative et C une classe du problème de classification) et sa complexité en prédiction est très faible, ce qui le rend adapté aux flux.\nNéanmoins, dans le cadre de l\u0027apprentissage hors-ligne, il a été prouvé qu\u0027en sélectionnant les variables (Koller et Sahami, 1996;Langley et Sage, 1994) et/ou en pondérant les variables (Hoeting et al., 1999) on obtient des résultats sensiblement meilleurs. De plus Boullé dans (Boullé, 2006b) a montré le lien entre pondération des variables et moyennage de plusieurs classifieurs naïf de Bayes dans le sens où, à la fin de l\u0027apprentissage, les deux processus produisent des modèles similaires. Ces modèles se différencient du classifieur naïf de Bayes par l\u0027ajout d\u0027une pondération sur chaque variable. Ces poids peuvent être optimisés directement comme cela a déjà été réalisé dans (Guigourès et Boullé, 2011) mais de manière hors-ligne.\nLe présent article présente une nouvelle méthode pour estimer incrémentalement les poids d\u0027un classifieur Naïf de Bayes Pondéré (NBP) dans le cadre des flux de données. Cette méthode utilise un modèle graphique proche d\u0027un réseau de neurones artificiels. Le plan de cet article est le suivant : notre modèle graphique ainsi que la méthode permettant d\u0027apprendre les poids à attribuer aux variables explicatives sont présentés au cours de la section 2. La section 3 décrit comment les estimations des probabilités conditionnelles à la classe (P (X i |C)), utilisées en entrée du modèle graphique, sont estimées. La section 4 présente une étude expérimentale de notre classifieur Naïf de Bayes Pondéré (NBP) entraîné incrémentalement sur les bases de données ayant servies au \"large scale learning challenge\". Enfin, la dernière section conclut cet article.\n2 Classifieur Naïf Bayésien Pondéré incrémental 2.1 Introduction : le classifieur Naïf de Bayes (NB) et le classifieur Naïf de Bayes Moyenné (NBM)\nLe classifieur Bayésien naïf est une méthode d\u0027apprentissage supervisé qui repose sur une hypothèse simplificatrice forte : les variables X i sont indépendantes conditionnellement à la classe à prédire. Cette hypothèse naïve ne permet pas de modéliser les interactions entre différentes variables. Cependant, sur de nombreux problèmes réels, cette limitation n\u0027a que peu d\u0027impact (Hand et Yu, 2001;Langley et al., 1992). L\u0027idée de départ de ce classifieur vient de la formule de Bayes : P (C|X) \u003d\n. La probabilité conditionnelle jointe P (X|C) étant difficilement estimable on utilise la version naïve (appelée par la suite NB) de ce classi-fieur. La probabilité de la classe devient dans ce cas :\noù j est l\u0027indice de la classe (j ? {1, ..., K}), i l\u0027indice de la variable explicative et k une classe d\u0027intérêt. La classe prédite est celle qui maximise la probabilité conditionnelle P (C k |X). Les probabilités P (X i |C k ) peuvent être estimées par intervalle à l\u0027aide d\u0027une discrétisation pour les variables numériques. Pour les variables catégorielles, cette estimation peut se faire directement si la variable prend peu de valeurs différentes ou après un groupage dans le cas contraire. Le dénominateur de l\u0027équation 1 normalise le résultat tel que k P (C k |X) \u003d 1. Un des avantages de ce classifieur dans le contexte des flux de données réside en sa faible complexité en déploiement, complexité qui ne dépend que du nombre de variables utilisées. L\u0027état de l\u0027art montre toutefois que le classifieur bayésien naïf peut être amélioré de deux manières : (i) en sélectionnant les variables (Koller et Sahami, 1996;Langley et Sage, 1994) ; (ii) en pondérant les variables (Hoeting et al., 1999) ce qui est proche d\u0027un moyennage de modèles bayésiens (BMA \u003d Bayesian Model Averaging) (Hoeting et al., 1999) ; ces deux processus, sélection-pondération, pouvant être mélangés de manière itérative. Le classifieur bayésien naïf moyenné résultant est similaire au classifieur bayésien naïf mais ajoute une pondération par variable tel que :\noù chaque variable explicative i est pondérée par un poids w i dans l\u0027intervalle [0, 1]). L\u0027approche revient à un moyennage de modèles et en possède les qualités. Le moyennage de modèles vise à combiner la prédiction d\u0027un ensemble de classifieurs de façon à améliorer les performances prédictives. Ce principe a été appliqué avec succès dans le cas du bagging (Breiman, 1996) qui exploite un ensemble de classifieurs appris sur une partie des exemples. Dans ces approches, le classifieur moyenné résultant procède par vote des classifieurs élé-mentaires pour effectuer sa prédiction. A l\u0027opposé des approches de type bagging, où chaque classifieur élémentaire se voit attribuer le même poids, le moyennage de modèles bayésiens (BMA \u003d Bayesian Model Averaging) (Hoeting et al., 1999) pondère les classifieurs selon leur probabilité a posteriori.\nL\u0027approche proposée : Naïf Bayésien Pondéré incrémental (NBP)\nLorsque l\u0027on se place dans le cadre de l\u0027apprentissage hors-ligne, les poids du classifieur Naïf de Bayes Moyenné peuvent être estimés de différentes manières : (i) par moyennage de modèles (Hoeting et al., 1999) ; (ii) par moyennage de modèles avec optimisation des poids basée sur un critère MDL (Minimum Description Length) (Boullé, 2006b) ; (iii) par optimisation directe des poids par une descente de gradient (Guigourès et Boullé, 2011). Toutefois toutes ces méthodes fonctionnent en chargeant toutes les données en mémoire et nécessitent de les relire plusieurs fois. L\u0027approche proposée dans cet article optimise directement les poids du classifieur et est capable de fonctionner sur flux de données.\nFIG. 1 -Modèle graphique pour l\u0027optimisation des poids du classifieur bayésien moyenné.\nLa première étape consiste à créer un modèle graphique (voir Figure 1)  (Whittaker, 1990) dédié à l\u0027optimisation des poids. Il permet de réécrire l\u0027équation 2 sous la forme d\u0027un modèle graphique où le classifieur bayésien naïf pondéré reçoit un poids par variable et par classe tel que présenté dans l\u0027équation 3. Les poids que nous cherchons à optimiser sont donc plus nombreux dans ce modèle graphique. En effet le poids n\u0027est plus seulement associé à la variable, mais à la variable conditionnellement à la classe, soit : (i) w ik le poids associé à la variable i et à la classe k et (ii) b k le biais lié à la classe k. Ce biais correspond à l\u0027estimation de la probabilité P (C) et peut donc varier au cours du temps.\nLa première couche du modèle graphique est une couche linéaire réalisant une somme pondérée H k pour chaque classe k , tel que\nLa seconde couche du modèle graphique est un Sof tmax tel que :\n. Finalement le modèle graphique proposé, dans le cas où les entrées sont les logs des estimation conditionnelles aux classes (log(p(X i |C k ), ?i, k), donne en sortie les valeurs P k (?k) telles que :\nc\u0027est à dire des P k \u003d P (C k |X). Les variables positionnées en entrée de ce modèle sont issues des résumés univariés construits sur le flux. Ceux-ci seront présentés dans la section suivante (section 3).\nL\u0027optimisation des poids est réalisée à l\u0027aide d\u0027une descente de gradient stochastique pour une fonction de coût donnée. Pour un exemple donné X, la règle de modification des poids est :\noù coût X est la fonction de coût appliquée à l\u0027exemple X et ?coût X ?wij la dérivée de la fonction de coût vis à vis des paramètres du modèle, ici les poids w ij . Le calcul de cette dérivée (détaillé dans l\u0027annexe de cet article) aboutit à :\noù T k désigne les valeurs de probabilité désirées (target) et P k les sorties obtenues. Il ne reste ensuite plus qu\u0027à inclure la partie couche linéaire de notre modèle graphique pour avoir les dérivées partielles ?coût ?w ik . La modification des poids a donc une complexité calculatoire très faible.\nLa méthode de descente de gradient en-ligne utilisée dans cet article est celle utilisée habituellement pour réaliser une rétropropagation et 3 principaux paramètres (Lecun et al., 1998) sont à prendre en compte : (i) la fonction de coût ; (ii) le nombre d\u0027itérations ; (iii) le pas d\u0027apprentissage.\nDans le cadre de la classification supervisée le meilleur choix pour la fonction de coût, du fait que les sorties du modèle graphique à apprendre prennent uniquement deux valeurs {0, 1}, est le log vraisemblance (Bishop, 1995), qui optimise log(P (C k |X)). Le nombre d\u0027ité-rations est dans notre cas égal à 1 du fait que le modèle est mis à jour après chaque exemple et seulement une fois. Etant donné que l\u0027apprentissage est réalisé sur un flux de données, nous choisissons de n\u0027effectuer qu\u0027une itération par exemple du flux, de ne pas utiliser ni d\u0027early stopping (Prechelt, 1997) ni d\u0027ensembles de validation (Amari et al., 1997). Finalement le seul paramètre à ajuster est le pas d\u0027apprentissage. Une valeur trop faible aboutit à une convergence longue pour atteindre le minimum de la fonction de coût, alors qu\u0027un pas trop grand ne permet pas d\u0027atteindre ce minimum. Dans le cas d\u0027un apprentissage hors-ligne il est possible de régler sa valeur par une méthode de validation croisée mais dans le cas de l\u0027apprentissage sur flux, ou en une passe, ceci n\u0027est pas envisageable. Pour les expérimentations de cet article un pas fixe (? \u003d 10 ?4 ) a été choisi. Cependant si la présence de dérive de concept est suspectée il peut être intéressant d\u0027avoir un pas d\u0027apprentissage adaptatif (Kuncheva et Plumpton, 2008).\nEstimation des densités conditionnelles\nCette section présente comment sont estimées les probabilités conditionnelles P (X i |C k ) qui doivent être placées à l\u0027entrée du modèle graphique présenté au cours de la section précé-dente. Les méthodes d\u0027estimation sont présentées ci-dessous brièvement n\u0027étant pas la contribution majeure de cet article.\nPour nos expérimentations trois estimations sont utilisées (voir Figure 2) pour calculer P (X i |C k ) pour chaque variable numérique explicative i et pour chaque classe k : (i) une méthode de discrétisation à deux niveaux basée sur des statistiques d\u0027ordre tel que décrite dans  (ii) une méthode de discrétisation à deux niveaux « cPiD » qui est une version modifiée de la méthode PiD (Gama et Pinto, 2006) (iii) une approximation gaussienne. L\u0027approche peut être la même dans le cas des variables catégorielles (non détaillée dans cet article) en mettant dans le premier niveau l\u0027approche count-min sketch (Cormode et Muthukrishnan, 2005) et dans le deuxième niveau la méthode de groupage MODL. Le lecteur intéressé pourra trouver un état de l\u0027art et davantage de détails sur les techniques d\u0027estimation de densités conditionnelles dans le chapitre 3 de (Salperwyck, 2012). Dans cet article le nombre de tuples utilisé est égal à 100 correspondant donc à une estimation des centiles. Le réglage de la valeur du nombre de tuples est discuté dans (Salperwyck, 2012). cPid et GkClass, décrits ci-dessous, sont deux méthodes permettant d\u0027obtenir les quantiles.\n3.1 cPid (Gama et Pinto, 2006) ont proposé une méthode de discrétisation à deux niveaux pour une variable numérique. Le premier niveau est un mélange entre les méthodes « Equal Width » et « Equal Frequency » (détaillé dans (Gama et Pinto, 2006) p. 663). Le premier niveau est actualisé de manière incrémentale et nécessite d\u0027avoir plus d\u0027intervalles que le second niveau. Le second niveau utilise l\u0027information contenue dans le premier niveau pour construire une deuxième discrétisation. De nombreuses méthodes peuvent être utilisées pour le second niveau : Equal Width, Equal Frequency, Entropy, Kmoyenne... L\u0027avantage de PiD est d\u0027avoir un premier niveau très rapide et purement incrémental. Dans cPiD nous apportons une modification afin d\u0027avoir une mémoire constante. L\u0027augmentation de la consommation mémoire de la méthode PiD est due à la création de nouveaux intervalles. En effet si un intervalle devient trop peuplé alors il est divisé en deux intervalles contenant chacun la moitié des individus. Notre modification consiste, suite à la division d\u0027un intervalle, à fusionner les deux intervalles consécutifs dont la somme des comptes est la plus faible. Ainsi le nombre d\u0027intervalles stockés reste toujours le même. Aucune comparaison n\u0027est réalisée dans cet article entre PiD et cPiD car notre intérêt se porte sur une utilisation à mémoire constante des méthodes.\nGKClass\nCet algorithme proposé dans (Greenwald et Khanna, 2001) est un algorithme destiné à calculer les quantiles en utilisant une mémoire de O( 1 log( )) dans le pire cas, avec N le nombre d\u0027éléments observés et l\u0027erreur souhaitée. Cette méthode ne requiert pas de connaitre au préalable la taille N du flux et est insensible à l\u0027ordre d\u0027arrivée des exemples. Un des avantages de cette méthode est que, selon le besoin, on peut soit définir l\u0027erreur maximale souhaitée, soit la mémoire maximale à utiliser. Dans le première cas l\u0027erreur maximale est fixée et le résumé consomme autant de mémoire que nécessaire pour que l\u0027erreur maximale ne soit pas dépassée. Dans le deuxième cas on fixe une quantité de mémoire maximale et on l\u0027utilise au mieux pour minimiser l\u0027erreur. Nous avons adapté cet algorithme pour qu\u0027il stocke directement les comptes par classe. Le second niveau utilise la méthode de discrétisation supervisée MODL (Boullé, 2006a).\nApproximation Gaussienne (AG)\nCette méthode suppose que la distribution des données se rapproche d\u0027une loi normale que l\u0027on va chercher à approximer. Pour cela il suffit de ne conserver que les trois valeurs par classe : la moyenne µ, l\u0027écart type (ou la variance ?) et le nombre n d\u0027éléments qui définissent cette gaussienne. Le maintien de ces trois valeurs peut se faire de manière incrémentale et donc cette méthode est parfaitement adaptée à une utilisation en-ligne ou sur les flux et ne comporte qu\u0027un seul niveau. Elle sera utilisée comme référence dans le cadre de nos expérimentations du fait que les bases de données du « Large Scale Learning » ont été générées à l\u0027aide de générateurs gaussiens. L\u0027indicateur d\u0027évaluation retenu est la précision (« accuracy ») c\u0027est à dire T P +T N T P +T N +F P +F N où T P , T N , F P et F N sont respectivement le nombre de vrai-positifs, vrai-négatifs, faux-positifs et faux-négatifs.\nExpérimentations\nProtocole\nLes expérimentations pour ce classifieur sont réalisées sur les bases du challenge Large Scale Learning 1 2 , proposé par le réseau d\u0027excellence PASCAL. Toutes ces bases contiennent 500 000 exemples étiquetés, ce qui peut est considéré comme suffisant pour évaluer un algorithme en-ligne. Nous utilisons les bases alpha, beta, delta et gamma, qui possèdent 500 variables numériques et les bases epsilon et zeta, qui en contiennent 2000. Les jeux de données sont séparés en ensemble de test/apprentissage. Les 100 000 premiers exemples sont pris comme ensemble de test et les autres comme ensemble d\u0027apprentissage.\n1. http://largescale.ml.tu-berlin.de/about/ 2. http://jmlr.csail.mit.edu/papers/topic/large_scale_learning.html\nRésultats\nPour la première partie des expérimentations, un classifieur Naïf de Bayes (NB) sans pondération est utilisé. Il utilise les estimations de densités conditionnelles aux classes issues des méthodes décrites dans la section précédente. Les résultats sont présentés dans le tableau 1 et montrent que l\u0027estimation des probabilités conditionnelles est précise pour les trois méthodes du fait que le classifieur naïf de Bayes obtient de bons résultats avec chacune d\u0027entre elles. Malgré le fait que les données du challenge aient été générées à l\u0027aide d\u0027un générateur gaussien les deux autres méthodes, cPid et GKClass, obtiennent des résultats similaires à la méthode basée sur l\u0027approximation Gaussienne (AG). Entre les deux méthodes cPid et GKClass, le résumé GKClass, apporte des garanties précision / mémoire utilisée, a des résultats comparable à cPid et ne fait pas d\u0027hypothèse sur la nature de la distribution des données. De ce fait il a été choisi pour la suite des expérimentations.\nAlpha\nBeta Delta # examples 40 000 100 000 380 000 40 000 100 000 380 000 40 000 100 000 380 000 TAB. 1 -Précision du classifieur naïf de Bayes sans pondération utilisant GKClass, cPiD et l\u0027approximation gaussienne pour calculer les probabilités conditionnelles.\nGrâce aux résultats présentés dans le tableau 1 nous savons que l\u0027estimation des densités conditionnelles est précise. Par conséquent nous pouvons à présent évaluer le comportement de notre classifieur Naïf de Bayes Pondéré (NBP). Les résultats comparent quatre classifieurs :\n1. un classifieur Naïf de Bayes (NB) entraîné hors-ligne et utilisant la discrétisation MODL (Boullé, 2006a) et toutes les données chargées en mémoire ;\n2. un classifieur Naïf de Bayes Moyenné (NBM) entraîné hors-ligne et utilisant la discréti-sation MODL (Boullé, 2006a) et l\u0027algorithme décrit dans (Boullé, 2006b) pour calculer les poids des variables explicatives -cette méthode est l\u0027une des meilleures de l\u0027état de l\u0027art (Guyon et al., 2009) ; 3. un classifieur Naïf de Bayes (NB) entraîné en-ligne et utilisant la méthode de discrétisa-tion à deux niveaux qui utilise GKClass au niveau 1 et la discrétisation MODL au niveau 2 ;\n4. notre classifieur Naïf de Bayes Pondéré (NBP) entraîné en-ligne et dont les poids sont estimés à l\u0027aide de notre méthode basée sur un modèle graphique. La méthode de discrétisation utilise GKClass comme niveau 1 et la discrétisation MODL comme niveau 2.\nLa \nConclusion\nLes résultats de notre version en-ligne du classifieur naïf de Bayes sont prometteurs. Ses performances sont meilleures que celles de la version en-ligne non pondérées et proche de la version pondérée hors-ligne. Cependant nos résultats pourraient encore être améliorés dans de prochains travaux. Notre première piste d\u0027amélioration serait d\u0027utiliser les résumés GKClass comme des « mini-batch » (Cotter et al., 2011) et de réaliser plusieurs itérations pour accélérer la descente de gradient. Notre seconde proposition serait d\u0027avoir un pas adaptatif pour la descente de gradient : rapide au début de l\u0027apprentissage puis plus lent par la suite, ou de prendre en compte le taux d\u0027erreur comme dans (Kuncheva et Plumpton, 2008).\nLe pas d\u0027apprentissage pourrait aussi être contrôlé par une méthode de détection de changement de concept afin de ré-augmenter le pas dès qu\u0027une détection a lieu et donc de ré-apprendre plus rapidement. Il faudrait de plus mettre à jour les résumés suite à la détection afin d\u0027avoir des estimations correspondant au nouveau concept. De nombreuses méthodes de détection existent mais afin de rester cohérent avec notre approche la grille MODL  pourrait être utilisée pour détecter les changements de distribution.\nAnnexe -Calcul de la dérivée de la fonction de coût\nCette annexe explicite le calcul de la dérivée de la fonction de coût pour le classifieur bayésien moyenné avec optimisation des poids par descente de gradient.\nLe modèle graphique permet d\u0027avoir directement en sortie la valeur des P (C k |X). Le but étant de maximiser la vraisemblance il suffit alors de minimiser le log vraisemblance. On décompose tout d\u0027abord la partie softmax en considérant que chaque sortie de la fonction sof tmax, avant la phase de normalisation, peut être vue comme étant la succession de deux étapes : une phase d\u0027activation suivi d\u0027une fonction recevant la valeur de l\u0027activation. Ici la fonction d\u0027activation peut être vue comme étant O k \u003d f (H k ) \u003d exp(H k ) et la sortie de la partie softmax de notre modèle graphique est :\nLa fonction de coût étant le log vraisemblance, il faut considérer deux cas : (i) soit on désire apprendre pour la valeur 1 ; (ii) soit on désire apprendre la valeur 0. On pose pour la suite :\nDans le cas où l\u0027on désire obtenir la valeur 1 en remplaçant (6) dans (7) :\nD\u0027où on obtient finalement :\nDans le cas où l\u0027on désire obtenir la valeur 0 l\u0027effet de l\u0027erreur est uniquement transmis par la normalisation issue de la fonction sof tmax (la dérivée de la fonction d\u0027erreur vis-à-vis d\u0027une unité de sortie pour laquelle la sortie désirée est 0 est nulle). On obtient à l\u0027aide de calculs similaires :\nOn conclut alors que :\noù T k désigne les valeurs de probabilité désirées (target) et P k la probabilité estimée par le modèle graphique.\nIl ne reste ensuite plus qu\u0027à inclure la partie couche linéaire de notre modèle graphique pour avoir les dérivées partielles ?Coût ?w ik .\nSummary\nA naive Bayes classifier is a simple probabilistic classifier based on applying Bayes\u0027 theorem with naive independence assumption. The explanatory variables (X i ) are assumed to be independent from the target variable (C). Despite this strong assumption this classifier has proved to be very effective on many real applications and is often used on data stream for supervised classification. The naive Bayes classifier simply relies on the estimation of the univariate conditional probabilities P (X i |C). This estimation can be provided on a data stream using a \"supervised quantiles summary\". The literature shows that the naive Bayes classifier can be improved (i) using a variable selection method (ii) weighting the explanatory variables. Most of these methods are related to off-line learning and need to store all the data in memory and/or require reading more than once each example. Therefore they cannot be used on data stream. This paper presents a new method based on a graphical model which computes the weights on the input variables using a stochastic estimation. The method is incremental and produces a Weighted Naive Bayes classifier for data stream. This method will be compared to classical naive Bayes classifier on the Large Scale Learning challenge datasets.\n"
  },
  {
    "id": "310",
    "text": "Introduction\nDe nombreux services pour la télévision requièrent une segmentation et un étiquetage corrects du flux (corpus thématiques issus d\u0027archives, TV à la demande...). Il faut ainsi disposer d\u0027un guide de programme complet, documentant aussi les inter-programmes, et précis à l\u0027image près. Un tel guide est malheureusement rarement disponible auprès des chaînes. Calculer ce guide de programme est le but de la structuration automatique des flux TV. Plusieurs approches ont été présentées dans la littérature pour ce faire. Qu\u0027elles exploitent des méta-données (Poli, 2008) ou des indices audio et vidéo (Naturel et Gros, 2008;Manson et Berrani, 2010;Ibrahim et Gros, 2011), toutes reposent sur une étape de classification supervisée nécessitant des connaissances a priori mais requièrent trop d\u0027annotation manuelle pour être facilement utilisables en pratique. Par ailleurs, l\u0027utilisateur doit définir les classes pertinentes pour un flux.\nDans cet article, nous proposons de réduire drastiquement l\u0027intervention a priori de l\u0027utilisateur en passant à une classification non supervisée. Le rôle résiduel de l\u0027utilisateur serait alors d\u0027étiqueter les classes qui émergent ainsi des données, plusieurs classes pouvant bien entendu partager la même étiquette. À l\u0027image du célèbre K-MEANS, les techniques de clustering reposent sur une représentation simple des données, et une notion de distance entre ces repré-sentations est également fournie par l\u0027utilisateur (Jain, 2010). Ce sont ces deux points que nous cherchons à éviter. Depuis quelques années, certains travaux ont tenté de mettre à profit les capacités discrimantes des techniques d\u0027apprentissage supervisé dans un cadre non-supervisé.\nLeur principe commun est de déduire une similarité à partir de classifications identiques répé-tées sur des problèmes d\u0027apprentissage factices (Shi et Horvath, 2005;Claveau et Ncibi, 2013). Elles permettent de ne pas avoir à expliciter la notion de distance entre données, mais reposent toujours sur une représentation classique des données sous forme attributs-valeurs qui n\u0027est pas adaptée à la nature multi-relationnelle de nos données. Dans la veine de ces derniers travaux, nous proposons donc une technique de clustering capable de manipuler ces données. Plus précisément, nous détournons la programmation logique inductive (PLI) -technique d\u0027apprentissage supervisé dont l\u0027expressivité permet de représenter naturellement ce type de données -pour fonctionner dans ce cadre non supervisé.\nProgrammation logique inductive et données relationnelles\nQue ce soit dans un cadre supervisé ou non supervisé, il est usuel de décrire les données à manipuler sous une forme propositionnelle, dite attribut-valeur, ou encore vectorielle. Les objets doivent alors tous avoir le même nombre d\u0027attributs, et les attributs sont considérés indépendamment (les relations ne sont pas exploitées). Dans notre cas, les objets à manipuler sont des segments correspondant à des programmes ou des inter-programmes, dont beaucoup (e.g. les publicités) sont répétés plusieurs fois (appelées ci-après occurrences).  ton et De Raedt, 1994, pour une présentation détaillée). La PLI est fondée sur la logique des prédicats : les données d\u0027apprentissage sont décrites en Prolog et le classifieur est un ensemble de clauses de Horn. C\u0027est cette expressivité qui permet de décrire les problèmes multi-relationnels. La figure 1 montre un bref extrait de description d\u0027une émission dans B en Prolog standard. On y voit la façon simple de décrire les relations entre les différentes occurrences grâce aux prédicats binaires next_occ/2 et next_in_stream/2. Dans B sont aussi données les définitions de prédicats pouvant être utiles pour inférer les règles de H. Dans l\u0027extrait précé-dent se trouvent les définitions du prédicat prev_occ/2, qui met en relation deux occurrences de la même émissions diffusées l\u0027une après l\u0027autre, et du prédicat interval/3 qui indique l\u0027intervalle de temps entre deux occurrences de deux émissions.\nLe  3 Du supervisé au non supervisé 3.1 Principes L\u0027idée principale de notre approche est de déduire une distance (ou une similarité) à partir de classifications répétées de deux émissions pour des tâches d\u0027apprentissage aléatoire en PLI. Les émissions couvertes souvent par les mêmes clauses inférées seront supposées proches. L\u0027algorithme 1 donne un aperçu global de la démarche. Comme pour le bagging (Breiman, 1996), la classification est répétée un grand nombre de fois en faisant varier les différents paramètres d\u0027apprentissage : les exemples (étape 3 qui divise les données en exemples positifs E + train et en un ensemble E OoB dit out-of-bag utilisé ensuite), les contre-exemples (étape 4), le langage d\u0027hypothèse (étape 5). À chaque itération, un compte des paires d\u0027émissions (x i , x j ) couvertes par les mêmes clauses (on parle de co-couvertures) est tenu à jour dans la matrice M co-couv en tenant compte des couvertures (une règle très discriminante \"rapporte plus\"). La dernière étape relève simplement de l\u0027emploi d\u0027une technique de clustering opérant sur cette matrice de co-couvertures, considérées comme des mesures de similarité. Dans nos expérimen-tations présentées dans la section suivante, nous utilisons le Markov Clustering (van Dongen, 2000). L\u0027avantage par rapport au K-MEANS/K-MEDOIDS est de ne pas nécessiter de fixer a priori le nombre de clusters attendus, et d\u0027éviter le problème de l\u0027initialisation de ces clusters. Inférence :\nfor all clause h l parmi H do 8:\nfor all paire e i , e j de E OoB telle que B, h e i , e j do 9:\nLa stratégie au coeur de cette approche est donc de varier les biais d\u0027apprentissage à chaque itération. Le premier de ces biais est bien sûr l\u0027ensemble d\u0027exemples utilisés. Pour nos expériences, nous utilisons un dixième des exemples positifs tirés aléatoirement à chaque ité-ration. C\u0027est sur les 90 % restants que sont appliquées les règles inférées pour trouver les co-couvertures. La génération des exemples négatifs est un point important de l\u0027algorithme. Il s\u0027agit dans notre cas d\u0027inventer des émissions, avec leurs différentes occurrences et leurs caractéristiques. Ces exemples doivent être suffisamment réalistes pour produire des tâches d\u0027apprentissage dont la difficulté assure la pertinence des règles trouvées et donc des co-couvertures produites. Pour générer ces contre-exemples, nous copions des parties de descriptions d\u0027émis-sions piochées aléatoirement dans B. Le format des règles autorisées, c\u0027est-à-dire le langage d\u0027hypothèse L H est lui aussi différent à chaque tour. En pratique, tous les modes des pré-dicats possibles sont décrits à l\u0027initialisation de l\u0027algorithme, et un sous-ensemble (un tiers) est ensuite choisi aléatoirement à chaque itération. Ces apprentissages sur des tâches supervisées factices conférent, par leur variété, des propriétés importantes à la similarité obtenue. Celle-ci mélange ainsi naturellement des descriptions complexes, opère par construction une sélection de caractéristiques, prend en compte les redondances des descripteurs, ignore ceux de mauvaise qualité, et elle est robuste aux données aberrantes.\nValidation expérimentale\nÉvaluer une tâche de découverte de connaissances comme le clustering est toujours délicat, puisqu\u0027elle suppose l\u0027existence d\u0027une vérité terrain dont on souhaite se passer. Les données que nous utilisons pour nos expériences celles de Naturel et Gros (2008)  Nous présentons en figure 3 les résultats du clustering relationnel après 1 000 itérations ainsi que plusieurs baselines fondés sur une représentation classique des données, c\u0027est-à-dire sous forme attribut-valeur. Les attributs utilisés pour décrire une émission sont les suivants : nombre d\u0027occurrences, durée moyenne, intervalle minimal entre deux occurrences, intervalle maximal, intervalle moyen, nombre maximal d\u0027occurrence sur une plage de 24h, durée entre la première et la dernière occurrence, présence ou non de toutes les occurrences dans la même journée et nombre moyen d\u0027émissions uniques apparaissant avant ou après les occurrences. Les algorithmes baseline sont le K-MEANS, EM, CobWeb, tels qu\u0027implémentés dans WEKA (Hall et al., 2009) ; pour chacun, nous ne rapportons que les résultats des meilleures configurations en terme d\u0027ARI. Nous indiquons aussi les résultats de notre système avec cette description attribut-valeur (i.e. sans les prédicats relationnels de L H ). Quelle que soit la mesure d\u0027éva-luation, notre technique de clustering offre de bien meilleurs résultats que les autres ; l\u0027apport de la représentation apparaît clairement. Les clusters obtenus sont néanmoins différents en nombre et en contenu des classes attendues en vérité terrain. Une analyse des différences montre que la classe bande-annonce est difficile à capturer (elle est distribuée sur plusieurs clusters) ; d\u0027autres cas problématiques sont causées par des émissions aux bornes de notre corpus ou pour lesquelles les trois semaines ne sont pas suffisantes pour identifier les schémas de récurrences.\nL\u0027examen des règles inférées à chaque itération permet aussi une validation indirecte de notre approche puisqu\u0027elles exploitent bien l\u0027aspect multi-relationnel de nos données. C\u0027est le cas de la règle suivante qui couvre ainsi les émissions diffusées à intervalle régulier : \nConclusions\nLa méthode de clustering que nous avons proposé nous permet bien de tirer au mieux profit de la nature particulières de nos données. Elle offre ainsi un moyen d\u0027obtenir une notion de distance même dans des espaces de description riches et non métriques. Bien sûr, bien qu\u0027il\n"
  },
  {
    "id": "311",
    "text": "Introduction\nLes données contenant une information temporelle constituent un défi pour le processus de découverte de connaissances (Yang et Wu, 2006). Les données temporelles sont complexes dans le sens où un objet de la base est décrit par une ou plusieurs séquences d\u0027éléments ordonnés dans le temps. Selon la nature des éléments temporels (catégoriels ou numériques, ponctuels ou continus dans le temps), il existe une grande diversité de méthodes d\u0027extraction de connaissances (Mörchen, 2007). Ici, nous nous intéressons aux données de séquences d\u0027évènements catégoriels et ponctuels, où chaque évènement d\u0027une séquence est associé à un temps t, et que nous appelons simplement séquences d\u0027évènements temporels. La fouille de séquences d\u0027évènements temporels trouve des applications dans de nombreux domaines : e.g., dans le domaine médical, Patnaik et al. (2011) explore des bases de dossiers médicaux électro-niques de patients à la recherche de motifs d\u0027évènements temporels fréquents ; dans le domaine du Web, Masseglia et al. (2008) et Saleh et Masseglia (2011) extraient des comportements fré-quents d\u0027utilisateurs par période de temps ; en sciences sociales, Studer et al. (2010) cherche à grouper des individus selon leur parcours de vie. La majeure partie des efforts de recherche s\u0027est focalisée sur l\u0027extraction de motifs fréquents dans les données de séquences d\u0027évène-ments temporels (ou TAS pour \"Temporally-Annotated Sequences\", voir e.g., (Giannotti et al., 2006)). Dans cet article, nous nous intéressons au problème de clustering de séquences : le but est de créer des groupes de séquences qui partagent des caractéristiques similaires. Dans la plupart des méthodes de l\u0027état de l\u0027art, il est nécessaire de définir une mesure de (dis)similarité entre séquences ainsi que le nombre de clusters à trouver et d\u0027autres paramètres : e.g., Studer et al. (2010) utilisent l\u0027approche des k-medoids couplée à une distance basée sur les opérations d\u0027insertion-suppression et de substitution (dont les coûts sont à définir) de transitions dans une séquence. Le paramétrage de telle méthode est souvent complexe et peut dépendre du domaine d\u0027application et de la quantité de données dont on dispose. Le choix d\u0027un clustering trop fin (un grand nombre de clusters) ne garantit pas que les clusters sont statistiquement valides et peut mener au sur-apprentissage alors qu\u0027un clustering grossier (peu de clusters) nous apporte une information peu précise sur la structure sous-jacente des données et nous offre ainsi un ré-sumé trop général des données. De plus, la dimension temporelle des séquences d\u0027évènements temporels est primordiale et doit être prise en compte pour grouper des séquences similaires, i.e. qui suivent la même distribution d\u0027évènements au cours du temps.\nNotre contribution est la suivante. Nous proposons KHC, une méthode de co-clustering de séquences d\u0027évènements temporels basée sur les modèles en grille (Bondu et al., 2013) : le coclustering (Dhillon et al., 2003;Nadif et Govaert, 2010) consiste à partitionner simultanément et de manière cohérente les trois dimensions de la base de séquences d\u0027évènements temporels ; ici, les séquences sont partitionnées en clusters, ainsi que les évènements, et le temps est discrétisé en intervalles. Nous en déduisons une mesure de dissimilarité entre clusters nous permettant d\u0027accéder par classification hiérarchique ascendante à la granularité nécessaire à l\u0027analyse. En section 3, nous validons expérimentalement la méthode sur des données synthé-tiques et réelles et proposons des indicateurs utiles à l\u0027interprétation des clusters révélés par la méthode.\nSéquences temporelles, modèles en grilles et clustering\nContexte et notations. Une séquence s d\u0027évènements temporels de taille k \u003e 0 est un ensemble d\u0027observations ordonnées s i \u003d (t i1 , e i1 ), (t i2 , e i2 ), . . . , (t i k i , e i k i ) , tel que ?j, 1 ? j ? i ki , t j ? R+ et e j ? E avec E un ensemble non-ordonné d\u0027évènements catégoriels. Une base de données de séquences temporelles est simplement un ensemble de séquences temporelles ainsi définies D \u003d {s 1 , . . . , s n }. Nous proposons de représenter un ensemble de séquences temporelles par une base de données à trois variables (ou dimensions) : S pour les identifiants de séquences, T pour la variable temps et E pour la variable évènement. Dans la suite, un objet (s, t, e) de D sera appelé un point de la base. Cette représentation tridimensionnelle des données se prête bien à l\u0027usage des modèles en grilles (Bondu et al., 2013) pour le clustering -plus précisément nous utilisons le cadre de travail MODL (Minimum Optimized Description Length) et la méthode de coclustering KHC 1 déjà instanciée dans le cas des données fonctionnelles (Boullé, 2012). Le but est de partitionner les variables catégorielles (identifiants de séquences et évènements) et de discrétiser la variable numérique \"temps\". Le résultat est une grille tridimensionnelle dont les cellules sont définies par un groupe d\u0027identifiants de séquence, un groupe d\u0027évènements et un intervalle de temps. Le meilleur modèle M * (i.e., la grille optimale) est la grille la plus probable connaissant les données. Pour obtenir le modèle de grille optimal M Bayesien, noté cost. Le critère cost établit un compromis entre la précision et la robustesse du modèle en grille et est défini comme suit : Critère d\u0027évaluation. Un modèle de grille (pour le coclustering de séquences temporelles) est optimal s\u0027il minimise le critère cost :\noù n est le nombre de séquences, a le nombre d\u0027évènements de E, N le nombre total d\u0027évè-nements temporels (i.e., le nombre de points de la base), k S (resp. k E , k T ) le nombre de clusters de séquences (resp. le nombre de clusters d\u0027évènements, le nombre d\u0027intervalles de temps), k \u003d k S k E k T le nombre de cellules de la grille, N i S (resp. N j T , N i E , N i S j T i E ) est le nombre cumulé de points du cluster de séquences i S (resp. dans l\u0027intervalle de temps j T , du cluster d\u0027évènements i E , de la cellule (i S , j T , i E ) de la grille), n i S (resp. n i E ) le nombre de séquences dans le cluster i S (resp. le nombre de valeurs d\u0027évènements dans le cluster i E ), et enfin n S i (resp. n E i ) le nombre de points de la séquence i (resp. le nombre de points ayant pour valeur d\u0027évènement i). Notons que B(n, k S ) est le nombre de divisions de n éléments en k S sous-ensembles et B(a, k E ) est défini de manière similaire. Les deux premières lignes correspondent à la probabilité a priori du modèle et constituent le terme de régularisation du modèle : les modèles complexes (beaucoup de clusters pour les variables catégorielles et/ou beaucoup d\u0027intervalles pour la variable numérique) seront pénalisés. Les deux dernières lignes correspondent à la vraisemblance du modèle : les modèles les plus proches des données seront préférés ; le cas extrême avec un point par cellule aura une vraisemblance maximale, mais une probabilité a priori très faible et donc une valeur de cost très forte. Une grille avec une faible valeur de cost indique une forte probabilité p(M | D) de la grille connaissant les données. En termes de théorie de l\u0027information, le logarithme négatif de probabilités s\u0027interprète comme une longueur de codage. Ainsi, selon le principe MDL (Minimum Description Length), le critère cost peut s\u0027interpréter comme la longueur de codage du 2. L\u0027uniformité se situe à chaque étage de la hiérarchie : le prior n\u0027est donc pas uniforme ; dans ce cas, l\u0027approche MAP n\u0027est pas une simple maximisation de la vraisemblance. modèle de grille plus la longueur de codage des données connaissant le modèle ; et une faible valeur de cost indique aussi une forte compression des données en utilisant le modèle M .\nLe critère cost est optimisé en suivant une stratégie gloutonne ascendante : (i) on part de la grille au grain le plus fin, (ii) on considère toutes les fusions possibles entre groupes de valeurs ou intervalles, et (iii) on réalise la meilleure fusion si le critère cost décroit après fusion. Ce processus est réitéré tant qu\u0027il y a amélioration du critère. La grille obtenue constitue une estimation de la densité jointe des séquences et des dimensions des évènements temporels (i.e., des trois variables S, T et E). Notons que KHC est libre de tout paramètre utilisateur (i.e., nous n\u0027avons pas à choisir le nombre de clusters de séquences ou d\u0027évènements, ni le nombre d\u0027intervalles de temps) ; de plus sa complexité en temps est sub-quadratique : ?(N ? N log N ) où N est le nombre de points de la base -pour les détails complémentaires voir (Boullé, 2012).\nMesure de dissimilarité et simplification de la structure de grille. Bien qu\u0027optimale, la grille générée par KHC peut s\u0027avérer trop fine pour une analyse directe par un utilisateur, e.g., plusieurs dizaines de clusters de séquences peuvent être générés. Nous proposons une méthode de simplification de la grille par fusions successives de clusters ou d\u0027intervalles, en choisissant la fusion qui dégrade le moins la qualité de la grille. Pour ce faire, nous introduisons une mesure de dissimilarité entre deux clusters (ou intervalles) qui caracté-rise l\u0027impact de la fusion sur le critère cost. Soient c .1 et c .2 deux clusters d\u0027une dimension de la grille M (i.e., deux groupes de valeurs d\u0027identifiants de séquences ou d\u0027évènements, ou deux intervalles contigus de temps). Soit M c.1?c.2 le modèle de grille après avoir fusionné c .1 et c .2 . La dissimilarité ?(c .1 , c .2 ) entre deux clusters est définie comme la différence du critère cost après et avant fusion :\nAinsi, si l\u0027on fusionne les clusters qui minimisent ?, nous obtenons la grille sub-optimale M (avec un grain plus grossier, i.e., simplifiée) qui dégrade le moins le critère cost et donc avec une perte d\u0027information minimale par rapport à la grille avant fusion. Le taux d\u0027information de la nouvelle grille M est défini par\noù M ? est le modèle nul, i.e., la grille dont aucune dimension n\u0027est partitionnée. En construisant ainsi une hiérarchie ascendante des clusters, en partant de M * et au pire jusqu\u0027à M ? , l\u0027utilisateur pourra s\u0027arrêter au niveau de grain voulu et nécessaire pour une analyse en contrô-lant le nombre de clusters ou le pourcentage d\u0027information gardée. Notons que les fusions s\u0027effectuent indistinctement sur toutes les dimensions en fonction de ?.\nValidation expérimentale\nDans cette section nous proposons des expériences sur données simulées afin de démontrer l\u0027efficacité de la méthode en termes de pertinence pour retrouver les motifs simulés dans les données ainsi qu\u0027en terme de temps de calcul pour des données allant jusqu\u0027au million de points. Nous rapportons aussi les résultats de la méthode sur un jeu de données réelles. Les expériences sont réalisées sur un PC de bureau cadencé à 3,8GHz avec 2Go de RAM.\nDonnées simulées\nExemple à 2 motifs. Considérons deux motifs M 1 et M 2 définis sur le domaine de valeurs de temps T \u003d [0, 1000] ? R + et l\u0027ensemble d\u0027évènements E \u003d {a, b, c, d, e, f, g, h, i, j, k, l} tels que :\n\u003d]600; 1000] alors e ? E  . Nous calculons la valeur de l\u0027indice de Rand ajusté (ARI) pour chaque grille générée pour évaluer la concordance entre les clusters de séquences trouvés par KHC et les deux motifs sous-jacents. Les résultats sont rapportés dans les figures 1(abc). Nous observons que pour des petits sous-ensembles de données de D, il n\u0027y a pas assez de points pour que KHC découvre de motifs significatifs : aucun cluster de séquences n\u0027est découvert pour N ? 64 (i.e., en moyenne 3 points par séquences). Pour CM \u003d 10 (10 séquences par motifs en figure 1(a)), à partir de N \u003d 128 points (soit en moyenne seulement 6 points par séquence), ARI \u003d 1 et les deux motifs sous-jacents sont découverts. Nous remarquons aussi que pour un niveau de bruit ? ? 0.1, N \u003d 128 points suffisent encore à trouver les deux clusters de séquences, puis plus le bruit augmente, plus le nombre de points nécessaires à la découverte des deux motifs augmente. Enfin, augmenter le nombre de points jusqu\u0027à 2 20 ne provoque pas de sur-apprentissage, la valeur de ARI est stable à 1. Les mêmes observations tiennent lorsque CM \u003d 50 ou CM \u003d 100 ; nous observons aussi que plus il y a de courbes par motifs (i.e. plus CM est grand), plus il faut de points pour découvrir les deux motifs. Temps de calcul. La figure 2 rapporte les temps de calcul des différentes versions de bases de données à 2 motifs pour CM \u003d 10, 50, 100, en fonction du nombre de points. D\u0027une manière générale, on observe que le temps de calcul augmente, comme attendu, avec le nombre de points d\u0027apprentissage, mais aussi avec CM et le niveau de bruit. Retenons aussi que pour la base la plus difficile, i.e., N \u003d 2\n20\n, CM \u003d 100, (soit en moyenne 5200 points par séquence) et ? \u003d 0.5, KHC retrouve les motifs recherchés en moins de 1h30. Ici, le temps de calcul dépend du nombre de valeurs de temps différentes prises dans T (potentiellement 2 20 ), car lors de la discrétisation de T , KHC explore toutes les coupures possibles entre deux valeurs de temps présentes dans les données ; nous avons réalisé les mêmes expériences avec des valeurs de temps entières prises dans T : dans ce cas, pour N \u003d 2 20 , CM \u003d 100 et ? \u003d 0.5, KHC trouve les motifs recherchés en 13 minutes. points, CM \u003d 10 et ? \u003d 0.5. Nous proposons 3 visualisations différentes basées sur la fréquence des cellules, l\u0027information mutuelle et le contraste des cellules -chacune d\u0027entre elles apportant une information différente sur les clusters de séquences découverts. Bien que KHC génère des grilles tridimensionnelles, la dimension S étant partitionnée en deux clusters de séquences, nous proposons des visualisations sur les deux autres dimensions pour chaque cluster de séquences. Nous pré-sentons ces visualisations dans la figure 3. Visualisation de la fréquence. En figures 3M 1 (a) et 3M 2 (a), la plus classique des visualisations consiste à représenter le nombre de points par cellule, i.e. N i S j T i E pour la cellule (i S , j T , i E ). Nous apercevons déjà les cellules les plus fréquentes qui correspondent à la définition des mo-tifs sous-jacents malgré le niveau de bruit ? \u003d 0.5. Visualisation de l\u0027information mutuelle. Pour un cluster de séquences c i S , l\u0027information mutuelle entre les variables T ? M et E ? M issues du partitionnement ? M des variables temps et évènement généré par le modèle de grille M , est défini comme suit :\nAinsi, les M I i1i2 représentent la contribution de la cellule c i1i2 à l\u0027information mutuelle. Fréquence Information mutuelle Contraste\nFIG. 3 -Visualisation de la fréquence, de l\u0027information mutuelle conditionnelle à un cluster de séquences et du contraste pour les 2 clusters trouvés par KHC correspondant aux 2 motifs sousjacents M 1 et M 2 . En abscisses, la discrétisation du temps en 7 intervalles et en ordonnées, la partition des évènements en 4 groupes : E \u003d {a, b, c} ? {d, e, f } ? {g, h, i} ? {j, k, l}.\nVisualisation du contraste. Pour le couple de variables partitionnées à visualiser (T ? M , E ? M ), le contraste entre un contexte, i.e., un cluster de séquences c i S et le reste des clusters de sé-quences {c i } i \u003di S est défini comme suit :\nComme précédemment, le signe des M I i1i2;i S qualifiera le contraste entre c i S et le reste des données (i.e., les clusters de séquences {c i } i \u003di S ). Les valeurs de M I i1i2;i S sont rapportées dans les figures 3M 1 (c) et 3M 2 (c). Prenons le cluster de séquences de c M1 . Les cellules blanches indiquent qu\u0027il n\u0027y a pas de contraste à cet endroit entre c M1 et le reste des données (ici, c M2 ) : par exemple la cellule ([0; 100], {g, h, i}), malgré le bruit, n\u0027est pas caractéristique de c M1 ; en effet, la probabilité du groupe d\u0027évènements {g, h, i}) dans l\u0027intervalle de temps ([0; 100] n\u0027est pas significativement différent selon qu\u0027on se trouve dans c M1 ou c M2 . De même la cellule ([401; 500], {d, e, f }) présente un contraste nul puisqu\u0027elle est commune aux deux motifs sous-jacents. Les cellules rouges indiquent ce qui caractérise c M1 par rapport à c M2 : dans ces cellules, la probabilité de points est bien supérieure pour c M1 que pour c M2 . Les cellules bleues indiquent un contraste négatif : la probabilité de points y est plus faible pour c M1 que pour c M2 .\nDonnées réelles\nA partir de la base de données DBLP (Ley, 2009), nous considérons tous les auteurs qui ont publié des articles parus dans les actes de neuf conférences dont la thématique première est les bases de données et/ou la fouille de données (CIKM, VLDB, SIGMOD, ICDE, ICDM, KDD, SDM, PAKDD, PKDD). Pour chaque auteur, nous considérons l\u0027année de publication et l\u0027évènement lié à la publication (i.e., le nom de la conférence). Nous constituons ainsi une base de données de séquences d\u0027évènements temporels à trois dimensions (auteur, année, évè-nement). Les points de la base sont dupliqués lorsqu\u0027un auteur a publié plusieurs fois dans la même conférence la même année.\nLa base D ainsi constituée est composée de plus \nc j ?X M c j \u003dc où P X M (c) est la probabilité d\u0027avoir un point avec une valeur du cluster c, c \\ v est le cluster c duquel on a retiré la valeur v, c j ? v est le cluster c j auquel on a rajouté la valeur v et M |c \\ v, c j ? v le modèle de grille M qui a subi les modifications précitées. Intuitivement, une valeur v i est représentative d\u0027un cluster c et dite typique, si elle est proche de c et très différente (en moyenne) des autres clusters c j \u003d c. Aussi, pour un cluster d\u0027auteurs, les auteurs qui ont peu publié (1 ou 2 fois, voir figure 5) sont souvent moins typiques que ceux qui sont les plus prolifiques, puisque si on les déplace vers un autre cluster, la différence de cost (dans la formule de la typicité) sera faible. Dans la figure 6, nous présentons pour chacun des quatre clusters d\u0027auteurs (un par ligne), les auteurs les plus typiques (colonne 1), ainsi que la fréquence de publications (colonne 2) et le contraste (colonne 3) par une grille à deux dimensions Année × Conférence. \nConclusion \u0026 discussion\nNous avons proposé une méthode de clustering et d\u0027analyse de séquences temporelles basée sur les modèles en grille. Les identifiants de séquence sont groupés en clusters, ainsi que les évènements, et la dimension temporelle est discrétisée en intervalles -le tout forme ainsi une grille tridimensionnelle (ou tri-clustering). Obtenir la grille optimale (au sens Bayésien) ne nécessite aucun paramétrage utilisateur. Pour exploiter la grille, nous avons proposé (i) une mesure de dissimilarité entre clusters afin de sélectionner le grain de la grille tout en contrôlant la perte d\u0027information, (ii), un critère (la typicité) pour identifier les valeurs les plus représen-tatives d\u0027un cluster, (iii) ainsi que deux critères basés sur l\u0027information mutuelle pour caracté-riser, interpréter et visualiser les clusters trouvés. Nos différentes propositions ont été validées sur des données simulées ainsi que sur des données réelles issues de DBLP. Notons qu\u0027une étude du comportement asymptotique des indicateurs proposés a été réalisée et qu\u0027une étude complète des trajectoires antenne-antenne d\u0027utilisateurs du mobile -que l\u0027on peut voir comme des séquences d\u0027évènements -a été menée à l\u0027échelle d\u0027un pays (voir (Guigourès, 2013)).\n"
  },
  {
    "id": "313",
    "text": "Introduction\nInternet fournit une gigantesque base de connaissances qui permet aujourd\u0027hui d\u0027obtenir des informations sur tous les sujets du monde. C\u0027est par exemple le cas des sites dédiés aux problèmes médicaux, pour lesquels les utilisateurs peuvent aisément soumettre des problèmes de santé et être au centre d\u0027espaces de discussion. Si certains sites se refusent à tout diagnostic en ligne, d\u0027autres en revanche font souvent étalage de situations individuelles difficiles qui conduisent bien souvent à l\u0027énoncé de diagnostics par des non-professionnels, dont certains évoquant les mots \"cancers\" ou \"tumeurs\", ce qui peut avoir un impact psychologique fort sur les visiteurs à la recherche d\u0027informations.\nDans ce travail, nous étudions ce phénomène et montrons que quel que soit le syndrome recherché, les résultats conduisent toujours à l\u0027énoncé des mots \"cancer\" ou \"tumeur\". Nous générons pour cela des syndromes en associant des symptômes à différentes parties du corps et analysons les résultats du moteur de recherche et le réseau de pages web sous-jacent (Watts, 2004). Des travaux proches ont par exemple été menés par M. Godwin sur les forums de type Usenet (Godwin, 1994), pour montrer que plus une discussion s\u0027étend dans le temps et plus la probabilité d\u0027y trouver une comparaison impliquant des analogies extrêmes s\u0027approche de un.\nLa Section 2 détaille notre objectif et la méthodologie suivie. La Section 3 présente les résultats expérimentaux. La Section 4 conclut l\u0027article et présente nos travaux futurs.\nObjectif et méthodologie\nNotre objectif est d\u0027étudier, pour une recherche donnée, la probabilité qu\u0027apparaissent les mots \"cancer\" ou \"tumeur\" en analysant les résultats renvoyés par Google. Deux études sont ainsi menées : (i) La première vise à évaluer le nombre de cliques nécessaires à l\u0027apparition de ces termes si les résultats sont consultés dans leur ordre d\u0027apparition. (ii) La seconde, plus générale, vise à dépasser l\u0027aspect temporel de la navigation, afin d\u0027évaluer la probabilité que ces termes apparaissent quelle que soit la chronologie de consultation des résultats. Nous analysons pour cela l\u0027ensemble du réseau de pages web sous-jacent et évaluons la probabilité de présence des termes sur deux niveaux.\nLa méthodologie que nous avons adoptée est la suivante. Nous générons un ensemble de syndromes en associant 5 symptômes à 10 parties du corps humain :\n-Symptômes : Douleur, Irritation, Tache, Grosseur, Saignement -Parties du corps : Tête, Yeux, Nez, Langue, Ventre, Bras, Main, Sexe, Jambe, Pied 50 requêtes sont ainsi obtenues par combinaisons de symptômes et de parties du corps. Nous effectuons ensuite une recherche automatique de ces syndromes à l\u0027aide du moteur de recherche Google et analysons, selon les approches (i) et (ii), les sites renvoyés. Dans cette première approche du travail, nous nous intéressons uniquement à la présence des termes et ne prenons pas en compte le contexte dans lequel ils apparaissent. On peut en effet supposer que leur seule présence peut avoir un impact psychologique fort chez l\u0027utilisateur qui se sait malade.\nRésultats expérimentaux 3.1 Nombre de cliques\nDans l\u0027hypothèse où l\u0027utilisateur parcourt les résultats les uns après les autres, la question est de savoir combien de cliques seront nécessaires pour parvenir à une page affichant les mots \"cancer\" ou \"tumeur\". En analysant l\u0027ensemble des résultats renvoyés par le moteur de recherche, nous avons identifié le nombre de cliques nécessaires pour atteindre ces termes. La Figure 1 montre la distribution du nombre de cliques obtenus avec les différentes requêtes. \nFIG. 1 -Distribution du nombre de cliques nécessaires pour les différentes requêtes\nNous pouvons observer que la majeure partie des requêtes effectuées conduit à des sites présentant les mots \"cancer\" ou \"tumeur\" en très peu de cliques. En moyenne, quel que soit le syndrome, 2.28 cliques sont nécessaires pour que l\u0027utilisateur soit dirigé vers une page affichant ces termes. Si ce nombre parait très faible, il s\u0027explique par le fait que les sites placés en tête de liste sont souvent des sites à très grandes audiences, affichant énormément d\u0027informations tels que des menus, des forums ou des sujets connexes, augmentant ainsi la probabilité que les mots recherchés y soient présents.\nVers une analyse réseau\nPour aller plus loin dans notre étude, nous nous sommes intéressés dans un second temps à la probabilité qu\u0027apparaissent les mots \"cancer\" ou \"tumeur\" quel que soit l\u0027ordre de consultation des résultats. Pour cela, nous collectons l\u0027ensemble des résultats renvoyés et générons le réseau de pages web sous-jacent (Barabasi, 2002;Borner et al., 2007) sur deux niveaux (résultats directs et page directement liées). La figure 2 montre un exemple de ce réseau.\nFIG. 2 -Exemple du réseau de pages web généré à partir de la recherche \"douleur tête\"\nAinsi, en générant et analysant ce réseau pour chacune des requêtes, nous avons calculé la probabilité de présence des mots \"cancer\" ou \"tumeur\" aux deux niveaux du réseau, c\u0027est-à-dire le pourcentage de pages présentant au moins l\u0027un des termes à chacun des niveaux. La Figure 3 montre la distribution de cette probabilité (a) au niveau 1 et (b) au niveau 2.\nComme attendu, on observe qu\u0027un fort pourcentage de requêtes a donné lieu à des taux de probabilité de présence relativement élevés au premier niveau du réseau (cf Figure 3(a)). La moyenne des résultats obtenus pour ce premier niveau est de 0.47. Ainsi, quels que soient les syndromes recherchés, la probabilité qu\u0027apparaissent les mots \"cancer\" ou \"tumeur\" dès les premiers résultats renvoyés par le moteur de recherche est de 0, 47.\nL\u0027observation la plus intéressante concerne les résultats obtenus pour le niveau 2 du réseau (cf Figure 3(b)). Nous pouvons en effet observer que globalement, la probabilité que soient pré-sents les mots \"cancer\" ou \"tumeur\" ne varie pas radicalement au second niveau. La moyenne des résultats obtenus pour ce second niveau est de 0.39. Quels que soient les syndromes recherchés, la probabilité que soient présents les mots \"cancer\" ou \"tumeur\" dans les pages citées par les résultats du moteur de recherche est de 0.39. Nous pouvons expliquer cette différence par le fait que les pages web font souvent référence à des sites qui ne sont pas nécessairement liés sémantiquement au sujet abordé. Les liens publicitaires, les liens vers les sponsors ou les \nConclusion et travaux futurs\nNous avons montré que quel que soit le syndrome recherché sur Internet, il existe toujours dans le réseau de pages web sous-jacent des pages énonçant les mots \"cancer\" ou \"tumeur\". Dans ce travail préliminaire, nous avons considéré uniquement la présence du mot sans prendre en compte l\u0027environnement dans lequel il intervient. L\u0027étude pourra être complétée à la fois en tenant compte du contexte et en éliminant les cas de faux positifs qui peuvent être déclenchés par de nombreux éléments parasites sur les pages.\n"
  },
  {
    "id": "314",
    "text": "Introduction\nLe clustering permet l\u0027exploration d\u0027ensembles de données en les résumant sous la forme de groupes homogènes plus facilement caractérisables et interprétables. Récemment, de nouveaux algorithmes ont été proposés pour répondre aux problèmes de traitement des grands volumes ou des flux de données (Aggarwal et al., 2003;Cao et al., 2006;Philipp Kranen et Seidl, 2011). Ces méthodes reposent généralement sur des algorithmes qui ne réalisent qu\u0027une seule passe sur les données initiales. Ceux-ci ne sont malheureusement applicables qu\u0027à des données vectorielles, pour lesquelles des structures incrémentales de description des clusters existent (Zhang et al., 1996).\nDans ce travail, nous nous intéresssons au cas général où les données ne sont pas nécessai-rement vectorielles, et où il n\u0027est donc pas possible d\u0027utiliser de telles structures. Une solution consiste à résumer chaque cluster par un sous-ensemble des données qui le compose, possiblement un seul point, appelé médoide, qui est le plus similaire aux autres données du cluster. Le problème est que la détermination des médoides, et donc l\u0027affectation d\u0027un nouveau point aux clusters existants dans un contexte incrémental, possède une complexité quadratique avec le nombre de données. Cela n\u0027étant pas envisageable dans des cas d\u0027usage réels, les algorithmes implémentent généralement des mécanismes d\u0027échantillonnage pour réduire le coût des calculs de l\u0027appartenance à un cluster. Cependant, échantillonner implique l\u0027introduction d\u0027une incertitude dans la représentation du cluster, qui, dans le cas d\u0027un algorithme en une passe, peut se traduire par une erreur d\u0027affectation d\u0027un point à un cluster, et qui est aggravé par le fait que cette donnée peut ensuite être, à tort, utilisée pour représenter le cluster auquel elle appartient.\nDans ce papier, nous proposons une méthode stochastique d\u0027affectation d\u0027un point à un cluster, qui s\u0027inspire des principes des inégalités de concentration en mettant en oeuvre des bornes théoriques qui vont estimer la distance réelle d\u0027un point à chaque cluster et ainsi gérer l\u0027incertitude liée à l\u0027échantillonnage. Nous comparons ici trois bornes théoriques : Bernstein, Hoeffding et Student. Nous proposons également de réduire artificiellement ces bornes à l\u0027aide d\u0027un pourcentage pour en accélérer la convergence, mais au prix d\u0027erreurs plus nombreuses.\nLes résultats expérimentaux sur des jeux de donnés artificiels ou réels issus du répertoire UCI Machine Learning Repository visent ici à évaluer l\u0027accélération du processus de clustering en une passe tout en comptabilisant les erreurs d\u0027affectation par rapport à un algorithme exhaustif, mais ne s\u0027intéressent pas à évaluer la qualité de la partition obtenue en tant que telle par les indices habituels (Rand, erreur de confusion). Nos résultats montrent que les meilleurs performances sont obtenues par la borne de Bernstein qui offre dans tous les cas le meilleur ratio \"nombre de comparaisons entre données par nombre d\u0027erreurs observées\". Les expéri-mentations montrent par ailleurs que la réduction des bornes théoriques permet d\u0027en améliorer les performances en pratique.\nCet article est organisé comme suit : la section 2 présente un état de l\u0027art succinct des principales méthodes d\u0027échantillonnage utilisées pour l\u0027accélération du clustering de données non vectorielles et leurs limites. La section 3 décrit les principes généraux de notre méthode de sélection de cluster et décrit les différentes bornes théoriques ainsi que leurs variantes ré-duites. Ensuite, la section 4 présente les résultats comparatifs expérimentaux entre la méthode exhaustive, qui est considérée comme la vérité terrain, et les approches basées sur les bornes théoriques et réduites sur l\u0027ensemble des bases de tests. Finalement, la section 5 présente les conclusions et les perspectives de ce travail.\nMéthodes d\u0027échantillonnage pour le clustering de données non vectorielles\nDeux approches principales existent pour le traitement de données non vectorielles (Hammer et Hasenfuss, 2007) : les approches basées sur des médoides qui limitent les coordonnées des centres des clusters à des exemples du jeu de données, et les approches basées sur des données relationnelles qui travaillent directement à partir des matrices de distances ou de (dis)similarité. Dans les deux cas, des méthodes d\u0027échantillonnage ont été proposées pour réduire la complexité quadratique de leur résolution.\nAinsi, l\u0027algorithme CLARA (Kaufman et Rousseeuw, 1990) réduit la complexité en échan-tillonnant alétoirement l\u0027ensemble du jeu de données. D\u0027autres méthodes, comme CLARANS (Ester et al., 1995) ou CURE (Guha et al., 1998) limitent la recherche des candidats mé-doides aux voisins des médoides actuels, tout comme la méthode floue Linearized Fuzzy CMedoids (Krishnapuram et al., 1999) qui utilise pour cela les degrés d\u0027appartenance des points aux clusters. D\u0027autres méthodes, comme l\u0027algorithme Leader Ant (Labroche, 2006) remplace la détermination exhaustive du médoide par un nombre fixé réduit de comparaisons aléatoires avec chaque cluster.\nDans (Zhu et al., 2012), les auteurs présentent des mécanismes d\u0027accélération pour les données relationnelles de type approximation de Nyström, visant à réduire la dimension de la matrice de distance, ou de type \"patch processing\", ne considérant qu\u0027un échantillon carré de la matrice de distances à la fois pour traiter de grands volumes de données.\nCependant, pour toutes ces méthodes, le taux d\u0027échantillonnage est un paramètre, qui est non seulement difficile à déterminer a priori car il manque de sens, mais il est également le même tout au long du processus de clustering indépendamment par exemple de la taille des clusters ou de la complexité de leur forme. Pour éviter cela, (Domingos et al., 2001) proposent une variante de k-means, limitée aux données vectorielles, qui utilise une borne de Hoeffding (Maron et Moore, 1994) pour minimiser le nombre de données nécessaires à la détermi-nation des centres de chaque cluster, tout en garantissant que l\u0027erreur commise reste bornée pour un taux d\u0027erreur fixé.\nSimilairement, l\u0027idée de ce papier est de proposer un cadre général pour accélérer les algorithmes de clustering en une passe, dans le cas de données non nécessairement numériques, à l\u0027aide d\u0027un mécanisme de mise en compétition des clusters reposant sur des bornes théoriques, qui va permettre de gérer l\u0027incertitude sur les distances issue de l\u0027échantillonnage.\n3 Accélération des algorithmes de clustering en une passe 3.1 Sélection des clusters dans les algorithmes en une passe Dans le cas particulier des algorithmes de clustering en une seule passe, les données sont considérées séquentiellement et l\u0027affectation à un cluster dépend uniquement de l\u0027estimation de la distance de cette donnée aux clusters existants. Bien sûr, l\u0027ordre des données a une influence directe sur la partition produite par ces méthodes. Généralement, celles-ci sont paramétrées à l\u0027aide d\u0027un seuil de distance qui est utilisé pour décider si une donnée est suffisamment proche des clusters existants pour en intégrer un ou bien si elle doit initier son propre cluster. Lorsque plusieurs (éventuellement tous) les clusters sont éligibles, le problème revient à déterminer le cluster qui optimise le mieux la fonction objectif de l\u0027algorithme de clustering. Ce problème est encore plus compliqué lorsque la distance est estimée à partir d\u0027un échantillon des données de chaque cluster comme dans ce travail.\nNous proposons un mécanisme de sélection des clusters inspiré du mécanisme de compétition ou \"racing\" introduit par (Heidrich-Meisner et Igel, 2009) qui peut s\u0027appliquer au problème de clustering pour adapter automatiquement la taille de l\u0027échantillon nécessaire à l\u0027affectation d\u0027un point à un cluster sur la base d\u0027une erreur maximale tolérée d\u0027affectation. Ce faisant, nous pouvons simultanément accélérer les algorithmes de clustering en une passe, en limitant le nombre de calculs de distances entre une nouvelle donnée et les données déjà classées, et également garantir une borne supérieure sur l\u0027erreur d\u0027affectation par rapport à un algorithme exhaustif qui réaliserait toutes les comparaisons possibles, sous l\u0027hypothèse que toutes les comparaisons sont indépendantes. En pratique, comme le montre nos expérimenta-tions, et bien que l\u0027hypothèse d\u0027indépendance ne soit pas nécessairement vérifiée, cela conduit à réduire drastiquement le nombre de comparaisons nécessaires tout en limitant les erreurs à des niveaux très faibles.\nMéthode statistique de compétition (racing) entre clusters\nLa technique de \"racing\" est un outil qui permet de prendre une décision dans le cas d\u0027une incertitude résultant d\u0027au moins deux variables aléatoires ayant un recoupement partiel de leur intervalle de confiance, étant donné un taux d\u0027erreur fixé. L\u0027idée du racing est que la comparaison entre deux (ou plus) variables aléatoires peut être affinée lorsque plus de réalisations des variables aléatoires sont observées. Avec peu de réalisations, la variance est très large et la plupart des distributions attachées aux variables aléatoires se recoupent. Lorsque plus d\u0027observations sont réalisées, la variance décroît et il existe un moment où les distributions se séparent (exception faite de distributions exactement identiques ou s\u0027il n\u0027y a pas assez de données à échantillonner pour chaque variable aléatoire). Après un certain nombre d\u0027observations, il devient possible de prendre une décision sur la relation des deux variables aléatoires (plus grand / plus petit en fonction de l\u0027objectif du problème) avec un certain niveau de confiance, qui correspond à la borne supérieure de l\u0027erreur qui est tolérée. Sur la base des relations estimées (plus petit / plus grand) les mauvais clusters candidats sont éliminés plus tôt, même si la variance demeure assez grande. Cela conduit à la concentration des efforts de calcul sur les meilleurs candidats (Horvitz et Zilberstein, 2001;Beyer et Sendhoff, 2007a,b).\nPlus formellement, dans notre algorithme de clustering en une seule passe, nous représen-tons la distance estimée entre le point actuel et chaque cluster i par une variable aléatoire X i . Comme déjà indiqué, nous faisons par la suite l\u0027hypothèse naïve que les X i sont indépendantes. La distance entre la donnée et le cluster est supposée comprise entre deux bornes a et b, à partir desquelles il est possible de calculer une étendue (ou \"range\" en anglais) R \u003d |a ? b|. Nous supposons également que nous connaissons un seuil de confiance noté 1 ? p, et où p ? [0, 1] est la probabilité d\u0027erreur. Nous proposons ci-après trois méthodes principales pour estimer les bornes de l\u0027intervalle de confiance associé à chaque variable aléatoire. De plus, nous proposons de modifier l\u0027expression théorique des bornes en introduisant un facteur de réduction r ? [0, 1] pour les rendre plus strictes au besoin. Il est intéressant de noter que si r \u003d 1, on retrouve l\u0027expression exacte des bornes théoriques.\nLa première borne est la borne de Hoeffding (Maron et Moore, 1994) :\noù X i,n représente la moyenne empirique des distances au cluster X i après n comparaisons et est définie comme suit :\n, où E(X i ) désigne la distance réelle au cluster X i , et l\u0027erreur de probabilité p indique les chances que la distance réelle soit hors des bornes.\nLa seconde borne est la borne plus récente de Bernstein (Heidrich-Meisner et Igel, 2009) qui repose sur l\u0027écart-type empirique\nn Bien que la borne de Bernstein soit connue pour être plus stricte que la borne de Hoeffding (Audibert et al., 2007;Mnih et al., 2008) et doit donc conduire à accélérer davantage le processus de compétition entre clusters, nous proposons de comparer le comportement des deux sur nos jeux de test.\nCependant, comme le montre les équations précédentes, une des limitations potentielles des bornes de Hoeffding et Bernstein est que l\u0027étendue R est un paramètre nécessaire au calcul des valeurs de ces bornes. Même si les espaces de description des données sont souvent bornés, ce qui permet de déduire la valeur du paramètre R, dans de nombreux cas il n\u0027est pas possible de connaître cette valeur a priori, ou il est peut-être trop complexe de le calculer exhaustivement, ce qui ferait perdre le gain de notre approche par ailleurs. Pour toutes ces raisons et également pour accélérer les calculs en resserrant la borne en la contraignant plus, nous proposons d\u0027éva-luer également la borne de Student qui est indépendante de l\u0027étendue des données, et qui fait l\u0027hypothèse que les distances aux clusters suivent une loi normale de variance inconnue.\ndésigne l\u0027estimateur non biaisé de la variance de la distance au cluster X i .\nImplémentation de la méthode de compétition\nAlgorithme 1 Algorithme de clustering en une passe avec compétition (X, D, T ) Entrée : X : jeu de données, D : matrice de distances, T : seuil de distance pour la construction d\u0027un nouveau cluster Sortie : P : partition de sortie du jeu de données X 1: initialiser l\u0027ensemble des clusters C \u003d ? 2: Pour Tout x ? X Faire 3:\ndéterminer le meilleur cluster c w pour x en utilisant le mécanisme de compétition de l\u0027algorithme 2 4: affecter x à c w ssi X cw ? T 5: Fin Pour 6: Retourner la partition calculée à partir de C L\u0027algorithme 1 détaille le schéma global de notre méthode de clustering : à chaque itéra-tion, un nouvel objet est considéré et les clusters existants sont mis en compétition par le biais de l\u0027algorithme 2. La compétition permet de filtrer graduellement l\u0027ensemble des clusters candidats, jusqu\u0027à ce qu\u0027il ne reste plus qu\u0027un seul candidat, ou bien qu\u0027il n\u0027y ait plus de données pour affiner la prise de décision. En effet, à chaque fois que la borne inférieure sur la distance empirique à un cluster est plus grande que la borne supérieure du meilleur cluster actuel, il est supprimé de la compétition. À l\u0027opposé, si la borne supérieure d\u0027un cluster est plus petite que la borne inférieure du vainqueur actuel, celui-ci le remplace et devient le nouveau meilleur cluster. Enfin, lorsqu\u0027il n\u0027y a pas de différences singitificatives entre les clusters restants, le vainqueur final de la compétition est celui qui minimise sa distance empirique moyenne avec la donnée. À l\u0027issue de la compétition, l\u0027objet est affecté au cluster vainqueur c w si sa distance X cw est inférieure à un seuil T passé en argument. Dans le cas contraire, la donnée construit un nouveau cluster.\nAlgorithme 2 Algorithme de compétition entre clusters (x, C, D) Entrée : x : objet du jeu de données X, C : ensemble des clusters existants, D : matrice de distances Sortie : c w : indice du cluster qui remporte la compétition 1: initialiser le cluster vainqueur c w \u003d ? 2: Tant Que la compétition n\u0027est pas finie Faire\n3:\nPour Tout clusters c ? C Faire 4:\nselectionner aléatoirement une nouvelle donnée x c dans le cluster c\n5:\nmettre à jour la distance moyenne empirique pour le cluster c avec la distance D(x, x c ) ainsi que les bornes [inf c , sup c ] pour le cluster c\n6:\nSi sup c \u003c sup cw ou c w \u003d\u003d ? Alors\n7:\nmettre à jour le cluster vainqueur c w \u003d c 8:\nFin Si\n9:\nFin Pour 10:\nsupprimer tous les clusters c ? C encore en compétition tels que inf c \u003e sup cw\n11:\nSi |C| \u003c 2 Alors\n12:\nRetourner c w\n13:\nFin Si 14: Fin Tant Que 15: Si la compétition se termine sans différence significative entre les clusters de C Alors\n16:\nRetourner le cluster c ? C qui minimise la distance moyenne empirique 17: Fin Si 4 Résultats expérimentaux Nous présentons ici les résultats comparatifs entre la méthode de détermination exhaustive du cluster le plus proche et notre méthode de compétition basée sur des bornes théoriques. Nous discutons ensuite l\u0027influence des paramètres de la méthode sur son efficacité.\nProtocole expérimental\nLes résultats sont présentés pour différentes valeurs du facteur de réduction r comprises entre 0 et 1 (quand r \u003d 1 on se trouve dans le cas de la borne théorique originale), pour une probabilité d\u0027erreur fixée pour l\u0027ensemble des tests à 0.1 et une valeur du range R exacte. La valeur du seuil qui détermine si un nouveau cluster doit être construit ou non est estimée, similairement à ce qui a été proposé pour l\u0027algorithme Leader Ant (Labroche, 2006), comme la moyenne des distances calculée sur un échantillon aléatoire d\u0027une taille égale à 10% de l\u0027effectif total du jeu de données. Enfin, du fait du calcul de la partition de manière exhaustive, seulement 5 tests ont été réalisés pour chaque jeu de données et chaque valeur du facteur de réduction de borne r.  (Asuncion et Newman, 2007).\nÉvaluation de la qualité des résultats : l\u0027objectif de notre évaluation n\u0027est pas de détermi-ner la qualité de la partition de manière classique (indice de Rand . . .), car, dans notre cas, la référence est donnée par la qualité de la partition de la méthode exhaustive. Nos expérimenta-tions visent donc à montrer l\u0027impact de notre méthode selon deux dimensions principalement : l\u0027accélération par la réduction du nombre de comparaisons et le nombre d\u0027erreurs par rapport à la méthode exhaustive. Nous n\u0027évaluons pas, dans ce travail, les temps de calcul des diffé-rentes méthodes pour mesurer l\u0027accélération, car nous souhaitons nous affranchir des optimisations d\u0027implémentation liées au langage choisi (Java) et du contexte d\u0027exécution (bibliothèques liées).\nDe façon à pouvoir comparer identiquement au cours du temps nos méthodes basées sur les bornes, s\u0027affranchir des éventuelles erreurs précédentes, et également déterminer à quel moment surviennent les erreurs d\u0027affectation par rapport à la méthode exhaustive, nos expé-rimentations sont basées sur une mesure d\u0027erreur avec correction de l\u0027affectation à chaque nouvelle donnée traitée. Ainsi, pour chaque point des jeux de données, notre protocole déter-mine le cluster idéal à l\u0027aide de la méthode exhaustive puis prédit le cluster qui serait choisi pour une affectation avec chacune des bornes. En cas de différence entre le cluster prédit et le cluster idéal, une erreur est comptabilisée pour la borne concernée. Dans tous les cas, le point est affecté au cluster idéal.\nJeux de données : du fait de l\u0027utilisation de la méthode exhaustive qui a une complexité quadratique comme référence pour notre mesure d\u0027erreur, nos tests ont été effectués sur des jeux de données d\u0027une taille intermédiaire, permettant de finir le calcul en un temps raisonnable, tout en donnant une idée du comportement des méthodes sur de grands jeux de données (notamment l\u0027accélération). De plus, de façon à varier les difficultés, les tests ont été conduits d\u0027une part sur des données artificielles générées à l\u0027aide de distributions de données normales avec un recouvrement plus ou moins important entre les groupes, et d\u0027autre part sur des données réelles issues du UCI Machine Learning Repository (Asuncion et Newman, 2007). Le détail des jeux de données retenus est présenté dans le tableau 1 qui indique pour chacun son nombre d\u0027objets (n), sa dimensionalité (nombre d\u0027attributs n att ) et le nombre de clusters k attendus. comme le montre la figure 1. La borne de Bernstein permet d\u0027obtenir dans tous les cas de meilleurs résultats que la borne de Hoeffding à la fois en terme de nombre de comparaisons et de nombre d\u0027erreurs. Cela est probablement dû à l\u0027utilisation de la variance empirique qui permet de resserer un peu plus la borne de Bernstein par rapport à celle de Hoeffding. La force de cette amélioration dépend du jeu de données mais l\u0027exemple de la figure 1 est représen-tatif avec un nombre de comparaisons inférieur d\u0027environ 30% pour Bernstein par rapport à Hoeffding.\nRésultats comparatifs\nLa borne de Student est plus contrainte que les précédentes car elle fait l\u0027hypothèse d\u0027une distribution normale des données. Elle réalise ainsi beaucoup moins de comparaisons en géné-ral, mais au prix d\u0027un nombre d\u0027erreurs beaucoup plus élevé. En effet, sur nos jeux de données de tests, les bornes de Bernstein et Hoeffding, dès lors que le facteur de réduction r ? 0.25 ne génèrent quasiment plus aucune erreur par rapport à la méthode exhaustive, alors que la borne de Student, même non réduite (r \u003d 1) commet des erreurs (voir la colonne de droite de la figure 1).\nEnfin, l\u0027accélération augmente avec le nombre d\u0027itérations. Plus le nombre de données déjà traitées augmente, meilleures sont les estimations, et donc plus les clusters candidats sont rapidement éliminés de la compétition, ce qui accélère le processus. Enfin, d\u0027autres tests non rapportés dans cet article, suggèrent que le mécanisme d\u0027accélération est également possible et bénéfique pour de petits jeux de données (comme Iris par exemple).\nDiscussion autour du paramétrage\nL\u0027accélération basée sur les bornes théoriques présentées dans la section 3 admet comme paramètre la probabilité d\u0027erreur p. Du point de vue du problème de clustering, cette erreur p indique qu\u0027il y a une probabilité non nulle qu\u0027un mauvais cluster soit retenu. L\u0027erreur d\u0027affectation est donc supposée être inférieure à cette probabilité p. En pratique, comme le montre la figure 2-Haut, sur nos données de test nous observons que l\u0027augmentation de la probabilité d\u0027erreur réduit le nombre de comparaisons. En revanche, seule la borne de Student voit son erreur d\u0027affectation augmenter, les bornes de Berstein et Hoeffding ne générant pas d\u0027erreurs avec la borne originale (r \u003d 1). Ce résultat doit encore être étudié, mais nous pensons pour le moment que cela est dû au fait que les bornes de Bernstein et Hoeffding sont lâches et se retrouvent par conséquent souvent dans un cas de décision ambigue entre plusieurs clusters candidats, l\u0027affectation se faisant alors généralement au cluster correct sans garantie, mais après avoir éliminé plusieurs candidats.\nLes bornes de Bernstein et Hoeffding nécessitent également de connaître à l\u0027avance l\u0027éten-due (ou \"range\" R) des distances entre données. La figure 2-Bas illustre le comportement observé sur nos jeux de données à l\u0027aide du jeu Art 1 lorsque l\u0027étendue R est multiplié par des puissances de 2. On observe dans ce cas que la borne de Bernstein résiste mieux à une surestimation de l\u0027étendue que la borne de Hoeffding, mais que dans tous les cas, il est toujours possible d\u0027accélérer les calculs même avec une étendue 2 fois supérieure à ce qui était prévue initialement.\nFIG. 2 -Haut : analyse de l\u0027influence de la probabilité d\u0027erreur p. Haut-Gauche : exemple représentatif avec la borne de Bernstein pour le jeu de données Art 6 , qui montre l\u0027influence de la probabilité d\u0027erreur sur le nombre de comparaisons. Haut-Droite : influence de la probabilité d\u0027erreur p sur le nombre d\u0027erreurs dans le cas de la borne de Student. Bas : influence de la sur-estimation de l\u0027étendue des distances (\"range\") R en fonction des bornes de Bernstein (gauche) et Hoeffding (droite) pour le jeu de données Art 1 .\nConclusion et perspectives\nCe papier présente une nouvelle méthode de clustering en une passe pour des données non vectorielles, qui repose sur le principe des inégalités de concentration pour définir un méca-nisme de compétition (ou \"racing\") qui estime la distance d\u0027un nouveau point aux clusters tout en minimisant le nombre de comparaisons nécessaires. Trois bornes, Bernstein, Hoeffding et Student, sont comparées ainsi qu\u0027une version réduite de chacune d\u0027entre elles. Nos résultats montrent que notre algorithme permet de réduire drastiquement le nombre de comparaisons né-cessaires par rapport à une méthode de clustering en une passe exhaustive en pratique, bien que les garanties théoriques des bornes ne puissent être assurées du fait de la possible dépendance des observations. Le facteur de réduction permet d\u0027améliorer encore les résultats observés, notamment pour les bornes de Bernstein et Hoeffding qui sont, par construction, plus lâches que la borne de Student. Nous observons également que cette accélération augmente avec le nombre de données déjà classées, ce qui nous laisse à penser que notre méthode est particulièrement adaptée pour le traitement de grands jeux de données. En conclusion, d\u0027un point de vue général, la borne de Bernstein offre le meilleur compromis entre l\u0027accélération (meilleur que Hoeffding) et le nombre d\u0027erreurs (meilleur que Hoeffding et Student).\nDans le futur, de nouveaux tests doivent être conduits sans la correction des erreurs à partir de la \"vérité terrain\" de l\u0027approche exhaustive, de façon à évaluer la qualité des partitions ainsi que l\u0027impact des erreurs d\u0027affectation sur les affectations suivantes. Ces tests pourront être conduits sur des données plus grandes, comme des données d\u0027usage sur Internet qui sont produites continuellement et pour lesquelles il n\u0027est pas possible d\u0027utiliser les algorithmes incrémentaux classiques limités aux données numériques. D\u0027autres modèles statistiques (Ustatistics) et d\u0027autres bornes (borne de Serfling) pourront également être envisagés pour obtenir des garanties théoriques.\nA plus long terme on pourra s\u0027intéresser à l\u0027utilisation de l\u0027information d\u0027ambiguité, c\u0027est-à-dire les cas où il n\u0027est pas possible de différencier les clusters sur la base des bornes estimées, dans un contexte incrémental pour déterminer notamment : quand un cluster doit être créé, quand plusieurs clusters peuvent être fusionnés ou plus généralement pour identifier les points les moins importants d\u0027un cluster dont l\u0027importance peut en conséquence être réduite.\nSummary\nSingle-pass incremental clustering relies on the efficient assignment of each new data point to one of the existing clusters. In the general case, where it is not necessarily possible to represent the clusters by a mean, the exhaustive assignment of a point a cluster has a quadratic complexity in term of the number of data objects. This paper proposes a novel stochastic assignment method that minimizes the number of comparisons between the new data and each cluster to guaranty, given an acceptable error rate, that the point is assigned to its nearest cluster. Several theoretical bounds are considered (Bernstein, Hoeffding and Student) and compared in this paper. Results observed on artificial and real data sets show that Berntein bound give the overall best results (especially when it is reduced) as it provides the best acceleration of the clustering while maintaining a very low number of errors.\n"
  },
  {
    "id": "315",
    "text": "Introduction\nLes méthodes de recherche de motifs spatiaux sont utilisées couramment pour construire des caractérisations de données spatiales (voir Selmaoui-Folcher et al. (2013)). Ces approches s\u0027appuient sur des représentations de l\u0027espace tels que des graphes de voisinage ou des chemins construits sur des courbes fractales (par ex. chemins de Hibert-Peano, voir Mari et Le Ber (2006)). Les graphes de voisinages contiennent une information spatiale riche, mais ils sont plus complexes à fouiller, tandis que les chemins sont faciles à fouiller, mais ils réduisent l\u0027information spatiale disponible.\nDans ce travail, nous confrontons ces deux représentations vis-à-vis de la caractérisation d\u0027un parcellaire agricole. En particulier, nous cherchons à savoir si l\u0027approximation par un chemin fractal permet de conserver une bonne caractérisation de l\u0027organisation spatiale des parcelles agricoles en vue de l\u0027application d\u0027une méthode de recherche de motifs. Cette question soulève deux difficultés. La première est l\u0027absence d\u0027étiquetage des données qui permettrait d\u0027évaluer une représentation sur une tâche de classification. Nous nous plaçons donc dans un contexte non-supervisé. La deuxième difficulté porte sur la comparaison des caractérisations obtenues, qui sont de natures différentes. Nous ne pouvons pas nous appuyer comme classiquement sur des calculs de corrélation (par ex. par une matrice de confusion) entre les localisations des motifs dans les chemins et dans les graphes.\nPour résoudre ces difficultés, nous utilisons des « sacs de noeuds » , inspirés des sacs de mots introduits dans le contexte de l\u0027analyse de texte (Salton et al. (1975) Un graphe S G \u003d E G , ? est construit à partir des données selon la méthode décrite par Guyet (2010). Un noeud v ? V est construit pour chaque parcelle, représentée par son barycentre. Chaque noeud v ? V est associé à une occupation du sol. Un arc e ? E G ? V × V lie deux parcelles voisines, c\u0027est-à-dire connexes ou séparées par un faible espace (séparation par une route ou imprécision géométrique des données). Les arcs ne sont pas étiquetés.\nUne méthode de calcul du chemin de Hibert-Peano Adaptatif (CHA) a été décrit par Quinqueton et Berthod (1981). Cette méthode a été utilisée par Da Silva (2013) pour extraire les structures spatiales de linéaires agricoles. Elle utilise un ensemble de points spatialement distribués (ici les barycentres des parcelles), qui sont parcourus de manière déterministe. Le chemin est ensuite simplifié pour se ramener à une succession de parcelles, qui est transcrite sous la forme d\u0027une séquence d\u0027occupations du sol. Pour l\u0027unification des notations, un CHA peut être décrit de la même façon qu\u0027un graphe, par S CHA \u003d E CHA , ? avec un noeud par item de la séquence et un arc pour deux items successifs. On peut noter que la séquence S CHA obtenue sur les mêmes données n\u0027est pas un sous-graphe de S G .\nPour atténuer l\u0027influence des paramètres de construction des CHA, nous construisons plusieurs chemins 1) en faisant varier aléatoirement les limites de la cellule initiale autour des limites géographiques qui définissent une zone (3 cadrages aléatoires initiaux) et 2) en géné-rant les chemins pour les 4 directions principales.\nLa caractérisation des régions par SdN débute par l\u0027énumération de toutes les sous-structures d\u0027une représentation de l\u0027espace. Soit S \u003d E, ? une représentation de l\u0027espace, chemin de Hilbert adaptatif ou graphe. Une sous-structure de S est un triplet , E , ? où\n, il existe e ? E tq e soit un arc entre v et u. Dans le cas des CHA, on s\u0027intéresse à des sous-séquences de taille fixe w s . Dans le cas des graphes de voisinage, on s\u0027intéresse à des sous-graphes contenant exactement w g arcs. L\u0027énumération de toutes les structures est possible en temps output-polynomial (Bonzini et Pozzi (2007)), sans seuil de fréquence à fixer. Pour les sous-graphes, nous utilisons l\u0027outil TGE de Uno (2005). Le « sac de noeuds » (SdN) d\u0027une sous-structure s S , noté SdN (s S ) ? 2 O , est un vecteur de présence / absence des types d\u0027occupations du sol dans la sous-structure s S . Nous avons préféré ne conserver que l\u0027information de présence / absence à la place d\u0027un dénombrement pour éviter la multiplication combinatoire des sacs de noeuds.\nComparaison des représentations sur les parcellaires\nLe parcellaire a été décomposé en 14 sous-zones aux caractéristiques variées : les zones les plus au nord sont caractérisées par des grands champs de céréales, tandis que les zones au sud correspondent à un secteur bocager constitué de petites parcelles de prairies. La décomposition T. Guyet et al. réduit le nombre de parcelles à prendre en compte pour obtenir des résultats plus rapidement et permet d\u0027étudier le caractère spécifique des motifs d\u0027une zone par rapport à une autre ou le comportement des deux méthodes en relation avec les caractéristiques des zones.\nLe nombre total de sacs (de 1 à 4 occupations du sol) distincts s\u0027élève à 475 pour les graphes (toutes zones comprises), 206 pour les chemins : le rapport moyen du nombre de sacs différents trouvés par les graphes et par les chemins dans les différentes zones s\u0027élève à 1,9. Le nombre moyen d\u0027éléments dans les sacs issus des graphes s\u0027élève à 879 (toutes zones confondues), et à 28 pour les sacs issus des chemins.\nEn considérant ensemble tous les sacs obtenus sur les 14 zones par le CHA d\u0027une part et par le graphe d\u0027autre part, on obtient un indice de corrélation (Spearman) I r (graphes, chemins) \u003d 0, 712, ce qui indique une forte corrélation ; pour le test du ? 2 (avec distribution sous H 0 simulée à cause des faibles valeurs), on obtient I ? (graphes, chemins) \u003d 30094, valeur de p \u003c 0, 0005, soit des distributions très différentes. On observe les mêmes résultats sur les zones prises séparément. Les proportions des sacs estimées par les deux mé-thodes sont donc différentes mais corrélées.\nPour expliciter ces distributions différentes, on s\u0027intéresse maintenant aux sacs oubliés par les CHA. On peut calculer le nombre d\u0027éléments par sac à partir duquel un sac présent dans le graphe disparaît dans le chemin (moyenne sur les 14 zones 1215, 6 ± 855, 9) et le rapporter au nombre maximum d\u0027éléments dans les sacs issus des graphes (81307, 0 ± 64722, 3) ou au nombre moyen (5891 ± 6322, 74). Le premier taux s\u0027élève à 2,1%, le second à 44,6%. Ces chiffres dépendent fortement du nombre de parcelles dans chaque zone et de la diversité des occupations représentées.\nSi on regarde plus précisément la répartition des sacs perdus, on observe qu\u0027il s\u0027agit la plupart du temps de « petits » sacs, comptant moins de 100 éléments. Au delà on peut séparer des zones plutôt homogènes -peu de sacs perdus et peu remplis -et des zones plutôt hétérogènes -sacs plus nombreux et plus remplis. Les premières rassemblent des zones de petites parcelles de bocage (prairies très majoritaires) et des zones de grands parcelles cultivées (céréales et maïs majoritaires) : les sacs perdus par la méthode CHA représentent des occupations et des voisinages très minoritaires. Les deuxièmes sont plus diversifiées en taille de parcelles et occupations du sol : les sacs perdus peuvent correspondre à des voisinages relativement fréquents même si non majoritaires.\nDiscussion et conclusion\nLa comparaison d\u0027une analyse fondée sur les données extraites par un chemin et d\u0027une analyse sur les données complètes a été réalisée dans le cadre des modèles de Markov en analyse d\u0027images par Benmiloud et Pieczynski (1995). Pour ces modèles, l\u0027analyse fondée sur un chemin, bien que moins ajustée à la réalité des données, s\u0027est montrée pertinente et acceptable en termes de rapidité.\nPour l\u0027étude que nous avons menée, nous aboutissons à une conclusion similaire tout en mettant en évidence certains manquements de la méthode fondée sur le CHA, qui conduit à oublier les motifs rares mis en évidence par la méthode appuyée sur le graphe de voisinage.\nFinalement, si on s\u0027intéresse à des voisinages fréquents et à des zones relativement homogènes, la recherche de motifs par linéarisation de l\u0027espace s\u0027avère pertinente et efficace. Le fait que les différences se trouvent principalement sur les motifs rares laisse également espé-\n"
  },
  {
    "id": "316",
    "text": "Introduction\nLe Web 2.0 favorise le développement des sites collaboratifs, où les utilisateurs échangent des connaissances, se structurent en communautés, développent des codes, des usages et une sémantique qui leurs sont propres. Dans le cadre de la recherche traditionnelle de fouille de la connaissance, cette évolution en relative autonomie peut se révéler problématique : il n\u0027existe aucune garantie que cette dernière se structure autour d\u0027une sémantique qui soit en adéquation avec les bases de connaissance de référence traditionnelles. La pertinence des conclusions peut alors ne pas ou peu refléter l\u0027évolution réelle du comportement des utilisateurs et de la sémantique de leurs échanges.\nNous proposons une méthode pour construire notre propre compréhension des contributions des utilisateurs, basée uniquement sur les données de celles-ci, afin d\u0027extraire la séman-tique des utilisateurs. Nous évaluons cette approche par la mesure d\u0027une valeur de confiance. Nous effectuons notre analyse dans le contexte des recettes de cuisine, dont les sites de partage communautaires sont nombreux et très populaires sur le Web français comme mondial.\nÉtat de l\u0027art\nLa recette de cuisine est un type de données particulier, composé d\u0027un ensemble d\u0027ingré-dients et de procédures d\u0027exécution. Ce type de données est exploité par de nombreux systèmes de recommandation. Le Cooking Assistant (Sobecki et al., 2006) définit un système de recommandation démographique basé sur une inférence à logique floue, efficace pour fournir une réponse globale à un besoin général. Mais la généralisation des caractéristiques conduit à une recommandation également généralisée. Pour prendre en compte la spécificité des ingrédients, Freyne et Berkovosky utilisent la relation de composition qui existe entre ingrédients et recettes pour propager des évaluations et déterminer un comportement utilisateur (Freyne et al., 2011). Cela nécessite néanmoins une phase constante de normalisation, un travail d\u0027expert consistant à vérifier ou annoter les ingrédients afin qu\u0027ils correspondent à une liste de référence.\nLes recettes de cuisine ont également été traitées par des approches de raisonnement à partir de cas. Le système CHEF (Hammond, 1986) est un système d\u0027adaptation par la critique, qui permet de prendre en compte la spécificité du type de données qu\u0027est l\u0027ingrédient, en relevant les problèmes découlant d\u0027une substitution. En revanche, une importante phase d\u0027apprentissage est requise. Le système MIKAS (Khan et Hoffmann, 2003) propose de contourner ce besoin par un recours à l\u0027expert. Cette aspect de la transmission de connaissance de l\u0027expert au système par l\u0027expérience plutôt que par le déclaratif est vu comme plus efficace et plus adapté aux conditions réelles. Il ne permet toutefois pas une évaluation indépendante des contenus, car dépendant des connaissances propres de l\u0027expert.\nExtraction de l\u0027information et structure a priori\nLa première étape consiste à extraire l\u0027information depuis des lignes d\u0027ingrédients librement saisies. Pour exploiter ces données, nous définissons la structure a priori comme étant : quantité -unité -ingrédient. En fonction de l\u0027existence d\u0027une valeur dans les champs quantité et unité nous obtenons les classes 1, 2, 3 et 5 du tableau 1, la classe 5 regroupant les lignes où il a été impossible d\u0027extraire de l\u0027information pour les champs quantité et unité. Cette classe est ensuite ventilée par une phase d\u0027apprentissage en deux étapes :\n-Recherche des incohérences dans les éléments identifiés : la présence d\u0027un ingrédient complexe en classe 1 permet de mettre en évidence une erreur de détection dans les autres classes. Par exemple, la ligne « 500g de corned beef » permet d\u0027identifier la ligne « corned beef » comme ingredient seul, corrigeant la première identification de « corned » comme quantificateur 1 . -Pour toutes les lignes de la classe 5 restantes, nous cherchons à les faire correspondre aux cas précédemment rencontrés. Dans le cas du jeu de données Marmiton, la phase d\u0027apprentissage réduit la classe 5 de 20 à 1,9 % de la population (figure 1).\nÉvaluation de la confiance des ingrédients et des recettes\nOrdonnant nos ingrédients en fonction de leur fréquence, nous observons sur la fonction cumul des ingrédients un effet longue traîne, phénomène commun à bon nombre de sites sociaux à usages libres. Eu égard à la distribution en loi de puissance de nos données sociales, nous appliquons à notre modèle le principe de Pareto, où 80 % des effets sont le produit de 20 % des causes. Dans le cas où les a ingrédients contenus dans 80 % des lignes représentent moins de 20 % des ingrédients les plus fréquents, nous définissons b \u003d 5a ingrédients comme ensemble représentatif des ingrédients. La valeur de confiance C i est alors attribuée à chaque 1. Une unité sans présence de quantité est appelée quantificateur. \nLa figure 2 illustre la confiance ainsi calculée des ingrédients de Marmiton. La valeur de confiance par recette C x est égale à :\noù I x est l\u0027ensemble des ingrédients de la recette x et u(i, x) est l\u0027unité associée à l\u0027ingrédient i dans la recette x. µ u(i,x),i est alors la fréquence de l\u0027unité u(i, x) dans les lignes contenant l\u0027ingrédient i et C i la confiance de l\u0027ingrédient i. \nConclusion et travaux futurs\nNous avons présenté une méthode pour évaluer la confiance d\u0027une publication utilisateur, comme étant la probabilité qu\u0027un autre utilisateur en saisisse la sémantique. Notre approche est indépendante de toute base de connaissance externe, afin de raisonner directement sur les termes manipulés. Cette méthode présente l\u0027avantage de ne pas être dépendant de la langue, ni de souffrir des problèmes de pertinence ou de couverture relatifs aux bases de connaissance. Nous projetons d\u0027exporter la connaissance extraite des contributions utilisateurs, ce qui permettra de définir sans apport extérieur l\u0027ontologie du système analysé, ou d\u0027enrichir une base extérieure. Enfin, l\u0027application de méthode de partitionnement de fouille de données, guidées par nos mesures de confiance, permettra d\u0027évaluer une structure interne de la sémantique du système et des relations déductibles qui existent entre les différents ingrédients (proximité) ou recettes (variantes, alternatives).\n"
  },
  {
    "id": "317",
    "text": "Introduction\nUn entrepôt de données est une base de données dédiée à l\u0027analyse en ligne pour l\u0027aide à la prise de décision. Grâce aux opérateurs OLAP, l\u0027utilisateur peut extraire des cubes de données correspondants à des contextes d\u0027analyse (Inmon, 1992). Dans un SGBDR, la construction d\u0027un cube OLAP nécessite le calcul d\u0027agrégats à partir des n-upelts stockés en lignes. Ce type de stockage est pénalisé par un coût de jointure important, ce qui implique un temps élevé pour l\u0027extraction et le traitement des données. Par ailleurs, l\u0027architecture orientée colonnes offre un mode de stockage et une technique de traitement plus adéquats au processus analytique en réduisant considérablement les accès au disque (Matei, 2010). Cependant, les SGBD orientés colonnes ne disposent pas d\u0027opérateurs de construction de cube OLAP. L\u0027objectif de ce travail est de proposer une nouvelle approche de construction de cube OLAP. Cette approche utilise un système de stockage et de traitement orienté colonnes pour réduire le temps d\u0027extraction de données et une nouvelle méthode de calcul de cube OLAP. Nous avons implémenté cette approche sous l\u0027SGBDR orienté colonnes MonetDB 2 Construction de cube OLAP L\u0027idée sous-jacente est d\u0027extraire les données constituant le cube une seule fois de l\u0027entrepôt de données stocké en colonnes, et d\u0027appliquer ensuite un ensemble de traitements pour calculer tous les agrégats possibles, à différents niveaux de granularité. Une fois le cube est calculé, il est matérialisé suivant l\u0027architecture orientée colonnes pour permettre à l\u0027analyse OLAP de bénéficier des avantages de cette architecture quant à la manipulation du cube (Slice, Dice, Drill-Down, Drill-Up, ..).\nL\u0027approche que nous proposons s\u0027appuie sur une architecture orientée colonnes pour l\u0027entreposage des données et un un nouveau algorithme de calcul de cube OLAP qui parcourt le treillis de cuboïdes et exploite la relation originelle. A la différence des algorithmes de la deuxième approche de construction de cube OLAP dans les bases de données orientées lignes, où le treillis de cuboïdes est créé au fur et à mesure suivant un plan d\u0027exécution, notre méthode crée le treillis de cuboides global par un pré-traitement et le croise avec les données extraites de l\u0027entrepôt (relation originelle). Cela permet de diminuer le nombre d\u0027opération de calcul. Plus précisément, cette approche est exécutée en trois phases :\nPhase 1 : Extraction des données Cette phase consiste à extraire les données à partir de l\u0027entrepôt de données. Le résultat de cette phase est une relation R composée de valeurs de dimensions et de la mesure qui satisfont les prédicats de la requête. Avec un entrepôt de données implémenté suivant l\u0027approche orientée colonnes, cette phase est plus performante qu\u0027avec un entrepôt de données implémenté suivant l\u0027approche orientée lignes. En effet, le mécanisme de stockage de l\u0027architecture orientée colonnes permet un accès rapide aux données des différentes colonnes et diminue considéra-blement le coût de jointure comparé à celui de l\u0027architecture orientée lignes.\nPhase 2 : Construction du treillis de cuboïdes Cette phase consiste à construire le treillis de cuboïdes et définir les combinaisons possibles du cube à calculer.\nPhase 3 : Calcul et matérialisation du cube Cette phase consiste à parcourir le treillis de cuboïdes pour calculer le cube en fonction de la relation obtenue de la première phase, ainsi, le cube est matérialisé colonne par colonne\nExpérimentations\nNous avons implémenté le modèle benchmark en étoile (SSBM) K. Dehdouh et al.\nCalcul du cube OLAP : Cette expérimentation consiste à comparer le temps de construction du cube OLAP en utilisant les deux fonctions ; Group by CUBE et ROLLUP, selon notre approche avec celui de l\u0027approche classique (orientée lignes). Nous exécutons quatre requêtes décisionnelles de construction de cube OLAP avec un nombre de dimensions qui augmente progressivement. Les résultats obtenus sont présentés dans la figure 1.\nManipulation du cube OLAP : Cette expérimentation est consacrée à l\u0027évaluation du temps relatif à la manipulation du cube OLAP matérialisé selon notre approche avec celui de l\u0027approche classique. Pour cela, nous avons construit, selon les deux approches, un cube OLAP à trois dimensions et qui répond à la requête décisionnelle suivante : Quelle est la somme des revenus des ventes par année, par marque de produit et par région des clients. Nous avons ensuite exécuté des opérations de forage (slice et dice) sur le cube. Les résultats que nous avons obtenus sont présentés dans la figure 2. \nRésultats des expérimentations\nSummary\nThis paper presents a new method of calculating the OLAP cube. The results obtained from experiments carried out has shown that our approach significantly optimizes the OLAP cube building time and reduced the response time of cube manipulation, compared to the traditional approach.\n"
  },
  {
    "id": "318",
    "text": "Introduction\nLe besoin d\u0027incorporer les préférences aux requêtes dans les technologies de l\u0027information est un verrou crucial pour une grande variété d\u0027applications allant de l\u0027e-commerce aux moteurs de recherche personnalisés. Un utilisateur accédant à un système d\u0027information peut avoir à reformuler plusieurs fois sa requête pour éliminer les résultats insatisfaisants et cheminer vers le résultat attendu. En particulier, cette expérience est très fréquente avec les recherches sur le Web en raison d\u0027une abondance d\u0027information et surtout, de l\u0027hétérogénéité des utilisateurs. Une observation cruciale alors est que « différents utilisateurs considèreront comme pertinent des résultats différents » car leurs préférences divergent.\nCependant, la construction manuelle de modèles de préférences par l\u0027utilisateur reste à la fois complexe et consommatrice de temps. De ce fait, l\u0027apprentissage automatique de ses préférences, en s\u0027appuyant sur les interactions entre lui et le système, joue un rôle critique dans de nombreuses applications. Ces interactions appelées, feedbacks utilisateurs implicites, ont une forme souvent rudimentaire indiquant si un utilisateur a réalisé une action particulière sur un objet (par exemple, le temps passé sur un objet). Ces feedbacks implicites sont souvent ambigus et d\u0027une granularité peu fine mais ils sont plus faciles à obtenir en abondance dans un système réel. Dans cet article, nous faisons l\u0027hypothèse que nous disposons d\u0027un ensemble d\u0027objets et d\u0027un ensemble de paires de préférences sur ces objets, c.-à-d., tel objet est préféré à tel autre. Notre objectif sera de construire un modèle de préférences à partir de ces données même si de telles préférences sont moins subtiles que des notes et peuvent parfois contenir des inconsistances.\nDans cet article, nous présentons une approche de construction de profils de préférences contextuelles basée sur l\u0027extraction de motifs séquentiels, nommée Sprex (Sequence-patternbased preference rule extraction). Cette approche se constitue de trois composants principaux : un extracteur de motifs séquentiels pour l\u0027extraction des règles de préférence contextuelle, une fonction de modélisation qui permet de trier et de sélectionner un sous-ensemble de règles de préférence contextuelle afin de créer un profil, et une fonction de préférence qui permettent de prédire la préférence de l\u0027utilisateur en utilisant le profil construit. L\u0027approche Sprex étend l\u0027expressivité des règles de préférence contextuelle par rapport à Agrawal et al. (2006). Auparavant, de telles règles permettaient seulement de préférer un item par rapport à un autre. Notre proposition permet de préférer un ensemble d\u0027items par rapport à un autre. Ce gain en expressivité des règles permet de construire des profils enrichis plus proches des préférences de l\u0027utilisateur. En outre, la réutilisation des outils d\u0027extraction de motifs séquentiels fréquents permet à l\u0027approche Sprex de générer les profils de préférences avec une très grande efficacité. L\u0027étude expérimentale montre la rapidité de l\u0027approche mais aussi son efficacité en terme de prédiction.\nL\u0027article est organisé de la manière suivante. La section 2 définit les notions préliminaires. La section 3 introduit les travaux antérieurs associés à notre problématique. La section 4 pré-sente l\u0027approche Sprex. Pour cela, nous définissons formellement la notion de séquences de préférence et ensuite introduisons globalement notre approche ; enfin, nous présentons égale-ment les méthodes Sprex-Build et Sprex-Predict qui composent l\u0027approche Sprex. La section 5 rapporte les expérimentations que nous avons menées sur des jeux de données réels, avant de conclure et de présenter les perspectives associées dans la section 6.\nDéfinitions préliminaires et problématique\nNous définissons les notions préliminaires liées à notre problématique et à notre approche dans cette section.\nSoit R \u003d {R 1 , R 2 , . . . , R n } un ensemble d\u0027attributs. Pour chaque attribut R i ? R, nous notons le domaine de valeurs de\nRi?R dom(R i ) l\u0027ensemble de toutes les valeurs d\u0027attributs, chaque valeur i ? I est un item. Soit T \u003d {i 1 , i 2 , . . . , i n } une transaction, chaque sous-ensemble I ? T est un itemset. Une base de transactions est un ensemble de transactions, chacune associée à un identifiant unique. Une paire de transactions, notée 1 , T 2 est un vecteur de deux transactions tel que 1 , T 2 \u003d 2 , T 1 Dans cet article, nous modélisons les préférences de l\u0027utilisateur comme un ordre partiel sur les itemsets tel que X Y indique que l\u0027utilisateur préfère l\u0027itemset X à l\u0027itemset Y . Définition 1 (Règle de préférence contextuelle étendue). Une préférence contextuelle est une règle de la forme C ? X Y décrivant que l\u0027utilisateur préfère l\u0027itemset X à l\u0027itemset Y si le contexte, représenté par l\u0027itemset C, est observé.\nCette définition étend la notion de règle de préférence contextuelle utilisée par Agrawal et al. (2006) et de Amo et al. (2012. En effet, la définition usuelle se limite à des itemsets de taille 1 pour X et Y . En d\u0027autres termes, l\u0027expressivité de nos règles de préférences contextuelles est plus forte. Exemple 1. La règle de préférence {viande} ? {carotte} {riz} exprime qu\u0027un utilisateur préfère des carottes au riz pour accompagner de la viande, et la règle de préférence {viande, samedi} ? {carotte, vin} {riz, soda} précise que le samedi, cet utilisateur préfère accompagner sa viande avec des carottes et du vin plutôt que du riz et du soda.\nIntuitivement deux transactions sont comparables suivant la règle C ? X Y si les deux contiennent C et si une seule des deux contient X et l\u0027autre Y . De manière formelle, étant données une règle de préférence contextuelle r \u003d C ? X Y et une paire de transactions\nUne préférence de l\u0027utilisateur est une paire de transactions U ? P qui spécifie que l\u0027utilisateur préfère T à U , également noté T U . Soit D une base de transactions décri-vant des objets (p. ex. des films), une base de préférences de l\u0027utilisateur P ? D × D est un ensemble de paires de transactions correspondant à un échantillon des préférences de l\u0027utilisateur sur les objets de D. À partir d\u0027une base de préférences utilisateurs P, nous définissons le support d\u0027une règle de préférence contextuelle r, noté supp P (r), comme le nombre de paires de transactions qui supportent r ; nous définissions également la confiance de la règle r, notée conf P (r), comme le ratio du nombre de paires de transactions qui supportent r sur le nombre total de paires de transactions qui supportent ou contredisent r. On a alors supp P (r) \u003d |{p ? P | r + p}| et conf P (r) \u003d |{p ? P | r + p}| / |{p ? P | (r\nDéfinition 2 (Règle de préférence contextuelle minimale). Une règle de préférence contextuelle r \u003d C ? X Y est minimale par rapport à une base de préférences utilisateurs P si et seulement s\u0027il n\u0027existe aucune règle r\nUn modèle de préférences sur une base de préférences utilisateurs P est un ensemble trié de règles de préférences contextuelles minimales, noté M P . Étant donnée une base de préférences d\u0027un utilisateur, on dit qu\u0027un modèle de préférence est un profil de préférences de cet utilisateur. Le problème de la construction de profil d\u0027un utilisateur est ainsi de construire un modèle de préférences à partir d\u0027une base de préférences de l\u0027utilisateur. \nTravaux antérieurs\nReprésentation séquentielle des préférences\nSoit I l\u0027ensemble de tous les items, un item de préférence est une paire L:i où L ? {C, P, N} est un label (parmi Contexte, Préferé et Non-préféré) et i ? I est un item. Étant données deux transactions T et U telles que T U , l\u0027itemset contextuel est un ensemble d\u0027items de pré-férence {C:c 1 , C:c 2 , . . . , C:c k } où {c 1 , c 2 , . . . , c k } ? T ? U ; l\u0027itemset préféré est l\u0027ensemble {P:x 1 , P:x 2 , . . . , P:x m } où {x 1 , x 2 , . . . , x m } ? T \\ (T ? U ) ; l\u0027itemset non-préféré est l\u0027ensemble {N:y 1 , N:y 2 , . . . , N:y n } où {y 1 , y 2 , . . . , y n } ? U \\ (T ? U ). Les itemsets contextuel, préféré et non-préféré sont appelés les itemsets de préférence. Nous définissons trois fonctions de label {? C , ? P , ? N }. Chacune génère un itemset de préférence à partir d\u0027un itemset usuel : par exemple, si I \u003d {i 1 , i 2 , . . . , i n }, alors ? C (I) \u003d {C:i 1 , C:i 2 , . . . , C:i n }, etc.\nDe manière plus simple, on note\n. Nous appelons alors la séquence C P P N N une séquence de préférence, il s\u0027agit d\u0027une liste ordonnée des itemsets de préférence sur les itemsets C, P et N qui indique la préférence de la transaction T \u003d C ? P sur la transaction U \u003d C ? N . Étant donnée une base de préférences de l\u0027utilisateur P, il existe une base de séquences de préférence, notée P S , dont chaque séquence (associée à un identificateur unique) correspond à une préférence unique p ? P. . Etant donnée une base de séquences de préférence P S , le support de la séquence de préférence s, dénoté par supp P S (s), est le nombre de séquences de préférence de P S qui contiennent s : supp P S (s) \u003d |{s ? P S | s s }|. Etant donné un seuil minimal de support ?, une séquence s est alors dite fréquente si supp P S (s) ? ?, on parle de séquence fréquente de préférence.\nSoit r \u003d C ? X Y une règle de préférence contextuelle, où C \u003d {c 1 , c 2 , . . . , c k }, X \u003d {x 1 , x 2 , . . . , x m } et Y \u003d {y 1 , y 2 , . . . , y n }, alors la règle r peut être réécrite en une séquence de préférence C X P Y N Propriété 1. Avec la paire de transactions U où T U représentée sous la forme d\u0027une séquence de préférence C I P I N on obtient les relations suivantes :\nAinsi, étant donnée une base de séquences de préférence P S construite à partir des pré-férences utilisateurs P, alors le support de la règle r \u003d C ? X Y par rapport à P est équivalent au support de la séquence de préférence C X P Y N par rapport à P S . De plus, le nombre de paires de transactions de P qui contredisent la règle r est équivalent au nombre de séquences de préférence de P S qui contiennent la séquence C Y P X N\nPhase de construction : Sprex-Build\nLa construction du modèle de préférences Sprex-Build est décrite par l\u0027algorithme 1 où un modèle de préférence M P (c.-à-d., le profil de l\u0027utilisateur) est généré à partir d\u0027une base de préférences utilisateurs P avec un seuil minimal de support ?, un seuil minimal de confiance ?, et une fonction de modélisation ?. Sprex-Build génère d\u0027abord une base de séquences de préférence P S à partir des préférences utilisateurs P en utilisant la correspondance vue dans la section précédente (ligne 1 à 4). Ensuite, à la ligne 5, la collection des séquences fréquentes de préférence F est extraite depuis P S (selon un seuil ?) en utilisant n\u0027importe quel algorithme d\u0027extraction de motifs séquentiels (dans notre partie expérimentale, nous utiliserons l\u0027approche PatternGrowth). Pour chaque séquence fréquente de préférence s de l\u0027ensemble F, Sprex-Build calcule sa confiance conf P S (s) et l\u0027ajoute au modèle M si conf P S (s) ? ?. Enfin, une fonction de modélisation ? définie par l\u0027utilisateur est utilisée pour construire le modèle de préférence final.\nAlgorithme 1 : Sprex-Build\nEntrées : Les préférences de l\u0027utilisateur P, un seuil minimal de support ?, un seuil de confiance minimal ?, et une fonction de modélisation ?. Sorties : Modèle de règles de préférences contextuelles M P .\nComme précédemment décrit, étant donné une paire de transactions U une séquence de préférence C I P I N doit contenir au moins 2 itemsets de préférence et un maximum de 7. par contre, le motif séquentiel 1 , C:i 2 }} sera systématiquement ignoré car il ne sert à aucune règle pertinente.\nPour cette raison, l\u0027extraction des séquences de préférence fréquentes est plus simple que l\u0027extraction de tous les motifs séquentiels (Agrawal et Srikant, 1995;Pei et al., 2001). De plus, comme un modèle de préférence est constitué uniquement de règles de préférences minimales, le problème de construction se réduit même à l\u0027extraction des séquences génératrices fréquentes introduites par Lo et al. (2008).\nLa méthode Sprex-Build utilise ensuite l\u0027ensemble des motifs séquentiels extraits afin de générer un modèle (ligne 10) via une fonction de modélisation. Avant d\u0027appliquer une fonction de modélisation, nous conservons pour chaque paire de transactions la meilleure règle qui la supporte. Cette notion de « meilleure » est modélisée par un ordre sur les règles déjà utilisé par Liu et al. (1998) pour la construction de classifieur. Ainsi, une règle est d\u0027autant meilleure que sa confiance est élevée ; en cas d\u0027égalité, on choisit alors celle dont le support est le plus grand. La fonction de modélisation effectue ensuite deux opérations : 1) sélection de règles de préférence contextuelle en respectant des contraintes définies par l\u0027utilisateur (p. ex. le filtrage basé sur la confiance, le support, la composition, la structure ou la taille des règles) ; et 2) nouveau tri des règles sélectionnées en respectant un ordre défini par l\u0027utilisateur en cas de besoin. Il s\u0027agit donc de conserver des règles sûres et générales pour que le modèle final soit à la fois précis et concis.\nPhase de prédiction : Sprex-Predict\nL\u0027utilisation par Sprex-Predict du modèle construit lors de la phase précédente pour effectuer une prédiction entre deux préférences est détaillée en dessous.\nÉtant donné un modèle M, une fonction de préférence ? retourne un score c entre 0 et 1 qui prédit entre les transactions T et U , celle qui est préférée par l\u0027utilisateur en se référant au modèle M. Si c \u003e 0.5, cela signifie que l\u0027utilisateur préfère la transaction T à la transaction U et ainsi Sprex-Predict retourne la préférence T M U ; si c \u003c 0.5, Sprex-Predict retourne la préférence U M T car l\u0027utilisateur préfère la transaction U à la transaction T ; sinon, SprexPredict retourne T ? M U , qui signifie l\u0027indécision de Sprex-Predict quant à la préférence de l\u0027utilisateur entre T et U .\nLa fonction de préférence utilise les règles du modèle M afin de déterminer quelle est la transaction préférée entre T et U . Une fonction simple de préférence est d\u0027utiliser la meilleure règle du profil r best qui permet de préférer T à U ou vice-et-versa. Si T est préférée à U selon r best , la fonction de préférence ? best retournera conf P (r best ) (une valeur \u003e 0.5). À l\u0027inverse, si U est préférée à T selon r best , la fonction de préférence ? best retournera 1 ? conf P (r b ) (une valeur \u003c 0.5). Si aucune règle du profil ne s\u0027applique, la fonction ? best retournera 0.5. À nouveau, nous utilisons un ordre défini sur la meilleure confiance, puis sur le meilleur support comme lors de la construction du modèle afin de choisir la meilleure règle.\nMalheureusement, la fonction de préférence ? best est souvent indécise car il est peu fré-quent que deux transactions soient directement comparables par une règle du modèle, selon l\u0027étude expérimentale menée dans de Amo et al. (2012). Au lieu de comparer directement les deux transactions avec ? best , nous organisons un vote par valeur pour prendre la décision « qui est le meilleur candidat ? ». Notre vote par valeur est un système de vote pour une élection à un siège dans lequel les électeurs E évalue les deux candidats en leur attribuant une valeur entre 0 et 1 en utilisant ? best . Les valeurs de chaque candidat sont additionnées, et celui ayant le plus haut score est le gagnant. Formellement, nous avons :\nDans les expérimentations, nous utiliserons exclusivement ? vote dont le rappel est toujours supérieur à ? best pour une précision comparable.\nÉvaluations expérimentales\nDans cette section, nous rapportons les évaluations expérimentales de notre approche Sprex sur les jeux de données réels Toutes nos expérimentations ont été effectuées sur un serveur de 16-Core 2.40GHz Intel Xeon avec 32 giga-octets de mémoire vive et avec le noyau Linux 2.6.32. L\u0027extraction de motifs séquentiels est réalisée en utilisant le principe PatternGrowth proposé par Pei et al. (2001). Toutes les méthodes sont implémentées en C++ Template et compilées par LLVM-Clang++.\nLe jeu de données concerné dans nos expérimentations est constitué des 800 156 notes attribuées par 6 040 utilisateurs sur 3 881 films. Chaque film constitue un ensemble d\u0027attributs incluant un identifiant unique, l\u0027année de sortie, les genres (multi-valeurs), les réalisateurs (multi-valeurs) et les principaux acteurs/actrices (multi-valeurs). Chaque utilisateur a évalué un certain nombre de films (de quelques uns à quelques milliers) avec des notes comprises entre 1 et 5 4 . Tous les films votés par un même utilisateur composent un jeu de données indépendant. Alors, soit D un jeu de films votés par un utilisateur, pour toute paire de films (m 1 , m 2 ) ? D, on a m 1 .rating \u003e m2.rating impose m 1 m 2 . Ainsi, chaque jeu de films correspond à une base de préférences de l\u0027utilisateur. La table 1 liste les 4 jeux de données testés dans nos expérimentations où chaque jeu est identifié par l\u0027identifiant de l\u0027utilisateur et contient plus de 500 films votés afin d\u0027assurer une validation croisée en 5 blocs telle que chaque bloc de données contient du moins 100 films votés.\nDans une première étape, les motifs séquentiels sont extraits avec le support minimal variant entre 0,4% et 1,8% pour créer des profils bruts qui contiennent les règles de préférence TAB. 1 -Jeux de données réels sur les films votés.\nFIG. 1 -Les temps moyens (en seconde) de construction de profils bruts par rapport à la variation du support minimal.\nFIG. 2 -Les tailles moyennes de profils par rapport à la variation du support minimal avec la confiance minimale fixée à 75% (a-d) et à la variation de la confiance minimale avec le support minimal fixé à 0,5% (e-h).\nFIG. 3 -Les précisions moyennes (a-d) et les rappels moyens (e-h) de profils par rapport à la variation du support minimal avec la confiance minimale fixée à 75%.\nFIG. 4 -Les précisions moyennes (a-d) et les rappels moyens (e-h) de profils par rapport à la variation de la confiance minimale avec le support minimal fixé à 0,5%.\ncontextuelle minimales avec la confiance minimale fixée à 50% afin de faciliter les tests suivants. Ensuite, pour chaque profil brut, deux fonctions de modélisation, MaxRule et MinRule, sont appliquées pour construire des profils finaux. La fonction MaxRule permet de construire des profils en sélectionnant les règles les plus spécifiques parmi toutes les règles minimales gé-nérées précédemment, par contre la fonction MaxRule construit un profil en utilisant les règles moins expressives comme introduites dans les approches proposées par Agrawal et al. (2006) et par de Amo et al. (2012). Dans cette étape, la confiance minimale a varié entre 70% et 90%. Enfin, les résultats obtenus par SVMRank sont utilisés comme référence. La figure 1 montre les temps moyens de construction de profils bruts à partir des 4 jeux de films. En effet, avec le support minimal 0,4%, les constructions se terminent en 700 secondes dont les nombres moyens de règles par profil sont respectivement 20 934, 13 208, 12 020 et 17 699. Après avoir appliqué la fonction de modélisation, la taille des profils est significativement réduite comme le montre la figure 2.\nNous avons testé la qualité de prédiction de chaque profil construit en mesurant la précision et le rappel de la prédiction. Pour cet objectif, deux séries de tests ont été effectuées en variant le support minimal (la figure 3) et en variant la confiance minimale (la figure 4). Les courbes montrent que la qualité de la prédiction augmente avec l\u0027expressivité des règles qui composent les profils. Par ailleurs, les courbes montrent également qu\u0027il y a une corrélation forte entre la qualité de prédiction et la diminution des seuils de support et de confiance. La table 2 présente un exemple des règles de préférence contextuelle extraites à partir du jeu de films U0048. \nConclusions\nDans cet article, nous avons présenté Sprex, une approche basée sur l\u0027extraction de sé-quences fréquentes pour la construction de modèles de préférences. L\u0027idée essentielle est de représenter les données d\u0027apprentissage et les règles de préférence utilisateur sous la forme de triplets formant des séquences de préférence. De cette manière, nous avons pu tirer profits des travaux issus de l\u0027extraction de motifs séquentiels fréquents et mettre en place une approche efficace. Par ailleurs, cette proposition a également l\u0027avantage d\u0027étendre l\u0027expressivité des préférences extraites. Nous avons évalué notre approche sur une base de données filmographique du monde réel, et les résultats expérimentaux ont montré que les modèles de préférence construits sont relativement stables par rapport à la variation du seuil minimum de support.\nNos recherches futures vont s\u0027orienter vers le développement de fonctions de modélisation et des fonctions de préférence afin d\u0027améliorer la précision et le rappel de la prédiction des préférences de l\u0027utilisateur. Enfin, nous pensons exploiter le potentiel des séquences au sein de l\u0027approche Sprex pour introduire la notion de temps dans les règles de préférences.\n"
  },
  {
    "id": "319",
    "text": "Introduction\nL\u0027évolution et le croisement des disciplines de la fouille de données et du génie logiciel ouvrent de nouveaux horizons pour la compréhension et l\u0027amélioration du logiciel et des pratiques de développement. A l\u0027aube de ce domaine émergent de nombreuses questions restent cependant d\u0027actualité du point de vue de la conduite de programmes de mesure logicielle, notamment relevées par Fenton (1994) et Kaner et Bond (2004). Nous proposons dans cet article quelques pistes pour répondre à ces problématiques et mettre en place un processus de mesure fiable et efficace.\nIl est utile pour la définition de la notion de qualité de s\u0027appuyer sur des modèles ou standards reconnus : du point de vue de la qualité produit, la référence de facto semble être l\u0027ISO 9126 et son futur successeur, la série 250xx SQuaRE. La maturité du processus de développe-ment est adressée par des initiatives largement reconnues telle que le CMMi ou l\u0027ISO 15504. Afin de clarifier la démarche de mesure, l\u0027approche Goal-Question-Metric proposée par Basili et al. et reprise par Westfal Westfall et Road (2005) permet une approche plus rigoureuse, qui préserve l\u0027efficacité de l\u0027analyse et le sens de ses résultats. Le code source est le type d\u0027artéfact le plus utilisé pour l\u0027analyse de projets logiciels. Du point de vue de l\u0027analyse statique (Louridas, 2006), les informations que l\u0027on peut récupérer d\u0027un code source sont les métriques, correspondant à la mesure de caractéristiques définies du logiciel (e.g. sa taille ou la complexité de son flot de contrôle), et les violations, correspondant au non-respect de bonnes pratiques ou de conventions de codage ou de nommage (e.g. l\u0027obligation de clause default dans un switch). Ces informations sont fournies par des analyseurs tels que Checkstyle, PMD ou SQuORE (Baldassari, 2012).\nLa gestion de configuration contient l\u0027ensemble des modifications faites sur l\u0027arborescence du projet, avec des méta-informations sur l\u0027auteur, la date ou l\u0027intention des changements. Le positionnement dans l\u0027arbre des versions est important, car les résultats ne seront pas les mêmes pour une version en cours de développement (développée sur le tronc) et pour une version en maintenance (développée sur une branche).\nLa gestion des tickets recense l\u0027ensemble des demandes de changement faites sur le projet. Ce peuvent être des problèmes (bugs), de nouvelles fonctionnalités, ou de simples questions.\nLes listes de diffusion sont les principaux moyens de communication utilisés au sein de projets logiciels. Il existe en général au moins deux listes, une dédiée au développement et l\u0027autre aux questions utilisateur. La définition des métriques accessibles à partir des référentiels identifiés sur le projet doit être fiable -i.e. l\u0027information recherchée est systématiquement présente et valide -et compréhensible pour garder la confiance des acteurs dans le processus.\nL\u0027implémentation du processus de collecte et d\u0027analyse doit être transparente pour que les acteurs puissent se l\u0027approprier, intégralement automatisée, et exécutée de manière régulière.\nLa présentation des informations est capitale. Dans certains cas une liste concise d\u0027arté-facts est suffisante, alors que dans d\u0027autres cas un graphique bien choisi sera plus adapté et délivrera en quelques secondes l\u0027essentiel du message.\nMise en pratique avec Eclipse\nCette approche a été mise en oeuvre dans le cadre de Polarsys, un groupe de travail de la fondation Eclipse qui a pour but, entre autres, de proposer un cadre d\u0027évaluation de la qualité des projets de la fondation. L\u0027arbre de qualité montré en figure 2 montre les exigences identifiées pour Eclipse et Polarsys, et leur organisation en attributs de qualité.\nFIG. 2 -Modèle de qualité proposé pour Polarsys.\nDu point de vue de l\u0027aide à la prise de décision, les axes de qualité choisis permettent d\u0027évaluer la maturité du projet immédiatement, selon les critères propres à l\u0027organisation. Par exemple le projet analysé sur la droite de la figure 2 montre une bonne qualité produit, mais une faible activité de communication et un petit nombre de participants, car l\u0027essentiel des contributions est fait par une unique société. Cette caractéristique peu visible a priori en fait un risque pour la pérennité du projet, rapidement identifiable sur l\u0027arbre de qualité. Du côté des équipes de développement, l\u0027aide à l\u0027amélioration de la qualité se fait au moyen de listes de violations et de points d\u0027action, qui permettent d\u0027identifier et d\u0027améliorer les pratiques non acquises et fournissent des recommandations pragmatiques pour l\u0027amélioration du code et du processus.\nConclusion\nLe prototype développé dans le cadre du groupe de travail Polarsys a permis de définir un socle méthodologique commun et un ensemble de métriques issues de sources nouvelles pour travailler ensemble sur la notion de qualité, et a plus généralement permis de démontrer la faisabilité d\u0027une telle solution. Le prototype a apporté la preuve que des connaissances pratiques pouvaient être extraites du projet pour l\u0027évaluation et l\u0027amélioration de la maturité. Ce travail a été présenté à la communauté Eclipse lors de la conférence EclipseCon France 2013 sise à Toulouse en Juin 2013, et son industrialisation est en cours pour une publication prévue en 2014. Il est également envisagé d\u0027étendre la démarche de qualité initiée pour Polarsys aux projets Eclipse pour compléter les initiatives actuelles PMI et Dash.\n"
  },
  {
    "id": "320",
    "text": "Introduction\nLa classification supervisée de documents vise à déterminer la ou les catégories potentielles d\u0027un document à partir de son contenu (les termes le composant). Dans un cadre supervisé, le processus se décompose généralement en 2 phases :\n1. la phase d\u0027apprentissage, qui vise à créer un modèle à partir d\u0027un ensemble d\u0027exemples étiquetés (documents dont la classe est connue), 2. la phase de classification, qui va déterminer la ou les catégories d\u0027un document dont la classe est inconnue par application du modèle.\nBien entendu, la qualité du modèle dépend de la qualité et du nombre d\u0027exemples disponibles. Ainsi plus il y a d\u0027exemples, plus les observations seront fiables et plus le modèle sera précis et efficace.\nIl peut cependant s\u0027avérer intéressant de pouvoir élaborer un modèle de classification fiable à partir d\u0027un faible nombre de descripteurs (Forman et Cohen, 2004). Par exemple, le dévelop-pement des réseaux sociaux, avec un nombre de plus en plus important de messages en temps réel mais d\u0027une taille limitée (comme un tweet limité à 140 caractères), implique la mise à disposition d\u0027outils capables de les classer rapidement avec un volume restreint de données. Dans ce contexte, l\u0027extraction de descripteurs pertinents et discriminants représente un défi intéressant. De même, il peut arriver que le nombre d\u0027exemples utiles à la création du modèle lors de la phase d\u0027apprentissage soit lui même très limité. De plus, déterminer la classe d\u0027un document est une opération longue qui, généralement, nécessite un expert du domaine. Certaines approches se fondent sur un nombre restreint d\u0027exemples, les approches de classification semi-supervisées qui utilisent les documents non étiquetées pour compléter l\u0027apprentissage supervisé ou encore d\u0027apprentissage actif qui consiste à construire l\u0027ensemble d\u0027apprentissage du modèle de manière itérative, en interaction avec un expert humain. Certaines de ces méthodes appliquées à un faible nombre d\u0027exemples sont présentées dans (Zeng et al., 2003;Lin et Cohen, 2010) mais elles impliquent d\u0027avoir un grand nombre de documents à disposition pour améliorer le modèle ce qui n\u0027est pas toujours possible.\nLa plupart des algorithmes actuels de classification supervisée de documents nécessitent un nombre suffisant d\u0027exemples pour créer un classifieur performant et les performances dé-croissent en même temps que le nombre d\u0027exemples diminue. Notre principale contribution est de proposer de nouvelles pondérations de descripteurs textuels pour traiter des jeux d\u0027exemples de petite taille. Ces méthodes de pondérations ont été intégrées dans des approches classiques de classification (Class Feature Centroid et Naive Bayes).\nL\u0027article est organisé de la manière suivante : dans la section 2, nous discutons de l\u0027intérêt de nouvelles pondérations. Des mesures permettant d\u0027extraire les descripteurs les plus pertinents d\u0027une classe sont présentées en section 3. L\u0027intégration de ces mesures au sein d\u0027algorithmes de classification en apprentissage supervisé est décrite dans la section 4. Les résul-tats expérimentaux, comprenant notamment une comparaison avec d\u0027autres types d\u0027approches, sont proposés en section 5. Enfin, la section 6 conclue et présente quelques perspectives.\nProposition\nLa classification supervisée de documents cherche à déterminer la catégorie (ou classe) d\u0027un document à partir de son contenu. Considérons C \u003d C 1 , C 2 , ..., C n un ensemble de n classes et\nl\u0027ensemble des documents de la classe j. Les documents sont représentés selon le modèle sac de mots (bag of words) de Salton (Salton et McGill, 1986) et tous les termes de tous les documents forment un dictionnaire (ensemble des termes apparaissant au moins une fois dans la collection de documents). L \u003d t 1 , t 2 , ..., t |L| est un dictionnaire qui contient |L| termes où t i (1 ? i ? |L|) est un terme unique dans le dictionnaire. Chaque document est représenté par un vecteur pondéré de termes sans indication de position dans le document.\n? ? D j \u003d {w 1j , w 2j , ..., w |L|j } est la représentation vectorielle du document j où w ij est le poids (ex : Fréquence, Booléen,...) du terme t i pour le document j. Le poids w ij d\u0027un terme dépend à la fois de son poids intra-classe et de son poids inter-classes. Le poids intra-classe permet d\u0027évaluer l\u0027importance du terme au sein de la classe (Est-ce un terme représentatif de la classe ?) alors que le poids inter-classes mesure pour le terme, le pouvoir discriminant de la classe par rapport aux autres classes (Est-ce que le terme représente toutes les classes ?). Des méthodes de pondérations sont présentées dans (Guan et al., 2009) et (Zhang et al., 2012). Ces pondérations, dérivées du T F -IDF , reposent à la fois sur la fréquence d\u0027un terme dans les documents au sein d\u0027une classe mais aussi sur la présence ou l\u0027absence de ce terme au sein des autres classes. Nous pensons que deux situations posent des problèmes :\n-En présence de classes composées d\u0027un petit nombre de documents, l\u0027impact de la fré-quence de document est trop importante. -En présence de classes sémantiquement proches puisqu\u0027un terme est considéré comme représentant une classe même s\u0027il n\u0027apparaît qu\u0027une seule fois dans la classe. Dans ce cas, le nombre d\u0027occurrences du terme au sein des classes n\u0027est pas pris en compte. De plus, ces pondérations ne sont appliquées que dans une approche de type Class-FeatureCentroid quand nous pensons qu\u0027elles peuvent être pertinentes dans d\u0027autres approches de classification en apprentissage supervisé.\nDans la section suivante, nous proposons de nouvelles pondérations pour répondre à ces problèmes.\nNouvelles pondérations\nNous présentons dans cette section de nouvelles méthodes de pondérations intra-classes (Section 3.2) et inter-classes (Section 3.3) dérivées du T F -IDF . Nous commençons par rappeler les principes de base du T F -IDF .\nLes principes du T F -IDF\nLe principe du T F -IDF est de donner un poids plus important aux termes les plus spéci-fiques d\u0027un document (Salton et McGill, 1986). Le T F -IDF est une méthode de pondération éprouvée dont l\u0027efficacité a été démontrée à de nombreuses reprises. Cette mesure repose sur le produit entre la fréquence du terme (T F : Term-frequency) et la fréquence inverse du document (IDF : Inverse Document Frequency). La fréquence du terme correspond au nombre d\u0027occurrences d\u0027un terme dans un document et représente le poids du terme au sein du document, aussi appelé poids intra-document. Pour un document d j et un terme t i , la fréquence du terme est calculée par :\nT F i,j \u003d n i,j k n k,j où n i,j correspond au nombre de fois où le terme t i apparait dans le document d j et où le dénominateur correspond au nombre total de termes du document d j . La fréquence inverse de document (IDF) mesure l\u0027importance du terme dans le corpus. L\u0027objectif est de donner un poids plus important aux termes qui apparaissent dans peu de documents. Il s\u0027agit du poids inter-documents qui est calculé en considérant le logarithme de l\u0027inverse de la fréquence de documents qui contiennent le terme dans le corpus :\navec |D|, le nombre de documents du corpus et |{d j : t i ? d j }| le nombre de documents qui contiennent le terme t i .\nLe T F -IDF est obtenu en multipliant les poids intra-document et inter-documents du terme t i pour le document\nPour évaluer la représentativité d\u0027un terme dans une classe et non dans un document, nous proposons une mesure adaptée sur les poids intra-classes et inter-classes d\u0027un terme.\nPoids Intra-classe\nDans un premier temps, nous proposons une méthode de pondération fondée sur la fré-quence de documents du terme dans la classe comme décrit dans (Guan et al., 2009), appelé inner-weight Df . Nous calculons le poids inner-weight Df ij du terme t i de la classe j selon la formule (1). Nous considérons que le terme le plus représentatif d\u0027une classe n\u0027est pas né-cessairement le terme le plus fréquemment utilisé dans la classe mais celui utilisé dans le plus grand nombre de documents de la classe. De plus, une telle mesure peut se révéler particuliè-rement pertinente pour le traitement de documents de longueurs déséquilibrées. En effet, dans ce cas, les descripteurs linguistiques présents dans les documents de plus grandes tailles auront un impact similaire aux termes présents dans les plus petits documents.\ninner-weight\nAvec : -DF j ti : Nombre de documents contenant le terme t i dans la classe C j -|d j | : Nombre de documents dans C j Néanmoins Inner-weight Df est confronté aux mêmes limites que celles présentées dans la Section 2 avec les classes composées d\u0027un faible nombre de documents, à savoir l\u0027impact de la fréquence de documents dans les classes les moins pourvues sera disproportionné par rapport aux classes les plus fournies. Ainsi, nous proposons une autre méthode de pondération fondée sur le terme plutôt que sur le document. Nous considérons la fréquence du terme plutôt que la fréquence du document. Cette méthode est appelée inner-weight T f . Nous définissons le poids inner-weight T f ij du terme t i de la classe j selon la formule (2).\nAvec : -T F j ti : Nombre d\u0027occurrences du terme t i dans la classe C j -|n j | : Nombre de termes total dans la classe C j Dans cette section, nous avons redéfini deux pondérations intra-classes appelées inner-weight\nDans la section suivante, nous nous intéressons à la pondération inter-classes.\nPoids Inter-classes\nNous proposons une première méthode que nous nommons inter-weight class définie par la formule (3). Cette méthode, fondée sur le nombre de classes qui contiennent le terme, a été utilisée dans (Guan et al., 2009 inter-weight\nAvec : -|d / ? C j | : Nombre de documents n\u0027appartenant pas à la classe C j -|d : t i / ? C j | : Nombre de documents n\u0027appartenant pas à la classe C j qui contient t i -|d| : Nombre de documents dans l\u0027ensemble des classes -|d ? C j | : Nombre de documents de la classe C j -|d : t i | : Nombre de documents dans l\u0027ensemble des classes contenant le terme t i -|d : t i ? C j | : Nombre de documents de la classe C j qui contient t i -En ajoutant 1, permet de prévenir le cas où t i est uniquement utilisé dans C j (quand |d :\nPour évaluer nos propositions de pondérations, nous les avons intégrées à des méthodes d\u0027apprentissage supervisé détaillées en section 4.\n4 Utilisation des mesures dans un contexte d\u0027apprentissage supervisé SVM (Support Vector Machine) et Naive Bayes sont considérés parmi les algorithmes de classification supervisée les plus performants. Cependant, ils ne sont pas toujours adaptés en présence de faibles volumes de données (Kim et al., 2006). Nos nouvelles méthodes de pondération peuvent être utilisées dans des approches Naives Bayes ou Class-Features-Centroid qui présentent les avantages suivants : (i) leur facilité de mise en oeuvre les rendent bien adaptées pour la classification automatique de documents ; (ii) elles sont toutes deux fondées sur des descripteurs pondérés au niveau des classes, ce qui rend nos mesures bien adaptées à ce type d\u0027algorithme ; (iii) enfin les modèles obtenus sont faciles à interpréter et à valider pour un utilisateur final.\nNouvelles mesures et classification Class-Feature-Centroid\nL\u0027approche Class-Feature-Centroid est un modèle récent présenté dans (Guan et al., 2009). Chaque classe est considérée selon le modèle vectoriel de Salton (Salton et McGill, 1986), sur la représentation en sac de mots. Chacune des classes est représentée par un vecteur de termes. ? ? C j \u003d {w 1j , w 2j , ..., w |L|j } est la représentation de la classe j où w ij est le poids du terme t i pour la classe j. \nNouvelles mesures et Naive Bayes\nNous avons aussi intégré nos pondérations dans une approche Naive Bayes. Le classifieur Naive Bayes est un classifieur de type probabiliste défini dans (Lewis, 1998) très utilisé pour la classification de texte car il donne de bons résultats malgré l\u0027hypothèse rarement vérifiée d\u0027indépendance conditionnelle à la classe des descripteurs. La probabilité qu\u0027un document non étiqueté (d) composé de i termes t i appartienne à la classe C j est donnée par\nAprès avoir calculé C j \u003d {w 1j , w 2j , ..., w |L|j } où w i,j est le poids du i eme terme de la classe C j , nous estimons la probabilité qu\u0027un document non étiqueté d appartienne à la classe C j : P (d ? C j ) \u003d P (C j ) i (w i,j + 1). Ajouter 1 permet de prévenir le cas où le terme n\u0027apparait pas dans la classe, quand la probabilité vaut zéro. Les expérimentations sont appelées N b \nAlgorithmes de comparaison\nDans la section suivante (section 5) nous comparons ces huit méthodes de classification supervisée (4 pondérations pour chacun des 2 algorithmes) avec différents algorithmes implé-mentés dans Weka 2 : -Deux implémentations de SVM, SMO, qui utilise un noyau polynomial (Platt, 1999) et LibSVM, qui utilise un noyau linéaire (Chang et Lin, 2011). -Une implémentation Naïve Bayes, DMNB (Su et al., 2008). -Un arbre de décision, LadTree, (Holmes et al., 2002). D\u0027autres expérimentations ont été réalisées, toujours avec Weka (NaiveBayes (John et Langley, 1995), NaiveBayes Multinomial (McCallum et al., 1998), LibSVM avec fonction à base radial et LibSVM avec noyau polynomial (Chang et Lin, 2011), J48 et RepTree (Quinlan, 1993)). Les résultats étant moins satisfaisants, ils ne sont pas présentés dans cet article 3 . Chacun des algorithmes a été testé en validation croisée (3-fold cross validation) et les résultats indiqués ci-après sont la moyenne de ces 3 itérations. Le nombre de 3 itérations a été retenu afin d\u0027avoir un nombre suffisant de données en apprentissage et test. Les différentes expérimentations fondées sur ces approches sont détaillées dans la section suivante.\nExpérimentations\nProtocole expérimental\nNous avons évalué nos propositions sur 2 corpus différents : -Le corpus Reuters-21578 4 , qui est fréquemment utilisé par la communauté pour évaluer la qualité des modèles, est un ensemble de dépêches écrites en anglais mises à disposition par l\u0027agence Reuters. Les news sont regroupées dans différentes catégories comme par exemple \"sucre\", \"huile\" ou \"or\", etc. Les documents ont été étiquetés manuellement. pour le corpus Tweet. Le Tableau 1 présente les caractéristiques des 2 corpus après pré-traitement.\nRésultats\nLa qualité des modèles est évaluée par la Précision, le Rappel et la F-mesure (moyenne harmonique de la Précision et du Rappel). Nous avons évalué la Micro-Moyenne (calcul global du Rappel et de la Précision sur l\u0027ensemble des classes) et la Macro-Moyenne (calcul du Rappel et de la Précision pour chaque classe, puis calcul de la moyenne des classes (Nakache et Metais, 2005)).\nPour étudier le comportement de nos pondérations au sein des approches Naive Bayes et Class-Feature-Centroid et pour comparer nos résultats avec les autres algorithmes de classification, nous avons réalisé plusieurs expérimentations sur les corpus \"Reuters-21578\" et \"Tweet\". Nous avons étudié l\u0027impact du nombre de termes, du nombre de documents et du nombre de classes sur les résultats de classification.\nConséquences du nombre de termes sur la classification\nTout d\u0027abord, sur le corpus Tweet, nous avons fixé le nombre de classes (5) et de documents (1186) et décidé de supprimer aléatoirement des termes afin de diminuer le nombre de termes par document. Nous avons réalisé sept expérimentations résumées dans le Tableau 2.\nNous présentons l\u0027évolution de la F-mesure en fonction des expérimentations dans le Tableau 3. Les Macro-Moyenne et Micro-Moyenne suivant la même tendance, nous ne reproduisons ici que la Micro-Moyenne. Ces expérimentations nous permettent de tirer 2 enseignements : (1) les nouvelles pondérations intégrées aux approches Naive Bayes et Class-FeatureCentroid donnent des résultats meilleurs que les algorithmes SVM et DMNB quand le nombre 6. Le Front National, dû au faible nombre d\u0027élus actifs sous Twitter, était sous représenté dans le corpus.\nTAB. 3 -Expérimentation 1 : évolution F-mesure (Micro-Moyenne) de termes est faible (expérimentations 6 et 7), (2) elles donnent de meilleurs résultats que les algorithmes LadTree et LibSVM dans tous les cas.\nConséquences du nombre de classes sur la classification\nEnsuite, sur le corpus \"Reuters-21578\", nous nous intéressons à l\u0027impact du nombre de classes. Nous fixons le nombre de documents par classe (50) et nous supprimons des classes pour passer de 28 à 2 classes. Nous avons réalisé 7 expérimentations, présentées dans le Tableau 4.\nComme nous pouvions le supposer, les algorithmes ont un meilleur comportement en pré-sence d\u0027un nombre limité de classes. Les algorithmes suivent la même tendance et nous déci-dons de ne pas reproduire ici le détail des résultats 7 . Nous pouvons préciser que LadTree est plus impacté par un grand nombre de classes que les autres et que les nouvelles pondérations intégrées dans des approches Naive Bayes ou Class-Feature-Centroid donnent des résultats lé-gèrement meilleurs que ceux des autres approches (des observations similaires sont présentées ci-après). Il est aussi intéressant de noter que les résultats sont similaires sur un corpus de langue française (Tweet) et un corpus de langue anglaise (Reuters-21578). A partir de ces expérimentations, nous pouvons conclure que les méthodes de pondérations proposées dans cet article intégrées dans des approches Naive Bayes ou Class-Feature-Centroid (1) sont légèrement meilleures que les autres algorithmes (seulement devancé par LadTree) , (2) sont plus résistantes que la plupart des algorithmes quand le nombre de documents disponibles en apprentissage diminue fortement.\nConclusions et perspectives\nLa classification d\u0027un faible volume de documents textuels reste une problématique d\u0027actualité. En effet, même si le nombre de documents ne cesse de croître, il existe de plus en plus de domaines d\u0027applications où nous devons rapidement et à partir d\u0027un faible volume être capable de classer. Par exemple les tweets nécessitent, pour être suivis, de pouvoir classer en temps réel l\u0027information disponible. Attendre d\u0027avoir un nombre suffisant d\u0027éléments n\u0027est pas toujours possible et le décideur souhaite avoir rapidement les documents classés. Dans cet article, nous avons proposé de nouvelles mesures particulièrement adaptées aux faibles volumes de données (dû à un faible nombre de documents ou à un faible nombre de descripteurs). Nous avons également montré comment ces mesures pouvaient être prises en compte dans un cadre supervisé notamment via Naive Bayes et une approche basée sur les centroides. Les expérimentations menées sur un corpus de tweets et un benchmark plus traditionnel ont permis de montrer que nos nouvelles mesures ont un meilleur comportement que les autres approches lors de la manipulation d\u0027un faible volume de documents. Simples à mettre en place, elles sont tout à fait adaptées à la manipulation de données évoluant rapidement comme les tweets. Notre proposition est fondée sur une extension de la mesure de T F -IDF . Nos travaux actuels évaluent la possibilité de prendre en compte d\u0027autres mesures comme par exemple OKAP I BM 25 (Robertson et al., 1999) afin de mieux appréhender l\u0027impact sur non seulement de petits volumes mais également des documents courts. Dans un contexte de classification, la proximité sémantique des classes est difficile à prendre en compte. Via nos propositions et notamment les mesures inter et intra classes, nous avons montré qu\u0027elles sont adaptées aux faibles volumes. Nous souhaitons à présent proposer à l\u0027utilisateur de pouvoir mieux pondérer ces mesures en fonction de la distribution et du volume de termes.\n"
  },
  {
    "id": "321",
    "text": "Biographie\nThomas Lebarbé est maître de conférences en informatique et sciences du langage à l\u0027Université Stendhal -Grenoble 3. Depuis sept ans, il consacre ses recherches aux humanités numériques autour de fonds patrimoniaux (notamment les Manuscrits de Stendhal) sans perdre de vue la dimension linguistique des matériaux sur lesquels il travaille. Il a soutenu une habilitation à diriger des recherches promouvant l\u0027interdisciplinarité intrinsèque et extrinsèque du traitement automatique des langues. Dans ses différentes fonctions, enseignant d\u0027informatique dans une université de lettres, directeur adjoint de la Maison des Sciences de l\u0027HommeAlpes et Chargé de Projet Humanités Numériques à l\u0027université Stendhal, il revendique le dé-cloisonnement disciplinaire comme un vecteur de sérendipité et un moyen de questionner les\n"
  },
  {
    "id": "322",
    "text": "Introduction\nDe nos jours, les institutions étatiques tout comme les entreprises, s\u0027appuient souvent sur l\u0027opinion publique pour orienter leurs décisions stratégiques. L\u0027analyse automatique d\u0027opinions a ainsi connu une véritable envolée depuis l\u0027apparition des réseaux sociaux tels que Twitter. Selon Pak et Paroubek (2010), une opinion peut être soit positive, négative, ou neutre, ce qui revient à un problème de classification en 3 classes. Un second point de vue, que nous adopterons, consiste à considérer qu\u0027une opinion ne peut pas être neutre, seulement objective. Ainsi, le problème peut être décomposé en une évaluation de l\u0027objectivité dans un premier temps, suivie dans le cas d\u0027un texte subjectif d\u0027une seconde étape de détection de la polarité. L\u0027objectif de ces travaux, dans le cadre d\u0027un projet européen de tourisme, est de présenter une méthode pour détecter la subjectivité puis la polarité d\u0027un tweet anglais ou français. Dans la suite de cet article, nous présentons section 2 un état de l\u0027art sur la détection d\u0027opinions dans des textes. La section 3 décrit notre méthode de détection de subjectivité et son évaluation sur des tweets. La section 4 est dédiée à la détection de polarité. Enfin, la section 5 résume et analyse les résultats obtenus et propose des pistes pour nos travaux futurs.\nLa détection d\u0027opinions dans des textes\nLa détection de subjectivité et de polarité sont des tâches similaires pouvant être résolues par le même type de méthodes. Il en existe 3 à l\u0027heure actuelle : les méthodes symboliques, statistiques et hybrides.\nLes méthodes symboliques créent un ensemble de règles de décision servant à classer un texte dans une catégorie. Ces règles peuvent être éditées de manière manuelle (très coûteux) ou semi-manuelle par un expert humain et sont directement appliquées aux textes à évaluer. Les règles définies peuvent être plus générales (moins coûteuses) en utilisant par exemple des formules telles que des calculs de valences Baccianella et Sebastiani (2010);¸Serban et Pécuchet (2012). Ces approches ne permettent néanmoins pas la détection d\u0027opinions pour des structures linguistiques compliquées. Les méthodes statistiques s\u0027appuient sur un corpus de textes transformés en vecteurs de caractéristiques encodant les textes (comme le nombre d\u0027occurrences) afin d\u0027en extraire des propriétés par apprentissage. Le clustering ne nécessite pas de corpus annoté mais si les caractéristiques sont mauvaises, les classes obtenues ne correspondent pas aux classes désirées Hatzivassiloglou et McKeown (1997). Les méthodes telles que SVM nécessitent un corpus annoté mais sont plus efficaces. Pang et Lee (2008) a démontré que la seule présence des mots suffit à obtenir de bonnes performances.\nLes méthodes hybrides mélangent apprentissage statistique et édition de règles (le plus souvent manuelles). La sélection de motifs séquentiels les plus fréquents Serrano et al. (2012) peut ainsi être classée dans la catégorie des méthodes hybrides puisqu\u0027elle est constituée d\u0027une partie d\u0027extraction automatique de motifs les plus fréquents et d\u0027une partie de sélection manuelle des motifs les plus pertinents pour un contexte donné. Ces méthodes semblent efficaces à condition d\u0027effectuer les bonnes combinaisons.\nNous utiliserons SVM pour détecter la polarité puisque c\u0027est une méthode très efficace et que Go et al. (2009) propose une méthode de constitution automatique de corpus. Pour détecter la subjectivité, en revanche, ne disposant pas de ressource d\u0027annotation manuelle, nous utiliserons une méthode hybride sans corpus annoté : les motifs séquentiels fréquents.\nDétection de subjectivité par motifs séquentiels fréquents\nPour détecter la subjectivité, les motifs séquentiels les plus fréquents sont d\u0027abord extraits des tweets et, parmi eux, nous avons ensuite sélectionné manuellement ceux permettant de détecter la subjectivité. Si l\u0027un des motifs au moins peut s\u0027appliquer à un tweet, alors celui-ci est considéré comme subjectif.\nTrois concepts de base sont utilisés dans cette méthode : les items, les itemsets et les sé-quences. Les items synthétisent les différentes informations sur un mot (le mot lui-même, son lemme -en français -ou son stem -en anglais -et sa catégorie grammaticale). Pour diminuer le bruit induit par les urls et les hashtags, nous les avons remplacé par le terme « URL » et le terme « HASH » et avons rajouté une annotation au termes subjectifs en utilisant Sentiwordnet comme référence. Les itemsets sont des ensembles d\u0027items. Enfin les séquences sont un ensemble d\u0027itemsets représentant un tweet. Il est ensuite nécessaire de transformer un corpus de tweets subjectifs en un ensemble de séquences. Nous avons constitué un corpus en ne ré-cupérant que des tweets présentant un émoticône Go et al. (2009) (300 000 anglais et 300 000 français). Nous avons supprimé ceux ne possédant pas une entité nommée de type lieu touristique puisque le thème du projet est le tourisme. Nous obtenons 5 000 tweets pour l\u0027anglais et 4 000 pour le français. Nous avons annoté nos propres corpus de test pour ne pas biaiser la validation en utilisant le corpus d\u0027apprentissage puisque la méthode de Go et al. (2009) est naïve (700 pour le français et 800 pour l\u0027anglais).\n3 paramètres doivent être fixés pour affiner l\u0027extraction de motifs : le nombre d\u0027itemsets minimum et maximum à considérer pour générer les motifs (suffisamment grand pour obtenir des opinions et sans limite maximum), nous l\u0027avons fixé à 3 pour ne pas couvrir les émotions ; le support, c\u0027est à-dire la fréquence minimum d\u0027apparition des motifs dans le corpus (suffisamment grand pour ne pas obtenir des règles trop précises mais suffisamment petit pour obtenir un nombre raisonnable de règles), nous l\u0027avons fixé entre 100 et 200 pour obtenir 1000 motifs environ ; et enfin le gap (nombre d\u0027itemsets à ignorer lors de la génération des motifs. Un gap de 0 correspond à des itemsets situés côte-à-côte dans le texte).\nNous obtenons les meilleures performances avec un gap de 2, avec une précision moyenne de 64% et un rappel moyen de 62% pour l\u0027anglais et une précision moyenne de 65% et un rappel moyen de 66% pour le français. Barbosa et Feng (2010) ont comparé quatre méthodes de détection de subjectivité pour l\u0027anglais en utilisant le taux d\u0027erreur (nombre de textes mal classés sur nombre total de textes) comme métrique. Les performances se situent entre 18 et 32. Dans notre cas, nous obtenons 34 pour le français et 33 pour l\u0027anglais ce qui reste acceptable.\nDétection de polarité\nPour détecter la polarité, nous sommes partis d\u0027une méthode SVM avec un noyau non linéaire RBF afin de classer des textes. Nous avons choisi la présence des mots comme caractéristiques du vecteur puisque Pang et Lee (2008) la recommande. Chaque terme est stemmé (pour l\u0027anglais) ou lemmatisé (pour le français) et associé à sa catégorie grammaticale. Les URL et les hastags qui pourraient bruiter l\u0027apprentissage sont supprimés, ainsi que les déter-minants. En suivant la méthode de Go et al. (2009) basée sur la polarité des émoticônes, nous sommes parvenus à créer un corpus annoté. Nous avons utilisé leur corpus de 1,6 millions de tweets pour l\u0027anglais et avons constitué notre propre corpus français de 300 000 tweets. Pour valider la méthode, nous avons voulu éviter une validation croisée étant donnée que la méthode d\u0027annotation est naïve. Nous avons ainsi annoté nos propres corpus de test (700 tweets français et 800 anglais). Il a ensuite été nécessaire de sélectionner les termes à conserver dans l\u0027index. Nous avons commencé par conserver les 1 000 plus fréquents pour l\u0027anglais et les 10 000 plus fréquents pour le français. En effectuant un rapide test, nous avons pu nous apercevoir qu\u0027en augmentant ou en diminuant ce nombre, nous perdions en précision moyenne et en rappel moyen. Ensuite, en observant les tweets classés par le système, nous avons pu constater que la négation n\u0027est généralement pas prise en compte. Nous proposons une méthode alternative aux n-grams, moins efficaces que la seule présence des motsPang et Vaithyanathan (2002), basée sur la position absolue des mots dans la phrase. Nous choisissons d\u0027inscrire non plus la présence des mots dans l\u0027index mais la position de chaque terme pondéré par la taille du tweet. Par exemple la phrase « it is time now » aura 0.25 pour it (position 1, tweet de taille 4), 0.5 pour is (position 2), 0.75 pour time (position 3) et 1 pour now (position 4). L\u0027inconvénient est qu\u0027un même mot peut apparaître plusieurs fois dans la phrase. Il devra donc apparaître autant de fois dans l\u0027index.\nEn utilisant la seule présence des mots, nous obtenons 74% de précision moyenne et 73% de rappel moyen pour l\u0027anglais et 62% de précision moyenne et 61% de rappel moyen pour le français. Avec la position absolue, les performances diminuent à 60% pour la précision et le rappel en anglais. En revanche, les performances françaises sont équivalents : 62.6% de précision et 57% de rappel. Compte-tenu des résultats, il est probable que l\u0027information de position absolue soit trop précise et qu\u0027il faille plutôt considérer une position relative. En comparant avec Go et al. (2009), ceux-ci ont obtenu 82% d\u0027accuracy sur leur corpus de test anglais, dans notre cas, nous atteignons tout de même 73% sur un corpus de test différent.\nConclusion et perspectives\nDans cet article, nous avons proposé une méthode permettant de détecter la subjectivité et la polarité d\u0027un tweet. Dans le cas de la détection de subjectivité les performances sont correctes mais inférieures à l\u0027état de l\u0027art puisque les motifs retournés présentent un bruit important dû aux émoticônes et à la ponctuation. Il sera donc nécessaire de les supprimer du corpus. De plus, les mots subjectifs ont tendance à disparaître dans les motifs remplacés par leur catégorie grammaticale, ce qui ne permet pas une sélection manuelle efficace. Il sera donc nécessaire de mettre en place une méthode pour que ces termes apparaissent dans les motifs. Enfin, dans le cas de la détection de polarité, nous avons obtenu des performances raisonnables bien que légèrement inférieures à ceux de la littérature, ce qui peut s\u0027expliquer par une difficulté à sélec-tionner les paramètres optimaux du SVM. Une piste d\u0027amélioration consisterait à supprimer de l\u0027index les termes présents de manière équivalente dans les deux classes et d\u0027ajouter les termes subjectifs non conservés car moins fréquents, en utilisant Sentiwordnet par exemple.\n"
  },
  {
    "id": "323",
    "text": "Introduction\nDe nombreux domaines d\u0027application génèrent en permanence d\u0027énormes quantités de données. L\u0027un des défis essentiels auquel les approches de fouilles de flots doivent faire face est la détection de changement dans ces données. En effet, l\u0027information disponible dans les flots change et évolue au fil du temps et les connaissances acquises au préalable peuvent s\u0027avérer non représentatives des nouvelles données. Dans un contexte d\u0027apprentissage, cela se traduit par le fait que des classes ou des concepts sous représentés (resp. surreprésentés) peuvent apparaître surreprésentés (resp. sous représentés) après une période plus longue. Savoir détecter le plus tôt possible les réels changements dans le flot permet alors de pouvoir réévaluer automatiquement les apprentissages précédents et surtout garantir que la connaissance extraite à un moment donné est vraiment représentative des données disponibles sur le flot. Dans cet article, nous proposons une nouvelle méthode de détection de changement définie pour traiter des donnés qualitatives : CDCStream (Change Detection in Categorical data Streams). L\u0027une des originalité de notre approche est de pouvoir mettre en évidence de manière non supervisée les changements dans le flot des données en exploitant efficacement une partie de l\u0027historique. Le modèle retenu de description du flot est sous la forme de lots de données : lorsque qu\u0027un nouveau lot arrive, CDCStream construit un résumé informatif et calcule différents tests statistiques afin de vérifier si un changement a eu lieu dans la distribution des données.\nLe reste de cet article est organisé de la manière suivante. CDCStream est décrit dans la section 2 et des expérimentations menées dans la section 3. Nous concluons dans la section 4.\nL\u0027approche CDCStream\nNous considérons une représentation classique de flots de données sous la forme d\u0027un flot infini divisé en lots : S \u003d {S 1 , S 2 , ..., S n , ...}. En outre, nous considérons que chaque exemple appartenant au flot est défini sur un ensemble d\u0027attributs qualitatifs, i.e. chaque attribut X j est défini sur un ensemble discret de valeurs nominales. Tout d\u0027abord nous résumons les données qualitatives à l\u0027aide de la méthode DILCA proposée dans (Ienco et al., 2012) afin de résumer la distribution des données sous-jacente. Nous proposons ensuite de surveiller les statistiques extraites des lots en utilisant l\u0027inégalité de Chebyshev (Aggarwal, 2007).\nLe principe de DILCA est de regrouper des données qualitatives entre elles via un ensemble de matrices, une pour chaque attribut, où chaque matrice contient les distances apprises entre chaque paire de valeurs d\u0027un attribut spécifique. Dans notre contexte, considérons l\u0027ensemble F \u003d {X 1 , X 2 , . . . , X m } de m attributs qualitatifs pour le lot S i . |X i | désigne la cardinalité de l\u0027attribut X i . On note Y l\u0027attribut cible, un attribut spécifique dans F pour lequel nous devons calculer les différentes distances. DILCA calcule une distance basée sur le contexte entre n\u0027importe quelle paire de valeurs (y i , y j ) de l\u0027attribut cible Y sur la base de la similitude entre les distributions de probabilité des y i et y j , compte tenu des attributs de contexte, appelée C(Y ) ? F \\ Y . Pour chaque attribut de contexte X i il calcule la probabilité conditionnelle pour les deux valeurs y i et y j étant données les valeurs x k ? X i , puis il applique la distance euclidienne. La distance euclidienne est normalisée par le nombre total de valeurs considérées :\nPour sélectionner un ensemble pertinent et non redondant de caractéristiques, les auteurs de (Ienco et al., 2012) proposent d\u0027adopter FCBF, une approche de sélection d\u0027attributs initialement proposée par (Yu et Liu, 2003). A la fin du processus, DILCA renvoie un modèle de distance M \u003d {M X l | l \u003d 1, . . . , m}, où chaque M X l est la matrice contenant les distances entre toutes les paires de valeurs d\u0027attribut X l , calculées en utilisant l\u0027équation 1. Chaque matrice générée M X l présente des caractéristiques intéressantes : elle est symétrique, la diagonale contient forcément des 0 et chaque valeur est délimitée entre 0 et 1. En fait, cet ensemble de matrices constitue une information utile pour résumer la distribution sous-jacente. L\u0027ensemble M de matrices peuvent alors être agrégées en une seule mesure par la formule suivante, qui ne tient compte que de la partie triangulaire supérieure de chaque matrice :\nD. Ienco et al.\nCette formule correspond à un résumé de l\u0027ensemble du lot en tenant compte à la fois de la corrélation entre les attributs et de la distribution des valeurs des attributs.\nPour déterminer si une distribution d\u0027un lot s\u0027écarte ou non d\u0027une distribution initiale, nous utilisons des techniques de contrôles statistiques des processus (Gama et al., 2004) fondées sur l\u0027inégalité de Chebyshev (Aggarwal, 2007). Ce choix ne nécessite aucune hypothèse sur les distributions des données et permet de montrer qu\u0027une variable aléatoire prendra, avec une grande probabilité, une valeur relativement proche de son espérance :\nDefinition 1 (Inégalité de Chebyshev) Soit X une variable aléatoire avec espérance µ X et écart-type ? X . Alors, pour tout k ? R\nL\u0027inégalité nécessite de calculer la moyenne et l\u0027écart-type et il n\u0027est pas possible de stocker l\u0027intégralité de ces valeurs. Nous proposons donc d\u0027évaluer les changements à la volée et de ne remonter que les véritables modifications dans la distribution des données (en utilisant un mécanisme d\u0027oubli) ou de ne faire remonter que les simples fluctuations sous forme d\u0027alarmes.\nExpérimentations\nNous comparons les résultats et le comportement de notre algorithme, CDCS., avec la méthode RSCD proposée dans (Cao et Huang, 2013) et reconnue comme la méthode la plus efficace pour détecter des changements dans des flots de données qualitatives. Nous comparons également notre approche avec la méthode supervisée, SDrif tC, de (Gama et al., 2004) qui possède un mécanisme de détection de changement et de variation en fonction de la précision du classifieur. Les expérimentations menées avec les jeux de données suivants : Electricity, Forest, Airlines, KDD99 ont été réalisées avec la plateforme MOA (Bifet et al., 2010). Dans le tableau 1, nous pouvons constater que l\u0027approche supervisée n\u0027est pas meilleure que les approches non supervisées. Ceci est un résultat intéressant dans la mesure où SDrif tC est la seule méthode qui exploite la précision afin de prendre une \"décision de détection de changement\" pour ré-apprendre le modèle sous jacent. Nous pouvons également constater que, la plupart du temps, le classifieur appris avec notre méthode obtient de bien meilleures performances que les autres approches non supervisées. Nous présentons également le comportement de CDCStream et RSCD dans le tableau 2 où nous mesurons le pourcentage des changements détectés par les deux méthodes pour différentes tailles de lots. Nous pouvons constater que les deux méthodes ont des comportements très différents. Par exemple, si nous analysons les résultats de Electricity, pour une taille de lot de 50 le nombre de changements est similaire. Par contre lorsque les lots grandissent les tendances sont très différentes. En particulier, la taille du lot pour b \u003d 500 et b \u003d 1 000 a un impact fort sur le pourcentage de changements découverts par RSCD.\nConclusion\nDétecter les changements dans un flux de données qualitatives n\u0027est pas simple. Dans cet article, nous avons présenté un nouvel algorithme qui extrait des résumés des différents lots et \n"
  },
  {
    "id": "324",
    "text": "Introduction\nUn des enjeux de l\u0027amélioration du confort et de la sécurité des personnes dépendantes à domicile passe par la détection des anomalies dans le comportement.Pour cela on essaie d\u0027établir un modèle personnalisé des activités de la vie quotidienne (AVQ), grâce l\u0027utilisation de plateformes expérimentales (Smart Homes) équipées de divers capteurs domotiques telle la plateforme CASAS 1 . Cet outil permet la modélisation du comportement et le reconnaissance d\u0027actions Cook et al. (2013)  Soulas et al. (2013), ou la détection de danger comme la chute, en combinaison avec des capteurs portés par la personne Lustrek et al. (2012).\nDans le cadre de notre étude nous nous intéressons à la survenue brutale d\u0027un problème dans la vie quotidienne, et la dégradation lente du rythme de vie résultant d\u0027une pathologie. Pour détecter ces changements nous prenons pour source d\u0027information le temps de présence et d\u0027inactivité dans certains lieux du logement, comme la chambre à coucher, la cuisine, le séjour, la salle de bain et les toilettes. Ces informations sont extraites des données fournies par les détecteurs de mouvement installés dans chaque pièce du logement.\nL\u0027article Botia et al. (2012) traite de la surveillance de personnes âgées dans leur logement, et propose une approche basée sur la mesure du temps d\u0027inactivité entre deux déclenchements de capteurs de mouvement. La décision est prise en suivant des règles qui définissent les situations de danger, telle qu\u0027une durée d\u0027inactivité qui dépasse un certain seuil. Ces seuils sont déterminés par apprentissage pour chaque pièce du logement. Les résultats de l\u0027étude de Botia montrent qu\u0027il est possible de détecter un dépassement en fixant un seuil à partir d\u0027une moyenne glissante des valeurs maximales de temps d\u0027inactivité enregistrées au cours des jours précédents. Ils montrent aussi que la prise en compte des différentes jours de la semaine n\u0027apporte pas d\u0027amélioration. Par contre Botia ne tient pas compte de la variabilité de l\u0027activité de la personne au cours de la journée et de la nuit.\nDans ce travail nous faisons l\u0027hypothèse qu\u0027il faut prendre en compte les plages horaires pour modéliser l\u0027activité dans le logement. Nous réalisons une analyse des données issues de capteurs pour déterminer l\u0027impact de la plage horaire sur l\u0027activité dans le logement, et nous proposons une heuristique pour estimer les durées d\u0027inactivité autorisées.  tivités, et encore plus difficile d\u0027obtenir des informations sur une véritable situation de danger, autrement que par la simulation. Nous avons donc basé notre apprentissage des seuils sur la seule vérité terrain disponible, c\u0027est à dire sans exemple d\u0027anomalies. Nous avons estimé quel pourrait être à priori le nombre maximum de faux positifs à partir duquel les seuils de détec-tions devraient être calculés. L\u0027apprentissage des seuils utilise les 28 premiers jours du dataset, la détection du nombre de dépassements des seuils est effectuée sur les 192 jours restants. Le seuil pour chaque zone et chaque tranche horaire est déterminé à partir de l\u0027histogramme des durées d\u0027inactivités, et il est tel que 99% des durées d\u0027inactivité relevées sur la durée d\u0027apprentissage lui soient inférieures. Le choix de 99% permet d\u0027obtenir un nombre de fausses alarmes inférieur à une par jour en moyenne. Des essais avec différentes durées d\u0027apprentissage (7 , 14, 21, 28 , 60, 90,et 120 jours ) montrent que les taux de fausses détection diminuent avec la durée d\u0027apprentissage. Cependant dans la pratique pour un système réel fonctionnant sur ce principe, Il faut réduire la durée d\u0027apprentissage afin d\u0027activer la détection d\u0027anomalies.\nExtraction des caractéristiques\nLa figure 2 présente les seuils calculés sur une période d\u0027analyse de 28 jours. Les couleurs choisies (vert, jaune, rouge) illustrent la sensibilité du système et la potentialité d\u0027une situation de danger non détectable. En vert sont représentés les seuils de durée d\u0027inactivité inférieurs ou égaux à 30 minutes, en jaune les seuils égaux à 1 heure, et en rouge les seuils supérieurs ou égaux à 2 heures. On constate que cette carte des seuils est très informative, dans le sens où les seuils calculés ont un grande variabilité en fonction de la plage horaire et de la zone, et qu\u0027elle reflète la réalité des activités de la vie quotidienne. On remarque que certains seuils sont très élevés. En effet dans ces plages horaires il y a très peu d\u0027activité dans les plages concernées, et les durées d\u0027inactivité très longues ont beaucoup de poids sur le calcul du seuil.\nConclusion et perspective\nNous avons repris ici une méthode permettant de détecter une situation de danger en utilisant uniquement les informations provenant de capteurs de mouvement. Nous avons montré l\u0027intérêt d\u0027apprendre ces seuils en tenant compte des plages horaires et des zones dans lesquelles surviennent les différents évènements des capteurs. Le résultat de nos expérimenta-tions montre qu\u0027un danger potentiel demeure dans certaines zones et à certains horaires. La précision peut être améliorée en prenant en compte la topologie du logement, afin d\u0027éliminer tous les évènements incohérents qui se succèdent dans des zones non connexes. La grande variabilité des seuils en fonction des plages horaires nécessite de prendre en compte de manière fine le nombre de déclenchements, car si ce nombre est faible, l\u0027estimation d\u0027une espérance est fortement biaisée. Concernant l\u0027évolution du comportement au cours du temps, il semble né-cessaire d\u0027étudier par la suite l\u0027évolution du modèle du comportement de la personne, à l\u0027aide de dataset beaucoup plus longs, ou avec un système fonctionnant en situation réelle.\nSummary\nIn the next decades elderly care will become a major concern. The information technologies can improve the comfort and security of the dependent persons at home. In this paper we propose a method for dangerous situations detection based on automatic thresholding of the inactivity durations of passive infrared motion detectors. Our contribution consists in learning the thresholds automatically, depending on the occupied room and the daily hour. The method is evaluated on real data. It allows the emergency services to operate quicker.\n"
  },
  {
    "id": "325",
    "text": "Introduction\nL\u0027explosion récente des technologies mobiles et des données géo-référencées a fait émerger un nouveau type de données, qualifiées de géographiques. De ce fait, une prolifération des systèmes d\u0027informations géographiques (SIG) a vu le jour pour assurer une meilleure exploitation de ces informations. En fait, pour atteindre cet objective on doit disposer d\u0027un ensemble d\u0027informations complet et cohérent. Des moyens d\u0027enrichissement ont été proposés pour aboutir à l\u0027enrichissement des BDG à des coûts réduits. Dans ce contexte, on trouve à titre d\u0027exemple MetaCarta (www.metacarta.com) et PIV (Lesbegueries et al., 2006) qui opèrent l\u0027enrichissement en liant les documents textuels aux entités géographiques correspondantes. Ces travaux, ne proposent pas (ou peu) de moyens pour gérer les contenus textuels des documents. L\u0027outil SDET (Mahmoudi et Faïz, 2010) est un autre moyen d\u0027enrichissement qui cherche à exploiter le contenu textuel pour extraire l\u0027essentiel sous forme d\u0027un résumé. Le présent travail s\u0027inscrit dans le contexte d\u0027enrichissement des SIG tout en proposant une vue structurée des informations sous format de BDG. Concrètement, notre approche s\u0027articule autour de deux grandes phases : une correspondance du texte en schéma conceptuel et une génération de la BDG à partir du schéma suite à son remplissage.\nNotre approche\nLe processus général de génération d\u0027une structure sous forme de BDG à partir de texte vise un enrichissement de la BDG initialement incarnée dans le SIG. L\u0027idée est de chercher un ensemble d\u0027attributs pouvant compléter les données existantes. Notre approche se décompose\nen deux grandes phases (cf. figure 1). La première phase de notre processus d\u0027enrichissement consiste à la génération de la structure de la BDG à partir de textes. Cette phase est déclenchée par une analyse morphosyntaxique résultant en un texte taggé. Le texte prétraité subit par la suite une annotation permettant de repérer les phases cibles. Inspirée du modèle proposé dans (Gaio et Nguyen, 2011), nous avons pu identifier différentes formes syntaxiques de phrases renfermant des données géographiques. En adoptant la même représentation symbolique, nous avons dégagé le modèle présenté par la figure 2.\nFIG. 2 -Modèles des phrases à détecter dans le texte (avec * signifie que le composant correspondant peut être présent zéro ou plusieurs fois). Les autres composants doivent apparaître au moins une fois.\nLes phrases annotées seront injectées comme entrée de la phase d\u0027extraction des concepts géographiques. Pour pouvoir dégager les éléments de base du modèle classes/relations nous nous sommes appuyés sur un ensemble de règles. Ces dernières sont dégagées empiriquement suite à l\u0027étude de corpus textuels se rapportant aux données géographiques. Le tableau 1 décrit ces règles pour chaque élément du modèle.Une fois les données extraites, une étape de filtrage s\u0027avère nécessaire pour la suppression des redondances et des noms propres au niveau des classes et des attributs. La tâche de suppression des redondances nécessite la détermination des synonymes de chaque élément du modèle. Ceci nous a amené à exploiter le thésaurus N. Hassini et al.\nWordNet (Miller et al., 1990) pour accomplir cette tâche. Pour ce faire, nous avons appliqué un ensemble de règles illustrées par le tableau 1. La deuxième phase de notre approche consiste à fournir un corpus d\u0027apprentissage annoté préalablement pour pouvoir dégager les règles d\u0027annotation. Pour accomplir l\u0027apprentissage nous avons adopté l\u0027algorithme de SVM à marges inégales (Li et Shawe-Taylor, 2003). Par annotation nous entendons le repérage des données qui vont servir comme valeurs des attributs dont nous disposons. Les règles dégagées vont être appliquées au texte annoté à la première phase. Ceci résulte en un remplissage de la BDG préétablie. La base résultante et la BDG incarnée au SIG vont subir un appariement pour pouvoir ajouter tout attribut non présent dans la base initiale.\nImplémentation\nL\u0027approche détaillée tout au long de ce papier a fait l\u0027objet d\u0027une implémentation qui a pu aboutir à l\u0027outil GDB Generator. Etant donné que le but est d\u0027enrichir les SIG, cet outil se présente comme un Plugin développé en Java et intégré dans le système OpenJump (http ://www. openjump. org). Si l\u0027utilisateur a affaire à des informations non stockées dans la base initiale, il peut lancer notre outil. En backoffice l\u0027outil s\u0027appuie sur un corpus de textes relatifs à l\u0027entité en question. En utilisant l\u0027environnement d\u0027édition visuel intégré à GATE (http ://www.gate.ac.uk), notre outil débute par l\u0027annotation des éléments géographiques dans les textes. Ceci étant accompli en définissant des règles d\u0027annotation en langage de grammaire JAPE (http ://gate.ac.uk/sale/tao/splitch8.htm).\nUne fois extraits et filtrés, ces éléments forment le modèle conceptuel de la base à créer en adoptant le SGBD postgresql (www.postgresql.org). La détection des données qui serviront à remplir la base a été accomplie sous le système Gate en intégrant deux sessions de MachineLearning pour appliquer l\u0027apprentissage au corpus préalablement annoté et l\u0027application des règles d\u0027annotation générées à notre corpus. La base créée servira à enrichir la base existante par des nouveaux attributs selon les besoins de l\u0027utilisateur(cf. figure 3).\nConclusion\nNous avons présenté dans cet article une approche pour automatiser la création et le remplissage d\u0027une BDG pour enrichir les SIG. La mise en oeuvre de notre approche a donné naissance à l\u0027outil baptisé GDB Generator. Ce dernier a été intégré au SIG OpenJUMP. Comme perspectives, nous proposons d\u0027enrichir nos règles d\u0027annotation en prévoyant plus de formes syntaxiques. La prise en compte de la composante spatio-temporelle est prévue aussi comme extension de notre approche.\n"
  },
  {
    "id": "326",
    "text": "Introduction\nLes réseaux sociaux sont dynamiques par nature. La détection de communautés a longtemps considéré uniquement une vue statique : la capture du réseau à un instant t. Récemment, des travaux sur la dynamique des communautés ont vu le jour. Certains auteurs essayent de suivre l\u0027évolution des communautés durant plusieurs tranches de temps Palla et al. (2007), d\u0027autre proposent de mettre à jour les communautés existantes en fonction des nouveaux évé-nements qui se produisent (ajout ou suppression de noeuds et/ou de liens) Nguyen et al. (2011). Enfin les derniers essayent de trouver des communautés consistantes sur plusieurs tranches de temps Aynaud et Guillaume (2011).\nUn des problème non encore exploré dans la littérature sur la dynamique des communautés est la prédiction : connaissant l\u0027évolution du réseau jusqu\u0027au temps t, peut-on prédire les communautés au temps t + 1 ? Dans cet article, nous proposons une approche générale de pré-diction des communauté basée sur la prédiction des interactions dans les réseaux complexes. Dans cette approche, étant donné l\u0027évolution du réseau jusqu\u0027au temps t, les interactions sont prédites pour le temps t + 1 et les communautés sont ensuite calculées sur ce réseau prédit. L\u0027hypothèse qui soutient cette démarche est la suivante : si on est capable de prédire l\u0027évo-lution du réseau avec précision, alors on peut utiliser le réseau prédit pour d\u0027autres tâches de prédiction (ici la prédiction des communautés).\nDans la suite de cet article, nous présentons d\u0027abord la prédiction des interactions (section 2) puis nous présentons son évaluation et son application à la prédiction des communautés (section 3). Nous terminons par des conclusions et perspectives (section 4).\nPrédiction des interactions\nLe problème de prédiction des interactions peut être défini comme suit : étant donné un réseau dynamique G \u003d (G 1 , ..., G n ) dont les tranches de temps sont non cumulatives (les liens correspondent aux interaction de la tranche de temps uniquement) quelle sera la structure du réseau (G n+1 ) correspondant à la tranche de temps n + 1 ? Ce problème peut être vu comme une généralisation de la prévision de lien : on ne se limite pas aux liens non existants mais on vérifie aussi que les liens existants resterons présents. De ce fait, les mêmes classes de méthodes peuvent être utilisées pour le résoudre. Dans ce qui suit nous présentons une solution basée sur la similarité et une solution par apprentissage supervisée. Dans les approches proposées, le temps joue un rôle important.\nModèle basés sur la similarité\nDans cette approche une fonction de similarité est définie et sa valeur est calculée pour chaque paire de noeuds potentielle. Un seuil est ensuite choisi pour décider de la création des interactions. La forme générale de la mesure de similarité proposée est :\nDans l\u0027équation 1, les fonctions f et g et les paramètres ? et ? sont à définir. W est la matrice des poids. f est la fonction temporelle, elle permet de prendre en compte l\u0027âge des interactions (donner plus d\u0027importance aux relations récentes par exemple). g est la fonction de similarité topologique qui mesure la proximité dans le graphe social. h est la fonction de similarité entre les attributs (lorsqu\u0027ils sont disponibles). Enfin neigh(i) est une fonction de voisinage (les voisins, les voisins et leurs voisins, la communauté par exemple).\nCe modèle est très intuitif. Cependant, il ne peut pas modéliser une large classe de relations possible entre les variables d\u0027entrée et la variable à prédire. Pour cette raison, nous proposons dans la suite un modèle plus général basé sur l\u0027apprentissage supervisé. \nEvaluation et discussion\nLes jeux de données utilisés pour les évaluations sont DBLP et Facebook wall. DBLP est un réseau de collaborations entre auteurs indexés sur http://dblp.uni-trier.de/. Pour chaque tranche de temps (années), une interaction existe entre deux auteurs s\u0027ils ont au moins une publication commune. Les liens sont pondérés par le nombre de publications communes. Le jeu de données Facebook wall (http://konect.uni-koblenz.de/networks/ facebook-wosn-wall) est un réseau construit à partir d\u0027un sous-ensemble d\u0027utilisateurs de la New Orleans. Pour chaque année, une interaction existe entre deux utilisateurs si l\u0027un deux a publié sur le mur de l\u0027autre. Les liens sont pondérés par le nombre de publications.\nEn raison du déséquilibre entre les classes, l\u0027aire sous la courbe ROC (Receiver Operating Characteristic) notée AUC (Area Under Curve) est utilisée pour évaluer les performances des approches de prévision des interactions.\nLa table 1 présente les résultats de l\u0027évaluation de la prédiction des interactions. Dans le jeu de données DBLP, on a à disposition les titres des articles et les noms des conférences ou journaux dans lesquels ils sont publiés. Pour chaque auteur et chaque année, on peut donc construire un vecteur TFIDF(Term Frequency Inverse Document Frequency) relatif aux mots contenus dans les titres de ses publications et les noms des conférences et/ou journaux dans lesquels il a publié. Ce sont ces vecteurs qui sont utilisés comme attributs.\nEnfin, les communautés sont calculées en utilisant l\u0027algorithme décrit dans Ngonmang et al. Les résultats de l\u0027évaluation du jeu de données DBLP sont présentés sur la figure 3 (a). On peut constater que pour plus de 30% des noeuds on a une prédiction parfaite avec une valeur de N M I \u003d 1. Plus de 50% des noeuds produisent un NMI supérieur à 0, 8 et enfin, plus de 70% des noeuds produisent un N M I \u003e 0, 6. Un constat similaire peut être fait pour les résultats sur Facebook wall\u0027s présentés à la figure 3 (b).\nConclusions et perspectives\nRécemment, de nombreux travaux sur la détection des communautés dans les réseaux dynamiques ont commencés. Un des problèmes encore non exploré est la prédiction des communautés. Dans cet article, nous avons dans un premier temps proposé des modèles pour la prédiction des interactions (un basé sur la similarité et l\u0027autre par apprentissage supervisé). Ensuite, nous avons utilisé ces modèles pour prédire les communautés. Des tests sur des jeux de données réels montre la faisabilité de notre approche.\nEn perspective, nous pensons prendre en compte l\u0027arrivée de nouveaux noeuds.\nRemerciement\nCe travail est partiellement financé par le projet FUI français AMMICO. \nSummary\nIn this paper, we propose a general approach for communities prediction based on a machine learning model predicting interaction in social networks. In fact, we believe that if one is able to predict the structure of the network with a high precision, then one just need to compute the communities on this predicted network to have the prediction of the community structure. Evaluation on real datasets (DBLP and Facebook walls) shows the feasibility of the approach.\n"
  },
  {
    "id": "327",
    "text": "Introduction\nLa veille économique s\u0027inscrit aujourd\u0027hui pleinement dans la stratégie de développement des entreprises. Or, la quantité d\u0027informations à leur disposition est considérable, rendant l\u0027analyse complexe. L\u0027entreprise partenaire de ces travaux publie quotidiennement des articles synthétisant des informations économiques émanant de différentes sources ou fruit d\u0027une dé-marche d\u0027investigation. Afin de les adresser au mieux aux lecteurs concernés, nous dévelop-pons un outil efficace de recommandation de ces articles d\u0027actualités économiques régionales, reposant sur son adéquation avec les besoins des utilisateurs. Pour personnaliser la recommandation, une enquête a été menée auprès des clients avec l\u0027appui des experts du domaines. Cela nous a permis d\u0027identifier trois critères principaux sur lesquels nous nous appuyons : les Thèmes (principaux événements économiques), les Secteurs d\u0027activité et la Localisation des informations. Les besoins des utilisateurs et le contenu informationnel des articles sont repré-sentés par une description sémantique des connaissances de ces trois critères au sein d\u0027une ontologie. Dans cet article, nous nous intéressons à la distinction entre pertinence et similarité, qui sont souvent amalgamés. Nous proposons donc une nouvelle mesure, Relevancy Measure, nous permettant de définir la pertinence d\u0027un article pour un profil donné en nous appuyant sur leurs descriptions ontologiques. Nous utilisons un système de recommandation basé sur le contenu avec une approche vectorielle. Cet article est organisé de la façon suivante : nous commençons par présenter la génération des vecteurs, puis nous introduisons les notions de similarité et de pertinence et définissons la mesure Relevancy Measure. Enfin, la section 4 propose une évaluation de nos algorithmes avant la présentation de nos conclusions.\nVectorisation\nLa description de chaque article et profil est contenue dans une base de connaissances ontologique. Afin d\u0027utiliser le modèle vectoriel, il est nécessaire de transformer ces descriptions en vecteurs. Cette modélisation est bien moins expressive qu\u0027une ontologie car les dimensions étant orthogonales dans le modèle vectoriel, tous les éléments de chaque vecteur sont considé-rés comme indépendants (Voorhees, 1994).\nGénération des vecteurs : La mise en relation d\u0027un article avec les critères qui lui sont associés est réalisée de façon semi-automatique via la plate-forme GATE (Cunningham, 2002). Les résultats de l\u0027analyse du texte par cette plateforme sont vérifiés, corrigés et validés par les rédacteurs de chacun des articles puis remontés dans l\u0027ontologie. La création des profils est réalisée manuellement lors de l\u0027inscription des clients. Les vecteurs décrivant articles et profils contiennent les critères qui leur ont été associés dans la base de connaissances ontologique.\nExpansion des vecteurs : D\u0027après ce qui a été dit précédemment, les vecteurs de description des articles et profils ne contiennent que les instances de critères avec lesquels ils sont directement en relation dans la base de connaissances. Nous nous basons sur les apports de (IJntema et al., 2010), dont nous avons adapté la méthode dite d\u0027expansion de vecteurs afin de conserver les connaissances de l\u0027ontologie dans les vecteurs. Les instances de chaque critère sont organisées de façon hiérarchique dans la base de connaissances. Pour chaque instance ajoutée aux vecteurs, les instances parents y sont elles aussi ajoutées.\nPertinence et Relevancy : P ertinence(x, y) : I ×I ? [0, 1] est une fonction qui permet de mesurer le degré de pertinence d\u0027un article x vis-à-vis d\u0027un profil y. Cette mesure de pertinence doit aussi respecter les propriétés de positivité et réflexivité. La pertinence est une notion largement utilisée dans le domaine de la recherche d\u0027informations. Dans notre cas, la pertinence n\u0027est pas binaire. Un article peut plus ou moins correspondre au besoin d\u0027informations d\u0027un utilisateur, c\u0027est pourquoi nous utilisons le modèle vectoriel pour l\u0027estimer. Contrairement aux approches classiques confondant les notions de similarité et de pertinence (Salton, 1971), nous les distinguons. Par exemple, si un profil montre un intérêt pour la Côte d\u0027Or, il est préférable de lui recommander un article plus précis, qui traite de Dijon, qu\u0027un article plus général qui traite de la Bourgogne. Il faut donc conserver une pertinence forte dans le sens de la spécia-lisation et la baisser fortement dans le sens de la généralisation du critère. Afin de résoudre D. Werner et al.\nce problème, nous utilisons un vecteur intermédiaire. Le sous-vecteur ? ? s c est composé des instances communes entre le vecteur de l\u0027article ? ? a c et celui du profil ? ? p c . Ainsi, nous définissions la pertinence pour un critère donné c de la façon suivante :\nAvec S c le sous-ensemble commun d\u0027éléments de l\u0027ensemble d\u0027instances en relation à la fois avec le profil I 2,c \u003d 4, car nous considérons que la perte de précision du profil par rapport à l\u0027article ne doit pas influencer plus de 20% du résultat. De plus, la perte de précision de l\u0027article par rapport au profil doit influencer fortement le résultat, ici 80%. La pertinence globale Relevancy( ? ? a , ? ? p ) est la somme des mesures de pertinence pour chacun des critères, éventuellement pondérées. Cette mesure est utilisée dans notre prototype pour trier les résultats (articles) proposés à l\u0027utilisateur en fonction de son profil :\nExpérimentations\nNous avons comparé de deux façons différentes (évaluation binaire et d\u0027ordre) les résultats de la recommandation d\u0027articles via les méthodes, similarité cosinus (C), similarité cosinus avec vecteur étendus (B) et Relevancy Measure avec vecteur étendus (A), qui permet de gérer la différence de précision entre les profils et les articles. Pour nos évaluations, un ensemble de 10 profils et 70 articles a été choisi. Pour l\u0027évaluation binaire, une sélection manuelle des articles pertinents a été réalisée pour chaque profil par des experts. Pour l\u0027évaluation de l\u0027ordre, un classement manuel des articles pertinents a été réalisé pour chaque profil par des experts. Dans les deux cas, le travail des experts est considéré comme la recommandation idéale, avec laquelle sont comparés les résultats des différents algorithmes.\nEvaluation Binaire : Pour évaluer la recommandation binaire nous utilisons les mesures classiques en recherche d\u0027informations, précision, rappel et F1-mesure, (Lewis et Gale, 1994) qui produisent des scores allant de 0 à 1. Tous les articles dont la corrélation avec le profil est supérieure à un seuil de 0,5 sont conservés. Les résultats de l\u0027évaluation de la recommandation  Salton, G. (1971). The SMART retrieval system -experiments in automatic document processing.\nVoorhees, E. M. (1994). Query expansion using lexical-semantic relations. In SIGIR \u002794, pp. 61-69. Springer London.\nSummary\nToday in the commercial and financial sectors, staying informed about economic news is crucial and complex because of the huge amount of information. To address this problem, we propose an innovative article recommender system based on a knowledge ontological model. We present also a novel method to evaluate the relevancy based on vector space model that we have perfected to overcome the mix up existing in models between the concepts of similarity and relevancy.\n"
  },
  {
    "id": "328",
    "text": "Introduction\nUne grande quantité d\u0027information textuelle sur la musique peut être extraite du Web. On y trouve notamment des données générées par les utitisateurs finaux (p. ex. : tags, critiques), des métadonnées (p. ex. : date de sortie, nom du parolier ) et, enfin, les paroles des chansons. Ces informations peuvent souvent être moissonnées au moyen des API qu\u0027offre un nombre croissant de services musicaux sur le Web, de même qu\u0027avec l\u0027aide d\u0027outils développés par la communauté de chercheurs dans le domaine de la recherche d\u0027information musicale. Cependant, les paroles de chansons ont reçu relativement peu d\u0027attention de la part des développeurs des systèmes de repérage pour la musique. Or, la recherche à partir des thèmes abordés dans les paroles peut être pertinente dans certains contextes, pour les chercheurs s\u0027intéressant à la musique populaire ou pour toute personne souhaitant trouver une musique pour un événement particulier (mariage, funérailles). Nous avons donc construit un système d\u0027exploration d\u0027une collection de chansons à partir des paroles. Après la constitution du corpus à partir de données provenant de diverses sources sur le Web, nous avons utilisé des algorithmes de fouille de textes pour en détecter les structures thématiques puis développé une interface de visualisation afin de naviguer dans la collection. Dans cet article, nous expliquons comment les données ont été recueillies et décrivons les différents traitements ayant été appliqués. Nous présentons également l\u0027interface de visualisation qui en résulte.\nTravaux connexes\nÉtant donné l\u0027abondance d\u0027informations musicales disponibles en accès libre sur le Web, il n\u0027est pas surprenant de constater qu\u0027un grand nombre de chercheurs ont développé des outils afin de collecter ces informations afin de faciliter le repérage de la musique. Logan et al. (2004)   Kleedorfer et al. (2008) appliquent différentes techniques de fouille ainsi qu\u0027une factorisation en matrices non-négatives (NMF) pour créer des clusters à partir des paroles, avec l\u0027objectif de permettre l\u0027exploration d\u0027une collection de chansons. La visualisation de corpus musicaux a aussi intéressé des chercheurs : plusieurs ont développé des représentations visuelles statiques pour l\u0027exploration de collections où l\u0027espace de représentation est organisé à l\u0027aide des cartes auto-organisatrices de Kohonen. Pour ce faire, la similarité entre chansons est calculée sur la base des fichiers audio (voir Islands of Music (Pampalk, 2001) et MusicRainbow (Pampalk et Goto, 2006)) ou des paroles dans le cas de SongWords, une application pour tablette tactile Baur et al. (2010). Cependant, l\u0027analyse des paroles se limite uniquement à une approche par Tf * idf. En conséquence, il semble pertinent de travailler au développement d\u0027interfaces pour l\u0027exploration de collections de chansons à partir des paroles afin de répondre aux besoins décrits précédemment.\nTraitement des données\nDans cette section, nous présentons la méthodologie utilisée pour le traitement des données.\nFiltrages et prétraitement linguistiques\nDifférents prétraitements linguistiques sont effectués sur les données : conversion des majuscules en minuscules, retrait des chiffres et des nombres (numériques et textuels), des accents et des symboles. Afin d\u0027éviter l\u0027introduction de bruit dans le modèle, nous utilisons un antidictionnaire classique enrichi de termes extraits de la langue populaire du Québec et de France que l\u0027on retrouve fréquemment dans les chansons (p. ex. : « té » (tu es), « chu » (je suis), « c\u0027te » (cette)). Nous appliquons finalement un processus de lemmatisation simple afin de ré-duire considérablement les dimensions de l\u0027espace tout en augmentant la fréquence des termes canoniques. Les premiers tests ont cependant montré que, malgré ces traitements, la taille du lexique était toujours importante. Nous présentons donc deux méthodes de sélection : à l\u0027aide de l\u0027analyse sémantique latente (LSA), afin de retenir uniquement les termes les plus repré-sentatifs, et en effectuant une sélection drastique consistant à ne retenir qu\u0027un pourcentage des termes les plus fréquents.\nClustering des données\nAprès l\u0027étape de prétraitement, nous appliquons un algorithme de K-moyennes afin d\u0027identifier des clusters de chansons partageant des descripteurs similaires. L\u0027évaluation des résultats d\u0027algorithmes de clustering reste encore aujourd\u0027hui une problématique ouverte importante, particulièrement lorsqu\u0027il n\u0027existe pas de référence. Nous avons évalué le clustering à l\u0027aide de la mesure Silhouette (Rousseeuw, 1987), laquelle permet de mesurer la cohésion ainsi que la distinction des clusters. Nous avons fait varier le nombre de clusters, la mesure de distance (e : Euclidienne, b : Manhattan) ainsi que la méthode de sélection des descripteurs. La baseline est calculée en faisant un tirage aléatoire. Les résultats de l\u0027évaluation (tableau 1) sont assez TAB. 1 -Évaluation du clustering à l\u0027aide de la mesure Silhouette faibles, suggérant des clusters proches les uns des autres et une certaine confusion sur le plan de la classification des chansons. La mesure Silhouette ne permet cependant pas de prendre en compte les chevauchements thématiques ni des spécificités du traitement de données textuelles. La méthode de sélection par LSA présente néanmoins des résultats intéressants étant donné le faible nombre de descripteurs (environ une centaine). Compte tenu des résultats précédents, nous avons choisi pour la suite des expériences de ne retenir que 2,5 % des termes les plus fréquents, fixé à 5 le nombre de clusters et utilisé une distance euclidienne. Une fois l\u0027étape de clustering terminée, nous utilisons le framework Gensim afin d\u0027indexer chaque cluster sépa-rément et de transformer chaque sous-collection en un modèle LSA (Deerwester et al., 1990). Cette transformation permet de récupérer les mots les plus représentatifs pour chaque cluster, que nous appellerons par la suite les mots-clés thématiques, qui peuvent apparaître comme les thèmes associés à chaque cluster. Enfin, nous calculons la distance entre chaque chanson et ces mots-clés, ainsi qu\u0027entre les chansons elles-mêmes avec un seuil minimum (0,4)  \nVisualisation des données\nPlusieurs outils ont été élaborés pour permettre la visualisation des réseaux, tel que Gephi (Bastian et al., 2009) ou Tulip (Auber, 2003). Dans le cadre de ce projet, nous avons utilisé Gephi, un logiciel libre flexible, puissant et particulièrement adapté pour mettre en lumière la structure des associations entre les noeuds d\u0027un réseaux ou d\u0027un graphe. Afin de produire une visualisation des données, nous avons transformé le modèle en un ensemble de noeuds (mots-clés thématiques, artistes et chansons) et de liens tels que :\n• Chaque mot-clé thématique est connecté aux chansons du cluster avec un poids déter-miné à l\u0027aide du cosinus.\n• Chaque chanson est connectée aux autres chansons avec un poids déterminé à l\u0027aide du cosinus (la connexion n\u0027est retenue que si le poids est supérieur au seuil de 0,4).\n• Chaque artiste est connecté à ses chansons ainsi qu\u0027aux différents mots-clés thématiques en fonction du nombre d\u0027occurrences de ceux-ci dans les chansons. Une fois les relations entre tous les éléments définies, une spatialisation est effectuée avec l\u0027algorithme Force Atlas de Gephi. Le résultat est exporté par la suite vers l\u0027interface Web.\nInterface de visualisation\nLa version actuelle de l\u0027interface contient 4 579 noeuds et 7 789 liens. Les points rouges représentent les mots-clés thématiques. La couleur des noeuds représentant chaque chanson ainsi que celle des liens est déterminée en fonction de son cluster d\u0027appartenance. La sélec-tion d\u0027une chanson ouvre un panneau latéral avec les métadonnées (le nom du chanteur, la couverture de l\u0027album, etc.) ainsi qu\u0027un nuage des termes les plus fréquents de cette chanson. Par ailleurs, les utilisateurs peuvent sélectionner n\u0027importe quel noeud pour obtenir une vue détaillée ou encore zoomer sur une zone particulière. Il est également possible d\u0027effectuer des recherches par mots-clés ou de visualiser chaque cluster séparément. Le système filtre alors la vue courante afin d\u0027afficher le noeud correspondant ainsi que tous les autres noeuds auxquels il est lié. La figure 1 présente une vue générale de la visualisation. Comme mentionné précédem-ment, la collection a été divisée en 5 clusters. Le tableau 2 présente les mots-clés thématiques pour chacun des clusters : Notamment, on remarque que le premier cluster semble réunir des chansons qui invitent à danser, bouger et chanter. On y trouve entre autres les chansons « Tu vas au bal » et « Père Noël noir », deux chansons à la fois drôles et entraînantes de Renaud. Le deuxième cluster regroupe plutôt des chansons sur l\u0027amour, par exemple la chanson « Mistral gagnant » de Renaud qui parle de l\u0027amour d\u0027un père pour sa fille. En revanche, les mots clés thématiques « revenir » et « regretter » du cluster bleu suggèrent des chansons à propos d\u0027amours déçues ou imaginaires. On y retrouve ainsi la chanson « Me jette pas » de Renaud. \nConclusion et perspectives\nDans cet article, nous avons présenté une méthodologie de fouille de textes permettant d\u0027indexer et de visualiser une collection de chansons. Ce projet est basé sur l\u0027hypothèse que l\u0027analyse, l\u0027organisation et la visualisation des paroles de chansons peuvent permettre aux utilisateurs de naviguer efficacement dans le contenu d\u0027une grande collection musicale. Notre système trouve des applications dans une grande variété de contextes. Nous prévoyons ainsi proposer une visualisation des chansons par décennie et par pays afin de permettre aux chercheurs de comparer les thèmes principaux abordés dans les chansons en fonction de l\u0027époque ou de l\u0027origine de la chanson. En combinant l\u0027analyse thématique des paroles au genre du chanteur, il serait possible aux chercheurs de répondre à une question telle que « Les thèmes abordés dans les chansons par les artistes masculins et féminins diffèrent-ils ? ». Nous prévoyons par ailleurs continuer à augmenter la taille du corpus et évaluer l\u0027ergonomie de l\u0027interface avec des chercheurs s\u0027intéressant à la musique populaire.\nSummary\nIn this paper, we present a text mining methodology and an information visualization interface that allows users to browse a large collection of French-language songs based on lyrics. We first harvested lyrics and metadata from various sources on the Web. After data preprocessing, we used clustering and Latent Semantic Analysis to identify a thematic structure and determine significant features. We then transformed the resulting model into a set of nodes and edges to obtain an interactive visualization system for the exploration of our song collection.\n"
  },
  {
    "id": "329",
    "text": "Introduction\nDans la plupart des pays développés, des bases de données géographiques (BD) sont initiées, même si certaines régions ne sont couvertes que partiellement. En particulier, des BD d\u0027occupation du sol (OCS) à grande échelle sont en cours de réalisation, par agrégation de données existantes et leur assemblage ne permet pas une description complète du territoire. D\u0027autre part, des images satellite de résolution sub-métrique couvrent avec une grande préci-sion géométrique de larges territoires et peuvent donc aider à compléter ces BD d\u0027OCS. Les méthodes de classification supervisée sont largement utilisées pour résoudre ce genre de problème de télédétection (Mountrakis et al., 2011). Cependant, elles sont souvent peu robustes, présentent un fort taux de confusion, sont limitées à certaines thématiques. Enfin, elles né-cessitent une sélection manuelle des classes et des zones d\u0027apprentissage afin de traiter le cas de classes composées de différentes apparences. Nous proposons ainsi ici une méthode d\u0027inspection hiérarchique d\u0027une BD existante, permettant d\u0027apprendre indifféremment la (ou les) apparence(s) de chaque thème qui la constitue. Ces informations sont ensuite fusionnées à différents niveaux afin d\u0027obtenir des résultats plus robustes.\nMéthodologie\nLa structure hiérarchique des BD géographiques permet de faire ressortir trois niveaux possibles d\u0027inspection : (1) le niveau objet, (2) le niveau thème, (3) le niveau BD. La BD initiale est projetée sur l\u0027image, formant une carte qui lui est superposable et où les pixels sont étiquetés par un thème : un pixel de cette grille ne peut avoir qu\u0027une seule étiquette. Cependant, il peut aussi n\u0027appartenir à aucun thème (\"non étiqueté\") et leur étiquetage est l\u0027objet de cette étude. Notre méthode est fondée sur le principe d\u0027une inspection hiérarchique, de manière ascendante. D\u0027abord, à chaque objet de la BD est associée une classification de toute l\u0027image. Puis, les classifications sont fusionnées au niveau du thème et la décision finale d\u0027éti-quetage est prise pour chaque pixel de l\u0027image (Pal, 2008). Le premier niveau d\u0027inspection permet d\u0027apprendre l\u0027apparence de chacun des objets. Il est composé de deux étapes : (1) une sélection, d\u0027un sous-ensemble de pixels intérieurs et d\u0027un extérieur qui permet de discriminer le mieux l\u0027objet du reste de l\u0027image et (2) une classification des pixels de toute l\u0027image en deux classes (intérieur / extérieur). La sélection des sous-ensembles de pixels est fondée sur la maximisation du rappel d\u0027une classification à deux classes des pixels de l\u0027objet. L\u0027étape de classification permet d\u0027obtenir pour chaque objet de chaque thème de la BD, une carte d\u0027appartenance au thème de l\u0027objet. L\u0027étape de fusion par thème permet de prendre en compte les différentes apparences d\u0027un thème et de ne privilégier aucune apparence. À cette étape, toutes les classifications calculées au niveau de l\u0027objet sont fusionnées par thème, afin d\u0027obtenir une seule carte d\u0027appartenance. On la considère comme la probabilité de chaque pixel de l\u0027image d\u0027appartenir au thème courant. Enfin, une décision d\u0027étiquetage est prise pour chaque pixel de l\u0027image en intégrant les résultats de toutes les cartes d\u0027appartenance de chaque thème. Une nouvelle classification est obtenue, permettant d\u0027associer à chaque pixel une étiquette de la BD initiale. Cette classification est accompagnée d\u0027une mesure de confiance dans la classification, dérivant des mesures d\u0027appartenance par thème, qui permet de définir des zones d\u0027incertitudes pouvant être étudiées à posteriori par un opérateur.\nExpérimentation\nUne vérité terrain de qualité est difficile à obtenir sur des bases de données couplées à des images satellites, ainsi nous avons décidé de générer une image à partir d\u0027une image réelle (image Pléiades à 50 cm de résolution), pour laquelle on connaît a priori le thème d\u0027appartenance (par rapport à une nomenclature donnée). L\u0027image reconstituée est composée d\u0027échan-tillons d\u0027une image Pléiades réelle (Fig. 1a), étiquetés en 9 thèmes eux-mêmes comprenant 10 objets (Fig. 1b). Les objets (100 pixels de côté) sont répartis sur une grille régulière. La diagonale de l\u0027image correspond à une zone non étiquetée dans la BD initiale, elle est composée de pixels pouvant appartenir à différents thèmes présents ou non dans la BD. Les 9 thèmes sé-lectionnés sont présentés dans la figure 1c. Cela correspond à une nomenclature très détaillée (thèmes aux apparences très proches) et à notre connaissance, aucun article n\u0027a jusqu\u0027à présent tenté d\u0027aborder un tel discernement. Afin de comparer notre méthode à l\u0027existant, et d\u0027étudier l\u0027influence du nombre d\u0027objets par thème de la BD, trois méthodes ont été appliquées sur diffé-rentes BD initiales (de 1 à 9 objets par thème) : (a) un SVM multi-classes, (b) les RF (Breiman, 2001) et (c) notre méthode. Les résultats des classifications pour la BD contenant 9 objets par thèmes sont visibles sur les figure 1d,e \u0026 f. La courbe de l\u0027évolution du score de bonne classification (pourcentage de pixels bien classés) en fonction du nombre d\u0027objets présents dans la BD simulée est présentée dans la figure 2. Les résultats de notre méthode montrent qu\u0027avec seulement 10% des objets dans la base, un taux de bonne classification supérieur à 70% des pixels est obtenu. Ce score atteint les 80% dès que l\u0027apprentissage est réalisé sur 30% des objets. Les trois classifications sont visuellement proches. Cependant, on peut noter que certains A. Gressin et al. thèmes sont confondus dans le cas des RF, alors qu\u0027ils sont bien dissociés avec les SVM. Notre méthode donne des résultats moins bruités que les RF, mais plus confus que les SVM. L\u0027évolu-tion du score de bonne classification en fonction du nombre d\u0027objets dans la BD initiale, pour chacune des trois méthodes comparées, est montrée sur la figure 2. Les résultats de notre mé-thode sont de meilleure qualité que les RF, quel que soit le nombre d\u0027objets dans la BD initiale. Les SVM obtiennent des résultats en moyenne légèrement supérieurs, mais sont moins stables que notre méthode (e.g. score de bonne classification inférieur pour trois objets), en particulier quand le nombre d\u0027objets est faible. Cette instabilité peut être expliquée par le choix des pixels d\u0027apprentissage par tirage aléatoire.  \nConclusion et perspectives\nDans cet article, nous avons présenté une méthode tirant profit d\u0027une BD existante, pour apprendre les différentes apparences de chaque thème qui la compose. Ce processus a été appliqué avec succès à un jeu de données complexe simulé et comparé favorablement à plusieurs méthodes standards de classification supervisée. D\u0027autre part, ce procédé a été développé dans un cadre plus général de détection de changements et de mise à jour de bases de données (Gressin et al., 2013) \n"
  },
  {
    "id": "330",
    "text": "Introduction\nLes graphes orientés sont des structures adaptées à la modélisation d\u0027un grand nombre de données complexes présentes dans le monde réel. Les réseaux de régulation génétique, par exemple, sont des graphes orientés où les noeuds représentent des gènes et les arcs, des relations d\u0027inhibition ou d\u0027activation. Le Web est modélisé par un graphe orienté dans lequel les noeuds sont des pages et les arcs des liens hypertextes. La circulation des emails dans une organisation, les graphes sociaux dans lesquels un individu peut suivre les activités d\u0027autres personnes, les réseaux de citations dans lesquels un article cite d\u0027autres articles sont également représentés par des graphes orientés.\nDu fait de ces nombreuses applications, ces structures sont beaucoup étudiées en théorie des graphes, et, plus récemment, dans le contexte de la fouille de données. La plupart des travaux portent sur des graphes non étiquetés ou dont les noeuds ne sont associés qu\u0027à une unique étiquette. Cependant, dans les graphes modélisant des données réelles, les noeuds, qui représentent des objets, peuvent être associés à de nombreuses caractéristiques. Les graphes dans lesquels les noeuds sont annotés avec des ensembles d\u0027attributs (ou itemsets) sont appelés graphes attribués et à l\u0027heure actuelle, encore peu de travaux se consacrent à leur analyse.\nL\u0027un des problèmes phares abordés dans la fouille de graphes étiquetés concerne l\u0027identification de sous-graphes qui apparaissent avec une fréquence minimum spécifiée par l\u0027utilisateur. Ces motifs récurrents peuvent apporter un éclairage particulier sur la manière dont les noeuds sont organisés. Fouiller des graphes attribués est d\u0027un grand intérêt, car cela permet d\u0027identifier des motifs structurels, mais également, de mettre en évidence des liens entre les attributs des noeuds.\nDeux raisons principales rendent la fouille des graphes attribués très difficile. En premier lieu, il est nécessaire de combiner de manière non triviale l\u0027exploration de la structure du graphe avec l\u0027identification d\u0027itemsets fréquents associés aux noeuds. La seconde raison est que, comme pour la fouille des graphes étiquetés, les performances sont très impactées par la présence de sous-graphes isomorphes (Yan et Han, 2002). Comme indiqué par (Yan et Han, 2002), dans le cas de graphes creux avec des labels diversifiés, le nombre de sous-graphes isomorphes est la plupart du temps faible. Cela n\u0027est plus le cas dans le cas des graphes attribués où la combinatoire sur les itemsets génère fréquemment un nombre important d\u0027isomorphismes de sous-graphes.\nLes contributions clés de notre travail sont les suivantes : i) nous présentons le problème de la fouille de structures fréquentes dans un ou plusieurs graphes attribués orientés, puis, ii) nous montrons comment l\u0027identification des motifs fréquents peut être réalisée en adoptant une stratégie d\u0027exploration de l\u0027arbre couvrant de chaque noeud. iii) Nous définissons une nouvelle forme canonique adaptée au parcours de l\u0027espace de recherche, iv) nous détail-lons la manière d\u0027étendre les motifs avec des arcs réentrants et v) nous traitons de la prise en compte des cycles. vi) Nous analysons le cas particulier des motifs automorphes et nous montrons comment l\u0027explosion combinatoire provoquée par l\u0027isomorphisme de sous-graphes peut être considérablement limitée. Avant de conclure, vii) nous présentons les résultats obtenus en analysant plusieurs jeux de données artificiels et réels avec l\u0027algorithme AADAGE (Automorphism Aware Directed Attributed Graph Explorer) que nous avons développé.\nEtat de l\u0027art\nDe nombreux algorithmes ont été proposés pour fouiller des graphes étiquetés. La quasitotalité des solutions proposées découlent de techniques développées pour la fouille d\u0027itemsets et sont basées sur une même idée qui consiste à explorer l\u0027espace des solutions en faisant grossir des sous-graphes candidats et en éliminant les motifs infréquents (Agrawal et al., 1993;Jiang et al., 2013). Là où les méthodes se différencient, c\u0027est dans leur manière de parcourir l\u0027espace de recherche et d\u0027éviter la génération de solutions redondantes. En effet, contrairement à la fouille d\u0027itemsets où il est facile de générer toutes les solutions de manière unique, dans la fouille de graphes, il est très fréquent de construire plusieurs fois le même motif.\nL\u0027une des méthodes privilégiées pour éviter d\u0027explorer plusieurs fois une même partie de l\u0027espace de recherche est d\u0027ajouter des arcs et des noeuds aux motifs candidats de manière ordonnée et d\u0027utiliser une représentation canonique des sous-graphes. L\u0027idée sous-jacente à l\u0027utilisation des formes canoniques est de n\u0027étendre qu\u0027une seule des solutions isomorphes. Pour éviter d\u0027effectuer des tests coûteux d\u0027isomorphisme complets, une représentation canonique adaptée à la manière d\u0027étendre les sous-graphes est choisie. Cette forme canonique permet de décider rapidement si un motif a déjà été examiné et donc, ne doit pas être étendu. La solution la plus utilisée est de créer une séquence de tous les arcs contenus dans le sous-graphe dans l\u0027ordre ou les arcs ont été ajoutés (Borgelt, 2007;Yan et Han, 2002). L\u0027utilisation des formes canoniques, si elle permet d\u0027éviter d\u0027explorer plusieurs fois le même sous-graphe, ne dispense pas de la nécessité d\u0027identifier toutes les manières de générer les motifs. Ainsi, si l\u0027on ordonne les noeuds dans l\u0027ordre lexicographique de leurs étiquettes, le motif de la figure 1(b), est canonique, mais il est présent 4 fois dans le graphe initial (a). Les motifs (c) et (d), qui sont eux aussi canoniques, peuvent être générés de 12 façons différentes (dans le graphe initial, il y a 12 manières possibles de choisir 2 noeuds b parmi les 4 fils étiquetés b du noeud a). Il est nécessaire de tenir compte de toutes ces formes car la configuration choisie conditionne les extensions futures du motif ; par exemple, la possibilité ou non d\u0027étendre chacun des noeuds b avec un noeud étiqueté c. Les motifs qui possèdent de nombreux isomorphismes de sous-graphes avec le graphe analysé posent des difficultés à tous les algorithmes existants, car le problème d\u0027isomorphismes de sous-graphes en lui même est NPcomplet. Les motifs (c) et (d), qui possèdent des automorphismes posent le plus de problèmes et contribuent de manière importante à la dégradation des performances des algorithmes, car le nombre d\u0027isomorphismes de sous-graphes avec le graphe initial est augmenté.\nDans le cas des graphes étiquetés creux, ce nombre reste toutefois relativement raisonnable. Au contraire, dans les graphes attribués, puisque plusieurs attributs sont associés aux noeuds, la probabilité d\u0027observer des sous-graphes partageant un ou plusieurs attributs identiques est grandement augmentée. Le problème devient alors très présent. Une manière de le contourner est d\u0027éliminer du graphe les items les plus fréquents mais l\u0027on se prive de tous les motifs contenant cet attribut, y compris certains qui peuvent potentiellement être intéressants.\nCertaines études traitent spécifiquement de l\u0027analyse des graphes dans lesquels les noeuds sont associés à des itemsets comme les travaux de Miyoshi et al. (2009) ou ceux de Fukuzaki et al. (2010). Les problèmes abordés dans ces études sont différents du notre dans le sens où ils traitent de l\u0027analyse de graphes non orientés et ont pour but de rechercher des sous-graphes partageant les mêmes itemsets.\nDans notre travail, nous cherchons à identifier la présence récurrente de certains attributs associés à des noeuds connectés. Dans ses objectifs, notre travail est plus proche de la fouille de séquence ou d\u0027arbres attribués. Dans une précédent étude Pasquier et al. (2013), nous avons travaillé sur l\u0027identification de sous structures reflétant des évolutions d\u0027itemsets. Cependant, l\u0027algorithme que nous avions proposé ne s\u0027appliquait que sur des jeux de données prenant la forme d\u0027arbres attribués.\nConcepts généraux et définition du problème\nDans cette section, nous introduisons les concepts et définitions nécessaires et présentons le problème de la fouille de graphes attribués orientés.\nPréliminaires\nSoit I \u003d {i 1 , i 2 , .., i n } un ensemble d\u0027items. Un itemset est un ensemble P ? I. Nous considérons que les items appartenant à un itemset sont triés selon l\u0027ordre lexicographique et peuvent être accédés par leur index (p.ex. I 2 pour référencer le deuxième item de I).\nUn graphe attribué orienté\nLa notation pow(I) représente l\u0027ensemble des parties de I. Pour un arc (u, v) ? E, u est le parent de v et v est le fils de u. Il existe un cycle dans le graphe orienté si un chemin peut être trouvé entre un noeud et lui même. Un graphe attribué orienté est enraciné si l\u0027on peut trouver un noeud v tel qu\u0027il existe un chemin entre v et n\u0027importe quel autre noeud du graphe.\nDeux graphes attribués\nChaque graphe attribué G 1 qui est isomorphique à G définit un plongement (embedding en anglais) de G 1 dans G. A noter que, dans certains cas, G 1 peut être plongé plusieurs fois dans G en effectuant des permutations dans la mise en correspondance des noeuds.\nDéfinition du problème\nLes données à analyser sont disponibles soit sous la forme d\u0027une base de données B de graphes orientés attribués, soit sous la forme d\u0027un unique graphe orienté attribué G, non nécessairement connexe. Dans le premier cas, il est possible de calculer, pour chaque motif identifié P , une fréquence par transaction qui est donnée par le nombre de graphes dans B pour lesquels P est un sous-graphe attribué. Dans le second cas, nous utilisons la mesure de fréquence définie par Bringmann et Nijssen (2008) qui est égale au nombre minimal d\u0027occurrences dans G de chacun des noeuds du motif. Nous pouvons avec cette mesure calculer une fréquence relative qui est donnée par la fréquence absolue divisée par le nombre de noeuds du graphe. Nous considérons qu\u0027un motif est fréquent si sa fréquence est supérieure ou égale à un seuil minimum. Le problème consiste à énumérer tous les motifs fréquents présents dans un jeu de données.\nLa forme canonique que nous utilisons n\u0027est pas basée sur une séquence d\u0027arcs, mais sur une liste de noeuds dans l\u0027ordre dans lequel ils ont été ajoutés à l\u0027arbre couvrant du sousgraphe en effectuant un parcours en profondeur d\u0027abord. Pour identifier de manière non ambiguë chaque arbre couvrant, il est suffisant d\u0027identifier chaque noeud par ses attributs et sa profondeur. Comme nous devons également représenter les liens réentrants, nous utilisons un troisième attribut qui contient l\u0027index du noeud pointé si celui-ci est déjà présent dans le motif.\nOrdre portant sur les itemsets\nDans notre application, les noeuds sont associés à des itemsets et il convient de définir un ordre total sur les itemsets. Etant donné deux itemsets, I et\nOrdre portant sur les noeuds\nLe code d\u0027un noeud est un triplet (d, Q, p) où d représente la profondeur du noeud dans l\u0027arbre couvrant, Q représente l\u0027ensemble des items associés au noeud et p vaut 0 si le noeud est utilisé pour la première fois ou est égal au numéro d\u0027ordre du noeud si celui-ci est déjà présent dans le motif. Pour comparer deux noeuds, il suffit de comparer leurs codes sous forme de triplets. Soit \nOrdre portant sur les sous-graphes attribués\nÉtant donné un motif M de racine r(M ), nous définissons un code sur la racine de ce motif code(r(M )), obtenu en concaténant les codes des noeuds présents dans le motif dans l\u0027ordre dans lequel ils ont été ajoutés. La forme canonique d\u0027un sous-graphe attribué est déterminée simplement en sélectionnant le plus petit code.\nSoit deux motifs M 1 et M 2 représentés respectivement par les codes\nn 2 }, on dit que C 1 \u003c C 2 ssi l\u0027une des assertions suivantes est vraie : (i)\nLa figure 2 présente quatre motifs isomorphes d\u0027un même graphe orienté attribué. Leurs codes sont respectivement représentés par les séquences suivantes :\ncode(r(M 1 )) \u003d (0,a,0) (1,cd,0) (2,cd,0) (2,cd,0) (1,cd,4) (1,cd,0) (2,cd,0) code(r(M 2 )) \u003d (0,a,0) (1,cd,0) (2,cd,0) (2,cd,0) (1,cd,3) (1,cd,0) (2,cd,0) code(r(M 3 )) \u003d (0,a,0) (1,cd,0) (2,cd,0) (2,cd,0) (1,cd,0) (2,cd,0) (1,cd,4) code(r(M 4 )) \u003d (0,a,0) (1,cd,0) (2,cd,0) (2,cd,0) (1,cd,0) (2,cd,0) (1,cd,3)\nEn utilisant l\u0027ordre des triplets définie précédemment, on trouve que code(r(M 4 )) \u003c code(r(M 3 )) \u003c code(r(M 2 )) \u003c code(r(M 1 )), donc le motif M 4 est sous forme canonique.\nParcours de l\u0027espace de recherche\nTous les items présents dans le graphe de départ sont listés et constituent les motifs initiaux qui vont être progressivement étendus. L\u0027arbre de recherche est construit en utilisant l\u0027ordre défini précédemment sur les graphes orientés attribués, en effectuant un parcours en profondeur d\u0027abord.\nEnumération des arbres couvrants\nL\u0027utilisation de la stratégie définie par Pasquier et al. (2013) permet d\u0027énumérer tous les arbres couvrants. La génération de nouveaux motifs est effectuée par extension des noeuds figurant dans le chemin le plus à droite du motif examiné (rightmost path extension) (Chi et al., 2004). Deux types d\u0027extensions sont possibles : l\u0027extension d\u0027itemset permet d\u0027ajouter un nouvel item à l\u0027itemset associé au noeud terminal le plus à droite, l\u0027extension de structure consiste à ajouter un fils à l\u0027un des noeuds composant le chemin droit. La méthode est complète, mais peut générer des motifs redondants qui prennent la forme d\u0027arbres isomorphes. A chaque génération, les motifs non canoniques sont écartés. L\u0027inconvénient de cette approche est qu\u0027elle ne permet de ne trouver que des motifs enracinés.\nIdentification des motifs canoniques et automorphes\nLe code sur les graphes défini précédemment possède, comme la plupart des codes utilisés par les algorithmes de fouille de graphes, la propriété suivante : chaque préfixe d\u0027un code canonique est lui même canonique (Huan et al., 2003;Yan et Han, 2002). L\u0027extension d\u0027un motif ne change pas l\u0027ordre des triplets lors de la génération d\u0027un motif canonique. Ainsi, chaque motif canonique ne peut être obtenu qu\u0027en ajoutant un nouveau triplet ou en ajoutant un item à l\u0027itemset associé au dernier triplet.\nEn utilisant la stratégie d\u0027extension du chemin droit, il est possible de vérifier la canonicité d\u0027un motif en ne testant que les noeuds appartenant au chemin droit. Soit les fonctions l et p qui, appliquées à un noeud retournent respectivement son dernier et son avant-dernier fils. Il est possible de déterminer la canonicité d\u0027un motif de la manière suivante : pour tous les noeuds n composant le chemin droit, si ?p(n) et code(p(n)) ? code(l(n)), alors, le motif est sous forme canonique. Il est, de la même manière, assez aisé d\u0027identifier les extensions qui produisent des motifs automorphes. S\u0027il existe un noeud n dans le chemin droit du motif tel que n a plus d\u0027un fils et code(p(n)) \u003d code(l(n)), alors le motif possède des automorphismes. Sur un tel motif, on appelle noeud de séparation le noeud du chemin droit, le plus proche de la racine, qui possède plus d\u0027un fils. Ainsi, les motifs illustrés en figures 1(c) et 1(d) possèdent des automorphismes. Dans ces deux figures, le noeud de séparation est le noeud a.\nPrise en compte des liens réentrants\nA partir de la stratégie d\u0027énumération des arbres couvrants décrite précédemment, il est possible de définir une méthode permettant d\u0027énumérer tous les sous-graphes. Pour cela, il faut identifier lorsqu\u0027une extension structurelle ajoute un noeud qui est déjà présent dans le motif examiné. Lorsque ce cas se produit, le noeud est ajouté dans le motif comme s\u0027il s\u0027agissait d\u0027une extension structurelle normale, mais l\u0027on indique qu\u0027il s\u0027agit d\u0027une réutilisation d\u0027un noeud déjà présent en mémorisant l\u0027index du noeud pointé. L\u0027exploration de l\u0027espace de recherche est stoppée à partir de ce noeud. En effet, du fait de notre stratégie d\u0027exploration, nous sommes certains que tous les noeuds faisant partie du chemin le plus à droite du motif examiné sont étendus lors de la phase de génération en cours. Toutes les structures ne faisant pas partie du chemin le plus à droite ont quant à elles, déjà été complètement explorées. Concrètement, la réutilisation d\u0027un noeud déjà présent est reflétée dans le code du graphe par l\u0027ajout d\u0027un triplet qui fait référence à un autre triplet. Cela ne génère aucun changement concernant les propriétés du code, la complétude de la méthode et la manière d\u0027éliminer les formes non canoniques. \nPrise en compte des cycles\nL\u0027extension d\u0027un motif peut donner lieu à la création d\u0027un cycle si et seulement si le nouveau noeud ajouté pointe sur l\u0027un des noeuds déjà présents dans le chemin droit. Lorsqu\u0027un cycle est créé, on procède de la même manière que pour les autres liens réentrants : un nouveau noeud est ajouté avec l\u0027index du noeud réutilisé, puis les extensions à partir de ce noeud sont stoppées. La complétude de la méthode n\u0027est pas affectée par cette opération. Il y a cependant un problème au niveau de la redondance des solutions si le noeud réutilisé est la racine du motif car dans ce cas, de nombreux graphes isomorphes sont générés en débutant l\u0027exploration à partir des autres noeuds appartenant au chemin droit.\nNous résolvons ce problème en générant tous les motifs ayant pour racine l\u0027un des noeuds du chemin droit. En comparant les codes des motifs ainsi obtenus, nous en choisissons un comme motif canonique (le plus petit code) et ne continuons l\u0027exploration que pour ce motif. Dans la figure 3, M 1 représente un motif trouvé dans le graphe G. Le noeud d, qui est à la racine du motif vient d\u0027être ajouté ce qui crée un cycle. Le cycle est composé des 3 noeuds appartenant au chemin droit (d, c et bc). Le motif enraciné en d est connu, il reste donc à générer deux autres motifs en fixant arbitrairement leur racine à bc (motif M 2 ) et c (motif M 3 ). Le motif qui a le plus petit code est bien évidemment celui enraciné en bc donc nous stoppons ici l\u0027exploration du motif M 1 .\n6 Traitement des motifs automorphes L\u0027utilisation de formes canoniques permet d\u0027élaguer l\u0027espace de recherche et cette optimisation est utilisée dans notre algorithme. Cependant, cela ne permet pas de résoudre le problème posé par la présence de motifs automorphes. Prenons par exemple le graphe orienté attribué représenté dans la figure 4(a). Ce graphe contient plusieurs sous-structures automorphes qui rendent l\u0027élagage basé sur l\u0027utilisation des formes canoniques moins opérant. Lorsque le motif illustré dans la figure 4(b) est analysé, il est conservé et étendu puisqu\u0027il est sous forme canonique. Or, ce motif peut être obtenu de douze façons différentes. L\u0027algorithme naïf consiste, pour chacune de ces formes, à étendre le motif en utilisant comme défini précédemment, l\u0027extension d\u0027itemset ou l\u0027extension de structure.\nL\u0027extension d\u0027itemset génère le motif illustré en (c). Ce motif, qui n\u0027est pas sous forme canonique, sera écarté. L\u0027extension de structure à partir du noeud b génère l\u0027un des motifs décrits en (e). Dans tous les cas, le motif obtenu n\u0027est pas sous forme canonique donc il sera écarté. La seule opération qui permet de générer un motif canonique est l\u0027extension de structure à partir du noeud de séparation (d).\nLorsque l\u0027on identifie un nouveau motif possédant des automorphismes, on sait que l\u0027on ne pourra pas faire d\u0027extension sur le dernier noeud, ni sur tous les noeuds situés après le noeud de séparation. En effet, pour un noeud de séparation n, on a code(p(n)) \u003d code(l(n)) donc, n\u0027importe quelle extension effectuée sur le motif de racine l(n) rendra son code inférieur à p(n), donc le motif ne sera plus canonique. Nous pouvons utiliser ce fait pour éviter de générer inutilement des motifs. Ceci est une première optimisation, mais son impact est limité.\nLa seconde optimisation consiste à supprimer certaines manière d\u0027obtenir des motifs automorphes qui ne permettent pas de générer de nouveaux motifs canoniques. Dans la figure 1, par exemple, le motif (d), qui est obtenu de 12 façons différentes, ne peut être étendu que par l\u0027ajout d\u0027un troisième fils au noeud de séparation a. Quelle que soit la manière dont le motif (d) a été obtenu, les possibilités d\u0027extension sont inchangées. Il est donc possible, dans ce cas, sans omettre de solution, de ne conserver qu\u0027une seule mise en correspondance du motif ; n\u0027importe laquelle.\nAvec le graphe tel que présenté dans la figure 4(a), la situation est plus compliquée car il est possible, à partir du motif, d\u0027ajouter un fils c au noeud a. Dans ce cas, il sera possible lors de la génération suivante, d\u0027appliquer n\u0027importe quel type d\u0027extension sur tous les noeuds du chemin droit. Dans l\u0027exemple, on pourrait en particulier ajouter e, f , g ou h en tant que fils du noeud c. Pour que tous les motifs possibles puissent être générés, il faut conserver les possibilités d\u0027étendre le motif avec 2 noeuds bc parmi les 3 noeuds bc ayant un fils, soit 3 2 \u003d 3 formes différentes. Dans un graphe réel, la situation est souvent beaucoup moins simple et déterminer quelles formes il est possible de supprimer n\u0027est pas une tâche facile. Pour éviter d\u0027effectuer des calculs portant sur la structure du graphe qui nuiraient aux performances de l\u0027algorithme, on choisit de ne conserver qu\u0027une seule forme parmi celles utilisant les mêmes noeuds. On passe ainsi à un nombre de formes déterminé par un nombre d\u0027arrangements possibles à un nombre de formes basé sur le nombre de combinaisons. Cette solution n\u0027est pas optimale mais elle est suffisante pour limiter l\u0027explosion combinatoire. Sur une configuration en étoile dans laquelle un noeud possède n fils avec une même étiquette, il existe n i\u003d1 n P k manières différentes de générer des motifs inclus. Après optimisation, ce nombre chute à n?1 i\u003d0 (n ? i) n i .\nRésultats expérimentaux\nLa méthode décrite dans l\u0027article a été implémentée en C++ avec la STL. Les expéri-mentations ont été effectuées sur un ordinateur avec Ubuntu 13.04 basé sur un processeur Intel c TM i5-2400 @ 3.10GHz avec 12 Gb de mémoire. Tous les temps d\u0027exécution incluent la phase de prétraitement et la sauvegarde des résultats.\nJeux de données artificiels\nUn ensemble de 1000 graphes orientés comprenant chacun 1000 noeuds et 5000 arcs ont été générés avec le programme de Johnsonbaugh et Kalin (1991). Cet ensemble de graphes a servi de base à la création de cinq jeux de données de graphes orientés attribués, nommés A1 à A5, dans lesquels les noeuds ont été associés à des itemsets de taille variant de 1 à 5. Pour simuler le fait que, comme dans beaucoup de jeux de donnés réels, la plupart des attributs sont rares, mais un petit nombre est très fréquent, nous avons attribué à chaque item une valeur entière aléatoire distribuée suivant une loi puissance (concrètement, nous avons utilisé une distribution de Pareto de paramètre ? \u003d 0.05).\nRéseau de citations de PubMed Central\nPubMed Central est une base de données bibliographique d\u0027articles scientifiques dans le domaine des sciences de la vie. Les articles en Open Source sont au nombre de 322 526 mais seulement 58 728 d\u0027entre eux citent au moins un autre article, lui même Open Source. Nous avons construit un graphe de citations où chaque noeud représente un article, chaque arc, un lien de citation et les attributs associés aux noeuds identifient les mots-clés de l\u0027article. Le graphe est très creux puisqu\u0027il ne contient que 60 012 arcs, cependant, certains attributs se retrouvent fréquemment associés à des sous-ensembles de noeuds connectés. \nJeux de données Google+ et Twitter\nÉvaluation des performances\nLes résultats de notre algorithme sont présentés dans la figure 5. La comparaison avec des algorithmes existants n\u0027est possible que sur des graphes dans lesquels les noeuds ne sont associés qu\u0027à une seule étiquette. Pour le jeu de données artificiel A1, le seul qui corresponde à un graphe étiqueté, une comparaison de AADAGE avec l\u0027implémentations de gSpan réal-isées dans le cadre du projet ParSeMiS (Wörlein et al., 2005) (une implémentation qui permet de fouiller les sous-graphes orientés) est présentée ( figure 5(a)). Les deux algorithmes, qui sont paramétrés pour énumérer l\u0027ensemble des sous-graphes orientés, enracinés et connectés, génèrent exactement les mêmes motifs. Pour les plus petites valeurs de support, la stratégie de filtrage des mises en correspondance permet de faire la différence avec gSpan. Les données Google+ et Twitter contiennent beaucoup de noeuds, beaucoup d\u0027attributs et surtout quelques attributs très fréquents (par exemple, l\u0027attribut \"sexe \u003d masculin\", présent dans 52% des noeuds) dont la seule présence génère tellement de motifs qu\u0027elle fait échouer la fouille. Dans la figure 5(d), les deux attributs de genre ont été enlevés du jeu de données Google+. Le jeu Twitter n\u0027a pas été modifié. Malgré sa taille, le jeu de données Google+ a pu être fouillé en fixant un support minimum de 3%. Le jeu de données Twitter, pourtant plus modeste, est traité jusqu\u0027à un support de 5%. La caractéristique commune de ces deux jeux de données est qu\u0027ils sont traités relativement rapidement jusqu\u0027à un seuil précis correspondant à la prise en compte d\u0027un nouvel attribut fréquent qui fait exploser le nombre de motifs.\nAvec le jeu de données de PubMed Central, il a été possible d\u0027utiliser un support très faible. Cela a permis d\u0027identifier quelques motifs intéressants. Un motif, par exemple, concerne un groupe d\u0027articles annotés avec \"p53\" qui est le nom d\u0027un oncogène. Ces articles sont cités par d\u0027autres articles annotés \"rapamycin\" (un médicament immunosuppresseur élaboré en 1975) qui sont eux-même cités par des articles annotés \"aging\" (des effets de la rapamycine sur la sénescence cellulaire ont été découverts en 2009) et \"mT OR\" (un gène qui est inhibé par la rapamycine). Ce motif représente de manière très condensée le cheminement de certaines recherches portant sur le vieillissement. \nConclusion et perspectives\nNous avons, dans cet article présenté des méthodes permettant de fouiller des graphes attribués orientés et montré leur efficacité lors de l\u0027analyse de plusieurs jeux de données. Nous nous sommes attachés aux configurations de graphes dans lesquelles, comme dans les données réelles, certains attributs sont partagés par un nombre important de noeuds. Nous avons montré que, selon la structure du motif en cours d\u0027analyse, il n\u0027est pas toujours nécessaire d\u0027étendre toutes les formes que peut prendre le motif. Nous nous sommes intéressés plus particulière-ment aux motifs automorphes, mais il existe d\u0027autres motifs pour lesquels des optimisations peuvent être effectuées. Par exemple, lorsque le motif de la figure 1(b), qui ne possède pas d\u0027automorphisme, est trouvé 12 fois sur le graphe de la figure 4(a), ont voit assez facilement que certaines mises en correspondance peuvent être écartées. Nous sommes convaincus que l\u0027analyse détaillée des motifs et de leurs mises en correspondance peut révéler d\u0027autres configurations permettant de réduire l\u0027espace de recherche et ainsi de traiter des jeux de données plus importants ou plus denses. C\u0027est une piste que nous avons l\u0027objectif d\u0027explorer.\nRemerciements. Ce travail a été financé par le contrat ANR-2010-COSI-012 FOSTER.\n"
  },
  {
    "id": "331",
    "text": "Introduction\nAvec l\u0027émergence du web 2.0, les internautes ne sont plus de simples consommateurs, ils sont également acteurs par le biais des messages qu\u0027ils peuvent déposer, des commentaires qu\u0027ils peuvent laisser et de toute action qu\u0027ils peuvent effectuer. Dans ce cadre, les messages laissés dans les réseaux sociaux représentent une source précieuse d\u0027informations, que de nombreuses recherches cherchent à analyser dans le but d\u0027en comprendre le contenu, d\u0027en extraire les relations cachées, mais aussi de prédire de l\u0027information. Le flux de messages peut être considéré comme une séquence ordonnée par la date de création des messages. On appellera \"item\" un élément représentant un message (mot du message, opinion ou sujet extrait, etc.). À un temps t, plusieurs items apparaissent donc dans cette séquence : l\u0027ensemble des items du message créé au temps t. Ce type de séquence est appelée \"séquence complexe\".\nDans le cas où les données sont formées d\u0027une unique et longue séquence, l\u0027extraction d\u0027épisodes est une tâche essentielle. Un épisode est un motif temporel composé d\u0027items \"relativement proches\", qui apparaît souvent tout au long de la séquence ou sur une partie de cette séquence (Mannila et al., 1997). Mannila (Mannila et al., 1997) a proposé les premiers algorithmes d\u0027extraction d\u0027épisodes : W inepi et M inepi qui seront la base de nombreux autres algorithmes proposés par la suite. Ces deux algorithmes extraient dans un premier temps les épisodes les plus petits, et forment incrémentalement des épisodes plus grands en se basant sur leur fréquence. Ces méthodes ont la caractéristique d\u0027extraire un ensemble complet d\u0027épisodes.\nL\u0027extraction d\u0027épisodes dans des séquences complexes est une problématique récente qui nécessite un algorithme adapté pour prendre en compte l\u0027existence de plusieurs items à chaque temps. Huang et Chang (Huang et Chang, 2008) proposent un algorithme appelé EMMA qui extrait un ensemble complet d\u0027épisodes à partir d\u0027une séquence complexe. Dans les deux premières phases, EMMA extrait un ensemble de motifs fréquents représentant des 1-uplet épi-sodes, associe un identifiant id à chaque 1-uplet épisode, puis encode la séquence avec ces id. Les épisodes sont construits incrémentalement pendant une troisième phase en concaténant des id. EMMA définit deux notions : borne et borne projetée. Une borne d\u0027un épisode P est un intervalle [t s , t e ] dans lequel P apparaît. Une borne projetée d\u0027un épisode P représente l\u0027intervalle de taille au maximum w dans lequel l\u0027algorithme cherche les id pour étendre P . EMMA est un algorithme rapide et facile à adapter.\nLa communauté d\u0027extraction de motifs fréquents admet que dans la plupart des applications, il est suffisant d\u0027extraire un ensemble d\u0027épisodes condensé et significatif. Par conséquent, les travaux récents se focalisent sur la détection d\u0027épisodes comportant certaines caractéris-tiques et contraintes : épisodes maximaux (Gan et Dai, 2011), ou approximativement fermés et non dérivables (Gan et Dai, 2012).\nTout comme il est possible d\u0027extraire des règles d\u0027association à partir de motifs, des règles d\u0027épisodes peuvent être extraites à partir d\u0027épisodes. Ces règles d\u0027épisodes peuvent aussi être utilisées dans un objectif de prédiction (Daurel, 2003). La majorité des règles d\u0027épisodes construites à partir d\u0027épisodes fermés ou maximaux ont la caractéristique d\u0027avoir un anté-cédent long (composé de nombreux items). De notre point de vue, elles ne sont pas adaptées à la prédiction au plus tôt d\u0027informations, car pour prédire une conséquence, il faut attendre l\u0027apparition de la totalité des items de l\u0027antécédent. Dans ce cas, pour détecter rapidement des événements, une règle d\u0027épisodes avec un antécédent plus petit est plus pertinente qu\u0027une règle avec un antécédent plus long. Par conséquent, notre premier objectif est d\u0027extraire des règles d\u0027épisodes composées d\u0027un antécédent de taille minimale, que nous appellerons \"règles d\u0027épisodes minimales\".\nDans la tâche d\u0027identification au plut tôt d\u0027informations, nous faisons l\u0027hypothèse qu\u0027il est inutile de chercher à former des règles d\u0027épisodes très complexes ou très précises, mais des règles d\u0027épisodes représentant les premiers signaux déclencheurs d\u0027un événement. Nous trouvons donc qu\u0027il est inutile d\u0027extraire les règles d\u0027épisodes contenant plusieurs fois le même item. Il est vrai qu\u0027une apparition multiple d\u0027un item porte plus d\u0027informations, mais l\u0027unicité des items permettra de diminuer la complexité de l\u0027algorithme. Nous appelons \"épisode sans répétition\" un épisode composé d\u0027items uniques. Notre second objectif est d\u0027extraire des épisodes sans répétition.\nNous souhaitons pouvoir anticiper/prédire des informations \"lointaines\". Notre troisième objectif est donc d\u0027extraire des règles d\u0027épisodes ayant une conséquence éloignée temporellement de l\u0027antécédent.\nLa notion \"minimale\" n\u0027est pas nouvelle dans l\u0027état de l\u0027art. Elle a été proposée dans (Rahal et al., 2004) dans le cadre d\u0027extraction de règles d\u0027association fiables, peu fréquentes ayant l\u0027antécédent le plus petit et la conséquence fixée par l\u0027utilisateur à partir de données transactionnelles dans le but de prédire au plus tôt la conséquence. Cette approche est différente de notre travail. En effet, nous voulons extraire les règles d\u0027épisodes (et non pas des règles d\u0027association), sans répétition et avec une conséquence temporellement éloignée de l\u0027antécédent.\nNous présentons maintenant l\u0027approche proposé pour atteindre nos objectifs.\nNotre approche : extraction de règles d\u0027épisodes minimales\nRappelons que notre objectif est de pouvoir anticiper au plus tôt des événements de façon fiable. Pour cela, nous proposons un algorithme d\u0027extraction de règles d\u0027épisodes qui, en plus d\u0027être fréquentes et fiables comme celles extraites par les algorithmes de l\u0027état de l\u0027art, sont sans répétition, minimales et avec une conséquence éloignée temporellement de l\u0027antécédent.\nNous utilisons les mêmes étapes d\u0027initialisation et d\u0027encodage comme dans l\u0027algorithme EMMA, mais nous proposons une autre approche pour obtenir des règles d\u0027épisodes avec les caractéristiques souhaitées. Le principe de notre algorithme est le suivant : chaque règle d\u0027épisodes est construite en fixant le préfixe de la future règle (1-uplet épisode), puis en fixant la conséquence, qui est la plus éloignée possible de ce préfixe, et enfin en complétant l\u0027antécédent avec des 1-uplet épisodes les plus proches possibles du préfixe. Nous détaillons ci-dessous cet algorithme.\nDéroulement de l\u0027algorithme\nIdentification du préfixe Chaque id obtenu dans la deuxième étape de EMMA représente un préfixe d\u0027un épisode potentiel.\nSoit id i un préfixe, nous construisons P roj ID f in (id i ) la liste des id apparaissant loin de id i (dans les dernières positions des bornes projetées de id i ). Nous construisons également P roj ID deb (id i ), la liste des id apparaissant à proximité de id i (présents au début des bornes projetées de id i ). La taille des intervalles \"début\" et \"fin\" est une proportion de w.\nIdentification de la conséquence id i est étendu avec les éléments id j ? P roj ID f in (id i ) qui représentent une conséquence potentielle d\u0027un épisode dont le premier élément est id i , formant ainsi un épisode candidat \u003c id i , id j \u003e. La fréquence de cet épisode candidat est calculée en utilisant le nombre de bornes dans sa liste de bornes. Si \u003c id i , id j \u003e n\u0027est pas fréquent, on arrête cette itération et on considère que id j ne peut pas être une conséquence de id i . On itère de nouveau pour étendre id i avec d\u0027autres id ? P roj ID f in (id i ). Si \u003c id i , id j \u003e est fréquent, alors la règle id i ? id j est construite. Si elle est fiable, elle est considérée comme minimale. Elle n\u0027est donc plus étendue et elle est ajoutée à la liste finale des règles d\u0027épisodes. Si la règle est fréquente mais pas fiable, alors le 2-uplet épisode \u003c id i , id j \u003e est étendu pour compléter l\u0027antécédent.\nComplétion de l\u0027antécédent Rappelons que nous cherchons à compéter l\u0027antécédent de façon à obtenir non seulement une règle fiable mais aussi une règle avec un antécédent minimal. Les itérations de complétion de l\u0027antécédent s\u0027arrêtent donc dès que la règle d\u0027épisodes est f iable, ou s\u0027il n\u0027y a plus d\u0027épisode candidat. À partir de \u003c id i , id j \u003e, id i est étendu itérativement avec les id dans sa liste P roj ID deb (id i ). Soit id s ? P roj ID deb (id i ), si l\u0027épisode candidat \u003c id i , id s , id j \u003e n\u0027est pas fréquent, alors le préfixe de l\u0027antécédent id i est étendu avec un autre id ? P roj ID deb (id i ), sinon la confiance de la règle id i , id s ? id j est calculée. Si la règle n\u0027est pas fiable, alors l\u0027épisode \u003c id i , id s \u003e est étendu avec d\u0027autres id ? P roj ID deb (id i , id s ), sinon elle est acceptable, et donc minimale, son antécédent n\u0027est donc plus étendu et elle est ajoutée à la liste finale de règles d\u0027épisodes.\nÉpisode sans répétition Comme nous l\u0027avons mentionné, notre algorithme a pour but d\u0027évi-ter l\u0027apparition multiple d\u0027un item dans les règles d\u0027épisodes. À chaque fois qu\u0027un épisode doit être étendu, une vérification est faite pour éviter de l\u0027étendre avec des id ayant des items en commun, ce qui aura également l\u0027avantage de réduire la complexité de l\u0027algorithme.\nConclusion et perspectives\nDans ce travail préliminaire, nous proposons un algorithme de fouille de séquence de données dans le but de prédire au plus tôt et de façon fiable, des événements. Notre objectif à court terme est de fouiller des messages issus des réseaux sociaux et de prédire l\u0027apparition d\u0027informations. Nous avons mis en évidence que l\u0027extraction de règles d\u0027épisodes est adaptée à nos données représentées sous la forme d\u0027une séquence complexe ordonnée selon le temps. Pour atteindre nos objectifs, nous avons déterminé plusieurs caractéristiques des règles d\u0027épisodes à extraire : elles doivent être sans répétition, fréquentes, fiables, ayant l\u0027antécédent le plus petit possible et la conséquence éloignée temporellement de l\u0027antécédent.\nL\u0027algorithme, tel que nous l\u0027avons défini, ne permet d\u0027extraire que des conséquences de taille 1 (1-uplet épisodes). Il est évident qu\u0027une conséquence plus longue sera plus porteuse d\u0027informations sur les futurs événements. Une de nos perspectives est donc d\u0027adapter cet algorithme de façon à ce qu\u0027il puisse extraire des conséquences plus longues.\n"
  },
  {
    "id": "332",
    "text": "Introduction\nLa minimalité est un concept essentiel de l\u0027extraction de motifs. Pour une fonction f et un langage L, un motif X est minimal si sa valeur pour f est distincte de celle de chacun de ses sous-ensembles. La collection de tous les motifs minimaux constitue une représentation condensée de L adéquate à f : il est possible de retrouver f (Y ) pour n\u0027importe quel motif Y ? L. Typiquement, l\u0027ensemble des itemsets libres (Boulicaut et al., 2000) est une représen-tation condensée de tous les itemsets (dans ce cas, f et L sont respectivement la fréquence et le langage des itemsets). Bien sûr, il est souvent plus efficace d\u0027extraire les motifs minimaux plutôt que la totalité des motifs. En plus, les motifs minimaux ont de nombreuses applications utiles notamment en extraction de connaissances : la production de bases de règles d\u0027association, la construction de classifieurs ou la génération des traverses minimales. La minimalité a été étudiée dans le cas de différentes fonctions (comme la fréquence (Calders et al., 2004) ou les fonctions condensables (Soulet et Crémilleux, 2008)) et avec des langages variés (dont les itemsets (Boulicaut et al., 2000) et les séquences ). Bien que la minimalité ait des avantages évidents (Li et al., 2006), elle reste peu explorée en comparaison de la maximalité (i.e., les motifs fermés). En particulier, à notre connaissance, il n\u0027existe pas de cadre suffisamment général comme celui proposé par Arimura et Uno (2009) pour les maximaux.\nNous pensons qu\u0027un des inconvénients importants des motifs minimaux réside dans leur difficile extraction. L\u0027efficacité limitée des algorithmes existants découle principalement de leur approche par niveaux (Boulicaut et al., 2000;Soulet et Crémilleux, 2008;Casali et al., 2005) (i.e., en largeur avec la méthode générer-et-tester). Comme ils stockent tous les candidats d\u0027un même niveau en mémoire durant la phase de génération, l\u0027extraction peut échouer par manque de mémoire. Pour éviter cet écueil, il semble préférable d\u0027adopter un parcours en profondeur qui souvent consomme moins de mémoire tout en restant très rapide. Cependant, vérifier si la minimalité est satisfaite ou non est vraiment difficile avec un parcours en profondeur. Dans le cas de la fréquence et des itemsets, la meilleure façon d\u0027évaluer la minimalité d\u0027un motif (disons abc) est de comparer sa fréquence avec celle de ses généralisations directes (ici, ab, ac et bc). Mais, quand le parcours en profondeur atteint le motif abc, seules les fré-quences de a et ab ont été précédemment calculées. Comme les fréquences de ac et bc ne sont pas connues, il est impossible de vérifier si la fréquence de abc est bien strictement inférieure à celle de ac et bc. Contributions. L\u0027objectif principal de cet article est de présenter un cadre générique et efficace pour l\u0027extraction des motifs minimaux, et ce en proposant un algorithme en profondeur peu consommateur en mémoire. Nous introduisons la notion de système minimisable d\u0027ensembles qui est au coeur de la définition de notre cadre. Ce dernier couvre un très large spectre de motifs minimaux incluant tous les langages et les mesures étudiés par Soulet et Crémilleux (2008); Arimura et Uno (2009). Une vérification rapide de la minimalité lors d\u0027un parcours en profondeur est réalisée grâce à la notion d\u0027objets critiques dont la définition découle du système minimisable d\u0027ensembles considéré. En s\u0027appuyant sur cette nouvelle technique, nous proposons l\u0027algorithme DEFME. Il extrait tous les motifs minimaux d\u0027un système minimisable d\u0027ensembles en utilisant un parcours en profondeur. À notre connaissance, il s\u0027agit du premier algorithme qui énumère tous les motifs minimaux en polynomial delay et avec une consommation linéaire d\u0027espace par rapport au jeu de données initial.\n2 Système d\u0027ensembles pour les motifs minimaux\nDéfinitions\nUn système d\u0027ensembles (F, E) est une collection F de sous-ensembles d\u0027un ensemble E (i.e. F est un sous-ensemble des parties de E). Un membre de F est appelé un ensemble acceptable. Un système d\u0027ensembles fortement accessible (F, E) est un système d\u0027ensembles où pour tous les ensembles acceptables X, Y satisfaisant X ? Y , il existe un élément e ? Y \\ X tel que Xe ? F 1 . Bien entendu, les itemsets rentrent dans ce cadre avec le système 1. Nous utilisons la notation Xe pour dénoter X ? {e}. \nCette définition indique que la couverture de l\u0027union de deux motifs correspond exactement à l\u0027intersection de leurs deux couvertures. Pour les itemsets, un opérateur de couverture naturel est l\u0027extension d\u0027un itemset X qui retourne tous les identifiants des tuples contenant X : cov I (X) \u003d {o ? O | X ? o}. La couverture permet surtout de dériver des informations utiles : la cardinalité de cov I (X) correspond à la fréquence de X. Dans le contexte des chaînes, la liste des index d\u0027une chaîne X définit également un opérateur de couverture :\nPour certains langages, le même motif est décrit par plusieurs ensembles distincts et alors, il est nécessaire d\u0027avoir une forme canonique (pour éviter de l\u0027extraire plusieurs fois, par exemple). Typiquement, considérons l\u0027ensemble {(a, 1), (b, 2), (r, 3)} correspondant à la chaîne abr. Son suffixe {(b, 2), (r, 3)} désigne la même chaîne br que {(b, 1), (r, 2)}. Dans notre cas, {(b, 1), (r, 2)} sera la forme canonique de br car le premier élément commence par 1. Pour retrouver la forme canonique d\u0027un motif, nous introduisons un nouvel opérateur :\nDans cette définition, la propriété (i) nous garantit que les formes canoniques de deux ensembles comparables (au sens de l\u0027inclusion) restent comparables. La propriété (ii) signifie que le système d\u0027ensembles (F, E) inclut toutes les formes canoniques. Continuons encore notre exemple sur les chaînes : on peut constater que ? S :\nSystème minimisable d\u0027ensembles\nPlutôt que de considérer un système d\u0027ensembles dans son intégralité, il peut s\u0027avérer judicieux d\u0027en sélectionner une partie qui apporte la même quantité d\u0027information (au sens de l\u0027opérateur de couverture). Pour cela, il est nécessaire que ce système d\u0027ensembles et l\u0027opéra-teur de couverture visé forment un système minimisable d\u0027ensembles : Définition 3 (Système minimisable d\u0027ensembles) Un système minimisable d\u0027ensembles est un tuple E), G, cov, ? où :\n-(F, E) est un système d\u0027ensembles fini et fortement accessible. Un ensemble acceptable dans F est appelé motif. -(G, E) est un système d\u0027ensembles fini et fortement accessible satisfaisant pour chaque couple d\u0027ensembles acceptables X, Y ? F tels que X ? Y et chaque élément e ? E, X\\{e} ? G ? Y \\{e} ? G. Un ensemble acceptable de G est appelé une généralisation.\nO est un opérateur de couverture. -? : F ? G ? F est un opérateur canonique tel que pour chaque ensemble acceptable X ? G, on ait cov(?(X)) \u003d cov(X).\nIllustrons maintenant le rôle de G par rapport à F dans le cas des chaînes. En fait, G S regroupe tous les suffixes de n\u0027importe quel motif de F S . Typiquement, {(b, 2), (r, 3)} ? G S est une généralisation de {(a, 1), (b, 2), (r, 3)} ? F S . Comme dit au-dessus, {(b, 2), (r, 3)} a une forme équivalente dans F S : ? S ({(b, 2), (r, 3)}) \u003d {(b, 1), (r, 2)}. Par convention, nous étendons la définition de cov S et G S en considérant que cov S (? S (X)) \u003d cov S (X). De plus, on voit que G S satisfait la propriété désirée par rapport à F S : pour tous les ensembles acceptables X, Y ? F S tels que X ? Y et chaque élément e ? E S , X \\ {e} ? G S ? Y \\ {e} ? G S . En effet, si X \\ {e} est un suffixe de X, cela signifie que e est la première lettre. Si nous considérons une spécialisation de X et que nous retirons la première lettre, nous obtenons aussi un suffixe qui appartient à G S . Par conséquent, S , E S ), G S , cov S , ? S est minimisable.\nBien évidemment, un système minimisable d\u0027ensembles peut être réduit à un système de cardinalité inférieure dont les motifs sont appelés motifs minimaux :\nLa définition 4 signifie qu\u0027un motif est minimal dès que sa couverture diffère de celle de toutes ses généralisations. Par exemple, pour l\u0027opérateur de couverture cov S , les motifs minimaux ont une couverture strictement plus petite que celles de leurs généralisa-tions. La chaîne ab n\u0027est pas minimale à cause de son suffixe b car cov S ({(b, 2))}) \u003d cov S ({(a, 1), (b, 2)}) \u003d {0, 7}. Dans notre exemple, la collection des chaînes minimales est M(S S ) \u003d {a, b, r, c, d, ca, ra, da}.\nEtant donné un système minimisable d\u0027ensembles S \u003d E), G, cov, ? l\u0027extraction des motifs minimaux consiste à énumérer tous les motifs minimaux de S.\nÉnumération des motifs minimaux\nLe but de cette section est de proposer une methode d\u0027extraction en profondeur des motifs minimaux (section 3.3). Pour cela, nous nous appuyons sur deux idées principales : l\u0027élagage de l\u0027espace de recherche (section 3.1) et la vérification rapide de la minimalité (section 3.2).\nAuparavant il est important de rappeler que les motifs minimaux suffisent pour déduire la couverture de tout motif. Nous considérons désormais un système minimisable d\u0027ensembles S \u003d E), G, cov, ? Les motifs minimaux M(S) consituent une représentation sans perte de tous les motifs de F :\nThéorème 1 (Représentation condensée) L\u0027ensemble des motifs minimaux est une représen-tation condensée de F adéquate pour le calcul de cov : pour tout motif X ? F, il existe\nLe théorème 1 signifie que la couverture de tout motif de S peut être déduite de M(S) (les preuves sont omises par manque de place). Par exemple, la couverture du motif non minimal\nIl est préférable d\u0027extraire M(S) plutôt que S car sa taille est plus petite que celle du système complet.\nÉlagage de l\u0027espace de recherche\nLe premier problème auquel nous sommes confrontés est plutôt classique. Étant donné un système minimisable d\u0027ensembles S \u003d E), G, cov, ? le nombre de motifs |F| est en général très grand (dans le pire des cas, il atteint 2 |E| ). Il est donc absolument nécessaire de ne pas parcourir l\u0027espace de recherche exhaustivement, mais de se concentrer sur les motifs minimaux. Heureusement, des techniques efficaces peuvent être utilisées pour élaguer l\u0027espace de recherche, grâce à la propriété d\u0027anti-monotonie de la contrainte de minimalité :\nThéorème 2 (Système indépendant) Si un motif X est minimal pour S, alors tout motif Y ? F satisfaisant Y ? X est aussi minimal pour S.\nLa preuve de ce théorème est fortement dépendante d\u0027un lemme clé indiquant qu\u0027un motif non minimal a une généralisation directe possédant la même couverture.\nLemme 1 Si X n\u0027est pas mininal, ?e ? X tel que X \\ {e} ? G et cov(X) \u003d cov(X \\ {e}).\nPar exemple, comme la chaîne da est minimale, les sous-chaînes d et a le sont également. En outre, comme ab n\u0027est pas minimal, la chaîne abr ne l\u0027est pas non plus. Cela signifie que la chaîne ab est un point de rupture dans l\u0027espace de recherche. En pratique, l\u0027élagage par antimonotonie est reconnu comme un outil puissant, quelque soit le type de parcours de l\u0027espace de recherche, par niveau ou en profondeur.\nVérification rapide de la minimalité\nLa difficulté principale de l\u0027extraction des motifs minimaux est de tester si un motif est minimal ou non. Comme mentionné précédemment, ceci est particulièrement difficile lors d\u0027un parcours en profondeur, car tous les sous-ensembles du motif à tester n\u0027ont pas été énumérés. En effet, les approches en profondeur n\u0027ont accès qu\u0027à la branche parente, au contraire des approches par niveau. Pour résoudre ce problème, nous introduisons le concept d\u0027objets critiques inspiré de celui d\u0027hyper-arête critique, introduit pour le calcul des traverses minimales par Murakami et Uno (2013). Intuitivement, les objets critiques d\u0027un élément e pour un motif X sont les objets qui ne sont pas couverts par X du fait de e.\nNous donnons maintenant une définition formelle des objets critiques, dérivant d\u0027un opé-rateur de couverture quelconque :\nDéfinition 5 (Objets critiques) Pour un motif X, les objets critiques d\u0027un élément e ? X, notés cov(X, e) sont l\u0027ensemble d\u0027objets qui appartiennent à la couverture de X privé de e mais pas à la couverture de e : cov(X, e) \u003d cov(X \\ e) \\ cov(e).\nIllustrons le concept d\u0027objets critiques sur notre exemple. Pour {(a, 1), (b, 2)}, les objets critiques cov S (ab, a) de l\u0027élément (a, 1) correspondent à ? (\u003d {0, 7} \\ {0, 3, 5, 7, 10}). Cela signifie que l\u0027ajout de a à b n\u0027a pas d\u0027impact sur la couverture de ab. En revanche, pour le même motif, les objets critiques de (b, 2) sont {3, 5, 10} (\u003d {0, 3, 5, 7, 10} \\ {0, 7}). C\u0027est à cause de l\u0027élément b que ab ne couvre pas les objets {3, 5, 10}.\nLes objets critiques sont centraux pour notre proposition : 1) les objets critiques caracté-risent facilement les motifs minimaux ; et 2) les objets critiques peuvent être calculés efficacement au sein d\u0027un algorithme en profondeur. Caractérisation de la minimalité La réciproque du lemme 1 indique qu\u0027un motif est minimal si sa couverture diffère de celle d\u0027une de ses généralisations. Nous pouvons reformuler cette définition grâce à la notion d\u0027objets critiques :\nPropriété 1 (Minimalité) X ? F est minimal si ?e ? X tel que X \\ e ? G, cov(X, e) \u003d ?.\nTypiquement, comme b est une généralisation de la chaîne ab, et cov S (ab, a) est vide, ab n\u0027est pas minimal. La propriété 1 signifie que tester si un candidat X est minimal ne néces-site que la connaissance des objets critiques relativement aux éléments de X. À la différence de la définition traditionnelle, il n\u0027est pas nécessaire de disposer d\u0027informations sur les sousensembles. Les objets critiques nous permettent donc de concevoir un algorithme en profondeur si (et seulement si) le calcul des objets critiques ne requiert pas non plus d\u0027informations sur les sous-ensembles. Calcul efficace d\u0027objets critiques Selon un parcours en profondeur, nous voulons mettre à jour les objets critiques d\u0027un élément e pour le motif X lorsqu\u0027un nouvel élément e est ajouté à X. Nous montrons que, dans ce cas, les objets critiques peuvent être calculés efficacement par intersection des objets critiques cov(X, e) avec la couverture du nouvel élément e\n:\nPropriété 2 L\u0027égalite suivante est vraie pour tout motif X ? F et deux élément e, e ? E : cov(Xe , e) \u003d cov(X, e) ? cov(e ).\nPar exemple, la définition 5 donne cov S (a, a) \u003d {1, 2, 4, 6, 8, 9}. Comme cov S (b) \u003d {0, 7}, on obtient que cov S (ab, a) \u003d cov S (a, a) ? cov S (b) \u003d {1, 2, 4, 6, 8, 9} ? {0, 7} \u003d ?. Il est intéressant de noter que la propriété 2 nous permet de calculer les objets critiques de tout élément d\u0027un motif X en ne disposant que de l\u0027information sur une seule branche du parcours. C\u0027est une situation idéale pour la conception d\u0027un algorithme en profondeur.\n3.3 Algorithme DEFME L\u0027algorithme DEFME prend en entrée le motif courant X et la queue tail des éléments restant à vérifier, il retourne tous les motifs minimaux contenant X, basés sur tail. Plus pré-cisément, la ligne 1 teste si X est minimal ou non. Si X est minimal, il est affiché (ligne 2). Les lignes 3 à 14 explorent le sous-arbre contenant X selon la queue. Pour chaque élément e pour lequel Xe est un motif de F (ligne 4) (propriété 1), la branche est construite à l\u0027aide des informations nécessaires. La ligne 7 met à jour la couverture et les lignes 8 à 11 calculent les objets critiques en utilisant la propriété 2. Enfin, la fonction DEFME est appelée récursivement à la ligne 12 avec la queue mise à jour à la ligne 5.\nAlgorithm 1 DEFME(X, tail)\nInput: X est un motif tail est l\u0027ensemble des items restant à utiliser pour générer les candidats. Valeurs initiales : X \u003d ?, tail \u003d E. Output: calcule les motifs minimaux de manière incrémentale polynomiale.\n1: if ?e ? X, cov(X, e) \u003d ? then 2: print X 3: for all e ? tail do 4:\nif Xe ? F then\n5:\ntail :\u003d tail \\ {e} 6:\ncov(Y, e) :\u003d cov(X) \\ cov(e)\n9:\nfor all e ? X do 10: Les théorèmes 3 et 4 montrent que l\u0027algorithme DEFME possède un comportement efficace, tant en terme d\u0027espace que de temps de calcul. Cette efficacité découle du calcul écono-mique des couvertures et des objets critiques, ainsi que la propriété suivante l\u0027explique :\nLa propriété 3 signifie que, pour un motif, le stockage de sa couverture plus celui de ses objets critiques est majoré par le nombre total d\u0027objets (i.e., |cov(?)|). La déduction de l\u0027espace de mémoire nécessaire pour l\u0027algorithme est donc directe : En pratique, l\u0027espace mémoire utilisé par l\u0027algorithme est très limité, car m est petit. En outre, le delai entre deux motifs est polynomial :\nThéorème 4 (Complexité polynomiale en délai) M(S) est énumerable en temps O(|E| 2 · |cov(?)|) par motif minimal. DEFME requiert un nombre polynomial d\u0027opérations entre deux motifs, en supposant que l\u0027oracle d\u0027appartenance agit en temps polynomial (ligne 4). En effet, le calcul de la couverture et celui des objets critiques (lignes 7 à 11) est linéaire avec le nombre d\u0027objets, grâce à la propriété 3 ; la boucle à la ligne 3 n\u0027excède pas |E| itérations et finalement, le nombre de retours en arrière consécutifs est au plus de |E|.\nlibres, pour laquelle plusieurs prototypes existent dans la littérature. Ensuite, nous utilisons DEFME pour extraire la collection des chaînes minimales et nous comparons sa taille avec celle des chaînes maximales. Tous les tests ont été effectués sur une machine Linux dotée d\u0027un processeur Opteron à 2,2 GHz et de 200 Go de RAM.\nExtraction des motifs libres\nNous avons réalisé un prototype de DEFME pour l\u0027extraction des motifs ensemblistes, en tant que preuve de concept, et nous l\u0027avons comparé avec deux autres prototypes : ACMINER, basé sur un algorithme par niveaux (Boulicaut et al., 2000) et NDI 2 utilisant un parcours en profondeur réordonnant les items (Calders et Goethals, 2005 Les meilleurs performances sont indiquées par l\u0027utilisation d\u0027une police grasse dans la table 1 pour les temps d\u0027exécution et la consommation mémoire. ACMINER est de loin le prototype le plus lent. Son approche par niveau est particulièrement pénalisée par l\u0027importance de la consommation mémoire. Excepté sur les données génomiques et sur chess, les temps d\u0027exécution de NDI surpassent clairement ceux de DEFME. À titre d\u0027information, la figure 1, à gauche, indique la vitesse de DEFME, selon différents seuils de support minimal. Cette figure représente le nombre de motifs minimaux calculés par seconde.\nConcernant la consommation mémoire, DEFME est (comme prévu) le plus efficace des algorithmes. Dans certains cas, augmenter la quantité de mémoire disponible ne suffirait pas à traiter les données les plus difficiles. Ici, ACMINER et NDI ne peuvent traiter les données génomique même avec 200 Go de mémoire, à des seuils de support minimal pourtant élévés. Plus précisément, la figure 1, à droite, représente le rapport entre les utilisations de mémoire de NDI et de DEFME, selon le seuil de support. On remarque grâce à ce rapport que NDI explose en mémoire. Rappelons que DEFME travaille en mémoire bornée et n\u0027est donc pas limité lors de faibles seuils de support.\nExtraction de chaînes minimales\nDans cette section, nous adoptons le formalisme des chaînes, utilisé dans notre exemple récurrent. Nous avons comparé notre algorithme d\u0027extraction de chaînes minimales avec le 2. Ce prototype permet d\u0027extraire les motifs libres en fixant le paramètre de profondeur à 1. 3. fimi.ua.ac.be/data/ et lisp.vse.cz/challenge/ecmlpkdd2004/ FIG. 1 -Ratio de la vitesse d\u0027extraction (à gauche) et de la consommation mémoire (à droite) de NDI par DEFME.\nprototype MAXMOTIF fourni par Takeaki Uno, extrayant les motifs minimaux. Notre but est de comparer la taille des représentations condensées fondées sur les chaînes minimales avec celles utilisant les chaînes maximales. Nous n\u0027avons pas reporté les temps d\u0027exécution car MAXMOTIF, développé en Java, est bien plus lent que DEFME. Les expérimentations ont été effectuées sur chromosom 4 et msnbc issu du dépôt UCI pour l\u0027apprentissage automatique 5 . La figure 2 reporte les quantités de chaînes, de chaînes minimales et de chaînes maximales extraites dans chromosom (à gauche) et msnbc (à droite), selon le seuil de support minimal. Le nombre de ces chaînes augmente bien-sûr lorsque ce seuil diminue. On note que les deux représentations condensées des minimaux et des clos deviennent particulièrement intéressantes pour de faibles seuils de support. Le nombre de chaînes minimales est clairement plus grand que celui de chaînes maximales, mais l\u0027écart n\u0027est pas aussi important que dans le cas des motifs libres ou fermés. À ces seuils de supports, l\u0027écart des moyennes des longueurs entre minimaux et maximaux est d\u0027un item pour chromosom et de trois pour msnbc.\nTravaux relatifs\nLa collection des motifs minimaux est une forme de représentation condensée. Un grand nombre de représentations condensées ont été proposées dans la littérature (Calders et al., 2004;Hamrouni, 2012) : les motifs fermés (Pasquier et al., 1999), les motifs libres (Boulicaut et al., 2000), les motifs essentiels (Casali et al., 2005), les motifs non-dérivables (Calders et Goethals, 2005), etc. Les deux idées fondatrices des représentations condensées sont les classes d\u0027équivalences souvent issues d\u0027un opérateur de fermeture (Hamrouni, 2012)  L\u0027extraction des motifs minimaux a de nombreuses applications et n\u0027est pas seulement utilisée pour accélérer l\u0027obtention des motifs fréquents. Leurs propriétés sont utiles pour certaines tâches. Par exemple, les motifs minimaux sont utilisés en conjonction de motifs fermés pour produire des règles non-redondantes ou informatives (Zaki, 2000;Pasquier et al., 1999). Les règles séquentielles bénéficient également de la minimalité (Lo et al., 2009). Il est aussi possible d\u0027exploiter les motifs minimaux pour extraire des règles de classification qui sont les éléments clés des classifieurs associatifs (Liu et al., 1998). Notre cadre est bien adapté pour extraire de telles règles de classification qui satisfont une mesure d\u0027intérêt impliquant des fré-quences. Supposons que l\u0027ensemble d\u0027objets O se répartisse en deux classes disjointes O \u003d O 1 ?O 2 , la confiance de la règle de classification X ? class 1 est |O 1 ? cov I (X)|/|cov I (X)|. Plus généralement, il est facile de montrer que n\u0027importe quelle mesure fondée sur des fré-quences (e.g., le lift, le bond) peut être dérivée des couvertures positive et négative. En plus, les motifs essentiels sont utiles pour dériver les traverses minimales qui correspondent exactement aux motifs maximaux de M( I , I), 2 I , cov I , Id Rappelons que la génération des traverses minimales est un problème important aux nombreuses applications en logique, en intelligence artificielle et apprentissage (Eiter et Gottlob, 2002;Murakami et Uno, 2013).\nLes représentations condensées de motifs minimaux ne sont pas limitées aux seules mesures impliquant la fréquence et au seul langage des itemsets. En effet, il est aussi possible d\u0027extraire des motifs minimaux adéquats à des fonctions d\u0027agrégat comme min,\n"
  },
  {
    "id": "333",
    "text": "Introduction\nLa programmation visuelle de composants de haut niveau permet d\u0027améliorer le dévelop-pement des chaînes de traitement. C\u0027est pourquoi les plate-formes libres pour l\u0027analyse de données (Weka, RapidMiner, Knime 1 ) proposent des environnements de développement visuel. Pour autant, il y est difficile d\u0027ajouter ses propres réalisations, par exemple des binaires compilés, car ces plate-formes requièrent l\u0027utilisation d\u0027une API dédiée, imposant par exemple un format spécifique pour la manipulation des données. Les possibilités de programmation structurée y sont également limitées : ces plate-formes sont conçues pour enrober leurs propres composants de base pour l\u0027apprentissage automatique.\nNous présentons ici la plate-forme KD-Ariane, un déploiement d\u0027outils pour la fouille de données dans l\u0027environnement de programmation visuelle Ariane. Ce déploiement facilite la conception de chaînes structurées de traitements pour l\u0027extraction de connaissance dans les données (Clouard (2009)). KD-Ariane est simple à prendre en main et possède un fort potentiel pédagogique. Les composants graphiques exécutent simplement des commandes système, rendant aisé l\u0027interfaçage avec les librairies disponibles. La puissance réside dans les possibilités de programmation structurée offertes par les boucles, les macro-composants et leur partage avec d\u0027autres utilisateurs sous forme de routines. Enfin, les chaînes développées avec KD-Ariane sont exportables en scripts shell ou perl.\nAriane : c\u0027est une plate-forme de programmation visuelle, initialement conçue pour valoriser les opérateurs de traitement d\u0027image de la librairie Pandore (Pandore (2013)). Nous illustrons ici son potentiel dans la réalisation de chaînes de traitement pour l\u0027extraction de connaissances dans les bases de données.\nLe principe est celui de la programmation visuelle : des composants sont assemblés pour réaliser la chaîne de traitements. Ces traitements sont réalisés sur des fichiers de données, afin de produire d\u0027autres données, ou des résultats sous forme de chaîne de caractère ou de valeur numérique.\nLes composants d\u0027Ariane\nUn composant est représenté par une boîte colorée, constituée de plots de liaison, de paramètres et de résultats. Sur la figure 1, une chaîne élémentaire de traitement est représentée. Les fichiers d\u0027entrée figurent sur la gauche et contiennent les données suivantes :\nFichier\nélémentaire mesure le support (i.e le nombre d\u0027occurrences) de chaque motif du fichier sample.sup dans les données de sample.bin. Elle ne conserve que les motifs fréquents, dont le support est supérieur à la constante minsup, ici fixée à 2.\nFIG. 1 -Anatomie d\u0027un opérateur.\nLe composant support effectue cette tâche en réalisant un appel au système : -exécuter le binaire support -avec les paramètres du composant (symbolisés par l\u0027utilisation de $p dans la propriété Binary) ; -indiquer au binaire les entrées (sympbolisées par $i) ; -rediriger le résultat vers le fichier de sortie (symbolisé par $o). -la conception de macro opérateurs ; -l\u0027enregistrement de ces macros dans une hiérarchie de routines ; -l\u0027exportation, le partage et la mutualisation de ces routines entre les utilisateurs.\nProgrammation visuelle structurée\nUn exemple de réalisation\nLa figure 2 illustre ces concepts avec une chaîne mesurant la complexité de l\u0027extraction des motifs fréquents. Pour cela, on souhaite chronométrer cette extraction dans une base de données classique de l\u0027UCI, pour un seuil de support minimum allant de 1 à 50, et compter le nombre de motifs obtenu.\nNous utilisons donc une boucle for (cf. la partie supérieur de la figure 2), qui itère un traitement paramétré pour des valeurs de support minimum de 1 à 50. Cette boucle prend en entrée les données de benchmark (fichier zoo.bin) ainsi qu\u0027un accumulateur destiné à recueillir les résultats de chaque itération. Cet accumulateur consiste en un fichier vide, créé à l\u0027aide de la commande echo -n.\nLe traitement à l\u0027intérieur de la boucle for, sur la partie inférieure de la figure, consiste à recueillir la date d\u0027exécution, extraire les motifs puis mesurer le temps écoulé depuis le recueil de date et enfin compter le nombre de lignes produites. Un simple echo assemble ces résultats et l\u0027ajoute à l\u0027accumulateur. Le résultat est visualisé à l\u0027aide d\u0027un script GnuPlot.\n"
  },
  {
    "id": "335",
    "text": "Introduction\nLa classification non-supervisée est une tâche importante dans l\u0027exploration de données non-étiquetées, elle vise à les organiser en groupes (ou classes) contenant des données similaires. Cette technique est utilisée avec succès dans de nombreux domaines d\u0027application tels que le marketing et la recherche d\u0027information. Cependant, dans plusieurs de ces applications, les données s\u0027organisent naturellement en groupes non-disjoints nécessitant donc de l\u0027émer-gence de groupes qui se chevauchent. Le domaine de recherche correspondant à cette problé-matique est la classification recouvrante (overlapping clustering), étudiée à travers différentes approches au cours du dernier demi-siècle (Shepard et Arabie, 1979;Diday, 1987;Banerjee et al., 2005;Cleuziou, 2008;Depril et al., 2008;Fellows et al., 2011).\nLe clustering recouvrant trouve ses applications dans de nombreux domaines nécessitant qu\u0027un individu appartienne à plusieurs classes. Par exemple, en analyse des réseaux sociaux, un acteur peut appartenir à plusieurs communautés (Tang et Liu, 2009;Wang et al., 2010;Fellows et al., 2011) ; en classification de vidéos, chaque entrée peut potentiellement avoir plusieurs genres différents (Snoek et al., 2006) ; en détection d\u0027émotions, une pièce de musique peut engendrer plusieurs émotions (Wieczorkowska et al., 2006), dans les systèmes de recherche d\u0027information, un document peut aborder plusieurs thématiques (Gil-García et Pons-Porrata, 2010;Pérez-Suárez et al., 2013), etc.\nContrairement aux méthodes floues, la modélisation des recouvrements suppose que chaque observation peut avoir une appartenance totale à plusieurs groupes simultanément (sans recours à un degré d\u0027appartenance). Quelle que soit l\u0027approche utilisée, clustering hié-rarchique ou partitionnement, les algorithmes existants produisent des groupes sans possibilité de contrôle sur la taille et la qualité des recouvrements. Bien que la méthode devrait idéale-ment révéler une classification qui convient le plus aux structures et formes sous-jacentes des données, un tel objectif n\u0027est généralement pas unique et la nature des recouvrements doit être utilisée comme un paramètre dans le processus de classification. Nous proposons dans cette étude, partant de l\u0027algorithme bien connu k-moyennes, deux nouvelles méthodes R 1 -OKM et R 2 -OKM permettant l\u0027ajustement des recouvrements de clusters selon deux principes de régu-lation à savoir : le nombre et la dispersion des groupes concernés.\nLa suite de l\u0027article est organisée ainsi : la Section 2 donne un bref aperçu des différentes approches de classification recouvrante et en particulier les algorithmes OKM et ALS. La Section 3 présente la motivation liée au contrôle des recouvrements ainsi qu\u0027une description des deux principes de régulation que nous proposons pour ajuster les recouvrements. Les deux dernières sections 4 et 5 exposent respectivement les résultats expérimentaux obtenus puis les conclusions et les perspectives de l\u0027étude.\nMéthodes de classification recouvrante\nLa tâche de classification recouvrante a été étudiée et partiellement résolue durant les trente dernières années par une série d\u0027études et de propositions de deux types : des solutions heuristiques ou théoriques. Nous appelons heuristiques les solutions qui consistent soit à modifier les sorties d\u0027approches usuelles de clustering (e.g. k-moyennes ou k-moyennes flou) telles que proposé dans Lingras et West (2004) ou Zhang et al. (2007), soit à proposer un nouveau processus intuitif de construction de classes recouvrantes telles que l\u0027algorithme CBC (Clustering by Committee) proposé par Pantel et Lin (2002) ou encore POBOC (Pole-Based Overlapping Clustering) proposé par Cleuziou et al. (2004). Ces deux types de contributions peuvent conduire à des résultats pertinents sans toutefois s\u0027appuyer sur des modèles théoriques, limitant de fait la possibilité de les améliorer ou de les généraliser.\nLes études théoriques sont, en revanche, des extensions de modèles usuels de classification non-supervisée, tels que les approches hiérarchiques, à base de modèles de mélanges ou de graphes. Les variantes recouvrantes des hiérarchies sont les pyramides (Diday, 1987) et plus généralement des hiérarchies faibles (Bertrand et Janowitz, 2003) ; elles visent à améliorer la correspondance entre l\u0027indice de distance induit par la structure et la mesure de dissimilarité initiale. Cependant, les structures pseudo-hiérarchiques recouvrantes sont soient restrictives en terme de configuration des recouvrements, soient complexes à générer et à visualiser.\nPlus récemment, les modèles de mélanges recouvrants ont été introduits (Banerjee et al., 2005;Heller et Ghahramani, 2007;Fu et Banerjee, 2008;Cleuziou et Sublemontier, 2008) ; ils sont motivés par la modélisation de processus biologiques et se fondent sur l\u0027hypothèse que chaque observation est le résultat d\u0027un mélange de lois et de combinaisons additives (Banerjee et al., 2005) ou multiplicatives (Heller et Ghahramani, 2007;Fu et Banerjee, 2008) de ces lois. Ce formalisme probabiliste permet de considérer non seulement des distributions gaussiennes mais peut se généraliser à toute loi exponentielle ; en revanche, les modèles génératifs ne sont pas paramétrables et n\u0027autorisent pas le contrôle des recouvrements notamment.\nNous concentrons notre étude sur un autre type de méthodes recouvrantes, celles formalisées par des critères objectifs et résolues de manière itérative. Deux types de modélisations des recouvrements ont été proposées et se réfèrent à deux hypothèses différentes :\n-le modèle additif a été introduit initialement par Shepard et Arabie (1979), réutilisé par Mirkin (1987) et Depril et al. (2008) et formalisé en terme de modèle de mélange par Banerjee et al. (2005). Il se fonde sur l\u0027hypothèse que les données situées à l\u0027intersection de plusieurs clusters résultent d\u0027une addition des caractéristiques de chacun des clusters ; les recouvrements sont alors modélisés par la somme des profils de ces clusters. Le modèle additif a été appliqué avec succès dans divers domaines tels que le marketing, l\u0027expression de gènes et la psychologie pour lesquels il semble effectivement adapté de modéliser les données multi-classes par une combinaison additive des caractéristiques de chaque classe. -le modèle géométrique a été introduit par Cleuziou (2008) puis réutilisé par BenN\u0027Cir et al. (2010). Il modélise les recouvrements comme une combinaison barycentrique des profils de clusters. Ce modèle est basé sur un raisonnement géométrique dans l\u0027espace (Euclidien) où les intersections de clusters correspondent à des recouvrements au sens spatial. Ce modèle a attesté expérimentalement sur des données textuelles et multi-média par exemple.\nNous détaillons dans la suite de cette section les algorithmes ALS (Depril et al., 2008) et OKM (Cleuziou, 2008) et leur modèle additif et géométrique respectivement.\nÉtant donnée une matrice\n, l\u0027algorithme ALS (Alternating Least Square) consiste à minimiser la fonction objective suivante :\nEn se basant sur le modèle additif, ALS optimise une somme d\u0027erreurs locales, propres à chaque observation x i , et définies par la distance Euclidienne entre l\u0027observation et la somme des profils des clusters auxquels x i appartient.\nDe même, la fonction objective utilisée dans OKM (Overlapping K-Means) est basée sur les erreurs locales, mais diffère dans la manière de combiner les profils de clusters. En se basant sur un modèle géométrique, OKM évalue l\u0027erreur locale à chaque donnée x i par la moyenne (barycentre) des profils :\nLes deux critères objectifs peuvent également s\u0027exprimer sous forme matricielle comme suit :\navec F la norme de Frobenius, et S définie pour chaque entrée par\nminimisation des critères objectifs (3) et (4) est réalisée par itération classique de deux étapes, décrites dans l\u0027Algorithme générique 1 :\n1. L\u0027étape d\u0027affectation correspond à un problème d\u0027optimisation discret, résolu dans ALS par l\u0027évaluation de toutes les affectations possibles 2 K pour chaque observation x i ou bien par la relaxation du problème en utilisant une heuristique telle que dans OKM.\n2. L\u0027étape de mise à jour des profils peut être réalisée de manière optimale en utilisant la pseudo-inverse de A ou S tel que proposé dans ALS ou bien à travers une mise à jour successive des profils pour éviter l\u0027inversion de la matrice qui est généralement coûteuses (utilisé dans OKM). Calculer les nouvelles affectations A sachant P 2:\nCalculer les nouveaux profils P sachant A Tant que J(A, P ) diminue Retourner les affectations finales A.\nPar la suite, le problème de la régulation des recouvrements est considéré pour le modèle géométrique sans perte de généralité puisque les principes de régulation proposés peuvent être appliqués aux modèles additifs.\nAjustement des recouvrements\nDans un processus d\u0027extraction de connaissances, l\u0027utilisateur ou l\u0027expert préfère géné-ralement avoir les moyens d\u0027interagir avec le système afin d\u0027explorer plusieurs alternatives concernant le nombre de clusters ou la métrique ou encore afin de contrôler le degré de \"flou\" du clustering. De même, dans un processus de recherche de classes recouvrantes, l\u0027expert pré-fèrera un système lui permettant d\u0027ajuster l\u0027importance des recouvrements relativement à ses attentes et à ses connaissances sur les données.\nPour rendre possible la régulation des recouvrements pour le modèle géométrique nous formalisons deux principes de régulation qui sont basés respectivement sur le nombre de clusters concernés et la dispersion des profils des groupes concernés. Pour visualiser ces principes de régulation, nous visualisons les zones de recouvrements avec des cellules de Voronoï dans un espace à deux dimensions dans un contexte de trois clusters. La Figure 1.(a) montre les cellules de Voronoï construites avec le modèle OKM en utilisant trois profils de classes de coordonnées respectives (4, 8), (2, 6) et (8, 3). Chaque zone de couleur est une cellule de Voronoï qui repré-sente une classe ou une intersection possible (recouvrement) de classes. Par exemple toutes les données situées dans la zone en jaune seront affectées uniquement à la classe 3 avec le modèle OKM, tandis que les données situées dans la zone en vert seront attribuées à l\u0027intersection des classes 1 et 3 (bleu ? jaune ? vert) et la zone en noir illustre l\u0027intersection des trois classes.\nL\u0027idée directrice de notre étude est donc de gérer différemment les classes et les combinaisons de classes de manière à limiter ou favoriser les affectations multiples aux situations où l\u0027amélioration induite est réellement significative.\nRégulation des recouvrements par le nombre de clusters (R 1 -OKM)\nNous proposons une première modélisation consistant à introduire une pondération, ajustable avec un paramètre ?, visant à réguler chaque erreur locale relativement au nombre d\u0027affectations de l\u0027individu. Le nouveau modèle s\u0027exprime par le critère objectif suivant :\navec k a i,k le nombre de classes auxquelles l\u0027objet x i appartient et ? ? R un paramètre contrôlant les recouvrements :\n1. ? \u003d 0 annule la pondération sur les tailles de combinaison conduisant ainsi au modèle OKM originel, 2. ? \u003e 0 pénalise les affectations à des combinaisons larges tant que ? augmente jusqu\u0027à aboutir à un modèle équivalent à k-moyennes (? ? +?), 3. ? \u003c 0 favorise les recouvrements tant que ? diminue jusqu\u0027à aboutir à un modèle totalement recouvrant avec des données affectées à toute les classes (? ? ??). \nRégulation des recouvrements par la dispersion des profils (R 2 -OKM)\nContrairement au modèle de régulation précédent qui se base sur le nombre de combinaisons de classes, le second modèle se base sur la dispersion des profils de classes afin de contrôler l\u0027importance des recouvrements. L\u0027hypothèses sous-jacente au modèle R 2 -OKM est que les recouvrements doivent être d\u0027autant plus pénalisés qu\u0027ils mettent en jeu des classes distantes les unes des autres. Inversement, les recouvrements sont autorisés (voire encouragés) pour les classes dont les profils sont plus proches.\nAfin de formaliser ce principe de régulation, nous proposons de favoriser ou pénaliser l\u0027erreur locale à chacune des données x i en utilisant le critère de dispersion suivant, quantifiant (le carré de) la distance moyenne de x i avec ses profils de classes :\nLe nouveau critère objectif du modèle R 2 -OKM se présente alors comme suit : \nProcessus d\u0027optimisation\nL\u0027algorithme d\u0027optimisation pour R 1 -OKM et R 2 -OKM suit le processus général de réaffec-tations décrit dans l\u0027Algorithme 1. La stratégie d\u0027affectation du modèle OKM (Cleuziou, 2008) demeure valide, mais en utilisant les nouveaux critères objectifs J R1?OKM et J R2?OKM .\nLa mise à jour des profils est effectuée successivement pour chaque cluster et nécessite la dérivation de nouveaux profils P * k,. garantissant la convergence des modèles proposés. Étant donnée la matrice A des affectations, les profils optimaux pour R 1 -OKM et R 2 -OKM sont obtenus par dérivation des critères (5) et (7) respectivement :\navec P i k le profil de la classe P k idéal vis-à-vis de l\u0027individu x i , c\u0027est-à-dire tel que l\u0027erreur locale pour x i est égale à zéro : p i k,j \u003d x i,j l a i,l ? l \u003dk a i,l p l,j . Nous avons proposé et formalisé deux principes de régulation des recouvrements dans le cadre des modèles géométriques ainsi que leur mise en oeuvre algorithmique. Notons que R 1 -OKM et R 2 -OKM sont deux généralisations de l\u0027algorithme k-moyennes : si les affectations sont restreintes à un seul groupe ou si les paramètres (? et ?) sont suffisamment pénalisant, le critère objectif est équivalent au critère des moindres carrés utilisé par k-moyennes. La convergence des méthodes proposées est assurée par un algorithme à faible coût en terme de complexité : O(T N KlogK) où T est le nombre d\u0027itérations.\nExpérimentation\nL\u0027évaluation des méthodes de classification non-supervisée est connue pour être une tâche difficile puisqu\u0027il n\u0027existe pas, par définition, de classification exacte sur laquelle se fonder pour mesurer la qualité d\u0027un clustering. La plupart des méthodes de classification recouvrantes ont été évaluées en utilisant la \"F-mesure\" combinant précision et rappel sur les paires d\u0027individus (Banerjee et al., 2005;Cleuziou, 2008;Fu et Banerjee, 2008). Cependant, dans le contexte recouvrant, la \"F-mesure\" ignore la multiplicité des affectations. Amigó et al. (2009) ont proposé une extension de leur métrique \"BCubed\" à la classification recouvrante qui permet d\u0027affiner le calcul des mesures de précision et de rappel en intégrant cette multiplicité. De la même manière que Suárez et al. (2013), nous avons décidé d\u0027utiliser la mesure ajustée \"F-BCubed\" pour une évaluation plus fine des classifications recouvrantes.\nTAB. 1 -Statistiques des jeux de données utilisés.  La taille des recouvrements a une grande influence sur la qualité des mesures de performance et sur la structure des classes attendues. Ainsi, plutôt que de fournir des tableaux de valeurs, nous avons positionné les scores (F-BCubed) de chaque méthode relativement au taux de recouvrement. La Figure 2 présente ces positionnements : chaque point sur la figure est obtenu par une moyenne sur dix exécutions de chaque algorithme dans les mêmes conditions initiales (initialisation des profils de clusters) et pour un nombre de classes égal au nombre d\u0027étiquettes de la référence.\nLes méthodes MOC, ALS et OKM étant sans contrôle possible sur les recouvrements, un unique clustering est produit pour chaque initialisation, ce qui se traduit par un seul point moyen sur les figures. En revanche, pour les trois autres algorithmes, les scores obtenus pour des paramétrages consécutifs ont été reliés afin d\u0027observer les tendances. Les lignes verticales en pointillés indiquent le taux de recouvrement de la classification de référence.\nLes principales observations que nous pouvons relever de ces résultats sont : -que les modèles additifs (MOC et ALS) ne parviennent pas à construire de recouvrements entre les classes pour EachMovie, Music emotion et Scene. Par contre, sur des données biologiques (Yeast), ces méthodes se caractérisent par une bonne qualité de classification et parviennent même à construire de larges recouvrements. -la capacité des principes de régulation proposés à produire des recouvrements adaptés à la structure sous-jacente des données : sur EachMovie et Scene, caractérisés par de faibles recouvrements, les méthodes existantes produisent des recouvrements larges conduisant à affaiblir leur score ; l\u0027ajustement des recouvrements offre un moyen d\u0027amé-liorer significativement la qualité des classifications générées. -les performances de k-moyennes flou seuillé sont rapidement limitées lorsque le nombre de clusters augmente. Cette méthode ne parviens pas à égaler les résultats de R 1 -ou R 2 -OKM sur les jeux de données Music emotion, Scene et Yeast avec 6, 6 et 14 clusters respectivement.\nConclusion\nNous avons proposé dans cette étude deux modèles généralisant k-moyennes afin de produire des classifications recouvrantes avec un contrôle sur la taille des recouvrements. Ces deux modèles offrent une régulation par le nombre et la dispersion des profils des clusters concernés, conduisant à des schémas de classification plus appropriés.\nPour compléter cette étude nous envisageons, dans une version plus étendue, de proposer une analyse détaillée ainsi qu\u0027une évaluation experte sur un jeu de données réel lié au domaine de la recherche d\u0027information. Les nouveaux principes de régulation des recouvrements, introduits dans ce présent travail, ne sont pas limités aux modèles géométriques et pourront être transposés aux modèles additifs (type ALS).\n"
  },
  {
    "id": "336",
    "text": "Introduction\nDe nos jours, les volumes de données textuelles gérées et échangées ne cessent d\u0027augmenter. Ceci est dû au fait qu\u0027un texte est plus riche en sémantique que tout autre support informationnel. Toutefois, pour les systèmes de gestion de bases de données, les informations gérées se présentent principalement sous format structuré. Bien que cette structuration offre un confort au niveau de l\u0027exploitation de données, elle présente une limite quant à la représen-tation des connaissances d\u0027une manière explicite. Généralement, les données textuelles sont plus significatives que les schémas conceptuels des bases de données, dans la mesure où elles permettent une description sémantique des relations entre les éléments d\u0027un domaine. Pour profiter de l\u0027abondance des données structurées et en même temps expliciter les sémantiques qui y sont incarnées, nous proposons une approche visant la génération des textes à partir de bases de données. Cette approche s\u0027articule autour de trois grandes phases : (i) une phase de prétraitement de données pour la génération d\u0027ontologies à partir des bases de données, (ii) une phase de génération de règles d\u0027association pour mieux expliciter la sémantique de l\u0027ontologie globale et (iii) une phase de génération d\u0027un extrait textuel. Dans le reste de cet article la section 2 détaille les trois étapes de base de notre approche. La section 3 est consacrée à la mise en oeuvre de notre approche.\nApproche proposée\nNotre approche pour la génération d\u0027un extrait textuel est basée essentiellement sur les ontologies comme ressources sémantiques. En fait, ce choix est justifié par la capacité des ontologies d\u0027offrir une modélisation sémantique des concepts et des relations associées. Étant donné que nous visons la production d\u0027un extrait textuel, une phase de génération de règles d\u0027association s\u0027avère indispensable pour guider le passage de l\u0027ontologie au texte. Concrète-ment, cette approche comporte principalement trois grandes phases (voir figure 1).\nFIG. 1 -Architecture générale de l\u0027approche proposée\nPhase de prétraitement des données\nCette phase vise l\u0027association d\u0027une ontologie globale aux bases de données alimentées comme inputs à notre approche. Ceci étant accompli en trois étapes. La première dite de conceptualisation, assure la transformation de chaque base de données fournie en entrée en une ontologie qu\u0027on qualifiera de locale. Une phase d\u0027ontologisation, pour aboutir une ontologie globale est accomplie par la suite pour faire face à l\u0027hétérogénéité des ontologies locales gé-nérées. Il s\u0027agit de procéder à un mapping pour découvrir la correspondance sémantique entre les éléments dans différentes ontologies locales. Pour ce faire, nous avons utilisé l\u0027algorithme FOAM (Framework for Ontology Alignment and Mapping) (Ehrig, 2007). L\u0027idée sous-jacente à ce dernier est de procéder à un calcul de similarité de certaines paires d\u0027entités (E 1 , E 2 ) des ontologies à intégrer. Une fois, l\u0027ontologie globale créée, une étape d\u0027opérationnalisation prend lieu. L\u0027objectif étant d\u0027évaluer la consistance de l\u0027ontologie.\nPhase de génération de règles d\u0027association\nL\u0027ontologie créée lors de la phase précédente est riche sémantiquement par rapport aux bases sources. Néanmoins, sa sémantique n\u0027est pas exprimée d\u0027une manière explicite. En fait, le passage à une vue textuelle est tributaire d\u0027une interprétation claire de l\u0027ontologie en question. C\u0027est l\u0027objectif de la phase en cours. Une fois l\u0027ontologie globale sollicitée moyennant une requête utilisateur, les données sont injectées comme inputs de la phase de génération de règles. Lors de cette phase nous avons adopté l\u0027algorithme Apriori (Agrawal et Srikant, 1994) pour déterminer les règles. Une fois les règles produites, ils seront considérés comme inputs pour déclencher la phase de génération de l\u0027extrait textuel.\nPhase de génération d\u0027un extrait textuel\nCette phase est assurée en suivant trois étapes : une planification de document, une planification de surface et enfin une réalisation de surface. Quant à la planification de document elle vise la détermination de la forme finale de l\u0027extrait textuel à générer. Plus explicitement, l\u0027ensemble de règles subit une sélection basée sur un calcul de support et de confiance pour maintenir un sous ensemble de règles jugées pertinentes. Une fois le contenu déterminé, il doit être structuré. La structuration du contenu consiste à analyser les règles d\u0027association sé-lectionnées afin d\u0027extraire les éléments d\u0027information qui les constituent. Une fois les règles analysées, une étape d\u0027organisation est nécessaire. Pour assurer cette structuration, nous proposons l\u0027utilisation des schémas de McKeown (1985). Ces derniers présentent un ensemble de spécifications sur la méthode d\u0027organisation des éléments d\u0027information. Une fois le contenu de notre extrait textuel est déterminé et structuré, il convient de convertir les éléments constituant les règles d\u0027association en termes lexicaux grâce à deux fonctions qui sont la lexicalisation et l\u0027agrégation. La dernière étape de cette phase est consacrée pour la réalisation de surface. Elle consiste en fait à traduire la représentation conceptuelle en extrait textuel compréhensible.\nImplémentation\nPour la mise en oeuvre de notre approche un ensemble de bases de données géographiques du domaine de l\u0027agriculture a été fourni comme input à notre système. Ces bases de données ont été téléchargées à partir d\u0027un système mondial d\u0027information sur l\u0027eau et l\u0027agriculture de la FAO (Food and Agriculture Organization) AQUASTAT 1 . Chacune de ces bases doit être convertie en une ontologie locale, via l\u0027activation du plugin DataMaster (Nyulas et al., 2007)  . L\u0027intégration des ontologies locales est réalisée grâce au plugin Prompt (Noy et Musen, 2001) en appliquant l\u0027algorithme FOAM. Pour l\u0027extraction des règles d\u0027association, l\u0027interrogation de notre ontologie est indispensable. Le résultat de la requête est enregistré sous format CSV. Ce fichier est exploité pour la génération des règles d\u0027association. Ces dernières sont exploitées moyennant l\u0027utilisation du langage prolog\n"
  },
  {
    "id": "337",
    "text": "Introduction\nLes graphes sont un puissant outil de représentation pour la découverte de connaissances dans de nombreux contextes. Ainsi, nous pouvons nous intéresser à des graphes qui décrivent des entités (noeuds) mises en relation (arêtes) : souvent, ces entités peuvent être décrites au moyen d\u0027attributs et les relations ou descriptions des attributs peuvent évoluer au cours du temps. Nous parlerons alors de graphe attribué et dynamique (Jin et al. (2007); Boden et al. (2012)). Développer de nouvelles méthodes pour la fouille de tels graphes est important, ne serait-ce que pour le potentiel applicatif des analyses d\u0027interactions sociales. L\u0027explicitation puis l\u0027exploitation de hiérarchies déclarant certaines relations entre attributs a été très étu-dié, notamment dans le contexte de la découverte de motifs et de règles d\u0027association multidimensionnelles (Srikant et Agrawal (1996); Han et Fu (1999); Chen et al. (2009) ;Plantevit et al. (2010)). Nous pensons que de telles hiérarchies, souvent faciles à expliciter, permettraient d\u0027ajouter de la connaissance du domaine sur des graphes attribués dynamiques pour améliorer la pertinence des fouilles réalisées. Nous décidons d\u0027étendre la proposition présen-tée dans Desmier et al. (2013) pour la découverte de motifs de co-évolution dans des graphes attribués dynamiques : chaque motif découvert va correspondre à trois ensembles qui sont (a) un ensemble de noeuds, (b) un ensemble de pas de temps, et (c) un ensemble d\u0027attributs tel qu\u0027une tendance d\u0027évolution (croissance, décroissance) est associée à chaque attribut. Prenons par exemple un graphe représentant un ensemble d\u0027aéroports pour les noeuds, reliés deux à deux par une arête s\u0027il existe au moins un vol direct entre les deux d\u0027aéroports et dont les attributs représentent le nombre de vols au départ et à l\u0027arrivée de chaque aéroport. Un motif pourrait par exemple être un groupe de 9 aéroports qui ont vu leur nombre de vols au départ et à l\u0027arrivée diminuer en décembre car il y a eu de fortes chutes de neige. Cependant ce type de motif peut entrainer beaucoup de redondances dûes à la nécessité d\u0027avoir un respect strict des tendances sur les attributs. Supposons par exemple que 11 aéroports ont vu leur nombre de vols au départ diminuer, 10 ont vu leur nombre de vols à l\u0027arrivée diminuer tandis que 9 d\u0027entre eux ont eu les deux à la fois : dans une approche comme celle de Desmier et al. (2013), trois motifs seront alors extraits. Afin d\u0027améliorer la pertinence des motifs de co-évolution calculés, nous introduisons ici l\u0027utilisation d\u0027une hiérarchie sur les attributs. Nous supposons que la hiérarchie est fournie par l\u0027analyste et nous étudions la découverte de motifs multi-niveaux, c\u0027est à dire pouvant contenir des éléments appartenant à plusieurs niveaux de la hiérarchie. Nous proposons une mesure de pureté pour vérifier que la tendance est respectée par un certain pourcentage des attributs fils. Cette mesure permet d\u0027accepter qu\u0027un ensemble de noeuds et de pas de temps ne respecte pas strictement la tendance des attributs bien qu\u0027il apporte une information suffisamment pertinente. Dans notre exemple, considérons une hiérarchie qui expliquerait que l\u0027attribut « Nombre de vols » est le parent du nombre d\u0027arrivée et du nombre de départs. Le motif extrait présenterait alors un groupe de 12 aéroports qui ont vu leur nombre de vols diminuer en décembre, en incluant ceux qui ont vu leur nombre de vols diminuer soit au départ soit à l\u0027arrivée. Cette multiplicité des niveaux des attributs permet d\u0027obtenir des motifs plus concis et donc d\u0027une certaine façon plus robustes au bruit ou aux erreurs : des motifs plus généraux sont découverts dont les noeuds ne respectent pas une co-évolution stricte. Il devient donc possible d\u0027intégrer au motif des entités ou des pas de temps pour lesquels la co-évolution n\u0027est en partie pas respectée parce qu\u0027ils ont eu temporairement un comportement différent.\nNos contributions sont les suivantes. Nous définissons un nouveau problème de fouille de données : la découverte de motifs hiérarchiques de co-évolution dans des graphes attribués dynamiques. Nous définissons ce type de motif comme une séquence de graphes connexes et dont les attributs co-évoluent, et nous introduisons des mesures qui permettent d\u0027évaluer leur pureté. Ensuite, nous proposons un algorithme qui calcule l\u0027ensemble des motifs qui satisfont une combinaison de contraintes spécifiée par l\u0027analyste. Une formalisation du problème et des mesures est faite dans la Section 2 et l\u0027algorithme est décrit dans la Section 3. Les résultats des expérimentations quantitatives et qualitatives sont présentés dans la Section 4. Un état de l\u0027art est proposé dans la Section 5 avant une brève conclusion en Section 6.\nMotifs Hiérarchiques de Co-évolution\nUn graphe attribué dynamique G se définit comme une séquence de graphes attribués G t sur T pas de temps, i.e., G \u003d {G t |t \u003d 1..T } avec G t \u003d (V, E t , A t ). V est un ensemble de noeuds, E t est un ensemble d\u0027arêtes qui dépendent du temps et A t est un vecteur de valeurs pour les attributs de A au temps t. A est un ensemble d\u0027attributs commun à tous les noeuds et à tous les temps. Une hiérarchie H sur A est un arbre dont le noeud All est la racine et les attributs de A les feuilles. Les arcs représentent une relation is_a. La relation de spécialisation (resp. généralisation) correspond à un parcours de haut en bas (resp. de bas en haut), c\u0027est à dire de la \nracine aux feuilles (resp. des feuilles à la racine). Cette hiérarchie représente une connaissance a priori de l\u0027analyste. La fonction parent(x) retourne les noeuds parents directs du noeud x, children(x) les noeuds fils directs, up(x) l\u0027ensemble des ancêtres de x, x inclus, down(x) l\u0027ensemble de ses descendants, x inclus. La fonction leaf (x) retourne les descendants de x qui sont des feuilles de la hiérarchie, i.e., leaf (x) \u003d down(x) ? A. Le domaine de la hiérarchie dom(H) contient tous les noeuds sauf le noeud racine. Un motif hiérarchique de co-évolution extrait à partir d\u0027un graphe attribué dynamique et d\u0027une hiérarchie est un ensemble de noeuds, de pas de temps et d\u0027attributs. Ses noeuds respectent la même tendance sur les attributs à chaque pas de temps et ils sont connectés dans le graphe par un chemin de taille maximale définie. Formellement, un motif hiérarchique de co-évolution\n. Par abus de langage, nous utiliserons indifféremment ? ou A pour désigner les attributs du motif. Un attribut associé à une tendance est noté a s ? ? tel que s ? {+, ?} et a ? A. On dit que s est le symétrique de s (i.e., si s \u003d + alors s \u003d ? et inversement). Par définition, ce motif M doit respecter une contrainte de co-évolution et une contrainte de diamètre. La première vérifie que la tendance associée à chaque attribut du motif est respectée pour au moins un de ses attributs fils par tous les noeuds à tous les temps. La deuxième vérifie que la longueur du plus court chemin entre chaque paire de noeuds à chaque pas de temps est inférieure à un seuil donné. coevolution(M) Soit ? condition la fonction de Kronecker telle que ? a s (v,t) \u003d 1 si le noeud v au pas de temps t respecte la tendance s pour l\u0027attribut a. Un motif M \u003d (V, T, ?) respecte la contrainte de co-évolution si ?a\nPour éviter de produire certains motifs, nous utilisons également des contraintes de taille\nNotons que ces définitions sont adaptées de celles décrites dans Desmier et al. (2013) pour prendre en compte l\u0027existence des hiérarchies et donc la possibilité qu\u0027un motif contienne des attributs appartenant à plusieurs niveaux.\nPar définition, la co-évolution requiert que la tendance soit vraie pour au moins l\u0027un des attributs fils, mais si elle n\u0027est vérifiée que pour une faible proportion des attributs fils, l\u0027information apportée par le motif n\u0027est pas satisfaisante. Il faut donc savoir si le motif apporte une information intéressante et s\u0027il est préférable de spécialiser l\u0027attribut ou de le conserver en l\u0027état. Considérons la Figure 1, soit le motif\n}} n\u0027est pas un motif valide par rapport à la contrainte de volume, le motif M 1 est plus précis que son parent M .\nSoit une mesure de pureté du motif M \u003d (V, T, ?), purity(M ) est le nombre de triplets (v, t, a s ), v ? V, t ? T, a s ? ? valides par rapport au nombre de triplets possibles :\nPour savoir si l\u0027attribut résume bien l\u0027information sur ses fils dans le motif, introduisons la contrainte pureMin(M) qui impose que la pureté du motif soit supérieure à un seuil ? ? [0, 1] fixé par l\u0027analyste. Il faut également savoir si la spécialisation de l\u0027attribut apporte un gain de pureté tel qu\u0027il compense la perte de généralité. Pour celà, nous définissons la contrainte gainMin(M) qui impose que la pureté du motif fils divisée par la pureté maximale de ses motifs parents soit supérieure à un seuil ? \u003e 1 fixé par l\u0027analyste. Formellement, le gain de pureté apporté par un motif M \u003d (V, T, ?) par rapport à ses motifs parents\ns avec a ? ? se définit ainsi :\nNous pouvons maintenant dire que la contrainte pureMin(M) impose que purity(M ) \u003e ? et que la contrainte gainMin(M) impose que gain(M ) \u003e ?.\nUn motif qui respecte gainMin(M) est un motif qui apporte une information plus pure que tous ses motifs parents. S\u0027il ne la respecte pas, ses motifs parents ont donc une pureté équivalente et apportent au moins autant d\u0027information. Notons que si un motif fils n\u0027apporte pas une gain de pureté suffisant, la pureté de ses descendants pourrait être bien supérieure à celle du motif parent (la pureté est toujours égale à 1 pour les attributs feuilles de la hiérar-chie). Cependant la spécialisation engendrerait beaucoup de motifs potentiels sans réel apport d\u0027information.\nUn motif hiérarchique de co-évolution M tel qu\u0027il est « inclus » dans un motif M n\u0027apporte aucune information supplémentaire, un motif ne doit être conservé que s\u0027il est maximal. Un motif hiérarchique de co-évolution M \u003d (V, T, ?) respecte maximal(M), i.e., s\u0027il n\u0027existe pas de motif hiérarchique de co-évolution\nDéfinition du problème : Étant donné un graphe attribué dynamique et une hiérarchie, notre problème consiste à extraire l\u0027ensemble des motifs hiérarchiques de co-évolution tels qu\u0027ils respectent les contraintes de co-évolution et de diamètre et potentiellement une conjonction de contraintes supplémentaires dont les seuils sont choisis par l\u0027analyste.\nL\u0027algorithme 1 présente notre proposition. L\u0027énumération peut être représentée par un arbre. Chaque noeud de l\u0027arbre contient 2 tri-sets (collections de trois ensembles), P contient les éléments présents dans le motif en construction et C contient les éléments encore à énumé-rer. Au début de l\u0027algorithme P \u003d ? et C \u003d }{children(All) × {?, +}}}. À chaque étape soit un élément est énuméré (noeud, temps ou attribut) soit un attribut a de P est spécia-lisé et un attribut de C est énuméré en conservant a non spécialisé. Au début de l\u0027algorithme et afin de pouvoir mieux exploiter les propriétés d\u0027élagage, un noeud est énuméré puis un temps et un attribut. A chaque étape, les éléments de C sont supprimés s\u0027ils ne peuvent pas être ajoutés à P en donnant un motif valide, i.e., s\u0027ils ne peuvent pas respecter les différentes contraintes. Si le nouveau motif P ne respecte pas les contraintes, l\u0027énumération est stoppée. Cependant la majorité des contraintes présentées dans la Section 2 n\u0027exhibe pas de propriété de monotonie et nous avons défini des bornes et des contraintes relaxées pour élaguer l\u0027espace de recherche.\nLorsqu\u0027un élément est énuméré, il est supprimé de C et ajouté à P , si l\u0027élément est un attribut a s , son symétrique a s est également supprimé de C. Lors de l\u0027étape de spécialisation d\u0027un attribut a de P , tous les motifs fils qui apportent un gain de pureté sont énumérés, i.e., ?a i ? children(a) t.q. gainMin(P i ) est respecté, le motif P i est énuméré et les a s j et a s j t.q. a j ? children(a) avec j \u003e i sont ajoutés à C. Tous les motifs contenant a ? P .? non spécialisé et b ? C.? sont également énumérés. L\u0027étape de spécialisation est présentée dans la Fig. 2, les deux noeuds fils de gauche représentent la spécialisation, tandis que les deux de droite représentent l\u0027énumération avec l\u0027attribut non spécialisé. Lors d\u0027une étape d\u0027énumération sans spécialisation, il ne faut considérer que les deux noeuds fils de droite comme exemple.\nFIG. 2 -Exemple d\u0027une étape de spécialisation. Les deux noeuds de gauche présentent la spécialisation de l\u0027attribut, les noeuds de droite présente l\u0027énumération d\u0027un autre attribut.\n. À chaque étape C est élagué en fonction de la contrainte de co-évolution, ne sont conservés que les v ? V t.q.\nDe plus, si coevolution(P ) n\u0027est pas respectée, aucun motif valide ne pourra être énuméré par la suite.\nLe diamètre n\u0027est pas monotone, l\u0027ajout d\u0027un noeud à un ensemble de noeuds peut diminuer ou augmenter le diamètre du sous-graphe induit. Il faut donc uniquement vérifier que le graphe induit par l\u0027ensemble de noeuds de P peut respecter la contrainte de diamètre en ajoutant tout ou partie de l\u0027ensemble de noeuds de C. Considérant la Fig. 1 et un diamètre maximal k \u003d 2, le motif 1 v 4 }{t 2 }{}} ne respecte pas diameter, pourtant si le noeud v 3 est ajouté, le motif résultant 1 v 3 v 4 }{t 2 }{}} respecte la contrainte. À l\u0027inverse si le noeud v 2 est ajouté à ce dernier, le motif 1 v 2 v 3 v 4 }{t 2 }{}} ne respecte plus diameter puisque \n) n\u0027est pas respectée, aucun motif valide ne pourra être énuméré. Ne sont conservés dans C que les v ? V t.q.\n. L\u0027espace de recherche est également élagué grâce à la notion de volume : l\u0027énumération est arrêtée si le volume du motif ne peut pas être supérieur à ?. Contrairement à Desmier et al. (2013) \nénu-mération continue, sinon plus aucun motif valide ne peut être énuméré (preuve disponible). Finalement, la contrainte de pureté, qui n\u0027est pas monotone puisqu\u0027elle dépend de la fonction volume, donne également lieu à élagage. Si\n\u003e ? l\u0027énumération continue, sinon aucun motif valide ne pourra être énuméré.\nLorsque C est vide, P \u003d (V, T, ?) est un motif final potentiel. S\u0027il vérifie sizeMinV(P ), sizeMinT(P ), sizeMinA(P ), volumeMin(P ), pureMin(P ) et maximal(P ) alors le motif est valide. Il est maximal s\u0027il n\u0027existe pas un ensemble de noeuds v ? V t.q. le motif (V ? v, T, ?) respecte les contraintes, il n\u0027existe pas t ? T t.q. le motif (V, T ?T, ?) respecte les contraintes et il n\u0027existe pas d\u0027attribut a\nrespecte les contraintes. L\u0027algorithme extrait donc tous les motifs hiérarchiques de co-évolution qui respectent les contraintes considérant les seuils minV ,minT , minA, V, ? et ?. Nous présentons des résultats expérimentaux obtenus sur des jeux de données réels pour illustrer l\u0027intérêt de notre approche. Toutes les expérimentations ont été réalisées sur une ferme de calcul. Chaque machine est équipée de 2 processeurs à 2,5GHz et 16GB de RAM et utilise la distribution « Scientific Linux ». L\u0027algorithme est implémenté en C++. L\u0027objectif des expérimentations est de répondre aux questions suivantes : Les motifs sont ils pertinents ? Est-il possible de les extraire en un temps acceptable ? Les nouvelles mesures qui ont trait à la pureté permettent-elle d\u0027obtenir des motifs plus faciles à interpréter ? « DBLP » est un graphe de co-auteurs créé à partir de la base DBLP Ceci est dû au fait que la majorité des triplets (v, t, a ? ) et (v, t, a + ) sont invalides dans le jeu de données car beaucoup d\u0027auteurs ne publient jamais dans certains journaux ou conférences. Considérant le seuil de gain, le nombre de motifs est également très dépendant de ?. Par contre, le comportement en terme de temps d\u0027exécution n\u0027est pas monotone. Ce temps diminue fortement puis ré-augmente. Ceci s\u0027explique par le fait que la hiérarchie possède beaucoup de niveaux. Lorsque ? est bas, les attributs sont spécialisés et il y a de nombreuses énumérations possibles. Avec une valeur moyenne pour ?, les attributs sont conservés à des niveaux moyens de la hiérarchie, ce qui permet de bien élaguer le reste des éléments puisque les auteurs ont des comportements similaires sur ces ensembles de conférences ou journaux. À l\u0027inverse, si ? est élevé, les attributs ne sont que très peu spécialisés et le reste des éléments est peu élagué. Enfin, avec le seuil de volume ?, le nombre de motifs est logiquement impacté mais on note que le temps de calcul l\u0027est également bien que la contrainte ne soit pas monotone. La hiérarchie a un effet important sur le résultat de l\u0027extraction. Pour le montrer nous avons créé cinq hiérarchies pour le jeu de données DBLP en enlevant à chaque fois un niveau de profondeur à celle proposée dans la Fig.  3(a). Les résultats de l\u0027expérimentation sont présentés dans la Dans les deux cas, les motifs contiennent les même temps, i.e., principalement les 6 premières semaines du jeu de données. Un motif en particulier est intéressant : lors de la première extraction il possède 65 aéroports avec « NbDep et « NbArr » qui diminuent ; dans la deuxième extraction il contient 83 aéroports incluant les 65 premiers avec « NbFlights » qui diminue. Les motifs extraits en augmentant ? contiennent principalement des attributs parents pour les délais et les vols, pourtant les motifs ont tous une pureté supérieure à 0, 9. Ils apportent donc une information plus concise tout en conservant une précision tout à fait acceptable. De plus, l\u0027arrêt à un niveau supérieur de la hiérarchie permet d\u0027augmenter le nombre d\u0027aéroports concernés qui pouvaient ne pas apparaitre dans la première extraction à cause d\u0027une erreur dans les données ou plus simplement parce-qu\u0027ils ne respectaient pas une tendance à un temps du motif alors qu\u0027ils respectaient toutes les autres évolutions.\nExpérimentations\nÉtat de l\u0027art\nLa fouille de graphes dynamiques est très étudiée. Lahiri et Berger-Wolf (2010) recherchent des sous-graphes similaires apparaissant périodiquement. Inokuchi et Washio (2010) proposent l\u0027extraction de sous-séquences de sous-graphes induits fréquents tels qu\u0027un graphe est un sousgraphe d\u0027un autre s\u0027il existe une fonction sur les noeuds, arcs, étiquettes et graphes de la sé-quence. Prado et al. (2013a) proposent un algorithme de fouille de sous-graphes planaires fréquents à partir d\u0027une base de données de graphes planaires. Ces motifs peuvent être utilisés comme base pour l\u0027extraction de motifs spatio-temporels. Les graphes attribués ont également été étudiés. Moser et al. (2009) ont proposé une méthode pour trouver des sous-graphes homogènes denses, c\u0027est à dire qui partagent un grand nombre d\u0027attributs. Silva et al. (2012) proposent l\u0027extraction de paires de sous-graphes et d\u0027ensembles d\u0027attributs booléens tels que les attributs sont fortement corrélés avec les sous-graphes. Mougel et al. (2012) recherchent des collections de k-cliques percolées homogènes, c\u0027est à dire des ensembles de cliques qui se chevauchent et partagent un ensemble d\u0027attributs. Prado et al. (2013b) proposent une mé-thode pour trouver des régularités sur les descripteurs des attributs dans des graphes attribués. Pour cela, ils utilisent les attributs associés aux noeuds ainsi que des propriétés topologiques calculées pour chaque noeud. Récemment des études ont également été faites sur les graphes attribués dynamiques. Boden et al. (2012) proposent d\u0027extraire des clusters dans des graphes attribués puis d\u0027associer les clusters similaires à des temps consécutifs. Jin et al. (2007) étu-dient les graphes dynamiques dont les noeuds sont associés à un poids. Ils extraient des groupes de noeuds connectés dont le poids suit une évolution similaire croissant ou décroissant sur des temps consécutifs. Desmier et al. (2013) extraient des ensembles de noeuds similaires ayant une même tendance dans le temps sur leurs attributs et une évolution différente du reste du graphe. Aucun de ces travaux n\u0027intègre de connaissance a priori via des hiérarchies. L\u0027utilisation de connaissances utilisateur dans le processus d\u0027extraction est très étudiée en fouille de données. Cela peut se faire en pré-traitement des données, en processus itératif lors de l\u0027extraction ou encore pendant l\u0027extraction. Nous présentons ici des méthodes utilisant une hiérarchie comme connaissance à priori. Srikant et Agrawal (1996) proposent l\u0027extraction de « séquences étendues ». Ils utilisent une taxonomie sur les objets pour extraire des séquences possédant plusieurs niveaux de hiérarchie. Han et Fu (1999)  \nConclusion\nNous nous sommes intéressés à l\u0027extraction de motifs hiérarchiques de co-évolution dans des graphes attribués dynamiques, c\u0027est à dire un sous-graphe connexe induit par un ensemble de noeuds de temps et d\u0027attributs appartenant à une hiérarchie et associés à une tendance respectée par les noeuds. Nous avons proposé plusieurs contraintes qui permettent d\u0027obtenir des motifs concis et sans perte d\u0027information et nous avons présenté un algorithme complet exploitant l\u0027ensemble des contraintes. Les résultats des expérimentations sur trois jeux de données réels montrent que l\u0027utilisation de la hiérarchie permet d\u0027obtenir un nombre restreint de motifs. Deux perspectives semblent intéressantes, premièrement, la possibilité d\u0027avoir une hiérarchie à héritage multiple ; deuxièmement créer une hiérarchie sur les temps et les noeuds du graphe.\nRemerciements Les auteurs remercient l\u0027ANR pour le financement de ce travail à travers le projet FOSTER (ANR-2010-COSI-012-02), ainsi que le Centre de Calcul du CNRS/IN2P3.\n"
  },
  {
    "id": "338",
    "text": "Introduction\nLa classification non-supervisée a pour but de regrouper les observations proches dans un même groupe, tandis que les observations éloignées doivent être affectées à des groupes différents. Cette définition pourrait être insuffisante dans de nombreuses applications de regroupement dans lesquelles un objet peut appartenir à la fois à plusieurs classes. Ce type de problématique est appelée classification recouvrante ou encore classification non-exclusive. Plusieurs applications réelles nécessitent d\u0027utiliser ce type de schéma de classification tels que le regroupement de documents où chaque document peut aborder plusieurs thèmes, la classification de vidéos où un film peut avoir différents genres (Snoek et al., 2006), la détection d\u0027émotions où un morceau de musique peut évoquer plusieurs émotions distinctes (Wieczorkowska et al., 2006).\nAfin de produire des classes non-exclusives, divers méthodes ont été proposées utilisants des approches hiérarchique (Diday, 1984), de partitionnement (Fu et Banerjee, 2008;Banerjee et al., 2005;Heller et Ghahramani, 2007), de corrélation (Bonchi et al., 2011) et de la théorie des graphes (Fellows et al., 2009(Fellows et al., , 2011. Nos travaux se limitent à l\u0027étude des méthodes recouvrantes basées sur le partitionnement. Les méthodes de partitionnement existantes utilisent des modèles de mélange de lois (Fu et Banerjee, 2008;Banerjee et al., 2005;Heller et Ghahramani, 2007) ou bien utilisent l\u0027approche k-moyennes (Cleuziou, 2008). Des exemples de ces méthodes sont OKM (Cleuziou, 2008) et Parameterized R-OKM (Ben N\u0027Cir et al., 2013. Ces dernière méthodes ne considèrent que la distance euclidienne entre chaque observation et ses représentants. En outre, la densité des observations dans un groupe pourrait être nettement différente des autres groupes dans l\u0027ensemble de données. La métrique euclidienne utilisée évalue seulement la distance Euclidienne entre l\u0027observation et le représentant, ou la combinaison de représentants, de clusters. Elle ne tient pas compte de la variation de la distance globale pour toutes les observations dans un groupe.\nRécemment, une nouvelle mesure de distance a été proposée par Tsai et Lin (2011). Elle intègre la variation de distance au sein d\u0027un même cluster et sert à régulariser la distance entre un objet et le représentant de cluster relativement à la densité interne du cluster en question. Nous proposons dans ce qui suit d\u0027adapter cette nouvelle mesure pour détecter des groupes non-disjoints avec des densités différentes.\nCet article est organisé comme suit : la Section 2 décrit deux méthodes existantes de classification recouvrante à savoir OKM et Parameterized R-OKM. Ensuite, la Section 3 présente le problème d\u0027identification des groupes avec une densité différente. La Section 4 décrit les méthodes proposées OKM-? et Parameterized R-OKM-? tandis que la Section 5 illustre les expériences réalisées sur des ensembles de données artificielles. La conclusion et les perspectives feront l\u0027objet de la Section 6.\nLa classification recouvrante\nNous décrivons dans cette partie deux méthodes existantes de classification recouvrantes basées sur l\u0027algorithme k-moyennes. Ces méthodes généralisent k-moyennes pour produire des recouvrements de classes.\nOverlapping K-Means\nLa méthode OKM cherche des recouvrements optimaux plutôt que des partitions optimales, étant donné un ensemble d\u0027objets à classifier\net N le nombre d\u0027objets, OKM recherche un k recouvrement de telle sorte que la fonction objective suivante soit optimisée :\noù x i désigne l\u0027image de x i définie par la combinaison des centres des clusters auxquels x i appartient :\nk?? i avec ? i l\u0027ensemble des affectations de l\u0027objet x i aux différents clusters, c\u0027est-à-dire les clusters auxquels x i appartient et c k correspond au représentant du cluster k. Le critère J de la fonction objective généralise le critère des moindres carrés utilisé dans la méthode k-moyennes. Pour minimiser ce critère, deux étapes principales sont exécutées itérativement :\n1. la mise à jour des représentants de classes (C) puis.\n2. l\u0027affectation des objets à ces représentants (?).\nles conditions d\u0027arrêt de la méthode reposent sur plusieurs critères à savoir le nombre d\u0027itéra-tions maximales et le seuil minimal d\u0027amélioration de la fonction objective entre deux itéra-tions. OKM produit des classes non-disjointes avec des zones de recouvrement trop larges. Cependant, les groupes ayants des zones de recouvrement larges ne sont pas appropriés à la plupart des applications réelles. Pour résoudre ce problème, une méthode récente, référencée Parameterized R-OKM (Ben N\u0027 Cir et al., 2013), propose un nouveau modèle qui produit des clusters non-disjoints avec possibilité de contrôle sur les zones de recouvrements.\nParameterized R-OKM\nAfin de contrôler la taille des recouvrements entre les classes, Parameterized R-OKM restreint l\u0027attribution d\u0027une observation à plusieurs groupes en fonction du nombre d\u0027affectations |? i |. Parameterized R-OKM est basée sur la minimisation du critère objetif suivant : une mesure, intégrant la variation de densité entre les classes, à été introduite par Tsai et Lin (2011). Cette technique à été utilisée dans k-moyenne floues et a été référencée (FCM-? ). FCM-? introduit la variation de densité pour chaque groupe de données afin de régulariser la distance entre un objet et le représentant. Cette mesure peut être mieux appliquée sur des données contenant des groupes de densités différentes. La nouvelle métrique de distance entre chaque objet x i et chaque représentant c k est définie par :\noù ? k représente la moyenne pondérée des distances dans un cluster k, définie par\nik , FCM-? minimise la fonction objective suivante :\nLa mise à jour des centres et la mise à jour des degrés d\u0027appartenance aux différents clusters sont déterminées par :\nL\u0027évaluation de FCM-? sur des données de densités equilibrées montre que cette méthode donne les mêmes résultats que FCM (Tsai et Lin, 2011). Par contre, si les données contiennent des groupes de différentes densités, FCM-? est plus performant que FCM.\nMéthodes Proposées\nDans les domaines d\u0027applications de la classification recouvrante, l\u0027algorithme d\u0027apprentissage doit être capable de détecter des groupes non disjoints avec des densités uniformes et non uniformes. Par conséquent, nous proposons d\u0027étendre OKM et Parameterized R-OKM pour détecter ces types de groupes en introduisant une régularisation de la densité dans la fonction objective de ces méthodes. Les nouvelles méthodes proposées sont désignées OKM-? et Parameterized R-OKM-?.\nOverlapping k-means-? (OKM-?)\nPour tenir compte des différences de densité entre les classes, nous introduisons un facteur de régularisation ? i pour chaque observation x i . Compte tenu de l\u0027ensemble des N observations, OKM-? minimise le critère objectif suivant :\ni\u003d1 où ? i est la valeur minimale des densités des groupes ? i auxquels l\u0027observation x i appartient :\navec ? k le poids local au cluster k qui mesure le degré de déviation des observations contenues dans le cluster k par rapport à leur image respective. Ce poids peut être décrit formellement par :\noù P ik une variable binaire indiquant l\u0027appartenance de l\u0027objet x i au cluster k.\nParameterized R-OKM-?\nEn se basant sur le même principe de régularisation de densité, nous proposons la mé-thode Parameterized R-OKM-? permettant la régularisation des variances de densité entre les classes. Le nouveau critère objectif proposé est décrit par :\ni\u003d1 où ? i le facteur de régularisation locale de l\u0027observation x i décrit de la même manière que dans l\u0027Equation (10) de OKM-?. Cependant, la nouvelle densité du groupe ? k est définie pour Parameterized R-OKM-? par :\nRésolution algorithmique\nLa minimisation de la fonction objective de chaque méthode proposée (OKM-? and Parameterized R-OKM-?) est réalisée par itération de trois étapes indépendantes : (1) calcul de représentants de groupes C, (2) affectation multiple (?) d\u0027observations à un ou à plusieurs groupes et (3) le calcul de poids (? k ) pour chaque classe.\nSachant que OKM-? est un cas particulier de Parameterized R-OKM-? (quand ? \u003d 0), nous présentons dans l\u0027algorithme 1 les différentes étapes de Parameterized R-OKM-?. Cet algorithme utilise la fonction ASSIGN ?? permettant l\u0027affectation multiple de chaque observation à un ou plusieurs groupes. Cette fonction utilise une heuristique, utilisée aussi dans OKM et Parameterized R-OKM, permettant de réduire l\u0027espace exponentiel (en terme de nombre de classes) des affectations possibles. Cette heuristique consiste à continuer l\u0027affectation de l\u0027observation au cluster le plus proche tant que le critère objectif est amélioré. Les différentes étapes de ASSIGN ?? sont décrites dans l\u0027algorithme 2.    \nExpérimentations\nCette section évalue l\u0027efficacité de OKM-? et Parameterized R-OKM-? sur des ensembles de données artificielles. Afin d\u0027évaluer la capacité des méthodes proposées à produire des groupes non-disjoints sur des ensembles de données ayant des densités différentes, nous avons généré deux jeux de données artificielles référencés \"Ensemble 1\" et \"Ensemble 2\". Le premier jeu de données contient deux classes où chaque classe est formée de 500 observations décrites dans un espace à deux dimensions. Les deux classes ont des densités différentes : la classe \"bleue\" a une grande densité par rapport à la classe \"rouge\". Pour le deuxième ensemble de données, nous avons modifié le rayon du groupe \"rouge\" qui devient plus grand.\nPour donner au lecteur une interprétation visuelle de la performance des méthodes proposées par rapport à OKM et Parameterized R-OKM, nous commençons par visualiser les classes obtenues dans un espace à deux dimensions. jeux de données artificielles. Ces résultats montrent que les méthodes proposées surpassent les méthodes originales en terme de F-mesure. Par exemple, en utilisant OKM-? sur \"l\u0027ensemble 2\", la F-mesure obtenue augmente de 0.817 à 0.921 par rapport à OKM. L\u0027amélioration de la F-mesure sur les deux jeux de données est due à l\u0027amélioration de la Précision. Ce résultat s\u0027explique par le fait que les méthodes originales construisent des recouvrements de plus en plus larges à mesure que la densité entre les deux classes diffère. Cependant, en utilisant les méthodes proposées, le taux de recouvrements est réduit.\nConclusion\nNous avons proposé dans ce travail deux nouvelles méthodes capables de produire des classes non-disjointes lorsque les données s\u0027organisent en groupes de densités différentes. Ces nouvelles méthodes s\u0027appuient sur une nouvelle mesure de distance qui régularise la variation de densité entre les classes obtenues. Des expériences réalisées sur des ensembles de données artificielles montrent l\u0027efficacité des méthodes proposées par rapport à celles existantes.\nL\u0027évaluation des méthodes proposées est effectuée sur des ensembles de données artificielles. Comme perspectives, nous envisageons de confirmer ces résultats sur des jeux de données réels.\n"
  },
  {
    "id": "339",
    "text": "Introduction\nLes réseaux complexes sont des graphes modélisant des systèmes réels. La structure de communautés (Fortunato, 2010) d\u0027un réseau complexe est une partition de l\u0027ensemble des noeuds, dont les parties (communautés) sont des groupes de noeuds densément interconnectés. Cette structure permet l\u0027étude du réseau à un niveau intermédiaire, par comparaison avec les plus classiques niveaux local (voisinage du noeud) et global (réseau entier). Le rôle communautaire décrit ainsi la position d\u0027un noeud dans le réseau à ce niveau. Il a été initialement introduit par Guimerà et Amaral (2005). Ces auteurs caractérisent le positionnement communautaire de chaque noeud au moyen de deux mesures topologiques ad hoc. Les noeuds sont ensuite caté-gorisés au moyen de seuils prédéfinis pour ces mesures. Cette approche peut être critiquée sur trois points. Premièrement, elle est définie seulement pour des réseaux non-orientés. Deuxiè-mement, les mesures utilisées ne prennent pas en compte tous les aspects de la connectivité communautaire d\u0027un noeud. Troisièmement, rien ne garantit que les seuils fixés empiriquement pour définir les rôles soient pertinents pour d\u0027autres données. Nous proposons des solutions à ces trois problèmes. Pour le premier, nous adaptons les mesures de Guimerà \u0026 Amaral aux réseaux orientés. Pour le deuxième, nous définissons des mesures supplémentaires distinguant trois aspects de la connectivité communautaire : diversité des communautés, hétérogénéité de la distribution des liens, et intensité de la connexion. Pour le troisième, nous proposons une méthode non-supervisée de définition des rôles. Afin d\u0027illustrer l\u0027intérêt de notre méthode, nous l\u0027appliquons à l\u0027étude du rôle communautaire d\u0027un type particulier d\u0027utilisateur de Twitter, appelé capitaliste social. Ces utilisateurs mettent en oeuvre deux principes simples pour accroître leur nombre de followers et donc leur visibilité. Follow Me, I Follow You (FMIFY) : le capitaliste promet aux utilisateurs qui le suivent de les suivre en retour. I Follow You, Follow Me (IFYFM) : le capitaliste suit un maximum d\u0027utilisateurs, en espérant être suivi en retour.\nDans la section suivante, nous décrivons l\u0027approche originale de Guimerà \u0026 Amaral. Nous mettons ensuite en évidence ses limitations et décrivons les trois modifications que nous proposons pour les résoudre. Dans la section 3, nous présentons les rôles obtenus sur le réseau Twitter et discutons du positionnement des capitalistes sociaux. Enfin, nous concluons en indiquant les perspectives ouvertes par ce travail.\nMéthode proposée 2.1 Approche originale\nPour caractériser les rôles des noeuds, Guimerà \u0026 Amaral définissent deux mesures, qui leur permettent de placer chaque noeud dans un espace bidimensionnel. Puis, ils proposent plusieurs seuils pour discrétiser cet espace, chaque zone ainsi définie correspondant à un rôle. La première mesure, le degré intra-module traite de la connectivité interne du noeud, i.e. des liens avec sa propre communauté. Elle est basée sur la notion de z-score. Comme celle-ci sera réutilisée plus loin, nous la définissons ici de façon générique.\nÉquation 1 (Z-score). Pour une mesure nodale quelconque f (u), permettant d\u0027associer une valeur numérique à un noeud u, le z-score Z f (u) par rapport à la communauté de u est :\ndé-notent respectivement la moyenne et l\u0027écart-type de f sur les noeuds de la communauté C i .\nLe degré intra-module z(u) correspond au z-score du degré interne, calculé pour la communauté du noeud considéré. On l\u0027obtient donc en substituant le degré interne d int à f dans l\u0027équation (1). La seconde mesure, appelée coefficient de participation, traite de la connectivité externe du noeud, i.e. relative à toutes les communautés auxquelles il est lié. \nOrientation des liens\nAspects de la connectivité externe\nLe coefficient de participation se concentre sur un aspect de la connectivité externe d\u0027un noeud : l\u0027hétérogénéité de la distribution de ses liens, relativement aux communautés auxquelles il est connecté. Mais il est possible de caractériser cette connectivité de deux autres manières. On peut considérer sa diversité, c\u0027est à dire le nombre de communautés concernées, ainsi que son intensité, i.e. le nombre de liens concernés. Par souci de simplicité, nous présen-tons les mesures dans un contexte non-orienté. L\u0027adaptation aux réseaux orientés peut se faire en distinguant les liens entrants et sortants, comme en section 2.2.\nDiversité. La diversité D(u) évalue le nombre de communautés différentes auxquelles le noeud u est connecté. Soit le nombre de communautés, autres que la sienne, auxquelles u est connecté. D(u) est définie comme le z-score d\u0027 relativement à la communauté de u.\nIntensité externe. L\u0027intensité externe I ext (u) mesure la force de la connexion de u à des communautés externes, en termes de nombre de liens. Soit d ext (u) le degré externe de u, i.e. le nombre de liens que u possède avec des noeuds n\u0027appartenant pas à sa communauté. I ext (u) est alors définie comme le z-score du degré externe. Hétérogénéité. L\u0027hétérogénéité H(u) quantifie la variation du nombre de connexions externes de u d\u0027une communauté à l\u0027autre. Nous utilisons l\u0027écart-type du nombre de liens externes que u possède par communauté, noté ?(u), et définission H(u) comme le z-score de ?, relativement à la communauté de u.\nIntensité interne. Pour représenter la connectivité interne du noeud, nous conservons la mesure z de Guimerà \u0026 Amaral. Celle-ci est construite sur la base du z-score, et est donc cohérente avec les mesures définies pour décrire la connectivité externe. Cependant, par symé-trie avec notre intensité externe, nous désignons z sous le nom d\u0027intensité interne, et la notons I int (u). Guimerà et Amaral (2005) supposent que les seuils sur les mesures établis de façons empiriques pour définir les rôles sont indépendants des jeux de données utilisés. Pourtant, seule P est normalisée sur un intervalle fixé. En effet, z n\u0027est pas limitée, et donc rien ne garantit que le seuil défini pour cette mesure reste cohérent pour d\u0027autres réseaux. Cet argument est d\u0027autant plus fort que toutes nos mesures présentées en section 2.3 sont des z-scores. De plus, leur nombre élevé (8) rend l\u0027utilisation de la typologie originale impossible. Afin de contourner ces problèmes, nous proposons d\u0027appliquer une méthode de classification non supervisée. Dans un premier temps, nous calculons l\u0027ensemble des mesures sur les données considérées. Ensuite, nous appliquons une analyse de regroupement. Chaque groupe ainsi identifié correspond a un rôle communautaire.\nIdentification non-supervisée des rôles\nRésultats\nLe réseau sur lequel nous avons travaillé comporte un peu moins de 55 millions de noeuds représentant les utilisateurs de Twitter, et près de 2 milliards d\u0027arcs orientés qui matérialisent les abonnements entre utilisateurs. La détection de communautés a été réalisée au moyen de l\u0027algorithme de Louvain (Blondel et al., 2008). L\u0027analyse de regroupement a ensuite été menée au moyen d\u0027une implémentation libre et distribuée de l\u0027algorithme des k-moyennes (Liao, 2009). Nous avons appliqué cet algorithme pour des valeurs de k allant de 2 à 15, et avons sélectionné la meilleure partition d\u0027après l\u0027indice de Davies et Bouldin (1979). Pour valider les résultats obtenus, nous étudions les rôles détectés pour les capitalistes sociaux, identifiés via la méthode proposée par Dugué et Perez (2013). Nous distinguons différentes catégories de capitalistes sociaux en fonction de deux de leurs caractéristiques topologiques. La première est le ratio. Il s\u0027agit du nombre de followees divisé par le nombre de followers. Ce critère permet de distinguer ceux qui appliquent la méthode FMIFY (ratio inférieur à 1) de ceux utilisant IFYFM (ratio supérieur à 1). La seconde est le degré entrant : nous séparons ceux de faible degré (entre 500 et 10000) et ceux de degré élevé (supérieur à 10000).\nCalculs effectués\nNous avons d\u0027abord appliqué l\u0027approche originale (non-orientée) de Guimerà \u0026 Amaral sur nos données. Les valeurs de z obtenues sont bien supérieures à celles observées dans Guimerà et Amaral (2005). Le seuil défini pour z n\u0027est ainsi plus utilisable pour l\u0027identification des rôles. Nous avons donc procédé à une analyse de regroupement qui identifie 2 rôles, contenant chacun trop de noeuds pour obtenir une information pertinente. Nous avons ensuite appliqué l\u0027approche originale orientée (section 2.2). L\u0027analyse de regroupement a identifié 6 rôles, ce qui montre l\u0027intérêt des mesures orientées. Néanmoins, lorsque l\u0027on regarde le positionnement des capitalistes sociaux au sein de ces groupes, certaines incohérences apparaissent. Une large majorité des capitalistes sociaux de degré élevé est ainsi classée comme non-pivots périphé-riques ou ultra-périphériques. Ces noeuds ont pourtant un degré entrant supérieur à 10000. Si ces noeuds ne sont pas pivots, donc peu connectés en interne, ils devraient néanmoins être connectés avec l\u0027extérieur. Cela vient des limitations de la participation, comme indiqué en section 2.3. Nous avons enfin appliqué le dernier groupe de 8 mesures, aboutissant aux résul-tats discutés dans le reste de cette section.\nÉtude des groupes\nL\u0027analyse de regroupement nous donne k \u003d 6 groupes (Tableau 1). Nous caractérisons ces groupes relativement à nos huit mesures, afin d\u0027en identifier les rôles. Dans les groupes 1, 4 et 5, presque toutes les mesures sont négatives et proches de 0. Il ne s\u0027agit donc pas de pivots (noeud largement connecté à sa communauté) ni de de noeud qualifiés de connecteurs (ayant une connexion privilégiée avec d\u0027autres communautés que la leur). La diversité entrante (resp. sortante) du groupe 4 (resp. 5) est l\u0027unique mesure positive, ceci indique que ces noeuds reçoivent (resp. envoient) des liens d\u0027un nombre relativement élevé de communautés et sont moins isolés. Toutes les mesures sont positives dans le groupe 6. L\u0027intensité interne reste proche de 0, donc on ne peut toujours pas parler de pivot. L\u0027intensité externe faible, mais positive, et la diversité élevée permettent de considérer ces noeuds comme des connecteurs. Toutes les mesures du groupe 3 sont largement positives L\u0027intensité interne élevée associe ce groupe au rôle de pivot. Les valeurs externes montrent que ces noeuds sont connectés à de nombreux noeuds présents dans de nombreuses autres communautés. Toutefois, les liens sortants sont plus nombreux, ces noeuds correspondent donc à des utilisateurs plus suiveurs que suivis. Toutes les mesures du groupe 2 sont particulièrement élevées. Pour une mesure donnée, la variante concernant les liens entrants est toujours largement supérieure, ce qui signifie que les utilisateurs représentés par ces noeuds sont particulièrement suivis. Nous associons ce groupe au rôle de pivot orphelin.\nGroupe\nTaille \nPositionnement des capitalistes sociaux\nAvec la méthode définie par Dugué et Perez (2013), nous détectons près de 160000 capitalistes sociaux. Nous étudions leur positionnement dans les 6 groupes identifiés. Les capitalistes sociaux de faible degré se retrouvent dans trois groupes : 3, 5 et 6. Les noeuds du groupe 3 sont des pivots connecteurs qui suivent plus d\u0027utilisateurs du réseau que la normale, ce qui est cohérent avec le comportement des capitalistes sociaux. Il semble également cohérent d\u0027observer que les capitalistes sociaux dont le degré sortant est supérieur au degré entrant sont près de deux fois plus présents dans ce groupe que les autres. La majorité des capitalistes sociaux de faible degré se place au sein du groupe 6, non-pivot connecteur. Ces noeuds légèrement plus connectés au sein de leur communauté et avec l\u0027extérieur que la moyenne, ont en revanche une diversité bien plus élevée. Les capitalistes sociaux qui s\u0027y situent semblent ainsi avoir débuté l\u0027application de leurs méthodes, en créant des liens avec de nombreuses autres communautés. Les capitalistes sociaux de degré élevé se placent presque exclusivement dans les groupes 2 et 3, pivots connecteurs et orphelins. Cela semble cohérent avec les degrés élevés de ces noeuds.\nLes noeuds classés dans le groupe 2 sont ceux de ratio inférieur à 0,7 avec beaucoup plus de followers que de followees, ce qui correspond à la définition du rôle donné par nos mesures. Notre approche établit une séparation entre capitalistes sociaux de faible degré, majoritairement connecteurs et non-pivots et ceux de degré élevé, classés pivots. \nRatio\nPerspectives\nLe travail présenté peut s\u0027étendre de différentes façons. Tout d\u0027abord, certains des rôles définis dans Guimerà et Amaral (2005) n\u0027apparaissent pas dans notre analyse. Il serait intéres-sant d\u0027étudier d\u0027autres réseaux afin de déterminer si cette observation reste valable. Une autre piste consiste à baser nos calculs sur des communautés recouvrantes (i.e. non-mutuellement exclusives). En effet, les réseaux sociaux que nous étudions sont réputés posséder ce type de structures, dans lesquelles un noeud peut appartenir à plusieurs communautés en même temps. L\u0027adaptation de nos mesures à ce contexte se ferait naturellement, en définissant des versions internes de l\u0027hétérogénéité et de la diversité.\n"
  },
  {
    "id": "342",
    "text": "Contexte et objectifs\nLe modèle Linked Data vise à favoriser l\u0027interopérabilité de sources de données hétéro-gènes sur le Web, en proposant un ensemble de bonnes pratiques de publication de données qui s\u0027appuient sur les technologies du Web sémantique. En particulier, il préconise de décrire les données selon le modèle RDF, de fournir les vocabulaires utilisés par les données sous la forme d\u0027ontologies et d\u0027interconnecter les différents jeux de données disponibles en identifiant les ressources équivalentes d\u0027un jeu de données à l\u0027autre.\nParmi les données publiées sous la forme de Linked Data, de nombreuses ressources font référence, de façon directe ou indirecte, à une localisation. Dans DBpedia 1 , par exemple, 1373482 objets sont décrits par les propriétés geo:long et geo:lat. Geonames 2 contient plus de 8500000 entités spatiales nommées et environ 125000000 triplets RDF. La présence d\u0027une information de localisation dans la description d\u0027une ressource signifie que cette ressource est liée d\u0027une manière ou d\u0027une autre à une entité géographique du terrain réel. Deux ressources dotées d\u0027informations de localisation identiques ou spatialement proches seront donc très susceptibles de présenter une relation sémantique. La prise en compte d\u0027informations de localisation s\u0027avère donc généralement importante pour l\u0027interconnexion, et est le plus souvent réalisée au travers de mesures de distance géographiques entre les ressources à interconnecter. Cependant, les informations de localisation associées à des ressources Linked Data peuvent s\u0027avérer très hétérogènes d\u0027une source de données à l\u0027autre, qu\u0027il s\u0027agisse d\u0027hétérogénéités liées au type de localisation utilisé (directe via des coordonnées ou indirecte via une adresse par exemple), à l\u0027origine de cette information, à sa précision ou au vocabulaire adopté pour la décrire. Cette hétérogénéité des informations de localisation peut rendre l\u0027utilisation de mesures de distances géographiques pour l\u0027interconnexion peu fiable voire impossible.\nDans cet article, nous proposons de mettre à profit des bases de données géographiques de référence pour l\u0027interconnexion et l\u0027exploitation de sources de données thématiques dotées d\u0027informations de localisation hétérogènes. En effet, nous proposons d\u0027ancrer les différentes ressources à interconnecter sur un référentiel géographique afin de favoriser la détection de relations d\u0027interconnexion entre ces ressources. Parmi les applications découlant de cet ancrage de données thématiques à une base de données géographique de référence, la visualisation cartographique permet leur mise en valeur en fournissant aux utilisateurs un moyen convivial de découverte de ces données. Nous proposons donc une approche de visualisation cartographique des données thématiques ainsi interconnectées au référentiel géographique faisant porter les informations qu\u0027elles renferment par les objets géographiques eux-mêmes et permettant leur visualisation à différentes échelles.\nLa section suivante dresse un état de l\u0027art des approches proposées dans le domaine de l\u0027interconnexion de données dotées d\u0027informations de localisation, et dans le domaine de la visualisation cartographique de données Linked Data. La section 3 présente l\u0027approche que nous proposons, pour l\u0027ancrage de Linked Data sur un référentiel géographique et la visualisation des données. La section 4 décrit la mise en oeuvre de cette approche sur des données décrivant des monuments historiques. La section 5 conclut cet article.\nInterconnexion et visualisation de données thématiques dotées d\u0027informations de localisation 2.1 Utilisation d\u0027informations de localisation pour l\u0027interconnexion\nL\u0027interconnexion désigne l\u0027étape du processus de publication de données sur le Web des données qui vise à créer des liens d\u0027équivalence entre les ressources des différentes sources de données qui représentent la même entité du monde réel. Cette tâche est généralement fondée sur l\u0027évaluation du degré de similarité entre ces ressources par comparaison de leurs proprié-tés, sous l\u0027hypothèse que plus la similarité entre les descriptions des différentes ressources est importante, plus la probabilité qu\u0027elles représentent la même réalité l\u0027est aussi Ferraram et al. (2013). L\u0027automatisation de cette tâche d\u0027interconnexion s\u0027appuie donc sur des approches et des mesures de similarité proposées dans des domaines ayant des besoins similaires d\u0027identification de relations de correspondance, comme la réconciliation de références ou l\u0027alignement d\u0027ontologies Heath et Bizer (2011).\nDifférentes approches ont été proposées afin de détecter automatiquement des relations de correspondance entre objets issus de bases de données géographiques hétérogènes qui représentent une même entité topographique du monde réel. Tout comme dans le domaine de l\u0027interconnexion de Linked Data, l\u0027appariement de données géographiques repose sur l\u0027éva-luation de la similarité des objets des différentes bases par comparaison de leurs propriétés. Le critère d\u0027appariement privilégié est la géométrie des données qui représente la forme et la localisation des entités du monde réel via des primitives géométriques (point, ligne ou polygone). Différentes approches ont été proposées pour traiter chaque cas de figure : comparaison de données ponctuelles Minami (2000)   Samal et al. (2004). Dans chaque cas, la quantification du niveau de similarité entre les géométries est réalisée à l\u0027aide d\u0027une mesure spécifique. Des approches multicritères combinant critères géométriques, attributaires ou encore topologiques ont également été proposées Olteanu (2008).\nA la croisée des deux domaines, des approches qui visent à trouver des équivalences entre des données Linked Data dotées d\u0027informations de localisation ont également été proposées. Ces approches s\u0027appuient sur les mesures de similarité entre géométries utilisées dans le domaine de l\u0027appariement de données géographiques Salas et Harth (2011). Elles sont le plus souvent appliquées sur des données issues de bases de données géographiques représentées selon le modèle RDF et publiées sur le Web des données. L\u0027approche proposée par Hahmann et Burghardt (2010) prend en considération des objets décrits par des géométries de type point et se fonde sur deux critères : le nom et la géométrie. La similarité entre deux points est estimée via un critère d\u0027inclusion bilatérale d\u0027un point candidat à l\u0027appariement dans un rectangle englobant centré sur un point potentiellement homologue. L\u0027approche proposée par Salas et Harth (2011) repose sur une projection cylindrique équidistante préalable des géométries, afin de pouvoir utiliser une mesure de distance de Hausdorff entre les géométries de type polygone que les auteurs souhaitent comparer pour détecter des cas de superposition entre géométries.\nVisualisation cartographique des Linked Data\nLa plupart des approches de visualisation cartographique de données Linked Data visent à permettre l\u0027affichage de données géographiques publiées selon le format RDF à l\u0027aide d\u0027outils de cartographie sur le Web. C\u0027est le cas par exemple pour les données géographiques publiées par l\u0027Ordnance Survey \nApproche proposée\nLes approches d\u0027interconnexion de données Linked Data dotées d\u0027informations de localisation proposées s\u0027appuient sur des mesures permettant de comparer des informations de localisation du même type, qu\u0027il s\u0027agisse de calculer des distances entre géométries ou d\u0027éva-luer la similarité de chaînes de caractères représentant des adresses postales ou des toponymes. Dans le cas de ressources présentant des informations de localisation de différentes natures, il est donc nécessaire de procéder au géoréférencement des informations disponibles afin de disposer de coordonnées géographiques pour chaque source de données à comparer. De plus, dans le cas d\u0027informations de localisation du même type, des différences de précision ou d\u0027origine des données peuvent engendrer des valeurs de distance géographique très importantes entre les ressources à comparer, alors que leurs géométries se rapportent bien à une même entité topographique du monde réel. C\u0027est pourquoi, nous proposons de mettre à profit des bases de données géographiques de référence comme ressources de support pour l\u0027interconnexion de sources de données thématiques dotées d\u0027informations de localisation hétérogènes. Suivant l\u0027approche proposée par Aleksovski et al. (2006) dans le domaine de l\u0027alignement d\u0027ontologies, nous proposons d\u0027ancrer les différentes ressources à interconnecter sur un ré-férentiel géographique afin de favoriser la détection de relations d\u0027interconnexion entre ces ressources. La figure 1 illustre cette approche sur un exemple d\u0027interconnexion de ressources localisées dans le premier cas, par des coordonnées géographiques et dans le second, par des adresses. Le géocodage via la base de données d\u0027adresses permet de disposer de coordonnées géographiques pour la seconde ressource. En revanche, ces coordonnées demeurent trop éloi-gnées de celles de la première ressource pour permettre une interconnexion. Le fait d\u0027ancrer les deux ressources sur le bâtiment auquel elles se rapportent permet finalement de les mettre en correspondance.\nL\u0027étape d\u0027ancrage des sources d\u0027informations thématiques au référentiel géographique né-cessite de détecter des liens de correspondance entre ressources thématiques et objets géogra-phiques auxquels elles se rapportent. Dans certains cas, il ne s\u0027agira pas nécessairement de liens d\u0027équivalence. Cependant, dans la mesure où les indications de localisation fournies avec les données thématiques font référence à des points de l\u0027espace proches des objets géographiques auxquels on devra les ancrer, les approches proposées pour l\u0027interconnexion de données dotées d\u0027informations de localisation pourront être mises à profit lors de cette étape. Nous proposons donc pour cette étape d\u0027ancrage de nous appuyer sur les outils d\u0027interconnexion de données existants, et de les compléter à l\u0027aide d\u0027approches proposées dans le domaine de l\u0027appariement de données géographiques.\nNotre approche doit permettre la transformation des données géographiques dans le modèle RDF, afin de permettre leur traitement via un outil d\u0027interconnexion de Linked Data disposant Celle-ci permet la transformation selon le modèle RDF de données représentées selon diffé-rents modèles, y compris des données géographiques sous format GML ou SHP (format vectoriel Shapefile). Les monuments historiques décrits par la base Mérimée sont localisés à l\u0027aide d\u0027adresses postales multiples. Nous avons donc dû procéder, après l\u0027étape de conversion, à un nettoyage de ces données à l\u0027aide de requêtes de type CONSTRUCT, afin de pouvoir traiter chaque adresse de façon individuelle lors de l\u0027interconnexion. Dans le cas de la BD TOPO R nous avons converti les données sur les bâtiments de Paris. Chaque bâtiment est décrit par une géométrie de type polygone. Dans le cas de la BD ADRESSE R il s\u0027agit de données décrivant des adresses de façon structurée, et associant à chaque adresse une géométrie de type point.\nL\u0027ensemble des données est ensuite stocké dans un triple store local (OpenRDF Sesame) pour faciliter l\u0027étape d\u0027interconnexion. \nMesure de distance entre géométries vectorielles\nLes indications de localisations généralement utilisées dans les sources de données Linked Data, sont le plus souvent des adresses, des géométries de type \" point \" ou des coordonnées de type longitude et latitude. Les géométries représentant les objets géographiques dans les bases de données géographiques décrivent la réalité d\u0027une manière plus détaillée : outre des points, on rencontre fréquemment des lignes ou des polygones. La mesure de distance que nous avons choisie d\u0027utiliser pour ancrer des données Linked Data sur le référentiel géographique calcule donc la distance minimale entre un point et une géométrie de type quelconque, ou retourne une valeur nulle si le point est inclus dans cette géométrie. Les coordonnées des géométries peuvent être exprimées dans différents systèmes de coordonnées. Nous commençons donc par 11. https ://www.assembla.com/spaces/silk/wiki/Home projeter toutes les coordonnées dans le même système de projection. Ainsi, nous pouvons faire reposer le calcul de notre mesure de distance sur le mesure de la distance euclidienne minimale entre deux objets de la bibliothèque Java Geotools 12 , et l\u0027intégrer sous forme de plugin dans l\u0027outil d\u0027interconnexion Silk ( (3)  figure 2).\nParamétrage de l\u0027interconnexion\nFIG. 3 -Interconnexions réalisées.\nLa première étape de notre démarche d\u0027interconnexion est l\u0027ancrage des données sur les monuments historiques issues de DBpedia sur les bâtiments de la base BD TOPO R ((1) figure 3). Dans ce cas, le seul critère d\u0027interconnexion pouvant être utilisé est la localisation. Nous avons donc créé et exécuté un script Silk utilisant notre mesure de distance entre les positions ponctuelles de monuments de DBPedia et les polygones décrivant les bâtiments dans la BD TOPO R Nous avons choisi un seuil maximal de 40 mètres, défini en nous appuyant sur le précision planimétrique connue de la BD TOPO R (10 mètres) et l\u0027estimation empirique de la précision des indications de localisation dans les données DBPedia.Dans le cas des monuments de la base Mérimée, localisés par des adresses, nous avons tout d\u0027abord procédé au géocodage des données. Nous avons donc réalisé une interconnexion entre les indications de localisation des monuments dans la base Mérimée et la base BD ADRESSE R Pour ce faire, nous avons combiné les attributs qui constituent les adresses textuelles dans les deux bases et les avons comparés via une mesure de Levenshtein modifiée pour devenir insensible au désordre des mots. Ainsi, nous avons pu affecter à chaque monument interconnecté les coordonnées géographiques correspondant à son ou ses adresse(s) ( (2)  figure 3). Nous avons ensuite pu procéder à l\u0027interconnexion des monuments Mérimée dotés de coordonnées avec les bâtiments BD TOPO R en réutilisant l\u0027approche adoptée pour les monuments DBPedia ((3) figure 3). Enfin, nous avons procédé à une dernière interconnexion ((4) figure 3) entre les monuments issus de Mérimée et les monuments de Dbpedia qui ne disposaient pas d\u0027informations de localisation fiables. Pour ce faire, nous avons utilisé les noms attribués aux monuments comme critère d\u0027interconnexion, et les avons comparés en nous servant de la distance de Levenshtein modifiée disponible dans Silk.\n12. http ://www.geotools.org/\nRésultats et évaluation\nPour les deux sources de données, sur les 2462 resources récupérées initialement, nous avons pu en interconnecter 1653 avec des bâtiments BD TOPO R Ce résultat s\u0027explique en partie par l\u0027étape préalable de géocodage que nous avons dû appliquer aux données Mérimée. Lors de cette étape, nous avions paramétré Silk de sorte à obtenir le plus possible d\u0027interconnexions justes, au risque de ne pouvoir géocoder certaines données Mérimée faute d\u0027interconnexions. Pour cette étape, nous avons comparé les liens générés avec un mapping de référence créé manuellement sur la zone du premier arrondissement de Paris. Cette comparaison a révélé une précision de 100% et un rappel de 90,35%. Cette précision peut se justifier par le fait que les liens comparés ne concernent que le 1er arrondissement. Le rappel s\u0027explique en partie par le paramétrage choisi et par la présence dans la base Mérimée d\u0027indications de localisation non conformes à des adresses comme \" face au 14 \" ou \" dans le cimetière \".\nFIG. 4 -Interconnexion du Palais de la porte dorée avec les deux sources de données.\nNous n\u0027avons pu procéder à une évaluation chiffrée des résultats d\u0027interconnexion avec la BD TOPO R faute de mapping de référence. Nous avons donc procédé par validation visuelle effectuée par une seule personne sur une interface cartographique. Ainsi, nous avons pu constater que l\u0027approche d\u0027ancrage sur un référentiel géographique permettait bien de mettre en correspondance des ressources qui n\u0027auraient pas pu l\u0027être par comparaison directe, en raison de localisations trop éloignées ou de dénominations différentes. C\u0027est le cas pour \" le palais de porte dorée \" nommé ainsi dans DBpedia, et intitulé \" Ancien Musée National des Arts Africains et Océaniens, devenu Cité nationale de l\u0027histoire de l\u0027Immigration\" dans la base Mérimée avec une distance supérieure à 60m entre les deux localisations (figure 4).\nLes interconnexions ont été réalisées sur un ordinateur bureautique sous Ubuntu12.04 LTS avec 3,8 Go de RAM et un processeur intel Core i5 2.40GHz*4, en utilisant la variante «Single Machine» de Silk. Nous avons utilisé les options de blocking disponibles pour optimiser le temps de calcul : chacune des interconnexions réalisées a pris moins de 3h30. Pour améliorer les temps de calcul d\u0027interconnexion ou traiter plus de données, nous aurions pu utiliser la variante \"Slik MapReduce\". Cependant la validation demeure manuelle ; il est donc impossible pour l\u0027instant de passer à l\u0027échelle pour cette étape. . La première permet l\u0027affichage, sous forme de couches, de données stockées sur des serveurs géographiques et envoyées conformément aux protocoles décrits par les spécifications de l\u0027OGC (Open Géospatial Consortium). La seconde nous fournit des fonds cartographiques et orthophotographiques produits par l\u0027IGN.\nOpenLayers permet également de créer d\u0027autres couches vectorielles à partir de données récupérées par différents protocoles, notamment le protocole HTTP. Elle permet donc d\u0027interroger un point d\u0027accès SPARQL. De plus, elle offre la possibilité de mettre en forme les données ainsi récupérées avant de les visualiser. Ainsi, on peut créer des couches de données vectorielles en utilisant les géométries des données géographiques de référence que nous avons préalablement converties et stockées en RDF pour l\u0027étape d\u0027interconnexion. De plus, on peut mettre à profit les liens d\u0027ancrage entre ressources externes et données géographiques pour récupérer des informations thématiques sur ces ressources externes. La figure 6 présente un exemple de co-visualisation des données géographiques de référence et des ressources théma-tiques ancrées sur ces données. Les bâtiments interconnectés avec des monuments historiques de la base Mérimée sont affichés dans des tons de bleu, la valeur du bleu variant selon le siècle de construction de monument en question fourni par la base Mérimée. Les bâtiments interconnectés avec des monuments historiques DBPedia sont dotés de contours noirs. Ceux interconnectés avec les deux sources sont à la fois en bleu et avec un contour noir. C\u0027est le cas par exemple du bâtiment qui héberge la bourse du commerce de Paris. \nVisualisation à différentes échelles\nAfin de fournir un affichage cartographique lisible à différentes échelles, on a généralement recours à des techniques dites de généralisation. La généralisation est un processus de synthèse d\u0027information, qu\u0027on peut comparer à un processus de résumé de texte qui réduit le nombre de mots mais garde les idées principales Ruas (2002). Elle vise à simplifier l\u0027information en passant d\u0027un niveau d\u0027échelle à un autre en gardant le même sens pour la carte. Dans notre cas, nous avons eu recours à une approche de simplification fondée sur une classification sous contrainte de contigüité spatiale Carvalho et al. (2009).\nFIG. 7 -Visualisation des îlots et des îlots agrégés à différentes échelles\nCette méthode consiste à unir les objets voisins qui présentent des propriétés similaires. Nous regroupons les objets en fonction du siècle de construction des monuments historiques. Cette technique ne peut pas être appliquée directement sur les bâtiments, compte tenu de la non-contigüité entre ces derniers. Nous avons donc appliqué cette méthode sur les îlots surfaciques créés à partir du réseau routier, en leur affectant le siècle de construction le plus fréquemment rencontré parmi les bâtiments correspondant à des monuments classés qu\u0027ils contiennent (partie gauche figure 7). Ceci nous a permis de générer des représentations plus agrégées pour un affichage à petite échelle (partie droite figure7).\nConclusion et perspectives\nDans cet article, nous avons proposé une approche visant à tirer parti des informations de localisation associées à des ressources Linked Data dans un processus d\u0027interconnexion. Cette approche consiste à interconnecter les ressources non pas directement entre elles, mais avec un référentiel géographique afin de pallier les difficultés d\u0027interconnexion liées à l\u0027hétérogé-néité des indications de localisation. Les interconnexions générées entre données thématiques et données géographiques ont été ensuite mise à profit dans une application de visualisation cartographique des données à différentes échelles reliant les informations thématiques directement aux objets géographiques. Les liens d\u0027ancrages générés entre données thématiques et données géographiques sont des relations d\u0027équivalence. Pour plus de cohérence sémantique, nous envisageons d\u0027étudier les différents types de liens sémantiques qui peuvent exister entre une ressource thématique et un objet géographique.\n"
  },
  {
    "id": "343",
    "text": "Introduction\nL\u0027activité d\u0027un réseau social en ligne, d\u0027un réseau de télécommunication, de requêtes sur un moteur de recherche, ou encore d\u0027un système de paiement par carte bancaire, peut être modélisé par un grand réseau d\u0027interactions ; chaque interaction peut en effet être vue comme un lien entre deux entités, apparaissant au déclenchement de l\u0027interaction. Le réseau est alors représenté par un flot de liens ordonnés chronologiquement.\nMalgré la diversité des systèmes modélisables par des flots de liens, la dynamique de ces flots de liens reste peu explorée. La dynamique des graphes étant elle-même un sujet d\u0027étude scientifique récent, il a fallu attendre ces dernières années et la publication de grands jeux de données à ce niveau de précision pour que l\u0027étude des flots de liens puisse émerger. La plupart des graphes dynamiques étudiés jusqu\u0027ici sont en effet constitués d\u0027instantanés capturés à une certaine fréquence (ex. un graphe par jour). L\u0027enjeu principal est de caractériser l\u0027évolution de ces systèmes pour mieux les comprendre et en particulier différencier une dynamique normale de comportements anormaux. Pour un analyste surveillant un système et levant des alertes en cas d\u0027anomalies, comme une fraude à la carte bancaire ou une intrusion dans un intranet, il est crucial de pouvoir identifier et valider de tels événements rapidement. En d\u0027autres termes, il s\u0027agit de déterminer où et quand la structure du réseau d\u0027interactions est anormalement altérée.\nDans cet article nous proposons une méthode d\u0027investigation à la fois statistique et visuelle. Celle-ci repose sur une méthode de fouille de données statistique pour détecter des événements significatifs, suivie de visualisations interactives pour les valider et les interpréter.\nÉtat de l\u0027art\nLa plupart des méthodes actuelles de fouille de graphes dynamiques (i.e. qui évoluent dans le temps) reposent sur des séries de graphes statiques représentant soit l\u0027état du graphe à l\u0027instant de chaque capture, soit l\u0027agrégation des noeuds et liens apparus entre cet instant et l\u0027instant de la précédente capture (par exemple le graphe des interactions du mois d\u0027avril dans un réseau social). Ces graphes sont communément représentés par des diagrammes noeudsliens, que l\u0027on trouve dans la majorité des systèmes de visualisation scientifique tels que Visone (Brandes et Wagner (2004)), SoNIA (Moody et al. (2005)), Gephi (Bastian et al. (2009)), ViENA (Windhager et al. (2011)), et GraphDiaries (Bach et al. (2012)).\nQuatre stratégies émergent pour la détection visuelle d\u0027événements dans l\u0027évolution d\u0027un graphe : 1) le morphing est une correspondance de entre deux instants montrant l\u0027animation du diagramme noeuds-liens (Ghani et al. (2012)) comme une vidéo (dont la position des noeuds peut varier ou non) via une chronologie ou un slider (Moody et al. (2005)), voir Gephi, NodeXL, SoNIA, TempoVis. Cette approche est intuitive mais l\u0027analyse est difficile si la densité de liens est élevée et le nombre de changements important (Brandes et al. (2012);Archambault et al. (2011)). 2) la comparaison de couches montre l\u0027évolution du graphe sur une vue unique via une série de small multiples représentant l\u0027état du graphe à différents instants (Archambault et al. (2011)). Ces small multiples sont intégrables à une chronologie (Bach et al. (2012)), voir GraphDiaries. Cette approche permet de comparer rapidement les différences, mais se limite à de petits graphes. 3) la fusion de couches intègre deux graphes statiques en un seul. Cette représentation \"deux-en-un\" distingue les deux instants du graphe par des couleurs et effets de style. Elle amplifie les différences structurelles, mais se limite à des graphes de faible densité (Krempel (2005)), voir ViENA. 4) la vue en 2.5D utilise la troisième dimension pour afficher les changements du graphe, mais son efficacité n\u0027est pas démontrée.\nVisualiser l\u0027ensemble du graphe dynamique entraîne de nombreux problèmes, notamment l\u0027occlusion, le croisement des liens dans des graphes de forte densité, et la difficulté à pré-server la carte mentale (Archambault et al. (2011)). Des métaphores et approches orientées pixels (matrices, codes barre (Albano et al. (2011))) contournent ces problèmes mais restent marginales. Pourtant, voir l\u0027ensemble du graphe n\u0027est pas toujours nécessaire ni souhaitable, en particulier dans un flot de plusieurs millions de liens. La visualisation d\u0027un sous-graphe bien choisi (grâce à une méthode statistique par exemple) permet de détecter des événements pertinents (Sun et al. (2007);Chau (2012)). Les approches existantes sont cependant basées sur des séries de graphes statiques, et ne sont pas adaptées aux flots de liens à cause des biais induits par les changements d\u0027échelle (Clauset et Eagle (2007)).\nContribution et organisation du papier\nNous proposons une méthode pour valider, par la visualisation, les événements statistiquement significatifs détectés par une méthode de fouille de données. On identifie ainsi où et quand un événement statistiquement significatif apparait dans le réseau. Nous décrivons notre méthode en Section 2, l\u0027appliquons sur l\u0027étude d\u0027un grand réseau social en ligne en Section 3, et concluons en Section 4.\nMéthode\nNotre méthode se compose des deux étapes suivantes.\n1. Identifier automatiquement des changements statistiquement significatifs dans la structure du graphe sur une fenêtre de temps glissante, en fonction d\u0027une métrique de graphe. Cette étape permet de traiter l\u0027ensemble du graphe rapidement.\n2. Valider la pertinence de chaque événement détecté en identifiant visuellement un motif anormal dans le sous-graphe correspondant à la fenêtre temporelle anormale, puis en vérifiant que ce motif est unique au cours du temps.\nNous posons les définitions nécessaires dans la section suivante.\nDéfinitions\nAucune définition concernant les flots de liens n\u0027étant à notre connaissance disponible dans la littérature, nous posons les définitions suivantes. Flot de liens : soit F \u003d {f 0 , f 1 , ..., f m } le multi-ensemble ordonné de triplets f i \u003d\u003c n x , n y , t i \u003e, avec la relation d\u0027ordre f i ? f i+1 si et seulement si t i ? t t+1 , où i ? N, t i ? R. F est appelé flot de liens. Chaque triplet représente un lien entre deux noeuds n x , n y observé à une date t i à la position i dans F . \nDétection d\u0027événements statistiquement significatifs\nNous appliquons la méthode Outskewer (Heymann et al. (2012)), une méthode de fouille de données non supervisée applicable à un échantillon de valeurs univarié. Cette méthode considère comme anormale une valeur extrême qui rend le coefficient d\u0027asymétrie 1 de la distribution de valeurs éloigné de zéro (i.e. symétrie parfaite de la distribution de valeurs). Outskewer peut détecter de multiples valeurs anormales dans un échantillon, et peut être étendu à l\u0027analyse d\u0027une série temporelle (potentiellement en temps réel). Soit X \u003d {x 0 , x 1 , ..., x n } une série temporelle. On considère le multi-ensemble de w o valeurs X i \u003d {x i?wo+1 , ..., x i }, où w o est appelée la taille de la fenêtre temporelle de détection d\u0027événements. Chaque valeur\n. Nous utilisons Outskewer sur tous ces w o multi-ensembles, et calculons le nombre de fois où x i est classée comme normale, probablement anormale, ou anormale. La classe finale est celle ayant obtenu le score le plus grand. Nous utilisons X \u003d S pour appliquer Outskewer à une statistique de graphe. Nous posons la définition suivante.\nÉvénement : ensemble consécutif de valeurs {x i , x i+1 , ..., x j }, i ? j classées comme anormales dans X. Par commodité, nous nous référons aux événements par la valeur i par la suite.\nAnalyse visuelle des événements\nCette étape a pour objectif de valider les événements détectés par Outskewer, et de les interpréter en les corrélant à des motifs anormaux dans la structure des sous-graphes G i w correspondants. Par exemple, la soudaine et fréquente répétition de l\u0027apparition d\u0027un lien contribue à diminuer brusquement le nombre de noeuds uniques observés via une fenêtre temporelle à cet instant. Cet événement peut être détecté automatiquement mais peut avoir plusieurs origines comme nous le verrons.\nNous proposons une visualisation interactive permettant d\u0027investiguer ces événements. Nous faisons l\u0027hypothèse que l\u0027utilisateur sait lire et interpréter un diagramme noeuds-liens. Notre prototype doit respecter les contraintes suivantes : a) représenter la structure du graphe par un diagramme noeuds-liens ; b) ne pas afficher tout le graphe, qui peut compter des millions de noeuds et liens, ce qui rendrait la visualisation illisible ; c) tenir compte de la possibilité que les événements détectés par Outskewer soient très espacés des uns des autres dans le temps.\nPour investiguer un événement (cf. section 2.5), l\u0027utilisateur extrait le sous-graphe G i w et y identifier un ou plusieurs motifs suspects selon ses critères. Il désambiguïse enfin l\u0027interpréta-tion de l\u0027événement : il détermine si le motif anormal apparaît seulement lors de l\u0027événement. Si c\u0027est le cas, alors il le considère comme corrélé à l\u0027événement.\nDescription du prototype\nNotre prototype étend les fonctionnalités du logiciel commercial Linkurious 2 , réalisé en HTML5 et Javascript pour s\u0027exécuter dans un navigateur Web. Il se connecte à une base de données de graphe Neo4j 3 . L\u0027interface se compose des quatre espaces suivants : 4) Les panneaux du bas sont la nouveauté apportée à Linkurious. Ils permettent de sélec-tionner un événement i dans une liste déroulante, de préciser une taille de fenêtre temporelle w (qui peut être différente de w), et d\u0027afficher le sous-graphe correspondant G i w . Un curseur permet de décaler la position de la fenêtre un peu avant ou un peu après la date de l\u0027événement (jusqu\u0027à 10 000 liens en amont ou en aval) pour observer l\u0027évolution du graphe autour de l\u0027évé-nement. Un second diagramme représente la totalité du flot de liens F permettant à l\u0027utilisateur de déplacer la fenêtre temporelle à n\u0027importe quelle date via un curseur. Un bouton permet de colorer le diagramme en fonction du nombre de liens apparaissant entre les noeuds visibles à l\u0027écran à chaque pourcentage de la largeur du diagramme : de gris (pas d\u0027apparition) à vert clair (le maximum d\u0027apparitions). Si un seul noeud est visible, alors la couleur correspond au nombre de liens ayant ce noeud pour extrémité. Ce diagramme fournit des indications sur la fréquence des apparitions des liens entre noeuds d\u0027un motif durant toute la durée de l\u0027évolu-tion du graphe. Ce diagramme est compacte (10 pixels par 500 pixels) pour répondre à une contrainte de hauteur, mais la précision d\u0027affichage et de positionnement du curseur dépend de sa largeur l et du nombre de liens m. Une largeur de 1 pixel représente en effet m/l liens. Une fonction de zoom serait une amélioration possible pour gagner en précision. 2) Identification d\u0027un motif anormal : L\u0027analyste sélectionne un événement i dans la liste dé-roulante des événements. Le sous-graphe G i w s\u0027affiche ; il peut diminuer la taille de la fenêtre temporelle pour l\u0027affichage (ainsi réduire la taille du sous-graphe) ou conserver la taille utilisée lors de la détection d\u0027événements (w). Il observe le diagramme noeuds-liens à la recherche de motifs suspects (au regard de la statistique utilisée par Outskewer), comme une étoile, une composante connexe, ou un lien très redondant. S\u0027il trouve un motif suspect, il explore l\u0027évo-lution du graphe autour de la date de l\u0027événement. Si ce motif reste visible avant ou après cette date, alors il est évident qu\u0027il n\u0027est pas corrélé à l\u0027événement. L\u0027analyste doit alors chercher un autre motif, ou considérer que l\u0027événement détecté par Outskewer n\u0027est pas valide. 3) Interprétation de l\u0027événement : Une fois un motif suspect identifié, l\u0027analyste cherche à déterminer s\u0027il est exceptionnel ou s\u0027il est récurrent. Cette étape permet de tester la corrélation de motifs pour un événement. Comme illustré dans la section suivante, l\u0027analyste sélectionne les noeuds du motif et cache les autres. Il applique alors la coloration de la barre du flot de liens, et voit en un coup d\u0027oeil si les liens entre ces noeuds apparaissent à d\u0027autres moments (périodes en vert). Si ce n\u0027est pas le cas, alors le motif entre ces noeuds est unique et corrélé à l\u0027événement. Sinon il déplace le curseur sur chaque période d\u0027apparition pour voir la forme des relations entre ces noeuds. Lors de cette opération la position des noeuds est stable, ce qui permet de comparer plusieurs versions du motif au cours du temps en réalisant un export de l\u0027image du graphe (via la barre d\u0027outils). Si le motif est redondant, alors l\u0027analyste ne peut pas affirmer qu\u0027il soit corrélé à l\u0027événement, bien qu\u0027il puisse être anormal si nous ne considérons que le graphe visible. Dans ce cas l\u0027événement est ambigu, et une analyse approfondie est né-cessaire. Au contraire, si le motif apparaît uniquement lors de l\u0027événement, alors il est corrélé à cet événement ; l\u0027analyste peut valider l\u0027événement en interprétant le contenu de ce motif. \nValidation expérimentale des événements\nApplication\nJeu de données Github\nDétection automatique d\u0027événements\nNous illustrons l\u0027intérêt de notre approche sur une statistique basique : le nombre de noeuds uniques S , et nous remarquons que le compte a par la suite été renommé \"mapserver-bot\". Le but de ce robot a été de migrer le code source et la liste de bogues du projet \"mapserver\" de Trac vers Github. Nous trouvons des traces de la discussion lancée le 19 mars 2012 sur la mailing-list publique 8 . L\u0027événement correspond donc à la date de migration du code source ou de la liste de bogues sur Github.com : le bug n°1 correspond en effet au début de l\u0027événement le 3 avril 2012 à 17 :37 :55 (t i\u003d413000 ). 6. https://github.com/mapserver/mapserver 7. https://github.com/mapserver-bot?tab\u003dactivity 8. https://lists.osgeo.org/pipermail/mapserver-dev/2012-March/012100.html ligne Github. Des travaux préliminaires ont montré l\u0027intérêt de représenter ces données comme une succession d\u0027apparition de liens dans un graphe dynamique, que nous formalisons par un flot de liens ordonnés chronologiquement. Peu de jeux de données de graphes dynamiques possèdent une telle granularité pour un volume de plusieurs millions de liens, ce qui constitue une opportunité pour développer des méthodes de détection d\u0027événements adaptées.\nNous avons combiné une méthode statistique, Outskewer, avec un système de visualisation. Outskewer détecte des événements statistiquement significatifs selon une statistique du graphe, et donne des instants de l\u0027évolution du graphe à investiguer. Ces événements n\u0027étant pas tous pertinents, nous avons proposé une étape de validation supplémentaire par la visualisation. Cette dernière montre un sous-graphe correspondant au moment des événements et facilite le suivi longitudinal des motifs anormaux présents dans ce sous-graphe, ce qui permet de localiser des motifs anormaux à ces instants, et de vérifier s\u0027ils semblent corrélés à l\u0027évé-nement. Nous avons illustré notre méthode sur les événements à investiguer, et montrons par plusieurs exemples que nous détectons des événements pertinents, et rejetons des événements proposés par Outskewer pour lesquels nous ne trouvons pas d\u0027anomalie dans le graphe. Nous avons ainsi illustré la complémentarité des statistiques et de la visualisation pour détecter des événements dans un grand flot de liens.\nDans des travaux futurs nous proposerons une évaluation du système avec un analyste, expert de son domaine, pour en évaluer l\u0027approche et en critiquer l\u0027IHM. Une comparaison avec des méthodes alternatives est aussi souhaitable. Enfin, nous tenterons de généraliser notre méthode par plusieurs cas réels comme la détection de fraudes ou la détection d\u0027intrusions.\nUne telle méthode sans a priori sur les données ni sur les événements est idéale lors d\u0027une démarche exploratoire. La visualisation, en particulier, offre une grande flexibilité quant à la nature des motifs anormaux à observer. Une fois une typologie des événements identifiée en fonction de la tâche d\u0027investigation, il est envisageable d\u0027automatiser entièrement la détection et la validation des événements en replaçant la visualisation par des statistiques sur les motifs. \n"
  },
  {
    "id": "344",
    "text": "Introduction\nL\u0027expansion de la requête est une approche souvent utilisée en recherche d\u0027information pour aider le modèle de recherche à mieux identifier les documents pertinents. Le succès de cette technique dépend du bon choix des termes d\u0027expansion et la façon d\u0027ajouter ces nouveaux termes à la requête. D\u0027un côté, si les nouveaux mots clés apportés par l\u0027approche d\u0027expansion ne sont pas pertinents pour le besoin d\u0027information, la possibilité de trouver des documents pertinents baisse, ce qui a un effet négatif sur le rappel et la précision. Pour cette raison, le contrôle de la qualité des termes d\u0027expansion est une étape indispensable de l\u0027expansion de la requête. D\u0027un autre côté, un bon choix des termes d\u0027expansion ne suffit pas pour garantir le succès de l\u0027expansion si ces termes n\u0027ont pas été intégrés correctement dans la requête. Dans cet article, nous nous concentrons sur les entités nommées, c\u0027est à dire les termes qui représentent des personnes, des lieux ou des événements. Ces termes, riches en information, ont motivé plusieurs études dans des domaines liés au Traitement du Langage Naturel. Ces études s\u0027intéressaient à reconnaitre les entités nommées dans un texte (Guo et al., 2009), à les désambiguïser (Hoffart et al., 2011), ou à les classifier (Nadeau et Sekine, 2007). Dans le domaine de la recherche d\u0027information, les entités nommées sont souvent utilisées pour l\u0027annotation (Kiryakov et al., 2004), l\u0027indexation (Buizza, 2011) ou la recherche (Petkova et Croft, 2007). Des travaux récents (Guo et al., 2009) ont remarqué l\u0027importance des entités nommées pour les requêtes des utilisateurs, car ils ont constaté que plus de 70% des requêtes web contiennent au moins une entité nommée. En revanche, ces travaux s\u0027intéressaient plutôt à la classification des entités nommées de la requête sans considérer l\u0027effet de ces éléments sur la performance du modèle de recherche. Dans notre article, nous étudions l\u0027expansion des entités nommées dans la requête, notamment par différentes méthodes de reformulation. Notre idée est construite sur trois éléments : premièrement, l\u0027utilisation des entités nommées dans les requêtes est fréquente et utile (Guo et al., 2009;Kiryakov et al., 2004). En deuxième lieu, certaines études ont constaté que les requêtes web contiennent souvent une seule entité nommée (Guo et al., 2009). La troisième élément est l\u0027avantage potentiel de l\u0027expansion de la requête qui peut améliorer significativement la performance d\u0027un modèle de recherche. Nous commençons en section 2 par une présentation des études précédentes sur l\u0027expansion sémantique des requêtes, avec un zoom sur le cas des entités nommées. En section 3 nous présentons notre approche de l\u0027expansion sémantique des entités nommées dont nous détaillons les différents étapes. La section 4 est consacrée à l\u0027explication et l\u0027évaluation de nos expériences.\nÉtat de l\u0027art\nLe but de l\u0027expansion de la requête est de trouver des nouveaux documents pertinents (i.e. améliorer le rappel), et de tirer les documents pertinents déjà trouvés par la requête initiale vers le haut de la liste des résultats (i.e. améliorer la précision). Nous pouvons diviser les approches d\u0027expansion de requête en deux catégories : la première catégorie regroupe les approches qui dépendent de la collection de documents. Ces approches peuvent être locales, comme les méthodes de retour de pertinence, ou globales. La deuxième catégorie contient les méthodes fondées sur l\u0027utilisation d\u0027une ressource externe comme une ontologie. Traditionnellement, pour ces deux catégories, les entités nommées sont souvent traitées comme les autres termes de la requête. Les approches fondées sur la collection de documents utilisent des calculs statistiques sur les termes dans les documents sans un traitement spécifique concernant les entités nommées. Par ailleurs, les méthodes fondées sur une ressource externe sont confrontées à la difficulté de traitement des entités nommées surtout quand la ressource externe décrit mal ces éléments (Navigli et Velardi, 2003). Par exemple, l\u0027utilisation de WordNet pour l\u0027expansion de la requête n\u0027a pas beaucoup de succès (Mandala et al., 1998) : l\u0027une des raisons est que les entités nommées dans cette ressource sont souvent manquantes, pas à jour, ou n\u0027ont pas (ou très peu) de synonymes dans leurs Synsets 1 . Ainsi, WordNet n\u0027est pas suffisant pour désambiguïser ou étendre les requêtes qui contiennent des entités nommées. Malgré ces difficultés liées à l\u0027expansion de la requête, d\u0027autres études en recherche d\u0027information confirment l\u0027importances des entités nommés dans les requêtes. Par exemple, certaines recherches sur les requêtes longues considèrent qu\u0027une sous requête contenant une entité nommée est un bon candidat qui doit être considéré pour la reformulation (Huston et Croft, 2010;Kumaran et Carvalho, 2009). D\u0027autres études, comme (Maxwell et Croft, 2013), proposent un algorithme pour classer des groupes nominaux identifiés dans la requête afin de les utiliser pour construire une nouvelle requête. Récemment, la disponibilité de ressources riches et ouvertes comme Wikipedia, a permis certains travaux explicitement dédiés à l\u0027étude de l\u0027expansion des entités nommées. Par exemple, Xu et al. (2008) ont utilisé Wikipedia pour extraire des termes qui sont sémantiquement proches des termes de la requête, alors que Brandao et al. (2011) ont étudiés des approches basées sur les pages et les Infobox de Wikipedia pour retrouver des expansions des entités nommées. Comme Xu et al. (2008) et Brandao et al. (2011, notre travail s\u0027intéresse aussi à l\u0027expansion des entités nommés, par contre, nous faisons le point sur les méthodes d\u0027intégration des termes d\u0027expansion dans la requête, ce qui n\u0027était pas abordé dans les travaux précédents. De plus, nous étudions l\u0027effet du rôle des entités nommées sur l\u0027expansion.\nL\u0027expansion sémantique des entités nommées\nDans cette section nous présentons notre approche d\u0027expansion de la requête fondée sur les entités nommées. Nous précisons dans les sous-sections suivantes les étapes de notre approche en considérant le cas des requêtes Web et l\u0027ontologie générique YAGO (Suchanek et Weikum, 2007) que l\u0027on utilise à la fois pour la désambiguïsation et pour le choix des termes d\u0027expansion. Cette ontologie est une combinaison de WordNet et des catégories de Wikipedia. Elle a l\u0027avantage d\u0027être une ressource riche en entités nommées, liées entre elles par une grande variété de relations sémantiques (Hoffart et al., 2011).\nLa désambiguïsation\nLa désambïguisation est un problème complexe, largement abordé en Traitement du Langage Naturel (Navigli, 2009). Alors que ce problème est implicitepment résolu par les mé-thodes locales d\u0027expansion de la requête, il reste un vrai défi pour les approches fondées sur l\u0027utilisation d\u0027une ressource externe. D\u0027un autre côté, la désambiguïsation des entités nommées est un domaine relativement récent. À notre connaissance, la première méthode qui a traité ce sujet a été proposée en 2006 : elle utilisait Wikipedia en combinaison avec une approche d\u0027apprentissage supervisé pour la désambïguisation des entités nommées (Bunescu et Pasca, 2006). Depuis, Wikipedia est devenu une ressource importante et à jour des entités nommées. Il faut noter également que dans le cas des requêtes web, la tâche de désambiguïsation est compliquée, car ces requêtes sont souvent très courtes, et contiennent donc peu d\u0027éléments de contexte. Dans cet article, nous utilisons l\u0027approche de désambiguïsation proposée par (Hoffart et al., 2011). Cette approche utilise l\u0027outil de Stanford NER (Named Entity Recognition) pour identifier les entités nommées dans une requête, puis elle applique une combinaison de trois techniques de désambiguïsation : le sens le plus utilisé (popularity prior), la similarité (syntaxe et surface commune) entre une entité nommée et le concept correspondant dans l\u0027ontologie, et enfin le graphe de cohérence entre les concepts. Quand il n\u0027y a pas de contexte, l\u0027algorithme va choisir la référence la plus courante pour l\u0027entité à désambiguïser. Ce comportement est cohé-rent avec un scénario de recherche sur le Web, où on peut considérer que si l\u0027utilisateur ne met\nTopic\nEntité Nommée Variations sémantiques 515\nAlexander Graham Bell \"Alexander Gram Bell\", \"Aleck Bell\", \"The father of the deaf\" 517 Titanic \"Jinx Titanic\" 478 Baltimore \"Baltimore City\", \"City of Baltimore\", Baltamore, Bmore, \"Baltimore riots\", Baltimoreans, \"Charm city\" Mobtown, \"Charm City\"\nTAB. 1 -Exemples de variations sémantiques trouvés dans YAGO pour des entités nommées.\nque l\u0027entité nommée dans sa requête, il souhaite alors orienter sa recherche vers la référence la plus courante de cette entité.\nLe choix des termes\nUne fois choisie la référence unique de l\u0027entité nommée dans Yago, la sélection des termes d\u0027expansion peut être effectuée. Ce choix dépend de la relation sémantique qu\u0027on souhaite utiliser. Dans l\u0027ontologie Yago, les relations sémantiques qui lient un concept à son entourage sont nombreuses 2 . Ces relations dépendent de la nature de chaque concept : par exemple une ville peut être reliée à sa surface ou à son nombre d\u0027habitants, alors que d\u0027autres types de relations sont utilisés dans le cas d\u0027une personne (sa date de naissance par exemple). Dans le cas de l\u0027expansion de la requête, le choix de la bonne relation sémantique n\u0027est pas une tâche facile, elle sera le sujet de prochaines études. Pour cet article nous considérons la relation \"Label\" qui existe pour tous les concepts. L\u0027avantage de cette relation est qu\u0027elle lie une entité nommée à ses différentes appellations que l\u0027on va appeler des synonymes pour simplifier. Ces synonymes peuvent être des alternatives orthographiques du terme (Baltimore-Baltamore), ou complètement différents au niveau de la syntaxe mais sémantiquement identiques au terme original (Baltimore-Mobtown). Le tableau 1 présente trois exemples d\u0027entités nommées de type différent (personne, objet et lieu) avec leurs synonymes. Dans ce tableau, on constate également qu\u0027un synonyme peut être une autre entité nommée (\"Aleck Bell\") ou un groupe nominal (\"The father of the deaf\").\nLa reformulation de la requête\nLa plupart des modèles de recherche considère la requête utilisateur comme un sac de mots, que l\u0027on modifie avec l\u0027expansion de la requête en ajoutant de nouveaux termes avec ou sans pondération. Selon le modèle de recherche d\u0027information utilisé, des alternatives au sac de mots peuvent être considérées pour représenter la requête. Dans notre travail nous utilisons le modèle de query likelyhood (Strohman et al., 2004) qui est le modèle par défaut de l\u0027outil de recherche d\u0027information Indri est de calculer les croyances pour chaque noeud dans le réseau, puis de combiner ces croyances selon l\u0027opérateur utilisé. Les équations 1 et 2 présentent comment Indri combine les croyances (b i ) pour les opérateurs #combine et #weight que nous utilisons dans notre approche.\nLe troisième opérateur que nous utilisons #syn est un opérateur virtuel, dans le sens où il n\u0027est pas associé à une équation de combinaison. Son rôle est de signaler au modèle de recherche qu\u0027il ne faut pas distinguer les occurrences de chacun des termes donnés dans la liste de synonymes. Le langage de requête d\u0027Indri permet également, avec les opérateurs #N et #uwN (unordered window), d\u0027exprimer le nombre maximum de mots autorisés entre des termes dans les documents du corpus pour qu\u0027une occurrence soit comptabilisé 4 . Nous expliquerons par la suite les trois approches que l\u0027on utilise pour l\u0027intégration des nouveaux termes dans la requête avec ces opérateurs. Pour simplifier la compréhension de ces approches, nous utilisons la requête démonstrative suivante (issue de TREC n?455n?455 après la suppression des mots vides) :\nJackie Robinson appear first game que l\u0027on considère comme la requête de base sans expansion.\nSac De Mots (SDM)\nDans le cadre de l\u0027expansion de la requête, l\u0027utilisation du principe \"sac de mots\" signifie l\u0027ajout de nouveaux termes à la requête originale, ce qui peut être codé par l\u0027opérateur #combine. Ainsi, l\u0027expansion de notre exemple démonstratif pourrait donner la requête suivante :\n#combine(Jackie Robinson appear first game #1(Jackie Robinson) #1(Jack Roosevelt Robinson)) Néanmoins, dans cette requête étendue tous les éléments constitutifs du #combine seront interprétés indépendamment les uns par rapport aux autres, et donc avec une même probabilité. Dans notre cas, il faudrait que #1(Jackie Robinson) et #1(Jack Roosevelt Robinson) aient, l\u0027un ou l\u0027autre, la même probabilité que chacun des autres termes de la requête étendue. L\u0027opérateur #syn nous permet de le faire avec cette formulation : #combine(Jackie Robinson appear first game #syn( #1(Jackie Robinson) #1(Jack Roosevelt Robinson)))\nLa Dépendance Séquentielle (DS)\nLa dépendance séquentielle a été proposée par Metzler et Croft (2005). L\u0027idée d\u0027origine a été fondée sur la reformulation des requêtes longues en trois parties pondérées : la première partie contient la requête originale sous la forme d\u0027un sac de mots, cette partie a le poids le plus important. La deuxième partie combine des fenêtres de taille 1 pour chaque paire de termes successifs. La troisième partie contient les mêmes paires de termes mais avec une taille de fenêtre égale à 4. Dans (Maxwell et Croft, 2013), les auteurs ont adapté cette approche en remplaçant les paires de termes par des groupes nominaux qu\u0027ils choisissent à partir de la requête initiale 5 en utilisant leur algorithme PhraseRank. Notre expansion de requête s\u0027inspire de ces travaux. Nous reformulons les requêtes initiales en trois parties, en utilisant les mêmes poids que dans (Metzler et Croft, 2005) et (Maxwell et Croft, 2013). Nous suivons aussi l\u0027approche de (Maxwell et Croft, 2013) pour calculer la taille des fenêtres des expressions dans la dernière partie :\n-requête initiale (coefficient de pondération \u003d 0.85) -extension avec les synonymes (issus de YAGO) dans des fenêtres de taille 1 (coefficient de pondération \u003d 0.1) -extension avec les même synonymes dans des fenêtres d\u0027une taille équivalant à quatre fois le nombre de mots du groupe nominal (coefficient de pondération \u003d 0.05) Ce qui donne la requête suivante pour notre exemple : #weight( 0.85 #combine(Jackie Robinson appear first game) 0.10 #combine(#syn(#1(Jackie Robinson) #1(Jack Roosevelt Robinson) appear first game) 0.05 #combine(#syn(#uw8(Jackie Robinson) #uw12(Jack Roosevelt Robinson) appear first game))))\nLe Concept Clé (CC)\nLa troisième approche, fondée sur le principe de concept clé de (Bendersky et Croft, 2008), contient deux parties. La première partie est la requête originale sous forme d\u0027un sac de mots, elle a le poids le plus important comme c\u0027est le cas dans l\u0027approche de dépendance séquentielle. La deuxième partie contient des groupes nominaux choisis dans la requête. Dans (Bendersky et Croft, 2008), ces groupes nominaux sont pondérés à l\u0027intérieur de cette deuxième partie en fonction du modèle probabiliste qui les a classés, mais comme dans (Maxwell et Croft, 2013), nous n\u0027utilisons pas cette pondération. Nous obtenons alors : Les poids (0.8, 0.2) sont ceux de (Bendersky et Croft, 2008;Maxwell et Croft, 2013).\nL\u0027algorithme final\nL\u0027Algorithme 1 illustre les principales étapes de notre expansion sémantique des entités nommées.\nTout d\u0027abord, nous commençons par initialiser la requête originale de l\u0027utilisateur en enlevant les mots vides pour limiter le bruit (ligne 2). Ensuite, le processus de désambiguisation de Hoffart et al. (2011) identifie les entités nommées et choisit un unique sens pour chacune d\u0027entre elles (ligne 3). En ligne 5, nous utilisons YAGO pour obtenir des termes d\u0027expansions pour chaque entité nommée. Ces candidats à l\u0027expansion passent ensuite par deux filtres : le premier (ligne 6) enlève les termes bruités (ceux qui contiennent moins de 3 caractères), et le deuxième assure qu\u0027un terme d\u0027expansion n\u0027existe pas déjà dans la requête (ligne 7). Avec cette liste filtrée de termes d\u0027expansion, la procédure de reformulation est finalement appelée (ligne 8) avec comme paramètre l\u0027approche que l\u0027on souhaite utiliser pour la reformulation (SDM, DS, ou CC).\nÉvaluation\nAlors que la majorité des approches de reformulation ou d\u0027expansion de requête s\u0027intéresse plutôt à la précision en mesurant le MAP, R-Prec ou P@x (Maxwell et Croft, 2013;Carpineto et Romano, 2012;Bendersky et Croft, 2008;Petkova et Croft, 2007), dans notre travail nous considérons de plus le rappel pour obtenir une meilleure caractérisation de nos résultats. Pour cela, nous utilisons deux groupes de mesures : les mesures de précision (MAP, P@10, P@20, R-Prec), et les mesures de rappel (R@30, R@200, R@dernierRang). Les tests statistiques que nous utilisons sont \"T-test\" et \"Randomization test\" comme recommandé par (Smucker et al., 2007). Par notre évaluation nous souhaitons répondre aux questions suivantes : Quelle est l\u0027effet des différentes méthodes d\u0027intégration de nouveaux termes (SDM, DS, ou CC) ? Notre approche améliore-t-elle les résultats du modèle de référence sans expansion ? En comparant avec une approche traditionnelle d\u0027expansion de la requête, où se situe notre approche ? Nous faisons nos expériences sur deux collections web : WT10G nommée (tableau 2). Ce choix a été fait manuellement pour garantir que les requêtes de test contiennent au moins une entité nommée. Pour l\u0027indexation et la recherche on utilise Indri5.5 sans modifier les paramètres par défaut.\nExpériences\nDans le tableau 3, nous constatons que l\u0027intégration des termes d\u0027expansion dans la requête a un effet très important sur les résultats. Nous observons que le fait d\u0027intégrer les termes d\u0027expansion en mode sac de mots (sans pondération) dans la requête dégrade toutes les mesures de performance. Alors qu\u0027avec les approches DS et CC, la précision et le rappel ont bien été améliorés. Ces améliorations sont même statistiquement significatives pour les mesures RPrec, R@30 et R@200 pour la collection Wt10g. Pour la collection ClueWeb09, les approches DS et CC améliorent toutes les mesures par rapport à la base. Les tests de significativité de ces améliorations sont plus souvent positifs pour les métriques de précision. En plus de comparer notre approche au modèle de référence sans expansion, il est intéressant de le comparer également à une autre approche d\u0027expansion. Dans cette étude, nous comparons nos résultats à ceux obtenus en utilisant une méthode de retour de pertinence aveugle (Pseudo Relevance Feedback PRF) souvent utilisée comme une approche de base dans les études sur l\u0027expansion de la requête. Pour cela, le tableau 3 propose également les résultats de l\u0027approche d\u0027expansion PRF par défaut de Indri sur la collection Wt10G, où nous avons fixé à 10 le nombre de documents de retour de pertinence et à 10 le nombre de termes d\u0027expansion. En observant ces résultats, nous remarquons que PRF a dépassé notre approche en MAP, alors que pour le rappel, plus on avance dans les rangs des documents trouvés plus notre approche obtient un meilleur rappel par rapport à PRF. D\u0027ailleurs, si on regarde le rappel au dernier rang, PRF diminue même le rappel du modèle de base sans expansion. Cela signifie que PRF a perdu des documents pertinents par rapport à la base sans expansion alors que notre approche en a trouvé des nouveaux.\nFinalement, nous nous intéressons à comprendre si le fait que l\u0027entité nommée est l\u0027objet principal de la requête ou non a un effet sur les résultats après son expansion. Pour cela, nous divisons (manuellement) les 26 requêtes de l\u0027expérience WT10G en deux groupes : le groupe A signifie que l\u0027entité nommée soit le sujet de base dans la requête 8 (ex.\"Alexander Graham Bell\"), dans le groupe B nous mettons les cas où l\u0027entité nommée n\u0027est pas le sujet de base, c\u0027est-à-dire que si on enlève les autres termes de la requête cette dernière n\u0027a plus le même sens (ex. Mexican food culture TAB. 4 -Nombre de requêtes (WT10G) améliorées (+) ou dégradées (-) par l\u0027expansion sé-mantique des entités nommées en utilisant l\u0027option CC de la reformulation pour chaque rôle (A et B).\nde l\u0027entité nommée a peu d\u0027effet sur le MAP quand ces entités sont étendues. Alors que pour le rappel, on constate que pour le groupe A, toutes les requêtes ont obtenu un R@dernierRang supérieur ou égal à celui de l\u0027approche sans expansion, mais pour le groupe B, une requête a une chance sur deux d\u0027améliorer ce rappel une fois étendue.\nDiscussion\nLes résultats précédents montrent le potentiel de l\u0027expansion sémantique des entités nommées. Bien que l\u0027ajout de nouveaux termes par l\u0027approche SDM ait un effet négatif sur la performance, les approches DS et CC réussissent à améliorer significativement plusieurs mesures d\u0027évaluations. En revanche, il faut prendre en compte que nous n\u0027avons utilisé aucune pondération des termes, alors que souvent la plupart des méthodes d\u0027expansion travaillent essentiellement sur ce sujet surtout avec le modèle SDM. Notre observation concernant les meilleurs résultats de l\u0027approche PRF par rapport à notre approche au niveau de la précision, signifie que l\u0027utilisation des documents de retour de pertinence aide à modifier le classement des documents de façon à tirer les documents pertinents vers le haut de la liste. En revanche, il est intéressant de savoir que sans avoir accès à ces documents riches en information, et malgré le peu de texte dans les requêtes de test, l\u0027expansion des entités nommées a réussi à trouver plus de documents pertinents que PRF, ce dernier ayant même perdu des documents pertinents trouvés par le modèle sans expansion. Finalement, concernant les deux catégories d\u0027entités nommées (A et B), nous constatons une amélioration plus stable au niveau du rappel quand l\u0027entité nommée est le sujet principal de la requête. Cette observation semble logique, car l\u0027ajout de synonymes pour ce genre d\u0027entités est probablement utile pour trouver plus de documents pertinents, surtout que dans ce cas il y a peu d\u0027ambiguïté si on suppose que l\u0027utilisateur pense au sens le plus courant quand il construit une requête qui ne contient que l\u0027entité nommée.\n"
  },
  {
    "id": "345",
    "text": "Introduction\nLe domaine médical, comme d\u0027autres domaines de spécialité, est caractérisé par l\u0027hétérogé-néité de ses acteurs et utilisateurs. Mentionnons par exemple les médecins, les patients, les infirmiers, les pharmaciens, les internes, les brancardiers, les administratifs, les aides soignants, les chercheurs, les biologistes qui interagissent quotidiennement dans la pratique médicale. Il est évident que tous ces acteurs jouent des rôles différents et, de la même manière, les besoins de ces acteurs, y compris les besoins en information, sont aussi différents. Par exemple, un mé-decin recherche typiquement des informations précises qui lui permettent de faire un diagnostic ou des prescriptions appropriées, un chercheur est souvent à la recherche des derniers travaux dans un domaine bien ciblé, alors qu\u0027un patient peut rechercher des informations plus ou moins générales afin de retrouver des explications sur une maladie ou un traitement. De manière plus générale, nous pouvons différencier les cas suivants (Pearson, 1998) : les informations créées par des spécialistes pour les spécialistes (le cas des médecins ou des chercheurs), les informations créées par des spécialistes pour les non spécialistes (le cas des patients), les informations créées par des non spécialistes pour les non spécialistes. Les textes correspondant à chaque cas ont des propriétés et fonctions différentes. De même, ils véhiculent des informations dont le niveau de spécialisation varie et qui peuvent nécessiter une expertise plus ou moins importante pour une compréhension correcte. Pour un système de recherche d\u0027information, il peut donc être important de pouvoir distinguer entre les différents types de textes et, de cette manière, de proposer une caractérisation supplémentaire de ces textes, en en distinguant par exemple le niveau de spécialisation. Cet objectif, distinction entre les textes véhiculant différents niveaux de spécialisation, correspond à la motivation principale de notre étude.\nParmi les travaux existants, nous pouvons par exemple citer ceux relatifs à la personnalisation de la recherche d\u0027information (Pasi, 2010). Parmi les méthodes proposées, afin d\u0027adapter les résultats de recherche à un utilisateurs, certaines exploitent le filtrage collectif (Herlocker et al., 2004), le filtrage basé sur le contenu (Kassab et Lamirel, 2006), le filtrage par la combinaison des deux (Basilico et Hofmann, 2004), le filtrage par la modélisation de l\u0027utilisateur (Hadjouni Krir, 2012). Nous proposons d\u0027aborder la question par l\u0027analyse du contenu des documents. Nous proposons d\u0027exploiter la particularité des documents, qui concerne la subjectivité des acteurs telle qu\u0027elle transparaît à travers leurs incertitudes et émotions. Plus particulièrement, nous proposons d\u0027exploiter les informations relatives à l\u0027emploi de l\u0027incertitude (i.e., possible, il semblerait, certain), de la polarité (i.e., absence, pas de, ni), et des émotions (i.e., un lexique spécifique comme peur ou colère, la ponctuation répétée et intensifiée comme !!!, les émoticônes comme :-), les mots avec des caractères répétés comme maaaaal). Nous utilisons également des modifieurs lexicaux (i.e., très, beaucoup). Notre hypothèse est que les différents types de documents médicaux présentent des spécificités liées à la subjectivité, qui peuvent être utilisées pour une distinction automatique entre ces types de documents.\nDans la suite de ce travail, nous décrivons d\u0027abord le matériel utilisé (section 2), ensuite la méthode (section 3). Nous présentons et discutons les résultats produits (section 4), et terminons avec des perspectives (section 5).\nCollecte et préparation du matériel\nNous utilisons deux types de matériel : corpus (section 2.1) et ressources (section 2.2).\nCorpus\nLes données étudiées portent sur le thème de la rhumatologie (maladies des os, des articulations ou des muscles). Elles sont constituées de trois corpus, tous collectés en mai 2013 :\n-le corpus scientifique contient des articles scientifiques rédigés par des médecins ou des chercheurs. Ce corpus est constitué à partir du portail médical CISMeF . Les message sont rédigés essentiellement par des patients. Les données source (pdf, doc, html, texte...) sont normalisées au format texte et converties en utf-8, en faisaint attention aux ligatures (e.g. oe ? oe, ae ? ae, fl ? fl), aux accents mal convertis (e.g. oˆ?oˆ? ô, i¨?i¨? ï, e´?e´? é), à la suppression des caractères non imprimables (e.g. retour à la ligne, tabulation verticale). La taille des corpus est indiquée dans le tableau 1 : les documents du corpus scientifique sont les plus longs, viennent ensuite les fils de discussion du forum et les documents cliniques. Les corpus sont échantillonnés, pour assurer leur comparabilité. Le nivellement est effectué par rapport au corpus scientifique, le plus petit de l\u0027ensemble. \nScient\nRessources\nUne partie importante des ressources est dédiée à la détection de la subjectivité et des émotions (section 2.2.1). D\u0027autres ressources sont spécifiques au domaine médical (section 2.2.2). Nous décrivons également comment les ressources sont ajustées (section 2.2.3).\nDétection de la subjectivité et des émotions dans les corpus\nLes ressources linguistiques exploitées contiennent plusieurs types de marqueurs : -L\u0027incertitude (n\u003d101) peut être exprimée aussi bien avec des verbes (i.e., supposer, apparaître, suspecter), des noms (i.e., possibilité, hypothèse), des adjectifs (i.e., vraisemblable, douteux), qu\u0027avec des adverbes (i.e., sûrement, peut-être). Deux degrés d\u0027incertitude sont distingués : l\u0027incertitude forte, qui influence fortement la fiabilité des informations (i.e., douteux, évocateur, hypothèse), et l\u0027incertitude faible, qui influence faiblement la fiabilité des informations (i.e., apparemment, certain, probablement) ; -La négation (n\u003d20) peut être exprimée de différentes façons également : avec des adverbes (i.e., ne, pas), des noms (i.e., absence, lacune), des adjectifs (i.e., négatif, impossible), des prépositions (i.e., sans) ou encore avec le préfixe non-; -Les modifieurs (n\u003d17) du degré d\u0027incertitude comme peu, très peu, fort peu, extrême-ment, vraiment. Leur interprétation dépend de la polarité du terme de base : par rapport à probable, très probable conduit à une diminution de l\u0027incertitude, tandis que très douteux, par rapport à douteux seul, conduit à une augmentation de l\u0027incertitude ; -Un lexique des émotions (Augustyn et al., 2008) contient 1 144 entrées (verbes, noms et adjectifs). Les entrées du lexique sont associées à plus d\u0027une trentaine d\u0027emotions (e.g. tristesse, dégoût, joie, honte). Ceci correspond à une catégorisation plus fine que celles habituellement utilisées dans les méthodes semi-automatiques (Ekman, 1992). Pour arriver à un niveau de généralisation, les émotions sont catégorisées en trois catégories : émotions positives, négatives et neutres. Par exemple, tristesse, dégoût et honte sont des émotions négatives, joie positive, anticipation, étonnement et surprise neutres.\nDétection de notions médicales dans les corpus\nLes ressources sémantiques se composent de termes appartenant à trois types sémantiques : maladie (maladies, problèmes médicaux ou troubles), procédure (actes médicaux effectués par les médecins) et médicament. Les maladies et les procédures proviennent de la terminologie médicale SNOMED international (Côté, 1996) ; (2) l\u0027UMLS (Lindberg et al., 1993), ou Unified Medical Language System, une collection de terminologies biomédicales dévelop-pées par la US National Library of Medicine ; (3) la base UCD (Unité commune de dispensation) couvrant les médicaments qui disposent d\u0027une autorisation de mise sur le marché et sont commercialisés en France. Nous avons au total 71 449 entrées dans la catégorie maladie, 25 148 dans la catégorie procédure, et 17 571 dans la catégorie médicament.\nAjustement des ressources\nLes ressources exploitées doivent être ajustées aux données traitées. Comme c\u0027est souvent le cas avec des ressources constituées dans d\u0027autres cadres, nous cherchons par exemple, à les rendre plus précises et/ou plus exhaustives.\nRendre les ressources plus précises. Certaines entrées peuvent avoir un sens différent dans les corpus par rapport à ce qui est prévu dans les ressources. Ceci concerne surtout le lexique des émotions. Par exemple, dans ce lexique, les mots comme irriter et irritation sont assignés à la catégorie de l\u0027émotion colère, tendu à la catégorie attirance, manque à la catégorie colère. Cependant, dans un corpus médical, dans des expressions comme :\n(1) La seule différence constatée réside dans la réponse à l\u0027intensité de l\u0027irritation, provoquant une extension progressive de la douleur... (2) Elever la jambe tendue jusqu\u0027à apparition d\u0027une douleur radiculaire (3) Les guidelines ... s\u0027accordent sur le manque de preuve pour recommander des interventions préventives pour la lombalgie aiguë.\nles entrées irritation, tendu ou manque ne signifient pas les émotions prévues dans le lexique, mais reçoivent un sens propre au domaine médical. Après le nettoyage du lexique, effectué afin de réduire le bruit lors de l\u0027annotation, nous gardons 1 032 entrées.\nLa situation est similaire avec la terminologie, dont l\u0027objectif est d\u0027assurer l\u0027exhaustivité des notions recensées et dont certaines peuvent être ambiguës dans certains contextes. Par exemple, les entrées comme PDF, THE, CI, base, élément, solution s\u0027avèrent trop ambiguës. Au total, 50 entrées de la terminologie SNOMED International ne sont pas considérées.\nRendre les ressources plus exhaustives. D\u0027un autre côté, il existe aussi des cas où des unités linguistiques, potentiellement intéressantes pour l\u0027annotation, sont absentes des ressources. Ceci peut être dû (1) à la spécificité des corpus, comme par exemple avec le corpus du forum, (2) aux limites de la chaîne d\u0027annotation, ou (3) à l\u0027incomplétude des ressources, malgré la recherche de l\u0027exhaustivité. Pour y remedier, nous effectuons plusieurs traitements :\n-Pour chaque entrée simple de la terminologie, ne se terminant pas par s ou x, susceptible de marquer le pluriel, nous générons la forme plurielle correspondante. Nous obtenons un total de 6 924 nouvelles entrées, dont le type sémantique (maladie, procédure ou médicament) est identique à celui de l\u0027entrée source. Parmi les nouvelles entrées, nous avons par exemple {achalasies, achalasie} ou {acholuries, acholurie}, appartenant à la catégorie des maladies. Ceci permet de répondre en partie aux limites de la chaîne de traitement (section 3.1.1), qui parfois n\u0027est pas adaptée aux données médicales et qui peut être fautive dans la reconnaissance des lemmes ; -Dans les corpus de forum, la difficulté principale est différente : elle est liée à la présence fréquente des mots mal orthographiés. Nous faisons l\u0027adaptation suivante : Annotation linguistique. L\u0027annotation linguistique est effectuée avec l\u0027étiqueteur morphosyntaxique TreeTagger (Schmid, 1994), qui assure la segmentation des documents en mots, la catégorisation des mots selon leurs catégories syntaxiques (i.e., alimentations est un nom, saignent un verbe), et leur lemmatisation (i.e., alimentations est lemmatisé vers alimentation, saignent vers saigner).\nAnnotation sémantique. L\u0027annotation sémantique consiste en repérage des termes et de divers marqueurs des ressources (incertitude, négation, modifieurs, émotions). Pour chaque entrée des ressources, détectée par la plateforme Ogmios, le type sémantique correspondant lui est associé. L\u0027annotation sémantique est effectuée avec les formes et les lemmes. En supplé-ment des ressources, nous détectons également des marques émotives non lexicales, particulièrement fréquentes dans le corpus du forum :\n-smileys ou émoticônes : comme par exemple \u003d), ;-), :-/, XD ; -marques de rire : comme lol, mdr, haha, hihi ; -ponctuations expressives : comme !!!??, !!!!!!!!!! ; -mots avec lettres répétées : comme maaaaaaal, grrrrrr, nooooooon ; Chaque marque émotive non lexicale est typée selon qu\u0027elle dénote une émotion positive (i.e., \u003d), mdr, looool), négative (i.e., :-(, :-/ ) ou neutre (i.e., ???!!?, grrrrrrrrrr, ohhhhh). La détec-tion de ces marques est réalisée avant la tokenisation de façon à s\u0027assurer que chaque marque est considérée comme un seul token par TreeTagger. Dans le cas contraire, TreeTagger fait par exemple de !!! une suite de trois tokens.\nBilan des annotations. Grâce aux différentes ressources et annotations, nous obtenons un jeu de plusieurs types sémantiques : -incertitude, négation et modifieurs ; -émotions (lexicales et non lexicales) : positives, négatives et neutres ; -notions médicales : maladies, médicaments et procédures ; -les mêmes notions médicales mais dont les unités comportent des erreurs d\u0027orthographe. Évaluation de l\u0027annotation. L\u0027évaluation des annotations concerne les termes médicaux, les marqueurs de la négation et de l\u0027incertitude, et les marques émotives non lexicales. Pour chaque corpus, 500 annotations sont contrôlées selon trois critères :\n-détection : est-ce qu\u0027une entrée donnée est détectée ? Si oui, est-ce qu\u0027elle est contextuellement correcte ou incorrecte ? -typage : pour chaque entrée détectée, reçoit-elle un type sémantique ? Si oui, est-ce que ce type est correct ou incorrect ? -lemmatisation : est-ce que la lemmatisation de chaque entrée est correcte ou incorrecte ? La précision (proportion d\u0027annotations correctes par rapport à toutes les annotations effectuées) est calculée de deux manières :\n-précision stricte P s : on considère les vrais positifs comme étant uniquement des tokens pour lesquels tous les paramètres sont corrects (détection, typage et lemmatisation) ; -précision lâche P l : la notion de vrais positifs est élargie aux tokens dont la détection est correcte, alors que le type peut être absent (dû à un défaut de format de sortie) ou le lemme incorrect (dû à la difficulté de TreeTagger à traiter les termes médicaux).\nCatégorisation automatique\nLa catégorisation automatique selon le degré de spécialisation des documents est effectuée avec une approche par apprentissage supervisé. En apprentissage supervisé, le système automatique a besoin d\u0027exemples annotés en fonction des catégories visées (corpus d\u0027apprentissage) pour pouvoir construire un modèle de classification. Ce modèle peut être appliqué à des nouvelles données, et le système peut faire des prédictions sur leur catégorisation. Nous utilisons divers algorithmes d\u0027apprentissage supervisé implémenté dans la plate-forme Weka (Witten et Frank, 2005), et dont nous gardons le paramétrage par défaut.\nCatégories à reconnaître\nNous cherchons à distinguer trois catégories : scientifiques, cliniques et forum. En rapport avec notre hypothèse, les utilisateurs de forum laissent libre cours à leurs émotions et jugements subjectifs, tandis que les documents scientifiques et cliniques doivent montrer plus de détachement et d\u0027objectivité. Nous nous attendons donc à ce que les documents du corpus forum soient beaucoup plus faciles à discriminer que les documents des deux autres corpus. Nous effectuons une catégorisation bicatégorie en testant les trois couples possibles de caté-gories : documents cliniques et des forum, documents cliniques et scientifiques, documents de forum et scientifiques. Nous effectuons aussi un test multicatégorie, où les trois catégories sont à discriminer en même temps. Les corpus (tableau 1) sont nivellés par rapport aux corpus scientifique, avec 265 documents dans chaque catégorie. -freq correspond à la fréquence brute du descripteur ; -norm correspond à la normalisation de la fréquence par le nombre de mots du document ; -tfidf correspond à la pondération de la fréquence brute par tfidf (term frequency*inverse document frequency) (Salton, 1991) : f req * log( tot nbdoc ), où f req est la fréquence du descripteur, tot le nombre de documents dans le corpus et nbdoc le nombre de documents où ce descripteur apparaît. Cette mesure permet d\u0027évaluer l\u0027importance du descripteur par rapport au corpus. Le poids augmente proportionnellement à la fréquence du descripteur dans le document. Nous avons au total 46 descripteurs.\nDescripteurs utilisés et leur pondération\nÉvaluation de la catégorisation\nNous effectuons une validation croisée (Sebastiani, 2002), qui permet aux algorithmes d\u0027utiliser deux ensembles distincts de données pour les étapes d\u0027entraînement et de validation. La validation croisée à n plis est effectuée n fois sur des partitions de données différentes et le résultat global correspond à la moyenne des performances. Nous effectuons une validation croisée à 10 plis. Les mesures d\u0027évaluation correspondent aux moyennes de toutes les itérations. Nous calculons les mesures d\u0027évaluation standard : précision (pourcentage de documents correctement catégorisés parmi tous les documents assignés à une catégorie), rappel (pourcentage de documents correctement catégorisés par rapport aux documents qui doivent être assignés à une catégorie) et f-mesure (leur moyenne harmonique).\nLa baseline correspond à l\u0027assignation des documents dans la catégorie par défaut. Typiquement, pour un test avec deux catégories (e.g. forum et clinique) et un nombre de documents égal dans chaque catégorie, une telle baseline produirait une précision de 50 % : tous les documents sont donc assignés à une seule catégorie. Par rapport à une telle baseline, nous calculons aussi le gain obtenu, qui correspond à l\u0027amélioration effective de la performance P par rapport à la baseline BL (Rittman, 2008) :\n4 Présentation et discussion des résultats\nAnnotations et leur évaluation\nLa méthode et les ressources sémantiques ont été exploitées pour effectuer une annotation sémantique des documents. Dans la figure 1, nous présentons les fréquences de différents types sémantiques dans les trois corpus. Nous pouvons par exemple voir que les notions médicales sont plus fréquentes dans les corpus scientifique et clinique ( figure 1(a)), tandis que les émo-tions, incertitudes et négations sont plus fréquentes dans le corpus forum (figures 1(b) et 1(c)). Parmi les émotions les plus fréquentes, nous observons par exemple peur, joie, tristesse, attirance et colère dans le corpus forum, joie (i.e., plaisir, rassuré, satisfaisant), peur (i.e., inquiet, anxieux, crainte, souci) et tristesse (i.e., désolé, effondrement, destabilisé) dans le corpus clinique, et doute, étonnement, peur (i.e., appréhender, menacer, craindre) et joie (i.e., plaisir, heureux, rassuré) dans le corpus scientifique.\nLes résultats d\u0027évaluation des annotations sont présentés dans le tableau 2, en termes des précisions stricte et lâche. Nous pouvons voir que la précision stricte, tout type sémantique confondu, est supérieure à 80 %, tandis qu\u0027avec la précision lâche nous gagnons 10 %. Cela veut dire que dans 90 % de cas, les entités sont correctement reconnues, bien que leurs lemmes ou leurs types sémantiques peuvent être mal détectés.\nCatégorisation automatique\nDans le tableau 3, nous présentons les performances de la catégorisation automatique des documents. Les résultats indiqués sont obtenus avec l\u0027algorithme RandomForest (Breiman,   -dans le discours scientifique, la subjectivité a été bien étudiée (Hyland, 1995;Light et al., 2004). Il apparaît qu\u0027elle peut avoir plusieurs rôles : entre autres, elle devient incontournable car elle permet à l\u0027auteur de se positionner par rapport aux travaux d\u0027autres chercheurs ou par rapport à ses propres résultats expérimentaux obtenus lors des expé-riences scientifiques. Là aussi, la subjectivité sert de moyen de précaution, de distance et de protection ; -dans les messages de forums, la subjectivité est exprimée de façon beaucoup plus géné-rale (très souvent sans relation avec les notions médicales par exemple). De plus, elle montre souvent un lien fort avec les émotions des patients.\nConclusion et Perspectives\nDans le domaine médical, où il existe plusieurs types d\u0027acteurs et d\u0027utilisateurs avec des besoins informationnels souvent différents, nous proposons d\u0027effectuer les expériences afin d\u0027observer si les traces de subjectivité permettent de différencier entre les documents scientifiques, cliniques et les messages de forum. Nous utilisons pour ceci les descripteurs relatifs à l\u0027incertitude, la négation, les modifieurs, les émotions (lexicales et non lexicales) et les notions médicales. Nos expériences montrent qu\u0027il existe en effet une corrélation forte de la subjectivité et des notions médicales avec ces différents types de documents, avec les performances de catégorisation automatique étant souvent supérieures à 0,90. Le travail effectué est réalisé avec les documents médicaux relevant du thème rhumatologie, mais nous pensons que ces résultats sont généralisables à d\u0027autres thèmes de la médecine. Cela voudrait dire que les descripteurs proposés peuvent être utilisés dans les moteurs de recherche pour apporter une caractérisation supplémentaire aux documents (ici, un niveau de spécialisation et les destinataires attendus). Dans cette optique, les résultats de recherche ne sont pas filtrés a priori, mais enrichis avec des annotations supplémentaires : il revient à l\u0027utilisateur de décider s\u0027il veut consulter un document avec un niveau de spécialisation donné. De la même manière, nous pensons que les descripteurs proposés et testés peuvent être appliqués dans la tâche de recherche d\u0027information appliquée à d\u0027autres domaines de spécialité.\nParmi les perspectives de ce travail, nous prévoyons d\u0027effectuer la catégorisation automatique de phrases, afin de détecter les catégories d\u0027émotions et de subjectivité des acteurs. Nous voulons aussi tester l\u0027impact individuel de différents types de descripteurs. Les classes de marqueurs portant sur l\u0027incertitude, la négation et leur interaction avec les modifieurs peuvent être affinées (Zadeh, 1972;Akdag et al., 1992;Cornelis et al., 2004;Akdag et al., 2001). Les descripteurs proposés peuvent être combinés avec d\u0027autres descripteurs exploités dans la littéra-ture (e.g. le lexique de manière générale, les informations syntaxiques et stylistiques, l\u0027analyse morphologique des termes). De même, une autre baseline, plus évoluée, peut être utilisée. Finalement, la méthode peut être adaptée à d\u0027autres domaines (e.g. juridique, financier), où il existe aussi des utilisateurs avec de différents types d\u0027expertise.\n"
  },
  {
    "id": "347",
    "text": "Introduction\nBien que réputé comme singulièrement difficile et considéré comme NP-complet, le problème du calcul des traverses minimales d\u0027un hypergraphe a suscité l\u0027intérêt de la communauté scientifique (Berge (1989); Kavvadias et Stavropoulos (2005); Hébert et al. (2007) ;Murakami et Uno (2013); Toda (2013)). Cet intérêt pour les traverses minimales est dû au fait qu\u0027elles présentent une solution pour de nombreuses applications dans des domaines variés tel que la cryptographie, le web sémantique, l\u0027e-commerce, etc. (Hagen (2008)).\nLa principale difficulté que pose l\u0027extraction des traverses minimales réside dans le nombre exponentiel de ces dernières, même quand l\u0027hypergraphe d\u0027entrée est simple. À titre d\u0027exemple, considérons l\u0027hypergraphe H \u003d (X , ?) avec l\u0027ensemble des sommets X \u003d (x 1 , x 2 , . . . , x 2n ) et l\u0027ensemble des hyperarêtes ? \u003d ({x 1 , x 2 }, {x 3 , x 4 }, . . . , {x 2n?1 , x 2n }). Le nombre de traverses minimales est égal à 2 n tandis que le nombre de sommets est égal à 2n. Les algorithmes d\u0027extraction des traverses minimales les plus performants sont des amélio-rations de l\u0027algorithme de Berge (1989). Ce dernier traite les hyperarêtes une à une en calculant à chaque itération i les traverses minimales de l\u0027hypergraphe constitué par les i-èmes hyperarêtes considérées. Avec pour objectif d\u0027optimiser le calcul des traverses minimales, notre approche repose sur cette idée en usant du paradigme \"diviser pour régner\". Le principe consiste à réduire ce nombre d\u0027itérations en partitionnant l\u0027hypergraphe en un nombre précis d\u0027hypergraphes partiels, équivalent au nombre de transversalité de l\u0027hypergraphe d\u0027entrée H. A partir de chaque hypergraphe partiel H i , nous calculons alors ce que nous appelons les traverses minimales locales à H i . Le produit cartésien de ces traverses minimales locales correspondra alors à un ensemble de traverses de H qui seront soumises à une vérification de la minimalité pour être considérées comme des traverses minimales. En outre, pour un hypergraphe H avec un nombre de transversalité égal à k, le fait de partitionner H en k hypergraphes H i permet d\u0027éliminer le test de la minimalité pour les ensembles de sommets de taille k qui seront considérés comme traverses minimales de H, sans aucun autre calcul supplémentaire.\nCet article est organisé comme suit : dans la section 2, nous reviendrons sur différents algorithmes, proposés dans la littérature, pour calculer les traverses minimales d\u0027un hypergraphe. Ensuite, dans la section 3, nous rappelons des notions-clés issues de la théorie des graphes et de la fouille de données, que nous combinerons pour introduire notre approche basée sur la notion de traverse minimale locale et d\u0027hypergraphe partiel. La section 4 présentera notre approche pour le calcul des traverses minimales à travers un algorithme original, appelé LOCAL-GENERATOR. Enfin, une étude expérimentale sur des hypergraphes aléatoires sera décrite dans la section 5.\nEtat de l\u0027art\nBerge a été le premier à proposer un algorithme dédié à l\u0027extraction des traverses minimales mais cette solution est impraticable sur des hypergraphes de grande taille car elle nécessite le stockage des traverses minimales temporaires, calculées après chaque ajout d\u0027une nouvelle hyperarête. En effet, l\u0027algorithme de Berge commence par calculer l\u0027ensemble des traverses minimales de la première hyperarête avant de le mettre à jour en ajoutant incrémentalement les autres hyperarêtes, une à une. D\u0027après Hagen (2008) et Takata (2007, l\u0027algorithme de Berge présente une complexité super-polynomiale en la taille de l\u0027entrée et de la sortie. Récemment, Boros et al. (2008) ont prouvé que le temps d\u0027exécution de l\u0027algorithme avait une borne supé-rieure sub-exponentielle.\nPlusieurs travaux ont ensuite tenté d\u0027améliorer l\u0027algorithme de Berge, parmi lesquels on peut citer ceux de Bailey et al. (2003), Dong et Li (2005) et Kavvadias et Stavropoulos (2005). L\u0027approche la plus performante est celle de ces derniers qui, pour remédier à la principale lacune de l\u0027algorithme pionner, à savoir une consommation excessive de mémoire, ont proposé une technique consistant à combiner les sommets qui appartiennent aux mêmes hyperarêtes à chaque itération de l\u0027algorithme. Alors que l\u0027algorithme de Berge effectue un parcours en largeur de l\u0027arbre des traverses minimales, Kavvadias et al. proposent un parcours en profondeur de l\u0027hypergraphe d\u0027entrée en introduisant la notion de noeud généralisé, qui représentent un ensemble de sommets appartenant aux mêmes hyperarêtes et qui est mis à jour. Enfin, notons que dans cet algorithme, la vérification de la minimalité est effectuée à travers la notion de noeud approprié.\nEn l\u0027assimilant à un problème de dualisation de fonctions booléennes et monotones, Fredman et Khachiyan ont proposé un algorithme incrémental pour l\u0027extraction des traverses minimales en vérifiant si deux formules f et g sont mutuellement duales, (i.e. si f (x) \u003d ¯ g(¯ x)). Cette approche a été reprise, de différentes manières, par Eiter et Gootlob ou encore par Boros (Eiter et al. (2002); Boros et al. (2003)). La version de ce dernier généralise le principe de la dualisation dans la mesure où les formules ne traitent plus que des variables booléennes mais manipulent, désormais, aussi des variables entières bornées.\nAdoptant un parcours en largeur d\u0027abord et opérant par niveau, l\u0027algorithme MTMINER de Hébert et al. (2007)   (2013)). Actuellement, ces algorithmes sont considérés dans la littérature comme les plus efficaces aussi bien en termes de temps d\u0027exécution qu\u0027en consommation mémoire, notamment sur des hypergraphes de grande taille.\nLe dernier algorithme en date dédié au calcul des traverses minimales, est celui de Toda. Il fait appel à des structures de données compressés BDD (diagramme de décision binaire Notre approche, qui repose sur le paradigme \"diviser pour régner\", est une extension de l\u0027algorithme de Berge. Alors que dans ce dernier, ainsi que dans les améliorations qui en ont été proposées, l\u0027idée est de traiter les hyperarêtes une à une, dans cet article, nous proposons de traiter les hyperarêtes ensemble par ensemble. L\u0027hypergraphe d\u0027entrée H se trouve alors partitionné en un nombre d\u0027hypergraphes partiels égal au nombre de transversalité k de H. Chaque hypergraphe partiel renferme des traverses minimales locales et le produit cartésien, combiné à un test de la minimalité, permet de retrouver l\u0027ensemble des traverses minimales de H. Le choix du nombre d\u0027hypergraphes partiels n\u0027est pas arbitraire puisqu\u0027il garantit que les traverses dont la taille est égale à k peuvent être directement considérées comme des traverses minimales de H. traverses minimales, basée sur la notion des traverse minimale locale, nous avons combiné des concepts de la théorie des hypergraphes (hypergraphe, traverse, nombre de transversalité) avec d\u0027autres de la fouille de données (ensemble essentiel, support). Le nombre de transversalité d\u0027un hypergraphe est la notion-clé de cette section, autour de laquelle est bâtie notre approche.\nDéfinition 1 HYPERGRAPHE SIMPLE (Berge (1989)) Soit H \u003d (X , ?) avec X \u003d {x 1 , x 2 , . . . , x n } un ensemble fini d\u0027éléments et ? \u003d {e 1 , e 2 , . . . , e m } une famille de parties de X . H constitue un hypergraphe simple sur X si :\n1.\nLes éléments de X sont appelés sommets et ceux de ? hyperarêtes de l\u0027hypergraphe et, dans la suite, nous ne considérerons que des hypergraphes simples.\nExemple 1 La figure 1 illustre un hypergraphe simple H \u003d (X , ?) tel que X \u003d (1, 2, 3, 4, 5, 6, 7, 8, 9) et ? \u003d { e 1 , e 2 , e 3 , e 4 , e 5 , e 6 } avec e 1 \u003d {1, 2}, e 2 \u003d {2, 3, 7}, e 3 \u003d {3, 4, 5}, e 4 {4, 6}, e 5 \u003d {6, 7, 8} et e 6 \u003d {7, 9}.  (Berge (1989)) Soit un hypergraphe H \u003d (X , ?) tel que X est l\u0027ensemble de sommets et ? \u003d {e 1 , e 2 , . . . , e m } est l\u0027ensemble des hyperarêtes de H. T ? X est une traverse de H si T e i \u003d ? ?i \u003d 1, . . . , m. ? H désigne l\u0027ensemble des traverses définies sur H. Une traverse T de ? H est dite minimale si 0 ? T s.t. T 0 ? ? H . On notera M H , l\u0027ensemble des traverses minimales définies sur H. Le nombre minimum de sommets d\u0027une traverse est appelé le nombre de transversalité de l\u0027hypergraphe H et on le désigne par : Berge (1989), M H représente les hyperarêtes de l\u0027hypergraphe transversal correspondant à H et défini sur X .\nExemple 2 Dans l\u0027exemple illustratif de la figure 1, l\u0027ensemble M H de l\u0027hypergraphe est : { {1, 4, 7}, {2, 4, 7}, {2, 5, 6, 7}, {2, 5, 6, 9}, {2, 4, 8, 9}, {2, 4, 6, 9}, {2, 3, 6, 7}, {2, 3, 6, 9}, {1, 5, 6, 7}, {1, 3, 6, 7}, {1, 3, 6, 9} et {1, 3, 4, 8, 9}. Un hypergraphe H peut être représenté par une matrice d\u0027incidence IM H définie par un triplet (?, X , R) où R ? ? × X est une relation binaire entre les hyperarêtes et les sommets. Cette matrice d\u0027incidence IM H , associée à l\u0027hypergraphe H \u003d (X , ?), est définie par : ?x i ? X et ?e j ? ?, R(e j , x i ) \u003d 1 si x i ? e j et R(e j , x i ) \u003d 0 sinon. La Table de la figure 1 représente la matrice d\u0027incidence associée à l\u0027hypergraphe H.\nDéfinition 3 SUPPORT D\u0027UN ENSEMBLE DE SOMMETS\nSoit l\u0027hypergraphe H \u003d (X , ?) et X un sous-ensemble de X . Nous définissons Supp(X) comme le nombre d\u0027hyperarêtes de H, renfermant au moins un élément de X : Supp(X) \u003d |{e ? ?|?x ? X ? R(e, x) \u003d 1}| Ainsi, l\u0027ensemble X peut être vu comme une disjonction de sommets (x 1 ? x 2 ? . . . ? x n ) tel que la présence d\u0027un seul sommet de X suffit à affirmer que X satisfait une hyperarête donnée, indépendamment des autres sommets. (Casali et al. (2005)) Soit l\u0027hypergraphe H \u003d (X , ?) et X ? X . X représente un ensemble essentiel de sommets si et seulement si :\nDéfinition 4 ENSEMBLE ESSENTIEL DE SOMMETS\nIl est important de souligner que les ensembles essentiels, extraits à partir d\u0027une matrice d\u0027incidence, vérifient la propriété d\u0027idéal ordre, i.e, si X est un ensemble essentiel, alors ?Y ? X, Y est aussi un ensemble essentiel. De plus, la notion de traverse peut être redéfinie par le biais du support d\u0027un ensemble de sommets et de la notion d\u0027ensemble essentiel, selon la proposition 1.\nProposition 1 TRAVERSE MINIMALE\nUn sous-ensemble de sommets X ? X est une traverse minimale de l\u0027hypergraphe H, si X est essentiel et si son support est égal au nombre des hyperarêtes de H, autrement dit, X est un ensemble essentiel tel que Supp(X)\u003d|?|.\nPreuve 1 Soit X un ensemble essentiel de sommets tel que Supp(X)\u003d|?|. Par conséquent, X e i \u003d ? ?e i ? ?, i \u003d 1, . . . , m. Donc, d\u0027après la définition 2, X est une traverse. La minimalité de X tient à son \"essentialité\". En effet, puisque X est essentiel, alors son support est strictement supérieur à celui de ses sous-ensembles directs. Par conséquent, 1 ? X s.t. Supp(X 1 )\u003d|?|. X est donc une traverse minimale. (Berge (1989)\nDéfinition 5 UNION ET PRODUIT CARTÉSIEN\nLe résultat de cette union est un hypergraphe dont l\u0027ensemble des sommets est constitué de ceux de H et de G, et l\u0027ensemble des hyperarêtes contient celles de H et G, qui par souci de simplification sera aussi noté H ? G :\n. . , ? m ) H × G représente le produit cartésien des deux hypergraphes dont le résultat est un hypergraphe dont l\u0027ensemble des sommets contient ceux des deux hypergraphes. Quant à l\u0027ensemble des hyperarêtes, il est aussi noté H × G et est égal au produit cartésien de ? et de ? autrement dit à l\u0027union de tous les couples possibles d\u0027hyperarêtes tels que le premier élément appartient à ? et le deuxième à ? :\nProposition 2 (Kavvadias et Stavropoulos (2005)) Soient H et G deux hypergraphes simples. Les traverses minimales de l\u0027hypergraphe H ? G sont des couples, minimaux au sens de l\u0027inclusion, générés par le produit cartésien des ensembles de traverses minimales de H et de G :\nDéfinition 6 HYPERGRAPHE PARTIEL(Berge (1989)) Un hypergraphe partiel H est la restriction d\u0027un hypergraphe H à un sous-ensemble d\u0027hyperarêtes ? incluses dans ? et aux sommets contenus dans ces hyperarêtes.\nDans le cadre de cet article, nous proposons d\u0027étendre la proposition 2 en considérant plus de deux hypergraphes. Plus précisément, à partir d\u0027un hypergraphe H\u003d(X , ?), dont le nombre de transversalité ? (H) est égal à k, et d\u0027une traverse minimale T \u003d {x 1 , x 2 , . . . , x k } de M H de taille k dont les sommets sont numérotés par ordre de support décroissant, nous proposons de construire k hypergraphes partiels H i \u003d (X i , ? i ), i \u003d 1, . . . , k tels que :\n-X i \u003d {x ? X | x ? e, ?e ? ? i } On peut remarquer que les hypergraphes partiels H i vérifient de façon évidente les propriétés suivantes :\n-? ? tel que e ? ? i ? ? j , i \u003d j. Les traverses minimales de l\u0027hypergraphe partiel H i sont appelées traverses minimales locales à H i et leur ensemble est noté par M Hi .\nExemple 3 L\u0027hypergraphe de l\u0027exemple illustratif de la Figure 1 a un nombre de transversalité égal à 3. Sachant que H possède 2 traverses minimales de cardinalité minimale égale à 3, {1, 4, 7} et {2, 4, 7}. Prenons, par exemple, {1, 4, 7}. Après avoir ordonné les trois sommets le composant, selon un ordre décroissant de support, nous obtenons les trois hypergraphes partiels, présentés par la Figure 2, tel que H 1 ne contient que les hyperarêtes auxquelles appartient le sommet 7 (dont le support est égal à 3), H 2 ne contient que celles auxquelles appartient 4 (dont le support est égal à 2) et H3 contient les hyperarêtes restantes, i.e., celles qui renferment le sommet 1. Il importe de noter qu\u0027en choisissant {2, 4, 7}, au lieu de {1, 4, 7}, le résultat reste le même.\nTraverses minimales locales : approche et algorithme\nOptimiser le calcul de ces traverses minimales revient donc principalement à réduire le nombre de candidats traités. Ceci passe par la réduction de la taille de l\u0027hypergraphe d\u0027entrée. L\u0027approche proposée dans cet article consiste à construire, à partir de l\u0027hypergraphe d\u0027entrée FIG. 2: Les 3 hypergraphes partiels dérivés de H : H 1 , H 2 et H3 H, k hypergraphes partiels (H 1 , H 2 , . . . , H k ) tel que k correspond au nombre de transversalité de H. Le calcul de l\u0027ensemble des traverses minimales locales, M H i de chaque hypergraphe partiel H i s\u0027en trouve amélioré puisque la taille de H i est relativement petite par rapport à celle de H. Ainsi, nous proposons d\u0027effectuer l\u0027union des hypergraphes partiels de façon à dé-terminer l\u0027ensemble des traverses minimales M H de H à partir des k-uplets,minimaux au sens de l\u0027inclusion, issus du produits cartésien des ensembles de traverses minimales locales déter-minées pour les hypergraphes partiels M H i . Dans ce qui suit, nous présentons l\u0027algorithme LOCAL-GENERATOR dédié au calcul des traverses minimales et basé essentiellement sur les notions de nombre de transversalité et d\u0027hypergraphe partiel.\nL\u0027algorithme LOCAL-GENERATOR\nL\u0027algorithme LOCAL-GENERATOR, dont le pseudo-code, est décrit par l\u0027Algorithme 1 prend en entrée une matrice d\u0027incidence (correspondant à l\u0027hypergraphe d\u0027entrée) et fournit en sortie l\u0027ensemble des traverses minimales. On suppose que les sommets de l\u0027hypergraphe sont triés par ordre lexicographique. LOCAL-GENERATOR démarre par un appel à la fonction GETMINTRANSVERSALITY, dont le pseudo-code est décrit par l\u0027Algorithme 2. Cette fonction recherche une traverse minimale dont la taille est minimale, égale au nombre de transversalité de l\u0027hypergraphe. Pour ce faire, la fonction parcourt les sommets, un par un. Pour chaque élément x i de X , GETMINTRANSVERSALITY supprime de la matrice d\u0027incidence IM H les hyperarêtes de ? qui contiennent x i . La fonction prend ensuite le sommet, différent de x i , ayant le plus grand support dans IM H et réactualise la matrice de la même manière, i.e., en retirant les hyperarêtes contenant ce dernier sommet. Le traitement, entre la ligne 11 et la ligne 15, s\u0027arrête dès que IM H se vide complètement de toute hyperarête. Le vecteur T tmp sert à stocker les sommets supprimés, un à un à partir de X, pour aboutir à ? \u003d ?. Si le nombre de sommets de T tmp est le plus petit, obtenu jusque-là dans la fonction, la cardinalité de T tmp est stockée dans min et les éléments de T tmp sont copiés dans T . Le traitement est répété pour tous les sommets de X. A chaque itération de la boucle de la ligne 5, l\u0027ensemble ? est réinitialisé avec les éléments de ? et T tmp est vidé des éléments qu\u0027il renferme. Au final, GETMINTRANSVERSA-LITY retourne la suite de sommets qui a permis de \"vider\" la matrice d\u0027incidence en un nombre minimum d\u0027étapes. Cette suite-là, contenue dans T tmp , représente une traverse minimale dont la taille est nécessairement égale au nombre de transversalité de l\u0027hypergraphe d\u0027entrée H.\nUne fois le nombre de transversalité calculé et une traverse minimale de taille minimale renvoyée par la fonction GETMINTRANSVERSALITY, l\u0027algorithme LOCAL-GENERATOR Algorithme 1 : LOCAL-GENERATOR Entrées : Une matrice d\u0027incidence IM H associée à H \u003d (X , ?) Sorties : M H , ensemble des traverses minimales de H début 1 initialiser(T : vecteur) ;\nconstruit, à partir de la matrice d\u0027incidence IM H , k hypergraphes partiels et fait appel à la fonction COMPUTE_TM pour construire leurs traverses minimales locales, stockées dans M H i (ligne 9). Cette fonction 3 prend en entrée une matrice d\u0027incidence associée à un hypergraphe partiel H i de H et calcule, par niveaux, l\u0027ensemble des traverses minimales locales à H i selon la définition 1. A la fin de la boucle de la ligne 6, LOCAL-GENERATOR a déjà calculé les ensembles des traverses minimales locales. Le produit cartésien (ligne 11) de ces ensembles M Hi , permet de construire l\u0027ensemble ? H . Chaque élément de ? H issu de ce produit cartésien représente une traverse. Il reste à vérifier sa minimalité. Un des intérêts de notre décomposition de l\u0027hypergraphe initial est d\u0027éviter de tester la minimalité des éléments de ? H dont la cardinalité est égale à k. En effet, ces derniers représentent des traverses minimales de H puisqu\u0027il ne peut pas exister une traverse minimale de taille inférieure au nombre de transversalité de H. Pour les traverses de taille supérieure à k, LOCAL-GENERATOR teste la minimalité (lignes 15 ? 16) suivant la Proposition 1. Si le support d\u0027un candidat X est strictement supérieur au maximum des supports de ses sous-ensembles directs alors X est une traverse minimale et est ajouté à M H .\n3. Dans les expérimentations, nous avons utilisé l\u0027algorithme MTMINER pour implémenter cette fonction. \nEtude Expérimentale\nDifférentes expérimentations ont été menées sur des jeux de données variés afin d\u0027éva-luer l\u0027algorithme LOCAL-GENERATOR. Le premier lot de jeux de données considérés dans cette étude expérimentale comporte des hypergraphes générés à partir des bases de données \"Accidents\" 4 et \"Connect-4\" 5 alors que le deuxième lot contient des hypergraphes aléatoires générés, à travers le \"random hypergraph generator\" implementé par Boros et al. (Boros et al. (2003)), en fonction du nombre de sommets, du nombre d\u0027hyperarêtes et de la taille minimale des hyperarêtes. dès que la taille des plus petites traverses minimales de l\u0027hypergraphe d\u0027entrée est élevée ce qui lui permet de partitionner l\u0027hypergraphe en plusieurs hypergraphes partiels. De plus, le nombre de traverses minimales est très important. Sur des hypergraphes renfermant peu de traverses minimales, LOCAL-GENERATOR peine à se montrer efficace puisque le nombre de traverses minimales devient négligeable par rapport au nombre de candidats traités et testés. Cette constatation est confirmée par le tableau 2 en observant les hypergraphes H1 et H4. Quand le nombre de traverses minimales a pratiquement doublé, l\u0027écart en secondes est passé de 420 à 1234 entre les deux algorithmes.\nConclusion\nDans cet article, nous avons introduit une nouvelle approche pour le calcul des traverses minimales d\u0027un hypergraphe. Cette approche repose sur le paradigme \"diviser pour régner\" afin de partitionner l\u0027hypergraphe d\u0027entrée en hypergraphes partiels, en fonction du nombre de transversalité. Le calcul des traverses minimales locales, correspondantes à ces hypergraphes partiels, permet de retrouver l\u0027ensemble des traverses minimales à travers un produit carté-sien combiné à un test de la minimalité. Ceci nous a permis d\u0027introduire un nouvel algorithme LOCAL-GENERATOR pour l\u0027extraction des traverses minimales. L\u0027étude expérimentale a confirmé l\u0027intérêt de notre approche sur un type précis d\u0027hypergrpahes renfermant des propriétés données. Les résultats obtenus nous incitent, par ailleurs, à réfléchir à la mise en place d\u0027un algorithme hybride qui s\u0027auto-adapte à la valeur du nombre de transversalité, i.e., si ce dernier est jugé petit, alors on évitera de générer les hypergraphes partiels. Par ailleurs, nous chercherons à explorer l\u0027espace de recherche en profondeur d\u0027abord dont l\u0027efficacité par rapport à l\u0027exploration par niveaux a été montré.\n"
  },
  {
    "id": "348",
    "text": "Introduction\nConstruire un modèle articulatoire de la parole, c\u0027est être capable d\u0027indiquer les mouvements des articulateurs (mâchoires, lèvres, etc.) à l\u0027origine de celle-ci (voir figure 1, à gauche). Les applications pratiques d\u0027un tel modèle sont nombreuses 1 . Nous exposons ici comment nous avons extrait un modèle articulatoire à partir de données recueillies auprès d\u0027un locuteur. Ce travail se situe dans la lignée des travaux initiés par Maeda (1990). Il a construit son modèle articulatoire (voir figure 1, à droite) au moyen d\u0027analyses en composantes principales sur des données de même type. Puis il l\u0027a évalué de façon acoustique en comparant les sons réels aux sons produits par un synthétiseur de sons piloté par son modèle. La nouveauté de notre démarche consiste en l\u0027utilisation d\u0027une méthode d\u0027analyse 3-way pour extraire le modèle, et de méthodes d\u0027apprentissage supervisé pour le valider. Notre éva-luation se fait en comparant de façon phonétique les sons prédits aux sons réels. L\u0027acoustique intervient de surcroît dans notre évaluation car nous mettons en parallèle les performances de notre modèle et celles du modèle acoustique formé des coefficients cepstraux ces dissimilarités, le stress brut est donné par la formule Stress\nC\u0027est par le choix de la fonction f que le stress est minimisé.\nLe 3-way MDS pour nos données Nos données, coordonnées des points des articulateurs sur les radiographies ont été transformées en autant de tableaux de distances entre images que de points. Les méthodes de 3-way MDS permettent de traiter simultanément plusieurs tableaux par MDS, d\u0027après Borg et Groenen (1997);Carroll (1972);Carroll et Chang (1970). Nous avons choisi la fonction smacofIndDiff du package SMACOF (de Leeuw et Mair, 2009), avec le paramètre \"idioscal\" correspondant à la variante \"idiosyncratic\" décrite dans Carroll (1972), qui s\u0027est avérée moins gourmande en mémoire vive que la méthode INDSCAL de Carroll et Chang (1970) que nous avions utilisée pour des données plus simples dans Busset et Cadot (2013). Avec 200 itérations, nous avons pu obtenir à partir des 50 points les positions des 732 images dans des espaces allant de 2 dimensions à 18 (64bits, 4 coeurs, 8 Gio de RAM, pour dim\u003d18, temps\u003d48h).\n"
  },
  {
    "id": "350",
    "text": "Introduction\nLe nombre de caméras de vidéosurveillance installées dans le monde augmente chaque jour. En France, le système de la RATP déployé sur Paris comprend 9000 caméras fixes et 19000 mobiles. Lors de faits particuliers (e.g., agressions, vols), les opérateurs de vidéo surveillance se basent sur les indications spatiales et temporelles de la victime et sur leur connaissance de la localisation des caméras pour sélectionner les contenus intéressants pour l\u0027enquête. Deux grands problèmes peuvent alors survenir : (1) le temps de réponse est long (jusqu\u0027à plusieurs jours de traitement) et (2) un risque important de perte de résultats à cause d\u0027une mauvaise connaissance du terrain (appel à des opérateurs extérieurs). Le but de notre recherche est de définir des outils d\u0027assistance aux opérateurs qui puissent, à partir d\u0027une trajectoire donnée, sélectionner de façon automatique les caméras pertinentes par rapport à la requête.\nContexte de la vidéosurveillance et problématique\nDans un système de vidéosurveillance le contenu est acquis par des caméras fixes et mobiles installées dans des contextes différents (à l\u0027extérieur ou à l\u0027intérieur des bus, des stations de métro, dans les rues, etc.).\nLorsqu\u0027une victime d\u0027une agression porte plainte, on lui demande de décrire les éléments qui pourront aider à trouver les segments vidéos pertinents. Les éléments d\u0027une telle description sont : la Localisation (e.g., Paris), la Date et l\u0027Heure (e.g., 10 octobre 2013 entre 10h et 11h), l\u0027Événement recherché (e.g., vol de sac), et la Trajectoire de la victime avant et après l\u0027événement (e.g., Rue de Rivoli, de la sortie du musée du Louvre jusqu\u0027à l\u0027entrée de la station de métro Chatelet). Les opérateurs, en se basant sur les aspects spatiaux et temporels de la requête et sur leur propre connaissance de la localisation des caméras, sélectionnent les caméras qui auront référencé r des informations concernant la trajectoire de la personne pour ensuite analyser les enregistrements vidéo.\nLe but et de définir un(des) nouveau(x) opérateur(s) prennant comme entrée une trajectoire cible définie comme un ensemble d\u0027unités u k (segments cartographiques de la rue de Rivoli) et un intervalle temporaire et qui calcule la liste d\u0027extraits vidéo pertinents pour la requête :\nce qui équivaut à dire que la caméra c i a potentiellement \"filmé\" le segment u k de la trajectoire de la requête entre t Dans notre approche nous proposons un modèle à quatre couches indépendantes : (1) Ré-seau routier, (2) Réseau de transport, (3) Objets, (4) Caméras.\nLa couche Réseau Routier est basée sur l\u0027approche de modélisation sous forme de graphe consacrée dans la littérature (Liu et al., 2012). Le réseau routier est un graphe G\u003d (N, A), dans lequel les noeuds sont les croisements des rues et les arcs sont les rues. Le Réseau de Transport est aussi modélisé sous forme de graphe. A ce niveau, les noeuds sont constitués par des stations de bus qui sont situées sur des segments de rues. Des séquences ordonnées de noeuds de transport forment des lignes (e.g., des lignes de bus). La couche Objets modélise les positions des objets fixes et mobiles par rapport aux couches sous-jacentes. Chaque Objet Mobile transmet périodiquement sa position en fonction de différentes stratégies (e.g., chaque fois que l\u0027objet change de segment) que nous ne traitons pas dans ce papier. Nous supposons que nous avons au moins une mise à jour de la position par segment de rue. L\u0027ensemble de points avec les timestamps associés forment leur trajectoire (Gutting et al., 2006). Au-dessus de toutes ces couches, nous modélisons les Caméras de Vidéosurveillance. Un schéma simplifié de cette couche est illustré dans la Figure 1. Cette couche est composée des caméras fixes et mobiles. Les caméras fixes ont une position 2D fixée au moment de l\u0027installation. Les caméras mobiles sont associées à un objet mobile (e.g., bus) et leur trajectoire est la même que celle de l\u0027objet. La nouvelle génération de caméras a des capteurs GPS incorporés et même des boussoles. Les technologies développées autour de ces caméras rendent possible l\u0027extraction automatique des caractéristiques de prise de vue, par exemple : l\u0027orientation, le zoom, la distance focale, etc. En se basant sur ces éléments, il est possible de modéliser le champ de vue et de tracer ces modifications dans le temps (Arslan Ay et al., 2010 Q1 : Sélectionner les caméras fixes dont le champ de vue a intersecté un segment donné :\nSEGMENT, geometry(F, C.POSITION),\u0027mask\u003danyinteract\u0027)\u003d\u0027TRUE\u0027 AND C.IDFOV\u003dF.IDFOV AND E.ID\u003didSegmentRequete AND ((F.TIMESTAMP\u003c\u003d t 2 AND F.TIMESTAMP \u003e\u003d t 1 ) OR (F.TIMESTAMP\u003ct 1 AND NOT EXISTS (SELECT F1.TIMESTAMP FROM FOV F1 WHERE F1.TIMESTAMP\u003eF.TIMESTAMP AND F1.TIMESTAMP\u003c\u003dt 1 AND F1.IDFOV\u003dF.IDFOV))) } La requête sélectionne les caméras dont la géométrie du champ de vue (calculée à partir des caractéristiques stockées dans la table FOV et de la position de la caméra par la fonction geometry(F, C.POSITION)) intersecte la géométrie du segment de rue donné (SDO_RELATE est un opérateur spatial qui vérifie si deux géométries ont n\u0027importe quel type d\u0027intersection (paramètre anyinteract)) dans l\u0027intervalle recherché.\nQ2 : Sélectionner les caméras mobiles dont la trajectoire a intersecté un segment donné :\nSELECT MC.IDMC FROM \u003c EDGES E, MO, MC\u003e WHERE { (SDO_RELATE(E.SEGMENT, MO.POSITION), \u0027mask\u003danyinteract\u0027) \u003d\u0027TRUE\u0027 AND t 1 \u003c\u003d MO.TIMESTAMP AND MO.TIMESTAMP\u003c\u003d t 2 AND MO.IDOBJ\u003dMC.IDOBJ AND E.ID\u003didSegmentRequete } La deuxième requête sélectionne les caméras mobiles associées aux objets mobiles dont la trajectoire a intersecté le segment donné entre t 1 et t 2 .\nConclusion et perspectives\nAfin d\u0027aider les opérateurs, nous proposons une approche de modélisation spatiale et temporelle des caméras de vidéosurveillance dans le cadre d\u0027un réseau routier, modélisation basée sur des informations spatiales et des trajectoires des objets mobiles. A partir de ce modèle on peut sélectionner la liste des caméras qui qui ont potentiellement \"filmé\" la trajectoire d\u0027une personne. Nous envisageons d\u0027étendre le modèle pour prendre en compte les réseau de camé-ras à l\u0027intérieur de stations de métro ou de train.Un autre aspect concerne l\u0027ordonnancement des résultats en fonction de la distance des caméras par rapport aux segments de la requête. \nRéférences\nSummary\nOur work concerns the assistance to the operators of videosurveillance in the search for partiular videos (e.g., attack of persons, lost object) by the automatic identification of a set of cameras likely to have filmed a required scene. This paper presents a multi-layer modeling (cameras fixed and mobile, road network, public transport network) whose characteristic consists in rather including calculation data of the geometry of the cameras viewpoints on the network layer than to store this geometry as such.\n"
  },
  {
    "id": "351",
    "text": "Introduction\nL\u0027extraction de motifs fréquents est une tâche essentielle en fouille de données. Les motifs permettent de résumer un jeu de données de manière intelligible et peuvent être utilisés pour d\u0027autres tâches comme l\u0027analyse d\u0027association, la classification supervisée associative, ou la classification à base de motifs. Des algorithmes efficaces ont été proposés pour extraire des motifs dans différents types de données comme les données transactionnelles, les séquences d\u0027évènements, et les graphes. Le principal inconvénient des techniques d\u0027extraction de motifs est l\u0027abondance des motifs produits, qui résulte de la nature combinatoire des algorithmes en oeuvre. Différentes solutions ont été proposées face à ce problème, comme l\u0027intégration de contraintes dans les algorithmes (Boulicaut et Jeudy, 2005), le filtrage des motifs par des mesures d\u0027intérêt (Blanchard, 2005;Blanchard et al., 2007), et l\u0027extraction de représentations condensées des motifs fréquents, i.e. un sous-ensemble des motifs qui permet de générer la totalité des motifs de manière exacte ou approchée (Calders et al., 2006). Malgré ces efforts, le problème reste à peine atténué, comme le rappelle l\u0027étude récente de Giacometti et al. (2013).\nDans cet article, nous proposons une méthode originale pour extraire un résumé compact, représentatif et intelligible des motifs fréquents dans des données transactionnelles ou séquen-tielles (par exemple une ou plusieurs séquences d\u0027évènements, du texte, des séquences biologiques). Ce résumé peut être lu et interprété directement, mais il offre aussi la possibilité de générer de manière approchée l\u0027ensemble des motifs fréquents et d\u0027estimer leur support. Dans le détail, notre approche consiste à extraire un nouveau type de motifs que nous appelons motifs récursifs, i.e. des motifs de motifs, à l\u0027aide d\u0027un algorithme hiérarchique agglomératif nommé RepaMiner. La nature hiérarchique de ces motifs nous permet de produire non pas un simple ensemble de motifs mais une véritable structure, nommée RPgraph, fondée sur les motifs et dérivée de dendrogrammes. Les spécificités de notre approche sont les suivantes :\n-L\u0027algorithme RepaMiner est un algorithme dynamique, comme certaines méthodes récentes de fouille de motifs (Vreeken et al., 2011 2 Extraction ascendante hiérarchique de motifs récursifs L\u0027algorithme RepaMiner (Recursive Pattern Miner) a été conçu à l\u0027origine pour analyser des données séquentielles, l\u0027analyse de données tabulaires étant un simple cas particulier. Cependant, pour présenter notre approche, il est plus clair de nous placer dans le cas classique d\u0027un jeu de données transactionnel. Nous adoptons ce point de vue dans les sections qui suivent. Nous considérons donc un ensemble I de littéraux nommés items, et un ensemble T de transactions où chaque transaction t est un ensemble t ? I.\nMotif récursif\nDefinition 1 (Motif récursif). Etant donné un ensemble I d\u0027items, un motif récursif est soit un item de I, soit une paire non ordonnée {x, y} où x et y sont des motifs récursifs. x et y sont appelés les parents du motif récursif. Example 1. Soit l\u0027ensemble d\u0027items I \u003d {a, b, c, d, e, f }. m 1 \u003d {{a, b}, c} et m 2 \u003d {{{a, b}, {c, d}}, {e, f }} sont deux motifs récursifs. Ils sont représentés dans la figure 1.\nUn motif récursif est donc une agrégation de deux éléments, et peut être représenté par un arbre binaire (figure 1). Les motifs récursifs généralisent la notion d\u0027itemset en y introduisant un ordre partiel (les niveaux de l\u0027arbre). Le support dans T d\u0027un motif récursif m est défini de manière classique comme étant le nombre de transactions de T qui contiennent tous les items de m. Les niveaux hiérarchiques des items ne sont donc pas pris en compte dans le calcul du support. Un motif récursif est qualifié de fréquent dans T si son support est supérieur à un seuil minsup défini par l\u0027utilisateur.\nL\u0027algorithme RepaMiner\nL\u0027espace de recherche des motifs récursifs fréquents dans T est de taille exponentielle par rapport au nombre d\u0027items. Dans RepaMiner, nous préférons adopter la stratégie gloutonne de la CAH afin de réduire l\u0027espace de recherche à une taille quadratique. En partant des motifs récursifs singletons, on agrège à chaque itération les deux motifs récursifs les plus proches au sein d\u0027un nouveau motif récursif, puis on met à jour les données. La similarité entre motifs est évaluée par la surface de leur intersection, définie par support(m 1 ? m 2 ) × (nombre d\u0027items dans m 1 ?m 2 ). Cette mesure est utilisée dans les algorithmes de pavage par itemsets (tiling). La maximisation de la surface sert aussi bien de mesure de similarité que de critère d\u0027agrégation puisqu\u0027elle s\u0027adapte autant aux singletons qu\u0027aux motifs issus d\u0027agrégations. Parmi une dizaine de mesure étudiées, nous avons constaté empiriquement que maximiser la surface permet à RepaMiner de minimiser l\u0027erreur de restauration des itemsets (Yan et al., 2005).\nInput : ensemble de transactions T , seuil de support minimal minsup. Output : Ensemble des motifs récursifs fréquents H. C ? {itemsets fréquents de taille 2} 9: end while 10: return H RepaMiner n\u0027utilise que des méthodes classiques de calcul d\u0027itemsets (motifs ensemblistes) de taille 2. C\u0027est la propriété dynamique de notre approche (création de nouveaux items à chaque itération) qui permet de construire des hiérarchies d\u0027items à l\u0027aide de méthodes d\u0027extraction d\u0027itemsets de taille 2. Le pseudo-code est résumé dans l\u0027algorithme 1. C est l\u0027ensemble des motifs récursifs candidats, parmi lesquels le meilleur (au sens du critère d\u0027agréga-tion) sera choisi. A la ligne 3, C est initialisé à l\u0027ensemble des itemsets de taille 2 fréquents dans T , accompagnés de leur support. A la ligne 5, on identifie le meilleur motif candidat m au sens du critère d\u0027agrégation. La procédure MiseAJourDonnées() de la ligne 7 modifie le jeu de données T . Un nouvel item est créé pour coder les occurrences du motif m, et toutes les occurrences des parents de m qui participent au support de m sont supprimées. L\u0027idée est de ne pas perdre l\u0027information représentée par les 1 dans les données, en la répartissant parmi les trois items. La procédure s\u0027achève en supprimant les items qui sont devenus non fréquents suite à la mise à jour des données. A la ligne 8, on calcule les itemsets de taille 2 fréquents dans T en prenant en compte les mises à jour effectuées sur T à la ligne 7. Ces itemsets constituent le nouvel ensemble C de candidats. Lorsqu\u0027il n\u0027y a plus aucun motif fréquent candidat au titre de meilleure agrégation, l\u0027algorithme retourne l\u0027ensemble H des motifs récursifs qui ont été extraits, i.e. l\u0027ensemble des agrégations qui ont été réalisées. Chaque motif de H est fréquent (par construction), et est accompagné de sa valeur de support au moment de l\u0027agrégation.\nRésultats de l\u0027algorithme. RepaMiner produit l\u0027ensemble H des motifs récursifs qui ont été générés à chaque itération. Chaque motif est accompagné de la valeur de support qu\u0027il présentait au moment de sa création. Pour visualiser l\u0027ensemble H, nous représentons chaque motif récursif maximal 1 par un dendrogramme indicé par le complément à 1 du support. Ce choix est justifié par le fait que le complément du support est une ultramétrique sur l\u0027ensemble des items produits par RepaMiner. Il est à noter que H ne constitue une hiérarchie unique et complète que dans le cas où le seuil de support minsup est nul. Avec RepaMiner, dans le cas général, le seuil minsup impose une coupure dans le dendrogramme. On obtient alors plusieurs hiérarchies déconnectées, chacune ayant à son sommet un motif récursif maximal.\nVisualisation du flux agglomératif à l\u0027aide d\u0027un RPgraph. Nous tirons profit des recouvrements entre les motifs récursifs maximaux de H pour construire une représentation plus synthétique, que nous nommons RPgraph (Recursive Pattern graph). Un RPgraph peut être vu comme une vue \"de dessus\" des dendrogrammes représentant chaque motif maximal. Cette vue nous prive de la hauteur d\u0027agrégation, i.e. le support du motif, mais permet de bénéficier de davantage d\u0027espace pour disperser les structures dans le plan. Nous profitons de cet espace pour réunir les dendrogrammes qui ont des intersections communes. Ce processus est illustré en figure 2.a. Au final, nous obtenons un graphe dont les noeuds représentent les motifs récur-sifs (soit un item, soit une agrégation), et les arcs relient les motifs agrégés. Par exemple, un motif récursif m \u003d {x, y} est représenté par un noeud m qui est relié à x et y par deux arcs (x, m) et (y, m). Les arcs sont orientés vers m pour montrer le \"flux agglomératif\" découvert dans les données. Le support du motif récursif m est représenté par la largeur des deux arcs. Les couleurs des arcs servent uniquement à repérer les deux branches (x, m) et (y, m) d\u0027une même agrégation, en leur donnant la même couleur.  \nConclusion\nNous avons proposé une méthode originale pour extraire un résumé compact, représentatif et intelligible des motifs fréquents dans des données transactionnelles ou séquentielles. Cette méthode repose sur la notion de motif récursif, et sur l\u0027algorithme dynamique RepaMiner qui permet de réaliser l\u0027extraction ascendante hiérarchique de ces motifs. Nous générons une véri-table structure nommée RPgraph qui permet de visualiser un \"flux agglomératif\" dans les données. Au final, RepaMiner peut être vu comme une Classification Ascendante Hiérarchique adaptée aux items (données transactionnelles) et aux évènements (données séquentielles). Ce travail se poursuit par des expérimentations qui montrent que le résumé généré est représentatif puisqu\u0027il permet d\u0027estimer précisément l\u0027ensemble des itemsets ou épisodes fréquents.\n"
  },
  {
    "id": "352",
    "text": "Introduction\nLa classification recouvrante consiste à organiser de manière non-supervisée un ensemble d\u0027individus en classes chevauchantes ou recouvrantes composées d\u0027individus similaires. L\u0027 étude des méthodologies associées vise avant tout à répondre aux besoins réels et pratiques communs à de nombreux domaines d\u0027application : qu\u0027il s\u0027agisse de classer des documents (textes, images, vidéos), de constituer des communautés de personnes sur des critères sociaux ou marketing ou encore d\u0027exhiber des groupes de gènes ou de molécules présentant des caractéristiques structurelles ou fonctionnelles communes, il est très fréquents d\u0027être confronté à des données qui s\u0027organisent naturellement en classes recouvrantes. Dans ces applications, le recours à des techniques usuelles de classification stricte ou disjointe apporterait un biais préjudiciable à l\u0027usage final de la classification.\nDepuis plus d\u0027une trentaine d\u0027années, la classification recouvrante est identifiée comme une problématique à part entière et donne lieu à des avancées constantes au fil de l\u0027évolution des techniques de classification traditionnelles. Partant des premiers travaux de Shepard et Arabie (1979) portant sur le clustering (recouvrant) additif, la problématique s\u0027est ensuite orientée vers l\u0027acquisition et la théorisation des classifications hiérarchiques recouvrantes ou empiétantes (Diday, 1987;Diatta et Fichet, 1994;Bertrand et Janowitz, 2003) avant d\u0027être reconsidérée plus récemment et de façon plus formelle du point de vue \"partitionnement\" (Banerjee et al., 2005;Cleuziou, 2008;Depril et al., 2012). Parmi ces dernières avancées on notera d\u0027une part les modèles additifs ALS (Depril et al., 2012) et son équivalent en terme de modèle de mélanges recouvrants MOC (Banerjee et al., 2005) et d\u0027autre part le modèle OKM (Cleuziou, 2008) que l\u0027on pourrait qualifier de modèle géométrique tant il modélise les intersections de clusters par un barycentre plutôt que par une somme des profils de clusters comme c\u0027est le cas des modèles additifs précédents. Ces trois dernières méthodes se fondent sur un même cadre théorique qui consiste à explorer de manière efficace l\u0027espace des solutions recouvrantes pour un nombre fixé de classes. Les algorithmes qui en découlent sont guidés par des critères objectifs quantifiant l\u0027accumulation des imprécisions liées à l\u0027affectation de chaque données à une classe ou à une combinaison de classes ; ce choix d\u0027affectation posant lui même un problème combinatoire puisque le nombre de combinaisons possibles d\u0027un ensemble fixé de classes est exponentiel.\nFort des avancées de cette dernière décennie dans le domaine de la classification recouvrante, nous nous intéressons à présent à la possibilité d\u0027étendre ces modèles à l\u0027utilisation de noyaux. La \"kernélisation\" des méthodes recouvrantes permettrait d\u0027en étendre l\u0027usage à des données de très grande dimensionalité (telles que les données textuelles), à des espaces non-euclidiens à l\u0027origine, d\u0027avoir recours aux mêmes procédés de projections que ceux qui ont fait leurs preuves en classification traditionnelle (non-recouvrante) et d\u0027envisager de nouvelles avancées dans les domaines du clustering spectral ou semi-supervisé recouvrant (Dhillon, 2004;Filippone et al., 2008;Kulis et al., 2009) par exemple.\nDans cette contribution nous présentons tout d\u0027abord les difficultés qui ont freiné l\u0027évolu-tion des modèles recouvrants vers le clustering à noyaux (Section 2). Nous proposons ensuite en Section 3 une nouvelle modélisation ensembliste des recouvrements. Cette modélisation se base sur le modèle OKM et permet à la fois de corriger certains biais du modèle originel et de rendre possible le passage aux noyaux via un algorithme adaptatif également présenté dans cette section. Nous confirmons enfin les attentes liées à ce nouveau modèle par le biais d\u0027une étude empirique préliminaire sur des jeux de données réels et artificiels (Section 4).\nProblématique des noyaux en classification recouvrante\nEn clustering, l\u0027astuce du noyau, consiste à réaliser le processus de classification (classiquement les réallocations successives) dans un espace induit par le noyau sans jamais calculer explicitement les projections des données de départ dans ce nouvel espace. Considérons X \u003d {x 1 , . . . , x n } dans R p l\u0027ensemble des données à traiter et K la matrice noyau induite par une projection implicite ?(.) telle que K i,j \u003d i ), ?(x j ) A priori, la méthode des kmoyennes ne peut pas être \"kernélisée\" directement puisqu\u0027elle considèrerait la minimisation du critère d\u0027inertie suivant\net que ce dernier s\u0027appuie sur k centre mobiles (modélisant les profils des clusters) {m c } k c\u003d1 dont on ne doit pas calculer explicitement les projections ?(m c ). Cependant en observant que les centres mobiles sont redéfinis à chaque itération de façon optimale par les moyennes (centres de gravité) des individus de chaque classe, (Dhillon, 2004) ont proposé de se passer des variables associées à ces centres et d\u0027intégrer leur définition dans le critère initial :\nCette astuce permet d\u0027envisager un clustering totalement identique à k-moyennes dans n\u0027importe quel espace induit par un noyau K, cependant il est important d\u0027observer que cette transformation a un coût (raisonnement sur les paires d\u0027individus).\nLorsque l\u0027on tente de kernéliser les méthodes de classification recouvrante de type \"réal-location dynamique\" (e.g. ALS, MOC ou OKM), il semblerait naturel de procéder de façon similaire. Si l\u0027on choisit -sans perte de généralité -le modèle OKM 1 , on serait amené à considérer le critère objectif suivant\noù 1 xi??c \u003d 1 si x i ? ? c et 0 sinon. Ce critère quantifie une somme de distances entre chaque individu x i et une combinaison (la moyenne dans OKM) des profils des clusters auxquels il appartient. Malheureusement, la définition d\u0027un profil de cluster ne correspond plus à un simple centre de gravité : les profils dépendent les uns des autres ce qui rend impossible la réécriture à partir des seules données relationnelles du noyau (produits scalaires entre individus).\nBen N\u0027Cir et Essoussi (2012) se sont intéressés à la kernélisation de OKM ; afin de contourner cette difficulté, ils ont proposé de conserver les variables de profils mais de limiter leur domaine de définition à l\u0027ensemble des individus X, à la manière de médoïdes. L\u0027individu choisi comme profil du cluster ? c sera celui qui minimise un critère d\u0027inertie sur ? c :\nOn voit effectivement que les profils de clusters pourront ainsi être choisis à partir des informations issues du noyau uniquement et une fois ces profils déterminés, les auteurs montrent que l\u0027heuristique utilisée dans OKM pour l\u0027affectation peut être réalisée à l\u0027identique dans l\u0027espace de projection. Néanmoins le recours aux médoïdes est une solution coûteuse nécessitant de considérer toutes les paires d\u0027individus de chaque cluster pour leur mise à jour. Enfin et surtout, la définition du médoïde (2) reste un choix arbitraire qui n\u0027est pas induit par le critère initial (1) et ne garantit pas la convergence de l\u0027algorithme qui en découle.\nDans la suite de l\u0027article nous proposons une approche différente pour le problème du clustering recouvrant à noyau. Il s\u0027agit d\u0027une approche ensembliste qui diffère légèrement du modèle OKM originel en corrigeant certains de ses inconvénients mais surtout permettant l\u0027utilisation des noyaux dans un processus recouvrant et convergeant.\n3 OKSETS : modèle et algorithme\nLe modèle de combinaison\nNous commencerons par l\u0027observation de Depril et al. (2012) concernant la méthode ALS, qui est également valable pour toute méthode de classification recouvrante fondée sur une combinaison de profils de clusters : dans ces approches, la notion de \"profils\" de clusters ne correspond plus à l\u0027idée intuitive de \"centres\" que l\u0027on peut avoir dans les méthodes nonrecouvrantes de type k-moyennes ; en effet, l\u0027optimisation du critère objectif conduit à des profils pouvant être éloignés des données des clusters qu\u0027ils représentent. Plus précisément, Cleuziou (2008) montre par dérivation de (1) que dans OKM les profils peuvent être mis à jour l\u0027un après l\u0027autre de façon optimale avec la règle suivante : Nous proposons de profiter de la nécessité de faire abstraction des variables de profils dans le passage aux noyaux, pour corriger le phénomène de \"sur-recouvrements\" que nous venons d\u0027évoquer. Pour cela nous commençons par introduire la notion de nuage dans une classification.\nDéfinition 1 Étant donné un ensemble de clusters ? \u003d {? 1 , . . . , ? q }, on appellera \"nuage\" de ? l\u0027application N (.) qui lui associe l\u0027union de ses extensions\nOn parlera également de \"nuage\" associé à un individu N (x i ) pour indiquer de façon analogue l\u0027union des clusters auxquels il appartient\nNous définissons ensuite un nouveau critère objectif pour la classification recouvrante. Ce nouveau critère est défini sur la base d\u0027une somme d\u0027erreurs locales modélisées par les distances des individus au centre de gravité de leur nuage associé :\nDe cette manière on est assuré que l\u0027affectation d\u0027un individu à un cluster se réalise sur la base de sa distance au centre de gravité du cluster. De plus, un individu sera affecté à plusieurs clusters si le centre de gravité du nuage de cette combinaison est plus proche de cet individu. Enfin, on montre que ce critère est adapté à l\u0027utilisation de noyaux :\net on observe que dans le cas d\u0027un clustering non-recouvrant, chaque individu appartient à un seul cluster x i ? ? c ? N (x i ) \u003d ? c , ce qui nous ramène exactement au critère objectif de l\u0027algorithme des k-moyennes à noyaux (cf. Section 2). Le modèle K-OKSETS défini précédemment est donc une généralisation recouvrante du modèle des k-moyennes à noyaux.\nL\u0027algorithme adaptatif\nContrairement au contexte du k-moyennes à noyaux où l\u0027on sait que le centre de gravité d\u0027un cluster correspond à sa représentation optimale, dans le contexte recouvrant nous avons évoqué le fait que cela n\u0027est plus vérifié. Ainsi un algorithme (batch) classique qui consisterait à réaffecter itérativement tous les individus avant de remettre à jour globalement tous les nuages n\u0027assurerait pas la décroissance du critère (3) et n\u0027aurait donc aucune raison de converger vers un recouvrement et des profils stables. Nous proposons donc un algorithme adaptatif guidé par le critère J K?OKSets (Figure 1). L\u0027étape cruciale de l\u0027algorithme réside dans la pro-\nK-OKSETS\nEntrées : X un ensemble de n individus, K une matrice noyau sur X, k le nombre de clusters attendu, {x 1 , . . . , L\u0027heuristique d\u0027affectation que nous utilisons consiste, pour un individu x i à : 1. Ordonner les clusters du plus proche au plus éloigné de x i selon la distance ?(\n2. Affecter x i au premier cluster puis aux suivants tant que l\u0027erreur locale (rappelée cidessous) diminue\n3. Revenir à l\u0027ancienne affectation si l\u0027erreur locale n\u0027est pas améliorée La structure de données sur laquelle se base l\u0027algorithme conserve les informations utiles à l\u0027ajout ou la suppression d\u0027un élément de manière à simplifier les calculs d\u0027erreurs locales et donc de l\u0027erreur globale. Elle est organisée de manière arborescente où chaque noeud de l\u0027arbre correspond à une combinaison de clusters et contient : l\u0027ensemble des individus affectés à cette combinaison, le nuage associé ainsi qu\u0027un score correspondant à la moyenne des produits scalaires sur les couples contenus dans le nuage : Figure 2, illustre une structure ainsi que la classification en 3 classes associée : par exemple l\u0027individu x 6 apparaît à l\u0027intersection des 3 clusters, on le retrouve donc dans le noeud d\u0027éti-quette ? 1 ? ? 2 ? ? 3 dans lequel on trouve également l\u0027information sur le nuage et le score associé à ce nuage. Ainsi la réaffectation d\u0027un individu x i nécessitera les étapes suivantes :\n1. Un premier parcours partiel de l\u0027arbre pour supprimer x i du noeud auquel il est affecté, puis une mise à jour de tous les nuages et scores des noeuds prédécesseurs :\n2. Un second parcours partiel pour l\u0027heuristique d\u0027ajout nécessitant les mises à jour inverses selon le même procédé.\nCompte-tenu de l\u0027augmentation exponentielle du nombre théorique de combinaisons de clusters avec le nombre de clusters k, la structure proposée n\u0027est envisageable que sous l\u0027hypothèse que seul un petit nombre de combinaisons est effectivement exploré par l\u0027heuristique d\u0027affectation et la condition que seule les combinaisons explorées soient stockées. Il n\u0027est en effet pas déraisonnable de penser que des combinaisons de clusters éloignés ont peu de chances d\u0027accueillir des individus qui sont affectés à leurs plus proches clusters d\u0027après l\u0027heuristique choisie. Nous observerons dans l\u0027étude empirique qui suit les premiers gages de confirmation de cette hypothèse.\nÉtude empirique\nNous proposons dans cette section une première évaluation empirique du modèle de classification ensembliste OKSETS et de son utilisation dans le cadre du clustering à noyau. Nous proposons en Figure 3 différentes classifications d\u0027un même jeu de données artificielles généré par deux gaussiennes légèrement chevauchantes de 500 individus chacune en deux dimensions. On observe en premier lieu (ligne supérieure de la Figure 3) que OKM et sa variante ensembliste génèrent des résultats comparables en terme de nature du recouvrement entre les deux classes (individus en noir) ; cependant OKSETS présente une \"bande\" de recouvrement plus étroite du fait de l\u0027absence de profils de clusters mobiles. Les autres visualisations rendent compte des classifications obtenues par K-OKSETS avec différents noyaux : polynomial de degré 2 et gaussiens avec variances de 0.5 et 2 ; nous visualisons les classifications (et en particulier les recouvrements) à la fois dans l\u0027espace initial et dans l\u0027espace de projection approximé par un positionnement multidimensionnel (MDS) à partir des distances euclidiennes induites par le noyau. On notera de façon générale la cohérence des classes et recouvrement générés et en particulier qu\u0027il semblerait possible de détecter par les recouvrements le contour des classes en utilisant un noyau approprié.\nDans un second temps, nous procédons à une évaluation dite \"externe\" des classifications obtenues. Il s\u0027agit alors de mesurer l\u0027adéquation entre la classification générée par l\u0027algorithme de façon totalement non-supervisée et une classification de référence attendue par des experts du domaine. Nous utilisons trois jeux de données réelles : Iris, EachMovie et Scene.\n-Iris (D.J. Newman et Merz, 1998)  visée ou non), il est constitué de 150 fleurs décrites selon 4 caractéristiques numériques, chacune des fleurs étant étiquetée par une seule des trois catégories Setosa, Versicolor ou Virginica, dont la première est réputée facilement identifiable, contrairement aux deux autres catégories plus mélangées. -EachMovies 2 est plutôt utilisé en classification \"multi-étiquettes\", il s\u0027agit d\u0027extraits de films décrits par les préférences d\u0027utilisateurs (3 notes par film) et organisés en classes de genres où chaque film peut être associé à plusieurs genres. Nous utilisons ici un sousensemble de 3 genres correspondant à 75 extraits de films associés en moyenne à 1.14 genres chacun (taux de recouvrement). (k-moyennes à noyau) qui ne produisent pas de recouvrements avec OKM, OKSETS et KOKSETS qui génèrent des classifications recouvrantes. Toutes les mé-thodes ont été paramétrées avec un nombre de clusters attendus (k) égal au nombre d\u0027étiquettes associées au jeu de données. Les résultats les plus remarquables pour chaque type de classification sont identifiés en gras dans le tableau. On observe sur Iris que, bien que la classification attendue soit non recouvrante, il est possible d\u0027obtenir une meilleure adéquation en autorisant des recouvrements limités : en effet sans recours aux noyaux OKSETS (linéaire) obtient un score de 0.82 (FBCubed) légèrement supérieur au meilleur score obtenu sans recouvrements (0.81 pour KkMeans avec noyau Gaussien). Ceci s\u0027explique en particulier par le chevauchement naturel entre les catégories Versicolor et Virginica qui conduit, lorsqu\u0027on l\u0027autorise, à un recouvrement ; en terme d\u0027adéquation, mieux vaut attribuer un individu à deux classes plutôt qu\u0027à une seule avec le risque de choisir la mauvaise. On notera que la projection opérée par un noyau polynomial n\u0027est pas pertinente sur Iris, tandis que le noyau Gaussien permet d\u0027amélio-rer la qualité des classifications ; en particulier le score obtenu par KOKSETS avec le noyau gaussien (? \u003d 1) est intéressant dans la mesure où il égale le meilleur score tout en réduisant les recouvrements (1.07 contre 1.14).\nLes tests réalisés sur les données EachMovie sont encore plus encourageants. La possibilité d\u0027introduire les noyaux dans le clustering recouvrant conduit à un score inégalé de 0.69 obtenu en alliant la qualité à la fois en précision et en rappel, et avec un taux de recouvrement identique à la référence (1.14). Enfin, les expérimentations comparatives menées sur Scene valident sur un jeu de données plus important, l\u0027intérêt de la modélisation ensembliste proposée dans OKSETS sans d\u0027avantage de gain à attendre de l\u0027utilisation de noyaux, que ce soit en partitionement ou en recouvrement.\nPour terminer cette première étude expérimentale, nous avons cherché à confirmer empiriquement l\u0027hypothèse utilisée par l\u0027algorithme KOKSETS selon laquelle le nombre de combinaisons (de clusters) explorées durant le processus de classification reste raisonnable par rapport à l\u0027ensemble théorique des combinaisons possibles. La Figure 4 rend compte d\u0027une première simulation qui tend à confirmer l\u0027hypothèse : par exemple pour 15 clusters sur Iris, au maximum 52 combinaisons seront considérées et stockées dans la structure, contre plus de 32 000 combinaisons théoriques envisageables.  \nConclusion et perspectives\nNous avons abordé dans cette contribution la problématique de la classification recouvrante à travers la possibilité d\u0027étendre de façon théoriquement bien fondée les modèles actuels vers le clustering à noyau. Nous nous sommes concentré pour le moment sur l\u0027extension du modèle OKM en proposant une version ensembliste légèrement corrigée nommée OKSETS pour Overlapping k-Sets qui se prête aisément au passage aux noyaux via l\u0027algorithme K-OKSETS (Kernelized-OKSETS). Les retours d\u0027expériences ont permis de confirmer la faisabilité et l\u0027intérêt de recourir aux noyaux en classification recouvrante.\nCependant, même si une solution théorique au problème est à présent effective et confirmée de manière pratique, une seconde phase d\u0027étude sera indispensable afin de proposer une expé-rimentation plus étendue faisant intervenir d\u0027avantage de jeux de tests, plus variés en terme de domaines et plus complexes en terme de recouvrements (nature et taille). Nous avons égale-ment observé que l\u0027utilisation de noyaux offre les possibilités inattendues de réguler la taille des recouvrements et dans une certaine mesure d\u0027utiliser les recouvrements pour détecter les contours des clusters ; il s\u0027agira alors d\u0027exploiter le potentiel de ces observations. Enfin, nous envisageons de la même manière d\u0027adapter le modèle additif ALS aux noyaux puis, à moyen terme, de s\u0027appuyer sur ces nouvelles avancées pour appréhender des problématiques nouvelles telles que le clustering spectral et semi-supervisé recouvrant.\n"
  },
  {
    "id": "353",
    "text": "Introduction\nLes approches de bi-partitionnement sont devenues un sujet d\u0027intérêt majeur en raison de leurs nombreuses applications dans le domaine de la fouille des données. Une méthode de bi-partitionnement, aussi appelée bi-clustering, co-clustering ou classification croisée, est une méthode d\u0027analyse qui vise à regrouper des données en fonction de leur similarité. La straté-gie classique des méthodes de bi-partitionnement cherche à trouver des sous-matrices ou des blocs, qui représentent des sous-groupes de lignes et des sous-groupes de colonnes d\u0027une matrice de données.\nUn des objectifs d\u0027une méthode de bi-partitionnement est la recherche d\u0027un couple de partitions, l\u0027une sur les observations (les lignes d\u0027une matrice de données), l\u0027autre sur les variables (colonnes d\u0027une matrice de données), tel que la \"perte d\u0027information\" due au regroupement soit minimale (Charrad et al., 2008) ; c\u0027est-à-dire de sorte que la différence entre l\u0027information apportée par la matrice de données initiale et celle apportée par le regroupement obtenu soit minimale. Depuis le premier algorithme de bi-partitionnement, appelé Block Clustering proposé par Hartigan (1972), de nombreuses techniques ont été proposées telles que l\u0027énumération exhaustive (Tanay et al., 2002), l\u0027analyse spectrale (Greene et Cunningham, 2010), les réseaux bayésiens (Shan et al., 2010) et d\u0027autres (Angiulli et al., 2006). La pondération de variables est un processus couramment utilisé dans le domaine de l\u0027apprentissage non supervisé, dont le but est de pondérer (ou sélectionner) des variables à partir d\u0027une base de données en appliquant un algorithme d\u0027apprentissage. La pondération de variables est difficile car, contrairement à l\u0027apprentissage supervisé, les données ne sont pas étiquetées (Guyon et Elisseeff, 2003;Tsai et al., 2012). La taille des bases de données pose un défi sans précédent pour la fouille de données massives. Afin de remédier au problème de la grande dimension des variables, nous proposons dans un cadre de bi-partitionnement, une pondération des sous-ensembles de variables au lieu de pondérer les variables séparément.\nEtat de l\u0027art\nDans le domaine de la classification, bien que la plupart des méthodes utilisées cherchent à construire des partitions soit sur l\u0027ensemble des observations soit sur celui des variables sé-parément. Il existe d\u0027autres méthodes de bi-partitionnement qui considèrent simultanément les deux ensembles (Hartigan, 1972;Govaert, 1983;Nadif et Govaert, 2010;Ayadi et al., 2012). Les méthodes de bi-partitionnement utilisant les cartes auto-organisatrices (SOM) (Kohonen et al. (2001)) ont été définies par plusieurs auteurs (Busygin et al., 2002;Cottrell et al., 2004;Benabdeslem et Allab, 2012). Ce type de méthodes rentrent dans la catégorie des approches basées sur le partitionnement car souvent, elles utilisent des algorithmes de classification simple appliqués séparément sur les lignes et les colonnes d\u0027une matrice des données. (Govaert, 1983) a défini un algorithme de bi-partitionnement nommé \"Croeuc\" pour les données quantitatives et qui consiste à déterminer une série de couples de partitions minimisant une fonction de coût sur la matrice des données en appliquant l\u0027algorithme des nuées dynamiques. (Long et al., 2005) ont proposé une approche de décomposition matricielle \"NBVD\" (Non-negative Block Value Decomposition) pour le bi-clustering. Cette approche permet de décomposer une matrice de données en trois composantes en procédant par un algorithme itératif appliqué sur des données non négatives. Dans la même catégorie de méthodes, (Labiod et Nadif, 2011) ont proposé une approche de factorisation matricielle appelée \"CUNMTF\" (Co-clustering under Nonnegative Matrix Tri-Factorization). L\u0027idée principale de cette approche est que la structure du bloc latent dans une matrice de données rectangulaire non négative est factorisée en deux facteurs plutôt que trois : la matrice des coefficients des lignes et la matrice des coefficients des colonnes qui indiquent respectivement le degré d\u0027appartenance d\u0027une ligne et d\u0027une colonne à un cluster. On retrouve dans la littératures plusieurs approches de bi-partitionnement, qui utilisent des algorithmes hiérarchiques (Caldas et Kaski, 2011;Mao et al., 2005;Getz et al., 2000a). L\u0027approche la plus utilisée dans cette famille de modèle est CTWC (Coupled two-way clustering) (Getz et al., 2000a). CTWC consiste à appliquer un algorithme de classification hiérarchique, le SPC \"Super Paramagnetic Clustering (SPC)\" (Getz et al., 2000b) sur les colonnes en utilisant toutes les lignes puis sur les lignes en utilisant toutes les colonnes.\nDans ce papier, une approche de pondération de blocs de variables en utilisant un modèle de bi-partitionnement, basée sur les cartes topologiques est proposée. Plusieurs méthodes de pondération de variables sont recensées dans la littérature scientifique. Nous trouvons des approches de pondération locale basées sur l\u0027apprentissage non supervisé (Blansché et al., 2006;Frigui et Nasraoui, 2004) ainsi que sur les k-means (Huang et al., 2005). Il existe aussi des mé-thodes de sélection locale de caractéristiques basées sur l\u0027apprentissage non supervisé (Basak et al., 1998;Liu et al., 2009;Grozavu et al., 2009;Chen et al., 2012). (Ouattara et al., 2013) proposent une extension des cartes topologiques pour le traitement des données multiblocs.\nDans ce papier, nous proposons une approche qui permet d\u0027aborder le problème de la pondération de \"blocs de variables\" dans un cadre de bi-clustering topologique. Pour cela, nous nous basons sur l\u0027algorithme BiTM (Biclustering using Topological Maps) (Chaibi et al., 2013). Notre modèle attribue à chaque bloc de variables un nouveau score de pondération constituant un vecteur des pondérations locale nommé F BR (Feature Block Relevance). La principale différence entre notre approche nommée FBR_BiTM et les méthodes existantes est que la pondération n\u0027est pas associée à une seule variable, mais à un bloc de variables.\nBi-partitionnement et pondération des blocs de variables\nLe modèle FBR_BiTM est constitué d\u0027un ensemble de cellules discrètes C de taille K appelées \"carte\". Pour chaque paire de cellules (c, r) de la carte, la distance ?(c, r) est définie par le plus court chemin reliant les cellules r et c sur la grille. Soit d l\u0027espace euclidien des données et A la matrice des données où chaque observation\n. L\u0027objectif de FBR_BiTM est de fournir des bi-clusters organisés dans une carte topologique et un vecteur des pondérations locales de chaque bloc de variables. Dans FBR_BiTM, chaque cellule c de C est associée à un prototype sous la forme d\u0027un vecteur :\npour sauvegarder les informations associées respectivement aux observations et aux variables.\nLe modèle que nous proposons dans ce papier se base sur la formulation du modèle BiTM (Chaibi et al., 2013) en introduisant le nouveau paramètre f l r qui sera estimé au cours de l\u0027apprentissage. Ainsi nous proposons de minimiser la nouvelle fonction de coût suivante :\nCette fonction de coût peut être réécrite de la manière suivante :\nOù :\n. . , g k } désigne l\u0027ensemble des vecteurs prototypes. F \u003d {f 1 , . . . , f k } représente l\u0027ensemble des vecteurs de pondération. ? z est la fonction d\u0027affectation des lignes. ? w est la fonction d\u0027affectation des colonnes. K T (?(r, k)) est la fonction de voisinage. En pratique, nous utilisons la fonction de voisinage suivante :\nT où T représente le paramètre contrôlant le rayon du voisinage.\nLa minimisation de J (? w , ? z , G, F ) se fait d\u0027une manière itérative avec la version nuées dynamiques par l\u0027exécution de 4 étapes jusqu\u0027à un nombre d\u0027itérations prédéfini (algorithme 1). De la même manière que les cartes topologiques, on fait décroître le rayon d\u0027apprentissage pour constituer deux phase : une phase d\u0027auto-organisation associée aux grandes valeurs et une phase de quantification associée aux petites valeurs.\nExpérimentations\nNous avons testé l\u0027algorithme FBR_BiTM avec des jeux de données du répertoire UCI (Frank et Asuncion (2010) Phase itérative 1-Affectation des observations : chaque observation x i est affectée au prototype g k le plus proche en utilisant la fonction d\u0027affectation :\n2-Affectation des variables : chaque variable x j est affectée au prototype le plus proche en utilisant la fonction d\u0027affectation :\n3-Mise à jour des prototypes : les composantes g l r des prototypes sont mis à jour suivant la formule ci-dessous :\n4-Mise à jour des pondérations : les composantes f l r des vecteurs des pondérations sont mis à jour suivant la formule ci-dessous :\nRÉPÉTER les phases 1, 2, 3 et 4 jusqu\u0027à t \u003d t max . TAB. 2 -Description des jeux de données simulées.\nProtocole de validation\nAfin de comparer FBR-BiTM avec les approches de bi-partitionnement, nous avons sé-lectionné les approches suivantes : BiTM (Chaibi et al. (2013)), CTWC (Getz et al. (2000a)), NBVD (Long et al. (2005)). Les résultats expérimentaux sont présentés dans les tableaux 3 et 4. Nous avons choisi la taille des cartes FBR-BiTM et BiTM selon l\u0027heuristique de Kohonen. Le nombre de clusters des variables (colonnes de la matrice A) est exactement le même pour l\u0027ensemble des approches FBR-BiTM, BiTM, CTWC, NBVD. Cependant, pour le nombre de clusters des observations (lignes de la matrice A), nous avons pris la même taille des cartes \nComparaison des performances de FBR-BiTM avec les approches de bi-partitionnement\nLe tableau 3 résume les résultats expérimentaux de l\u0027indice de pureté. Nous remarquons que FBR-BiTM fournit des résultats équivalents à ceux de l\u0027approche BiTM. Dans la plupart des cas, nous constatons des résultats comparable et souvent meilleur avec notre approche BiTM ou FBR-BiTM. Nous observons aussi la difficulté d\u0027obtenir de grandes valeurs de l\u0027indice de pureté pour la base isolet5. Le tableau 4 présente l\u0027indice de rand. Nous observons que FBR-BiTM fournit un indice de rand meilleur que celui des autres approches dans 4 bases sur 9. Les résultats de FBR-BiTM restent compétitifs et équivalents aux résultats obtenus avec les autres approches. Nous constatons après cette étude comparative, que notre approche FBRBiTM est une méthode qui ne perturbe pas le bi-partitionnement topologiques BiTM. \nCas particulier : application aux bases de données simulées binaires\nDans les bases de données réelles, il est très difficile d\u0027obtenir les étiquettes des classes des variables. Afin de valider le clustering des variables de notre modèle FBR-BiTM, nous avons utilisé des bases de données simulées étiquetées en lignes (observations) et en colonnes (variables) décrites dans le tableau 2. Les tableaux 5 et 6 montrent les résultats obtenus avec les indices de pureté et de rand pour le partitionnement des observations et des variables.\nNous constatons à travers les 2 indices de performance que notre approche est meilleure ou équivalente à BiTM dans le cas du clustering des observations dans la plupart des bases de données. Cependant, nous remarquons une légère baisse des performances de FBR-BiTM au niveau du clustering des variables. en fonction des groupes de lignes et de colonnes. Cette organisation est très claire dans le cas des bases binaires (voir la figure 1(a)).\nRésultats visuels\nCes figures peuvent être obtenues par toutes les méthodes de bi-partitionnement. Cependant, en utilisant cette visualisation, il est difficile d\u0027analyser les blocs ou les bi-clusters obtenus. Afin de faciliter cette tâche, nous proposons de visualiser les bi-clusters en utilisant l\u0027organisation topologique du modèle FBR-BiTM. Ainsi, chaque cellule de la carte est associée au cluster des observations et des variables. Cette organisation est illustrée par les figures 1(b) et 2(c) en organisant les cellules selon l\u0027ordre des blocs de variables obtenus. Dans le cas de la base Lung Cancer par exemple, la figure 2(b) représente la carte topologique associée au modèle FBR-BiTM. Cette figure représente la topologie des groupes obtenus en appliquant l\u0027algorithme FBR-BiTM. Nous remarquons une répartition des données au niveau de chaque cellule. Plus la couleur est rouge, plus les variables ont de fortes valeurs. Nous avons organisé la carte selon les blocs de variables obtenus. Le résultat est illustré dans la figure 2(c). Dans la première cellule par exemple, nous remarquons que les variables ont changé de disposition de manière à créer une organisation au niveau de la cellule. Nous constatons clairement dans cette première cellule (en haut à gauche de la carte) que les blocs de variables se comportent différemment à l\u0027intérieur de cette cellule. Ce comportement est illustré par une couleur. Plus la couleur est rouge, plus le bloc de variable tend vers de fortes valeurs. Dans ce cas, nous remarquons que les premiers blocs de variables (totalement à gauche de la cellule) ont une couleur plutôt rouge. Par contre, le second bloc (au milieu de la cellule) est moins \"important\" car il est constitué de variables d\u0027une couleur bleu. Enfin, le troisième bloc (totalement à droite) est constitué des variables moyennement importantes (couleur verte). Nous nous sommes focalisés dans cette analyse sur la base Lung Cancer. En fait, cette analyse peut être également réalisée sur les autres bases de données.\nDistribution des blocs de variables : la figure 2(  \n"
  },
  {
    "id": "354",
    "text": "Introduction\nDans cet article, nous proposons une solution originale à un problème classique de bases de données qui consiste à prédire/estimer les valeurs manquantes dans une base de données relationnelle incomplète. De nombreuses approches ont été proposées pour traiter cette question, à la fois dans la communauté des bases de données et dans celle de l\u0027apprentissage automatique, fondées sur des dépendances fonctionnelles (Atzeni et Morfuni (1986)), des règles d\u0027association (Ragel (1998)), des règles de classification (Liu et al. (1997)), des techniques de clustering (Fujikawa et Ho (2002)), etc. Nous explorons quant à nous une nouvelle idée, issue de l\u0027intelligence artificielle, qui consiste à exploiter les proportions analogiques (Prade et Richard (2012)) pouvant exister dans les données.\nLa suite de l\u0027article est organisée comme suit. Dans la section 2, nous rappelons les notions de base concernant les proportions analogiques. La section 3 présente le principe général de l\u0027approche que nous proposons pour estimer les valeurs manquantes, inspirée par la technique de classification proposée dans (Bayoudh et al. (2007); Miclet et al. (2008)). La section 4 est consacrée à une expérimentation visant à évaluer les performances de la méthode et à comparer cette dernière avec une technique classique d\u0027estimation (kNN). Finalement, la section 5 rappelle les contributions principales de l\u0027article et trace quelques perspectives de recherche.\nRappels sur les proportions analogiques\nLa présentation qui suit est tirée principalement de Miclet et Prade (2009). Une proportion analogique est une proposition de la forme « A est à B ce que C est à D », ce qui sera noté : Comme noté dans (Prade et Richard (2013)), l\u0027idée de proportion est fortement liée à celle d\u0027extrapolation, autrement dit à l\u0027objectif de deviner/calculer une nouvelle valeur à partir de valeurs existantes, ce qui est bien le but que nous nous fixons ici.\n3 Principe de l\u0027approche 3.1 Idée générale L\u0027approche que nous proposons s\u0027inspire d\u0027une méthode de « classification par analogie » introduite dans (Bayoudh et al. (2007)), où les auteurs décrivent un algorithme appelé FA-DANA. Ce dernier utilise une mesure de dissimilarité analogique entre quatre objets, qui estime dans quelle mesure ces objets sont loin d\u0027être en proportion analogique. En deux mots, la dissimilarité analogique ad entre quatre valeurs booléennes est le nombre minimal de bits qui doivent être modifiés pour obtenir une analogie valide. Par exemple ad(1, 0, 1, 0) \u003d 0, ad(1, 0, 1, 1) \u003d 1 et ad(1, 0, 0, 1) \u003d 2. Ainsi, en désignant par A la relation quaternaire de proportion analogique, on a :\nLorsque, au lieu d\u0027avoir quatre valeur booléennes, on manipule quatre vecteurs booléens dans B n , il faut ajouter les évaluations ad obtenues pour chaque composante de façon à obtenir la dissimilarité analogique entre les vecteurs, ce qui conduit à un entier dans l\u0027intervalle [0, 2n]. L\u0027algorithme, qui prend en entrée un ensemble d\u0027apprentissage S d\u0027éléments déjà classifiés, un nouvel élément d à classifier, et un entier k, procède comme suit :\n2. Tri de ces n triplets par valeur croissante de leur ad. 3. Si le k e triplet a la valeur entière p pour ad, alors notons q le plus grand entier tel que le q e triplet a la valeur p. 4. Résolution des q équations analogiques sur l\u0027étiquette de la classe. On retient le vainqueur des q votes, que l\u0027on affecte comme étant la classe de d.\nApplication à la prédiction de valeurs inconnues 3.2.1 Cas des attributs booléens\nCette méthode peut être adaptée au cas de la prédiction de valeurs nulles dans une base de données transactionnelles de la façon suivante. Soit une relation r de schéma (A 1 , . . . , A m ) et t un n-uplet de r comportant une valeur manquante pour l\u0027attribut A i : t[A i ] \u003d NULL. Pour prédire la valeur de t[A i ] qui est 0 ou 1 dans le cas d\u0027une base de données transactionnelle, on applique l\u0027algorithme précédent en considérant que A i correspond à la classe cl à déterminer. L\u0027ensemble S d\u0027apprentissage correspond à un échantillon (d\u0027une taille fixée à l\u0027avance) des n-uplets de la relation r (privée de l\u0027attribut A i qui n\u0027intervient pas dans le calcul de ad mais représente la « classe ») ne comportant aucune valeur manquante. Par ailleurs, on ignore les attributs A h , h \u003d i tels que t[A h ] \u003d NULL lors du calcul visant à prédire la valeur de t[A i ].\nCas des attributs numériques\nDans le cas d\u0027attributs numériques, une première solution consiste à se ramener au cas booléen : un attribut numérique A est dérivé en autant d\u0027attributs booléens qu\u0027il y a de valeurs dans le domaine actif de A. Cette solution peut cependant paraître discutable car la binarisation conduit à se limiter à des cas d\u0027analogie assez limités (égalité ou non égalité).\nUne deuxième solution consiste à rechercher des analogies de type\nla première étant appelée proportion géométrique et la seconde proportion arithmétique. Ces définitions peuvent être raffinées en introduisant une certaine tolérance sur les relations d\u0027analogie, de façon à couvrir plus de cas. Ainsi on peut considérer que (100 : 50 :: 80 : 39) est presque vrai (on a 39 au lieu de 40).\nUne façon de procéder est d\u0027adopter une vision graduelle de la dissimilarité analogique, et de considérer que la valeur de AD n\u0027est plus un entier mais un réel. Dans le cas d\u0027une proportion géométrique (formule (1)), pour l\u0027attribut A i , on peut ainsi ajouter à AD la valeur\nDans cette formule, la multiplication par 2 a pour but de rendre la pénalité commensurable avec celle appliquée dans le cas booléen (où la dissimilarité analogique peut prendre l\u0027une des valeurs 0, 1 ou 2). Notons que si max(ad, bc) \u003d 0, cette formule est inapplicable et l\u0027on doit alors se limiter à rechercher une proportion arithmétique (formule (2)).\nDans le cas d\u0027une proportion arithmétique, une solution est de définir, pour chaque attribut numérique, une fonction d\u0027appartenance associée au terme flou « proche de ». Pour un attribut donné A i , on peut définir une fonction trapézoïdale µ Ai de telle sorte que :\nLa pénalité à appliquer se définit alors par :\nExpérimentation préliminaire\nLe principal objectif de l\u0027expérimentation qui a été menée est de comparer les résultats obtenus à l\u0027aide de cette technique avec ceux produits par l\u0027approche classique des plus proches voisins, donc d\u0027estimer son efficacité relative en termes de précision (i.e., de pourcentage de valeurs correctement prédites). Nous détaillons ici uniquement des résultats expérimentaux portant sur des bases de données transactionnelles, l\u0027extension au cas d\u0027attributs numériques étant actuellement en cours.\nUn ensemble de données de plus de 50 000 n-uplets contenant 20 attributs sur des accidents de la route est utilisé (Geurts et al. (2003)). Un échantillon E est extrait en choisissant 1000 n-uplets de la relation au hasard. Un sous-ensemble M de E est soumis à des modifications, autrement dit, un certain pourcentage de valeurs de chacun de ses n-uplets est remplacé par NULL. Ensuite, l\u0027algorithme FADANA est exécuté pour prédire les valeurs manquantes : pour chaque n-uplet d possédant des valeurs manquantes, un échantillon aléatoire D d\u0027une taille fixée de l\u0027ensemble E ? M (donc complètes) est choisi. À chaque fois, la méthode des k plus proches voisins (kNN) a été aussi utilisée, en prenant le même nombre de n-uplets et la même valeur de k. Rappelons que la méthode kNN est fondée sur un calcul de distance entre le tuple à compléter et les tuples de l\u0027ensemble d\u0027apprentissage (on ne retient que les k plus proches, et une procédure de vote, analogue à celle de FADANA, permet de prédire la valeur manquante).\nOn a cherché également à évaluer la proportion  83,5 84,16 84,33 83,16 84,16 83,83 84,16 kNN 82,83 84,16 83,83 83,66 84 83,33 83,33 FADANA \u0026 kNN 74,66 76,66 77,33 76 77 75,83 76,16 5,66 6,33 5,5 6,33 5,83 5,83 5 6,66 5,33 5,66 5,33 9,33 10,33 10 8,33 9,83 9,66 Le tableau 1 montre comment varie la précision en fonction de la valeur de k utilisée dans l\u0027algorithme. On constate une remarquable stabilité, aussi bien pour FADANA que pour kNN, et l\u0027on peut voir que même avec une valeur de k assez petite, le vote conduit à des résultats corrects dans la grande majorité des cas.\nTAB. 2 -Évolution de la précision en fonction de la taille de l\u0027ensemble d\u0027apprentissage (proportion de tuples modifiés : 70%, k \u003d 40, ratio de valeurs modifiées par tuple : 40%) Le tableau 2, quant à lui, montre l\u0027impact que la taille de l\u0027ensemble d\u0027apprentissage a sur la précision. On constate, ce qui n\u0027est guère surprenant, qu\u0027un trop petit ensemble affecte négativement les performances, mais qu\u0027à partir de 30 ou 40 tuples, on atteint pour FADANA un niveau de précision quasiment optimal (autour de 85%). Notons qu\u0027il est illusoire d\u0027espérer atteindre 100% puisqu\u0027il existe en général des tuples qui ne sont en relation de proportion analogique avec aucun triplet de la relation initiale.\nUn résultat intéressant est qu\u0027il existe un pourcentage non négligeable (autour de 6%) de valeurs qui sont prédites correctement par FADANA mais pas par kNN, et réciproquement. Ceci permet d\u0027envisager d\u0027utiliser une méthode hybride, qui utiliserait FADANA dans la plupart des cas, mais basculerait vers kNN pour prédire les valeurs dont on peut prévoir qu\u0027elles seront incorrectement estimées par FADANA. L\u0027intuition qu\u0027on peut avoir est que ces dernières se caractérisent par des valeurs élevées de dissemblance analogique ad dans la liste construite à l\u0027étape 2 de l\u0027algorithme (voir sous-section 3.1) mais ceci reste à confirmer expérimentalement.\nConclusion\nDans cet article, nous avons présenté une méthode originale de prédiction de valeurs manquantes dans les bases de données relationnelles, fondée sur la notion de proportion analogique. Nous avons également montré comment un algorithme proposé dans le cadre de la classification automatique pouvait être adapté à cette fin. Les résultats obtenus, quoique préliminaires, apparaissent encourageants, puisque l\u0027approche conduit à une précision meilleure en moyenne que celle de la technique classique des plus proches voisins.\nIl conviendra notamment, dans des travaux futurs, de i) comparer l\u0027approche de prédiction par analogie, au delà de kNN, avec d\u0027autres approches de la littérature ; ii) traiter de façon plus raffinée les attributs catégoriels en prenant en compte des notions telles que synonymie, hyponymie/hypernymie, etc. iii) étudier la façon dont on doit traiter les valeurs prédites lors du processus d\u0027interrogation de la base de données. Cela nécessitera certainement d\u0027utiliser un\n"
  },
  {
    "id": "355",
    "text": "Introduction\nLes forums de santé en ligne sont des espaces d\u0027échanges où les patients, sous couvert d\u0027anonymat, relatent très librement leurs expériences personnelles. Ces ressources s\u0027avèrent très riches pour les professionnels de santé qui ont accès à des échanges entre patients, entre patients et professionnels et même entre professionnels. Même si tous les patients ne s\u0027expriment pas dans les forums de santé, ces derniers représentent une base volumineuse et variée des connaissances et des perceptions qu\u0027ont les patients de leur maladie et des soins qui leur sont éventuellement prodigués. Dans le cadre du projet Parlons de nous 1 , nous cherchons à associer différents marqueurs (émotions, risques, incertitudes, etc.) à des objets médicaux (mé-dicaments, traitements, etc.) pour identifier des cooccurrences fréquentes (e.g. une association de type Médiator et peur). Dans cet article, nous nous focalisons sur l\u0027identification des émo-tions. Si de nombreuses approches ont été proposées pour l\u0027analyse de la polarité des textes (positif et négatif ), on trouve peu d\u0027approches pour l\u0027analyse des sentiments (joie, colère, tristesse, etc.). Nous avons utilisé le lexique des mots d\u0027émotions de (Mohammad et Turney, 2010) pour annoter automatiquement un corpus de messages. Une sous partie de ce corpus a été annotée manuellement. L\u0027étude de l\u0027accord entre ces annotateurs nous a permis de montrer qu\u0027il était difficile, même pour des humains, d\u0027associer une émotion précise à un message. Nous avons donc décidé de donner deux informations aux professionnels de santé : la polarité du texte (positive ou négative) et les émotions associées. Pour obtenir ces deux informations, nous avons travaillé sur la recherche des meilleurs descripteurs. Des expérimentations sur des jeux de données réelles ont montré l\u0027efficacité de cette approche et des discussions avec les professionnels de santé ont montré l\u0027intérêt médical d\u0027identifier de telles informations.\nÉtat de l\u0027art\nDepuis le début des années 2000, l\u0027analyse de sentiments, également appelée fouille d\u0027opinions (opinion mining), a connu un intérêt croissant. Beaucoup de communautés se sont inté-ressées à ce domaine et ont donné des définitions et interprétations variées (e.g. psychologie, sciences sociales, linguistique computationnelle, traitement automatique du langage, fouille de données, etc.). L\u0027analyse de sentiments vise l\u0027extraction des états affectifs exprimés explicitement ou implicitement dans des textes (Liu, 2012). Elle englobe les tâches suivantes : 1) l\u0027analyse de subjectivité porte sur la détection de la présence de sentiments via l\u0027identification d\u0027expressions ou des mots dit subjectifs ; 2) l\u0027analyse de polarité porte sur la détection de la polarité positive, négative ou neutre des textes ; 3) l\u0027analyse des émotions porte sur la catégorie émotionnelle du texte (e.g. colère, dégoût, peur, etc.) ; 4) l\u0027analyse d\u0027intensité porte sur les différents niveaux d\u0027intensité de la polarité et de l\u0027émotion (e.g. très positif, très triste, etc.). Ces approches offrent une granularité plus précise sur les opinions et les émotions exprimées. Dans ce travail, nous nous focalisons sur la troisième tâche. Comme la plupart des méthodes semiautomatiques de la littérature, nous utiliserons la typologie des émotions de (Ekman, 1992) qui décrit six émotions.\nLes méthodes appliquées pour analyser les sentiments sont très nombreuses et générale-ment, spécifiques aux types des textes : aux tweets (Roberts et al., 2012), aux titres de presse (Strapparava et Mihalcea, 2008), etc. et aux domaines d\u0027application : l\u0027analyse de media sociaux (Balahur, 2013) ou l\u0027identification de mails suicidaires (Pestian et al., 2012).\nQuelle que soit la tâche d\u0027analyse de sentiments étudiée (polarité et émotions), la plupart des travaux porte soit sur la création de ressources permettant de décrire les sentiments, soit sur l\u0027utilisation de ces ressources pour classifier des textes selon les sentiments étudiés. Dans la première catégorie de travaux, la plupart des méthodes associe les mots des textes à des termes appartenant à des ressources préalablement annotées par des sentiments. La plupart des ressources ont été construites pour l\u0027anglais et l\u0027analyse de la polarité (e.g. General Inquirer (Stone et al., 1966)). Toutefois des ressources plus spécifiques, comme le lexique de (Mohammad et Turney, 2010) ont été créées pour les mots chargés d\u0027émotions. Pour la classification proprement dite, la plupart des approches utilisent des techniques d\u0027apprentissage basées sur des attributs spécifiques incluant les mots d\u0027émotions (Strapparava et Mihalcea, 2008) pour construire un modèle statistique à partir d\u0027un corpus de textes et l\u0027utiliser pour la détection des sentiments dans d\u0027autres textes. Si beaucoup de ces méthodes s\u0027avèrent efficaces sur des corpus de textes importants, elles se retrouvent limitées dans le cas des textes courts comme les tweets ou spécifiques comme les forums de santé. sur des émotions portant sur des objets médicaux, nous avons utilisé le MESH 2 pour repé-rer des entités médicales et nous avons filtré 6% des messages n\u0027en contenant pas. Dans un message, plusieurs émotions sont généralement exprimées du fait de sa longueur. Nous avons donc choisi de segmenter les messages en phrases et gardé 3 000 phrases pour constituer un Corpus Annoté Automatiquement (CAA). Toutes les phrases contenant plusieurs émotions ont été étiquetées par l\u0027émotion majoritaire. Un sous-ensemble de ce corpus (600 phrases) a été annoté manuellement par 60 non professionnels de santé 3 . Nous le notons CAM (Corpus Annoté Manuellement).\nPour évaluer l\u0027accord entre les annotateurs, nous avons utilisé la mesure Kappa. 150 phrases issues du CAM ont été annotées par deux annotateurs non professionnels (Kappa égal à 0, 26 soit accord très faible) et par un annotateur professionnel de santé et un non professionnel (Kappa de 0, 46 soit accord modéré). Cette expérimentation préliminaire souligne la difficulté de la tâche d\u0027annotation manuelle. Par ailleurs, le désaccord entre annotateurs est essentiellement dû à la variabilité entre personnes et non à leur sensibilité au domaine de la santé. Un premier biais consiste à ne considérer que le point de vue de l\u0027annotateur qui est parfois très différent de celui de l\u0027auteur. En effet, les messages des forums de santé traitent de la maladie, des traitements, etc. Ces informations sont par nature négatives et l\u0027annotateur aura, par empathie, tendance à associer une émotion telle que la tristesse à une information factuelle comme la description d\u0027un diagnostique. Un deuxième biais réside dans le fait que le corpus est rédigé en langue anglaise, alors que les annotateurs sont des français natifs. Par ailleurs, en étudiant les phrases ayant suscité des écarts d\u0027annotations, nous avons remarqué qu\u0027il est plus facile d\u0027identifier la polarité que de trouver l\u0027émotion elle même. Il est également plus facile de prédire les émotions positives car les émotions négatives partagent un vocabulaire très proche. Nous avons noté également que la surprise est la plus difficile à identifier. La qualité de notre corpus annoté est finalement assez discutable mais cette étude donne une bonne intuition des difficultés liées à l\u0027obtention d\u0027un corpus de qualité et de la méthodologie à suivre pour améliorer sa qualité. Dans la suite, à partir de ces différents constats, nous avons décidé d\u0027évaluer différentes méthodes permettant de caractériser un texte extrait d\u0027un forum en nous basant sur : 1) une classification bi-classes pour identifier la polarité des émotions (les émotions positives correspondant à la joie, les émotions négatives à la colère, la peur, la tristesse et le dégoût). L\u0027émotion surprise a été éliminée du fait de sa neutralité. 2) une classification multi-classes pour les 6 émotions : une phrase ne peut être associée qu\u0027à une seule classe d\u0027émotion ; 3) une classification multi-labels nous permettant d\u0027associer une phrase à plusieurs classes d\u0027émotions.\nProtocole expérimental\nPrétraitements : les messages dans les forums contiennent des mots qui ne se retrouvent pas forcément dans les dictionnaires classiques (argot, mise en forme particulière, abrévia-tions, émoticons, etc.). Il est donc nécessaire de les normaliser en généralisant leur contenu. Pour cela, nous avons appliqué les prétraitements correspondant à la chaîne mise en place par (Balahur, 2013)  Classification : nous avons utilisé les attributs ci-dessous afin de trouver les meilleurs descripteurs des émotions : 1) les attributs basés sur les N-Grammes (U,U+B) ; 2) les mots d\u0027émotions (ME) : si une phrase contient deux mots correspondant à l\u0027émotion joie, elle prendra la valeur 2 pour cet attribut ; 3) les smileys (SMI) : l\u0027ensemble des émoticons ( :-), :-( ...) a été classé selon les six émotions. Si une phrase contient un smiley correspond à la joie, elle prendra la valeur 1 pour cet attribut ; 4) les intensifieurs (INT) : ponctuations ( ! !, ? ?, etc.), lettres répétées (looool) et mots en majuscules (HATE). Si une phrase contient un intensifieur, elle prendra la valeur 1 pour cet attribut ; 5) le contexte de l\u0027émotion (CONT) : nous utilisons deux attributs que nous appelons émotion voisine et globale. Une phrase prend la valeur vraie pour cet attribut si une phrase qui l\u0027entoure exprime la même émotion et pour l\u0027émotion globale on considère tout le message ; 6) les patrons : en nous inspirant de l\u0027approche de (Béchet et al., 2013), nous enrichissons les attributs en utilisant des patrons obtenus en appliquant un algorithme de recherche de motifs séquentiels. Pour cela, nous utilisons le thésaurus médical MeSH pour identifier des mots d\u0027émotion, le lexique des mots d\u0027émotions pour identifier des traces d\u0027émotions et un lemmatiseur pour obtenir la catégorie grammaticale des mots. Chaque phrase est alors considérée comme une séquence d\u0027itemsets correspondant à une combinaison de ces trois informations. Nous avons ensuite utilisé l\u0027algorithme GSP (Zhang et al., 2002) pour obtenir les sous séquences fréquentes. Nous n\u0027avons conservé que celles contenant au moins une entité médicale et un mot d\u0027émotion. Ces motifs ont ensuite été utilisés comme attributs. Une phrase est étiquetée vraie pour un patron si sa forme syntaxique respecte le patron.\nÉvaluation : la qualité de la classification bi-classes est évaluée en utilisant les mesures classiques de précision P , rappel R et la F-measure F . Pour la classification multi-classes, nous calculons à la fois la moyenne au niveau micro F mi et macro F ma . Pour la classification multi-labels, on utilise alors d\u0027autres mesures comme le Hamming loss HL, l\u0027exactitude A et la F-mesure au niveau macro F ma .\nMise en oeuvre : nous utilisons les implémentations de Weka 4 pour la classification biclasses et multi-classes et Meka 5 pour la classification multi-labels. Nous utilisons SVM comme classifieur avec sa mise en oeuvre SMO dans Weka en utilisant les paramètres par défaut. Nous utilisons le classifier chain CC mis en oeuvre dans Meka pour la classification multi-labels. Nous avons utilisé deux jeux de données : le corpus CAM et le corpus CAA. Nous réalisons une validation croisée (à 10 folds).\nRésultats et discussions\nLa table 1 présente les résultats obtenus sur le corpus CAM. Nous ne présentons pas les résultats obtenus sur le corpus CAA qui sont similaires. La classification bi-classes donne les meilleurs résultats, ce qui semble relativement cohérent car cette tâche s\u0027est également avérée plus facile pour les annotateurs humains. La classification multi-labels présente de meilleurs résultats que la classification multi-classes moins efficace lorsqu\u0027un exemple peut être associé à plusieurs classes. Par ailleurs, nous constatons le peu de différence de la F-mesure micro et macro pour la classification multi-classes, qui semble suggérer que toutes les classes sont aussi difficiles à identifier. Ces résultats sont à mettre en parallèle avec l\u0027accord interannotateurs (voir section 3). Dans les deux cas, la tâche est difficile mais les méthodes semi-automatiques semblent détecter des régularités plus systématiquement sauf cas particuliers comme par exemple l\u0027ironie. On peut également conclure que les meilleurs descripteurs sont les unigrammes combinés aux bigrammes et aux mots d\u0027émotions (U+B+ME). La prise en compte des smileys et des intensifieurs n\u0027améliore pas la classification. En effet, ils sont souvent utilisés à des fins d\u0027ironie que nous ne captons pas en regardant uniquement leur présence dans la phrase. De même, le contexte n\u0027est pas un attribut intéressant. En effet, un message est souvent long (7 phrases en moyenne dans notre corpus) et contient de nombreux sentiments (jusqu\u0027à 6 émotions dans 41% des messages). Deux phrases consécutives contiennent donc très souvent des émotions différentes non corrélées. Pour finir, les patrons se sont également avérés peu efficaces car non définis pour chaque classe. Il est important de noter que, quand les différents classifieurs se trompent, ils placent souvent l\u0027exemple dans une classe \"proche\" appartenant à la même polarité. Ces mauvaises prédictions sont dues au fait que les classes partagent de nombreux mots (comme colère, dégoût et tristesse). Les ressources utilisées étant des dictionnaires et un lemmatiseur, la méthode pourra être reproduite pour d\u0027autres langues en utilisant des ressources similaires.\nBi-classes\nMulti \nConclusions et perspectives\nNous avons décrit dans cet article une méthode d\u0027analyse d\u0027émotions dans les messages des forums de santé. La principale difficulté a résidé dans l\u0027acquisition de données annotées et cette étape devra encore être améliorée. Pour l\u0027extraction des émotions, nous avons comparé différents attributs, pour différentes tâches de classification (bi-classes, multi-classes et multilabels) et montré que les plus efficaces étaient les unigrammes combinés aux bigrammes et aux mots d\u0027émotions pour la classification bi-classes. Toutefois, suggérer une étiquette, malgré la précision obtenue, semble pertinent aux professionnels de santé impliqués dans cette étude. Les perspectives associées à ce travail sont nombreuses. Du point de vue de la tâche d\u0027analyse des émotions, nous allons appliquer notre méthode sur des jeux de données plus importants et non spécifiques à la santé, comme ceux du challenge SEMEVAL 6 . Nous prendrons également en compte les \"inverseurs de sens\" (shifters). Nous validerons la généricité de notre approche sur un corpus en français. En effet, notre méthode repose uniquement sur des lexiques et un outil de lemmatisation. Nous avons également identifié des perspectives liées au domaine d\u0027application. Le forum spine-health est spécialisé dans la thématique \"douleur\" et la pathologie 6. http://www.cse.unt.edu/ rada/affectivetext/\n"
  },
  {
    "id": "356",
    "text": "Introduction\nAvec l\u0027arrivée du Web 2.0, on assiste à un foisonnement de services de réseautage social, qui mettent l\u0027utilisateur, en tant que créateur de contenu, au centre des préoccupations. Ces services permettent de partager des ressources comme des vidéos (YouTube), des photos (Flickr) ou des ressources annotées (Del.icio.us), d\u0027échanger des informations et de construire des relations personnelles ou professionnelles (Facebook, LinkedIn) ou encore de diffuser des news (Twitter, blogs). Les utilisateurs disposent ainsi de plusieurs espaces d\u0027informations sur différents réseaux sociaux, où ils partagent des informations personnelles telles que leurs noms et prénoms, leur lieu géographique, leur âge, leurs adresses électroniques, leurs numéros de téléphone, leurs relations, les institutions fréquentées, etc. (Gross et Acquisti (2005); Little et al. (2011);Stutzman (2006)). Selon la politique d\u0027accès aux données définie par le réseau social, certaines informations sont par construction publiques ou semi-publiques (accessibles aux autres utilisateurs du réseaux social), pour d\u0027autres c\u0027est l\u0027utilisateur qui choisit de restreindre l\u0027accès à ses contacts ou d\u0027ouvrir l\u0027accès à tous.\nAu vu de la diversité des réseaux sociaux, ces espaces permettent l\u0027accès à une grande quantité d\u0027informations en puisant dans des ressources très variées, complémentaires et parfois insoupçonnées, qui pourraient surprendre l\u0027utilisateur lui-même s\u0027il en connaissait réellement toute l\u0027étendue. Réconcilier les différentes profils d\u0027un utilisateur à travers ses divers réseaux sociaux lui permet de construire un espace d\u0027informations global afin de mieux gérer et protéger ses données. Le principal problème réside dans le fait que ces informations sont souvent incomplètes, ambiguës, non mises à jour voire fausses. Il existe déjà, sur le Web, des outils agrégateurs comme FriendFeed Dans cet article, nous proposons une approche de réconciliation de profils des utilisateurs à travers ses différents réseaux sociaux en exploitant la topologie de l\u0027ensemble des réseaux sociaux interconnectés et les informations publiques fournies par les utilisateurs dans leurs profils. Notre approche repose sur l\u0027observation que, d\u0027une part, les réseaux sociaux sont interconnectés grâce aux liens transversaux que certains utilisateurs déclarent sur leurs différents profils (Golbeck et Rothstein (2008)  Carmagnola et Cena (2009) ont introduit la notion de facteur d\u0027importance afin de pondérer la contribution de chaque attribut pour déterminer la similarité de deux profils. Dans notre approche, les attributs sont considérés de la même manière dans les règles. Néanmoins, il y a un ordre d\u0027exécution des règles, de la plus contraignante à la moins contraignante. De plus, notre approche découvre de nouveaux profils par propagation de ceux découverts et réconciliés à l\u0027itération précédente. L\u0027évaluation est basée sur des données réelles provenant de quatre réseaux sociaux, tandis que leur expérience est limitée à de petits systèmes fermés. La différence est importante. En effet, dans les réseaux sociaux ouverts, certains utilisateurs ne renseignent pas leur identité réelle, contrairement aux réseaux fermés, où ils pensent que leur identité n\u0027est pas menacée. Par conséquent, les données que nous considérons sont susceptibles d\u0027être fortement bruitées ou erronées ; ce qui constitue un défi non négligeable dans notre contexte. Cortis et al. (2012); Raad et al. (2010) ont proposé le calcul d\u0027une similarité sémantique entre les attributs des profils ; bien que ces approches soient originales, elles ne sont évaluées que sur des petits ensembles de données (par ex., 50 profils (Raad et al. (2010))). Bartunov et al. (2012); Buccafurri et al. (2012); Jain et al. (2013); Narayanan et Shmatikov (2009) ont étudié les propriétés topologiques des réseaux. Buccafurri et al. (2012) adoptent un raisonnement récursif et considèrent que deux profils sont similaires et donc susceptibles de référencer un même utilisateur, s\u0027ils ont des pseudonymes similaires et les utilisateurs auxquels ils sont liés sont similaires. Cette approche a deux inconvénients que notre approche élimine. Le premier est que les profils ayant des pseudonymes différents sont ignorés, même s\u0027ils pourraient identifier un même individu. Le second est que les profils réconciliés découverts ne sont pas propagés pour découvrir des nouveaux. Bartunov et al. (2012) proposent une approche combinant les attributs d\u0027un profil et la topologie du réseau en utilisant les champs aléatoires conditionnels. Cette approche est robuste car elle peut être aussi utilisée quand les valeurs des attributs ne sont pas disponibles, ce qui est le cas des réseaux anonymisés. Cependant, comme elle s\u0027appuie sur un modèle probabiliste, les valeurs des paramètres nécessitent l\u0027utilisation de techniques d\u0027apprentissage supervisé et de disposer d\u0027une base d\u0027exemples d\u0027apprentissage.\nNotations et Formalisation du Problème\nDans notre approche, nous représentons un ensemble de réseaux sociaux interconnectés par un graphe orienté étiqueté, où les noeuds représentent les profils des utilisateurs, et les arcs représentent les liens existant entre eux. Chaque profil possède une uri identifiant sa page sur le Web et un ensemble d\u0027attributs décrits sur cette page. Chaque arc possède une étiquette décrivant le type de lien.\n4\nNous considérons deux types de liens, les liens d\u0027amitié entre les utilisateurs d\u0027un même réseau social et les liens transversaux reliant deux profils d\u0027un même utilisateur appartenant à deux réseaux différents. Plus formellement, un ensemble de n réseaux sociaux est un graphe défini comme suit :\nA représente l\u0027ensemble des attributs définis dans un profil, P a (v) représente la(les) valeur(s) associée(s) à l\u0027attribut a ? A dans le profil v. \nLe problème de réconciliation des profils d\u0027un même utilisateur à travers différents réseaux sociaux peut être réduit au problème de détermination des liens transversaux me manquants noté mirror et formalisé comme suit :\nNotre approche de réconciliation des profils\nUne solution intuitive au problème posé consiste à comparer chaque paire de profils (v i , v j ), non reliée par le lien me, pour chaque paire de réseaux (i, j), \nSélection des paires de profils candidats\nA partir de l\u0027analyse des différents profils d\u0027un même utilisateur sur ses réseaux sociaux, nous avons constaté qu\u0027un utilisateur a souvent tendance à déclarer en tant qu\u0027ami au moins un même utilisateur dans ses différents profils. De plus, deux utilisateurs ayant différents profils déclarent qu\u0027ils sont amis sur plusieurs d\u0027entre eux (Golbeck et Rothstein (2008)). Notre hypothèse de base de sélection des paires de profils candidats repose sur ce constat. Par ailleurs, dans un réseau social, il existe des utilisateurs qui déclarent explicitement les liens entre leurs différents profils, ce qui permet de déterminer les amis communs déclarés dans deux profils appartenant à deux réseaux différents.\nEn\nL\u0027ensemble des paires de profils candidats, pour les réseaux sociaux i et j, est formellement défini comme suit :\nDétermination des paires de profils mirror\nUne fois l\u0027ensemble des paires de profils candidats, S, construit, il s\u0027agit de déterminer parmi ces candidats les profils qui référencent un même utilisateur en exploitant les valeurs des attributs définis dans ces profils. Dans ce qui suit, nous présentons les attributs considérés par notre approche. Nous détaillons, ensuite, les règles permettant de déterminer les paires de profils mirror ainsi que l\u0027algorithme de réconciliation que nous avons définis.\nLes attributs\nDans la majorité des réseaux sociaux, certains attributs sont rendus publics soit par défaut soit c\u0027est l\u0027utilisateur qui choisit de les publier. Krishnamurthy et Wills (2009) ont identifié un ensemble d\u0027attributs souvent publics dans les 12 plus répandus réseaux sociaux. Notre approche de réconciliation vise à exploiter un grand nombre de réseaux sociaux, nous avons donc besoin d\u0027isoler un ensemble d\u0027attributs accessibles, publics et permettant de déterminer si deux profils référencent le même utilisateur. Dans cet article, nous nous focalisons sur les attributs suivants : le pseudonyme, les noms et prénoms, les adresses électroniques et les liens vers des pages Web. L\u0027attribut pseudonyme, noté p, est un attribut public dont la valeur est toujours accessible. C\u0027est l\u0027identifiant du profil sur un réseau social donné. Il fait généralement partie de l\u0027uri de la page du profil sur le Web. Des études comme dans (Buccafurri et al. (2012); Perito et al. (2011);Zafarani et Liu (2009)) ont montré que les utilisateurs des réseaux sociaux ont tendance à utiliser des pseudonymes identiques ou comportant des sous-chaînes identiques sur leurs différents profils.\nPour évaluer la similarité de deux pseudonymes, nous avons choisi d\u0027utiliser la distance de Levenshtein. En effet, cette distance permet de capturer les variations des sous-chaînes de caractères en calculant le nombre suppressions ou d\u0027insertions de caractères d\u0027un mot p 1 pour qu\u0027il soit identique à un mot p 2 . Prenons l\u0027exemple de deux profils référençant le même utilisateur dont le pseudonyme sur la page www.flickr.com/photos/cospics est \"cospics\" et le pseudonyme sur la page www.livejournal.com/users/cos/profile est \"cos\", le nombre de suppressions est de 4. Le seuil de similarité est défini dans les expérimen-tations. L\u0027attribut nom, noté n, est un attribut dont la valeur représente les noms et/ou prénoms renseignés par l\u0027utilisateur. En effet, ces valeurs ne correspondent généralement pas à des champs distincts et bien identifiés dans un réseau social et ne sont pas renseignés avec le même niveau de détail d\u0027un réseau social à un autre. Le nom est un attribut ambigu, en particulier, lorsque les valeurs correspondent à des noms et prénoms communs. Il est également sensible dans le sens où l\u0027utilisateur préfère rester anonyme, même s\u0027il renseigne cette information, souvent il ne révèle pas son véritable nom. Il est donc clair que l\u0027utilisation seule de cet attribut est insuffisante pour réconcilier deux profils.\nPour mesurer la similarité de deux valeurs de l\u0027attribut n, nous utilisons la mesure de Jaccard qui permet de calculer le nombre de mots communs sans prendre en compte leur ordre d\u0027occurrence dans la chaîne de caractères. Par exemple, la mesure de Jaccard pour les chaînes \"Barack, Obama\" et \"Obama Barack\" est égale à 1 et à 2 3 pour les chaînes \"Barack, Hussein, Obama\" et \"Obama Barack\" . L\u0027attribut adresse électronique, noté m, est un attribut multivalué dont les valeurs correspondent aux différentes adresses électroniques d\u0027un utilisateur. Si renseignée, il s\u0027agit d\u0027un attribut qui permet d\u0027identifier de manière unique un utilisateur. Cependant, cette information n\u0027est pas souvent renseigné, et l\u0027utilisation seule de cet attribut est insuffisante pour réconcilier deux profils.\nPour comparer les valeurs de l\u0027attribut m de deux profils, il s\u0027agit de déterminer si l\u0027une des adresses électroniques d\u0027un profil est identique à l\u0027une des adresses électroniques de l\u0027autre profil. Dans le cas positif la similarité est égale 1. Sinon, elle est égale à 0.\nL\u0027attribut liens vers d\u0027autres pages Web est un attribut multivalué dont les différentes valeurs correspondent à différentes url référençant des liens vers des pages Web. Nous distinguons deux types de liens, ceux qui référencent des liens vers des profils de réseaux sociaux, noté s, de ceux qui référencent des liens vers d\u0027autres pages, noté w. Le but étant de pouvoir analyser séparément la contribution de chacun des types. En effet, les liens vers les réseaux sociaux, figurant sur le profil d\u0027un utilisateur, pourraient être des liens vers ses autres profils. Ce qui pourrait correspondre à un lien transversal. Par ailleurs, les liens vers d\u0027autres pages Web sont des liens vers des ressources que l\u0027utilisateur souhaite partager avec les autres et qui pourraient correspondre à ses pages personnelles.\nDans cet article, nous nous limitons dans cette première étude à rechercher si les valeurs de l\u0027attribut w ou s, pour deux profils, ont au moins une url commune sans analyser le contenu de ces pages, en particulier les liens vers les réseaux sociaux, qui pourraient être pertinents pour la réconciliation des profils. L\u0027attribut lieu, noté l, est un attribut dont la valeur correspond aux différents lieux renseignés par l\u0027utilisateur. Il s\u0027agit d\u0027un attribut public souvent renseigné par les utilisateurs. Néanmoins, cet attribut est souvent ambigu lorsqu\u0027on est amené à comparer les valeurs de deux profils différents, et ce pour plusieurs raisons :\n-le lieu peut être incomplet, ce qui ne permet pas de l\u0027identifier de manière unique en tant qu\u0027entité géographique. Même si les deux valeurs sont identiques, par exemple pour la valeur \"Paris\" il existe plusieurs entités géographiques avec ce même label \"Paris\" au \"Texas\" ou \"Paris\" en \"France\". \nLes règles\nPour déterminer si deux profils v i et v j référencent un même utilisateur, nous avons défini un ensemble de règles exploitant les attributs présentés ci-dessus à appliquer à chaque paire de S. Chaque règle prend en considération la contribution d\u0027un ou plusieurs attributs. Étant donné qu\u0027aucun attribut ne constitue l\u0027identité de l\u0027utilisateur, nous supposons que plus le nombre d\u0027attributs qui correspondent, selon la mesure de similarité définie, est grand pour deux profils plus la probabilité qu\u0027ils référencent un même utilisateur est forte. Ainsi, les règles définies sont classées par ordre de pertinence noté k. La règle la plus pertinente, d\u0027ordre maximale, est celle qui détermine que tous les attributs correspondent ; dans ce cas k \u003d |A|. La règle la moins pertinente est celle qui détermine que un seul attribut correspond ; dans ce cas k \u003d 1.\nSoit le prédicat noté match(P a (v i ), P a (v j )) qui retourne vrai si les valeurs de l\u0027attribut a pour les profils v i et v j correspondent selon la mesure de similarité définie pour l\u0027attribut a. Une règle d\u0027ordre k, noté R k est définie comme suit :\nCes liens mirror découverts sont exploités et considérés comme des liens me pour sélec-tionner de nouveaux couples de profils candidats C Ainsi, les règles sont ré-exécutées pour C et ainsi de suite, jusqu\u0027à ce qu\u0027il n y ait plus de nouvelles paires de profils mirror. Pour réaliser une première évaluation, nous avons appliqué notre approche de réconcilia-tion aux 2 réseaux sociaux Flickr et LiveJournal. Tout d\u0027abord, les paires de profils candidats sont obtenues en exploitant les liens transversaux comme expliqué dans la section 4.1. L\u0027ensemble des paires de profils mirror, noté M, est ensuite généré en appliquant les règles présentées dans la section 4.2.2.\nArcs\nLes premiers résultats que nous avons obtenus, à l\u0027issue de la 1 ère itération, pour un seuil fixé arbitrairement ? \u003d 0.7, et pour chacune des règles vérifiée, sont décrits dans la table 3. Au total 3424 couples vérifient les règles sur environ 16000 couples candidats dont 0.03% vérifie la règle d\u0027ordre 4, 0.91% vérifie les règles d\u0027ordre 3, 7.65% vérifie les règles d\u0027ordre 2 et 91.41% pour les règles d\u0027ordre 1. Comme l\u0027attribut adresse électronique n\u0027est pas renseigné, dans la collection considérée, pour au moins un profil d\u0027un couple candidat, l\u0027attribut m n\u0027intervient dans aucune règle. Ces résultats montrent qu\u0027un faible pourcentage de paires de profils vérifient les règles d\u0027ordre k ? 2, et le plus grand pourcentage vérifie la règle d\u0027ordre 1. Ce qui confirme le fait que les utilisateurs préfèrent généralement ne pas divulguer trop d\u0027informations personnelles. Les résultats montrent aussi que l\u0027attribut pseudonyme est l\u0027attribut intervenant dans les règles vérifiées par le plus grand nombre de couples de profils candidats.\nPour l\u0027évaluation des résultats, nous avons défini trois sous-ensembles de l\u0027ensemble de couples mirror M : (i) corrects référençant le même utilisateur, noté C ; (ii) faux référençant deux utilisateurs différents, noté F et (iii) indéterminés pour lesquels nous ne disposons pas d\u0027informations suffisantes pour décider. La précision est donc calculée en utilisant la formule suivante : précision\u003d |C| |M| . Il faut noter que nous avons choisi dans ce calcul de tenir compte de l\u0027ensemble des couples indéterminés pour évaluer au mieux notre approche. L\u0027évaluation est faite manuellement et ce sur la totalité des 3424 couples vérifiant les règles en utilisant les uri des pages des profils.\nLes résultats des règles d\u0027ordre 1 montrent que les attributs générant le plus grand nombre de paires de profils mirror faux sont le nom (54.55%) et ensuite le pseudonyme (30.19% Ces résultats soulignent qu\u0027une similarité forte de deux noms n\u0027est pas à elle seule un élé-ment pertinent. En effet, l\u0027attribut nom est un attribut non seulement ambigu mais aussi sensible, souvent l\u0027utilisateur fournit une information erronée de manière délibérée afin de ne pas dévoiler son identité. Par exemple, en analysant précisément les pages de profils appartenant au même utilisateur, souvent les noms sont partiellement ou complètement différents.\nPour mieux comprendre les résultats obtenus pour les attributs nom et pseudonyme, nous avons procédé à d\u0027autres tests en utilisant un seuil de similarité plus grand et nous avons constaté que la précision augmente significativement pour les règles d\u0027ordre 1 utilisant le pseudonyme, mais l\u0027impact reste non significatif pour les règles utilisant le nom. Comme expliqué dans la section 4.2.1, dans notre approche, deux pseudonymes très similaires diffèrent très peu en termes de nombre et de séquencement de caractères ; et deux noms sont similaires s\u0027ils possèdent un grand nombre de mots en communs.\nAfin de propager la découverte des couples mirror pour les réseaux Flickr et LiveJournal sans dégrader la précision, nous avons supprimé la règle d\u0027ordre 1 portant sur l\u0027attribut nom \nConclusions et perspectives\nDans cet article, nous avons présenté notre approche de réconciliation de profils dans les réseaux sociaux qui exploite la topologie du graphe et les attributs publics définis dans les profils et accessibles dans de nombreux réseaux sociaux. Les résultats obtenus sur la collection de données issues des réseaux LiveJournal et Flickr ont montré la pertinence des attributs considérés et l\u0027efficacité des règles que nous avons définies. La précision a atteint 94% et le nombre de liens découverts est passé de 148 liens transversaux me initialement renseignés entre LiveJournal et Flickr à 2768 au bout de 4 itérations. De plus, cette précision peut être contrôlée en appliquant les règles dont l\u0027ordre est supérieure à une valeur k fixée, contraignant ainsi la similarité d\u0027au moins k attributs. Elle peut également être contrôlée en triant, de manière croissante, les paires de profils mirror découverts (v i , v j ), pour chaque noeud v i , selon l\u0027ordre de la règle et le nombre de liens f riend communs entre (v i , v j ). Le nombre de liens découverts sur les paires de réseaux sociaux Twitter et YouTube comportant peu de noeuds et peu d\u0027informations renseignées sur les profils est faible. Dans ce cas, d\u0027autres éléments peuvent être prises en compte : (i) la topologie du graphe peut être davantage exploitée en distinguant la nature des liens f riend entrant ou sortant et en considérant leur nombre ; (ii) analyser le contenu ou les tags associés aux ressources sont des éléments qui pourraient être pertinents pour déterminer la similarité de deux profils ; (iii) exploiter l\u0027attribut lieu en désambiguïsant les valeurs associées et en définissant une mesure de similarité entre deux valeurs de lieu définies dans deux profils différents. Ces derniers aspects font l\u0027objet de nos travaux actuels.\n"
  },
  {
    "id": "357",
    "text": "Introduction\nEn raison de l\u0027évolution des nouvelles technologies, le domaine de la criminalistique informatique se heurte à des problèmes qui étaient encore anecdotiques il y a quelques années. Bien que des outils existent pour aider les enquêteurs, leur portée est limitée aux premières étapes du processus d\u0027investigation défini par (Palmer, 2001). La collecte et l\u0027étude des caractéristiques des pièces à conviction sont d\u0027importantes phases du processus, toutefois il est également nécessaire de déduire de nouvelles connaissances telles que les raisons de l\u0027état actuel des pièces à conviction (Carrier et Spafford, 2004) pour produire des conclusions utiles dans un procès. La reconstruction d\u0027évènements peut être vue comme un processus utilisant un ensemble de pièces à conviction pour produire une chronologie décrivant les évènements composant un incident. Dans ce papier, nous présentons l\u0027approche SADFC (Semantic Analysis of Digital Forensic Cases) qui permet de reconstruire et d\u0027analyser des chronologies à partir de sources de données hétérogènes (traces laissées sur une scène de crime). La section suivante passe en revue les approches de reconstruction existantes. La section 3 présente ensuite notre approche et expose notamment les aspects relatifs à la gestion des connaissances et aux possibilités de raisonnements offertes. La section 4 introduit un prototype ayant permis la validation expérimentale de notre approche. Enfin, les travaux futurs sont présentés dans la section 5.\nÉtude des approches de reconstruction d\u0027évènements\nVolume et hétérogénéité des données La taille des données à traiter et leur hétérogénéité (due à l\u0027utilisation de nombreuses sources de traces numériques telles que les fichiers de journalisation, les historiques de navigateur Web...) sont deux challenges majeurs de la reconstruction d\u0027évènements. Dans une large part des approches existantes, des solutions sont proposées pour l\u0027extraction automatique des évènements à partir de sources hétérogènes et la construction de la chronologie. Pour cela, des extracteurs automatiques et dédiés à chaque source d\u0027évène-ments sont utilisés pour peupler un élément de stockage central (base de données (Chen et al., 2003), ontologie (Schatz et al., 2004), etc.). Concernant l\u0027analyse de scénarios, les approches existantes proposent des fonctionnalités permettant de corréler des évènements (Schatz et al., 2004) ou d\u0027aider l\u0027enquêteur dans la lecture de la chronologie en produisant des évènements de haut niveau conceptuel à partir d\u0027évènements extraits depuis des sources de traces (Hargreaves et Patterson, 2012). Toutefois, aucune des approches étudiées ne proposent une solution complète pour assister les enquêteurs dans l\u0027interprétation et l\u0027analyse des chronologies.\nExigences légales Les approches de reconstruction d\u0027évènements doivent également satisfaire un ensemble d\u0027exigences telles que la crédibilité des conclusions produites, l\u0027intégrité des données utilisées et la reproductibilité du processus d\u0027investigation (Baryamureeba et Tushabe, 2004). De plus, (Gladyshev et Patel, 2004) avance qu\u0027une formalisation du problème de reconstruction d\u0027évènements est nécessaire afin de mieux structurer le processus de reconstruction, de faciliter son automatisation et d\u0027assurer la complétude de la reconstruction. Un problème récurrent parmi les approches étudiées est le manque de fondements théoriques permettant de valider et d\u0027expliquer les conclusions produites.\nReprésentation de connaissances et analyse de chronologies cybercriminelles\nPour répondre aux limites précédentes, nous présentons l\u0027approche SADFC qui permet d\u0027assister les enquêteurs depuis l\u0027extraction des traces numériques jusqu\u0027à l\u0027interprétation de la chronologie. Cette approche s\u0027appuie sur l\u0027analyse avancée de chronologies cybercriminelles basée sur une représentation des connaissances décrivant les activités d\u0027un utilisateur sur un ordinateur. Bien que les informations temporelles des évènements soient une dimension primordiale, d\u0027autres aspects doivent également être pris en compte pour offrir des fonctions d\u0027analyse avancées. L\u0027approche SADFC introduit une nouvelle représentation sémantiquement riche des évènements et de leurs interactions avec l\u0027environnement intégrant les notions de scène de crime (espace virtuel où se déroule un ensemble d\u0027évènements illicites), d\u0027évène-ments (action survenant à un instant donné), d\u0027incidents (ensemble des évènements illicites et d\u0027évènements corrélés à ces derniers), de traces (résidus laissés par un évènement et permettant sa reconstruction), d\u0027objets (ressources utilisées, générées, modifiées ou supprimées par les évènements) et de sujets (processus ou personnes initiant ou subissant les évènements). Le modèle proposé définit également les relations entre ces concepts parmi lesquelles les relations de composition (liant un évènement avec les évènements le composant), de participation (liant un sujet aux évènements auxquels il prend part), d\u0027utilisation (liant un évènement aux objets Y. Chabot et al.\nqu\u0027il utilise) ou encore de corrélation (liant deux évènements interdépendants). Pour aider les enquêteurs à mener à bien leurs enquêtes, des opérateurs de construction de chronologies et d\u0027analyse sont proposés. Trois ensembles d\u0027opérateurs sont définis dans l\u0027approche SADFC :\n-Les opérateurs d\u0027extraction ayant pour fonction d\u0027extraire les informations pertinentes contenues dans les traces numériques et de peupler la base de connaissances en consé-quence. -Les opérateurs d\u0027inférence permettant d\u0027enrichir la base avec de nouvelles connaissances déduites à partir des connaissances existantes. Par exemple, la seule information disponible pour déterminer le sujet impliqué dans un évènement d\u0027un navigateur Web est son identifiant de session, présent dans certaines traces numériques produites par les navigateurs. Pour identifier le sujet impliqué dans d\u0027autres actions, nous utilisons un opérateur d\u0027inférence basé sur l\u0027hypothèse suivante : soit e i la première visite d\u0027une page Web d\u0027une session s, e j la dernière visite de cette même session, t i la date de début de e i et t j la date de fin de e j , un évènement survenant sur la machine à une date comprise dans l\u0027intervalle de temps défini par t i et t j implique la personne identifiée par la session s. -Les opérateurs d\u0027analyse utilisés pour aider les enquêteurs dans l\u0027interprétation des informations portées par une chronologie. Les opérateurs proposés dans cette section permettent de dispenser les enquêteurs des tâches les plus fastidieuses de la reconstruction d\u0027évènements, leur permettant ainsi de se concentrer sur des tâches où leur expertise et leur expérience sont les plus utiles.\nImplémentation\nPour valider la pertinence du modèle et la viabilité des opérateurs proposés, un prototype a été développé. Ce dernier est centré sur une ontologie (permettant notamment l\u0027utilisation de processus automatiques pour raisonner sur les connaissances) OWL implémentant le modèle présenté dans la section précédente. L\u0027ontologie proposée est divisée en trois couches. La couche Provenance Knowledge Layer contient des informations sur la manière dont l\u0027investigation est menée (actions entreprises par les enquêteurs, informations utilisées pour parvenir à une conclusion, etc.). La couche Common Knowledge Layer contient des connaissances gé-nériques sur les évènements telles que des informations temporelles, les ressources ou encore les personnes et les processus participant à leur exécution. La couche Specialized Knowledge Layer contient les caractéristiques spécifiques portées par chaque évènement. La modélisation de connaissances spécialisées au sein de l\u0027ontologie permet de bénéficier de l\u0027expertise des acteurs du domaine durant l\u0027analyse de la chronologie. Le prototype propose également une implémentation des trois ensembles d\u0027opérateurs au sein d\u0027une application Java. Les opérateurs d\u0027extraction prennent la forme d\u0027un ensemble d\u0027extracteurs et de ponts permettant de peupler l\u0027ontologie à partir de sources de traces. Les opé-rateurs d\u0027inférence sont implémentés à l\u0027aide de requêtes SPARQL/Update recherchant des motifs particuliers dans l\u0027ontologie et ajoutant des connaissances en conséquence. Enfin, le prototype implémente un opérateur permettant d\u0027identifier des couples d\u0027évènements potentiellement corrélés en se basant sur des critères tels que la proximité temporelle, l\u0027utilisation de ressources communes, les sujets participants ou encore des règles formulées par les experts du domaine. Par exemple, soit un évènement représentant la visite d\u0027une page web et un évènement de création de marque-page pour cette même page Web, ces deux évènements sont corrélés car ils utilisent une même ressource (la page Web) et sont créés par le même processus (le navigateur Web Firefox par exemple).\nConclusion et travaux futurs\nDans cet article, nous avons présenté l\u0027approche SADFC permettant d\u0027aider les enquêteurs durant la reconstruction et l\u0027analyse de chronologies dans le respect des contraintes juridiques. Notre principale contribution est l\u0027introduction d\u0027un nouveau modèle pour décrire des incidents cybercriminels et d\u0027opérateurs permettant le peuplement de l\u0027ontologie ainsi que l\u0027analyse des connaissances. L\u0027implémentation de ces éléments au sein d\u0027un prototype a permis de valider expérimentalement la faisabilité et la pertinence de l\u0027approche proposée. Les travaux futurs s\u0027intéressent à l\u0027intégration de nouvelles sources de traces, à l\u0027enrichissement de l\u0027ontologie et à la conception de nouveaux opérateurs d\u0027analyse.\n"
  },
  {
    "id": "358",
    "text": "Introduction et motivations\nL\u0027abondance des documents, notamment sur le Web, dans de nombreuses langues a rendu nécessaire l\u0027existence d\u0027une Recherche d\u0027Information Multilingue. La RIM consiste ainsi à formuler une requête dans une langue source et à rechercher des documents pertinents dans des langues cibles (RIM) Nie (2010). Dans un contexte multilingue, la requête ainsi que les documents ne sont pas représentés dans un même espace d\u0027indexation étant exprimés dans des langues différentes. Par conséquent, la mise en correspondance de leurs descripteurs sera impossible. Ainsi, pour permettre une recherche multilingue, l\u0027enjeu consiste à représenter les documents et la requête dans un même espace d\u0027indexation.\nDans cet article, notre motivation est double : en premier lieu, il s\u0027agit de déployer les techniques d\u0027ECT qui permettent d\u0027inférer des relations de traduction entre des unités linguistiques pour l\u0027identification de lexiques bilingues à partir des corpus parallèles, et, en deuxième lieu, exploiter ces lexiques bilingues dans le cadre de la RIM Lavecchia et al. (2008);Latiri et al. (2010).\nLa revue de la littérature du domaine de la RIM montre qu\u0027il existe plusieurs types d\u0027approches pour modéliser la tâche de la RI multilingue. Nous pouvons citer les approches basées sur la traduction automatique qui s\u0027appuient sur la traduction automatique d\u0027une requête ou des documents de la collection. La traduction automatique de la requête est plus explorée Herbert et al. (2011) malgré qu\u0027elle souffre d\u0027un manque de précision comparée à celle basée sur la traduction d\u0027une collection de documents dans laquelle un contexte d\u0027information nettement plus important est utilisé, diminuant ainsi les risques de mauvaise traduction. De plus, les approches basées sur des corpus d\u0027apprentissage parallèles ou comparables Wu et He (2010) qui s\u0027appuient, pour la traduction des requêtes, sur un thésaurus ou des corpus comparables ou parallèles pour trouver des co-occurrences de termes Hazem et al. (2011);Bo et al. (2011). En outre, les approches basées sur des lexiques bilingues qui se basent principalement sur l\u0027expansion de requêtes. Elle consiste à reformuler la requête à l\u0027aide de dictionnaires grâce aux lexiques bilingues Levow et al. (2005). Enfin, les approches à base d\u0027un language pivot (l\u0027interlingua) qui s\u0027appuient sur un langage unifié (pivot) permettant de représenter la sémantique des différentes langues Hahn et al. (2004). La recherche multilingue se facilite en effectuant la traduction du corpus et des requêtes dans un même langage pivot. L\u0027ensemble de propositions que nous introduisons dans cet article s\u0027inscrit dans la famille des méthodes qui font appel à des méthodes de traduction, basées sur un lexique bilingue. Définition 1 Une règle d\u0027association inter-langues, notée par RAIL, est une implication de la forme : R : S S ? S C telles que S S et S C sont deux séquences de termes fermées fréquentes en langue source et cible, de tailles respectives n et m mots Latiri et al. (2010).\nUne règle d\u0027association inter-langues est également appréciée par les deux métriques de support et de confiance Agrawal et Skirant (1994). De plus, une règle d\u0027association interlangues est dite valide si sa confiance est supérieure ou égale au seuil minimal de confiance La génération des règles associatives inter-langues est réalisée à partir d\u0027un corpus parallèle Anglais-Allemand par un parcours de l\u0027espace de recherche qui s\u0027effectue au niveau de la phrase. La phase de génération est précédée par une étape d\u0027extraction de séquences fréquentes. Nous apportons, pour effectuer cette étape, des adaptations à l\u0027algorithme BFSM Chang (2004) liées au contexte de la fouille de données textuelles. L\u0027algorithme qui permet de dériver les règles d\u0027association inter-langues à partir de l\u0027ensemble des séquences inter-langues extraites est décrit dans Latiri et al. (2010).\nNous proposons dans ce qui suit, de déployer l\u0027ensemble des règles d\u0027association interlangues générées pour étudier leur apport dans la RI multilingue.\nDéploiement des règles d\u0027association inter-langue pour la RIM\nNous proposons deux réflexions de recherche en RIM à base des RAIL, à savoir : (i) Traduction d\u0027une requête pour la RI multilingue par les règles d\u0027association inter-langues ; et, (ii) Traduction des termes de l\u0027index par les règles d\u0027association inter-langues.\nTraduction d\u0027une requête pour la RI multilingue par les règles d\u0027association inter-langues\nPartant d\u0027une requête exprimée dans une langue source, il s\u0027agit de définir un modèle de RI capable de restituer des documents formulés dans chacune des langues cibles de la collection multilingue. Dans notre contexte, la traduction d\u0027une requête peut être conduite en deux étapes. D\u0027une part, une première étape qui consiste à dériver des règles d\u0027association inter-langues à partir des corpus parallèles (l S , l C ). Nous exploitons dans ce cadre le processus d\u0027extraction des règles d\u0027association inter-langues défini dans Latiri et al. (2010) et explicité dans la section 3. Ainsi, le résultat de cette étape est un ensemble de lexiques bilingues. D\u0027autre part, une deuxième étape consistant à déployer le lexique bilingue extrait dans la traduction de requête. La traduction de requêtes consiste à remplacer chacun des termes de la requête dans sa langue source avec des termes dans la langue cible. Nous soutenons l\u0027idée que les termes de la requête à traduire, exprimée en langue source l S , figurent dans les prémisses des règles inter-langues dérivées. De ce fait, leurs traductions potentielles sont représentées par les conclusions de ces règles d\u0027association inter-langues. Le but de cette étape est donc de supprimer certaines traductions jugées inadéquates dans le contexte de la requête Req S . Nous suggérons d\u0027utiliser la mesure de confiance pour ne garder que les règles d\u0027association valides par rapport à un seuil minimal de confiance minconf , qui permet de retenir les meilleures traductions avec une confiance assez élevée. Cette étape diminue ainsi l\u0027ambiguïté dans le choix des traductions sans forcément l\u0027éliminer.\nTraduction des termes de l\u0027index des documents par les règles d\u0027association inter-langues\nNous partons d\u0027une requête Req S formulée dans une langue source l S et d\u0027une collection bilingue {C l S , C l C }. Pour chaque document d de la collection C l C , son index dans la langue cible, noté Index l C (d) est généré par un processus classique d\u0027indexation. Ensuite, le processus d\u0027extraction de règles d\u0027association inter-langues est lancé sur les corpus bilingue {C l S , C l C }. Le but de ce processus est de générer des corrélations entre des unités linguistiques de la langue cible l C , dans laquelle est représenté l\u0027index d\u0027un document de la collection, et celles de la langue source l S . Dans ce contexte, les corrélations inter-langues sont générées dans le sens contraire que celui considéré pour la traduction de requêtes, i.e., de la langue cible vers la langue source. La sélection des RAILs les plus pertinentes pour la traduction se fait sur la base du seuil minimal de confiance minconf . L\u0027index Index l C (d) est par la suite traduit en utilisant ces règles inter-langues valides, où chaque terme t i de l\u0027index est traduit par le terme ou la séquence de termes qui apparaît dans une corrélation inter-langues contenant le terme t i . Cette nouvelle représentation de l\u0027index des documents d\u0027une collection multilingue permet ainsi une interrogation monolingue, avec une requête Req S exprimée en langue source l S et un corpus de documents en langue cible l C , dont les index des documents sont traduits moyennant les lexiques bilingues.\n5 Évaluation expérimentale\nCadre d\u0027évaluation\nNous considérons une collection fournie dans le cadre du projet MUCHMORE 1 . Cette collection regroupe des résumés de documents scientifiques dans le domaine médical, obtenus à partir du site web de Springer, rédigés en anglais et en allemand. Dans de nombreux cas toutefois, la version anglaise est une reformulation complète du résumé allemand, ce qui rend difficile un alignement au niveau des phrases. Ce corpus est considéré comme étant un corpus parallèle bruité. Premièrement, les corpus sont étiquetés et lemmatisés afin d\u0027extraire les lemmes des mots pleins (noms, verbes, adjectifs, adverbes). Seuls les noms et les adjectifs sont pris en compte par nos algorithmes de génération des RAILs.\nScénarios d\u0027évaluation\nNous avons réalisé deux scénarios d\u0027évaluation basés sur la collection MUCHMORE. La base d\u0027évaluation comparative(baseline), notée dans la suite de l\u0027article MT est le résultat donné dans Volk et al. (2003) où les auteurs ont utilisé la collection MUCHMORE pour éva-luer une approche de RIM à base de traduction automatique des requêtes de l\u0027allemand vers l\u0027anglais.\nScénario 1 : Traduction des requêtes par les RAILs. Le premier scénario, noté dans la suite de l\u0027article Trad-Req, consiste à interroger le corpus exprimé en anglais avec des requêtes exprimées en allemand de la collection MUCHMORE. Dans ce cas, les termes de la requête sont traduits de l\u0027allemand (langue source) vers l\u0027anglais (langue cible) en utilisant les règles d\u0027association inter-langues dérivées à partir du corpus parallèle allemand-anglais.\nScénario 2 : Traduction des index par les RAILs. Le deuxième scénario, noté dans la suite Trad-index, consiste à interroger le corpus en allemand avec des requêtes exprimées en anglais. Dans ce cas, chaque terme de l\u0027index relatif à chaque document du corpus de la collection sont traduits de l\u0027allemand (langue cible) vers l\u0027anglais (langue source) moyennant les lexiques bilingues illustrés par les RAILs (cf., sous-section 4.2).\nRésultats expérimentaux et discussion\nLe Tableau 1 synthétise les résultats de la recherche multilingue utilisant la collection MUCHMORE. À la lecture du Tableau 1, nous constatons que le déploiement des RAILs pour la traduction de requêtes (scénario Trad-Req) réalise une amélioration significative de la MAP \nConclusion\nCet article met en évidence le déploiement des règles d\u0027association inter-langues dans le cadre de la RI multilingue. L\u0027ensemble des propositions introduites se basent sur une fouille efficace d\u0027un corpus parallèle aligné au niveau des phrases à partir d\u0027une collection bilingue. L\u0027objectif est d\u0027extraire des corrélations inter-langues entre les unités linguistiques utilisées ultérieurement dans la RI multilingue. L\u0027évaluation expérimentale menée sur la collection de documents MUCHMORE a montré une amélioration significative de la pertinence système. Par ailleurs, ces propositions restent extensibles, dans le sens où les résultats peuvent être améliorés si nous utilisons d\u0027autres métriques statistiques lors de l\u0027extraction des RAILs. \n18`18ème\nConférence sur le Traitement Automatique des Langues Naturelles, TALN 2011, Montpellier, France, pp. 211-222. \n"
  },
  {
    "id": "360",
    "text": "Introduction\nIn database research, the last two decades have witnessed a growing interest in preference queries on the one hand. Motivations for introducing preferences inside database queries are manifold Hadjali et al. (2008). First, it has appeared to be desirable to offer more expressive query languages that can be more faithful to what a user intends to say. Second, the introduction of preferences in queries provides a basis for rank-ordering the retrieved items, which is especially valuable in case of large sets of items satisfying a query. Third, a classical query may also have an empty set of answers, while a relaxed (and thus less restrictive) version of the query might be matched by items in the database.\nApproaches to database preference queries may be classified into two categories according to their qualitative or quantitative nature Hadjali et al. (2008).\nDans la dernière, les préférences sont exprimées quantitativement grâce à une fonction de score monotone, le score global étant positivement corrélé avec les scores partiels. Dans les approches qualitatives, les préférences sont définies au travers de relations binaires. Comme ces relations peuvent être définies en termes de fonctions de score, cette famille est plus générale que la précédente.\nDans cet article, une vision qualitative est adoptée, à savoir l\u0027approche dite Skyline, introduite dans (Börzsönyi et al. (2001)). Étant donné un ensemble de points dans l\u0027espace, une requête skyline retrouve les points qui ne sont dominés par aucun autre au sens de l\u0027ordre de Pareto. Ce problème correspond à la recherche des extrema dans un ensemble de vecteurs (Kung et al. (1975)). Quand le nombre de dimensions sur lesquelles les préférences sont exprimées devient grand, de nombreux tuples peuvent être incomparables. Quelques approches ont été proposées pour définir un ordre entre deux tuples incomparables dans le contexte des requêtes skyline, fondées sur : -le nombre de tuples que chacun de ces deux tuples domine (notion de dominance kreprésentative proposée dans Lin et al. (2007), -des ordres de préférence entre les attributs : par exemple les notions de k-dominance et de k-fréquence introduites dans Chan et al. (2006a,b), ou -la représentativité : Tao et al. (2009) redéfinissent l\u0027approche de Lin et al. (2007) pour retourner les points du skyline les plus représentatifs possibles en présentant uniquement un représentant par cluster de points présent dans le skyline. D\u0027autres approches ont cherché à flexibiliser le concept de skyline selon différentes directions, voir par exemple Hadjali et al. (2011).\nIci, nous nous intéressons à un problème différent, celui de la possible présence de points exceptionnels dans la relation à laquelle la requête skyline est adressée. De telles exceptions peuvent correspondre à du bruit ou à la présence de points atypiques, ou non représentatifs dans la collection considérée. L\u0027impact de tels points sur le Skyline peut évidemment être important s\u0027ils en dominent d\u0027autres, plus représentatifs. Deux stratégies peuvent être envisagées pour gérer les exceptions. La première consiste à éliminer les anomalies par la mise en place d\u0027une procédure de nettoyage des données ou de contraintes de saisie. Néanmoins, il n\u0027est pas toujours aisé de distinguer entre des points erronés et des points qui représentent simplement des cas exceptionnels. Une meilleure solution est donc de définir une approche tolérante aux exceptions, i.e., qui mette en avant des points représentatifs de la base de données non dominés par d\u0027autres éléments représentatifs, tout en signalant les éventuelles exceptions. Dans cet article, nous décrivons une telle approche et interprétons la notion de représentativité à l\u0027aide de celle de typicité (Zadeh, 1984). Nous proposons une nouvelle définition du skyline basée sur la typicité et nous montrons que celle-ci permet i) de retrouver les meilleurs compromis sans pour autant évincer les points potentiellement intéressants, quoiqu\u0027exceptionnels, et ii) d\u0027offrir un outil flexible pour visualiser les réponses.\nLa suite de ce papier est organisée comme suit. La Section 2 fournit quelques rappels à propos de l\u0027ordre de Pareto, des requêtes Skyline, et de la notion de représentativité tout en motivant l\u0027approche proposée. La Section 3 présente notre solution et définit le concept de skyline graduel tolérant aux exceptions. La Section 4 donne les principaux éléments de mise en oeuvre de notre approche tandis que la Section 5 présente les premiers résultats obtenus sur un jeu de données réelles. Enfin, la Section 7 rappelle les principales contributions et propose des améliorations possibles de ce travail.\nRappel sur les requêtes Skyline et motivations\nUne requête skyline sur D appliquée à un ensemble de points S, notée SKY D (S), selon des relations d\u0027ordre i , retourne l\u0027ensemble de points qui ne sont dominés par aucun autre point de S :\nSelon le contexte, on peut essayer, par exemple, de maximiser ou minimiser les valeurs de dom(D i ), en supposant que dom(D i ) est un domaine numérique. Pour illustrer le principe de l\u0027approche proposée, considérons le jeu de données issu de la base Iris (Fisher (1936)), représenté sous forme graphique dans la Figure 1 \nFIG. 1 -Le jeu de données Iris\nEn ordonnée, figurent les largeurs de sépale tandis qu\u0027en abscisse apparaissent les longueurs de sépale. La requête skyline : select * from iris skyline of sepallength max, sepalwidth max recherche les points iris qui maximisent les dimensions largeur et longueur des sépales (points entourés de la Figure 1) .\nDans ce jeu de données, les points sont organisés en deux groupes qui correspondent respectivement aux intervalles sur les abscisses [4, 5.5] et [5.5, 7]. Par définition, les points du skyline sont à la frontière de l\u0027espace à deux dimensions décrits par les points du jeu Iris. Mais ces points sont très distants des zones décrites par les deux groupes et sont donc peu représen-tatifs du jeu Iris. Il pourrait être intéressant pour un utilisateur de pouvoir visualiser les points \"quasi dominants\", plus proches des groupes de points et donc plus représentatifs de la base. La notion de typicité introduite dans la section suivante va nous permettre de modéliser cette notion de représentativité.\nCalculer un ensemble flou de valeurs typiques\nLa typicité d\u0027un élément dans un ensemble indique dans quelle mesure cet élément est similaire à beaucoup d\u0027autres points de l\u0027ensemble. La notion de valeur floue typique d\u0027un ensemble a été largement étudiée dans le domaine des résumés de données et dans celui du raisonnement approximatif. Zadeh (1984) définit x comme étant un élément typique d\u0027un ensemble flou A ssi i) x a un haut degré d\u0027appartenance à A et ii) la plupart des éléments de A sont similaires à x. Dans le cas où A est un ensemble non flou, comme ce sera le cas dans la suite, la définition devient : x est dans A et la plupart des éléments de A sont similaires à x.\nDans (Dubois et Prade (1984)), les auteurs définissent un indice de typicité basé sur la fré-quence et la similarité. Dans la suite de cet article, nous adaptons leur définition comme suit. Considérons un ensemble E de points à deux dimensions, correspondant aux attributs splength (longueur du sépale) et spwidth (largeur du sépale). Nous dirons qu\u0027un point est d\u0027autant plus typique qu\u0027il est proche de nombreux autres points. La relation de proximité sera basée sur la distance euclidienne. Soit par exemple deux points p 1 and p 2 de la base Iris (cf. page précé-dente). la distance d(p 1 , p 2 ) entre ces deux points est définie comme suit :\nNous considérons que ces deux points sont proches l\u0027un de l\u0027autre si d(p 1 , p 2 ) ? ? où ? est un seuil prédéfini. Pour le jeu Iris, ce seuil est fixé à ? \u003d 0.5. La fréquence d\u0027un point est définie de la façon suivante :\nCe degré est ensuite normalisé en un degré de typicité dans [0, 1] :\nNous utiliserons également les notations suivantes :\nTYP(E) représente l\u0027ensemble flou des points un tant soit peu typiques de l\u0027ensemble E tandis que TYP ? (E) rassemble les points de l\u0027ensemble E dont la typicité dépasse le score ?. Un extrait du calcul de la typicité des points de la base Iris est présenté en \nSkyline tolérant aux exceptions\nComme expliqué en introduction, notre objectif est de revisiter la définition du skyline afin de prendre en compte la typicité des points dans la base de données et ainsi d\u0027éviter la perte des points qui seraient dominés par des exceptions ou des anomalies.\nVision booléenne\nUne première idée consiste à restreindre le calcul du skyline à un sous-ensemble de E correspondant aux points qui sont suffisamment typiques. La définition correspondante donne :\nUne telle approche réduit le coût d\u0027évaluation du skyline puisque seuls les points typiques au degré ? sont considérés dans le calcul. Il n\u0027est cependant pas possible avec cette définition de discriminer les points du résultat selon leur degré de typicité. En effet, le skyline obtenu est un ensemble classique (non flou). La Figure 2 illustre ce cas de figure et montre les maxima (croix entourées sur la figure) obtenus avec les points typiques à un degré ? 0.7 (croix simples). Cette première définition a également l\u0027inconvénient d\u0027exclure les points atypiques qui peuvent pourtant être des réponses valides. Une définition plus prudente consiste à garder les points atypiques dans le calcul du skyline et à transformer l\u0027équation (2) en :\nLa Figure 3 illustre cette nouvelle définition. Elle représente les points (points entourés) de la base Iris qui ne sont pas dominés par des points typiques (croix) au degré ? \u003d 0.7 au moins. Avec l\u0027équation (2), les points atypiques sont éliminés, alors qu\u0027avec l\u0027équation (3), le skyline devient plus large et englobe les extrema atypiques. Cette approche permet de relaxer les requêtes skyline de façon à transformer la ligne en un bandeau formé des points normaux du skyline et d\u0027éventuels substituts.\nLes principaux inconvénients de cette approche sont : i) le possible grand nombre de points retournés, ii) le fait qu\u0027on ne puisse pas différencier, parmi les points du skyline, ceux qui ne sont pas du tout dominés de ceux qui le sont fortement. \nVision graduelle\nUne troisième version permet de calculer un skyline graduel, vu comme un ensemble flou, qui préserve la nature graduelle de la relation de typicité en entrée. Ainsi, aucun seuil (?) n\u0027est appliqué aux degrés de typicité. Un point appartient totalement au skyline (degré d\u0027appartenance de 1) s\u0027il n\u0027est dominé par aucun autre point. Un point n\u0027appartient pas du tout au skyline (degré d\u0027appartenance de 0) s\u0027il est dominé par au moins un point totalement typique. Un point dominé par des points peu typiques appartient fortement au skyline alors qu\u0027un point dominé par des points très typiques appartient peu au skyline. L\u0027appartenance d\u0027un point p au skyline est donc dépendante du plus fort degré de typicité des points qui le dominent. Il en découle la définition suivante :\noù deg(¬(q D p)) \u003d 1 si q ne domine pas p (i.e., (q D p) est faux), 0 sinon. Ainsi, dans le cas où p est dominé par un point quelconque, son degré d\u0027appartenance au skyline est fixé par la non typicité de ce point. L\u0027équation (4) peut être réécrite comme suit :\nAvec la base de points Iris, on obtient le résultat présenté dans la figure 4. On observe que les points du skyline classique appartiennent totalement au skyline graduel. Néanmoins, on peut trouver des substituts intéressants qui sont plus ou moins typiques. Cette approche appa- \nFIG. 4 -Skyline graduel du jeu de données Iris\nraît intéressante en termes de visualisation. En effet, le score associé à chacun des points permet un affichage restreint des points selon leur degré d\u0027appartenance (e.g., les points dont l\u0027appartenance au skyline dépasse un degré ?). Un affichage en 3 dimensions des points (comme le montre la Figure 4), où le degré d\u0027appartenance au skyline donne la valeur en hauteur, permet d\u0027accentuer l\u0027effet \"ligne d\u0027horizon\" du skyline. Une pente se dessine des points optimaux vers les points les plus typiques ou complètement dominés. En 2 dimensions, il serait égale-ment possible de distinguer les courbes de niveau, symbolisant des zones d\u0027intérêt. Dans ces zones, l\u0027utilisateur peut choisir des points qu\u0027il considère intéressants. Même si ces points ne sont pas nécessairement optimaux, ils peuvent susciter l\u0027intérêt de l\u0027utilisateur dans la mesure où ils représentent de bonnes alternatives aux points optimaux dans le cas, par exemple, où ceux-ci apparaissent trop exceptionnels pour être crédibles. Enfin, un élément du skyline graduel possède deux degrés : un degré d\u0027appartenance au skyline, et un degré de typicité qui permet de savoir dans quelle mesure il est exceptionnel.\nOn peut imaginer différentes formes de navigation dans les zones pour découvrir les points : un simple parcours de la zone pour afficher les caractéristiques des points, l\u0027application de différents filtres dans la zone d\u0027étude comme la recherche de la diversité des réponses (sur certains attributs à spécifier), la typicité, des zooms sur des zones d\u0027intérêt, etc.\nÉléments d\u0027implémentation\nLa mise en oeuvre effectuée vise à montrer l\u0027intérêt de la notion de typicité dans le calcul d\u0027une requête skyline. Deux phases sont nécessaires au calcul du skyline graduel : i) le calcul de la typicité et ii) le calcul du skyline. Il existe de nombreux algorithmes pour évaluer les requêtes skyline : l\u0027algorithme Block-Nested-Loops (BNL) (Börzsönyi et al. (2001)) ; l\u0027algorithme Divide and Conquer (Börzsönyi et al. (2001)) ; une proposition utilisant un B-tree ou un R-tree (Börzsönyi et al. (2001)) ; un algorithme basé sur des structures de type Bitmap (Tan et al. (2001)) ; une amélioration du BNL Sort-Filter-Skyline (Chomicki et al. (2003(Chomicki et al. ( , 2005), et aussi (Bartolini et al. (2008)) qui s\u0027appuie sur un préclassement des tuples afin de limiter le nombre de tuples à lire et à comparer. Nous avons choisi de suivre l\u0027approche proposée dans (Tan et al. (2001)) qui permet d\u0027implémenter facilement la formule (5).\nL\u0027algorithme proposé dans Tan et al. (2001) permet de retourner progressivement les points du skyline. La structure de données centrale à cette implémentation est celle d\u0027un tableau de booléens ou bitmap. Un index bitmap est défini pour chacune des dimensions du skyline : chaque colonne désigne une valeur possible de la dimension et chaque ligne fait référence à un tuple de la base. La valeur de 1 à la croisée d\u0027une ligne l et d\u0027une colonne c indique que le tuple référencé en ligne l a comme valeur celle désignée par la colonne c. Ensuite, chaque point p de la base S est testé pour savoir s\u0027il appartient ou non au skyline. Pour cela, deux autres structures de données sont créées. La première, appelée A, désigne les tuples qui sont aussi bons que p, la seconde B désigne ceux qui sont meilleurs que p sur une dimension. A et B sont définies comme des tableaux de booléens dont les colonnes font référence aux tuples de S. Elles sont remplies à l\u0027aide des index bitmap. L\u0027organisation de cet index selon des valeurs de dimension décroissantes facilite la création de A et B. Algorithm 1 Algorithme principal du calcul du skyline graduel Require: d distance, n cardinalité de la base S, les points de la base p ? S, l\u0027ensemble des dimensions {d i } Ensure: skyline des points : ?p ? S, Sky grad (p) Prétraitement : création des index bitmap sur les d i Prétraitement : Calcul de la typicité des points T : ?p ? S, T yp(p) for all p ? S do // Recherche des points qui dominent p Création de A Création de B A :\u003d A AND B A :\u003d A MULT T Sky grad (p) :\u003d 1 ? M ax(A ) end for Le calcul de la typicité utilise une distance minimale d, évidemment dépendante des attributs sur lesquels porte la requête skyline. Si n est la cardinalité de la base, le temps nécessaire au calcul de la typicité est au plus en n 2 . En effet, l\u0027Algorithme 2 nécessite pour chaque point p de la base, de rechercher parmi tous les points de la base ceux qui sont proches de p. Il repose sur la distance euclidienne entre deux points p et p , notée dist(p, p ).\nAlgorithm 2 Calcul de la typicité Require: d distance, n cardinalité de la base S, Ensure: typicité des points :\nOn peut envisager deux façons de calculer la typicité : soit à la demande, sur les attributs spécifiés dans la clause skyline, soit au préalable, et dans ce cas un précalcul de la typicité sur différents ensembles d\u0027attributs pertinents doit être effectué. Dans le premier cas, se pose la question du surcoût d\u0027une telle opération. Le second cas soulève le problème de la mise à jour de la typicité en cas d\u0027insertion/suppression d\u0027un élément. Dans tous les cas, les index utilisés dans le cadre de la recherche des plus proches voisins (comme les kdtree) peuvent être exploités. En outre, comme précisé auparavant, le calcul du skyline graduel portant uniquement sur un fragment de la base, le surcoût du au calcul de la typicité ne devrait pas être pénalisant.\nEnfin, l\u0027algorithme proposé ici peut être adapté pour calculer en parallèle le skyline graduel en segmentant les tableaux A, B, A et T . De même, la création des structures A et B peut être parallélisée en segmentant leur remplissage (à condition de distribuer ou de partager les index et les valeurs de typicité).\nExpérimentation sur un jeu de données réelles\nL\u0027approche proposée a été testée sur un extrait du site de vente de voitures d\u0027occasion Le bon coin moteur essence, ce qui correspond à 441 annonces. La figure 5 montre les résultats obtenus. Kilométrage (x10 4 )  Table 3 montre un extrait de la 0.6-coupe du skyline graduel, qui comporte des points plus typiques. Elle offre des compromis aux annonces de la table 2, plus représentatifs de la base, et plus rassurants, tout en restant attractifs (le kilométrage est plus faible et le prix, sensiblement plus élevé). Mentionnons aussi que le temps nécessaire au pré-calcul de la typicité des éléments sélectionnés est deux fois plus important (de l\u0027ordre de 0,54\n"
  },
  {
    "id": "361",
    "text": "Introduction\nLes systèmes de classification automatique usuels ne permettent pas d\u0027interagir directement avec un algorithme d\u0027apprentissage, et par conséquent, les résultats qu\u0027ils produisent sont, en pratique, souvent en décalage avec les points de vue des utilisateurs. Pour personnaliser ces systèmes, une solution est d\u0027intégrer l\u0027utilisateur, dans le processus d\u0027apprentissage, pour qu\u0027il génère, explicitement via un support visuel, ses propres classifieurs (Ware et al., 2001). L\u0027utilisateur devient donc le coach qui va annoter, en positif ou négatif, un nombre limité d\u0027exemples pour entraîner un algorithme à apprendre ses préférences. Ensuite, cet algorithme doit être capable de généraliser et de produire des prédictions personnalisées pour le reste des exemples non-classés. Interactivement, l\u0027utilisateur peut corriger les mauvaises pré-dictions afin de renforcer le modèle. De tels systèmes d\u0027apprentissage ont récemment suscité un intérêt croissant et ont trouvé des applications dans plusieurs domaines (e.g CueFLIK pour la classification d\u0027images et CueT pour la classification d\u0027alarmes (Amershi, 2011)).\nDans le domaine télévisuel, il existe des systèmes de recommandation automatique (Bambini et al., 2011) mais ils sont, à notre connaissance, mono-label et ne prennent pas en compte des labels subjectifs de perception qualitative (e.g. « ce film me fait peur ») qui sont à l\u0027évi-dence importants pour les utilisateurs. Dans ce contexte, notre objectif final, qui dépasse le cadre de cet article, est de concevoir un système de classification interactive personnalisée pour mieux assister les utilisateurs dans leur recherche de contenus numériques. Ce travail requiert en préalable un état des lieux sur les capacités respectives des techniques de classification multi-label adaptées aux contraintes d\u0027interactivité. Nous nous focalisons donc ici sur une comparaison expérimentale de 12 méthodes de classification multi-label avec des mesures d\u0027évaluation adaptées à notre cas d\u0027utilisation final, et des jeux de données classiques de difficulté croissante. En complément, nous effectuons une première évaluation des temps d\u0027apprentissage et de prédiction, ainsi que de la vitesse de généralisation des classifieurs en variant le nombre d\u0027exemples d\u0027apprentissage.\nLa suite de cet article est organisée de la manière suivante : la section 2 définit plus pré-cisément les objectifs de notre étude. La section 3 présente les 12 méthodes de classification multi-label sélectionnées pour cette comparaison. Les jeux de données d\u0027évaluation et le protocole expérimental sont décrits dans la section 4. Les résultats obtenus sont résumés et discutés dans la section 5.  (Madjarov et al., 2012)  Pour qu\u0027un classifieur soit approprié à notre futur système de classification de VoD, il doit vérifier 3 propriétés essentielles : (1) pour chaque exemple sélectionné par l\u0027utilisateur, les premiers labels qu\u0027il suggère doivent être plus discriminants et plus pertinents que les labels suivants ; (2) les prédictions de labels qu\u0027il fournit à l\u0027utilisateur pour un exemple sélectionné doivent être proches des vrais labels, et (3) pour apprendre un modèle et fournir des prédictions à l\u0027utilisateur, il doit être le plus rapide possible. L\u0027évaluation de ces propriétés est basée sur les trois mesures suivantes.\nDéfinition du problème\nLa mesure Rank-Loss (Madjarov et al., 2012) permet d\u0027évaluer la conservation de l\u0027ordre des labels :\nLa mesure Log-Loss (Read, 2010) permet d\u0027évaluer le taux de dissimilarité entre les vrais labels et les labels prédits :\nL\u0027efficacité de résolution des algorithmes est évaluée ici par les temps d\u0027apprentissage et de prédiction en terme de secondes.\nMéthodes de classification multi-label\nLes approches de classification multi-label se divisent en trois grandes familles selon (Madjarov et al., 2012) : 1) Méthodes de transformation : elles transforment le problème d\u0027apprentissage multi-label en plusieurs problèmes de classification ou régression mono-label, 2) Méthodes adaptées : elles customisent des algorithmes d\u0027apprentissage mono-label pour les adapter au cas multi-label et 3) Méthodes ensemble : elles utilisent des ensembles de classifieurs issus de la première ou la deuxième famille d\u0027approches.\nPour notre comparaison expérimentale, nous évaluons 12 classifieurs multi-label, parmi lesquels nous retrouvons tous les classifieurs recommandés par (Madjarov et al., 2012) sauf RF-PCT que nous étudierons dans une prochaine étude expérimentale. Le choix des méthodes est fonction de leur fréquence d\u0027utilisation dans la littérature, la disponibilité de leur implé-mentation et de leur appartenance aux différentes classes de méthodes. Les implémentations de ces algorithmes sont disponibles sur MEKA 1 . Dans tous les cas, les classifieurs sont exé-cutés avec leurs paramètres par défaut à l\u0027exception de ML-kNN pour lequel, étant donné la petite taille des ensembles d\u0027apprentissage, nous avons fixé son paramètre k à 1.\nDans la 1 ` ere famille, nous sélectionnons 5 méthodes : (1) Binary Relevance (BR), (2) Classifier Chain (CC), (3) Label Powerset (LP), (4) Calibrated Label Ranking (CLR) (Madjarov et al., 2012), et (5) une méthode qui retourne toujours la combinaison de labels la plus fré-quente dans l\u0027ensemble d\u0027apprentissage (Baseline). Les méthodes de transformation utilisent des classifieurs binaires et les plus utilisés sont : Support Vector Machine (SVM) et l\u0027arbre de décision C4.5 (Read, 2010;Madjarov et al., 2012). Nous avons choisi ici C4.5 comme classifieur de base pour de meilleures performances en temps d\u0027apprentissage car il exploite un sous-ensemble d\u0027attributs contrairement à SVM qui requiert la totalité de l\u0027ensemble. Ce choix est important dans notre contexte de VoD où le nombre d\u0027attributs peut être très important.\nDans la 2 ` eme famille, nous choisissons 2 méthodes : (1) AdaBoost.MH et (2) ML-kNN (Madjarov et al., 2012). Enfin, dans la 3 ` eme famille, nous évaluons 3 méthodes : (1) RAndom k labEL sets (RAkEL), (2) Hierarchy Of multi-label classifiERs (HOMER), (3) Ensemble de Classifier Chains (ECC) (Madjarov et al., 2012) et Ensemble de Binary Relevance (EBR) (Read, 2010).\nEvaluation\nPour la comparaison expérimentale des classifieurs, nous avons sélectionné 5 jeux de données souvent utilisés dans la littérature multi-label (Tab. 1). La dimensionnalité de leurs espaces d\u0027attributs est petite comparée à la grande dimensionnalité d\u0027un catalogue de VoD. Néanmoins, elles fournissent un premier aperçu sur le comportement de chaque classifieur. Afin d\u0027éva-luer la performance prédictive et la rapidité de chacun des classifieurs, nous avons élaboré un protocole expérimental simple qui simule la phase initiale dans laquelle l\u0027utilisateur crée TAB. 1 -Description des jeux données avec DL : nombre de combinaisons de labels diffé-rentes et Lcard : nombre de labels associé en moyenne aux exemples. . Ensuite, pour chaque mesure, nous évaluons la performance moyenne de chaque classifieur sur les 5 bases de test de chaque corpus : chaque classifieur est entraîné avec tous les sous-ensembles d\u0027apprentissage de u 1 à u p . Puis, pour chaque classifieur, nous calculons, pour chaque taille de sous-ensemble d\u0027apprentissage, la performance moyenne sur les 5 corpus.\nCorpus\nDans toutes nos expérimentations, w a été fixé à 20 (i.e., 100 ensembles testés pour les 5 validations croisées) et p à 6. Le nombre d\u0027ensembles testés est suffisant pour obtenir une performance moyenne de chaque classifieur avec un faible écart-type, et la taille maximale d\u0027un sous-ensemble d\u0027apprentissage (i.e., 64) est conforme au nombre d\u0027exemples qu\u0027un utilisateur pourrait annoter au maximum dans un cas d\u0027usage réel.\nRésultat expérimentaux\nLes résultats présentés, dans les graphiques de la Fig. 1, sont des moyennes et des écarts-types de performances sur les 5 jeux de données. Les classifieurs sont ordonnés en fonction de leur performance globale qui tient compte de leurs performances moyennes obtenues pour toutes les tailles des sous-ensembles d\u0027apprentissage.\nLog-Loss pour la prédiction des labels\nComme nous le constatons sur la Fig. 1, ML-kNN  \nRank-Loss pour l\u0027ordonnancement des labels\nGlobalement, les classements des classifieurs avec les mesures Log-Loss et Rank-Loss sont assez semblables car elles sont intuitivement corrélées. Cependant, CLR devient la meilleure approche pour Rank-Loss et Adaboost.MH passe de la 2 ` eme place à la 6 ` eme place. Il n\u0027est pas surprenant que CLR obtienne les meilleurs résultats car elle a été conçue spécialement pour améliorer la qualité du classement des labels. De même, ML-kNN vise à minimiser cette mesure. En outre, lorsque le nombre d\u0027exemples d\u0027apprentissage augmente, CLR et ML-kNN sont les plus efficaces pour améliorer la qualité du classement des labels.\nTemps d\u0027apprentissage/prédiction\nComme toutes nos expérimentations ont été menées avec des implémentations de la librairie MEKA, que nous ne maîtrisons pas, les cumuls des temps d\u0027apprentissage et de pré-dictions calculés ne donnent qu\u0027une tendance de la complexité algorithmique (Fig. 1). Par exemple, EBR devrait être plus rapide que ECC mais nous observons ici le contraire. Parmi les meilleures approches (ECC, EBR et ML-kNN) pour les mesures précédentes, ML-kNN semble le plus rapide. En effet, il n\u0027apprend pas de modèle mais nécessite seulement quelques millisecondes pour estimer les probabilités a priori/a posteriori à partir du sous-ensemble d\u0027apprentissage, et moins d\u0027une demi seconde pour la prédiction parce que le nombre de voisins fixé est faible ici (i.e., k \u003d 1). Par ailleurs, EBR et ECC nécessitent plus de temps mais sont suffisamment rapides pour terminer dans les premières secondes. Dans une prochaine étape, nous allons étendre cette étude expérimentale avec d\u0027autres approches intéressantes de classification multi-label telle que RF-PCT, et nous sommes en train d\u0027analyser le passage à l\u0027échelle des différentes approches afin de s\u0027approcher des tailles de données réelles rencontrées dans le domaine de la VoD.\nConclusion et Travaux futurs\n"
  },
  {
    "id": "362",
    "text": "Introduction\nEn termes de performance de classification de textes, KNN se classe parmi les classifieurs les plus performants, un résultat obtenu d\u0027une multitude de tests de comparaison effectués sur le corpus Reuters Yang (1999). En contraste avec ses performances de classification, il est reconnu que cet algorithme est lent puisqu\u0027il requiert qu\u0027une mesure de similarité soit calculée entre tous les documents d\u0027apprentissage et le nouveau document. Il est caractérisé par un apprentissage très rapide, il est facile à apprendre, il est robuste aux ensembles d\u0027apprentissage bruités et il est efficace si le corpus est grand Bhatia et Vandana (2010). Un inconvénient majeur du KNN reste le temps qu\u0027il met pour classer un nouveau document. Différentes solutions ont été proposées pour réduire la complexité de calcul. Nous nous intéressons, dans ce papier, aux méthodes de sélection de prototypes. Plus précisément, nous étudions l\u0027impact de diffé-rentes méthodes de sélection de prototypes sur la performance de la catégorisation de textes avec le classifieur KNN. Essentiellement, voici comment se structure la suite du papier, la section 2 présente une série de méthodes de sélection de prototypes, en décrivant leurs principales caractéristiques. La section 3 présente les différentes expérimentations effectuées sur les différents corpus de textes pour comparer les différentes méthodes de sélection de prototypes. La conclusion générale résume le travail effectué et les résultats obtenus.\nSélection de prototypes\nDepuis la création de l\u0027algorithme des plus proches voisins en 1967Hart et Cover (1967, une grande variété de techniques de sélection de prototypes ont fait leur apparition pour remédier aux principaux inconvénients associés à l\u0027algorithme et ses variations José (2002); Olvera-López et al. (2010); Garcia et al. (2012). Leur objectif principal consistait à améliorer le temps de classification du KNN.\nPrincipe des méthodes de sélection de prototypes\nÉtant donné un ensemble d\u0027apprentissage DT, l\u0027objectif d\u0027une méthode de sélection de prototypes (notée dans la suite SP) est d\u0027obtenir le sous-ensemble d\u0027instances DS ? DT tel que DS ne contient pas d\u0027instances inutiles et lorsqu\u0027on classe une nouvelle instance Q par la règle KNN en agissant sur DS au lieu de DT nous avons P (DS) P (DT ) Olvera-López et al. (2010). Selon le type de sélection, ces algorithmes peuvent être classés en trois catégories.\nAlgorithme de condensation\nCes algorithmes essayent de trouver une réduction significative de l\u0027ensemble des instances de telle façon les résultats de classification avec KNN sont aussi proches que possible de ceux obtenus en utilisant tous les cas originaux. Ils cherchent les instances qui correspondent à leurs voisins les plus proches. Étant donné que ces instances fournissent les mêmes informations de classification que leurs voisins, elles peuvent être retirées sans dégrader l\u0027exactitude de la classification des autres instances qui les entourent. Nous distinguons dans cette catégorie la méthode la plus ancienne CNN décrite par Hart (1968). La performance de l\u0027algorithme CNN n\u0027est pas bonne, mais elle a inspiré la construction de nouvelles méthodes telles que RNN Gates (1972), SNN Ritter et al. (1975), TCNN Tomek (1976), POP Riquelme et al. (2003) et FCNN Angiulli (2005).\nAlgorithme d\u0027édition\nLes algorithmes d\u0027édition tentent de découvrir et de supprimer les instances bruitées. Les instances bruitées peuvent provoquer des erreurs de classification. Par conséquent, leur suppression devrait aider à augmenter l\u0027exactitude de la classification. Le procédé est décrémental et une instance est éliminée si elle est mal classifiée par un vote à la majorité sur ses K plus proches voisins. C\u0027est l\u0027algorithme ENN de Wilson (1972). ENN permet de résoudre le problème d\u0027instances bruitées avec une bonne performance, mais le taux de réduction reste toujours faible en le comparant à d\u0027autres méthodes Olvera-López et al. (2010). Une autre variante de la méthode ENN est ALLKNN Tomek (1976).\nAlgorithmes hybrides\nCes algorithmes permettent à la fois une élimination des instances bruitées et inutiles. Aha et al. (1991) ont proposé une série d\u0027algorithmes dont IB3 est la version la plus aboutie. Cano et al. (2003) ont présenté une étude expérimentale de différents algorithmes évolutionnaires, en fonction de leurs résultats, l\u0027approche génétique CHC a obtenu les meilleures performances en précision et en réduction. En outre, CHC est la méthode qui exigeait moins de temps d\u0027exécu-tion. La méthode Drop3 Wilson et Martinez (2002), utilise le filtrage de bruit avant de trier les instances de DS. Les objets restants sont classés par mesure de distance avec l\u0027objet de classe différente qui est le plus proche restant dans DS, et donc les objets loin de la frontière de dé-cision réelle sont supprimés en premier. la méthode SSMA Garcia et al. (2012) a été proposée pour couvrir un inconvénient majeur des méthodes évolutionnaires classiques : leur manque de convergence face à de grands problèmes.\nÉtude expérimentale\nNotre étude se concentre sur un problème particulier, il s\u0027agit de voir si l\u0027utilisation de l\u0027une des techniques de sélection de prototypes aidera à améliorer la catégorisation de textes avec les K plus proches voisins point de vue efficacité et efficience.\nCorpus utilisés et mesures d\u0027évaluation\nNous menons une étude expérimentale impliquant différentes tailles d\u0027ensembles de documents pour mesurer la performance des méthodes de sélection de prototypes en termes de précision, de capacités de réduction et d\u0027exécution dans le cadre de la catégorisation de textes. Les textes des différents corpus subissent un ensemble de traitements pour récupérer une représentation numérique exploitable par l\u0027algorithme d\u0027apprentissage. Cette représentation est appelée représentation vectorielle. Pour prédire la classe d\u0027un nouveau document, l\u0027algorithme cherche les k plus proches voisins de ce nouveau document en calculant la distance euclidienne et ensuite par vote majoritaire prédit la réponse la plus fréquente de ces k plus proches voisins. Nous avons calculé, dans chaque expérience, le taux de réduction (Red), l\u0027exactitude (A), la F-mesure micro (F µ ) la F-mesure macro (F m ), le temps de réduction (TR) en secondes et le temps de classification (TC) en secondes.\nRésultats et discussion\nDans le tableau 1 sont donnés les meilleurs résultats des différentes expériences réalisées avec le corpus Webk. Pour ce corpus, les quatre méthodes IB3, RNG, SSMA et ENN combinées avec KNN donnent les meilleurs résultats en termes d\u0027Exactitude. Par contre aucune méthode de condensation n\u0027a amélioré les résultats de KNN. En examinant le taux de ré-duction, nous constatons que les trois méthodes MCNN, CHC et SSMA donnent les taux de réduction les plus élevés. Comme on peut le voir sur le tableau 1, ALLKNN et FCNN donnent un taux de Fmesure (micro ou macro) le plus élevé par rapport à l\u0027ensemble des méthodes, mais elles sont moins précises que KNN. Il est particulièrement remarquable que plus le taux de réduction augmente plus le temps de classification diminue. CHC et SSMA produisent un taux de réduction le plus élevé mais on voit bien qu\u0027elles nécessitent un temps de réduction très élevé. En termes de rapidité de classification ce sont les approches de condensation MCNN, POP et FCNN qui sont les meilleures. Nous remarquons à partir de ces expériences et en tenant compte à la fois de l\u0027exactitude, et du taux de réduction que la méthode SSMA est meilleur que KNN dans la plupart des cas. En d\u0027autres termes, elle permet un meilleur compromis entre exactitude et taux de réduction mais elle reste toujours gourmande en temps de réduction, par exemple elle nécessite environ 259 secondes pour réduire un corpus de 4199 documents indexé avec 300 termes. Dans le cas du corpus 20NewsGroups, les expériences sont effectuées avec 80 % pour l\u0027apprentissage et les 20 % restants pour le test. En examinant le tableau 2, les résultats montrent que les deux méthodes MCNN et ENRBF qui donnent le meilleur taux de réduction par rapport aux autres, donnent aussi un meilleur compromis entre le temps de classification et le taux de réduction. Par contre les méthodes POP, IB3 et FCNN nécessitent un temps de classification plus élevé par apport à KNN. Une dernière remarque, qui est peut-être très importante, est que CNN et DROP3 sont assez lentes en temps de réduction, également SSMA et CHC qui n\u0027ont pas participé pour ce corpus à cause de cette contrainte de temps. Les expériences avec le corpus Reuters sont effectuées avec 80 % du corpus, l\u0027équivalent de 9100 documents, pour l\u0027apprentissage et 20 % pour le test. D\u0027après les résultats présentés dans le tableau 3, POP et SSMA offrent les meilleurs résultats de classification en termes de F mesure. Aucune méthode de condensation n\u0027a pu améliorer le 1NN. Lorsque K\u003d10, les méthodes de condensation POP, FCNN dépassent KNN en termes de Fmesure bien que les taux de ré-duction restent faibles, en particulier celui de la méthode POP. On peut remarquer aussi que les méthodes les plus performantes sont les approches incrémentales de condensation. En termes d\u0027exactitude, de réduction et du temps de classification RNG , SSMA et CHC offrent un bon taux, mais elles sont lentes pendant la réduction.\nConclusion\nDe nombreuses méthodes de SP ont été étudiées Garcia et al. (2012), mais une conclusion précise ne peut être donnée sur la meilleure méthode. Nous réalisons que le choix dépend alors du problème à résoudre, mais les résultats des différentes expériences obtenus par plusieurs chercheurs pourraient toujours nous aider à s\u0027orienter vers certaines méthodes qu\u0027ils considèrent comme intéressantes. En effet cette étude bibliographique et expérimentale nous a permis de découvrir plusieurs méthodes qui sont intéressantes sur le plan performance et sur le plan efficacité. En général, les meilleures méthodes de SP en termes de performance sont RNG et SSMA, mais elles ont pour principal défaut le temps de réduction qui reste élevé. Les meilleures méthodes pour la réduction sont MCNN, CHC et SSMA. Nous avons constaté que les méthodes hybrides permettent des taux de réduction élevés, tout en préservant la performance mais elles sont les plus lentes. Des méthodes plus rapides permettant d\u0027atteindre des taux de réduction élevés sont les approches de condensation comme MCNN, mais nous constatons que cette dernière n\u0027est pas en mesure d\u0027améliorer le KNN en termes de précision. Certaines méthodes présentent des différences claires lorsqu\u0027il s\u0027agit d\u0027un grand corpus (voir tableau 2), c\u0027est le cas de POP, FCNN et le cas également de ENRBF qui ont amélioré le KNN en temps de classification avec un taux de réduction intéressant. Les autres méthodes comme AllKNN, IB3, ENN ont pu améliorer KNN qu\u0027avec le petit corpus WebK. \n"
  },
  {
    "id": "364",
    "text": "Introduction\nLa recherche dans l\u0027ingénierie et la gestion des connaissances s\u0027était de plus en plus concentrée sur les problèmes de l\u0027acquisition, de la conservation et du transfert des connaissances. Cependant, considérant la masse de connaissances à préserver dans la mémoire d\u0027une entreprise, cette dernière est amenée à engager une réflexion afin de repérer celles qui devraient faire l\u0027objet d\u0027une capitalisation. Dans cette perspective, Saad et al. (2005) ont proposé une méthode de repérage des connaissances cruciales composée de deux phases. L\u0027objectif de la première phase est d\u0027inférer un modèle de préférences des décideurs qui se traduit par des règles de décision. La deuxième phase a pour objectif est de classer les \"connaissances potentiellement cruciales\" en utilisant les règles précédemment inférées. La première phase est basée sur l\u0027approche constructive de Belton et Pictet (1997) qui repose sur une concertation entre les décideurs pour déterminer un ensemble des règles collectivement accepté à partir des différentes règles individuelles. Dans un contexte organisationnel distribué, cette procédure est difficilement applicable quand des contraintes de temps et de distance géographique s\u0027ajoutent à une masse de connaissances grandissante et à un nombre de décideurs important. Notre objectif est, donc, de proposer une approche argumentative, basée sur la théorie des ensembles approximatifs pour automatiser la procédure de résolution de conflits entre les déideurs dans une organisation. Le papier est structuré comme suit : Section 2 détaille l\u0027approche argumentative. Section 3 illustre notre étude expérimentale et la section 4 résume notre contribution.\nApproche argumentative multicritères\nDans ce papier, nous proposons une approche argumentative basée sur les notions de force, borne, noyau, qualité d\u0027approximation (QA) et règle de décision définies par la méthode DRSA(Dominance-based Rough Set Approach) proposée par Greco et al. (2001). Elle repose sur un protocole d\u0027interaction et des stratégies de construction et d\u0027évaluation d\u0027arguments et de contre-arguments (Bouzayane et al., 2013).\nProtocole d\u0027interaction\nLe protocole d\u0027intéraction est décrit par deux processus argumentatifs. Le processus argumentatif exécuté par un initiateur est déclenché par la réception du message call_for_argument de la part de l\u0027intermédiaire, suite auquel l\u0027initiateur construit son premier argument envoyé au décideur récepteur. Il reste en attente d\u0027une réponse. Il peut recevoir trois types de messages : un accept, un reject ou un contre-argument (message justify). S\u0027il s\u0027agit d\u0027un contre-argument, l\u0027initiateur procède à une phase d\u0027évaluation afin de raisonner sur l\u0027action à entreprendre. Le processus argumentatif exécuté par un récepteur est déclenché, ainsi, dès la réception d\u0027un message justify de la part de l\u0027initiateur. Le récepteur s\u0027engage dans une phase d\u0027évaluation de l\u0027argument reçu afin de décider s\u0027il le rejette, l\u0027accepte ou le contre-argumente.\nStratégie de construction d\u0027un argument\nLa stratégie de construction d\u0027un argument que nous définissons est composée de trois étapes successives, soumises à un pré ordre croissant (Bouzayane et al., 2013) :\n-Etape ? 1 : le décideur initiateur, Dm init , cherche, parmi les règles supportant la classification défendue de K i , à sélectionner la ou les règles de force maximale. L\u0027ensemble des règles sélectionnées est noté ?. Si |?|\u003d1, la règle est rétenue pour faire l\u0027objet d\u0027un argument. Sinon, passe à l\u0027étape ? 2 . -Etape ? 2 : L\u0027initiateur choisit parmi les règles de l\u0027ensemble ?, la ou les règles qui ont le plus de critères en commun avec son noyau. L\u0027ensemble des règles construit est nommé ? . Si |? |\u003d1, cette règle est rétenue. Sinon, passe à l\u0027étape ? 3 . -Etape ? 3 : Une règle de ? est sélectionnée si et seulement si elle contient un nombre maximal de critères dans sa prémisse. L\u0027ensemble de règles construit est noté ? . Si |? |\u003d1, la règle est rétenue pour faire l\u0027objet d\u0027un argument. Sinon l\u0027initiateur sélec-tionne aléatoirement une règle de l\u0027ensemble le plus sélectif ? . Pour construire un contre-argument, le décideur raisonne non seulement sur ses informations locales mais aussi sur l\u0027argument reçu est évalué (Bouzayane et al., 2013).\nEvaluation d\u0027un argument ou d\u0027un contre-argument\nL\u0027évaluation d\u0027un argument ou d\u0027un contre-argument dépend de deux critères : l\u0027impact de l\u0027argument reçu sur la QA du récepteur et le niveau de certitude selon lequel la connaissance est classée (son appartenance à la borne). Le récepteur doit, ainsi, raisonner sur les actions à entreprendre en calculant la nouvelle QA (NAQ) résultante une fois l\u0027argument soit accepté. On note OAQ , la qualité d\u0027approximation initiale que le récepteur avait juste avant la réception de l\u0027argument. Avant de répondre, le récepteur doit raisonner comme suit : S. Bouzayane et I. Saad -Si la QA initiale du décideur récepteur est maximale (OAQ\u003d1), donc si la nouvelle QA reste aussi maximale (NAQ\u003d1), le récepteur accepte, sinon il rejette. -Si la qualité d\u0027approximation initiale du récepteur est inférieur à 1 (OAQ \u003c 1), donc :\n-Si (NAQ \u003c OAQ) : Il rejette. Cela parce que la nouvelle classification, si elle est acceptée, elle augmente le nombre des connaissances dans la borne et influe négative-ment la qualité du décideur en diminuant la valeur de sa qualité d\u0027approximation.\n-Si (NAQ \u003e OAQ) : Il accepte. En effet, la nouvelle classification aidera à améliorer sa qualité d\u0027approximation et à corriger , ainsi, quelques classifications incertaines.\n-Si (NAQ \u003d OAQ) : Il raisonne sur sa borne. Si la connaissance objet du conflit appartient à sa borne, ie classée avec incertitude, il contre argumente. Sinon, il rejette.\nExpérimentations et résultats\nNotre étude expérimentale (développée à l\u0027aide du languge JAVA) est basée sur des données réelles déjà testées et validées dans (Saad et al., 2005)  toutes les courbes extraites, nous traitons le cas où plus que la moitié des connaissances objets de conflits appartiennent à la borne et celui où la majorité de ces connaissances ont été classées avec certitude. La première courbe (cf. Fig.1) montre que notre approche a réussi à résoudre jusqu\u0027à 81% des conflits dans le premier cas et jusqu\u0027à 63% dans le deuxième ce qui prouve sa sensibilité par rapport à la borne. Plus que les connaissances objets de conflits appartiennent à la borne, plus que la chance de conclure le processus argumentatif avec succès augmente. Dans la deuxième courbe (cf. Fig.2) nous traitons le taux d\u0027amélioration de la QA qui représente la différence entre la QA initiale d\u0027un décideur et sa QA finale obtenue après avoir discuté tous les conflits. Nous remarquons que le taux d\u0027amélioration de la QA sur des connaissances de la borne est plus élevé que celui des classifications certaines. Cela prouve que les décideurs ayant une QA moins élevée sont plus proches à changer leurs préférences. Notre méthode est, ainsi, sensible aux profils des décideurs engagés dans le système argumentatif. Ces courbes tracent aussi un taux d\u0027amélioration jusqu\u0027à 0.62 pour un récepteur et 0.15 pour un initiateur.\n"
  },
  {
    "id": "365",
    "text": "Introduction\nLe but de cet article est d\u0027introduire un cadre d\u0027élagage basé sur les symétries pour la fouille de données. Dans d\u0027autres domaines, tels que la programmation par contraintes et la satisfaisabilité propositionnelle, les symétries sont souvent exploitées pour élaguer l\u0027espace de recherche et améliorer les performances des solveurs. Notre but est de montrer l\u0027apport d\u0027un élagage à base de symétries dans un cadre de fouille de données. Dans cet article, nous nous intéressons au cas de l\u0027extraction de motifs ensemblistes et nous prenons comme exemple d\u0027application l\u0027algorithme APRIORI (Agrawal et Srikant, 1994).\nLes symétries sont un concept fondamental en informatique, mathématiques, physiques et plein d\u0027autres domaines. Elles existent dans divers problèmes réels. Les symétries sont communément exploitées dans la résolution de problèmes combinatoires, tels que les problèmes d\u0027ordonnancement. Par exemple, dans un problème d\u0027ordonnancement où certaines machines peuvent être interchangeables, partant d\u0027un ordonnancement valide, on peut en obtenir un autre valide en permutant ces machines. Exploiter les symétries permet de réduire le coût de la recherche de solution, en évitant d\u0027explorer des branches symétriques de l\u0027espace de recherche. Beaucoup de travaux ont ainsi porté sur les symétries dans les problèmes de satisfaction de contraintes (CSP) (e.g. (Puget, 1993;Gent et Smith, 2000)), satisfaisabilité propositionnelle (e.g. (Benhamou et Saïs, 1994;Crawford et al., 1996)) et recherche opérationnelle (e.g. (Margot, 2003;Liberti, 2012)).\nEn fouille de données, les symétries sont principalement étudiées en fouille de graphes (e.g. (Desrosiers et al., 2007;Vanetik, 2010)). Récemment, (Jabbour et al., 2012) a proposé un nouveau cadre pour casser les symétries dans les problèmes de fouille de données. Dans (Jabbour et al., 2012), les symétries représentent des permutations entre les items laissant invariant le jeu de données. L\u0027exploitation des symétries dans ce cadre s\u0027effectue en pré-traitement permettant de supprimer des items du jeu de données initial. Cette approche est différente de celle utilisée dans les problèmes CSP et SAT où les symétries sont éliminées en ajoutant au réseau de contraintes, des contraintes supplémentaires appelées prédicats d\u0027élimination de symétries. (Jabbour et al., 2012), mentionne aussi que les symétries peuvent être intégrées dans les algorithmes de type APRIORI pour l\u0027élagage de l\u0027espace de recherche. Dans cet article, nous explorons en profondeur cette piste. Notre motivation vient du fait que l\u0027élagage basé sur les symétries peut être généralisé à d\u0027autres tâches de fouille de données ce qui n\u0027est pas le cas pour l\u0027approche présentée dans (Jabbour et al., 2012). Notre approche nous permet aussi d\u0027exploiter efficacement les symétries présentes au sein d\u0027une même transaction, ce qui n\u0027était pas le cas avec l\u0027approche de (Jabbour et al., 2012).\nNous avons choisi l\u0027algorithme APRIORI, algorithme pionnier de l\u0027extraction de motifs fréquents et de règles d\u0027association, comme exemple pour montrer la faisabilité de notre approche d\u0027élagage à base de symétries. Les expérimentations menées montrent la faisabilité et l\u0027efficacité de notre approche. À la limite de nos connaissances, notre approche est la première approche opérationnelle permettant l\u0027exploitation des symétries dans les problèmes d\u0027extraction de motifs ensemblistes.\nDéfinitions\nExtraction de motifs\nSoit I un ensemble d\u0027items et T un ensemble d\u0027identifiants de transactions. Un ensemble I ? I est appelé itemset ou motif. Une transaction est un couple (tid, I) où tid est un identifiant de transaction (tid ? T ) et I est un itemset (I ? I). Une base de données transactionnelle D est un ensemble fini de transactions ayant chacune un identifiant unique. Nous notons T id (D) \u003d {tid|(tid, I) ? D} l\u0027ensemble des identifiants des transactions de D. I items (O) dénote l\u0027ensemble de tous les items appartenant à l\u0027objet syntaxique O (e.g. base de données transactionnelle, motif, etc). On dit que la transaction (tid, I) supporte un motif J si J ? I.\nLa couverture d\u0027un motif I dans D est l\u0027ensemble des transactions de D supportant I :\nExemple 1 Soit le jeu de données transactionnel D (cf. Commençons par définir formellement les symétries dans le cadre de l\u0027extraction de motifs fréquents.\nDéfinition 1 (Permutation) Une permutation p d\u0027un ensemble fini S est une bijection de S vers S.\nChaque permutation p peut être représentée par un ensemble de cycles, c 1 . . . c n où chaque cycle c i \u003d (a 1 , . . . , a k ) est une liste d\u0027éléments de S tel que p(a j ) \u003d a j+1 pour j \u003d 1, . . . , k? 1, et p(a k ) \u003d a 1 . Dans la suite, pour des raisons de clarté, nous omettons les cycles de la forme (a, a).\nNous notons par P(I items (D)) (resp. P(T id (D))) l\u0027ensemble de toutes les permutations dans I items (D) (resp. dans T id (D)).\nNous notons par\nExemple 2 Considérons le jeu de données D de la Table 1 Comme l\u0027ensemble des transactions est invariant par symétrie, toutes les propriétés d\u0027un motif telles que la fréquence, la fermeture et la maximalité sont préservées. La proposition suivante montre que la symétrie préserve la fréquence d\u0027un motif.\nPreuve 1 ? • f étant une symétrie, le support de I est égal à celui de ?(I). Par conséquent,\nDétection de symétries dans les bases de données transactionnelles\nUn moyen commun pour détecter les symétries est d\u0027encoder les données sous forme de graphe et de rechercher les automorphismes de graphe. La plupart des outils de détection de symétries se basent principalement sur les automorphismes d\u0027un graphe coloré. Les couleurs des sommets sont utilisées pour contraindre le lien avec des sommets de la même couleur, i.e. deux sommets de la même couleur peuvent être permutés. La première implémentation pour calculer les automorphismes de graphes, appelée nauty, est proposée dans (McKay, 1981). D\u0027autres implémentations améliorées utilisant des heuristiques d\u0027élagage provenant du domaine de la théorie des groupes sont présentées dans (Aloul et al., 2003;Junttila et Kaski, 2007).\nAinsi, pour détecter les symétries dans un jeu de données transactionnel D, il suffit d\u0027encoder D sous forme de graphe coloré G de telle sorte que les symétries dans D correspondent aux automorphismes de G.\nDéfinition 3 Un graphe coloré est un triplet G \u003d (V, E, ?) où V est l\u0027ensemble de ses sommets, E ? V × V l\u0027ensemble de ses arêtes et ? est une fonction de V dans N qui associe un entier positif (une couleur) à chaque sommet.\nNous montrons maintenant comment un jeu de données transactionnelles peut être encodé sous forme de graphe coloré non orienté.\nLa figure 1 décrit la conversion de D (cf.  \nÉlagage à base de symétries\nDans cette section, nous montrons comment les symétries peuvent être exploitées afin d\u0027élaguer l\u0027espace de recherche dans le problème d\u0027extraction de motifs fréquents. Nous utilisons l\u0027exemple de l\u0027algorithme APRIORI (Agrawal et Srikant, 1994) (cf. Algorithme 1). Cet algorithme se base sur la propriété d\u0027anti-monotonie : si un motif est fréquent alors toutes ses généralisations le sont aussi. Il procède par une recherche par niveau des motifs fréquents. En effet, il commence par extraire l\u0027ensemble F 1 des motifs fréquents de taille 1 (ligne 1). Puis il calcule itérativement les ensembles de F 2 (fréquents de taille 2) jusqu\u0027à (F n ) tel que F n+1 \u003d ? (lignes 2-8).\nNous décrivons dans ce qui suit comment nous avons intégré notre approche d\u0027élagage à base de symétries dans l\u0027algorithme APRIORI. Notre approche contribue essentiellement à réduire l\u0027ensemble des motifs candidats tout comme la propriété d\u0027anti-monotonie. Notre approche est décrite dans l\u0027algorithme 2.\nAlgorithme 1 : APRIORI Données : D : jeu de données, ? : seuil de support minimal Sortie : l\u0027ensemble de tous les motifs fréquents dans D F1 ? {motifs fréquents de taille 1};\nAlgorithme 2 : APRIORI Sym Données : D : jeu de données, ? : seuil de support minimal, S : symétries dans D Sortie : l\u0027ensemble de tous les motifs fréquents dans D F1 ? {motifs fréquents de taille 1}; Lignes 1-4. Cette partie est similaire à APRIORI. Néanmoins, l\u0027ensemble des symétries S est utilisé pour améliorer le calcul des itemsets fréquents (ligne 1). Par exemple, si {a} est fréquent (resp. non fréquent) alors, pour tout ? • f ? S, le motif {?(a)} est aussi un motif fréquent (resp. non fréquent). Nous présentons dans cette section quelques résultats expérimentaux montrant la faisabilité et l\u0027apport de notre approche.\nNous présentons dans ce qui suit des expérimentations menées sur des jeux de données du répertoire de l\u0027UCI 1 et le jeu de données réel BMS-WebView-2 (Zheng et al., 2001). Ce je de données contient des données sur les flux de clics enregistrés sur des sites de e-commerce. La Table 2  Les symétries sont présentes dans les jeux de données indépendamment de toute tâche d\u0027extraction de motifs, ainsi notre approche d\u0027élagage basée sur les symétries procède en deux étapes. Nous commençons par extraire les symétries en utilisant Saucy. Ensuite, l\u0027entrée de notre algorithme APRIORI Sym sera composée de l\u0027entrée standard de l\u0027algorithme APRIORI (i.e. jeu de données et un seuil de fréquence minimale) à laquelle on ajoute l\u0027ensemble des symétries extraites.\nAfin de quantifier le gain obtenu par APRIORI Sym , nous introduisons les deux mesures suivantes :\n: permet de quantifier le gain obtenu en terme de temps d\u0027exécution. Notons que cette mesure ne prend pas en considération le temps nécessaire pour extraire les symétries étant donné que cette opération n\u0027est effectuée qu\u0027une et une seule fois pour chaque jeu de données considéré.\n#bdd scans(APRIORI)\n: permet de quantifier le gain obtenu en terme de réduction du nombre d\u0027opérations de calcul de fréquences et, par conséquent, du nombre de scans de la base de données. Un autre résultat important, est que l\u0027algorithme APRIORI sym garde les mêmes performances qu\u0027APRIORI lorsqu\u0027il s\u0027agit de jeux de données ne contenant pas de symétries (cf .  Table 4). Ainsi, notre approche d\u0027élagage basée sur les symétries peut être généralisée à d\u0027autres algorithmes d\u0027extraction de motifs leur permettant de tirer profit d\u0027une éventuelle présence de symétries dans les jeux de données. Nous montrons dans la section 5 la faisabilité de cette généralisation.\nRésultats expérimentaux\nNotons que le jeu de données BMS-WebView-2 contient 10 symétries. Nous n\u0027observons pourtant aucune influence de ces symétries sur l\u0027élagage de l\u0027espace de recherche. En effet, l\u0027efficacité de l\u0027élagage par symétrie n\u0027est pas proportionnelle au nombre de symétries détec-tées : si les symétries détectées n\u0027apparaissent que dans très peu de transactions, comme c\u0027est le cas pour BMS-WebView-2, tous les motifs les impliquant sent non fréquents et sont alors efficacement élagués par APRIORI.\nDiscussion\nLes résultats expérimentaux montrent que notre approche d\u0027élagage à base de symétries intégrée à l\u0027algorithme APRIORI permet d\u0027améliorer considérablement les performances de ce dernier. Pour un jeu de données ne contenant pas de symétries, APRIORI sym a les même performances que APRIORI. Ceci est expliqué par le fait que les symétries sont extraites une et une seule fois pour un jeu de données donné. Le gain en terme de temps d\u0027exécution et ainsi toujours supérieur ou égal à zéro (M T A (APRIORI Sym ,APRIORI ) ? 0).\nLes symétries sont ainsi une propriété importante dans les données qui devrait être considérée dans le but d\u0027améliorer les performances des algorithmes d\u0027extraction de motifs actuels sur certaines familles de jeux de données.\nGénéralisation de l\u0027utilisation des symétries\nDans cette section, nous proposons une généralisation de notre approche à d\u0027autres problèmes d\u0027extraction de motifs. Nous nous plaçons dans le cadre unificateur de (Mannila et Toivonen, 1997) 6 Symétries en fouille de données : état de l\u0027art En fouille de données, les symétries sont une propriété structurelle qui peut être exploitée pour élaguer l\u0027espace de recherche ou réduire la taille de la sortie. Les symétries peuvent aussi, dans certaines applications, être vues comme étant une connaissance pertinente à extraire. Elles permettent, par exemple, aux analystes de données de mieux comprendre certaines corrélations entre les items. Beaucoup d\u0027intérêt a été accordé à l\u0027exploitation des symétries dans le domaine de l\u0027intelligence artificielle, notamment pour les problèmes SAT (Benhamou et Saïs, 1994;Crawford et al., 1996) et CSP (Puget, 1993). En fouille de données les symétries sont principalement étudiées dans le cadre de la fouille de graphes (Vanetik, 2010;Desrosiers et al., 2007).\nDans (Desrosiers et al., 2007), les auteurs exploitent les symétries de sous graphes pour élaguer l\u0027espace de recherche lors de la génération de sous graphes candidats. Les symétries (au sens automorphismes) sont utilisées dans (Vanetik, 2010) en tant que mesure d\u0027intérêt. Dans, (Garrido, 2011), les auteurs proposent une approche pour détecter les symétries dans les réseaux sociaux.\nLes symétries sont présentes aussi dans le clustering de données. Par exemple, (Murtagh et Contreras, 2010) traite le cas de clustering hiérarchique dans de grandes masses de données pour détecter des symétries et d\u0027autres motifs intéressants. Les symétries sont considérées dans ce cas comme une structure révélant des propriétés intrinsèques et invariantes dans les données.\nNous constatons enfin que peu de travaux se sont intéressés aux symétries dans le cadre de l\u0027extraction de motifs ensemblistes. Nous mentionnons tout de même deux travaux traitant un cas particulier de symétries (Minato, 2006;Medina et al., 2005). En effet, les symétries traitées dans ces papiers, sont dites symétries de paires. Elles permettent d\u0027échanger à chaque fois un couple d\u0027items en laissant les autres items inchangés. (Minato, 2006) propose un algorithme ZBDD efficace permettant la détection des symétries de paires et discute les propriétés des items symétriques. Néanmoins, ce travail reste restreint à des cas très particuliers de symétries. Les paires d\u0027items symétriques, appelés items clones par Medina et Nourine dans (Gély et al., 2005), sont utilisées pour expliquer pourquoi, dans certains cas, le nombre de règles d\u0027une couverture minimale d\u0027une relation est exponentiel en nombre d\u0027items.\nConclusion et travaux futurs\nNous avons proposé dans cet article une approche d\u0027élagage à base de symétries pour les problèmes d\u0027extraction de motifs ensemblistes. Les symétries sont des propriétés structurelles qu\u0027on détecte dans un grand nombre de bases de données. Elles ont déjà montré leur inté-rêt dans divers problèmes de satisfaction, d\u0027optimisation et d\u0027énumération. Dans cet article, nous avons montré comment une telle propriété peut être exploitée dans la phase d\u0027élagage de l\u0027algorithme pionnier d\u0027extraction de motifs fréquents APRIORI permettant de réduire considérablement le nombre d\u0027accès à la base et de calcul de fréquences. Nous avons montré expéri-mentalement que dans certains cas, les calculs de fréquences sont réduits d\u0027environ 24% grâce à l\u0027exploitation des symétries. L\u0027un des points forts de cette approche réside dans le fait que la détection des symétries ne s\u0027effectue qu\u0027une et une seule fois pour chaque jeu de données et est indépendante de toute tâche d\u0027extraction de motifs. Nous avons montré aussi que notre approche peut être généralisée à d\u0027autres tâches d\u0027extraction de motifs tant que le prédicat de sélection en question est stable par symétrie.\nDans nos futurs travaux, nous envisageons d\u0027étendre notre approche d\u0027élagage à base de symétries à la fouille de séquences. D\u0027autres problèmes de fouille de données tel que le clustering pourrait aussi tirer profit de la présence des symétries dans les données. Nous souhaitons aussi trouver un moyen d\u0027exploiter les symétries locales dans les données (symétries dans une portion des données uniquement).\nSummary\nIn this paper, we show how symmetries, a fundamental structural property, can be used to prune the search space in itemset mining problems. Our approach is based on a dynamic integration of symmetries in APRIORI-like algorithms to prune the set of possible candidate patterns. More precisely, for a given itemset, symmetry can be applied to deduce other itemsets while preserving their properties. We also show that our symmetry-based pruning approach can be extended to the general Mannila and Toivonen pattern mining framework. Experimental results highlight the usefulness and the efficiency of our symmetry-based pruning approach.\n"
  },
  {
    "id": "367",
    "text": "Introduction\nTwitter offre des fonctionnalités de microblogging qui sont utilisées par des millions de personnes à travers le monde pour publier des messages courts. Ces personnes créent et partagent de l\u0027information liée à divers types d\u0027évènements, allant d\u0027évènements personnels banals à des évènements importants et/ou globaux, quasiment en temps-réel. L\u0027explosion du nombre d\u0027utilisateurs de ce service de réseautage social a entraîné l\u0027apparition d\u0027un phénomène de surcharge informationnelle. Pour lutter contre cela, il est nécessaire de doter les utilisateurs de moyens leur permettant d\u0027identifier plus facilement les éléments d\u0027information les plus intéressants et de se tenir au courant des derniers évènements significatifs.\nL\u0027information brute produite par Twitter est délivrée sous la forme d\u0027un flux de messages courts. Par conséquent la manière dont ceux-ci arrivent au fil du temps recèle une part importante de leur signification. La dynamique temporelle des thématiques les plus populaires sur ces réseaux est constituée d\u0027une succession de focus et dé-focus, autrement dits, une succession de pics de popularité (Leskovec et al., 2009). C\u0027est pourquoi de nombreuses approchesallant de méthodes basées sur la fréquence des mots (Benhardus et Kalita, 2013) jusqu\u0027à des méthodes plus complexes reposant sur des modèles de thématiques probabilistes dynamiques (Lau et al., 2012) -ont été proposées dans le but d\u0027identifier ce genre de thématiques. Dans cet article, nous présentons un système implémentant la méthode décrite par Guille et Favre (2014). Contrairement à la majorité des méthodes existantes, celle-ci prend en compte l\u0027aspect social du flux traité en considérant la fréquence de création de liens dynamiques entre utilisateurs. Un utilisateur crée un lien dynamique en insérant une ou plusieurs mentions (i.e. « @pseudonyme ») dans un tweet. Ce lien entre utilisateurs est dynamique car lié au contenu du tweet et sa durée de vie. Par ailleurs, cette méthode localise plus précisément dans le temps les thématiques que les méthodes existantes et traite les grands volumes de données plus efficacement que celles à base de modèles de thématiques probabilistes.\nLa suite de cet article est organisée comme suit. Dans la section 2 nous décrivons le fonctionnement du système puis dans la section 3 nous détaillons le cadre de la démonstration. Enfin dans la section 4 nous concluons. \nLe système de détection des thématiques populaires\nL\u0027objectif du système est d\u0027identifier des thématiques à la fois riches de sens et précisément localisées dans le temps. Son fonctionnement est schématisé par la figure 1.\nEntrée. Le système traite un flux de messages produit par Twitter. Le vocabulaire des termes employés dans ces messages est noté V . L\u0027axe temporel est discrétisé en partitionnant les messages dans des tranches temporelles de même durée (cf. la figure 1 pour une illustration de ce pré-traitement). Ce pré-traitement est commun à toutes les méthodes existantes.\nSortie. Le système génère une liste de thématiques, ordonnées selon leur popularité. Une thématique est définie par un terme principal, une liste pondérée de termes liés et un intervalle temporel. Par exemple, la thématique : {[\"google\", {(\"chrome\",0.8), (\"os\", 0. Traitement. La tâche d\u0027identification des thématiques populaires est décomposée en 3 problèmes : (1) l\u0027identification des termes principaux et des intervalles temporels, lesquels sont associés à un score de popularité ; (2.a) la sélection de termes liés pertinents ; (2.b) la construction du graphe des redondances et du graphe de thématiques, duquel est extrait la liste finale de thématiques. La méthode se déroule comme suit. Tout d\u0027abord, le problème (1) est résolu pour chaque terme appartenant au vocabulaire V à travers l\u0027analyse de l\u0027anomalie dans la fréquence de création de liens dynamiques. Ensuite, pour chaque couple de terme principal et intervalle temporel, le problème (2.a) est résolu afin d\u0027identifier l\u0027ensemble pondéré de termes liés. Chaque thématique ainsi constituée est insérée dans le graphe de thématiques si elle n\u0027est pas redondante avec une autre thématique déjà présente (2.b). Les redondances constatées sont modélisées par un second graphe, qui permet d\u0027identifier les thématiques à fusionner à la fin du processus, avant d\u0027extraire la liste des thématiques populaires qui sera retournée à l\u0027utilisateur.\nFIG. 2 -En haut, la frise chronologique générée automatiquement. L\u0027utilisateur navigue dans le temps à l\u0027aide du ruban. Il obtient des détails lorsqu\u0027il sélectionne une thé-matique. En bas, les scores de popularité, où l\u0027aire de couleur verte correspond à la thé-matique sélectionnée dans la frise. Interfaces utilisateur. En plus de la liste de thématiques triées par popularité décroissante, le système permet à l\u0027utilisateur d\u0027explorer les résultats de trois autres manières : (i) en navigant dans le graphe de thématiques, (ii) en parcourant la frise chronologique et (iii) en explorant le graphique interactif des scores en fonction du temps.\nCadre de la démonstration\nLe système est capable de traiter à la fois des données statiques, afin par exemple de fournir une vision rétrospective des thématiques marquantes durant une période d\u0027observation, et des données dynamiques, autrement dit le flux délivré en temps réel par Twitter. Dans ce cas, le système met à jour le modèle de façon incrémentale et il peut alors être utilisé pour suivre les thématiques les plus populaires en temps réel. Nous décrivons ci-après les données qui seront traitées lors de la démonstration.\nDonnées statiques. Deux corpus seront analysés : le premier correspond à l\u0027intégralité des tweets publiés par plus de 50.000 utilisateurs anglophones d\u0027octobre à décembre 2009 (Yang et Leskovec, 2011). Le second correspond à des tweets en français publiés durant la campagne électorale ayant précédée l\u0027élection présidentielle de 2012. Chaque corpus contient plusieurs millions de tweets.\nDonnées dynamiques. Le système interroge le flux public de Twitter et collecte en permanence des tweets francophones mentionnant François Hollande. Les thématiques populaires sont mises à jour chaque fois qu\u0027une nouvelle tranche temporelle de tweets est disponible.\nScénario. Le scénario de la démonstration consistera à analyser ces données à l\u0027aide du système, en faisant varier ses paramètres et en explorant les résultats avec ses interfaces. L\u0027efficacité de la méthode implémentée permet de traiter plusieurs millions de tweets en moins d\u0027une minute avec un PC standard. La figure 2 montre un extrait des résultats/interfaces géné-rés à partir du corpus statique francophone. La figure 3 montre un extrait des résultats/interfaces générés à partir des données collectées en temps réel par le système.\nConclusion\nNous avons présenté un système de détection de thématiques populaires sur Twitter. Les URL données dans l\u0027en-tête de cet article permettent le téléchargement du prototype et la consultation des thématiques détectées en continu à partir du flux public de Twitter. Afin de faciliter encore plus sa réutilisation, le système sera intégré à la plateforme d\u0027analyse et de fouille de données sociales SONDY (Guille et al., 2013).\nRemerciements. Ces travaux ont été partiellement financés par l\u0027ANR et le projet ImagiWeb (contrat ANR-2012-CORD-002-01).\n"
  },
  {
    "id": "368",
    "text": "Introduction\nLa classification non supervisée (clustering) de données constitue une tâche fondamentale et classique de structuration, pour l\u0027analyse exploratoire de jeux de données. Elle a été l\u0027objet d\u0027un nombre considérable de travaux depuis des décennies, à la fois comme objet général d\u0027analyse de données ou dans des contextes plus appliqués (bioinformatique Hu et Yoo (2004), segmentation d\u0027images Hong et al. (2008), etc . . .). Toutefois, la nature même du problème ne fournit généralement pas de vérité-terrain simple, tant sur la composition des classes que leur nombre. Les algorithmes proposés dans la littérature optimisent des critères très divers et font des hypothèses elle aussi diverses sur les propriétés de cohérence intra-classe.\nDepuis une dizaine d\u0027années, un axe de recherche, le clustering d\u0027ensemble, s\u0027est déve-loppé, élaborant des critères et des méthodes pour aggréger différentes partitions d\u0027un même jeu de données, construites par des critères antagonistes.\nPlusieurs méthodes sont alors envisageables pour pallier l\u0027incertitude quant à la plausibilité du résultat : -l\u0027entremise de connaissances obtenues par le biais d\u0027un « expert », ou inhérentes au domaine dont est issu le jeu de données, permet de formuler des hypothèses sur le méca-nisme générateur sous-jacent et donc sur la nature des regroupements les plus vraisemblables ; -la recherche d\u0027un compromis explicite entre les différents résultats, en les combinant et suivant généralement un critère à optimiser qui définit les propriétés attendues dans le clustering devant réaliser le consensus. Il existe deux grandes familles complémentaires d\u0027approches, pour traiter le problème de la combinaison de clustering. La première approche permet d\u0027établir une étude des mécanismes de génération des partitions (p.ex. Von Der Gablentz et al. (2000); Dudoit et Fridlyand (2003)). Celle-ci tend à améliorer la fiabilité du résultat dès lors que des variations sont appliquées dans le protocole expérimental, où différents algorithmes avec des paramétrages différents peuvent être utilisés. Différentes projections peuvent être également utilisées dans l\u0027espace des attributs pour engendrer chaque partition. La seconde approche procède par la définition d\u0027une fonction de consensus qui correspond à un critère à optimiser dépendant du choix d\u0027un modèle de représentation des partitions (i.e. hypergraphe des clusters, matrice d\u0027associations) et d\u0027une mesure de distance adaptée à cette dernière (p.ex. Strehl et Ghosh (2003); Topchy et al. (2005)).\nProblématique et enjeux Il n\u0027existe pas à ce jour, à notre connaissance, de théorie qui valide un ensemble de propriétés axiomatiques devant être implicitement vérifiées par chaque méthode en particulier, au-delà de la taxonomie propre à la discipline et distinguant les fonctions de consensus et les méthodes génératives. Plus généralement, chaque méthode fonde son propre compromis sur une base opérationnelle qui ne permet pas de justifier l\u0027ensemble des choix appliqués pendant la construction du résultat final.\nPar ailleurs, chaque résultat de clustering est naturellement définissable comme la donnée d\u0027une partition P , définie sur un ensemble d\u0027objets ?, figurant un jeu de données quelconque. Construire un résultat satisfaisant vis-à-vis des partitions originales représente en soi une gageure à cause de la nature combinatoire des partitions. De plus, la nature exploratoire de la tâche rend délicate la définition de propriétés permettant de construire inductivement un résul-tat sans introduire de biais dans le raisonnement.\nUne présentation algébrique peut alors contribuer à la compréhension des mécanismes intervenant lors de la combinaisons de partitions de sorte à définir effectivement la nature du raisonnement applicable sur celles-ci et en particulier comment établir qu\u0027une partition soit la représentation légitime du consensus de plusieurs autres partitions.\nEn économie et plus particulièrement dans la théorie du choix social, un consensus est considéré comme la réalisation d\u0027un arrangement ou d\u0027une combinaison à partir d\u0027un ensemble de profils formulés par des individus de sorte à obtenir un profil commun, avec la contrainte que ce dernier représente fidèlement les préférences initiales du plus grand nombre possible d\u0027individus. Dans ce cas, il est alors possible de formuler l\u0027analogie associant un résultat de clustering au profil propre à un individu. Par conséquent, il est aisé d\u0027interpréter l\u0027agrégation de deux objets a, b ? ? comme une relation de nature préférentielle résultant des paramètres propres à l\u0027algorithme de clustering et du critère statistique qu\u0027il a employé, puis de formuler le problème de satisfaction par la recherche d\u0027une partition qui soit maximalement compatible avec l\u0027ensemble de départ (i.e. typiquement, par l\u0027usage d\u0027une relation d\u0027ordre partielle). L\u0027intérêt de ce paradigme est qu\u0027il permet de contraindre les procédures de décisions qui vont régir explicitement comment les profils peuvent se combiner entre eux et ainsi d\u0027imposer des modalités propres aux usages espérés dans un contexte applicatif.\nUn exemple immédiat est de considérer les interactions attendues entre des partitions dans un système de recommandations : ceux-ci ont pour but de mettre en relation un individu ? dont le profil de préférences P ? est proche de celui d\u0027un autre individu ?. La logique sous-jacente est alors très similaire à celle d\u0027un système multiagent où les individus ont la faculté de raisonner (i.e. réaliser des déductions) depuis la connaissance induite par leurs propres préférences et de celles exprimées par d\u0027autres individus afin de modifier leur propre comportement. La relation entre deux individus ? et ? va donc dépendre de la permissivité de la relation définissant la proximité entre P ? et P ? et décidant quand un individu peut avoir accès aux préférences de l\u0027autre, et réciproquement.\nCependant, le but de cette article n\u0027est pas de proposer un système logique modélisant l\u0027ensemble des cadres applicatifs susceptibles d\u0027inclure des partitions mais de présenter la construction d\u0027un calcul algébrique basé sur le treillis des partitions où l\u0027analogie précédente va permettre de qualifier assez simplement le pendant algébrique de la définition du consensus. Cette approche sera mis en perspective avec celle présentée dans Barthelemy et Leclerc (1995).\nContributions\n-Modélisation d\u0027un nouvel opérateur algébrique sur le treillis des partitions ; -Définition d\u0027une fonction de consensus exploitant cette construction ; Cet article est une version condensée du rapport de recherche disponible dans Dumonceaux et al. (2013).\nDans la prochaine section, on présentera succinctement l\u0027algèbre permettant de manipuler des partitions et la nature du raisonnement applicable entre celles-ci et comment l\u0027exploiter par le biais d\u0027opérateurs adjoints au treillis.\nConsensus de partitions\nUn treillis est un ensemble partiellement ordonnée (P, ?) dont les éléments sont soit mutuellement comparables par sa relation (réflexive, antisymétrique et transitive), ou pour lesquels il existe deux bornes accessibles par cette même relation et symbolisées par les opérateurs ? et ? (idempotent, commutatif et associatif). La relation d\u0027ordre partielle (?) permet de distinguer les éléments vérifiant des propriétés particulières et se confond avec la relation de déduction ( qui lie une proposition logique prise pour hypothèse et sa conséquence. Les éléments d\u0027un treillis sont alors une représentation concrète des énoncés d\u0027un système logique.\nSoit (? ? , ?, ?) le treillis des partitions, où ? est l\u0027ensemble support pour lequel chaque partition P ? ? ? est le résumé d\u0027une séquence d\u0027opérations d\u0027agrégation dont les clusters c ? ? sont les représentations idoines et mutuellement disjoints. La relation de raffinement entre deux partitions P ? Q est alors vérifiée dès lors que chacun des clusters de P est inclus dans un cluster de Q. Par exemple, sachant P \u003d 12|3|45|6 et Q \u003d 123|456, P est alors plus fine que Q. En particulier, imaginons que P soit un résultat d\u0027expérience ou l\u0027énonciation d\u0027une hypothèse dans un problème quelconque, on écrit alors que de P , on déduit Q puisque on remarque aisément que toutes les paires d\u0027éléments dans ? × ? sont préservées dans Q 1 Dans ? ? , la partition P ? Q est telle que les clusters sont simultanément inclus dans P et dans Q, chacun des clusters de P et de Q sont simultanément inclus dans P ? Q. Par exemple, étant donné P \u003d 12|345|67 et Q \u003d 123|45|67, alors P ?Q \u003d 12|3|45|67 et P ?Q \u003d 12345|67. Le point de vue logique adopté ici considère P et Q comme des hypothèses, autrement dit P, Q sont les prémisses du raisonnement pour lequel on peut appliquer les règles d\u0027introductions classiques des connecteurs logiques, et ainsi calculer les propositions correspondantes.\nCependant, le treillis des partitions n\u0027étant pas distributif, il est impossible de décomposer sous une forme invariable et minimale une partition comme la composition de plus petite partitions atomiques et figurant une unique association entre deux éléments pris dans ? (cf. Birkhoff (1937)). La conséquence immédiate est qu\u0027on ne peut déduire la suite exacte des agrégations d\u0027une partition particulière et par induction, faire appel de multiples fois à (?) revient à inclure lors de la combinaison de partitions, un nombre croissant d\u0027associations entre des éléments qui n\u0027ont pas été formulées explicitement.\nUne autre conséquence est l\u0027impossibilité de définir des opérateurs adjoints à la structure comme c\u0027est l\u0027usage pour le treillis des parties d\u0027un ensemble 2 ? avec les opérateurs d\u0027implication (?) et de différence (?). Ces opérateurs étant définis comme des connections de galois, requérant la propriété de distributivité, on choisit alors d\u0027amender cette définition. Par exemple, Q ? P est définit par {R | R ? Q ? P } dont le résultat R figure la plus grande partition dont les parties communes avec Q sont compatibles avec P et on impose que le ré-sultat soit choisi parmi 2 P , et ainsi on a toujours Q ? P ? P quelque soit P . L\u0027usage de cet opérateur va permettre de minorer l\u0027importance des petits clusters déja inclus dans ceux d\u0027une autre partition. Arrow (1951) établit un ensemble de trois propriétés axiomatiques qui devrait être vérifié par toute méthode de consensus, et démontre dans la foulée que ses critères ne peuvent être satisfait simultanément, ce fait est communément appelé paradoxe de Arrow, dont le précurseur fut Condorcet. Une solution communément admise est d\u0027envisager la relaxation de l\u0027une des propriétés voire d\u0027enfreindre une ou plusieurs de ces contraintes. Ces critères sont les suivants : -L\u0027indifférence face aux alternatives non-pertinentes : deux éléments x, y ? ? seront agrégés dans le consensus indépendamment des éléments dans ? ? {x, y} et leur agré-gation dépend uniquement de la position exprimée sur ce couple par l\u0027ensemble des partitions dans P ; -Optimum de Pareto : si toutes les partitions sont unanimement d\u0027accord sur l\u0027agrégation de deux éléments x, y ? ?, alors ils sont également agrégées au niveau de la partition résultant du consensus ; -Non-didactorial : il ne doit pas exister de sous-ensemble P ? P tel que le consensus soit obtenu à l\u0027unanimité sur P , rejetant les préférences induites par les partitions dans P ? P . On définit une fonction de consensus par f : ? n ? ? ? ? . Une fonction qui satisfait de manière évidente à chacun de ses trois critères est celle requérant l\u0027unanimité :\n1. En particulier, soit une bijection ? : ? ? ?, telle que P? représente la partition résultante, alors P ? Q ? P? ? Q? et la relation d\u0027ordre dans le treillis est indépendante de l\u0027étiquetage employé pour l\u0027ensemble support. mais celle-ci est largement susceptible de renvoyer des résultats peu probants car ne sauvegardant pas suffisamment d\u0027associations entre éléments. Il faut donc assouplir la contrainte. La règle suivante agrège l\u0027ensemble des consensus unanimement obtenus par l\u0027ensemble de toutes les majorités formées sur P :\nDans notre méthode alg(.), P conserve pour chaque partition, chacun des clusters qui n\u0027est strictement pas inclus dans un cluster d\u0027une autre partition de l\u0027ensemble. Si l\u0027on se représente l\u0027ensemble des clusters de P comme un hypergraphe H, alors l\u0027hypergraphe H résultant de P forme une famille de Sperner telle que ?e, f ? H , e ? f et f ? e. Les associations préservées dans le consensus est alors le résultat de l\u0027intersection entre chaque paire d\u0027hyperarêtes dans H :\nFIG. 1 -Comparaison des résultats obtenues pour chaque méthode et sur les deux versions du jeu.\nConcernant le premier jeu, Fig. 1 montre que notre méthode est compatible (i.e. par la relation d\u0027ordre) avec un nombre inférieur de partitions mais en revanche, elle intègre un plus grand nombre d\u0027associations qui demeurent compatibles avec celles déjà incluses. La même chose est observable en dupliquant la première partition, cependant cela est dû en grande partie au fait qu\u0027aucun cluster de P 1 ne sera filtré. P 1 devient librement combinable avec les clusters préservés dans les autres partitions et on obtient alg(P m ) \u003d p 1 ? alg(P). Par ailleurs, ce comportement peut devenir préjudiciable si le nombre de partitions identiques s\u0027accroît.\nConclusion et Perspectives\nNous avons proposé une approche constructiviste dans le cadre de la définition réalisant partiellement l\u0027adjonction avec les opérateurs traditionnelles du treillis et pour lesquelles, il n\u0027existe pas de cadre formel décrivant l\u0027interprétation de la dualité les liant. Par ailleurs, nous avons proposé la modélisation du problème de consensus entre des partitions par l\u0027usage de l\u0027un de nos opérateurs.\nIdentifier explicitement les contextes hypothétiques dans lequel une partition peut-être vraie, semble une voie prometteuse. En effet, la construction de celles-ci procède invariablement d\u0027une étape d\u0027uniformisation et qui résulte en une perte d\u0027information, susceptible de\n"
  },
  {
    "id": "369",
    "text": "Introduction\nEn apprentissage non-supervisé, la plupart des méthodes de partitionnement souffrent d\u0027une part d\u0027un problème commun de stabilité des résultats par rapport aux paramètres d\u0027initialisations des algorithmes. En effet, les partitions fournies par les algorithmes des K-moyennes ou des cartes topologiques auto-organisées (SOM), par exemple, dépendent du choix des centres de classes initiaux, du voisinage initial et final des cellules de la carte topologique,etc. D\u0027autre part, en fonction de la méthode de classification utilisée, les partitions peuvent être différentes. Ainsi, en classification ascendante hiérarchique la partition obtenue dépend de la stratégie d\u0027agrégation utilisée (critère de ward, lien moyen, lien complet, etc). Récemment, Strehl et Ghosh (2002) ;Fred et Jain (2003) ont proposé alors d\u0027agréger les différentes partitions afin d\u0027accroître significativement les performances de la partition finale. Ce concept connu sous le terme \"d\u0027ensemble clusters\" reprend les concepts plus anciens de recherche de consensus de partitions proposés par Régnier (1983) et Gordon et Vichi (1998). Dans cette communication, on intéresse aux méthodes d\u0027ensemble clusters dédiées aux cartes topologiques. Comme les méthodes de partitionnement classiques (K moyennes, CAH), les algorithmes d\u0027apprentissage de type auto-organisés SOM (Kohonen, 1998) sont fortement dé-pendants des paramètres d\u0027initialisation. Nous cherchons donc à améliorer le partitionnement des observations offert par l\u0027algorithme SOM en adaptant les techniques de \"cluster ensemble\" aux cartes topologiques. L\u0027approche proposée repose sur la méthode d\u0027analyse de données multi-tableaux STATIS (Lavit et al., 1994) pour déterminer une matrice compromis représentant au mieux la similarité entre les partitions issues des cartes topologiques. La fusion des cartes topologiques est alors obtenue à travers une classification basée sur cette matrice compromis. La section 2 suivante présente le problème de fusion de SOM. La section 3.1 présente la mé-thode STATIS. La section 3.2 présente la méthode proposée et ses applications aux données réelles issues de l\u0027UCI et sur des données simulées.\nFusion de SOM\nLa démarche des méthodes d\u0027\"ensemble clusters\" se résume en deux étapes : une étape de diversification par la création d\u0027un ensemble de partitions et une étape d\u0027agrégation des partitions. Dans le cadre de la fusion de SOM nous désignons par C l\u0027ensemble des cartes topologiques. Cet ensemble peut être obtenu de diverses manières. Il peut s\u0027agir : de résul-tats obtenus par application répétée d\u0027un même algorithme avec différentes initialisations des paramètres (Jiang et Zhou, 2004;Georgakis et al., 2005). Dans le domaine des réseaux de neurones, le problème de recherche de consensus consiste d\u0027une part à apprendre indépendamment B cartes SOM, d\u0027autre part à synthétiser les résultats de ces SOM en regroupant les neurones similaires des différentes cartes. Soit La méthode proposée dans cette communication est basée sur la classification d\u0027une matrice de compromis˜Cocompromis˜ compromis˜Co représentant au mieux la relation entre les partitions et qui s\u0027obtient en utilisant la méthode STATIS que nous présentons dans la section suivante. \noù D est la matrice des poids associés aux individus, en général ils sont choisis uniformément égaux à 1/N. On définit par S la matrice des produits scalaires entre les tableaux et dont les entrées S(a, b) valent HS(X a , X b ). Il est habituel de normaliser les matrices X b . Les entrées de la matrice S deviennent alors des coefficients de corrélation vectorielle R V entre les objets X b tel que :\nComme en Analyse en Composantes Principales, la diagonalisation de la matrice S fournit une représentation euclidienne des tables X b dans un espace de dimension réduit permettant de visualiser les différences et les ressemblances entre les matrices X b . Ce qui constitue l\u0027étude de l\u0027inter-structure dans STATIS. Une étude plus fine au niveau des individus permettant de comprendre la relation entre les tables est réalisée à travers l\u0027étude de intra-structure.\nL\u0027intra-structure repose sur la détermination d\u0027une matrice compromis˜Cocompromis˜ compromis˜Co de même nature que les matrices H b telle que˜Coque˜ que˜Co soit le plus corrélé possible au sens du produit scalaire HS avec les matrices X b . La recherche du compromis est formalisée comme un problème de recherche de la meilleure combinaison linéaire des matrices X b , celle qui maximise la corrélation vectorielle avec les matrices X b :\nComme en ACP, la solution est le premier facteur principal µ défini comme le vecteur propre associé à la plus grande valeur propre de la matrice des R V :\nApproche de Fusion de SOM basée sur STATIS\nOn s\u0027intéresse au consensus des partitions de l\u0027ensemble ? \u003d {? 1 , . . . , ? B } issues de SOM. Notre approche de recherche de consensus est basée sur la matrice compromis˜Cocompromis˜ compromis˜Co fournie par STATIS. Les matrices X b sont les matrices d\u0027adjacence associées aux partitions ? b avec :\nPour déterminer le consensus on utilise usuellement la matrice Co \u003d  \nÉvaluation\nL\u0027ensemble ? des partitions est obtenu à travers 30 applications de SOM en faisant varier des paramètres d\u0027initialisations. Nous calculons ensuite le consensus des 30 partitions à l\u0027aide de la méthode STATIS. Cet expérience est répété 25 fois pour les données IRIS, WINE, GLASS, IONOSPHERE et \"Image segmentation (IS)\" issues de UCI. Sur les données simulées D1 et D2 qui sont de types multi-vues, chaque vue est une table composée de 5 variables, le consensus est défini sur 10 partitions obtenues sur les vues. Cet expérience est aussi répété 25 fois. Afin de positionner la méthode proposée, que nous appelons, CSTATIS par rapport à quelques méthodes de consensus présentées dans la littérature, nous réalisons la même expé-rience avec des algorithmes de recherche de consensus basés sur la factorisation de matrice non-négative (NMF, Weighted NMF) présentés par Ding et al. (2006), la méthode d\u0027ensemble cluster (CSPA) présentée par Strehl et Ghosh (2002). Le tableau 1 présente les caractéristiques des différentes tables. Nous utilisons l\u0027indice de pureté suivant pour évaluer la similarité entre deux partitions : TAB. 2 -Résultats du consensus de partition, on observe, la moyenne des puretés des algorithmes sur 25 expériences. CSTATIS est le résultat du consensus obtenu à l\u0027aide de STATIS, NMF est le résultat du consensus obtenu à l\u0027aide de l\u0027algorithme de factorisation de matrice non-négative NMF, WNMF est le résultat de la version pondérée de NMF et CSPA est le résul-tat de l\u0027algorithme d\u0027ensemble clusters.\nFIG. 1 -Réprésentation de la carte consensus ; Les figures en haut correspondent à la carte consensus. En bas, une carte de l\u0027ensemble de diversification pour chaque table. On observe une bonne conservation de la topologie des observations sauf pour la table IS\n"
  },
  {
    "id": "370",
    "text": "Introduction\nIntroduite par (Agrawal et Srikant, 1995), la fouille de données séquentielles permet de découvrir des corrélations entre des événements selon une relation d\u0027ordre (e.g. le temps). En intégrant des connaissances sous forme d\u0027a priori dans le processus de fouille, l\u0027extraction de motifs sous contraintes contribue à réduire le nombre de motifs en ciblant les motifs potentiellement intéressants (Dong et Pei, 2007). De plus, elle permet souvent de concevoir des algorithmes plus efficaces en réduisant l\u0027espace de recherche. De nombreux algorithmes sont proposés dans la littérature pour l\u0027extraction de motifs séquentiels (Dong et Pei, 2007). Malheureusement, ces méthodes ne traitent que quelques classes particulières de contraintes (monotonicité et anti-monotonicité) avec des techniques dédiées. Ce manque de généricité est un frein à la découverte de motifs pertinents car chaque nouveau type de contraintes entraîne la conception et le développement d\u0027une méthode ad hoc.\nPour lever ce frein, des travaux récents visent à croiser les techniques de Programmation Par Contraintes (PPC) et de fouille pour l\u0027extraction sous contraintes de motifs d\u0027itemsets (Guns et al., 2011;Khiari et al., 2010). Le point commun de ces travaux est de modéliser le problème de la fouille de motifs en un problème de CSP. Une telle modélisation présente l\u0027avantage d\u0027être flexible en permettant de définir de nouvelles contraintes sans s\u0027occuper de leur résolution. Mais, les méthodes proposées sont conçues pour des données ensemblistes et la dimension séquentielle reste quasiment non exploitée, à l\u0027exception des travaux de (Coquery et al., 2012) qui portent sur un cas particulier de chaîne (et non sur une base de séquences). L\u0027originalité de notre travail consiste à proposer une première modélisation PPC de l\u0027extraction de motifs séquentiels sous contraintes -à partir d\u0027une base de séquences -dans un cadre déclaratif permettant de traiter simultanément des contraintes de nature quelconque. Les contraintes traitées dans le cadre de cet article incluent les contraintes de fréquence, clôture, taille, gap et celles portant sur les items (voir (Métivier et al., 2013)  2 Extraction de motifs séquentiels\nMotifs séquentiels\nEtant donné un ensemble I de littéraux distincts appelés items, une séquence s \u003d 1 , . . . , i n est une liste ordonnée non vide d\u0027items. Une séquence S a \u003d 1 , . . . , a n est incluse dans une autre séquence S b \u003d 1 , . . . , b m s\u0027il existe des entiers 1 ? i 1 \u003c . . . \u003c i n ? m tels que a 1 \u003d b i1 , . . . , a n \u003d b in . Si la séquence S a est incluse dans S b , alors S a est une sous- ). Un motif séquentiel fréquent est un motif ayant un support minimal supérieur ou égal à un certain seuil minsup.\nFouille de motifs séquentiels sous contraintes\nLes contraintes permettent à l\u0027utilisateur de définir plus précisément ce qu\u0027il considère comme intéressant pour ne conserver que les motifs pertinents (Dong et Pei, 2007). Un exemple classique de contraintes est celle de support minimal. Nous passons en revue quelques autres contraintes classiques et traitées par la suite : Fermeture. Cette contrainte permet d\u0027obtenir une représentation condensée des motifs en éli-minant les redondances entre motifs : Un motif fréquent s est un motif fermé fréquent, s\u0027il n\u0027existe pas de motif fréquent s tel que s s et sup(s) \u003d sup(s ). Par exemple, avec minsup \u003d 2, le motif b c de la Table 1 est fermé contrairement au motif c Contrainte d\u0027item. Cette contrainte spécifie le sous-ensemble d\u0027items qui doivent apparaître ou non dans les motifs extraits. Par exemple, soit la contrainte  (a)). Mais seule la séquence 1 supporte p [1,2] .\n3 Modélisation de la fouille séquentielle sous contraintes\nProgrammation par contraintes\nLa Programmation par Contraintes (PPC) est un paradigme puissant pour résoudre des problèmes combinatoires, se basant sur des techniques issues de l\u0027intelligence artificielle et de la recherche opérationnelle. La PPC se base sur le principe suivant : (1) l\u0027utilisateur spécifie le problème d\u0027une façon déclarative comme un problème de satisfaction de contraintes (CSP) ; (2) le solveur cherche l\u0027ensemble complet et correct de solutions du problème. Un CSP est un triplet (X , D, C) où X \u003d {X 1 , . . . , X n } est un ensemble fini de variables ayant pour domaines finis D \u003d {D 1 , . . . , D n } et C \u003d {C 1 , . . . , C m } est un ensemble de contraintes où chaque C i est une condition sur un sous-ensemble de X . L\u0027objectif est de trouver une affectation complète de valeur d i ? D i à chaque variable X i satisfaisant toutes les contraintes de C.\nUne technique de modélisation importante en PPC sont les contraintes globales qui dé-crivent un ensemble de propriétés que doit satisfaire un ensemble de variables. Nous présen-tons succinctement deux contraintes globales, Among et Regular, permettant de modéliser les contraintes décrites en Section 2. La contrainte Among. Cette contrainte restreint le nombre d\u0027occurrences de certaines valeurs dans une séquence de n variables (voir (Beldiceanu et Contejean, 1994) pour plus de détails). La contrainte Regular. Soit M un automate fini déterministe et X un ensemble de variables, la contrainte Regular(X, M ) impose que la séquence de valeurs de X appartient au langage régulier reconnu par M (Pesant, 2004).\nModèle\nVariables. Soit I \u003d {i 1 , . . . , i n } un ensemble de n items, EOS un symbole n\u0027appartenant pas à I désignant la fin d\u0027une séquence, SDB un ensemble de m séquences et la taille de la plus grande séquence de SDB. Un motif séquentiel inconnu p de taille est modélisé par les variables P 1 , P 2 , . . . P , chaque P i a pour domaine D i \u003d I ? {EOS}. On introduit les m variables booléennes T s telle que (T s \u003d 1) ssi (p est une sous-séquence de s) : (S s \u003d 1) ? (p s). Alors, sup(p) \u003d ? s?SDB T s . Modélisation de \"p s\". Pour chaque séquence s, nous générons un automate A s permettant de capturer toutes les sous-séquences de s. Ensuite, nous imposons la contrainte Regular indiquant que le motif p doit être reconnu par l\u0027automate A s . Pour réduire le nombre d\u0027états de A s , pour chaque séquence s, nous considérons uniquement ses items fréquents dans SDB. La figure 1a montre un exemple d\u0027automate généré pour la troisième séquence de la Table 1. Modélisation de l\u0027extraction de motifs séquentiels. Soit minsup un seuil minimal de fré-quence. Ce problème est modélisé par les contraintes suivantes : -\nLes motif fermés sont les motifs maximaux des classes d\u0027équiva-lence des motifs partageant la même fréquence. Dans notre modélisation, un motif maximal est un motif ayant le plus petit nombre de variables P i instanciées à EOS. Ce problème est formulé sous forme d\u0027une contrainte de minimisation sur la taille des motif : (1) pour chaque variable P i , nous posons une fonction de coût unaire c i tel que c i \u003d 1 si P i \u003d EOS ; 0 sinon ; (2) minimiser la fonction c(p) \u003d Pi?p c i . Ainsi, le calcul des fermés se ramène à une contrainte de minimisation sur la taille des motifs et sur l\u0027ensemble de tous les motifs fréquents (contraintes 1 et 2). Enfin, à chaque fois qu\u0027un motif fréquent est prouvé fermé, une contrainte est ajoutée dynamiquement pour interdire de redécouvrir une nouvelle fois ce motif. Contrainte de Gap. Pour modéliser la contrainte gap [M, N ], il suffit de modifier la construction de l\u0027automate A s de telle sorte à ne garder que les transitions respectant la contrainte de gap. Soit A ). La figure 1b montre le nouvel automate obtenu à partir de celui de la figure 1a avec un gap [1,1].\nÉxperimentations\nDes expérimentations ont été réalisées dans le cadre d\u0027une application de fouille de textes visant à découvrir des relations entre des gènes et des maladies rares (MR) dans les textes biomédicaux. La fouille de séquences a pour objectif d\u0027extraire des motifs séquentiels utilisés comme patrons linguistiques. Cette application est détaillée dans (Béchet et al., 2012). Nous avons testé différentes tailles de corpus (de 50 à 500 phrases). Nous rapportons le nombre de motifs fermés extraits et les temps CPU (en secondes) pour les extraire. Nous indiquons entre parenthèses le nombre de motifs extraits lorsque la résolution n\u0027a pas terminé au bout de 10 heures de calcul. Toutes les expériences ont été menées sur un processeur AMD Opteron 2, 1 GHz et une mémoire vive de 256 GO, en utilisant la bibliothèque toulbar2 2 . B) Résultats. Des résultats de la table 2, nous pouvons dresser les remarques suivantes. i) Correction et complétude. Notre approche calcule l\u0027ensemble correct et complet de motifs séquentiels. Nous avons comparé les motifs séquentiels extraits par notre approche avec ceux trouvés par (Béchet et al., 2012), et les deux approches renvoient le même ensemble de motifs. ii) Pertinence des motifs extraits. Notre approche a permis d\u0027extraire plusieurs motifs linguistiques pertinents. De tels motifs permettent de mettre en évidence l\u0027expression des relations linguistiques entre gène et MR, comme par exemple, ces deux motifs traduisant une notion de causalité : (be) (cause) (by) (mutation) (in) (the) (GEN E) et (be) (dominant) (f requently) (cause) (by) (GEN E) (gene) iii) Temps CPU. Le temps d\u0027exécution de notre approche augmente en fonction de la taille du corpus. Toutefois, pour les corpus de grande taille (? 200) et pour des valeurs de minsup ? 2%, notre approche ne parvient pas à terminer l\u0027extraction de tous les motifs fermés dans un délai de 10 heures. En effet, l\u0027espace de recherche augmente drastiquement et le solveur passe beaucoup plus de temps pour trouver la première solution. Enfin, en raison du caractère gé-nérique de notre approche, il est très difficile de rivaliser et de se comparer avec les meilleurs algorithmes de fouille développés pour quelques contraintes. À l\u0027opposé, nous pouvons combiner de manière très élégante et déclarative plusieurs contraintes de nature diverse, ce qui constitue un point important pour l\u0027extraction de motifs pertinents.\nConclusion\nNous avons proposé dans cet article une nouvelle approche croisant des techniques de programmation par contraintes et de fouille pour l\u0027extraction de motifs séquentiels. Notre modèle offre un cadre générique et déclaratif pour modéliser et résoudre des contraintes de nature hé-térogène. La faisabilité de notre approche a été mis en évidence par des expériences sur une étude de cas pour la découverte de relations gène-MR à partir d\u0027articles PubMed.\n"
  },
  {
    "id": "371",
    "text": "Introduction\nL\u0027essor du m-learning, favorisé par le développement continu des nouvelles technologies mobiles pousse à l\u0027évolution des méthodes d\u0027apprentissage pour s\u0027adapter à ce nouveau type d\u0027apprentissage. Dans le cadre de l\u0027apprentissage au sein des entreprises, nous cherchons à développer un système m-learning dont les principaux enjeux sont : (1) l\u0027apprentissage au travail quel que soit l\u0027heure, le lieu, le dispositif de délivrance, les contraintes technologiques des processus d\u0027apprentissage et adapté au profil de l\u0027apprenant ; (2) l\u0027apprentissage sans rupture au travers des différents contextes. Nous proposons une approche pour un système mlearning contextuel et adaptatif intégrant des stratégies de recommandation de scénarios de formations sans risque de rupture. Dans l\u0027objectif de développer un tel système, nous commençons par identifier différents niveaux d\u0027hétérogénéité : hétérogénéité sémantique et hétérogé-néité d\u0027usage. Hétérogénéité sémantique : Les ressources sont conçues et développées par des organisations et des formateurs différents, constituant généralement des contenus d\u0027apprentissage autonomes mais aussi hétérogènes au niveau sémantique. Ainsi, des conflits surviennent puisque les systèmes n\u0027utilisent pas la même interprétation de l\u0027information. Les besoins immédiats demandent l\u0027application de standards en vigueur pour rendre les contenus d\u0027apprentissage réutilisables pour assurer l\u0027interopérabilité des plateformes e-learning hétérogènes. \nOntologie du m-learning\nUn contenu d\u0027apprentissage est une instanciation d\u0027objets pédagogiques, ou LOs (Learning Objects). IEEE définit un LO comme \"toute entité, sur un support numérique ou non, pouvant être utilisée, réutilisée et référencée au cours d\u0027un processus de formation\". L\u0027idée fondamentale derrière la création des LOs est la possibilité de construire un parcours de formation autour de composants de petite taille qui peuvent être sélectionnés, combinés avec d\u0027autres LOs et réutilisés selon les besoins des apprenants dans différents contextes d\u0027apprentissage (Abel, 2007). Seulement ces LOs sont souvent conçus et développés par des organisations et des auteurs différents constituant des contenus autonomes et sémantiquement hétérogènes. Il est alors indispensable de penser à une modélisation partagée des LOs en vue de les rendre facilement accessibles, exploitables, réutilisables et interopérables. Différentes normes ont été définies pour aider à l\u0027élaboration de systèmes d\u0027apprentissage, des LOs associés, leur représentation et leur interrelation. L\u0027application de ces normes, garantit l\u0027interopérabilité et la qualité du système. Parmi ces normes, nous citons LOM (Learning Object Metadata) (lom) qui s\u0027intéresse à la description des contenus d\u0027apprentissage. Il définit la structure d\u0027une instance de métadon-nées pour la description d\u0027un LOs. Il est constitué d\u0027un ensemble de 80 éléments divisés en 9 catégories accomplissant chacune une fonction différente. Afin d\u0027implémenter les différents descripteurs de LOM, une modélisation dans un langage structuré est nécessaire. La représen-tation du modèle abstrait de LOM dans un format spécifique est appelé binding. Aujourd\u0027hui il existe 2 bindings du schéma LOM : le binding XML et le binding RDF. Le binding XML est facile à implémenter, cependant il reste insuffisant pour la représentation de tous les élé-ments de LOM puisqu\u0027il ne permet pas d\u0027exprimer la sémantique de ces éléments. Le binding RDF définit un ensemble de constructions RDF qui facilitent l\u0027introduction des métadonnées de LOM dans le web, et il est complété par RDFS pour la définition des classes, des propriétés, etc. L\u0027avantage de ce deuxième type de binding c\u0027est qu\u0027il rajoute de la sémantique aux élé-ments de LOM, sauf qu\u0027il n\u0027est pas assez expressif pour définir toutes les contraintes de LOM. Ce manque d\u0027expressivité nous mène à penser à l\u0027utilisation d\u0027un autre formaliste plus puissant : OWL. Utiliser une ontologie OWL du LOM pour indexer les ressources pédagogiques permet une meilleure compréhension des éléments et des valeurs proposées et en conséquence faciliter leurs descriptions. Nous appelons cette ontologie \"modèle des Learning Objects\". L\u0027informatique sensible au contexte fait référence à des systèmes capables de percevoir un ensemble de conditions d\u0027utilisation, le contexte, afin d\u0027adapter en conséquence leur comportement en termes de délivrance d\u0027informations et de services (Schilit et Theimer, 1994). Le contexte d\u0027apprentissage est un aspect crucial en m-learning afin de déterminer selon le F. Soualah Alila et al.\ncontexte quelles ressources à envoyer, de quelle manière, à quel moment, sur quelle interface, etc. Pour bien comprendre et appliquer cette sensibilité au contexte, il est plus simple de passer par une catégorisation des variables du contexte. Selon (Schilit et Theimer, 1994) le contexte se décompose en trois sous classes où chacune des variables répond à l\u0027une des questions \"où suis-je ?\", \"avec qui suis-je ?\", \"Quelles sont les ressources de mon environnement proche ?\". (Ryan et al., 1998) catégorisent le contexte en identité de l\u0027utilisateur, ressources de l\u0027environnement proche, localisation de l\u0027utilisateur et période temporelle d\u0027exécution de l\u0027interaction. Ici, pour avoir une meilleure visibilité du contexte d\u0027apprentissage, nous proposons d\u0027organiser les données qui constituent ce dernier en quatre dimensions : dimension spatiale, dimension temporelle, dimension utilisateur et dimension device (Soualah Alila et al., 2013). La prochaine étape est de trouver un moyen de représenter le contexte. Cette représentation doit fournir un cadre cohérent pour mémoriser et traiter les informations du contexte pour ré-agir aux changements de l\u0027environnement. Il en résulte alors le \"modèle de contexte\". Il existe plusieurs méthodes de représentation du modèle contextuel (XML, UML, Topic maps), cependant aucun de ces modèles n\u0027assure l\u0027interopérabilité des données au niveau sémantique. De plus une représentation du contexte doit permettre d\u0027effectuer des raisonnements en vue d\u0027une adaptation. Nous soutenons qu\u0027une modélisation à base d\u0027un squelette ontologique est plus appropriée. Le modèle de contexte vient compléter le modèle des Learning Objects pour former ainsi une ontologie de domaine du m-learning. lon le moyen de transport, deux parcours de formation peuvent comporter des LOs différents. Tout comme le temps de parcours entre deux points varie en fonction du moyen de transport utilisé, le temps nécessaire pour parcourir un ensemble de briques de formations peut varier en fonction du support de diffusion. Enfin, la disponibilité de chaque support de formation varie dans le temps, tout comme la disponibilité des moyens de transport. Le problème général qui nous est posé est de proposer à un apprenant un panel de LOs correspondant à son contexte actuel et permettant d\u0027optimiser son expérience d\u0027apprentissage. Cette optimisation intervient sur différents plans : la minimisation de la durée de la formation et la maximisation du gain de compétences. Nous proposons de comparer l\u0027efficacité de certaines metaheuristiques de type recherche locale. Nous envisageons actuellement l\u0027implémentation d\u0027heuristiques simples (Hill-climbing, recherche locale orientée) et un peu plus complexes (Randomized Variable Neighborhood Search, Recuit Simulé).\nConclusion\nNous proposons une approche pour un système m-learning permettant aux formateurs de représenter leur savoir-faire en utilisant des règles métier et une ontologie pour assurer une hétérogénéité des connaissances. Ensuite, dans un environnement de mobilité, elle permet de prendre en compte les contraintes de l\u0027environnement et les contraintes utilisateur. Enfin, la partie métaheuristique de notre approche permet une combinaison dynamique de morceaux de la formation en fonction de ces contraintes.\n"
  },
  {
    "id": "372",
    "text": "Introduction\nLe clustering d\u0027un ensemble d\u0027objets en un nombre de groupes pré-déterminé est un problème souvent difficile suivant le critère d\u0027optimisation ou le modèle choisi. Le choix optimal du nombre de groupes (identifié de manière univoque par la variable k dans le reste de l\u0027article) l\u0027est probablement davantage. Le principe généralement accepté du rasoir d\u0027Occam, favorisant un nombre minimal de clusters, s\u0027oppose à leur exhaustivité, sans qu\u0027un compromis satisfaisant pour tous soit possible a priori. En pratique, ce paramètre est donc souvent laissé à la discrétion du praticien par les logiciels d\u0027analyse de données, même récents. Dans le cas d\u0027une approche exploratoire, où k peut être inconnu, une heuristique est souhaitable.\nDans cet article, nous nous limitons à l\u0027algorithme de clustering spectral, et proposons une nouvelle manière extrêmement simple, peu coûteuse, et bien fondée, d\u0027estimer k à partir du spectre de laplacien propre à cet algorithme. Le test de Bartlett pour l\u0027égalité des variances est utilisé depuis longtemps pour déterminer le nombre de facteurs à retenir dans le contexte d\u0027une Analyse en Composantes Principales (ACP) (James, 1969). Nous montrons qu\u0027il est possible de l\u0027adapter assez facilement pour estimer k dans le contexte de l\u0027algorithme de clustering spectral.\nDans un premier temps, nous rappelons l\u0027état de l\u0027art du clustering spectral, ainsi que des méthodes d\u0027estimation automatiques de k existantes. Nous décrivons ensuite notre méthode, in fine matérialisée par un algorithme simple. L\u0027efficacité de la méthode est illustrée par des expériences sur des données synthétiques et réelles de la littérature. L\u0027analyse critique de nos résultats nous permet de formuler quelques perspectives, données en conclusion.\nFondamentaux du clustering spectral\nLes bases du clustering spectral remontent à la théorie des graphes. Il a été popularisé par (Shi et Malik, 2000) et (Ng et al., 2001). Considérant une collection de N éléments, représentée par une matrice symétrique de similarités 1 entre couples d\u0027éléments S, l\u0027algorithme de clustering spectral en k groupes peut être résumé comme suit :  (Ng et al., 2001) :\nversion random walk (Shi et Malik, 2000) :\navec I la matrice identité de taille N . Remarquons que la multiplicité de la valeur propre 0 dans la décomposition spectrale de ces laplaciens peut être interprétée comme le nombre de composantes connexes du graphe sous-jacent (von Luxburg, 2006), i.e. le nombre de clusters que forment ses noeuds. Une autre variante notable de normalisation est Zelnik-Manor et Perona, 2004). Une implémentation R récente est d\u0027ailleurs basée sur cette dernière (Karatzoglou et al., 2013). En inspectant l\u0027équation (1), remarquons que L sym \u003d I ? L alt . Conséquemment, l\u0027adaptation de l\u0027algorithme 1 utilisant L alt considère les vecteurs propres majeurs, et relie k à la multiplicité de la valeur propre 1.\n3 État de l\u0027art sur la détermination du nombre de clusters\nLe lien entre le paramètre k de l\u0027algorithme 1 et la multiplicité de la valeur propre 0 dans le spectre du laplacien normalisé n\u0027est strictement valable que pour des composantes connexes. Les graphes considérés peuvent cependant contenir des composantes faiblement connectées entre elles, sans être totalement disjointes : par exemple, les similarités calculées via une Radial Basis Function (RBF) n\u0027égalent jamais exactement 0, induisant nécessairement une seule composante connexe. Le but de l\u0027algorithme est alors précisément d\u0027identifier cette structure.\nFIG. 1 -Profil des plus petites valeurs propres pour synth2 et synth1 (voir la section 5 pour une description).\nDans le reste du document, pour gérer la variabilité des jeux de données tant en termes de domaine que de distribution, nous utilisons la variante de RBF proposée par (Karatzoglou et al., 2013). Cette dernière adapte le rayon de la fonction à chaque élément selon la médiane de ses K plus proches voisins. Comme préconisé par les auteurs, nous avons retenu K \u003d 5 pour nos expériences, ainsi que pour le calcul des spectres présentés dans la figure 1.\nLa figure 1a montre que le profil des plus petites valeurs propres peut nous renseigner sur la probable valeur optimale de k. Intuitivement, une seule valeur propre égale exactement 0, k ? 1 autres sont approximativement égales à 0, et le reste est significativement supérieur à 0 : la meilleure valeur de k est ainsi marquée par la différence absolue entre la k La plupart des travaux de la littérature détermine l\u0027eigengap vraisemblable de manière empirique, soit en comparant les candidats à un seuil arbitraire, soit en analysant le taux de croissance du profil des valeurs propres via le scree test de Cattell (Cattell, 1966). La figure 1b illustre néanmoins que même dans des cas relativement simples a priori, l\u0027application de ce test peut être problématique. Une procédure d\u0027optimisation itérative a également été proposée, mais demeure complexe, tant du point de vue conceptuel que computationnel (Zelnik-Manor et Perona, 2004). Nous proposons une alternative simple et efficace, en adaptant le test de Bartlett pour l\u0027égalité des variances au clustering spectral. À l\u0027instar du scree test, il était originellement employé à déterminer le nombre de facteurs à extraire dans le contexte d\u0027une ACP (James, 1969).\nDescription de la méthode\nConsidérant un échantillon de N individus définis sur p variables, l\u0027ACP calcule les q facteurs représentatifs de la matrice de covariance de l\u0027échantillon, en faisant l\u0027hypothèse implicite que des échantillons uni-dimensionnels générés par n\u0027importe lequel des k \u003d p ? q facteurs restants doivent avoir une variance identiquement faible. Dans ces conditions, la statistique de test ci-après suit une loi du ? 2 (James, 1969) :\navec ? i la i ème valeur propre dans l\u0027ordre décroissant (conventionnel avec l\u0027ACP), ¯ ? k la moyenne des k valeurs propres mineures, et\n?j ). L\u0027algorithme 2 permet ainsi de trouver simplement la plus petite valeur de q acceptable. Cet algorithme est quadratique selon p. Comme la décomposition spectrale est elle-même cubique, le surcoût calculatoire est modeste.\nEntrée : Le vecteur des p valeurs propres, un niveau de risque ?, e.g. 5% Résultat : La plus petite valeur de q acceptable q ? 0 ; répéter q ? q + 1 ; s ? statistique de l\u0027équation (3) ; /* on contraint q \u003c p ? 1 car l\u0027équation (3) n\u0027est définie que pour k \u003e 1 */ jusqu\u0027à q \u003d p ? 2 ou P ? 2 (X \u003c s) ? 1 ? ?; /* on obtient le minimum de q ne menant pas au rejet de l\u0027hypothèse nulle */ Algorithme 2 : Un algorithme simple pour déterminer le nombre de facteurs de l\u0027ACP La détermination de k pour l\u0027algorithme de clustering spectral est analogue au problème du nombre de facteurs de l\u0027ACP : au lieu de rechercher les q plus grandes valeurs propres d\u0027une matrice de covariance, nous nous intéressons alors aux k plus petites valeurs du spectre d\u0027un laplacien (voir section 2). Il suffit alors d\u0027adapter l\u0027algorithme 2 à la recherche de la plus grande valeur acceptable pour k \u003d N ?q (en effet, N \u003d p pour un laplacien). Dans le contexte du clustering, k \u003c\u003c N : il est donc plus efficace de faire démarrer la recherche à k \u003d 2, i.e. initialiser q à p?2 dans l\u0027algorithme 2, et le décrémenter à chaque itération, avec une condition d\u0027arrêt adaptée.\nD\u0027autre part, nous avons constaté empiriquement qu\u0027avec k \u003c\u003c N , l\u0027ensemble de valeurs propres du laplacien normalisé {? i } i?q est très proche de 1 en moyenne : cela permet d\u0027approcher q\ndans l\u0027équation (3), menant à un critère ne dépendant que des k valeurs propres mineures. En entrelaçant les algorithmes 1 et 2, une extraction incré-mentale des valeurs propres depuis les plus petites peut alors être arrêtée précocément. Comme k \u003c\u003c N , nous obtenons ainsi un algorithme de clustering spectral quadratique selon N , incluant la détermination automatique de k.\nRésultats expérimentaux\nNous avons implémenté notre méthode sous la forme d\u0027un package R, speccalt 2 , i.e. une alternative à la fonction specc du package R kernlab. L\u0027interface, très simple, ne requiert qu\u0027une matrice de similarité ; le paramètre k, optionnel, est estimé automatiquement en cas d\u0027absence. Nous avons utilisé la normalisation de laplacien L alt , suggérée dans (Zelnik-Manor et Perona, 2004;Karatzoglou et al., 2013), plus stable en pratique pour réaliser le clustering. Toutefois, l\u0027algorithme 2 repose toujours sur L rw Jeu de données Notre État de l\u0027art indice de Rand (vérité terrain) méthode corrigé synth1(3) 3 4 ± 0,00 0,88 ± 0,18 synth2(3) 3 5 ± 0,00 0,97 ± 0,12 synth3(3) 3 3 ± 0,00 0,90 ± 0,21 synth4(5) 5 5 ± 0,00 0,76 ± 0,18 synth5(4) 4 4 ± 0,00 0,89 ± 0,21 synth6(3) 2 4 ± 0,00 0,58 ± 0,00 iris(3) 2 4 ± 0,00 0,54 ± 0,00 isolet ( . Comme l\u0027algorithme 1 est sensible aux minima locaux à travers sa dépendance à k-means, l\u0027indice de Rand corrigé est estimé par 20 exécutions indépendantes sur chaque jeu de données. Le même procédé est appliqué pour la méthode de (Zelnik-Manor et Perona, 2004), eu égard à sa nature itérative. En revanche l\u0027estimation de k par l\u0027algorithme 2 est déterministe. Ces résultats sont résumés dans la figure 2.\nNous constatons d\u0027abord que notre heuristique obtient de meilleurs résultats que la mé-thode de référence. Elle est satisfaisante dans les premiers cas, mais moins pour isolet, synth6, et iris (2 clusters découverts, contre respectivement 5, 3 et 3 d\u0027après la vérité terrain). Ceci est d\u0027ailleurs reflété par une nette dégradation des indices de Rand respectifs. La vérité terrain d\u0027isolet n\u0027est pas caractérisée par des frontières de décision tranchées, ce qui induit notre mé-thode à identifier le nombre minimal de clusters. Les cas de synth6 et iris sont plus subtils : en suivant exactement l\u0027algorithme 2, nous aurions identifié respectivement 62 et 29 clusters. En effet, notre méthode ne pénalise pas un nombre excessif de clusters, ou l\u0027existence de très petits clusters : chaque point du cercle peu dense de synth6 est ainsi identifié comme un cluster. La nature quasiment discrète d\u0027iris (i.e. toutes ses valeurs ont au plus une décimale) semble également problématique. Pour pallier ceci, notre implémentation de l\u0027algorithme 2 borne explicitement k par 20. Si 1 ? ? n\u0027est atteint pour aucune des valeurs autorisées, ce seuil est abaissé au plus grand quantile mesuré pour k ? [2, 20]. Par souci d\u0027équité, des bornes de valeurs de k identiques ont été imposées à la méthode de (Zelnik-Manor et Perona, 2004).\nConclusion\nDans cet article, nous avons proposé une méthode simple, peu coûteuse, et performante pour estimer automatiquement k dans le contexte du clustering spectral, ainsi que l\u0027attestent nos résultats expérimentaux. Toutefois, nous avons également identifié des limites à l\u0027approche, par sa focalisation exclusive sur la caractérisation de variétés dans les données.\nL\u0027algorithme spectral utilise k-means en tant qu\u0027étape intermédiaire : ce dernier, équivalent à un algorithme EM sur un mélange de gaussiennes isotropes, ouvre la voie à une possible combinaison de notre méthode avec une estimation bayésienne de k, par exemple en utilisant notre heuristique comme a priori.\n"
  },
  {
    "id": "373",
    "text": "Introduction\nDe nos jours, les réseaux sociaux ont connu une croissance importante. Sur Twitter ou sur Facebook la plus part des utilisateurs renseignent seulement 20% de leurs profils. La détection du profil peut être utilisée dans plusieurs domaines, par exemple du point de vue marketing, les entreprises peuvent être intéressées à déterminer quels types de personnes préfèrent leurs produits. Dans la littérature, beaucoup de travaux ont focalisé sur la classification d\u0027une conversation ou d\u0027un texte donné et plus précisément la détection de l\u0027âge de l\u0027auteur, de son genre, sa personnalité, sa langue native, etc. Argamon et al. (2009);Schler et al. (2006);Koppel et al. (2003);Pennebaker (2011).\nLes travaux réalisés par Koppel et al. (2003) ont montré qu\u0027au niveau du genre il y a des différences linguistiques entre les hommes et les femmes. En effet, les hommes qui préfèrent catégoriser les choses, utilisent plus de déterminants (le/la, cette/ce, un/une, etc.) et de quantificateurs (deux, plus, peu, etc.). Les femmes, s\u0027intéressent aux relations et plus que les hommes recourent aux pronoms personnels (je, tu, moi, etc.). La suite de ce papier est organisée comme suit, dans la section 2 nous présentons notre méthode d\u0027apprentissage en focalisant sur le choix des classes et l\u0027algorithme employé. La dernière section présente notre étude expérimentale. \nExpérimentation et évaluation\nNous avons utilisé les corpus discernés de la conférence CLEF 2 2013. Nous avons effectué l\u0027expérimentation avec un extrait du corpus d\u0027entrainement. En faite, pour la dimension genre, qui a comme moyenne (baseline) de précision 0.5, nous avons obtenu de bons résultats, 58,16% des documents ont été bien classés. Pour la dimension âge qui a comme moyenne 33% les résultats sont prometteurs et témoignent de l\u0027efficacité de la méthode. En effet, 57% des documents ont été bien classés. Comme le montre la figure 1, nous avons trouvé que la méthode d\u0027apprentissage fondé sur les arbres de décision donne de meilleurs résultats. \nConclusion\nNous avons effectué la catégorisation de documents en vue de fournir une classification de l\u0027auteur d\u0027un texte donné selon ses caractéristiques. Les résultats obtenus sont encourageants et surtout pour la dimension genre. La sélection manuelle du contenu des classes a montré ses limites face à des corpus de langues peu connues par le chercheur. L\u0027automatisation de cette tâche s\u0027avère d\u0027une grande utilité, et l\u0027utilisation de dictionnaires bilingues ou multilingues pourra faire face aux insuffisances linguistiques.\nIl s\u0027est avéré que l\u0027utilisation des classes lexicales à elle seule n\u0027est pas suffisante, cependant nous comptons intégrer d\u0027autres aspects comme l\u0027aspect syntaxique, morphologique, sémantique, etc. D\u0027un autre coté, pour pouvoir mieux effectuer la détection du profil de l\u0027auteur nous pensons s\u0027ouvrir sur d\u0027autres dimensions, à part l\u0027âge et le genre nous allons aborder aussi la détection de la langue native, la détection des données géographiques de l\u0027auteur et la détection du niveau linguistique, etc. \nSummary\nIn this paper, we present a method for profiling the author of an anonymous text. Our approach is based on learning the author profile with a focus on dimensions age and gender. First, we computed a ranked list of words that occur in the corpus and we grouped them into classes according to their similarities. Then, we calculated the CF (class frequency) score of each class for each document in order to find the stylistic differences between men and women, on the one hand, and those between different age intervals on the other hand. Our system has shown a high level of accuracy and effectiveness in treating the gender dimension.\n"
  },
  {
    "id": "374",
    "text": "Introduction\nUn réseau complexe est la représentation d\u0027un système complexe sous forme de graphe. Ils sont devenus très populaires en tant qu\u0027outil de modélisation durant la dernière décennie car ils permettent de mieux comprendre le fonctionnement et la dynamique de certains systèmes . Un réseau complexe ordinaire ne contient que des noeuds et les liens existant entre eux ; cependant il est possible de l\u0027enrichir avec différents types de données : orientation et/ou poids des liens, dimension temporelle, attributs associés aux noeuds ou aux liens, etc. Cette souplesse a permis d\u0027utiliser les réseaux complexes pour étudier les systèmes du monde réel dans de nombreux domaines : sociologie, physique, génétique, informatique, etc. (Newman, 2003).\nLa nature complexe des systèmes modélisés entraine la présence de propriétés topologiques non-triviales au sein des réseaux correspondants. Parmi celle-ci, la structure de communautés est l\u0027une des plus répandue et des plus étudiées. Informellement, on peut définir une communauté comme un groupe de noeuds densément interconnectés relativement au reste du réseau (Newman, 2003). Cependant, dans la littérature, cette notion est formalisée de très nombreuses différentes façons (Fortunato, 2010). Il existe en fait des centaines d\u0027algorithmes destinés à dé-tecter les structures de communautés, caractérisés par l\u0027utilisation d\u0027une définition et/ou d\u0027un traitement différents. Certains sont basés sur la mesure de modularité, une mesure similarité entre noeuds, le principe de compression des données, la notion de signification statistique, les mécanismes de diffusion de l\u0027information, la percolation de cliques, etc. (cf. (Fortunato, 2010) pour une revue détaillée). La plupart des méthodes existantes traitent des réseaux ordinaires mais de nouvelles méthodes apparaissent pour analyser les réseaux les plus riches en exploitant les directions et poids des liens, puis le temps, et plus récemment les attributs des noeuds (Ruan et al., 2013;Tian et al., 2008;Zhou et al., 2009). Ces dernières se concentrent sur la recherche de groupes de noeuds denses en termes de liens, et dont les attributs sont homogènes. Même si cela n\u0027est pas toujours indiqué explicitement, ces méthodes exploitant les attributs s\u0027appuient sur l\u0027hypothèse que les noeuds d\u0027une même communauté doivent être similaires en termes d\u0027attributs. Bien que les algorithmes diffèrent en termes de nature des communautés détectées, de complexité algorithmique, de qualité du résultat et d\u0027autres aspects (Fortunato, 2010), leur production peut toujours être essentiellement décrite comme une liste de groupes de noeuds. Plus précisément, dans le cas de communautés mutuellement exclusives, il s\u0027agit d\u0027une partition de l\u0027ensemble des noeuds. D\u0027un point de vue applicatif, la question est alors de donner un sens à ces groupes relativement au système étudié. L\u0027interprétation manuelle de petites communautés est possible, mais la méthode ne s\u0027applique pas bien à de très grands réseaux.\nSeuls quelques travaux ont essayé de s\u0027attaquer explicitement à ce problème. Dans (Lancichinetti et al., 2010), les communautés sont caractérisées en comparant les distributions de plusieurs mesures purement topologiques. Dans (Tumminello et al., 2011) et (Labatut et Balasque, 2012), les auteurs se concentrent sur les attributs nodaux, et identifient les plus représentatifs pour chaque communautés au moyen de divers outils statistiques classiques. Les méthodes de détection de communautés exploitant les attributs sont généralement aussi en mesure de donner ce type d\u0027information, car ces attributs sont identifiés pendant le processus de détection. Cependant, aucune des méthodes citées ne prend en compte toutes les données qu\u0027un réseau riche peut contenir (attributs, topologie et dimension temporelle), qui plus est de manière systématique. Il existe donc un besoin pour un tel procédé, qui permettrait de caractériser les communautés des réseaux complexes riches. Dans ce travail, nous proposons une solution à ce problème, sous la forme d\u0027une méthode d\u0027analyse des réseaux attribués dynamiques. Pour cela, nous détectons les changements communs dans les mesures topologiques et les valeurs d\u0027attribut sur une période de temps donnée. Plus précisément, nous cherchons à trouver les motifs séquentiels fréquents les plus représentatifs pour chaque communauté. Ces motifs peuvent ensuite être utilisées à la fois pour caractériser la communauté, et pour identifier ses anomalies, i.e. ses noeuds ayant un comportement non-standard. Les motifs fréquents représentent la tendance générale des noeuds dans la communauté considérée, alors que les anomalies peuvent correspondre à des noeuds ayant un rôle spécifique dans la communauté, ou situés à sa frontière. Nous illustrons notre méthode en l\u0027appliquant à un réseau dynamique de co-auteurs extrait de la base de données bibliographiques DBLP 1 . Notre première contribution est de considérer la caractérisation de communauté comme un problème spécifique, distinct de celui de la détection de la communauté. La méthode à appliquer doit être indépendante de la technique utilisée pour détecter les communautés, se fonder sur une approche systématique facilement reproductible, et être la plus automatisée possible. Notre deuxième contribution est l\u0027introduction d\u0027une nouvelle représentation des réseaux attribués dynamiques. Elle prend la forme d\u0027une base de données contenant des séquences de mesures topologiques, d\u0027attributs nodaux et d\u0027information communautaire, pour plusieurs tranches temporelles. Ce type de représentation avait précédemment été utilisé pour la repré-sentation de données naturelles (Mabroukeh et Ezeife, 2010), mais pas celle de graphes. Notre troisième contribution est la définition d\u0027une méthode basée sur l\u0027extraction sous contraintes de motifs séquentiels qui tire parti de cette représentation pour caractériser les communautés. Enfin, notre dernière contribution concerne une application à un réseau du monde réel. Dans la section suivante, nous donnons une description détaillée de notre méthode. Dans la Section 3, nous présentons nos résultats expérimentaux obtenus sur les données DBLP. La Section 4 décrit les travaux connexes, et la Section 5 présente les extensions possibles de notre travail.\nMéthode\nUn réseau dynamique attribué est constituée de différentes tranches temporelles, chacune représentée par un sous-réseau distinct, contenant les liens entre les noeuds pour un intervalle de temps donné. Ces tranches temporelles sont séquentielles. Habituellement, les noeuds et leurs attributs sont les mêmes pour chaque tranche, tandis que les liens entre eux et les valeurs des attributs peuvent changer. Nous proposons de caractériser les communautés d\u0027un réseau dynamique attribué en fonction de l\u0027évolution commune des mesures topologiques et des attributs de leurs noeuds. Le processus que nous proposons inclut 4 étapes. La première consiste à identifier une structure de communautés de référence. La seconde vise à créer la structure de données permettant une représentation séquentielle des mesures topologiques et des attributs des noeuds. Nous calculons d\u0027abord les valeurs de toutes les mesures topologiques sélection-nées, puis nous discrétisons les valeurs des mesures et des attributs. Lors de la troisième étape, nous recherchons des motifs séquentiels fréquents et nous extrayons les noeuds qui supportent chaque motif. La quatrième étape consiste à choisir les motifs les plus représentatifs pour caractériser les communautés selon différents critères.\nDétection des communautés. Pour détecter comment les noeuds évoluent en fonction de l\u0027appartenance communautaire, nous avons d\u0027abord besoin d\u0027une structure de communautés de référence. Il serait possible d\u0027appliquer une méthode dynamique, cependant cela entraîne des complications dues aux fusions, séparations, disparitions et apparitions de communautés au cours du temps. Pour cette raison, dans ce premier travail, nous avons décidé d\u0027utiliser des communautés statiques, détectés sur une version intégrée du réseau.\nNous créons d\u0027abord un nouveau réseau en agrégeant tous les liens dans le temps. Un poids est attribué à chaque lien en fonction de son nombre d\u0027occurrences, afin de représenter sa stabilité dans le temps. Nous appliquons ensuite un algorithme de détection de communautés classique pour identifier nos communautés statiques. À cette fin, nous avons sélectionné Louvain (Blondel et al., 2008), qui est une méthode reconnue pour obtenir des structures de communautés de bonne qualité. La complexité temporelle de cette algorithme est O(n log n) où n est nombre de noeuds. La structure de communautés en résultant est utilisée dans le reste de notre analyse. Bien que les communautés soient statiques, les changements dans la structure du réseau seront néanmoins considérés lors du traitement des mesures topologiques des noeuds.\nConstitution de la base de données. La deuxième étape consiste à représenter le réseau d\u0027une manière appropriée pour l\u0027extraction des motifs séquentiels fréquents. Un réseau dynamique attribué G \u003d 1 , . . . , G ? est constitué d\u0027une séquence de tranches temporelles, chacune prenant la forme d\u0027un réseau attribué séparé G j (1 ? j ? ?), représentant une période de temps donnée. Chaque réseau statique G j contient n noeuds et correspond aux connexions entre ces noeuds pour la période j. Les noeuds et leurs attributs disponibles sont supposés être statiques, c\u0027est à dire rester les mêmes pour tout G j . Au contraire, les valeurs des attributs et la structure du réseau peuvent changer au cours du temps. Un descripteur de noeud est soit un attribut nodal, soit une mesure topologique nodale.\nNous avons sélectionné les mesures topologiques nodales les plus répandues (degré, transitivité locale), ainsi que certaines mesures permettant de caractériser les noeuds en termes de position dans leur communauté (degré interne, coefficient de participation et enchâssement). Le degré d d\u0027un noeud est son nombre total des voisins directs et sob degré interne d int est le degré calculé dans sa seule communauté. La transitivité locale, T \u003d ? 1)), d\u0027un noeud est la densité des triangles auxquels il appartient. Plus précisément, il s\u0027agit du rapport entre le nombre de triangles auxquels le noeud appartient effectivement ( et le nombre maximal de triangles auxquels il pourrait appartenir, étant donné son degré (dénominateur). Le degré interne normalisé z et le coefficient de participation P sont deux mesures définies par Guimerá et Amaral (2005). La première exprime combien le noeud est connecté à sa propre communauté, relativement aux autres noeuds de sa communauté. Il s\u0027agit du z-score de son degré interne, i.e. du nombre de voisins directs dans sa propre communauté. Un hub au sens de Guimerá et Amaral (2005) est un noeud dont le degré interne est élevé relativement au autres noeuds de sa communauté. La seconde mesure caractérise la distribution des voisins du noeud parmi toutes les communautés :\n, où d c est le nombre de liens entre le noeud considéré et la communauté c. Il s\u0027agit plus précisément de mesurer l\u0027hétérogénéité de cette distribution. On obtient une valeur proche de 1 si tous les voisins sont répartis uniformément entre toutes les communautés, et 0 s\u0027ils sont tous réunis dans la même communauté. L\u0027enchâssement, e \u003d d int /d, évalue à quel point les voisins d\u0027un noeud appartiennent à sa propre communauté (Lancichinetti et al., 2010). À la différence du degré interne normalisé, l\u0027enchâssement est donc normalisé par rapport au noeud, et non pas à sa communauté. Toutes ces mesures topologiques peuvent être calculées en un temps linéaire par rapport à n.\nToutes nos mesures topologiques sont à valeurs réelles, nous avons donc dû les discrétiser pour les adapter à cette définition. Un\ndescripteur pour la tranche temporelle j. Une base de données M est l\u0027ensemble des tuples (?, ? ? , C ? ) où ? est l\u0027identifiant du noeud, ? ? est sa séquence et C ? est l\u0027étiquette de sa communauté (tels que définis à l\u0027étape 1). La création de M revient à réaliser séquentiellement le calcul des mesures topologiques et la détection de communautés, donc sa complexité temporelle est aussi O(n log n).\nFouille de motifs séquentiels émergents. La troisième étape a pour but de trouver des motifs séquentiels émergents permettant de caractériser les communautés. Pour chaque communauté, nous recherchons les motifs séquentiels supportés par la majorité de ses membres. Avant de présenter la façon dont nous traitons la fouille, nous donnons quelques définitions nécessaires à la bonne compréhension de l\u0027algorithme que nous avons utilisé.\nUn itemset T est un sous-ensemble de I. Une séquence s \u003d 1 , . . . , t m est une liste d\u0027itemsets ordonnés dans le temps, où m est la longueur de la séquence. Une séquence ? \u003d 1 , . . . , a m est une sous-séquence d\u0027une autre séquence\n. . , a m b im On dit également que ? est une super-séquence de ?. La taille d\u0027une communauté C est le nombre total de noeuds qu\u0027elle contient. Le support d\u0027une séquence s pour une communauté donnée C est le rapport du nombre de noeuds supportant s à la taille de C : sup(s, C) \u003d |{? ? C|s ? ? }|/|C|. Le taux de croissance d\u0027une séquence s pour une communauté donnée C est le rapport du support de s dans C au support de s dans C, où C est le complémentaire de C dans le réseau (i.e. tous les noeuds du réseau sauf ceux de C) : Gr(s, C) \u003d sup(s, C)/sup(s, C). Le taux de croissance mesure l\u0027émergence de la séquence s dans la communauté C. Plus il est élevé, et plus la séquence s est caractéristique de la communauté C.\nCompte tenu d\u0027un seuil de support minimal min sup , un motif séquentiel fréquent est une séquence dont le support est supérieur ou égal à min sup . Un motif séquentiel fréquent fermé pour une communauté donnée est un motif séquentiel qui n\u0027a pas de super-séquence pour le support minimal spécifié. Dans notre problème, nous caractérisons chaque communauté selon le motif séquentiel fréquent fermé. La méthode Clospan (Yan et al., 2003) a été définie pour identifier tous les motifs séquentiels fermés existants, pour un seuil donné, avec une complexité en O(n 2 ). Cependant, nous voulons que les motifs séquentiels identifiés soient représentatifs des communautés dans lesquelles ils sont identifiés. Il est donc nécessaire de prendre en compte le taux de croissance des motifs afin de tenir compte de leur émergence. Pour ce faire, nous appliquons la méthode de post-traitement définie dans (Plantevit et Cremilleux, 2009) pour calculer les taux de croissance de séquences d\u0027article classés. Au final, notre approche est donc la suivante : d\u0027abord nous identifions les motifs séquentiels fermés d\u0027une communauté pour un support minimum donné. Puis, nous calculons les supports de ces motifs pour le reste du réseau entier. Enfin, nous en déduisons le taux de croissance de chacun des motifs séquentiels fermés.\nSélection des motifs séquentiels. Une fois que les motifs séquentiels ont été extraits pour chaque communauté comme expliqué à l\u0027étape 3, nous devons sélectionner les plus représen-tatifs afin de caractériser la communauté. Pour cela, nous extrayons d\u0027abord les noeuds supportant chaque motif. Dans un premier temps, nous avons décidé d\u0027effectuer ce calcul de façon naive, ce qui entraine une complexité en O(rn), où r est le nombre de motifs. Mais il faut souligner que ce calcul pourrait être accéléré en l\u0027intégrant à Clospan. On choisit ensuite les motifs selon deux approches distinctes : (1) motif dont le support est le plus élevé et (2) motif dont le taux de croissance est le plus élevé. Les résultats de l\u0027étape 3 nous donnent directement le support et le taux de croissance. Cependant, dans certaines communautés, le motif de taux de croissance maximal ne couvre pas une partie significative des noeuds constituant la communauté. Il est alors nécessaire d\u0027identifier d\u0027autres motifs, dans un souci de représentativité. Nous sélectionnons alors le motif dont l\u0027ensemble des noeuds le supportant est le plus différent possible de celui du premier motif, afin de couvrir au maximum la partie de la communauté ignorée par celui-ci. La distance entre les ensembles des noeuds supportant est calculée au moyen du coefficient de Jaccard. En cas d\u0027égalité, nous utilisons le taux de croissance comme critère secondaire. Si la couverture totale est toujours insuffisante, on réitère en sélectionnant d\u0027autres motifs selon les mêmes modalités. Cette itération se poursuit jusqu\u0027à ce qu\u0027au plus cinq noeuds ne soient pas couverts. Nous qualifions de déviant ces noeuds ne supportant aucun motif représentatif de la communauté. Comme nous devons comparer chaque paire de motifs pour chaque communauté, le temps nécessaire à la sélection des motifs représentatifs est de l\u0027ordre de O(r 2 /p) où p est le nombre de communautés. La complexité totale de notre mé-thode est donc O(n 2 + r 2 /p). Il faut noter que r dépend grandement des nombres de tranches temporelles ? et d\u0027items |I|.\nRésultats\nNous présentons les expérimentations réalisées sur des données réelles. Nous avons choisi de traiter le réseau dynamique attribué de co-auteurs décrit dans (Desmier et al., 2012) et extrait de la base de données DBLP. Chacun des 2145 noeuds représente un auteur. Deux noeuds sont reliés si les auteurs correspondants ont publié au moins un article ensemble. Chaque tranche temporelle correspond à une période de 5 ans. Il y a au total 10 tranches temporelles allant de 1990 à 2012. Les périodes consécutives ont un chevauchement de 3 ans pour des raisons de stabilité. Pour chaque auteur, à chaque tranche temporelle, la base de données fournit le nombre de publications pour 43 conférences et journaux. Nous utilisons ces informations pour définir les 43 attributs nodaux correspondants, et nous en rajoutons deux : le nombre total de publications dans les conférences et dans les journaux. On a donc un total de 45 attributs. Nos descripteurs sont ces attributs, ainsi que les mesures topologiques décrites à la Section 2.\nLes mesures topologiques sont discrétisées différemment, en fonction de leur nature. Pour le degré, nous utilisons les seuils 3, 10 et 30. Pour la transitivité, qui est définie sur [0; 1], il s\u0027agit de 0, 35, 0, 5 et 0, 7. Pour l\u0027enchâssement, lui aussi défini sur [0; 1], les intervalles sont 0, 3 et 0, 7. Ces intervalles ont été déterminés de manière à tenir compte des distributions de ces mesures sur l\u0027ensemble des noeuds et des tranches temporelles : les différents seuils correspondent aux zones de faible densité. Pour les mesures de Guimerá et Amaral (2005), nous utilisons les seuils définis dans l\u0027article original, c\u0027est-à-dire : 2, 5 pour z et 0, 05, 0, 6, et 0, 8 pour P . Le seuil utilisé pour z permet de distinguer les hubs (z \u003e 2, 5) des non-hubs (z ? 2, 5) communautaires. Pour les deux attributs relatifs aux nombres de publications dans des conférences et revues, nous considérons les valeurs 1, 2, 3, 4 et \u003e 5. Pour le nombre total de publications, nous avons identifié les seuils 5, 10, 20 et 50 comme étant les plus pertinents.\nL\u0027algorithme Louvain identifie 127 communautés dans le réseau pondéré global, pour une modularité de 0, 59. Cette valeur signifie que ce réseau est hautement modulaire. 96 de ces communautés ne contiennent qu\u0027un seul noeud. Parmi les communautés restantes, 17 contiennent plus de 10 noeuds, la plus grande en ayant 335. Nous recherchons ensuite les motifs séquentiels de ces communautés, pour un support minimum de 0, 3. Il ne nous a pas été possible de descendre en dessous de cette valeur en raison du coût spatial de l\u0027algorithme Clospan utilisé. Pour chacune des communautés dont la taille est supérieure à 40, nous détections plus de 5000 motifs, dont la plupart ne comprennent que des mesures topologiques.\nLes motifs les plus supportés sont toujours une séquence de z \u003c 2, 5 pour toutes les communautés, avec des longueurs variables. Cela signifie que la majorité des noeuds de chaque TAB. 1 -Longueur des motifs les plus supportés pour une sélection de communautés.\ncommunauté ont un rôle de non-hub communautaire. Pour mémoire, Amaral \u0026 Guimerà défi-nissent un hub communautaire comme un noeud dont le degré interne est largement supérieur au degré interne moyen de sa communauté. Ainsi, le motif détecté signifie que la majorité des noeuds ont un degré interne relativement faible (ce qui est attendu), et ce durablement (ce qui ne l\u0027est pas forcément). Bien que ce type de motif soit présent dans toutes les communautés, on peut faire une distinction en considérant la longueur de la séquence, qui mesure cette durée. Dans le Tableau 1, nous listons la longueur des motifs séquentiels les plus supportés, en indiquant leur communauté, la taille de celle-ci et la valeur de support du motif. Les communautés dont les tailles sont comprises entre 39 et 45 (communautés 40, 55 et 77 ) obtiennent de longues séquences (resp. 8, 7 et 7 ). Surtout, les supports pour les communautés 55 et 77 atteignent même la valeur maximale de 1. Cela signifie que, dans ces communautés, aucun hub n\u0027existe ; ou bien si un hub apparait, il disparait rapidement. Cette observation est particulièrement intéressante, et traduit l\u0027absence d\u0027un meneur communautaire qui structurerait la communauté par ses connexions multiples. Pour la communauté 115, la longueur de la sé-quence est 1 et sa valeur de support est également 1. Cela signifie que tous les noeuds de cette communauté ont tenu le rôle de non-hub simultanément au moins une fois au cours du temps, mais qu\u0027il existe des hubs pour le reste des tranches temporelles. Pour les communautés 38, 40 et 75, le support est inférieur à 1, ce qui signifie que si une écrasante majorité de noeuds tient le rôle de non-hub pour de longue durées, en revanche un petit nombre de noeuds occupe la place de hub, éventuellement par intermittence. Nous extrayons les auteurs qui ne suivent pas les motifs les plus supportés pour ces trois dernières communautés. Pour la communauté 38, il s\u0027agit de Philip S. Yu, Jiawei Han et Beng C. Ooi. Comme supposé, ces noeuds ont un nombre de connexions remarquablement élevé à l\u0027intérieur de leurs communautés, et les auteurs qu\u0027ils représentent ont effectivement des rôles de meneurs dans leur domaine. Une analyse plus approfondie des données montre également qu\u0027ils publient un total de plus de 10 articles par tranche temporelle. En outre, ils n\u0027occupent jamais de rôle non-hub. Les noeuds déviants pour les communautés 40 et 75 sont respectivement Hans-Peter Kriegel et Divesh Srivastava. Là aussi, il s\u0027agit d\u0027auteurs importants dans leur communauté. Les séquences qui les caractérisent confirment qu\u0027ils sont productifs et n\u0027occupent jamais le rôle non-hub au fil du temps.\nPour les communautés dont les tailles sont comprises entre 39 et 45, nous ne constatons aucune motif émergent contenant une conférence ou revue. Les motifs les plus émergents ont un taux de croissance supérieur à 1, 79, ce qui signifie qu\u0027il n\u0027existe pas de motif séquentiel très distinctif pour ces communautés. Pour la plupart des grandes communautés, le motif le plus émergent inclut une conférence ou une revue spécifique, ce qui peut s\u0027interpréter en termes de thématique de la communauté. Les autres descripteurs constituant le motif sont des mesures topologiques. Comme pour les motifs les plus supportés, l\u0027item z \u003c 2, 5 apparait le plus souvent dans les motifs détectés. Cependant, ces motifs les plus émergents ne permettent pas de couvrir la majorité des noeuds des communautés concernées. C\u0027est pourquoi, comme nous l\u0027avons expliqué dans la section 2, nous recherchons des motifs supplémentaires en minimisant l\u0027intersection de leurs supports. Ces motifs sont généralement constitués de mesures topologiques, et n\u0027ont pas un taux de croissance très élevé. Dans la suite, nous nous concentrons sur les communautés donnant les résultats les plus intéressants. Pour chacune, nous décrivons le motif le plus émergent et nous présentons les noeuds déviants, qui ne supportent ni le motif le plus émergent ni les motifs supplémentaires. Chaque motif est représenté formellement entre crochets, comme une séquence d\u0027itemsets, eux-mêmes représentés entre parenthèses.\nPour la communauté 61, le motif le plus émergent est \u003c(ICML PUB. NUM \u003d 1) (DEGRE 3-10, Z \u003c 2,5)\u003e avec un taux de croissance 3, 52 et support 0, 30. Ce motif fait référence aux auteurs qui sont publiés une fois dans ICML, après quoi leur degré se stabilise entre 3 et 10 et ils occupent tous le rôle de non-hubs. Nous extrayons 7 motifs supplémentaires afin de couvrir tous les noeuds de la communauté. Parmi eux, les plus intéressants sont \u003c(Z \u003c 2,5) (Z \u003c 2,5) (Z \u003c 2,5, TOTALE CONF PUB. NUM 1-5) (AAAI PUB. NUM \u003d 1)\u003e avec un taux de croissance de 1, 69 et un support de 0, 30 et \u003c(PART. COEFF 0.05-0.6, CIKM PUB. NUM \u003d 1)\u003e avec un taux de croissance de 1, 40 et un support de 0, 30. Le premier motif se réfère à des noeuds qui restent des noeuds non-hubs pendant un certain temps, puis commencent à publier dans des conférences, avant de publier à AAAI tout en perdant leur statut de non-hub (sans pour autant devenir massivement des hubs). Le second n\u0027a pas de dimension temporelle, mais nous montre l\u0027existence ponctuelle de noeuds publiant à CIKM tout en occupant une position périphérique dans leur communauté, i.e., en étant significativement connecté à d\u0027autres communautés. Les noeuds déviants pour cette communauté sont Alex A. Freitas, Claire Cardie , Edwin P. D. Pednault. Parmi ces auteurs, Alex A. Freitas ne publie pas pendant les 8 premières tranches temporelles, puis commence à publier de manière très active, à différentes conférences telles qu\u0027ICML ou AAAI, et dans des journaux. Pour les deux autres auteurs, tandis que Claire Cardie publie régulièrement à ICML au cours des 6 premières tranches temporelles, Edwin P. D. Pednault ne publie ni à ICML, ni AAAI ou CIKM .\nLe motif \u003c(PODS PUB. NUM \u003d 1)\u003e est le plus émergent dans la communauté 75. Son taux de croissance est 3, 59 et son support 0, 40. Ce motif montre que %40 des auteurs de cette communauté publie au moins une fois à PODS et ce groupe est émergent par rapport au reste du réseaux pour cette communauté. Quatre motifs supplémentaires sont nécessaires pour couvrir le reste de la communauté. Ces motifs se réfèrent à des noeuds occupant des positions de nonhub périphérique, et dont la transitivité est très élevée. Les noeuds déviants sont Ninghui Li, Li Feifei et Abdullah Mueen, qui ont la particularité de ne jamais publier dans PODS. Le motif le plus émergent de la communauté 106 est \u003c(Z \u003c 2,5) (Z \u003c 2,5) (Z \u003c 2,5) (Z \u003c 2,5) (Z \u003c 2,5) (PART. COEFF 0.05-0.6, KDD PUB. NUM\u003d1)\u003e avec un taux de croissance 2, 87 et support 0, 40. Ce motif fait référence à des noeuds durablement non-hubs, qui deviennent périphériques tout en publiant à KDD. Nous identifions 4 motifs supplémentaires pour finir de couvrir la communauté. Ceux-ci concernent les noeuds à la fois ultrapériphériques et bien intégrés à leur propre communauté. Les noeuds déviants sont Stan Matwin, qui publie plus d\u0027un article par tranche temporelle à KDD, et qui n\u0027est pas durablement non-hub ; et Hua-Jun Zeng qui n\u0027a jamais publié dans KDD. Ce dernier est aussi caractérisé par un fort accroissement du nombre d\u0027articles produits, alors qu\u0027il ne publie pas pendant les 5 premières tranches temporelles.\nLe motif le plus émergent de la communauté 45 est \u003c(VLDB PUB. NUM\u003d3) (DEGRE 3-10 Z \u003c 2,5)\u003e avec un taux de croissance de 6, 40 et un support de 0, 30. Cette séquence nous dit qu\u0027il y a un groupe remarquable d\u0027auteurs qui ont publié 3 fois en conférence VLDB, puis dont le degré s\u0027est stabilisé entre 3 et 10 et qui ont occupé des positions non-hubs dans leur communauté. Nous avons dû identifier 6 autres motifs pour couvrir le reste de la communauté. L\u0027un d\u0027eux est \u003c(Z \u003c 2,5, TOTAL CONF. PUB. NUM 1-5) (Z \u003c 2,5, EMBED 0.3-0.7, ICDE PUB. NUM\u003d 1)\u003e, avec un taux de croissance de 2, 30 et un support de 0, 30. Ce motif couvre les noeuds non-hubs qui ont publié entre 1 et 5 fois dans des conférences, puis dont l\u0027enchâsse-ment s\u0027est stabilisé à une valeur relativement élevée, tout en gardant leur position de non-hub et en publiant à ICDE . Les noeuds déviants sont Ingmar Weber et Anastasia Ailamaki, qui ne publient pas pour les sept premières tranches temporelles, puis deviennent de plus en plus productifs au cours des trois dernières tranches.\nPour résumer nos observations, le motif le plus émergent dans à peu près toutes les communautés comprend habituellement le fait d\u0027être non-hub et d\u0027avoir un petit nombre de publications dans divers conférence ou journaux. En fonction des quelques conférences ou journaux apparaissant dans ces motifs, il est possible de déduire le thème principal des communautés. Pour certaines communautés, cependant, les motifs séquentiels émergents sont purement topologiques (pas d\u0027attributs). On peut alors supposer que les membres de ces communautés ne publient pas de façon suffisamment homogène pour que cela transparaisse dans les motifs. Une autre raison peut être simplement que les membres de la communauté sont reliés pour des raisons autres que thématiques, auquel cas cela n\u0027apparait pas dans les attributs sélectionnés pour notre étude. En ce qui concerne les noeuds déviants, on peut distinguer différents types de profils. Certains peuvent correspondre à des auteurs dont la thématique principale est diffé-rente de celle de la communauté dans laquelle ils ont été placés. Dans certains cas, nous avons détecté des auteurs qui avaient visiblement changé de thématique, ou bien qui débutaient dans une thématique donnée. Il peut également s\u0027agir d\u0027auteurs actifs dans un autre domaine, dont les conférences et journaux ne font pas partie de ceux retenus dans les données que nous avons considérées ici. Un autre profil est celui du chercheur en train de monter en charge, et dont la position communautaire et le nombre de publications sont en train d\u0027évoluer de conjointement.\nTravaux connexes\nIl existe de nombreuses méthodes de détection de communautés dans les réseaux complexes ordinaires (i.e., ne contenant que des noeuds et des liens), basées uniquement sur l\u0027information topologique. Une revue détaillée de ces méthodes est donnée dans (Fortunato, 2010). Dans le cas des réseaux plus riches, contenant des attributs nodaux, une communauté est essentiellement définie comme un groupe de noeuds à la fois densément interconnectés, et similaires en termes d\u0027attributs. Certaines méthodes tentent d\u0027estimer des partitions optimisant directement ces deux critères simultanément (Cruz et al., 2011). D\u0027autres transforment encodent d\u0027abord les attributs sous forme d\u0027information structurelle, et détectent ensuite les communautés de façon plus classique (Ruan et al., 2013;Zhou et al., 2009). Il existe également des méthodes de recherche de motifs (ici de petits sous-graphes densément connectés tels que des cliques) possédant des attributs homogènes (Gunnemann et al., 2010;Moser et al., 2009;Mougel et al., 2010).\nIl existe aussi des travaux qui se concentrent sur la détection de communautés dans les réseaux dynamiques (Robardet, 2009). Leur but principal est l\u0027observation de l\u0027évolution de la structure de communautés, à travers des évènements tels que la formation, la dissolution, la croissance, la diminution et la fusion. Pour des réseaux dynamique attribuées, il existe des méthodes pour trouver les groupes de noeuds structurellement similaires dont les attributs changement de la même façon au cours du temps (Desmier et al., 2012). Ces méthodes ne cherchent pas des communautés. L\u0027objectif de ces travaux est la découverte de motifs intéressants en tenant compte de la structure des liens et des attributs pour les réseaux dynamiques attribuées. Pour regrouper les noeuds, ces méthodes utilisent des contraintes, comme par exemple une distance limite entre noeuds de la même motif, ou la nécessité de partager les mêmes voisins.\nBien qu\u0027il existe de nombreuses techniques différentes pour identifier les structures de communautés, il y a peu d\u0027auteurs qui travaillent à caractériser les communautés obtenues. Dans (Lancichinetti et al., 2010), les auteurs comparent les distributions de certaines mesures topologiques afin de comprendre la forme générale des communautés, et tentent de les caractériser en fonction du type de système modélisé (biologique, informatique, social, etc.). Ils n\u0027utilisent pas d\u0027attributs nodaux et ne traitent que des réseaux statiques. Dans (Tumminello et al., 2011), les auteurs proposent une méthode statistique inspirée d\u0027approches utilisées en génétique, pour caractériser les communautés en termes d\u0027attributs surexprimés. Cependant, cette étude n\u0027a pas recours à des mesures topologiques et se limite également aux réseaux statiques. Dans (Labatut et Balasque, 2012), les auteurs interprètent les communautés d\u0027un réseau attribué social. Ils utilisent la régression statistique et l\u0027analyse discriminante pour identifier les valeurs des attributs les plus caractéristiques de chaque communauté. Cependant, ici encore, le réseau est statique et l\u0027information structurelle n\u0027est pas exploitée.\nEnfin, le travail présenté dans (Prado et al., 2013) n\u0027est pas directement concerné par la détection ou la caractérisation de communautés, mais est néanmoins lié à notre approche. Prado et al. (2013) ont introduit l\u0027idée d\u0027utiliser les mesures topologiques et les attributs nodaux ensemble dans une perspective d\u0027exploration de données. Leur but était de trouver des motifs reflétant la covariation de mesures topologiques et d\u0027attributs sur l\u0027ensemble du réseau. Ils se sont concentrés sur des réseaux statiques et n\u0027ont pas considéré de structures communautaires.\nConclusion\nNous traitons le problème de la caractérisation des communautés dans des réseaux complexes dynamiques et attribués. Nous proposons une nouvelle représentation de l\u0027information encodée dans le réseau, permettant de stocker simultanément l\u0027information topologique, les attributs nodaux et la dimension temporelle. Nous utilisons cette représentation pour effectuer une fouille de motifs séquentiels fréquents. Chaque communauté peut ensuite être caractéri-sée par ses motifs les plus distinctifs. Nous tirons également parti des motifs pour détecter et caractériser les noeuds déviants dans chaque communauté. Nous appliquons notre méthode à un réseau de collaboration scientifique construit à partir des données publiques de la base DBLP. Les résultats montrent que notre méthode est capable de caractériser les communautés en fonction, notamment, de leur thématique. Les noeuds déviants identifiés correspondent à différents types de profils, tels que des chefs de file communautaires, des chercheurs émergents, ou d\u0027autres en train de changer de thématique de recherche.\nA notre connaissance, il s\u0027agit de la première formulation de la caractérisation de communautés comme un problème de fouille de données. Notre but était de surmonter les limitations des rares travaux existants (Lancichinetti et al., 2010;Tumminello et al., 2011;Labatut et Balasque, 2012) en proposant une approche systématique, tenant compte à la fois de la structure, des attributs nodaux et du temps. La représentation des données que nous utilisons n\u0027avait jamais été appliquée au traitement de graphes. Le processus proposé pour extraire les motifs les plus pertinents en se basant sur une recherche de motifs séquentiels sous contraintes est original et nous avons montré la richesse des interprétations apportées sur un cas réel de réseau. Certaines étapes de l\u0027implémentation présentée ici étaient relativement naives, nous comptons les améliorer afin de réduire le temps nécessaire au calcul. En particulier, l\u0027identification des ensembles de support pourrait être intégrée à Clospan.\nAfin de restreindre la complexité conceptuelle de cette première approche, nous avons volontairement limité notre méthode d\u0027analyse en ne considérant pas l\u0027évolution des communautés au cours du temps. Dans des travaux ultérieurs, nous envisageons d\u0027appliquer un algorithme de détection de communautés approprié en insérant cette information dans la base de données utilisée pour la fouille de motifs. Nous comptons également appliquer notre méthode d\u0027analyse à d\u0027autres types de réseaux pour explorer ses capacités de caractérisation. Comme autre perspective, nous pouvons aussi mieux exploiter nos représentations de réseaux attribués dynamiques. Ici, nous nous sommes seulement intéressés à l\u0027extraction de séquences fréquentes. Cependant, notre représentation des données du réseau peut également être utilisée pour interroger les noeuds selon certaines mesures ou attributs topologiques spécifiques. Ainsi, dans nos expériences, nous avons vu qu\u0027il y avait beaucoup de noeuds qui n\u0027appartiennent pas à une communauté. Il serait intéressant de bien regarder ces noeuds et mieux comprendre en quoi ils sont différents des autres pour, par exemple, formuler des hypothèses sur leur isolement.\n"
  },
  {
    "id": "375",
    "text": "Introduction\nTwitter offre des fonctionnalités de microblogging qui sont utilisées par des millions de personnes à travers le monde pour publier des messages courts. Ces personnes créent et partagent de l\u0027information liée à divers types d\u0027évènements, allant d\u0027évènements personnels banals à des évènements importants et/ou globaux, quasiment en temps-réel. L\u0027explosion du nombre d\u0027utilisateurs de ce réseau a entraîné l\u0027apparition d\u0027un phénomène de surcharge informationnelle. Pour lutter contre cela, il est nécessaire de doter les utilisateurs de moyens leur permettant d\u0027identifier plus facilement les éléments d\u0027information les plus intéressants et de se tenir au courant des derniers évènements significatifs.\nL\u0027information brute produite par Twitter est délivrée sous la forme d\u0027un flux de messages. Par conséquent la manière dont ceux-ci arrivent au fil du temps recèle une part importante de leur signification. La dynamique temporelle des thématiques les plus populaires est constituée d\u0027une succession de focus et dé-focus, autrement dits, une succession de pics de popularité. C\u0027est pourquoi de nombreuses approches -allant de méthodes basées sur la fréquence des mots jusqu\u0027à des méthodes plus complexes reposant sur des modèles de thématiques probabilistes dynamiques -ont été proposées dans le but d\u0027identifier ce genre de thématiques. Ces méthodes reposent sur des stratégies variées de détection des pics et produisent des résultats très différents. Nos travaux s\u0027intéressent au filtrage et à l\u0027identification de thématiques à partir de l\u0027information contenue dans un flux de messages produits par Twitter afin de, entre autres, fournir une vue rétrospective des thématiques les plus populaires ou bien recommander des éléments d\u0027information intéressants en temps réel. Une bonne solution doit satisfaire deux critères : d\u0027une part les thématiques identifiées doivent être précisément localisées dans le temps et intelligibles et d\u0027autre part, la méthode doit pouvoir passer à l\u0027échelle et traiter de grands volumes de données. Afin de conserver une complexité temporelle raisonnable, beaucoup de méthodes existantes assimilent une thématique à un simple mot. C\u0027est le cas par exemple d\u0027approches basées sur l\u0027analyse de la fréquence des mots telles que la méthode « Peaky Topics » (Shamma et al., 2011) ou la méthode basée sur l\u0027indice MACD (Rong et Qing, 2012). Un pic de popularité se traduit par l\u0027augmentation soudaine de la fréquence d\u0027un mot. Cette défini-tion n\u0027est pas toujours appropriée à cause de l\u0027ambiguïté possible. Pour pallier à ce problème, Benhardus et Kalita (2013) proposent d\u0027étudier des n-grammes de mots, mais les n-grammes ne peuvent capturer les relations entre des mots trop éloignés dans le corps d\u0027un message et sont très sensibles au bruit. Afin d\u0027identifier des thématiques plus explicites, de nombreuses méthodes se basant sur des modèles probabilistes ont été développées, telles que OLDA (AlSumait et al., 2008) ou Online-LDA (Lau et al., 2012). Un pic de popularité se traduit alors par une variation soudaine de la distribution des thématiques. Cependant, l\u0027introduction de la dimension temporelle dans ces modèles augmente la complexité des mécanismes d\u0027inférence mis en oeuvre (il faut notamment faire en sorte que les thématiques qui évoluent peu dans le temps restent comparables au sein du modèle, et également faire en sorte que le modèle ait un niveau de sensibilité constant de sorte qu\u0027il puisse détecter de nouvelles thématiques au fil du temps), ce qui limite leur capacité de passage à l\u0027échelle. Par ailleurs, Aiello et al. (2013) ont montré que les méthodes à base de modèles probabilistes dynamiques ne sont pas efficaces sur des flux sociaux trop hétérogènes au sein desquels de nombreux éléments d\u0027information sont relatés simultanément. Qui plus est, la très vaste majorité des méthodes existantes ignore un aspect important de ces flux, précisément leur aspect social. En effet, un message ne se limite pas à un simple contenu textuel et il est notamment possible d\u0027y insérer une ou plusieurs « mentions » (à l\u0027aide de la syntaxe « @pseudonyme » dans le corps des messages). Lorsque l\u0027auteur d\u0027un message insère une mention, il crée en réalité un lien dynamique vers un autre utilisateur. Ce lien est considéré comme dynamique puisque sa création est datée et liée à un contenu particulier, celui du message. Les mentions permettent aux utilisateurs d\u0027exprimer leur volonté d\u0027engager la discussion à propos du contenu du message avec la ou les personnes ciblées, et traduisent donc l\u0027intérêt qu\u0027ils portent à la thématique liée. À notre connaissance, les travaux menés par Takahashi et al. (2011) sont les seuls à intégrer cette caractéristique. La méthode qu\u0027ils proposent repose sur une modélisation probabiliste du comportement de chaque utilisateur du réseau en terme de création de liens dynamiques. En détectant les points de rupture par rapport à ces comportements standards, il est possible d\u0027identifier des thématiques émer-gentes, une thématique étant définie comme un simple mot. Outre cette définition qui limite l\u0027intérêt des résultats obtenus, la méthode souffre de la complexité de la phase d\u0027apprentissage du modèle puis de détection qui rend quasiment impossible sa mise en oeuvre dans des conditions réelles (i.e. un réseau comportant un grand nombre d\u0027utilisateurs). Globalement, il apparaît nécessaire de développer des méthodes mieux adaptées. Le reste de cet article est organisé comme suit. Dans la section suivante, nous présentons une nouvelle méthode pour la détection de thématiques populaires sur Twitter, puis nous présentons les résultats obtenus dans la section 3.\n2 Méthode proposée L\u0027objectif de la méthode est d\u0027identifier des thématiques à la fois riches de sens et précisé-ment localisées dans le temps, tout en tenant compte de l\u0027aspect social du flux de messages. \nFIG. 1 -Grands principes de la méthode proposée.\nEntrée. Nous traitons un flux produit par Twitter contenant M messages. Le vocabulaire des termes employés dans ces messages est noté V . Nous discrétisons l\u0027axe temporel en partitionnant les messages en n tranches temporelles de même durée (cf. la figure 1 pour une illustration de ce pré-traitement). Cette étape de pré-traitement est commune à toutes les approches de détection de thématiques temporelles citées précédemment. On note ? t (i) la fonction qui donne le nombre de messages inclus dans la i ème tranche temporelle qui contiennent au moins une occurrence du terme t. ? t (i) désigne la fonction qui donne le nombre de messages inclus dans la i ème tranche temporelle qui contiennent au moins une « mention » et le terme t. Les séries temporelles correspondantes sont notées ? t et ? t .\nSortie. La méthode génère une liste de thématiques, ordonnées selon leur popularité. Une thématique est définie par un terme principal, une liste pondérée de termes liés et un intervalle temporel. Par exemple, la thématique : {[\"google\", {(\"chrome\",0.8), (\"os\", 0.8), (\"desktop\", 0.75) }], [\u002719/11/09\u0027 ;\u002720/11/09\u0027]}, capture l\u0027évènement créé par la sortie de Google Chrome OS le 19 novembre 2009.\nGrands principes de la méthode. Nous décomposons la tâche d\u0027identification des thé-matiques populaires en 3 problèmes : (1) l\u0027identification des termes principaux et des intervalles temporels, chaque couple étant associé à un score de popularité ; (2.a) la sélection de termes liés pertinents ; (2.b) la construction du graphe des redondances et du graphe de théma-tiques, duquel est extrait la liste finale de thématiques. La méthode se déroule comme suit. Tout d\u0027abord, le problème (1) est résolu pour chaque terme appartenant au vocabulaire V . Ensuite, pour chaque couple de terme principal et intervalle temporelle, le problème (2.a) est résolu afin d\u0027identifier l\u0027ensemble pondéré de termes liés. Chaque thématique ainsi constituée est insérée dans le graphe de thématiques si elle n\u0027est pas redondante avec une autre thématique déjà présente (2.b). Les redondances constatées sont modélisées par un second graphe, qui permet d\u0027identifier les thématiques à fusionner à la fin du processus, avant d\u0027extraire la liste des thé-matiques populaires qui sera retournée à l\u0027utilisateur. La figure 1 décrit le déroulement de la méthode ainsi que les IHM permettant de visualiser les thématiques identifiées.\nIdentification des termes principaux et des intervalles temporels\nL\u0027objectif de cette première étape est de produire une liste ordonnée de thématiques, chacune étant définie par un terme principal, un intervalle temporel et un score caractérisant sa popularité. Notre hypothèse est que la fréquence de création de liens dynamiques (i.e. fréquence des « mentions ») liée à une thématique est un meilleur indicateur du niveau d\u0027attention qu\u0027elle reçoit de la part des utilisateurs que sa fréquence d\u0027apparition globale. Nous utilisons donc cette mesure pour localiser dans le temps les thématiques et estimer leur niveau de popularité.\nCalcul du score de popularité. Nous définissons d\u0027abord la fréquence de mentions attendues en chaque tranche temporelle pour le terme t : f a(t) (eq. 1). Nous exprimons ensuite l\u0027anomalie de la fréquence des mentions liées à ce terme à la i ème tranche temporelle selon la formule donnée par l\u0027équation 2. Identification de la période de popularité. Identifier l\u0027intervalle I durant lequel un terme t était le plus populaire revient à trouver l\u0027intervalle qui maximise la valeur de score(t, I). Or, nous venons de montrer que ce score est obtenu en sommant la fonction d\u0027anomalie. Par conséquent, identifier l\u0027intervalle le plus populaire revient à résoudre un problème du type « sous-séquence contiguë de somme maximale » (SSCSM). Nous résolvons ce problème de type SSCSM à l\u0027aide de l\u0027algorithme en temps linéaire décrit par Bentley (1984).\nSélection de termes liés pertinents\nAfin de préciser une thématique décrite par un terme principal t et un intervalle temporel I, nous sélectionnons un ensemble de termes liés S. Afin de limiter la surcharge d\u0027information pour l\u0027utilisateur, il faut que cet ensemble soit d\u0027une taille raisonnable et qu\u0027il contienne des termes pertinents durant l\u0027intervalle temporel.\nL\u0027ensemble des termes liés potentiels est réduit aux p termes les plus co-occurrents avec t durant I. Pour sélectionner les termes les plus pertinents parmi ceux-ci, nous proposons de calculer un poids w q ? [0; 1] pour chaque terme t q ? S basé sur la corrélation entre la dynamique temporelle de t et t q durant l\u0027intervalle I. Seuls les termes dont le poids dépasse un certain seuil noté ? sont conservés. Les paramètres p et ? sont fixés par l\u0027utilisateur de la méthode. Nous estimons la corrélation entre les séries ? t et ? t q à l\u0027aide du coefficient récemment proposé par Erdem et al. (2012). Ce coefficient, développé à l\u0027origine par les auteurs pour analyser des données boursières non-stationnaires, est particulièrement adapté aux données que nous traitons. Par soucis de concision, nous donnons directement la formule d\u0027approximation de la corrélation entre la dynamique du terme t et le terme lié t\nLa preuve que |? O | 1 est donnée par Erdem et al. (2012 \nConstruction des graphes de thématiques et des redondances\nAfin de générer l\u0027ensemble final des thématiques retourné à l\u0027utilisateur, nous construisons deux structures de graphe : le graphe de thématiques et le graphe des redondances. Le premier est un graphe orienté composé de noeuds appartenant à deux classes : les noeuds représentant les termes principaux, lesquels sont annotés par un intervalle et un score, et les noeuds repré-sentant les termes liés. Ces derniers sont connectés à l\u0027aide d\u0027arcs pondérés, dirigés vers les termes principaux. Le second est un simple graphe non-orienté servant à représenter la redondance entre certaines thématiques. Chaque thématique T \u003d (t, I, S) générée par la résolution des deux problèmes précédemment décrits est insérée dans le graphe des thématiques si elle n\u0027est pas jugée redondante avec une thématique déjà présente. Une thématique T 1 est jugée redondante avec T 2 si le terme principal t 1 serait mutuellement connecté avec le terme t 2 et si l\u0027intersection entre I 1 et I 2 mesurée par |I1?I2| min(|I1|,|I2|) est importante (i.e. dépasse un seuil ? \u003c 1). Dans le cas où la thématique à insérée T 1 est jugée redondante, sa définition est mise de côté et une arrête liant t 1 et t 2 est ajoutée au graphe des redondances. Les thématiques étant ordonnées selon leur score de popularité, l\u0027utilisateur peut paramétrer le nombre k de thématiques qu\u0027il souhaite, et seules le k plus populaires et non-redondantes lui seront pré-sentées. Une fois les deux graphes construits, l\u0027identification des thématiques qui peuvent être fusionnées ensemble consiste en l\u0027identification des composantes connexes au sein du graphe des redondances. Cela se fait en temps linéaire à l\u0027aide de l\u0027algorithme décrit par Hopcroft et Tarjan (1973). Le terme principal de la thématique fusionnée devient l\u0027agrégation des termes principaux, et seuls les p termes liés avec les p plus grands poids sont conservés. En parcourant les noeuds de la classe principale du graphe de thématiques on reconstruit les thématique une par une à partir des annotations du noeud principal et des termes liés qui y sont connectés. TAB. 2 -Trois des thématiques identifiées : la position en terme de popularité est donnée devant le terme principal (en gras).\nnovembre aux USA, #7 tiger, woods (composé donc issu d\u0027une fusion), victime d\u0027un accident, #32 water, i.e. l\u0027eau trouvée sur la Lune par la NASA). L\u0027hypothèse selon laquelle ? (fréquence des mentions) est un meilleur indicateur de popularité qu\u0027? a été vérifiée expérimentalement, puisqu\u0027une version de la méthode se basant exclusivement sur ? a obtenu une précision@n systématiquement inférieure et a globalement détectée les thématiques avec du retard.\n"
  },
  {
    "id": "376",
    "text": "Introduction\nDepuis les années 1990, les progrès de l\u0027informatique et des capacités de stockage permettent la manipulation de très gros volumes de données : il n\u0027est pas rare d\u0027avoir des espaces de description de plusieurs milliers, voire de dizaines de milliers de variables. On pourrait penser que les algorithmes de classification sont plus efficaces avec un grand nombre de variables, mais la situation n\u0027est pas aussi simple que cela. Le premier problème qui se pose est l\u0027augmentation du temps de calcul. En outre, le fait qu\u0027un nombre important de variables soit redondant ou non pertinent pour la tâche de classification perturbe considérablement le fonctionnement des classifieurs. De plus, la plupart des algorithmes d\u0027apprentissage exploitent des probabilités dont les distributions peuvent être difficiles à estimer en présence d\u0027un très grand nombre de variables. L\u0027intégration d\u0027un processus de sélection de variables dans le cadre de la classification des données de grande dimension devient donc un enjeu central. Dans la littérature, trois types d\u0027approches pour la sélection de variables sont principalement proposés : les approches directement intégrées aux méthodes de classification, dites «embedded», les mé-thodes basées sur des techniques d\u0027optimisation, dites «wrapper», et finalement, les approches de filtrage. Des états de l\u0027art exhaustifs ont été réalisés par de nombreux auteurs, comme Ladha et al. (Ladha et Deepa, 2011), (Bolón-Canedo et al., 2012) ou (Guyon et Elisseeff, 2003). Nous ne faisons donc ci-après qu\u0027un rapide tour d\u0027horizon des approches existantes. Les approches « embedded » intègrent la sélection des variables dans le processus d\u0027apprentissage (Breiman et al., 1984). Les méthodes les plus populaires de cette catégorie sont les méthodes basées sur les SVM et les méthodes fondées sur les réseaux de neurones. A titre d\u0027exemple, SVM-EFR (Recursive Feature Elimination for Support Vector Machines) (Guyon et al., 2002) est un processus intégré qui effectue la sélection des variables de façon itérative en utilisant un classificateur SVM et en supprimant les variables les plus éloignées de la frontière de décision. De leur côté, les méthodes « wrappers » utilisent un critère de performance pour la recherche d\u0027un sous-ensemble de prédicteurs pertinents (Kohavi et John, 1997). Le plus souvent, c\u0027est le taux d\u0027erreur (mais cela peut être un coût de prédiction ou l\u0027aire sous la courbe ROC). A titre d\u0027exemple, la méthode WrapperSubsetEval commence avec un ensemble vide de variables et se poursuit jusqu\u0027à ce que l\u0027ajout de nouvelles variables n\u0027améliore plus les performances, en exploitant la validation croisée pour estimer la précision de l\u0027apprentissage pour un ensemble donné de variables (Witten et Frank, 2005). Les comparaisons entre méthodes, comme celle de Forman (Forman, 2003), mettent clairement en évidence que, sans tenir compte de leur efficacité, l\u0027un des principaux inconvénients de ces deux catégories de méthodes est qu\u0027elles sont très gourmandes en temps de calcul. Cela proscrit leur utilisation dans le cas de données fortement multidimensionnelles. Dans ce contexte, une alternative possible est alors d\u0027exploiter les méthodes de filtrage. Les approches par filtrage sont des méthodes de sélection qui sont utilisées en amont et indé-pendamment de l\u0027algorithme d\u0027apprentissage. Basées sur des tests statistiques, elles sont plus légères en termes de temps de calcul que les autres approches. La méthode du chi-carré exploite un test statistique courant qui mesure l\u0027écart à une distribution attendue en supposant que les variables sont indépendantes des étiquettes de classe (Ladha et Deepa, 2011). Le gain d\u0027information est également l\u0027une des méthodes les plus courantes de l\u0027évaluation de variables. Ce filtre univarié fournit une classification ordonnée de toutes les variables. Dans cette approche, les variables retenues sont celles qui obtiennent une valeur positive du gain d\u0027information (Hall et Smith, 1999). Dans la méthode MIFS (Mutual Information Feature Selection), une variable est ajoutée à un sous-ensemble de variables déjà sélectionnées si son lien avec la classe cible surpasse la connexion moyenne avec les prédicteurs déjà sélectionnés. La méthode prend en compte à la fois la pertinence et la redondance (Hall et Smith, 1999). La méthode CBF (Consistency-based Filter) évalue la pertinence d\u0027un sous-ensemble de variables par le niveau de cohérence des classes lorsque les échantillons d\u0027apprentissage sont projetés sur ce sous-ensemble (Dash et Liu, 2003). La méthode MODTREE est un procédé de filtrage qui repose sur le principe du calcul de la corrélation par paire. Elle fonctionne dans l\u0027espace des paires d\u0027individus décrits par des indicateurs de co-étiquetage attachés à chaque variable d\u0027origine. Pour cela, un coefficient de corrélation par paire, qui représente la corrélation linéaire entre deux éléments, est utilisé. Le calcul des coefficients de corrélation partiels permet alors d\u0027effectuer une sélection de variables pas à pas (Lallich et Rakotomalala, 2000). L\u0027hypothèse de base de la méthode Relief, qui tire son inspiration du principe des plus proches voisins, est de considérer une variable pertinente si elle discrimine bien un objet dans la classe positive par rapport à son voisin le plus proche dans la classe négative. Le score des variables est cumulatif et calculé grâce à un tirage aléatoire de données-échantillons. ReliefF, une extension de Relief, ajoute la capacité de résoudre les problèmes multi-classes. Cette variante est aussi plus robuste et capable de traiter des données incomplètes et bruitées (Konokenko, 1995). ReliefF est considérée comme l\u0027une des méthodes de sélection à base de filtres les plus efficaces. Comme tout test statistique, les approches par filtrage sont connues pour avoir un comportement erratique dans le cas de variables de très faibles fréquences ; ce qui représente une situation habituelle dans la classification de texte (Ladha et Deepa, 2011). Nous montrons éga-lement dans cet article que, malgré leur diversité, toutes les approches de filtrages existantes s\u0027avérent inopérantes, voir néfastes, dans le cas de données très déséquilibrées, fortement multidimensionnelles et bruitées, avec un degré de similitude élevé entre classes. Nous proposons comme alternative une nouvelle méthode de sélection de variables et de contraste basée sur la métrique de maximisation d\u0027étiquetage, récemment développée, et nous comparons ses performances avec des techniques classiques dans le contexte d\u0027aide à la validation des brevets. Nous étendons ensuite la portée de notre étude à des données textuelles de référence habituellement utilisées. La suite du document est structurée comme suit. La section 2 présente notre nouvelle approche de sélection de variables. La section 3 détaille les données utilisées. La section 4 compare les résultats de la classification avec et sans l\u0027utilisation de l\u0027approche proposée sur les différents corpus de données. La section 5 présente nos conclusions et perspectives.\nMaximisation d\u0027étiquetage pour la sélection de variables\nLa maximisation d\u0027étiquetage (F-max) est une métrique non biaisée d\u0027estimation de la qualité d\u0027une classification non supervisée qui exploite les propriétés des données associées à chaque cluster sans examen préalable des profils de clusters (Lamirel et al., 2004). Son principal avantage est d\u0027être tout à fait indépendante des méthodes de classification et de leur mode opératoire. Lorsqu\u0027elle est utilisée après l\u0027apprentissage, elle peut être exploitée pour établir des indices globaux de qualité de clustering (Lamirel et al., 2010) ou pour l\u0027étiquetage de clusters (Lamirel et Ta, 2008). Considérons un ensemble de clusters C résultant d\u0027une méthode de clustering appliquée sur un ensemble de données D représentées par un ensemble de variables F . La métrique de maximisation d\u0027étiquetage favorise les clusters avec une valeur maximale\n, eux-mêmes définis comme suit :\noù W f d représente le poids de la variable f pour la donnée d et F c représente l\u0027ensemble des variables représentées dans les données associées au cluster c.\nTenant compte de la définition de base de la métrique de maximisation d\u0027étiquetage, son exploitation pour la tâche de sélection de variables dans le contexte de l\u0027apprentissage supervisé devient un processus simple, dès lors que cette métrique générique peut s\u0027appliquer sur des données associées à une classe aussi bien qu\u0027à celles qui sont associées à un cluster. Le processus de sélection peut donc être défini comme un processus non paramétré basé sur les classes dans lequel une variable de classe est caractérisée en utilisant à la fois sa capacité à discriminer une classe donnée (F P c (f ) index) et sa capacité à représenter fidèlement les données de la classe (F R c (f ) index). L\u0027ensemble Sc des variables qui sont caractéristiques d\u0027une classe donnée c, appartenant à un ensemble de classes C, se traduit par :\noù C /f représente le sous-ensemble de C dans lequel la variable f est représentée. Enfin, l\u0027ensemble de toutes les variables S C sélectionnées est le sous-ensemble de F défini comme : \noù k est un facteur d\u0027amplification qui peut être optimisé en fonction de la précision obtenue.\nLes variables actives d\u0027une classe sont celles pour lesquelles le gain d\u0027information est supérieur à 1 dans celles-ci. Etant donné que la méthode proposée est une méthode de sélection et de contraste basée sur les classes, le nombre moyen de variables actives par classe est donc comparable au nombre total de variables sélectionnées dans le cas des méthodes de sélection usuelles.\nDonnées expérimentales\nUn des buts poursuivi par le projet QUAERO est celui d\u0027exploiter les informations bibliographiques pour aider des experts à juger de l\u0027antériorité des brevets. Il s\u0027agit donc, dans un premier temps, de prouver qu\u0027il est possible d\u0027associer ces informations de manière pertinente aux classes de brevets, autrement dit de les classifier correctement dans ces classes. Nos données source expérimentales principales contiennent 6387 brevets au format XML du domaine pharmacologique, regroupés en 15 sous-classes de la classe A61K (préparation mé-dicale). Les citations bibliographiques dans les brevets sont extraites de la base de données Medline 1 . 25887 citations ont été extraites à partir de ces 6387 brevets. L\u0027interrogation de la base de données Medline avec les citations extraites permet de récupérer les notices bibliographiques de 7501 articles. Chaque notice est ensuite marquée par le premier code de classement du brevet citant (Hajlaoui et al., 2012). Le résumé de chaque notice est traité et transformé en un sac de mots (Salton, 1971) en utilisant l\u0027outil TreeTagger (Schmid, 1994). Pour réduire le bruit généré par cet outil, un seuil de fréquence de 45 (soit le seuil moyen de 3/classe) est appliqué sur les descripteurs extraits. Il en résulte un espace de description seuillé de dimension 1804. Une dernière étape de pondération TF-IDF (Salton, 1971) est appliquée. La série de notices étiquetées et ainsi pré-traitées représente le corpus final sur lequel l\u0027apprentissage est effectué. Ce dernier corpus est fortement déséquilibré, la plus petite classe contenant 22 articles (classe A61K41) et la plus grande en contenant 2500 (classe A61K31). La similarité inter-classes calculée en utilisant une corrélation cosinus indique que plus de 70% des couples de classes ont une similitude comprise entre 0,5 et 0,9. Ainsi, la capacité d\u0027un modèle de classification à détecter précisément la bonne classe est fortement réduite. Une solution habituellement utilisée pour faire face à un déséquilibre dans des données des classes est un sous-échantillonnage des grosses classes (Good, 2006) et/ou un sur-échantillonnage des petites classes (Chawla et al., 2002). Toutefois, le ré-échantillonnage, qui introduit de la redondance dans les données, n\u0027améliore pas les performances avec cet ensemble de données, comme cela a été montré par Hajlaoui et al. (2012). Nous proposons donc ci-après, une solution alternative qui est d\u0027élaguer les variables jugées non pertinentes et de contraster celles jugées fiables.\nA titre complémentaire, 3 ensembles de données textuelles de référence sont également utilisés dans nos expériences :\n-Les corpus R8 et R52 sont des corpus obtenus par Cardoso Cachopo 2 à partir des ensembles de données R10 et R90 issus de la collection Reuters 21578 3 . Le but de ces adaptations est de ne retenir que les données ayant une seule étiquette. Considérant uniquement les documents monothématiques et les classes qui ont encore au moins un exemple d\u0027apprentissage et un exemple de test, R8 est une réduction à 8 classes du corpus R10 (10 classes plus fréquentes) et R52 est une réduction à 52 classes du corpus R90 (90 classes). -Le corpus Amazon tm (AMZ) est un ensemble de données UCI (Bache et Lichman, 2013) dérivé des avis de clients du site web Amazon et exploitable pour l\u0027identification des auteurs. Pour évaluer la robustesse des algorithmes de classification à un grand nombre de classes cibles, 50 des utilisateurs les plus actifs qui ont fréquemment postés des com-1. http://www.ncbi.nlm.nih.gov/pubmed/ 2. http://web.ist.utl.pt/~acardoso/datasets/ 3. http://www.research.att.com/~lewis/reuters21578.html mentaires dans ces newsgroups sont identifiés. Le nombre de messages collectés pour chacun d\u0027entre eux est de 30. Chaque message comprend le style linguistique des auteurs tels que l\u0027utilisation de chiffres, la ponctuation, les mots et les phrases fréquentes.\nExpériences et résultats 4.1 Expériences\nPour effectuer nos expériences nous prenons d\u0027abord en considération différents algorithmes de classification qui sont mis en oeuvre dans la boite à outils Weka 4 : arbres de dé-cision (J48) (Quinlan, 1993), forêts aléatoires (RF) (Breiman, 2001), k-plus-proches-voisins (KNN) (Aha et al., 1991), des algorithmes bayésiens usuels, à savoir, bayésien naïf multinomial (MNB) et réseau bayésien (BN), et enfin, l\u0027algorithme SMO-SVM (SMO) (Platt, 1999). Les paramètres par défaut sont utilisés lors de l\u0027exécution de ces algorithmes, à l\u0027exception de KNN pour lequel le nombre de voisins est optimisé sur la base de la précision résultante. Nous mettons ensuite plus particulièrement l\u0027accent sur les tests d\u0027efficacité des approches de sélection de variables, y compris notre nouvelle proposition (FMC). Nous incluons dans notre test un panel d\u0027approches de filtrage qui sont applicables avec des données de grande dimension, en exploitant une nouvelle fois la plateforme Weka. L\u0027ensemble des méthodes testées comprend : chi-carré (CHI), gain d\u0027information (GI), CBF, incertitude symétrique (IS) (Yu et Liu, 2003), ReliefF (RLF), Analyse en Composantes Principales (PCA) (Pearson, 1901). Les paramètres par défaut sont utilisés pour la plupart de ces méthodes, sauf pour PCA pour lequel le pourcentage de variance expliquée est accordé en fonction de la précision obtenue. Dans un premier temps nous expérimentons les méthodes séparément. Dans une deuxième phase, nous combinons la sélection des variables fournies par les différentes méthodes avec la méthode de contraste que nous avons proposée (eq. 6). Une validation croisée en 10 feuillets (10-fold cross-validation) est utilisée dans l\u0027ensemble de nos expériences.\nRésultats\nLes différents résultats sont présentés dans les tableaux 1 à 8. Ils se basent sur les mesures de performance standard (taux de vrai positif (TP) ou Rappel (R), taux de faux positifs (FP), Précision (P), F-mesure (F) et ROC) pondérées par la taille des classes, puis moyennés sur toutes les classes. Pour chaque table et chaque combinaison de méthodes de sélection et de classification, un indicateur de gain/perte de performance (TP Incr) est calculé en utilisant le taux TP de SMO sur les données originales comme référence. Enfin, comme les résultats s\u0027avèrent identiques pour chi-carré, gain d\u0027information et incertitude symétrique, ils ne figurent qu\u0027une seule fois dans les tableaux comme résultats de type chi-carré (et sont notés CHI+). Pour notre collection principale de brevets, le tableau 1 met en évidence que les performances de toutes les méthodes de classification sont faibles sur l\u0027ensemble de données considéré si aucun processus de sélection de variables n\u0027est exécuté. Il confirme également dans ce contexte la supériorité des méthodes SMO, KNN et bayésiennes sur les deux autres méthodes basées sur les arbres de décision. En outre, SMO fournit la meilleure performance globale en termes de discrimination comme le montre sa valeur de ROC la plus élevée. Toutefois, la méthode \nTAB. 2 -Résultats de classification après la sélection de variables (classifieur BN).\nn\u0027est clairement pas exploitable dans un contexte opérationnel d\u0027évaluation de brevets, comme celui de QUAERO, en raison de la grande confusion entre les classes, mettant ainsi en évidence son incapacité intrinsèque à faire face à l\u0027effet d\u0027attraction des plus grandes classes. Chaque fois qu\u0027une méthode usuelle de sélection de variables est appliquée en association avec les mé-thodes de classification les meilleures dans notre contexte, son exploitation altère légèrement la qualité des résultats, comme il est indiqué dans le tableau 2. Le tableau 2 souligne également que la réduction du nombre de variables par la méthode FMC est similaire à CHI+ (en termes de variables actives ; voir la section 2 pour plus de détails) mais que son exploitation stimule les performances des méthodes de classification, et en particulier celles des méthodes bayésiennes (tableau 3), conduisant à des résultats de classification impressionnants dans un contexte de classification très complexe : précision de 0,987%, soit 94 données mal classées parmi un total de 7252 avec la méthode BN. Les résultats présentés dans le tableau 4 illustrent plus précisé- TAB. 3 -Résultats de classification après la sélection de variables FMC. ment l\u0027efficacité de la procédure de contraste F-max qui agit sur les descriptions des données (eq. 6). Dans les expériences relatives à ce tableau, le contraste est appliqué individuellement sur les variables extraites par chaque méthode de sélection et, dans une deuxième étape, un classifieur BN est appliqué sur les données résultantes contrastées. Les résultats montrent que, quel que soit le type de méthode de sélection de variable utilisé, les performances de classification qui en résultent sont renforcées chaque fois que le contraste F-max est appliqué en aval de la sélection. L\u0027augmentation moyenne de performance est de 44%. Le tableau 5 illustre finalement les capacités de l\u0027approche FMC à faire face efficacement aux problèmes de déséquilibre et de similitude des classes. L\u0027examen des variations des taux TP (surtout dans les petites classes) dans ce dernier tableau montre que l\u0027effet d\u0027attraction de données des plus grandes classes, qui se produit à un niveau élevé dans le cas de l\u0027exploitation des données originales, est pratiquement systématiquement surmonté chaque fois que l\u0027approche FMC est exploitée. La capacité de l\u0027approche à corriger un déséquilibre de classes est également clairement mise en évidence par la répartition homogène des variables actives dans les classes, ceci malgré des tailles très hétérogènes de classe. Le résumé des résultats sur les 3 ensembles de données 2.3Ghz et avec une mémoire de 8Go.) Sur ces ensembles de données, des remarques similaires à celles mentionnées pour l\u0027ensemble des données-brevets peuvent être faites au sujet de la faible efficacité des méthodes usuelles de sélection de variables et des méthodes de ré-échantillonnage. Le tableau 8 montre également que la valeur du facteur d\u0027amplification du contraste, qui est exploité pour obtenir les meilleures performances, peut varier au fil des expériences (de 1 à 4, dans ce dernier contexte). Cependant, l\u0027on peut observer qu\u0027en prenant une valeur fixe pour ce facteur, par exemple la plus élevée (ici 4), l\u0027on ne dégrade pas les résultats. Ce choix représente donc une bonne alternative pour faire face au problème de paramétrage. TAB. 7 -Résultats de classifications après sélection de variables FMC (classifieur MNB/BN).\nLes 5 variables (lemmes) les plus contrastées des 8 classes issues du corpus Reuter8 sont présentées dans le tableau 6. Le fait que les grandes lignes des thématiques couvertes par les classes puissent être clairement mises en évidence de cette manière illustre bien les capacités d\u0027extraction de sujets de la méthode FMC. Enfin, l\u0027obtention de très bonnes performances en combinant l\u0027approche de sélection de variables FMC avec une méthode de classification comme MNB est un réel avantage pour l\u0027exploitation à grande échelle, sachant que la méthode MNB a des capacités incrémentales et que les deux méthodes ont des temps de calcul faibles. \nConclusion\nNotre objectif principal était de développer une méthode efficace de sélection et de contraste de variables qui pourrait permettre de surmonter les problèmes habituels liés à la classification supervisée de gros volumes de données textuelles. Ces problèmes sont liés à des classes déséquilibrées avec un degré élevé de similitude entre elles, hébergeant des données fortement multidimensionnelles et bruitées. Pour ce faire, nous avons proposé d\u0027adapter une métrique développée récemment dans le cadre non supervisé au contexte de la classification supervisée. Grâce à diverses expériences sur de grands ensembles de données textuelles, nous avons illustré de nombreux avantages de notre approche, et surtout sa grande efficacité pour améliorer les performances des classifieurs dans un tel contexte, tout en mettant l\u0027accent sur les classifieurs les plus flexibles et les moins gourmands en temps de calcul, comme les classifieurs bayésiens. Un autre avantage de cette méthode est qu\u0027il s\u0027agit d\u0027une approche sans paramètre qui s\u0027appuie sur un schéma simple d\u0027extraction de variables ; elle peut donc être utilisée dans de nombreux contextes, comme dans ceux de l\u0027apprentissage incrémental ou semi-supervisé, ou encore, dans celui de l\u0027apprentissage numérique en général. Une autre perspective intéres-sante serait d\u0027adapter cette technique au domaine de l\u0027exploration de textes afin d\u0027enrichir des ontologies et des lexiques grâce à l\u0027exploitation à grande échelle des corpus existants.\nRemerciements Ce travail a été réalisé dans le cadre du programme QUAERO 5 soutenu par OSEO 6 , Agence française de développement de la recherche.\n5. http://www.quaero.org 6. http://www.oseo.fr/\n"
  },
  {
    "id": "377",
    "text": "Introduction\nLes méthodes automatiques de recherche d\u0027images fournissent un moyen d\u0027aide à la dé-cision dans de nombreux domaines d\u0027application. Dans le domaine médical, elles permettent d\u0027assister les radiologues lors de leur travail d\u0027interprétation d\u0027images en identifiant des images similaires au sein de bases de données. Un cas typique d\u0027utilisation est la recherche par l\u0027exemple où l\u0027on souhaite retrouver des images similaires à un exemple d\u0027image donné en requête correspondant à un examen médical. Pour ce faire les images sont généralement décrites par leurs caractéristiques bas-niveaux (e.g., couleur, texture) induites de leurs pixels et une mesure de distance est utilisée pour rechercher des images similaires dans l\u0027espace des caractéristiques. Cependant, face à la complexité des nouvelles générations d\u0027images médicales, les processus de recherche d\u0027images basés sur le contenu peuvent s\u0027avérer insuffisants. Une des limites principales est liée au problème du saut sémantique : les caractéristiques bas-niveaux ne sont plus suffisamment discriminantes pour caractériser le contenu visuel haut-niveau des images.\nPour proposer une solution à ces problèmes, des travaux récents ont montré l\u0027intérêt de caractériser le contenu des images par des termes sémantiques (Kwitt et al., 2012). Ces termes peuvent être utilisés pour décrire un nombre important d\u0027informations relatives au contenu visuel des images ( Fig. 1(a-b)). Ils peuvent être dérivés des observations des radiologues ou automatiquement prédits à partir de caractéristiques bas-niveaux extraits des pixels. Par consé-quent, l\u0027intégration de termes sémantiques dans les processus de recherche d\u0027images est une solution prometteuse pour faire face aux problèmes liés au saut sémantique. Cependant, les systèmes actuels de recherche d\u0027images basés sur les annotations sémantiques ne considèrent pas les relations intrinsèques (e.g., sémantiques, anatomiques) qui existent entre ces termes lors de la comparaison des images.  (Tousch et al., 2012) ont montré que l\u0027utilisation de vocabulaires contrôlés (comme les ontologies) pour l\u0027annotation d\u0027images pouvait fournir des solutions efficaces pour faire face à ces limites. Les ontologies peuvent être employées pour modéliser les relations sémantiques entre les termes utilisés pour la description des images. De plus, des travaux en traitement automatique des langues naturelles (Pivovarov et Elhadad, 2012) ont déjà proposés des approches pour mesurer la proximité sémantique entre des termes issus d\u0027une ontologie ( Fig. 1(c)). Par ailleurs, dans le domaine de la comparaison de vecteurs, des distances ont été proposées pour considérer la proximité relative entre les éléments composant les vecteurs, permettant de prendre en compte leurs corrélations (Turney et Pantel, 2010). En couplant ces trois stratégies, il existe ainsi une opportunité de considérer les relations sémantiques entre les termes lors de la comparaison d\u0027images et d\u0027améliorer les processus de recherche d\u0027images.\nCet article propose une nouvelle approche basée sur la proximité sémantique du contenu des images, dédiée à la recherche d\u0027images similaires au sein de bases de données (Sec. 2). Elle combine une distance hiérarchique permettant de prendre en compte les relations entre les éléments d\u0027un vecteur (Kurtz, 2012) et une mesure de similarité ontologique permettant d\u0027évaluer automatiquement la proximité sémantique entre des termes extraits d\u0027une ontologie. Cette stratégie offre un moyen de capturer les corrélations sémantiques entre les annotations décrivant les images lors de leur comparaison. Cette approche est générique et peut ainsi être utilisée pour un large champ d\u0027applications allant de la recherche d\u0027images par l\u0027exemple à la classification. Les résultats obtenus dans le domaine de la recherche d\u0027images tomodensitomé-triques du foie se sont montrés encourageants et justifient l\u0027intérêt de cette approche (Sec. 3).\n2 Approche sémantique proposée L\u0027approche proposée est composée de deux étapes (  \nAnnotation des images\nSoit I A une nouvelle image requête. La première étape consiste à décrire le contenu visuel de l\u0027image (ou d\u0027une région d\u0027intérêt spécifique de l\u0027image) par le biais d\u0027un ensemble de termes sémantiques extraits d\u0027une ontologie ?. Cette étape peut être réalisée de façon automatique (par un algorithme d\u0027apprentissage prédisant les termes à partir de caractéristiques extraites de la région d\u0027intérêt) ou de façon manuelle. Dans le cadre de ces travaux, chaque image a été annotée par un radiologue via des termes de ?.\nPour rendre les descriptions d\u0027images comparables, l\u0027approche proposée nécessite la créa-tion d\u0027un sous-vocabulaire de termes extraits de ?. Ce sous-vocabulaire, spécifique à l\u0027application considérée, est défini en accord avec les radiologues. Il est noté X \u003d {x 0 , x 1 , . . . , x k?1 } où les k termes x i ? ? sont des termes potentiellement utilisables pour décrire le contenu des images. Une fois l\u0027image I A annotée, il devient possible de la caractériser via un vecteur de termes sémantiques A \u003d 0 , a 1 , . . . , a k?1 où chaque élément a i ? A est une valeur binaire représentant la présence ou l\u0027absence du terme x i ? X .\nComparaison d\u0027images\nLes vecteurs de termes caractérisant les images peuvent ensuite être comparés via le calcul d\u0027une distance hiérarchique nommée HSBD (Hierarchical Semantic-Based Distance) (Kurtz, 2012). Lors de son calcul, HSBD permet de considérer des valeurs de proximité entre les éléments des vecteurs comparés. Ces valeurs de proximité sémantique sont préalablement calculées par l\u0027intermédiaire d\u0027une mesure de similarité ontologique.\nSimilarité entre termes sémantiques\nOn trouve dans la littérature différents types de mesures permettant d\u0027évaluer la similarité entre des termes appartenant à une ontologie. Parmi ces mesures, les mesures structurelles permettent de quantifier la similarité entre termes en se basant sur la structure de l\u0027ontologie. Notre contexte applicatif nécessitant la proposition d\u0027outils automatiques, notre étude s\u0027est ainsi penchée sur l\u0027utilisation de telles mesures.\nConsidérons deux termes sémantiques x i , x j ? X . Un ensemble de liens connectant x i et x j dans l\u0027ontologie est défini par path(x i , x j ) \u003d {l 0 , . . . , l n?1 }. Pour quantifier une va-leur de similarité sémantique entre deux termes x i et x j , une méthode intuitive a été proposée dans (Al-Mubaid et Nguyen, 2006). Elle est basée sur une stratégie par branches qui évalue la longueur du chemin minimum entre les termes et leur profondeur taxonomique au sein des branches considérées : plus le chemin entre les termes est long, plus les termes sont différents sémantiquement. L\u0027idée est d\u0027évaluer la spécificité commune (SC) des deux termes en soustrayant la profondeur de leur premier ancêtre commun (PAC) à la profondeur D c de la branche principale auquel ces termes appartiennent. La spécificité commune est utilisée pour prendre en compte le fait que les couples de termes se trouvant dans les niveaux les plus bas de la hié-rarchie sont plus similaires que les couples situés à un niveau plus élevé. Nous avons modifié la définition originale de cette mesure pour la normaliser et pour attribuer un poids égal à la longueur du chemin entre les termes et à leur spécificité commune :\nnorm est un facteur de normalisation évaluant la valeur de similarité maximale entre termes et SC(x i , x j ) \u003d D c ? depth(P AC(x i , x j )) représente la spécificité commune des termes.\nPour modéliser l\u0027ensemble des valeurs de similarité sémantique entre les k termes x i appartenant au vocabulaire X , nous définissons une matrice de dissimilarité notée M sem (de taille k × k) où la valeur de chaque case M sem i,j est obtenue via la mesure s ? (x i , x j ), ?x i , x j ? X .\nDistance entre vecteurs de termes\nLe calcul de HSBD, entre deux vecteurs A et B, requière une matrice de dissimilarité modélisant les corrélations entre les éléments des vecteurs composant A et B. Pour ce faire, HSBD est initialisée avec la matrice de similarité sémantique M sem définie précédemment. Avant de pouvoir calculer HSBD, la stratégie adoptée (basée sur un modèle fin-à-grossier) nécessite de définir un moyen de fusionner hiérarchiquement les différents termes représentés par les vecteurs en « clusters » de termes (i.e., des termes de niveaux sémantiques plus éle-vés). Cette étape de pré-traitement repose sur la construction d\u0027un dendrogramme D induit par M sem et modélisant la hiérarchie de fusion des termes. La construction de D est réalisée par le biais d\u0027un algorithme de clustering hiérarchique ascendant. Il est à noter que cette étape de pré-traitement ne doit être effectuée qu\u0027une seule fois pour une matrice M sem donnée. Une fois que le dendrogramme D a été construit, la distance HSBD peut être calculée. Son calcul se décompose en deux étapes principales :\n-Étape 1. Calcul des sous-distances hiérarchiques Durant un processus de fusion ité-ratif scannant chaque étage du dendrogramme (de ses feuilles jusqu\u0027à sa racine), les vecteurs liés à A et B, et induits par la fusion des termes composant chaque cluster de l\u0027étage courant, sont construits. Après chaque itération, la distance de Manhattan D L1 est calculée entre chaque couple de vecteurs créé. Ces « sous-distances » permettent d\u0027évaluer la similarité entre A et B à différents niveaux de sémantiques. -Étape 2. Fusion des sous-distances Les sous-distances hiérarchiques calculées pour tous les étages du dendrogramme, et l\u0027énergie sémantique nécessaire pour aller d\u0027un étage à l\u0027autre, sont ensuite fusionnées en une fonction qui est finalement intégrée pour fournir la valeur de la distance HSBD. Pour plus de détails sur le calcul de HSBD, les lecteurs peuvent se référer à (Kurtz et al., 2013). \nValidation expérimentale\nL\u0027approche proposée a été validée dans le contexte de la recherche d\u0027images similaires au sein d\u0027une base de données de N \u003d 77 images tomodensitométriques. Chaque image repré-sente une coupe de foie affectée d\u0027une lésion (e.g., tumeur, kyste). Chaque lésion a été annotée (12 termes en moyenne) par un radiologue par l\u0027intermédiaire de termes appartenant à un sousvocabulaire de 72 termes extraits de l\u0027ontologie médicale RadLex (Langlotz, 2006).\nLes expériences ont consisté à considérer successivement chaque image de la base comme une requête afin de rechercher des images similaires via la distance HSBD combinée à la mesure ontologique. Pour ce faire, l\u0027image requête a été comparée à l\u0027ensemble des N ? 1 images qui ont ensuite été classées par ordre de similarité (Fig. 3(a)). Les valeurs de similarité ainsi calculées ont ensuite été comparées via l\u0027indice NDCG (Normalized Discounted Cumulative Gain) à des valeurs de similarité de référence définies par un groupe de radiologues pour 30 × 30 couples d\u0027images. L\u0027indice NDCG permet d\u0027estimer la pertinence d\u0027un résultat d\u0027expé-rience de classement par rapport à un classement de référence. Pour chaque image requête, la moyenne de cet indice a été calculée pour chaque K \u003d 1, . . . , 30, évaluant ainsi la qualité du classement pour différents nombres d\u0027images recherchées. Les résultats obtenus par HSBD ont également été comparés à ceux obtenus par d\u0027autres distances de l\u0027état de l\u0027art : la distance Euclidienne D L2 , l\u0027intersection de vecteurs D ? et la distance EMD (Rubner et al., 2000).\nLa figure 3(b) présente les scores NDCG obtenus pour les 4 distances évaluées. À partir de ces résultats, on observe que les scores NDCG sont toujours supérieurs quand la comparaison des images est effectuée avec la distance HSBD que quand elle est effectuée avec les 3 autres distances. En particulier, l\u0027approche proposée conduit à de meilleurs résultats que ceux obtenus avec la distance EMD qui permet également de prendre en compte les relations sémantiques entre les termes (via une initialisation avec la mesure ontologique présentée). Par ailleurs, les moins bons scores ont été obtenus avec les distances D L2 et D ? qui ignorent durant leur calcul les relations sémantiques entre les éléments des vecteurs.\n"
  },
  {
    "id": "378",
    "text": "Introduction\nL\u0027expression en langage naturel recèle des informations riches que les analystes souhaitent souvent explorer. Dans le cadre de l\u0027activité de la Société Succeed Together qui consiste, entre autres, à recueillir et analyser des informations produites lors de séminaires interactifs, les animateurs développent et structurent les discussions établies avec les participants. Les réponses ou remarques apportées par les participants peuvent alors être consignées puis traitées, une phase de regroupent est au préalable nécessaire. Le but est ainsi de mettre en exergue des sentiments partagés par les participants selon une thématique donnée. Dans ce cadre, les travaux menés par le LIRMM (Laboratoire d\u0027Informatique, de Robotique et de Microélectronique de Montpellier) liés au traitement automatique des données textuelles, permettent aux experts de Succeed Together d\u0027analyser semi-automatiquement et à plus grande échelle les données. Ainsi, nous avons focalisé notre étude sur la représentation des données textuelles par des méthodes de TAL (Traitement Automatique du Langage Naturel). Ceci permet, en particulier, d\u0027améliorer les méthodes de classification et/ou regroupement effectuées par le deuxième collaborateur académique du projet (I3S / Université de Nice).\nDans un premier temps, en section 2, nous décrivons les méthodes de représentation des descripteurs textuels. Une application spécifiquement dédiée au projet a été développée. Cette application est décrite en section 3. Les résultats issus des données fournies par la société sont décrits et analysés en section 4. Enfin, quelques perspectives sont données en sections 5.\nDescripteurs textuels pour les tâches de Clustering\nLa sélection de descripteurs pertinents à partir de textes est une étape indispensable pour une tâche de clustering (regroupement) qui consiste à regrouper les documents ayant des contenus sémantiques proches. Pour appliquer les algorithmes de regroupement, il est dans un premier temps nécessaire d\u0027établir une représentation pertinente des documents (Béchet (2009)). Dans cet article, nous nous concentrons sur la représentation vectorielle de Salton et al. (1975).\nNous nous appuierons sur le principe sac de mots appliqué aux textes des séminaires de la société Succeed Together. Plusieurs types de représentations sont alors possibles :\n-Représentation booléenne : Le vecteur booléen donne des informations liées à la pré-sence ou l\u0027absence d\u0027un descripteur dans un document. -Représentation fréquentielle : La représentation fréquentielle de base revient à considérer le nombre d\u0027occurrences d\u0027un terme i dans un document j, la normalisation par rapport aux nombre de mots dans un document peut être aussi appliquée. -Pondération TF-IDF : La mesure TF-IDF consiste à calculer l\u0027importance et la discriminance d\u0027un mot dans un document relativement à une collection (Salton et al. (1975)).\nLogiciel de Vectorisation\nDans le cadre du projet, nous avons développé un logiciel dédié à l\u0027extraction des descripteurs utiles pour l\u0027étape de clustering afin de représenter les textes sous forme d\u0027une matrice. Le format ARFF choisi pour cette représentation est typique des logiciels de clustering tel que Weka (Nom-Préposition-Nom, NomNom, Nom-Adjectif, Adjectif-Nom, etc). Les syntagmes extraits sur la base de patrons morpho-syntaxiques se révèlent en général plus pertinents. -Traitement statistique. Le logiciel permet de ne conserver que les n-grammes présents au moins N occ fois, ce seuil étant déterminé par l\u0027utilisateur. Précisons qu\u0027un élagage trop strict peut engendrer un résultat nul si la taille du corpus est insuffisante. -Génération de différentes sorties. Le logiciel développé propose différents types de sorties (cf. Figure 1).\nExpérimentations\nDans cette section, nous discutons les résultats obtenus en utilisant différents paramètres du logiciel développé dans le cadre du projet.\nFiltres de n-grammes. La Figure 2 indique le nombre de n-grammes moyen extrait en fonction de n et en fixant le nombre d\u0027occurrences N occ \u003d 3. Les résultats indiquent que 1. http ://www.cs.waikato.ac.nz/ml/weka/ 2. Sygfran : http ://www.lirmm.fr/?chauche/ExempleAnl.html pour le corpus donné, de nombreux n-grammes sont extraits pour n \u003d 1. Avec un nombre n supérieur ou égal à 2, le nombre de n-grammes extraits est significativement plus faible. Cette même constatation a été relevée quel que soit N occ .\nFiltres morpho-syntaxiques. Sans filtrage syntaxique (FS), de nombreux n-grammes retournés se révèlent non pertinents. Par exemple, les 2-grammes des collaborateurs ou notre présence. À partir d\u0027un des fichiers caractéristiques fourni par la société, 437 n-grammes sont extraits (avec N occ \u003d 2). En appliquant les filtres syntaxiques, seulement 5% des 2-grammes sont conservés par rapport à l\u0027extraction sans FS. Par exemple, nous retenons les syntagmes libre service, nouvelles technologies, service client, satisfaction client, contact humain, relation client. De tels termes se révèlent sémantiquement plus pertinents.\nFiltres fréquentiels. La Figure 2 indique le nombre moyen de 1-grammes extraits en fonction du seuil N occ . Par exemple, 96 n-grammes sont extraits avec un seuil égal à 2. Il est notable qu\u0027à partir de N occ \u003d 6, très peu de 1-grammes sont extraits.\nPar ailleurs, nous avons comparé un classement de type fréquentiel avec un classement sur la base d\u0027une pondération TF-IDF. Nous avons calculé le recouvrement moyen des termes selon ces classements. Par exemple, avec N occ \u003d 2, environ 44% des 1-grammes sont communs aux deux mesures (en tenant compte des 10 premiers termes de plus haut score).\nConclusion et perspectives\nDans le cadre de ce projet, nous nous sommes concentrés sur l\u0027extraction des descripteurs linguistiques à partir des données textuelles fournies par la société Succeed Together. Nous avons évalué l\u0027utilisation de différentes méthodes de Traitement Automatique du Langage à  Salton, G., A. Wong, et C. S. Yang (1975). A vector space model for automatic indexing. Commun. ACM 18(11), 613-620.\nSummary\nAutomatic processing of textual data enables users to analyze semi-automatically and on a large scale the data. This analysis is based on two successive processes: (i) representation of texts, (ii) gathering of textual data (clustering). The software described in this paper focuses on the first step of the process by offering expert a parameterized representation of textual data.\n"
  },
  {
    "id": "379",
    "text": "Introduction\nUtilisée à l\u0027origine comme un outil d\u0027aide à la décision, les arbres de décision sont très populaires en fouille et en analyse visuelle des données. Ce succès s\u0027explique notamment par le fait qu\u0027ils utilisent un formalisme transparent et simple à comprendre (Murthy, 1998).\nEn pratique, la génération automatique d\u0027arbres de décision depuis des données est possible grâce à l\u0027induction d\u0027arbre de décision, une technique bien connue (Quinlan, 1986). Malheureusement, les arbres de décision générés depuis des données issues du monde réel peuvent être très grands et difficiles à exploiter. De nombreuses approches de simplification ont donc été proposées. La plus connue, l\u0027élagage (pruning) (Breslow et Aha, 1997), consiste à supprimer les parties de l\u0027arbre qui ont un faible pouvoir explicatif. D\u0027autre part, il existe des approches qui travaillent directement sur les données, en utilisant des méthodes de preprocessing (Parisot et al., 2013a). Enfin, la classification non supervisée (ou clustering) est un outil particulièrement utile pour la fouille de données, et à priori, cette technique peut donc être également exploitée pour obtenir des arbres de décision plus simples : en conséquence, une étude récente a proposé une nouvelle approche de classification non supervisée pour obtenir des arbres de décisions plus simples (Parisot et al., 2013b).\nDans cet article, nous présentons une méthode permettant d\u0027adpater une classification non supervisée existante afin de simplifier l\u0027arbre de décision obtenu à partir de chaque cluster.\nContribution\nLa méthode proposée (Figure 1) a pour but d\u0027adapter une classification non supervisée pouvant être obtenue au moyen de toute technique existante non hiérarchique (k-means, EM, etc.) (Gan et al., 2007). Plus précisément, l\u0027adaptation d\u0027une classification non supervisée composée de clusters C 1 , . . . , C n revient à effectuer une succession de déplacements d\u0027éléments entre clusters, de C i vers C j (i \u003d j), sans création ni suppression de clusters. De manière à s\u0027assurer durant l\u0027adaptation que la classification obtenue n\u0027est pas trop éloignée de la classification initiale, l\u0027indice de Jaccard (Jaccard, 1908) est utilisé pour comparer leur similarité (classifications similaires si indice proche de 1, différentes si indice proche de 0).\nLa phase d\u0027adaptation se matérialise sous la forme d\u0027un algorithme (Algorithme 11) dont les paramètres d\u0027entrée sont les suivants : un jeu de données comprenant un attribut classe, une classification non supervisée initiale, ainsi qu\u0027un indice Jaccard minimum sous lequel il ne faut pas descendre. Le résultat obtenu est une classification non supervisée adaptée dans laquelle chaque cluster sert à produire un arbre de décision simplifié.\nAlgorithm 1 Algorithme d\u0027adaptation de clusters\nnbP asses ? 0, j ? 1 while j \u003c jaccardIndexLimit ? nbP asses \u003c nbP assesLimit do for all item à deplacer do cherche un cluster C\u0027 cible pour item if C\u0027 existe then déplace item dans C\u0027 j ?calcul indice de Jaccard end if end for nbP asses ? nbP asses + 1 end while Le principe de l\u0027algorithme est le suivant : des déplacements entre clusters sont effectués tant que l\u0027indice Jaccard minimum spécifié n\u0027est pas atteint. De plus, il se peut qu\u0027au bout d\u0027un certain nombre d\u0027itérations, l\u0027algorithme ne trouve plus de déplacement possible à effectuer, et par conséquent l\u0027indice Jaccard minimum spécifié ne peut jamais être atteint : un nombre de passes maximum est donc également prévu.\nÀ priori, les éléments susceptibles d\u0027être déplacés d\u0027un cluster à un autre sont à chercher parmi ceux qui rendent les arbres de décisions complexes, c\u0027est-à-dire : les éléments mal classifiés/expliqués par l\u0027arbre de décision, et les éléments classifiés par des branches ayant un faible pouvoir de classification. À posteriori, il faut ensuite s\u0027assurer que le fait de retirer chacun de ces éléments du cluster d\u0027origine a un effet réellement positif sur la taille de l\u0027arbre de décision généré depuis le cluster d\u0027origine. Nous calculons donc l\u0027arbre de décision DT avant retrait, puis l\u0027arbre de décision DT\u0027 après retrait, et nous comparons les tailles de DT et de DT\u0027 : si la taille de DT\u0027 est inférieure à celle de DT, alors l\u0027élément peut être retiré.\nUne fois qu\u0027un élément a été sélectionné pour être déplacé dans un autre cluster, nous proposons de choisir le cluster pour lequel l\u0027arbre de décision est le mieux impacté par l\u0027ajout de l\u0027élément. En d\u0027autres termes, nous calculons pour chaque cluster l\u0027arbre de décision DT avant ajout, puis l\u0027arbre de décision DT\u0027 après ajout, et nous comparons la taille de DT et DT\u0027. Si pour tout cluster, la taille de DT\u0027 est toujours supérieure à la taille de DT, alors il n\u0027y pas de cluster cible : il n\u0027y a donc pas de déplacement. Sinon, le cluster cible est le cluster pour lequel le ratio entre taille de DT\u0027 et taille de DT est le meilleur : le déplacement est donc effectué.\nExpériences\nUn prototype a été réalisé en Java, en utilisant des fonctionnalités offertes par Weka (Hall et al., 2009). De plus, des tests ont été réalisés sur une sélection de données UCI (Bache et Lichman, 2013) : nous avons comparé les arbres obtenus dans les situations suivantes : -CAS1 : Génération de l\u0027arbre de décision sur le jeu de données complet.\n-CAS2 : Classification non supervisée du jeu de données avec l\u0027algorithme k-means (pour différentes valeurs de k), et génération de l\u0027arbre de décision pour chaque cluster. -CAS3 : Application de notre méthode d\u0027adaptation sur les clusters obtenus précédem-ment, et génération de l\u0027arbre de décision pour chacun des clusters. La génération des arbres de décision a été effectuée en utilisant l\u0027algorithme C4.5, via l\u0027implémentation J48 : l\u0027algorithme a été configuré en désactivant la fonction d\u0027élagage, de manière à obtenir des arbres de décision initiaux très complexes.  Les résultats obtenus, notamment avec des classifications composées de 3 clusters, (Table  1), montrent que notre méthode permet de simplifier les arbres de décision de chaque cluster. Sur l\u0027ensemble des jeux de données testés, les gains les plus spectaculaires concernent les jeux de données cmc, vehicle, credit-a, adult : en effet la taille moyenne des arbres de décision peut être réduite jusqu\u0027à 50%. Un bémol toutefois : dans certains cas comme mushroom, k-means produit des clusters ayant des arbres de décision déjà bien simplifiés : notre méthode ne permet pas de les simplifier davantage.\nConclusion et perspectives\nDans cet article, nous avons présenté une méthode permettant d\u0027utiliser le résultat d\u0027une classification non supervisée pour simplifier efficacement les arbres de décision de chacun des clusters. La méthode procède par déplacements d\u0027éléments entre clusters en réduisant au fur et à mesure la taille des arbres de décision de chacun des clusters. Les futurs travaux concerneront la recherche d\u0027une solution similaire pour les flux de données.\n"
  },
  {
    "id": "380",
    "text": "Introduction\nDans le cas de la classification automatique comme pour la détection de communautés, après avoir construit une partition à l\u0027aide d\u0027une méthode, il convient d\u0027évaluer sa qualité. Pour ce faire, on peut faire appel à des critères externes ou internes. Les premiers permettent de comparer le résultat obtenu avec un résultat attendu, par exemple une partition faisant office de \"vérité terrain\" alors que les seconds quantifient la qualité de la partition proposée. Parmi les critères externes, utilisables aussi bien en classification automatique que pour la détection de communautés, on peut citer le taux de bien classés, la pureté, l\u0027indice de Rand ou sa version ajustée, l\u0027entropie ou encore l\u0027information mutuelle, éventuellement normalisée ou ajustée, mais aussi la V-mesure, l\u0027homogénéité ou la complétude (Hubert et Arabie (1985); Vinh et al. (2010); Rosenberg et Hirschberg (2007)).\nEn classification automatique, les critères internes peuvent eux-mêmes être subdivisés en critères dont l\u0027usage est spécifique à une distance ou à une méthode, comme par exemple l\u0027iner-tie intra ou interclasses, et en critères non spécifiques, comme les indices de Dunn, de Davies et Bouldin ou de Silhouette (Rousseeuw (1987); Dunn (1973); Davies et Bouldin (1979)) Dans le domaine de la détection de communautés, on pourra citer la couverture, la conductance, la performance ou encore le coefficient de clustering mais le critère le plus couramment employé est la modularité (Yang et Leskovec (2012)).\nIntroduite par Newman pour juger de la qualité d\u0027un partitionnement des sommets d\u0027un graphe, la modularité a ensuite été utilisée directement pour identifier des classes telles que les sommets à l\u0027intérieur d\u0027une classe soient fortement reliés et qu\u0027ils aient peu de relations avec ceux des autres classes (Newman (2006)). Bien que des travaux récents aient souligné des problèmes liés à l\u0027optimisation de ce critère pour déterminer un partitionnement optimal notamment, la limite de résolution rendant difficile la détection de classes de faible taille ou dans des graphes creux ou encore l\u0027existence de partitions à forte modularité dans des graphes sans structure communautaire, c\u0027est un critère qui a néanmoins produit de bons résultats dans la pratique, notamment pour la détection de communautés dans un réseau social (Guimera et al. (2004); Good et al. (2010); Lancichinetti et Fortunato (2011)). De plus, elle présente des propriétés intéressantes. Premièrement, elle est calculable sur des graphes valués ou non valués, et ne nécessite pas de normalisation préalable. Ensuite, elle repose sur des concepts intelligibles, où on cherche à former des classes entre sommets mieux reliés entre eux que dans une formation aléatoire. Enfin, contrairement à d\u0027autres critères, la modularité permet de comparer des partitions n\u0027ayant pas nécessairement le même nombre de classes. Cependant, la modularité ne peut pas être utilisée pour évaluer la qualité d\u0027un partitionnement dans un contexte de classification automatique et, à notre connaissance, il n\u0027existe pas de critère ayant les propriétés de la modularité de Newman qui soit adapté à des éléments décrits par des attributs vectoriels. C\u0027est la raison pour laquelle, dans cet article, nous définissons un critère de mesure de la qualité d\u0027une partition d\u0027éléments représentés par des vecteurs, inspirée de la modularité et qui pourra être utilisée pour comparer deux partitions. Ce critère sera décrit dans la section suivante. La section 3 est consacrée à l\u0027adaptation de ce nouveau critère à l\u0027heuristique de la méthode de Louvain. Nous proposons une nouvelle méthode de détection de communautés dans un réseau d\u0027information appelée 2Mod-Louvain, basée sur l\u0027optimisation en parallèle de la modularité de Newman et de la modularité que nous introduisons. Enfin, dans la section 4, à travers des expérimentations, nous évaluons les performances de cette méthode et sa robustesse à des dégradations des données. . Chaque élément v ? V est décrit par un vecteur d\u0027attributs qui, par souci de simplification des notations, est aussi noté v :\nOn suppose de plus qu\u0027une masse m v égale à 1 est associée à chaque élément v de V . La somme de ces masses est égale à N , le nombre d\u0027éléments de V .\nI(V ) désigne l\u0027inertie de V par rapport à son centre de gravité g ou simplement comme inertie interne de V ou moment centré d\u0027ordre 2 et défini de la façon suivante :\nde V par rapport à un élément v est la somme des carrés des distances entre les éléments de V et v.\nÉtant donnée une partition P \u003d {C 1 , . . . , C r } en r classes disjointes de V , Q inertie (P), le critère de mesure de la qualité de la partition P, que nous introduisons, est défini par :\noù c(v) est la classe de l\u0027élément v et ? est la fonction de Kronecker qui vaut 1 si ses arguments sont égaux et 0 sinon. Ainsi, alors que la modularité considère la force du lien et vise à regrouper les éléments les plus fortement liés, notre critère exploite la norme entre les éléments et vise à regrouper ceux qui sont les moins dissemblables ; ce qui apparait, après normalisation, dans le second terme de l\u0027équation :\nLe critère Q inertie compare la norme de chaque paire d\u0027éléments (v, v ) issus d\u0027une même communauté (contrôlé par la fonction de Kronecker), avec une valeur attendue d 2 (v, v ) définie par :\nIl s\u0027agit donc de comparer une fonction du carré de la distance entre v et v , à une fonction du carré des distances de chacun de ces éléments v et v aux autres éléments de V . Si la valeur attendue est plus grande que la valeur réelle, alors les deux éléments sont de bons candidats à appartenir à une même classe. C\u0027est la raison pour laquelle, dans le critère global Q inertie (P), la valeur observée est soustraite de la valeur attendue alors que dans le cas de la modularité c\u0027est la valeur attendue qui est retranchée de la force du lien observé.\nLe critère Q inertie (P) que nous proposons varie entre -1 et 1. En effet, la partie gauche de la soustraction (6), comprenant les produits d\u0027inerties pour toutes les paires de sommets, vaudra au plus 1. De même, la partie droite du critère Q inertie (P) (équation 5) ne pourra pas dépasser 1. Les deux parties étant strictement positives, le critère, contraint par les valeurs de la fonction de Kronecker, varie entre -1 et 1.\nCe critère présente plusieurs propriétés intéressantes. Premièrement, il conserve la même valeur, quelle que soit la transformation affine que l\u0027on applique aux attributs, autrement dit l\u0027ajout d\u0027une constante et/ou la multiplication par un scalaire des vecteurs associés aux élé-ments à classer n\u0027a pas d\u0027incidence sur la valeur du critère. Enfin l\u0027ordre des attributs n\u0027a aucune incidence sur le résultat.\nEn revanche, ce critère présente aussi certaines limites. Il est indéfini si les vecteurs numé-riques sont identiques, car l\u0027inertie totale est alors nulle. Ceci n\u0027est pas réellement un inconvénient lors de la détection de communautés dans un réseau d\u0027information car dans ce cas, les attributs n\u0027apportant aucune information, la détection des communautés sera basée uniquement sur les données relationnelles. De plus, comme la modularité de  \nMéthode 2Mod-Louvain\nComme nous l\u0027avons indiqué en introduction, une des applications immédiates du critère Q inertie est la détection de communautés dans un réseau d\u0027information représenté par un graphe avec attributs G \u003d (V, E) où V est l\u0027ensemble des sommets, E ? V ×V est l\u0027ensemble des arêtes et où chaque sommet v ? V est associé à un vecteur v \u003d (v 1 , . . . , v j , . . . , v T ) à valeurs réelles (Zhou et al. (2009)).\nDans cette section, en tirant parti du critère de modularité basée sur l\u0027inertie Q inertie , nous proposons une méthode, appelée 2Mod-Louvain, dédiée à la détection de communautés dans ce type de réseaux. Cette méthode basée sur le principe d\u0027exploration de la méthode de Louvain, exploite conjointement le critère Q inertie et la modularité de Newman Q N G (P) puisqu\u0027elle consiste à optimiser le critère global QQ + (P) défini par :\navec :\noù (v, v ) prend toutes les valeurs de V × V , k v est le degré du sommet v, A désigne la matrice d\u0027adjacence associée à G, m est la somme des poids de toutes les arêtes du graphe et ? est la fonction de Kronecker. Il n\u0027est pas utile de normaliser les deux critères Q N G (P) et Q inertie (P) car leurs bornes sont identiques comme indiqué dans la section précédente.\nLa méthode 2Mod-Louvain est détaillée dans l\u0027algorithme 1, qui comporte deux étapes. La première est une phase itérative qui vise à déplacer un sommet de sa classe vers celle d\u0027un de ses voisins dans le graphe si ce changement induit un gain de la modularité globale QQ + (P). La seconde est une phase de fusion qui consiste à construire un nouveau graphe dont les sommets correspondent aux communautés obtenues à l\u0027issue de la phase précédente. Cette seconde phase fait intervenir deux procédures Fusion_Matrice_Adjacence et Fusion_Matrice_Inertie. La procédure Fusion_Matrice_Adjacence est identique à celle mise en oeuvre dans la méthode de Louvain (Aynaud et al. (2010)). La procédure Fusion_Matrice_Inertie est décrite dans la section suivante. Placer u dans la communauté B;\n12\nMettre à jour la partition P suite au transfert de u dans B;  \nPar la suite, A \\ {u} désigne la classe A privée du sommet u et B ? {u} la classe B augmentée du sommet u. Dans un souci de simplification des notations dans la suite nous notons le terme D [v, v ] de la matrice abusivement D vv . La modularité basée sur l\u0027inertie de la partition P vaut :\nLa modularité de la partition P vaut quant à elle :\nLe gain de modularité lors du passage de P à P a donc pour valeur :\nDe plus, on peut remarquer que la variation de modularité induite par la suppression de u de sa classe d\u0027origine sera la même quelle que soit sa classe d\u0027affectation. Par conséquent le calcul de variation de modularité peut être effectué en considérant uniquement la différence induite par l\u0027insertion de u dans sa nouvelle communauté d\u0027affectation, décrite par le premier terme de l\u0027équation 20.\nCes calculs nous permettent de montrer que notre critère bénéficie lui aussi de la possibilité d\u0027être calculé de façon incrémentale. Le gain de modularité basée sur l\u0027inertie repose uniquement sur des informations locales relatives au sommet déplacé et à sa distance avec les autres sommets.\nÉvaluation de la méthode 2Mod-Louvain sur des réseaux artificiels\nDans cette section, nous nous proposons d\u0027évaluer la méthode 2Mod-Louvain qui optimise le critère global QQ + basé à la fois sur la modularité de Newman et la modularité par rapport à l\u0027inertie. Pour ce faire, nous étudions sa robustesse sur des réseaux artificiels vis-à-vis d\u0027une dégradation de la structure de communautés définie par rapport aux relations, ou des classes définies par rapport aux attributs, ou encore d\u0027une augmentation de la taille du réseau d\u0027information ou d\u0027une variation de la densité des liens.\nOn construit un réseau de référence R comportant 168 liens et 99 sommets uniformément répartis entre 3 catégories à l\u0027aide du modèle proposé par Dang en fixant à 2 le nombre d\u0027arêtes introduites avec chaque nouveau sommet (Dang (2012)). De plus, chaque sommet est décrit par une valeur d\u0027attribut réelle qui suit une loi normale d\u0027écart-type 7, centrée autour d\u0027une valeur propre à sa classe d\u0027origine. Ainsi la première classe a un centre µ 1 \u003d 10, la deuxième un centre µ 2 \u003d 40 et la troisième un centre µ 3 \u003d 70. La classe d\u0027origine du sommet sert de vérité terrain pour l\u0027évaluation. Le tableau 1 montre la répartition des arêtes entre les classes dans le graphe R.  (Combe et al. (2013) \nEn exploitant l\u0027information vectorielle en plus de l\u0027information relationnelle, la méthode 2Mod-Louvain gagne en robustesse par rapport à la méthode de Louvain face à une dégradation de l\u0027information relationnelle et par rapport à la méthode des K-Means en cas de dégradation de l\u0027information vectorielle. De plus, lorsque la taille du réseau augmente, la méthode proposée permet de parer à la multiplication des classes qui survient alors avec la méthode de Louvain (4 classes contre 10). Les K-means conservent de bons résultats dans le cas où la taille du ré-seau évolue, car l\u0027information des attributs demeure de bonne qualité mais ils sont globalement avantagés par rapport aux autres méthodes du fait que le nombre de classe est fourni en paramètre. Ainsi, l\u0027utilisation simultanée des deux types d\u0027information à travers des mesures de modularité adaptées permet de détecter plus efficacement des communautés dans des réseaux d\u0027information. Enfin, la méthode 2Mod-Louvain produit aussi des résultats meilleurs ou du même ordre que Totem, une autre méthode utilisant les deux types d\u0027information.\nConclusion\nDans cet article, nous avons étudié le problème du partitionnement de graphes avec noeuds et arêtes valués. En nous inspirant de la modularité de Newman et Girvan conçue pour la détection de communautés dans un réseau social, nous avons introduit une mesure de modularité basée sur l\u0027inertie. Cette mesure est adaptée pour évaluer la qualité d\u0027une partition d\u0027éléments représentés dans un espace vectoriel réel. Nous avons également présenté 2Mod-Louvain, un algorithme qui combine notre critère de modularité basée sur l\u0027inertie avec la modularité de Newman et Girvan pour détecter des communautés dans des réseaux d\u0027information. Nous avons démontré formellement que ce nouvel algorithme peut être optimisé dans sa phase itérative, lui permettant d\u0027être aussi efficient que l\u0027algorithme de Louvain lors d\u0027un passage à l\u0027échelle. Comme le montrent nos expérimentations, en exploitant de manière conjointe les données relationnelles et vectorielles, la méthode 2Mod-Louvain détecte plus efficacement \nSummary\nThe modularity was introduced by Newman to estimate the quality of a graph vertex partition but this measure doesn\u0027t consider the values describing the nodes in the graph. In this article, we introduce a new complementary modularity measure, based on the inertia and specially conceived to evaluate the quality of a partition over vector space elements and consequently. We propose 2Mod-Louvain, a method using our inertia based quality criteria combined with Newman\u0027s modularity in order to detect communities in information networks. Our experiments show that combining the relational information and the vector information when partitioning a network detects communities more efficiently than methods using only one type of information. Our method is in addition, more robust to data degradation.\n"
  },
  {
    "id": "381",
    "text": "Introduction\nLa prosopographie est une science auxiliaire de l\u0027histoire dont l\u0027objectif est d\u0027étudier les biographies des membres d\u0027une catégorie spécifique de la société, en particulier leur origine, leur carrière, leurs déplacements et leur environnement familial. Il existe à nos jours plusieurs bases de données prosopographiques souvent peuplées à partir de fiches de description de personnes et à capacité d\u0027interrogation réduite à l\u0027extraction d\u0027un ensemble de fiches. Le projet Personae propose d\u0027intégrer un ensemble de bases de données prosopographiques de la Renaissance au sein d\u0027un portail sémantique. Des mappings entre les ontologies locales de chaque base de données et l\u0027ontologie du portail assurent l\u0027accès à l\u0027ensemble des concepts présents dans les bases reliées. L\u0027interface d\u0027interrogation principale de ce portail repose sur des formulaires d\u0027interrogation classiques et les résultats retournés se présentent sous la forme de tableaux listant les réponses pertinentes dans les différentes bases. Malgré l\u0027apport de cette solution sur le plan d\u0027intégration sémantique et d\u0027agrégation des données de différentes bases, elle demeure inefficace quand à la présentation de vue synthétique sur les données prosopographiques mettant en évidence les relations entre plusieurs personnages, les étapes \"clés\" dans la carrière d\u0027un personnage, etc. Or, depuis plusieurs années le domaine du Visual Analytics n\u0027a cessé d\u0027apporter des réponses à ce genre de problème à travers différentes approches de visualisation de données permettant une exploration interactive et progressive de données Keim et al. (2008). Dans le cadre de ce projet, nous nous sommes appuyés sur des approches de visualisation de graphes et de cartes géographiques pour proposer deux interfaces de visualisation adaptées à la nature des données prosopographiques étudiées et répondant aux besoins d\u0027interrogation du portail mis en place. Les interfaces de visualisation proposées s\u0027appuient d\u0027une part sur des mesures de similarité et de proximité sémantique pour l\u0027identification et la construction de réseaux personnels et professionnels à la Renaissance. D\u0027autre part, elles exploitent les dimensions spatiales et temporelles très présentes en termes de prosopographie pour proposer des visualisations cartographiques retraçant la carrière d\u0027une personne ou les flux de déplacements entre les différents lieux de l\u0027époque. Le reste de l\u0027article est organisé comme suit. Dans la section 2 nous présentons ProsoGraph, l\u0027interface de visualisation de ré-seaux socio-professionnels. Ensuite, dans la section 3, nous présentons ProsoMap, l\u0027interface de visualisation cartographique spatio-temporelle. Enfin, dans la section 4, nous développons les conclusions de ce travail et exposons les travaux en cours et futurs en lien avec notre projet.\n2 ProsoGraph : Visualisation de graphes de réseaux socioprofessionnels de la Renaissance\nLe portail Personae intègre plusieurs bases de données de prosopographies de la Renaissance et offre ainsi une vue agrégée sur une grande quantités de données bibliographiques de personnages permettant d\u0027aboutir à des descriptions potentiellement complètes de la carrière d\u0027un personnage ainsi que du réseau socio-professionnel qu\u0027il ait pu développer. Ces données s\u0027apparentent naturellement à la définition de réseaux sociaux du fait de l\u0027existence de personnes avec des relations et interactions entre elles. Partant de ce fait et dans le but d\u0027assurer une meilleur exploration des données du portail, nous avons développé l\u0027interface ProsoGraph qui permet de visualiser les données prosopographiques sous la forme de graphes où les noeuds correspondent à des personnages, des lieux ou des professions de l\u0027époque et les arcs correspondent aux liens entre ces différents objets. ProsoGraph s\u0027appuie sur l\u0027API de visualisation de graphes Sigma JS (http ://sigmajs.org/). L\u0027interface communique avec le portail à travers des requêtes SPARQL utilisant l\u0027ontologie du portail et redistribuées ensuite sur l\u0027ensemble des bases pour constituer une réponse globale. Les réponses, au format XML, sont par la suite parsées pour construire et visualiser le graphe correspondant. Selon la nature de la requête formulée et du graphe obtenu, ProsoGraph permet différents types d\u0027exploration des données et d\u0027interprétation des réseaux obtenus. La figure 1 montre une illustration dans le cas où la requête permet d\u0027extraire le réseau sociau-professionnel d\u0027une personne (gauche) et dans le cas où la reqûete porte sur les intéractions autours d\u0027un lieu d\u0027intérêt (droite).\nN. Messai et T. Devogele\nFIG. 1 -Un exemple de réseau socio-professionnel (gauche) et d\u0027intéractions autour d\u0027un lieu d\u0027intérêt (droite)\nlayers.org) et des cartes issues de différentes sources (GoogleMap, OpenStreetMap, GeoPortail, etc.). La première représentation se focalise sur la carrière d\u0027un individu. La deuxième traite des flux de déplacements entre les différents lieux. Pour relier les lieux à des coordonnées géographiques, une URI Geonames(http ://www.geonames.org/) est utilisée. Il faut noter que des lieux historiques disparus peuvent être ajoutés à la base Geonames. L\u0027interface ProsoMap career retrace la trajectoire de vie d\u0027un individu. Par rapport à la représentation classique de la time geography, une représentation en 2D a été retenue, la durée de résidence de l\u0027individu dans un lieu est définie par la taille du cercle représentant ce lieu. Cette trajectoire est orientée par la nuance de gris (blanc pour le début de vie et noir pour la fin). L\u0027interface ProsoMap flow, représente les flux de déplacements d\u0027une population extraite du portail à l\u0027aide de plusieurs critères obligatoires tels que le lieu et optionnels tels que la période, la profession, etc. Pour chaque lieu, deux cercles concentriques sont dessinés. Le plus petit représente le nombre d\u0027individus de cette ville n\u0027ayant pas changé de ville durant la période. Le plus grand décrit le nombre d\u0027individus de cette ville ayant été dans la ville durant cette période. La ville étudiée est reliée aux villes d\u0027où provient une partie de la population ou d\u0027où part une autre partie durant cette période. La figure 2 donne une illustration des deux representations implémentées dans ProsoMaps.\nConclusion\nDans cet article, nous avons proposé deux interfaces complémentaires ProsoGraph et ProsoMap pour la visualisation de données prosopographiques. Ces interfaces offrent des approches génériques facilitant la navigation entre les concepts et l\u0027analyse visuelle impossible avec un grand nombre de fiches prosopographiques. Les deux interfaces communiquent avec le portail à travers le requêtage des concepts ontologiques de haut niveau : personnes, lieux, fonctions et des relations temporelles entre ces concepts. \nSummary\nThis paper presents two visualization tools ProsoGraph and ProsoMap developed within a collaborative project which focuses on the access and exploration of prosopographic datasets of the Renaissance period in France. The project aims at modeling and implementing a semantic portal ensuring access to existent prosopographic datasets. ProsoGraph and ProsoMap are developed based on social networks visualization techniques and spatio-temporal data visualization techniques, respectively. They interact with the portal through a semantic layer and allow additional advanced querying capabilities.\n"
  },
  {
    "id": "382",
    "text": "Introduction\nEn 1993, Rakesh Agrawal, Tomasz Imielinski et Arun N. Swami publiaient l\u0027un des articles fondateurs de la découverte de motifs \"Mining Association Rules between Sets of Items in Large Databases\" dans les actes de \"the ACM SIGMOD International Conference on Management of Data\" en introduisant le problème de l\u0027extraction des règles d\u0027association inté-ressantes. Formellement, ce problème consiste à énumérer toutes les règles du type X ? I où X est un ensemble d\u0027items et I un item non présent dans X telles que les probabilités P (X ? I) et P (X ? I|X) soient suffisamment élevées. Moins qu\u0027une finalité nouvelle, Agrawal et al. (1993) ont surtout substitué à la traditionnelle recherche heuristique, la recherche complète et consistante. En effet, le problème de la découverte de règles de classification (où I est seulement une valeur de classe) était déjà une thématique de recherche active dans le domaine de l\u0027intelligence artificielle mais les algorithmes n\u0027étaient pas exhaustifs (Breiman et al., 1984), (Quinlan, 1986), (Piatetsky-Shapiro, 1991). Ces notions de complétude et consistance très importantes en bases de données justifient la publication de Agrawal et al. (1993) dans ACM SIGMOD. De même, la généralisation proposée l\u0027année suivante par Agrawal et Srikant (1994) (où la conclusion de la règle est désormais un ensemble d\u0027items) est publiée dans VLDB 1 .\nDepuis 20 ans, la communauté de la découverte de motifs n\u0027a cessé de puiser son inspiration dans l\u0027article fondateur (Agrawal et al., 1993) comme en témoignent ses nombreuses citations : 26 ème article le plus cité de l\u0027informatique selon CiteSeer, le 5 ème le plus cité dans le domaine de l\u0027exploration de données selon Microsoft Acamedic Research, plus de 11 900 citations selon GoogleScholar 2 . Pour mieux définir ce qu\u0027est la découverte de motifs, revenons au problème initial qui se divise en deux sous-problèmes : (1) trouver tous les motifs ensemblistes (itemsets) présents dans au moins x% des transactions et (2) générer à partir de ces motifs toutes les règles d\u0027association intéressantes. Cette subdivision illustre les deux grandes problématiques qui ont animées la découverte de motifs pendant 20 ans : l\u0027extraction de motifs et l\u0027utilisation des motifs. Premièrement, l\u0027extraction de motifs consiste à énumérer tous les motifs d\u0027un langage (e.g., les itemsets ou les séquences) qui satisfont une contrainte (e.g., une fréquence minimale). Il est toutefois possible de se contenter d\u0027une représentation condensée de ces motifs i.e., une fraction des motifs qui garantit encore la regénération totale des règles. Ces trois dimensions (i.e., langage, contrainte et représentation) illustrées dans (Mannila et Toivonen, 1997) donnent lieu à un très grand nombre de problèmes. Deuxièmement, l\u0027utilisation des motifs consiste à combiner plusieurs motifs pour constuire des modèles plus complexes et/ou plus généraux. Typiquement, des classifieurs sont construits en regroupant des règles de classification, ellesmêmes dérivées de motifs ensemblistes.\nCet article ambitionne de faire une étude des travaux de la découverte de motifs réalisés de 1995 à 2011. Plutôt que de proposer une étude bibliographique basée sur quelques dizaines d\u0027articles et forcément parcellaire, nous avons opté pour une vision plus globale en s\u0027appuyant sur plusieurs centaines d\u0027articles. Il s\u0027agit de 1030 articles consacrés à la découverte de motifs issus des 6223 articles publiés dans les 5 conférences majeures de l\u0027exploration de données : KDD, PKDD, PAKDD, ICDM et SDM (le protocole est détaillé en section 2). Outre la vision globale, notre corpus d\u0027étude est suffisamment conséquent pour quantifier des phénomènes et ce dans le temps. Les traitements automatisés porteront sur les titres des publications tandis que nous nous appuierons parfois manuellement sur les résumés afin de lever certaines ambiguïtés. Dans un premier temps, nous proposons de positionner la découverte de motifs au sein de l\u0027exploration de données (section 3). Nous désirons également mieux cerner le domaine à la lumière des 3 dimensions mentionnées ci-avant : le langage, la contrainte et la représentation (section 4). Nous souhaitons pouvoir répondre à des questions simples : quels sont les langages les plus étudiés ? les motifs maximaux ont-ils été davantage étudiés que les générateurs ? etc. Au-delà des réponses, nous nous attachons à une quantification précise de ces phénomènes.  5 . Ce choix a peut-être tendance à exclure des travaux plus matures publiés en revue et à l\u0027inverse des travaux plus prospectifs publiés en atelier. De même, ce travail manque des travaux publiés dans des conférences connexes notamment en base de données (e.g., VLDB) ou en Recherche d\u0027Information (e.g., ICKM). Cependant, intégrer ces conférences à l\u0027étude aurait dilué l\u0027essence de l\u0027exploration de données (et donc, celle de la découverte de motifs). Au final, nous estimons que les 350 publications annuelles issues des 5 conférences retenues constitue un échantillon significatif pour une étude statistique de l\u0027ensemble de la production mondiale.\nCette étude porte sur les titres des publications indexées sous The DBLP Computer Science Bibliography 6 pour les 5 conférences. Même si le volume et les types de publication (e.g., articles longs, posters, tutoriaux, panels) varient selon la conférence et les années, la très grande majorité concerne des articles longs et courts. Par ailleurs, la première édition des conférences KDD, PAKDD et SDM n\u0027ont pas été indexées sous DBLP. Au final, le tableau 1 récapitule pour les 5 conférences l\u0027année de la première édition, l\u0027année de la première indexation des publications sous DBLP et le nombre total de publications indexées. Si les traitements automatisés portent exclusivement sur les titres, la validation manuelle de ces traitements s\u0027est appuyée sur les résumés lorsque le titre se révélait insuffisant. Le traitement de l\u0027intégralité des articles permettrait probablement d\u0027améliorer le filtrage automatique (à condition d\u0027utiliser des méthodes du TAL pour circonscrire avec justesse les contributions de l\u0027article analysé du reste).\nLa figure 1 (a) reporte le nombre de publications en exploration de données entre 1995 et 2011 pour chacune des conférences. Ce nombre n\u0027a cessé de croître sur ces 17 années (excepté en 2007 et en 2010) traduisant l\u0027essor du domaine. Cette augmentation globale s\u0027explique à la fois par la création de nouvelles conférences (jusqu\u0027en 2002) et l\u0027augmentation du nombre de publications pour chacune des conférences (e.g., plus 88% de publications depuis 2002).\nFrontière de la découverte de motifs\nLa difficulté majeure consiste à déterminer quelles sont les publications concernant la dé-couverte de motifs. Nous ne nous limitons pas aux travaux dédiés à l\u0027extraction de motifs mais considérons également les travaux où ces derniers sont utilisés (par exemple pour construire un modèle). Par motif, nous entendons motif local au sens de Hand (2002) i.e., qui ne décrit qu\u0027une portion de la base de données. Pour cette raison, nous considérons que les arbres de dé-cision, les réseaux bayésiens, les réseaux de neuronnes ou les machines à vecteurs de support (SVM) ne sont pas des travaux concernant l\u0027extraction ou l\u0027utilisation de motifs locaux. Nous décrivons maintenant le processus semi-automatique qui par raffinement successifs, a permis de déterminer les publications concernant la découverte de motifs.\n1. Filtrage automatique : Cette première étape a consisté à conserver tous les articles susceptibles de concerner la découverte de motifs. Nous avons retenu toutes les publications dont le titre mentionne un terme relatif à la découverte de motifs. Cette liste a été établie en nous appuyant sur les trois dimensions mentionnées en introduction :\n(a) Langage : pattern, item, sequence, rule, tree, graph, string\nBien sûr, cette liste de termes est élargie à leurs variations (e.g., pour le terme string, on considère substring, strings, etc). Ainsi, 1590 publications ont été retenues soit environ un quart de la base de données.\n2. Filtrage manuel : Cette seconde étape a consisté à éliminer tous les faux positifs i.e., les publications ne concernant pas la découverte de motifs mais contenues dans les 1590 publications obtenues à l\u0027étape 1. Pour ce faire, nous avons lu un à un chaque titre et si nécessaire, nous avons consulté le résumé de l\u0027article voire son intégralité pour lever toute ambiguïté. 1030 publications ont été retenues par le filtrage manuel sachant que le résumé a été consulté dans 126 cas.\nIl est clair que la solution de filtrage automatique a été priviligiée pour éviter de parcourir les 6223 publications. Même si le filtrage repose sur une liste de termes assez ouverte, des articles pertinents ont été manqués (i.e., des faux négatifs). Afin d\u0027estimer ce nombre de faux négatifs, nous avons tiré au hasard 100 publications parmi les 4633 éliminées à l\u0027étape 1. 5 de ces publications concernaient bien la découverte de motifs. Par conséquent, nous estimons qu\u0027environ 232 (\u003d 0.05 × (6223 ? 1590)) publications relatives à la découverte de motifs ont été manquées par notre approche. Dans la suite, nous faisons l\u0027hypothèse que les 1030 publications retenues sont un échantillon représentatif de la découverte de motifs.\nClairement notre approche repose sur une part de subjectivité à l\u0027étape 1 dans le choix des termes retenus et à l\u0027étape 2 dans l\u0027élimination des faux positifs. Nous pensons néanmoins que ce protocole n\u0027introduit pas plus de subjectivité qu\u0027une étude bibliographique traditionnelle.\nDécroissance de la découverte de motifs\nLe tableau 1 reporte le nombre de publications relatives à la découverte de motifs par confé-rence et calcule quelles proportions ces publications représentent. La découverte de motifs, en correspondant à environ 1 article sur 6 de l\u0027exploration de données, en constitue un réel sousdomaine. Parmi les 8489 auteurs qui ont contribués aux 5 conférences, 1685 d\u0027entre eux (soit 19, 85%) ont participé à au moins une publication en découverte de motifs. 851 auteurs (soit 10, 02%) se sont cantonnés à des publications en découverte de motifs.\nLa figure 1 (b) présente l\u0027évolution de la place de la découverte de motifs au sein de l\u0027exploration de données. Nous remarquons une décennie florissante entre 1997 et 2006 où la part de la découverte de motifs est supérieure à sa part moyenne de 17%. Pendant 5 années, entre 1998 et 2003, 1 article sur 5 est même consacré à la découverte de motifs, l\u0027âge d\u0027or de la dé-couverte de motifs en quelque sorte. Jusqu\u0027en 1999, l\u0027essor de la découverte de motifs est plus important que celui de l\u0027exploration de données pourtant conséquent (cf. figure 1). Il est certain que KDD, PKDD et PAKDD étaient les conférences privilégiées pour diffuser les travaux naissants relatifs à la découverte de motifs (Piatetsky-Shapiro, 2000). A l\u0027inverse, d\u0027autres travaux (e.g., les arbres de décision) disposaient déjà de tribunes reconnues avec les conférences en apprentissage et intelligence artificielle. Peut-être que la reconnaissance des conférences \"Data Mining\" et leur attractivité accrue a pu attirer davantage de ces travaux à partir de 2002. Quoiqu\u0027il en soit, depuis cette date, la part de la découverte de motifs a progressivement diminué jusqu\u0027à atteindre moins de 10% en 2011 (proche des 7% de 1995). Cette baisse relative se traduit aussi par une baisse absolue avec une chute de 99 articles en 2005 à 59 articles en 2011.\nPour finir, nous introduisons un nouvel indicateur afin d\u0027estimer le dynamisme d\u0027un domaine via son ensemble de publications. Pour commencer, la fraîcheur mesure le degré de nouveauté d\u0027une publication dans le domaine (par rapport à la période de 1995 à 2011) : F reshness(p) \u003d (year(p) ? 1995)/16 où year(p) est l\u0027année de la publication de p. Plus la fraîcheur d\u0027une publication est proche de 1, plus elle est récente i.e., proche de 2011. A l\u0027inverse, une fraîcheur égale à 0 signifie que la publication date de 1995. Nous étendons alors cet indicateur à un ensemble de publications P en en calculant la valeur moyenne : F reshness(P ) \u003d 1/|P | × p?P F reshness(p). Cette métrique donne une tendance grossière sur le dynamisme d\u0027un domaine via ses publications. Lorsque la fraîcheur d\u0027un ensemble de publications atteint 1, cela signifie que les publications se concentrent sur les dernières années de la période 1995-2011. Par exemple, la fraîcheur des 6223 publications retenues pour l\u0027étude est de 0,659 en raison de la hausse du nombre de publications annuelles. En comparaison, la fraîcheur des 1030 publications de la découverte de motifs est seulement de 0,590. Ce retrait traduit donc un affaiblissement de la découverte de motifs par rapport au reste de l\u0027exploration de données. Dans la suite, nous utiliserons également la fraîcheur pour caractériser le 4 Typologie des publications\nLes langages\nPréparation des données Nous utilisons à nouveau une démarche par filtrage automatique puis correction manuelle. Tout d\u0027abord, une liste de termes est associée à chaque type de langage comme indiqué par les deux premières colonnes du tableau 2 (sans les variantes). Aucun filtrage automatique n\u0027est procédé pour les langages portant sur des motifs relationnels et gé-nériques. Le langage \"generic\" correspond aux approches destinées à plusieurs langages (Mannila et Toivonen, 1997). Ces listes sont alors utilisées pour réaliser une première classification automatique de chaque publication sachant qu\u0027une partie n\u0027est pas classée (27.57% soit 284 articles). Ensuite, toutes les publications (y compris celles non-classées) sont parcourues manuellement pour une classification définitive (en utilisant le titre et le résumé). Lors de cette phase, il a été observé que par défaut le mot \"pattern\" réfère implicitement à \"itemset\" puisque 135 articles comportant ce mot correspondent aux motifs ensemblistes. La dernière colonne du tableau 2 reporte la fraîcheur des publications associées à chaque langage en mettant en gras celles avec une dynamique favorable par rapport à l\u0027exploration de données (\u003e 0, 659). Sans surprise, les règles d\u0027association et les motifs ensemblistes à l\u0027origine de la découverte de motifs sont les plus étudiés à hauteur d\u0027environ 2/3 de l\u0027ensemble. Ensuite, environ un quart concerne les séquences et les graphes. La découverte de motifs dans les données spatiotemporelles et les données relationnelles est assez marginale. Plus étonnant, nous constatons que très peu de travaux se sont attaqués à des approches génériques en terme de langage. Une explication probable est la difficulté de proposer des approches translangagières tant sur le plan théorique que sur le plan implémentation 7 . La fraîcheur élevée de ces articles (0,705) dénote cependant un intérêt plutôt récent pour ce type de travaux.\nComplexification des langages Le tableau 2 montre que globalement plus un langage est complexe, moins il y a d\u0027articles qui lui sont dédiés. Premièrement, la complexification des 7. Cependant, les articles de revue (absents de nos données) sont peut-être plus appropriés pour les approches translangagières faisant souvent la synthèse de travaux publiés précédemment pour des langages distincts. langages entraine une marginalisation de par la difficulté intrinsèque liée à la combinatoire du problème. En effet, plus un langage est complexe, plus il est difficile d\u0027extraire exhaustivement tous les motifs. Par exemple, avec 3 items, il est possible de former 80 séquences distinctes contre seulement 8 itemsets. Deuxièmement, cette complexification s\u0027inscrit dans le temps comme le décrit la figure 2 : aux itemsets succèdent les séquences, puis les graphes. En fait, la capitalisation de savoir-faire diminue de langage en langage le nombre de verrous à lever. Typiquement, les méthodes d\u0027élagage de l\u0027espace de recherche pour les motifs ensemblistes (fondées sur l\u0027anti-monotonie par exemple) sont transposables aux autres langages.\nFIG. 2 -Evolution du nombre de publications pour les principaux langages.\nNéanmoins, nous observons deux exceptions avec les arbres et les motifs ensemblistes qui sont respectivement moins étudiés que les graphes et les règles. Les arbres sont parfois simplifiés pour être traités comme une variante des séquences ou être traités comme un cas particulier des graphes. L\u0027exception la plus notable concerne les motifs ensemblistes plus simples que les règles d\u0027association et pourtant moins étudiés. Le fait que les règles soient particulièrement étudiées avant 2000 s\u0027explique d\u0027abord historiquement puisque l\u0027extraction de règles de classification était déjà une thématique de recherche importante en intelligence artificielle, bien avant 1993. De plus, l\u0027article fondateur de Agrawal et al. (1993) a désigné les règles d\u0027association comme objectif ultime au détriment des itemsets considérés comme un outils intermédiaire (même si techniquement, l\u0027obtention des itemsets est la phase la plus difficile). La faible fraî-cheur des articles concernant les règles (0,455) renforce l\u0027hypothèse de cet ancrage historique.\nLimite de la complexification Tandis que le nombre de publications concernant les règles et les itemsets diminue, les deux langages les plus complexes (les séquences et les graphes) prennent une part toujours plus grande au sein de la découverte de motifs (cf. la figure 2). Par exemple, sur les 4 dernières années, la proportion d\u0027articles consacrée aux graphes dé-passe celle consacrée aux règles. Néanmoins, cette complexification semble atteindre ses limites puisqu\u0027aucun langage successeur aux graphes (motifs relationnels ou spatio-temporels) ne semble pour l\u0027instant prendre la relève de manière significative. Ces données ne sont peutêtre pas à disposition en suffisamment grande quantité et il est possible que celles disponibles soient réduites à des langages plus simples comme les graphes. Etant donné la fraîcheur non négligeable pour le langage \"spatial\", cette conclusion provisoire pourrait être révisée prochainement. Enfin, à la complexification de la nature du langage pourrait succéder la complexifica- TAB. 3 -Répartition des publications suivant la contrainte.\ntion des caractéristiques des données, nous avons observé que certains mots clés sont de plus en plus présents dans les titres : données incertaines (\"uncertain\" avec une fraîcheur de 0,892), données massives (\"massive\" avec 0,729) ou données dynamiques (\"dynamic\" avec 0,687).\nLes contraintes\nPréparation des données Nous utilisons à nouveau une démarche par filtrage automatique puis correction manuelle. Tout d\u0027abord, une liste de termes est associée à chaque type de contraintes (cf. les deux premières colonnes du tableau 3). Ces listes sont utilisées pour réali-ser une première classification automatique de chaque publication mettant en avant la notion de contrainte. Ensuite, les 510 publications classées sont parcourues manuellement pour une classification définitive en utilisant le titre et le résumé. Le terme \"significant\" regroupe les articles distinguant les motifs statistiquement valides des autres tandis que \"interestingness\" se rapporte à des approches plus variées souvent fondées sur des connaissances subjectives. A noter que le terme \"generic\" correspond aux approches dédiées à une classe de contraintes. Comme pour les langages (et pour les mêmes raisons), peu de publications sont consacrées aux contraintes génériques même si ces dernières sont plutôt récentes (cf. leur fraîcheur de 0,637).\nL\u0027obsession de la fréquence Globalement, la contrainte de fréquence minimale en représen-tant 50% des publications est de très loin la plus utilisée. En effet, de nombreuses publications s\u0027attaque au sous-problème 1 (présenté en introduction) pour proposer une méthode plus efficace, pour l\u0027adapter à un langage différent, pour se limiter à une représentation condensée, etc. Plus qu\u0027une nécessité applicative, nous pensons que cette utilisation récurrente de la contrainte de fréquence découle du paradigme imposé par l\u0027article originel de Agrawal et al. (1993). Premièrement, remplacer la contrainte de fréquence par une autre plus sélective, c\u0027est remettre en cause la regéneration exhaustive de toutes les règles intéressantes (le sous-problème 2). Hors cette exhaustivité est un fondement du paradigme qui entraine une surabondance assumée : \"When we started doing data mining, we were concerned that we were generating too many rules, but the companies we worked with said, \u0027this is great, this is what exactly what we want !\u0027 \" (Winslett, 2003). Dès lors, la contrainte inspire toujours la peur de trop ou mal filtrer ce qui conduirait à perdre des motifs pertinents. Cette crainte s\u0027illustre aussi avec l\u0027obsession de diminuer le seuil de fréquence minimal quitte à extraire des motifs non-significatifs.\nDeuxièmement, l\u0027évaluation de l\u0027approche dans (Agrawal et al., 1993) ne repose pas sur l\u0027évaluation de la qualité des motifs extraits comme on valide par exemple la qualité d\u0027un classifieur avec une validation croisée. Plus généralement, la plupart des articles d\u0027extraction de motifs n\u0027évaluent pas la qualité des motifs extraits mais la rapidité de leur extraction ou la quantité de mémoire requise. De ce point de vue, améliorer le processus d\u0027extraction de motifs revient à diminuer le coût temporel et/ou spatial, mais surtout pas à en modifier le résultat, i.e., les motifs fréquents. De plus, la contrainte de fréquence minimale, quel que soit le langage dispose de propriétés intéressantes (souvent issues de l\u0027anti-monotonie) qui facilitent l\u0027extraction. L\u0027évaluation d\u0027une méthode fondée sur une autre contrainte est alors doublement désavantageuse. De manière grossière, comme une contrainte pertinente ne satisfait en général pas la propriété d\u0027anti-monotonie, l\u0027algorithme d\u0027extraction associé sera donc moins efficace que l\u0027extraction des motifs fréquents. Par ailleurs, il est difficile de montrer que les motifs extraits suivant une nouvelle contrainte sont de meilleure qualité que ceux extraits avec la contrainte de fréquence minimale car il n\u0027existe pas de protocole objectif de validation.\nFIG. 3 -Evolution du nombre de publications pour les principales contraintes.\nVers plus de qualité Désormais, quelque soit le langage, l\u0027extraction des motifs fréquents est une tâche maîtrisée. Pour cette raison, le nombre de publications mettant en avant les motifs fréquents diminue spectaculairement depuis 2005 (cf. la figure 3) expliquant en partie la dé-croissance de la découverte de motifs. Le défi combinatoire de la découverte de motifs cède sa place à celui de la qualité des motifs extraits. Ainsi, le recours à une contrainte pour affiner le filtrage gagne en légitimité suivant une perspective envisagée par Agrawal : \"we need work to bring in some notion of \u0027here is my idea of what is interesting,\u0027 and pruning the generated rules based on that input.\" (Winslett, 2003). Cependant, la définition de ces contraintes demeure une problématique complexe. Déjà, Fayyad et al. (2003)  Malgré cette difficulté, la fraîcheur de certaines catégories et termes semble dessiner un renouveau de l\u0027extraction de motifs sous contrainte. Même si la fraîcheur des contraintes dé-diées aux contrastes est seulement de 0,596, on observe des fraîcheurs élevées de 0,893 pour \"discriminative\", de 0,812 pour \"contrast\" et de 0,736 pour \"subgroup\". Ce regain d\u0027intérêt est aussi marqué pour les motifs significatifs (avec une fraîcheur de 0,651) et surtout pour les contraintes d\u0027utilité (avec une fraîcheur de 0,724). Cette dynamique est également visible sur le graphique de droite de la figure 3. Enfin, une autre voie connexe au filtrage par seuillage TAB. 4 -Répartition des publications suivant le type de représentation condensée.\nest le classement des motifs en utilisant une mesure pour les ordonner comme l\u0027illustrent les termes \"ranking\" et \"top-k\" avec une fraîcheur respective de 0,823 et 0,762.\nLes représentations condensées\nPréparation des données Nous utilisons à nouveau une démarche par filtrage automatique puis correction manuelle. Les deux premières colonnes du tableau 4 donne les représentations condensées et leurs termes associés. Ces listes permettent de classer automatiquement chaque publication en fonction de son titre. Ensuite, les 117 publications classées sont vérifiées manuellement pour une classification définitive en utilisant le titre et le résumé. La fraîcheur de chaque type de représentation condensée est indiquée dans la dernière colonne. Pour rappel, l\u0027objectif des représentations condensées est de réduire les redondances entre motifs (Calders et al., 2004). Les notions de bordures se contentent des motifs les plus spéci-fiques ou les plus généraux au sens de l\u0027inclusion. Les motifs fermés et générateurs (libres ou clés) exploitent le même principe mais à fréquence égale. A noter que le terme \"other\" regroupe des articles se concentrant principalement sur les bases génériques de règles d\u0027association.\nSuccès des représentations condensées : bordures puis motifs fermés 11,35% des publications relative à la découverte de motifs exploitent la notion de représentation condensée. Ce succès découle de leur indéniable bénéfice et de leur validation aisée (i.e., un contexte mé-thodologique opposé à celui des contraintes). Le concept de représentation condensée s\u0027est rapidement imposée car il concilie la diminution du nombre de motifs et la conservation de l\u0027exhaustivité via la regénération. De ce point de vue, elle s\u0027inscrit parfaitement dans la perspective du sous-problème 2. En outre, les travaux sur les représentations condensées sont faciles à évaluer. D\u0027une part, la validité de la regénération peut être formellement démontrée. D\u0027autre part, la qualité de la condensation peut être estimée empiriquement en calculant le ratio de compression. Le plus souvent, ce gain en compression s\u0027accompagne d\u0027un gain de vitesse et d\u0027une diminution des ressources mémoires consommées.\nParmi les différentes représentations, les bordures constituent la première représentation à succès à avoir été proposée (cf. figure 4) même si le nombre de travaux concernant les bordures décroît régulièrement depuis 1997-2000. Par nature, ces motifs maximaux/minimaux ont des propriétés extrêmes (e.g., fréquence très basse) et ne permettent pas d\u0027inférer les propriétés des autres motifs (e.g., déduire la fréquence d\u0027un motif plus général). Les motifs libres et fermés en dépassant ces limites se sont largement imposés. Finalement, le succès écrasant des fermés face aux générateurs s\u0027explique par une conjonction de facteurs : moins nombreux, plus faciles à extraire et validité statistique plus forte (car ils véhiculent plus de régularités que les libres).\nFIG. 4 -Evolution du nombre de publications pour les représentations condensées.\nBonne représentation mais mauvais modèle Désormais, les techniques concernant les représentations condensées sont bien maîtrisées et ce pour la plupart des langages. Le nombre de publications sur cette thématique a atteint son paroxysme entre 2005 et 2008. Seules les publications dédiées aux motifs générateurs et fermés se maintiennent dans le paysage de la découverte de motifs. Cependant, la taille des représentations condensées (exactes ou approximatives) reste trop importante pour autoriser une analyse globale des motifs. Il est alors né-cessaire de mettre en oeuvre d\u0027autres mécanismes pour réduire leur taille soit en opérant un filtrage individuel des motifs (usage de contrainte) ou en opérant un filtrage collectifs des motifs (construction de modèle). Cette dernière option ressemble à l\u0027objectif initial des représenta-tions condensées mais en l\u0027étendant à des motifs non-comparables (par rapport à l\u0027inclusion). Cette direction peut être vue comme une valorisation des motifs : \"to make frequent pattern mining an essential task in data mining, much research is needed to further develop patternbased mining methods.\" (Han et al., 2007). Les termes \"collection\" et \"pattern-based\" avec une fraîcheur respective de 0,739 et 0,723 montre un intérêt récent pour cette piste.\nConclusion\nLe titre \"Extraction efficace des motifs fréquents fermés\" serait probablement le meilleur résumé de la découverte de motifs, reprenant tous les codes de l\u0027article originel : être consistant et exhaustif pour régénérer ; et ce le plus efficacement possible. De facto, tout ce qui concerne l\u0027efficacité de l\u0027extraction est désormais maîtrisé nous laissant face au principal problème : parmi l\u0027abondance de motifs, trouver ceux qui sont réellement pertinents pour l\u0027utilisateur. Traiter de nouveaux langages et de nouvelles sortes de données a juste déplacé ce problème ; utiliser des représentations condensées l\u0027a à peine atténué. De nombreux travaux récents s\u0027attaquent à ce challenge en s\u0027appuyant sur un post-traitement pour sélectionner un sous-ensemble de motifs, sur des filtrages plus subtils ou sur des classements élaborés.\nEnfin, si la flexibilité offerte par le choix du langage et de la contrainte offre un spectre d\u0027applications a priori large, l\u0027ancrage applicatif de la découverte de motifs reste moins fort que celui de l\u0027exploration de données. En effet, de nombreux termes orientés application tels que \"mobility\", \"multi-document\", \"patent\", \"multi-task\", \"advertising\", \"malware\", \"media\", etc ; caractérisent les titres de l\u0027exploration de données avec une fraîcheur supérieure à 0,9. A\n"
  },
  {
    "id": "385",
    "text": "Introduction\nLa Catégorisation de textes joue un rôle très important dans la recherche d\u0027information et la fouille de textes. Cette tâche a été couronnée de succès en faisant face à une grande variété d\u0027applications. Ce succès est du principalement à la participation croissante de la communauté d\u0027apprentissage machine. Dans ce travail, nous nous intéressons à l\u0027algorithme des K-plus proches voisins (Cover et Hart, 1967). Ce dernier développé tout d\u0027abord par (Fix et Hodges, 1989) est devenu l\u0027un des algorithmes les plus populaires dans la catégorisation de textes. Il est robuste et placé parmi les meilleurs algorithmes (Sebastiani, 2002). Toutefois, il présente certaines limites, (i) stockage mémoire énorme car il faut stocker l\u0027ensemble complet d\u0027apprentissage et (ii) coût élevé de calcul car il doit explorer l\u0027ensemble d\u0027apprentissage en entier pour pouvoir classer un nouveau document. Une solution intéressante à base d\u0027automate cellulaire appelée CAkNN (Cellular Automaton combined with k-NN) a été proposée dans (Barigou et al., 2012) pour réduire le temps de classification, dans le cadre du filtrage de spam. Les expériences réalisées sur le corpus LingSpam ont montré que la méthode CAkNN permet d\u0027atteindre de meilleures performances de classification comparée à d\u0027autres travaux publiés dans le domaine de filtrage de spam. Dans ce papier, nous allons reprendre cette solution pour la catégorisation de textes et nous allons montrer à travers un ensemble d\u0027expériences que CAkNN permet de réduire le temps de classification par une sélection d\u0027un minimum d\u0027instances d\u0027apprentissage pour la classification d\u0027un nouveau document et ceci sans que la performance prédictive n\u0027en soit affectée. Ce papier est organisé comme suit :la section 2 est dédiée aux travaux connexes. Dans la section 3 nous décrivons le principe de la méthode KNN. La section 4 décrit notre contribution pour améliorer cette méthode. Les expériences et les résultats sont présentés dans la section 5, et la conclusion est donnée dans la section 6.\nTravaux connexes\nDifférentes solutions ont été proposées pour réduire la complexité de calcul. Comme le souligne (Bhatia et SSCS, 2010), nous distinguons les méthodes de sélection d\u0027instances et les méthodes de réduction du temps de calcul. Les premières visent la réduction du nombre d\u0027exemples dans la base d\u0027apprentissage par certaines techniques d\u0027édition en éliminant certains exemples qui sont redondant dans un certain sens (Gates, 1972). Les deuxièmes méthodes accélèrent la procédure de recherche lors de la classification par la mise en structures bien organisées de l\u0027ensemble d\u0027apprentissage (Liu et al., 2006). Cependant, pour des dimensions très importantes, l\u0027espace requis croit d\u0027une manière exponentielle.\nL\u0027algorithme des K-plus proches voisins\nL\u0027algorithme des k-plus proches voisins (KNN) est une méthode d\u0027apprentissage à base d\u0027instances. Il ne comporte pas de phase d\u0027apprentissage en tant que telle. Les documents faisant partie de l\u0027ensemble d\u0027apprentissage sont seulement enregistrés. Lorsqu\u0027un nouveau document à classer arrive, il est comparé aux documents d\u0027apprentissage à l\u0027aide d\u0027une mesure de similarité. Ses k plus proches voisins sont alors considérés : on observe leur catégorie et celle qui revient le plus parmi les voisins est affectée au document à classer. La méthode utilise donc deux paramètres : le nombre k et la fonction de similarité. Une mesure de similarité très utilisée et que nous avons adoptée dans ce papier est la similarité cosinus (équation 1), qui consiste à quantifier la similarité entre deux documents en calculant le cosinus de l\u0027angle entre leurs vecteurs.\nNous définissons f comme étant la fonction KNN qui attribue une classe à une nouvelle instance Q. Dans notre cas, Cette fonction, utilise le vote majoritaire pondéré donné en équation 2.\nMéthode proposée\nDans cette section nous reprenons la méthode étudiée dans (Barigou et al., 2012) pour cette fois-ci la catégorisation de textes. Nous proposons une solution originale permettant de surmonter l\u0027un des inconvénients majeurs de la méthode KNN qui est le coût de classification dans une tâche comme la catégorisation de textes où nous manipulons des milliers de documents voire des milliers de milliers. Le principe de cette méthode est comme suit : au lieu de faire participer toutes les instances d\u0027apprentissage pour la recherche des k-voisins ce qui va augmenter le temps de calcul, une sélection d\u0027un sous ensemble réduit d\u0027instances est tout d\u0027abord réalisée. Cette opération de sélection a comme conséquence une réduction significative du temps de classification. L\u0027approche proposée utilise la machine cellulaire CASI (Atmani et Beldjilali, 2007) \nReprésentation des instances d\u0027apprentissage\nNous proposons une nouvelle stratégie de représentation des documents d\u0027apprentissage ; ces derniers vont être encodés dans une structure cellulaire. L\u0027ensemble d\u0027apprentissage est tout d\u0027abord pré traité pour construire l\u0027index. Nous distinguons trois étapes :\n1. établir une liste initiale de termes en effectuant une segmentation de texte en mots ; 2. éliminer les mots inutiles en utilisant une liste prédéfini de mots vides et enfin ; 3. utiliser une variante de l\u0027algorithme de Porter pour effectuer la racinisation des différents mots retenus. Les termes sont liés à leurs documents par deux matrices de voisinage IM et OM :\nest une prémisse de r) Alors IM(t,r)\u003d1 Sinon IM(t,r)\u003d0 ; -?d ? {d\nSélection des instances\nLe processus de sélection permet de déterminer la contribution de chaque document d\u0027apprentissage. Les documents ayant un plus grand nombre de termes communs avec l\u0027instance à classer seront sélectionnés pour participer dans sa classification. Nous allons tout d\u0027abord, définir les concepts suivants :\n1. NTT :le Nombre Total des Termes du vocabulaire V trouvés dans Q. ; 2. T(? ) : le seuil défini par :\nLe processus de sélection des instances passe par trois étapes : -Initialisation de la couche CelTerm ; -Recherche de l\u0027ensemble des documents A \u003d {d i } tel que d i ? Q \u003d ? ; -Recherche du sous ensemble E ? A vérifiant la condition donnée dans (l\u0027inéquation 5). La recherche des instances est réalisée par l\u0027application de la fonction booléenne globale ?f act • ?rule qui va récupérer les documents partageant au moins un terme avec Q (sous ensemble A). Les deux fonctions booléennes sont définies comme suit :\n?rule : (ET, IT, ST, ER, IR, SR) ? ?? ? ?rule(ED + (OM * ER), ID, SD, ER, IR, ¬(ER)). chaque document d i de l\u0027ensemble A se voit attribuer une valeur N T C(d i ).\nCette valeur correspond au nombre total de cellules actives obtenues par le produit booléen du vecteur ET de la couche CelTerm avec le vecteur OM T (d i ) Le seuil T(?) est ensuite appliqué afin de réduire davantage les données d\u0027apprentissage et obtenir l\u0027ensemble E qui sera utilisé par la méthode KNN.\nÉtude expérimentale et Résultats\nDans ce qui suit nous évaluons la solution proposée et la comparons avec la méthode KNN.\nCorpus et mesures de performances\nNous utilisons, dans ce papier, le corpus 20 NewsGroups. Ce corpus traite 20 catégories, chaque catégorie représente 5% du corpus, il contient au total 18828 documents. Nous avons utilisé 80% du corpus pour l\u0027apprentissage et 20% de ce corpus pour le test. Pour évaluer les performances des deux méthodes KNN et CAkNN, nous calculons pour chaque catégo-rie la mesure F 1 (ci) (Sebastiani, 2002), donnée par la formule suivante : F 1(ci) \u003d 2 * ? * ? ?+? avec ? la précision et ? le rappel. La mesure F1 globale, sur toutes les classes, est calculée à travers une moyenne des résultats obtenus pour chaque catégorie. Le temps nécessaire pour classifier une nouvelle instance est calculé comme suit : dans le cas de la méthode KNN, ce temps considère le parcours de l\u0027ensemble d\u0027apprentissage D en entier. Par contre, dans le cas de CAkNN, le temps de classification est calculé en sommant le temps de sélection du sous ensemble E avec le temps de classification en considérant seulement le sous ensemble E.\nRésultats expérimentaux\nLes figures 1 et 2 regroupent les résultats obtenus pour le corpus 20Newsgroups dans le cas de ? \u003d 5. A partir de ces figures nous observons la contribution de la méthode CAkNN, qui consomme moins de temps, tout en restant plus performante que la méthode KNN.\nFIG. 1 -Performance de classification du corpus 20NG en fonction du seuil K.\nFIG. 2 -Temps de classification de 20 % du corpus 20NG en fonction du seuil K.\nNous en dégageons deux résultats intéressants, le premier concerne l\u0027efficacité de l\u0027approche. La qualité de prédiction de CAkNN est meilleure que celle du classifieur KNN. Le deuxième concerne la réduction du temps de classification obtenue grâce à la réduction drastique des instances d\u0027apprentissage. Les résultats indiquent que l\u0027écart entre les résultats avant et après application de la méthode CAkNN sont suffisamment significatifs.\nConclusion\nDans ce papier, nous avons proposé une nouvelle solution pour améliorer le temps de classification de la méthode KNN. Nous n\u0027avons pas besoin de tout l\u0027ensemble d\u0027apprentissage pour classifier une nouvelle instance. Dans ce travail, nous proposons de sélectionner un sous ensemble réduit de documents pour cette tâche de catégorisation. Ce problème de sélection est traduit en un problème de manipulation d\u0027opérations booléennes par l\u0027utilisation de l\u0027automate cellulaire de la machine CASI. Cet automate est premièrement censé filtrer les instances pouvant produire du bruit et deuxièmement, assurer la convergence de l\u0027algorithme KNN en un temps de calcul intéressant. Comme perspective, nous prévoyons une étude comparative entre la méthode CAkNN et les autres solutions proposées dans (Bhatia et SSCS, 2010)  (Gates, 1972), (Hart, 1968) et (Wilson et Martinez, 1989).\nRéférences Atmani, B. et B. Beldjilali (2007). Knowledge discovery in database : Induction graph and cellular automaton. Computing and Informatics Journal 26,2, \n"
  },
  {
    "id": "386",
    "text": "Résumé. Dans cet article nous présentons une approche conceptuelle d\u0027aide à la décision dans la conception de systèmes complexes. Cette approche s\u0027appuie sur le formalisme de l\u0027analyse de concepts formels par similarité (ACFS) pour la classification, la visualisation et l\u0027exploration de données de simulation afin d\u0027aider les concepteurs de systèmes complexes à identifier les choix de conception les plus pertinents. L\u0027approche est illustrée sur un cas test de conception de cabine d\u0027un avion de ligne fourni par les partenaires industriels et qui consiste à étudier les données de simulation de différentes configurations du système de ventilation de la cabine afin d\u0027identifier celles qui assurent un confort convenable pour les passagers la cabine. La classification des données de simulation avec leurs scores de confort en utilisant l\u0027ACFS permet d\u0027identifier pour chaque paramètre de conception simulé la plage de valeurs possibles qui assure un confort convenable pour les passagers. Les résultats obtenus ont été confirmés et validés par de nouvelles simulations.\nIntroduction\nLa phase de conception de systèmes est une étape cruciale dans le processus de production des systèmes complexes au cours de laquelle plusieurs aspects sont étudiés pour garantir la performance du système ainsi que sa conformité aux besoins de l\u0027utilisateur final. Ces aspects sont souvent traduits sous la forme d\u0027un ensemble de paramètres de conception et de contraintes associées qui font l\u0027objet de nombreuses simulations. Lorsqu\u0027il s\u0027agit de la conception de systèmes complexes, les simulations produisent des données volumineuses qui doivent par la suite être analysées afin d\u0027identifier les configurations optimales. Dans ce contexte, le recours aux outils d\u0027aide à la décision est essentiel pour aider les concepteurs à faire des choix rationnels. Il existe plusieurs méthodes et outils d\u0027aide à la décision qui ont été utilisés dans divers domaines d\u0027application tels que l\u0027économie, l\u0027industrie, etc. Ehrgott et al. (2010). Le choix d\u0027une méthode en particulier dépend à la fois des données à analyser (format, volume, etc.) et de la méthode elle-même (performance, visualisation, etc.). Un état de l\u0027art sur les principales approches d\u0027aide à la décision dans le contexte industriel est donné dans Aviso et al. (2008). Dans ce travail, nous sommes particulièrement intéressés par les approches qui s\u0027appuient sur des structures conceptuelles telles que les treillis de concepts et le formalisme d\u0027Analyse de Concepts Formels (ACF) associé Ganter et Wille (1999). L\u0027utilité de l\u0027utilisation des treillis est prouvée par les nombreuses approches qui s\u0027appuient sur cette structure comme support pour la navigation en recherche d\u0027information, comme ensemble réduit de motifs et de règles d\u0027association en fouille de données, comme un ensemble d\u0027arbres de décision en apprentissage automatique, comme outil de prédiction et d\u0027aide à la décision, etc. Ganter et al. (2005). Cependant, malgré ces nombreuses applications réussies, les approches basées sur l\u0027ACF sont souvent confrontées à la rigidité de son format d\u0027entrée qui nécessite que les données soient représentées par une relation binaire ce qui est loin d\u0027être le cas des données réelles. Afin de contourner cette limite, l\u0027ACF propose de procéder à des échelonnages des données non binaires pour les transformer en binaires. Cette binarisation est subjective et souvent accompagnée d\u0027une perte d\u0027information. Pour palier à cette limite, plusieurs travaux ont proposé des extensions du formalisme de l\u0027ACF à des données non binaires Ferré et Ridoux (2000); Ganter et Kuznetsov (2001); Messai et al. (2008). Parmis ces méthodes, l\u0027ACF par Similarité (ACFS) considère la similarité entre les données pour les grouper en treillis appelés treillis de concepts multi-valués (MV) Messai et al. (2008). En s\u0027appuyant sur la similarité entre les données, l\u0027ACFS permet de générer différents treillis MV avec différents niveaux de granularité ce qui permet l\u0027exploration progressive de données Messai et al. (2010).\nDans ce travail nous nous appuyons sur l\u0027ACFS pour définir une approche de classification et d\u0027analyse de données de simulation. Cette approche prend en compte les deux aspects quantitatif (les valeurs numériques des paramètres) et qualitatif (le confort dans la cabine) des données de simulation afin d\u0027aider les concepteurs dans leurs choix de conception de systèmes complexes. L\u0027approche proposée est appliquée à un cas test fourni par les partenaires industriels du projet et qui correspond à un système de ventilation d\u0027une cabine d\u0027avion. Les données de simulation relatives à ce cas test ont été classées et analysées à l\u0027aide des treillis MV pour identifier les configurations de paramètres de conception qui assurent le confort des passagers dans la cabine. L\u0027analyse est facilitée par deux techniques de visualisation de treillis MV proposées dans ce travail.\nLe reste de l\u0027article est organisé comme suit : La section 2 présente le contexte d\u0027étude et le cas test cabine l\u0027avion. La section 3 rappelle les définitions de base de l\u0027ACFS. La Section 4 décrit brièvement deux techniques de visualisation de treillis MV pour l\u0027aide à la décision. La Section 5 discute les détails de l\u0027application de l\u0027ACFS au cas test, les résultats obtenus et l\u0027évaluation des ces résultats. La section 6 conclut ce travail.\n2 Contexte de l\u0027étude : la conception collaborative de systèmes complexes\nLe présent travail de recherche s\u0027inscrit dans le cadre du projet CSDL 1 qui implique 27 partenaires industriels et académiques et vise à fournir un environnement collaboratif pour la conception de systèmes complexes. Les différentes solutions proposées sont étudiées à travers une phase de simulation qui produit des données volumineuses. La plate-forme visée par le projet devrait permettre une analyse efficace de telles données afin d\u0027identifier les configurations pertinentes des paramètres qui permettent de valider les choix de conception du système.\nFIG. 1 -Paramètres de conception du cas test cabine d\u0027avion (gauche) et leurs intervalles de valeurs pour les simulations (droite).\nLes partenaires industriels du projet ont fourni un cas test qui correspond à un système de climatisation d\u0027une cabine d\u0027un avion de ligne. Pour ce cas test, l\u0027objectif est d\u0027identifier les configurations de conception pertinentes qui assurent des conditions de confort en terme de température et de vitesse de l\u0027air à l\u0027intérieur de la cabine. Les intervalles de variations des paramètres de conception dans ce cas test ainsi que l\u0027ensemble des simulations sont obtenus en utilisant le modèle de calcul à éléments finis proposé dans Bui et al. (2011). Les 13 paramètres de conception suivants sont considérés dans ce cas test : les angles d\u0027injection de l\u0027air au niveau des 4 sièges des passagers (Alpha_1..4), la vitesse de l\u0027air (Uair_1..4), la température de l\u0027air soufflé à l\u0027entrée principale (Tair_In), la température de l\u0027air soufflé par le ventilateur principal (Tair_P), la vitesse de l\u0027air soufflé à l\u0027entrée principale (Uair_In), la température extérieure (T_Ext), et la conductivité du fuselage thermique (Kappa_F). Les valeurs moyennes de température et de vitesse pour chacun des sièges des quatre passagers ( Figure 1) ont été calculées pour évaluer le confort des passagers. Cela a abouti à huit critères de sortie (deux par passager) liées au confort auxquels s\u0027ajoute une mesure de l\u0027énergie consommée par le système de climatisation utilisée pour estimer le prix associé au confort.\nDans le reste de cet article, nous proposons une approche basée sur l\u0027ACFS pour la classification et la visualisation des données de simulation relatives au cas test cabine d\u0027avion.\n3 Introduction à l\u0027Analyse de Concepts Formels par Similarité Messai et al. (2008Messai et al. ( , 2010) est une méthode de classification et d\u0027analyse des données qui étend l\u0027ACF à des données complexes représentées par des contextes multi-valués  \nPour un seuil de similarité ?, l\u0027ensemble de tous les intervalles de valeurs similaires possibles qui peuvent être définis sur W , noté par I ? , est l\u0027ensemble des intervalles de la forme\nLe choix de ? reflète les exigences en précision à prendre en compte lors de l\u0027analyse des données. Un faible seuil ? signifie que seules les valeurs les plus proches seront considérés comme similaires tandis qu\u0027un ? plus élevé signifie que des valeurs dont la différence est élevée peuvent également être considérées comme similaires. Le choix du seuil dépend ainsi fortement des données à analyser et de l\u0027objectif de l\u0027analyse. Il est possible de choisir soit le même seuil de similarité pour tous les attributs d\u0027un contexte soit un seuil distinct pour chaque attribut. Dans ce dernier cas, ? est un vecteur (? i ) 0?i\u003c|M | de seuils élémentaires correspondant aux attributs du contexte.\nEn s\u0027appuyant sur la similarité entre les valeurs des attributs, l\u0027ACFS étend la définition de partage d\u0027attributs entre objets comme suit. Etant donné deux objets \nPlus généralement, l\u0027ensemble maximal valide contenant A par rapport à B ? M est : En s\u0027appuyant sur ces ensembles maximaux, l\u0027ACFS définit les opérateurs de dérivation suivants pour A ? G et B ? M × I ? :\nA ? est l\u0027ensemble maximal d\u0027attributs MV partagé par tous les objets dans A et B ? est l\u0027ensemble maximal d\u0027objets qui partage tous les attributs MV dans B. Il a été démontré dans Messai et al. (2008Messai et al. ( , 2010 \nDans l\u0027exemple de contexte MV dans le Tableau 1, et pour ? \u003d (10, 10, 10, 10, 1, 1) :  (A, B)  \nVisualisation de Concepts Multi-Valués\nAfin de faciliter l\u0027identification des concepts MV d\u0027intérêt à partir du diagramme de Hasse, nous proposons deux techniques suivant la stratégie consistant à montrer une vue d\u0027ensemble d\u0027abord et de zoomer et filtrer par la suite sur demande Keim et al. (2008).\nLa première technique de visualisation consiste à filtrer et attribuer des couleurs en fonction des mesures des scores définis par l\u0027utilisateur. Dans la Section 2, nous montrons comment certains intervalles de la température et la vitesse de l\u0027air sont utilisés pour déterminer la classe de confort dans la cabine conformément aux normes internationales. Un dégradé de couleurs est attribué en fonction du \"score global\" d\u0027un concept défini par l\u0027utilisateur. Par exemple, le score pour le \"maximum de confort\" est attribué quand les scores de vitesse et la température et sont égaux à 2 (voir Section 5.2). Dans ce cas la couleur correspondante est à la borne supé-rieure du dégradé (par exemple rouge, comme dans la Figure 4). Les concepts avec des scores inférieurs sont de couleur jaune. Cette simplification est importante dans le cas de conception des systèmes complexes, où le nombre de paramètres est souvent élevé, parce qu\u0027elle donne une vue d\u0027ensemble qui permet de comparer les concepts et d\u0027identifier rapidement les plus pertinents. Alternativement, l\u0027utilisateur peut filtrer les concepts qui sont en dessous d\u0027un seuil de score. Ceci est particulièrement important lors de l\u0027extraction des classes de confort.\nLorsque les concepts pertinents sont identifiés, la deuxième technique de visualisation intervient pour aider à identifier les intervalles dans ces concepts. Cette technique que nous proposons s\u0027appuie sur un \"heat map\" conceptuel où chaque concept est représenté sous la forme d\u0027une ligne de rectangles (Figure 3). Chaque rectangle représente un attribut, sa couleur indique la position dans l\u0027intervalle où la valeur de l\u0027attribut se situe dans une échelle chromatique continue du bleu au rouge. La largeur de chaque rectangle est proportionnelle à la taille de l\u0027intervalle. Si un attribut n\u0027est pas présent dans le concept, le rectangle correspondant est montré vide afin de maintenir l\u0027ordre cohérent des attributs. Figure 2. Les couleurs indiquent le score de l\u0027attribut sur une échelle de couleurs (du bleu au rouge) et la largeur indique la taille de l\u0027intervalle de valeurs de l\u0027attribut.\nFIG. 3 -Visualisation du treillis MV de la\nApplication de l\u0027ACFS au cas test cabine d\u0027avion\nContraintes pour guider l\u0027exploration des données de simulation\nDans la section précédente nous avons présenté une formalisation de l\u0027ACFS sur des données numériques. Cependant, cette approche est générique et peut en conséquence être appliquée à d\u0027autre types de données Messai et al. (2010) auquel cas des mesures de similarité relatives à ces données doivent être utilisées. En s\u0027appuyant sur la définition de ces mesures de similarité, l\u0027ACFS peut être appliquée en suivant l\u0027intuition de \"grouper ensemble les données similaires\". Comme montré précédemment, cette opération nécessite également le choix de seuils se similarité. La variation du seuil de similarité entraine la variation des treillis MV obtenus en terme de nombre de granularité des concepts MV.\nDans ce travail nous sommes intéressés par l\u0027étude des résultats de simulation des paramètres de conception d\u0027un système afin d\u0027identifier les configurations des paramètres d\u0027entrée qui produisent des paramètres de sortie répondants à des contraintes prédéfinies. Pour cela nous avons exprimé ces contraintes à travers les seuils de similarité afin de guider le processus de classification de l\u0027ACFS. Dans l\u0027exemple détaillé précedamment (Table 1 et  Figure 2) le choix de ? \u003d (10, 10, 10, 10, 1, 1) suit l\u0027idée de définir des contraintes sur les paramètres de sortie T 1 et V 1 afin d\u0027extraire les intervalles de variation des paramètres d\u0027entrée T _Ext, T air_In, T air_P et U air_In. En effet, les seuils ? i \u003d 10 définis pour les paramètres d\u0027entrée dé-passent la différence maximale entre les valeurs de chacun de ces paramètres. Cela signifie que toutes les valeurs d\u0027un même attribut peuvent être considérées comme similaires et qu\u0027en conséquence il n\u0027y a pas de contrainte de classification effective définie sur ces valeurs. Cependant la valeur ? i \u003d 1 pour T 1 et V 1 signifie que les valeurs de T 1 (respectivement V 1) ne peuvent être regroupées ensemble dans un même concept MV que si leur différence ne dé-passe pas 1. Cette contrainte est exprimée afin de n\u0027obtenir dans le treillis MV à construire que des concepts où la variation maximale des valeurs de T 1 et de V 1 ne dépasse pas 1 et où il n\u0027y a pas de contrainte sur la variation des autres paramètres. Le treillis obtenu permet ainsi d\u0027obtenir directement les intervalles de variation des paramètres d\u0027entrée correspondant à certaines valeurs de T 1 et de V 1. Par exemple, le concept MV dont l\u0027extension est {2, 4} dans la \nDonnées de simulations relatives au cas test cabine d\u0027avion\nDans la suite de cet article nous étudions les données de simulations relatives au cas test cabine d\u0027avion introduit précédemment. Le jeu de données étudié correspond aux résultats de simulation de 100 configurations. Chaque configuration correspond à une valeur par paramètre d\u0027entrée (13 en tout) prise dans l\u0027intervalle de variation correspondant (voir Figure 1). Pour chaque configuration de paramètre d\u0027entrée les valeurs de simulation de 9 paramètres de sorties sont calculées pour étudier le confort dans la cabine ainsi que l\u0027énergie dissipée pour assurer ce confort. Ces paramètres de sortie sont la température moyenne et la vitesse de l\u0027air dans chacun des 4 sièges de passager dans la cabine (T 1 à T 4 et V 1 à V 4) et l\u0027énergie dissipée.\nAfin de faciliter l\u0027identification des configurations correspondant au confort au niveau des sièges de la cabine, nous avons calculé des scores de confort en fonction des valeurs de la température et de la vitesse de l\u0027air au niveau de chaque siège. Nous nous sommes appuyé dans ce calcul sur les standards ANSI/ASHRAE ASHRAE (2004) pour définir trois niveaux de confort (0 : inconfortable, 1 : acceptable et 2 : confortable) comme suit :\nNous avons remplacé dans le jeu de données de simulation étudié les valeurs de T et de V par les scores correspondants avant de procéder à la classification en utilisant l\u0027ACFS.\nExtraction des classes de confort et des configurations de paramètres d\u0027entrée correspondantes\nL\u0027objectif de notre étude est de déterminer les paramètres d\u0027entrée qui permettent d\u0027obtenir des situations confortables pour les passagers dans la cabine. Partant du fait que la température dans la cabine est plus importante que la vitesse de l\u0027air dans la définition du confort des passagers, nous centrons notre analyse sur les simulations qui permettent d\u0027avoir les températures qui correspondent au confort maximal (score(T) \u003d 2). Nous notons cet ensemble de simulations par S silver et nous appliquons l\u0027ACFS à cet ensemble en suivant la stratégie détaillées précédemment qui consiste à définir des contraintes en terme de seuil de similarité uniquement pour les paramètres de sorties T et V . Etant donné que nous souhaitons identifier des classes de conforts correspondantes aux trois scores de température et de vitesse définis précédemment, nous avons choisi ? \u003d 0 pour ne regrouper que les expériences donnant le même score pour ces paramètres de sortie. Le treillis MV obtenu est donné dans la \nEvaluation des résultats obtenus à travers de nouvelles simulations\nAfin d\u0027évaluer les résultats obtenus nous avons procédé à de nouvelles simulations en considérant comme intervalles de variation des paramètres d\u0027entrée ceux extraits pour les différentes classes de confort et données dans le Tableau 2. Nous avons d\u0027abord effectué une série de 12 simulations à partir des intervalles extraits pour la classe de confort maximal. Les 12 simulations ont produit des températures et des vitesses de score 2 (T entre 22.5\n• C et 23.5\n• C V ? 0.2) pour chacun des 4 sièges et correspondent ainsi à la classe de confort maximal. Le treillis MV construit pour ces simulations est constitué d\u0027un seul concept correspondant à la classe de confort maximal et les intervalles de variation des paramètres d\u0027entrée extraits à partir de ce treillis sont donnés dans le Tableau 3.\nLes colonnes 2 et 3 du Tableau 3 correspondent respectivement aux deux classes de confort maximal extraites à partir des S Silver puis des 12 nouvelles simulations. La comparaison entre ces deux colonnes montre que les intervalles de valeurs des paramètres d\u0027entrée sont approximativement les même dans les deux cas. En sachant que dans les deux cas ces configurations TAB. 3 -Les intervalles de valeurs extraits pour les 13 paramètres d\u0027entrée à partir des résultats des nouvelles simulations.\nont aboutis à des situations de confort dans la cabine, nous pouvons déduire qu\u0027il y a une convergence des résultats. Les intervalles extraits pour la classe de confort peuvent donc être retenus comme solution optimale possible pour le problème de choix de configuration assurant le confort dans la cabine.\nNous avons procédé de la même manière pour évaluer les résultats obtenus pour les autres classes de confort identifiées. 12 nouvelles simulations ont été effectuées pour chaque classe de confort. De la même manière, ces simulations ont également permis de vérifier la convergence des résultats obtenus. Ces résultats montrent l\u0027utilité de l\u0027approche présentée pour assister le concepteur de système dans le choix des meilleurs configurations possibles des paramètres de conception parmi les données de simulation de ces paramètres. La généricité du formalisme de l\u0027ACFS rend l\u0027approche proposée adaptable et réutilisable pour répondre à des besoins similaires dans d\u0027autres contextes.\nConclusion\nDans cet article nous avons présenté une approche qui s\u0027appuie sur les structures conceptuelles pour assister les concepteurs de systèmes complexes dans le choix des configurations pertinentes des paramètres de conception de leurs systèmes. L\u0027approche proposée s\u0027appuie sur l\u0027ACFS pour étudier le confort des passagers dans une cabine d\u0027avion. Le confort est étudiée à travers la simulation de la température et de la vitesse de l\u0027air au niveau de 4 sièges de passager dans la cabine. L\u0027utilisation des l\u0027ACFS et le choix approprié des seuils de similarité des valeurs de paramètres ont permis d\u0027identifier les principales classes de confort à partir des données de simulation et d\u0027extraire pour chaque classe de confort les configurations des paramètres d\u0027entrée qui permettent d\u0027assurer le niveau de confort correspondant. L\u0027identification des classes de confort a été facilitée par deux techniques de visualisation de treillis MV adap-\n"
  },
  {
    "id": "387",
    "text": "Résumé\nL\u0027analyse formelle de concepts (AFC) est un formalisme de représentation et d\u0027extraction de connaissance fondé sur les notions de concepts et de treillis de concepts (Galois).\nL\u0027AFC a été exploitée avec succès dans plusieurs domaines en informatique tels le génie logiciel, les bases et entrepôts de données, l\u0027extraction et la gestion de la connaissance et dans plusieurs applications du monde réel comme la médecine, la psychologie, la linguistique et la sociologie.\nDans cette présentation, nous allons explorer le potentiel de l\u0027AFC et de quelques extensions de cette théorie (ex. analyse triadique de concepts) dans l\u0027analyse de réseaux sociaux en vue de découvrir des connaissances à partir de réseaux homogènes simples (ex.  Elle collabore avec quelques équipes de recherche en France, dont le LIMOS de l\u0027université Blaise Pascal, le laboratoire ERIC de l\u0027université Lumière Lyon 2, et le laboratoire d\u0027informatique de l\u0027université François Rabelais.\n"
  },
  {
    "id": "388",
    "text": "Introduction\nLa Caisse Nationale des Allocations Familiales (CNAF), branche \"famille\" de la sécu-rité sociale française, gère un réseau régional de Caisses d\u0027Allocations Familiales (CAF) dont l\u0027objectif est de venir en aide aux familles et aux personnes en difficulté financière, pour des raisons de santé, familiales ou professionnelles. A ce titre, elles versent différentes prestations à leurs allocataires dans quatre grands domaines : le logement, la naissance du jeune enfant, l\u0027entretien de la famille et la garantie de revenus. Dans un souci d\u0027amélioration de la qualité de service, la CNAF veut mettre en oeuvre une politique nationale de gestion des réclamations.\nSelon un travail préliminaire réalisé par la CNAF, une réclamation est définie comme \"tout mécontentement exprimé à l\u0027égard d\u0027une décision, d\u0027une procédure ou d\u0027un service de la Caisse d\u0027Allocations Familiales, quelle qu\u0027en soit la forme, et pour lequel une réponse est explicitement ou implicitement attendue\". Dans une logique marketing, la gestion des réclama-tions est un élément fondamental dans la gestion de la relation client (Customer Relationship Management ou CRM), comme le soulignent (Stauss et Seidel, 2004 \nFIG. 1 -Processus général d\u0027analyse automatique des réclamations.\nDans ce cadre, le travail confié au laboratoire de recherche ERIC consiste à étudier de façon exploratoire les opportunités offertes aujourd\u0027hui par les techniques de fouille de données, et en particulier de fouille de textes, pour réaliser une analyse automatique des réclamations envoyées par les allocataires. Des travaux préalables concernant la gestion des réclamations (complaint management en anglais) ont été listés dans la littérature -voir à ce sujet le large panorama proposé par (Ngai et al., 2009) -, mais très peu d\u0027efforts semblent avoir été entrepris pour traiter ce problème spécifique. Il est intéressant de noter que (Bae et al., 2005) ont déjà essayé d\u0027utiliser des cartes auto-organisatrices (Self-Organized Maps ou SOM, c.f. (Kohonen, 2001)), c\u0027est-à-dire une technique issue de l\u0027apprentissage non supervisé.\nLa démarche adoptée dans cette étude exploratoire est illustrée dans la figure 1. Elle se décompose en quatre étapes :\nEtapes 1 et 2 pour -identifier les documents contenant les réclamations et les rendre exploitables en vue d\u0027analyses automatiques, -récupérer les données disponibles sur les allocataires ayant rédigé ces réclamations. Etapes 3 et 4 pour -à partir des informations sur les allocataires réclamants, établir une typologie des récla-mants en utilisant des techniques classiques d\u0027analyse des données, -à partir du contenu textuel des documents, établir une typologie des réclamations sur la base d\u0027analyses statistiques et sémantiques, -croiser les types de réclamations et les caractéristiques des allocataires afin de définir des groupes d\u0027individus au comportement homogène en matière de réclamation.\nCet article s\u0027organise comme suit. Tout d\u0027abord, nous présentons en détail dans la section 2 les données issues d\u0027un échantillon des réclamations envoyées par des allocataires à leur CAF. Nous donnons à cette occasion les différents prétraitements qui ont rendu les analyses ultérieures possibles. Dans la section 3, nous présentons les techniques de fouille de données qui ont été choisies pour construire les deux typologies. La section 4 donne les résultats des deux typologies obtenues, ainsi que la mise en correspondance réalisées entre la typologie des allocataires réclamants et celle des réclamations. Enfin, la section 5 propose une conclusion à ce travail, ainsi que des pistes d\u0027études futures.\n2 Préparation du jeu de données 2.1 Données brutes de la CAF Les allocataires peuvent adresser des réclamations à leur CAF par différents canaux, allant de l\u0027appel téléphonique à la lettre manuscrite en passant par le mail, le(s) site(s) internet ou la lettre dactylographiée. L\u0027un des objectifs de ce travail étant l\u0027automatisation des traitements, il a fallu déterminer quels formats de données rendaient possible une analyse automatique. Concernant les courriers reçus par voie postale, il a été décidé d\u0027identifier automatiquement les courriers dactylographiés pour ne retenir que ceux-ci et d\u0027écarter les courriers manuscrits. Tous les courriers électroniques via différents sites Web ont été retenus. Une partie des documents fournis n\u0027ayant pas été identifiée comme des réclamations, il a fallu mettre en place une procédure pour discriminer automatiquement les réclamations à partir de leur contenu textuel. Cette procédure est brièvement présentée dans la section qui suit.\nLes données transmises par la CAF du Rhône pour ce travail sont des réclamations récep-tionnées entre janvier 2010 et mars 2012. Parmi les 174000 documents parvenus à la CAF pendant cette période, seul un échantillon de 12534 documents a pu être traité dans la durée de l\u0027étude. Parmi ces 12534 documents, 2385 documents ont été identifiés automatiquement comme étant des réclamations ; ils ont donc été retenus pour les analyses ultérieures. Chaque document contient un texte, en général plutôt court et aisément identifiable à l\u0027aide de techniques automatiques. De plus, il peut être associé aux informations ou données qui décrivent l\u0027allocataire au moment de la réclamation, comme l\u0027état civil (civilité, sexe, âge) et des variables construites par les agents de la CAF. Au final, 39 variables descriptives ont été sé-lectionnées initialement pour l\u0027étude. Un allocataire qui réclame plusieurs fois voit son profil dédoublé comme s\u0027il s\u0027agissait de plusieurs personnes différentes, le profil pouvant en effet varier au fil du temps.\nPrétraitements des données\nDans cette partie, nous détaillons les traitements nécessaires pour transformer les données brutes brièvement décrites dans la section précédente dans un format propice à l\u0027emploi d\u0027algorithmes de fouille de données. Ces traitements représentent au moins 70% de l\u0027effort investi dans ce travail, ce qui justifie la place qui leur est dédiée dans cet article.\nConcernant les informations liées à la description des allocataires réclamants, nous avons sélectionné 19 variables parmi les 39 initiales mises à notre disposition. Cette sélection permet de limiter la redondance de l\u0027information (par exemple, plusieurs variables traitent du nombre d\u0027enfants dans le foyer) et d\u0027éviter à ce que le nombre de valeurs manquantes soient trop éle-vées (des nouvelles variable descriptives ont été créées à partir de 2011). Afin de permettre l\u0027emploi des méthodes d\u0027analyse des données présentées dans la section suivante, certaines variables qualitatives ont été recodées et les variables continues ont été discrétisées. La plupart du temps, cette discrétisation se base sur des intervalles définis antérieurement par la CAF.\nConcernant les réclamations proprement dites, le travail de prétraitement des documents a été plus important. Il se décompose en six grandes étapes que nous listons ci-dessous.\nReconnaissance des caractères. La diversité dans la forme des documents fournis (dactylographiés, électroniques. . .) implique d\u0027utiliser un OCR (Optical Character Recognition) afin de transformer les images de certains documents fournis au format .pdf ou .tiff dans un format texte. Après une revue de plusieurs solutions logicielles existantes sur le marché, nous avons opté pour le logiciel ABBYY FineReader 1 . Celui-ci nous a permis d\u0027obtenir des taux de bonne reconnaissance très corrects lorsque les documents sont dactylographiés : le logiciel commet entre 2,5 et 10% d\u0027erreur dans la reconnaissance des caractères ; erreur estimée en comparant (sur la base d\u0027un échantillon de 50 documents) le texte extrait manuellement par nos soins au texte automatiquement reconnu.\nDistinction entre écriture manuscrite et dactylographiée. La deuxième étape consiste à ne conserver que les documents dactylographiés afin d\u0027assurer une qualité minimale aux textes reconnus à partir des images. Cette distinction a été simple à réaliser en fixant un seuil de 70% au nombre minimum de chiffres ou de lettres contenus dans un document. Lorsque ce seuil est dépassé, c\u0027est-à-dire lorsque d\u0027autres symboles sont trop fréquents statistiquement (comme \u0027 ?\u0027, \u0027 !\u0027 ou #\u0027), il est clair que la reconnaissance a échoué. Le taux de reconnaissance des documents dactylographiés est alors de 100% sur notre échantillon.\nExtraction du texte des réclamations. Les fichiers textes issus de l\u0027étape d\u0027OCR contiennent tous les éléments présents dans le document d\u0027origine. Le texte de la réclamation est donc situé au milieu des informations personnelles de l\u0027allocataire. Le but de cette troisième étape est de réussir à ne conserver que le texte ou le corps de la réclamation. Pour ce faire, nous utilisons des expressions régulières car elles permettent de rechercher des chaînes de caractères caractéristiques. Pour les réclamations issues du site web national de la CAF, par exemple, cet exercice s\u0027est avéré aisé grâce à la présence dans le document d\u0027une balise \"Message :\".\nCorrection du texte. Des erreurs peuvent être introduites dans le texte, imputables à l\u0027allocataire lui-même ou à l\u0027algorithme d\u0027OCR. Afin de corriger certaines de ces erreurs, nous avons mis en place une étape de correction automatique. Celle-ci se base sur deux mécanismes. Tout d\u0027abord, nous avons recours à un lexique de mots de la langue française (le dictionnaire Gutemberg 2 ), à une liste de mots utilisés à la CAF (par exemple \"APL\" et \"AAH\" qui correspondent à des prestations spécifiques), et à une liste de néologismes détectés manuellement (par exemple \"sms\" ou \"tweet\"). Les mots de la réclamation sont comparés à ce vocabulaire et directement indexés s\u0027ils s\u0027y trouvent : nous les appelons les mots reconnus. Le deuxième mécanisme consiste à comparer les mots non reconnus avec le vocabulaire en calculant une distance entre ces mots. Dans notre cas, nous avons employé la distance de Levenshtein (Lcvenshtcin, 1966), normalisée par la taille du mot à corriger afin de ne pas pénaliser les mots trop longs (Cohen et al., 2003), puis la distance de Jaro-Winkler (Corston-Oliver et Gamon, 2004) en cas d\u0027ex-aequo. Après plusieurs tentatives, nous avons fixé un seuil de 0,2 à cette distance, ce qui permet par exemple de corriger \"prochainemant\" en \"prochainement\" (score de 0,077), mais pas \"prme\" en \"prime\" (score de 0,25) ou \"priscilla\" en \"principal\" (score de 0,444). Même s\u0027il arrive que certaines corrections amènent à introduire des erreurs, les corrections effectuées avec le seuil de 0,2 sont la plupart du temps pertinentes.\nIdentification des réclamations. Les documents transmis par la CAF du Rhône ne sont pas tous des réclamations et une étape d\u0027identification est nécessaire. Habituellement, ce travail est réalisé manuellement par un technicien de la CAF à l\u0027aide d\u0027une note de service qui définit la notion de réclamation ainsi que des critères ou des expressions pour les repérer. Ainsi sont considérés comme réclamations les textes contenant des tournures de phrases exprimant une incompréhension, une protestation ou une contestation. Il a été nécessaire d\u0027automatiser cette étape en nous basant sur la note de service. Brièvement, des expressions comme \"je ne comprends pas votre décision\" ont été remplacées par l\u0027association des mots \"comprend\", \"pas\" et \"décision\". C\u0027est la présence d\u0027une majorité de mots clefs associés à l\u0027une des expressions typiques qui permet d\u0027indiquer automatiquement qu\u0027un texte relève ou non d\u0027une réclamation. Cette méthode a permis d\u0027automatiser le processus de discrimination afin de ne retenir que des réclamations. Bien sûr, une partie des textes a été retenue à tort. Pour estimer cette proportion, 100 documents ont été tirés au hasard. Un expert humain a lu les documents, a vérifié pour chaque document s\u0027il s\u0027agissait ou non d\u0027une réclamation et a comparé avec l\u0027identification faite automatiquement par la machine. 76% des réclamations sont retrouvées par la machine (score de rappel) et 73% des réclamations retenues par la machine en sont bien (score de pré-cision). Sachant qu\u0027un agent de la CAF est également susceptible de retenir ou d\u0027écarter à tort des documents, le taux de reconnaissance automatique des réclamations a été jugé acceptable mais des travaux plus poussés devraient permettre d\u0027améliorer les performances (voir la section 5).\nSuppression des mots-outils et stématisation. Afin d\u0027optimiser les résultats des méthodes statistiques et sémantiques que nous abordons dans la section prochaine, il est souvent préfé-rable de retirer préalablement des textes les \"mots outils\" (stopwords), c\u0027est-à-dire des mots fréquemment utilisés dans la langue française pour construire les phrases, comme \"et\", \"avec\", etc. Des listes préconstruites de mots outils sont disponibles pour la plupart des langues, et en particulier pour le français 3 . Une deuxième technique permet de restreindre le nombre de mots en supprimant les préfixes et les suffixes, et ce afin de se rapprocher du radical des termes. Cette technique est appelée \"stématisation\" (stemming en anglais), elle ne doit pas être confondue avec la lemmatisation. Dans notre cas, elle nous permet de diminuer de manière significative le nombre de termes présents dans notre vocabulaire : les 10 248 termes présents initialement dans les textes du projet ont été ramenés à 7256 formes stématisées, soit une diminution de 29% de la taille du vocabulaire.\nAnalyse des réclamations\nDans cette partie, nous motivons le choix des techniques employées pour analyser les ré-clamations, puis nous donnons le protocole expérimental que nous avons suivi pour obtenir les résultats présentés dans la section suivante.\nChoix des techniques de fouille de données\nCombiner ACM et CAH Afin de construire une typologie des allocataires qui font une réclamation, nous proposons d\u0027utiliser l\u0027Analyse des Correspondances Multiples (ACM) en combinaison avec une Classification Ascendante Hiérarchique (CAH), techniques éprouvées issues de l\u0027analyse des données (Lebart et al., 1995).\nBrièvement, l\u0027ACM est une méthode de décomposition factorielle qui fournit une repré-sentation graphique synthétique d\u0027une grande quantité de données décrites par des variables qualitatives. Elle synthétise l\u0027information, met en évidence les informations intéressantes ainsi que les liens qui les caractérisent et aboutit à la création d\u0027axes factoriels pour représenter les individus et/ou les modalités. En revanche l\u0027ACM ne fournit pas de typologie proprement dite. Pour construire la typologie, nous appliquons la CAH avec le critère de Ward. La classification permet de faire émerger des groupes ou classes d\u0027allocataires au profil semblable. Le nombre de classes est sélectionné à partir du plus grand \"saut\" constaté dans le critère de Ward et en faisant appel à un expert métier pour attester du meilleur niveau de granularité.\nL\u0027intérêt d\u0027effectueur une classification après une méthode factorielle est double : (1) l\u0027ACM procède à une réduction du nombre de variables. La classification est faite avec les axes factoriels issus de la méthode factorielle et non pas avec les variables d\u0027origine ; (2) sur le graphique de l\u0027ACM représentant les individus (ici les allocataires réclamants), il est possible de visualiser l\u0027appartenance des individus aux classes, c\u0027est-à-dire la typologie des allocataires.\nExtraction de thématiques L\u0027un des objectifs de ce projet est de construire une typologie des réclamations de manière automatique. La démarche générale consiste à travailler directement à partir du contenu textuel des documents en utilisant le moins d\u0027information a priori, ce afin de faire émerger des groupes ou catégories de réclamations homogènes, que nous appellerons des thématiques (topics en anglais). Les textes sont placés dans la même thématique à partir du moment où leur auteur emploie un vocabulaire similaire, et dans des thématiques différentes lorsque le vocabulaire employé est différent.\nIl existe plusieurs méthodes d\u0027extraction des thématiques : par exemple les modèles à base de distance (Velcin et Ganascia, 2007), inspirés de l\u0027algorithme classique des c-moyennes, les modèles reposant sur la factorisation de matrices de type LSA (Deerwester et al., 1990) ou NMF (Paatero et Tapper, 2006) , ou les modèles graphiques basés sur la statistique bayésienne (Steyvers et Griffiths, 2007). Relevant de ce dernier type, le modèle qui a été choisi pour cette étude est LDA (Latent Dirichlet Allocation), proposé par (Blei et al., 2003). Il a été choisi car il s\u0027agit d\u0027un modèle aux performances reconnues qui a déjà été appliqué à de nombreux corpus de natures variées (voir par exemple (Bíró et al., 2008)).\nLa méthode LDA utilise les principes des modèles graphiques et de la statistique bayé-sienne, appliqués aux données textuelles. Elle se base sur une représentation en \"sac de mots\" (bag of words) où un document est traité comme un ensemble de mots dont la position dans le texte n\u0027est pas prise en compte. Cette hypothèse simplificatrice entraîne une perte dans la précision des résultats de l\u0027analyse, mais elle rend possible le traitement de corpus volumineux.\nLe modèle calculé par LDA contient un ensemble de catégories (les thématiques) et précise comment les documents sont répartis sur ces catégories. Chaque catégorie correspond à une thématique décrite comme une distribution sur l\u0027ensemble du vocabulaire de mots choisi. La thématique est souvent caractérisée par les mots clefs qui contribuent le plus à la catégorie (top keywords). L\u0027étiquetage (nommage) des catégories peut s\u0027effectuer en observant les expressions que l\u0027on peut reconstituer à partir de ces mots clefs et en s\u0027aidant des textes du corpus qui contiennent ces expressions. Par exemple, voici une liste de mots clefs les plus pertinents pour illustrer une thématique qui pourrait être obtenue : \"réponse\", \"allocation\", \"enfants\", \"montant\", \"logement\", \"situation\", \"été\", \"part\", \"aide\", \"allocataire\". En se basant sur ces mots et sur les textes des réclamations, on retrouve des expressions telles que : \"montant [de l\u0027] allocation logement\", \"montant [de l\u0027] aide [au] logement\", ou encore \"réponse [de votre] part\". Cette thématique couvre des textes tels que le suivant : \"Me mr comme je vais déménager le 1er mai 2010 je souhaite que l\u0027aide au logement d\u0027avril 2010 soit versée sur mon compte. Le loyer d\u0027avril a été intégralement payé au propriétaire. L\u0027aide au logement d\u0027avril me permettra de compléter le paiement du loyer du nouveau logement. Je perçois toujours l\u0027ASS, la dédite a été déjà envoyée au propriétaire je passerai pour remplir un nouveau dossier.\"\nProtocole expérimental\nConcernant la typologie des réclamations, les expérimentations ont été réalisées en deux temps. Il s\u0027agit tout d\u0027abord de réduire le vocabulaire en introduisant deux méthodes de filtrage des mots. Ensuite viennent les expérimentations proprement dite pour lesquelles se pose la question de la sélection du nombre de catégories thématiques.\nPremière étape : réduction du vocabulaire. Il est nécessaire de réduire encore la taille du vocabulaire qui contient encore 7256 mots après l\u0027étape de stématisation. Pour cela, deux filtres sont employés et permettent de réduire le vocabulaire à 364 mots.\nLe premier filtre consiste à éliminer du vocabulaire les mots qui apparaissent trop peu dans le corpus. En effet, ces mots apportent du \"bruit\" dans l\u0027analyse alors qu\u0027ils n\u0027apportent rien dans la constitution des catégories thématiques. La \"rareté\" du mot (sparsity en anglais) est simplement calculée en prenant le ratio du nombre de documents contenant le mot par le nombre total de documents dans le corpus. Chaque mot est donc associé à une valeur numé-rique située entre 0 et 1 : plus la valeur est proche de 0, plus le mot est rare et doit être ignoré. Il faut donc définir un seuil ? s en dessous duquel les mots sont retirés du vocabulaire et donc de l\u0027analyse.\nLe second filtre consiste à éliminer du vocabulaire les mots trop fréquents dans les textes, mais dans des proportions similaires, et qui peuvent être assimilés à des mots outils. Pour cela, nous nous sommes basés sur la mesure classique de TF.IDF qui est le produit entre la fréquence du mot dans un texte en particulier (Term Frequency ou TF) et le logarithme de l\u0027inverse de la fréquence du mot dans tous les documents du corpus (Inverse Document Frequency ou IDF). Cette mesure est d\u0027autant plus proche de zéro que le mot est peu fréquent dans un texte et qu\u0027il est au contraire présent tout au long du corpus. Là encore, il s\u0027agit de définir un seuil ? T en dessous duquel le mot est retiré de la description du texte. Attention, contrairement à la mesure précédente, le mot est éliminé d\u0027un texte, et non pas nécessairement du vocabulaire dans son ensemble. En effet, il se peut qu\u0027un mot soit exceptionnellement fréquent dans un texte (score de TF élevé), ce qui compense le fait qu\u0027il se trouve dans tout le corpus (score de IDF faible).\nEtant donné un nombre de catégories thématiques choisi à l\u0027avance, il faut être en mesure de déterminer les meilleures valeurs pour les deux seuils ? s et ? T . Pour cela, nous avons fait varier ces valeurs, ainsi que le nombre de catégories, et nous avons calculé à chaque fois le modèle LDA associé sur un échantillon d\u0027apprentissage en relançant 20 fois l\u0027algorithme afin de faire varier son initialisation (et donc réduire la chance de tomber dans un optimum local). L\u0027échantillon d\u0027apprentissage est constitué des deux tiers des 2385 réclamations de l\u0027analyse. La mesure de perplexité, que l\u0027on trouve dans l\u0027article de (Blei et al., 2003), permet de confronter les modèles obtenus à un échantillon de test en se basant sur un calcul de vraisemblance. Pour ne pas être biaisée par la taille du vocabulaire qui varie en fonction des seuils utilisés, cette mesure doit être normalisée par la taille du vocabulaire (Corston-Oliver et Gamon, 2004). Il est ainsi possible de sélectionner automatiquement les seuils qui mènent aux résultats qui optimisent cette mesure, sans avoir recours à une expertise humaine.\nDeuxième étape : calcul du modèle. Une fois déterminées les valeurs optimales pour les seuils permettant de filtrer le vocabulaire, il reste le problème de trouver le nombre \"optimal\" de catégories thématiques. Ce problème est lié à celui, très classique en apprentissage non supervisé, de déterminer le nombre optimal de classes (clusters). Pour ce faire, nous avons tout d\u0027abord calculé la mesure de perplexité en faisant varier le nombre de catégories de 2 à 20, avec une relance de 20 fois pour chaque valeur. La courbe obtenue présentant une forme en \"U\" caractéristique en apprentissage automatique, nous en avons conclu que les valeurs trop faibles ou trop élevées étaient à écarter. Les valeurs trop élevées dégradent les résultats de la mesure de la même manière que pour le phénomène de sur-apprentissage (overfitting) constaté en apprentissage automatique supervisé. Parmi les valeurs intermédiaires restantes, et comme nous sommes dans le cadre d\u0027un travail exploratoire mené sur un cas d\u0027étude réel, nous avons finalement choisi de déterminer le nombre exact de thématiques en ayant recours à des experts métier. \nTypologie des allocataires réclamants\nPour construire la typologie des allocataires réclamants, les dix premiers axes factoriels (qui expliquent 46 % de l\u0027intertie) sont retenus comme variables pour la CAH. Pour la CAH, le critère du \"saut\" dans la mesure est maximum pour des valeurs de 3, 8 et 5 classes. L\u0027évaluation des experts nous a permis de conclure que la typologie la plus informative est obtenue pour 8 classes d\u0027allocataires. Il est possible de visualiser la typologie des allocataires sur le graphique synthétique de l\u0027ACM en colorant chaque individu selon la catégorie à laquelle il appartient (cf. figure 2). A partir des caractéritiques communes des allocataires constituant chacune des classes, une étiquette est donnée à chaque classe.\nA titre d\u0027exemple, Les individus en gris se démarquent des autres par une forte proportion de personnes qui ont entre 20 et 25 ans, qui sont célibataires et qui perçoivent l\u0027allocation au logement sociale. 73 % d\u0027entre eux ont un quotient familial inférieur à 300 et tous ont un revenu brut annuel inférieur à 20 000e. Enfin, aucun d\u0027entre eux n\u0027a d\u0027enfant et, en toute logique, aucun ne touche les prestations familiales. Nous interprétons cette classe comme correspondant à des \"étudiants\".\nLes individus colorés en orange se caractérisent quant à eux par une absence de personnes sans enfants. Plus de 90% d\u0027entre eux ont au moins trois enfants (67% avec trois enfants et 25% avec quatre enfants ou plus). Parallèlement 90% d\u0027entre eux touchent le complément familial et 99% les prestations familiales. Ces allocataires sont âgés de 40 à 50 ans. On trouve très peu de célibataires et de personnes avec un fort quotient familial dans cette classe qui correspond aux \"familles nombreuses\".\nTypologie des réclamations\nLe protocole expérimental permet de sélectionner le meilleur modèle en 14 catégories et avec des valeurs de seuil respectivement de 0,98 et 0,03 pour ? s (rareté) et ? T (TF.IDF). L\u0027un des avantages de la méthode LDA est de pouvoir identifier les mots qui caractérisent ces différentes thématiques (mots clefs principaux ou top keywords en anglais) et d\u0027analyser plus qualitativement les thématiques : pour chaque thématique, nous précisons les dix premiers mots clefs, nous proposons une étiquette, et nous donnons une réclamation type dans laquelle nous soulignons la présence d\u0027un ou plusieurs mots clefs.\nA titre d\u0027exemple, dans la première catégorie se retrouvent les mots-clefs suivants : dossier, demande, droit, familiales, réponse, allocation, compte, été, prestations, pourriez. Cette catégorie regroupe donc des documents qui abordent le thème des allocations familiales et on retrouve dans les textes des expressions comme \"Allocation familiales\" ou \"prestations familiales\". La deuxième catégorie est aussi relativement marquée ; elle contient des réclamations qui font suite à un changement de situation de l\u0027allocataire avec les mots-clefs suivants : mois, situation, été, caf, informations, allocataire, changement, dossier, reçu, compte. Les récla-mations de cette thématique abordent un \"changement de situation\" et/ou la prise en \"compte [d\u0027]informations\". Voici un exemple de texte de cette catégorie : \"madame monsieur suite au mail que j\u0027ai recu le 20 mars 2010 de la part de la caf de lyon dont la copie se trouve en piece jointe je souhaiterai contester la decision qui a ete prise en effet le fait que je sois en collocation n\u0027a pas ete enregistre par les services de la caf de lyon ce qui a entraine une retenue de mon aide au logement pour les mois de janvier 2010 hauteur de 239 62 euros et de fevrier 2010 hauteur de 45 euros de plus j\u0027ai recu une notification de dette de 807 84 euros suite cette erreur je souhaiterai que ma situation ainsi que mon dossier soient regularise\".\nMise en correspondance des typologies\nLa typologie des allocataires a montré que ces derniers pouvaient être classés en huit groupes avec des caractéristiques différentes. La typologie des réclamations, elle, a fait émer-ger quatorze thématiques. Se pose alors la question suivante : certains de ces thèmes sont-ils le fait d\u0027une catégorie particulière d\u0027allocataires ? Les réclamations sur les prestations familiales ne sont-elles écrites que par les familles nombreuses, par exemple ? Pour répondre à ces questions il est possible de croiser les deux typologies.\nDes analyses statistiques basées sur un test d\u0027indépendance (p-value\u003d0,0005) et une analyse factorielle des correspondances simples indiquent qu\u0027il n\u0027y a pas d\u0027indépendance entre le type de réclamations et le type d\u0027allocataires. Il existe en effet des thèmes de réclamations privilégiés selon les catégories d\u0027allocataires. Par exemple, les familles nombreuses semblent réclamer davantage sur les thèmes du changement de situation, de l\u0027allocation logement et du RSA. Les retraités réclament quant à eux principalement sur l\u0027allocation logement (mais dans des proportions semblables aux autres allocataires) ainsi que sur le RSA et le montant des droits. Comme on peut s\u0027y attendre, ils sont sous-représentés dans le thème des allocations familiales, leurs enfants étant des adultes.\nEn revanche, les analyses statistiques ne permettent pas de conclure qu\u0027une catégorie de réclamation est imputable à une classe particulière d\u0027allocataires. En effet, les personnes en situation de handicap et les célibataires apparaissent presque toujours en première position, mais ceci est dû au fait qu\u0027ils sont plus nombreux que tous les autres allocataires réclamants. L\u0027objet de l\u0027étude présentée dans cet article est d\u0027étudier l\u0027opportunité d\u0027utiliser des techniques de fouille de données pour réaliser une analyse sémantique des réclamations faites par les allocataires. L\u0027étude a été menée dans une optique exploratoire.\nAprès avoir identifié les différentes sources de données, l\u0027étude a mis en évidence l\u0027importance et la difficulté de la phase de préparation des documents et des données. Nous avons proposé et réalisé une succession d\u0027étapes pour traiter des problèmes de la reconnaissance de caractères, de la sélection des documents dactylographiés, de l\u0027extraction du texte contenant le corps du document, de la correction des fautes d\u0027orthographe, de l\u0027identification des textes qui sont proprement dits des réclamations, et enfin de la construction automatique du vocabulaire de mots pour décrire ces réclamations. En se plaçant dans une optique nationale de traitement automatique des réclamations, nous avons cherché à automatiser ces différentes étapes afin que les agents de la CAF interviennent le moins possible dans l\u0027opération. Nous avons aussi montré que grâce à des techniques de fouille de données, et en particulier de fouille de textes, il est possible de savoir à la fois qui sont les allocataires qui réclament et de savoir sur quelles thématiques portent les réclamations.\nS\u0027agissant d\u0027une étude exploratoire réalisée dans un temps assez court, nous pouvons dès à présent lister des améliorations possibles de ce travail avec deux horizons.\nA court terme, une amélioration consisterait à compléter les résultats obtenus dans la typologie des réclamations en lançant les algorithmes sur l\u0027intégralité du volume de documents fournis, contenant ou non une réclamation. Une typologie de l\u0027ensemble des documents permettrait de voir si les réclamations se distinguent significativement des autres documents et se retrouvent dans les mêmes catégories ou non. De plus, à l\u0027heure actuelle, la seule étape manuelle du processus proposé est l\u0027étiquetage des thématiques extraites par la méthode LDA. En utilisant des travaux développés dans notre laboratoire, une automatisation est possible et pourrait être intégrée sans rencontrer de grandes difficultés.\nA plus long terme, plusieurs extensions peuvent être étudiées. Concernant la discrimination automatique des documents qui relèvent d\u0027une réclamation, la solution proposée s\u0027inspire de la démarche manuelle suivie par les agents de la CAF. Mais un nombre encore trop important de textes qui ne sont pas des réclamations sont retenus à tort. L\u0027identification automatique des réclamations pourrait certainement être améliorée en utilisant d\u0027autres techniques de fouille de données telles que l\u0027apprentissage automatique supervisé. Dans cette étude, les typologies des réclamants et des réclamations ont été extraites de manière synchronique, c\u0027est-à-dire que la dimension temporelle n\u0027a pas du tout été prise en compte. Pourtant, il semble qu\u0027il s\u0027agisse d\u0027un aspect très important dans la gestion d\u0027une réclamation. Une autre extension de cette étude serait alors d\u0027étudier s\u0027il est possible de replacer l\u0027allocataire réclamant dans une chronologie. Des techniques de fouille de données pourraient être utilisées, par exemple, pour extraire des \"trajectoires\" d\u0027allocataires dans lesquelles s\u0027inscrivent les réclamations.\nRéférences\nBae, S., S. Ha, et S. Park (2005). A web-based system for analyzing the voices of call center customers in the service industry. Expert Systems with Applications 28(1), 29-41.\n"
  },
  {
    "id": "389",
    "text": "Introduction\nL\u0027Analyse Formelle de Concepts (Ganter et Wille, 1999), notée de manière abrégée AFC, est une méthode de classification automatique d\u0027objets décrits par des attributs au travers d\u0027une relation binaire. Le résultat d\u0027une telle classification est un treillis de concepts (appelé aussi treillis de Galois (Barbut et Monjardet, 1970)) où chaque concept regroupe tous les objets ayant en commun un ensemble d\u0027attributs. On peut naviguer dans le treillis de manière simple et intuitive, des concepts les plus spécifiques (les concepts regroupant beaucoup de caractéristiques partagées par peu d\u0027objets) aux moins spécifiques (les concepts regroupant beaucoup d\u0027objets partageant peu de caractéristiques).\nL\u0027AFC est exploitée dans différents domaines en tant que méthode d\u0027extraction de connaissances et les différentes publications sur le sujet, notamment Carpineto et Romano (2004); Valtchev et al. (2004), ont permis d\u0027en identifier les forces et les limites. Certaines de ces limites ont pu être contournées en utilisant différentes approches.\nL\u0027Analyse Relationnelle de Concepts (ARC) (Huchard et al., 2007) est une extension de l\u0027AFC qui permet de prendre en compte non seulement les caractéristiques des objets, mais aussi les relations que les objets entretiennent entre eux. L\u0027ARC consiste à appliquer itérati-vement un algorithme de l\u0027AFC pour gérer les données relationnelles : les objets sont décrits par des attributs et par leurs relations vers d\u0027autres objets. Les concepts découverts à une ité-ration donnée sont propagés le long des relations, pour permettre la découverte de nouveaux concepts à l\u0027itération suivante. L\u0027ARC apparaît plus intuitive à utiliser sur des données relationnelles telles que des bases de données ou des langages de modélisation orientés objet comme UML. Nous proposons dans cet article une adaptation de l\u0027ARC en vue de l\u0027utiliser en tant que méthode d\u0027extraction de connaissances sur des données de mesure de qualité de l\u0027eau des cours d\u0027eau d\u0027Alsace.\nCe travail s\u0027inscrit dans le projet ANR FRESQUEAU 1 dont le but est le développement de nouvelles méthodes d\u0027étude, de comparaison et d\u0027exploitation de tous les paramètres disponibles sur les cours d\u0027eau et les plans d\u0027eau. Il approfondit une étude précédente menée avec l\u0027AFC (Bertaux et al., 2009a) et des approches statistiques (Bertaux et al., 2009b).\nPropager le long des relations les concepts découverts d\u0027une itération à une autre permet certes la découverte de concepts intéressants, mais engendre souvent une explosion combinatoire, et les motifs intéressants sont difficiles à extraire à partir du grand ensemble de concepts construits. Plusieurs stratégies peuvent être utilisées pour pallier cette complexité, incluant la séparation des objets initiaux en plusieurs sous-ensembles après une analyse préliminaire ou l\u0027introduction de requêtes (Azmeh et al., 2011). Nous nous intéressons dans cet article à l\u0027utilisation de l\u0027ARC pour explorer interactivement les données en laissant l\u0027utilisateur/trice choisir avant chaque itération de l\u0027AFC quels contextes (formels et relationnels) il ou elle veut utiliser.\nLes données avec lesquelles nous travaillons ne sont pas initialement sous forme de relation binaire mais de nombreux travaux traitent de l\u0027échelonnement des données en vue d\u0027obtenir une relation binaire (Ganter et Wille, 1999) ou une représentation sous forme de structures de patron (Ganter et Kuznetsov, 2001). Ces approches ont été appliquées précédemment sur des données similaires à celles de notre projet (Bertaux et al., 2009a) et nous considérons par la suite uniquement des données sous forme de relations binaires.\nDans cet article nous allons tout d\u0027abord présenter l\u0027AFC puis le principe général du processus d\u0027ARC pour mettre en évidence les différents points de variations qui permettraient d\u0027en améliorer l\u0027utilisation dans un contexte de fouilles de données. Nous présenterons un exemple du type de données que nous avons dans le projet FRESQUEAU et les conséquences des variations sur ces données. Nous conclurons ensuite par une courte discussion.\nAnalyse Formelle de Concepts\nL\u0027AFC telle que présentée par Ganter et Wille (1999)   L\u0027AFC permet la création de tous les concepts d\u0027un contexte donné, concepts formant un treillis de concepts aussi appelé treillis de Galois. Un concept c 1 est plus général (resp. plus spécifique) qu\u0027un concept c 2 si l\u0027extension de c 1 contient (resp. est contenue par) l\u0027extension de c 2 . De manière duale l\u0027intension d\u0027un concept est contenue par l\u0027intension d\u0027un concept plus spécifique. Deux concepts donnés ont une unique borne supérieure et une unique borne inférieure.\nLes treillis sont généralement représentés par leur diagramme de Hasse. Le treillis du tableau 1 est représenté par la figure 1. Les flèches représentent la relation de généralisation, c\u0027est-à-dire que le concept pointé est plus général que le concept d\u0027origine. Compte-tenu de l\u0027inclusion de l\u0027intension d\u0027un concept dans les concepts plus spécifiques et de l\u0027inclusion de l\u0027extension d\u0027un concept dans les concepts plus généraux, chaque objet (resp. attribut) n\u0027est indiqué qu\u0027une fois dans le concept le plus spécifique (resp. le plus général) le contenant. Par exemple le concept 6 regroupe les stations BREI0001 et DOLL001 qui possèdent les attributs petit cours et eau fraîche et vive que l\u0027on retrouve par la relation de générali-sation vers les concepts 2 et 7.\n3 Extension de l\u0027ARC pour l\u0027analyse exploratoire L\u0027Analyse Relationnelle de Concepts (ARC) (Huchard et al., 2007) est une extension de l\u0027AFC permettant de prendre en compte, en plus des caractéristiques des objets, les relations existant entre ces objets.\nL\u0027algorithme 1 présente les principales étapes de l\u0027ARC. Le paramètre d\u0027entrée de l\u0027ARC est une famille relationnelle de contextes RCF \u003d (K, R) composée de n contextes objet- \nOn peut voir dans le tableau 2 un exemple de famille relationnelle de contextes. On peut voir sur la partie gauche deux contextes objetattribut taxons et stations et sur la droite le contexte objet-objet presenceDeTaxon qui relie les objets du contexte stations aux objets de taxons 2 . \nAlgorithme 1: Processus de l\u0027Analyse Relationnelle de Concepts. figure 2 présente les deux treillis obtenus après cette étape d\u0027initialisation sur notre exemple. On remarque que la relation presenceDeTaxons n\u0027est pas encore prise en compte à cette étape du processus et les deux treillis sont indépendants.\nÀ l\u0027étape p :\nContextes objet-ottribut Contextes objet-objet \nTAB. 3 -Échelonnement de la relation presenceDeTaxons et extension du contexte stations à l\u0027étape 1. -ETENDRE-REL rajoute à K i les relations obtenues par l\u0027échelonnement des relations dont K i est le domaine. L\u0027échelonnement consiste à inclure les relations objet-objet en tant qu\u0027attributs relationnels. Ils sont obtenus en utilisant les concepts des treillis de l\u0027étape p?1 et un opérateur d\u0027échelonnement (c\u0027est-à-dire ?, ?). Par exemple, si l\u0027opéra-teur d\u0027échelonnement ? est choisi pour échelonner une relation R j donnée, les colonnes de R j sont remplacées par des attributs de la forme ?R j : C, où C est un concept dans le treillis construit à partir des objets de l\u0027image de\nAinsi dans notre exemple nous avons étendu le contexte objet-attribut stations avec le contexte objet-objet presenceDeTaxon échelonné par l\u0027opérateur ?. Le contexte étendu est présenté par la table 3. La station FECH001 est reliée aux concepts 0, 1 et 3 par la relation ?presenceDeTaxon car on y trouve la présence de Athericidae que l\u0027on retrouve dans les concepts 0 et 1 et la présence de Boreobdella que l\u0027on retrouve dans les concepts 0 et 3. Si on utilisait l\u0027opérateur d\u0027échelonnement ?, la station FECH001 serait reliée uniquement au concept 0 par la relation ?presenceDeTaxon car il s\u0027agit du seul concept où l\u0027on retrouve les deux taxons présents dans cette station. -MAJ-TREILLIS met à jour les treillis de l\u0027étape p ? 1 dans le but de produire,\n, associé à K i concaténé à tous les contextes objet-objet échelonnés dont K i est le domaine. L\u0027algorithme s\u0027arrête lorsqu\u0027un point fixe est atteint c\u0027est-à-dire lorsque l\u0027on obtient une famille de treillis isomorphe à la famille de treillis obtenue à l\u0027étape précédente et que les extensions des contextes restent inchangées. Dans le cas de notre exemple, les treillis de la figure 3 sont les treillis finaux. Le nombre d\u0027itérations est prévisible lorsque les relations entre contextes ne forment pas un circuit. Mais dans certains cas, par exemple lorsqu\u0027un contexte objet-objet a le même ensemble domaine et image, le nombre d\u0027itérations n\u0027est pas prévisible (seule une borne est connue) et peut être très grand selon les données.\nLes treillis relationnels s\u0027interprètent différemment des treillis de concepts classiques car ils doivent être considérés ensemble. Le treillis stations de la figure 3 doit être considéré avec le treillis taxons pour pouvoir être correctement interprété. On trouve dans les intensions de concepts des attributs faisant référence à d\u0027autres concepts. Par exemple Concept_8 possède l\u0027attribut relationnel presenceDeTaxons : Concept_2 ce qui signifie que tous les objets de Concept_8 sont liés par la relation presenceDeTaxons à au moins (car l\u0027opérateur d\u0027échelonnement utilisé est l\u0027opérateur ?) un objet de l\u0027extension de Concept_2 appartenant au treillis taxons.\nL\u0027avantage d\u0027un tel processus est que les concepts obtenus ont dans leur intension des relations à d\u0027autres concepts en plus des attributs classiques. Ces relations permettent l\u0027extraction de motifs construits à partir de plusieurs contextes interconnectés, comme cela a été fait dans Dolques et al. (2009) et Dolques et al. (2010, qui ne pourraient pas être facilement obtenus à partir du processus classique de l\u0027AFC.\nCependant un problème majeur de ce type de processus est la difficulté potentielle à appré-hender le résultat. Dans des travaux précédents en ingénierie dirigée par les modèles, les données extraites de modèles de taille moyenne peuvent être facilement appréhendées par l\u0027ARC. Mais dans un contexte de fouille de données, la taille des données est beaucoup plus importante. Le temps de calcul est dépendant du nombre de concepts à générer et celui-ci est exponentiel par rapport au nombre minimum des attributs ou des objets dans le pire des cas. Ainsi, si les relations entre objets sont nombreuses et ont peu de similarité d\u0027un objet à l\u0027autre, le temps de calcul peut augmenter de manière exponentielle et le résultat peut apparaître difficile à comprendre par un utilisateur à cause du nombre de concepts à considérer simultanément. Ceci est particulièrement avéré quand seuls de petits motifs sont nécessaires alors que de nombreuses relations connectent les objets entre eux et que ces relations forment un circuit. Dans de tels cas, nous pensons qu\u0027il peut être plus pertinent d\u0027avoir une approche exploratoire.\nNous listons ci-dessous les différentes variations possibles sur l\u0027algorithme pour mettre en pratique une approche exploratoire. Nous énumérons les points de variation possibles dans l\u0027algorithme qui peuvent affecter le résultat en changeant les contextes pris en compte à chaque étape. Nous proposons pour chaque point de variation un scénario alternatif à partir du processus précédemment décrit qui implique l\u0027utilisateur en lui demandant d\u0027effectuer des choix. Toutes ces variations ou seulement un sous-ensemble peuvent être appliquées selon la granularité voulue.\n-étape d\u0027initialisation, lignes 4 à 5 Construire les treillis pour des contextes objet-attribut choisis concaténés à des contextes objet-objet choisis. -ETENDRE-REL, ligne 9 Plutôt que d\u0027utiliser toutes les relations et d\u0027échelonner toutes les relations objet-objet à chaque étape, sélectionner un sous-ensemble de la famille relationnelle de contextes et différents opérateurs d\u0027échelonnement pour chaque contexte objet-objet sélectionné. Note : les treillis pour les images des relations objet-objet sélec-tionnées auront dû être calculés lors d\u0027une étape précédente (mais pas nécessairement p ? 1). À cette étape, des contextes objet-attribut peuvent aussi être sélectionnés et le treillis correspondant peut être construit. -MAJ-TREILLIS, ligne 10 Mettre à jour seulement les treillis pour les relations sélec-tionnées. -arrêt, ligne 11 Si un point fixe n\u0027est pas atteint, laisser la décision d\u0027arrêter à l\u0027expert.  La figure 4 représente les données par un schéma. Chaque noeud représente un ensemble d\u0027objets qui est traduit en un contexte objet-attribut. Chaque arête étiquetée représente une relation entre les ensembles d\u0027objets qui est traduite en un contexte objet-objet. Pour chaque arête nous considérons la relation dans les deux directions.  \nContextes objet-attribut\nstation-taxon-20-99\nAsellus Athericidae Baetis Bithynia Boreobdella\nstation-taxon-1-19\nAsellus Athericidae Baetis Bithynia Boreobdella\nstation-taxon-100+\nAsellus Athericidae\nTAB. 4 -Famille relationnelle de contextes obtenue à partir de nos données d\u0027exemples en considérant les directions de relation de la figure 5.\nNous souhaiterions voir émerger de ces données des relations entre les différents types d\u0027information qui permettent de décrire une station. Il serait par exemple intéressant de voir apparaître des règles suivant le format : « la modalité M du trait de vie T dans un cours d\u0027eau de type E implique la présence du caractère physico-chimique C », ce que l\u0027on peut obtenir en orientant les relations de la figure 4 de telle sorte que la relation traits des taxons aille du contexte taxons vers traits de vie et que toutes les autres relations aient stations pour domaine comme cela est présenté par la figure 5.\nLa famille relationnelle de contextes pour cette configuration particulière est présentée par la table 4. Nous rajoutons dans les contextes objet-attribut un attribut identifiant pour chaque objet afin de permettre la création d\u0027un concept pour chaque objet après l\u0027étape d\u0027initialisation. La figure 6 présente les extraits de la famille de treillis obtenue nécessaires pour extraire la règle « la présence à un niveau moyen d\u0027individus d\u0027un taxon dont la durée de vie est supérieure à un an implique une demande chimique en oxygène (DCO) élevée » 3 . On peut en effet remarquer que Concept_3 est plus spécifique que Concept_41. Un attribut introduit dans Concept_3 impliquera ainsi un attribut introduit dans Concept_41 puisque tous les attributs de Concept_41 sont hérités par Concept_3. Concept_3 regroupe les stations qui abritent une quantité moyenne d\u0027individus appartenant à des taxons regroupés par 3. compte tenu de la taille de l\u0027exemple, cette règle n\u0027est pas à considérer comme générale.\nConcept_63. Concept_63 regroupe de son côté les taxons dont les individus ont une durée de vie supérieure à un an. Concept_41 regroupe les stations dont l\u0027un des caractères physico-chimiques (de niveau 2, donc élevé) est DCO. On en tire l\u0027implication déduite. À partir du treillis complet on peut obtenir l\u0027ensemble des règles d\u0027implication entre traits de vie et caractères physico-chimiques en considérant tous les cas où des caractères physico-chimiques sont introduits par un concept et des traits de vie sont introduits par un concept plus spécifique.\nMais des règles suivant le format « la modalité M du trait de vie T peut apparaître quand le caractère physico-chimique C est présent » peuvent aussi être pertinentes. Afin qu\u0027elles puissent émerger, il serait nécessaire de changer (par rapport à la configuration précédente) la direction des relations entre traits de vie et taxons et entre taxons et stations. Il existe encore de nombreuses autres configurations dont les résultats sont potentiellement pertinents et faire varier les opérateurs d\u0027échelonnement permet d\u0027augmenter encore l\u0027expressivité des règles que l\u0027on peut obtenir grâce à l\u0027ARC.\nConcept_19\nExtrait du treillis du contexte stations Extrait du treillis du contexte caractères physico-chimiques\nConcept_8\nConcept_21\nConcept_27\nConcept_29\nExtrait du treillis du contexte taxons Si nous considérons le schéma de la figure 4 comme un graphe, l\u0027exploration consiste à analyser les différentes arêtes jusqu\u0027à obtenir un chemin entre les caractères physico-chimiques et les traits de vie. En comparant les résultats obtenus lors d\u0027une exploration avec ceux obtenus en utilisant l\u0027ARC classique sur les mêmes relations, nous trouvons que les treillis résultants sont plus petits et plus faciles à lire dans le premier cas.\nCombiner l\u0027ensemble des configurations possibles n\u0027est pas envisageable car la multiplication des relations risque d\u0027entraîner une explosion combinatoire du nombre de concepts obtenus, augmentant ainsi le temps de calcul et la complexité des concepts obtenus. Dans le cas de notre exemple, considérer toutes les relations dans les deux directions échelonnées seulement par ? entraîne la création de 120 concepts contre 66 et 63 pour les deux configurations évoquées précédemment. Nous envisageons donc une approche où l\u0027utilisateur/trice explore différentes configurations en effectuant différents choix à chaque étape du processus comme présenté dans la section 1.\nConclusion et discussion\nDans cet article, nous avons présenté une approche exploratoire pour assister l\u0027utilisation de l\u0027Analyse Relationnelle de Concepts de manière plus appropriée pour un processus d\u0027extraction de connaissances. Nous avons plusieurs raisons de vouloir modifier le processus original de l\u0027ARC : obtenir des résultats pertinents plus rapidement en calculant moins de treillis (de préférence seulement les treillis qui nous intéressent), diminuer la complexité de la fouille de données relationnelles, ou laisser l\u0027expert guider le processus de découverte en se basant sur son intuition et les motifs d\u0027apprentissage qui apparaissent au cours du processus.\nPlusieurs questions se posent sur cette approche d\u0027extraction de concepts à partir de données relationnelles. L\u0027étape d\u0027initialisation a un fort impact sur les structures qui peuvent être découvertes par la suite. Elle peut accélérer le processus, si les relations objet-objet contiennent les informations nécessaires à l\u0027expert, ou à l\u0027inverse, elle peut cacher à l\u0027expert les informations pertinentes. Néanmoins, le problème le plus important vient du fait que les modifications à chaque étape rendent la construction de concepts non monotone et qu\u0027il est possible de construire des exemples où le processus diverge (itérant sur plusieurs configurations récur-rentes).\nDans le processus original de l\u0027ARC, quand le point fixe est atteint, les treillis des deux dernières étapes sont isomorphes, ainsi quand un concept en référence un autre via un attribut relationnel, le concept référencé peut être trouvé dans un treillis de la même étape. Mais dans le processus exploratoire que nous proposons, quand un concept en référence un autre, le concept référencé est dans un treillis de l\u0027étape précédente et peut référencer un concept lui-même dans une étape antérieure. Il est alors nécessaire de trouver des solutions pour présenter aux experts une information suffisamment simple à interpréter. Nous pensons malgré tout qu\u0027une telle approche exploratoire est plus applicable qu\u0027une approche systématique qui itère jusqu\u0027à atteindre un point fixe et qui donne des résultats difficilement interprétables par un expert.\n"
  },
  {
    "id": "390",
    "text": "Introduction\nLa classification des images satellitaires haute résolution est de plus en plus complexe. La complexité et l\u0027hétérogénéité des données satellitaires haute résolution ne permettent plus l\u0027utilisation des méthodes de classification pixélique. En effet, quoi qu\u0027elle permette d\u0027offrir dans certains cas un résultat qui reflète de manière fidèle la réalité du terrain étudié, elle reste tributaire d\u0027une conformité, d\u0027une répartition homogène des classes recensées et d\u0027une bonne stabilité radiométrique des zones d\u0027apprentissage. Lesquelles sont aussi limitées du point de vue caractéristique. Ces difficultés ont catalysé la recherche de nouvelles approches d\u0027analyse d\u0027image exploitant mieux les informations présentes dans l\u0027image ainsi que les connaissances expertes qui peuvent être une source de connaissances très fructueuse Sellaouti et al. (2012b). Dans ce sens, l\u0027analyse d\u0027image basée objets semble être prometteuse Blaschke (2010). En effet, elle permet de converger d\u0027un espace image composé de pixel vers un espace objet ou chaque objet représente une entité homogène selon des critères bien définis ce qui permet d\u0027avoir une vue plus globale de l\u0027image. Le deuxième avantage de cette approche objet est qu\u0027elle reste propice à l\u0027intégration des connaissances expertes grâce à sa capacité d\u0027intégrer des connaissances de haut niveau telles que la taille, la forme et les connaissances spatiales ce qui permet de diminuer le gap sémantique.\nLa classification orientée objet est une approche composée de deux étapes dont la première consiste à la construction des objets et qui est généralement une segmentation. La deuxième étape est l\u0027identification des objets extraits. La classification orientée objet se base sur les ré-gions extraites lors de la phase de segmentation mais aucune interactivité n\u0027existe entre ces deux processus. Cette classification utilise directement les régions extraites lors de la première étape sans mettre en cause la segmentation. Cependant, une région mal segmentée est générale-ment mal classée. Ceci s\u0027explique par le fait qu\u0027elle se base sur des caractéristiques qui peuvent être erronées. Une remise en cause de la segmentation par la classification et une collaboration entre eux s\u0027impose.\nDans cet article, nous proposons une approche orientée objet sémantique collaborative permettant une collaboration entre les deux étapes d\u0027extraction et d\u0027identification des objets en intégrant des connaissances expertes. En effet, la première étape de l\u0027algorithme permet d\u0027extraire des régions homogènes et de leur assigner un score de confiance. Ce score sera ensuite utilisé pour une croissance de région sémantique dont résultera la classification finale.\nCe papier est organisé comme suit : dans la section 2, nous reviendrons sur différents algorithmes de classification orientée objet proposées dans la littérature. Ensuite, dans la section 3, nous introduisons notre approche. La section 4 sera consacrée à l\u0027étude expérimentale.\n2 Etat de l\u0027art des approches de classification orientée-objet L\u0027approche orientée objet est une approche qui se base sur la création d\u0027un ensemble de régions représentant les objets de l\u0027image afin de les classer. Plusieurs travaux ont été présentés dans la littérature. Nous proposons de décomposer ces approches en trois classes : approche naturelle, approche hiérarchique et approche collaborative. Nous présentons dans ce qui suit, un aperçu sur ces travaux.\nApproche naturelle\nL\u0027approche naturelle repose sur la structure classique de la classification orientée-objet qui est composée d\u0027une phase de segmentation de l\u0027image en un ensemble d\u0027objets suivi d\u0027une classification de ces objets. Walter (2004) présente une approche pour la détection des changements apparus dans les zones urbaines. Il propose une approche divisée en deux parties, la première consiste en une classification supervisée avec l\u0027algorithme de maximum de vraisemblance. La base d\u0027apprentissage est extraite à partir d\u0027une base GIS existante. Dans la deuxième partie, une mise en correspondance entre les objets classés et les objets existants dans la base GIS est appliquée pour détecter les changements. L\u0027auteur utilise toutes les bandes spectrales. Les caractéristiques utilisées sont des caractéristiques spectrales et texturales. Ericksson (2004) utilise une approche orientée objet pour détecter les couronnes d\u0027arbres. En effet, il commence par une segmentation qui va lui permettre de détecter les couronnes d\u0027arbres. Cette segmentation est la croissance de région par mouvement Brownien. Une fois les couronnes des arbres détectées, Ericksson procède à une classification de ces couronnes en se basant sur un ensemble de règles spécifiques à chaque classe. La couronne qui vérifie les règles d\u0027une classe est affectée à cette dernière. Bendhiaf et Sellaouti (2009), utilisent les couronnes extraites par l\u0027algorithme de croissance de région pour procéder à une classification basée sur les indices de formes et la texture. Lefebvre et al. (2011) présentent une classification orientée-objet basée sur les ondelettes et la théorie des évidences. Une première étape de segmentation utilisant la ligne de Partage des Eaux (LPE) est utilisée sur une image de contours créée à partir des composantes horizontales et verticales de la décomposition en ondelettes afin d\u0027éviter un résultat sur-segmenté. Ensuite, une agrégation des petits objets non exploitables dans l\u0027étape suivante est effectuée en fusionnant les régions voisines de petite taille et de valeur moyenne de luminance proche. La deuxième étape est la caractérisation des objets extraits avec des attributs de luminance et de texture. La dernière étape est la classification basée sur la théorie de l\u0027évidence en fusionnant les critères de similarité de luminance et de texture.\nApproche hiérarchique\nCette approche utilise une représentation multi-échelle de l\u0027image. En effet, elle repose sur une segmentation multi-échelle, l\u0027image est donc représentée par une famille d\u0027images allant de la percepftion de l\u0027image la plus fine vers la plus grossière.\nHofmann (2001) présente une approche orientée objet pour la classification des zones urbaines, ils présentent deux approches utilisées par le logiciel eCognition pour extraire les objets d\u0027intérêt. Le départ de l\u0027approche est une segmentation multi-échelle qui permet de créer un réseau hiérarchique représentant l\u0027image. La segmentation multi-échelle peut être réalisée de deux façons différentes qui sont Bottom-up et Top-down. L\u0027approche top-down commence par générer les objets du niveau le plus haut. Tous les objets des niveaux le plus bas sont des sous objets du niveau supérieur. L\u0027approche Bottom-up, quant à elle, opère inversement. La segmentation commence par générer les petits objets. Tous les objets générés dans les niveaux plus hauts sont alors considérés comme des super objets du niveau initial. Le logiciel offre deux classifieurs de base : le classifieur des k plus proches voisins et un classifieur flou. Marangoz et al. (2004) utilisent cette approche pour détecter les routes et les bâtiments dans une image IKONOS. Karsenty et al. (2007) utilisent la même approche pour évaluer la perméabilité des sols en zone urbaine à l\u0027aide d\u0027imagerie très haute résolution et de données laser scanner à Curitiba au Brésil. De même, pour Jacquin et al. (2008) qui utilisent cette approche pour voir l\u0027effet de l\u0027expansion urbaine sur les inondations et les anticiper. Giraudon et al. (1992) ont développé le système MESSIE. Ce système est une architecture multi-spécialiste, bâti autour d\u0027une architecture de type tableau noir, dont l\u0027objectif est de réaliser une interprétation basée uniquement sur une connaissance générique des objets sans utiliser de connaissances exogènes à la donnée image. Les auteurs montrent comment une modélisation des objets physiques de la scène exprimée sous les quatre points de vue forme, contexte, aspect et fonction peut améliorer la classification. Ce système est capable de manipuler des connaissances ponctuelles (informations radiométriques) et structurelles (propriétés géométriques, relations spatiales). Chaque spécialiste a une tâche spécifique et indépendante des autres spécialistes. Pour communiquer entre eux, les spécialistes utilisent une zone mé-moire commune qui est le tableau noir. Plusieurs rôles sont assignés aux spécialistes tels que l\u0027extraction des objets, le calcul des caractéristiques structurelles, l\u0027évaluation des hypothèses, l\u0027étiquetage des objets, la détection des conflits, etc. Le système peut facilement accepter de nouvelles connaissances expertes et de nouveaux spécialistes. Forestier et al. (2008), quant à eux, proposent une approche orientée objet collaborative et multi-stratégie. Elle intègre un ensemble de classifieurs non supervisés et présente une nouvelle approche qui permet de faire collaborer les différents classifieurs. L\u0027originalité de cette approche est qu\u0027elle intègre le processus de collaboration durant l\u0027étape de classification. En effet, elle est divisée en trois étapes. La première est une classification initiale où chaque classifieur est lancé avec ses propres paramètres, permettant de créer des objets. La deuxième étape consiste en un raffinement des résultats divisés en deux parties à savoir d\u0027une part l\u0027évaluation de la similarité des classes et d\u0027autre part, leur raffinement. La dernière étape est l\u0027unification où les résultats raffinés sont unifiés avec un algorithme de vote.\nApproche hybride\nPour résumer, toutes les approches décrites ont une limite commune, c\u0027est leur dépendance à la phase de segmentation. En effet, cette phase initiale est la plus importante vu qu\u0027une mauvaise segmentation entraine nécessairement une mauvaise classification. Dans toutes les approches précitées, l\u0027étape de segmentation n\u0027est jamais mise en cause par la classification.\nApproche de classification orientée-objet collaborative\nUne approche de segmentation qui favorise nettement la notion de collaboration est l\u0027approche région. En effet, son architecture itérative permet d\u0027introduire de nouvelles connaissances le long du processus de segmentation. Dans ce cadre, nous introduisons une approche collaborative entre les algorithmes de croissances de régions et une COO supervisée permettant une collaboration entre les deux étapes d\u0027extraction et d\u0027identification des objets en intégrant des connaissances expertes. Notre approche est composée de deux étapes :\n-Une première étape de prétraitement qui est une étape de préparation des données pour l\u0027étape suivante. Elle permet de décomposer l\u0027image en un ensemble de régions homogènes en se basant sur des propriétés bas niveau. Ensuite, une classification initiale va permettre d\u0027assigner un score de confiance pour chacun des objets extraits par la segmentation. -La deuxième étape est la classification sémantique basée sur une croissance guidée par les scores des régions calculés dans la première étape.\nPrétraitement\nSegmentation\nLe choix de l\u0027algorithme de segmentation n\u0027est pas très important dans cette approche tant qu\u0027il vérifie le critère de sur-segmentation. En effet, et vues les propriétés des algorithmes de croissances de région qui se basent sur la fusion des fragments d\u0027un objet afin de détecter l\u0027objet tout entier, il est évident qu\u0027une sous-segmentation de l\u0027image implique une perte de certains objets. Le choix de la sur-segmentation s\u0027impose donc. Nous avons ainsi choisi l\u0027al-gorithme \"watershed\" qui permet une sur-segmentation de l\u0027image, permettant de déterminer un ensemble de régions de départ, qu\u0027on notera R.\nClassification\nNous proposons ici une classification permettant d\u0027affecter à chacune des régions de R une classe C en calculant un score de confiance d\u0027appartenance de R à C, par rapport à l\u0027ensemble des classes présentes dans l\u0027image. Ce score permettra d\u0027évaluer la légitimité des régions en se basant sur les connaissances fournies par l\u0027expert. Nous utilisons le score de similarité proposé par Derivaux et al. (2007). Il est basé sur une approche orientée attribut vu qu\u0027il utilise les connaissances bas niveaux sur l\u0027image qui sont formalisées sous forme de descripteurs bas niveaux. Ce score permet de vérifier la validité des valeurs des attributs d\u0027une région selon les intervalles définis par l\u0027expert dans la base de connaissances. La mesure de similarité locale compare les valeurs des attributs d\u0027une région avec les attributs de l\u0027objet à classer. Nous pré-sentons dans ce qui suit les formules permettant de calculer le score de similarité, précédées par un ensemble de notations nécessaires pour une représentation formelle de l\u0027approche proposée Sellaouti et al. (2012a).\nNote 1 (région) : Soit R l\u0027ensemble des régions r i obtenu à partir de la segmentation. R \u003d {r i } i? [1,NR] où N R représente la cardinalité de R. \nNote 2 (classe) : Soit C l\u0027ensemble des classes présentes dans l\u0027image. C\noù v(r i ,a k ) est la valeur de l\u0027attribut a k pour la région r i . \nDéfinition 3 (Ensemble de similarité) : Nous définissons l\u0027ensemble SIM comme étant l\u0027ensemble des scores de similarité de toute région r i ? R par rapport à toute classe c j ? C.\nClassification sémantique\nAprès la phase de prétraitement explicitée ci-dessus, nous procédons à l\u0027étape de classification sémantique qui consiste en un traitement itératif permettant à partir des ensembles de régions R, de classes C et de scores de similarité SIM, de créer une hiérarchie de croissances basée sur la confiance en chaque région. La création de la hiérarchie est précédée par un calcul basé sur les scores de similarité que nous explicitons dans ce qui suit.\nPour une région r i ? R, nous définissions l\u0027ensemble des classes qui maximisent le score de similarité Sim(r i , c) parmi toutes les classes c ? C. Nous notons ?(r i ) cet ensemble :\nDéfinition 4 Pour chaque région r i ? R, nous définissions S max (r i ) et C max (r i ) comme suit :\nS max (r i ) représente le score de similarité maximal de la région r i pour l\u0027ensemble des classes de C. Dans le cas où ?(r i ) comporte plus qu\u0027une classe, nous déduisons qu\u0027il y a une confusion et que cette région n\u0027est plus une région de confiance mais une région conflictuelle. Dans ce cas, C max (r i ) prendra arbitrairement l\u0027une des classes de ?(r i ) et S max (r i ) aura la valeur 0. Dans le cas où l\u0027ensemble contient une valeur unique, alors cette dernière sera affectée à C max (r i ) et S max (r i ) sera le score de similarité Sim(r i , C max (r i )) de la classe Cmax pour la région r i et cette région aura comme classe C max (r i ).\nLe calcul des C max (r i ) et S max (r i ) servira comme départ pour l\u0027algorithme itératif de la hiérarchie de la classification sémantique que nous proposons. Le diagramme de la figure 1 illustre les étapes de cet algorithme. En effet, chaque itération de cet algorithme concerne un niveau de croissance de la hiérarchie. Chacune de ces itérations est composée de deux phases principales (les phases 1 et 2 du diagramme), à savoir l\u0027extraction des germes et la croissance sémantique. Dans la première phase, nous commençons par extraire les régions de départ que nous appelons germes. Ces derniers représentent les régions de confiances parmi l\u0027ensemble des régions candidates. Ensuite dans la deuxième phase, en se fondant sur des connaissances expertes, et selon la classe d\u0027appartenance du germe, nous procédons à une croissance sémantique à partir des germes déterminés dans la première phase.\nPour une itération k (k ? 1), nous désignerons par RegionsCandidates k?1 l\u0027ensemble des régions candidates à l\u0027extraction des germes, par RegionsContraintes k?1 l\u0027ensemble des germes déja traités, par RegionsGermes k l\u0027ensemble des germes extraits et par RegionF u? sionne k l\u0027ensemble des régions fusionnées. Les phases 3 et 4 permettent de mettre à jour les ensembles RegionsCandidates k et RegionsContraintes k . Nous détaillons dans ce qui suit les deux phases constituant une itération.\nExtraction des germes\nLe choix des germes de départ est très important pour le processus de la croissance séman-tique. Visant à exploiter toutes les informations disponibles et souhaitant converger vers une\nFIG. 1 -Diagramme de croissance de region sémantique.\napproche sémantique, le choix des germes reposera sur la confiance que nous avons en les ré-gions non encore traitées dans l\u0027image. L\u0027ensemble des germes de niveau k (RegionsGerme k ) sera alors celui qui maximise la confiance et plus précisément le score de similarité. Ces ré-gions seront extraites à partir de l\u0027ensemble RegionCandidates k?1 des régions non encore traitées dans les niveaux précédents de la hiérarchie. Les germes extraits à ce niveau sont les régions ri qui maximisent Smax(ri). Nous notons formellement :\nCroissance sémantique\nNous présentons dans ce qui suit le principe de l\u0027algorithme de croissance sémantique (la fonction SemCroiss appelée à la phase 2 du diagramme 1) qui prend comme entrée l\u0027ensemble des germes, des régions candidates à la croissance et des régions contraintes. Il permet la fusion de chaque germe avec ses régions voisines en se fondant sur les connaissances expertes spécifiques à la classe du germe et en prenant en compte les régions contraintes. La croissance sémantique est illustrée par l\u0027algorithme 1.\nCet algorithme itère sur l\u0027ensemble de germes appliquant pour chacun deux fonctions (ExtraireZoneCroissance et Croissance) mettant en oeuvre les deux principaux procédés composant la croissance sémantique, à savoir l\u0027extraction de la zone de croissance dans dans un premier lieu et la croissance au sein de cette zone dans un second lieu. Notons que nous nous limiterons dans cet article à la présentation générale de notre approche en faisant abstraction aux types de classes traitées. Or, comme la technique de croissance sémantique que nous proposons est intrinsèquement dépendante des spécificités des classes, nous nous contentons ici de présenter les principes des fonctions ainsi que leurs entrées et sorties : L\u0027extraction de la zone de croissance : Connaissant la classe du germe, nous proposons de limiter l\u0027espace de croissance en se basant sur les connaissances expertes. En effet, celles-ci permettent dans plusieurs cas de réduire l\u0027ensemble des régions candidates à la fusion. Les connaissances expertes peuvent être représentées de plusieurs manières, telles que les ontologies, les règles logiques, etc. Cette phase est tributaire de la classe traitée. En effet, si nous prenons l\u0027exemple de la classe végétation, rares sont les informations qui peuvent être utiles pour limiter l\u0027espace de recherche. N\u0027ayant ni forme géométrique bien définie, ni superficie limitée, la zone de croissance de cette classe ne peut pas vraiment être limitée. Par contre, si nous prenons l\u0027exemple des classes bâtiment et route, elles possèdent toutes les deux des caractéristiques géométriques et des formes bien définies ce qui permet de limiter l\u0027espace de croissance. L\u0027appel de la fonction ZoneCroissance dans une itération de l\u0027algorithme 1 permet de géné-rer, à partir, d\u0027une région germe r donnée et des régions candidates à la croissance, l\u0027ensemble des régions pertinentes qui peuvent fusionner avec le germe r, que l\u0027on notera ZC_r. Cette fonction prend en considération les régions déjà fusionnées que ce soit pour d\u0027autres germes de l\u0027itération courante (i.e. F usion) ou dans les itérations précédentes (i.e. RegionsContraintes). L\u0027ensemble ZC_r serait égal à l\u0027ensemble des candidats privé des régions élaguées dans le cas où la classe C max (r) supporte l\u0027intégration de connaissances expertes comme expliqué pré-cédemment. Dans le cas contraire, ZC_r comporterait toutes les régions candidates sauf celle déjà fusionnée dans l\u0027itération courante (i.e. ZC_r \u003d RegionCandidates\\F usion).\nCroissance : Contrairement à la croissance de région de base qui utilise les propriétés bas niveau (radiométrie, texture) sans prendre en compte la nature des objets traités, la croissance de région sémantique est, quant à elle, une croissance spécifique pour chaque germe selon sa classe d\u0027appartenance. Prenons l\u0027exemple de la classe route qui possède des pro-priétés qui la différentient des autres classes. En effet, elle présente une surface homogène, et elle est majoritairement composée d\u0027asphalte et ayant des contours linéaires et parallèles et une largeur variant dans un intervalle bien déterminé. A partir de ces connaissances nous pouvons détecter la zone de croissance de chaque germe en cherchant sa direction et en utilisant les informations sur sa largeur. L\u0027appel de la fonction Croissance dans une itération de l\u0027algorithme 1 permet de croître à partir d\u0027un germe r dans la zone de croissance ZC_r. La détermination des régions pertinentes qui peuvent fusionner avec ce germe r dépendra de la classe du germe et des connaissances expertes la concernant. Cette fonction retournerait l\u0027ensemble F usionnes_avec_r des germes fusionnés avec r. L\u0027union de ces ensembles pour chaque itération formerait l\u0027ensemble de retour de l\u0027algorithme SemCroiss.  Notre approche est une classification sémantique basée sur les connaissances expertes. Les connaissances que nous avons utilisés sont extraites à partir du dictionnaire de données FODOMUST réalisé par les experts géographes 1 . Il a été modélisé sous forme d\u0027une ontologie formée d\u0027une hiérarchie de concepts reliés entre eux par des relations par Derivaux et al. (2007). Chaque concept est défini par une étiquette (e.g. maison, route) et un ensemble d\u0027attributs (e.g. air, forme) ou chaque attribut est associé à un intervalle représentant les valeurs que  L\u0027évaluation numérique confirme l\u0027intérêt de notre approche. Le tableau 1 détaille le ré-sultat des mesures de rappel et de précision pour les deux classes route et bâtiment. Le rappel varie entre 87.35% et 94.24% alors que la précision varie entre 85.28% et 87.40%. Les valeurs de la précision sont inférieures à celles du rappel à cause des erreurs de classification de la phase de prétraitement.\nEtude Expérimentale\n"
  },
  {
    "id": "391",
    "text": "Introduction\nDe nombreux programmes de construction d\u0027alignements multiples à partir d\u0027un ensemble de séquences protéiques (appelés aligneurs) ont été développés. Cependant, il n\u0027existe pas un aligneur capable de bien aligner tous les types de séquences. C\u0027est la raison pour laquelle le laboratoire de bioinformatique de l\u0027IGBMC de Strasbourg a développé un système expert pour l\u0027alignement multiple de séquences protéiques appelé Alexsys (Aniba et al., 2009). Le système Alexsys propose d\u0027identifier, pour un ensemble de séquences protéiques données, les aligneurs permettant d\u0027obtenir de bons alignements en se basant sur des techniques de fouilles de données (classification supervisée) exploitant des caractéristiques des séquences. spécifiques ont été définies (Tsoumakas et al., 2010;Fürnkranz et al., 2008). Nous les utiliserons dans le cadre de l\u0027alignement multiple de séquences protéiques.\nLa séquence protéique est une suite d\u0027acides aminés (aussi appelés résidus). Chaque acide aminé est représenté par une lettre. Pendant l\u0027évolution des espèces, des changements appelés mutations peuvent se produire. En comparant les séquences entre elles et en cherchant les résidus ou les suites de résidus qui sont conservés dans une même famille de protéines, nous pouvons beaucoup apprendre sur les résidus essentiels pour certaines fonctions Les alignements multiples de séquences sont un outil important de la biologie moderne.\nLe nombre de programmes d\u0027alignement multiple disponible est sans-cesse croissant. Cependant, aucun programme actuel n\u0027est capable de construire un alignement multiple de haute qualité pour tous les cas possibles. Les principaux critères utilisés pour décider du programme d\u0027alignement à utiliser sont : la qualité de l\u0027alignement, le temps d\u0027exécution et l\u0027utilisation de mémoire (Thompson et al., 2011). La qualité de l\u0027alignement est généralement le critère le plus important. Dans notre étude, nous avons utilisé les aligneurs suivants, connus pour leurs performances et leurs fiabilités : ClustalW , Dialign, Mafft, Muscle, Probcons, T-coffee et Kalign. Une description des programmes d\u0027alignement ainsi qu\u0027un résumé de leurs avantages et de leurs inconvénients est présenté dans (Edgar et Batzoglou, 2006). L\u0027estimation de la qualité de l\u0027alignement est un point critique. La fonction de score la plus utilisée est le \" Sum of Pairs \". Actuellement, la qualité d\u0027un algorithme est générale-ment estimée en comparant les résultats obtenus avec des alignements de référence prédéfinis (Benchmarks). Par conséquent, les données de référence doivent être de haute qualité. Un des premiers benchmarks à grande échelle construit pour l\u0027alignement multiple des séquences se nomme BaliBase (Bahr et al., 2001). Les séquences utilisées dans la base de données de BaliBase ont toute une structure 3D connue. Les alignements sont construits à partir de ces structures 3D et raffinées à la main par des experts pour garantir l\u0027alignement correct des résidus conservés. Les alignements sont organisés sous la forme de plusieurs ensembles de références, représentant des problématiques réelles de l\u0027alignement multiple. L\u0027autre benchmark utilisé lors de cette étude, OXBench (Raghava et al., 2003), contient des alignements multiples de séquences protéiques construits automatiquement par des aligneurs.\nAmélioration de l\u0027existant : Alexsys\nLes divers algorithmes d\u0027alignement multiple peuvent produire des alignements différents pour un même ensemble de séquences à aligner. Les qualités de ces différents alignements dépendent des caractéristiques des séquences à aligner. Les biologistes aimeraient expliciter les relations entre les caractériques des séquences protéiques à aligner et la force/faiblesse de différents algorithmes d\u0027alignement, afin de pouvoir choisir le meilleur aligneur dans chaque cas. C\u0027est la raison pour laquelle le système Alexsys \"Alignment Expert System\" a été déve-loppé (Aniba et al., 2009). Dans cette section, nous décrivons l\u0027existant puis les améliorations apportées.\nAlexsys : un système expert pour l\u0027alignement multiple\nDans Alexsys, les connaissances ou règles du système expert ne sont pas recueillies auprès des experts, mais obtenues par apprentissage artificiel à partir de jeux de données. Les données d\u0027apprentissage proviennent de deux jeux de données de référence (BaliBase (Bahr et al., 2001) et OxBench (Raghava et al., 2003)) qui représentent des problèmes réels et des cas difficiles d\u0027alignement multiples. Les données consistent en un ensemble de 890 alignements choisis par les biologistes. Les attributs utilisés par Alexsys pour représenter un ensemble de séquences à aligner appartiennent à quatre catégories : physiques, structurels, fonctionnels et physicochimiques.\nLa classe de chaque aligneur est définie comme \"fort\" ou \"faible\" sur la base du score défini comme le nombre de paires de résidus alignés de la même façon dans l\u0027alignement produit par l\u0027aligneur et dans l\u0027alignement de référence. Au-dessus d\u0027un seuil de 0,5, un aligneur est considéré comme \"fort\", et en-dessous de cette valeur, un aligneur est considéré comme \"faible\". Cette valeur de seuil, bien que choisie par les biologistes, reste arbitraire. Alexsys construit alors un modèle par aligneur sous la forme d\u0027une forêt aléatoire.\nAméliorations\nNous avons poursuivi les travaux d\u0027Alexys. Tout d\u0027abord, nous avons considéré une version enrichie de la base de données contenant 1058 instances et gardant les même attributs. L\u0027attribut de pourcentage d\u0027identité (PCID) est un des attributs les plus importants utilisés par Alexsys. Nous avons défini une autre méthode pour le calculer. Cette méthode se base sur l\u0027algorithme d\u0027alignement global Needleman-Wunsch (Needleman et Wunsch, 1970) dans lequel l\u0027identité entre deux séquences est calculée pour chaque couple de séquences de l\u0027ensemble de séquences. Puis le maximum, le minimum, la moyenne et l\u0027écart type d\u0027identités obtenues sont calculés. Ainsi les valeurs de quatre attributs liés au PCID sont générées pour chaque ensemble de séquences utilisé dans la base de données.\nLa génération de l\u0027attribut de PCID consomme beaucoup de temps et de mémoire Nous avons défini et évalué une autre méthode moins exigeante en ressources nous permettant d\u0027estimer la similarité entre les séquences. C\u0027est la méthode de comptage des n-gram. Nous avons créé plusieurs attributs de n-gram en changeant la longueur des mots d\u0027acides aminés. Les attributs 1-gram, 2-gram, 3-gram, et 4-gram ont été testés.\nLe taux de bonne prédiction macro est évalué en utilisant le PCID ou le 1-gram séparé-ment, ou les deux conjointement, en plus des attributs usuels d\u0027Alexsys. Nous ne considérons pas seulement le seuil de 0,5 pour étiqueter les bons et mauvais aligneurs, mais nous avons essayé de faire varier le seuil de score. Plusieurs seuils ont été testés de 0 à 1, avec un pas de 0,1. L\u0027évaluation est faite en moyennant 5 répétitions d\u0027une validation croisée en 5 plis. Nous utilisons les forêts aléatoires implémentées dans Weka (Witten et al., 2011), comme dans la version originale d\u0027Alexsys. En comparant le comportement des nouveaux attributs (Figure 1), nous remarquons qu\u0027en utilisant les attributs de 1-gram avec l\u0027attribut de PCID en même temps, le taux de bonne prédiction macro est meilleur qu\u0027en cas d\u0027utilisation de l\u0027attribut de PCID uniquement, et bien sûr en utilisant toujours tous les autres attributs préalablement définis. Par ailleurs, les résultats ne sont pas présentés ici, mais nous avons observé que les attributs de 1-gram obtiennent de meilleurs performances que ceux de 2-gram, 3-gram et 4-gram. Dans le cas du seuil à 0,5, 1-gram et PCID ensemble donnent un taux de bonne prédiction macro de 90% pour les modèles créés pour les sept aligneurs.\nChoix du meilleur aligneur\nPour chaque aligneur, nous lui attribuons l\u0027étiquette \"meilleur\" si son score d\u0027alignement est le score maximum à epsilon près. Nous autorisons une marge de epsilon car il peut y avoir plusieurs alignements intéressants pour les biologistes Un objectif de ce travail est d\u0027identifier un seuil raisonnable pour epsilon. Puisque les scores varient de 0 à 1, il est clair qu\u0027une valeur de 1 pour epsilon conduirait à considérer tous les aligneurs comme \"meilleurs\". Par la suite, lors des différents test, epsilon variera entre 0 à 0,2.\nNotre problème consiste à trouver les aligneurs étiquetés \"meilleur\", autrement dit l\u0027ensemble des aligneurs capables de produire des alignements multiples de score maximum ou presque. C\u0027est de la classification multi-étiquettes. En calculant la cardinalité, qui est le nombre moyen des étiquettes \"meilleur\" sur tous les exemples (Figure 2), nous remarquons que deux aligneurs (en moyenne) sont \"meilleurs\" pour epsilon égal à zéro (où seulement le score maximum est choisi pour être \"meilleur\"). Ce résultat est surprenant. Nous nous attendions à avoir seulement un meilleur aligneur pour epsilon égal à 0. En vérifiant les données en cas de epsilon égal à 0, nous remarquons que 35% des exemples ont plus que deux aligneurs acceptables comme \"meilleurs\". La figure 2 montre aussi la vitesse à laquelle le nombre d\u0027aligneur acceptables augmente en fonction de epsilon. Le nombre d\u0027aligneurs acceptables tend vers 7 pour les grandes valeurs de epsilon. L\u0027implémentation de Weka des forêts aléatoires est encore utilisée, en faisant 5 validations croisées en 5-plis. Nous calculons la moyenne et l\u0027écart-type, sur ces 5 itérations de la validation croisée, des différentes mesures de performance de la classification multi-étiquettes afin de déterminer la valeur de epsilon la plus pertinente.\nEn observant les mesures de performance basées sur l\u0027exemple en fonction de la cardinalité (Figure 3), nous remarquons que beaucoup de mesures n\u0027apportent pas d\u0027information autre que : en augmentant epsilon donc la cardinalité, c\u0027est-à-dire quand plus d\u0027aligneurs deviennent acceptables, ces mesures augmentent. En particulier, la précision et le rappel, donc la mesure-F1, et le taux de bonnes prédictions et le rappel pour la classe négative croissent quasilinéairement avec la cardinalité. Le taux de bonne prédiction de l\u0027ensemble et 1-hammingLoss croissent également mais avec une pente moins régulière. Seule l\u0027AUC a un comportement différent et atteint rapidement un maximum, reste constante puis diminue quand epsilon et la cardinalité augmentent.\nNous observons un comportement similaire des mesures de performance basées sur l\u0027éti-quette, micro et macro, en fonction de epsilon à la Figure 4. De plus, epsilon\u003d0,01 apparaît comme un point singulier. Tout d\u0027abord, c\u0027est le maximum pour la macro AUC. Rappelons que nous cherchons le plus petit epsilon (et cardinalité) ayant de \"bonnes\" performances pour prédire le (ou les) meilleur aligneur. À cette valeur 0,01 de epsilon, le taux de prédiction de l\u0027ensemble est à 25%, c\u0027est-à-dire qu\u0027une fois sur quatre on obtient le sous-ensemble des bons aligneurs et seulement eux, parmi 2 7 \u003d 128 sous-ensembles possibles. Enfin, la plupart des mesures macro et micro sont à 0,75, en particulier 3 fois sur 4 pour la précision et pour le rappel, c\u0027est-à-dire que 3/4 des aligneurs prédits acceptables font réellement parti des meilleurs et que 3/4 des meilleurs aligneurs sont proposés. \nConclusion\nLe système Alexsys (Aniba et al., 2009) sert à prédire le programme à utiliser pour construire un bon alignement multiple d\u0027un ensemble de séquences protéiques. Nous avons amélioré l\u0027approche d\u0027Alexsys en reprenant le calcul du PCID et en lui ajoutant des attributs à partir du 1-gram plus rapides à calculer et complémentaires, c\u0027est-à-dire que la combinaison de ces attributs et du PCID améliore le taux de bonne prédiction par rapport au PCID utilisé seul dans Alexsys.\nLa seconde contribution de ce travail a consisté à reformuler le problème en considérant qu\u0027un aligneur est acceptable si son score est maximum à epsilon près. L\u0027analyse des diffé-rentes mesures de performance spécifiques à la classification multi-étiquette a été nécessaire pour déterminer la plus petite valeur de epsilon adéquate. En effet la plupart des mesures augmentent quand epsilon augmente et que tous les aligneurs deviennent acceptables. Cependant l\u0027AUC nous a permis d\u0027identifier un premier pic pour epsilon égal à 0,01. À cette valeur, le taux de bonne prédiction de l\u0027ensemble, c\u0027est-à-dire la prédiction d\u0027exactement tous les aligneurs acceptables et seulement eux, est de 25%, c\u0027est-à-dire nettement au dessus de l\u0027aléatoire à 1/128 \u003d 0, 8%. Cette mesure est particulièrement exigeante. La plupart des autres mesures de performance sont à 75%, en particulier la précision et le rappel micro et macro.\nLes perspectives de ce travail sont nombreuses. Une première perspective concerne l\u0027apprentissage de tri (ranking) multi-étiquettes. Une autre perspective consiste à tenir compte de\n"
  },
  {
    "id": "392",
    "text": "Introduction\nLe monitoring du trafic routier est effectué, dans la majorité des cas, grâce à des capteurs dédiés qui permettent d\u0027estimer le nombre de véhicules traversant la portion routière sur laquelle ils sont installés. Les coûts prohibitifs d\u0027installation et de maintenance pour ce genre de capteurs limitent leur déploiement au réseau routier primaire (c.à-d. les autoroutes et les grandes artères seulement). Par conséquent, ce genre de solutions produit une information incomplète sur l\u0027état du réseau routier, ce qui complique l\u0027extraction de connaissances sur la dynamique des mouvements dans ce réseau et sur l\u0027adéquation entre le réseau et son usage.\nUne solution alternative (ou complémentaire) consiste à exploiter des traces GPS d\u0027objets mobiles recueillies par des dispositifs ad hoc (par exemple des smartphones). Ces traces peuvent être obtenues lors de campagnes d\u0027acquisition spécifiques (bus, taxis, flotte d\u0027entreprise, etc.) ou par des mécanismes de crowdsourcing en proposant à des utilisateurs de soumettre leurs propres trajets. On peut ainsi obtenir un volume important d\u0027information couvrant le réseau de façon beaucoup plus complète que des capteurs.\nLe clustering (ou classification non supervisée) figure parmi les techniques d\u0027analyse les plus utiles à de telles fins exploratoires. La majorité des travaux traitant du clustering de trajectoires s\u0027est focalisée sur le cas du mouvement libre (Nanni et Pedreschi, 2006), (Benkert et al., 2006), (Lee et al., 2007), (Jeung et al., 2008) en faisant abstraction des contraintes liées à la topologie du réseau routier, qui jouent pourtant un grand rôle dans la caractérisation de la similarité entre les trajectoires analysées. Parmi les travaux ayant traité le cas contraint (Kharrat et al., 2008) ; (Roh et Hwang, 2010). El Mahrsi et Rossi (2012b) proposent de représenter les relations entre différentes trajectoires sous forme d\u0027un graphe et de s\u0027intéresser au clustering de ce dernier pour découvrir des groupes de trajectoires de profils similaires. Les auteurs étendent ce travail dans (El Mahrsi et Rossi, 2012a) en s\u0027intéressant aux regroupements de segments routierstoujours en se basant sur une représentation par graphes -afin d\u0027enrichir la connaissances des groupes de trajectoires et d\u0027apporter un moyen supplémentaire de les interpréter. Nous proposons, dans cet article, de conserver cette représentation des données sous la forme d\u0027un graphe. Plus précisément, nous modéliserons les relations qu\u0027entretiennent les trajectoires et les segments routiers sous forme d\u0027un graphe biparti et nous étudierons deux approches différentes de classification de ses sommets.\nLe reste de l\u0027article est organisé comme suit. La section 2 présente notre modèle de données ainsi que les approches que nous proposons. Section 3 illustre notre étude expérimentale et démontre l\u0027intérêt de ces approches et leur capacité à mettre en valeur des structures de clusters intéressantes tant au niveau des trajectoires qu\u0027au niveau des segments routiers. Enfin, une conclusion sera dressée dans la section 4.\nApproches de classification\nDans le cas contraint, une trajectoire T est modélisée sous forme d\u0027une succession de segments routiers appartenant à l\u0027ensemble de tous les segments constituant le réseau. Nous modélisons les données sous forme d\u0027un graphe biparti G \u003d (T , S, E). T est l\u0027ensemble des trajectoires, S est l\u0027ensemble de tous les segments du réseau routier et E est l\u0027ensemble des arêtes modélisant les passages des trajectoires de T sur les segments de S.\nDans un premier temps, nous proposons de projeter le graphe G afin d\u0027étudier séparément les graphes correspondant respectivement aux trajectoires d\u0027une part et aux segments d\u0027autre part (Section 2.1). Dans un second temps, le graphe biparti est traité directement grâce à une approche de biclustering (Section 2.2).\nApproche par projections de graphes\nLa projection du graphe G sur l\u0027ensemble de ses sommets représentant les trajectoires T produit un graphe G T \u003d (T , E T , W T ), décrivant les relations de similarité entre les trajectoires. Une arête e relie deux trajectoires T i et T j si celles-ci partagent au moins un segment routier.\nLa pondération ? la plus basique de cette arête peut consister en un comptage des segments communs entre les deux trajectoires. Si nous pondérons avec la mesure de similarité proposée par El Mahrsi et Rossi (2012b) la projection coïncide avec la définition du graphe de similarité entre trajectoires, introduite par les mêmes auteurs. C\u0027est cette stratégie de pondération que nous adoptons par la suite. Le poids ? est donc la similarité cosinus entre les deux trajectoires T i et T j exprimée comme suit :\nOù w s,T \u003d n s,T ·length(s) s ?T n s ,T ·length(s ) · log |T | |{Ti:s?Ti}| est un tf-idf modifié attribué à chaque segment routier s en fonction de sa longueur, son importance dans la trajectoire T et sa fréquence dans le jeu de données.\nDe façon analogue, la projection du graphe G sur l\u0027ensemble des segments S produit le graphe G S \u003d (S, E S , W S ) décrivant les relations entre segments routiers. Ici, une arête e relie deux segments s\u0027il y a au moins une trajectoire qui les visite tous les deux. Là également, nous opterons pour la pondération proposée par El Mahrsi et Rossi (2012a) pour affecter les poids W S au lieu d\u0027un comptage simple des trajectoires communes.\nNous nous proposons d\u0027effectuer le clustering de chacun de ces deux graphes de façon isolée (c.à-d. chacun est traité à part) pour obtenir une classification des trajectoires et une des segments. Pour ce faire, nous utiliserons l\u0027algorithme de détection de communautés dans les graphes par optimisation de la modularité préconisé par Noack et Rotta (2009). Ce choix -qui est motivé par la tendance de ces graphes à avoir des sommets à fort degré et par l\u0027efficacité des approches basées sur la modularité dans ce cas précis -n\u0027écarte pas la possibilité d\u0027utiliser d\u0027autres algorithmes de clustering de graphes tels que le clustering spectral (Meila et Shi, 2000) ou le clustering par propagation de labels (Raghavan et al., 2007).\nPour un jeu de données composé de n trajectoires qui parcourent un réseau routier composé de m segments, la complexité algorithmique théorique pour effectuer le clustering de trajectoires est de O(n 3 ) tandis que celle du clustering de segments est de O(m 3 ) (Noack et Rotta, 2009). Cependant, les complexités observées en pratiques sont plutôt quadratiques.\nNous croiserons ensuite les deux classifications et essayerons d\u0027interpréter chacune d\u0027entre elles en fonction de l\u0027autre.\nApproche par biclustering\nNous proposons ici d\u0027étudier directement le graphe sous sa forme bipartie G \u003d (T , S, E). Pour cela, nous appliquons une approche de biclustering sur la matrice d\u0027adjacence du graphe : les segments sont représentés en colonnes, les trajectoires en lignes et l\u0027intersection d\u0027une ligne et d\u0027une colonne indique le nombre de passages d\u0027une trajectoire sur un segment. Le but d\u0027un biclustering est de réordonner les lignes et les colonnes de manière à faire apparaitre et à extraire des blocs de densités homogènes dans la matrice d\u0027adjacence du graphe biparti G. Une fois ces blocs extraits, on en déduit deux partitions obtenues simultanément, une de segments et une de trajectoires.\nUne structure de biclustering, que nous notons M, est définie par un ensemble de paramètres de modélisations décrits dans le Tableau 1. Le but d\u0027un algorithme de biclustering va être d\u0027inférer la meilleure partition du graphe.\nGraphe G\nModèle de biclustering M T : ensemble des trajectoires C T : ensemble des clusters de trajectoires S : ensemble des segments C S : ensemble des clusters de segments E \u003d T ? S : ensemble des passages des trajectoires sur les segments C E \u003d C T ? C S : biclusters de trajectoires et de segments TAB. 1 -Notations.\nEn appliquant ce type d\u0027approches, les trajectoires sont regroupées si elles parcourent des segments communs et les segments sont regroupés s\u0027ils sont parcourus par des trajectoires communes. L\u0027avantage de cette technique est qu\u0027elle ne requière pas de pré-traitement sur les données, ni de définition de mesure de similarité entre trajectoires ou entre segments. L\u0027inconvénient principal réside dans la complexité algorithmique de ce type d\u0027approches qui peut s\u0027avérer très élevée.\nNous choisirons ici d\u0027utiliser l\u0027approche MODL (Boullé, 2011) afin d\u0027inférer notre structure de biclustering. Cette approche non-paramétrique a des capacités de passage à l\u0027échelle nous permettant de l\u0027utiliser pour le problème que nous traitons dans cet article. Un critère est construit suivant une approche MAP (Maximum A Posteriori) :\nabord, une probabilité a priori P (M) dépendant des données est définie. Elle spécifie les paramètres de modélisation en attribuant à chacun d\u0027eux une pénalisation correspondant à leur longueur de codage minimale, obtenue grâce aux statistiques descriptives des données. Ainsi, plus une structure de biclustering sera parcimonieuse, moins elle sera coûteuse. Ensuite, la vraisemblance des données connaissant le modèle P (D|M) est définie. Elle mesure le coût de recodage des données D avec les paramètres du modèle M. Donc, le modèle de biclustering le plus probable est le modèle le plus fidèle aux données initiales. En d\u0027autres termes, la vraisemblance favorise les structures informatives. La définition du critère global est donc un compromis entre une structure de biclustering simple et synthétique, et une structure fine et informative.\nD\u0027un point de vue algorithmique, l\u0027optimisation est réalisée à l\u0027aide d\u0027une heuristique gloutonne ascendante, initialisée avec le modèle le plus fin, c\u0027est-à-dire avec un segment et une trajectoire par cluster. Elle considère toutes les fusions entre les clusters et réalise la meilleure d\u0027entre elles si cette dernière permet de faire décroitre le critère optimisé. Cette heuristique est améliorée avec une étape de post-optimisation, pendant laquelle on effectue des permutations au sein des clusters. Le tout est englobé dans une métaheuristique de type VNS (Variable Neighborhood Search, Hansen et Mladenovic (2001)) qui tire profit de plusieurs lancements de l\u0027algorithme avec des initialisations aléatoires différentes. L\u0027algorithme est détaillé et évalué dans Boullé (2011).\nLa complexité algorithmique est en O(|E| |E| log(|E|)) avec |E| le nombre d\u0027arcs du graphe biparti G, qui correspondent, dans le cas présent, au nombre de passages de trajectoires sur les segments. Cette complexité est calculée au pire des cas, c\u0027est-à-dire lorsque chaque trajectoire couvre chaque segment (matrice d\u0027adjacence du graphe biparti pleine). En pratique, l\u0027algorithme est capable d\u0027exploiter l\u0027aspect creux habituellement observé dans ce type de données.\nÉtude expérimentale\nNous décrivons les données utilisées dans cette étude dans la section 3.1. Les résultats obtenus et leur interprétation sont donnés dans la section 3.2 et la section 3.3.\nDonnées utilisées\nAfin de tester notre proposition, nous utilisons des jeux de données synthétiques étiquetées (c.à-d. générées de façon à contenir des clusters de trajectoires qui sont supposés être les clusters naturels par la suite). La stratégie de génération de ces données est la suivante. L\u0027espace couvert par le réseau routier (le rectangle minimal englobant tous ses sommets) est quadrillé en grille contenant des zones rectangulaires de tailles égales. Un cluster de trajectoires est alors généré comme suit. Une zone dans la grille du réseau routier est sélectionnée au hasard. Tous les sommets inclus dans cette zone sont sélectionnés pour jouer le rôle de points de départ éventuels pour les trajectoires appartenant au cluster. De façon similaire, une deuxième zone est sélectionnée au hasard et ses sommets sont retenus pour jouer le rôle de points d\u0027arrivée. Pour chaque trajectoire à inclure dans le cluster, un sommet de départ (resp. d\u0027arrivée) est tiré au hasard parmi les sommets de départ (resp. d\u0027arrivée). La trajectoire est générée comme étant le plus court chemin reliant les deux sommets sélectionnés. Le nombre de trajectoires dans chaque cluster est fixé au hasard entre deux seuils paramétrables.\nPour illustrer les différentes informations qu\u0027on peut tirer avec l\u0027approche proposée et pour des soucis de clarté et de visibilité nous nous contentons de montrer les résultats obtenus sur un jeu de données composé de 85 trajectoires seulement. Ces trajectoires sont répandues sur cinq clusters distincts (cf . FIG. 1 \nAnalyse des clusters de trajectoires\nLe clustering par optimisation de la modularité du graphe des trajectoires produit, au départ, un partitionnement contenant trois clusters seulement et ne détecte donc pas les clusters naturels présents dans les données. Ce problème de résolution est d\u0027ailleurs l\u0027une des limitations des approches basées sur la modularité où certaines communautés restent fusionnées et ne sont donc pas détectées. Cependant, l\u0027implémentation que nous utilisons (celle décrite dans Rossi et Villa-Vialaneix (2011)) résout ce phénomène en effectuant une descente récursive sur les communautés découvertes et produit donc une hiérarchie de clusters emboités. Le deuxième niveau de cette hiérarchie révèle l\u0027existence de huit clusters. La matrice croisée de ceux-ci avec les clusters naturels est illustrée dans TAB. 2 qui montre que les clusters trouvés sont purs. Trois des clusters originaux ont été retrouvés de façon exacte tandis que les deux autres ont été éclatées sur plusieurs clusters plus fins (le cluster 1 est éclaté en trois classes et le cluster 3 sur deux). Ce choix de \"sur-partitionnement\" reste, cependant, tout à fait légitime et justifiable au vu des différences assez notables entre trajectoires constituant chacun de ces deux clusters. Le biclustering génère une partition des trajectoires fidèle aux motifs générés aléatoirement. La matrice de confusion (Tableau 3) montre que les classes de trajectoires retrouvées par le biclustering sont pures, seules deux classes artificielles ont été scindées en deux par la méthode MODL. Cette technique sera donc préférée puisqu\u0027elle découvre des motifs similaires aux motifs obtenus par maximisation de modularité, en étudiant directement le graphe biparti, se passant ainsi de toute projection et pré-traitements.\nAnalyse croisée des clusters\nNous proposons maintenant d\u0027étudier la matrice d\u0027adjacence du graphe biparti d\u0027origine. On a réordonné les lignes et les colonnes de cette matrice de manière à rapprocher les trajectoires et les segments regroupés dans les mêmes clusters (voir Figure 2).\nOn observe dans le cas de l\u0027étude de graphes projetés (Figure 2(a)) que les clusters regroupent des segments parcourus par les mêmes trajectoires, peu importe la quantité de trafic supportée. Les segments peu empruntés seront donc rattachés aux segments très empruntés par les rares trajectoires communes. Cela se caractérise dans la matrice par la présence de cellules (intersection des clusters de trajectoires et de segments) avec des distributions hétérogènes : certains segments sont couverts par toutes les trajectoires, d\u0027autres ne sont parcourus que par quelques trajectoires.  A contrario, les clusters de segments obtenus par biclustering sont corrélés avec leur usage. On va donc pouvoir caractériser ces usages dans le réseau et ainsi détecter les hubs (Figure 3(a)), les axes secondaires (Figure 3(b)) ou encore les ruelles peu empruntées. Le résultat obtenu ici est donc une caractérisation de la structure topologique sous-jacente du réseau, dont l\u0027information sur les usages est apportée par les trajectoires. Cela se matérialise sur la Figure 2 Définition (Contribution à l\u0027information mutuelle). La contribution à l\u0027information mutuelle, notée mi(c S , c T ), est définie de la manière suivante :\noù P (c S , c T ) est le probabilité pour un passage d\u0027appartenir à une trajectoire de c T et de couvrir un segment de c S , P (c S ) est la probabilité de parcourir un segment du cluster c S et P (c T ), la probabilité d\u0027être sur une trajectoire de c T .\nUne contribution positive à l\u0027information mutuelle signifie que le nombre de passages des trajectoires du cluster c T sur les segments du cluster c S est supérieur à la quantité de trafic attendu en cas d\u0027indépendance des clusters de trajectoire et de de segments. Dans le cas d\u0027une contribution négative, on observe une quantité de trafic inférieure à la quantité attendue. Enfin, une contribution à l\u0027information mutuelle nulle montre une quantité attendue de trafic ou alors un trafic très faible ou nul.\nLa Figure 4(b) présente les contributions à l\u0027information mutuelle de chaque couple de biclusters. Le bicluster en haut à gauche est très caractéristique dans le sens où le cluster de segments n\u0027est traversé que par un cluster de trajectoires et le cluster de trajectoire passe principalement par ce cluster de segments. Dans le cas présent, le cluster de trajectoires contient 21,6% des trajectoires étudiées et le cluster de segments 17,3% des segments du jeu de données. On s\u0027attend donc, en cas d\u0027indépendance, à observer 21,6% × 17,3% \u003d 3,7% des parcours totaux. Or ici, on observe 17,3% du parcours totaux, ce qui représente un important excès de trafic sur le groupe de segments par le groupe de trajectoires, par rapport au résultat attendu en cas d\u0027indépendance.\nL\u0027information mutuelle présente une information différente de celle apportée par la matrice de fréquence. On observe sur certains clusters de segments, un nombre de parcours significatifs par plusieurs clusters de trajectoires. Ce type de clusters est caractéristique des hubs routiers. Certains de ces clusters présentent peu de contrastes en terme d\u0027information mutuelle, ce qui signifie que, malgré la nature de hub du cluster, le trafic y est plutôt bien réparti.\nConclusion\nDans cet article nous avons étudié la classification des données de trajectoires sous un angle de clustering de graphes bipartis. L\u0027apport principal de cette étude se situe sur le plan méthodo-logique où nous avons montré l\u0027intérêt de ce genre d\u0027approches pour extraire des connaissances utiles sur le comportement des usagers du réseau routier. Nous avons notamment étudié le problème, dans un premier ordre, comme étant un problème de détection de communautés dans deux graphes séparés décrivant les trajectoires d\u0027une part et les segments routiers d\u0027une autre part. Nous avons, ensuite, étudié le biclustering direct du graphe biparti décrivant les trajectoires et les segments en même temps. Les algorithmes de clustering utilisés ici (par optimisation de la modularité dans les cas des projections du graphe biparti et MODL pour le biclustering) servent à illustrer l\u0027intérêt de notre formulation du problème. Il est donc tout à fait possible de les remplacer par d\u0027autres algorithmes de clustering de graphes tels que le clustering spectral.\nIl serait intéressant de tester nos approches sur des données réelles et d\u0027en comparer les résultats avec des faits réels. Il est également intéressant d\u0027étudier leur comportement en présence de données bruitées où les clusters à découvrir sont moins évidents.\n"
  },
  {
    "id": "393",
    "text": "Résumé\nL\u0027accès croissant à une information pléthorique et le développement de gisements de données ambitieux posent aujourd\u0027hui deux grands types de difficultés aux historiens.\nLe premier consiste à mettre en relation des gisements qui ont été développés de manière indépendante. C\u0027est par exemple le cas pour l\u0027intégration d\u0027un ensemble de bases de données prosopographiques développées entre 1980 et 2010 au Lamop, ou même dans le cadre d\u0027un projet dont le seul lien est une problématique spatiale et temporelle (projet ANR-DFG, Euroscientia).\nLe deuxième tient en la nature des données introduites dans ces différents systèmes : elles sont souvent hétérogènes, ambiguës, floues. Pour que le chercheur puisse se les approprier, les données doivent faire l\u0027objet d\u0027un véritable travail, afin de comprendre comment elles ont été obtenues, structurées. L\u0027historien doit donc les évaluer et les valider s\u0027il souhaite les mettre en relation. Cette évaluation nécessitant, elle-même de pouvoir être commentée, partagée et critiquée par d\u0027autres chercheurs.\nDans les deux cas, il est nécessaire de développer des outils d\u0027appropriation, qui permettent d\u0027entrer dans le réel historique contenu dans les stocks de données. C\u0027est là la fonction du projet Histobase, un système permettant d\u0027entrer dans la structuration des gisements, d\u0027en évaluer l\u0027information, d\u0027ajouter des couches d\u0027interprétation (qualification de l\u0027information historique) de les évaluer et de partager les données « obtenues ». Chacune des analyses individuelles et collectives fait l\u0027objet d\u0027une mémorisation. Il faut pour cela laisser une place importante aux historiens en tant qu\u0027expert en prêtant une attention particulière aux processus métiers qu\u0027ils mettent en oeuvre.\nBiographie\nStéphane Lamassé et Julien Alerini sont docteurs en histoire médiévale et moderne et enseignent à l\u0027Université de Paris 1.\n"
  },
  {
    "id": "394",
    "text": "Introduction\nLa classification de séries temporelles (TSC) est un sujet qui a été intensivement étudié durant les dernières années. Le but est de prédire la classe d\u0027un objet (une série temporelle ou courbe) ? i \u003d 1 , x 1 ), (t 2 , x 2 ), . . . , (t mi , x mi ) (où x k , (k \u003d 1..m i ) est la valeur de la courbe au temps t k ), étant donné un ensemble de séries temporelles labellisées d\u0027apprentissage. Les problèmes de TSC sont différents des problèmes de classification supervisée dans les bases transactionnelles puisqu\u0027il y a une dépendance temporelle entre les attributs ; ainsi l\u0027ordre des attributs importe. La TSC est applicable dans de nombreux domaines dont les données sont des séries temporelles : e.g., pour le diagnostic médical (par exemple la classification d\u0027élec-trocardiogramme de patients) mais aussi dans d\u0027autres domaines comme la maintenance de machines industrielles, la finance, la météo, . . . Le grand nombre d\u0027applications a succité de nombreuses approches ; toutefois la majorité de la communauté s\u0027est attachée à suivre le processus suivant (Liao, 2005) : (i) choisir une nouvelle représentation des données, (ii) choisir une mesure de similarité (ou une distance) pour comparer deux séries temporelles et enfin (iii) utiliser l\u0027algorithme (NN) du plus proche voisin (avec la mesure choisie sur la représentation choisie) comme classifieur. Ding et al. (2008) propose un état de l\u0027art des différentes représen-tations et mesures ainsi qu\u0027une étude expérimentale comparative basée sur le classifieur NN. Il en ressort que le classifieur NN couplé avec une distance Euclidienne ou Dynamic Time Warping (DTW) présente les meilleures performances prédictives pour les problèmes de TSC.\nPlus récemment, Bagnall et al. (2012) démontre expérimentalement que les performances de certains classifieurs augmentent fortement en utilisant certaines représentations (par rapport au domaine temporel original) ; ainsi, pour un classifieur donné, il existe une forte variance de performance selon la transformation de données utilisée. Pour pallier ce problème, Bagnall et al. (2012) proposent une méthode ensembliste basée sur trois représentations (ainsi que sur les données originales) : les résultats expérimentaux démontrent (i) l\u0027importance de la repré-sentation dans les problèmes de TSC et (ii) qu\u0027une simple combinaison ensembliste de plusieurs représentations permet d\u0027atteindre des performances prédictives très compétitives. Nous adhérons à cette conclusion sur l\u0027importance des représentations ; toutefois une des faiblesses de certains classifieurs ensemblistes est la perte en interprétabilité due à la combinaison (par pondération) des classifeurs.\nUn exemple illustratif : les graphiques de la figure 1 confirment l\u0027intérêt du changement de représentation : si à partir des données originales (a), il n\u0027est pas évident de différencier les deux classes (bleu/rouge), de simples transformations (ici l\u0027intégrale cumulative (b) et la double intégrale cumulative (c)) facilitent la discrimination des classes. En effet, après transformation par double intégrale cumulative, les courbes (séries) ayant des valeurs supérieures à 100 sont bleues et les courbes avec des valeurs inférieures à -100 sont rouges. Sur cet exemple jouet (extrait de la base TwoPatterns de la base de l\u0027UCR (Keogh et al., 2011)), une transformation et deux descripteurs nous permettent de caractériser les deux classes de courbes. Dans cet article, nous proposons un processus de construction de descripteurs interprétables pour le problème de TSC. Notre contribution est donc essentiellement méthodologique. La section suivante décrit les différentes étapes de notre processus : (i) une étape de transformation des données originales en de nouvelles représentations ; (ii) une étape de coclustering ; (iii) l\u0027exploitation des résultats du coclustering pour construire de nouveaux descripteurs et ainsi une nouvelle base de données ; et enfin le classifieur utilisé. La section 3 rapporte la validation expérimentale de notre processus.\nProcessus de construction de descripteurs\nNotations. Pour le problème de classification supervisée de séries temporelles (TSC), une série temporelle est définie par une paire (? i , y i ) où ? i est un ensemble d\u0027observations ordonnées ? i \u003d 1 , x 1 ), (t 2 , x 2 ), . . . , (t mi , x mi ) de longueur m i et y i une valeur de classe. Une base de données de séries temporelles D est définie comme un ensemble de paires D \u003d {(? 1 , y 1 ), . . . , (? n , y n )}, où chaque série temporelle peut avoir un nombre d\u0027observations différent donc une longueur différente 1 . Le but est de construire un classifieur à partir de D pour prédire la classe de nouvelles séries temporelles ? n+1 , ? n+2 , . . .\nPour ce faire, nous appliquons le processus de construction de descripteurs décrit par la figure 2 dont chaque étape est détaillée dans la suite.\nData transformation\nRecoding data \nTransformations/Représentations\nDe nombreuses méthodes de transformation ont été proposées dans la littérature pour représenter les séries temporelles : par exemple les transformations polynomiales, symboliques, spectrales, en ondelettes, . . . (voir (Ding et al., 2008) pour une vue synthétique sur les repré-sentations). Pour notre processus, nous utilisons les données originales ainsi que six représen-tations :\nLes dérivées : DV et DDV Nous utilisons les dérivées et dérivées doubles des séries temporelles (i.e. les différences et différences doubles locales entre les valeurs au temps t et t ? 1).\nCes transformations nous permettent de représenter l\u0027évolution locale (croissante/décroissante, accélération/décélération) des séries.\nLes intégrales cumulatives : IV et IIV Nous utilisons aussi les intégrales cumulatives (simples et doubles) des séries temporelles (calculées via l\u0027approximation par la méthode des trapèzes).\nCes transformations nous permettent de représenter l\u0027évolution globale (accumulée) des séries.\nLe spectre de puissance : PS. Une série temporelle peut-être décomposée en une combinaison linéaire de sinusoïdes d\u0027amplitudes p, q et de phase w. Ainsi :\nOn appelle transformée de Fourier la série de paires ? i,F T \u003d 1 , q 1 ), . . . (p mi , q mi ) Et, ? if le spectre de puissance (PS) est obtenu par la somme des carrés des coefficients de Fourier :\nLes f k représentent la fréquence et les a k la puissance du signal. Cette transformation nous permet de représenter la série dans le domaine de fréquence.\net où ¯ x et s 2 sont la moyenne et la variance de la série originale. L\u0027ACF décrit comment les valeurs originales séparées par une certaine durée évoluent ensemble. L\u0027ACF permet de détecter des structures d\u0027autocorrélation dans les séries temporelles.\nAinsi pour une base de données de séries temporelles D orig , nous construisons six nouvelles bases de données :\nDans la suite, par souci de généralisation, un objet d\u0027une de ces représentations sera appelé \"courbe\" au lieu de série temporelle puisque D P S n\u0027est plus dans le domaine temporel.\nCoclustering\nUne courbe peut être vue comme un ensemble de points (X, Y ) décrits par ses valeurs en abscisses et en ordonnées. Un ensemble de courbes peut être vu comme un ensemble de  (Ramsay et Silverman, 2005), elle s\u0027adapte bien pour le cas particulier des courbes comme définies ci-dessus. KHC est libre de tout paramétrage utilisateur, robuste (évite le surapprentissage), supporte des bases de données de courbes de plusieurs millions de points et sa complexité en temps est de ?(N ? N log N ) où N est le nombre de points de la base : c\u0027est donc une méthode adaptée à notre problématique.\nKHC est une méthode basée sur l\u0027estimation de densité constante par morceaux et suit l\u0027approche MODL (Boullé, 2006) (similaire à une approche Bayésienne (MAP) Maximum A Posteriori). Le modèle optimal M , i.e., la grille optimale est obtenue par optimisation (gloutonne bottom-up) d\u0027un critère Bayésien qui mise sur un compromis entre précision et robustesse du modèle :\nLa grille obtenue constitue un estimateur non-paramétrique de la densité jointe des courbes et dimensions des points. Du point de vue de la théorie de l\u0027information, selon Shannon (1948), les logarithmes négatifs de probabilités s\u0027interprètent comme des longueurs de codage. Ainsi, le critère cost peut être interprété comme la longueur de codage du modèle (la grille) plus la longueur des données D connaissant le modèle M , selon le principe de Minimum Description Length (MDL (Rissanen, 1978)).\nUn exemple de visualisation de résultat de coclustering. La figure 3 présente un exemple de visualisation de deux clusters de courbes de la grille optimale obtenue pour la base de données TwoPatterns (transformée par IIV). Le graphique (a) (resp. (b)) présente un cluster dont les courbes sont majoritairement de classe c 1 (bleu dans l\u0027exemple introductif de la figure 1),\nFIG. 3 -Représentation de l\u0027information mutuelle des cellules pour deux clusters de courbes obtenus par la méthode KHC sur la base TwoPatterns entière (transformée par IIV) : (a) cluster dont les courbes sont majoritairement de classe c 1 ; (b) cluster dont les courbes sont majoritairement de classe c 2 . Plus les couleurs sont vives, plus la différence de distribution de points entre la cellule courante (donc du cluster) et le reste des données est significative.\n(resp. c 2 , rouge dans l\u0027exemple introductif). La grille optimale obtenue par KHC est composée de 133 clusters de courbes, 7 intervalles pour X et 22 intervalles pour IIV . L\u0027estimateur de densité jointe obtenue (i.e. la grille optimale) est plus fin que nécessite le problème de départ : en effet, la base TwoPatterns est un problème de classification à 4 classes or nous obtenons 133 clusters de courbes ; ce qui nous donne un potentiel de caractérisation fine des classes du problème, lorsque la représentation s\u0027y prête.\nConstruction de descripteurs\nA partir de chaque résultat de coclustering obtenus sur chacune des représentations utilisées -k C attributs numériques (un pour chaque cluster C de courbes issu de M rep ) dont la valeur pour une courbe c id est la distance définie par d(c id , C) \u003d cost(M rep,c id ?C ) ? cost(M rep ), i.e., la différence de coût entre le modèle optimal M rep et M rep,c id ?C , la grille optimale dans laquelle on a intégré la courbe c id au cluster de courbes C. Intuitivement, la distance d mesure la perturbation qu\u0027apporte l\u0027intégration d\u0027une courbe à un cluster de la grille optimale.\n-Un attribut catégoriel indiquant l\u0027index du cluster de courbes i C le plus proche d\u0027un objet courbe c id selon la distance définie ci-dessus (i.e. au sens du critère cost utilisé pour l\u0027optimisation de la grille). -k Y attributs numériques (un pour chaque intervalle i Y de Y issu de M rep ) dont la valeur pour une courbe c id est le nombre de points de c id dans l\u0027intervalle i Y . Ainsi pour une courbe donnée c id , nous avons les informations suivantes fournies par les descripteurs (pour chaque représentation) : (i) la distance de c id à tous les clusters de courbes, (ii) l\u0027index du cluster de courbes le plus proche et (iii) le nombre de points de c id dans chaque intervalle de Y .\nClassification supervisée\nNous avons vu que notre processus de construction de descripteurs peut générer des centaines de descripteurs par représentation. Ainsi, l\u0027ensemble total d\u0027attributs générés F tot peut contenir plusieurs milliers d\u0027attributs. Le classifieur en fin de processus doit pouvoir supporter un grand nombre d\u0027attributs et doit être capable de sélectionner les attributs pertinents pour la tâche de classification supervisée. Nous choisissons le classifieur sélectif Naive Bayes (SNB (Boullé, 2007)) qui répond à ces attentes. Notons aussi que le prédicteur SNB exploite des prétraitements (de type MODL) de variables numériques par discrétisation et de variables catégorielles par groupement de valeurs en utilisant des estimateurs de densité conditionnelle robustes. Ainsi les varibles construites profitent de ces prétraitements et offrent un potentiel d\u0027interprétabilité (voir section 3). De plus, le SNB est libre de tout paramétrage utilisateur, ce qui facilite l\u0027utilisation de l\u0027ensemble du processus.\nValidation expérimentale\nL\u0027implémentation de notre processus utilise des outils déjà existants (KHC pour le coclustering et SNB pour la classification supervisée, disponibles sur http://www.khiops.com). Le branchement entre ces outils est encore au stade de prototype et a été réalisé avec MATLAB.\nProtocole. Pour valider notre processus, nous utilisons 26 bases de données de classification de séries temporelles : 17 bases de l\u0027UCR (Keogh et al., 2011) et 9 nouvelles bases introduites dans . Une description succinte des caractéristiques de ces données est présentée dans la table 1. Cet ensemble de données présente une grande variété de bases tant en terme d\u0027applications, qu\u0027en terme de nombre d\u0027instances, de classes et en longueur de série. Les expériences sont menées en suivant un protocole train-test prédéfini pour chaque base. Nous comparons notre processus de classification, qu\u0027on appellera ici MODL-TSC, avec : (i) DTW-NN un classifieur basé sur le plus proche voisin et la distance Dynamic Time Warping, considéré par la littérature comme difficile à battre ; (ii) TSC-ENSEMBLE  qui exploite de multiples représentations via une méthode ensembliste et l\u0027agorithme du plus proche voisin (NN).\nNotons que depuis 2012, il existe d\u0027autres bases de données répertoriées par l\u0027UCR pour la TSC. Toutefois, nous nous limitons à ces 26 bases pour nos expérimentations comparatives car les performances prédictives de nos concurrents (rapportées de  TAB. 1 -Description des bases de données de séries temporelles.\nsont accessibles que pour ces bases. D\u0027autre part, les bases de séries de l\u0027UCR sont un cas particulier du cadre général dans lequel nous nous plaçons, puisque pour une base donnée, toutes les séries sont de la même longueur et utilisent le même domaine temporel (i.e., les t k sont identiques).\nRésultats. Les résultats en terme de taux d\u0027erreur sont reportés dans la table 2. Le meilleur résultat pour chaque base est mis en gras. Premièrement, les résultats globaux (Taux d\u0027erreur moyen, nombre de victoires et rang moyen) indiquent que MODL-TSC est très compétitif par rapport aux deux méthodes de l\u0027état de l\u0027art. Même si nous avons l\u0027avantage numérique, cet ensemble de données ne nous permet pas de montrer qu\u0027il y a une différence significative de performance entre les trois méthodes. En effet, nous avons procédé au test de Friedman (Demsar, 2006) et ne pouvons rejeter l\u0027hypothèse nulle.\nNotons les performances remarquables de MODL-TSC sur les bases OSULeaf, FordA, ElectricDevices et ARSim. Sur ces bases, la différence de performance est d\u0027au moins 0,1 (i.e. 10%) par rapport à ses concurrents. Ici, l\u0027apport des représentations (via les nouveaux descripteurs) est certainement à l\u0027oeuvre dans notre processus, alors que TSC-ENSEMBLE n\u0027exploite que 3 représentations et que DTW-NN se base sur les données originales. A l\u0027inverse, les performances de MODL-TSC sont dramatiques pour les bases ECG200, Coffee et OliveOil. La différence de performance tourne en notre défaveur (au moins 0,1 par rapport aux deux concurrents). Nous pensons que cette différence peut être dûe à deux raisons : (i) les bases d\u0027apprentissage de ECG200, OliveOil et Coffee sont très petites (quelques dizaines de courbes), ce qui rend l\u0027apprentissage difficile ; (ii) nous n\u0027avons pas encore trouvé la bonne représentation qui Ces expériences rappellent l\u0027importance des représentations pour la TSC d\u0027une manière gé-nérale, et en particulier dans notre processus. Nous pouvons donc espérer une amélioration des performances prédictives en rajoutant des représentations de la littérature dans notre processus.\nInterprétation : un exemple. Pour la représentation IV de la base TwoPatterns, la grille optimale obtenue par KHC est composée de 224 clusters de courbes, 11 intervalles pour X et 9 intervalles pour Y IV . Les deux variables les plus pertinentes (parmi toutes les variables générées à partir de toutes les représentations) selon les prétraitements MODL du SNB sont issues de la représentation IV et sont :\n1. v 1 , le nombre de points dans l\u0027intervalle I Y IV \u003d] ? ?; ?3, 9082] 2. v 2 , l\u0027index du cluster le plus proche Le groupement de valeurs pour v 2 et la discrétisation pour v 1 fournissent les tables de contingence suivantes en apprentissage (cf tables 3 et 4) : Nous observons (table 3) que le nombre de points p d\u0027une courbe dans I Y IV (i.e. le nombre de points dont la valeur est inférieure à -3,9082) est pertinent pour caractériser sa classe. En effet, en apprentissage, les courbes telles que p ? 7 sont de classe c 1 ; lorsque p \u003e 29 elles sont très majoritairement de classe c 4 et lorsque 7 \u003c p ? 12 elles sont majoritairement de classe c 3 .\nDans la table 4, nous observons tout d\u0027abord que le prétraitement supervisé par groupement de valeurs MODL sur la variable v 2 (\"index du cluster de courbes le plus proche\") produit 4 groupes : G 1 , (resp. G 2 , G 3 et G 4 ) constitués de 56, (resp. 53, 53 et 62) index de clusters qui sont majoritairement de classe c 4 (resp. c 3 , c 2 , c 1 ). La forme diagonale de la table de contingence (table 4) indique la pertinence de l\u0027attribut v 2 pour caractériser la classe d\u0027une courbe. En effet, par exemple, si i C l\u0027index du cluster de courbes le plus proche d\u0027une courbe c id appartient au groupe G 2 (i.e. i C ? G 2 ), alors c id est considéré comme très similaire aux courbes de classe c 3 . De plus, la variable \"index du cluster de courbes le plus proche\" est un indicateur de la pertinence de la représentation pour notre processus dans le problème courant de TSC. Dans cet exemple, la variable v 2 à elle seule permet de caractériser environ 95% de la base, donc la représentation IV est très pertinente pour caractériser les classes de la base TwoPatterns. A l\u0027inverse, pour la représentation originale (D V ), la grille optimale générée par KHC est composée de 255 clusters de courbes mais le prétraitement indique que la variable \"index du cluster de courbes le plus proche\" n\u0027est pas pertinente pour caractériser les classes de la base TwoPatterns.\nConclusion \u0026 Perspectives\nNous avons proposé MODL-TSC, un processus générique de construction de descripteurs pour le problème de la classification supervisée de séries temporelles (TSC). Ce processus est libre de tout paramétrage utilisateur et donc simple d\u0027utilisation. Il se décompose en trois étapes : (i) génération de multiples représentations des données par le biais de transformations ; (ii) application d\u0027une technique de coclustering sur chacune des représentations ; iii construction de descripteurs à partir des résultats du coclustering. La nouvelle base de données objets-attributs -dont les objets (identifiant les séries temporelles) sont décrits par des attributs issus des diverses représentations générées-est notre base d\u0027apprentissage. Pour classer de nouvelles séries nous utilisons un classifieur naïf Bayésien sélectif. Les résultats expéri-mentaux ont montré que les performances prédictives de MODL-TSC sont très compétitives et comparables aux meilleures approches de la littérature.\nLes premiers résultats expérimentaux sont prometteurs et confirment l\u0027importance des transformations dans la TSC. En effet, selon les applications, certaines transformations faciliteront la découverte de motifs caractérisant les classes de séries temporelles. De plus, la combinaison de plusieurs représentations par le biais de notre processus MODL-TSC permet d\u0027atteindre des performances prédictives très compétitives. Nous avons utilisé quelques représentations simples dans ces travaux préliminaires pour démontrer le bien fondé de ce processus de construction de descripteurs -ce qui nous laisse un potentiel d\u0027amélioration des performances pour les données où MODL-TSC est moins performant que ses concurrents, pour peu qu\u0027on trouve la bonne représentation. \"Chercher la ou les bonnes représentations via une transformation\" est certainement la principale perspective à ce travail et ce que nous pouvons recommander à ceux qui s\u0027intéressent à la TSC. La littérature sur la TSC regorge de représen-tations (voir (Wang et al., 2012) pour une vue d\u0027ensemble) et trouver une bonne représentation pour la TSC est toujours un sujet d\u0027actualité (e.g. ). D\u0027autre part, une perspective pratique sera d\u0027identifier la ou les bonnes représentations pour un domaine d\u0027application spécifique (e.g., ECG, consommation électrique, . . . ).\n"
  },
  {
    "id": "395",
    "text": "Introduction\nLa découverte de motifs est une tâche centrale en fouille de données et est utilisée avec succès dans un grand nombre d\u0027applications. Une limite bien connue des processus de fouille de données est la production d\u0027un grand nombre de motifs qu\u0027il n\u0027est pas possible d\u0027examiner manuellement et parmi lesquels l\u0027information utile est diluée. L\u0027extraction de motifs sous contraintes permet de cibler l\u0027information recherchée selon les centres d\u0027intérêt de l\u0027utilisateur. Un prolongement récent de cette voie de recherche est la prise en compte de l\u0027intérêt d\u0027un motif en fonction des autres motifs extraits, afin de produire des ensembles de motifs qui satisfont des propriétés sur l\u0027ensemble des motifs considérés conjointement (Raedt et Zimmermann, 2007;Khiari et al., 2010). Notre travail se situe dans cette lignée et porte sur la notion de requêtes skylines (Börzsönyi et al., 2001). Notre originalité est d\u0027introduire la souplesse dans la relation de dominance caractérisant les skylines dans le contexte de la fouille de données et de montrer l\u0027apport de la Programmation Par Contraintes (PPC) pour cela.\nLa notion de skylines a été récemment étendue à la fouille de données pour extraire des motifs skylines (appelés skypatterns) (Soulet et al., 2011). Les skypatterns traduisent les préférences d\u0027un utilisateur selon une relation de dominance. Dans un espace multidimensionnel où chaque dimension définit une préférence, un point p 1 domine un autre point p 2 ssi p 1 est meilleur ou égal à p 2 sur toutes les dimensions, et est strictement meilleur sur au moins une dimension. Par exemple, un utilisateur peut préférer les motifs ayant une fréquence peu élevée, une petite taille et une confiance élevée. Dans ce cas, un motif p 1 domine un autre motif p 2 ssi : f req(p 1 ) ? f req(p 2 ) ? taille(p 1 ) ? taille(p 2 ) ? conf iance(p 1 ) ? conf iance(p 2 ), où au moins une\nFIG. 1 -Exemple de points skylines dans un espace à deux dimensions.\nde ces inégalités est stricte. Les skypatterns sont intéressants à double titre : ils permettent de s\u0027affranchir de la notion de seuil sur les mesures et la relation de dominance exprime une forme d\u0027intérêt global sur toutes les dimensions qui est facilement compréhensible par l\u0027utilisateur. Néanmoins, le cadre actuel est rigide et des motifs pouvant en réalité s\u0027avérer intéressants (les motifs dominés proches des skypatterns) ne sont pas proposés. L\u0027exemple suivant (portant sur des points skylines) illustre cette situation. Exemple. L\u0027entraîneur d\u0027une équipe de football souhaite recruter un ou plusieurs joueurs pour la prochaine saison. Chaque joueur sur le marché est caractérisé par le nombre de buts qu\u0027il a marqués et le nombre de passes décisives qu\u0027il a effectuées durant la saison actuelle (cf. figure 1). Seuls les joueurs p 1 , p 2 , p 3 , p 4 et p 5 sont des skylines, les autres joueurs (i.e. p 6 , p 7 , p 8 , p 9 et p 10 ) étant dominés par au moins un joueur skyline. Néanmoins, ces joueurs non-skylines peuvent s\u0027avérer intéressants pour un recrutement : -pour le recrutement d\u0027un attaquant, l\u0027entraîneur privilégierait le nombre de buts marqués. Outre les joueurs p 1 et p 2 , les joueurs non-skylines p 6 et p 9 sont des candidats intéressants. -pour le recrutement d\u0027un milieu de terrain offensif, l\u0027entraîneur privilégierait le nombre de passes décisives. Outre les joueurs p 4 et p 5 , les joueurs non-skylines p 7 et p 8 deviennent intéressants. -pour le recrutement d\u0027un joueur polyvalent, l\u0027entraîneur privilégierait plutôt le compromis entre le nombre de buts marqués et le nombre de passes décisives réalisées. Outre les joueurs p 3 et p 4 , le joueur non-skyline p 10 est alors un candidat intéressant. Par ailleurs, les joueurs skylines sont très recherchés et ils sont coûteux parce qu\u0027ils sont fortement sollicités : leurs salaires pourraient être hors de portée du budget du club. Les joueurs non-skylines proches des joueurs skylines peuvent ainsi devenir d\u0027un grand intérêt pour l\u0027entraî-neur. Nous verrons comment de tels joueurs sont découverts grâce à l\u0027introduction de la souplesse dans la relation de dominance. Contributions. Nous proposons une approche nouvelle et efficace pour l\u0027extraction de skypat-terns (durs et souples) en utilisant le cadre PPC. Nous montrons comment l\u0027extraction des (soft-)skypatterns peut être modélisée et résolue avec des techniques PPC. Un tel choix présente deux avantages majeurs. Tout d\u0027abord, nous sommes en mesure d\u0027améliorer l\u0027étape d\u0027extraction grâce à l\u0027ajout de contraintes postées dynamiquement, ces contraintes étant déduites de l\u0027ensemble des motifs candidats déjà extraits. De plus, l\u0027aspect déclaratif du cadre PPC permet de gérer de manière unifiée plusieurs types de relaxation des skypatterns. Finalement, la pertinence de l\u0027approche est mise en évidence par une étude de cas en chémoinformatique pour la découverte de toxicophores.\nLa section 2 de cet article introduit les concepts de base. La section 3 présente le problème de l\u0027extraction de soft-skypatterns. Notre méthode d\u0027extraction des skypatterns (durs et souples) fondée sur la PPC est détaillée en section 4. La section 5 présente un état de l\u0027art synthétique. Les résultats de nos expérimentations sur la découverte de toxicophores sont donnés en section 6.\nExtraction des skypatterns\nExtraction de motifs sous contraintes\nSoit I un ensemble de littéraux distincts appelés items. Un itemset (ou motif) est un sousensemble non nul de I. La langage d\u0027itemsets correspond à L \u003d 2 I \\?. Une base de données transactionnelle est un multi-ensemble de motifs de L. Une entrée de la base de données est appelée transaction et est un élément de L. La table 1 (gauche) présente un ensemble de données transactionnelles D où chaque transaction t i rassemble des articles décrits par des items notés A,. . .,F . L\u0027exemple classique est une base de données de supermarchés dans laquelle chaque transaction correspond à un client et chaque item de la transaction à un produit acheté par ce client. Un prix est associé à chaque produit (cf. table 1, à droite).\nLa fouille de motifs sous contraintes a pour objectif d\u0027extraire l\u0027ensemble des motifs de L qui satisfont une requête (i.e., une conjonction et/ou disjonction de contraintes). La contrainte de fréquence est l\u0027exemple le plus classique, elle conduit à extraire les motifs X i dont le nombre d\u0027occurrences dans D dépasse un seuil minimal min f r fixé par l\u0027utilisateur : freq(X i ) ? min f r . D\u0027autres mesures quantifient l\u0027intérêt des motifs recherchés comme par exemple la taille (nombre d\u0027items composant un motif), le prix moyen (prixMoy : moyenne des prix associés aux items d\u0027un motif), ou encore l\u0027aire (aire(X i ) \u003d f req(X i ) × taille(X i )). Dans de nombreuses applications, on souhaite caractériser des contrastes entre sous-ensembles de transactions, tels que par exemple en chémoinformatique des molécules toxiques versus non toxiques. A cet effet, le taux de croissance est une mesure de contraste très courante (Novak et al., 2009) que nous utiliserons en section 6. Soit D une base de données partitionnée en deux sous-ensembles \n|D1|×f req(Xi,D2) . Par ailleurs, la communauté porte maintenant une grande attention aux ensembles de motifs (Raedt et Zimmermann, 2007;Khiari et al., 2010;Guns et al., 2011) où l\u0027intérêt d\u0027un motif dépend des autres motifs extraits. La conception de classifieurs, les top-k motifs, ou encore les skypatterns se situent dans cette lignée.\nNotion de skypattern\nCette section présente la problématique de l\u0027extraction des skypatterns. Nous commençons par définir la notion de dominance.\nConsidérons l\u0027exemple de la table 1 et supposons que M \u003d{f req, aire}. Le motif BCD domine le motif BC car f req(BCD)\u003df req(BC)\u003d5 et aire(BCD)\u003eaire(BC). Pour M \u003d{f req, taille, prixM oy}, le motif BDE domine BCE car f req(BDE)\u003df req(BCE) \u003d 4, taille(BDE)\u003dtaille(BCE) \u003d 3 et prixM oy(BDE)\u003eprixM oy(BCE). Étant donné un ensemble de mesures M , si un motif est dominé par un autre par rapport aux mesures de M , il est considéré comme non-intéressant et ne doit pas être dans le résultat final. Cette idée est au coeur de la notion de skypatterns. DÉFINITION 2 (Opérateur skypattern) : Étant donnés un ensemble de motifs P ? L et un ensemble de mesures M ? M, un skypattern de P par rapport à M est un motif non-dominé dans P par rapport à M . L\u0027opérateur skypattern Sky(P, M ) renvoie tous les skypatterns de P par rapport à M : Sky(P, M ) \u003d {X i ? P | ?X j ? P, X j M X i } Le problème de l\u0027extraction des skypatterns consiste à évaluer la requête Sky(L, M ). Ainsi, pour le jeu de données de la table 1, Sky(L,{f req,size})\u003d{ABCDEF, BCDEF, ABCDE, BCDE, BCD, B, E}. Les skypatterns sont représentés à la figure 2(a). L\u0027aire hachurée à la figure 2(a), nommée zone interdite, est la zone où il ne peut pas y avoir de skypatterns. La zone de dominance est au-dessus de la ligne bleu, cette dernière est appelée bordure de l\u0027aire de dominance. Elle marque la frontière entre ces deux zones.\nL\u0027extraction des skypatterns est un problème difficile en raison du nombre très élevé de motifs candidats (i.e. |L|) et une énumération naïve de L n\u0027est pas réaliste. Par exemple, avec 1000 items une approche naïve aurait besoin de calculer (2 1000 ? 1)×|M | valeurs de mesures, et ensuite de comparer les 2 1000 motifs entre eux. Dans (Soulet et al., 2011), les auteurs proposent une méthode efficace qui tire partie des relations théoriques entre les représentations condensées de motifs et les skypatterns, rendant ainsi le processus d\u0027extraction faisable lorsque la représentation condensée des motifs peut être extraite. Toutefois, cette méthode est limitée à la version dure de la relation de dominance.\nExtraction des soft-skypatterns\nCette section montre comment introduire de la souplesse dans la problématique des skypatterns. L\u0027idée consiste à relâcher la relation de dominance afin de capturer les motifs prometteurs de la zone interdite. À cette fin, nous définissons deux types de soft-skypatterns : les edge-skypatterns qui appartiennent à la bordure de l\u0027aire de dominance (voir la section 3.1) et les ?-skypatterns qui sont proches de cette bordure (voir la section 3.2).\nEdge-skypattern\nComme pour les skypatterns, les edge-skypatterns sont définis selon une relation de dominance et un opérateur Sky. Ces deux notions sont reformulées comme suit : DÉFINITION 3 (Dominance stricte) : Étant donné un ensemble de mesures M ? M, un motif X i domine strictement un motif X j par rapport à M , noté X i M X j , ssi ?m?M , m(Xi)\u003em(Xj). DÉFINITION 4 (Opérateur edge-skypattern) : Étant donnés un ensemble de motifs P ? L et un ensemble de mesures M ? M, un edge-skypattern de P par rapport à M , est un motif non-strictement dominé dans P par rapport à M . L\u0027opérateur Edge-Sky(P, M ) retourne tous les edge-skypatterns de P par rapport à M : Edge-Sky(P, M )\u003d{X i ? P || ?X j ? P :X j M X i } Le problème d\u0027extraire les edge-skypatterns consiste à évaluer la requête Edge-Sky(L, M ). Notons que Sky(P, M ) ? Edge-Sky(P, M ). La figure 2(a) illustre les 28 \u003d 7+(4+8+3+4+2) edge-skypatterns extraits de l\u0027exemple de la table 1. On remarque que tous les edge-skypatterns appartiennent à la bordure de l\u0027aire de dominance, 7 d\u0027entre eux sont des skypatterns durs.\n?-skypattern\nComme illustré par l\u0027exemple donné en introduction, l\u0027utilisateur peut être intéressé par des skypatterns exprimant un compromis entre les mesures. Les ?-skypatterns expriment un tel compromis.\nDÉFINITION 6 (Opérateur ?-skypattern) : Étant donné un ensemble de motifs P ? L et un ensemble de mesures M ? M, un ?-skypattern de P par rapport à M est un motif non-dominé dans P par rapport à M . L\u0027opérateur ?-Sky(P, M ) retourne tous les ?-skypatterns de P par rapport à M : ?-Sky(P, M ) \u003d {X i ? P | ?X j ? P : X j ? M X i } Le problème d\u0027extraction des ?-skypatterns consiste à évaluer la requête ?-Sky(P, M ). Notons que Sky(P, M ) ? ?-Sky(P, M ) (l\u0027égalité a lieu pour ?\u003d0) et que Edge-Sky(P, M ) ? ?-Sky(P, M ). En appliquant l\u0027opérateur ?-Sky(P, M ) sur notre exemple (cf. table 1), avec ?\u003d0.25, nous obtenons 10 \u003d 6+1+3 nouveaux motifs (cf. figure 2(b)), auxquels il faut ajouter les 28 edge-skypatterns (cf. section 3.1).\n4 Mise en oeuvre de l\u0027extraction à l\u0027aide de la PPC L\u0027extraction des skypatterns (durs ou souples) s\u0027effectue en deux étapes. La première consiste à accroître la zone interdite par extensions successives. Pour cela, on construit une suite de motifs X 1 , X 2 , ..., X n où chaque X i+1 améliore X i selon au moins une des mesures, étendant ainsi la zone interdite courante. Le processus s\u0027arrête lorsque la zone interdite ne peut plus être étendue. Soit Cand \u003d{X 1 , X 2 , ..., X n } l\u0027ensemble des points ainsi obtenus. Cand constitue un sur-ensemble des skypatterns recherchés, qu\u0027il suffit de filtrer dans une seconde étape. La mise en oeuvre s\u0027effectue de manière simple et déclarative grâce aux CSP dynamiques. L\u0027implantation a été réalisée en Gecode en étendant l\u0027extracteur de motifs (basé CSP) développé par (Khiari et al., 2010).\nExemple introductif\nL\u0027exemple de la figure 3 illustre la construction de la zone interdite pour deux mesures m 1 et m 2 . Soit X 1 le premier motif extrait avec \u003cm 1 (X 1 )\u003d2, m 2 (X 1 )\u003d2\u003e ; il ne peut y avoir de skypattern dans l\u0027aire délimitée par X 1 (zone interdite en vert à la figure 3). On recherche alors un motif X qui améliore l\u0027ensemble des éléments de Cand , ici X 1 , selon au moins une des mesures (i.e. tel que (m 1 (X)?2)?(m 2 (X)?2)). Soit X 2 \u003c3, 4\u003e un tel motif (X 2 améliore X 1 pour m 1 et m 2 ), alors la zone bleue devient interdite. On recherche alors un motif X qui améliore X 2 selon au moins une des mesures. Soit X 3 \u003c5, 4\u003e un tel motif (X 3 améliore X 2 pour m 1 ), alors la zone rouge devient interdite. On recherche alors un motif X qui améliore X 3 selon au moins une des mesures. Soit X 4 \u003c6, 3\u003e un tel motif, alors la zone jaune devient interdite. À nouveau, on cherche un motif X qui améliore l\u0027ensemble des éléments de Cand . Soit X 5 \u003c4, 5\u003e un tel motif. Supposons désormais qu\u0027il n\u0027existe aucun motif améliorant X 5 pour au moins une des\ndeux mesures. L\u0027ensemble Cand des motifs ainsi obtenus contient les motifs X 1 , X 2 , X 3 , X 4 et X 5 et constitue un sur-ensemble des skypatterns. Cand est alors filtré afin de ne retenir que les motifs qui sont des skypatterns.\nExtraction des skypatterns à l\u0027aide des CSP dynamiques\nUn problème de satisfaction de contraintes (CSP) est défini par un triplet (X , D, C) où X est un ensemble de variables, D est l\u0027ensemble de leurs domaines finis, et C est un ensemble de contraintes portant sur X . Pour l\u0027extraction de motifs en fouille de données (Khiari et al., 2010), X est l\u0027ensemble des motifs inconnus, chacun d\u0027eux ayant L\u003d2 I \\? comme domaine. C peut contenir des contraintes arithmétiques sur les mesures, des contraintes ensemblistes sur les motifs, ainsi que des propriétés spécifiques que doivent vérifier certains motifs (par exemple, être un motif fermé pour une ou plusieurs mesures).\nUn CSP dynamique (DCSP) est une séquence de CSP, où chaque CSP résulte de changements apportés au précédent (Verfaillie et Jussien, 2005). Ces changements peuvent affecter les variables (ajout/suppression), les domaines (ajout/suppression de valeurs), les contraintes (ajout/suppression). Dans notre approche, variables et domaines restent identiques. Seules sont ajoutées de nouvelles contraintes permettant d\u0027agrandir (dynamiquement) la zone interdite comme l\u0027illustre l\u0027exemple de la figure 3. Considérons le DCSP P 1 , P 2 , . . ., P n où chaque\n) où X i est solution de la requête q i (X). La contrainte closed M (X), qui impose que X soit un motif fermé pour les mesures de M , permet de réduire le nombre de motifs redondants. En effet, les skypatterns fermés par rapport à M constituent une représentation condensée exacte de l\u0027ensemble des skypatterns (Soulet et al., 2011).\nCand est l\u0027ensemble des solutions successives obtenues en résolvant le DCSP P 1 , P 2 , ..., P n . Chaque fois qu\u0027un nouveau motif X i (solution de la requête q i (X)) est extrait, on recherche alors un nouveau motif X permettant d\u0027agrandir la zone interdite dans au moins une de ses mesures (requête q i+1 (X)). Le processus s\u0027arrête pour la valeur n 0 telle que la requête q n0+1 (X) ne possède aucune solution (on ne peut plus agrandir la zone interdite). Cand est alors filtré afin de ne conserver que les motifs qui sont des skypatterns.\nExtraction des soft skypatterns à l\u0027aide des CSP dynamiques\nL\u0027extraction des ?-skypatterns s\u0027effectue de manière analogue aux skypatterns. Le seul changement concerne les contraintes disjonctives ajoutées dynamiquement. Considérons le DCSP P\nComme précédemment, Cand est l\u0027ensemble des solutions successives obtenues en résolvant le DCSP P 1 , P 2 , ..., P n . Chaque fois qu\u0027un nouveau motif X i est extrait, on recherche alors un nouveau motif X permettant d\u0027agrandir la zone interdite. Le processus s\u0027arrête lorsqu\u0027on ne peut plus agrandir cette zone et Cand est alors filtré pour ne contenir que les motifs qui sont des ?-skypatterns.\nÉtat de l\u0027art\nLe calcul des skylines dans les bases de données s\u0027apparente à la recherche d\u0027un vecteur maximal en géométrie algorithmique (Matousek, 1991), au calcul de la frontière Pareto (Kung et al., 1975) et à la recherche d\u0027une solution optimale en optimisation multi-critères (Steuer, 1992). (Jin et al., 2004) ont introduit la notion de thick skylines dans les bases de données. Un thick skyline est soit un skyline P , soit un point P dominé par un skyline P et tel que P soit proche de P (leur distance est inférieure à un seuil La notion de ?-skypattern que nous proposons est une extension des thick skylines dans le cadre de l\u0027extraction de motifs. La notion de skypattern, ainsi que leur calcul à partir de représentations condensées selon différentes mesures, a été introduite par (Soulet et al., 2011). (Gavanelli, 2002) a proposé une première méthode fondée sur la PPC pour déterminer la frontière Pareto. Cette technique est proche de notre méthode de résolution mais elle est développée uniquement pour les skylines durs portant sur les n-uplets de la base.\nExpérimentations\nLa toxicologie est la science qui étudie les effets toxiques des substances chimiques sur les organismes vivants. Un défi majeur en chémoinformatique est d\u0027identifier les caractéristiques des structures chimiques liées à une activité toxique donnée (e.g. CL50\n1 .). Les fragments 2 molécu-laires responsables des propriétés toxiques d\u0027une substance chimique sont appelés toxicophores et leur découverte est bien souvent à la base des modèles de prédiction en (éco)toxicité (Verhaar et al., 1992;Benigni et Bossa, 2011). L\u0027objectif de la présente étude menée en collaboration avec le laboratoire CERMN 3 , est d\u0027étudier l\u0027apport de soft-skypatterns pour la découverte semiautomatique de toxicophores.\nProtocole expérimental\nNous avons utilisé un jeu de données issu du site de l\u0027ECHA 4 . Pour évaluer le potentiel toxique d\u0027une substance, des indicateurs de toxicité également appelés endpoints permettent de quantifier leur impact sur la santé humaine et sur l\u0027environnement. Nous avons utilisé l\u0027indicateur quantitatif de toxicité vis à vis des organismes aquatiques CL50. En fonction de la valeur de cet indicateur, les molécules sont réparties dans les trois catégories suivantes : H400 très toxique (CL50 ? 1 mg/L), H401 toxique (1 mg/L \u003c CL50 ? 10 mg/L), et H402 nocif (10 mg/L \u003c CL50 ? 100 mg/L). Dans cette étude, nous nous concentrons uniquement sur les classes H400 et H402 afin de maximiser le contraste entre classes. Le jeu de données D utilisé contient 567 molécules, 372 de la classe H400 et 195 de la classe H402. Les molécules sont représentées en utilisant 1450 sous-graphes fréquents fermés, initialement extraits de D 5 avec un seuil de fréquence de 1%.\nSkypattern\nEdge-skypattern TAB. 2 -Analyse de la performance de l\u0027extraction de soft-skypattern.\nAfin de découvrir des toxicophores candidats, nous combinons des mesures de contraste très utilisées en fouille (Novak et al., 2009) et caractérisant les motifs du point de vue de leur présence dans les données telles que le taux de croissance et la fréquence avec des mesures exprimant des connaissances chimiques telles que l\u0027aromaticité d\u0027une molécule qui est un indicateur connu de la toxicité. Notre méthode offre un moyen naturel de combiner simultanément dans un même cadre ces mesures provenant de différentes origines. Nous présentons maintenant ces mesures. Taux de croissance. Cette mesure permet de caractériser une molécule d\u0027une classe (toxique) par rapport à une autre classe (non-toxique). Une substance chimique qui possède dans sa structure des fragments moléculaires d\u0027un motif avec une valeur élevée du taux de croissance est particulièrement susceptible d\u0027être toxique. Fréquence. Les motifs de faible fréquence sont souvent dus à des artefacts dans les données et constituent du bruit. Afin d\u0027assurer une représentativité de l\u0027information extraite, nous imposons une contrainte de fréquence minimale. Aromaticité. L\u0027intérêt de cette mesure est qu\u0027elle véhicule une hypothèse toxicophore : plus la valeur d\u0027aromaticité est forte, plus la molécule possédant ces fragments moléculaires tend à être toxique puisque ses métabolites peuvent mener à des espèces réactives pouvant interagir de manière néfaste avec des biomacromolécules. L\u0027aromaticité d\u0027un motif est calculée en utilisant la fonction mean \u003d min+max 2 des valeurs d\u0027aromaticité de ses fragments moléculaires. Enfin, la redondance est réduite à l\u0027aide des skypatterns fermés qui sont une représentation exacte condensée de l\u0027ensemble des skypatterns pour ces trois mesures (cf. section 4.2).\nNous avons réalisé plusieurs expérimentations avec différentes combinaisons de ces trois mesures. Pour le paramètre ?, nous avons considéré deux valeurs : 10% et 20%. Une analyse qualitative des soft-skypatterns extraits effectuée par les chimistes permet l\u0027identification de toxicophores connus. Toutes nos expérimentations ont été réalisées sur un processeur Intel core i3 à 2,13 GHz ayant 4 Go de RAM. Nous analysons d\u0027abord les performances de notre approche sur un plan quantitatif. Ensuite, nous procédons à une analyse qualitative de résultats.\nAnalyse des performances et de la qualité des skypatterns extraits\nLa table 2 compare les performances des trois opérateurs aussi bien en termes de nombre de skypatterns extraits qu\u0027en temps de calcul, pour différentes combinaisons de mesures.\nL\u0027augmentation du nombre de mesures conduit à un plus grand nombre de soft-skypatterns extraits. En effet, un motif domine rarement tous les autres motifs sur l\u0027ensemble des mesures. Néanmoins, dans nos expérimentations, ce nombre reste raisonnablement réduit, au plus, il y a un maximum de 1052 ?-skypatterns. Par ailleurs, les temps de calcul indiquent que notre approche est efficace (moins de 1 heure), même avec l\u0027augmentation du nombre de mesures (sauf pour ? \u003d 0, 2, où le nombre de ?-skypatterns et le temps de calcul augmentent sensiblement). De ces résultats, nous dressons le bilan suivant.\nTout d\u0027abord, en utilisant le taux de croissance et la fréquence, seulement huit skypatterns ont été détectés mais trois toxicophores bien connus ont été retrouvés. Deux d\u0027entre eux sont des composés aromatiques : le chlorobenzène (code smiles 6 : {Clc}) et le phénol {c1(ccccc1)O}). La contamination de l\u0027eau et des sols par les produits chimiques aromatiques est en effet très répandue puisqu\u0027on les retrouve dans de nombreux produits industriels tels que des pesticides, des solvants, des colorants et même des explosifs. Beaucoup de ces produits peuvent s\u0027accumuler dans la chaîne alimentaire et ont un effet néfaste aussi bien sur les plantes que sur les animaux et l\u0027homme. Le troisième toxicophore mis en évidence correspond au motif organophosphate ({OP, OP\u003dS}) qui figure également dans de nombreux pesticides. Concernant les soft skypatterns, aucune information supplémentaire n\u0027est venue enrichir notre liste de toxicophores dans ce premier cas. On peut cependant souligner une énumération plus précise des cycles aromatiques substitués par un atome de chlore (ex. {Clc(ccc)c, Clcccc}) et des organophosphates (ex. {OP(\u003dS)O), COP(\u003dS)O}).\nEnsuite, en considérant le taux de croissance et l\u0027aromaticité, ou la fréquence et l\u0027aromaticité, les résultats sont relativement similaires. Bien que la liste des toxicophores détectés grâce aux skypatterns soit limitée en comparaison de celle obtenue précédemment (taux de croissance et fréquence), l\u0027extraction des soft-skypatterns permet d\u0027identifier de façon beaucoup plus exhaustive des cycles aromatiques de différentes natures. En effet, cette nature peut varier en fonction i) de la présence/absence d\u0027hétéroatomes (N, S, O), ii) du nombre de cycles, iii) de la pré-sence/absence de substituants. Les edge-skypatterns permettent ainsi d\u0027extraire des composés aromatiques azotés (ex. indole {ncc, c1cccccc1}, benzoimidazole {ncnc, c1ccccc1}), des composés aromatiques soufrés (ex. benzothiophene {scc, c1ccccc1}), des composés aromatiques oxygénés (ex. benzofurane {coc, c1ccccc1}) et des composés aromatiquess hydrocarbonés (ex. naphthalène {c1ccc2ccccc2c1}). Les ?-skypatterns viennent compléter cette liste avec de nouveaux systèmes polycycliques (ex. biphényle {c1ccccc1c2ccccc2}). Il est également important de noter que dans ce cas, les ?-skypatterns permettent de détecter un autre type de toxicophore très néfaste pour les organismes aquatiques, à savoir les amines aromatiques (ex. aniline {c1(ccccc1)N}).\nEnfin, les meilleurs résultats ont été obtenus en considérant simultanément trois mesures (taux de croissance, fréquence et aromaticité). En effet le phénol, le chlorobenzène et le motif organophosphate sont détectés dès l\u0027énumération des skypatterns. De plus, une information concernant les cycles aromatiques azotés est également extraite. Les edge et ?-skypatterns permettent ensuite de retrouver l\u0027ensemble des cycles aromatiques plus \"exotiques\" discutés précé-demment. De plus, les edge-skypatterns permettent d\u0027identifier de façon beaucoup plus efficace les organophosphates (ex. {COP(\u003dS)O, O(P(OC)\u003dS)C, O(CC)P\u003dS}).\nLa figure 4 montre la répartition des skypatterns (durs et souples) pour les trois mesures considérées. Les skypatterns durs sont situés dans différentes régions (cf. les patterns p 1 et p 2 et ceux inclus dans les quatre ellipses e 1 , e 2 , e 3 et e 4 ). Le motif p 1 correspond à un cycle aromatique substitué par un atome de chlore, alors que p 2 désigne un motif organophosphate. Les autres skypatterns inclus dans e 1 , e 2 , e 3 et e 4 correspondent respectivement aux cycles aromatiques azotés (ex. {nc}), aux cycles aromatiques alkylés (ex. {cC}), au chlorobenzène et au phénol.\n6. Code smiles est une notation en ligne pour décrire la structure des molécules chimiques : http://www. daylight.com/dayhtml/doc/theory/theory.smiles.html Concernant les soft-skypatterns, la plupart des edge-skypatterns sont situés sur la bordure du volume de dominance correspondants aux motifs inclus dans e 1 . Ces motifs complètent la liste des cycles aromatiques qui n\u0027ont pas été trouvés lors de l\u0027extraction des skypatterns durs, tels que les cycles aromatiques soufrés (ex. {cs}) et le biphényle. Enfin, pour les ?-skypatterns, les plus informatifs sont ceux qui se trouvent autour des motifs appartenant à e 1 (ex. naphtalène et aniline).\nConclusion\nDans cet article, nous avons proposé d\u0027introduire de la souplesse dans la problématique des skypatterns. Nous avons montré comment celle-ci permet de découvrir des motifs intéressants qui seraient manqués autrement. En s\u0027appuyant sur la PPC, nous avons proposé une méthode efficace pour l\u0027extraction des skypatterns et des soft-skypatterns. L\u0027apport et l\u0027efficacité de notre approche sont mises en évidence par une étude de cas en chémoinformatique portant sur la découverte de toxicophores. Les résultats expérimentaux montrent l\u0027intérêt d\u0027utiliser les soft-skypatterns afin d\u0027obtenir de nouvelles connaissances en chimie. Dans le futur, nous voulons étudier l\u0027introduction de la souplesse sur de nouvelles tâches comme le clustering, l\u0027apport des soft-skypatterns pour la recommandation et d\u0027étendre notre approche aux skycubes.\nRemerciements. Ce travail a été soutenu par l\u0027Agence Nationale de la Recherche, référence ANR-10-BLA-0214. Nous voulons aussi remercier Bertrand Cuissart, Guillaume Poezevara et Ronan Bureau pour les discussions stimulantes et fructueuses à ce sujet.\n"
  },
  {
    "id": "397",
    "text": "Introduction\nLa théorie des hypergraphes se propose de généraliser la théorie des graphes en introduisant le concept d\u0027hyperarête où les arêtes ne relient plus un ou deux sommets, mais un nombre quelconque de sommets. De ce fait, un hypergraphe est défini comme une extension du traditionnel graphe, dans lequel les liens entre des ensembles de sommets sont appelés hyperarêtes. Une traverse minimale correspond à un ensemble de sommets qui intersecte toutes les hyperarêtes d\u0027un hypergraphe, en étant minimal au sens de l\u0027inclusion. L\u0027extraction de ces traverses minimales a fait l\u0027objet de plusieurs travaux dans littérature du fait de la diversité de ses applications dans des domaines variés tels que l\u0027intelligence artificielle, la fouille de données, la cryptographie, le web sémantique, etc. Hagen (2008).\nPour résoudre ce problème, plusieurs approches ont été envisagées. Mannila et Toivonen (1997) ont prouvé que les traverses minimales d\u0027un hypergraphe H peuvent être calculées à partir de la bordure négative d\u0027un ensemble d\u0027itemsets vérifiant la contrainte d\u0027anti-monotonie Mannila et Toivonen (1997), Hébert et al. (2007). C\u0027est en se basant sur ces liens entre la fouille de données et la théorie des hypergraphes que nous proposons une nouvelle définition des traverses minimales, ainsi que leur représentation de manière succincte.\nNotre idée, inspirée des travaux de Hamrouni et al. est donc de chercher un sous-ensemble représentant de manière concise et exacte l\u0027ensemble des traverses minimales Hamrouni et al. (2008). Ce sous-ensemble, qu\u0027on appellera ensemble irrédondant de traverses minimales, sera construit en considérant l\u0027ensemble des hyperarêtes auxquelles appartient chaque sommet, appelé extension et en construisant un hypergraphe irrédondant limité à un sous-ensemble des sommets initiaux ayant des extensions différentes. L\u0027espace de recherche s\u0027en trouve alors ré-duit puisque plusieurs candidats ne seront pas générés par l\u0027algorithme. Cette intuition se base sur le fait que si deux sommets X et Y appartiennent aux mêmes hyperarêtes, (i.e, ils ont la même extension) et si X appartient à une traverse minimale T alors en substituant X par Y dans T on obtient une nouvelle traverse minimale. De plus, le risque de redondance est éliminé puisque Y n\u0027est pas pris en compte dans l\u0027exploration de l\u0027espace de recherche.\nCet article est organisé comme suit : dans la section 2, nous proposons une nouvelle dé-finition de la notion de traverse minimale et nous introduisons la notion de traverse minimale irrédondante qui nous permettra de proposer, dans la section 3, un algorithme original, appelé IMT-EXTRACTOR. Enfin, une étude expérimentale sur des hypergraphes aléatoires générés à partir des données issues du site de marque-page social DEL.ICIO.US sera décrite dans la section 4.\nTraverses minimales irrédondantes\nDans cette section, nous proposons de présenter des définitions et notations que nous utiliserons tout au long des sections suivantes. Pour aboutir à notre approche d\u0027extraction des traverses minimales, nous avons défini la notion de traverse minimale irrédondante à partir des notions de la théorie des hypergraphes suivantes. Berge (1989) Soit H \u003d (X , ?) avec X \u003d {x 1 , x 2 , . . . , x n } un ensemble fini d\u0027éléments et ? \u003d {e 1 , e 2 , . . . , e m } une famille de parties de X . H constitue un hypergraphe sur X si :\nDéfinition 1 HYPERGRAPHE\n2.\nDéfinition 2 TRAVERSE MINIMALE ET NOMBRE DE TRANSVERSALITÉ Berge (1989) Soit un hypergraphe H \u003d (X , ?). T ? X est une traverse de H si T e i \u003d ? ?i \u003d 1, . . . , m. ? H désigne l\u0027ensemble des traverses définies sur H. Une traverse T de ? H est dite minimale si 1 ? T t.q. T 1 ? ? H . On notera M H , l\u0027ensemble des traverses minimales définies sur H.\nLe nombre minimum de sommets formant une traverse est appelé le nombre de transversalité de l\u0027hypergraphe H et on le désigne par : ? (H) \u003d min |T|.\nDéfinition 3 EXTENSION D\u0027UN SOMMET\nSoit un hypergraphe H \u003d (X , ?) et x ? X . E \u003d (e 1 , e 2 , . . . , e l ) ? ? est une extension de x si x e i , ? e i E. Nous noterons EXTENT(x), l\u0027extension de x. Le lien entre l\u0027extension d\u0027un sommet x et son poids est donné par la formule : P oids (H) (x) \u003d |Extent(x)|. De plus, pour X ? X , on définit le poids de X de la façon suivante : P oids (H) (X ) \u003d | {e ? ? | (? x ? X, tel que x ? e)} | Définition 4 CLASSE DE SUBSTITUTION Une classe de substitution est un ensemble formé de tous les sommets de X qui ont la même extension. Ainsi, si deux sommets x i et x j appartiennent à des classes de substitution distinctes, alors EXTENT(x i ) \u003d EXTENT(x j ) et réciproquement, si deux sommets ont des extensions différentes, ils relèveront de classes de substitution différentes.\nDéfinition 5 REPRÉSENTANT D\u0027UNE CLASSE DE SUBSTITUTION\nEtant donnée une classe de substitution S, on dit que x X est le représentant de cette classe S si x est le premier élément de la liste triée par ordre lexicographique des sommets qui forment la classe S.\nLa méthode originale et efficace pour calculer les traverses minimales d\u0027un hypergraphe proposée dans cet article, consiste à réduire l\u0027hypergraphe en le représentant de manière plus concise mais sans perte d\u0027information. Elle exploite le fait que parmi l\u0027ensemble des traverses minimales, il y a une redondance d\u0027information. Cette redondance est liée au fait que deux ou plusieurs sommets qui ont la même extension, (i.e, appartiennent exactement aux mêmes hyperarêtes) tiennent, à tour de rôle, la même position dans une traverse minimale mais ne peuvent y appartenir en même temps.\nNotre approche pour une représentation concise des traverses minimales repose sur deux notions importantes : les classes de substitution et leurs Représentants. Les classes de substitution, calculées sur l\u0027hypergraphe H\u003d(X , ?), permettent tout d\u0027abord de construire un hypergraphe irrédondant H associé à H.\nDéfinition 6 HYPERGRAPHE IRRÉDONDANT ET TRAVERSE MINIMALE IRRÉDONDANTE\nSoit l\u0027hypergraphe H\u003d(X , ?), X ? X l\u0027ensemble des représentants des différentes classes de substitution associées aux sommets de H et ? l\u0027ensemble des hyperarêtes de H privées de éléments de X -X et défini par ? \u003d {e i ? X , e i ? X \u003d ?, ?e i ? ?}, alors l\u0027hypergraphe H \u003d (X , ? ) est appelé hypergraphe irrédondant associé à H.\nEn remarquant que toute traverse minimale de H constitue une traverse minimale irré-dondante de H, on en déduit que M H , l\u0027ensemble des traverses minimales de H (i.e des traverses irrédondantes de H) permet de construire l\u0027ensemble des traverses minimales de H. En effet, toute traverse minimale irrédondante T \u003d {x 1 , ...x l } de H, composée de l repré-sentants x i , i \u003d 1, .., l des classes de substitution S i , i \u003d 1, .., l permet de générer i\u003d1,l |S i | traverses minimales en remplaçant les représentants de chaque classe de substitution par les autres éléments de la classe. Dans la section suivante, nous introduisons, à partir du cadre méthodologique présenté, l\u0027algorithme IMT-EXTRACTOR pour l\u0027extraction des traverses minimales irrédondantes.\nL\u0027algorithme IMT-EXTRACTOR\nL\u0027algorithme IMT-EXTRACTOR, dont le pseudo-code, est décrit par l\u0027Algorithme 1 prend en entrée un hypergraphe et fournit en sortie l\u0027ensemble des traverses minimales. On suppose que les sommets de l\u0027hypergraphe sont triés par ordre lexicographique. L\u0027algorithme effectue un parcours en largeur, i.e, il opère par niveau pour générer les ensembles essentiels de candidats en exploitant la propriété d\u0027idéal ordre vérifiée par les ensembles essentiels de sommets.\nL\u0027algorithme commence par calculer les extensions de chaque sommet de X à partir desquelles seront construites les différentes classes de substitution (ligne 3). Cette tâche est effectuée par la procédure SEARCH-SUBSTITUTION. Cette procédure fournit en sortie les diffé-rentes classes de substitution, conformément à la proposition 4 avec pour chaque classe la liste des sommets qui la compose et son représentant. A partir de ces données, un nouvel hypergraph H est généré à l\u0027aide de la procédure CHANGE-HYP suivant la définition 6. Opérant retourner MT par niveau, l\u0027algorithme IMT-EXTRACTOR invoque la procédure GETMINTRANSVERSALITY (ligne 5) pour calculer le nombre de transversalité, (cf. définition 2) noté level dans l\u0027algorithme, et qui correspond au niveau renfermant les plus petites traverses minimales. Le nombre de transversalité correspond ainsi à la taille des premières traverses minimales candidates. La procédure GENERATE-CANDIDATES détermine ensuite tous les sous-ensembles de sommets de X de cardinalité égale à level (ligne 6). Ceci permet à l\u0027algorithme d\u0027éviter le balayage de l\u0027espace de recherche compris entre 1 et level ? 1. Une fois les ensembles de sommets de taille level générés, IMT-EXTRACTOR calcule le poids de chaque candidat avant de vérifier s\u0027il est strictement supérieur au poids maximum de ses sous-ensembles directs ou pas. Si cette dernière propriété est vérifiée et que le poids du candidat est égal à la cardinalité de ? (ligne 10), alors le candidat est stocké dans M T irr , l\u0027ensemble des traverses minimales irrédondantes et il est supprimé de l\u0027ensemble des candidats (ligne 12). Si, de plus, le poids du candidat est strictement inférieur à la cardinalité de ? alors il servira pour la génération des candidats de taille level + 1. Sinon, il est élagué de C i (ligne 14). Le processus est itéré jusqu\u0027à épuisement des candidats. A chaque itération, les candidats de taille i+1 (C i+1 ) sont générés à partir de C i par la procédure APRIORI-GEN(C i ) (Agrawal et Ramakrishnan (1994) \nEtude expérimentale\nDans cette expérimentation, quatre hypergraphes (H1, H2, H3 et H4) ont été générés à partir des données DEL.ICIO.US en considérant que les sommets correspondent aux utilisateurs et que chaque hyperarête représente une communauté constituée par les utilisateurs ayant partagé, au moins une ou plusieurs pages web. Le tableau 1 détaille les caractéristiques de chacun de ces hypergraphes. Les deux premières colonnes fournissent les probabilités minimale (p l ) et maximale (p u ) qu\u0027un sommet appartienne aux hyperarêtes dans l\u0027hypergraphe. La troisième colonne et la quatrième colonne indiquent respectivement le nombre de sommes |X | et le nombre d\u0027hyperarêtes |?| de l\u0027hypergraphe alors que la cinquième indique le nombre de Représentants, i.e., le nombre de classes de substitution. Ainsi, le nombre de sommets qui ne seront pas pris en compte lors de la génération des candidats est égal à |X | -Repr. Le nombre de transversalité ? (H) est donné dans la sixième colonne. Enfin, la septième colonne représente le nombre de traverses minimales alors que l\u0027avant-dernière représente celui des traverses minimales irrédondantes. Pour ce qui est de la dernière colonne, nous y trouvons le taux de compacité, noté ?, de chacun des hypergraphes, et dont la valeur est donnée par la formule : TAB. 1 -Caractéristiques de nos jeux de données.\nLe fait que O-M2D cible directement le niveau correspondant au nombre de transversalité, grâce à la procédure GETMINTRANSVERSALITY, permet à l\u0027algorithme d\u0027éviter la génération de candidats et les tests associés dans des niveaux où il ne peut pas y avoir de traverses minimales. Ceci est surtout avantageux quand le nombre de transversalité est élevé comme c\u0027est le cas pour H3 (15) et, encore plus pour H4 (23). Venons-en maintenant à IMT-EXTRACTOR. Le nombre des Représentants étant toujours inférieur ou égal au nombre de sommets, les temps d\u0027exécution de IMT-EXTRACTOR s\u0027en trouvent améliorés par rapport à ses concurrents. En analysant les cardinalités de |M H | et de IMT(H ), et le taux de compacité ?, nous constatons,\n"
  },
  {
    "id": "398",
    "text": "Introduction\nComprendre les changements de comportement des clients est un problème essentiel qu\u0027affronte les acteurs de la grande distribution alimentaire. Avec la concurrence féroce que se livre les acteurs de ce milieu, ne pas détecter assez tôt ces phénomènes, peut s\u0027avérer risqué. La trace des articles achetés par un client est l\u0027information la plus importante que possède les enseignes de la grande distribution pour détecter ces changements. En effet, les produits proposés constituent l\u0027élément essentiel sur lequel repose la fréquentation ou non de la clientèle. L\u0027étude de ces traces a été premièrement mise en lumière par Agrawal et Srikant (1994). Ils ont été les précurseurs de ce pan de recherche qu\u0027est l\u0027analyse du panier de la ménagère. Contrôler l\u0027évo-lution de ce panier permettrait de déceler au plus tôt les événements qui font que les clients entament un processus d\u0027attrition ou au contraire manifestent un intérêt grandissant pour les produits de l\u0027enseigne. A titre d\u0027exemple, la baisse de la qualité d\u0027un ou plusieurs produits au cours du temps ou encore l\u0027absence trop longue d\u0027un produit recherché par la clientèle sont des facteurs qui peuvent engendrer une baisse de fréquentation voire une perte de clientèle. A l\u0027inverse, une campagne promotionnelle sur de nouveaux produits \"attractifs\" peut élargir la base d\u0027achats des clients et du coup, améliorer la fidélisation vis à vis de l\u0027enseigne. Par ailleurs, quand on sait que des associations fortes entre produits peuvent exister, par exemple en recherchant les règles d\u0027association ou des liens de similarité, la baisse de qualité ou encore l\u0027absence d\u0027un produit peut provoquer un impact de baisse d\u0027achat non seulement sur les produits directement concernés, mais aussi indirectement sur les produits associés. Cet incidence sur les produits associés est bien souvent peu ou mal voire pas du tout analysée. Song et al. (2001) évaluent les modèles issus des données au fil du temps. L\u0027apparition ou la disparition d\u0027un modèle, l\u0027augmentation ou la diminution soudaine de leur support ou encore le changement d\u0027une partie de ces modèles sont évalués au moyen de critères présentés dans leur article. Chen et al. (2005) ou encore Bottcher et al. (2009) ont présenté de même des études analogues. Sachant que les modèles qu\u0027ils extraient tous, sont des règles d\u0027associations, on peut se poser la question de savoir comment est-ce qu\u0027ils les analysent. Il est connu que l\u0027extraction de règles d\u0027associations génèrent un problème au niveau du nombre de motifs extraits. Choi et al. (2005) et Chen (2007) utilisent diverses techniques afin de classer ce flot de règles extraites par ordre de pertinence. C\u0027est ici que se situe le principal apport de notre article. Nous choisissons de représenter les motifs de façon globale et ainsi de constater plus aisément les changements de tendances.\nDans cet article, nous proposons une approche qui permet de détecter de façon précoce de nouvelles tendances produits afin, d\u0027une part de limiter l\u0027attrition des clients et d\u0027autre part, d\u0027améliorer leurs fidélisation. Dans la section 2, nous explicitons précisément notre méthodo-logie que nous illustrons en section 3 avec des cas réels extraits de la base de données achats et clients d\u0027une enseigne de la grande distribution. Enfin, en section 4, nous présentons notre conclusion.\nNotre approche\nA notre sens, dans le paradigme de la grande distribution, quand on a l\u0027objectif de contribuer à la mise en oeuvre de stratégies proactives pertinentes, il faut pouvoir apporter des ré-ponses à trois phases. La première concerne la modélisation des liens entre articles fortement liés qui sera largement inspirée des réseaux sociaux. La deuxième phase concerne la recherche d\u0027articles clés dans chaque cluster. Enfin, la dernière phase se focalise sur l\u0027analyse de l\u0027évo-lution des clusters, en prenant en compte particulièrement la question des cycles et de la saisonnalité, afin de détecter de véritables tendances fortes. En premier lieu, nous présenterons la façon dont nous construisons notre réseau d\u0027articles. Puis, nous expliquerons comment déceler les articles pivots et enfin comment détecter les changements de tendances.\nConstruction du réseau\nLes réseaux sociaux sont un ensemble d\u0027entités sociales reliées entre elles par des liens pondérés ou non qui représentent des interactions. Raeder et Chawla (2009) proposent de modéliser le panier de la ménagère sous forme de réseau social qu\u0027il appelle réseau d\u0027articles. Ils construisent ce réseau de manière intuitive en considérant qu\u0027un article (noeud) est relié à un autre article si ils ont été vendus simultanément. La transition aura un poids correspondant au nombre de ventes simultanées. Un algorithme de détection de communautés est proposé afin de détecter les meilleurs regroupements d\u0027articles possibles au sein de ce réseau potentiellement très dense. Nous proposons à l\u0027inverse de créer ces clusters directement en calculant les indices de similarité de Jaccard entre les articles. Une transition existera entre deux articles si l\u0027indice de similarité respecte le seuil fixé par l\u0027utilisateur. En ne se fixant que le seuil de la similarité, on ne passe pas à coté des niches comportementales. En effet, le support, c\u0027est-à-dire le nombre d\u0027occurrences d\u0027un article ou d\u0027un ensemble d\u0027articles, n\u0027est pas un critère discriminant. De cette façon, si une minorité de clients commence à élaborer un schéma jamais vu, nous serons à même de le voir. Afin de garder, des informations sur une fenêtre variable, nous construisons notre réseau d\u0027articles avec des flags attachés aux transitions. Les flags marqueront les périodes sur lesquelles la transition existe.\nDiversité\nDans une enseigne, il est indubitable que des articles ont une plus grande importance que d\u0027autres. Dans cette optique, (Liu et al., 2010) a introduit la notion de diversité. Cette mesure caractérise à quel point le voisinage d\u0027un noeud est divers, même quand on ne dispose que de peu d\u0027information sur les noeuds. Dans notre contexte, nous disposons de beaucoup d\u0027informations sur les noeuds articles, comme leur nomenclature ou encore leur marque. Dans un magasin, les articles sont rangés selon leur rayon, leur famille puis sous famille d\u0027appartenance. Si un article est souvent lié dans notre réseau à plusieurs articles qui n\u0027appartiennent pas au même rayon, il se révèle donc d\u0027une forte diversité. Prenons l\u0027exemple du client qui achète un article à forte diversité. Il est fort probable qu\u0027il cherche à acquérir des articles qui lui sont fortement liés. Il devra par conséquent, effectuer des déplacements dans le magasin. Ces déplacements sont la clé pour atteindre le potentiel de vente maximum du client. En effet, le client sera susceptible d\u0027être tenté par des articles qui sont sur son parcours mais qu\u0027il n\u0027était pas venu acheter à l\u0027origine. Il en découle donc un potentiel d\u0027amélioration non négligeable de chiffres d\u0027affaires. Déceler les variations de ventes inhabituelles de ces articles, entre autres, relève d\u0027une importance capitale dans la fidélisation de la clientèle et de la pérennisation d\u0027un magasin. Les articles à forte diversité, de par leur importance, seront les véritables garants de la stabilité des ventes de l\u0027enseigne.\nSaisonnalité et rythme d\u0027achats\nPour mieux prendre en compte les aspects cycliques et saisonniers précisés en amont, nous nous fixons à la période mensuelle pour pouvoir analyser au mieux les tendances et les installations de celles-ci. Pour de multiples raisons, le mois est le compromis parfait :\n-il pondère au mieux les différences entre les rythmes d\u0027achat des clients en prenant simplement en compte le versement du salaire mensuel qui est le véritable garant des achats d\u0027un client -Pour deux articles fortement liés comme la confiture et le pain, clairement leur fréquence d\u0027achat pour une même personne n\u0027est la même. La cinétique de ces articles ne permet pas aux clients de les acheter tout le temps ensemble, -ce choix nous évite les épiphénomènes, essentiellement , lors de la disparition ou l\u0027apparition de connexions entre articles. Le choix de regrouper les articles au fil des périodes grâce à la similarité en occultant les questions de supports pondèrent les variations saisonnières des ventes. En effet, les quantités vendues seules, pourraient amener des résultats biaisés et potentiellement erronés.\nDétection de tendances\nPour un noeud, l\u0027apparition ou la disparition, la diversité ou la banalité et l\u0027évolution du nombre de voisins sont autant de paramètres qui indiqueront la stabilisation d\u0027une tendance dans notre environnement. Nous détectons l\u0027installation d\u0027une tendance si on observe un changement du rythme d\u0027apparitions de l\u0027article sur un nombre de mois prédéfini. Exemple : Si un article est considéré comme un pivot et à un rythme d\u0027apparition constant sur une période et qu\u0027il cesse d\u0027être pivot pendant un laps de temps défini. Nous déclenchons donc une alerte aux décideurs. \nApplication\nFIG. 1 -Réseau d\u0027articles le plus dense du mois d\u0027avril 2012\nUn second cas intéressant est celui de la disparition d\u0027un article 009119 et de la compensation de celui-ci par un autre de même type 149290. La figure 3 nous présente le nombre de clients ayant acheté le produit 009119 et le produit de compensation 149190. En avril (point d\u0027abscisse 15), nous constatons selon nos paramètres de départ pour notre algorithme, une tendance à la disparition du 009119. D\u0027après la figure 3, on suppose que cela correspond à une rupture de stock. De plus, à la même période, nous constatons l\u0027apparition du 149290. La figure 4 nous montre que l\u0027article 09119 est un article pivot (en rouge). En regardant, l\u0027article sur la figure 1, on constate le poids rédhibitoire de l\u0027article 149290. Au total, l\u0027article prendra 3 mois pour acquérir un statut d\u0027article pivot au même titre que son homologue. \nConclusion\nDans ce papier, nous présentons une façon originale de détecter les tendances produits en analysant les connexions entre ceux-ci. En modélisant les relations entre articles sous forme de réseau et en marquant l\u0027importance des articles, nous obtenons une vision globale des achats des clients à un moment donné. L\u0027évolution de ces réseaux peut être interprétée de différentes façons et le but avant tout est de prendre des décisions assez tôt pour gérer au mieux les événe-ments. Les décideurs commerciaux pour conforter la tendance devront gérer correctement le stock de l\u0027article et de ces satellites dans le but de répondre à la demande. De plus, dans le cas d\u0027une rupture inévitable, communiquer correctement sur un autre article de substitution afin\n"
  },
  {
    "id": "399",
    "text": "Introduction\nL\u0027annotation sémantique de documents du web est une des étapes pour assurer le succès des applications du Web Sémantique en tant que support à un accès partagé et unifié aux connaissances et documents, y compris de domaines spécifiques. En faisant appel à une ontologie, on assure un meilleur partage et une meilleure interprétation de ces annotations. La qualité des annotations sémantiques nécessite donc des ontologies de domaine de qualité. Même si des ressources comme WordNet 1 ou Yago 2 offrent des vocabulaires riches et répondent aux besoins de l\u0027annotation de connaissances générales en langue anglaise, les connaissances précises des ontologies de domaine présentent l\u0027avantage de mieux rendre compte de l\u0027information vé-hiculée par des documents et du sens des formulations linguistiques.\nDans un processus dual, quand les ontologies sont utilisées pour l\u0027annotation sémantique de collections de textes de domaines particuliers, leur construction à partir de ces textes contribue à ce que l\u0027ontologie couvre mieux les concepts et relations nécessaires à la caractérisation du contenu des documents. L\u0027ingénierie d\u0027ontologies à partir de textes a ainsi obtenu des résul-tats significatifs ces 10 dernières années   (Maedche, 2002), mais cette activité reste longue et complexe.\nLorsque l\u0027on veut enrichir ou peupler une ontologie existante, les annotations textuelles peuvent fournir des informations pour trouver des indices linguistiques de relations séman-tiques. Une première catégorie de systèmes utilise les annotations linguistiques : par exemple, RelExt de Schutz et Buitelaar (2005) exploite catégories et dépendances syntaxiques. Un deuxième ensemble d\u0027approches traite des annotations sémantiques en utilisant l\u0027ontologie à enrichir, comme la méthode proposée dans le projet CrossMarc (Valarakos et al., 2004) , ou celle de Navigli et Velardi (2006).\nNotre objectif est similaire à ces travaux : partant d\u0027un noyau d\u0027ontologie en adéquation avec une collection de textes, nous cherchons à l\u0027enrichir par analyse automatique de ces textes. La chaîne de traitements que nous proposons enrichit un noyau d\u0027ontologie avec des relations sémantiques définissant des restrictions de relations existant dans l\u0027ontologie afin de décrire des concepts précis. Notre approche est originale au sens où nous introduisons dans ce processus deux types de connaissances sur le texte, jusque là peu utilisés conjointement : des connaissances de niveau sémantique, à savoir les concepts déjà reconnus dans le texte, et des connaissances de niveau discursif, liées à l\u0027architecture du texte au sens de Virbel et Luc (2001). Nous avons évalué ce processus en réalisant une ontologie de plantes à partir de fiches de jardinage dont la structure est explicitée par des étiquettes XML. Le noyau d\u0027ontologie est construit à partir d\u0027une analyse manuelle de la structure de ce type de document. Il est composé du concept principal relié aux concepts racines de hiérarchies de concepts issues de l\u0027exploitation des valeurs des champs, la sémantique de ces relations étant fournie par le nom du champ. Les concepts pivots deviennent alors des sous-concepts du concept principal. Par exemple, dans un site web de jardinage, le concept pivot est celui de plante, chaque page décrit une plante spécifique (concept pivot de la page) à l\u0027aide de concepts relatifs à la nature des sols, à l\u0027entretien ou aux parties de la plante.\nLe processus d\u0027enrichissement vise à décrire précisément les concepts pivots à l\u0027aide de propriétés spécifiques. Elles sont exprimées par des relations qui restreignent les relations du noyau d\u0027ontologie ayant pour domaine le concept principal en spécialisant leur co-domaine. Pour cela, le texte et les balises des champs du document sont analysés. Identifier la correspondance entre les balises des champs et les propriétés du concept pivot est ce que nous appelons par la suite la sémantisation de la structure des documents, un processus à deux étapes :\n1. affecter un rôle aux champs de cette structure. Après lecture des fiches, l\u0027ontologue définit F C , l\u0027ensemble des champs porteurs du concept pivot et F R , l\u0027ensemble des champs porteurs de relations. Ces derniers contiennent a priori au moins une relation entre le concept pivot et un concept existant dans le noyau d\u0027ontologie et dont on s\u0027attend à trouver une trace linguistique dans le texte présent dans ce champ. 2. formuler les concepts et relations associées à chaque champ. C\u0027est là une originalité de la démarche. La recherche des relations ne s\u0027appuie pas sur la notion de patron lexico-syntaxique tels qu\u0027ils sont définis dans (Auger et Barriere, 2008). Elle repose sur l\u0027expression déclarative des concepts et relations pouvant être reconnus grâce à chaque balise XML et au texte qu\u0027elles encadrent. Cette expression est le fruit d\u0027une interprétation par l\u0027ontologue de la sémantique des documents. Cette approche généralise celle établie pour d\u0027autres types de documents structurés dans (Laignelet et al., 2011).\nEnrichissement de l\u0027ontologie par les relations\nPour identifier les concepts à relier au concept pivot, nous utilisons l\u0027annotation sémantique de ces textes à l\u0027aide des concepts déjà présents dans l\u0027ontologie. Dans une ontologie O \u003d (C, R), C est l\u0027ensemble des concepts du domaine, et R, l\u0027ensemble des relations liant les concepts. Nous définissons alors :\n-\n, avec c i et c j ? C, et r ? R définit une relation autre que la subsomption, respectivement de domaine c i et de co-domaine c j . -porte_c(f, c), avec c ? C et f ? F C , est un prédicat qui indique que le champ f est porteur du concept c. -porte_r(f, r), avec r ? R et f ? F R , est un prédicat qui indique que le champ f est porteur de la relation r(c i , c j ) où un des concepts c i ou c j est le concept principal et l\u0027autre est une super-classe d\u0027un des concepts exprimés dans ce champ. -a_label(c, t), avec c ? C et t un terme, indique que c a pour étiquette (rdfs :label) t. L\u0027algorithme d\u0027extraction de relations produit un ensemble de restrictions sur des relations r(ci, cj) ? R, où c i est le concept pivot de d, et c j spécialise le co-domaine de r.\nAnnotation des documents avec les concepts\nL\u0027algorithme d\u0027annotation de TextViz 3 (Reymonet et al., 2009) est utilisé pour annoter chacun des documents d de la collection avec le noyau d\u0027ontologie. Il s\u0027appuie sur les labels de concepts, sur les termes présents dans le texte et sur une distance sémantique. Soit le concept c ? C, d un document, et f ? F R ? F C . Le prédicat annote(c, f, d) indique que c annote le champ f de d. TextViz génère cette annotation si un des labels de c apparaît dans le contenu textuel du champ f du document d. Étant donné T f,d , les annotations de TextViz sont équivalentes au résultat de la règle : ?t ? T f,d , ?c ? C, si a_label(c, t) alors annote(c, f, d).\nIdentification des relations\nLe processus d\u0027identification de relations explore, pour chaque champ f de d tel que f ? F R , les annotations de f et le fichier déclarant les relations associées à f . Étant donnés c p le concept pivot du document d, un concept c i qui annote le texte du champ f , le processus vise à identifier la relation r ? R qui peut exister entre \nLe cas d\u0027étude du jardinage\nDans le projet MOANO 4 , une ontologie doit permettre la recherche sémantique d\u0027informations dans un livre de jardinage numérisé 5 . Ce livre, écrit en français, présente une grande collection de plantes et fournit, pour chacune, des informations sur son aspect, des conseils sur son entretien et des soins contre les maladies ou parasites. L\u0027annotation de ce livre doit donc s\u0027appuyer sur une ontologie dédiée aux plantes d\u0027un point de vue non pas de la botanique scientifique, mais du jardinage, qui doit contenir des concepts liés aux plantes, à leur aspect physique, aux conditions de leur développement, ainsi que des conseils pour les entretenir ou prévenir les maladies ou parasites. Pour décrire dans l\u0027ontologie chacune des variétés de plante \nEvaluation\nDeux ontologues ont enrichi le noyau d\u0027ontologie à partir de 10 documents du site choisis au hasard. A partir de chacun des documents, ils ont défini des concepts spécifiques de plante, et leurs propriétés, représentations que l\u0027on peut considérer comme les résultats attendus du processus automatique. Nous avons ensuite appliqué l\u0027algorithme d\u0027enrichissement à partir de ces 10 documents, puis calculé le rappel et la précision par rapport aux relations retenues par les ontologues. Le Rappel est évalué à 0.73 et la Précision à 0.8. Ces chiffres sont donc prometteurs mais difficiles à interpréter. Si l\u0027on étudie précisément pourquoi des relations ont été mal interprétées ou n\u0027ont pas été trouvées, on identifie 4 causes de problèmes : l\u0027incomplétude d\u0027une ressource (labels de l\u0027ontologie, liste de champs à analyser), la qualité de l\u0027annotation par les concepts (erreurs de l\u0027algorithme d\u0027annotation de TextViz, annotation des noms de champs), la qualité de l\u0027ontologie (signature ambigüe des relations, absence de labels de concepts), une définition incorrecte de la sémantique des champs, et les défauts de l\u0027analyse linguistique (négations ou intervalles numériques non traités). Les problèmes les plus fréquents sont liés à l\u0027analyse linguistique, en particulier à la gestion des négations, que nous sommes en train de mettre en place pour les corriger.\nConclusion\nNous avons montré que dans des collections de documents textuels semi-structurées et thé-matiquement homogènes, où chaque document décrit un concept d\u0027un même type dans toute la collection, l\u0027exploitation de la structure des documents peut être un atout majeur pour automatiser la modélisation du concept décrit dans chaque document. Nous avons ainsi défini une nouvelle approche pour enrichir une ontologie noyau en exploitant la structure des documents de la collection et leur annotation sémantique par les concepts du noyau. Les résultats obtenus 6. l\u0027encyclopédie botanique du site \"Jardin !L\u0027encyclopédie\" http ://nature.jardin.free.fr/\n"
  },
  {
    "id": "400",
    "text": "Introduction et présentation du problème\nL\u0027étude des populations par l\u0027analyse de détails d\u0027appel téléphonique est un problème auquel s\u0027intéressent les opérateurs de téléphonie mobile depuis quelques années. Différentes études ont été menées sur le sujet, dont certaines s\u0027intéressent à déterminer la structure communautaire des populations grâce au trafic inter-antennes (Blondel et al., 2010), (Guigourès et Boullé, 2011). Ces études de journaux d\u0027appels téléphoniques permettent de mettre évidence une segmentation naturelle du territoire liée à la langue ou aux zones d\u0027influences des grandes métropoles à l\u0027échelle d\u0027un pays, ou encore au profil économique et social des quartiers (bourgeois, populaire, étudiant ...) dans des études plus locales. De telles analyses intéressent les opérateurs de tous les pays, notamment ceux en voie de développement, où les besoins en terme de télécommunications sont amenés à être importants et où les usages demeurent actuellement encore inconnus.\nPour aller plus loin, on peut s\u0027intéresser à délimiter des zones géographiques où le comportement des usagers de téléphones mobiles est différent en fonction de la période de temps étudiée. Une analyse temporelle du trafic au niveau des groupes d\u0027antennes permet de caractériser les zones géographiques grâce aux excès et déficits d\u0027appels dans chacune des périodes étudiées.\nUne telle étude apporte aussi bien une information sur les différentes plages horaires qui structurent la journée, la semaine, le mois ou l\u0027année des utilisateurs de téléphones portables, que sur les lieux où les phénomènes temporels sont observés.\nUne des contraintes dont il faut tenir compte dans ce type d\u0027études est la volumétrie des données. Les données dont nous disposons et sur lesquelles nous souhaitons étudier les corrélations spatio-temporelles, sont un enregistrement quotidien des appels inter-antennes passés en France métropolitaine entre le 13 Mai et le 13 Octobre 2007. Le nombre d\u0027antennes réparties sur le territoire français est de 17895 entre lesquelles ont transités 1,12 milliards d\u0027appels. Dans la section 2, nous présentons des méthodes adaptées à ce type d\u0027analyse et justifions le choix de la solution utilisée. La section suivante présentera les résultats de l\u0027analyse des corrélations spatiales, puis la section 4 les corrélations temporelles. Une dernière partie conclut cet article en faisant un bilan de l\u0027analyse.\nÉtat de l\u0027art et choix de la solution\nLa première question qu\u0027il est important de se poser est la représentation des données. En effet, un appel est décrit par l\u0027antenne source, l\u0027antenne cible et le jour de l\u0027appel. Une précédente étude (Blondel et al., 2010) propose de représenter ces données sous forme d\u0027un graphe non orienté, définissant ainsi un réseau d\u0027antennes liées par le nombre d\u0027appels transitant entre elles. Nous préférons conserver le format tabulaire pour traiter ce problème, et ainsi garder l\u0027orientation naturelle des appels entre les antennes. Blondel et al. (2010) proposent de partitionner le réseau d\u0027antennes suivant une approche de maximisation de modularité. La modularité (Newman, 2006) évalue la qualité de la partition d\u0027un graphe en cliques, ou communautés dans la terminologie des réseaux sociaux, qui sont des ensembles de noeuds fortement connectés entre eux. Cette technique possède l\u0027avantage d\u0027être efficacement optimisable en terme de complexité (Blondel et al., 2008) et donc de pouvoir traiter de très grands volumes de données, comme c\u0027est le cas ici. Cependant, l\u0027hypothèse faite lorsqu\u0027on emploie ce type d\u0027approches est très forte en supposant que le réseau est modulaire, c\u0027est-à-dire qu\u0027il peut se subdiviser en groupes de noeuds fortement connectés entre eux. Si le réseau possède en effet une structure communautaire, la maximisation de modularité sera un outil très efficace. Cependant, la structure sous-jacente du réseau dans le cas présent étant inconnue, on ne peut faire d\u0027hypothèse sur la nature des motifs que nous souhaitons faire émerger des données. Nous devons donc trouver une alternative qui permette de considérer toutes les structures possibles du réseau.\nÉtat de l\u0027art\nLe concept de blockmodeling est à l\u0027origine des premiers travaux d\u0027analyse des structures dans les graphes menés par les sociologues dès les années 1950 dans le contexte de l\u0027analyse des réseaux sociaux (Nadel, 1957). Pour utiliser ce type d\u0027approches, nous devons étudier les données sous leur forme tabulaire. Les lignes et les colonnes de cette matrice représentent respectivement les antennes source et cible, et les valeurs indiquent la fréquence d\u0027appels entre couples d\u0027antennes. Il est ainsi possible de réorganiser les lignes et les colonnes dans le but de découper la matrice en blocs homogènes. Cette technique est appelée Blockmodeling. Une fois les blocs extraits, une partition des antennes représentées à la fois par les lignes et les colonnes peut être réalisée. Cette segmentation simultanée est appelée coclustering. En procédant ainsi, nous sommes capables de capturer des structures plus complexes que les structures retrouvées par maximisation de modularité, méthode qui correspond finalement à un blockmodeling diagonal.\nDe nombreuses méthodes de blockmodeling ont été proposées pour extraire des groupes dans les réseaux. Certaines se basent sur l\u0027optimisation d\u0027un critère (Doreian et al., 2004) permettant d\u0027isoler des blocs homogènes en se focalisant sur les blocs vides comme il est suggéré dans (White et al., 1976). Des approches déterministes plus récentes se sont intéressées à l\u0027optimisation de critères qui mesurent la qualité de la partition en blocs, en termes de résumé des données d\u0027origine (Reichardt et White, 2007). D\u0027autres utilisent le blockmodeling stochastique. Dans ces modèles génératifs, une variable latente indiquant l\u0027appartenance ou non à un cluster est associée à chaque noeud. Conditionnellement à leur variable latente, la probabilité d\u0027observer un arc entre deux acteurs suit une loi de probabilité (Bernoulli dans les cas les plus simples) dont les paramètres dépendent uniquement de la paire de clusters désignés par la variable latente. Les premières approches nécessitaient une paramétrisation du nombre de clusters par l\u0027utilisateur (Nowicki et Snijders, 2001), alors que les méthodes les plus récentes préfèrent le déterminer automatiquement en utilisant un processus de Dirichlet (Kemp et Tenenbaum, 2006).\nOutre la diversité des structures pouvant être inférées dans le réseau par les approches de coclustering, il est également possible réaliser un coclustering avec des variables numériques (Nadif et Govaert, 2010), (Boullé, 2012). Des blocs sont extraits des données et produisent une discrétisation de la ou des variables numériques. On peut ainsi appliquer, pour une seconde analyse, un blockmodeling sur des données tabulaires dont les lignes sont les antennes source et les colonnes une variable numérique temporelle. Ainsi, il est possible de trouver des structures temporelles par utilisation de la même méthode.\nDans le cas d\u0027une analyse de comptes rendus d\u0027appels, la technique employée doit présenter plusieurs propriétés :\n-Passage à l\u0027échelle : avec près de 18000 antennes et 1,12 milliards d\u0027appels, on ne peut pas se permettre d\u0027avoir une complexité algorithmique trop forte, ce qui est souvent le défaut des techniques de blockmodeling et de coclustering. -Généricité : les données traitées peuvent être aussi bien numériques que catégorielles.\nCe point est important car l\u0027analyse que nous menons porte à la fois sur des variables catégorielles (les Antennes) que numériques (le temps). -Absence de paramétrage : les données sont complexes et leur structure inconnue, un paramétrage de la structure de coclustering (e.g le nombre de clusters de chaque variable ou la distribution des antennes dans les groupes) serait trop complexe avec un tel jeu de données. -Fiabilité : la méthode utilisée ne doit pas produire de résultats lorsqu\u0027il n\u0027existe aucune structure sous-jacente. Elle doit donc être résistante au bruit et ne pas surapprendre. -Finesse et interprétabilité : l\u0027approche doit capturer toute l\u0027information présente dans les données afin d\u0027extraire des motifs fins. Des outils destinés à l\u0027interprétation des résul-tats doivent également être proposés afin d\u0027exploiter de manière efficace les résultats.\nÉtant donné le volume de données, la plupart des méthodes de coclustering ont une complexité algorithmique telle qu\u0027on ne pourrait pas les appliquer directement sur la base de données. Une idée serait donc de travailler sur un échantillon de données. Cependant, avec 17895 antennes et 1,12 milliards d\u0027appels, le nombre d\u0027appels moyen entre 2 antennes est d\u0027environ 3, 5. Ainsi, un échantillonnage entrainerait une perte d\u0027information importante. Parmi les approches de coclustering, nous retiendrons l\u0027approche MODL  1 .\nL\u0027approche MODL\nAfin de bien formaliser le problème, le tableau 1 liste les caractéristiques des données ainsi que les paramètres de modélisation du coclustering que nous cherchons à déterminer.\nD : Données\nM S : modèle de coclustering spatial  L\u0027analyse que nous menons est divisée en deux phases. Une première s\u0027intéresse aux corrélations entre antennes source et cible alors que la seconde se focalise sur la dimensions temporelle des appels. C\u0027est pourquoi nous introduisons deux modèles distincts : l\u0027un sera dit spatial M S et l\u0027autre temporel M T . Dans les deux cas, l\u0027approche MODL cherche à déduire les paramètres du modèle M S (resp. M T ) à partir des données D.\nDans un premier temps, les deux variables étudiées sont catégorielles, il s\u0027agit des antennes source et cible. Le but de l\u0027étude est de grouper les antennes source dont les appels sont distribués de manière similaire sur les antennes cible et vice-versa. Dans un second temps, une variable est catégorielle, les antennes source, et l\u0027autre numérique, le temps. On connait le nombre d\u0027appels sortant quotidiennement des antennes. On va donc chercher à grouper les antennes et discrétiser en même temps la variable temporelle de manière à ce que le trafic sortant des groupes d\u0027antennes soit stationnaire à l\u0027intérieur de chaque intervalle de temps. -le prior : noté P (M S ) (resp. P (M T )), il pénalise le modèle en spécifiant la distribution a priori des paramètres de ce dernier. Il est construit hiérarchiquement et uniformément à chaque étape afin d\u0027être non-informatif (Jaynes, 2003). -la vraisemblance : Une fois les paramètres du modèle spécifiés, la vraisemblance P (D|M S ) (resp. P (D|M T )) est définie comme la probabilité d\u0027observer les données initiales connaissant les paramètres du modèle étudié.\nLe produit du prior et de la vraisemblance resulte en la probabilité a posteriori du modèle. Le logarithme négatif de cette dernière probabilité est utilisé pour construire le critère.\nDéfinition (Coût du Modèle spatial). Le modèle spatial M S , représentation synthétique des données D est optimal s\u0027il minimise le critère suivant :\n(1)\nDéfinition (Coût du Modèle temporel). Le modèle temporel M T , représentation synthétique des données D est optimal s\u0027il minimise le critère suivant :\nest une somme des nombres de Stirling de second ordre, c\u0027est-à-dire le nombre de manières de partitionner |V S | éléments en k sous ensembles non-vides.\nLes deux premières lignes de l\u0027équation 1 et la première de l\u0027équation 2 représentent le prior alors que la dernière représente la vraisemblance dans les deux cas. D\u0027un point de vue théorie de l\u0027information, un logarithme négatif de probabilité correspond à une longueur de codage. Ainsi, le logarithme négatif du prior est la longueur de codage du modèle alors que le logarithme négatif de la vraisemblance est la longueur de description des données pour une paramétrisation du modèle donnée. Minimiser la somme de ces deux termes a donc une interprétation naturelle en terme de MDL (Minimum Description Length) (Grünwald, 2007). D\u0027un point de vue algorithmique, l\u0027optimisation est réalisée à l\u0027aide d\u0027une heuristique gloutonne ascendante démarrant du clustering le plus fin (une antenne par cluster) et réalisant à chaque étape la fusion de clusters qui décroit le plus le critère. Une post-optimisation améliore cette heuristique en effectuant des permutations au sein des clusters. Cet algorithme, de complexité en O(m ? m log m), est détaillé dans .\nAnalyse des corrélations spatiales\nLa première étape est une analyse des appels entre antennes source et antennes cible. On obtient un total 2141 clusters d\u0027antennes source et 2107 clusters d\u0027antennes cible. Le nombre d\u0027antennes par cluster est compris entre huit et neuf, ce qui est très fin. La difficulté est donc de savoir interpréter les résultats. À l\u0027échelle de la France, le nombre de clusters ne permet pas d\u0027avoir une vision synthétique du regroupement d\u0027antennes. Cependant, à huit antennes par cluster, des résultats de cette finesse peuvent être un atout pour une analyse locale.\nAnalyse à l\u0027échelle nationale\nDans un premier temps, nous nous intéressons à une analyse à l\u0027échelle du pays. Le niveau de finesse du modèle obtenu ne permet pas d\u0027avoir une vue synthétique de la structure de coclustering à l\u0027échelle du pays. C\u0027est pourquoi nous proposons de construire une classification hiérarchique ascendante des clusters. Nous fusionnons pour cela deux à deux les clusters de manière à détériorer le moins possible le critère optimisé afin d\u0027obtenir le modèle le plus probable suite à une fusion (de clusters source ou cible). Ce processus nous permet de simplifier notre modèle de manière maitrisée. Afin de quantifier la perte en terme d\u0027informativité du modèle, nous introduisons une mesure que nous appelons taux d\u0027informativité. Cette définition nous permet de construire une courbe du taux d\u0027informativité en fonction du nombre de clusters du modèle simplifié. Cette courbe fait figure de courbe de Pareto du meilleur modèle pour un nombre de clusters donnés. On observe que l\u0027impact des premières fusions sur le modèle est relativement faible. Ainsi, il est possible de réduire le nombre de clusters de plus de 2000 à 85 sur les deux dimensions (antennes source et antenne cible) tout en conservant environ 75% d\u0027informativité du modèle. On utilisera ce niveau de grain pour notre étude à l\u0027échelle nationale, le modèle restant ainsi informatif et suffisemment simple pour être étudié dans sa globalité. Les résultats sont présentés sur la Figure 2. La corrélation entre les clusters d\u0027antennes et leur position géographique est très forte bien que la position des antennes ne soit pas une contrainte dans l\u0027algorithme de coclustering. On en déduit alors que les habitants d\u0027une même zone géographique appellent vers les mêmes endroits. On voit sur cette carte que la France peut être séparée en zones géographiques bien délimitées mais pas nécessairement corrélées avec les frontières des régions administratives ou des départements.\nUne analyse locale\nOn se propose maintenant d\u0027exploiter la finesse des résultats obtenus. Pour cela, nous utilisons le modèle optimal le plus fin (M * S ) et nous faisons un zoom sur une métropole française. L\u0027agglomération de Toulouse est divisée en sept principaux clusters, affichés en Figure 3a. Un premier cluster regroupe les antennes du centre-ville (ronds jaune pâle), un autre cluster (ronds vert clair) correspond au quartier de la rive gauche de la ville qui est plus résidentielle que la rive opposée. Le cluster modélisé par des ronds rose pâle correspond au quartier étudiant de Rangueil ainsi qu\u0027à des zones urbaines sensibles, comme Empalot. Les ronds vert pâle couvrent le quartier du Mirail qui possède les mêmes caractéristiques que le précédent cluster. Les ronds orange sont localisés dans des zones périphériques résidentielles aux caractéristiques socio-économiques relativement diverses, quartiers plus aisés vers le Sud Est et plus populaires vers le Nord-Est. Enfin les carrés rouges couvrent la commune de Blagnac qui regroupe plusieurs zones d\u0027activités de l\u0027agglomération Toulousaine.\nAfin de mieux comprendre les raisons de ce regroupement d\u0027antennes source, on s\u0027intéresse à la distribution des appels sortants de ces groupes. L\u0027information mutuelle quantifie la dépendance entre deux variables, ici les antennes source et cible. Cette mesure, notée MI, est définie de la manière suivante (Cover et Thomas, 2006) :  \nAnalyse des corrélations spatio-temporelles\nDans cette seconde étude, nous proposons d\u0027effectuer un coclustering des antennes source et de la variable temporelle. Les données sont donc constituées de 17895 antennes émettrices et de 1,12 milliards d\u0027appels enregistrés sur 5 mois avec une précision à la journée. Le groupement d\u0027antennes source est différent du groupement obtenu dans la Section 3. Ici les antennes groupées entre elles sont similaires car leurs hausses et baisses de trafics sortants se font sur les mêmes périodes temporelles. Nous obtenons 6129 clusters d\u0027antennes source et 117 intervalles de temps. Contrairement à l\u0027analyse du trafic inter-antennes, les clusters sont dispersés sur l\u0027ensemble du territoire Français, ce qui rend une projection sur une carte ininterprétable et ce, pour n\u0027importe quel niveau de grain du modèle. Pour mieux comprendre les phénomènes qui ont mené à une telle structure du coclustering, nous allons étudier l\u0027information mutuelle entre les clusters d\u0027antennes sources et les périodes de temps trouvées. Pour visualiser cette information, nous avons simplifié de modèle de la même manière que dans l\u0027étude précédente et nous avons tracé un calendrier des excès et déficits du trafic sur la Figure 4.\nOn observe du 13 Mai au 5 Juillet, ainsi que du 1er Septembre au 13 Octobre, une discréti-sation régulière et périodique qui correspond au découpage semaine/weekend. On voit qu\u0027en semaine le trafic est en excès pour le cluster du milieu et en déficit pour le cluster du bas, le contraste est d\u0027autant plus fort entre ces clusters qu\u0027on étudie une période éloignée des vacances d\u0027été. Pour les weekend, la tendance s\u0027inverse mais dans une moindre mesure. Le trafic est donc mieux équilibré entre les différents clusters d\u0027antennes. On peut expliquer ces phénomènes par l\u0027activité concentrée sur des zones géographiquement restreintes et généralement urbaines pendant la semaine. Le cluster du haut reste, quant à lui, toujours en déficit en dehors de l\u0027été, bien que ce déficit ait tendance à être plus léger pendant les weekends.\nPendant la période estivale, l\u0027alternance semaine/weekend disparait. On observe alors un déficit d\u0027appels sur toute la période dans le cluster du milieu, alors que le cluster du haut voit un excès significatif d\u0027appels par rapport au trafic habituel et par rapport au trafic de la période. Quant à la dernière zone, on y observe un léger excès d\u0027appels sortant. C\u0027est à cette période de l\u0027année que les contrastes sont les plus forts, c\u0027est pourquoi on va s\u0027y intéresser et tracer une projection géographique des clusters d\u0027antennes sources sur une carte de France ( Figure 5). Pendant cette période des vacances scolaires (4 Juillet -4 Septembre), on observe un excès d\u0027appels au niveau des côtes Atlantiques et du Languedoc-Roussillon principalement. Cela signifie qu\u0027à cette période, un nombre d\u0027appels bien plus important que le trafic habituel a été émis depuis ces zones. On peut donc qualifier ces zones de saisonniaires car elles sont caractérisées par une répartition des appels très déséquilibrée au cours de l\u0027année, ce qui explique cet excès d\u0027appels à cette période de l\u0027année. C\u0027est pour cette même raison que la Côte d\u0027Azur ne connait pas un excès d\u0027appels aussi fort que la Vendée par exemple. Le trafic y est mieux réparti sur l\u0027année, bien qu\u0027elle soit la première destination estivale des français. Les grandes agglomérations, et notamment Paris, sont colorées en bleu. Ceci peu s\u0027expliquer par la baisse d\u0027activité dans les villes à cette période. On peut donc supposer un déplacement des populations en été, depuis les centres urbains vers les côtes et les zones touristiques. Remarquons tout de même que la couleur tracée sur cette carte n\u0027indique pas la fréquence des appels mais bien l\u0027information mutuelle, les antennes parisiennes demeurant les antennes les plus émettrice, même à cette période de l\u0027année.\nConclusion\nNous avons proposé une étude d\u0027un compte rendu d\u0027appels enregistré pendant cinq mois entre les 17895 antennes téléphoniques françaises, ce qui représente un total de 1,12 milliards d\u0027appels. Après avoir présenté des études similaires, ainsi que des méthodes adaptées à ce type d\u0027études, nous avons justifié le choix et détaillé la méthode utilisée : l\u0027approche MODL. Nous avons pu mener deux études de nature différente en utilisant une unique méthode, possédant les propriétés de généricité et de passage à l\u0027échelle suffisantes pour réaliser une analyse complète de nos données. On observe dans cette étude que les antennes groupées dans un même cluster, de part les distributions similaire des appels sortant (resp. rentrant), sont géographiquement très proches et dessinent des frontières précises, aussi bien au niveau national que local. D\u0027autre part, lors d\u0027une étude temporelle, nous avons pu dresser un calendrier de la période temporelle étudiée et déterminer des zones où les appels sortants sont distribués identiquement dans chaque intervalle de temps. Les zones obtenues ont perdu la proximité géographique observée dans la première partie de l\u0027étude mais sont caractéristiques des zones qu\u0027ils décrivent : urbaines, rurales ou touristiques. Les périodes quant à elles, montrent une différence de comportement entre les vacances d\u0027été et les périodes scolaires où on observe une périodicité semaine/weekend où les excès et déficits de trafic s\u0027inversent suivant la nature de la zone. Ainsi, par exemple, les excès de trafic se concentrent dans les zones touristiques en Août et les déficits dans les zones urbaines. Dans des prochains travaux, il serait intéressant de mener une étude où plusieurs dimensions temporelles sont embarquées simultanément dans l\u0027algorithme de coclustering, afin de voir comment se caractérisent les comportements dans les zones géographiques en fonction du jour de la semaine et de l\u0027heure de la journée.\n"
  },
  {
    "id": "401",
    "text": "Introduction\nL\u0027objectif de ces travaux est d\u0027obtenir un classifieur évolutif pour faciliter les interactions homme machine sur interface tactile (tablette, smartphone, tableau interactif, etc.). Autrement dit, nous souhaitons que le classifieur soit capable de s\u0027adapter à la manière d\u0027écrire et de dessiner de l\u0027utilisateur. Cette manière de dessiner va probablement évoluer au fur et à mesure que l\u0027utilisateur va s\u0027habituer à utiliser l\u0027interface tactile. L\u0027utilisateur commence par dessiner lentement et attentivement ses gestes lorsqu\u0027il est novice, alors qu\u0027il les fait d\u0027une manière plus fluide et plus rapide quand il devient expert. Il faut alors que le classifieur s\u0027adapte et suive cette évolution. Il est également souhaitable que le classifieur supporte l\u0027ajout de nouvelles classes « à la volée » pour permettre à l\u0027utilisateur de personnaliser et d\u0027adapter l\u0027application à ses besoins. Pour cela, il est nécessaire de disposer d\u0027un système performant et réactif aux changements de son environnement.\nPlusieurs systèmes de classification évolutifs sont apparus ces dernières années pour ré-pondre au besoin de classifieurs fonctionnant en environnement non stationnaire. Ils utilisent des algorithmes d\u0027apprentissage incrémental pour améliorer leurs performances pendant leur utilisation et s\u0027adapter à tout changement. Parmi ces classifieurs évolutifs, les systèmes d\u0027infé-rence floue (SIF) se distinguent par leurs excellentes performances (Angelov et Zhou, 2008), leur capacité à mettre en oeuvre un apprentissage incrémental et leur bon comportement face à l\u0027ajout de classe. Dans la plupart de ces approches, l\u0027apprentissage incrémental des conclusions repose sur l\u0027algorithme des moindres carrés récursifs. Or cet apprentissage incrémental lié aux moindres carrés récursifs à l\u0027inconvénient de perdre ses capacités d\u0027adaptation avec le temps. Cet article étudie les différentes possibilités existantes, dans la littérature dédiée au contrôle, pour introduire de l\u0027oubli dans l\u0027algorithme des moindres carrés récursifs, dans le contexte de la classification à base de SIF. L\u0027objectif est de préserver dans le temps les capacités d\u0027adaptation des SIF. Ces différentes techniques sont transposées dans le système d\u0027inférence floue Evolve (Almaksour et Anquetil, 2011), puis sont comparées pour la reconnaissance de gestes manuscrits.\nLa section 2 présente l\u0027architecture et l\u0027apprentissage incrémental des systèmes d\u0027inférence floue. Plusieurs approches pour intégrer de l\u0027oubli dans l\u0027algorithme des moindres carrés ré-cursifs sont ensuite présentées section 3. Ces différentes techniques sont évaluées et comparées sur la problématique de reconnaissance de gestes manuscrits en section 4. Enfin, la section 5 conclue et discute des perspectives de ces travaux.\nSystèmes d\u0027inférence floue d\u0027ordre un\nCette section présente l\u0027architecture de notre système d\u0027inférence floue (SIF) Evolve. Nous détaillons ensuite l\u0027algorithme des moindres carrés récursifs utilisé pour son apprentissage. Enfin, nous expliquons les limites de cet algorithme ainsi que la nécessité d\u0027intégrer de l\u0027oubli.\nArchitecture du système d\u0027inférence floue\nNous travaillons ici avec des systèmes d\u0027inférence floue (SIF) d\u0027ordre un -dit de TakagiSugeno -qui offrent de bonnes performances face à des problèmes de classifications complexes. De plus, ils supportent bien l\u0027ajout de nouvelles classes « à la volée » et peuvent facilement être entrainés de manière incrémentale en temps réel. Des classifieurs évolutifs basés sur des SIF ont été proposés par Angelov et Zhou (2008) et par Almaksour et Anquetil (2011).\nUn système d\u0027inférence floue d\u0027ordre un se compose d\u0027un ensemble de règles floues de la forme suivante :\noù x ? R n le vecteur des caractéristiques etˆyetˆ etˆy (i) ? R c le vecteur d\u0027appartenance au c diffé-rentes classes.\nLes prémisses des règles sont définies par l\u0027appartenance floue à des prototypes C (i) -des regroupements (clusters) dans l\u0027espace d\u0027entrée -qui sont caractérisés par un centre µ (i) et une matrice de covariance ? (i) . Le degré d\u0027appartenance floue ? (i) (x) est calculé par une fonction à base radiale hyper-ellipsoïdale, en fonction de la distance de Mahalanobis d\nLes conclusions des règles donnent l\u0027appartenance floue aux différentes classes, et pour les SIF d\u0027ordre un, ces degrés d\u0027appartenance sont obtenus par des fonctions linéaires des entrées :\noù l\nk (x) représente la fonction conséquence linéaire de la règle i pour la classe k :\nL\u0027inférence floue de type somme-produit est ensuite utilisée pour calculer la sortie du système pour chaque classe :\noù r représente le nombre de règles floues. La classe de x est choisie comme étant l\u0027étiquette correspondant à la composante maximale de la sortie du systèmê\nApprentissage incrémental\nDans un problème d\u0027apprentissage incrémental en-ligne, les données d\u0027apprentissage ne sont disponibles qu\u0027au fur et à mesure. Le système doit donc être appris en utilisant les premières données arrivées, puis continuer à évoluer, de manière transparente du point de vue de l\u0027utilisateur, lorsque les données suivantes arrivent.\nAinsi, les prototypes sont adaptés incrémentalement à l\u0027arrivée de chaque nouvelle donnée. Ce processus d\u0027adaptation permet de mettre à jour les centres des prototypes en fonction de chaque nouvelle donnée d\u0027apprentissage disponible, et de recalculer de manière récursive les matrices de covariances des prototypes. Un algorithme de clustering incrémental est utilisé pour créer de nouveaux prototypes, et donc de nouvelles règles, lorsque que cela est opportun.\nLe problème de l\u0027apprentissage incrémental des conséquences dans un SIF d\u0027ordre un peut être résolu par la méthode des Moindres Carrés Récursifs (MCR) pondérés par les activations des règles (Angelov et Zhou, 2008). Soit ? (i) la matrice de tous les paramètres des consé-quences linéaires de la règle i :\noù c représente le nombre de classes, et n la taille du vecteur des caractéristiques. Ces matrices peuvent être récursivement apprises :\nOù les matrices de covariances\n?1 sont également mises à jour récur-sivement :\nNécessité et principe de l\u0027oubli\nPlus le nombre de données d\u0027apprentissage augmente, plus les modifications apportées aux conclusions par l\u0027algorithme des MCR seront faibles. En effet, le poids accordé à chaque donnée étant équivalent, plus le nombre de données augmente, plus le poids de chaque donnée individuelle est faible. Cette propriété est illustrée par le fait que la matrice de covariance décroit avec le temps : P t \u003c P t?1 . Il est dit que le gain de l\u0027algorithme tend vers zéro.\nCette propriété a pour conséquence de réduire la réactivité du système -la capacité d\u0027apprentissage de nouveautés -avec le temps. Ainsi, au bout d\u0027un certain temps, l\u0027algorithme ne sera plus capable de s\u0027adapter aux changements dans les données, ni d\u0027apprendre de nouvelles classes (dans un temps raisonnable). Il est donc nécessaire de limiter le poids des anciennes données, autrement dit d\u0027introduire de l\u0027oubli dans l\u0027algorithme des moindres carrés récursifs, pour maintenir les capacités d\u0027apprentissage du système.\nL\u0027introduction d\u0027oubli dans l\u0027algorithme des moindres carrés récursifs est un sujet qui a été beaucoup étudié dans la littérature dédiée au contrôle de systèmes physiques complexes (Lughofer et Angelov, 2011). Pour cela, il faut travailler sur la matrice de covariance qui repré-sente la distribution des données dans l\u0027algorithme des moindres carrés récursifs. Le principe de l\u0027oubli est d\u0027introduire une mise à jour temporelle de cette matrice : Cette formulation de l\u0027oubli -dite bayésienne -permet d\u0027unifier les différentes techniques d\u0027oubli pour l\u0027algorithme des moindres carrés récursifs (Kulhavy et Zarrop, 1993). Chaque technique est alors caractérisée par sa fonction d\u0027oubli F(·).\nLa propriété la plus importante d\u0027un algorithme d\u0027estimation récursif avec oubli est que sa matrice de covariance reste bornée (Salgado et al., 1988), (Parkum et al., 1992). En effet, sans borne inférieure à la matrice de covariance, le gain de l\u0027algorithme va tendre vers zéro et le système va perdre ses capacités d\u0027adaptation. Sans borne supérieure, la matrice de covariance peut exploser sous l\u0027effet de d\u0027oubli, et engendrer des instabilités qui mènent à l\u0027écroulement du système.\nDifférentes formes d\u0027oubli\nCette section présente les principales techniques pour introduire de l\u0027oubli dans l\u0027algorithme des moindres carrés récursifs. La plus simple est l\u0027utilisation d\u0027un facteur d\u0027oubli exponentiel, mais elle est sujette au problème d\u0027explosion de la matrice de covariance (covariance « wind-up » problem). Plusieurs autres techniques ont été proposées pour tenter de pallier ce problème comme utiliser un facteur d\u0027oubli variable ou de l\u0027oubli directionnel.\nFacteur d\u0027oubli constant\nLe principe du facteur d\u0027oubli exponentiel est d\u0027introduire une pondération exponentiellement décroissante des anciennes données dans le calcul de la matrice de covariance : \nCependant, le choix de ce facteur d\u0027oubli est complexe. Si le facteur d\u0027oubli est trop faible, la matrice de covariance va tendre vers zéro et l\u0027algorithme va perdre ses capacités d\u0027adaptation. Si le facteur d\u0027oubli est trop fort, la matrice de covariance va exploser et l\u0027algorithme va devenir instable jusqu\u0027à provoquer l\u0027effondrement du système. Ce phénomène -appelé covariance « wind-up » -est dû au fait que l\u0027on oubli davantage d\u0027information que l\u0027on en reçoit.\nPour éviter ce problème, il est nécessaire d\u0027ajuster le facteur d\u0027oubli à la quantité d\u0027information nouvellement disponible. C\u0027est le principe du facteur d\u0027oubli variable dans le temps.  et al. (1981) proposent ainsi d\u0027utiliser l\u0027erreur quadratique a posteriori (pondé-rée) comme métrique : ?\nFacteur d\u0027oubli variable\nUne autre mesure possible, reflétant la quantité d\u0027information acquise par l\u0027algorithme, est la trace de la matrice de covariance :\nUn tel facteur d\u0027oubli variable permet à l\u0027algorithme de conserver un gain non nul au cours du temps et le système peut ainsi s\u0027adapter à une évolution des données. Cette approche permet aussi d\u0027éviter l\u0027explosion de la matrice de covariance, mais seulement si l\u0027information est bien répartie selon les dimensions de l\u0027espace d\u0027entrée. Dans le cas contraire, certains éléments de la matrice de covariance peuvent tendre vers zéro pendant que d\u0027autres explosent. Pour éviter cela, il faut non seulement faire varier le facteur d\u0027oubli dans le temps mais également dans l\u0027espace, c\u0027est ce qu\u0027on appelle l\u0027oubli directionnel.\nOubli directionnel\nL\u0027oubli directionnel -Directional Forgetting (DF) -a été proposé par Hägglund (1985) et Kulhavy (1987) puis amélioré par Bittanti et al. (1990) et Cao et Schwartz (2000. C\u0027est une technique qui fait varier l\u0027oubli dans le temps mais aussi dans l\u0027espace en introduisant de l\u0027oubli seulement dans les directions où de l\u0027information arrive.\nLa fonction d\u0027oubli utilisée est donc paramétrée par la donnée courante :\nL\u0027avantage de cette technique est qu\u0027elle maintient la matrice de covariance bornée, et garantit ainsi les performances du système. Dans les directions où beaucoup d\u0027information arrive, on oublie beaucoup ; mais on oublie peu dans les directions qui sont peu excitées.\nRésultats expérimentaux\nDans cette section, nous étudions l\u0027impact de ces différentes approches d\u0027oubli dans le cadre de l\u0027apprentissage incrémental de systèmes d\u0027inférence floue pour la classification. \nProtocole d\u0027évaluation\nFIG. 1 -Exemples de gestes -ILGDB groupe 1 (gestes libres).\nPour que l\u0027évaluation soit représentative de l\u0027utilisation d\u0027un système évolutif, nous utilisons le mode d\u0027évaluation prédictif séquentiel (Gama et al., 2009). Comme un classifieur évolutifs essaye d\u0027abord de reconnaître un geste, puis s\u0027en sert ensuite pour apprendre une fois qu\u0027il dispose de sa vrai étiquette, nous évaluons nos systèmes d\u0027une manière similaire. Ainsi, chaque donnée est d\u0027abord utilisée comme donnée de test, puis ensuite comme donnée d\u0027apprentissage. Les taux d\u0027erreur sont ensuite calculés sur une fenêtre entre chaque point de test.\nPerformances en environnement non stationnaire\nNous évaluons ces différents systèmes sur un scénario non stationnaire simulant des changements de concept brusques. Pour simuler ces changements de concept, nous utilisons les 21 scripteurs du groupe 1 (gestes libres) à la suite. D\u0027un scripteur à l\u0027autre, les gestes d\u0027une même classe sont différents ce qui oblige le classifieur à changer complètement son modèle.\nLes taux d\u0027erreur sont calculés sur les 56 derniers gestes (sur 173) de chaque scripteur pour évaluer les performances une fois le changement de concept appris. Les résultats (moyennés sur 20 ordres différents de présentation des scripteurs) sont présentés Figure 2.\nTout d\u0027abord, les performances du système Evolve sans oubli sont sans équivoque. Sans oubli le système n\u0027est pas capable à long terme de s\u0027adapter aux changements de concepts. L\u0027utilisation d\u0027un facteur d\u0027oubli exponentiel est indispensable, mais son choix est difficile. Si l\u0027utilisation d\u0027un facteur d\u0027oubli variable dans le temps peut donner de bons résultats sur des problèmes de régression, ce n\u0027est pas du tout le cas dans notre contexte. L\u0027utilisation de la trace de la matrice de covariance comme mesure d\u0027information (Evolve VF (trace)) n\u0027empêche pas certains éléments de la matrice de tendre vers zéro et le système perd très vite ses capacités d\u0027adaptation. Au contraire, l\u0027utilisation de l\u0027erreur a posteriori (Evolve VF (error)) n\u0027empêche pas l\u0027explosion de la matrice de covariance lorsque le système est inégalement excité suivant les directions de l\u0027espace.\nEnfin, l\u0027oubli directionnel est la seule approche stable, ce qui est logique car elle garantit que la matrice de covariance reste bornée. On observe cependant une légère augmentation du taux d\u0027erreur tout au long de l\u0027expérience. Cela vient du fait que certaines valeurs de la matrice de covariance se stabilisent à des valeurs relativement faibles, ce qui réduit légèrement les capacités d\u0027adaptation du système.\nConclusion\nDans cet article nous avons étudié comment introduire de l\u0027oubli dans l\u0027apprentissage incrémental de classifieurs évolutifs basés sur des systèmes d\u0027inférence floue (SIF). Plusieurs approches existent, principalement tirées du domaine de la régression, pour introduire de l\u0027oubli dans l\u0027algorithme des moindres carré récursifs utilisé pour l\u0027apprentissage des SIF. \nSummary\nThis paper study the use of forgetting for the incremental learning of evolving classifiers based on fuzzy inference systems. Several techniques are presented and compared on handwritten gesture recognition task in changing environement.\n"
  },
  {
    "id": "402",
    "text": "Introduction\nDans cet article, nous nous intéressons à la problématique d\u0027évolution d\u0027une ontologie dé-diée à la représentation de relations n-aires pour l\u0027annotation de documents dans le cadre du Web Sémantique. Une relation n-aire est une relation qui est définie entre au moins deux arguments. Nous nous appuierons sur la représentation de relations n-aires sans arguments diffé-renciés telles que proposées par le W3C 1 , ce qui correspond au cas le plus général d\u0027utilisation des relations n-aires, le cas 3. Nous avons de plus choisi d\u0027utiliser le \"patron 1\" qui consiste à représenter une relation n-aire à l\u0027aide d\u0027un concept, relié à ses arguments par des propriétés, comme dans l\u0027exemple de la figure 1. Nous nous intéressons plus particulièrement aux relations n-aires entre des données quantitatives expérimentales, ce qui suppose d\u0027apporter une attention particulière à la gestion des arguments numériques (i.e. les quantités) et leurs unités de mesure. Supposons que l\u0027on veuille ajouter la relation n-aire O2Permeability_Relation de la figure 1 à notre ontologie, MapOpt Ontology, dédiée à la représentation de relations n-aires entre des données expérimentales quantitatives dans le domaine du risque alimentaire microbiologique étendu aux emballages (Touhami et al., 2011). L\u0027existence d\u0027interdépendances entre la relation et ses six arguments nécessite d\u0027effectuer un nombre important de changements dans l\u0027ontologie pour y parvenir (e.g. ajout de concepts, de propriétés, de restrictions). En outre, ces changements peuvent engendrer des incohérences dans l\u0027ontologie. Nous présentons ici la représentation formelle des changements applicables à notre ontologie permettant de modifier sa structure tout en maintenant sa cohérence.\nLe processus global de gestion de l\u0027évolution de notre ontologie ainsi que de ses artefacts dépendants est celui de Stojanovic (2004), et nous présentons dans cet article l\u0027étape de la représentation du changement de l\u0027ontologie et la sémantique du changement. Dans notre approche une relation n-aire constitue un ensemble d\u0027interdépendances (entre la relation et ses arguments, entre une quantité et les unités de mesures associées) à représenter et à traiter conjointement dans le processus d\u0027évolution de l\u0027ontologie.\nDans la section 2, nous présentons la modélisation de l\u0027ontologie dédiée à la représentation de relations n-aires et la notion d\u0027ontologie structurellement cohérente. Les changements élé-mentaires et composés permettant de faire évoluer l\u0027ontologie sont détaillés dans la section 3. Enfin, nous concluons et présentons les perspectives de ce travail dans la section 4. 2 L\u0027ontologie dédiée à la représentation de relations n-aires D\u0027après Reymonet et al. (2006), la modélisation d\u0027une ontologie est fortement influencée par la tâche, le domaine d\u0027intérêt et l\u0027application pour laquelle elle a été conçue. Pour nous, la tâche à réaliser est l\u0027annotation de relations n-aires dans des documents pour l\u0027interrogation ; le domaine d\u0027intérêt est l\u0027étude de données expérimentales quantitatives des sciences du vivant ; l\u0027application, enfin, est la construction d\u0027un entrepôt de données ouvert sur le Web (Buche et al., 2012).\nDéfinition 1 Pour un ensemble de concepts C, sa hiérarchie de spécialisation, notée H C , est une relation acyclique, H C ? C × C, telle que : si (c 1 , c 2 ) ? H C alors c 1 est un sousconcept (un fils) de c 2 , c 2 est un superconcept (ou père) de c 1 , H * C est la fermeture réflexive, antisymétrique et transitive de\nDéfinition 2 Soit C un ensemble de concepts et quatre concepts de C, Simple_Concept, Quantity, Symbolic_Concept et Relation tel que Symbolic_Concept Simple_Concept et Quantity Simple_Concept. Nous définissons les ensembles des concepts suivants :\nDéfinition 3 Une ontologie dédiée à la représentation de relations n-aires est définie par le tuple suivant : M O :\u003d (C, H C , P, H P , I, F) avec :\n-C : l\u0027ensemble des concepts de l\u0027ontologie (cf. définition 2) et H C sa hiérarchie (cf. définition 1) ; -P : l\u0027ensemble des propriétés telles que définie par le W3C 2 , où une propriété est définie comme une relation binaire entre des instances de concepts, des littéraux et des types de données primitifs, et H P sa hiérarchie telle que donnée dans la définition 1 où la notion de \"concept\" est remplacée par celle de \"propriété\" ; -I : l\u0027ensemble des instances de concepts ; -F : un ensemble de fonctions retournant un sous-ensemble d\u0027instances ou de concepts de l\u0027ontologie.\nNous présentons ci-dessous plusieurs fonctions de F, utilisées dans la suite du papier : -la fonction signature de C rel dans 2 Carg qui permet d\u0027associer à une relation r ? C rel le domaine de valeurs de chacun de ses arguments (c 1 , . . . , c n ) où c i ? C arg ; -la fonction dimension de C quantity dans I Dimension 3 qui permet d\u0027associer à une quantité q ? C quantity sa dimension d ? I Dimension ; -la fonction linkconcepts de C rel ×P × R, où R \u003d {allValuesFrom, someValuesFrom, hasValue}, dans C arg qui permet d\u0027associer à un concept c 1 ? C rel un autre concept c 2 ? C arg via la propriété p ? P à l\u0027aide d\u0027une restriction de propriétés 4 . Pour la représentation du changement nous introduisons ci-dessous la notion d\u0027ontologie CC-cohérente, en nous inspirant de Stojanovic (2004) pour adapter ses contraintes de cohé-rences à la structure particulière de notre ontologie. Définition 4 Une ontologie, définie par le tuple (C, H C , P, H P , I, F) cf. à la définition 3, est dite CC-cohérente si elle vérifie un ensemble de contraintes de cohérences structurelles, noté CC.\nNous avons identifié 39 contraintes de cohérences structurelles dont 16 portent sur les concepts, 7 sur les propriétés et 12 sur les instances. Nous présentons ci-dessous quelques exemples de ces contraintes :\n-CC1 : une relation a au moins deux arguments : ?r ? C rel , |signature(r)| ? 2 -CC2 : Tous les concepts appartenant à la signature d\u0027une relation doivent être définis : ?r ? C rel , ?c ? signature(r) \u003d? c ? C arg -CC3 : Chaque quantité doit être associée à une seule valeur de dimension :\n?q ? C quantity \u003d? |dimension(q)| \u003d 1 3 Évolution de l\u0027ontologie dédiée à la représentation de relations n-aires L\u0027évolution de la structure de l\u0027ontologie telle que donnée dans la définition 4, se traduit par divers changements tels que l\u0027ajout d\u0027une relation n-aire, l\u0027ajout ou la suppression d\u0027un de ses arguments, etc. Pour des raisons de simplicité et de lisibilité, nous présentons l\u0027évolution de la structure de notre ontologie à travers un cas d\u0027utilisation, détaillé dans la sous-section 3.1. Puis, nous présentons dans les sous-sections 3.2 et 3.3 les changements élémentaires et composés qui permettent une telle évolution.\nCas d\u0027utilisation : ajouter une relation n-aire à MapOpt Ontology\nSupposons que la relation n-aire O2Permeability_Relation présentée dans la figure 1 ne soit pas définie dans MapOpt Ontology. L\u0027ontologue doit commencer par ajouter ses six arguments : le nouveau concept Packaging comme sous-concept du concept Symbolic_Concept et les nouveaux concepts Partial_Pressure, Relative_Humidity, Thickness et O2Permeability comme sous-concepts du concept Quantity. L\u0027ontologue doit ensuite définir le concept O2Per-meability_Relation comme sous-concept du concept Relation, ainsi que les propriétés et les restrictions de propriétés permettant de relier la relation à ses arguments.\nL\u0027ontologue doit donc, pour ajouter une nouvelle relation n-aire dans l\u0027ontologie, effectuer un nombre important d\u0027opérations de modification. Il doit également veiller à ce que les modifications effectuées respectent les contraintes de cohérences présentées dans la section 2. Afin d\u0027alléger la tâche de l\u0027ontologue et d\u0027éviter des erreurs lors de manipulations fastidieuses de l\u0027ontologie, nous définisson un ensemble de changements élémentaires et composés permettant de faire évoluer l\u0027ontologie tout en préservant sa CC-cohérence.\nNotion de changement élémentaire\nNous appelons changement élémentaire, une opération de modification de l\u0027ontologie portant sur une seule entité : un concept, une propriété ou une instance. Pour faire évoluer l\u0027ontologie, nous avons identifié 60 changements élémentaires dont 8 s\u0027appliquent aux concepts (e.g. CreateHierarchyConceptLink), 41 aux propriétés (e.g. CreateConceptsLinked) et 5 aux instances de concepts (e.g. CreateInstance). Comme dans Stojanovic (2004), la formalisation des changements repose sur la définition d\u0027un ensemble de pré-conditions (un ensemble d\u0027assertions qui doivent être vraies pour pouvoir appliquer le changement) et de post-conditions (qui correspondent au(x) résultat(s) du changement) pour chaque type de changement, élémentaire ou composé. Les pré-conditions associées à un changement correspondent au sous-ensemble de contraintes de cohérences CC portant sur les parties de l\u0027ontologie affectées par le changement considéré. La vérification des pré-conditions d\u0027un changement permet d\u0027éviter d\u0027appliquer ce changement sur une ontologie non CC-cohérente. Les post-conditions garantissent que le changement a bien été appliqué, mais pas la CC-cohérence de l\u0027ontologie.\nNous présentons ci-dessous l\u0027exemple du changement élémentaire nécessaire pour faire évoluer MapOpt Ontology dans le cas d\u0027utilisation décrit dans la section 3.1.\nDéfinition 5 CreateLinkConcepts (c 1 , p, c 2 , allValuesFrom) : :\u003d Sémantique : associer un concept c 2 au concept c 1 via une propriété p en utilisant la restriction allValuesFrom Pré-condition : c 1 ? C rel , c 2 ? C arg , p ? P, {c | c ? C arg ? linkconcepts(c 1 , p, allV alues-F rom) \u003d c} \u003d ? Post-condition : c 1 ? C rel , c 2 ? C arg , p ? P, linkconcepts(c 1 , p, allV aluesF rom) \u003d c 2 .\nCe changement est considéré comme élémentaire étant donné qu\u0027il ne permet de manipuler qu\u0027une seule entité : le concept c 1 . Comme pré-condition, il s\u0027agit de vérifier : i) que les concepts c 1 et c 2 et la propriété p sont bien définies et ii) qu\u0027aucun argument n\u0027est déjà associé directement au concept c 1 via la propriété p à l\u0027aide de la restriction allValuesFrom.\nChangements composés\nUn changement composé est une opération de modification de l\u0027ontologie qui affecte plusieurs entités et peut se décomposer en une succession de changements élémentaires. Pour faire évoluer l\u0027ontologie, nous avons identifié 12 changements composés (e.g. AddConcept, AddHierarchyOfSymbolicConcept). Nous présentons ci-dessous le changement composé né-cessaire pour faire évoluer MapOpt Ontology dans le cas d\u0027utilisation décrit dans la section 3.1.\nu 1 , . . . , u nu ? I U nit_Concept 5 , p 1 , . . . , p n ? P , o 1 , . . . , o n ? C arg , rs 1 , . . . , rs n ? R Sémantique : ajouter un concept newc c, qui peut être une relation ((c, Relation) ? H * C ) ou un concept simple ((c, Simple_Concept) ? H * C ) avec sa hiérarchie, et ses restrictions. Traitement du concept :\n-Créer le concept newc -Créer un lien hiérarchique de subsomption entre newc et c : CreateHierarchyConceptLink(newc, c) Traitement des restrictions :\n1. CreateLinkDimension (newc, hasDimension, d, hasV alue) 2. ?i ? [1, n u ], CreateLinkUnit (newc, hasU nitConcept, u i , allV aluesF rom) 3. CreateRestrictionMinMax (newc, hasN umericalV alue, min, max) 4. ?i ? [1, n], CreateLinkConcepts (newc, p i , o i , rs i )). Post-condition : newc ? C, (newc, c) ? H C .\nLe changement composé AddConcept permet de créer soit une nouvelle relation, soit un nouveau concept, sous-concept de Simple_Concept, (i.e. une quantité, un concept symbolique ou un concept simple), qui sera utilisé comme argument d\u0027une relation. Pour tous ces concepts, il y a un traitement en commun : créer le nouveau concept et le lier avec son concept père. Ensuite, il n\u0027y a aucun traitement supplémentaire pour les concepts simples et les concepts symboliques. Pour les quantités, il s\u0027agit de leurs associer leur dimension (traitement des restrictions 1), leurs unités de mesures (traitement des restrictions 2) et éventuellement un intervalle 5. Les unités de mesure sont définies comme des instances du concept Unit_Concept ? C\n"
  },
  {
    "id": "403",
    "text": "Introduction\nL\u0027extraction de motifs fréquents est une tâche importante dans le domaine de la fouille de données. Initialement centrée sur la découverte d\u0027ensemble d\u0027items (itemsets) fréquents (Agrawal et al., 1993), les premiers travaux ont été étendus pour extraire des motifs structurels comme les séquences (Agrawal et Srikant, 1995), les arbres (Chi et al., 2004a) ou les graphes (Washio et Motoda, 2003).\nAlors que l\u0027extraction d\u0027itemsets fréquents recherche les combinaisons fréquentes d\u0027items, l\u0027extraction de motifs structurels recherche des sous-structures fréquentes. La plupart des travaux existants se focalisent sur un seul type de problème (fouille d\u0027itemsets ou fouille structurelle). Toutefois, afin de représenter des données plus complexes, il semble naturel de considérer des collections structurées d\u0027itemsets. Dans cet article, nous introduisons le problème de fouille d\u0027arbres attribués. Les arbres attribués sont des arbres dans lesquels les noeuds sont associés à des itemsets.\nLes arbres attribués peuvent être utilisés dans de nombreuses applications de fouilles de données spatio-temporelles. Dans le cas d\u0027études épidémiologiques, par exemple, l\u0027espace géographique peut être découpé en zones qui sont représentées par les noeuds de l\u0027arbre, les itemsets décrivent les caractéristiques de ces zones à un temps donné et les arêtes symbolisent des relations de voisinage avec d\u0027autres zones au temps suivant. Les motifs fréquent trouvés sont ainsi susceptible de donner un éclairage nouveau sur les déterminants d\u0027une pathologie. D\u0027autres applications peuvent être imaginées dans divers domaines comme l\u0027analyse des arbres de retweet, la fouille d\u0027arbres phylogénétiques, la fouille de documents XML ou l\u0027analyse de journaux d\u0027activité Web (Web log analysis).\nLes contributions clés de notre travail sont les suivantes : 1) nous présentons le problème de la fouille de sous-structures ordonnées et non ordonnées dans une collection d\u0027arbres attribués, 2) nous définissons les formes canoniques des arbres attribués, 3) nous proposons une méthode permettant l\u0027énumération des arbres attribués qui est basée sur la combinaison de deux opéra-tions : l\u0027extension de la structure arborescente et l\u0027extension des itemsets associés aux noeuds, 4) nous présentons trois variantes d\u0027un algorithme permettant d\u0027extraire les motifs fréquents dans des arbres attribués, 5) nous montrons les résultats expérimentaux effectués sur plusieurs jeux de données artificiels et un jeu de données réel.\nCet article est organisé de la manière suivante : la section 2 présente les concepts de base et définit le problème, la section 3 propose un bref aperçu de l\u0027état de l\u0027art et met l\u0027accent sur les quelques études qui combinent la fouille d\u0027itemsets et la fouille structurelle, la section 4 décrit la méthode en insistant sur l\u0027exploration de l\u0027espace de recherche, le calcul des fréquences et l\u0027élagage des candidats, la section 5 montre les résultats de la fouille de plusieurs jeux de données artificiels et réels et finalement, la section 6 conclut l\u0027article et présente de possibles extensions de notre travail.\nConcepts généraux et définition du problème\nDans cette section, nous introduisons les concepts et définitions nécessaires et présentons le problème de fouille d\u0027arbres attribués.\nPréliminaires\nSoit I \u003d {i 1 , i 2 , .., i n } un ensemble d\u0027items. Un itemset est un ensemble P ? I. Les items appartenant à un itemset sont triés selon l\u0027ordre lexicographique. L\u0027ensemble D des itemsets présents dans une base de données est noté {P1, P2, ..., Pm} avec ?P ? D, P ? I. D est une base de données de transactions.\nUn arbre S \u003d (V, E) est un graphe orienté acyclique et connecté dans lequel V est l\u0027ensemble des noeuds et E \u003d {(u, v)|u, v ? V } est l\u0027ensemble des arêtes. Un noeud particulier r ? V est considéré comme étant la racine, et pour chacun des autres noeuds x ? V , il existe un unique chemin allant de r à x. S\u0027il existe un chemin allant d\u0027un noeud u à un noeud v dans S \u003d (V, E), alors u est un ancêtre de v (v est un descendant de u). Si (u, v) ? E (i.e. u est un ancêtre direct de v), alors u est un parent de v (v est un enfant de u). Dans un arbre ordonné, les fils de chaque noeuds sont ordonnés, sinon, l\u0027arbre est non ordonné. Dans cette article, sauf spécification contraire, nous considérons que les arbres sont non ordonnés. Un arbre attribué est un triplet T \u003d (V, E, ?) où (V, E) représente l\u0027arbre sous-jacent et ? : V ? D est une fonction qui associe un itemset ?(u) ? I à chaque noeud u ? V . La taille d\u0027un arbre attribué est le nombre des items associés à l\u0027ensemble des noeuds.\nDans cet article, nous utilisons une représentation textuelle d\u0027un arbre attribué basée sur celle définie par Zaki (2002) pour les arbres étiquetés. Notre représentation se distingue par l\u0027écriture des noeuds qui est générée en listant tous les items présents dans l\u0027itemset associé et par le fait que, par simplicité, nous omettons les $s finaux. Par exemple, la représentation textuelle de l\u0027arbre attribué A2 illustré dans la figure 1 est \"a c $ cde ab $ a\".\nLes arbres attribués peuvent être vus comme des itemsets organisés selon une structure arborescente. L\u0027inclusion sur les arbres attribués peut donc être définie en considérant soit l\u0027inclusion d\u0027itemsets, soit l\u0027inclusion structurelle. Pour l\u0027inclusion portant sur les itemsets, on considère que l\u0027arbre attribué T 1 est contenu dans un autre arbre attribué T 2 si les deux arbres attribués ont la même structure et que, pour chaque noeud de T 1 , l\u0027itemset qui lui est associé est inclus dans l\u0027itemset du noeud correspondant de T 2 . Plus formellement,\n. L\u0027inclusion structurelle est quant à elle représentée par le concept classique de sous-arbre (Balcázar et al., 2010;Chi et al., 2004a;Hido et Kawano, 2005;Nijssen et Kok, 2003;Termier et al., 2004;Xiao et al., 2003;Zaki, 2002).\nA partir de la définition précédente, nous généralisons la notion de sous-arbre attribué de la façon suivante. T 1 \u003d (V 1 , E 1 , ? 1 ) est un sous-arbre attribué d\u0027un arbre attribué T 2 \u003d (V 2 , E 2 , ? 2 ) et on note T 1 \u003c T 2 si T 1 est un sous-arbre attribué isomorphe de T 2 , i.e. il existe une correspondance ? :\n). Si T 1 est un sous-arbre attribué de T 2 , on dit que T 2 est un super-arbre attribué de T 1 . T 1 est un sous-arbre attribué induit de T 2 ssi T 1 est un sous-arbre attribué isomorphe de T 2 et ? préserve la relation parent-enfant. T 1 est un sous-arbre attribué enchâssé de T 2 ssi T 1 est un sous-arbre attribué isomorphe de T 2 et ? préserve la relation ancêtre-descendant. La figure 1 montre un exemple d\u0027une base de données d\u0027arbres attribués composée de trois arbres attribués différents ainsi que deux ensembles de motifs communs ; l\u0027un contenant des arbres attribués induits et l\u0027autre des arbres attribués enchâssés.\nTous les algorithmes de fouille d\u0027arbres travaillant avec des arbres non-ordonnés doivent prendre en compte le problème d\u0027isomorphisme. Pour éviter la génération redondante de solutions équivalentes, un arbre est choisi en tant que forme canonique et les formes alternatives sont écartées (Asai et al., 2002;Chi et al., 2004b;Nijssen et Kok, 2003;Xiao et al., 2003;Zaki, 2004). Dans de précédents travaux, les formes canoniques sont basées sur l\u0027ordre lexi-cographique des étiquettes des noeuds. Dans notre travail, nous définissons un ordre basé sur l\u0027ordre des itemsets associés aux noeuds. Etant donné deux itemsets P et Q (P \u003d Q), on dit que\nDe la définition précédente, un ordre, ?, portant sur les arbres attribués peut être défini. A partir de cet ordre, une forme canonique pour les arbres attribués isomorphes est facilement définie en utilisant la méthode présentée par Chi et al. (2004a).\nLe problème de la fouille d\u0027arbres attribués est que le nombre de motifs fréquents est souvent très important. Dans des applications réelles, générer toutes les solutions peut s\u0027avérer très coûteux ou même impossible. De plus, un nombre important de solutions contient des informations redondantes. Dans la figure 1, par exemple, l\u0027arbre attribué \"a e\" est présent dans toutes les transactions mais le motif est déjà encodé dans \"a cde\".\nDepuis la proposition de Mannila et Toivonen (2005) d\u0027importants efforts on été consacrés à l\u0027élaboration de représentations condensées qui résument toutes les solutions dans un ensemble restreint. L\u0027ensemble des motifs fermés est un exemple d\u0027une telle représentation condensée (Pasquier et al., 1999). Un arbre attribué T est un arbre attribué fermé si aucun de ses super-arbres attribués ne possède le même support que lui. Dans cet article, nous introduisons une autre représentation résumée des motifs qui est définie uniquement en fonction de la relation contenu dans. On dit que T est un arbre attribué c-fermé (contenu fermé) s\u0027il n\u0027est pas contenu (comme défini précédemment) dans un autre arbre attribué avec le même support.\nDéfinition du problème\nSoit une base de données B d\u0027arbres attribués et un arbre attribué T , la fréquence par transaction de T est représentée par le nombre d\u0027arbres attribués dans B pour lesquels T est un sous-arbre attribué. Un arbre attribué est fréquent si sa fréquence par transaction est supérieure ou égale à un seuil minimum. Le problème consiste à énumérer tous les motifs fréquents dans un ensemble d\u0027arbres attribués.\nEtat de l\u0027art\nLes premiers algorithmes de fouille d\u0027arbres étiquetés sont dérivés de l\u0027approche Apriori (Agrawal et al., 1993). Ceux-ci consistent en une succession d\u0027itérations comprenant une génération des candidats suivie d\u0027un calcul de fréquences à l\u0027issue duquel les motifs non fréquents sont écartés. Deux stratégies sont possibles pour la génération des candidats : extension et jointure. Avec l\u0027extension, un nouveau candidat est généré en ajoutant un noeud à un arbre fréquent (Asai et al., 2002;Nijssen et Kok, 2003). Avec la jointure, un nouveau candidat est créé en combinant deux arbres fréquents (Hido et Kawano, 2005;Zaki, 2004). La combinaison de ces deux principes a également été étudiée (Chi et al., 2004b).\nD\u0027autres algorithmes de fouille d\u0027arbres sont dérivés de l\u0027approche FP-growth (Han et al., 2000). Ces algorithmes, qui adoptent le principe de pattern-growth, permettent d\u0027éviter le coûteux processus de génération de candidats. Cependant, cette approche ne peut pas être adaptée simplement au problème de la fouille d\u0027arbres. Les implémentations existantes sont limitées dans le type d\u0027arbres qu\u0027elles peuvent manipuler (Xiao et al., 2003;Wang et al., 2004).\nTrouver des représentations condensées des motifs fréquents est une extension naturelle de la fouille de motifs. Pour la fouille d\u0027itemsets, la notion de fermeture a été formellement définie par Pasquier et al. (1999). Plusieurs travaux ont exploré ce thème dans le contexte de la fouille d\u0027arbres et ont proposé des méthodes de fouille adaptées ainsi que diverses implémentations (Chi et al., 2004c;Termier et al., 2004Termier et al., , 2008. A notre connaissance, aucune méthode n\u0027a été proposée dans le cadre général des arbres attribués.\nRécemment, nous avons vu un intérêt croissant pour la fouille d\u0027itemsets organisés selon une certaine structure. Miyoshi et al. (2009) travaillent sur des graphes étiquetés avec des attributs quantitatifs associés aux noeuds. Ce type de structure permet de résoudre le problème en combinant un algorithme \"classique\" d\u0027exploration de sous-graphe pour le graphe étiqueté, et un algorithme existant de fouille d\u0027itemsets pour les attributs quantitatifs. Cette approche ne peut pas être utilisée si les attributs associés aux noeuds ont tous la même importance. Fukuzaki et al. (2010) étudient des graphes dans lesquels les noeuds sont associés à des itemsets. Notre travail diffère de cette étude dans le sens où nous recherchons des motifs dans lesquels les itemsets associés aux noeuds ne sont pas forcément identiques.\nFouille d\u0027arbres attribués fréquents\nNous nous intéressons principalement à l\u0027identification de sous-arbres attribués induits, ordonnés ou non. Bien que l\u0027on se focalise sur les sous-arbres attribués induits, nous définissons une méthode générale qui est capable d\u0027extraire également les sous-arbres attribués enchâssés.\nÉnumération des arbres attribués\nEn utilisant l\u0027opérateur ?, il est possible de construire un arbre des candidats Q représen-tant l\u0027ensemble de l\u0027espace de recherche (Ayres et al., 2002) de la manière suivante. La racine de Q est étiquetée avec ?. Récursivement, pour chaque noeud terminal n ? Q, des fils n sont ajoutés tel que n ? n . Les fils d\u0027un noeud n ? Q sont générés soit par extension d\u0027arbre, soit par extension d\u0027itemset.\nExtension de la structure arborescente\nPour l\u0027extension de la structure arborescente (extension d\u0027arbre), nous utilisons une variante de la technique connue sous le nom de rightmost path extension (Asai et al., 2002;Nijssen et Kok, 2003). Un arbre attribué T peut être étendu pour générer de nouveaux arbres attribués de deux façons différentes. Dans le premier cas, un nouveau fils N est ajouté au noeud terminal situé le plus à droite de T . Dans le second cas, un nouveau frère N est ajouté à l\u0027un des noeuds situés sur le chemin le plus à droite de T (Chehreghani, 2011). Dans l\u0027approche classique, N représente n\u0027importe quel noeud valide figurant dans la base de données. Dans notre approche, de nouveaux noeuds N sont créés à partir des noeuds valides Q de la base de données. Dans les faits, chaque noeud Q, associé à un itemset de taille k, génère un ensemble de k noeuds N \u003d {N 1 , .., N k } qui sont utilisés pour l\u0027extension d\u0027arbre. Chaque N i est associé à un itemset de taille 1 ; l\u0027unique item étant le ième item de ?(Q). Par exemple, dans la figure 1, les noeuds qui peuvent être utilisés pour l\u0027extension du motif \"a cde\" sont \"ab\", \"a\" (de A2), \"abc\" et \"c\" (de A3). A partir du noeud \"abc\", trois extensions sont générées (\"a\", \"b\" et \"c\") alors que le noeud \"ab\" génère \"a\" et \"b\". Les noeuds \"a\" et \"c\" génèrent respectivement les extensions \"a\" et \"c\". Trois nouveaux candidats différents sont ainsi obtenus en ajoutant chacune de ces extensions au motif candidat : \"a cde a\", \"a cde b\" et \"a cde c\" Pour les arbres ordonnés, la complétude et la non-redondance de cette méthode a été dé-montrée (Asai et al., 2002). Pour les arbres non ordonnés, la méthode peut générer des motifs redondants sous la formes d\u0027arbres isomorphes. Les candidats dupliqués sont détectés et écartés avant la phase d\u0027extension au moyen d\u0027un test de canonicité.\nExtension des itemsets associés aux noeuds\nPour l\u0027extension des itemsets associés aux noeuds (extension d\u0027itemset), nous utilisons une variante de la méthode présentée par Ayres et al. (2002). Grâce à cette variante, un nouvel item I est ajouté à l\u0027itemset associé au noeud terminal le plus à droite de l\u0027arbre attribué candidat T . Les items utilisés pour l\u0027extension d\u0027itemset sont dérivés de l\u0027itemset associé à ce noeud dans la base de données. La contrainte est que le nouvel item doit être lexicographiquement situé après n\u0027importe lequel des items associés au noeud terminal le plus à droite de T .\nCalcul des fréquences\nNous organisons nos données dans une structure stockant toute l\u0027information nécessaire au processus de fouille. Notre structure est une extension de la représentation verticale introduite par Zaki (2002Zaki ( , 2004. Brièvement, chaque sous-arbre attribué candidat est associé à son motif et à un ensemble de données annexes permettant de localiser précisément toutes ses occurrences dans la base de données. Les premiers candidats, composés d\u0027un noeud unique associé à un seul item, sont générés en parcourant la base de données. En utilisant uniquement cette structure, il est facile de calculer le nombre d\u0027occurrences de chaque motif. De plus, cette même structure est suffisante pour générer toutes les extensions possibles d\u0027un motif donné. Quand un motif de taille k est traité, toutes ses occurrences sont étendues par les méthodes d\u0027extension d\u0027arbre et d\u0027extension d\u0027itemset décrites plus haut pour générer de nouveaux candidats de taille (k + 1) qui sont eux même stockés dans la structure de données.\nParcours de l\u0027espace de recherche\nPlusieurs techniques peuvent être utilisées pour élaguer l\u0027espace de recherche.\nÉlagage des candidats\nLe principe, énoncé par Agrawal et al. (1993) il y a une vingtaine d\u0027années, peut être appliqué à la fouille d\u0027arbres attribués : i) tout sous-motif d\u0027un motif fréquent est fréquent, et ii) tout super-motif d\u0027un motif non fréquent est non fréquent. Comme la fréquence suit une fonction anti-monotone (l\u0027extension d\u0027un motif ne peut pas donner un nouveau motif avec une fréquence supérieure), il est possible d\u0027arrêter l\u0027exploration d\u0027une branche lorsque la fréquence d\u0027un candidat est inférieure au support minimum. Par exemple, dans la figure 1, durant la fouille des arbres attribués, lorsque l\u0027on détermine que la fréquence du motif \"a c a\" est inférieure au support minimum, il n\u0027est pas nécessaire de générer des extensions de ce motif (e.g. \"a c ab\", \"a c a $ b\", \"a c a $ $ c\"). Dans le cas de la fouille d\u0027arbres attribués non ordonnées, l\u0027extension est également stoppée si le candidat examiné n\u0027est pas sous forme canonique.\nÉnumération des arbres attribués c-fermés\nEn énumérant uniquement les arbres attribués qui ne sont pas contenus dans un autre arbre attribué ayant le même support, l\u0027espace de recherche peut être considérablement réduit. L\u0027énumération d\u0027arbres attribués c-fermés nécessite de stocker tous les motifs fréquents identifiés ainsi que leur fréquence par transaction et leur nombre total d\u0027occurrences.\nSoit T un arbre attribué candidat, T l\u0027ensemble de tous les sous-arbres attribués fréquents identifiés précédemment et X l\u0027ensemble de tous les candidats générés par l\u0027extension de T . Nous distinguons deux sous ensembles de X : X I , l\u0027ensemble des motifs générés par extension d\u0027itemset de T et X T , l\u0027ensemble motifs obtenus par extension d\u0027arbre de T . Nous définissons deux fonctions : f t qui donne la fréquence par transaction d\u0027un sous-arbre attribué et f o qui retourne son nombre total d\u0027occurrences. T est un arbre attribué c-fermé si ?T ? T U X I tel que\n. Cependant, identifier une extension d\u0027itemset de T avec la même fréquence par transaction que T ne permet pas de stopper l\u0027exploration des autres candidats de X . La condition supplémentaire suivante doit également être satisfaite : ?T ? T U X I :\n. Dans la figure 1, par exemple, le premier candidat à être examiné est \"a\" avec une fré-quence par transaction de 3. Par extension d\u0027itemset, nous construisons X I \u003d {\"ab\", \"ac\"}. Le candidat \"ab\" a une fréquence par transaction de 3, donc, comme \"a\" \u003c I \"ab\", \"a\" n\u0027est pas c-fermé. Cependant, le motif \"a\" apparaît 7 fois dans la base de données alors que la fréquence d\u0027occurrence du candidat \"ab\" est 3. Les 4 occurrences où \"a\" apparaît dans un itemset qui ne contient pas \"b\" peuvent éventuellement générer des motifs qui eux sont cfermés. C\u0027est effectivement le cas dans la figure 1 où l\u0027extension du noeud droit du motif \"a\" génère le candidat \"a e\" avec un fréquence par transaction de 3.\nÉnumération d\u0027arbres attribués fermés\nOn dit que T est un arbre attribué fermé si ?T ? T U X tel que\nIl faut également supprimer les arbres attribués non fermés stockés dans T , i.e. tous les motifs qui sont des sous-arbres attribués de T avec la même fréquence par transaction. Le test de fermeture nécessite d\u0027effectuer plusieurs tests d\u0027isomorphisme de sous-arbres qui sont des opérations coûteuses.\nDescription des algorithmes\nLa figure 2 illustre la structure de l\u0027algorithme IMIT. Pour commencer, un ensemble de sous-arbres attribués de taille 1 est construit en parcourant la base de données (ligne 1). Chaque candidat est alors traité dans une boucle. La fonction GetF irst retourne le plus petit candidat par rapport à l\u0027opérateur ? (ligne 3). Le candidat traité est supprimé de la liste des candidats (ligne 4). S\u0027il est fréquent et s\u0027il est sous forme canonique (ligne 5), il est ajouté à la liste des solutions (ligne 6) et toutes ses extensions sont ajoutées à la liste des candidats (ligne 7).\nCet algorithme, qui implémente la méthode d\u0027élagage proposée dans la section 4.3.1 est suffisant pour énumérer toutes les solutions mais son espace de recherche est immense. Pour limiter les redondances dans l\u0027ensemble des solutions, nous avons développé un algorithme d\u0027extraction des sous-arbres attribués fermés appelé IMIT_CLOSED reprenant les principes IM IT (D, minSup) 1: C ? {all asubtrees of size 1 in D} 2: while C \u003d ? do 3:\nif isCanonical(T ) and f t (T ) ? minSup then exposés dans la section 4.3.3 (figure 3). Comme cela est montré dans la section 5, l\u0027algorithme est coûteux et ne passe pas à l\u0027échelle.\nForts de cette constatation, nous avons développé IMIT_CONTENT_CLOSED, un algorithme permettant l\u0027extraction des sous-arbres attribués c-fermés dont le principe est présenté dans la section 4.3.2. Faute de place, cet algorithme n\u0027est pas présenté ici. Cependant, il peut être facilement déduit de l\u0027algorithme IMIT_CLOSED (figure 3) en remplaçant \u003c par \u003c I et en supprimant les lignes 12 à 14. L\u0027utilisation de la relation \u003c I à la place de \u003c permet de se limiter aux tests d\u0027inclusion d\u0027itemsets qui sont beaucoup moins coûteux que les tests d\u0027isomorphismes. Les lignes 12 à 14 suppriment de l\u0027ensemble des solution trouvées précédemment celles qui contiennent un sous-arbre attribué du candidat actuellement examiné. Ce test n\u0027est pas nécessaire dans le cas de l\u0027extraction de motifs c-fermés. Les expérimentations montrent que ce troisième algorithme constitue le meilleur compromis entre la non redondance des solutions et le temps d\u0027exécution.\nRésultats experimentaux\nTous les algorithmes sont implémentés en C++ avec la STL. Les expérimentations ont été effectuées sur un ordinateur avec Ubuntu 12.04 LTS basé sur un processeur Intel c TM i5-2400 @ 3.10GHz avec 8 Gb de mémoire. Tous les temps d\u0027exécution incluent la phase de prétraitement et l\u0027affichage des résultats.\nJeux de données artificiels\nNous avons modifié le programme proposé par Zaki (2002) pour générer des arbres attribués avec différentes tailles d\u0027itemsets. Nous avons utilisé les mêmes paramètres que Zaki (2002) sauf pour le nombre de sous-arbres que nous avons fixé à 10000. Nous avons construit cinq jeux de données en faisant varier la taille des itemsets. Dans T10K, tous les noeuds sont associés à des itemsets de taille 1. Dans T10K-3 et T10K-5, les noeuds sont associés respectivement à des itemsets de taille 3 et 5. Dans T10K-1/10, les noeuds sont associés à des itemsets de taille variant de 1 à 10 alors que dans T10K-1/20, la taille des itemsets varie de 1 à 20.\nIM IT _CLOSED(D, minSup)\n1: C ? {all asubtrees of size 1 in D} 2: while C \u003d ? do 3:\nif isCanonical(T ) and f t (T ) ? minSup then 6:\nif ?T ? T : T \u003c T and f t (T ) \u003d f t (T ) and ?T ? X I : f t (x) \u003d f t (T ) then \nDonnées de journaux d\u0027activité Web\nNous avons élaboré un jeu de données à partir du journal d\u0027activité Web de notre université. Cependant, au lieu d\u0027étiqueter les noeuds avec les URLs des pages consultées, nous leur avons associé un ensemble de mots clés portant sur le contenu des pages. Cette approche permet de capturer les habitudes de navigation des utilisateurs même dans le cas où la structure du site Web change. Le jeu de données est composé de 126 396 arbres annotés avec des itemsets de taille 10 (10 mots clés par page).\nÉvaluation des performances\nLa figure 4 montre les temps d\u0027exécution d\u0027IMIT_CONTENT_CLOSED pour la fouille de motifs c-fermés induits et non-ordonnés sur les 5 jeux de données artificiels. A titre de comparaison, nous avons ajouté dans la figure les temps d\u0027exécution de SLEUTH (Zaki, 2004), une implémentation de référence du paradigme d\u0027extension de classes d\u0027équivalence, pour fouiller T10K. IMIT_CONTENT_CLOSED est environ deux fois plus lent que SLEUTH pour toutes les valeurs de support, sauf pour les plus petites pour lesquelles SLEUTH est pénalisé par le coût de la jointure sur des millions de motifs fréquents. Bien qu\u0027IMIT_CONTENT_CLOSED soit en général plus lent que SLEUTH, les résultats sont satisfaisants car notre algorithme est conçu pour fouiller des arbres attribués. Il est normal d\u0027être moins performant que des implé-mentations dédiées à la fouille d\u0027arbres étiquetés.\nLa figure montre également que la fouille d\u0027arbres attribués demande beaucoup plus de ressources que la fouille d\u0027arbres étiquetés ; et la différence est largement sous-estimée car seuls les motifs c-fermés sont listés. Extraire tous les motifs génère un grand nombre de so-  lutions et prend énormément de temps. Pour donner une idée, la fouille du jeu de données T10K-3 avec un support minimum de 1% génère plus de 12 millions de motifs en 15 heures (figure 5). Rechercher uniquement les sous-arbres c-fermés permet de réduire à la fois le nombre de motifs et le temps d\u0027exécution. Ainsi, avec un support minimum de 1% , 200 motifs c-fermés sont identifiés en 4 secondes. Comme illustré dans la même figure, la recherche de motifs fermés permet de réduire encore le nombre de motifs. Avec un support de 1%, par exemple, le nombre de motifs tombe à 103. Cependant, à cause des tests coûteux d\u0027isomorphisme d\u0027arbres, en contrepartie, les performances s\u0027effondrent lorsque les motifs sont nombreux. La figure 6 montre les temps d\u0027exécution et le nombre de motifs c-fermés trouvés dans les données de journaux d\u0027activité Web. Ce jeu de données réel est plus important que ceux utilisés précédemment et la fouille ne peut être réalisée raisonnablement avec un support inférieur à 10%. La fouille avec un support de 6% retourne 360 motifs en 6 heures.\nConclusion et perspectives\nDans cet article, nous avons introduit le problème de la fouille d\u0027arbres attribués. Nous avons exploré des méthodes permettant d\u0027énumérer tous les motifs fréquents ou seulement les fermés. Ces méthodes se sont révélées inefficaces à cause, dans le premier cas, du nombre Remerciements. Ce travail a été financé par le contrat ANR-2010-COSI-012 FOSTER.\n"
  },
  {
    "id": "404",
    "text": "Introduction\nL\u0027extraction de motifs fréquents est une tâche importante dans le domaine de la fouille de données. Initialement centrée sur la découverte d\u0027ensemble d\u0027items (itemsets) fréquents (Agrawal et al., 1993), les premiers travaux ont été étendus pour extraire des motifs structurels comme les séquences (Agrawal et Srikant, 1995), les arbres (Chi et al., 2004a) ou les graphes (Washio et Motoda, 2003).\nAlors que l\u0027extraction d\u0027itemsets fréquents recherche les combinaisons fréquentes d\u0027items, l\u0027extraction de motifs structurels recherche des sous-structures fréquentes. La plupart des travaux existants se focalisent sur un seul type de problème (fouille d\u0027itemsets ou fouille structurelle). Toutefois, afin de représenter des données plus complexes, il semble naturel de considérer des collections structurées d\u0027itemsets. Dans cet article, nous introduisons le problème de fouille d\u0027arbres attribués. Les arbres attribués sont des arbres dans lesquels les noeuds sont associés à des itemsets.\nLes arbres attribués peuvent être utilisés dans de nombreuses applications de fouilles de données spatio-temporelles. Dans le cas d\u0027études épidémiologiques, par exemple, l\u0027espace géographique peut être découpé en zones qui sont représentées par les noeuds de l\u0027arbre, les itemsets décrivent les caractéristiques de ces zones à un temps donné et les arêtes symbolisent des relations de voisinage avec d\u0027autres zones au temps suivant. Les motifs fréquent trouvés sont ainsi susceptible de donner un éclairage nouveau sur les déterminants d\u0027une pathologie. D\u0027autres applications peuvent être imaginées dans divers domaines comme l\u0027analyse des arbres de retweet, la fouille d\u0027arbres phylogénétiques, la fouille de documents XML ou l\u0027analyse de journaux d\u0027activité Web (Web log analysis).\nLes contributions clés de notre travail sont les suivantes : 1) nous présentons le problème de la fouille de sous-structures ordonnées et non ordonnées dans une collection d\u0027arbres attribués, 2) nous définissons les formes canoniques des arbres attribués, 3) nous proposons une méthode permettant l\u0027énumération des arbres attribués qui est basée sur la combinaison de deux opéra-tions : l\u0027extension de la structure arborescente et l\u0027extension des itemsets associés aux noeuds, 4) nous présentons trois variantes d\u0027un algorithme permettant d\u0027extraire les motifs fréquents dans des arbres attribués, 5) nous montrons les résultats expérimentaux effectués sur plusieurs jeux de données artificiels et un jeu de données réel.\nCet article est organisé de la manière suivante : la section 2 présente les concepts de base et définit le problème, la section 3 propose un bref aperçu de l\u0027état de l\u0027art et met l\u0027accent sur les quelques études qui combinent la fouille d\u0027itemsets et la fouille structurelle, la section 4 décrit la méthode en insistant sur l\u0027exploration de l\u0027espace de recherche, le calcul des fréquences et l\u0027élagage des candidats, la section 5 montre les résultats de la fouille de plusieurs jeux de données artificiels et réels et finalement, la section 6 conclut l\u0027article et présente de possibles extensions de notre travail.\nConcepts généraux et définition du problème\nDans cette section, nous introduisons les concepts et définitions nécessaires et présentons le problème de fouille d\u0027arbres attribués.\nPréliminaires\nSoit I \u003d {i 1 , i 2 , .., i n } un ensemble d\u0027items. Un itemset est un ensemble P ? I. Les items appartenant à un itemset sont triés selon l\u0027ordre lexicographique. L\u0027ensemble D des itemsets présents dans une base de données est noté {P1, P2, ..., Pm} avec ?P ? D, P ? I. D est une base de données de transactions.\nUn arbre S \u003d (V, E) est un graphe orienté acyclique et connecté dans lequel V est l\u0027ensemble des noeuds et E \u003d {(u, v)|u, v ? V } est l\u0027ensemble des arêtes. Un noeud particulier r ? V est considéré comme étant la racine, et pour chacun des autres noeuds x ? V , il existe un unique chemin allant de r à x. S\u0027il existe un chemin allant d\u0027un noeud u à un noeud v dans S \u003d (V, E), alors u est un ancêtre de v (v est un descendant de u). Si (u, v) ? E (i.e. u est un ancêtre direct de v), alors u est un parent de v (v est un enfant de u). Dans un arbre ordonné, les fils de chaque noeuds sont ordonnés, sinon, l\u0027arbre est non ordonné. Dans cette article, sauf spécification contraire, nous considérons que les arbres sont non ordonnés. Un arbre attribué est un triplet T \u003d (V, E, ?) où (V, E) représente l\u0027arbre sous-jacent et ? : V ? D est une fonction qui associe un itemset ?(u) ? I à chaque noeud u ? V . La taille d\u0027un arbre attribué est le nombre des items associés à l\u0027ensemble des noeuds.\nDans cet article, nous utilisons une représentation textuelle d\u0027un arbre attribué basée sur celle définie par Zaki (2002) pour les arbres étiquetés. Notre représentation se distingue par l\u0027écriture des noeuds qui est générée en listant tous les items présents dans l\u0027itemset associé et par le fait que, par simplicité, nous omettons les $s finaux. Par exemple, la représentation textuelle de l\u0027arbre attribué A2 illustré dans la figure 1 est \"a c $ cde ab $ a\".\nLes arbres attribués peuvent être vus comme des itemsets organisés selon une structure arborescente. L\u0027inclusion sur les arbres attribués peut donc être définie en considérant soit l\u0027inclusion d\u0027itemsets, soit l\u0027inclusion structurelle. Pour l\u0027inclusion portant sur les itemsets, on considère que l\u0027arbre attribué T 1 est contenu dans un autre arbre attribué T 2 si les deux arbres attribués ont la même structure et que, pour chaque noeud de T 1 , l\u0027itemset qui lui est associé est inclus dans l\u0027itemset du noeud correspondant de T 2 . Plus formellement,\n. L\u0027inclusion structurelle est quant à elle représentée par le concept classique de sous-arbre (Balcázar et al., 2010;Chi et al., 2004a;Hido et Kawano, 2005;Nijssen et Kok, 2003;Termier et al., 2004;Xiao et al., 2003;Zaki, 2002).\nA partir de la définition précédente, nous généralisons la notion de sous-arbre attribué de la façon suivante. T 1 \u003d (V 1 , E 1 , ? 1 ) est un sous-arbre attribué d\u0027un arbre attribué T 2 \u003d (V 2 , E 2 , ? 2 ) et on note T 1 \u003c T 2 si T 1 est un sous-arbre attribué isomorphe de T 2 , i.e. il existe une correspondance ? :\n). Si T 1 est un sous-arbre attribué de T 2 , on dit que T 2 est un super-arbre attribué de T 1 . T 1 est un sous-arbre attribué induit de T 2 ssi T 1 est un sous-arbre attribué isomorphe de T 2 et ? préserve la relation parent-enfant. T 1 est un sous-arbre attribué enchâssé de T 2 ssi T 1 est un sous-arbre attribué isomorphe de T 2 et ? préserve la relation ancêtre-descendant. La figure 1 montre un exemple d\u0027une base de données d\u0027arbres attribués composée de trois arbres attribués différents ainsi que deux ensembles de motifs communs ; l\u0027un contenant des arbres attribués induits et l\u0027autre des arbres attribués enchâssés.\nTous les algorithmes de fouille d\u0027arbres travaillant avec des arbres non-ordonnés doivent prendre en compte le problème d\u0027isomorphisme. Pour éviter la génération redondante de solutions équivalentes, un arbre est choisi en tant que forme canonique et les formes alternatives sont écartées (Asai et al., 2002;Chi et al., 2004b;Nijssen et Kok, 2003;Xiao et al., 2003;Zaki, 2004). Dans de précédents travaux, les formes canoniques sont basées sur l\u0027ordre lexi-cographique des étiquettes des noeuds. Dans notre travail, nous définissons un ordre basé sur l\u0027ordre des itemsets associés aux noeuds. Etant donné deux itemsets P et Q (P \u003d Q), on dit que\nDe la définition précédente, un ordre, ?, portant sur les arbres attribués peut être défini. A partir de cet ordre, une forme canonique pour les arbres attribués isomorphes est facilement définie en utilisant la méthode présentée par Chi et al. (2004a).\nLe problème de la fouille d\u0027arbres attribués est que le nombre de motifs fréquents est souvent très important. Dans des applications réelles, générer toutes les solutions peut s\u0027avérer très coûteux ou même impossible. De plus, un nombre important de solutions contient des informations redondantes. Dans la figure 1, par exemple, l\u0027arbre attribué \"a e\" est présent dans toutes les transactions mais le motif est déjà encodé dans \"a cde\".\nDepuis la proposition de Mannila et Toivonen (2005) d\u0027importants efforts on été consacrés à l\u0027élaboration de représentations condensées qui résument toutes les solutions dans un ensemble restreint. L\u0027ensemble des motifs fermés est un exemple d\u0027une telle représentation condensée (Pasquier et al., 1999). Un arbre attribué T est un arbre attribué fermé si aucun de ses super-arbres attribués ne possède le même support que lui. Dans cet article, nous introduisons une autre représentation résumée des motifs qui est définie uniquement en fonction de la relation contenu dans. On dit que T est un arbre attribué c-fermé (contenu fermé) s\u0027il n\u0027est pas contenu (comme défini précédemment) dans un autre arbre attribué avec le même support.\nDéfinition du problème\nSoit une base de données B d\u0027arbres attribués et un arbre attribué T , la fréquence par transaction de T est représentée par le nombre d\u0027arbres attribués dans B pour lesquels T est un sous-arbre attribué. Un arbre attribué est fréquent si sa fréquence par transaction est supérieure ou égale à un seuil minimum. Le problème consiste à énumérer tous les motifs fréquents dans un ensemble d\u0027arbres attribués.\nEtat de l\u0027art\nLes premiers algorithmes de fouille d\u0027arbres étiquetés sont dérivés de l\u0027approche Apriori (Agrawal et al., 1993). Ceux-ci consistent en une succession d\u0027itérations comprenant une génération des candidats suivie d\u0027un calcul de fréquences à l\u0027issue duquel les motifs non fréquents sont écartés. Deux stratégies sont possibles pour la génération des candidats : extension et jointure. Avec l\u0027extension, un nouveau candidat est généré en ajoutant un noeud à un arbre fréquent (Asai et al., 2002;Nijssen et Kok, 2003). Avec la jointure, un nouveau candidat est créé en combinant deux arbres fréquents (Hido et Kawano, 2005;Zaki, 2004). La combinaison de ces deux principes a également été étudiée (Chi et al., 2004b).\nD\u0027autres algorithmes de fouille d\u0027arbres sont dérivés de l\u0027approche FP-growth (Han et al., 2000). Ces algorithmes, qui adoptent le principe de pattern-growth, permettent d\u0027éviter le coûteux processus de génération de candidats. Cependant, cette approche ne peut pas être adaptée simplement au problème de la fouille d\u0027arbres. Les implémentations existantes sont limitées dans le type d\u0027arbres qu\u0027elles peuvent manipuler (Xiao et al., 2003;Wang et al., 2004).\nTrouver des représentations condensées des motifs fréquents est une extension naturelle de la fouille de motifs. Pour la fouille d\u0027itemsets, la notion de fermeture a été formellement définie par Pasquier et al. (1999). Plusieurs travaux ont exploré ce thème dans le contexte de la fouille d\u0027arbres et ont proposé des méthodes de fouille adaptées ainsi que diverses implémentations (Chi et al., 2004c;Termier et al., 2004Termier et al., , 2008. A notre connaissance, aucune méthode n\u0027a été proposée dans le cadre général des arbres attribués.\nRécemment, nous avons vu un intérêt croissant pour la fouille d\u0027itemsets organisés selon une certaine structure. Miyoshi et al. (2009) travaillent sur des graphes étiquetés avec des attributs quantitatifs associés aux noeuds. Ce type de structure permet de résoudre le problème en combinant un algorithme \"classique\" d\u0027exploration de sous-graphe pour le graphe étiqueté, et un algorithme existant de fouille d\u0027itemsets pour les attributs quantitatifs. Cette approche ne peut pas être utilisée si les attributs associés aux noeuds ont tous la même importance. Fukuzaki et al. (2010) étudient des graphes dans lesquels les noeuds sont associés à des itemsets. Notre travail diffère de cette étude dans le sens où nous recherchons des motifs dans lesquels les itemsets associés aux noeuds ne sont pas forcément identiques.\nFouille d\u0027arbres attribués fréquents\nNous nous intéressons principalement à l\u0027identification de sous-arbres attribués induits, ordonnés ou non. Bien que l\u0027on se focalise sur les sous-arbres attribués induits, nous définissons une méthode générale qui est capable d\u0027extraire également les sous-arbres attribués enchâssés.\nÉnumération des arbres attribués\nEn utilisant l\u0027opérateur ?, il est possible de construire un arbre des candidats Q représen-tant l\u0027ensemble de l\u0027espace de recherche (Ayres et al., 2002) de la manière suivante. La racine de Q est étiquetée avec ?. Récursivement, pour chaque noeud terminal n ? Q, des fils n sont ajoutés tel que n ? n . Les fils d\u0027un noeud n ? Q sont générés soit par extension d\u0027arbre, soit par extension d\u0027itemset.\nExtension de la structure arborescente\nPour l\u0027extension de la structure arborescente (extension d\u0027arbre), nous utilisons une variante de la technique connue sous le nom de rightmost path extension (Asai et al., 2002;Nijssen et Kok, 2003). Un arbre attribué T peut être étendu pour générer de nouveaux arbres attribués de deux façons différentes. Dans le premier cas, un nouveau fils N est ajouté au noeud terminal situé le plus à droite de T . Dans le second cas, un nouveau frère N est ajouté à l\u0027un des noeuds situés sur le chemin le plus à droite de T (Chehreghani, 2011). Dans l\u0027approche classique, N représente n\u0027importe quel noeud valide figurant dans la base de données. Dans notre approche, de nouveaux noeuds N sont créés à partir des noeuds valides Q de la base de données. Dans les faits, chaque noeud Q, associé à un itemset de taille k, génère un ensemble de k noeuds N \u003d {N 1 , .., N k } qui sont utilisés pour l\u0027extension d\u0027arbre. Chaque N i est associé à un itemset de taille 1 ; l\u0027unique item étant le ième item de ?(Q). Par exemple, dans la figure 1, les noeuds qui peuvent être utilisés pour l\u0027extension du motif \"a cde\" sont \"ab\", \"a\" (de A2), \"abc\" et \"c\" (de A3). A partir du noeud \"abc\", trois extensions sont générées (\"a\", \"b\" et \"c\") alors que le noeud \"ab\" génère \"a\" et \"b\". Les noeuds \"a\" et \"c\" génèrent respectivement les extensions \"a\" et \"c\". Trois nouveaux candidats différents sont ainsi obtenus en ajoutant chacune de ces extensions au motif candidat : \"a cde a\", \"a cde b\" et \"a cde c\" Pour les arbres ordonnés, la complétude et la non-redondance de cette méthode a été dé-montrée (Asai et al., 2002). Pour les arbres non ordonnés, la méthode peut générer des motifs redondants sous la formes d\u0027arbres isomorphes. Les candidats dupliqués sont détectés et écartés avant la phase d\u0027extension au moyen d\u0027un test de canonicité.\nExtension des itemsets associés aux noeuds\nPour l\u0027extension des itemsets associés aux noeuds (extension d\u0027itemset), nous utilisons une variante de la méthode présentée par Ayres et al. (2002). Grâce à cette variante, un nouvel item I est ajouté à l\u0027itemset associé au noeud terminal le plus à droite de l\u0027arbre attribué candidat T . Les items utilisés pour l\u0027extension d\u0027itemset sont dérivés de l\u0027itemset associé à ce noeud dans la base de données. La contrainte est que le nouvel item doit être lexicographiquement situé après n\u0027importe lequel des items associés au noeud terminal le plus à droite de T .\nCalcul des fréquences\nNous organisons nos données dans une structure stockant toute l\u0027information nécessaire au processus de fouille. Notre structure est une extension de la représentation verticale introduite par Zaki (2002Zaki ( , 2004. Brièvement, chaque sous-arbre attribué candidat est associé à son motif et à un ensemble de données annexes permettant de localiser précisément toutes ses occurrences dans la base de données. Les premiers candidats, composés d\u0027un noeud unique associé à un seul item, sont générés en parcourant la base de données. En utilisant uniquement cette structure, il est facile de calculer le nombre d\u0027occurrences de chaque motif. De plus, cette même structure est suffisante pour générer toutes les extensions possibles d\u0027un motif donné. Quand un motif de taille k est traité, toutes ses occurrences sont étendues par les méthodes d\u0027extension d\u0027arbre et d\u0027extension d\u0027itemset décrites plus haut pour générer de nouveaux candidats de taille (k + 1) qui sont eux même stockés dans la structure de données.\nParcours de l\u0027espace de recherche\nPlusieurs techniques peuvent être utilisées pour élaguer l\u0027espace de recherche.\nÉlagage des candidats\nLe principe, énoncé par Agrawal et al. (1993) il y a une vingtaine d\u0027années, peut être appliqué à la fouille d\u0027arbres attribués : i) tout sous-motif d\u0027un motif fréquent est fréquent, et ii) tout super-motif d\u0027un motif non fréquent est non fréquent. Comme la fréquence suit une fonction anti-monotone (l\u0027extension d\u0027un motif ne peut pas donner un nouveau motif avec une fréquence supérieure), il est possible d\u0027arrêter l\u0027exploration d\u0027une branche lorsque la fréquence d\u0027un candidat est inférieure au support minimum. Par exemple, dans la figure 1, durant la fouille des arbres attribués, lorsque l\u0027on détermine que la fréquence du motif \"a c a\" est inférieure au support minimum, il n\u0027est pas nécessaire de générer des extensions de ce motif (e.g. \"a c ab\", \"a c a $ b\", \"a c a $ $ c\"). Dans le cas de la fouille d\u0027arbres attribués non ordonnées, l\u0027extension est également stoppée si le candidat examiné n\u0027est pas sous forme canonique.\nÉnumération des arbres attribués c-fermés\nEn énumérant uniquement les arbres attribués qui ne sont pas contenus dans un autre arbre attribué ayant le même support, l\u0027espace de recherche peut être considérablement réduit. L\u0027énumération d\u0027arbres attribués c-fermés nécessite de stocker tous les motifs fréquents identifiés ainsi que leur fréquence par transaction et leur nombre total d\u0027occurrences.\nSoit T un arbre attribué candidat, T l\u0027ensemble de tous les sous-arbres attribués fréquents identifiés précédemment et X l\u0027ensemble de tous les candidats générés par l\u0027extension de T . Nous distinguons deux sous ensembles de X : X I , l\u0027ensemble des motifs générés par extension d\u0027itemset de T et X T , l\u0027ensemble motifs obtenus par extension d\u0027arbre de T . Nous définissons deux fonctions : f t qui donne la fréquence par transaction d\u0027un sous-arbre attribué et f o qui retourne son nombre total d\u0027occurrences. T est un arbre attribué c-fermé si ?T ? T U X I tel que\n. Cependant, identifier une extension d\u0027itemset de T avec la même fréquence par transaction que T ne permet pas de stopper l\u0027exploration des autres candidats de X . La condition supplémentaire suivante doit également être satisfaite : ?T ? T U X I :\n. Dans la figure 1, par exemple, le premier candidat à être examiné est \"a\" avec une fré-quence par transaction de 3. Par extension d\u0027itemset, nous construisons X I \u003d {\"ab\", \"ac\"}. Le candidat \"ab\" a une fréquence par transaction de 3, donc, comme \"a\" \u003c I \"ab\", \"a\" n\u0027est pas c-fermé. Cependant, le motif \"a\" apparaît 7 fois dans la base de données alors que la fréquence d\u0027occurrence du candidat \"ab\" est 3. Les 4 occurrences où \"a\" apparaît dans un itemset qui ne contient pas \"b\" peuvent éventuellement générer des motifs qui eux sont cfermés. C\u0027est effectivement le cas dans la figure 1 où l\u0027extension du noeud droit du motif \"a\" génère le candidat \"a e\" avec un fréquence par transaction de 3.\nÉnumération d\u0027arbres attribués fermés\nOn dit que T est un arbre attribué fermé si ?T ? T U X tel que\nIl faut également supprimer les arbres attribués non fermés stockés dans T , i.e. tous les motifs qui sont des sous-arbres attribués de T avec la même fréquence par transaction. Le test de fermeture nécessite d\u0027effectuer plusieurs tests d\u0027isomorphisme de sous-arbres qui sont des opérations coûteuses.\nDescription des algorithmes\nLa figure 2 illustre la structure de l\u0027algorithme IMIT. Pour commencer, un ensemble de sous-arbres attribués de taille 1 est construit en parcourant la base de données (ligne 1). Chaque candidat est alors traité dans une boucle. La fonction GetF irst retourne le plus petit candidat par rapport à l\u0027opérateur ? (ligne 3). Le candidat traité est supprimé de la liste des candidats (ligne 4). S\u0027il est fréquent et s\u0027il est sous forme canonique (ligne 5), il est ajouté à la liste des solutions (ligne 6) et toutes ses extensions sont ajoutées à la liste des candidats (ligne 7).\nCet algorithme, qui implémente la méthode d\u0027élagage proposée dans la section 4.3.1 est suffisant pour énumérer toutes les solutions mais son espace de recherche est immense. Pour limiter les redondances dans l\u0027ensemble des solutions, nous avons développé un algorithme d\u0027extraction des sous-arbres attribués fermés appelé IMIT_CLOSED reprenant les principes IM IT (D, minSup) 1: C ? {all asubtrees of size 1 in D} 2: while C \u003d ? do 3:\nif isCanonical(T ) and f t (T ) ? minSup then exposés dans la section 4.3.3 (figure 3). Comme cela est montré dans la section 5, l\u0027algorithme est coûteux et ne passe pas à l\u0027échelle.\nForts de cette constatation, nous avons développé IMIT_CONTENT_CLOSED, un algorithme permettant l\u0027extraction des sous-arbres attribués c-fermés dont le principe est présenté dans la section 4.3.2. Faute de place, cet algorithme n\u0027est pas présenté ici. Cependant, il peut être facilement déduit de l\u0027algorithme IMIT_CLOSED (figure 3) en remplaçant \u003c par \u003c I et en supprimant les lignes 12 à 14. L\u0027utilisation de la relation \u003c I à la place de \u003c permet de se limiter aux tests d\u0027inclusion d\u0027itemsets qui sont beaucoup moins coûteux que les tests d\u0027isomorphismes. Les lignes 12 à 14 suppriment de l\u0027ensemble des solution trouvées précédemment celles qui contiennent un sous-arbre attribué du candidat actuellement examiné. Ce test n\u0027est pas nécessaire dans le cas de l\u0027extraction de motifs c-fermés. Les expérimentations montrent que ce troisième algorithme constitue le meilleur compromis entre la non redondance des solutions et le temps d\u0027exécution.\nRésultats experimentaux\nTous les algorithmes sont implémentés en C++ avec la STL. Les expérimentations ont été effectuées sur un ordinateur avec Ubuntu 12.04 LTS basé sur un processeur Intel c TM i5-2400 @ 3.10GHz avec 8 Gb de mémoire. Tous les temps d\u0027exécution incluent la phase de prétraitement et l\u0027affichage des résultats.\nJeux de données artificiels\nNous avons modifié le programme proposé par Zaki (2002) pour générer des arbres attribués avec différentes tailles d\u0027itemsets. Nous avons utilisé les mêmes paramètres que Zaki (2002) sauf pour le nombre de sous-arbres que nous avons fixé à 10000. Nous avons construit cinq jeux de données en faisant varier la taille des itemsets. Dans T10K, tous les noeuds sont associés à des itemsets de taille 1. Dans T10K-3 et T10K-5, les noeuds sont associés respectivement à des itemsets de taille 3 et 5. Dans T10K-1/10, les noeuds sont associés à des itemsets de taille variant de 1 à 10 alors que dans T10K-1/20, la taille des itemsets varie de 1 à 20.\nIM IT _CLOSED(D, minSup)\n1: C ? {all asubtrees of size 1 in D} 2: while C \u003d ? do 3:\nif isCanonical(T ) and f t (T ) ? minSup then 6:\nif ?T ? T : T \u003c T and f t (T ) \u003d f t (T ) and ?T ? X I : f t (x) \u003d f t (T ) then \nDonnées de journaux d\u0027activité Web\nNous avons élaboré un jeu de données à partir du journal d\u0027activité Web de notre université. Cependant, au lieu d\u0027étiqueter les noeuds avec les URLs des pages consultées, nous leur avons associé un ensemble de mots clés portant sur le contenu des pages. Cette approche permet de capturer les habitudes de navigation des utilisateurs même dans le cas où la structure du site Web change. Le jeu de données est composé de 126 396 arbres annotés avec des itemsets de taille 10 (10 mots clés par page).\nÉvaluation des performances\nLa figure 4 montre les temps d\u0027exécution d\u0027IMIT_CONTENT_CLOSED pour la fouille de motifs c-fermés induits et non-ordonnés sur les 5 jeux de données artificiels. A titre de comparaison, nous avons ajouté dans la figure les temps d\u0027exécution de SLEUTH (Zaki, 2004), une implémentation de référence du paradigme d\u0027extension de classes d\u0027équivalence, pour fouiller T10K. IMIT_CONTENT_CLOSED est environ deux fois plus lent que SLEUTH pour toutes les valeurs de support, sauf pour les plus petites pour lesquelles SLEUTH est pénalisé par le coût de la jointure sur des millions de motifs fréquents. Bien qu\u0027IMIT_CONTENT_CLOSED soit en général plus lent que SLEUTH, les résultats sont satisfaisants car notre algorithme est conçu pour fouiller des arbres attribués. Il est normal d\u0027être moins performant que des implé-mentations dédiées à la fouille d\u0027arbres étiquetés.\nLa figure montre également que la fouille d\u0027arbres attribués demande beaucoup plus de ressources que la fouille d\u0027arbres étiquetés ; et la différence est largement sous-estimée car seuls les motifs c-fermés sont listés. Extraire tous les motifs génère un grand nombre de so-  lutions et prend énormément de temps. Pour donner une idée, la fouille du jeu de données T10K-3 avec un support minimum de 1% génère plus de 12 millions de motifs en 15 heures (figure 5). Rechercher uniquement les sous-arbres c-fermés permet de réduire à la fois le nombre de motifs et le temps d\u0027exécution. Ainsi, avec un support minimum de 1% , 200 motifs c-fermés sont identifiés en 4 secondes. Comme illustré dans la même figure, la recherche de motifs fermés permet de réduire encore le nombre de motifs. Avec un support de 1%, par exemple, le nombre de motifs tombe à 103. Cependant, à cause des tests coûteux d\u0027isomorphisme d\u0027arbres, en contrepartie, les performances s\u0027effondrent lorsque les motifs sont nombreux. La figure 6 montre les temps d\u0027exécution et le nombre de motifs c-fermés trouvés dans les données de journaux d\u0027activité Web. Ce jeu de données réel est plus important que ceux utilisés précédemment et la fouille ne peut être réalisée raisonnablement avec un support inférieur à 10%. La fouille avec un support de 6% retourne 360 motifs en 6 heures.\nConclusion et perspectives\nDans cet article, nous avons introduit le problème de la fouille d\u0027arbres attribués. Nous avons exploré des méthodes permettant d\u0027énumérer tous les motifs fréquents ou seulement les fermés. Ces méthodes se sont révélées inefficaces à cause, dans le premier cas, du nombre Remerciements. Ce travail a été financé par le contrat ANR-2010-COSI-012 FOSTER.\n"
  },
  {
    "id": "405",
    "text": "Introduction\nUne approche récente de l\u0027analyse exploratoire de données multidimensionnelles consiste en l\u0027extraction d\u0027invariants topologiques (Carlsson, 2009). Ces invariants permettent de caractériser la topologie de la population si l\u0027on suppose qu\u0027elle est définie par une collection de sous-variétés de l\u0027espace R D dont sont issues les données. Les nombres de Betti sont de tels invariants : le premier encode le nombre de composantes connexes, le second le nombre de cycles indépendants, le troisième le nombre de cavités indépendantes, etc... Ces nombres expriment numériquement des caractèristiques topologiques que nous extrayons visuellement lors de l\u0027analyse de nuages de points présentés dans un repère cartésien à deux dimensions. Ces invariants topologiques le sont par homotopie une classe très large de transformations non linéaires incluant les homéomorphismes, les similitudes et les isométries, on s\u0027attend donc à ce que l\u0027informaiton topologique soit plus robuste à la chaîne de mesure que l\u0027information de nature géométrique.\nIl existe des techniques d\u0027extraction des nombres de Betti à partir d\u0027un complexe simplicial (de Silva). Un complexe simplicial est un ensemble de simplexes tels que l\u0027intersection de deux simplexes du complexe est soit vide soit fait aussi partie du complexe. Le plongement dans R D d\u0027un k-simplexe est l\u0027enveloppe convexe de (k + 1)-points de cet espace. Un complexe simplicial plongé est donc une collection de variétés qui peut servir de modèle topologique et géométrique à la population génératrice des données. Nous devons lui associer un modèle de densité de probabilité afin de modéliser complètement le processus de génération des données.\nLes cartes de Kohonen (Kohonen, 1989) ou leur pendant génératif que sont les Generative Topographic Map (GTM) (Bishop et al., 1998) ainsi que les Generative Principal Manifolds (Tibshirani, 1992) sont des modèles topologique dont la dimension intrinsèque doit être fixée a priori. Les modèles de mélange de gaussiennes classiques (McLachlan et Peel, 2000) permettent de modéliser la densité du nuage de points, mais n\u0027encodent aucune information topologique. Nos précédents travaux sur le Graphe Génératif Gaussien (Aupetit, 2005), que l\u0027on peut voir comme une version générative des Topology Representing Network (TRN) de (Martinetz et Schulten, 1994) \nHypothèses générales et description du modèle\nLes données sont un nuage de points dans R D , supposées issues d\u0027une collection de variétés, perturbées par un bruit centré gaussien isovarié, dont la variance ? 2 est inconnue. Nous supposons que cette collection de variétés peut être approchée par une complexe simplicial de Delaunay dont les sommets w \u003d (w 1 , ..., w N ) appartiennent à R D . Nous définissons le Complexe Simplicial Génératif (CSG) comme la convolution d\u0027un bruit gaussien isovarié à un complexe simplicial de Delaunay de sommets w. Les composants de ce modèle de mélange sont les différents simplexes du complexe : les sommets, les arêtes, les triangles, les tétra-èdres... Les paramètres de ce modèle sont la position des sommets du complexe, la proportion des composants du mélange, et la variance du bruit gaussien. Nous utilisons l\u0027algorithme EM pour trouver le jeu de paramètres qui maximise la vraisemblance, et nous utilisons le critère BIC (Schwarz, 1978) pour sélectionner le modèle de complexité optimale. La complexité du modèle est déterminée par son nombre de sommets, d\u0027arêtes, de triangles... de simplexes que l\u0027on conserve. Optimiser BIC revient donc à retirer certains simplexes du modèle, et donc à sculpter le complexe de Delaunay pour obtenir un sous-complexe de Delaunay BIC-optimal, dont on suppose que la topologie modélise ainsi au mieux celle de la population. Nous dé-taillons le modèle CSG et l\u0027algorithme d\u0027apprentissage.\nLe Simplexe Génératif (SG)\nUn simplexe génératif (SG) est une fonciton densité de probabilité basée sur un simplexe plongé. Soit S \nCeci peut être vu comme la convolution d\u0027une densité gaussienne multidimensionnelle et d\u0027un simplexe. Pour former le modèle de mélange nous remplaçons les composants gaussiens classiques, sources ponctuelles de données perturbées par un bruit gaussien, par des sources simpliciales. Le modèle de mélange associé est le Complexe Simplicial Génératif.\nLe Complexe Simplicial Génératif (CSG)\nUn Complexe Simplicial Génératif est un mélange de simplexes génératifs S \u003d {S \nExpériences\nVariétés topologiques connues : le tore et la sphère\nNous testons la technique sur des variétés plongée dans R 3 dont la topologie est connue : une sphère de rayon 1 et un tore de petit rayon 3 et de grand rayon 10. Les nombres de Betti attendus pour la sphère sont (1, 0, 1, 0...), et pour le tore (1, 2, 1, 0...). Le CSG est comparé au Witness Complex (WitC) tel qu\u0027implémenté dans le code Javaplex (Adams et Tausz). Le nombre de prototypes (30 pour la sphère, 40 pour le tore) et leur position sont identiques pour le CSG et le WitC, ils sont obtenus par un modèle de mélange gaussien classique maximisant le critère BIC. La méthode infiniteBarcodes de Javaplex retourne les nombres de Betti recherchés à partir d\u0027une filtration du complexe simplicial.\nPour la sphère, 1000 points sont tirés aléatoirement avec un bruit gaussien centré d\u0027écart-type ? ? {0.05, 0.1, 0.2}. Pour le tore, 2000 points sont tirés aléatoirement avec un bruit gaussien centré d\u0027écart-type ? ? {0.01, 0.05, 0.1}. Dans chaque cas, CSG et WitC sont relancés 100 fois à partir d\u0027une initialisation aléatoire de leurs paramètres. Une réponse est considérée comme correcte uniquement si les nombres de Betti attendus sont trouvés. Le résultat calculé est le pourcenatge de réponses correctes sur les 100 essais.\nBase d\u0027images : COIL-100\nCOIL-100 (Nene et al., 1996) est un corpus d\u0027images de 100 objets différents photographiés en rotation. Chaque image est prise après que l\u0027objet a été tourné de 5 degrés autour d\u0027un axe vertical, il y a donc 72 images par objet. Les images couleurs sont transformées en niveaux de gris et leur taille est réduite. Un objet est représenté par un nuage de 72 points dans l\u0027espace de dimension le nombre de pixels des images. Afin de pouvoir calculer le complexe de Delaunay, nous projetons les données sur les 5 premières composantes principales du nuage de points. Du fait de la rotation complète de l\u0027objet, ce nuage de points à la topologie d\u0027un anneau dont les nombres de Betti sont (1, 1, 0...). Nous testons le CSG sur 5 objets (oignon, flacon, tomate, boîte, chat décoratif ), et nous le lançons 10 fois pour chacun.\nResultats\nLa sphère et le tore\nOn peut trouver les résultats obtenus sur la sphère et le tore dans les tableaux 1 et 2. Pour les variances les plus faibles, WitC domine le CSG, mais ils sont tous les deux fiables. Pour ? \u003d 0.2, les performances de CSG diminuent alors que celles de WitC sont stables. Ici, la filtration avantage WitC : la cavité à l\u0027intérieur de la sphère persiste. Alors que pour le CSG, si la variance grandit trop, une corde à l\u0027intérieur de la sphère peut apparaître malgré les différentes étapes d\u0027élagage et ajouter des cycles non désirés dans le modèle.\nFIG. 1 -Exemple de 2000 points tirés uniformément à la surface du tore.\nFIG. 2 -Cinq objets sélectionnés dans la base de données COIL-100.\nLe tore est plus complexe à modéliser que la sphère : si la variance du bruit est trop grande, l\u0027intérieur du tore se remplit. Des cycles et des cavités parasites peuvent apparaître entrainant une baisse de performance que l\u0027on peut observer ici. Alors que les résultats sont toujours acceptables pour le CSG, ceux du WitC ne sont plus suffisants. Les erreurs du WitC se font en général sur le nombre de cycles, alors que le nombre de composantes connexes et de cavités sont corrects.\nTAB. 2 -Tore.\nCOIL-100\nLa rotation de la boîte, du flacon et du chat décoratif génère pour chacun de ces objets des images nettement différentes les unes des autres, et crée donc pour chaque objet un nuage de points de topologie annulaire. Le CSG retrouve systématiquement les nombres de Betti attendus dans ce cas (1, 1, 0...). En analysant la structure du complexe simplicial, on observe qu\u0027il est constitué de 11 sommets et 11 arêtes formant un cycle. La dimension maximale des simplexes vaut 1 montrant que la projection sur les 5 premières composantes principales n\u0027a pas fait perdre d\u0027information. Pour les deux autres objets, oignon et tomate, le CSG optimal contient seulement 5 sommets et tous les simplexes jusqu\u0027à la dimension 5, avec la signature topologique d\u0027une boule (1, 0, 0...) sans cycle ni cavité. Ce résultat peut s\u0027expliquer par le fait que ces deux objets sont plutôt invariants par rotation autour de l\u0027axe vertical, les 72 images associées à chacun sont donc très similaires et forment un nuage de points compact dans l\u0027espace des pixels, sans structure topologique complexe.\n"
  },
  {
    "id": "406",
    "text": "Introduction\nLa plupart des Systèmes de Recherche d\u0027Information (SRI) utilisent des termes simples pour indexer et retrouver des documents. Cependant, cette représentation n\u0027est pas assez pré-cise pour représenter le contenu des documents et des requêtes, du fait de l\u0027ambigüité des termes isolés de leur contexte : si l\u0027on considère le mot composé « pomme de terre », les mots simples pomme et terre ne gardent pas leur propre sens que dans l\u0027expression « pomme de terre » et si on les utilise séparément ils deviennent une source d\u0027ambigüité.\nUne solution à ce problème consiste à utiliser des termes complexes à la place des termes simples isolés (Boulaknadel, 2006). L\u0027hypothèse est que les termes complexes sont plus aptes à désigner des entités sémantiques que les mots simples et constituent alors une meilleure représentation du contenu sémantique des documents (Mitra et al., 1997).\nNotre objectif consiste à acquérir des termes complexes représentatifs du contenu informationnel du corpus. Les termes complexes extraits doivent représenter le contenu des textes sous une forme compréhensive par l\u0027ordinateur et riche en information. Ces termes extraits sont utilisés pour effectuer l\u0027indexation de corpus textuels, les termes d\u0027indexation sont alors plus complets et plus précis, ils permettent d\u0027atteindre une meilleure performance pour le SRI.\nLes termes complexes peuvent être sélectionnés statistiquement, linguistiquement ou en combinant les deux approches. Les techniques statistiques permettent de découvrir des séries de mots ou de combinaisons de mots qui occurrent fréquemment dans un corpus. Les techniques linguistiques visent à extraire les dépendances ou les relations entre les termes grâce aux phénomènes langagiers.\nDans (Haddad, 2002), l\u0027auteur fait l\u0027indexation des documents et des requêtes après l\u0027analyse linguistique et l\u0027extraction des syntagmes nominaux (SNs). Les résultats des expérimen-tations montrent que l\u0027intégration des SNs dans l\u0027indexation permet d\u0027obtenir de meilleures performances par rapport à l\u0027utilisation des unitermes.\nLe et Chevalet (Diem et Chevallet, 2006) utilisent une méthode d\u0027extraction de connaissances hybride qui fusionne l\u0027association entre les paires de termes extraits statistiquement avec les relations sémantique extraites linguistiquement. Les SNs sont organisés en réseaux de dépendance syntaxique (tête et expansion/ modificateur) en ajoutant les associations statistiques et sémantiques. L\u0027information sémantique est étudiée à travers les relations : synonymie, hyperonymie, causalité.\nLes auteurs dans (Woods et al., 2000;Haddad, 2003) ont montré que l\u0027indexation avec des SNs extraits linguistiquement affecte plus positivement les résultats d\u0027un SRI que celle avec des groupes de mots extraits statistiquement.\nMéthodologie suivie\nL\u0027approche hybride d\u0027extraction de connaissances montre son efficacité dans l\u0027augmentation de la performance des SRIs. Nous choisissons alors de combiner entre une approche linguistique basée sur l\u0027extraction des syntagmes nominaux (SNs) et sur un filtrage statistique basé sur l\u0027information mutuelle (IM) car cette mesure est adaptée aux termes rares (Daille, 1994;Thanopoulos et al., 2002) ce qui est le cas des SNs, pour la représentation du contenu textuel du corpus.\nExtraction des syntagmes nominaux\nNous effectuons, d\u0027abord l\u0027analyse linguistique avec un étiqueteur, qui génère une collection étiquetée. Ensuite, on utilise cette collection étiquetée et on en extrait un ensemble de SNs. Les syntagmes nominaux candidats sont extraits par repérage de patrons syntaxiques. Nous adoptons la définition des patrons syntaxiques dans (Haddad, 2002), où un patron syntaxique est une règle sur l\u0027ordre d\u0027enchainement des catégories grammaticales qui forment un SN :\n-V : le vocabulaire extrait du corpus -C : un ensemble de catégories lexicales -L : le lexique ? V × C Un patron syntaxique est une règle de la forme :\n..Y n Avec Yi ? C et X un syntagme nominal, exemples : SUBC ADJQ : « échelle planétaire », « relation diplomatique », etc. Nous nous basons dans nos travaux sur les 10 patrons syntaxiques les plus susceptibles de contenir le maximum d\u0027informations (Haddad, 2002), alors on ne va étudier que les SNs composés de deux ou de trois termes.\nSélection des meilleurs descripteurs\nBien que les SNs soient généralement pertinents, il peut être nécessaire de n\u0027en sélectionner que les « meilleurs » du corpus, pour cela nous utilisons un filtrage statistique qui consiste à employer une mesure statistique afin de leurs donner un score de qualité. Pour notre cas, ce filtrage statistique est effectué en calculant l\u0027information mutuelle (IM) entre les composants de chaque SN et en fixant différents seuils de cette mesure pour ne conserver que les SNs plus pertinent pour l\u0027indexation. La mesure de l\u0027information mutuelle consiste à comparer la probabilité d\u0027apparition des cooccurrences de mots (m1, m2) à la probabilité d\u0027apparition de ces mots séparément. Cette mesure est donnée par (Church et Hanks, 1990) :\nOù P est la probabilté. Dans notre cas l\u0027IM est utilisée pour détecter les syntagmes nominaux les plus pertinents pour les utiliser ensuite dans notre processus de RI. Donc pour les syntagmes composés de deux mots X :\u003d Y 1 Y 2 Avec Yi ? C et X un SN, l\u0027IM va être calculée de la façon suivante :\nPour les SNs composés de trois mots X :\u003d Y 1 Y 2 Y 3 , l\u0027IM va être calculée de la façon suivante : Si Y 2 est une préposition, alors :\nSinon\nOù P(Y i ) est une estimation de la probabilité d\u0027apparition du mot Y i qui est calculée à partir de la fréquence d\u0027apparition du mot Y i dans le document où il apparait, normalisée par N le nombre de mots contenu dans le même document. P(Y i , Y j ) est une estimation de la probabilité que les deux mots apparaissent ensemble dans le même document. Cette probabilité est estimée par la fréquence d\u0027apparition du couple (Y i , Y j ) divisé par N.\nExpérimentations et résultats\nPour tester notre approche hybride d\u0027extraction de SNs, nous avons utilisé le SRI Lemur 1 , le modèle vectoriel et la mesure de pondération tf.idf (Salton et Yang, 1973). Nous enlevons par la suite les mots vides lors de l\u0027indexation et cela avec le SRI Lemur. Pour évaluer notre approche d\u0027indexation, on se focalise sur la précision à faible taux de rappel et cela en étudiant la précision à 3, 5 et 10 documents (P_3, P_5, P_10), la précision à 11 point de rappel (11 pt_avg) et la F-mesure sont aussi étudiés.\nPour ces expérimentations le corpus initial sera noté Corpus-I, le corpus qui contient les SNs sera noté Corpus-SN et le corpus qui contient des SNs après filtrage sera noté Corpus-Fi\nRésultats du filtrage avec l\u0027IM\nSelon les expériences on a déterminé que le score de l\u0027information mutuelle pour les SNs varie entre -2 et 12. Nous varions alors S_Min et S_Max entre cet intervalle pour déterminer les meilleurs paramètres pour le filtrage statistique.\nAprès analyse du Tableau 3.1, nous remarquons que l\u0027approche de filtrage selon l\u0027IM a permis l\u0027amélioration de la P_3 lorsque S_Max\u003d 7, 8 et 9 avec une amélioration de 1.4% par rapport au Corpus-I. Nous remarquons aussi l\u0027amélioration de la P_5 10, et pour S_Min\u003d 2 et 6, la meilleure amélioration est de 2.1% pour S_Max\u003d 8 par rapport au Corpus-Fi. Pour la P_10, trois stratégies ont permis l\u0027amélioration de cette mesure avec S_Max\u003d 4, 6 et 7, la stratégie S_Max\u003d 6 a permis une amélioration de 0.2% par rapport au Corpus-I. L\u0027analyse de la 11 pt_avg, montre que cinq stratégies du Corpus-Fi ont permis l\u0027améliora-tion de cette mesure lorsque S_Max\u003d 2, 4, 6, 7, 8 et 10, la meilleure amélioration est obtenue pour S_Max\u003d 8, ce qui a permis d\u0027augmenter cette mesure de 4.6% par rapport au Corpus-Fi.\nPour la F-mesure le meilleur score est obtenu pour le Corpus-Fi lorsque S_Max\u003d 8 avec une augmentation de 2% par rapport au Corpus-I et de 3.2% par rapport au Corpus-SN.\nNous remarquons que pour la 11 pt_avg la meilleure performance est obtenue par le Corpus-Fi pour S_Max\u003d8, c\u0027est à dire que les SNs les plus pertinents ont un score d\u0027IM infé-rieur à 8. Nous remarquons aussi que les résultats obtenus par le Corpus-SN sont inférieurs à ceux obtenus par le Corpus-I, cela peut être expliquer par le fait que les SNs sélectionnés ne sont pas tous de bonne qualité.\nConclusion\nCet article présente notre méthode d\u0027indexation basée sur la sélection et le filtrage des SNs. Nous avons opté pour une méthode hybride d\u0027extraction des connaissances, qui combine à la fois l\u0027analyse linguistique fondée sur l\u0027extraction des SNs et l\u0027analyse statistique. Les SNs candidats sont extraits d\u0027un corpus étiqueté par repérage de patrons syntaxique. Nous procédons par la suite à un filtrage statistique basé sur l\u0027IM pour ne sélectionner que les SNs les plus pertinents.\nLes résultats ont montré que cette méthode permet d\u0027améliorer les performances de notre SRI et que la meilleure performance est obtenue pour le cas où S_Max\u003d 8, malgré que les SNs étaient de mauvaise qualité car la stratégie Corpus-SN qui utilise les SNs a dégradé les performances du SRI.\nPour les travaux futurs, c\u0027est le processus de filtrage qui sera mis en question, que ce soit pour le filtrage linguistique qui permettra d\u0027extraire des SNs de meilleurs qualités, que ce soit pour le filtrage statistique.\n"
  },
  {
    "id": "407",
    "text": "Introduction\nL\u0027extraction de règles d\u0027association, consistant à découvrir des associations entre les conjonctions de variables binaires (ou motifs) d\u0027une base de données, est une tâche importante en fouille de données. La recherche d\u0027algorithmes efficaces de telles règles a été un problème majeur de cette communauté. Depuis le célèbre algorithme Apriori (Agrawal et Srikant, 1994), il y a eu de nombreuses variantes et améliorations. L\u0027importance de l\u0027extraction des règles négatives fut mise en évidence par (Brin et al., 1997) qui indiquent que de la connaissance précieuse peut se cacher dans ces règles. Ainsi (Brin et al., 1997) utilisent le test du ? 2 pour déterminer la dépendance entre deux motifs et ensuite une mesure de corrélation afin de trouver la nature de cette dépendance (positive ou négative). (Savasere et al., 1998) combinent les motifs fréquents 1 positifs avec la connaissance du domaine afin de détecter les associations négatives. Cette approche est difficile à généraliser puisqu\u0027elle dépend de la connaissance du domaine. (Boulicaut et al., 2000) recherchent deux types de règles négatives, les règles du type X ? Y ? Z et X ? Y ? Z, et pour cela ils proposent une approche basée sur les contraintes. (Teng et al., 2002) proposent un algorithme détectant uniquement les règles négatives du type X ? Y . Quant à , (Antonie et Zaïane, 2004) et (Cornelis et al., 2006), ils extraient des règles négatives grâce à un algorithme basé sur l\u0027algorithme fondateur Apriori (Agrawal et Srikant, 1994). ) utilisent en plus du couple de mesures (support 2 , confiance 3 ), les deux mesures suivantes : une mesure d\u0027intérêt qui n\u0027est autre que la valeur absolue de la nouveauté (Lavrac et al., 1999) et une mesure nommée ratio incrément de la probabilité conditionnelle qui n\u0027est autre que la mesure de Shortliffe (Shortliffe, 1976). Quant à (Antonie et Zaïane, 2004), ils utilisent comme mesure supplémentaire, le coefficient de corrélation (Pearson, 1896). Nous nous sommes focalisés dans cet article sur les techniques basées sur l\u0027algorithme pionnier Apriori, et plus particulièrement sur les travaux de , (Antonie et Zaïane, 2004) et (Cornelis et al., 2006). A l\u0027issue d\u0027une étude approfondie de chacune des trois techniques, nous avons mis en évidence essentiellement les deux failles suivantes : (1) un nombre encore trop important de règles inintéressantes et (2) un parcours de recherche des règles non optimisé. Pour remédier au premier problème (nombre important de règles inintéressantes), nous retenons un sous-ensemble de motifs fréquents, les motifs raisonnablement fréquents, en éliminant ceux qui vont conduire à des règles non pertinentes c\u0027est-à-dire les règles éliminées par toute mesure d\u0027intérêt évaluant l\u0027écart à l\u0027indépendance de la règle comme par exemple la mesure de Piatetsky-Shapiro (Piatetsky-Shapiro, 1991). L\u0027avantage de ce choix est que l\u0027élimination intervient dans la première phase de l\u0027algorithme et non plus en deuxième phase (i.e. l\u0027extraction des règles) ou dans une phase de post-traitement des règles. De plus, nous utilisons également une mesure supplémentaire au couple de mesures (support, confiance) pour sélectionner les règles valides 4 , la mesure M G (Guillaume, 2010) qui est une amélioration de la mesure de Shortliffe (Shortliffe, 1976) utilisée par , et qui évalue non seulement l\u0027écart de la règle par rapport à l\u0027indépendance 5 mais également par rapport au point d\u0027équilibre 6 (Blanchard et al., 2005). L\u0027intérêt de prendre en compte le point d\u0027équilibre est développé dans (Blanchard et al., 2005). Cette mesure plus sélective que celle utilisée dans  va permettre d\u0027éliminer une nouvelle caté-gorie de règles inintéressantes. Pour remédier au deuxième problème (parcours de recherche des règles non optimisé), nous démontrons que seulement la moitié des règles négatives potentiellement valides sont à étudier, et ceci en fonction de la valeur de la confiance de la règle positive par rapport au support de la conclusion. De plus, parmi les 4 règles à étudier, nous avons de nouveau utilisé la propriété d\u0027anti-monotonicité de la confiance 7 , propriété abandonnée par (Antonie et Zaïane, 2004) et , à laquelle nous en avons ajouté une nouvelle dégagée par (Guillaume et Papon, 2012) et qui repose sur la mesure que nous allons utiliser, la mesure M G . L\u0027article s\u0027organise donc de la façon suivante. La section 2 présente et motive les choix retenus pour optimiser l\u0027extraction des règles d\u0027association positives et négatives. La section 3 développe l\u0027algorithme proposé et la section 4 évalue notre technique sur plusieurs bases de données. L\u0027article se termine par une conclusion et des perspectives. \n6. Le point d\u0027équilibre est le cas où lorsque X est réalisé, il y a autant de chances de voir se réaliser Y que Y , ainsi nous avons les relations suivantes : \nsup(M2) ? 1 sans pour autant être pertinentes comme le montre la valeur de la nouveauté (Lavrac et al., 1999)  \nParcours optimisé pour la recherche des règles valides\nAucune technique d\u0027élagage pour le parcours des règles n\u0027est utilisée par (Antonie et Zaïane, 2004), (Cornelis et al., 2006) et . En effet la propriété d\u0027antimonotonicité de la confiance n\u0027est valable que pour les règles positives. Cependant, il est possible de restreindre et de diviser par 2 le nombre de règles négatives à étudier en fonction (1) soit du signe de la nouveauté ; (2) soit de la réponse à la question suivante \"la réalisation de la prémisse augmente-t-elle les chances d\u0027apparition de la conclusion ?\". La réponse à cette question peut être obtenue grâce à la nouveauté puisque sup\n8. On entend par motif omniprésent, un motif ayant une très forte valeur pour son support.\n] ou grâce à la mesure de Shortliffe, propriété non exploitée par ) malgré une utilisation de cette mesure. Nous explicitons cette restriction du nombre de règles négatives à évaluer grâce aux liens suivants entre les différentes règles.\nNous avons le lien suivant entre les règles antinomiques X ? Y et X ? Y : si la règle X ? Y est potentiellement intéressante c\u0027est-à-dire si la réalisation de X augmente les chances d\u0027apparition de Y (question précédente) ou encore si la confiance de la règle est supérieure à la probabilité d\u0027apparition du motif conclusion Y (autrement dit si conf (X ? Y ) \u003e P (Y ) ), alors la règle antinomique X ? Y ne pourra pas être intéres-sante puisque dans ce cas-là, la confiance de la règle antinomique est inférieure à la proba-\n9 . De la même façon, nous avons le lien suivant entre les règles\n10 . Pour finir, nous avons le lien suivant entre les règles symétriques\n11 . De ces trois liaisons précédemment établies entre les règles, nous pouvons en déduire que si la règle X ? Y est potentiellement intéressante alors les règles  (Antonie et Zaïane, 2004) considèrent ensuite le couple de motifs (Y, X) et refont le calcul du coefficient de corrélation\n10.\nNous allons utiliser ce résultat des liaisons d\u0027intérêt entre les règles négatives afin de diminuer le nombre de règles à évaluer en le divisant par 2. Pour cela, nous devons savoir si la confiance de la règle X ? Y est supérieure au support de la conclusion (c\u0027est-à-dire si conf (X ? Y ) \u003e sup(Y )), ce qui nous garantit d\u0027obtenir une règle potentiellement intéressante.  vérifient également le potentiel intérêt des règles grâce à la valeur absolue de la nouveauté. Autrement dit, avant de tester la validité des règles au regard du support et de la confiance,  \nAinsi, la recherche de l\u0027appartenance de la règle à cette zone où conf (X ? Y ) \u003e sup(Y ) avant de tester la contrainte de la confiance nous assure d\u0027éliminer une partie des règles inintéressantes. C\u0027est ce que nous allons retenir dans notre proposition pour répondre non seulement à une exigence de rapidité d\u0027exécution de l\u0027algorithme (moins de règles à évaluer puisque nous divisons ce nombre par 2) mais également au problème du nombre important de règles restituées, règles pas toujours pertinentes (les règles inintéressantes qui sont éliminées sont celles où la prémisse n\u0027augmente pas les chances d\u0027apparition de la conclusion). Cependant cette zone où les règles sont potentiellement intéressantes est encore trop importante et peut générer encore des règles inintéressantes. C\u0027est le cas où la confiance de la règle est bien supérieure au support de la conclusion mais également inférieure au point d\u0027équilibre (Blanchard et al., 2005)  \n) . Maintenant, nous devons optimiser le parcours des règles dans le cas répulsif et par conséquent, utiliser une méta-règle permettant de passer de la règle X ? Y à la règle X ? Y . Comme (Guillaume et Papon, 2012) ont dégagé des méta-règles uniquement à partir des règles positives X ? Y , pour pouvoir faire cette transition des règles X ? Y aux règles X ? Y , nous allons utiliser la méta-règle permettant de passer de la règle X ? Y à la règle X ? Y , et par conséquent, celle que nous venons de décrire précédemment. Nous résumons les deux méta-règles qui vont être utilisées pour optimiser la recherche des règles :\nPour finir, les algorithmes existants reposant sur le couple (support, confiance) extraient des règles du type Lors de la recherche des motifs raisonnablement fréquents, nous allons rechercher en même temps ces conjonctions de motifs négatifs, motifs que nous noterons¨Xnoterons¨ noterons¨X. Cette recherche simultanée va renforcer notre souhait d\u0027extraire des règles les plus pertinentes possibles. En effet, la contrainte supplémentaire suivante sup( ¨ X) ? min¨supmin¨ min¨sup sur les motifs X impose, comme pour la deuxième contrainte des motifs raisonnablement fréquents (à savoir sup(X) ? max sup ), d\u0027être en présence de motifs X non omniprésents. Pour un seuil d\u0027exigence identique (c\u0027est-à-dire min sup \u003d min¨supmin¨ min¨sup et max sup \u003d 1 ? min sup ), cette nouvelle contrainte est plus restrictive que la deuxième contrainte (à savoir sup(X) ? max sup ) puisque si nous avons sup(X) ? max sup , alors nous avons les équivalences suivantes : \nFIG. 1 -Exemple de règles où les motifs ont des supports relativement élevés (courbe de gauche) et où les motifs ont des supports proches du seuil minimal min sup (courbe de droite).\nLa contingence des ensembles X ei?BD 14 , Y ei?BD et (X ei?BD ? Y ei?BD ) est la même pour les deux courbes sauf pour l\u0027ensemble (X ei?BD ? Y ei?BD ) qui est plus faible pour la courbe de gauche. Comme la contingence des ensembles X ei?BD et (X ei?BD ? Y ei?BD ) est la même dans les deux cas de figure, les deux règles X ? Y associées à ces deux contingences ont la même valeur pour la confiance. Cependant la règle associée à la courbe de droite de la Figure 1 est plus pertinente que celle de la courbe de gauche puisque la probabilité d\u0027avoir une intersection aussi importante entre X ei?BD et Y ei?BD est plus faible que pour le cas de la courbe de gauche. Nous savons que la confiance ne peut pas discerner ces deux types de règles et l\u0027ajout de cette nouvelle contrainte sur les motifs¨Xmotifs¨ motifs¨X nous assure d\u0027éliminer un certain type de règles non pertinentes. Nous n\u0027ajouterons pas, comme pour les motifs positifs, une valeur maximale à ne pas dépasser sur les supports des motifs¨Xmotifs¨ motifs¨X car elle est en partie présente avec la contrainte du support minimum sur les motifs positifs. Nous présentons maintenant notre algorithme.\nAlgorithme\nTout d\u0027abord, nous définissons ce que nous entendons par règle valide et donc les 6 contraintes Ct 1 à Ct 6 que doivent vérifier les règles. \nelse if attraction négative then 10: \n, et telle que Ct 6 : C 1 ? C 2 est minimal au regard des motifs négatifs raisonnablement fréquents X ou Y . La contrainte Ct 6 est celle présente dans (Cornelis et al., 2006) où les motifs C 1 et C 2 lorsqu\u0027ils sont des motifs raisonnablement fréquents négatifs X et Y , doivent également être minimaux c\u0027est-à-dire qu\u0027il n\u0027existe pas par exemple pour le motif X un sous-ensemble X ? ? X tel que X ? soit également raisonnablement fréquent.\nL\u0027algorithme d\u0027extraction des RAPN (voir l\u0027algorithme 1) commence par rechercher les motifs raisonnablement fréquents grâce à la fonction funct_RF (ligne 1). Cette recherche est similaire à celle utilisée par (Agrawal et Srikant, 1994) pour générer les motifs fréquents en rajoutant deux contraintes supplémentaires : un seuil maximal max sup qui ne doit pas être dépassé par le support de X et un seuil minimum min¨supmin¨ min¨sup pour le support des motifs¨Xmotifs¨ motifs¨X, ce qui permet de vérifier les contraintes Ct 1 et Ct 2 des règles valides définies dans cette même section. A partir des motifs raisonnablement fréquents, on va rechercher les motifs négatifs raisonnablement fréquents minimaux grâce à la fonction func_NRFM (ligne 2). Cette recherche sert ensuite à s\u0027assurer que la règle vérifie la contrainte Ct 6 . Cette fonction est similaire à celle exposée dans (Cornelis et al., 2006) en rajoutant la contrainte du support maximum (i.e. \nExpérimentations\nLes quatre algorithmes ont été développés en Java et incorporés au logiciel libre WEKA (Waikato Environment for Knowledge Analysis) (Witten et Frank, 2005). Les expérimentations ont été effectuées sur les 4 bases de données UCI KDD (Hettich et Bay, 1999)  . Tout d\u0027abord, nous avons effectué une étude comparative des 4 algorithmes sur la base de données Abalone dont les résultats sont résumés dans la figure 2 et où nous avons fait varier les valeurs du seuil minimum pour le support et la confiance comme indiqué dans les deux premières colonnes du tableau. Pour chacun des algorithmes, nous avons restitué le temps d\u0027exécution total en secondes (colonne Temps) et le nombre total de règles négatives extraites (colonne # Négatives). Par manque de place, nous n\u0027avons pas fait resortir le nombre de règles positives. De plus pour notre algorithme, nous avons restitué dans la dernière colonne (colonne # Nouvelles R.) le nombre de règles extraites du typë X ? ¨ Y , règles non présentes dans les 3 algorithmes existants. Pour finir, nous avons retenu comme seuil minimum pour le coefficient de corrélation nécessaire pour l\u0027algorithme de (Antonie et Zaïane, 2004), la valeur 0,60 et la valeur 0,10 pour la mesure d\u0027intérêt utilisée dans . La mesure de Shortliffe utilisée dans ) a le même seuil que celui de la confiance. Quant à notre algorithme, nous avons retenu les valeurs suivantes pour les différents seuils : max sup \u003d 0, 80, min¨supmin¨ min¨sup \u003d min sup et min MG \u003d 0, 60.\nFIG. 2 -Etude comparative des 4 algorithmes sur la base Abalone.\nNous remarquons que c\u0027est notre algorithme qui restitue, pour tous les cas de figure de cette base Abalone, le nombre le plus faible de règles négatives. Nous observons également que le nombre de règles du typë X ? ¨ Y restituées par uniquement notre algorithme est conséquent mais reste globalement inférieur au nombre de règles restituées par (Cornelis et al., 2006). Quant au temps d\u0027extraction, notre algorithme arrive en première place suivi de (Cornelis et al., 2006), (Antonie et Zaïane, 2004) et .\nFIG. 3 -Résultats de notre algorithme sur les 4 bases de données.\nLa deuxième étude réalisée s\u0027est concentrée sur notre algorithme et nous avons souhaité connaître les temps d\u0027extraction (colonne Temps) et le nombre de règles négatives (colonne # Négatif ) extraites sur différentes bases de données UCI, et cela pour différents seuils pour le support et la confiance comme indiqué dans la figure 3. Les autres seuils nécessaires pour notre algorithme sont les mêmes que pour l\u0027étude précédente. Nous constatons des temps d\u0027extraction raisonnables et le nombre de règles négatives extraites est raisonnable en général sauf pour ce nouveau type de règles¨Xrègles¨ règles¨X ? ¨ Y où une étude complémentaire est nécessaire afin de ne retenir que les plus pertinentes.\nConclusion\nDans cet article, nous avons proposé un algorithme d\u0027extraction de RAPN optimisé par rapport à ceux présents dans la littérature et reposant sur l\u0027algorithme fondateur Apriori. Les deux optimisations ont porté sur une diminution du nombre de règles et sur un parcours optimisé de recherche des règles valides. La diminution du nombre de règles a été rendue possible en éliminant certains motifs fréquents qui ne pouvaient pas conduire à des règles intéressantes car ayant soit une valeur pour la confiance trop faible, soit un écart à l\u0027indépendance trop faible.\n"
  },
  {
    "id": "408",
    "text": "Introduction\nDe nombreux acteurs de l\u0027informatique doivent faire face à l\u0027arrivée massive de données. Les plus connus sont Google et Yahoo avec le traitement des logs pour la publicité en-ligne, Facebook et Twitter qui modélisent les données provenant de leurs centaines de millions d\u0027utilisateurs, les opérateurs téléphoniques pour la gestion de réseaux de télécommunications. La volumétrie de ces données continue de croître rapidement et les quantités ne sont plus compatibles avec l\u0027utilisation de la plupart des méthodes hors-ligne qui supposent de pouvoir accéder à toutes les données. Dans ces conditions, il est préférable de traiter les données à leur passage ce qui impose d\u0027y accéder une seule fois et dans leur ordre d\u0027arrivée. On parle alors d\u0027un accès sous la forme d\u0027un flux de données.\nEn classification supervisée, on appelle concept P (C|X) la probabilité conditionnelle de la classe C connaissant les données X. Les flux de données peuvent ne pas être stationnaires et comporter des changements de concept si le processus qui génère les données varie au cours du temps. Dans ce cas le modèle de classification supervisée doit être adapté au fur et à mesure que le concept change.\nCet article propose une nouvelle méthode de détection de changement basée sur l\u0027observation des données du flux. Notre méthode utilise deux fenêtres et permet d\u0027identifier si les données de ces deux fenêtres proviennent ou non de la même distribution. Elle est capable de détecter les changements de diverses natures (moyenne, variance...) sur la distribution des données conditionnellement ou non aux classes. Notre méthode a l\u0027intérêt de n\u0027avoir aucun a priori sur la distribution des données, ni sur le type de changement. De plus, à part la taille de la fenêtre, elle ne requiert aucun paramètre utilisateur.\nCet article présente tout d\u0027abord, dans la section 2, les approches existantes de l\u0027état de l\u0027art auxquelles l\u0027approche proposée sera comparée. La section 3 présente notre méthode de détec-tion ainsi qu\u0027une validation expérimentale sur des données artificielles. La section 4 montre comment notre méthode de détection peut être utilisée au sein d\u0027une méthode de gestion de la dérive de concept. La dernière partie conclut cet article.\n2 Etat de l\u0027art des méthodes de détection L\u0027état de l\u0027art sur la gestion de la dérive de concept est abondant (Bifet et al., 2009;Žlio-baite, 2010). Les méthodes de gestion de la dérive peuvent se diviser en plusieurs familles : détection de changement, ensemble de classifieurs, pondération des données selon leur ancienneté... Le but de cet article étant de présenter une nouvelle méthode de détection, l\u0027état de l\u0027art présenté ici se focalise donc sur la famille des méthodes de détection de changement.\nLe but de la classification supervisée sur flux de données est d\u0027optimiser les performances du classifieur. Dans ce cas, l\u0027état de l\u0027art peut encore être divisé en deux familles :\n-méthodes sans classifieur : les auteurs s\u0027intéressent directement aux distributions d\u0027inté-rêt du flux de données : P (X), P (C), P (X|C) -méthodes avec classifieur : les auteurs s\u0027intéressent à la performance, liée à P (C|X), d\u0027un classifieur. D\u0027un point de vue bayésien cela revient à détecter les variations d\u0027une des quantités de la formule suivante : P (C|X) \u003d (P (C)P (X|C)) /P (X) avec :\n-P (C) la proportion des classes dans les données -P (X) la probabilité des données -P (X|C) la probabilité conditionnelle des X connaissant la classe C. A notre connaissance il n\u0027existe pas de méthodes permettant de détecter directement les variations dans la distribution jointe P (X, C).\nLa méthode proposée dans cet article ayant vocation à détecter les changements sur les trois termes mentionnés ci-dessus, on présente dans les sous sections suivantes une brève description des méthodes de l\u0027état de l\u0027art testées de façon comparative dans cet article.\nMéthodes sans classifieur\nLes méthodes sans classifieur sont essentiellement des méthodes basées sur des tests statistiques qui sont utilisés pour détecter un changement entre différentes fenêtres d\u0027observations. \n. On utilise le test t de Welch pour tester l\u0027hypothèse nulle suivante : « les moyennes de deux populations sont égales ». Ce test retourne une p-value qui permet de rejeter ou non l\u0027hypothèse nulle.\nTest de Kolmogorov-Smirnov : Le test d\u0027hypothèse de Kolmogorov-Smirnov est utilisé pour déterminer si un échantillon suit bien une loi donnée ou bien si deux échantillons suivent la même loi. Ce test est basé sur les propriétés des fonctions de répartition empirique. Nous utiliserons ce test pour vérifier si deux échantillons suivent la même loi. Soient deux échan-tillons de tailles N 1 et N 2 possédant respectivement les fonctions de répartition empirique F 1 (x) et F 2 (x). La distance de Kolmogorov-Smirnov est définie de la manière suivante :\nL\u0027hypothèse nulle, stipulant que les deux échantillons proviennent de la même distribution, est rejetée avec une confiance ? si :\nK ? se retrouve à l\u0027aide des tables de Kolmogorov-Smirnov.\nCette méthode a été proposée par (Bondu et Boullé, 2011)   (Boullé, 2006) appliquée à ces exemples. Si la variable X i est discrétisée en au moins 2 intervalles cela signifie qu\u0027il y a au moins 2 zones où la distribution des exemples conditionnellement à la fenêtre W est significativement différente. Dans ce cas la méthode conclut qu\u0027un changement s\u0027est produit.\nMéthodes avec classifieur\nLes méthodes avec classifieur observent les performances du classifieur et détectent un changement quand les performances varient de manière significative. Ces méthodes ont comme hypothèses de départ que le classifieur est un processus stationnaire et que les données sont indépendantes et identiquement distribuées (iid). Bien que ces hypothèses ne soient pas toujours validées, ces méthodes ont prouvé leur intérêt sur diverses expérimentations (Gama et al., 2004;Baena-García et al., 2006;Bifet et al., 2009). Les deux principales méthodes avec classifieur référencées dans l\u0027état de l\u0027art sont décrites ci-dessous. DDM : La méthode DDM, proposée par Gama et al. (Gama et al., 2004), détecte les changements en observant l\u0027évolution du taux d\u0027erreur du classifieur. L\u0027algorithme prend en entrée une distribution binomiale provenant de la variable binaire qui indique si l\u0027exemple est bien classé (0) ou mal classé (1) par le classifieur. Cette loi binomiale est approximée par une loi normale après avoir vu 30 exemples. La méthode estime pour chaque exemple la probabilité qu\u0027il soit mal classé \nPour les expérimentations les paramètres sont fixés à ? \u003d 90% et ? \u003d 95%. Sur des jeux de données synthétiques et réels EDDM détecte plus rapidement que DDM les changements graduels.\n3 Une nouvelle méthode de détection supervisée\nPrésentation\nLa méthode proposée dans cet article pour détecter les changements de concept s\u0027inspire de la méthode MODL P (W |X i ) (Bondu et Boullé, 2011) présentée dans la section 2.1. Le flux de données est constitué de d variables explicatives (X i , i ? {1, ..., d}). Notre méthode fait l\u0027hypothèse d\u0027indépendance des variables conditionnellement aux classes. Le test de changement est donc réalisé par variable X i sur les données provenant de deux fenêtres. La première fenêtre W ref contient les données du concept de départ. La deuxième W cur est une fenêtre glissante/sautante sur le flux qui permet de capturer les données d\u0027un éventuel nouveau concept. Le choix de la taille des fenêtres est un compromis entre la réactivité aux changements et le nombre de mauvaises détections. Une grande taille de fenêtre permet de détecter avec plus de confiance des motifs potentiellement plus complexes. Une plus petite taille permet d\u0027être plus réactif. Fixer cette taille dépend du flux observé et du type de changements que l\u0027on veut dé-tecter. De manière générale on peut traiter ce problème en utilisant plusieurs tailles de fenêtre en parallèle.\nLes données sont étiquetées par fenêtre : W ? {W ref , W cur }. Notre intérêt porte sur la détection de la dérive du concept dans le cadre de la classification supervisée. Par conséquent on s\u0027intéresse à la probabilité de la fenêtre connaissant à la fois la classe C et la variable\nParmi les méthode de l\u0027état l\u0027art de discrétisation/groupage bivarié capables de prendre en compte la variable X i et la classe C nous avons choisi d\u0027utiliser l\u0027approche MODL (Boullé, 2009) pour ses caractéristiques : pas d\u0027a priori sur la distribution des données, faible sensibilité aux valeurs atypiques, pas de paramètres utilisateur, régularisation pour éviter le surapprentissage.\nUn changement est détecté quand la variable à expliquer (dans notre cas W ) peut être discriminée à partir de l\u0027observation de X i et C. Ceci se traduit dans l\u0027approche MODL bivariée par l\u0027observation d\u0027une grille de discrétisation/groupage de plus d\u0027une cellule. Si la grille n\u0027a qu\u0027une seule cellule, alors la distribution des données n\u0027a pas changé entre les deux fenêtres.\nLa complexité algorithmique de cette méthode est en O(T ? T log(T )) où T est la somme de la taille des deux fenêtres (référence et courante).\nValidation expérimentale : détection sans classifieur\nDans la suite de cette section on s\u0027intéresse au comportement de la méthode proposée sur des données artificielles provenant de la simulation de différents types de changement. Le comportement dans le cas stationnaire est étudié dans la section 3.2.1, la capacité à détecter un changement dans la section 3.2.2 et la capacité à détecter différents types de changement dans la section 3.2.3. Les expérimentations comparent les méthodes de l\u0027état de l\u0027art présentées précédemment ainsi que notre approche : -test t de Welch avec signification statistique de 1%, 5% et 10% -test de Kolmogorov-Smirnov (KS) avec signification statistique de 1%, 5% et 10% -méthode supervisée MODL P (W |X i ) -méthode supervisée bivariée MODL P (W |X i , C)\nComportement dans le cas stationnaire\nLe but de cette première expérimentation est d\u0027étudier le comportement des méthodes dans le cadre d\u0027un flux stationnaire où l\u0027on ne doit détecter aucun changement. Les fenêtres de ré-férence et courante ont la même taille. Etant donné que la méthode fait l\u0027hypothèse d\u0027indépen-dance des variables, une seule variable numérique X a est utilisée pour la validation expérimen-tale. La distribution des classes se fait selon une gaussienne de paramètres (µ 1 \u003d ?1, ? 1 \u003d 1) pour la classe 1 et une gaussienne de paramètres (µ 2 \u003d 1, ? 2 \u003d 1) pour la classe 2. Diffé-rentes tailles de fenêtre ont été choisies entre 10 et 5 000 exemples. Les expérimentations sont réalisées 1000 fois.\nLes résultats sont présentés dans le tableau 1. Ceux-ci confirment la robustesse de la méthode bivariée MODL car aucune détection n\u0027est observée sur P (W |X a , C). Par contre quelques fausses alarmes ont lieu sur P (W |X a ) (Bondu et Boullé, 2011)   TAB. 1 -Nombre de mauvaises détections selon la méthode détection et la taille de la fenêtre pour 1000 expérimentations.\nGrille bivariée pour la détection de changement\nCapacité à détecter un changement\nLe but de cette expérimentation est d\u0027observer le temps nécessaire (en nombre d\u0027exemples) pour détecter un changement en fonction de la taille de la fenêtre et des méthodes. La plupart des méthodes sont capable de détecter ce type de changement. Le point important est ici d\u0027analyser la vitesse de détection.\nLes fenêtres de référence et courante ont la même taille. Une seule variable X a est utilisée. Le concept 1 est défini ainsi :\n-la classe 0 suit une distribution N 0 (µ \u003d ?1, ? \u003d 1) ; -la classe 1 suit une distribution N 1 (µ \u003d 1, ? \u003d 1). On simule ce changement en changeant la moyenne de la classe 0, qui passe de -1 à 2 et sa variance qui passe de 1 à 0,5. Pour la classe 1, seule la moyenne change en passant de 1 à 0. On obtient donc le concept 2 suivant :\n-la classe 0 suit une distribution N 0 (µ \u003d 2, ? \u003d 0, 5) ; -la classe 1 suit une distribution N 1 (µ \u003d 0, ? \u003d 1). Différentes tailles de fenêtres ont été testées entre 10 et 5 000 exemples et les expérimentations sont réalisées 1000 fois. La position du changement dans la fenêtre est tirée de manière aléa-toire car dans un cas réel la position du changement n\u0027est pas connue. Les résultats obtenus correspondent aux délais moyens de détection du changement en fonction de la taille de la fenêtre et des méthodes étudiées.\nLes résultats sont présentés dans le tableau 2. On observe que jusqu\u0027à une taille de fenêtre de 100 exemples la détection est difficile dans la fenêtre où a lieu le changement. A partir d\u0027une taille de 200 exemples, le délai moyen est inférieur à la taille de la fenêtre et ce pour toutes les méthodes. L\u0027augmentation du seuil de signification à 5% et 10% entraine, pour les tests de Welch et de Kolmogorov-Smirnov, une baisse du délai de détection. La méthode basée sur MODL P (W |X a ) est légèrement plus longue à détecter que les méthodes paramétriques à 1%. Cependant, si on prend notre méthode MODL P (W |X a , C) alors celle-ci est meilleure que les deux autres méthodes paramétrées à 1%. 15  14  14  15  15  15  15  15  20  29  26  24  29  28  26  28  29  30  41  36  33  43  40  36  41  40  50  62  53  49  67  57  54  65  58  100  103  90  86  110  96  90  109  96  200  178  160  150  185  166  160  195  163  300  241  218  211  252  227  214  269  218  500  367  337  321  375  344  330  404  325  1000  665  620  598  678  636  613  719  600  2000  1224  1188  1154  1260  1188  1174  1334  1120  5000  2886  2766  2741  2911  2781  2756  3081  2671 TAB. 2 -Délai moyen de détection d\u0027un changement selon la méthode de détection et la taille de la fenêtre pour 1000 expérimentations.\nDifférents types de changements détectés\nLes expériences de cette section ont pour but d\u0027observer quels types de changement (moyenne, variance, inversion des classes) les différentes méthodes sont capables de détecter. Pour tous les types de changement testés, le concept de départ est le concept 1 défini ainsi : la classe 0 suit une distribution N 0 (µ \u003d 0, ? \u003d 0, 5) et la classe 1 suit une distribution N 1 (µ \u003d 2, ? \u003d 1). Différents changements sont appliqués au concept 1 pour expérimenter le comportement des différentes méthodes.\n-Changement de moyenne. On simule ce changement en changeant la moyenne de la classe 0, qui passe de 0 à 1. On obtient le concept 2 : la classe 0 suit une distribution N 0 (µ \u003d 1, ? \u003d 0, 5) et la classe 1 suit une distribution N 1 (µ \u003d 2, ? \u003d 1). -Changement de variance. On simule ce changement en changeant la variance de la classe 0, qui passe de 1 à 0,5. On obtient le concept 2 : la classe 0 suit une distribution N 0 (µ \u003d 0, ? \u003d 1) et la classe 1 suit une distribution N 1 (µ \u003d 2, ? \u003d 1). -Inversion des classes. On simule ce changement en inversant les étiquettes des classes du concept 1. Pour ces expérimentations et pour les différents types de changement, les tailles des fenêtres ont été fixées à 1000. La fenêtre de référence contient le concept 1 et la fenêtre courante le concept 2. Le tableau 3 présente les résultats en termes de nombre de détections pour 1000 expérimentations. Les meilleures méthodes sont celles qui sont capables de réaliser le plus de détections. TAB. 3 -Nombre de détections par changement pour les différentes méthodes de détection.\nToutes les méthodes sont capables de détecter de manière fiable un changement dans la moyenne. Le test de Welch n\u0027est pas capable de détecter les changements de variance et ces expériences le confirment. Quelques détections (19) pour le test à 10% se produisent, mais celles-ci correspondent à des fausses détections et non pas à une détection de changement de variance. Le test de Kolmogorov-Smirnov et la détection MODL P (W |X a ) se comportent très bien pour les détections dans les changements de moyenne et variance. Tous les tests précé-dents détectent seulement des changements dans la répartition des données mais sans s\u0027inté-resser à la classe. Par conséquent aucun d\u0027entre eux n\u0027est capable de détecter un changement qui n\u0027intervient que par rapport aux classes. Contrairement à ces tests statistiques, la méthode de détection bivariée MODL P (W |X a , C) est capable de détecter ce genre de changements. Les expérimentations confirment cette capacité.\nApplication à la gestion de la dérive de concept\nDétecter les changements requiert ensuite de réagir à ces derniers. Si l\u0027on sait détecter les changements de concepts au cours du temps on pourra alors (Žliobaite, 2010) : (i) soit réapprendre le modèle de classification à partir de zéro ; (ii) soit adapter le modèle courant ; (iii) soit adapter un résumé des données sur lequel se fonde le modèle courant ; (iv) soit travailler avec la séquence des modèles de classification appris au cours du temps. Cette section propose d\u0027intégrer la méthode de détection précédente dans un algorithme de gestion de la dérive de concept afin de remplacer le classifieur après une détection de changement.\nAlgorithme MDD\nNous proposons d\u0027intégrer notre méthode de détection à un nouvel algorithme que nous appelons MDD : MODL Drift Detection Method (Algorithme 1). Notre algorithme n\u0027utilise pas les performances du classifieur pour détecter les changements contrairement aux algorithmes DDM et EDDM décrits dans la section 2.2. Il ne dépend donc pas du type de classifieur.\nLe remplacement de l\u0027ancien classifieur par un nouveau se fait suite à une détection sur au moins l\u0027une des variables par la méthode bivariée MODL P (W |X i , C). Le remplacement n\u0027est effectif que lorsque le taux d\u0027erreur du nouveau classifieur est plus faible que l\u0027ancien. Ce taux d\u0027erreur est calculé, pour nos expérimentations, à l\u0027aide de la méthode tauxErreur qui correspond à une moyenne mobile exponentielle de paramètre ? \u003d 1/tailleW cur . Le paramètre n min est le nombre minimum d\u0027exemples utilisé pour comparer les performances des deux classifieurs, sa valeur est fixée à 30 (n min \u003d 30) pour toutes les expérimentations. Cette même valeur est utilisée par les méthodes DDM et EDDM avant qu\u0027elles ne commencent à chercher un changement.\nProtocole expérimental\nNotre algorithme est configuré avec une même taille de fenêtre de 1000 pour la fenêtre de référence et la fenêtre sautante. La problématique du réglage de la taille de la fenêtre n\u0027est pas abordée en détail dans cet article. Une grande taille de fenêtre permet de détecter avec plus de confiance ainsi que des motifs plus complexes. Une plus petite taille permet d\u0027être plus réactif. Fixer cette taille dépend du flux observé et des changements que l\u0027on veut détecter. De manière générale on peut traiter ce problème en utilisant plusieurs tailles de fenêtre en parallèle comme le propose (Lazarescu et al., 2004).\nOn peut Deux types de classifieur sont utilisés : -un classifieur bayésien naïf utilisant une estimation de densité conditionnelle des classes basée sur des résumés à deux niveaux (Salperwyck et Lemaire, 2012). Le premier niveau est un résumé de quantiles à 100 tuples et le second niveau la discrétisation MODL. -un arbre de Hoeffding (Domingos et Hulten, 2000) avec un résumé de quantiles à 10 tuples et un classifieur bayésien naïf utilisant la discrétisation MODL. On présente les résultats pour le générateur basé sur un hyperplan en mouvement proposée dans (Hulten et al., 2001). Ce générateur est configuré avec 10 attributs, une vitesse de changement de 10 ?3 et 10% de bruit de classe.\nUne méthode de l\u0027état de l\u0027art pour l\u0027évaluation en-ligne des classifieurs (Gama et al., 2009) est utilisée. La méthode choisie mesure la performance en utilisant les exemples du flux comme données de test avant qu\u0027ils ne soient appris. La mesure utilisée est la précision moyenne du classifieur entre le début du flux et l\u0027instant t.  Algorithme 1: Notre algorithme MDD (MODL Drift Detection) de remplacement du nouveau classifieur après changement de concept.\nRésultats\nLa figure 1 présente les résultats pour le jeu de données basé sur un « hyperplan en mouvement ». Sur cette figure toutes les méthodes permettent au classifieur d\u0027avoir les mêmes performances respectivement pour le naïf Bayes et l\u0027arbre de décision jusqu\u0027à 100 000 et 200 000 exemples appris. Pour le classifieur bayésien naïf, entre 100 000 et 200 000 exemples DDM devient meilleur, notre méthode MDD reste relativement stable et EDDM bien moins bon. Après 300 000 exemples alors que MDD reste stable, DDM devient bien moins bon et EDDM meilleur, mais moins bon que notre méthode. On observe que notre méthode de dé-tection est meilleure et beaucoup plus stable que les deux autres méthodes. Pour l\u0027arbre de décision, entre 200 000 et 600 000 exemples DDM devient meilleur, la méthode de détection MDD reste relativement stable et EDDM bien moins bon. Après 600 000 exemples alors que MDD continue à s\u0027améliorer, DDM devient bien moins bon et EDDM un peu meilleur. Comme pour le classifieur bayésien naïf, on observe que notre méthode de détection est bien meilleure et beaucoup plus stable que les deux autres méthodes sur ce jeu de données. La figure 2 présente le nombre de détections et d\u0027alertes des différents méthodes. On observe que le comportement de DDM et EDDM n\u0027est pas stable bien que la vitesse de changement (la vitesse de rotation de l\u0027hyperplan) soit constante. Ces méthodes ont tendance à détecter beaucoup de changements sur certaines périodes et aucun sur d\u0027autres. Leurs hypothèses de départ (processus stationnaire et données iid), ne sont sans doute pas valides, ce qui aboutit à un manque de robustesse. Notre méthode de détection n\u0027utilise pas le classifieur pour la détection mais seulement les données du flux. On observe que son nombre de détections est relativement régulier pour un changement ayant une vitesse constante. \nConclusion\nCet article présente une nouvelle méthode de détection de changement dans les flux de données. Celle-ci est basée sur l\u0027observation du changement des distributions des exemples dans deux fenêtres. Notre méthode n\u0027a pas d\u0027a priori sur la distribution des données ni sur le type de changement à détecter. Elle est capable de détecter des changements rapides ou lents, que cela soit sur la moyenne, la variance ou tout autre changement dans la distribution. Notre méthode fait l\u0027hypothèse d\u0027indépendance des variables conditionnellement aux classes et est capable de détecter des changements sur les probabilités P (W |X i , C) en utilisant l\u0027approche MODL. Chaque fenêtre est étiquetée et un changement est détecté s\u0027il existe un modèle de discrétisation/groupage permettant de distinguer les deux fenêtres.\nLa nouvelle méthode que nous proposons utilise le critère bivarié MODL P (W |X i , C) qui, dans le cadre de notre méthode, est à la fois : (i) robuste dans le cas d\u0027un flux stationnaire, (ii) rapide à détecter tous types de changements dans la distribution des données, (iii) capable d\u0027utiliser l\u0027information de classe.\nNotre méthode a été comparée à deux méthodes de l\u0027état de l\u0027art détectant les variations de performance d\u0027un classifieur. Nous avons proposé un nouvel algorithme, appelé MDD, basé sur notre méthode de détection permettant au classifieur de se mettre à jour. Celle-ci n\u0027utilise pas le classifieur pour la détection mais seulement les données du flux. Ses performances, en termes de précision, sont meilleures et plus constantes que les deux méthodes de l\u0027état de l\u0027art testées.\n"
  },
  {
    "id": "409",
    "text": "Introduction\nNotre travail se situe dans le cadre d\u0027un système d\u0027agrégation de descriptifs de commerces. Dans ce système, chaque commerce est décrit (entre autres) par un jeu de tags. Des connaissances sur la compatibilité ou l\u0027incompatibilité de ces tags peuvent aider à prendre la décision d\u0027agréger ou pas deux descriptifs. Par exemple, un descriptif contenant le tag fitness ne peut pas être agrégé avec un descriptif contenant le tag menuisier car, a priori, un même commerce ne peut pas à la fois rendre des services de menuiserie et avoir un lien avec le fitness. Par contre, les tags peinture et carrelage sont compatibles car ils peuvent, par exemple, être associés à un magasin d\u0027ameublement. Pour acquérir ces informations de compatibilité, nous disposons de données dont la structure est identique à celle d\u0027une folksonomie. Les folksonomies sont le produit de systèmes d\u0027annotation collaborative tels Flickr ou Delicious qui permettent à une communauté d\u0027utilisateurs d\u0027annoter manuellement des ressources (urls, photos) à l\u0027aide de descripteurs (tags). Le but de notre étude est de tester diverses approches pour l\u0027acquisition de relations entre tags à partir de folksonomies. L\u0027approche retenue sera ensuite intégrée au système d\u0027agrégation de descriptifs décrit plus haut. En plus de tester les approches état-de-l\u0027art, nous explorons aussi l\u0027apport de l\u0027apprentissage automatique pour la détection de relations sémantiques à partir de folksonomies, ce qui, à notre connaissance, n\u0027a pas encore été proposé.\nL\u0027article comprend 5 sections. La section 2 décrit les approches existantes pour l\u0027identification de relations entre tags à partir de folksonomies. Les expériences menées sont décrites en section 3. Les données expérimentales et les résultats obtenus sont présentés en section 4. Les perspectives de travail sont discutées en section 5. Hotho et al. (2006b) définissent une folksonomie comme un 4-uplet F :\u003d (U, T, R, Y ) où U \u003d {u 1 , ...u n } est un ensemble d\u0027utilisateurs, T \u003d {t 1 , ...t m } est un ensemble de tags, R \u003d {r 1 , ...r p } est un ensemble de ressources et Y ? U × T × R. Un triplet (u, t, r) ? Y correspond à l\u0027attribution du tag t à la ressource r par l\u0027utilisateur u. T ur :\u003d {t ? T |(u, t, r) ? Y } est l\u0027ensemble des tags donnés à la ressource r par l\u0027utilisateur u. Les travaux visant à identifier des relations entre tags à partir de folksonomies font état de trois types d\u0027approches : statistiques, vectorielles et exploitation de ressources sémantiques 1 . Les approches statistiques sont les plus fréquemment employées. Elle portent sur la cooccurrence des tags, c\u0027est-à-dire le nombre de fois où t 1 et t 2 ont été attribués à une même ressource par un même utilisateur, et qui est définie ainsi : w(t 1 , t 2 ) \u003d |{(u, r) ? U × R|t 1 , t 2 ? T ur }|. Les mesures peuvent être asymétriques, comme dans le cas de Schmitz (2006) qui utilise la probabilité qu\u0027une ressource ayant reçu le tag t 1 soit également annotée avec le tag t 2 ou symétriques comme la mesure Jaccard employée par Hassan-Montero et Herrero-Solana (2006) dans le but de construire une hiérarchie de concepts.\nTravaux apparentés\nLes approches vectorielles consistent à représenter chaque tag dans un espace vectoriel dont les dimensions correspondent soit aux autres tags, soit aux ressources, soit aux utilisateurs. Par exemple, dans l\u0027espace vectoriel correspondant aux ressources, chaque tag t est représenté par un vecteur t \u003d {w(t, r 1 ), ..w(t, r |R| )} où w(t, r) :\u003d |{(t, u) ? T × U |t ? T ur }|. Les vecteurs sont comparés avec la mesure Cosinus. Ces approches ont été employées par Specia et Motta (2007)  Les approches exploitant des ressources sémantiques existantes consistent à projeter les tags de la folksonomie dans une ressource sémantique structurée en graphe 2 afin d\u0027en déduire des relations sémantiques entre les tags -voir par exemple les travaux de Djuana et al. (2011). Cette technique a également été employée par Cattuto et al. (2008) ainsi que Markines et al. (2009) pour évaluer les approches vectorielles et statistiques. Pour cela, ils se basent sur la corrélation entre le score donné par la mesure à évaluer et la distance taxonomique de Jiang et Conrath (1997) calculée à partir de WordNet.\nExpériences\nNos expériences ont consisté à tester les approches statistiques et vectorielles ainsi qu\u0027à tester l\u0027apport de l\u0027apprentissage automatique. Nous avons utilisé deux types de ressources :\n(i) un ensemble de triplets (u, t, r) correspondant à l\u0027attribution d\u0027un tag t au lieu r par la source Internet u. La structure de ces données est identique à celle d\u0027une folksonomie, à 1. Plus anecdotiquement, d\u0027autres travaux exploitent la structure de graphe des folksonomies et utilisent une version adaptée de l\u0027algorithme PageRank (Hotho et al. 2006).\n2. Par exemple : taxonomie, ontologie, thésaurus.\nceci près que R est un ensemble de lieux (et non de documents) et que U est un ensemble de sources Internet décrivant les lieux (et non des utilisateurs). Toutes les approches testées s\u0027appuient sur l\u0027ensemble des tags (T ) et l\u0027ensemble des ressources (R) ainsi que sur la fonction R(t) \u003d {r ? R|t ? T ur , ?u} qui renvoie l\u0027ensemble des lieux ayant reçu le tag t, quelles que soient les sources ayant attribué ce tag 3 .\n(ii) un arbre de tags initialement conçu pour la catégorisation de commerces (exemple de chemin : RACINE \u003e manger \u003e restaurant \u003e chic). Les relations hiérarchiques ainsi que la distance entre deux tags sont potentiellement des indicateurs de leur compatibilité.\nCinq expériences ont été menées :\n• OVERLAP (approche statistique) : OVERLAP est une mesure de type statistique dérivée de Jaccard. Elle correspond à un coefficient de chevauchement entre les lieux ayant reçu t 1 et les lieux ayant reçu t 2 :\n• COSINUS (approche vectorielle) : Dans cette approche, chaque tag t est représenté par un vecteur t \u003d {inter(t, t 1 ), ...inter(t, t |T | )} où inter(t, t i ) \u003d |R(t)?R(t i )|. Ici, nous considérons que deux tags sont d\u0027autant plus compatibles qu\u0027ils ont des patrons de co-occurrences similaires (i.e. ils ont tendance à apparaître avec les mêmes tags). Par rapport à OVERLAP, cette approche a l\u0027avantage de permettre de rapprocher deux tags même s\u0027ils n\u0027ont pas ou peu été attribués aux mêmes lieux. La similarité des vecteurs de deux tags est évaluée avec la mesure Cosinus :\n• ML_TAGTREE (apprentissage automatique à partir d\u0027informations issue d\u0027une ressource sémantique) : Cette approche exploite des informations tirées de l\u0027arbre de tags. À partir de cet arbre de tags, nous pouvons extraire, pour chaque paire de tags (t 1 , t 2 ), 10 variables, décrivant soit des propriétés associées aux noeuds représentant les tags, soit des calculs de distance entre les tags : 10. nb. de tags dans {t 1 , t 2 } ayant pour propriété de déclencher, lors de l\u0027ajout du tag à un lieu, l\u0027ajout automatique de tags plus génériques que lui 4\nCes 10 variables ont été utilisées pour apprendre un modèle de classification à partir d\u0027exemples de paires de tags annotées comme COMPATIBLE ou INCOMPATIBLE. L\u0027apprentissage a été réa-lisé avec C5, un outil de génération d\u0027arbre de décision 5 qui est une version améliorée de l\u0027algorithme de Quinlan (1996).\n• ML_SIMPLE : Dans cette expérience, nous utilisons toujours C5 auquel nous fournissons 4 variables très simples pour apprendre l\u0027arbre de décision :\n• ML_ALL : Dans cette expérience, nous utilisons toujours C5, auquel nous fournissons toutes les informations utilisées dans les expériences précédentes. Le modèle de classification est donc appris à partir de 16 variables : la mesure Overlap, la mesure Cosinus, les 10 variables de ML_TAGTREE et les 4 variables de ML_SIMPLE.\nDonnées expérimentales et résultats Données\nNous disposons de 15 millions de lieux, 3696 tags et 100 sources fournis par la société Nomao (http://fr.nomao.com). Pour apprendre les différents modèles de classification et évaluer nos expériences, nous avons constitué un jeu de 590 paires de tags annotées avec 2 classes : COMPATIBLE ou INCOMPATIBLE. Un tiers des paires a été annoté par au moins deux annotateurs 6 . Le taux de désaccord entre annotateurs est de 12%, soit un Kappa (Carletta, 1996) de 0,77. La répartition COMPATIBLE/INCOMPATIBLE est de 41%/59% respectivement.\nRésultats Les approches ont été évaluées par validation croisée à 10 blocs. Pour les approches OVERLAP et COSINUS, un seuil de compatibilité a été appris en faisant varier sur les données d\u0027apprentissage le seuil à partir duquel on considère que deux tags sont compatibles ; puis ce seuil a été évalué sur les données de tests. Nous avons comparé les résultats des approches deux à deux et pour chaque couple, nous avons appliqué le t-test unilatéral apparié. Nous considérons qu\u0027une approche est significativement meilleure que l\u0027autre si la valeur p du t-test est en-dessous de 5%. Nous obtenons les classements suivants (les valeurs p sont indiquées entre parenthèses) :\n- \nConclusion et perspectives\nNous avons présenté une étude visant à identifier des compatibilités entre descripteurs de lieux dans le but de l\u0027intégrer à un système d\u0027agrégation de descriptifs de lieux : la présence de tags incompatibles empêchera l\u0027agrégation de deux descriptifs. Nous avons introduit l\u0027utilisation de l\u0027apprentissage automatique et nous avons testé trois modes de représentation des données : informations issues d\u0027une ressource sémantique, un jeu de 4 indices \"simples\" et la combinaison des informations utilisées dans toutes nos expériences. Au final, c\u0027est l\u0027apprentissage automatique basé sur le jeu des 4 indices qui a été choisi car cette approche se place parmi les meilleures en termes de taux d\u0027erreur et qu\u0027elle est également la plus robuste.\nLa première perspective de travail est l\u0027intégration de la compatibilité des tags dans le système d\u0027agrégation et l\u0027évaluation de son apport. Pour autant, notre méthode de détection de tags compatibles reste perfectible. Nous pensons à l\u0027exploitation de ressources linguistiques plus riches que notre arbre de tags, comme par exemple le réseau lexical JEUX DE MOTS (Lafourcade, 2007) qui a l\u0027avantage d\u0027indiquer des relations lexicales entre mots (synonymie, hyponymie, etc.). Les autres perspectives de travail concernent la variation des différents paramètres de C5, l\u0027essai d\u0027autres approches d\u0027apprentissage automatique (SVM, Naive Bayes...) ainsi que l\u0027annotation de nouveaux exemples sélectionnés via des approches d\u0027apprentissage actif (Settles, 2009).\nRéférences Carletta, J. (1996). Assessing agreement on classification tasks : The kappa statistic. Computational Linguistics 22(2), 249-254.\n"
  },
  {
    "id": "410",
    "text": "Introduction\nL\u0027étude des génomes nous apprend beaucoup sur le vivant. Chaque organisme possède un génome dont la composition est le résultat de l\u0027histoire évolutive propre à cet organisme. Cette information biologique codée par l\u0027ADN est divisée en unités discrètes, les gènes. Ces gènes codent pour les protéines, véritables \"rouages\" des réseaux biologiques au niveau cellulaire. Le projet de séquençage du génome humain (3, 4 Gb, 1 Gb représentant un milliard de paires de bases ou acides nucléiques A, C, G et T), initié en 1990, a duré 13 ans pour un coût total de 2, 7 milliards de dollars. Les techniques récentes (2nd Generation Sequencing) ou futures (3rd Generations sequencing) permettent des vitesses de séquençage bien plus élevées, de l\u0027ordre de 50 Gb/ jour, soit un génome comme celui de l\u0027Homme entièrement séquencé en moins de 3 heures (HiSeq Systems, Illumina Inc.).\nOn comprend aisément le déluge de données brutes qu\u0027il faudra savoir stocker et analyser. Nous nous intéressons à l\u0027exploitation de ces données pour en extraire des connaissances, notamment sur les protéines codées par ces gènes et leurs rôles dans la réalisation d\u0027une action biologique.\nCes informations devraient permettre à terme, de concevoir de nouveaux médicaments spé-cifiques à un individu donné, individu pour lequel nous aurons identifié précisément les interactions entre les protéines de son génome. La plupart des interactions fonctionnelles entre protéines sont réalisées sous la forme de complexe protéique (assemblage macro-moléculaire de plusieurs protéines).\nNous proposons d\u0027apprendre un modèle permettant de prédire un sous-ensemble de ces assemblages : les complexes protéine-protéine impliquant trois protéines ou trimères.\nCet article est organisé de la manière suivante : la section 2 présente le contexte de l\u0027étude que nous avons réalisé ainsi que le protocole mis en oeuvre pour apprendre notre modèle pré-dictif. Les données utilisées pour évaluer et valider notre approche seront également présentées dans la section 2. Les résultats obtenus sont analysés selon deux visions : l\u0027une purement informatique (sections 2.1 et 2.2) et l\u0027autre plus orientée biologie (section 3). Enfin, nous présentons plusieurs perspectives à ce travail en fin d\u0027article.\nContexte biologique de l\u0027étude\nLe postulat essentiel de ce travail est que deux protéines qui interagissent dans une espèce donnée vont être soumises, de manière corrélée (ou conjointe), aux contraintes évolutives imposées à l\u0027une ou à l\u0027autre. En d\u0027autres termes, les deux protéines vont co-évoluer, permettant ainsi à l\u0027interaction d\u0027être maintenue et à la fonction biologique d\u0027être conservée dans le temps. L\u0027exemple extrême de coévolution entre protéines en interaction est le gain ou la perte conjointe de deux protéines : si l\u0027association entre deux protéines est nécessaire pour qu\u0027une fonction biologique soit réalisée, la perte d\u0027un des deux partenaires (resp. le gain) entraînera la perte (resp. le gain) de l\u0027autre. La comparaison des profils de présence/absence de protéines (nommés profils évolutifs ou profils phylogénétiques, ) chez différentes espèces permet donc de détecter la co-évolution entre protéines et donc de prédire si deux protéines sont susceptibles d\u0027interagir. Ce type d\u0027approche est au coeur de plusieurs démarches similaires , Marcotte et al. (1999), de Vienne et Azé (2012).\nAfin de déterminer le profil évolutif d\u0027une protéine d\u0027intérêt, l\u0027ensemble des protéines qui lui sont similaires, dans un ensemble d\u0027espèces de référence, est calculé. Ces protéines similaires à une protéine donnée mais dans une autre espèce sont nommées des orthologues. Pour une protéine donnée, le profil évolutif que nous manipulons représente l\u0027ensemble des espèces dans lesquelles notre protéine possède au moins un orthologue. Il peut être représenté sous la forme d\u0027un vecteur de booléens comme montré sur la Figure 1.\nÉtant donné un ensemble de complexes protéiques avérés, nous réalisons l\u0027extraction, pour chaque complexe, des interactions binaires entre protéines. Dans le cadre d\u0027un apprentissage supervisé, ces interactions binaires entre protéines représentent l\u0027ensemble des exemples positifs. À partir de l\u0027ensemble des protéines présentes dans ces complexes, nous générons l\u0027ensemble des paires de protéines n\u0027appartenant pas à des complexes. Ces paires de protéines représentent alors les exemples négatifs.\nNous pouvons alors apprendre un modèle permettant de prédire l\u0027interaction de deux protéines à partir de la connaissance de leur profil évolutif. Nous pouvons considérer les profils évolutifs associés aux différentes protéines comme des 1-motifs fréquents. L\u0027étude des paires de protéines via leurs profils évolutifs, correspond alors à l\u0027étude des 2-motifs fréquents que nous obtenons en calculant simplement l\u0027intersection des profils associés à chaque protéine (voir Figure 1).\nRappelons que notre objectif est de pouvoir prédire si deux protéines vont interagir. Nous avons donc besoin de définir un ensemble de critères permettant d\u0027associer, à partir des profils évolutifs des protéines, un score à chaque paire de protéines étudiée. Pour cela, nous proposons d\u0027utiliser des métriques classiquement utilisées dans l\u0027évaluation des règles d\u0027association pour définir l\u0027ensemble des critères numériques associés à une paire de protéines. Ces métriques, souvent appelées mesures de qualité, sont pour certaines d\u0027entre elles non symétriques, i.e. elles n\u0027évaluent pas de la même manière les règles A ? B et B ? A. Dans notre cadre de travail, les interactions entre protéines ne sont pas orientées et nous ne devons donc pas prendre en considération cette information. Ainsi, pour une paire de protéine (A, B) donnée, nous calculons les mesures de qualité associées aux règles A ? B et B ? A dans le cas des mesures dites non symétriques.\nLes critères retenus pour évaluer l\u0027interaction entre deux protéines A et B sont : -n A , n B , n AB qui représentent respectivement le nombre d\u0027espèces ayant au moins un orthologue pour la protéine A, B et pour les protéines A et B -les mesures non symétriques : la confiance (et son symétrique : le rappel), la confiance centrée, la moindre contradiction, l\u0027indice de Jaccard, Loevinger, TEC, LAP, GAN, Zhang, Pearl -les mesures symétriques : Lift, Dice, Pearson, GiniIndex, IQC Le Tableau 1 présente les expressions analytiques des mesures de qualité symétriques et non symétriques. Dans ce tableau, les notations suivantes sont utilisées :\nAinsi, une paire de protéines (A, B) est décrite par 28 valeurs réelles qui caractérisent le lien entre A et B. Afin de déterminer si ce lien est fonctionnel, i.e. se traduit par une interaction entre A et B, un modèle prédictif est appris.\nNous présenterons dans la suite les détails liés à la mise en oeuvre de l\u0027apprentissage su-\nMesures non symétriques Confiance\nIndice de Jaccard\nTAB. 1 -Mesures de qualité utilisées pour décrire les profils évolutifs. Nous renvoyons le lecteur à (Lallich et al., 2007) et (Lenca et al., 2003) pour une étude détaillée de chacune de ces mesures. Les métriques n A , n B et n AB également utilisées pour décrire un profil évolutif ne sont pas rappelées dans ce tableau.\npervisé de ce modèle. Nous avons retenu une approche de type \"ensemble learning\" où un ensemble de classifieurs binaires sont combinés pour construire un méta-classifieur.\nNotons que le choix des descripteurs (Tableau 1) et leur utilité pour décrire les interactions protéine-protéine ont été décrits et évalués dans un article précédent (de Vienne et Azé (2012)). Nous ne discutons donc pas ces choix dans le présent article.\nCombinaison de classifieurs\nDes travaux antérieurs (Juan et al. (2008), de Vienne et Azé (2012)) ont montré que les hypothèses biologiques sur lesquelles reposent notre travail permettent d\u0027apprendre des modèles qui s\u0027avèrent efficaces en prédiction d\u0027interaction protéine-protéine. Ces modèles permettent d\u0027ordonner efficacement les paires de protéines par probabilité décroissante d\u0027être en interaction.\nPar contre, ces approches ne permettent pas de reconstruire nativement les complexes protéiques associés à ces paires de protéines. Cette reconstruction n\u0027est immédiate que dans le cas de complexes n\u0027impliquant que deux protéines. Pour un trimère ABC, le nombre d\u0027interactions potentiels est égal à 3 : AB, BC, AC et les approches actuelles ne garantissent pas que la totalité de ces interactions soient prédites, ni qu\u0027elles aient des scores comparables permettant ainsi de les identifier facilement. Le travail présenté ici tente de répondre à cette question.\nPour traiter ce problème, nous avons utilisé comme données d\u0027apprentissage l\u0027ensemble des complexes binaires avérés de l\u0027organisme modèle Escherichia Coli (noté E. coli dans la suite de l\u0027article).\nCet organisme contient 4078 protéines réparties en de multiples monomères, 66 dimères, 45 trimères et quelques complexes impliquant plus de trois protéines.\nParmi l\u0027ensemble des approches existantes pour calculer les orthologues, nous avons retenu celle proposée par Moreno-Hagelsieb et Latimer (2008). Ils ont proposé une combinaison de plusieurs méthodes afin de détecter avec une grande fiabilité les orthologues des protéines de E. coli chez N \u003d 1050 espèces et ont rendu ces données publiques.\nNous (i) Les six classifieurs suivants ont été utilisés pour apprendre des méta-classifieurs : règles de décision (JRip, PART), arbres de décision (J48 et RandomForest), Bayésien Naïf et Ré-gression Logistique. Ces classifieurs ont été appris avec la boîte à outils Weka (Hall et al., 2009).\n(ii) L\u0027échantillonnage des données est réalisé d\u0027une part de manière à contrôler le nombre de positifs et de négatifs dans les données et d\u0027autre part, pour palier au faible nombre d\u0027exemples positifs disponibles.\nDe nombreux travaux ont montré l\u0027intérêt des approches de type \"Ensemble Learning\" par rapport à l\u0027utilisation de classifieurs \"classiques\" (voir Quinlan (1996), Opitz et Maclin (1999), Bauer et Kohavi (1999) pour quelques approches de références). Nous avons utilisé comme données d\u0027apprentissage les 66 dimères d\u0027E. coli, soit 132 protéines différentes. À partir de ces 132 protéines, nous construisons l\u0027ensemble des 8580 paires de protéines représentant des interactions négatives. Cet ensemble est réduit à 8279 paires de protéines négatives après application de la contrainte n AB ? 2 à chaque profil évolutif. Cette contrainte minimale permet d\u0027assurer une certaine cohérence avec notre hypothèse de travail reposant sur la coévolution (dans au moins deux espèces différentes) des protéines en interaction.\nNous procédons de la même manière pour créer l\u0027ensemble de test constitué uniquement des 45 trimères. Ces 45 trimères sont constitués à partir de 135 protéines différentes. Notre ensemble de test est donc constitué de 135 exemples positifs (paires de protéines impliquées dans un trimère) et de 8910 négatifs (le filtre n AB ? 2 ne rejette aucune paire).\nLes six classifieurs listés précédemment sont utilisés pour apprendre deux méta-classifieurs différents :\nM 1 ce premier méta-classifieur est construit de la manière suivante : (i) l\u0027ensemble des données d\u0027apprentissage (les dimères) est utilisé pour apprendre chacun des six classifieurs, (ii) chacun des six classifieurs obtenu est appliqué sur le jeu de test (les trimères), et (iii) pour chaque exemple, le nombre de votes positifs est utilisé pour évaluer l\u0027appartenance à la classe positive. Ainsi, une valeur variant de 0 à 6 sera associé à chaque exemple.\nM n 2 ce second méta-classifieur ne diffère que sur le point (i) où un ensemble de n échantillons des données d\u0027apprentissage est construit par tirage sans remise et les six classifieurs sont appris sur cet échantillon. Chaque échantillon contient 50 exemples positifs et 1000 exemples négatifs. Afin d\u0027éviter tout biais dû à l\u0027échantillonnage aléatoire dans la construction des échan-tillons utilisés par l\u0027approche M n 2 , nous avons itéré le processus 100 fois et les votes ont été moyennés. Ainsi, une valeur variant de 0 à 6 × n est associée à chaque exemple du jeu de test.\nCes deux méta-classifieurs sont ensuite utilisés pour deux tâches : (A) élaguer l\u0027ensemble des paires potentiellement en interaction et (B) détecter les trimères.\nL\u0027élagage des paires potentiellement en interaction est simplement réalisée en utilisant la contrainte suivante : si score(exemple) \u003d 0 alors élagage(exemple), où score(exemple) représente le nombre de votes positifs associés à l\u0027exemple. Les performances des différents classifieurs pour la tâche (A) sont présentées dans le Tableau 2.\nLe méta-classifieur M 1 se comporte comme le Bayésien Naïf, car le Bayésien Naïf est le classifieur qui prédit le plus de positifs et tous les positifs prédits par les différents classifieurs sont également prédits par le Bayésien Naïf.\nConcernant le méta-classifieur M n 2 , nous ne présentons que les résultats pour n \u003d 50 car nous avons constaté qu\u0027à partir de n \u003d 50 échantillons utilisés pour composer le méta-classifieur, les performances observées ne variaient plus (par manque de place, nous ne pouvons discuter plus précisément cet aspect de nos résultats). Pour la tâche (B), nous proposons l\u0027algorithme pT ri, présenté dans la section suivante, qui permet de reconstruire les trimères, à partir des paires non élaguées.\nExtraction des trimères à partir de l\u0027évaluation des paires de protéines par le méta-classifieur\nSachant que nous nous focalisons sur la recherche de trimères, nous proposons l\u0027algorithme pT ri (voir Algorithme 1) pour identifier ces trimères. pT ri exploite le nombre de votes positifs associés aux paires de protéines pour identifier les paires les plus prometteuses. Nous considérons la liste des paires ordonnées par nombre de votes positifs décroissant.\npT ri parcourt cette liste par valeur décroissante du nombre de votes positifs. Les paires de protéines sont extraites de la liste une par une pour reconstituer des trimères. L\u0027étape initiale consiste à extraire la première paire de la liste. Puis la liste est parcourue en appliquant la règle suivante : lorsqu\u0027une nouvelle paire de protéines AB est prise en considération :\n-soit il existe une protéine X telle que la paire AX (resp. BX) a déjà été rencontrée précédemment (avec un nombre de votes positifs plus élevé que AB) alors : -s\u0027il n\u0027existe pas de protéine Y tel que BY (resp. AY ) ait été précédemment rencontré, alors le trimère AXB est prédit et toutes les paires impliquant l\u0027une des trois protéines A, B ou X sont supprimées de la liste ordonnée des paires considérées. -soit il existe Y tel que BY (resp. AY ) ait été rencontrée alors la paire AB est simplement supprimée. L\u0027hypothèse sous-jacente est que les paires AX (resp. BX) et BY (resp. AY ) appartiennent chacune à un autre trimère. -sinon, la paire AB est ajoutée à l\u0027ensemble des paires prédites. Cette paire représente alors la première paire d\u0027un trimère dont aucune des protéines n\u0027a déjà été identifiée. Nous avons appliqué pT ri sur les données de test où les interactions entre protéines ont été évaluées d\u0027une part en utilisant le méta-classifieur composé des 6 classifieurs appris sur l\u0027ensemble des données relatives aux dimères et d\u0027autre part, sur le méta-classifieur obtenu par combinaison des 6 × n classifieurs appris sur des échantillons aléatoires des données relatives aux dimères.\nLe Tableau 3 présente les résultats obtenus pour les approches pT ri(M 1 ) et pT ri(M 50 2 ). Les performances de pT ri peuvent être résumées selon les deux axes suivants.\nRéduction du nombre de paires de protéines à analyser\nLa quantité de paires à analyser a été réduite de manière drastique pour les deux méthodes. \nAugmentation du nombre de trimères correctement identifiés\nOutre le nombre de paires de protéines correctement identifiées, l\u0027approche pT ri(M 50 2 ) permet également de prédire correctement un plus grand nombre de trimères que l\u0027approche pT ri(M 1 ). En effet, en utilisant les 64 paires de protéines correctement identifiées par l\u0027ap-  \nApplication aux données biologiques\nNous nous sommes focalisés ici sur la prédiction de complexes hétérotrimèriques impliquant des protéines de structures et fonctions différentes. Ces trimères sont impliqués dans plusieurs rôles clés de la cellule : régulation des concentrations en différents substrats assurant le maintien de la cellule dans un état quasi-stationnaire, dégradation et synthèse de l\u0027ADN/ARN.\nLe méta-classifieur M 50 2 nous permet d\u0027identifier correctement les trois protéines de 30 des 45 trimères. Ces 30 prédictions se décomposent en 27 trimères parfaitement prédits (l\u0027intégra-lité des interactions) et 3 trimères partiellement prédits (2 interactions sur 3), ce qui s\u0027avère parfaitement suffisant pour reconstruire le trimère original.\nPour 12 trimères, nous n\u0027arrivons à identifier qu\u0027une seule des trois interactions, enfin pour 3 trimères, aucune des interactions n\u0027est identifiée.\nParmi les 19 paires de protéines incorrectement élaguées par le méta-classifieur M 50 2 (voir Tableau 2), nous retrouvons 8 des 9 paires impliquées dans les 3 trimères non identifiés. Ces 8 paires ayant été élaguées avant l\u0027application de l\u0027algorithme pT ri, il devient impossible d\u0027identifier correctement les trimères concernés.\nPrises en compte des spécificités structurales : cas des transporteurs membranaires type ABC\nL\u0027objectif initial de cette étude concerne la prédiction des interactions pour un trimère ABC, pour lequel le nombre d\u0027interactions potentiels est égal à trois : AB, BC, AC. Néan-moins quels renseignements pouvons nous retenir des prédictions partielles de ces assemblages protéiques, i.e lorsque seule une voire deux interactions du trimère original sont correctement prédites ? le corollaire de cette question étant : quels enseignements pour la suite, nous fournissent ces trimères \"atypiques\" ?\nParmi les complexes pour lesquels la prédiction est incomplète (de 1 à 2 interactions sur les 3 potentielles), figurent les transporteurs ABC. Ces transporteurs ABC, situés au niveau des membranes de la cellule, permettent l\u0027assimilation ou l\u0027élimination au niveau cellulaire d\u0027une large variété de métabolites et constituent une cible thérapeutique de choix comme les traitements antibactériens (Cangelosi et al., 1990).\nCe transporteur, qui est ATP-dépendant (i.e. nécessitant une source d\u0027énergie ATP pour fonctionner), est représenté schématiquement Figure 2. Il est constitué d\u0027une structure extramembranaire, récepteur des métabolites provenant de l\u0027environnement, d\u0027une sous-structure membranaire ou perméase permettant le transfert des métabolites au niveau de la membrane hydrophobe sans altérer leurs structures et enfin d\u0027une partie intra-membranaire correspondant au site de fixation des molécules d\u0027ATP (Dawson et Locher, 2006). Nous voyons ici que d\u0027un point de vue structural, la partie extra-cellulaire (site de fixation des métabolites) et le site de fixation de l\u0027ATP ne sont jamais connectés. Dès lors pour reprendre la notation utilisée précédemment pour un trimère ABC donné, (où A désigne le site extra-cellulaire, B la perméase et C le site de fixation de l\u0027ATP), seules les interactions directes AB et BC s\u0027avèrent pertinentes. L\u0027information associée à la localisation de la protéine (intra ou extra cellulaire) peut être exploitée en post-traitement des prédictions effectuées par pT ri.\nPrenons l\u0027exemple présenté sur la Figure 3 pour illustrer ce post-traitement. La Figure 3-(a) représente un sous-graphe du graphe des prédictions effectués par pT ri. Nous pouvons y identifier une prédiction aberrante : proX ? modC. Ces deux protéines ne peuvent pas interagir physiquement car proX est une protéine intra-cellulaire et modC est une protéine extra-cellulaire. Une information évidente pour un expert du domaine qui peut donc supprimer manuellement le lien prédit entre proX et modC.\nSi nous réappliquons pT ri uniquement sur les paires de protéines présentes dans ce sousgraphe (à l\u0027exception de proX ? modC), alors nous obtenons les quatre trimères présentés sur la Figure 3-(b).\nNous pouvons voir qu\u0027une simple connaissance expert injectée dans les prédictions permet automatiquement à notre algorithme d\u0027effectuer les bonnes prédictions d\u0027interactions.\nCe type de configuration se reproduit pour un autre sous-graphe pour lequel 2 trimères sont prédits après une simple intervention de l\u0027expert.\nAprès application de ce nouveau post-traitement, 9 des 12 trimères partiellement prédits (1 interactions sur 3) sont alors extraits avec deux interactions prédites sur les trois, permettant ainsi d\u0027identifier 39 des 45 trimères.\nConclusion\nDans ce contexte spécifique de la biologie, cette méthode par combinaison de classifieurs s\u0027avère performante. Elle permet de réduire de manière drastique le taux de faux négatifs, rejetant ainsi 94, 74% des paires de protéines. Les résultats montrent que l\u0027échantillonnage aléatoire des données avec renforcement du taux de positifs a un impact positif sur la qualité des prédictions observées. Concernant la reconstruction des trimères de protéines, l\u0027extraction des 2-motifs fréquents en tenant compte du nombre de votes positifs associés à chaque paire de protéine, permet de retrouver le modèle mutlimérique dans 66, 67% des cas. L\u0027étude des contacts structuraux directs a permis de montrer que ce taux pouvait atteindre 86, 67% de bonnes prédictions, en accord avec le principe même de coévolution où, en grande majorité, la modification évolutive d\u0027une protéine \"touche\" principalement son interactant protéique direct. À très court terme, cette approche offre des perspectives remarquables dans les reconstructions des réseaux de signalisation cellulaire ou de docking, visant à prédire, au sein même de la cellule, quelles protéines interagissent entre elles et comment, d\u0027un point de vue structural, celles-ci peuvent s\u0027assembler. Une généralisation de l\u0027algorithme pT ri pour pouvoir l\u0027étendre aux multimères de plus de 3 partenaires s\u0027avère essentielle. En outre, afin de réduire un peu plus le taux de faux positifs, nous souhaiterions utiliser l\u0027ensemble des classifieurs pour vérifier la qualité de l\u0027annotation faite sur les données en isolant les interactions indirectes entre protéines prédites dans un même complexe. Enfin à plus long terme, il serait intéressant d\u0027introduire une procédure d\u0027apprentissage actif en recoupant les votes associés au méta-classifieur et les données multi-sources mises à disposition par les experts en biologie.\nRéférences\nBauer, E. et R. Kohavi (1999). An empirical comparison of voting classification algorithms : Bagging, boosting and variants. Machine Learning 36(Issue 1-2), 105-139.\n"
  },
  {
    "id": "411",
    "text": "Résumé\nLa réponse cellulaire d\u0027un organisme vivant à un signal donné, hormone, stress ou médi-cament, met en jeu des mécanismes complexes d\u0027interaction et de régulation entre les gènes, les ARN messagers, les protéines et d\u0027autres éléments tels que les micro-ARNs. On parle de réseau d\u0027interaction pour décrire l\u0027ensemble des interactions possibles entre protéines et de ré-seau de régulation génique pour représenter un ensemble de régulations entre gènes. Identifier ces interactions et ces régulations ouvre la porte à une meilleure compréhension du vivant et permet d\u0027envisager de mieux soigner par le biais du ciblage thérapeutique. Puisque les techniques expérimentales de mesure à grande échelle, récemment développées, fournissent des données d\u0027observation de ces réseaux, ce problème d\u0027identification de réseau, généralement appelé inférence de réseau en biologie des systèmes, s\u0027inscrit dans le cadre général de la fouille de données et plus particulièrement de l\u0027apprentissage artificiel. Voilà maintenant quelques années que cette problématique a été posée à notre communauté et durant lesquelles les échanges entre biologistes et informaticiens ont non seulement permis aux biologistes d\u0027étoffer leurs boîtes à outils mais aussi aux informaticiens de concevoir de nouvelles méthodes de fouille de données.\nEn partant des deux problématiques distinctes que sont l\u0027inférence de réseau d\u0027interaction et l\u0027inférence de réseau de régulation, je montrerai que ces deux tâches d\u0027apprentissage posent, chacune de manière différente, la problématique de la prédiction de sorties structurées. L\u0027inférence de réseau d\u0027interaction entre protéines, vue comme un problème transductif de prédiction de liens, peut être résolue comme un problème d\u0027apprentissage d\u0027un noyau de sortie à partir d\u0027un noyau d\u0027entrée. L\u0027inférence de réseau de régulation, impliquant la modélisation d\u0027un système dynamique, peut être abordée par l\u0027approximation parcimonieuse et structurée de fonctions à valeurs vectorielles. Je présenterai un ensemble de nouveaux outils de régression à sortie dans un espace de Hilbert, fondés sur des noyaux à valeur opérateur, qui fournissent d\u0027excellents résultats en inférence de réseaux biologiques. Des expériences in silico sur des données artificielles, chez la levure du boulanger ou chez l\u0027homme illustreront mes propos. En fin d\u0027exposé, je tracerai quelques perspectives concernant les \" nouveaux \" défis dans le domaine de la bioinformatique et dans celui de la prédiction de sorties structurées. \n"
  },
  {
    "id": "412",
    "text": "Introduction\nContexte. Depuis plusieurs années, dans les secteurs de l\u0027Internet, de l\u0027analyse décisionnelle ou encore de la génétique sont collectées et analysées des données de plus en plus volumineuses et complexes. Ce phénomène connu sous le nom de Déluge des données (ou Big Data) soulève de nombreuses problématiques. En particulier, être capable de stocker, partager et analyser de telles quantités de données constitue un enjeu d\u0027étude essentiel, comme le soulignent Schuett et Pierre (2012). La théorie des graphes est particulièrement appropriée pour étudier les réseaux sociaux, où les connexions entre utilisateurs peuvent facilement être représentées et analysées en utilisant des graphes, le plus souvent orientés. Dans cet article, nous considérons le graphe des relations entre utilisateurs (anonymisés) de Twitter créé en 2009 par Cha et al. (2010). Ce graphe orienté contient plus de 50 millions de sommets et près de 2 milliards d\u0027arcs.\nLes capitalistes sociaux. Nous nous intéressons particulièrement au comportement d\u0027utilisateurs particuliers nommés capitalistes sociaux, observé par Ghosh et al. (2012). Ces utilisateurs, qui ne sont ni des spammeurs, ni des robots, partagent un objectif commun : acquérir un nombre maximum d\u0027utilisateurs qui les suivent -followers. En effet, plus le nombre de followers d\u0027un utilisateur est élevé, plus il peut être influent sur le réseau. Au-delà de cet intérêt évident, le nombre de followers a un impact direct sur le classement des tweets de l\u0027utilisateur sur le moteur de recherche de Twitter. Ces utilisateurs ne sont pas sains pour un réseau social : en suivant des utilisateurs sans regarder le contenu de leurs tweets, les capitalistes sociaux donnent de l\u0027influence à des utilisateurs tels que les spammeurs.\nNotre contribution. Les résultats que nous proposons dans cet article sont de deux types. Nous nous concentrons tout d\u0027abord sur la recherche de méthodes efficaces et haut niveau permettant de stocker et manipuler le graphe des relations entre utilisateurs de Twitter en ayant recours à des ressources informatiques raisonnables 1 . Nous nous intéressons ensuite à la dé-tection des capitalistes sociaux. En particulier, nous montrons que ces derniers peuvent être détectés efficacement en appliquant des mesures de similarité sur les voisinages du graphe des relations. Pour notre étude, nous utilisons un graphe collecté en 2009 par Cha et al. (2010), contenant les utilisateurs anonymisés de Twitter ainsi que les relations qui existent entre eux. Plus précisément, pour mettre en oeuvre nos mesures, nous considérons le graphe des spammeurs de Twitter, qui contient près de 40000 spammeurs (détectés par Ghosh et al. (2012)) ainsi que leurs voisins, pour un total de 15 millions de sommets et 1 milliard d\u0027arcs (Section 2). Nous verrons dans la suite de cet article que travailler sur un tel graphe est suffisant pour nos besoins. Finalement, pour valider notre méthode de détection, nous comparons nos résultats à une liste de 100000 capitalistes sociaux potentiels détectés de manière ad-hoc par Ghosh et al. (2012) lors de leur étude sur les spammeurs. Nous observons que nos algorithmes détectent une grande majorité de ces utilisateurs, qui ont la quasi-totalité de leur voisinage inclus dans le graphe des spammeurs (Section 3).\n2 Graphe des spammeurs : stockage et définition Stockage. Dans leurs travaux respectifs, Cha et al. (2010) et Ghosh et al. (2012) ne donnent aucun détail sur la méthode qu\u0027ils ont employée pour manipuler le graphe de Twitter. Dans cet article, notre premier objectif est ainsi de trouver un procédé haut niveau pour stocker et traiter le graphe des relations de Twitter en utilisant des ressources informatiques raisonnables. La méthode que nous suggérons dans cet article peut donc être reproduite facilement sur un simple serveur avec un unique processeur disposant d\u0027une certaine quantité de mémoire vive (environ 24 Go pour étudier efficacement le graphe des spammeurs). Remarquons que les mesures développées dans l\u0027article auraient pu être réalisées via des traitements sur la liste d\u0027adjacence du graphe mais cela n\u0027est pas une méthode haut niveau extensible à d\u0027autres problèmes.\nPour parvenir à cet objectif, nous avons utilisé des bases de données pour stocker le graphe et l\u0027analyser. Nous avons exploré la possibilité d\u0027utiliser tout type de bases de données, qu\u0027elles soient dites SQL, NoSQL ou orientées graphes. Les résultats de nos expérimentations montrent que, même pour de simples mesures telles que celles que nous mettons en oeuvre, il est plus efficace d\u0027utiliser une base de données orientée graphes. Par exemple, si MySQL permet de rapidement charger les données, l\u0027exécution de requêtes calculant l\u0027intersection de deux voisinages (un outil nécessaire pour nos mesures) est relativement lente (plusieurs jours pour les exécuter sur tous les sommets ou plusieurs jours pour poser les index efficaces). Avec Cassandra (NoSQL), le simple fait d\u0027obtenir le degré de chaque sommet peut demander de nombreuses heures de traitement. Par conséquent, nous avons essayé plusieurs bases de données orientées graphes, telles que OrientDB ou Neo4j. Dans les deux cas, nous n\u0027avons pas été capables de charger le graphe en un temps raisonnable (moins d\u0027une semaine). Finalement, Dex (voir Martínez-Bazan et al. (2012)) est apparue comme une solution viable pour plusieurs raisons : orientée graphes, haute performance et dotée d\u0027une API haut niveau. Sur simple demande, nous avons obtenu une licence temporaire fournissant un accès complet à toutes les fonctionnalités. Cependant, si nous avons été capables de stocker le graphe des relations de Twitter en utilisant Dex (avec un temps de chargement de quelques heures), nos algorithmes n\u0027ont parfois pas pu s\u0027exécuter sur la totalité du graphe à cause de dysfonctionnements. Ces problèmes techniques -détectés par nos expérimentations-sont toujours en cours de correction. Cependant, ils ne sont pas apparus sur un graphe de plus petite taille (dit des spammeurs) qui reste cohérent avec nos objectifs, et nos algorithmes s\u0027exécutent ainsi en quelques heures. Nous allons voir que détecter des utilisateurs particuliers dans ce graphe avec des mesures de similarité sur les voisinages peut permettre d\u0027obtenir des informations pertinentes dans le graphe des relations. En effet, la majorité des utilisateurs détectés par nos algorithmes ont une grande proportion de leur voisinage dans le graphe des spammeurs.\nCapitalistes sociaux\nDe façon identique aux comportements observés sur Internet, où les administrateurs de sites webs effectuent de l\u0027échange de liens dans le but d\u0027accroître leur visibilité, certains utilisateurs cherchent à obtenir un maximum de followers afin d\u0027augmenter leur influence. Pour parvenir à cet objectif, ces utilisateurs exploitent deux techniques relativement simples et basées sur la réciprocation du lien follow : FMIFY (Follow Me and I Follow You -l\u0027utilisateur assure à ses followers qu\u0027il les suivra en retour) et IFYFM (I Follow You, Follow Me -ces utilisateurs suivent d\u0027autres utilisateurs en espérant que ceux-ci les suivent en retour). Ces comportements ont été mis en lumière par Ghosh et al. (2012) : lors d\u0027une étude sur les spammeurs, ils ont observé une classe d\u0027utilisateurs réels (i.e. ni des spammeurs, ni des faux comptes) répondant beaucoup aux sollicitations des spammeurs, les qualifiant ainsi de capitalistes sociaux.\nDéfinition 1 (Capitaliste social). Un capitaliste social est un utilisateur appliquant les principes FMIFY/IFYFM dans le but d\u0027augmenter son influence sur le réseau social Twitter.\nIl est intéressant de noter que plusieurs comptes célèbres sur Twitter (comme par exemple celui de Barack Obama) sont connus pour avoir appliqué ces principes.\nMesures de similarité. Dans le but de détecter des capitalistes sociaux, nous utilisons deux mesures de similarité sur le voisinage des utilisateurs, à savoir l\u0027indice de chevauchement (introduit par Simpson (1943)) et le ratio. La première nous permet de détecter de potentiels capitalistes sociaux, alors que la dernière nous sert à classifier ces capitalistes sociaux selon leur utilisation de l\u0027un ou l\u0027autre des principes FMIFY et IFYFM. \n. Cela nous permet de détecter des utilisateurs susceptibles d\u0027être des capitalistes sociaux. En effet, les voisinages entrants et sortants de ces derniers doivent être fortement liés ; en d\u0027autres termes, ils doivent suivre la majorité de leurs followers (principe FMIFY), ou inversement être suivis par la majorité des utilisateurs qu\u0027ils suivent (principe IFYFM). En particulier, cela signifie que leur indice de chevauchement doit être proche de 1, l\u0027ensemble\n, respectivement. Nous utilisons par la suite la Définition 3 pour classifier ces utilisateurs.\nIntuitivement, les utilisateurs qui suivent le principe IFYFM doivent avoir un ratio supérieur à 1, tandis que les utilisateurs qui appliquent FMIFY doivent avoir un ratio inférieur à 1. Dans les deux cas, le ratio attendu doit être proche de 1. Nous observons cependant un comportement qui engendre un ratio très inférieur à 1 chez certains utilisateurs, que nous appelons passifs. Contrairement aux autres capitalistes sociaux, ces utilisateurs considèrent leur degré entrant comme suffisant. Ils cessent donc d\u0027utiliser les principes mentionnés ci-dessus mais continuent à accumuler des followers, notamment grâce à l\u0027influence dont ils disposent.\nDétection dans le graphe des spammeurs. Nous prétendons que détecter les capitalistes sociaux dans un tel graphe peut fournir des informations pertinentes à propos des capitalistes sociaux dans le graphe des relations entre utilisateurs. Pour illustrer cela, nous utilisons une liste de 100000 utilisateurs considérés comme des capitalistes sociaux par Ghosh et al. (2012). Ces derniers ont été répérés de manière ad-hoc : en partant d\u0027une liste de 40000 spammeurs potentiels, ils ont considéré comme capitalistes sociaux les utilisateurs répondant le plus aux sollicitations des spammeurs. Ces derniers doivent donc se retrouver dans le graphe des spammeurs. Plus précisément, comme l\u0027illustre la Figure 1, la majorité de ces utilisateurs possède la quasi-totalité de leur voisinage dans le graphe des spammeurs. Ces premières observations nous permettent de valider la pertinence de nos mesures de similarité basées sur les voisinages, et donc de poursuivre l\u0027étude des capitalistes sociaux sur le graphe des spammeurs. Quelle que soit la contrainte imposée sur le degré entrant, nous observons les comportements IFYFM et FMIFY : pour un degré supérieur à 500, 62% des utilisateurs ont un ratio supérieur à 1 et 24% ont un ratio entre 0.7 et 1, respectivement. Nous remarquons également que des capitalistes sociaux dits passifs sont présents lorsque le degré est supérieur à 10000 : 14% des 5344 sommets ayant un indice de chevauchement supérieur à 0.8 ont un ratio inférieur à 0.7.\nValidation des résultats. Afin de confirmer nos observations, nous utilisons la liste de cent mille capitalistes sociaux détectés de manière ad-hoc par Ghosh et al. (2012). Rappelons que ces derniers devraient être détectés par notre méthode, un fait visible sur la table droite de la Figure 2. Nous aimerions mentionner qu\u0027environ 12500 utilisateurs de la liste de Ghosh et al. (2012) possèdent moins de 500 followers. Pour mieux illustrer la cohérence de nos résultats, nous montrons maintenant que les utilisateurs détectés comme capitalistes sociaux potentiels par nos algorithmes ont leur voisinage presque entièrement contenu dans le graphe des spammeurs ( Figure 3). Ainsi, la plupart des utilisateurs que nous détectons peuvent être considérés comme des capitalistes sociaux dans le graphe des relations. En effet, par la Définition 2, tout utilisateur ayant moins de 10% de son voisinage à l\u0027extérieur du graphe des spammeurs aura un indice de chevauchement d\u0027au moins 0.72 dans le graphe des relations. \n"
  },
  {
    "id": "415",
    "text": "Introduction\nLes approches de bi-partitionnement sont devenues un sujet d\u0027intérêt en raison de ses nombreuses applications dans le domaine de l\u0027exploration des données. Une méthode de bipartitionnement, aussi appelée classification croisée, bi-clustering ou co-clustering, est une méthode d\u0027analyse qui vise à regrouper des données en fonction de leur similarité. La stratégie classique des méthodes de bi-partitionnement cherche à trouver des sous-matrices ou des blocs, qui représentent des sous-groupes de lignes et des sous-groupes de colonnes. Depuis le premier algorithme de bi-partitionnement, appelé Block Clustering proposé par Hartigan (1972), de nombreuses techniques ont été proposées telles que l\u0027énumération exhaustive (Tanay et al. (2002)), l\u0027analyse spectrale (Greene et Cunningham (2010)), les réseaux bayésiens (Shan et al. (2010)) et d\u0027autres (Angiulli et al. (2006), Charrad et al. (2008)). L\u0027approche Block Clustering (Hartigan (1972)) permet de diviser la matrice des données en plusieurs sous-matrices correspondant à des blocs. Le principe de base de cette méthode est de faire des permutations des lignes et des colonnes afin de définir la structure de bloc. De plus, l\u0027auteur Hartigan (1972) a proposé deux autres algorithmes de bi-partitionnement : le premier (One-Way Splitting) est principalement basé sur le partitionnement des observations en utilisant des fonctions ayant une variance intra-classe supérieure à un seuil donné afin de diviser la classe associée. Le second algorithme (Two-Way Splitting) procède par des divisions successives des lignes et des colonnes. Le même principe a été repris dans l\u0027approche CTWC proposée par Getz et al. (2000a). CTWC consiste à appliquer un algorithme de classification hiérarchique, le SPC (Super Paramagnetic Clustering) introduit par Getz et al. (2000b) sur les colonnes en utilisant toutes les lignes et vice versa.\nLes algorithmes de k-means ont longuement été utilisés dans le bi-partitionnement. En effet, Govaert (1983) a défini trois algorithmes de bi-partitionnement : Croeuc, Crobin et Croki2. Ces algorithmes consistent à déterminer une série de couples de partitions minimisant une foction de coût sur la matrice des données en appliquant la méthode des nuées dynamiques alternativement sur les lignes et les colonnes. Croeuc, Crobin et Croki2 diffèrent par le type des données à traiter. En effet, Croeuc est destiné à des données quantitatives. Crobin est appliqué sur des données binaires. Croki2 est utilisé pour un tableau de contingence.\nRécemment, de nouvelles approches de bi-partitionnement basées sur la décomposition matricielle sont proposées (Paatero et Tapper (1994), Long et al. (2005), Yoo et Choi (2010), Labiod et Nadif (2011), Shang et al. (2012). Les auteurs de Long et al. (2005) ont proposé une approche nommée NBVD qui décompose une matrice des données en trois composantes en procédant par un algorithme itératif appliqué sur des données non négatives. L\u0027approche nommée Coclustering Under Nonnegative Matrix Tri-Factorization (CUNMTF) introduite par Labiod et Nadif (2011) appartient à cette même famille. Les auteurs montrent que le double kmeans est équivalent à un problème algébrique de NMF sous certaines contraintes appropriées.\nLes méthodes de bi-partitionnement utilisant des cartes auto-organisatrices (SOM) (Kohonen et al. (2001)) ont été définis par plusieurs auteurs. Nous citons l\u0027approche DCC (Double Conjugated Clustering) de Busygin et al. (2002) et KDISJ (Kohonen for Disjonctive Table) de Cottrell et al. (2004) ainsi qu\u0027une autre variante récente introduite par Benabdeslem et Allab (2012). L\u0027inconvénient de la méthode DCC est l\u0027utilisation de deux cartes (une carte pour les observations et une carte pour les variables). Ces cartes sont construites indépendamment avec la même dimension. En ce qui concerne KDISJ, cette méthode est uniquement dédiée aux données catégorielles.\nDans ce papier, nous proposons une nouvelle approche (BiTM) de bi-partitionnement utilisant les cartes topologiques. BiTM ne nécessite aucune pré-organisation de la matrice des données en utilisant une seule carte qui représente simultanément la partition des observations et la partition des variables. Notre approche permet aussi de fournir de nouvelles visualisations. Le reste de cet article est organisé comme suit : dans la section 2, nous présentons le modèle et l\u0027algorithme, la section 3 est dédiée à la méthodologie et les résultats expérimentaux. Enfin, nous concluons cet article par une conclusion et quelques perspectives.\n2 Bi-partitionnement topologique : modèle BiTM Le modèle BiTM est constitué d\u0027un ensemble de cellules discrètes C de taille K appelées \"carte\". Cette carte a une topologie discrète définie comme un graphe non orienté, qui est généralement une grille à 2 dimensions. Pour chaque paire de cellules (c, r) de la carte, la distance ?(c, r) est définie par le plus court chemin reliant les cellules r et c sur la grille. Soit d l\u0027espace euclidien des données et D la matrice des données où chaque observation\nd . L\u0027objectif de BiTM est de fournir des bi-clusters organisés dans une carte topologique. Pour cela, l\u0027ensemble des lignes (observations) I \u003d {1, . . . , N } de la matrice des données D est partitionné en K groupes {P 1 , P 2 , . . . , P k , . . . , P K }. De même, l\u0027ensemble des colonnes\nNous définissons deux matrices binaires Z \u003d (z ik ) et W \u003d (w jl ) pour sauvegarder les informations associées respectivement aux observations et aux variables.\nOù ? est la fonction d\u0027affectation. Avec z ik et w jl , nous pouvons déterminer des blocs de données B l k \u003d {x ij |z ik × w jl \u003d 1}. Dans BiTM, chaque cellule c de C est associée à un prototype sous la forme d\u0027un vecteur :\nk . Nous proposons de minimiser la nouvelle fonction de coût suivante :\n) la fonction de voisinage. T représente la fonction contrôlant le rayon du voisinage. De même que pour les cartes auto-organisatrices, nous utilisons la fonction\n) pour définir le voisinage. La minimisation de˜Rde˜ de˜R(? w , ? z , G) est obtenue par l\u0027exécution itérative de 4 étapes jusqu\u0027à un nombre d\u0027itérations prédéfini (algorithme 1).\nL\u0027ordre topologique dans le modèle BiTM\nLa décomposition de la fonction de coût˜Rcoût˜ coût˜R qui dépend de la valeur de T , peut être réécrite de la manière suivante :\nLa fonction de coût˜Rcoût˜ coût˜R est décomposée en deux termes. Afin de maintenir l\u0027ordre topologique entre les blocs, la minimisation du premier terme entraîne le bloc qui correspond à deux cellules voisines. En effet, si les cellules c et r sont voisines dans la carte, la valeur de ?(r, k) est faible et dans ce cas, la valeur de K T (?(r, k)) est élevée. La minimisation du second terme correspond à la minimisation de l\u0027inertie des données locales affectées à un bloc B j r , j \u003d 1 . . . L. Pour différentes valeurs de T , chaque terme de la fonction de coût a une importance relative dans le processus de minimisation. On peut, donc, définir deux étapes pour l\u0027exploitation de l\u0027algorithme : Phase itérative 1-Affectation des observations : chaque observation x i est affectée au prototype g k le plus proche en utilisant la fonction d\u0027affectation :\n2-Mise à jour des prototypes : les vecteurs des prototypes sont mis à jour en fonction des affectations des observations :\n3-Affectation des variables : chaque variable x j est affectée au prototype g l k le plus proche en utilisant la fonction d\u0027affectation :\n4-Mise à jour des prototypes : les vecteurs des prototypes sont mis à jour en fonction des affectations des variables :\nRÉPÉTER les phases 1, 2, 3 et 4 jusqu\u0027à t \u003d t max .\n-La première étape correspond à des valeurs élevées de T . Si le premier terme est dominant alors la priorité est de préserver la topologie. -La deuxième étape correspond à des valeurs faibles de T où le deuxième terme est pris en compte dans la fonction de coût. Par conséquent, l\u0027adaptation locale et l\u0027algorithme BiTM converge vers l\u0027algorithme Crouec proposé par Govaert (1983).\nExpérimentations\nNous avons testé l\u0027algorithme BiTM avec des jeux de données de la base UCI (Frank et Asuncion (2010)). Le tableau 1 indique les paramètres de chaque jeu de données (nombre d\u0027observations, nombre de variables et nombre de classes réelles, ainsi que la taille de la carte utilisée pour l\u0027apprentissage). Afin d\u0027évaluer les performances de BiTM, nous avons utilisé trois indices, la pureté, le rand et le NMI (Normalized Mutual Information) (Strehl et al. (2002) \nComparaison de BiTM avec les approches de partitionnement\nPour cette première expérimentation, nous comparons les résultats de notre approche BiTM avec les approches suivantes : SOM classique (Kohonen et al. (2001)), HCL (Eisen et al. (1998)), NMF (Paatero et Tapper (1994)) et ONMTF (Long et al. (2005) \nComparaison de BiTM avec les approches de bi-partitionnement\nAfin de comparer BiTM avec les approches de bi-partitionnement, nous avons sélectionné trois approches : CTWC (Getz et al. (2000a)), NBVD (Long et al. (2005)) et CUNMTF (Labiod et Nadif (2011)). Les résultats expérimentaux sont présentés dans les tableaux 5, 6 et 7. Nous signalons que CTWC ne fournit pas de résultats avec la base Movement Libras.\nLe tableau 5 résume les résultats expérimentaux de l\u0027indice pureté. Nous remarquons que BiTM fournit les meilleurs résultats sur toutes les bases de données. Dans la plupart des cas, nous constatons une différence remarquable entre les résultats sur l\u0027indice pureté obtenu avec notre méthode et les autres approches. En effet, pour la base Movement libra par exemple, BiTM obtient 0.712, NBVD 0.33 et CUNMTF 0.333. La même constatation pour la base LungCancer où BiTM obtient 1, CTWC 0.718, NBVD 0.875 et CUNMTF 0.843. Nous observons aussi la difficulté d\u0027obtenir de grandes valeurs de l\u0027indice pureté pour la base isolet5.\nComme indiqué dans le tableau 6, BiTM fournit un indice de rand similaire et même meilleur que celui obtenu par les autres méthodes dans la majorité des cas.\nLe tableau 7 présente les résultats expérimentaux obtenus avec BiTM, CTWC, NBVD et CUNMTF avec l\u0027indice NMI. Notre approche BiTM fournit les plus hautes valeurs de l\u0027indice NMI pour toute les bases de données excepté pour la base glass. \nVisualisation\nDans cette section nous montrons l\u0027apport visuel de l\u0027approche proposée. Il est clair que BiTM se base sur les visualisations intuitives des cartes auto-organisatrices. Les figures 1(a), 1(b) et 1(c) sont des cartes de BiTM obtenues à partir du jeu de données isolet5. La figure 1(a) est dédiée à la visualisation de la base de données organisées en fonction des groupes de lignes et de colonnes. Cette figure peut être obtenue par toute méthode de bi-partitionnement. Cependant, en utilisant cette visualisation, il est difficile d\u0027analyser les blocs ou les bi-clusters obtenus. Afin de faciliter cette tâche, nous proposons de visualiser les bi-clusters en utilisant l\u0027organisation topologique du modèle de BiTM. Ainsi, chaque cellule de la carte est associée à la partition des observations et des variables. Cette organisation est illustrée par la figure 1(b). Par exemple, les groupes en haut à gauche de la carte ont des valeurs faibles représentées par des couleurs bleues. Tandis que ceux avec de fortes valeurs (au milieu en bas) sont représentés par des couleurs plus vives (rouge). La figure 1(c) indique la cardinalité de chaque cellule. Les cellules sont représentées par un carré dont la taille varie proportionnellement avec le nombre d\u0027observations associées.\nIl est également possible de zoomer sur chaque cellule de la carte pour analyser l\u0027organisation des observations et des variables dans chaque cellule. Les résultats obtenus en zoomant sur la carte 1(b) sont représentés dans la figure 2. Comme nous utilisons la notion du voisinage dans le modèle BiTM, nous constatons que la couleur est relativement similaire lorsque les variables sont proches.\nFinalement BiTM a l\u0027avantage de proposer une visualisation de la base de données et des bi-clusters. Ce résultat permet aux utilisateurs/experts une meilleure compréhension de la cohérence des données.\nConclusion et perspectives\nNous avons constaté après l\u0027étude comparative avec des méthodes de partitionnement et de bi-partitionnement que BiTM est une méthode de bi-partitionnement efficace. La princi-  pale nouveauté du BiTM est l\u0027utilisation d\u0027un modèle topologique pour organiser la matrice des données en blocs homogènes, tout en prenant en compte simultanément les lignes et les colonnes. La série d\u0027expériences que nous avons réalisées nous ont permis de valider notre méthode et d\u0027analyser ses performances à partir de nombreux critères. Ces résultats expéri-mentaux démontrent que notre algorithme identifie les bi-clusters et a de bonnes performances par rapport à certains algorithmes de classification croisée. Nombreuses sont les perspectives qu\u0027offre notre approche telle que l\u0027amélioration de l\u0027utilité de BiTM en l\u0027adaptant à des données binaires et mixtes.\n"
  },
  {
    "id": "416",
    "text": "Introduction\nL\u0027alignement d\u0027ontologies se positionne comme une pierre angulaire du Web sémantique. Il facilite la réconciliation des ressources décrites par des ontologies différentes. Ce processus permet la production des correspondances entre les entités de deux ontologies. Dans ce contexte, une multitude de méthodes d\u0027alignement ont émergé ces dernières années (Euzenat et al., 2011). Ces méthodes réussissent à produire une bonne qualité d\u0027alignement en se basant sur une configuration adéquate des paramètres. Ces paramètres sont fixés en amont au processus d\u0027alignement. Cependant, un tel paramétrage peut s\u0027avérer parfois non adéquat puisqu\u0027il ne prend pas en considération la nature intrinsèque des ontologies. A titre d\u0027exemple, le système FALCON-AO (Hu et Qu, 2008) est un outil d\u0027alignement comportant 21 paramètres différents, qui peuvent être fournis. Ces paramètres posent un problème quant à la détermination de leur combinaison optimale avant d\u0027entamer la phase d\u0027alignement.\nEn outre, il est à noter qu\u0027un paramétrage particulier ne peut pas être universellement optimal. Dans ce qui suit, nous proposons une nouvelle approche pour le paramétrage automatique d\u0027une méthode d\u0027alignement. Cette approche repose sur l\u0027exploitation de l\u0027intégrale de Choquet dans le but de déterminer une configuration optimale des paramètres au cours du processus d\u0027alignement en fonction des ontologies à aligner. L\u0027opération de paramétrage est caractérisée par un aspect totalement automatique et ne nécessitant pas l\u0027intervention de l\u0027utilisateur. Ainsi, les résultats encourageants, fournis après la phase d\u0027évaluation, montrent que le processus de paramétrage automatique s\u0027avère très intéressant.\nL\u0027intégrale de Choquet\nL\u0027intégrale de Choquet est considérée comme un opérateur d\u0027agrégation (Kaci, 2011). Il permet l\u0027amélioration de la puissance de l\u0027analyse multicritères par la prise en compte de l\u0027interaction entre les critères (Grabisch, 1996). En effet, cette notion permet de modéliser les phénomènes d\u0027interaction entre les critères et la dépendance préférentielle. Elle utilise des mesures floues pour prendre en compte l\u0027importance relative de chaque critère ainsi que les interactions mutuelles entre eux.  \n. ? a ?(nc) représentent les indices permutés des critères.\nLa plupart des méthodes d\u0027agrégation multicritères se basent sur la somme pondérée, qui met en valeur l\u0027importance de chaque critère indépendamment. L\u0027intégrale de Choquet se base sur la notion d\u0027interaction pour la résolution des problèmes multicritères. Elle permet de tenir compte de l\u0027importance de chaque critère mais aussi de l\u0027importance relative entre ces derniers. Pour cela, il faut distinguer la notion de l\u0027importance globale de chaque critère et l\u0027importance relative due à son interaction avec les autres. Outre les propriétés usuelles des opérateurs d\u0027agrégation et la modélisation de l\u0027importance relative des critères, la famille de l\u0027intégrale de Choquet a la distinction de permettre la représentation de phénomènes d\u0027interaction mutuelle qui peuvent exister. Cependant, pour interpréter le comportement de l\u0027intégrale de Choquet, nous sommes amenés à calculer deux indices, à savoir : l\u0027indice de Shapley et l\u0027indice d\u0027interaction entre les critères.\nL\u0027importance globale d\u0027un critère i n\u0027est pas déterminée uniquement par la mesure floue µ(i), mais elle prend en compte toutes les mesures µ(D) pour un sous-ensemble D ? N c de toutes les coalitions d pour i ? D. En effet, nous pouvons avoir, µ(i) quasiment nulle suggérant que le critère i est sans importance. Cependant, nous pouvons avoir en joignant i à une coalition D ? N c , une valeur de µ(D ? {i}) qui soit plus grande que µ(D) suggérant ainsi l\u0027importance du critère i dans la décision. Le calcul de l\u0027importance globale se base ainsi sur la notion d\u0027indice de Shapley, issue de la théorie des jeux coopératifs (Grabisch, 1996). Pour tout critère i l\u0027indice de Shapley est défini par : \nd\u0027interaction entre les critères i et j est la moyenne de la quantité de synergie entre i et j en présence d\u0027un groupe de critères D : \nAgrégation de similarité par l\u0027intégrale de Choquet\nLa forme la plus simple de l\u0027agrégation est la moyenne arithmétique ou pondérée. Ce type d\u0027opérateur n\u0027est pas adapté pour l\u0027agrégation des mesures de similarités vu qu\u0027il exige que les mesures donnent des valeurs de façon indépendante. De toute évidence, cette condition n\u0027est pas satisfaite. Ces mesures présentent une très grande interaction entre elles. Ainsi, l\u0027utilisation de la moyenne pondérée peut conduire à un résultat biaisé car un groupe de mesures très similaires peut facilement submerger d\u0027autres. Par conséquent, l\u0027utilisation de l\u0027intégrale de Choquet avec une fonction de capacité appropriée permet d\u0027éviter ce problème. L\u0027étape primordiale dans l\u0027utilisation de l\u0027intégrale de Choquet est de modéliser l\u0027interaction entre les mesures via une fonction de capacité adéquate. Pour réaliser cette tâche, nous avons opté à l\u0027utilisation de l\u0027approche proposée par (Marichal et Roubens, 2000). Les auteurs ramènent le problème à un programme linéaire qui tient compte des contraintes préférentielles du décideur. Pour maximiser la valeur de la correspondance, V corr \u003d ? 1 .SimT erm + ? 2 .SimT opo + ? 3 .SimSemant (composante terminologique SimT erm, composante topologique SimT opo et composante sémantique SimSemant), suivant les propriétés de chaque ontologie, il faut tenir compte d\u0027un certain nombre de contraintes : -(i) si les valeurs de SIMTERM et SIMTOPO sont plus importantes que celle de SimSemant, alors il est préférable de les favoriser, sinon favoriser SIMSEMANT ; -(ii) si la valeur de SIMTOPO est la plus petite, alors favoriser SIMTERM et SIMSEMANT ; -(iii) si la valeur de SIMTERM est la plus petite, alors favoriser SIMTOPO et SIMSEMANT.\nChacune de ces contraintes modélise l\u0027interaction positive ou négative qui peut exister entre les critères SIMTERM, SIMTOPO et SIMSEMANT. Les préférences sus décrites peuvent être représentées en utilisant un système linéaire. En outre, l\u0027utilisateur est amené à assigner un poids à chaque critère qui reflète son importance relative dans le cadre de chaque contrainte. En effet, la solution optimale à notre problème d\u0027agrégation revient à résoudre à ce système linéaire sur notre ensemble N c \u003d {SimT erm, SimT opo, SimSemant} :\nLa première contrainte donne une importance relative aux composantes terminologique et topologique ensemble. Pareillement, dans le cas d\u0027une faible contribution de la composante topologique, la deuxième contrainte favorise la coalition entre la composante terminologique et celle sémantique. La troisième contrainte favorise la jointure entre SIMTOPO et SIMSEMANT au détriment de SIMTERM ou toute autre jointure. Les trois dernières inégalités favorisent une seule composante parmi les trois, à condition que l\u0027interaction entre les deux composantes restantes soit négative. La résolution du système résultant de l\u0027ensemble de ces contraintes a été menée en utilisant Kappalab R package 1 (Grabisch et al., 2008).\nÉtude expérimentale\nL\u0027agrégation par l\u0027intégrale de Choquet a été appliquée sur trois mesures de similarité ré-sultant des trois modules d\u0027alignement. La base Benchmark 2 de la campagne OAEI 2012 a été utilisée pour évaluer notre méthode de paramétrage. La base Benchmark comporte 111 tests. Chaque test permet d\u0027évaluer la puissance de la méthode d\u0027alignement sur un aspect particulier. Cette base comporte 4 sous cas, qui diffèrent selon la taille, tout en gardant les mêmes caractéristiques. Nous avons opté à l\u0027utilisation de la sous base FINANCE. Les tests sont systématiquement dérivés d\u0027une ontologie de référence et introduisent à un certain nombre de fluctuations. Ces modifications permettent d\u0027évaluer le comportement de la méthode d\u0027alignement face aux changements subis par les ontologies objet du processus d\u0027alignement. En effet, ces ontologies montrent des altérations qui peuvent être catégorisées selon 6 niveaux, à savoir : -(i) les noms d\u0027entités peuvent être supprimés, remplacés par des synonymes ou traduits ; -(ii) les commentaires peuvent être supprimés ou traduits ; -(iii) les liens hiérarchiques peuvent être supprimés, étendus (i.e., par rapport à l\u0027ontologie 101) ou aplatis ; -(iv) les instances peuvent être supprimées ; -(v) les propriétés peuvent être supprimées ou ayant leurs restrictions de classes éliminées ; -(vi) les classes peuvent être multipliées ou réduites.\nUne interaction positive (ou de moins en moins négative), reflète un indice de Shapley aussi important. La figure 1, montre que pour l\u0027ontologie 101, SIMTERM a une importance qui dépasse SIMTOPO et SIMSEMANT. Avec l\u0027absence des noms d\u0027entités, la famille 20x marque une importance élevée pour SIMTOPO dépassant ainsi SIMTERM et SIMSEMANT. Les ontologies des familles 22x et 23x se distinguent par une morphologie structurelle pauvre (hiérarchie aplatie ou supprimée), ce qui défavorise la coalition de SIMTOPO avec les autres composantes. Les indices de Shapley pour les familles 24x, 25x et 26x sont très proches, vu que toutes les composantes interagissent négativement à ce niveau. De point de vue mesure de similarité, les trois composantes à la fois sont incapables de fournir de bonnes valeurs, puisque les ontologies des familles 24x, 25x et 26x sont doublement altérées, i.e., terminologiquement et structurellement. Comme l\u0027illustre la figure 2, et par rapport à un paramétrage figée, la moyenne de l\u0027amélioration varie entre 6% (la famille 25x) et 12% (la famille 20x).\nFIG. 1 -Indices de Shapley par familles de tests.\nFIG. 2 -Les valeurs de F-Mesure par famille de tests.\n"
  },
  {
    "id": "417",
    "text": "Introduction\nLa classification non supervisée est un problème étudié depuis plusieurs décennies, et récemment de nouvelles approches ont été développées pour s\u0027adapter au challenge induit par les nouvelles méthodes d\u0027acquisition automatique des données et le nombre croissant d\u0027application produisant des données massives. Ces données doivent être étudiées par des algorithmes suffisamment efficaces afin de pouvoir exploiter les connaissances qu\u0027elles contiennent. En procédant à une classification non supervisée, on cherche à construire des ensembles homogènes d\u0027individus, c\u0027est-à-dire partageant un certain nombre de caractéristiques identiques. Classiquement, les méthodes fonctionnent de la façon suivante : l\u0027utilisateur fixe le nombre de classes, un partitionnement est ensuite généré puis évalué par l\u0027utilisateur même ou par des critères d\u0027homogénéité, le partitionnement obtenu pouvant être remis en question selon son évaluation. Nous proposons une approche différente qui consiste à présenter et évaluer une par une les classes sans en fixer préalablement le nombre. L\u0027approche générique est basée sur un processus itératif qui va extraire les classes les unes après les autres permettant ainsi l\u0027exploration pas à pas des données. L\u0027approche propose à l\u0027utilisateur en priorité les classes les plus pertinentes (selon un critère donné) et lui laisse le soin de décider quand arrêter le processus. L\u0027approche peut être intégrée dans un système interactif qui lui permettra d\u0027étudier les classes individuellement ou les unes par rapport aux autres. Cet article est organisé de la façon suivante. Nous allons d\u0027abord présenter notre approche itérative ainsi que les critères utilisés pour l\u0027extraction de classes puis présenter certains de nos résultats expérimentaux avant de conclure.\nMéthode proposée\nNous proposons une approche itérative générique qui va extraire les classes les unes après les autres pour permettre une exploration pas à pas des données. Le processus itératif est répété à la demande de l\u0027utilisateur. À chaque itération, une nouvelle classe est extraite : une méthode d\u0027optimisation (un extracteur) recherche la meilleure classe à extraire selon un critère d\u0027é-valuation donné afin d\u0027obtenir un sous-ensemble de données homogène et séparé des autres objets. La classe est alors proposée à l\u0027utilisateur, qui va pouvoir l\u0027analyser à l\u0027aide d\u0027outils de visualisation interactifs. L\u0027utilisateur pourra ensuite demander au système d\u0027extraire une nouvelle classe, pour poursuivre son exploration des données. L\u0027extraction d\u0027une nouvelle classe prendra en compte les classes extraites précédemment.\nExtraction d\u0027une classe\nIl existe plusieurs méthode pour extraire une classe homogène à partir d\u0027un ensemble de données. Dans cet article, nous introduisons une méthode d\u0027extraction de classes basée sur la détection de limite de classe. Une classe est extraite à partir d\u0027un « centre » : nous calculons la distance entre chaque objet et le centre de la classe et cherchons alors la première augmentation abrupte dans ces valeurs qui indiquera la limite de la classe extraite. Un algorithme d\u0027optimisation pourra être utilisé pour chercher un centre de classe, qui produit une classe homogène selon un critère donné. Nous avons testé deux méthodes pour détecter la limite de la classe, la méthode CUSUM Basseville et Nikiforov (1993) et une méthode de détection de pics présentée dans Palshikar (2009), appliquée sur le différentiel des distances. Une fois la limite de classe évaluée, tous les objets qui ont une distance inférieure à cette limite appartiennent à la classe extraite.\nÉvaluation d\u0027une classe\nPour proposer à l\u0027utilisateur les classes les plus pertinentes, il faut définir des critères d\u0027évaluation. La plupart des critères d\u0027évaluation en classification non supervisée évaluent l\u0027ensemble de données dans son intégralité et ne donnent pas une évaluation des classes indépendamment les unes des autres. Nous proposons donc deux nouveaux critères d\u0027évaluation pour évaluer les classes indépendamment les unes des autres, dérivés de critères classiques en classification non supervisée. Dans l\u0027ensemble des critères définis, C k représente la k-ième classe extraite. Le premier critère est le rapport d\u0027inertie IR (rapport entre l\u0027inertie intra-classe et l\u0027inertie totale des données, normalisé par le nombre d\u0027objets dans la classe et dans l\u0027ensemble de données :\nLe second critère proposé est le rapport de limite de la classe CLR, représentant le rapport entre la distance du dernier objet de la classe sur la distance du premier objet hors de la classe :\nNous pouvons alors utiliser l\u0027un ou l\u0027autre des critères qui produisent une évaluation de la compacité d\u0027une classe sphérique. Cependant, comme chaque classe est extraite individuellement, nous devons également nous assurer que les classes extraites sont différentes les unes des autres. Nous proposons donc d\u0027ajouter une pénalisation des classes selon leur chevauchement avec les classes précédemment découvertes. Le critère de pénalité OP calcule une pé-nalité selon l\u0027intersection et l\u0027union de la classe extraite avec les classes précédentes (? ? 0 représente le poids de la pénalité selon l\u0027importance que l\u0027on donne aux chevauchements).\nMéthode de détection de limite\nUne première expérimentation a servie à étudier l\u0027impact de la méthode de détection de limite de la classe. Nous avons, pour cela, utilisé des données artificielles en deux dimensions constituées de trois classes (figure 1), deux étant relativement proches, la dernière plus éloignée. Pour cet ensemble de données, nous illustrons la méthode de détection de limite de la classe centrée à l\u0027origine et avons donc calculé les distances de chaque points à l\u0027origine. Sur la figure 1, nous pouvons voir en haut les nuages de points des classes et en bas le graphique des distances calculées. La méthode de détection de limite de classes consiste, alors, à appliquer la méthode CUSUM (figure 2) ainsi que la méthode de détection de pics (figure 3) sur les distances. On remarque sur la figure 2 que la méthode CUSUM permet de détecter seulement le plus grand changement (minimum de la fonction CUSUM) qui ne correspond pas forcément à la limite de la classe considérée. Cependant, comme nous pouvons le voir sur la figure 3, l\u0027approche basée sur les pics du différentiel de distances, permet de détecter plusieurs changements brusques sur les distances, il nous suffit, alors, de choisir le premier pic détecté pour déterminer la limite de la classe considérée.\nProcessus itératif\nUne seconde expérimentation a servi à évaluer l\u0027approche itérative d\u0027extraction de classes avec les différents critères proposés. Nous avons donc appliqué notre approche sur un ensemble de données artificiel simple à deux dimensions pour en expliquer aisément le fonctionnement. Les données ont été créées avec quatre distributions gaussiennes, trois d\u0027entre elles étant proches les unes des autres, la quatrième plus éloignée (cf. figure 4). Nous avons appliqué notre approche en utilisant la méthode de détection de pics pour déterminer la limite de la classe, le critère d\u0027évaluation CLR + OP . Sur la figure 5, nous pouvons voir la première classe extraite, il s\u0027agit de la classe isolée. Nous voyons, sur la figure 6, la deuxième classe extraite, qui représente un regroupement de trois petites classes proches les unes des autres. En continuant le processus, les trois classes (figures 7, 8 et 9) constituant la classe 2 sont extraites l\u0027une après l\u0027autre. On remarque alors (haut de la figure) que la pénalité de chevauchement s\u0027applique sur ces classes puisqu\u0027elles sont contenues dans la classe 2. Nous avons ensuite appliqué notre algorithme avec le critère d\u0027évaluation IR, sans modifier les autres paramètres. Sur la figure 10, on observe les cinq classes obtenues. Ici les trois classes proches les une des autres ont été découvertes individuellement avant la classe qui les englobe. Blansché (2012) la méthode CUSUM a été utilisée pour détecter la limite de la classe. Une fois une classe extraite dans un sous-ensemble de données, des méthodes de visualisation peuvent être utilisées pour l\u0027étudier. Nous présentons dans la figure 11, la première classe extraite sur l\u0027ensemble de données Ionosphere de l\u0027 UCI Blake et Merz (1998)  \nApplication au biclustering\nConclusion\nNous avons proposé dans cet article une nouvelle approche de classification non supervisée où les classes sont obtenues les unes après les autres suivant un processus itératif sans préciser Concernant les travaux futurs, nous avons prévu de mettre en place une plate-forme interactive d\u0027exploration pas à pas des données. L\u0027approche propose à l\u0027utilisateur en priorité les classes les plus pertinentes (selon un critère donné) et lui laisse le soin de décider, permettant ainsi à l\u0027utilisateur d\u0027étudier les classes individuellement ou les unes par rapport aux autres (treillis, visualisation,. . .).\n"
  },
  {
    "id": "418",
    "text": "Introduction\nL\u0027intégration de données vise à combiner des données issues de sources différentes afin de permettre leur exploitation à travers une interface d\u0027interrogation appelée schéma global. Le travail que nous présentons ici est une des phases de réalisation d\u0027un système d\u0027inté-gration de type médiation (approche non matérialisée) sémantique. Ce dernier est un triplet J \u003d G, S, MM (Lenzerini (2002)) où, dans notre proposition, le schéma global G et les schémas locaux S sont décrits par des ontologies. Nous construisons le schéma global semiautomatiquement à partir des schémas sources, ce qui permet de définir naturellement un mapping GAV. Pour autant, comme montré dans Niang et al. (2011), notre construction incrémen-tale de G induit que l\u0027ajout et le retrait de sources ont un coût de mise à jour de G très limité. Nous montrons ici qu\u0027en plus cela n\u0027a aucun effet sur le mapping que nous définissons, car ce mapping n\u0027est calculé qu\u0027au moment où une requête doit être évaluée, en fonction de l\u0027état courant de G et uniquement pour ce qui concerne la requête.\nNotre contribution vise une intégration dynamique au sein de communautés web. Une source peut rejoindre le système intégré le temps d\u0027échanger ses données ou de contribuer à une tâche particulière et peut se retirer à tout moment. Les collaborations qui se multiplient dans de nombreux domaines suscitent la constitution de solides ontologies de référence, publiquement accessibles. C\u0027est dans ce cadre que nous avons mis au point dans Niang et al. (2011) notre construction du schéma global, en exploitant une ontologie de référence. Ce schéma global est une TBox spécifiée en DL-Lite A , une logique de description (LD) (Calvanese et al. (2007)) connue pour traiter des requêtes dans un système d\u0027intégration avec une complexité LOGSPACE en la taille des données interrogées. Nous exploitons dans cet article cette LD pour la ré-écriture d\u0027une requête q, exprimée dans les termes de G, en des sous-requêtes ciblant des sources susceptibles d\u0027y répondre.\nProcessus de ré-écriture de requêtes\nNotre proposition dans Niang et al. (2011) pour la construction semi-automatique d\u0027une ontologie globale pouvant servir de support d\u0027intégration dans un système de médiation, synthétisée dans la figure 1, est scindée en deux phases. D\u0027abord, une ontologie intermédiaire appelée Accord (notée A dans la figure 1) est générée pour chaque source. Elle décrit la partie de l\u0027ontologie locale concernée par le processus d\u0027intégration. Cette phase s\u0027appuie sur des techniques d\u0027appariement d\u0027ontologies : ontologie source OL et ontologie de référence OM . Ensuite, chaque Accord est automatiquement et incrémentalement concilié dans l\u0027ontologie globale OG. La conciliation est faite en liant sémantiquement les concepts locaux via une taxonomie calculée à partir de l\u0027ontologie de référence OM . Les sources restent ainsi indépendantes les unes vis-à-vis des autres tout en étant liées sémantiquement dans l\u0027ontologie globale. Notre proposition a été d\u0027abord construite dans le cadre du projet SIC-Sénégal ayant pour but de permettre à plusieurs partenaires travaillant sur le bassin du fleuve Sénégal de partager et intégrer leurs données, entre autres agricoles. Dans l\u0027exemple donné dans le tableau 1 1 , l\u0027ontologie globale G est représentée sous forme d\u0027une TBox T g telle que T g \u003d {T ai }, T m , où (i) {T ai } est l\u0027ensemble des TBox des accords A si construits à partir des ontologies locales des sources S i , et (ii) T m est calculée à partir de l\u0027ontologie de référence pour concilier les différents T ai dans T g . Ce tableau 1 montre un extrait de T g conciliant T a1 et T a2 , des deux accords A s1 et A s2 . Les expressions de T a1 et T a2 expriment le fait que les sources S 1 et S 2 , à partir desquelles ont été respectivement construits A s1 et A s2 , détiennent des informations sur (1) des quantités de production de variétés de culture, ici : tomate, oignon et sorgho, et (2) des statistiques agricoles concernant des tomates et du riz paddy, lesquels ont un attribut prix. Les expressions de T m , située en bas du tableau, concilient T a1 et T a2 par le fait que, par exemple : oignon de T a1 et tomate de T a2 sont des légumes, sorgho de T a1 et riz de T a2 sont des céréales, les céréales et les légumes sont des produits végétaux, mais sont disjoints.\nTAB. 1 -Extrait d\u0027une TBox globale T g conciliant les Accords de deux sources.\nConsidérons dans ce qui suit (i) l\u0027ontologie globale G, dont T g est celle montrée dans le tableau 1 ; (ii) les deux sources S 1 et S 2 dont les accords A s1 et A s2 sont ceux conciliés par T g , et (iii) la requête conjonctive suivante exprimée dans les termes de G :\nq(x, a, b) ? Legume(x), prix(x, a), production(x, b) Cette requête demande le prix et la production de légumes. Nous décrivons dans la suite comment nous pouvons y répondre en reportant l\u0027interrogation sur les sources S1 et S2.\nDéfinition de règles servant au calcul des mappings\nEn partant uniquement des informations présentes dans T a1 et T a2 des Accords A s1 et A s2 (tableau 1), ni S 1 ni S 2 ne peuvent répondre à q, ne serait-ce que parce que le prédicat Legume de q n\u0027est défini dans aucun de leur vocabulaire. Cependant, si on considère T m -taxonomie située dans la partie basse du tableau 1 et reliant les concepts des T ai , montrant dans notre exemple que les oignons et les tomates sont des légumes -il devient envisageable que S 1 et S 2 puissent permettre de construire une réponse pour q. La première étape de notre processus de ré-écriture est d\u0027utiliser les relations sémantiques définies dans T g \u003d {T ai }, T m afin de ramener les atomes d\u0027une requête q, exprimée sur T g , vers les sources qu\u0027ils impliquent (S i est représentée par T ai ). Pour ce faire, nous définissons un ensemble de règles sémantiques qui permettent par un raisonnement par chainage avant de déduire de T g les expressions de mapping correspondant aux atomes de la requête. Ces expressions sont de la forme : Appliquées en chainage avant, les règles fournies en haut de la prochaine page permettent de calculer un ensemble M g d\u0027expressions de mapping pour chaque atome g de q. A chaque fois, l\u0027atome g est utilisé comme déclencheur du raisonnement et T g comme base de faits initiale. Le résultat est un ensemble d\u0027expressions M g de la forme g(v) ? S i (?(w)).\nLes règles (1) à (3) correspondent au cas où l\u0027atome est un concept atomique A(x). Dans (1), concept(A, T ai ) est vrai si A apparaît en position de concept dans une expression de T ai .\nDans ce cas, S i peut renseigner sur A, ce qu\u0027indique l\u0027expression de mapping en conclusion de la règle. Ainsi toutes les sources contenant ce concept seront considérées par le processus de ré-écriture. (2) et (3) permettent de rechercher des sous-concepts qui seraient dans les sources. Grâce à elles, en partant de l\u0027atome Legume(x) on peut arriver à Tomate(x) de S 2 et Oignon(x) et Tomate(x) de S 1 . Les règles (4) à (7) peuvent se comprendre comme les trois premières mais s\u0027appliquent à un attribut atomique U (x, y). Elles servent à rechercher d\u0027éventuels sousattributs au sein d\u0027une même source. La règle (7) permet, dans une même source, d\u0027interroger l\u0027attribut d\u0027un concept sur ses sous-concepts. Les règles (8) à (13) concernent un rôle atomique P (x, y), les 2 premières fonctionnant comme (1) pour le concept ou (4) pour l\u0027attribut : elles s\u0027appliquent s\u0027il y a dans une T ai une expression impliquant ce rôle. (10) et (11) permettent de rechercher des sous-rôles (dans une même source) quand (12) et (13) servent à interroger des rôles sur des sous-concepts (dans une même source). Nous donnons ci-après les mappings obtenus en appliquant les règles (1) à (13) pour chaque atome de la requête q :\nLes mappings de M prix ne ramènent qu\u0027à S 2 qui est capable de fournir le prix d\u0027instances de tomate et de riz, tandis que les mappings de M production indiquent que la source S 1 est capable d\u0027apporter des réponses sur la production de variétés de cultures, d\u0027oignons et de tomates. Les réponses qui peuvent être obtenues à partir de S 1 et S 2 sont partielles puisque S1 ne peut fournir que la production (de tomate ou d\u0027oignon) et S 2 que le prix (de tomate ou de riz). C\u0027est en combinant les différents mappings que l\u0027on aura une réponse globale pour q, c\u0027est l\u0027objet de la prochaine sous-section.\nDépliage de requêtes globales et validation des ré-écritures obtenues\nLe dépliage d\u0027une requête q(x) ? g 1 (z 1 ), ..., g n (z n ) par rapport à un ensemble de mappings GAV g i (x i ) ? q i (x i , y i ) est la requête u obtenue en remplaçant dans q chaque atome g i (z i ) par q i (? i (x i , y i )), où ? i est une fonction qui associe x i à z i , et y i à de nouvelles variables. Chaque dépliage u sert à calculer une partie du résultat que le système intégré peut fournir pour q. L\u0027union de ces parties constitue l\u0027ensemble des réponses à q. L\u0027ensemble des dépliages u de q est obtenu en calculant le produit cartésien R \u003d M g1 × ... × M gn . Chaque requête q r de ce produit est une ré-écriture candidate. En considérant les mappings obtenus précédemment pour les atomes de la requête q, résumés dans le tableau 2, certaines des 18 ré-écritures candidates de q sont les suivantes :\nTAB. 2 -Résumé des sources pouvant répondre pour chaque atome de la requête q.\nUne ré-écriture candidate ne constitue pas forcément une ré-écriture valide de q. A titre d\u0027exemple, q r15 qui demande pour S 1 la production de tomates et pour S 2 le prix d\u0027oignons n\u0027est pas valide. Les tomates et les oignons sont certes des légumes, mais l\u0027expression Tomate ¬ Oignon déclarée dans la TBox T g indique que les tomates et les oignons sont disjoints. Pour vérifier la consistance des dépliages, nous utilisons l\u0027algorithme Consistent de Calvanese et al. (2007) qui construit des requêtes booléennes q unsat à partir des inclusions négatives présentes dans T g , qui représentent des contraintes d\u0027intégrité servant à assurer la consistance des réponses apportées par le système d\u0027intégration. Considérons ci-après les requêtes q unsat obtenues à partir de T g du tableau 1 et qui concernent les atomes de q : q unsat1 ? Tomate(x), Oignon(x) -car Tomate ¬ Oignon est dans T g q unsat2 ? Tomate(x), Riz_paddy(x) -inféré de Legume ¬ Cereale q unsat3 ? Oignon(x), Riz_paddy(x) -idem Afin de vérifier la consistance d\u0027une ré-écriture candidate q r , nous remplaçons d\u0027abord dans q r les requêtes sur les sources par les requêtes correspondantes sur le schéma global. Dans notre contexte il suffit de retirer toute mention des sources. Soit q r le résultat de cette étape, il faut ensuite appliquer l\u0027algorithme Consistent en évaluant les requêtes q unsat sur le corps de q r considéré comme une instance canonique. Si le résultat de l\u0027évaluation d\u0027une des requêtes q unsat est true alors la ré-écriture q r est invalide. A titre d\u0027exemple, soit la requête q r15 suivante, obtenue à partir de q r15 : q r15 (x,y,z)?Tomate(x), prix(x,y), Tomate(x), production(x,z), Oignon(x) \n"
  },
  {
    "id": "419",
    "text": "Introduction\nActuellement, de nombreuses recherches (Stein et al., 2007) traitent de la recherche de similitudes notamment à cause de l\u0027augmentation importante du plagiat sous toutes ses formes et dans tous les domaines : l\u0027enseignement avec les élèves et étudiants (quatre étudiants sur cinq déclarent avoir recours au copier-coller), la recherche scientifique avec les publications et thèses (Bao et Malcolm, 2006) (plagiat de thèses notamment) et l\u0027industrie avec les problèmes de copie de brevets ou de codes sources. Les outils existants pour rechercher des documents similaires sont principalement basés sur la recherche de segments dits n-gram (n représentant la taille en mots du segments) identiques (Oberreuter et al., 2010) pour détecter les copies et commencent tout juste à proposer la détection de copie par traduction dite la copie inter-langue (Kent et Salim, 2009) à travers les travaux de ces dix dernières années. L\u0027approche proposée consiste à prendre en compte le document comme une agrégation de documents plus petits et récursivement que chacun des documents le composant soit lui-même l\u0027agrégation de documents plus petits. Cette hiérarchie permet de déterminer des mots-clés à chacun des niveaux et ainsi détecter des similitudes normalement indétectables à l\u0027échelle globale. Cette approche repose sur l\u0027hypothèse que lorsqu\u0027on paraphrase ou reformule un texte, on garde le sens de celui-ci et ainsi on garde les mots-clés principaux, porteurs du plus haut niveau sémantique du texte.\nAprès avoir présenté rapidement l\u0027état de l\u0027art et l\u0027approche, nous décrivons d\u0027abord comment extraire et utiliser des mots-clés, puis nous présenterons les niveaux hiérarchiques proposés (taille, organisation, obtention). Enfin, nous présentons l\u0027évaluation de notre approche en la comparant à la méthode classique n-gram.\n2 État de l\u0027art et approche\nLa notion de similitude\nUne similitude est un rapport, une relation qui existe entre deux choses semblables. Cela peut aller de la simple ressemblance jusqu\u0027à l\u0027identité. Lorsqu\u0027on parle de similitude textuelle ou de document similaire, on distingue plusieurs types de similitudes allant de la ressemblance à l\u0027identité : la reformulation qui consiste à reprendre la sémantique d\u0027un texte et à l\u0027exprimer différemment ; la paraphrase qui consiste à reprendre les éléments d\u0027un texte, dans l\u0027ordre d\u0027origine mais en les formulant différemment normalement dans le but d\u0027éclairer, d\u0027expliciter, ou pour développer certains points ; la citation qui consiste à faire la copie mots à mots d\u0027une portion de texte en informant le lecteur de l\u0027origine extérieure de celle-ci ; la copie qui consiste à faire la copie mots à mots d\u0027un texte ou d\u0027une partie d\u0027un texte sans citer la source ; et la traduction qui consiste à faire la copie mots à mots d\u0027un texte ou d\u0027une partie d\u0027un texte sans citer la source et en traduisant dans une autre langue. Selon la langue, la forme finale est donc potentiellement différente de l\u0027originale.\nRecherche et comparaison de documents\nLa recherche de documents s\u0027effectue selon deux approches. La première est basée sur le style de l\u0027auteur, elle est dite stylistique (Iyer et Singh, 2005), part du postulat (Jardino et al., 2005) que deux textes du même auteur contiennent un grand nombre de correspondance et qu\u0027inversement, deux textes de deux auteurs différents contiennent un petit nombre de correspondances. On recherche de documents présentant un style identique ou approchant en utilisant la structure de phrase, la grammaire ou par observation stylistique (Stamatatos, 2009). Cette approche est plutôt récente et bien que très rapide, elle pose encore un certain nombre de problème notamment sur l\u0027aspect de la précision. La seconde approche est basée sur le contenu du document (White et Joy, 2004;Iyer et Singh, 2005;Eissen et Stein, 2006)  \nNotre approche\nNotre approche consiste à extraire sur chaque document les mots-clés principaux d\u0027un document dans sa globalité mais également dans les sous-documents construits par découpages successifs. Pour chaque niveau hiérarchique, on combine les mots-clés 3 par 3 pour former des triplets de recherche.\nExtraction et utilisation de mots-clés\nLa notion de mots-clés\nLe terme mot-clé désigne de manière générale un mot qui a une importance particulière et il est notamment utilisé lors des recherches d\u0027informations. Il existe plusieurs manières d\u0027extraire des mots-clés afin de garantir leur pertinence. La plus \u0027traditionnelle\u0027 est la méthode fréquentielle. Plus un mot-clé apparaît souvent, plus il est important. En ajoutant le regroupement par racine d\u0027un même mot (regroupement de \u0027cheval\u0027 et de \u0027chevaux\u0027 sous le lemme \u0027cheva*\u0027 par exemple) et en n\u0027utilisant que les mots les plus informatifs sémantiquement, on obtient les mots-clés théoriquement les plus représentatifs du contenu sémantique d\u0027un texte.\nLes groupes de mots-clés\nA partir de la liste des mots-clés trouvés, il est important de savoir si il faut chercher les documents similaires avec un, deux, trois ou plus, mots-clés combinés. Afin de déterminer ce nombre, nous avons utilisé une petite base de documents sur laquelle nous avons extrait les 10 principaux mots-clés extraits sur l\u0027analyse de l\u0027intégralité du document. Puis ils ont été utilisés pour retrouver le document.\nLe tableau 1 présente la pertinence de recherche d\u0027un document par nombre de mots-clés utilisés dans la recherche. Le nombre de requêtes de recherche variant d\u0027un minimum de 100 requêtes à un maximum de 25200. Au final, on observe un bon compromis entre résultats et nombre de requêtes effectuées en utilisant des groupes de 3 mots-clés et avec des résultats allant de 72 à 87% de récupération du document tout en étant à 120 requêtes par document. Le passage à 4 mots-clés ne fait gagner qu\u00270,5% de récupération supplémentaire alors qu\u0027il double le nombre de requêtes (passage de 120 à 210 par document).\nNiveaux hiérarchiques 4.1 Méthodes de segmentation\nIl existe de nombreuses manières de procéder à la segmentation d\u0027un texte. Bien qu\u0027il existe des méthodes indépendante du formalisme, ou des méthodes par modèles thématiques, les principales sont encore le segmentation sémantique qui consiste à découper en fonction de bloc logique lié à la rédaction ou à la lecture (découpage par parties, par paragraphes, par sous-paragraphes, par phrases, découpage par ponctuation) et la segmentation dimensionnelle qui consiste à découper en bloc de dimension donnée sans prendre en compte le contenu du document. Cette segmentation peut elle-même être effectuée de manière absolue (découpage par bloc de 1000 mots, 500 mots...) ou de manière relative (découpage par bloc couvrant 25%, 10%...).\nTaille et pertinence des différents niveaux\nLes mots-clés sont extraits principalement par méthode fréquentielle ce qui implique qu\u0027il soit nécessaire d\u0027avoir un nombre minimum de mots pour générer des mots-clés. Statistiquement, des segments de moins de 40 mots ne comportent jamais de mots-clés intéressants, ceux de moins de 70 mots très rarement et au mieux, des mots représentés 2 fois au maximum. En effet, les auteurs ont tendance à éviter de faire des répétitions dans des zones trop proches. Ainsi, la méthode fréquentielle ne peut s\u0027appliquer que sur des segments d\u0027au moins 70 mots. Il est impossible de travailler avec des phrases, rarement avec des paragraphes. La taille minimale des segments doit donc être entre 70 et 100 mots afin d\u0027obtenir un nombre minimal d\u0027occurrences des mots.\nDistribution des segments\nDans le cas de la segmentation dimensionnelle absolue, on peut légitimement se demander la pertinence de la position d\u0027un segment de taille donnée dans l\u0027ensemble du document qui pourrait avoir une multitude de positions différentes. Une approche naïve consiste à positionner un découpage donné comme une sous-partie directe d\u0027une partie plus grande et de procéder par découpage successif c\u0027est à dire un découpage par partie disjointe. Dans le but de couvrir des segments différents et potentiellement intéressants, une alternative est d\u0027utiliser la taille du segment pour créer un masque de récupération que l\u0027on utilise en balayant le texte pour géné-rer plusieurs segments non disjoints i.e. avec une intersection non vide. Dans l\u0027approche par segments non disjoints, il convient de définir le déplacement du masque (pas de masque) ou le nombre maximal de parties acceptables par niveau hiérarchique (le pas étant alors égal au rapport taille du document / nombre de parties). L\u0027expérimentation montre qu\u0027un pas de masque du tiers de la taille du document permet d\u0027obtenir des résultats satisfaisants sans augmenter de manière trop importante le nombre de segments générés.\nÉvaluation et tests\nLa base de tests et protocole\nLa base de tests est composée de 200 textes de 2500 mots environ dont on dispose des sources effectivement utilisées pour les écrire. Certains reprises sont de la copie au mot près (copie ou citation), d\u0027autres sont des paraphrases ou même des reformulations. La taille des sections copiées allant de la simple phrase (6-8 mots) à celle d\u0027une partie complète (1000 mots). On peut avoir plusieurs reprises dans un même document. Les sources utilisées sont disponibles en ligne et d\u0027origine diverses : Wikipédia, publications de recherche, mémoires de stage, article de presse. Le protocole est composé d\u0027une extraction de mots-clés de manière hiérarchique sans et avec glissement puis la recherche de documents en utilisant les triplets de mots-clés parmi ceux extraits. L\u0027ensemble des documents utilisés est accessible librement sur internet. La recherche est également effectuée en utilisant la méthode traditionnelle des n-gram avec n égal à 8. Les résultats sont présentés dans le tableau 1 et permettent d\u0027observer les différents résul-tats avec l\u0027approche mots-clés et avec l\u0027approche n-grammes. L\u0027approche mots-clés présente plusieurs points plutôt positifs. Elle permet la détection des documents similaires présentant des similitudes supérieurs à 100 mots à 90% bien qu\u0027il y ait des disparités entre types de document (83% copie, 69% paraphrase, 37% reformulation) mais aussi entre sources (94% internet, 75% articles et 60% mémoire). Elle permet également de trouver des similitudes de type paraphrase ou reformulations impossible à trouver avec l\u0027approche n-gram (sauf si inférieur à 100 mots). Enfin, elle est trois fois plus rapide que l\u0027approche n-gram en calcul et 40% plus rapide dans l\u0027exécution des requêtes moteur (simplicité de requête). L\u0027ajout du glissement permet l\u0027amélioration des résultats de l\u0027ordre de 5% mais en ajoutant environ 5% de requêtes.\nRésultats\n"
  },
  {
    "id": "420",
    "text": "Introduction\nLe contexte de nos recherches est celui de l\u0027aide à l\u0027observation de l\u0027activité sur simulateurs pleine échelle du groupe EDF utilisés pour la formation et le perfectionnement des agents de conduite de centrale nucléaire (Pastré, 2005), (Champalle et al., 2011). Ce projet de recherche est mené avec l\u0027Unité de Formation Production Ingénierie (UFPI) d\u0027EDF. L\u0027UFPI forme les personnels d\u0027EDF dans les métiers de la production d\u0027électricité. Parmi ses formations, l\u0027UFPI forme des opérateurs de conduite de centrales nucléaires. Pour cela, les formateurs organisent des séances de simulation sur simulateurs pleine échelle, ré-plique intégrale à l\u0027échelle 1 des salles de commande des centrales. Durant les simulations, les formateurs pilotent le simulateur et observent les réalisations des opérateurs. Ces derniers doivent réaliser un transitoire. Il s\u0027agit de faire passer le simulateur d\u0027un état initial e0 à un instant t0 à un état final n à tn. Pour cela, les opérateurs conduisent l\u0027installation selon des actions attendues organisées en familles d\u0027objectifs pédagogiques elles mêmes déclinées en objectifs pédagogiques de plus bas niveaux. Ces actions sont réparties dans des grilles d\u0027observation complétées par les formateurs (Agency, 2004). L\u0027observation et l\u0027analyse des interactions individuelles et collectives des opérateurs est une activité critique et particulièrement dense (Samurçay et Rogalski, 1998). Afin de limiter la surcharge cognitive inhérente à ces tâches, les simulateurs disposent d\u0027outils permettant d\u0027enregistrer l\u0027activité des opérateurs tels que les journaux de bord, la vidéo, la téléphonie, etc. Les données collectées permettent aux formateurs de revenir sur les difficultés rencontrées par les opérateurs afin de leur apporter des solutions pour améliorer leurs pratiques. Ces données sont cependant difficilement exploitables dû à leur grande quantité et à leur très bas niveau néces-sitant une expertise forte que ne possède pas les jeunes formateurs. Notre objectif est donc de proposer des modèles et des outils afin d\u0027apporter aux formateurs une aide à l\u0027observation à l\u0027analyse et au débriefing des activités des opérateurs. Pour cela, notre approche est basée sur la modélisation, la représentation et la transformation des traces d\u0027activités des simulations. Cet article est organisé comme suit : la section 2 présente un état de l\u0027art et un positionnement par rapport aux systèmes d\u0027observation et d\u0027analyse des activités de opérateurs dans des environnements de formation. La section 3 présente le principe de notre approche d\u0027observation et d\u0027analyse des activités des opérateurs. La section 4 détaille les modèles de trace et de transformation que nous proposons. La dernière section présente nos conclusions et perspectives.\nTravaux relatifs\nExploiter les traces numériques de l\u0027activité de opérateurs est une pratique répandue dans les environnements informatisés dédiés à la formation. Dans cette partie nous étudions diffé-rentes approches exploitant les traces numériques d\u0027activité suivant deux angles :\n-l\u0027assistance aux formateurs dans l\u0027observation, l\u0027analyse et la conduite du débriefing ;  (Dunand et al., 1989), assiste le formateur dans le débriefing des évaluations des agents de conduite d\u0027EDF. Basé sur un système expert, SEPIA enregistre les paramètres du simulateur et les actions des opérateurs afin de fournir, lors du débriefing, une correction du scénario. Le projet PPTS (Pedagogical Platoon Training System) (Joab et al., 2002) assiste des formateurs dans le suivi et l\u0027analyse des manoeuvres des équipages de simulateurs de char LE-CLERC. PPTS utilise un ITS afin d\u0027exploiter les traces numériques de la simulation et faire émerger les niveaux de compétences attendus : technique, tactique et stratégique. A la fin de la simulation, PPTS génère une synthèse et des remarques sur les compétences des équipages. Les outils présentés ci-dessus utilisent les traces numériques de l\u0027activité des stagiaires afin de diagnostiquer et analyser leurs comportements. Ils sont cependant basés sur des systèmes fermés dont la mise en oeuvre est généralement lourde, et nécessitent une longue et étroite collaboration avec les experts. Leurs connaissances sont de plus \"statiques\" et il n\u0027est pas possible pour le formateur  -M-trace première : les observés sont issus des données collectées par les sources de traçage du simulateur tels que les journaux de bord et les annotations vidéo ; -M-trace des objectifs pédagogiques : les observés représentent les attendus que les opé-rateurs doivent valider tels que \"acquitter l\u0027alarme\" ou \"ajuster la pressions\" ; -M-trace famille d\u0027objectifs pédagogiques : les observés correspondent aux objectifs gé-néraux de la formation tels que \"conduire l\u0027installation\" ou \"travailler en équipe\". Ces niveaux de m-trace sont obtenus via des transformations à base de règles. Comme le montre la figure 1, chaque observé d\u0027une trace de niveau n est en relation avec ses observés d\u0027origines de la trace de niveau n ? 1. Les observés de la M-trace première, sont quant à eux en relations avec les données collectées par le simulateur.\nFIG. 1 -Principe d\u0027analyse par transformation et visualisation de traces\nUne telle organisation permet aux formateurs d\u0027explorer, d\u0027analyser et de mieux comprendre les raisons, collectives ou individuelles, des réussites et des échecs des opérateurs pour préparer et conduire les sessions de débriefing. Par exemple, si le formateur veut comprendre les raisons pour lesquelles l\u0027observé \"Gestes professionnels\" de la trace \"famille d\u0027objectifs pé-dagogiques\" est KO 2 (voir la figure 1), il peut naviguer dans ses différents observés origines, à savoir \"Régulation tension alternateur\", \"Utiliser les bonnes consignes\", \"Régulation tempé-rature\" et \"Pression ajustée\". Selon la règle 9, pour que l\u0027observé \"Gestes professionnels\" soit OK, il faut que tous ses observés origines soient OK. Dans la mesure où un de ces observés est KO, l\u0027observé \"Gestes professionnels\" est KO lui aussi.\nModèle de trace et de transformation.\nQuel que soit le niveau de la trace, son modèle et le simulateur, nous considérons qu\u0027une m-trace doit embarquer son identité afin d\u0027être localisable et exploitable dans le temps pour des traitements statistiques ou des analyses à grande échelle. Ainsi, toute m-trace possède un identifiant unique, une date de début et de fin, un niveau, un type de simulateur, un type de formation (initiale ou maintien de compétences), une catégorie de formation (sommative ou formative) ainsi qu\u0027un scénario de simulation (ilotage, perte d\u0027alimentation, etc.) ( figure 2(1)). Chaque type d\u0027observé recensé est caractérisé par un identifiant, une date de début et de fin, un label, l\u0027identifiant du sujet générateur, sa nature (évalué ou non), son rôle (opérateur, superviseur,etc.) et un attribut de réalisation (OK ou KO). Ce modèle peut être spécialisé pour ajouter des attributs liés à l\u0027action tracée. Afin de permettre l\u0027exploration des niveaux de traces, chaque observé possède un lien vers sa règle et ses observés origines, ou vers les données collectées par les sources de traçage s\u0027il s\u0027agit de la trace première.\nUne transformation génère une trace cible de niveau n à partir d\u0027une trace source de niveau n ? 1. Comme le montre la figure 2(2), les transformations sont basées sur un ensemble de \nConclusion et perspectives\nCet article traite de la problématique de l\u0027observation et de l\u0027analyse de l\u0027activité sur simulateur pleine échelle. Ce travail de recherche, mené en partenariat avec l\u0027UFPI d\u0027EDF, est appliqué dans le cadre des formations et du maintien de compétences des opérateurs de conduite de centrale nucléaire. L\u0027objectif est de de proposer des modèles et des outils pour assister les formateurs dans les phases de préparation et de conduite des débriefings. L\u0027approche que nous avons proposée consiste à transformer les traces premières, issues des données collectées par le simulateur, afin d\u0027extraire des informations de haut niveau sur l\u0027activité des opérateurs. Nous avons pour cela proposé des modèles de trace et de transformation adaptés aux spécificités de nos propositions telles l\u0027exploration de M-Traces et la saisie et le partage des connaissances d\u0027observations des formateurs. Sur la base de nos modèles, nous avons également conçu un prototype, appelé D3KODE (Define, Discover, and Disseminate Knowledge from Observation to Develop Expertise), pour stocker, transformer et visualiser les traces. Ce prototype à été éva-lué sur la base d\u0027un protocole comparatif mené avec une équipe de 8 formateurs de l\u0027UFPI. Le dépouillement et l\u0027analyse des résultats est en cours de réalisation. Nos travaux futurs visent à traiter le deuxième objectif du projet qui concerne l\u0027exploitation des traces pour le retour d\u0027expérience afin d\u0027affiner les besoins et optimiser les programmes de formation des années à venir.\n"
  },
  {
    "id": "421",
    "text": "Introduction\nLa sélection de variables joue un rôle très important en classification lorsqu\u0027un grand nombre de variables sont disponibles. Ainsi, certaines variables peuvent être peu significatives, corrélées ou non pertinentes. La sélection de variables permet également d\u0027accélérer l\u0027étape d\u0027apprentissage et de réduire la complexité des algorithmes. Une méthode de sélection repose principalement sur un algorithme de recherche et un critère d\u0027évaluation pour mesurer la pertinence des sous-ensembles potentiels de variables.\nEn apprentissage supervisé, la sélection de variables a largement été étudiée car il est connu que la sélection de variables peut améliorer la qualité d\u0027un classificateur (Zhang et al., 2009). Parmi les méthodes supervisées, nous citons le coefficient de corrélation de Pearson (Rodgers et Nicewander, 1988), le score de Fisher (Duda et al., 2000) et le gain de l\u0027information (Cover et Thomas, 2006). La sélection de variables a reçu peu d\u0027attention en apprentissage non supervisé en comparaison au cas supervisé. Le problème devient plus difficile en raison de l\u0027absence des étiquettes des classes pour guider la sélection. Ainsi se pose la question importante, comment évaluer la pertinence d\u0027un sous-ensemble de fonctionnalités sans avoir recours aux étiquettes de classe ? Dans la littérature deux approches sont souvent utilisées pour éva-luer la pertinence d\u0027un sous-ensemble de variables sélectionnées, (Kohavi et John, 1997;Yu et Liu, 2003) : l\u0027approche de type filtrage (filter approach) et celle de type enveloppante (wrapper approach). Les approches enveloppantes évaluent les variables en utilisant un algorithme d\u0027apprentissage qui sera finalement utilisé dans le processus de classement. Cependant, les mé-thodes enveloppantes sont généralement coûteuses en temps et ne peuvent pas être appliqués sur de grandes masses de données, (Kohavi et John, 1997). Ce sont les méthodes de type filtrage qui nous ont intéressés, car elles sont beaucoup plus efficaces. Les critères d\u0027évaluation sont totalement indépendants du discriminateur utilisé. Les variables sont alors traitées avant le processus d\u0027apprentissage. Les travaux de (Caruana et Freitag, 1994;John et al., 1994;Koller et Sahami, 1996) sur la sélection de variables montrent les différentes approches traitant ce problème d\u0027optimisation. Parmi les méthodes de sélection d\u0027attributs dans un contexte non supervisé, nous nous sommes intéressés principalement au score Laplacien qui est le critère d\u0027évaluation le plus utilisé dans la littérature.\nPlusieurs travaux ont tenté d\u0027exploiter le principe du Score Laplacien (SL). Dans (Benabdeslem et Hindawi, 2011), les auteurs proposent une variante du SL qui utilise deux types de contraintes semi-supervisé sur les données : des contraintes Must-Link et des contraintes Cannot-Link. Ce score calcule la variance entre les données qui n\u0027ont pas la même étiquette. Dans (Cai et al., 2010) les auteurs ont proposé un nouveau score appelé MCFS. Cette méthode vise à sélectionner les variables de manière à conserver la structure multi-cluster des données. MCFS mesure les corrélations entre variables d\u0027une manière non supervisée, c\u0027est une méthode efficace pour traiter de grande dimension, mais limitée par le choix du nombre de classes.\nDans (Zhang et hua Zhou Songcan Chen, 2007) les auteurs utilisent le même principe en proposant une nouvelle méthode appelée SSDR (Semi-supervised dimensionality reduction). Cette approche préserve la structure des données et utilise des contraintes semi-supervisés dé-finies par les utilisateurs. D\u0027autres auteurs proposent une méthode de sélection de variables semi-supervisé en combinant des scores calculés sur la base de données étiquetées et non éti-quetés (Kalakech et al., 2011). La combinaison est simple, mais peut considérablement biaiser le résultat pour les variables ayant un meilleur score dans le cas supervisé et celles ayant de mauvais scores pour la partie non supervisée et vice-versa.\nL\u0027hypothèse sous-jacente au score Laplacien est que la structure des données dans l\u0027espace des attributs est localement préservée dans l\u0027espace d\u0027attributs de sortie. En représentant cette structure par les graphes de similarité ou de distance, des données similaires dans l\u0027espace d\u0027entrée doivent aussi l\u0027être quand elles sont projetées sur un vecteur d\u0027attributs pertinents. Inspirés des travaux récents en classification non supervisée hiérarchique et aussi du modèle de classification hiérarchique AntTree (Azzag et al., 2003), nous nous sommes intéressés à l\u0027étude du score laplacien auquel nous avons intégré de nouvelles contraintes non supervisées hiérarchiques. Le score que nous définissons est appelé SLH (Score laplacien hiérarchique). La principale contribution que nous proposons est d\u0027utiliser une approche de construction de graphe, autre que celle qui se base sur le k-N N où k est fixé a priori. Dans notre approche nous utilisons un algorithme de classification hiérarchique autonome où la structure d\u0027arbre fournie permet de définir un nouveau score intégrant des contraintes non supervisées basées sur l\u0027arborescence.\n2 Score Laplacien sous contraintes hiérarchiques 2.1 Le score Laplacien Soit un ensemble de N observations X \u003d {x 1 , ..., x N }. Une observation x i est un vecteur de m dimensions (variables), f ri désigne le i ème échantillon de la r ème variable, r \u003d 1, ..., m. Ainsi, nous définissons la r ème variable par\nT . Le score Laplacien sélec-tionne les variables pertinentes qui préservent au mieux la structure locale et qui produisent de grandes valeurs de variances. Nous supposons que les données appartenant à la même classe soient proches les unes des autres. Le SL de la r ème variable doit être ainsi minimisé avec la fonction suivante (He et al., 2005) :\nLe score Laplacien Hiérarchique\nL\u0027idée de notre approche pour la sélection de variables est d\u0027utiliser le principe des k-plus proches voisins fournis par la structure d\u0027arbre d\u0027AntTree (Azzag et al., 2003). Dans la littéra-ture, de nombreux algorithmes d\u0027apprentissage ont été proposés pour découvrir des structures sous-adjacentes dans les données en construisant un graphe de voisinage pour effectuer une analyse spectrale, (Belkin et Niyogi, 2001;Roweis et Saul, 2000;Tenenbaum et al., 2000). L\u0027algorithme AntTree a l\u0027avantage d\u0027être complètement autonome et d\u0027avoir une complexité très faible de ?(n log n). Dans le modèle AntTree, chaque noeud de l\u0027arbre (interne ou feuille) représente une donnée x i . Ainsi, les noeuds de l\u0027arbre seront successivement ajoutés du plus haut niveau vers les niveaux inférieurs (figure 1). Toutes les données doivent passer un test de similarité où leur propriété de voisinage est vérifiée.\nSoit une observation x i qui va se connecter à un noeud de l\u0027arbre x pos si et seulement si cette action augmente la valeur de T Dist (x pos ). T Dist (x pos ) est la valeur maximale de distance (distance euclidienne) entre les noeuds fils de x pos . La règle consiste à comparer x i avec son plus proche x i + (x i + est un noeud de x pos ). Dans le cas où les deux noeuds sont suffisamment\n, alors x i se connecte à x pos . Sinon, x i se déplace vers x i + . Ainsi T Dist augmente localement à chaque fois qu\u0027un noeud se connecte à l\u0027arbre.\nAvec le nouveau score SLH, nous souhaitons définir un algorithme complètement autonome pour la sélection de variables. L\u0027algorithme SLH est essentiellement basé sur le score Laplacien auquel nous avons ajoutés des contraintes hiérarchiques. Ainsi, au lieu d\u0027utiliser le graphe des k plus proches voisins, nous proposons d\u0027utiliser AntTree qui avec sa structure hié-rarchique fournira automatiquement pour chaque observation connectés à x i . La figure 1 montre un exemple où l\u0027observation x 1 a quatre voisins (k 1 \u003d 4), x 2 a seulement deux voisins du niveau inférieur (k 2 \u003d 2). En utilisant cette topologie, la nouvelle matrice d\u0027adjacence est définie comme suit :\nAinsi, le critère SLH du score Laplacien sous contraintes hiérarchiques est défini comme suit :\n2 S ij , nous donnons l\u0027avantage aux attributs respectant la structure hiérarchique. En maximisant i (f rj ? ? r ) 2 , le score SLH sélectionne les variables ayant les plus grandes valeurs de variance locale, et qui sont les plus représentatives de la topologie de l\u0027arbre construit. Le processus de démonstration est le même que celui présenté dans (He et al., 2005). L\u0027Algorithme 1 présente les trois étapes nécessaires pour la sélection de variables par SLH.\nExpérimentations\nDans cette section, plusieurs expérimentations ont été réalisées sur plusieurs bases de données réelles. Ces expérimentations sont présentées en deux parties : la qualité du clustering et la qualité de la classification supervisée en utilisant l\u0027algorithme du plus proche voisin (1-N N  \nComparaison dans un cadre non supervisé\nPour évaluer la qualité du clustering, nous utilisons deux mesures : la pureté et l\u0027Information Mutuelle Normalisée (NMI -Normalized Mutual Information) (Strehl et al., 2002) ; chacune doit être maximisée. Pour faciliter la comparaison entre les méthodes nous appliquons l\u0027algorithme K-means sur la base de données en prenant en compte que les variables sélectionnées.\nDans ces expérimentations, pour construire le graphe k-NN du SL nous fixons le paramètre k \u003d 5. Nous évaluons ensuite la qualité du clustering avec différentes valeurs pour le nombre de clusters de la manière suivante : Les figures 2 jusqu\u0027à 6 montrent les courbes des performances sur le clustering (Pureté et NMI) par rapport au nombre de variables sélectionnées. De manière générale notre approche SLH obtient de meilleurs résultats par rapport aux autres méthodes. Dans la figure 2, nous observons que l\u0027algorithme SLH fournit des résultats raisonnables par rapport aux critères de Pureté et du NMI. Pour les données Coil20 (Fig. 3), SLH est meilleur en terme de Pureté et NMI lorsque le nombre de variables est au alentour de 50 à 100 pour les trois expérimentations. Nous notons que pour K \u003d 30, notre algorithme est nettement meilleur que les autres. De plus, dans la figure 4, la pureté de SLH augmente de manière constante. Les mêmes remarques sont observées pour la base Sonar (Fig.5). Pour Soybean (Fig.6) SLH atteint une valeur de Pureté \u003d 100% pour la plupart des cas (K \u003d 4, 6 et 8) en utilisant seulement 9 variables.\nDans les tables 3.1 jusqu\u0027à 3.1, nous résumons les résultats du clustering obtenus sur toutes les bases testées. Les résultats numériques obtenus avec la base AR10P (Tab. 3.1) montrent une amélioration des performances du NMI de SLH. Pour la Pureté, SLH est de même qualité que SL et MaxVariance. Dans le tableau 3.1 nous remarquons que les résultats fournis par coil20, pour 20 clusters et 100 variables sélectionnées, donnent une valeur de NMI de 68% pour SLH, ce qui est mieux que si on avait utilisé les 1024 variables (66,0%). Dans le tableau 3.1, les résultats de Pureté et du NMI en utilisant 100 variables ne sont pas les meilleurs, mais restent proches de ceux utilisant 617 variables. Pour la base Sonar SLH est la seule méthode qui permet d\u0027obtenir de meilleurs résultats.     \nComparaison dans un cadre supervisé\nDans cette partie nous souhaitons évaluer les différents critères de sélection de variables en utilisant le classifieur 1-NN. Pour chaque donnée x i , nous cherchons le plus proche voisin\n. L\u0027erreur de classification est calculée comme suivant : \nConclusions et perspectives\nEtudier la sélection de variables en mode non supervisé est un vrai challenge pour la communauté scientifique en raison du manque d\u0027informations sur les labels des données. Pour relever ce défi, nous avons proposé une approche autonome de sélection de variables nommée SLH, qui est une variante du score Laplacien et utilise la structure et la topologie de l\u0027arbre défini par AntTree. Notre algorithme est autonome et ne nécessite aucun paramètre. Les ré-sultats expérimentaux sur plusieurs jeux de données montrent que l\u0027algorithme SLH réalise des performances plus élevées en mode supervisé et non supervisé. Comme perspective, nous nous sommes fixés comme objectif d\u0027introduire de nouvelles contraintes non supervisées hié-rarchiques pour la sélection de variables. L\u0027idée est d\u0027utiliser un autre type de lien dans le graphe qui représenterait des liens faibles.\n"
  },
  {
    "id": "422",
    "text": "Travaux existants\nLe subspace clustering est un domaine assez récent (Parsons et al., 2004), (Kriegel et al., 2009) qui vise à déterminer conjointement les clusters et leurs sous-espaces associés. Contrairement aux approches classiques de clustering dans lesquelles, la phase de partitionnement peut être précédée d\u0027une phase de sélection ou de pondération des attributs, le subspace clustering ne dissocie pas la définition de l\u0027espace et celle du groupe de données. En conséquence, une donnée peut théoriquement appartenir à plusieurs clusters, dès lors que ceux-ci sont définis dans un sous-espace qui leur est propre.\nLe subspace clustering a été défini dans deux principaux travaux (Parsons et al., 2004), (Kriegel et al., 2009) qui clarifient la terminologie et distinguent le subspace clustering d\u0027autres domaines proches comme le biclustering et le coclustering. On distingue plusieurs méthodes de subspace clustering en fonction du mécanisme de sélection des attributs lors de la construction des clusters. Certains algorithmes reposent sur des mécanismes de pondération des attributs, d\u0027autres recherchent tous les sous-espaces potentiels de manière ascendante (des espaces de 1 dimension vers l\u0027espace contenant toutes les dimensions) ou inversement descendante.\nLes méthodes qui reposent sur un mécanisme de sélection/pondération des attributs appartiennent au domaine du soft subspace clustering (Gustafson et Kessel, 1979), (Candillier et al., 2005). L\u0027idée principale de ces méthodes est d\u0027affecter un poids à chaque attribut et d\u0027utiliser une optimisation alternée pour rechercher un maximum local à une fonction objectif. Il existe toutefois de nombreuses limitations à ces méthodes : définition du nombre de clusters a priori, pas de garantie d\u0027une convergence vers un optimum global, affectation (éventuellement floue) de chaque donnée à un (ou plusieurs) cluster(s) défini(s) dans un unique sous-espace.\nD\u0027autres approches reposent sur une exploration systématique de tous les sous-espaces éli-gibles en partant des sous espaces les plus petits. Ces approches ascendantes sont basées sur un mécanisme de recherche d\u0027itemsets fréquents. Par exemple, l\u0027algorithme CLIQUE (Agrawal et al., 1998) intègre un mécanisme d\u0027agrégation de sous-ensembles denses de basse dimensionnalité pour retrouver les sous-ensembles denses de plus haute dimensionnalité. Toutefois, la complexité de ce type d\u0027algorithme est grande par rapport au nombre d\u0027attributs.\nÀ l\u0027inverse des méthodes ascendantes, les méthodes descendantes commencent par étudier l\u0027ensemble des attributs avant de déterminer et de sélectionner les attributs caractéristiques pour réduire le nombre de dimensions. Ce type d\u0027algorithme est efficace lorsque la répartition des données vérifie l\u0027hypothèse de localité définie dans Kriegel et al. (2009) : \"une sélection locale des données suffit à estimer une orientation locale des données\".\nCette définition de localité repose sur des calculs de type k plus proches voisins qui utilisent l\u0027ensemble des attributs pour définir le voisinage local. Cette hypothèse ne semble pas pertinente dans la pratique car, dans le cas d\u0027un espace de grande dimension, de nombreux attributs non caractéristiques affectent le calcul du voisinage et donc le choix des attributs caractéristiques. De nombreux algorithmes utilisent l\u0027heuristique des k plus proches voisins (Achtert et al., 2007), (Friedman et Meulman, 2004). Plusieurs paramètres sont estimés localement pour chaque cluster comme l\u0027orientation du voisinage, et utilisés ensuite pour agréger au cluster les données vérifiant une relation de proximité. Toutefois, comme précédemment, ces algorithmes affectent une donnée à un unique cluster et son sous-espace.\nEnfin, l\u0027algorithme CASH  diffère des approches descendantes précé-dentes car il ne repose pas sur l\u0027hypothèse de localité. Dans ce modèle, les clusters sont modéli-sés par des hyperplans. L\u0027espace des hyperplans contenant au minimum une donnée est divisé en grille et parcouru afin de déterminer quels sont les hyperplans contenant de nombreuses données. La réitération de ce calcul et la modélisation en hyperplan permet de construire les sous-espaces. Cette méthode possède cependant une complexité rédhibitoire.\nNous décrivons dans la section suivante le modèle de l\u0027algorithme SNOW qui, comme l\u0027algorithme CASH ne repose pas sur l\u0027hypothèse de localité et possède une complexité moindre.\n3 Algorithme Snow SNOW est un algorithme qui détermine à chacune de ses itérations un cluster et son sousespace associé. Chaque itération est indépendante des précédentes et repose sur un processus en 4 étapes principales : (1) la génération aléatoire d\u0027un cluster potentiel ; (2) la détermination de l\u0027hyper-cube propre à ce cluster potentiel ; (3) le calcul d\u0027un pas de densité des données pour chacun des attributs ; (4) l\u0027extension de l\u0027hyper-cube à partir du pas de densité pour obtenir un cluster maximal.\nGénération aléatoire d\u0027un cluster potentiel. Contrairement aux approches de l\u0027état de l\u0027art qui déterminent les clusters à partir du voisinage d\u0027une seule donnée, notre approche se base sur une sélection aléatoire de plusieurs données pour former la graine du premier cluster potentiel. Cette sélection de plusieurs données amène plus de robustesse dans la détermination des attributs caractéristiques du cluster car, contrairement au voisinage local, elle permet de considérer des plages de valeurs plus importantes et d\u0027être donc moins sensible aux variations locales de densité des attributs ou aux points aberrants. Enfin, à chaque itération, la distribution aléatoire initiale des points favorise l\u0027émergence d\u0027attributs caractéristiques différents ce qui assure une bonne couverture de l\u0027espace des solutions. Comme CASH, SNOW recherche un modèle reliant les données et non les données mutuellement proches.\nDétermination de l\u0027hyper-cube du cluster potentiel. On définit l\u0027hypercube H C du cluster potentiel C comme le produit des intervalles I C j sur chacun des attributs j de l\u0027espace initial R m . Chaque intervalle I C j sur l\u0027attribut j pour le cluster C est défini comme l\u0027intervalle minimal englobant l\u0027ensemble des valeurs des points x ? C sur l\u0027attribut j : où x j désigne la valeur de l\u0027attribut j du point x. À cette étape, on ajoute au cluster potentiel l\u0027ensemble des données contenues dans l\u0027hypercube.\nCalcul d\u0027un pas de densité. Cette étape vise à déterminer une densité locale au cluster potentiel. Pour chaque attribut j, on définit la séquence S I C j comme l\u0027ensemble ordonné des valeurs x j de l\u0027attribut j pour tout x ? C sur l\u0027intervalle I C j . Le pas de densité ? j est ensuite simplement défini comme la distance maximale observée sur l\u0027attribut j entre deux valeurs consécutives de S I C j (s i désigne le i ème élément de la séquence S I C j ) :\nj Détermination du cluster et de l\u0027hypercube maximal. Dès lors que le pas ? j est déterminé pour tout attribut j ? [1, m], notre algorithme agrège itérativement au cluster potentiel les points dont les coordonnées sont situées à une distance inférieure à ? j des frontières de son hypercube H C pour tous les attributs j. L\u0027hypercube associé au cluster est ensuite mis à jour et le processus d\u0027agrégation de nouveaux points est réitéré jusqu\u0027à ce qu\u0027aucun candidat ne puisse plus être ajouté au cluster, qui est alors maximal.\nParamétrage. L\u0027algorithme SNOW repose sur deux paramètres fixés par l\u0027utilisateur. Le premier est le nombre maximal d\u0027itérations ? . Il permet d\u0027optimiser la couverture, la qualité des clusters et de leurs sous-espaces associés par rapport au temps de calcul. Le second paramètre k est le nombre de données sélectionnées pour générer les graines de clusters potentiels. Une petite valeur de k diminue le temps de calcul mais une plus grande valeur de k permet une meilleure estimation de la densité des attributs du cluster potentiel et donc d\u0027obtenir de meilleures performances en conjonction avec le paramètre ? .\nExpérimentations\nCette section présente les deux expérimentations conduites pour valider notre algorithme. La première expérimentation propose la comparaison de notre algorithme SNOW avec la mé-thode COPAC (Achtert et al., 2007) à l\u0027aide d\u0027une mesure moyenne des F1-mesures des clusters découverts les plus pertinents. Nous utilisons l\u0027implémentation de COPAC fournie par le framework ELKI . 2 bases de données sont utilisées : (1) un premier jeu de données artificiel de référence nommé P arsons (Parsons et al., 2004), dont les clusters rapprochés peuvent poser problème aux méthodes comme COPAC reposant sur l\u0027hypothèse de localité ; (2) le même jeu de données modifié en séparant plus les clusters afin qu\u0027il soit plus favorable à la méthode COPAC reposant sur l\u0027hypothèse de localité.\nLa seconde expérimentation vise à évaluer la capacité de la méthode SNOW à produire des clusters interprétables sur la base de données réelles nommée Auto MPG (Quinlan, 1993).\nRésultats comparatifs sur la base P arsons. Nous utilisons une méthode d\u0027évaluation, qui consiste, à partir d\u0027un ensemble de clusters cibles connus, à intégrer dans le score total chaque meilleur cluster généré par rapport à chaque cluster cible. Soit C 1 , . . . , C p la liste des clusters générés par l\u0027algorithme et S 1 , . . . , S q les clusters cibles. Pour chaque cluster cible S i , on note C j le cluster ayant le meilleur score F1 par rapport à S i . Nous proposons comme score général de l\u0027algorithme la moyenne des meilleurs scores F 1 par rapport à chaque cluster cible S i :\nDiscussion des résultats sur la base P arsons et P arsons modifiée. Les meilleurs résul-tats obtenus par l\u0027approche COPAC lors de nos expérimentations sont de 49,84%. Ce résultat est dû à la proximité des clusters 2 à 2. En effet, le calcul du voisinage inclut un attribut dans le sous-espace, et par conséquent le calcul de l\u0027orientation du voisinage intègre des données de clusters différents, faussant le résultat final. SNOW, quand à lui, n\u0027est pas sensible à l\u0027hypothèse FIG. 1 -Score de SNOW sur P arsons en fonction de ? et de k et par rapport à COPAC.\nTAB. 1 -2 des clusters générés par SNOW sur la base Auto-MPG.\nde localité. Pour k \u003d 5 et ? \u003d 10 5 (i.e. quand on génère 10 5 clusters candidats aléatoirement), le score est de 94.84%.\nPour P arsons modifiée, COPAC obtient un score de 93.16%. La proximité des clusters est donc bien la cause de la mauvaise performance de COPAC sur le premier jeu de données. SNOW obtient un score légèrement supérieur à COPAC, de 97.4%.\nJeu de données réelles Auto MPG. Contrairement aux données de P arsons, il n\u0027existe pas d\u0027étiquettes de clusters théoriques permettant une évaluation objective des résultats. Nous proposons donc, à l\u0027image de la démarche suivie dans (Candillier et al., 2005), d\u0027étudier avec cette base réelle la pertinence et l\u0027interprétabilité des clusters découverts. SNOW produisant un grand nombre de clusters, nous avons retenu expérimentalement deux clusters parmi les plus denses dans leur sous-espace pour conduire notre interprétation. La densité des clusters a été calculée à partir du nombre de données du cluster divisé par le volume du cluster (la longueur des intervalles de l\u0027hypercube est bornée pour ce calcul au minimum à 0.1). SNOW a été lancé avec comme valeurs de paramètres ? \u003d 30000 et k \u003d 10. Les clusters ayant un effectif inférieur à 50 sont supprimés et les clusters restants sont triés par ordre de densité décroissante. Les 10 premiers clusters de ce classement sont relativement homogènes. L\u0027analyse rapportée dans le tableau 1 illustre les deux profils principaux de clusters ainsi découverts. Discussion des résultats de SNOW. D\u0027après le tableau 1, le premier cluster représente le segment des petites voitures, légères et économiques, le deuxième cluster celui des grosses voitures plus puissantes. Toutefois, on remarque l\u0027accélération, l\u0027année et l\u0027origine ne sont pas des attributs caractéristiques du premier cluster, tandis que pour le deuxième cluster seule l\u0027accé-lération ne semble pas caractéristique. De manière inattendue et d\u0027après SNOW, l\u0027accélération n\u0027est pas une caractéristique importante des voitures identifées comme puissantes.\n"
  },
  {
    "id": "424",
    "text": "Introduction\nAu-delà de sa stricte définition d\u0027entité administrative et politique, le territoire, selon Guy Di Méo, témoigne d\u0027une \"appropriation à la fois économique, idéologique et politique de l\u0027espace par des groupes qui se donnent une représentation particulière d\u0027eux-mêmes, de leur histoire, de leur singularité\" (Di Méo (1998)). Dans ce contexte éminemment subjectif, la caractérisation et la compréhension des perceptions d\u0027un même territoire par les différents acteurs est difficile, mais néanmoins particulièrement intéressante dans une perspective d\u0027amé-nagement du territoire et de politique publique territoriale. Le travail présenté s\u0027inscrit dans le cadre du projet Senterritoire 1 , qui adopte une démarche pluridisciplinaire, initiée à partir d\u0027une méthode automatique et visant à fournir aux géographes et aux environnementalistes, une aide à la découverte de connaissances. Nos contributions portent, dans cette publication, sur l\u0027accès à l\u0027information spatiale et proposent (1) d\u0027affiner et d\u0027enrichir les patrons d\u0027extraction d\u0027informations existants dans la littérature afin d\u0027améliorer l\u0027identification du sens de l\u0027entité spatiale extraite et (2) de définir une approche originale utilisant différentes techniques de fouille de textes afin de distinguer une entité spatiale d\u0027une entité d\u0027organisation. La suite de l\u0027article est organisée de la façon suivante. En section 2, nous présentons les défini-tions préliminaires et les travaux du domaine. En section 3, nous décrivons la méthode hybride Text2Geo. En section 4, nous présentons les expérimentations réalisées sur le jeu de données du bassin de Thau et concluons dans la section 5.  (Maurel et al. (2011)). De nombreuses mé-thodes permettent de reconnaitre les ENs en général et les ES en particulier (Nadeau et Sekine (2007)). On trouve des approches statistiques consistant généralement à étudier les termes cooccurrents par analyse de leur distribution dans un corpus (Agirre et al. (2000)) ou par des mesures calculant la probabilité d\u0027occurrence d\u0027un ensemble de termes (Velardi et al. (2001)). Ces méthodes ne permettent pas toujours de qualifier des termes comme étant des ENs et notamment les ENs de type Lieu ou Organisation. On trouve également des méthodes de fouille de données fondées sur l\u0027extraction de motifs. Ces derniers permettent de déterminer des règles de transduction utilisant des informations syntaxiques propres aux phrases pour repérer les ENs ). Pour la reconnaissance des classes d\u0027ENs, de nombreuses approches s\u0027appuient sur des méthodes d\u0027apprentissage supervisé comme les SVM (Joachims (1998)). Les algorithmes exploitent divers descripteurs (positions des candidats, étiquettes grammaticales, informations lexicales, etc.) et des données expertisées/étiquetées. Dans cet article, nous combinons de telles méthodes d\u0027apprentissage supervisé associées à des patrons linguistiques. phrase à partir de grammaires) ; (4) l\u0027analyse sémantique (identifier de sens potentiel véhi-culé par des mots ou des groupes de mots sur la base des syntagmes retenus). Cette chaîne de TALN est définie avec Linguastream 4 qui intègre notamment l\u0027étiqueteur grammatical TreeTagger 5 et le langage prolog pour la définition des grammaires DCG (analyses syntaxique et sémantique). Sur la base de la grammaire définie par (Lesbegueries (2007)) dans ces phases d\u0027analyses syntaxique et sémantique, nous avons mis en place de nouveaux patrons dédiés à l\u0027extraction des entités spatiales et des entités d\u0027organisation. Cette extraction se fait selon deux étapes : L\u0027étape 1 extrait les ESA qui constituent les types primitifs de notre processus d\u0027extraction. Ces types primitifs sont soit des entités nommées de lieu (Montpellier, France...), soit des indicateurs spatiaux (la région, la ville) ou alors des indicateurs de relation (Le sud...). Ceci se traduit en logique par des règles comme :\nPour chaque règle définie ci dessus, \"?\" dans l\u0027expression \"ESA ? N omT oponymique\" signifie que l\u0027expression ESA est composée de l\u0027expression NomToponymique, correspondant à un nom de lieu. Les deux premières définitions ESA sont récursives, ce qui permet de produire des patrons de tailles variables afin d\u0027identifier des instances telles que : Les régions rurales du sud de la France, la ville de Madrid, les communes de l\u0027agglomération du bassin de Thau. L\u0027étape 2 extrait les instances les plus complexes : les entités spatiales relatives, composées d\u0027une ESA et précédées d\u0027une relation d\u0027ordre topologique suivant des règles du type : ESR ? Relation, ESR. ; ESR ? Relation, ESA. Relation ? Adjacence |Orientation |Inclusion |Distance |F orme géometrique, Adjacence ? \"prés\" |\"lapériphérie\" |etc.\nDans cette chaîne de traitements, nous proposons deux contributions. Dans un premier temps, nous avons ajouté des règles à la grammaire afin d\u0027améliorer l\u0027identification des ESR et ESA. Dans un second temps, nous avons proposé un nouveau type de règles pour repérer de manière spécifique les entités nommées de type Organisation.\nDéfinition de nouveaux patrons pour l\u0027identification des ESA et ESR. Pour annoter les ENs spatiales, nous nous sommes appuyés sur la typologie classique du domaine qui identifie des sous-classes : les lieux géographiques naturels (lacs, mers, etc), les constructions humaines (buildings, installations, etc.), les axes de circulations (routes, etc.), les adresses (rue, code postal, etc.). Nous avons ajouté des règles (patrons) permettant d\u0027améliorer l\u0027identification des ESA et ESR. Par exemple, l\u0027ajout d\u0027un patron lié à la distribution des ES permet d\u0027identifier le cas lié à la distribution des relations spatiales. Ainsi, dans la phrase Les environs de Lyon, Marseille..., nous identifions deux entités spatiales. Vers une méthode hybride. Nous proposons d\u0027apprendre un modèle permettant de distinguer une entité de type Organisation et une ES. Pour cela, nous avons étiqueté manuellement un ensemble de phrases en deux classes correspondant aux deux types d\u0027entités. Nous n\u0027avons pas considéré les phrases dites ambiguës, c\u0027est-à-dire présentant une ES et une Organisation. Pour l\u0027apprentissage supervisé, nous avons utilisé la méthode classique SVM (Joachims (1998)). Les descripteurs utilisés sont les mots des phrases qui représentent un \"sac de mots\". Nous avons complété ces descripteurs en considérant les patrons définis dans les sections précé-dentes comme des descripteurs à part entière. Pour cela, dans la représentation vectorielle de nos textes, nous avons ajouté des attributs de type booléen signifiant qu\u0027une phrase peut contenir un motif de type \u003cConceptOrg, Entité\u003e (motif propre à une organisation) ou \u003cConceptSpa, Entité\u003e (motif propre à une Entité Spatiale). ConceptOrg représente les prépositions typiques précédant une Organisation (avec, par, etc). ConceptSpa se décline en trois sous-concepts pré-cédant, en général, une Entité Spatiale : Préposition spatiale : en, sur, etc. ; Indicateur de relation : sud, vers, etc. ; Indicateur spatial : ville, région, etc. Cette représentation a deux avantages : 1) Elle donne plus de poids à certains mots propres au domaine de la Recherche d\u0027Information Géographique (prépositions spatiales et d\u0027organisation, indicateurs spatiaux et de relation). Dans un contexte plus général, de tels mots peu porteurs de sens sont souvent moins pris en compte voire supprimés ; 2) contrairement à l\u0027approche sac de mots classique qui ne prend pas en considération l\u0027ordre des mots, les nouveaux descripteurs prennent en compte un ordre partiel et se révèlent déterminants comme le montrent les expérimentations.  TAB. 2 -Classification des phrases.\nExperimentations\nConclusion et perspectives\nDans le cadre du projet Senterritoire, nous avons proposé une méthode hybride qui permet l\u0027extraction d\u0027informations spatiales et la recherche d\u0027informations. Ces approches exploitent 6. http ://www.cs.waikato.ac.nz/ml/weka/\n"
  },
  {
    "id": "425",
    "text": "Introduction\nL\u0027objectif de la détection de communautés dans les graphes, ou encore dans les réseaux sociaux, est de créer une partition des sommets, en tenant compte des relations qui existent entre ces sommets dans le graphe, de telle sorte que les communautés soient composées de sommets fortement connectés (Fortunato (2010)). Ainsi, les principales méthodes de détection de communautés proposées dans la littérature se concentrent sur la structure des liens, en ignorant les propriétés des sommets. Or dans de nombreuses applications, les réseaux sociaux peuvent être représentés par des graphes dont les sommets ont des attributs qui peuvent être pris en compte pour détecter plus efficacement les communautés. Ceci a conduit à revisiter cette probléma-tique afin d\u0027opérer cette détection non seulement à partir des relations décrites par le graphe, mais aussi à partir d\u0027attributs caractérisants les sommets et cela a donné lieu récemment à l\u0027introduction de méthodes qui exploitent ces deux types de données (Moser et al. (2007);Zhou et al. (2009);Li et al. (2008); Cruz Gomez et al. (2011);Combe et al. (2012); Dang et Viennet (2012)).\nDans cet article, nous proposons ToTeM, une méthode de classification de graphes à vecteurs d\u0027attributs qui reprend le principe de la méthode de Louvain, basée sur l\u0027optimisation de la modularité, en l\u0027étendant de façon à permettre la prise en compte d\u0027attributs numériques d\u0027une manière symétrique à ce qui existe pour les relations (Blondel et al. (2008)). Après avoir défini plus formellement le problème de la détection de communautés dans un réseau d\u0027information dans la section 2, nous rappelons brièvement le principe de la méthode de Louvain et introduisons ToTeM dans la section 3 avant de décrire des critères globaux de partitionnement dans la section suivante.\nÉnoncé du problème et notations\nÉtant donné un graphe G \u003d (V, E) où V \u003d {v 1 , . . . , v i , . . . , v n } est l\u0027ensemble des sommets et E ? V × V est l\u0027ensemble des arêtes non étiquetées. On suppose que chaque sommet v i ? V est associé à un vecteur d i \u003d (w i1 , . . . , w ij , . . . , w iT ) à valeurs réelles de sorte que G forme un réseau d\u0027information (Zhou et al. (2009)). Dans un problème de partitionnement de réseau d\u0027information, les liens et les attributs sont considérés, de telle sorte que d\u0027une part il doit y avoir de nombreuses arêtes au sein de chaque classe et relativement peu entre elles et d\u0027autre part, deux sommets appartenant à la même classe sont plus proches en termes d\u0027attributs que deux sommets appartenant à des classes différentes. Ainsi, l\u0027objectif est de partitionner l\u0027ensemble V des sommets en r classes disjointes formant une partition P \u003d {C 1 , . . . , C r } où r est a priori inconnu et de telle sorte que les sommets appartenant à un même groupe soient connectés et homogènes vis-à-vis des attributs. Dans la suite, on notera A la matrice d\u0027adjacence de G telle que A ij indique la valuation de l\u0027arête entre i et j si elle existe et vaut 0 s\u0027il n\u0027existe pas d\u0027arête entre i et j. Le degré du sommet i, noté k i , est égal à j A ij et c i désignera la classe d\u0027appartenance de i dans la partition P.\nLa méthode ToTeM\nLa méthode ToTeM que nous proposons est une extension de la méthode de Louvain qui consiste elle-même à optimiser le critère de modularité (Blondel et al. (2008); Newman et Girvan (2004)) :\noù (i, i ) prend toutes les valeurs de V × V , m est la somme des poids de toutes les arêtes du graphe et ? est la fonction de Kronecker qui vaut 1 si ses arguments sont égaux et 0 sinon.\nL\u0027algorithme comporte deux phases. À partir de la partition discrète, la première phase consiste à essayer de déplacer successivement chaque sommet vers la classe de ses voisins et à l\u0027affecter à la classe ayant apporté le plus fort gain de modularité. Lorsque plus aucune amélioration n\u0027est possible, dans une seconde phase, un nouveau graphe pondéré est formé à partir des classes obtenues à l\u0027issue de la première phase. Chaque classe devient un sommet du nouveau graphe et une arête entre deux sommets a pour poids la somme des poids des arêtes présentes entre des sommets contenus précédemment dans les classes correspondantes. Les deux phases sont répétées jusqu\u0027à ce qu\u0027il n\u0027y ait plus de modification possible. Le gain de modularité induit par le déplacement d\u0027un sommet isolé i vers une classe C l est égal à :\noù in est la somme des poids des arêtes ayant leurs deux extrémités dans la classe C l , tot est la somme des poids des arêtes adjacentes aux sommets de C l , k i,in est la somme des poids des arêtes de i aux sommets de C l (Blondel et al. (2008)). La méthode ToTeM, que nous introduisons dans l\u0027Algorithme 1, repose sur l\u0027optimisation d\u0027un critère global permettant de classer les sommets en se souciant à la fois de la qualité des classes d\u0027un point de vue relationnel mais également du point de vue des attributs. Pour ce qui est de la qualité par rapport aux relations, on peut retenir la modularité. Le gain est alors mesuré suivant la formule 2. Pour ce qui est de la qualité par rapport aux attributs, plusieurs mesures sont envisageables comme le taux d\u0027inertie inter-classes ou l\u0027indice de Calinski, détaillés dans les sections suivantes.\nGain d\u0027inertie et critères de qualité globale\nIl est possible d\u0027améliorer l\u0027efficacité de l\u0027algorithme ToTeM lorsque le critère global est basé sur l\u0027inertie inter-classes d\u0027une partition en remarquant que la variation d\u0027inertie interclasses induite par la réaffectation d\u0027un sommet peut être calculée uniquement à l\u0027aide d\u0027information locale. Étant donné V l\u0027ensemble des n sommets du graphe représentés dans un espace vectoriel défini par les attributs et muni d\u0027une distance euclidienne à laquelle est associée une norme A tout sommet x de V est associé un poids positif m x et, sans perte de généralité, on peut supposer qu\u0027initialement tous les sommets ont le même poids. On note g le centre de gravité de V et pour toute classe C l de P, g l son centre de gravité et m l la somme des poids des éléments de cette classe C l . Considérons deux partitions P et P telles que P \u003d (A, B, C 1 , . . . , C r ) et P \u003d (A \\ {x} , B ? {x} , C 1 , . . . , C r ). Par la suite, A \\ {x} désigne la classe A privée du sommet x et B ? {x} la classe B augmentée du sommet x.\nL\u0027inertie inter-classes I inter (P) associée à la partition P est égale à :\n. L\u0027inertie inter-classes de la partition P obtenue en retirant x de sa classe A et en l\u0027affectant à la classe B vaut :\nLa variation d\u0027inertie inter-classes induite par le déplacement du sommet x de la classe A vers la classe B est donnée par :\ng A\\{x} et g B?{x} sont eux aussi calculés facilement en utilisant seulement l\u0027effectif repré-senté par le sommet x et les classes A et B ainsi que leurs centres de gravités g A , g B :\nLes valeurs des poids associés aux classes peuvent aussi être recalculées à l\u0027aide de l\u0027information locale :\n4.1 Synthèse des informations du graphe et des attributs dans la seconde phase L\u0027opération de synthèse des informations du graphe consiste, à l\u0027instar de ce qui est opéré dans la méthode de Louvain, à fusionner les sommets affectés à une même classe de façon à n\u0027en faire qu\u0027un seul sommet. Ainsi, à partir de la partition P \u003d (C 1 , . . . , C r ) obtenue à l\u0027issue de la première phase, un nouveau graphe G \u003d (V , E ) est crée. Ce graphe comporte autant de sommets qu\u0027il y a de classes dans P et chaque sommet v l de V incarne une classe C l de P . La valuation de l\u0027arête éventuellement présente entre les sommets v y et v z de V est égale à la somme des valuations des arêtes présentes entre des sommets de G appartenant aux classes C y et C z de P qui ont été représentées par v y et v z dans V . Soit ? la fonction qui indique, pour un sommet de V , par quel sommet de V il est représenté, alors le poids associé à une arête se calcule de la façon suivante :\nEnfin, les arêtes internes aux classes de P deviennent des boucles dans G . De plus, il est nécessaire de transférer les informations relatives aux attributs sur le nouveau graphe G . Pour cela, on affecte les poids des classes d\u0027origine aux sommets de destination et le centre de gravité de la classe d\u0027origine devient le vecteur d\u0027attributs du sommet de destination. Ainsi, pour tout sommet v\nCritères de qualité globale\nLe critère de qualité globale intervenant dans l\u0027algorithme ToTeM doit être une fonction d\u0027une mesure de qualité de la partition par rapport aux relations et d\u0027une mesure de sa qualité par rapport aux attributs. La modularité peut être utilisée comme mesure de la qualité par rapport aux relations. Pour ce qui est de la qualité par rapport aux attributs, une première solution envisageable peut consister à prendre le taux d\u0027inertie inter-classes. Ce qui conduit à une première mesure de qualité globale définie par :\noù I(P) désigne l\u0027inertie totale de V . Cependant, le taux d\u0027inertie inter-classes n\u0027est pas conçu pour comparer des partitions ayant un nombre de classes différent. En effet, il varie structurellement avec le nombre de classes de la partition de sorte qu\u0027il est maximum pour la partition discrète. Une solution simple visant à palier ce biais structurel consiste à tenir compte du nombre de classes |P| de la partition pour définir un critère global :\nContrairement au précédent, ce critère donne un avantage aux partitions à faible nombre de classes. Une alternative pour palier cet inconvénient consiste à avoir recours à des indices conçus dans le but de déterminer le nombre de classes dans le cas du partitionnement de données vectorielles, comme par exemple l\u0027indice de Calinski-Harabasz, celui de Dunn ou celui de Davies-Bouldin (Calinski et Harabasz (1974); Davies et Bouldin (1979)).\nUne autre solution pour comparer deux partitions P et P de taille respective r et r consiste à utiliser la probabilité critique résultant de tests de comparaison de variance. En effet, sous l\u0027hypothèse nulle selon laquelle les classes ne sont pas significativement différentes au sein de la partition P, la statistique F (P) définie par :\nsuit une loi de Fisher-Snedecor F (r?1, n?r) à (r?1, n?r) degrés de liberté où V inter désigne la variance entre les classes et V T la variance totale. On peut donc calculer la probabilité\n"
  },
  {
    "id": "427",
    "text": "Introduction\nTandis que dans les méthodes de fouille de données classiques, les données sont stockées dans une seule table, la Fouille de données mutli-tables (en anglais, Multi-Relational Data Mining, MRDM) s\u0027intéresse à l\u0027extraction de connaissances à partir de bases de données relationnelles multi-tables (Knobbe et al., 1999). Typiquement, en MRDM les individus sont contenus dans une table cible en relation un-à-plusieurs avec des tables secondaires. En apprentissage supervisé, une variable cible devrait être définie au sein de la table cible. La nouveauté en MRDM est de considérer les variables se trouvant dans les tables secondaires (variables secondaires) pour prédire la classe. Plusieurs solutions ont été proposée dans la littérature, notamment la Programmation Logique Inductive PLI (Džeroski, 1996) qui utilise le formalisme logique ou encore la propositionalisation qui opèrent par mise à plat afin de pouvoir utiliser un classifieur monotable classique (Kramer et al., 2001).\nDans cet article, nous introduisons un espace de modèles basé sur des itemsets de variables secondaires. Ces itemsets permettent de construire de nouvelles variables binaires dans les tables secondaires. Ensuite nous évaluons la pertinence de ces variables pour la tâche de classification supervisée. Afin de prendre en compte le risque de sur-apprentissage, qui augmente Un itemset permet de construire une nouvelle variable binaire A ? dans la table secondaire selon que les enregistrements de cette table sont couverts ou non par l\u0027itemset. Le critère d\u0027éva-luation proposé se décompose en la somme de deux termes : (i) un coût de codage évaluant la construction de l\u0027itemset ?, et (ii) un critère qui estime la pertinence de la variable A ? par rapport à la variable cible, qui exploite l\u0027approche d\u0027évaluation des variables secondaires binaires introduite dans (Lahbib et al., 2011).\nLe reste de cet article est organisé comme suit. La partie 2 introduit l\u0027espace des variables construites à base d\u0027itemsets de variables secondaires et présente son critère d\u0027évaluation. Dans la partie 3 nous évaluons la méthode sur un jeu de données réelles. Enfin, la partie 4 conclut cet article et discute sur des travaux futurs.\nConstruction de variables à base d\u0027itemsets\nEn se basant sur le modèle de classification de (Gay et Boullé, 2012), un itemset ? est une conjonction d\u0027expressions de la forme (x ? S x ), où x est une variable secondaire et S x est soit un groupe de valeurs si x est catégorielle, soit un intervalle si x est numérique. À chaque itemset ? nous associons une variable secondaire binaire A ? . Celle-ci est évaluée à « vrai » pour les enregistrements secondaires couverts par ?, et à « faux » sinon. \nnombre d\u0027intervalles (resp. groupes de valeurs) de la variable secondaire x ? X numérique (resp. catégorielle)\nA priori hiérarchique d\u0027un itemset de variables secondaires. Nous utilisons l\u0027a priori hiérarchique défini ci-dessous. Soulignons qu\u0027une distribution uniforme est utilisée à chaque étage 1 de la hiérarchie des paramètres des modèles.\n1. le nombre k de variables secondaires qui constituent l\u0027itemset est uniformément distribué entre 0 et m. 2. pour un nombre de variables k, chaque sous-ensemble de k variables qui constituent l\u0027itemset est équiprobable dans un tirage avec remise. 3. pour une variable secondaire catégorielle qui figure dans l\u0027itemset, le nombre de groupes est nécessairement 2 (I x \u003d 2). 4. pour une variable secondaire numérique qui figure dans l\u0027itemset, le nombre d\u0027intervalles est soit 2, soit 3 de façon équiprobable. 5. pour une variable secondaire numérique (respectivement, catégorielle), et pour un nombre d\u0027intervalles (respectivement, nombre de groupes) donné, toutes les partitions en I x intervalles (respectivement, en I x groupes de valeurs) sont équiprobables. 6. pour une variable secondaire catégorielle x appartenant à l\u0027itemset, le choix du groupe de valeurs i x sur lequel porte la condition est équiprobable. 7. pour une variable secondaire numérique x appartenant à l\u0027itemset, si la variable est discrétisée en deux intervalles, le choix de celui sur lequel porte la condition est équi-probable. Lorsqu\u0027il y a 3 intervalles, celui qui figure dans l\u0027itemset est nécessairement l\u0027intervalle du milieu. En utilisant la définition de l\u0027espace de modèles ainsi que sa distribution a priori, le coût de construction C c (A ? ) d\u0027un itemset ? est donné dans l\u0027équation 1.\n1. Cela ne signifie pas que l\u0027a priori hiérarchique est un a priori uniforme sur l\u0027espace des itemsets, ce qui serait équivalent à une approche par maximum de vraisemblance.\nLes deux premiers termes de l\u0027équation 1 correspondent au choix du nombre de variables secondaires qui apparaissent dans l\u0027itemset ainsi que le choix de ces variables parmi toutes les variables de la table secondaire. Le troisième termes représente le choix des partitions des valeurs des variables secondaires catégorielles ainsi que le choix des groupes impliqués dans l\u0027itemset où S dénote le nombre de Stirling de deuxième espèce. La troisième ligne correspond au choix de la discrétisation des variables secondaires numériques ainsi que les intervalles sur lesquels portent les conditions de l\u0027itemset. Le critère de l\u0027équation 1 est un log négatif de probabilités, ce qui exprime une longueur de codage Shannon (1948). C c (A ? ) peut être donc interprété comme un coût de codage de l\u0027itemset ?. Par ailleurs, il peut être vu comme un coût de construction de la variable A ? associée à ?.\nÉvaluation d\u0027une variable secondaire binaire. La variable A ? associée à ? est une variable binaire construite dans la table secondaire. Lahbib et al. (2011) fournissent une approche d\u0027estimation de densité de probabilité conditionnelle d\u0027une telle variable vis-à-vis de la variable cible, ainsi qu\u0027un critère permettant d\u0027évaluer sa pertinence C e (A). Ce critère est rappelé dans l\u0027équation 2. \nLe coût de construction agit comme un terme de régularisation afin de prévenir le risque de sur-apprentissage lié au grand nombre d\u0027itemsets potentiellement considérés. Les variables secondaires A ? construites sur la base d\u0027itemsets complexes, avec un nombre important de variables dans l\u0027itemset, sont pénalisées par rapport à des variables construites plus simples. Soit ? ? un itemset vide, ne contenant aucune variable secondaire, où aucun enregistrement secondaire n\u0027est couvert par l\u0027itemset. Le coût d\u0027évaluation global C r A ? ? de l\u0027itemset vide est : \nExpérimentations\nNous avons évalué notre approche en utilisant la bases de données Digits (Lecun et al., 1998). Il s\u0027agit de classer des images représentant des chiffres manuscrits de 0 à 9. Ces données initialement à plat 2 ont été reformatées afin d\u0027obtenir un schéma relationnel (figure 2) constitué de deux tables : la table cible DIGIT et la table secondaire PIXEL décrivant les pixels qui composent chaque image. Cette dernière est décrite par trois variables secondaires : X_POSITION, Y_POSITION et GRAY_LEVEL qui représentent la position du pixel en abscisse et en ordonnée dans l\u0027image originale ainsi que son niveau de gris.\nNous générons aléatoirement des itemsets basés sur des partitionnements (discrétisation dans le cas numérique et groupement de valeurs dans le cas catégoriel) des variables secondaires qui le constituent en 2, 4 et 8 partiles. Un classifieur Bayésien Naïf est employé en exploitant l\u0027estimation de la densité de probabilité conditionnelle de la variable construite comme décrit dans (Lahbib et al., 2011).\nLa figure 2 illustre les performances de classification (Précision) du Bayésien Naïf utilisant les variables générées et ceci pour différents nombres d\u0027itemsets (10, 1000, 1000 et 10 000). Ces résultats sont comparés à ceux obtenus avec le système Relaggs (Krogel et Wrobel, 2001). Relaggs est une méthode de propostionalisation qui consiste à générer pour chaque variable secondaire plusieurs agrégats (les effectifs, la somme, la moyenne, le min, le max, l\u0027écart type,. . .). Ces agrégats sont ensuite ajoutés à la table cible et un Bayésien Naïf classique est employé. Nous reportons également les performances obtenues avec un Bayésien Naïf utilisant la représentation monotable initiale. On peut constater que notre approche dépasse largement Relaggs et ceci pour tous les nombres d\u0027itemsets générés. Par ailleurs, avec suffisamment d\u0027itemsets, notre approche atteint des performances comparables à celles d\u0027un Bayésien Naïf qui utilise la représentation à plat.\nConclusion\nDans cet article, nous avons proposé une approche de prétraitement multivarié des variables secondaires dans le contexte de la classification de données multi-tables. La méthode consiste à construire de nouvelles variables à partir d\u0027itemsets de variables secondaires. La pertinence 2. http://yann.lecun.com/exdb/mnist/\n"
  },
  {
    "id": "428",
    "text": "Introduction\nLes ontologies ont contribué au succès des moteurs de recherche sémantique et sont de plus en plus utilisées pour améliorer la recherche d\u0027information sur le web et la reformulation des requêtes. Cependant, la construction d\u0027ontologies est généralement un processus long et coûteux, le recourt aux ontologies modulaires constitue une piste prometteuse. Une ontologie modulaire est une ontologie qui référence un fragment d\u0027une ontologie de domaine et a l\u0027avantage d\u0027être réutilisée ultérieurement. La composition d\u0027ontologies permet de construire une ontologie modulaire à partir d\u0027un ensemble de modules ontologiques qui constituent un réseau, et permet d\u0027améliorer l\u0027organisation des concepts sémantiques.\nUn état de l\u0027art a permis de constater que les approches de composition d\u0027ontologie proposées ne considèrent pas les relations sémantiques entre les termes, elles ne sont donc pas expressives et ne peuvent pas être efficacement utilisées pour la recherche sémantique sur le Web. Nos travaux précédents ont proposé en un premier lieu une approche de recherche d\u0027information basée sur le RàPC pour reformuler la requête de l\u0027utilisateur et lui recommander des résultats fondés sur les cas stockés (Elloumi-Chaabene et al., 2010), et par la suite une nouvelle méthode de composition de modules ontologiques basée sur des mesures de similarité séman-tique (Elloumi-Chaabene et al., 2011). L\u0027objectif du présent travail est de proposer un système hybride de recherche d\u0027information basée sur le RàPC et la composition d\u0027ontologies, ayant pour but d\u0027améliorer la précision des résultats fournis aux utilisateurs et de répondre à ses besoins. Notre contribution porte sur l\u0027intégration de : 1/la composition de modules ontologiques pour construire une ontologie modulaire ce qui améliore le processus d\u0027enrichissement de requête ; 2/ RàPC pour prendre en considération les préférences de l\u0027utilisateur ; 3/une base de connaissances (BC) qui prend en considération le contenu des documents Web jugés pertinents par l\u0027utilisateur lors des recherches précédentes afin d\u0027enrichir la requête. Notre motivation est d\u0027utiliser des requêtes passées afin d\u0027améliorer la précision des résultats fournis aux utilisateurs et d\u0027utiliser aussi les informations dans les documents Web pertinents pour enrichir la requête de l\u0027utilisateur.\nCe papier est organisé comme suit : la section 2 introduit notre système qui intègre la composition d\u0027ontologie et le RàPC, que nous détaillons dans les sous sections. Dans la section 3, nous présentons les expérimentations menées. La section 4 conclut et aborde nos perspectives.\n2 Un système hybride de recherche d\u0027information basé sur le raisonnement à partir de cas et la composition d\u0027ontologies\nLe système proposé se compose de quatre composants principaux : (1) un composant pour le RàPC (2) un composant pour la composition d\u0027ontologies (3) un composant pour la base de connaissances et (4) un composant pour la classification de documents. L\u0027architecture générale du système est présentée par la figure 1. Dans les sous-sections suivantes nous détaillons les différents éléments du système proposé. \nPremière recherche\nLa première recherche sur le Web offre une reformulation de la requête initiale de l\u0027utilisateur. En effet, le processus de recherche se déroule comme suit : l\u0027utilisateur sélectionne les modules ontologiques nécessaires pour la composition, pose sa requête, qu\u0027on enrichit par l\u0027ontologie modulaire et la BC. En effet, la similarité sémantique est calculée entre les concepts de l\u0027ontologie et les termes de la requête. Les concepts les plus similaires à la requête sont utilisés pour l\u0027enrichir. La mesure de similarité utilisée est PMI_IR (Pointwise Mutual Information). Elle a été adaptée au Web par (Turney, 2001) en définissant p(a) la probabilité du terme \" a \" dans le web. Cette probabilité est estimée à partir du nombre de résultats retournés en cherchant le terme a sur le web. La mesure PMI_IR est calculée selon (1)\nUn enrichissement par la BC est ensuite effectué. L\u0027utilisateur valide la requête enrichie et il choisit le domaine de recherche. Une recherche sur le web commence en même temps que la construction de l\u0027ontologie de domaine choisi. Le résultat de cette première recherche est donc un ensemble de documents et une ontologie de domaine qui vont permettre de passer à la recherche avancée.\nLa composition d\u0027ontologies. La composition d\u0027ontologies vise la construction d\u0027une ontologie modulaire en utilisant un ensemble de modules ontologiques ceci permet d\u0027améliorer leur structure en considérant les relations taxonomiques et non-taxonomiques implicites entre les concepts. Elle est évaluée par des mesures de cooccurrence basées sur le web. Les principales étapes de la méthode proposée sont les suivantes : (1) La réorganisation des modules ontologiques, qui consiste à déterminer les modules qui ont des concepts en communs et collecter l\u0027ensemble des concepts qui se chevauchent pour chaque paire de modules. (2) La classification des concepts obtenus en modules en fonction de leur similarité sémantique en utilisant les mesures de cooccurrence (Turney, 2001). L\u0027idée principale est de construire un graphe de cooccurrence et d\u0027appliquer un algorithme de clustering pour réorganiser les concepts et déter-miner l\u0027ensemble des nouveaux modules. (3) La construction d\u0027une structure hiérarchique des modules ontologiques qui constituent l\u0027ontologie modulaire.\nLa base de connaissances. Ce composant gère la BC composée d\u0027une base de faits, une base de règles et un moteur d\u0027inférence. Le système remplit automatiquement les règles de cette base par les termes les plus fréquents des documents jugés pertinents par l\u0027utilisateur. Elle est utilisée pour enrichir la requête de l\u0027utilisateur. En effet, suite au raisonnement fait par le moteur d\u0027inférence sur les règles et les faits de la base, de nouvelles conclusions sont obtenues. Ces conclusions sont les termes significatifs extraits des documents pertinents de recherches antérieures qui vont être ajoutées à la requête pour l\u0027enrichir. Les règles de cette base sont des règles de la logique d\u0027ordre zéro écrites sous la forme :\nRequest? \u003e BestT erm1, , , BestT ermi, , , BestT ermN. Avec N le nombre de documents choisis et BestTerm i le terme ayant la plus haute fré-quence dans le document i. Lorsqu\u0027une nouvelle requête est soumise au système, le moteur d\u0027inférence utilise ces mots clés pour faire un raisonnement : 1/ si la requête existe, il renvoie les termes significatifs extraits de documents pertinents retournés lors d\u0027une recherche précé-dente pour cette requête (nouvelles conclusions) 2/ si la requête n\u0027existe pas, la reformulation se fera seulement par l\u0027ontologie modulaire.\nRecherche avancée\nL\u0027utilisateur sélectionne les documents pertinents à partir de ceux récupérés dans la première recherche. Ensuite, le système insère d\u0027une part un nouveau cas et sa solution, et d\u0027autre part, utilise des techniques de fouille de texte pour extraire les termes les plus fréquents et les ajouter à la BC. A partir de l\u0027ontologie du domaine construite, l\u0027utilisateur choisit le concept de recherche. En utilisant WordNet (Miller, 1990), les synonymes, hyponymes et hyperonymes sont insérés dans la BDC pour mettre à jour la signature sémantique du cas( c\u0027est une liste de termes qui apparaissent fréquemment avec le concept du cas). En se basant sur le concept choisi et la BDC, une nouvelle recherche est possible et comprend : 1/ une désambiguïsation : le système offre les sens du concept choisi à partir de WordNet et une recommandation basée sur un algorithme de désambiguïsation sémantique ; 2/ l\u0027ajout de termes : l\u0027utilisateur ajoute des concepts de son choix afin d\u0027enrichir l\u0027ontologie ; 3/ les recommandations : ce sont les cas similaires de recherche (des URLs recommandés qui correspondent à des cas pertinents de recherche similaires stockés dans la BDC).\nLe raisonnement à partir de cas. La combinaison des ontologies et le mécanisme de RàPC peut améliorer les performances des recherches sur le Web sémantique. Le but ici est d\u0027enrichir automatiquement la requête à l\u0027aide de requêtes antérieures effectuées par l\u0027utilisateur. Ce composant gère la BDC, en adoptant le modèle vectoriel pour représenter le cas. Typiquement un cas contient au moins deux parties : une description de situation représentant un \"problème\" et une \"solution\" utilisée pour remédier à cette situation. Le problème comprend le domaine général, le concept spécifique de la recherche ainsi que la signature sémantique. La solution correspondante est composée des documents jugés pertinents par l\u0027utilisateur ainsi que deux vecteurs, domaine et module, associés aux concepts du domaine et concept de recherche respectivement. Ces deux vecteurs correspondent aux poids des concepts du domaine et de la signature sémantique calculés par la mesure TF*IDF dans les documents résultats. A l\u0027aide de la BDC, on peut insérer un nouveau cas, le mettre à jour ou bien rechercher des cas similaires.\nClassification des résultats\nA partir des choix effectués par l\u0027utilisateur au cours de l\u0027étape précédente (la désambiguï-sation etc.) ainsi que les requêtes similaires dans la BDC, le système reformule la requête et recherche de nouveaux résultats sur le web. Les documents récupérés sont classés par ordre de pertinence par rapport à la requête et ceux sélectionnés par l\u0027utilisateur sont ajoutés à la BDC et leurs termes les plus fréquents sont ajoutés à la BC.\nLa classification des documents. Afin de récupérer les documents les plus pertinents, le modèle de Salton (Salton et Rocchio, 1983) est utilisé (les termes remplacés par des concepts). Un filtrage par les vecteurs domaine et module est appliqué pour ne garder que les documents dans la même thématique et appartenant au même module. Chaque document est représenté par un vecteur Dj \u003d (d1j ; d2j ; ... ; dnj). Où dij est le poids du concept ci dans le document Dj , N étant le nombre de concepts dans la signature sémantique. Le vecteur Q \u003d (Q1 ;Q2 ; ... ;Qn) représente la requête, où Qi est le poids du concept ci dans la requête. La mesure de similarité entre un document et une requête est calculée avec la formule cosinus :\nExpérimentation\nUn prototype supportant le système proposé a été développé pour fournir une interface utilisateur qui permet la manipulation des ontologies, la gestion de la BC et l\u0027affichage des résultats à partir du moteur de recherche sur le web (en utilisant l\u0027API de Bing). L\u0027évaluation expérimentale des performances du système proposé est menée en comparant les résultats de la précision moyenne des différents systèmes. Quatre scénarios ont été testés : 1/ une recherche classique (c\u0027est une recherche par mots clés sans effectuer une reformulation de la requête), 2/ une reformulation de requêtes en utilisant l\u0027approche de composition d\u0027ontologies, 3/une recherche basée sur l\u0027approche RàPC et finalement 4/ une recherche hybride. L\u0027évaluation expérimentale a été conduite en utilisant le SRI expérimental LEMUR 1 , largement utilisé par la communauté RI. Les différents tests sont menés sur la collection INEX 2010 2 . Les résultats illustrés par la figure 2 représentent la précision exacte à 10, 30, 50 et 100 documents, et on observe une amélioration significative de la pertinence de l\u0027information ré-cupérée lors de la reformulation de requête pour le quatrième scénario.  \nConclusion\nCet article présente notre système hybride pour la recherche d\u0027information sur le Web inté-grant le RàPC et la composition d\u0027ontologies. Tout d\u0027abord, nous nous sommes appuyés sur le RàPC dans le but de prendre en considération les cas déjà rencontrés. Nous avons aussi utilisé une nouvelle méthode de composition des modules ontologiques pour améliorer, d\u0027une part, la structure des modules d\u0027ontologiques et leur organisation interne, et d\u0027autre part, le degré de parenté sémantique entre les concepts et la structure globale de l\u0027ontologie modulaire. Nous avons également intégré une BC pour enrichir la requête de l\u0027utilisateur par les termes les plus fréquents extraits de documents pertinents. Plusieurs techniques de recherche d\u0027information ont été intégrées pour améliorer les résultats de recherche sur le Web. L\u0027expérimentation et l\u0027évaluation menées montrent une amélioration du taux de précision. Dans nos travaux futurs, nous envisageons d\u0027intégrer une nouvelle composante à notre système pour supporter le traitement des systèmes question-réponse afin d\u0027améliorer les résultats de recherche en répondant automatiquement aux questions posées en langage naturel.\n"
  },
  {
    "id": "429",
    "text": "Introduction\nLa problématique de classification non supervisée (aussi appelée clustering) a été longuement étudiée pendant de nombreuses années avec des approches comme k-means et kmédoïdes. En général, le problème consiste à partitionner un ensemble de n objets en k classes non vides et deux à deux disjointes. C\u0027est un champ de recherche difficile pour plusieurs raisons : le choix de la mesure de dissimilarité entre les objets dépendant principalement de l\u0027application mais influant fortement sur les résultats, la définition du critère à optimiser, la taille de l\u0027espace de recherche avec pour conséquence la nécessité de définir des heuristiques conduisant souvent à un optimum local. Poser des contraintes sur la solution recherchée permet d\u0027une part, de modéliser plus finement les applications réelles et d\u0027autre part de restreindre la taille de l\u0027espace de recherche. Néanmoins, la plupart des algorithmes classiques n\u0027ont pas été développés pour la classification non supervisée sous contraintes et doivent être adaptés, si possible, pour prendre en compte les contraintes posées par l\u0027utilisateur. Développer des solveurs généraux applicables à une grande variété de problèmes pose de nouveaux défis.\nD\u0027autre part, des avancées récentes en Programmation par Contraintes (PPC) ont rendu ce paradigme beaucoup plus puissant. Plusieurs travaux (De Raedt et al. (2008) (Boizumault et al. (2011)) ont étudié l\u0027intérêt de la PPC pour modéliser des problèmes de fouille de données et ont montré l\u0027apport de la déclarativité inhérente à la PPC.\nDans ce papier, nous proposons un cadre pour modéliser la classification non supervisée sous contraintes en Programmation par Contraintes. L\u0027intérêt de notre approche est de fournir un modèle déclaratif permettant de spécifier le problème de classification non supervisée et d\u0027intégrer facilement des contraintes. Dans notre modèle, nous faisons l\u0027hypothèse que nous disposons d\u0027une mesure de dissimilarité entre les paires d\u0027objets et que le nombre k de classes est fixé (en théorie aucune limite n\u0027est donnée sur la valeur de k, mais plus k est grand, plus la complexité est élevée). Nous considérons, dans ce papier, le problème de trouver une partition minimisant le diamètre maximal des classes et nous montrons que notre cadre intègre naturellement des contraintes sur les instances ou sur les classes.\nIl est reconnu en PPC que le choix du modèle est fondamental, mais nous insistons aussi sur le fait que la stratégie de recherche est tout aussi importante pour améliorer l\u0027efficacité. Plusieurs stratégies sont étudiées, différant sur la manière d\u0027ordonner les points ou reposant sur des résultats théoriques. Des expérimentations sur des bases de données classiques montrent l\u0027intérêt de notre approche. Contrairement à la plupart des travaux existants, notre modèle permet de trouver un optimum global, et nous ne pouvons espérer qu\u0027il soit compétitif en terme de temps de calcul avec des méthodes heuristiques. Nous comparons la qualité des solutions obtenues par notre méthode avec celles obtenues par la méthode FPF (Gonzalez (1985)), mé-thode très efficace pour la classification non supervisée optimisant le diamètre maximum des classes, mais conduisant à une solution approchée.\nDes travaux récents (Guns et al. (2011)), (Métivier et al. (2012)) ont déjà proposé d\u0027utiliser la PPC pour la classification non supervisée conceptuelle. Le problème est formalisé comme la recherche d\u0027un ensemble de k-motifs fréquents, deux à deux non recouvrants, dont l\u0027ensemble couvre toutes les données : les transactions sont vues comme des objets et les motifs comme des définitions en intension des classes. Plusieurs critères d\u0027optimisation sont considérés comme maximiser la taille minimale d\u0027une classe ou minimiser la différence entre les tailles des classes. Notons que ces approches sont donc adaptées à des bases de données qualitatives alors que notre approche peut traiter tout type de données dès lors que l\u0027on dispose d\u0027une mesure de dissimilarité entre les données. Davidson et al. (2010) propose un cadre SAT pour la classification non supervisée sous contraintes, mais uniquement pour un problème à 2 classes (k \u003d 2) : il traite les contraintes sur les instances (\"must-link\" et \"cannot-link\") et des contraintes sur les classes (diamètre des clusters, séparation entre les clusters). Son algorithme converge vers un optimum global. Notre approche est plus générale dans la mesure où le nombre de classes n\u0027est pas limité à 2.\nLe papier est organisé comme suit. Dans la section 2, nous rappelons des notions sur la classification non supervisée à base de contraintes et sur la Programmation par Contraintes. La section 3 est dédiée à la présentation de notre modèle et la section 4 aux expérimentations. La conclusion et une discussion sur les travaux futurs sont données dans la section 5.\nPréliminaires\nClassification non supervisée\nLa classification non supervisée (ou clustering) consiste à regrouper les données dans des classes (ou clusters), de manière à regrouper dans un même cluster les données similaires et à séparer les données distantes dans des clusters différents. Etant donnés une base de données de n objets O \u003d {o 1 , . . . , o n } d\u0027un espace X et une mesure de dissimilarité d(o i , o j ) entre deux objets o i et o j , l\u0027objectif est de regrouper les objets en différentes classes de telle manière que la partition obtenue satisfasse un critère donné. Le problème de clustering peut être donc formulé comme un problème d\u0027optimisation.\nLes problèmes de clustering sont variés, dépendant de différents critères, comme la structure souhaitée (une partition, une hiérarchie, des classes recouvrantes, . . .), ou encore le critère à optimiser. Dans ce papier, nous nous intéressons à la recherche d\u0027une partition des objets en k classes C 1 , . . . , C k telle que : (1) pour tout i, (4) un critère E est optimisé. Le critère optimisé peut être, entre autres :\n-Critère des moindres carrés :\nCe critère est équi-valent au critère des moindres carrés dans le cas de la distance euclidienne.\nmaximal des clusters, où le diamètre d\u0027un cluster est la distance maximale entre chaque paire de ces objets. L\u0027algorithme k-means représente chaque cluster par la moyenne des points du cluster et tend à minimiser le critère des moindres carrés. L\u0027algorithme k-médoïdes choisit des objets pour représenter des clusters et cherche à minimiser le critère d\u0027erreur absolue. A chaque ité-ration, les deux méthodes réduisent la valeur du critère jusqu\u0027à un optimum, en général local.\nL\u0027algorithme FPF (Furthest Point First) (Gonzalez (1985)) minimise le diamètre maximal des clusters. L\u0027algorithme commence par choisir un point comme le représentant du premier cluster et affecte tous les points à ce cluster. A l\u0027itération suivante, le point le plus loin du premier représentant est choisi comme le représentant du second cluster. Les points qui sont plus proches du second représentant que du premier sont réaffectés au second cluster. L\u0027algorithme réitère de cette façon : choisir comme nouveau représentant le point le plus loin des représen-tants existants et réaffecter les points. Il s\u0027arrête après k itérations, ayant ainsi formé k clusters. La complexité en temps est O(nk). Si d opt est l\u0027optimum global alors l\u0027algorithme garantit de trouver un clustering avec le diamètre maximal des clusters d tel que d opt ? d ? 2d opt , si la mesure utilisée satisfait l\u0027inégalité triangulaire. De plus, Gonzalez a prouvé que trouver d tel que d ? (2 ? opt est NP-Difficile pour tout \u003e 0, lorsque les points sont dans un espace à 3 dimensions. L\u0027algorithme modifié (M-FPF) proposé dans (Geraci et al. (2006)) est plus rapid en temps mais la solution trouvée est identique que celle de FPF.\nLa classification non supervisée sous contraintes tient compte des contraintes définies par l\u0027utilisateur pour identifier les clusters. Ces contraintes peuvent être posées sur les clusters ou sur des instances (points). Les contraintes sur les clusters imposent des conditions sur la forme, la taille ou d\u0027autres caractéristiques. Par exemple, la contrainte sur la taille exprime que chaque cluster doit avoir un nombre minimum ? de points (contrainte de capacité minimale) : ?c ? [1, k], |C c | ? ?, ou encore que chaque cluster doit avoir un nombre maximum ? de points (contrainte de capacité maximale) :\nUn autre exemple est la contrainte de séparation minimale, qui impose que la distance entre chaque paire d\u0027objets de différents clusters soit supérieure à un seuil ? :\nLes contraintes sur des instances sont en général posées sur des paires d\u0027objets individuels. Deux types de contraintes sont souvent utilisés : \"must-link\" et \"cannot-link\". Une contrainte must-link indique que deux objets o i et o j doivent être dans le même cluster : ?c ? [1, k], o i ? C c ? o j ? C c . Une contrainte cannot-link indique que deux objets ne peuvent pas appartenir au même cluster :\nLes contraintes définies par l\u0027utilisateur sont nécessaires dans les applications réelles. Cependant, peu d\u0027algorithmes sont adaptables pour gérer de telles contraintes. Il n\u0027existe pas de solution générale pour étendre un algorithme traditionnel (k-means, k-médoïdes, etc.) avec des contraintes. Notre approche se basant sur la Programmation par Contraintes permet d\u0027ajouter directement des contraintes définies par l\u0027utilisateur, sans modifier le modèle.\nProgrammation par contraintes\nLa Programmation par Contraintes (PPC) est un paradigme puissant pour résoudre des problèmes combinatoires, se basant sur des techniques issues de l\u0027intelligence artificielle ou de la recherche opérationnelle. La PPC se base sur le principe suivant : (1) le programmeur spécifie le problème d\u0027une façon déclarative comme un problème de satisfaction de contraintes ; (2) le solveur cherche des solutions en intégrant la propagation de contraintes à la recherche. Un problème de satisfaction de contraintes (Constraint Satisfaction Problem -CSP) est un triplet\n.., C t } est un ensemble de contraintes où chaque contrainte C i est une condition sur un sous-ensemble de X.\nUne solution d\u0027un CSP est une affectation complète de valeur a i ? D i à chaque variable x i satisfaisant toutes les contraintes de C. Un problème d\u0027optimisation sous contraintes (Constraint Optimization Problem -COP) est un CSP auquel est associée une fonction objectif. Une solution optimale d\u0027un COP est une solution du CSP qui optimise la fonction objectif.\nEn général, les CSP sont NP-Difficiles. Cependant, les techniques utilisées par les solveurs permettent de résoudre un grand nombre d\u0027applications réelles de façon efficace. Les techniques les plus connues reposent sur la propagation de contraintes et des stratégies de recherche.\nLa propagation de contraintes consiste, pour une contrainte c, à supprimer du domaine des variables de c des valeurs pour lesquelles on peut déterminer qu\u0027elles ne peuvent participer à une solution de c. A chaque contrainte est associé un ensemble de propagateurs, dépendant du choix de consistance pour cette contrainte. Par exemple, si la contrainte c est posée avec la consistance d\u0027arc, les propagateurs sont implantés de manière à supprimer toutes les valeurs inconsistantes avec c du domaine des variables. Si c est posée avec la consistance de borne, les propagateurs modifient seulement les bornes inférieures et supérieures des domaines de variables. Le choix de la consistance est indiqué par le programmeur lorsque les contraintes sont posées.\nLe fait que chaque contrainte soit réalisée par un ensemble de propagateurs implique que toute formule ou relation mathématique ne peut être une contrainte, seules celles pour lesquelles on peut construire un ensemble de propagateurs sont disponibles. Nous disposons des relations arithmétiques, logiques et de relations plus complexes représentées sous forme des contraintes globales.\nLe solveur recherche des (les) solutions en itérant deux étapes : propagation des contraintes et branchement. Le solveur propage toutes les contraintes jusqu\u0027à un état stable, dans lequel soit le domaine d\u0027une variable est réduit à l\u0027ensemble vide, soit aucun domaine ne peut se réduire davantage. Dans le premier cas, il n\u0027existe pas de solution et le solveur effectue un retour en arrière. Dans l\u0027autre cas, si tous les domaines sont singletons, une solution est trouvée, sinon le solveur choisit une variable dont le domaine est non singleton et découpe le domaine en deux parties, ce qui crée deux nouvelles branches dans l\u0027arbre de recherche. Le solveur explore ensuite chaque branche, la propagation de contraintes peut devenir de nouveau active suite aux modifications du domaine d\u0027une variable.\nLa stratégie de l\u0027exploration de l\u0027arbre de recherche peut être déterminée par le programmeur. Avec la stratégie de recherche en profondeur d\u0027abord, le solveur ordonne les branches par l\u0027ordre spécifié par le programmeur et explore en profondeur chaque branche. Pour un problème d\u0027optimisation, la stratégie de recherche en profondeur devient la stratégie branchand-bound : chaque fois qu\u0027une solution du CSP est trouvée, la valeur de la fonction objectif sur cette solution est calculée et une nouvelle contrainte est ajoutée, imposant qu\u0027une nouvelle solution soit meilleure que celle-ci. Le solveur effectue une recherche exhaustive, la solution retournée est donc garantie d\u0027être optimale. Les choix de variables et de valeurs à chaque branchement sont extrêmement importants, car ils peuvent aider à réduire de façon drastique l\u0027arbre de recherche. Pour plus de détails sur la programmation par contraintes, le lecteur est invité à consulter l\u0027ouvrage de Rossi et al. (2006).\nAfin d\u0027illustrer un problème d\u0027optimisation sous contraintes et les stratégies de recherche, considérons l\u0027exemple suivant. un cercle bleu est un état stable mais non encore une solution, un carré rouge est un état échec (pas de solution), un losange vert est une solution intermédiaire et le losange orange est la solution optimale. Pour chaque état stable, la branche à gauche est le cas où la variable choisie reçoit la valeur choisie, la branche à droite est l\u0027autre cas, où la valeur choisie est supprimée du domaine de la variable.\nModélisation des problèmes de clustering sous contraintes\nNous présentons dans cette section un modèle en PPC pour le clustering sous contraintes. Nous disposons d\u0027une collection de n points (objets) et d\u0027une mesure de dissimilarité entre ces objets. Sans perte de généralité nous supposons que les points sont indexés et nommés par leur indice. La distance entre deux points i, j est notée d(i, j). Nous considérons le cas où le nombre k de clusters est connu à l\u0027avance. Le modèle a pour objectif de trouver une partition des points en k clusters minimisant le diamètre maximal des clusters. Affecter un point à un cluster devient associer ce point au représentant du cluster. Nous introduisons pour chaque point i ? [1, n] une variable entière G[i] qui donne la valeur du représentant associé.\nModèle\nNous introduisons également une variable D qui représente le diamètre maximal. C\u0027est une variable entière, car nous utilisons des solveurs sur des variables entières. Le domaine de D est l\u0027intervalle formé par la distance minimale et la distance maximale entre chaque paire de points. Dans nos expérimentations, lorsque la distance n\u0027est pas entière, elle est multipliée par 100 et seule la partie entière est conservée.\nNotre modèle permet de trouver la solution optimale, c\u0027est-à-dire une affectation complète des variables I, G et D qui satisfait les contraintes suivantes. -Deux points à une distance supérieure au diamètre maximal doivent être dans des clusters différents. Ceci est représenté par les contraintes réifiées suivantes : -La contrainte de séparation minimale indique que la séparation entre deux clusters doit être au moins ?, ou que deux points à une distance inférieure à ? doivent être dans le même cluster. Pour chaque i \u003c j ? [1, n] tel que d(i, j) \u003c ?, nous posons la contrainte :\n. -La contrainte de diamètre maximum indique que le diamètre de chaque cluster doit être au plus ?, autrement dit que deux points à une distance supérieure à ? doivent être dans des clusters différents. Pour chaque i \u003c j ? [1, n] tel que d(i, j) \u003e ?, nous posons les contraintes :\nPour les contraintes sur les couples de points :\n-Une contrainte must-link sur i, j se traduit par le fait que les points ont le même repré-sentant :\n. -Une contrainte cannot-link sur i \u003c j est exprimée par :\nStratégie de recherche Les stratégies de choix des variables et des valeurs doivent être données au solveur. Les variables sont choisies dans l\u0027ordre I, D puis G. Cet ordre indique que les représentants de cluster doivent être identifiés en premier, puis pour chaque borne sur le diamètre maximal le solveur essaye de déterminer l\u0027affectation de points aux clusters. \nAméliorations du modèle\nLes solveurs de PPC réalisant une recherche exhaustive, ce modèle permet de trouver la solution optimale. Afin d\u0027améliorer l\u0027efficacité du modèle, différents aspects sont considérés. La version 1.1 repose sur un ordonnancement préalable de points où : le premier point est loin des autres points, le deuxième point est loin du premier mais aussi assez loin des autres points, le troisième point est loin des deux premiers et aussi assez loin des restes, . . .Pour réaliser cette heuristique, supposons que les p premiers points soient identifiés, le (p + 1)-ème point est le arg max i?[p+1,n] f (i), où\nLa version 1.2 ordonne les points avec l\u0027algorithme FPF, en prenant k \u003d n (autant de classes que de points). Ainsi, chaque point est choisi par l\u0027algorithme FPF et l\u0027ordre de choix donne l\u0027ordre des points.\nAmélioration des contraintes Avec un k fixé et sans connaissances de contraintes utilisateur, il est prouvé dans Gonzalez (1985) que le diamètre d F P F calculé par l\u0027algorithme FPF vérifie d opt ? d F P F ? 2d opt , avec d opt le diamètre optimal. Cette connaissance implique des bornes pour la variable D, à savoir [d F P F /2, d F P F ]. De plus, pour chaque couple de points i, j : -si d(i, j) \u003c d F P F , nous ne posons pas la contrainte réifiée (1) sur i, j, Les résultats montrent que pour la base ionosphere, les contraintes ML aident en général à réduire l\u0027espace et le temps de recherche, ce qui n\u0027est pas le cas avec les contraintes CL. Cependant pour la base kdd_synthetic_control, les contraintes utilisateur diminuent la performance, car la différence entre les diamètres optimaux dans les cas avec ou sans contraintes est souvent significative. Le diamètre optimal dans le cas avec contraintes est souvent deux fois plus grand que celui sans contraintes. Nous pensons que le critère de diamètre maximal pourrait ne pas être le plus approprié pour la structure réelle de cette base, ce qui pourrait expliquer les résultats.\nConclusion\nNous présentons dans ce papier un modèle en programmation par contraintes pour la classification non supervisée. Le modèle trouve un optimum global qui minimise le diamètre maximal des clusters. Des données qualitatives ou quantitatives peuvent être considérées. Le modèle est extensible directement aux contraintes sur des clusters ou sur des instances. Nous présentons également des stratégies de recherche pour améliorer l\u0027efficacité du modèle. Des expérimentations sur des jeux de données classiques montrent l\u0027intérêt de notre approche.\n"
  },
  {
    "id": "430",
    "text": "Introduction\nLes documents textuels sont tellement abondants sur le Web que l\u0027information pertinente est souvent difficile à retrouver. Dans l\u0027objectif d\u0027offrir une meilleure navigation dans les corpus de documents, que ce soit pour l\u0027exploration du contenu ou la recherche d\u0027information, l\u0027extraction de thématiques (topic extraction) se distingue comme une tâche de fouille de textes dont l\u0027objectif est d\u0027extraire, automatiquement et sans catégories données a priori, des théma-tiques (sujets) à partir de grands corpus de documents. L\u0027extraction de thématiques a été étudiée par différentes communautés, que ce soit celle de la fouille de données (Anaya-Sánchez et al., 2008), du Traitement Automatique des Langues (Blei et al., 2003), de la linguistique computationnelle (Ferret, 2006) ou de la recherche d\u0027information (Zamir et al., 1997), d\u0027où l\u0027existence de différentes méthodes dédiées à cette tâche. A ces méthodes spécifiques, peuvent s\u0027ajouter des méthodes adaptées notamment de l\u0027apprentissage automatique non supervisé. Les résultats produits par l\u0027ensemble de ces méthodes prennent des formes hétérogènes : partitions, matrices, distributions de probabilités sur les mots, etc. Cela pose clairement un problème de comparaison de ces résultats. Dans cet article, nous proposons une nouvelle mesure de qualité, la Vraisemblance Généralisée, qui permet d\u0027évaluer et de comparer différentes méthodes d\u0027extraction de thématiques. La mesure proposée est calculée dans un nouvel espace de description où les documents sont décrits par les thématiques et que nous appelons l\u0027espace latent. Nous proposons également des opérateurs pour transformer les résultats des méthodes d\u0027extraction de thématiques vers l\u0027espace latent afin de calculer cette mesure.\nLa section 2 est consacrée à la présentation des principales méthodes d\u0027extraction des thé-matiques. La section 3 présente les principales mesures de qualité ainsi que la nouvelle mesure intitulée Vraisemblance Généralisée. Les expérimentations et les résultats sont ensuite présen-tés en section 4. La conclusion et les perspectives de recherche sont données en section 5.\nMéthodes d\u0027extraction de thématiques\nLa plupart des méthodes d\u0027extraction de thématiques nécessite que le corpus de documents soit mis sous forme d\u0027une matrice V où les lignes représentent les documents et les colonnes représentent les mots (modèle vectoriel de (Salton et al., 1975)). Chaque élément V ij de la matrice contient le poids du mot w j dans le document d i , qui reflète son importance dans le document. Le plus simple est de pondérer les mots par leurs fréquences d\u0027apparition dans les documents (fréquence TF), même s\u0027il existe d\u0027autres types de pondération.\nIl faut cependant noter que certaines méthodes d\u0027extraction de thématiques, notamment celles issues de la linguistique computationnelle (Ferret, 2006), manipulent les documents sous forme d\u0027un graphe où les noeuds représentent des unités linguistiques (mots, phrases, documents, etc.) et les arêtes représentent des relations entre elles, par exemple des relations sémantiques ou de co-occurrences. L\u0027extraction des thématiques est ensuite effectuée en utilisant les algorithmes classiques issus de la théorie des graphes, comme le clustering spectral (Ng et al., 2002). Nous avons choisi de ne pas intégrer ce type de représentation dans notre travail, du moins pour le moment.\nDans cette présentation, nous proposons de regrouper les méthodes d\u0027extraction de thé-matiques en trois grandes familles : les méthodes à base de distance, les méthodes à base de factorisation de matrices et les modèles de thématiques probabilistes.\nMéthodes à base de distance\nLes méthodes à base de distance se fondent sur le calcul d\u0027une distance pour mesurer la similarité entre les documents. La plupart des méthodes de cette catégorie sont des méthodes de classification automatique non supervisée. Même si ce n\u0027est pas leur vocation initiale, ces méthodes peuvent être utilisées pour l\u0027extraction de thématiques en considérant que chaque classe définit une thématique et regroupe ainsi les documents qui y sont relatifs. La caractéri-sation des thématiques peut ensuite se faire en post-traitement en prenant par exemple les mots les plus fréquents dans chaque classe, ou en cherchant les mots les plus discriminants.\nDans les méthodes à base de distance, on trouve principalement les méthodes de partitionnement et les méthodes hiérarchiques. Les méthodes de paritionnement, comme l\u0027algorithme des K-Means, commencent par répartir aléatoirement les documents sur un certain nombre de classes et, à chaque itération, les documents sont réaffectés de telle sorte que chacun soit dans la classe dont il est la plus proche (au sens de la mesure de similarité utilisée).\nPlusieurs variantes des K-Means existent, comme FCM (Fuzzy C-Means), qui permet une classification floue des documents, c\u0027est-à-dire un document n\u0027est pas affecté à une seule classe mais il appartient à plusieurs classes avec différents degrés d\u0027appartenance (Dunn, 1973). Les méthodes de partitionnement sont généralement de faible complexité, ce qui les rend adaptées aux grands volumes de données.\nLes méthodes hiérarchiques procèdent à la construction des classes au fur et à mesure par agglomération, ou par division. En agglomération, chaque classe contient, au départ, un seul document. Les deux classes les plus proches, en termes de distance, sont ensuite fusionnées récursivement jusqu\u0027à ce que tous les documents soient dans la même classe. En division, tous les documents sont dans une seule classe qui est divisée, récursivement, jusqu\u0027à ce que chaque document soit dans une classe. Dans (Pons-Porrata et al., 2003), une méthode hié-rarchique est proposée pour la classification de documents en se basant sur une distance qui prend en compte les entités temporelles et les noms de lieux. Les méthodes de classification hiérarchiques offrent la possibilité de contrôler la granularité des classes, et d\u0027avoir ainsi des classes aussi fines ou grandes que souhaité. En revanche, les méthodes hiérarchiques souffrent du problème de complexité, ce qui les rend inadaptées aux grands volumes de documents.\nQu\u0027elles soient à base de partitionnement ou hiérarchique, ces méthodes n\u0027ont pas été initialement créées pour extraire des thématiques. Cependant, un simple post-traitement permet d\u0027extraire des thématiques dans le sens où les centroïdes correspondent à des vecteurs dans l\u0027espace du vocabulaire de mots. Cela explique la présence de ces méthodes dans cette étude.\nMéthodes à base de factorisation de matrices\nEn algèbre linéaire, la factorisation de matrices est une approche qui peut permettre l\u0027extraction des thématiques. Le principe général est de partir de la matrice d\u0027occurences V , puis de trouver une factorisation de la matrice V en un produit de deux matrices W et H. H est une matrice dont les lignes sont constituées par des combinaisons de mots. Ce nouvel espace est appelé \"espace sémantique latent\" et il est défini par les thématiques, chaque thématique étant une combinaison de mots. W est une matrice de projection des documents dans le nouvel espace sémantique, où chaque élément W ij représente le degré d\u0027appartenance du document d i à la thématique c j . L\u0027analyse sémantique latente, LSA (Latent Semantic Analysis), permet de faire cette factorisation en effectuant une décomposition en valeurs singulières (Deerwester et al., 1990). Cependant, comme cette dernière peut produire des valeurs négatives, la méthode pose un problème d\u0027interprétabilité des résultats (Lee et Seung, 1999). Pour contourner ce problème, la factorisation non négative de matrices, NMF (Non-negative Matrix Factorization), a été proposée par (Lee et Seung, 1999). NMF permet de trouver une factorisation non-unique d\u0027une matrice non-négative V en un produit de deux matrices non-négatives W et H, de telle sorte que V ? W H. L\u0027objectif est de minimiser la fonction objectif J NMF suivante :\n(1) (Lee et Seung, 2001) décrivent une méthode itérative basée sur NMF pour l\u0027extraction de thématiques. Le problème de factorisation est ramené à un problème d\u0027optimisation de la fonction J NMF sous les contraintes de non négativité. Le problème est ensuite résolu en utilisant la méthode de Lagrange qui donne lieu aux deux règles de mise à jour suivantes :\nModèles de thématiques probabilistes\nLes modèles de thématiques probabilistes (probabilistic topic models) sont une famille de modèles graphiques qui ont pour objectif la découverte de thématiques dans des corpus de documents. Le principe est de considérer un document comme un mélange probabiliste de thématiques latentes, c\u0027est-à-dire un document est composé de plusieurs thématiques avec différentes proportions. Parallèlement à cela, chaque thématique est définie par une distribution de probabilités sur les mots. Par exemple, une thématique relative à la génétique associe des probabilités plus importantes sur les mots ADN, gène, cellule, etc. que sur les autres mots. Un modèle de thématiques probabiliste peut être vu, d\u0027un autre angle, comme un processus de génération de documents à partir d\u0027un vocabulaire (ensemble fixe de mots, notés w i ), tout en prenant en compte le fait que chaque document est un mélange probabiliste de plusieurs thématiques, notées z j . En supposant que les distributions p(w i |z j ) sont connues pour tout i, j, le processus simplifié de génération d\u0027un document d est le suivant :\n1. Se fixer une distribution de probabilités sur les thématiques p(z j |d) 2. Pour chaque mot w à générer :\n(a) Choisir aléatoirement une thématique z parmi les z j suivant la distribution fixée dans (1).\n(b) Choisir un mot w parmi w i dans le vocabulaire suivant la distribution p(w i |z).\nA partir de là, la procédure consiste à inverser le processus génératif en utilisant la loi de Bayes afin d\u0027estimer les valeurs des paramètres p(w i /z j ) et p(z j /d). Ceci est réalisé en utilisant les techniques d\u0027apprentissage et d\u0027inférence des modèles graphiques probabilistes, comme le Gibb\u0027s sampling ou l\u0027inférence variationnelle.\nLes modèles de thématiques probabilistes proposés dans la littérature partagent globalement le principe génératif exposé ci-dessus, mais diffèrent principalement dans la manière de choisir les distributions de probabilités p(z i |d), p(w i |z j ). Dans PLSA (Hofmann, 1999), aucune hypothèse de la distribution des thématiques sur les documents n\u0027est posée ; chaque document est traité à part. Dans LDA (Blei et al., 2003), chaque thématique est caractérisée par une distribution multinomiale sur les mots qui lui sont associés. LDA utilise une loi de Dirichlet pour permettre un choix judicieux des paramêtres des distributions multinomiales, et ainsi pallier les limites de PLSA.\nLes modèles de thématiques probabilistes diffèrent également par leur structure. En effet, certains supposent l\u0027existence d\u0027autres variables latentes que les thématiques, par exemple des variables temporelles ou d\u0027opinion, et permettent ainsi d\u0027extraire ces connaissances en même temps que les thématiques.\nLes trois familles de méthodes exposées ci-dessus ont des inspirations différentes. Néan-moins, il a été montré que des liens théoriques existent entre ces méthodes. La méthode NMF est équivalente à Kernel K-Means, une version des K-Means avec noyau (Ding et al., 2005). La méthode PLSA est équivalente à NMF en prenant la divergence de Kullback-Leibler dans la fonction objectif (Gaussier et Goutte, 2005).\nEvaluation des méthodes d\u0027extraction de thématiques\nLes différentes méthodes d\u0027extraction de thématiques produisent des résultats de forme hétérogène : partitions de documents, distributions de probabilités sur les mots, matrices, etc. Cela pose le problème de comparaison des résultats. Pour résoudre ce problème, nous proposons un nouvel espace de description commun aux différentes méthodes et une nouvelle mesure de qualité qui se calcule dans cet espace. Cela permet ainsi de comparer des approches de nature différente et de manière quantitative. Cette section présente les mesures de qualité existantes et la nouvelle mesure que nous proposons.\nMesures existantes\nLes méthodes d\u0027extraction de thématiques sont généralement évaluées de manière qualitative ou quantitative. L\u0027approche qualitative a recours au jugement humain pour qualifier les thématiques sans donner aucun indice quantitatif pour comparer les méthodes entre elles. A contrario, l\u0027approche quantitative permet de mesurer plus finement la qualité des modèles, qu\u0027elle soit basée sur le jugement humain ou non. Le jugement humain est utilisé pour évaluer les thématiques selon deux critères : word intrusion et topic intrusion (Chang et al., 2009). Une des mesures quantitatives qui n\u0027utilisent pas le jugement humain (automatiques) est la vraisemblance mais elle se calcule seulement sur les modèles probabilistes et sur les méthodes d\u0027apprentissage de type EM (Dempster et al., 1977), ce qui n\u0027est pas le cas de toutes les mé-thodes d\u0027extraction de thématiques.\nLa problématique d\u0027évaluation des thématiques se retrouve classiquement en apprentissage non supervisé avec les mesures recensées par (Halkidi et al., 2001). Celles-ci peuvent être réparties en deux catégories : mesures externes et mesures internes. Les mesures externes évaluent la qualité des résultats par rapport à une référence définie par les classes a priori des documents. Comme exemples de ce type de mesures on peut citer le F-score (moyenne harmonique du rappel et de la précision), l\u0027entropie (mesure de désordre dans l\u0027ensemble des thématiques) et la pureté (ratio moyen de la classe majoritaire dans chacune des thématiques). Les mesures internes ne font pas appel à des connaissances extérieures. Par exemple, l\u0027inertie intra-classes est utilisée comme fonction objectif dans la méthode des K-Means, ou, la cohé-sion (Steinbach et al., 2000), qui mesure la similarité Cosinus entre les documents d\u0027une même thématique.\nMême si ces mesures peuvent être utilisées pour évaluer les thématiques, en considérant que chaque thématique correspond à une classe, elles ne sont pas dédiées à cette tâche. A notre connaissance, il n\u0027existe pas à ce jour de mesure automatique qui permette d\u0027évaluer toutes les méthodes présentées de manière uniforme, et ainsi de pouvoir les comparer.\nNouvelle mesure : la Vraisemblance Généralisée\nLa mesure que nous proposons, intitulée Vraisemblance Généralisée (V G), est une mesure quantitative interne qui permet d\u0027évaluer plusieurs méthodes d\u0027extraction de thématiques, même si ces dernières sont basées sur des modèles mathématiques différents. Le bien-fondé de la mesure V G repose sur le fait qu\u0027il existe une analogie entre les différentes méthodes. En effet, toutes ces méthodes permettent, d\u0027une manière ou d\u0027une autre, de projeter les documents dans un espace de description formé par les thématiques et de décrire ces thématiques par des mots (cf. figure 1). Notre idée consiste à proposer la mesure V G qui se calcule dans l\u0027espace latent ainsi que des transformations des résultats des méthodes vers l\u0027espace latent. \nFIG. 1 -Espace latent : les documents sont projetés dans l\u0027espace latent caractérisé par les thématiques z 1 , z 2 (matrice W ) et les thématiques sont décrites par les mots (matrice H).\nLa mesure V G est calculée à partir de deux matrices : la matrice de projection W , matrice de projection des documents dans l\u0027espace latent et la matrice de l\u0027espace latent H qui définit cet espace. La matrice H est caractérisée par un ensemble de vecteurs, pas nécessairement orthonormés, décrits dans l\u0027espace des mots (cf. figure 1). Ce sont donc des vecteurs positifs ou nuls. La matrice W est caractérisée par un ensemble de vecteurs correspondant aux documents, décrits dans l\u0027espace des thématiques. En d\u0027autres termes, les matrices W et H sont telles que :\n-W ik est le score d\u0027appartenance du document d i à la thématique z k .\n-H kj est le score d\u0027appartenance du mot w j à la thématique z k .\nNous définissons trois transformations vers l\u0027espace latent pour trois méthodes issues des principales approches présentées dans la section 2 (LDA, NMF et FCM). Pour la méthode LDA, W ik est la probabilité p(d i |z k ), et H kj est la probabilité p(z k |w j ). Pour la méthode NMF, les deux matrices sont directement obtenues par factorisation. Pour la méthode FCM, W ik est le degré d\u0027appartenance du document d i à la classe z k , et le vecteur H k * est le centroïde de la classe z k . Les deux matrices W et H doivent être normalisées (si elles ne l\u0027étaient pas déjà) afin d\u0027avoir un même ordre de grandeur quelque soit la méthode utilisée et d\u0027éviter ainsi un biais éventuel dans le calcul de la mesure : -|Z| k\u003d1 W ik \u003d 1, ?i ? {1..|D|} (normalisation des lignes de W ).\n|Z|} (normalisation des lignes de H).\nOù Z est l\u0027ensemble de thématiques et D est l\u0027ensemble de documents. Sous ces hypothèses, nous définissons score(d i , w j ) le score de vraisemblance d\u0027une occurrence du mot w j dans le document d i comme suit :\nEn d\u0027autres termes, score(d i , w j ) est obtenu en multipliant la ligne de la matrice W qui correspond au document d i (i ème ligne) par la colonne de la matrice H qui correspond au mot w j (j ème colonne). Ensuite, le score de vraisemblance d\u0027un document d est défini comme suit, V étant l\u0027ensemble des mots du corpus (vocabulaire) :\nw?V Où n(d, w) est le nombre d\u0027occurrences du mot w dans le document d. En passant au log :\nw?V La mesure V G est basée sur la moyenne géométrique des scores individuels sur les documents, score(d i ), chaque score étant lui-même un produit calculé sur chaque mot du vocabulaire (équation 5). La multiplication géométrique a donc la forme d\u0027un produit de produits. Pour normaliser, il suffit de mettre à la puissance inverse du nombre de termes dans la multiplication. Celui-ci est égal à la double somme n(d, w). Au final, la mesure V G est calculée avec d?D w?V la formule suivante :\nLa mesure V G peut être calculée sur un corpus de test différent du corpus sur lequel les thé-matiques sont extraites (corpus d\u0027apprentissage) mais ceci suppose que le modèle soit prédictif, c\u0027est-à-dire capable d\u0027affecter les nouveaux documents de test aux thématiques déjà extraites. Ceci n\u0027est malheureusement pas le cas de toutes les méthodes, notamment les méthodes d\u0027apprentissage non supervisé. Afin de mieux interpréter le résultat de la mesure V G, nous avons donc choisi, pour le moment, de l\u0027évaluer en ne travaillant que sur le corpus d\u0027apprentissage.\nExpérimentations\nDans cette section, nous présentons le protocole expérimental (corpus, prétraitements, outils, paramètres des méthodes, etc.), ainsi que les résultats et la discussion. \nProtocole expérimental\nLes tests sont effectués sur deux corpus : AP et Elections. AP est un corpus de documents de l\u0027agence de presse Associated Press (Harman, 1993), également utilisé dans (Blei et al., 2003). Elections est un corpus de documents Web (médias, blogs, réseaux sociaux, etc.), qui traitent des élections présidentielles françaises de 2012. Ces documents ont été collectés durant la période du 16/03/2012 au 16/04/2012 par la plateforme de veille AMIEI (http ://www.amisw.com). Le tableau 1 résume les contenus des deux corpus, après les pré-traitements suivants :\n-Suppression de mots outils (stopwords), par exemple le, sur, dans.\n-Racinisation (stemming), par exemple les mots logement, loger deviennent log.\n-Suppression des mots qui occurrent une seule fois dans le document.\nLes tests sont réalisés en choisissant une méthode de chaque famille : LDA pour les modèles de thématiques probabilistes, NMF pour les méthodes à base de factorisation de matrices et FCM pour les méthodes à base de distance. Afin de limiter le risque de tomber dans des optima locaux, le même test est réalisé 5 fois et la moyenne est retenue. Les paramètres de la méthodes LDA sont fixés comme suit : ? \u003d 50, ? \u003d 0.01, nombre d\u0027itérations \u003d 1000. Les paramètres de la méthode FCM sont fixés comme suit : m \u003d 1.1, nombre maximum d\u0027ité-rations \u003d 20. Pour exécuter LDA, nous nous sommes appuyés sur l\u0027outil Mallet (McCallum, 2002). Pour NMF, nous avons utilisé notre propre implémentation, et pour FCM, nous avons utilisé le langage R (R, 2012 \nRésultats et discussion\nLes résultats d\u0027exécution des trois méthodes LDA, NMF et FCM sont représentés dans le tableau 2. La comparaison des trois méthodes par la mesure V G est représentée dans la figure 2. Les résultats sur les cas extrêmes sont représentées dans la figure 3. Les méthodes LDA et NMF présentent un comportement similaire au vu de la variation de la mesure V G en fonction du nombre de thématiques (cf. figure 2). En effet, cette dernière augmente avec l\u0027augmentation du nombre de thématiques. Ceci est en concordance avec l\u0027intuition car un trop petit nombre de thématiques mène à mélanger plusieurs thématiques dans une seule, et donne ainsi des résultats de moins bonne qualité. En revanche, un grand nombre de thématiques permet de mieux séparer les thématiques, permet aux thématiques de petite taille d\u0027émerger, et donne ainsi un résultat de meilleure qualité. Si le nombre de théma-tiques est encore plus grand (proche du nombre de documents), les résultats convergent vers un modèle où une thématique est extraite pour chaque document. La valeur de la mesure V G continue à augmenter sans pour autant que le résultat soit forcément de meilleure qualité. Ce problème est similaire au problème de surapprentissage (overfitting) connu dans le domaine de l\u0027apprentissage statistique. LDA demeure la méthode qui donne les meilleurs résultats, en termes de la mesure V G, par rapport à NMF et FCM, et ce sur les deux corpus (cf. figure 2). La qualité des résultats donnés par la méthode FCM est remarquablement inférieure, en termes de la mesure V G, à celle des deux autres méthodes. Ceci est conforme aux exemples donnés sur le tableau 2. En effet, les thématiques extraites par la méthode FCM sont mélangées et très difficiles à interpréter.\nL\u0027objectif du test sur les cas extrêmes est d\u0027analyser le comportement de la mesure V G dans les deux cas extrêmes Crisp et Uniforme (cf. section 4.1). Suivant V G, Crisp et Uniforme sont des configurations moins bonnes que celle produite par NMF (cf. figure 3). Cela confirme que ces deux cas extrêmes ne donnent pas de bons résultats et qu\u0027un bon ensemble de thématiques constitue en général un compromis entre les deux extrêmes, à savoir quelques thématiques pertinentes pour un document.\nConclusion\nLes méthodes d\u0027extraction de thématiques, étant issues de domaines variés, produisent des résultats de forme hétérogène, ce qui empêche leur comparaison de manière uniforme. Dans cet article, nous avons proposé une mesure d\u0027évaluation, la Vraisemblance Généralisée, qui permet d\u0027évaluer dans un cadre commun les méthodes d\u0027extraction de thématiques. Pour calculer la mesure, les résultats de ces méthodes sont transformés dans un espace latent qui plonge les documents dans l\u0027espace latent des thématiques.\nLa mesure de qualité V G a permis de comparer trois méthodes d\u0027extraction de thématiques (LDA, NMF et FCM) sur deux corpus différents. Les résultats ont donné l\u0027avantage à la mé-thode LDA, suivie de NMF puis de FCM. Les résultats donnés par la méthode d\u0027apprentissage FCM étaient d\u0027une qualité inférieure, en termes de la mesure V G, par rapport aux deux autres méthodes. Ceci nous semble conforme avec une analyse qualitative des thématiques extraites par cette méthode. En effet, ces dernières étaient mélangées et très difficiles à interpréter.\nIl serait intéressant, en complément à ce travail, de tester le comportement de la mesure V G sur des corpus de test (différents des corpus d\u0027apprentissage). Cela nécessiterait la définition des opérations de prédiction pour les méthodes d\u0027extraction de thématiques afin de pouvoir affecter les nouveaux documents aux thématiques. Il serait également intéressant d\u0027envisager des analyses plus poussées afin de vérifier que l\u0027on n\u0027introduit pas de biais dans la comparaison des méthodes avec notre approche de transformation des résultats vers l\u0027espace latent. En effet, il se peut que les transformations employées jouent en désavantage de certaines méthodes en dégradant ainsi artificiellement la qualité de leurs résultats.\n"
  },
  {
    "id": "432",
    "text": "Introduction\nLes cartes cognitives (Axelrod, 1976) sont un modèle de base de connaissances populaire pour aider à la prise de décision. Elles fournissent un moyen de communication visuel facile pour analyser un système complexe. Elles ont été utilisées dans de nombreux domaines, tels que la biologie, la sociologie, la politique. . . Une carte cognitive est un graphe où chaque noeud est un concept et des influences étiquettent chacune un arc avec une valeur. Celle-ci appartient à un ensemble prédéfini qui peut contenir des valeurs symboliques tel que {+, ?} (Axelrod, 1976) ou {nul, faible, moyen, fort} (Dickerson et Kosko, 1994) ou qui peut être un intervalle tel que [?1, 1] (Kosko, 1986). En combinant les valeurs des influences composant les chemins entre deux concepts, il est possible de calculer l\u0027influence propagée entre ces deux concepts.\nIl est difficile pour un concepteur de construire une carte cognitive tout en assurant sa qualité. En effet, il arrive parfois que deux chemins d\u0027influence entre deux concepts mènent à des conclusions qui se contredisent. Cet article propose une méthode pour valider la qualité d\u0027une carte cognitive. La validation est basée sur un critère de qualité qui est utilisé pour contrôler une base de connaissances. La validation est souvent partagée en deux catégories : la vérifica-tion et le test (Ayel et Laurent, 1991). La vérification est basée sur un critère qui ne nécessite pas d\u0027information externe, autrement dit ce type de critère dépend uniquement de la cohérence interne de la base. Le test est basé sur un critère qui nécessite de l\u0027information externe, comme une spécification de contraintes. Il y a eu de nombreux travaux sur des bases de connaissances mais il n\u0027y a pas eu de travaux dédiés à la validation des cartes cognitives à notre connaissance. Pour vérifier une carte cognitive, cet article introduit la notion de carte non-ambiguë. Une carte cognitive est non-ambiguë si pour toute paire de concepts, les influences propagées sur les différents chemins entre ces deux concepts ne sont pas contradictoires avec l\u0027influence propagée entre ces deux concepts. Pour tester une carte cognitive, cet article introduit la notion de carte cognitive cohérente. Pour cela, une spécification est définie. L\u0027idée est de construire cette spécification sur une hiérarchie de concepts telle que les concepts les plus bas dans la hiérarchie sont les concepts de la carte. Grâce à cette hiérarchie et à la carte, on peut calculer l\u0027influence hiérarchique (Chauvin et al., 2011) entre deux concepts de la hiérarchie selon les influences propagées entre les concepts spécialisant ces deux concepts dans la carte. Une spécification est un ensemble de contraintes. Une contrainte est un triplet fait d\u0027un concept source et d\u0027un concept destination appartenant tout deux à la hiérarchie ainsi que d\u0027une valeur. Une carte cognitive est cohérente avec une contrainte si la valeur de cette contrainte n\u0027est pas contradictoire avec celle de l\u0027influence hiérarchique entre les deux concepts. Cette même carte est cohérente avec une spécification si elle est cohérente avec l\u0027ensemble des contraintes.\nDans la partie 2, le modèle classique des cartes cognitives ainsi que l\u0027influence propagée entre deux concepts sont rappelés. Dans la partie 3, nous décrivons comment vérifier une carte cognitive. Dans la partie 4, nous décrivons comment tester une carte cognitive.\nCarte cognitive\nUne carte cognitive est un graphe orienté dans lequel les noeuds sont des concepts, représentés par de brefs textes. Les arcs représentent les influences entre ces concepts et sont étiquetés par une valeur d\u0027influence appartenant à un ensemble de valeurs prédéfini.\nDefinition 1 (Carte cognitive) Une carte cognitive définie sur un ensemble de concepts C et un ensemble de valeurs I est un graphe orienté CM \u003d (C, A, etiq I ) où les concepts de C sont les noeuds du graphe, A ? C ×C est un ensemble d\u0027arcs appelés influences qui lient les concepts et etiq I : A ? I est une application étiquetant chaque arc de A avec une valeur de I.\nRoute sinueuse (RS)\nRoute secondaire (R2)\nExemple 1 Carte1 (figure 1) est définie sur l\u0027ensemble de valeurs I \u003d {+, ?}. Cette carte représente quelques causes d\u0027accident de la route. La valeur + représente une influence positive. La valeur ? représente une influence négative. Ainsi, si on considère les concepts P (Pluie) et RG (Route glissante), circuler sous la pluie augmente les risques que la route soit glissante.\nL\u0027influence propagée d\u0027un concept sur un autre est calculée selon les chemins d\u0027influences minimaux entre les concepts et l\u0027influence propagée sur chacun de ces chemins.\nde c 1 à c 2 tel que p est une sous-séquence de p. On note P c1,c2 l\u0027ensemble des chemins minimaux de c 1 à c 2 .\nL\u0027influence propagée IP sur un chemin d\u0027influence p est la valeur de l\u0027influence d\u0027un concept sur un autre suivant ce chemin (Genest et Loiseau, 2007).\nDefinition 3 (Influence propagée sur un chemin)\nL\u0027influence propagée I entre deux concepts c 1 et c 2 agrège les influences propagées sur chacun des chemins minimaux entre ces deux concepts. Cette influence propagée introduit deux nouvelles valeurs. La valeur 0 signifie qu\u0027il n\u0027y a pas d\u0027influence entre les deux concepts. La valeur ? signifie que l\u0027influence entre les deux concepts est contradictoire : il existe deux chemins entre les concepts tels que leurs influences propagées sont opposées.\nDefinition 4 (Influence propagée entre deux concepts)\nExemple 2 Dans Carte1, il y a deux chemins minimaux entre Pluie et Accident :\nPour vérifier une carte cognitive, deux critères de qualité sont proposés : la propreté et la non-ambiguïté. Avant de définir ces critères, nous devons définir la non-contradiction entre une valeur et une autre. Une valeur est contradictoire avec une autre si elle exprime une information qui est opposée à l\u0027autre. Avec l\u0027ensemble de valeurs {+, ?}, une contradiction apparaît donc dès que deux valeurs sont différentes 1 .\nDefinition 5 (Non-contradiction) Soient i 1 , i 2 ? I deux valeurs. i 1 est non-contradictoire avec i 2 , noté i 1 i 2 , ssi i 1 \u003d i 2 .\nUne carte cognitive est propre si l\u0027influence directe entre deux concepts n\u0027est pas contradictoire avec l\u0027influence propagée entre ces deux concepts.\nDefinition 6 (Carte cognitive propre) CM est une carte cognitive propre ssi :\nLa non-ambiguïté vérifie que les valeurs d\u0027influences propagées sur les chemins entre deux concepts sont non-contradictoires avec la valeur de l\u0027influence propagée entre ces concepts.\nDefinition 7 (Carte cognitive non-ambiguë) CM est une carte cognitive non-ambiguë ssi : ?c 1 , c 2 ? C, ?p ? P c1,c2 , IP(p) I(c 1 , c 2 )\nExemple 3 La carte cognitive de gauche de la figure 2 est non-propre tandis que la carte cognitive de droite est propre mais ambiguë. \nTest d\u0027une carte cognitive\nPour tester une carte cognitive, trois nouvelles notions doivent être introduites : les notions de hiérarchie, de contrainte et de spécification. Une spécification et une hiérarchie sont fournies par le concepteur en vue de la validation de la carte cognitive.\nUne hiérarchie H \u003d (C, est un ensemble de concepts C associé à une relation de pré-ordre de simple héritage. Les concepts élémentaires d\u0027un ensemble de concepts selon une hiérarchie sont les feuilles des arbres représentant la relation d\u0027héritage. \nFIG. 3 -Une hiérarchie de concepts (H1).\nExemple 4 H1 (figure 3) est une hiérarchie ordonnant quelques concepts. Les concepts élé-mentaires de H1 apparaissent doublement encadrés.\nL\u0027influence ontologique (Chauvin et al., 2011) permet de calculer l\u0027influence entre deux concepts d\u0027une ontologie dans une carte cognitive ontologique. Ces définitions peuvent s\u0027appliquer à la notion de hiérarchie moyennant une adaptation. On note elem H (c) les concepts élé-mentaires plus petits qu\u0027un concept c selon la hiérarchie H. L\u0027influence hiérarchique entre deux concepts est définie comme une agrégation des influences propagées entre tous les concepts élémentaires du premier concept et ceux du second concept. L\u0027influence hiérarchique introduit elle aussi deux nouvelles valeurs : la valeur ? signifie que l\u0027influence entre deux concepts n\u0027est pas négative et la valeur signifie que l\u0027influence entre deux concepts n\u0027est pas positive.\nDefinition 8 (Influence hiérarchique entre deux concepts selon une hiérarchie) Soient c 1 , c 2 ? C deux concepts. L\u0027influence hiérarchique entre c 1 et c 2 selon H est : Exemple 5  Exemple 6 Soient les contraintes C 1 \u003d A, + et C 2 \u003d MCT, + S1 \u003d {C 1 , C 2 } construite sur H1 et {+, ?, 0, ?, ?} est une spécification.\nPour tester une carte cognitive, deux critères de qualité sont proposés : la cohérence et la compatibilité. Une carte cognitive est cohérente avec une contrainte si l\u0027influence hiérarchique entre les concepts d\u0027une contrainte n\u0027est pas contradictoire avec la valeur de cette contrainte.\nDefinition 9 (Carte cognitive cohérente avec une spécification selon une hiérarchie) CM est cohérente avec c \u003d 1 , c 2 , s selon H ssi I H (c 1 , c 2 ) s. CM est cohérente avec Spec selon H ssi CM est cohérente avec toutes les contraintes de Spec selon H.\nParfois, imposer une valeur stricte à une influence hiérarchique par l\u0027intermédiaire d\u0027une contrainte peut être trop contraignant pour l\u0027utilisateur. Celui-ci peut préférer indiquer simplement que l\u0027influence doit être proche de la valeur de la contrainte. Ainsi, plutôt que de vérifier si deux valeurs sont non-contradictoires, nous allons vérifier leur compatibilité. Intuitivement, les valeurs positives sont incompatibles avec les négatives et inversement. Pour les valeurs extrêmes situées aux bornes telles que + ou 0, la notion de compatibilité devient plus discutable. Si un utilisateur spécifie une contrainte ayant pour valeur +, nous considérons qu\u0027il ne s\u0027attend pas à obtenir 0. Si c\u0027était le cas, il aurait plutôt dû fournir par exemple une contrainte de valeur ?. Enfin, nous considérons que la valeur ? n\u0027est compatible qu\u0027avec elle-même.\nDefinition 10 (Compatibilité d\u0027une valeur) Soient i 1 , i 2 ? I + deux valeurs. On définit que i 1 est compatible avec i 2 , noté i 1 i 2 , tel que :\nOn définit la compatibilité d\u0027une carte cognitive avec une spécification de manière similaire à la cohérence en utilisant cette fois la notion de compatibilité.\nDefinition 11 (Carte cognitive compatible avec une spécification selon une hiérarchie) CM est compatible avec c \u003d 1 , c 2 , s selon H ssi I H (c 1 , c 2 ) s. CM est compatible avec Spec selon H ssi CM est compatible avec toutes les contraintes de Spec selon H.\nExemple 7 Carte1 est cohérente avec C 1 selon H1 car I H (MCT, A) \u003d +. Cependant, elle n\u0027est pas cohérente avec C 2 car I H (MT, MCT) \u003d ?. Ainsi, elle n\u0027est pas cohérente avec S1. En revanche, elle est compatible avec C 1 puisqu\u0027elle est cohérente avec C 1 et est aussi compatible avec C 2 car I H (MT, MCT) +. Ainsi, Carte1 est compatible avec S1.\nConclusion\nNous avons présenté une formalisation de la validation des cartes cognitives pour laquelle nous avons introduit les critères de propreté et de non-ambiguïté. Le modèle des cartes cognitives est associé à une hiérarchie de manière à pouvoir tester la cohérence ainsi que la compatibilité d\u0027une carte par rapport à une spécification de contraintes.\n"
  },
  {
    "id": "433",
    "text": "Introduction\nDans de nombreuses applications pratiques de gestion de clusters, résultat d\u0027une opération de classification non supervisée (clustering), les objets à classifier évoluent dans le temps. Le but est donc d\u0027obtenir des clusters optimaux à chaque pas de temps (Falkowski et al. (2006)). Tang et al. (2008) et Zhang et al. (2009) ont proposés des méthodes de clustering évolutif dans le but de produire des clusters optimaux qui reflètent les dérives à long terme dans les objets tout en étant robuste aux variations à court terme. Chi et al. (2009) ont développé cette idée en proposant deux cadres évolutifs pour le clustering spectral : PCQ (Preserving Cluster Quality) et PCM (Preserving Cluster Membership). Les deux cadres ont été proposés afin d\u0027optimiser la modification de la fonction de coût proposée initialement par Chakrabarti et al. (2006).\nNotre travail adapte le principe d\u0027incrémentabilité afin de le généraliser à un ensemble d\u0027algorithmes de clustering. Le cadre proposé consiste à estimer les états de données à l\u0027aide des proximités qui sont à la fois actuels et passés. Puis, il effectue un clustering statique sur les estimations de ces états. Ce cadre de suivi de clustering évolutif d\u0027une manière incrémen-tale a été utilisé pour étendre une variété d\u0027algorithmes de clustering statiques tels que les C-Moyennes Floues (CMF) de Bezdek (1984), les k-moyennes de Mac-Queen (1967) et les approches spectrales de clustering présentées par Filippone et al. (2008).\nLe reste de ce papier est organisé comme suit : en section 2, nous présentons le cadre évolutif du clustering. La section 3 présente les résultats d\u0027expérimentation du cadre proposé sur une variété différente d\u0027algorithmes de clustering statiques. La section 4 conclue le papier et présente les travaux futures.\nNous traitons le clustering évolutif comme étant un problème de suivi par un regroupement statique ordinaire. Pour ceci, nous étudions des matrices de proximité, notées W t , comme la réalisation d\u0027un processus aléatoire non stationnaire indexé par des mesures de temps discrètes. Elles sont données par l\u0027équation (1).\nOù W t est une matrice déterministe inconnue des états non observés et N t est une matrice de bruit de moyenne nulle. ? t change au fil du temps pour réfléchir à long terme des dérives dans les proximités. Nous présentons une approche plus simple qui implique une mise à jour recursive des estimations de ces états en utilisant un seul paramètre ? nommé facteur d\u0027oubli.\nEstimation de la matrice de proximité\nUne meilleure estimation peut être obtenue en utilisant un lissage de la matrice de proximité W t définie dans l\u0027équation (2).\nCette matrice lissée est un candidat dans l\u0027estimation de ? t . Des méthodes d\u0027estimation ont été proposées dans Ledoit et Wolf (2003), Schäfer et Strimmer (2005) et Chen et al. (2010). D\u0027une manière générale, ils calculent la différence entre la matrice de proximité réelle et lissée donnée par l\u0027équation (3).\nPuisque N t , N t?1 , ..., N 0 sont mutuellement indépendants et ont une moyenne nulle et la variance conditionnelle de W t?1 est nulle, le risque de l\u0027espérance conditionnelle de la fonction de perte peut être alors exprimé dans l\u0027équation (4).\nLa dérivée première correspondante au facteur d\u0027oubli est donnée par l\u0027équation (5). Pour les k-moyennes, la figure 2 montre que si nous estimons ? t , la valeur de l\u0027indice de Rand (1971) est plus grande pour cadre évolutif. Par contre avec ? t \u003d 0.5, la figure 2 montre qu\u0027il y a égalité entre l\u0027algorithme des k-moyennes dans les cadres statique et évolutif.\nD\u0027après la figure 3, nous remarquons que lorsque nous estimons ? t , la valeur de l\u0027indice de Rand est plus grande dans le cadre évolutif que celui statique. L\u0027algorithme de CMF statique ne fonctionne pas bien dès que les clusters commencent à se chevaucher vers environ le 9 ème pas de temps. Avec ? t \u003d 0.5, les deux cadres fournissent les mêmes résultats. Pour assurer le bon fonctionnement du cadre proposé, nous allons étendre sa comparaison avec les résultats de deux cadres PCQ et PCM de Chi et al. (2009). Le tableau 1 montre que le cadre proposé avec l\u0027algorithme de CMF reste toujours plus efficace que les deux cadres PCQ et PCM puisque le résultat de la valeur de l\u0027indice de Rand (1971)  De même, nous comparons l\u0027approche spectrale de clustering évolutif avec celle statique avec un ? t estimé et fixe. La figure 3 montre que l\u0027algorithme fonctionne bien avec l\u0027aspect incrémental et donne une valeur de l\u0027indice de Rand (1971) plus grande que celle obtenue avec le cadre statique. Ce qui implique le bon fonctionnement du cadre avec les approches spectrales.\nAvec ? t \u003d 0.5, nous remarquons, d\u0027après 3, que le comportement est le même pour les deux cadres statique et évolutif.\nConclusion\nLe cadre proposé dans ce travail surpasse généralement par celui statique en produisant des clusters qui reflètent les tendances à long terme tout en étant robuste aux variations à court terme. Il est universel dans le sens qu\u0027il permet à n\u0027importe quel algorithme de clustering statique d\u0027être étendu à un caractère évolutif qui fournit une méthode explicite pour la sélection du facteur d\u0027oubli, contrairement aux méthodes existantes. L\u0027objectif était de suivre avec pré-cision la matrice réelle de proximité à chaque pas de temps. Cela a été accompli en utilisant une mise à jour récursive avec un facteur d\u0027adaptation d\u0027oubli qui contrôle la quantité de poids à appliquer aux données historiques.\nL\u0027expérimentation a donné des résultats reflétant la performance du cadre adapté dans la performance de l\u0027opération du clustering par rapport à celle statique.\nComme perspectives de travail, nous proposons d\u0027élargir les expérimentations sur d\u0027autres jeux de données et la possibilité d\u0027étendre ce cadre afin qu\u0027il puisse supporter des larges BD par réduction de l\u0027échelle et l\u0027échantillonnage des données.\n"
  },
  {
    "id": "435",
    "text": "Introduction\nDans un projet de fouille de données, la phase de préparation des données vise à extraire une table de données pour la phase de modélisation (Pyle, 1999), (Chapman et al., 2000). La préparation des données est non seulement coûteuse en temps d\u0027étude, mais également critique pour la qualité des résultats escomptés. La préparation repose essentiellement sur la recherche d\u0027une représentation pertinente pour le problème à modéliser, recherche qui se base sur des étapes complémentaires de construction et de sélection de variables. La sélection de variables a été largement étudiée dans la littérature (Guyon et al., 2006). Dans ce papier, nous nous focalisons sur l\u0027approche filtre, qui évalue la corrélation entre les variables explicatives et la variable cible indépendamment de la méthode de classification utilisée, et est adaptée à la phase de préparation des données dans le cas d\u0027un grand nombre de variables descriptives.\nLa construction de variables (Liu et Motoda, 1998) est un sujet nettement moins étudié dans la littérature scientifique, qui représente néanmoins un travail considérable pour l\u0027analyste de données. Celui-ci exploite sa connaissance du domaine pour créer de nouvelles variables potentiellement informatives. En pratique, les données initiales sont souvent issues de bases de données relationnelles et ne sont pas directement exploitables pour la plupart des techniques de classification qui exploitent un format tabulaire attributs-valeurs. La fouille de données relationnelle, en anglais Multi-Relational Data Mining (MRDM), introduit par (Knobbe et al., 1999) vise à exploiter directement le formalisme multi-tables, en transformant la représen-tation relationnelle. En programmation logique inductive (ILP) (Džeroski et Lavra?, 2001), les données sont recodées sous forme de prédicats logiques. D\u0027autres méthodes, dénommées propositionalisation Kramer et al. (2001) effectuent une mise à plat au format tabulaire de la représentation multi-tables par création de nouvelles variables. Par exemple, la méthode Relaggs (Krogel et Wrobel, 2001) exploite des fonctions de type moyenne, médiane, min, max pour résumer les variables numériques des tables secondaires ou des comptes par valeur pour les variables catégorielles secondaires. La méthode Tilde (Blockeel et al., 1998), (Vens et al., 2006) permet de construire des agrégats complexes exploitant des conjonctions de condition de sélection d\u0027individus dans les tables secondaires. L\u0027expressivité de ces méthodes se heurte néanmoins aux problèmes suivants : complexité du paramétrage de la méthode, explosion combinatoire du nombre de variables construites difficile à maitriser et risque de sur-apprentissage croissant avec le nombre de variables produites.\nDans ce papier, nous proposons un cadre visant à automatiser la construction de variables, avec un objectif de simplicité, de maîtrise de la combinatoire de construction des variables et de robustesse vis-à-vis du sur-apprentissage. La partie 2 présente un formalisme de description d\u0027un domaine de connaissance, basé sur un format des données multi-tables en entrée et une liste des règles de construction de variables. La partie 3 introduit un critère d\u0027évaluation des variables construites selon un approche Bayesienne, en proposant une distribution a priori sur l\u0027espace des variables constructibles. La partie 4 analyse le problème d\u0027échantillonnage dans cet espace et propose un algorithme efficace et calculable pour produire des échantillons de variables construites de taille désirée. La partie 5 évalue l\u0027approche sur de nombreux jeux de données. Finalement, la partie 6 conclut cet article et propose des pistes de travaux futurs.\nSpécification d\u0027un domaine de construction de variables\nNous proposons dans cette section un cadre formel pour spécifier un domaine de connaissance permettant de piloter efficacement la construction de variables potentiellement utiles pour la classification. L\u0027objectif n\u0027est pas ici de proposer un nouveau formalisme expressif et généraliste de description de connaissance, mais simplement de préciser le cadre sur lequel s\u0027appuieront les algorithmes de construction de variables présentés dans la partie 4. Ce domaine de connaissance se décline en deux parties : description de la structure des données et choix des règles de construction de variables utilisables.\nStructure des données\nLa structure la plus simple est la structure tabulaire. Une instance est représentée par une liste de variables, chacune étant définie par son nom et son type. Les types usuels, numérique ou catégoriel, peuvent être étendus à d\u0027autres types spécialisés, comme par exemple les dates, les heures ou les textes. Les données réelles étant souvent issues de bases de données relationnelles, il est naturel d\u0027étendre cette structure au cas multi-tables. On propose ici de prendre en compte ces structures en s\u0027inspirant des langages informatiques structurés ou orientés objets. L\u0027objet d\u0027étude statistique (instance) appartient à une table principale. Un objet principal est alors défini par une liste de variables, dont les types peuvent être simple (numérique, caté-goriel...) comme dans le cas tabulaire, ou structuré : sous-objets (enregistrements d\u0027une table secondaire en relation 0-1) ou tableau de sous-objets (enregistrement d\u0027une table secondaire en relation 0-n). Dans le cas de la classification supervisée, la variable cible est une variable caté-gorielle de l\u0027objet principal. La figure 1 présente un exemple de l\u0027utilisation de ce formalisme.\nL\u0027objet principal est le client (Customer), avec des objets secondaires usages en relation 0-n. Les variables sont de type simples (Cat, Num ou Date) ou structurés (ObjectArray(Usage)). Les variables identifiant (ici préfixées par #) servent essentiellement à établir le lien avec une base relationnelle ; elles ne sont pas considérées comme des variables descriptives. -TableSelection(ObjectArray, Num)-\u003eObjectArray : sélection d\u0027objets du tableau selon une conjonction de critères d\u0027appartenance à des intervalles ou d\u0027égalité à des valeurs -TableCount(ObjectArray)-\u003eNum : effectif du tableau -TableMode(ObjectArray, Cat)-\u003eCat : valeur la plus fréquente -TableCountDistinct(ObjectArray, Cat)-\u003eNum : nombre de valeurs différentes -TableMean(ObjectArray, Num)-\u003eNum : valeur moyenne -TableMedian(ObjectArray, Num)-\u003eNum : valeur médiane -TableMin(ObjectArray, Num)-\u003eNum : valeur minimum -TableMax(ObjectArray, Num)-\u003eNum : valeur maximum -TableStdDev(ObjectArray, Num)-\u003eNum : écart type -TableSum(ObjectArray, Num)-\u003eNum : somme des valeurs En exploitant la structure des données présentée sur la figure 1 et les règles de construction précédentes (enrichies ici de la règle YearDay pour les dates), on peut par exemple construire les variables suivantes pour enrichir la description d\u0027un client :\n-MainProduct \u003d TableMode(Usages, Product), -LastUsageYearDay \u003d TableMax(Usages, YearDay(useDate)), -NbUsageProd1FirstQuarter \u003d TableCount(TableSelection(Usages, YearDay(useDate) ? [1 ;90] and Product \u003d \"Prod1\").\nEvaluation des variables construites\nIl s\u0027agit d\u0027exploiter les connaissances du domaine pour piloter efficacement la construction de variables potentiellement utiles pour la prédiction de la variable cible. Dans le formalisme introduit en partie 2, la structure des données peut avoir plusieurs niveaux de profondeur, voire posséder la structure d\u0027un graphe. Par exemple, une molécule est un graphe dont les noeuds sont les atomes et les arcs sont les liaisons entre atomes. Les règles de calcul peuvent être utilisées comme opérandes d\u0027autres règles, conduisant à des formules de calcul de longueur quelconque. On est alors confronté à un espace de variables constructibles en nombre potentiellement infini. Cela pose les deux problèmes principaux suivants :\n1. explosion combinatoire pour l\u0027exploration de cet espace, 2. risque de sur-apprentissage.\nOn propose une solution à ces problèmes en introduisant un critère d\u0027évaluation des variables selon une approche Bayesienne permettant de pénaliser les variables complexes. On introduit à cet effet une distribution a priori sur l\u0027espace de toutes les variables, et un algorithme efficace d\u0027échantillonnage de l\u0027espace des variables selon leur distribution a priori.\nEvaluation d\u0027une variable\nLa construction de variables vise à enrichir la table principale par de nouvelles variables qui seront prises en entrée d\u0027un classifieur. Les classifieurs usuels prenant en entrée uniquement des variables numériques ou catégorielles, on ne s\u0027intéresse qu\u0027à l\u0027évaluation de ces variables.\nPrétraitement supervisé. Le prétraitement supervisé MODL 1 consiste à partitionner une variable numérique en intervalles ou une variable catégorielle en groupes de valeurs, avec une estimation de densité conditionnelle constante par partie. Les paramètres d\u0027un modèle de pré-traitement sont le nombre de parties, la partition, et la distribution multinomiale des classes dans chaque partie. Dans l\u0027approche MODL, le prétraitement supervisé est formulé en un problème de sélection de modèles, traité selon une approche Bayesienne en exploitant un a priori hiérarchique sur les paramètres de modélisation. Le meilleur modèle est le modèle MAP (maximum a posteriori). En prenant le log négatif de ces probabilités, qui s\u0027interprète comme une longueur de codage (Shannon, 1948) dans l\u0027approche minimum description length (MDL) (Rissanen, 1978), cela revient à minimiser la longueur de codage d\u0027un modèle d\u0027évaluation M E (X) (via une partition supervisée) d\u0027une variable X plus la longueur de codage des données D Y de classe connaissant le modèle et les données descriptives\nOn a c(X) ? N ent(Y |X) où N est le nombre d\u0027individus de l\u0027échantillon d\u0027apprentissage et ent(Y |X) l\u0027entropie conditionnelle de la variable de classe connaissant la variable descriptive. Le critère 1 et son optimisation sont détaillés dans (Boullé, 2006) pour la discrétisation supervisée et dans (Boullé, 2005) pour le groupement de valeurs supervisé.\nModèle nul et filtrage des variables. Le modèle nul M E (?) correspond au cas particulier d\u0027une seule partie (intervalle ou groupe de valeurs) et donc de la modélisation directe des classes via une multinomiale, sans utiliser la variable descriptive. La valeur du critère c(?) revient au cout d\u0027encodage direct des classe cibles : c(?) ? N ent(Y ). Le critère d\u0027évalua-tion d\u0027une variable est alors utilisé dans une approche filtre de sélection de variables Guyon et al. (2006) : seules les variables dont l\u0027évaluation est meilleure que celle du modèle nul sont considérées comme informatives et retenues à l\u0027issue de la phase de préparation des données.\nPrise en compte de la construction des variables. Quand le nombre de variables natives ou construites devient important, le risque qu\u0027une variable soit considérée à tort comme informative devient critique. Afin de prévenir ce risque de sur-apprentissage, on propose dans cet article d\u0027exploiter l\u0027espace de construction de variables décrit dans la partie 2 en introduisant une distribution a priori sur l\u0027ensemble des modèles M C (X) de construction de variables. On obtient alors une régularisation Bayesienne pour l\u0027obtention des variables, permettant de pé-naliser a priori les variables les plus \"complexes\". Cela se traduit par un coût de construction L(M C (X)) supplémentaire dans le critère d\u0027évaluation des variables, qui devient celui de la formule 2.\nL(M C (X)) est le log negatif de la probabilité a priori (longueur de codage) d\u0027une variable X native ou construite, que nous définissons maintenant.\nDistribution a priori des variables\nUne variable à évaluer est une variable numérique ou catégorielle de la table principale, soit native, soit obtenue par application récursive des règles de construction de variables. L\u0027espace des variables ainsi défini étant potentiellement infini, définir une probabilité a priori sur cet espace pose de nombreux problèmes et implique de nombreux choix. Afin de guider ces choix, on propose de suivre les principes généraux suivants :\n1. la prise en compte de variables construites a un impact minimal sur les variables natives, 2. l\u0027a priori est le plus uniforme possible, pour minimiser les biais, 3. l\u0027a priori exploite au mieux les connaissances du domaine.\nCas des variables natives. Dans le cas où il n\u0027y a pas de variables constructibles, le problème se limite au choix d\u0027une variable à évaluer parmi K variables numériques ou catégorielles de la table principale. En utilisant un a priori uniforme pour ce choix, on a p(\nCas des variables construites. Dans le cas où des variables sont constructibles, il faut d\u0027abord choisir si l\u0027on utilise une variable native ou construite. Un a priori uniforme (p \u003d 1/2) sur ce choix implique un surcoût de log 2 pour les variables natives, ce qui contrevient au principe d\u0027impact minimal sur les variables natives. On propose alors de considérer le choix d\u0027une variable construite comme une possibilité de choix supplémentaire en plus des K variables natives. Le coût d\u0027une variable native devient alors L(M C (X)) \u003d log(K + 1), avec un impact minimal de log(1 + 1/K) ? 1/K par rapport au cas sans construction de variables.\nChoisir une variable construite repose alors sur la hiérarchie suivante de choix : -choix de construire une variable, -choix de la règle de construction parmi les R règles de construction applicables (de code retour numérique ou catégoriel), -pour chaque opérande de la règle, choix d\u0027une variable native ou construite, dont le type est compatible avec le type de l\u0027opérande attendu. En utilisant un a priori hiérarchique, uniforme à chaque niveau de la hiérarchie, le coût d\u0027une variable construite se décompose sur les opérandes de la règle R utilisée selon la formule récursive 3, où les variable X op sont les variables natives ou construites applicables en opérandes de la règle.\nop?R Le cas de la règle TableSelection qui extrait une sous-table d\u0027objets selon une conjonction de critères exploite de façon similaire une hiérarchie de choix : nombre de variables de sélec-tion, liste des variables de sélection et pour chaque variable de sélection, choix de l\u0027opérande de sélection (intervalle ou valeur), avec dans le cas des intervalles, choix de la granularité (nombre de partiles) et de l\u0027intervalle (index du partile).\nLa figure 2 donne un exemple d\u0027une telle distribution a priori sur l\u0027ensemble des variables constructibles à l\u0027aide des règles de construction TableMode, TableMin, TableMax et YearDay, pour le problème de gestion de la relation client présenté sur la figure 1. Par exemple, le coût de sélection de la variable native Age est L(M C (Age)) \u003d log 3. Celui de la variable construite T ableM in(U sages, Y earDay(Date)) exploite une hiérarchie de choix, conduisant à L(M C (T ableM in(U sages, Y earDay(Date)))) \u003d log 3+log 3+log 1+log 1+log 1.\nFIG. 2 -Distribution de probabilité a priori pour la construction de variables, pour un problème de gestion de la relation client\nL\u0027a apriori de construction de variables ainsi défini correspond à une hiérarchie de distributions multinomiales de profondeur potentiellement infinie (HDMPI). Les variables natives sont obtenues dès le premier niveau de la multinomiale, alors que les variables construites ont des probabilités a priori d\u0027autant plus faibles qu\u0027elles sont complexes et exploitent les parties profondes du prior HDMPI.\nConstruction d\u0027un échantillon de variables\nL\u0027objectif est ici de construire un nombre donné de variables pour créer une représentation des données potentiellement informative pour la classification supervisée. Nous proposons de construire un échantillon de variables par tirage selon leur distribution a priori. Nous présen-tons un premier algorithme \"naturel\" de construction d\u0027échantillon, et démontrons qu\u0027il n\u0027est ni efficace ni calculable. Nous proposons alors un second algorithme répondant au problème.\nTirages aléatoires successifs\nAlgorithm 1 Tirages aléatoires successifs Require: K {Nombre de tirages} Ensure: V \u003d {V }, |V| ? K {Echantillon des variables construites}\nTirer V selon le prior HDMPI 4:\nAjouter V dans V 5: end for L\u0027algorithme 1 consiste à tirer successivement K variables selon le prior HDMPI. Chaque tirage consiste à partir de la racine de l\u0027arbre du prior et à descendre dans la hiérarchie par des tirages multinomiaux successifs, jusqu\u0027à l\u0027obtention d\u0027une variable native ou construite, ce qui correspond à une feuille de l\u0027arbre du prior. Cet algorithme naturel ne peut être utilisé dans le cas général, car il n\u0027est ni efficace, ni calculable, ce que nous démontrons ci-dessous.\nL\u0027algorithme 1 n\u0027est pas efficace. Soit un domaine de connaissance comportant V variables natives évaluables dans la table principale et pas de variable constructible. Le prior HDMPI se réduit à une multionomiale avec V valeurs équidistribuées. Si l\u0027on effectue K tirages selon cette multinomiale, l\u0027espérance du nombre de variables différentes obtenues est V (1?e ?K/V ) (Efron et Tibshirani, 1993). Dans le cas ou K \u003d V , on retrouve la taille d\u0027un échantillon boostrap, à savoir 1 ? 1/e ? 63% des variables obtenues avec V tirages. Pour obtenir 99% des variables, il faut K ? 5V tirages, ce qui n\u0027est pas efficace. Si de plus, il y a des variables constructibles, la multinomiale à la racine du prior HDMPI comporte cette fois K + 1 choix équidistribués. Les tirages ne permettent de construire des variables que dans 1/(K + 1)% des cas. Il est à noter que ce problème d\u0027innefficacité se produit à tous les niveaux de profondeur du prior HDMPI pour le tirage des opérandes des règles en cours de construction.\nL\u0027algorithme 1 n\u0027est pas calculable. Soit un domaine de connaissance comportant une seule variable native numérique x et une seule règle de construction f (Num, Num)-\u003eNum. Les variables que l\u0027on peut construire sont x, f (x, , x), x))... Le nombre de Catalan C n \u003d (2n)! (n+1)!n! permet de dénombrer de telles expressions. C n correspond au nombre de façons différentes de placer des parenthèses autour de n+1 facteurs ou au nombre d\u0027arbres binaires à n + 1 feuilles. Chaque variable correspondant à un arbre binaire à n feuilles (nombre de x dans la formule) vient en C n?1 exemplaires différents, chacun avec un probabilité a priori de 2 ?(2n?1) selon le prior HDMPI. On peut alors calculer l\u0027espérance de la longueur s(V ) d\u0027une variable calculée (en nombre de feuilles dans son arbre binaire de calcul). En utilisant la formule de Stirling pour approximer les nombres de Catalan, on établit dans la formule 4 que cette espérance est infinie.\nCela signifie que si l\u0027on tire au hasard une variable selon le prior HDMPI, parmi l\u0027ensemble des expressions faisant intervenir f et x, l\u0027algorithme 1 mettra en moyenne un temps infini avant de produire une variable. L\u0027algorithme 1 n\u0027est donc pas calculable dans le cas général. Propager la construction de façon récursive en distribuant les K i tirages sur la multinomiale du sous-niveau 9:\nTirages aléatoires simultanés\nend if 10: end for Comme il n\u0027est pas possible de tirer les variables individuellement, nous proposons de créer un échantillon directement sur la base de plusieurs tirages simultanés. Dans le cas d\u0027une loi multinomiale simple ayant K événements de probabilités (p 1 , p 2 , . . . , p K ), la probabilité qu\u0027un échantillon de n tirages ait pour effectifs n 1 , n 2 , . . . , n K par évènement est :\nL\u0027échantillon le plus probable est obtenu en maximisant l\u0027expression 5, qui par maximum de vraisemblance correspond aux effectifs n k \u003d p k n. Dans le cas par exemple d\u0027une multinomiale équidistribuée avec p k \u003d 1/K et de n \u003d K tirages, on remarque que l\u0027expression 5 atteint son maximum pour n k \u003d 1 et que donc tous les évènements sont tirés, ce qui remédie au problème d\u0027efficacité décrit en partie 4.1. L\u0027algorithme 2 exploite ce tirage par maximum de vraisemblance récursivement. Les tirages sont répartis selon les variables natives ou construites à chaque niveau du prior HDMPI, ce qui fait que le nombre de tirages demandés diminue en descendant dans la hiérarchie du prior. En cas d\u0027égalité entre plusieurs choix (par exemple, 1 tirage pour K variables) le choix est fait au hasard, avec priorité aux variables natives en cas de choix entre variables natives ou construites. En répartissant les tirages par branche de la hié-rarchie du prior HDMPI, avec des effectifs décroissants selon la profondeur de la hiérarchie, l\u0027algorithme 2 est à la fois efficace et calculable. La taille de l\u0027échantillon obtenu étant potentiellement inférieure au nombre de tirages demandés, on réitère l\u0027appel à l\u0027algorithme 2 en doublant le nombre de tirage demandés à chaque appel, jusqu\u0027à obtenir le nombre de variables désiré ou qu\u0027aucune variable supplémentaire ne soit produite entre deux appels successifs.\nEvaluation\nNous évaluons la méthode proposée en nous focalisant sur les axes suivants : capacité à générer de grands nombres de variables sans problème d\u0027explosion combinatoire, résistance au sur-apprentissage et apport pour la prédiction.\nBases multi-tables de petite taille\nDans cette première évaluation, nous utilisons 20 bases issues de la communauté du data mining multi-tables, en ignorant les variables de la table principale et nous focalisant sur la construction de variables par propositionalisation au moyen des règles de construction présen-tées dans la partie 2.2. Afin d\u0027avoir un résultat de référence, on utilise une méthode apparentée à Relaggs (Krogel et Wrobel, 2001), basée sur l\u0027utilisation des mêmes règles de construction (celles de la partie 2.2, excepté TableSelection), et de l\u0027effectif par valeur pour chaque variable secondaire catégorielle. Suite à cette construction de variable, on exploite un prédicteur Bayesien naif avec sélection de variables et moyennage de modèles (SNB) (Boullé, 2007), qui est à la fois robuste et performant dans le cas de très grands nombres de variables.\nLes bases utilisées 2 sont des bases issues du traitement d\u0027image (bases Elephant, Fox, Tiger, et base Miml, avec les variables cibles Desert, Mountains, Sea, Sunset, Trees), de la chimie moléculaire (Diterpenses , Musk1, Musk2, et Mutagenesis avec trois représentations), de la médecine (Stulong, avec les variables cibles Cholrisk, Htrisk, Kourisk, Obezrisk et Rarisk), ainsi que la base TicTacToe (Asuncion et Newman, 2007) traitée en multi-tables avec les neuf cases du jeu en table secondaire. Ces bases sont de petite taille, avec de 100 à 2000 individus.\nEn utilisant l\u0027algorithme 2, nous contrôlons la taille de la représentation en générant 1, 10, 100, 1000, 10000 et 100000 variables par jeu de donnée dans les échantillons d\u0027apprentissage dans un processus en validation croisée stratifiée à 10 niveaux, ce qui représente au total environ 20 millions de variables construites.\nEvaluation de la performance. Dans une première analyse, nous collectons l\u0027AUC moyenne en test pour chaque nombre de variables générées. La méthode Relaggs, qui s\u0027appuie sur une construction de variables par application systématique de règles, ne permet pas de contrô-ler la combinatoire du nombre de variables produites, qui ici varie d\u0027une dizaine à environ 1400 variables selon la base. Elle reste néanmoins applicable dans le cas de petites bases et fournit ici une performance de base compétitive. Les résultats présentés sur la figure 3 montrent que la performance de notre approche croit systématiquement avec le nombre de variables construites, et atteint ou dépasse la performance de Relaggs sur 17 des 20 bases quand le nombre de variables construites est suffisant. Pour trois des bases (Fox, Musk1 et Musk2), les performance de notre approche sont nettement moins bonnes que celles de Relaggs. Ces trois bases sont soit très bruitées (Fox avec AUC\u003d0.7) soit très petites (Musk1 et Musk2 avec moins de 100 instances) et correspondent exactement aux trois bases avec la plus grande variance des résul-tats pour Relaggs (entre 10 et 15% d\u0027écart type de l\u0027AUC). Pour ces trois bases, le nombre d\u0027instances est insuffisant pour compenser le coût de régularisation (critère 2) : la plupart des variables construites sont alors éliminées, ce qui fait chuter la performance. \nFIG. 3 -AUC en test en fonction du nombre variables construites\nEvaluation de la robustesse. Dans une seconde analyse, l\u0027expérience est réalisée après ré-affectation aléatoire des classes pour chaque jeu de données, afin d\u0027évaluer la robustesse de l\u0027approche. Nous collectons le nombre de variables sélectionnées, avec (critère 2) ou sans (critère 1) prise en compte du coût de construction des variables. Sans régularisation de construction, en moyenne 0.2% des variables sont sélectionnées à tort, avec des pourcentages diminuant avec la taille de la base (de 0.8% pour les plus petites bases à 0.02% pour les plus grandes). Par exemple, pour la base Musk1 qui comprend 92 instances, plus de 700 variables parmi 100000 sont identifiées à tort comme informatives. Avec prise en compte de la régularisation de construction, la méthode est extrêmement robuste : les 20 millions de variables générées sont identifiées comme étant non informatives, sans aucune exception.\nBases de taille moyenne\nDans une seconde évaluation, nous utilisons trois bases de taille moyenne : Connect4, PokerHands et Digits (Asuncion et Newman, 2007). La base Connect4 correspond à l\u0027ensemble des positions légales après 8 coups du jeu \"Puissance 4\". Il s\u0027agit de prédire l\u0027issue de la partie (victoire, nul, ou défaite) pour deux joueurs jouant les coups parfaits (le jeu a été \"cassé\"). Le format tabulaire initial est représenté sous forme multi-tables, avec une table secondaire comportant les 6 * 7 \u003d 42 cellules du jeu avec trois variables secondaires X, Y et V al. 70% des 67557 instances sont utilisées en apprentissage et 30% en test. La base PokerHand correspond à une main de cinq cartes au poker, avec le rang et la couleur (variables numériques) de chaque carte. Il s\u0027agit de reconnaitre le type de main parmi 10 classes (rien, paire, double paire, brelan...). Le format tabulaire initial est représenté sous forme multi-tables, avec une table secondaire comportant les cinq cartes d\u0027une main. 25000 instances sont utilisées en apprentissage et 1000000 en test. La base Digits est une base d\u0027images décrites par 28 * 28 \u003d 784 pixels, avec pour objectif la reconnaissance d\u0027un chiffre manuscrit de 0 à 9. Le format tabulaire initial est représenté sous forme multi-tables, avec une table secondaire comportant les 784 pixels avec trois variables secondaires X, Y et GrayLevel. 60000 instances sont utilisées en apprentissage et 10000 en test.\nNous évaluons le taux de bonne prédiction en test, plus discriminant ici que l\u0027AUC. Les trois problèmes sont difficiles dans leur représentation initiale pour le prédicteur SNB, qui obtient des performances de 72.4%, 50.1% et 87.5%. La figure 4 montre que les performances de notre approche croissent avec le nombre de variables construites, et dépassent significativement celle du classifieur SNB pour la représentation initiale. \nConclusion\nNous avons proposé dans cet article un cadre visant à automatiser la construction de variables pour la classification supervisée. Sur la base d\u0027une description d\u0027un format multi-table des données et d\u0027un choix de règles de construction de variables, nous avons défini une distribution a priori sur l\u0027ensemble de toutes les variables constructibles au moyen de ce formalisme. Nous avons démontré qu\u0027effectuer des tirages successifs selon cette distribution a priori pose des problèmes critiques d\u0027efficacité et de calculabilité, puis proposé un algorithme efficace permettant d\u0027effectuer simultanément un grand nombre de tirages de façon à construire une représentation comportant le nombre de variables désiré. Les expérimentations montrent que l\u0027approche résout à la fois le problème de l\u0027explosion combinatoire qui intervient dans les approches de construction de variables par application systématique de règles, et le problème de sur-apprentissage quand les représentations comportent de très grands nombres de variables. Les performances obtenues en classification sont très prometteuses. Dans des travaux futurs, nous prévoyons d\u0027améliorer la prise en compte des connaissances du domaine, en premier lieu en étendant la liste des règles de construction disponibles avec des spécialisations potentielles par domaine applicatif. Une autre piste de travail consiste à rechercher les meilleures variables à construire, en échantillonnant leur distribution a posteriori plutôt que leur distribution a priori. \n"
  },
  {
    "id": "436",
    "text": "Introduction\nLe volume de données numériques actuellement disponible nécessitent de disposer de mé-thodes efficaces permettant de les structurer, résumer, comparer et regrouper. Dans tous ces cas, il est indispensable de disposer d\u0027une mesure de similarité permettant d\u0027évaluer la proximité entre les objets considérés. Les illustrations les plus récentes se situent dans le domaine de la bioinformatique pour l\u0027alignement des sous-séquences d\u0027ADN ou d\u0027acides aminés ((Sander et Schneider, 1991;Chothia et Gerstein, 1997)) ou dans la détection d\u0027intrusion dans les réseaux où les différentes séquences d\u0027accès sont analysées et comparées à une base de signatures de comportements malveillants. En ce qui concerne les données séquentielles, de nombreux travaux ( (Levenshtein, 1966;Herranz et al., 2011;Keogh, 2002;Wang et Lin, 2007)) se sont intéressés à des séquences simples, c\u0027est-à-dire une liste ordonnée d\u0027éléments atomiques. Or, dès lors que l\u0027on s\u0027intéresse à des séquences d\u0027objets plus complexes, le calcul de similarité se confronte à la nature même des objets comparés. Les trajectoires d\u0027objets mobiles, les informations topologiques en biologie moléculaire ( (Wodak et Janin, 2002)) sont des exemples de telles données. Pour illustration, supposons que nous souhaitons comparer les trois séquences complexes suivantes : S 1 \u003d b}{a, c}}, S 2 \u003d b}{a, c}} et S 3 \u003d d}{a, b}{c}{d}}. Le calcul classique de la plus longue sous-séquence commune entre S 1 et S 2 , noté LCS(S 1 , S 2 ), est la sous-séquence c}} de longueur 3. De même, LCS(S 1 , S 3 ) \u003d b}{c}} de longueur 3. La mesure de la plus longue sousséquence commune nous amène à conclure que la séquence S 1 peut être considérée équidis-tante des séquences S 2 et S 3 . Or, la séquences S 1 est quasi identique à la séquence S 2 (hormis l\u0027interversion des deux premiers ensembles). Il est donc important de comparer autrement l\u0027impact des sous-séquences pour réellement mesurer la similarité de séquences d\u0027objets complexes. Il suffit de comparer le nombre de sous-séquences communes qui est de 40 entre S 1 et S 2 alors qu\u0027il est de 14 entre S 1 et S 3 . Ce résultat reflète mieux la similarité entre S 1 et S 2 car il prend en compte les différentes structures et combinaisons présentes dans une séquence complexe. La problématique se confronte à la combinatoire associée, l\u0027efficacité computationelle et nous amène à poser les questions suivantes : (i) Etant donné une séquence complexe, comment compter sans énumérer le nombre de sous-séquences distinctes ? (ii) Pour un couple de séquences, comment efficacement compter le nombre de sous-séquences communes ? Dans ce contexte, notre contribution est double : un cadre théorique pour définir une mesure de similarité pour les séquences complexes basée sur le nombre de sous-séquences communes et un algorithme qui met en oeuvre de façon efficace la mesure de similarité proposée. Cette approche est basé sur la technique de la programmation dynamique afin de compter efficacement toutes les sous-séquences communes entre deux séquences.\nL\u0027article est organisé de la façon suivante. La section 2 présente les définitions préliminaires à notre proposition. Les sections 3 et 4 détaillent notre contribution, présentent de nouveaux résultats combinatoires et discutent la complexité ainsi que la complétude de notre algorithme. La section 5 présente deux études expérimentales. La section 6 fait le bilan et dresse les perspectives associées à ce travail.\nConcepts préliminaires\nDéfinition 1 (Séquence) Soit I un ensemble fini d\u0027 items. Un ensemble (ou itemset) X est un sous-ensemble non vide de I. Une séquence S est une liste ordonnée 1 · · · X n telle que\nDéfinition 2 La similarité entre deux séquences S et T , notée sim(S , T ), est définie comme le nombre de toutes les sous-séquences communes entre S et T divisé par le nombre maximal de sous-séquences de S ou T : sim(S, T ) \u003d\nDe manière usuelle, l\u0027ensemble des parties d\u0027un ensemble X est noté P(X) et P ?1 (X) est l\u0027ensemble des parties de X sans l\u0027ensemble vide (i.e., P ?1 (X) \u003d P(X) \\ {?}).  \nCette notion d\u0027ensemble de positions critiques est cruciale dans notre approche puisqu\u0027elle permet de focaliser les calculs uniquement sur les dernières positions où une répétition apparaît pour une séquence S donnée. Le lemme suivant formalise cette intuition.\nPreuve 1 Voir le rapport technique Egho et al. (2012).\nRemarque. Les éléments de l\u0027ensemble ?(S ) • P ?1 (S[ ? Y ) ne sont pas néces-sairement disjoints. Pour s\u0027en convaincre, considérerons la séquence S \u003d b}{b, c}} et Y \u003d {a, b, c}, alors L(S, Y ) \u003d {1, 2}, et la séquence est construite deux fois dans les ensembles ?(\n. Afin de prendre en compte ce chevauchement d\u0027éléments, nous nous appuyons pour le calcul de R(S, Y ) sur le principe d\u0027inclusion-exclusion.\nPreuve 2 Voir le rapport technique Egho et al. (2012).\nExemple 2 Nous illustrons le processus complet de comptage ?( b, d}}).\nComptage efficace de toutes les sous-séquences communes\nDans cette section, nous allons étendre les résultats théoriques précédemment énoncés afin de compter toutes les sous-séquences distinctes communes entre deux séquences S et T . Comme pour la Section 3, nous présentons dans un premier temps, l\u0027idée générale avant d\u0027énoncer les résultats formels ainsi que l\u0027algorithme de comptage. Supposons que nous concaténons la séquence S avec un itemset Y et observons la variation des ensembles ?(S, T ) et ?(S • Y, T ). Deux cas sont possibles : Si aucun item dans Y n\u0027apparaît dans les itemsets des séquences S et T , alors la concaténation de l\u0027itemset Y avec la séquence S n\u0027a aucun effet sur l\u0027ensemble ?(S • Y, T ) (i.e., le nombre de sous-séquences communes ne change pas, ?(S • Y, T ) \u003d ?(S, T )). Ou si au moins un élément de Y apparaît dans l\u0027une des séquences S ou T (ou les deux), alors de nouvelles séquences communes apparaissent dans ?(S • Y, T ). De la même manière que pour la méthode de comptage des sous-séquences distinctes d\u0027une séquence unique, des répétitions peuvent se produire et il est nécessaire de définir un terme de correction. De manière formelle,\net A(S, T, Y ) représente le nombre de sous-séquences communes supplémentaires qui devraient être ajoutées au compte après la concaténation de Y et R(S, T, Y ) le terme de correction.\nDe même que pour le problème des sous-séquences distinctes pour une seule séquence, l\u0027ensemble des positions critique joue un rôle dans le calcul de A(S, T, Y ) et R(S, T, Y ). Le lemme suivant formalise cette observation :\nPreuve 3 Voir le rapport technique Egho et al. (2012).\nComme pour le Lemme 1, le calcul de\nPreuve 4 Voir le rapport technique Egho et al. (2012).\nLe théorème 2 permet de concevoir un algorithme simple qui s\u0027appuie sur la technique de programmation dynamique. Pour deux séquences données S et T , de tailles n et m respectivement, l\u0027algorithme produit une n × m-matrice, notée M , tel que la valeur de la cellule M i,j correspond au nombre de sous-séquences communes entre S i et T j (i.e., M i,j \u003d ?(S i , T j )). Considérons les deux séquences S 1 et S 2 dans D ex , alors ?(S 1 , S 2 ) \u003d 21. Le {?} {a} {b, c, d} {a, d} \nExpériences\nNous invitons le lecteur à lire la section des expérimentations dans le rapport technique Egho et al. (2012). où nous étudions le passage à l\u0027échelle de notre mesure ainsi que son application dans le domaine de regroupement de séquences de protéines dans le domaine biologique.\n"
  },
  {
    "id": "437",
    "text": "Introduction\nTraiter des données de plus en plus volumineuses fait partie des défis que se donne la communauté des chercheurs en fouille de données. Une des manières de répondre à ce défi est de définir des méthodes de fouille de données mettant en oeuvre le parallélisme, sur une ou plusieurs machines, et en utilisant le processeur (CPU) et/ou le calculateur graphique (GPU). Un certain nombre d\u0027algorithmes ont déjà donné lieu à parallélisation sur GPU à la fois en fouille de données (voir un aperçu dans (Jian et al., 2011)), en visualisation scientifique (Weiskopf, 2006) ou en visualisation d\u0027informations et plus spécifiquement en affichage de graphes (Frishman et Tal, 2007).\nDans le domaine de la fouille visuelle de données, les approches pouvant afficher le plus de données relèvent soit des approches orientées pixels et assimilées (voir (Keim et al., 1995) où 530.000 valeurs sont visualisées), soit des approches utilisant des densités (voir (Fua et al., 1999) où les coordonnées parallèles peuvent afficher jusqu\u0027à 200.000 données). Cependant on peut constater que les interactions sont le point d\u0027achoppement de ces méthodes visuelles pour le traitement de grands volumes de données (voir discussion dans (Florek, 2006)). Lorsque l\u0027utilisateur formule une requête graphique, le système doit bien souvent faire des calculs supplémentaires et réafficher les données, partiellement ou entièrement, ce qui peut ralentir l\u0027utilisateur dans son processus d\u0027exploration (et rendre la méthode inutilisable).\nDes approches de visualisation ont été étudiées pour remédier à ces inconvénients, comme pour le MDS (Ingram et al., 2009) en abaissant à la fois la complexité algorithmique et en parallélisant la méthode sur GPU. Cependant cette complexité est encore importante si l\u0027on veut traiter des millions de données et il s\u0027agit d\u0027une approche peu interactive (sans redéfini-tion de l\u0027espace de visualisation). Dans (Florek, 2006), l\u0027objectif a été cette fois d\u0027obtenir des temps d\u0027affichage permettant les interactions. Le facteur d\u0027accélération entre l\u0027implémentation sur CPU (non parallélisée) et l\u0027implémentation sur GPU va jusqu\u0027à 60 (150.000 données en dimension 4 sont affichées en 0.3s sur GPU au lieu de 20s sur CPU). \nFIG. 1 -\nTAB. 1 -Complexité et parallélisation choisie pour chacune des étapes de notre méthode en fonction du nombre de données n, de la dimension des données m, du nombre de POIs k et du nombre d\u0027interactions réalisées I au cours d\u0027une session utilisateur.\nNotre objectif consiste donc à proposer une approche de fouille visuelle de données qui 1) puisse visualiser des volumes de données multidimensionnelles dépassant les limites actuelles des approches existantes, 2) permette à l\u0027utilisateur d\u0027avoir des interactions graphiques les plus rapides possible. Ces objectifs passent par le choix d\u0027une méthode visuelle ayant une complexité faible et pouvant se paralléliser sur CPU et GPU. Nous avons donc choisi comme visualisation candidate une méthode répondant le mieux possible à ces critères (Da Costa et Venturini, 2006). Elle fait partie des méthodes radiales comme RadViz (Hoffman et al., 1999). Ces approches considèrent que des points d\u0027intérêt (POIs, appelés aussi \"ancres dimensionnelles\"), sont disposés sur un cercle par exemple, et que les données viennent se positionner à l\u0027intérieur du cercle en fonction de leur ressemblance avec les POIs (voir figure 1). Ainsi, les POIs peuvent représenter des cas particuliers importants parmi les données, ou des hypothèses à tester. Chaque choix ou configuration des POIs vient donner une disposition particulière des données. L\u0027utilisateur dispose de plusieurs interactions afin de modifier cette disposition et faire apparaître de nouvelles informations.\n2 Parallélisation d\u0027une méthode radiale : entre CPU et GPU Si l\u0027on note d 1 , ..., d n les n données multidimensionnelles décrites selon m attributs, et P OI 1 , ..., P OI k les k points d\u0027intérêt choisis, une session utilisateur se déroule avec notre visualisation selon 5 étapes décrites dans la première colonne de la table 1. La parallélisation des différentes étapes de cet algorithme est résumée dans ce même tableau 1. La lecture des données (étape E1), effectuée une seule fois, est traitée par le CPU avec plusieurs tâches de lecture en parallèle.\nLe choix des POIs (étape E2) est effectué de manière automatique pour la première visualisation, ensuite c\u0027est l\u0027utilisateur qui ajuste les points d\u0027intérêt grâce aux interactions. La méthode automatique est appelée une seule fois, et son exécution reste peu coûteuse car elle porte sur un échantillon de données de taille fixe. Cet algorithme est donc exécuté sur le CPU, sans parallélisation particulière.\nLe calcul des similarités entre les données et les POIs (étape E3) est l\u0027opération la plus coû-teuse puisque l\u0027on doit calculer ?(n * k) distances en dimension m. Cette multitude de calculs numériques se prête bien à une parallélisation sur le GPU. La tâche générique traitée par un coeur du GPU porte sur plusieurs données, définies comme un sous-ensemble de {d 1 , ..., d n }, dont on va calculer la similarité avec les POIs ainsi que les coordonnées d\u0027affichage. Chaque paquet est traité par une tâche concurrente. En CUDA, les tâches sont regroupées en plusieurs blocs qui forment une grille. Le résultat du traitement d\u0027un bloc est stocké temporairement dans la mémoire du GPU. Une fois tous les blocs traités, le résultat final (coordonnées d\u0027affichage des données) est recopié dans la mémoire principale et peut ensuite être utilisé par les autres étapes de notre méthode sur le CPU. L\u0027utilisation des différents types de mémoire est un point clé d\u0027un programme CUDA, car ces mémoires ont des rôles et des performances très différents. Principalement, notre programme a été optimisé afin de ne lire qu\u0027une seule fois les données, et nous avons choisi de stocker les informations nécessaires aux calculs (POIs, leurs coordonnées, etc) dans la mémoire constante du GPU qui est très rapide (mais limitée à 64ko).\nDans l\u0027objectif d\u0027effectuer des comparaisons, cette étape E3 a également été parallélisée sur le CPU. Pour cela nous avons utilisé le même principe global : les données sont découpées en paquet, et chaque paquet est traité par des tâches concurrentes sur le CPU (par exemple, de 1 à 12 tâches).\nL\u0027affichage des données (étape E4) a un coût linéaire en n, et dans notre implémentation actuelle, nous avons réalisé cette étape de manière concurrente sur le CPU. En ce qui concerne les interactions, un pré-traitement a souvent lieu avant de renvoyer l\u0027exécution vers une des étapes précédentes de l\u0027algorithme (E3 ou E4). Le seul pré-traitement ayant un coût de calcul est lié à la sélection, puisqu\u0027il faut détecter quelles données appartiennent au cadre dessiné par l\u0027utilisateur. Ce calcul est principalement proportionnel au nombre de pixels sélectionnés, et reste dans un temps acceptable sur le CPU.\nRésultats\nBases, matériel, paramétrage\nLes bases utilisées pour nos tests sont représentées dans la table 2. Nous avons conçu un générateur de données qui génère n données en dimensions m selon 5 classes de même effectif. De cette manière, nous avons pu tester l\u0027évolution de la complexité des opérations en fonction de n, m et k. Le matériel utilisé dans ces tests est courant : un portable Asus avec un i7 cadencé à 2,2GHz, un GPU Nvidia GTX560M (192 coeurs). Les résultats présentés dans la suite ont été calculés en moyenne sur 10 essais. Les temps sont exprimés en ms. Pour les étapes utilisant le CPU (E1 et E4, mais aussi une version CPU de l\u0027étape E3 utilisée dans nos comparaisons CPU-GPU), nous avons déterminé le meilleur nombre de tâches concurrentes à utiliser en utlisant de nombreux jeux de données (y compris ceux de la table 2) et en faisant croître le nombre de tâches concurrentes de 1 à 12. Nous avons mesuré le temps mis par le CPU pour accomplir les travaux demandés. Par manque de place, nous ne donnons que les conclusions obtenues : à l\u0027issue de ces tests, nous avons conclu que les meilleurs paramètres pour les étapes ayant lieu sur CPU sont 4 tâches pour E1, 8 pour E3 et 4 pour E4 (paramètres utilisés dans le reste de l\u0027article).\nPour le GPU, le paramétrage consiste à déterminer deux valeurs : le nombre de blocs B et le nombre de tâches par bloc T . L\u0027efficacité de ce découpage dépend du matériel. Nous avons sélectionné plusieurs bases de données et nous avons fait varier ces deux paramètres. Dans la suite, nous avons sélectionné les valeurs B \u003d 256 et T \u003d 128 pour les résultats de l\u0027approche GPU.\nPerformances de la méthode et apports du parallélisme\nNous avons mesuré les temps mis par notre méthode et ses variantes pour traiter, étape par étape, différents jeux de données (voir tableau 3). Les deux premières colonnes du tableau montrent que le temps de lecture des fichiers au format CSV est globalement divisé par 2 grâce à la parallélisation. La colonne E2-CPU1 donne les temps nécessaires pour le choix initial des POIs. Ici il s\u0027agit d\u0027un algorithme heuristique qui échantillonne des points au hasard et choisit le groupe de points qui maximise une mesure donnée. Le nombre d\u0027itérations de cet algorithme est suffisant, d\u0027après nos expériences précédentes, pour obtenir un résultat satisfaisant. Cette étape E2 s\u0027exécute donc en un temps négligeable par rapport aux autres étapes. C\u0027est la raison pour laquelle nous n\u0027avons pas réalisé sa parallélisation (qui serait possible cependant).\nLes colonnes E3-CPU1, E3-CPU8 et E3-GPU permettent de comparer entre elles des approches utilisant un parallélisme d\u0027intensité croissante pour le calcul des similarités. Les résul- Les deux dernières colonnes (E4-CPU1 et E4-CPU4) permettent de quantifier l\u0027efficacité de la parallélisation de l\u0027affichage sur CPU. La différence entre les colonnes permet de gagner quelques secondes sur les plus grosses bases, cependant, elle n\u0027est pas aussi importante que prévue et doit pouvoir être améliorée.\nConclusions\nNous avons présenté dans cet article une parallélisation d\u0027une méthode visuelle radiale afin de rendre possible le traitement de millions de données, notamment du point de vue de la vitesse d\u0027affichage et du temps nécessaire pour les interactions. Cette parallélisation s\u0027est appuyée à la fois sur le CPU et le GPU, en fonction des complexités a priori des opérations traitées.\nLe principal résultat de l\u0027article est de montrer que cette approche permet des affichages et des interactions dont les temps vont, pour des millions de données, de 0,2 à 10 secondes, tout en utilisant du matériel classique. Ce résultat montre que les approches radiales repré-sentent une catégorie de méthodes se prêtant bien au passage à l\u0027échelle à condition d\u0027utiliser des algorithmes parallèles. Nos résultats permettent aussi d\u0027augmenter de manière significa-\n"
  }
]