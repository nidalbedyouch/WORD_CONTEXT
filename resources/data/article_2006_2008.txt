Introduction
Les algorithmes de Séparateurs à Vaste Marge proposés par (Vapnik, 1995) et les méthodes de noyaux permettent de construire des modèles précis et deviennent des outils de classification de données de plus en plus populaires. On peut trouver de nombreuses applications des SVM (réf. http://www.clopinet.com/isabelle/Projects/SVM/applist.html) comme la reconnaissance de visages, la catégorisation de textes ou la bioinformatique. Cependant, les SVM demandent la résolution d'un programme quadratique dont le coût de calcul est au moins d'une complexité égale au carré du nombre d'individus de l'ensemble d'apprentissage et la quantité de mémoire nécessaire les rend impossible à utiliser sur de grands ensembles de données à l'heure actuelle (Lyman et al., 2003). Il y a besoin de permettre le passage à l'échelle des SVM pour traiter de grands ensembles de données sur des machines standard. Une heuristique possible pour améliorer l'apprentissage des SVM est de décomposer le programme quadratique en une série de plus petits problèmes (Boser et al, 1992), (Chang et al, 2003), (Osuna et al, 1997), (Platt, 1999). Au niveau de la mise en oeuvre, les méthodes d'apprentissage incrémental (Cauwenberghs et al, 2001), (Do et Poulet, 2006), (Do et Poulet, 2003), (Fung et Mangasarian, 2002), , (Syed et al, 1999) permettent de traiter de grands ensembles de données par mise à jour des solutions partielles en chargeant consécutivement les sous-ensembles d'apprentissage en mémoire sans avoir à charger l'ensemble total. Les algorithmes parallèles et distribués (Do et Poulet, 2006), ) utilisent des machines en réseaux pour améliorer le temps d'exécution de l'apprentissage. Les algorithmes d'apprentissage actif (Do et Poulet, 2005), (Tong et Koller, 2000) choisissent un sous-ensemble d'individus (ensemble actif) représentatif pour la construction du modèle.
Dans cet article, nous continuons à développer nos algorithmes de boosting de PSVM et LS-SVM   (Do et Poulet, 2007) et (Do & Fekete, 2007) pour classifier simultanément un grand nombre d'individus et de dimensions. Nous présentons une classe d'algorithmes de boosting se basant sur les NSVM (Mangasarian, 2001), PSVM (Fung et Mangasarian, 2001) et LS-SVM (Suykens et Vandewalle, 1999)   (Golub et Van Loan, 1996) pour traiter des ensembles de données ayant un très nombre de dimensions. Enfin nous avons construit les algorithmes de boosting, Adaboost (Freund et Schapire, 1995), Arc-x4 (Breiman, 1998) de NSVM, PSVM et LS-SVM pour la classification d'ensembles de données ayant simultanément un grand nombre d'individus et de dimensions. Les performances des algorithmes sont évaluées sur de grands ensembles de l'UCI (Blake et Merz, 1998), comme Adult, KDDCup 1999, Forest Covertype, Reuters-21578 et de RCV1-binary. Les résultats sont comparés avec ceux obtenus avec LibSVM (Chang et Lin, 2003), Perf-SVM (Joachims, 2006) et CB-SVM (Yu et al., 2003).
Le paragraphe 2 présente brièvement les algorithmes de NSVM, PSVM et LS-SVM, le paragraphe 3 décrit les extensions pour la classification d'ensembles de données ayant un grand nombre de dimensions. Dans le paragraphe 4 nous présentons les algorithmes de boosting pour le traitement de très grands ensembles de données (simultanément en nombre d'individus et de dimensions) puis quelques résultats dans le paragraphe 5 avant la conclusion et les travaux futurs.
Quelques notations sont utilisées dans cet article. Tous les vecteurs sont représentés par des matrices colonne. Le produit scalaire de deux vecteurs x et y est noté x.y. La norme d'un vecteur v est ||v||. La matrice inverse de X est notée par X -1 . e est un vecteur colonne de 1. I représente la matrice identité. (1) D(Aw -eb) + z ? e z ? 0 où une constante c > 0 est utilisée pour contrôler la marge et les erreurs.
Algorithmes de NSVM, PSVM et LS-SVM
Le plan optimal (w, b) est obtenu par la résolution du programme quadratique (1) dont la mise en oeuvre est coûteuse en temps et mémoire vive.
Algorithme de NSVM
L'algorithme de generalized SVM (GSVM) (Mangasarian, 1998)  T et E = (A -e), on peut réécrire le problème (2) en : (Mangasarian, 2001) a proposé d'utiliser la méthode itérative de Newton pour résoudre efficacement le problème d'optimisation (3). Le principe de la méthode de Newton est de minimiser successivement les approximations au second ordre de la fonction objectif ? en se basant sur un développement de Taylor au second ordre au voisinage de X v . Le problème 
Algorithme de PSVM
L'algorithme de proximal SVM (PSVM) (Fung et Mangasarian, 2001) modifie aussi l'algorithme de SVM (1) en :
-maximisant la marge par (1/2) ||w, b|| 2 -minimisant les erreurs par (c/2) ||z|| 2 sous la contrainte :
En substituant z dans la fonction objectif ? du programme quadratique (1), nous obtenons alors (7) :
Pour ce problème d'optimisation (7), nous calculons les dérivées partielles en w et b. Cela nous donne le système linéaire de (n+1) inconnues (w 1 , w 2 , …, w n , b) suivant :
L'algorithme de PSVM ne demande que la résolution d'un système linéaire (8) à n+1 inconnues au lieu du programme quadratique (1), donc il est capable de traiter un très grand nombre d'individus en un temps restreint sur une machine standard. Par exemple, la classification d'un million de points en dimension 20 est effectuée en 13 secondes sur un PC (P4, 2,4GHz, 512Mo RAM).
Algorithme de LS-SVM
L'algorithme de LS-SVM proposé par (Suykens et Vandewalle, 1999)  En substituant z dans la fonction objectif ? du programme quadratique (1), nous obtenons alors (9) :
Pour ce problème d'optimisation (9), nous calculons également les dérivées partielles en w et b. Cela nous donne le système linéaire de (n+1) inconnues (w 1 , w 2 , …, w n , b) suivant :
L'algorithme de LS-SVM a la même complexité que celle du PSVM, il ne demande que la résolution d'un système linéaire (10) à n+1 inconnues au lieu du programme quadratique (1), donc il est capable de traiter un très grand nombre d'individus en un temps restreint sur une machine standard.
Extensions pour un grand nombre de dimensions
Certaines applications comme la bioinformatique ou la fouille de textes nécessitent de traiter des données ayant un nombre très important de dimensions et un nombre d'individus plus réduit. Dans ce cas la matrice de taille (n+1)x(n+1) est trop importante et la résolution du système à (n+1) inconnues nécessite un temps de calcul élevé. Pour adapter l'algorithme à ce type de données nous avons appliqué le théorème de Sherman-Morrison-Woodbury (11) aux systèmes d'équations des algorithmes de NSVM, PSVM et LS-SVM.
(
Pour l'algorithme de NSVM, on note :
l'inversion de la matrice ?''(X i ) dans la formule (6) est re-écrite par (12) :
Ensuite nous appliquons la formule de Sherman-Morrison-Woodbury (11) dans la partie droite de la formule (12), nous obtenons (13) :
L'inversion de la matrice (I + P T P) de taille (n+1)x(n+1) est ramenée à inverser la matrice (I + PP T ) (de taille mxm). Cette nouvelle formulation permet à l'algorithme de NSVM de traiter facilement des ensembles de données ayant un nombre de dimensions très important.  Entrée : 
RNTI -X -
TAB. 2 -Algorithme Adaboost de NSVM, PSVM et LS-SVM
Remarquons que l'utilisation des algorithmes de NSVM, PSVM et LS-SVM dans l'algorithme Adaboost est très intéressante parce que ces algorithmes traitent beaucoup plus rapidement les sous-ensembles (échantillons) de données qu'avec la résolution d'un programme quadratique d'un SVM standard.
RNTI -X -
Nous proposons également d'utiliser l'algorithme Arc-x4 (Breiman, 1998) avec les algorithmes de NSVM, PSVM et LS-SVM de manière similaire à l'algorithme Adaboost. Le principe de l'algorithme Arc-x4 est semblable à Adaboost. Il utilise le mécanisme de vote majoritaire au lieu d'un vote avec des poids et il a également une manière différente pour prendre un échantillon en se concentrant sur les erreurs de toutes les étapes précédentes. L'algorithme Arc-x4 de NSVM, PSVM et LS-SVM donne de très bonnes performances en classification de grands ensembles de données.
Quelques résultats
Nous avons développé le programme en C/C++ sous Linux en utilisant la librairie Lapack++ (réf. http://math.nist.gov/lapack++/) pour bénéficier de bonnes performances en calcul matriciel. Nous avons aussi développé des algorithmes spécifiques pour les matrices creuses. Nous allons en présenter une évaluation en prenant en compte le taux de précision et le temps d'apprentissage. Nous avons sélectionné 4 grands ensembles de données de l'UCI (Blake et Merz, 1998)  Nous avons utilisé les algorithmes de boosting de NSVM (AdaNSVM, Arcx4NSVM), PSVM (AdaPSVM, Arcx4PSVM) et LS-SVM (AdaLSSVM, Arcx4LSSVM) pour effectuer la classification sur un PC (Pentium IV, 2,4GHz et 1024Mo RAM). Les résultats sont comparés avec ceux obtenus avec l'algorithme SVM standard LibSVM (Chang et Lin, 2003) et deux nouvelles versions de SVM, Perf-SVM (Joachims, 2006) et CB-SVM (Yu et al., 2003). Nous mettons dans les tableaux les meilleurs résultats en caractères gras et les deuxièmes en souligné. Nous ne faisons que comparer les premiers résultats expérimentaux des algorithmes de boosting de NSVM, LS-SVM avec LibSVM parce que l'algorithme de PSVM est très semblable à l'algorithme de LS-SVM.
Les ensembles de données Reuters-21578 et RCV1-binary sont utilisés pour évaluer les performances en classification d'ensembles ayant simultanément un grand nombre d'individus et dimensions. Nous avons utilisé Bow (réf. http://www.cs.cmu.edu-/~mccallum/bow) en prétraitement des données Reuters-21578. Chaque document est vu comme un vecteur de mots, nous avons obtenu 29406 mots (dimensions) sans sélection de dimensions. Nous avons effectué la classification des 10 classes les plus nombreuses. Cet ensemble de données ayant plus de deux classes nous avons utilisé l'approche "un contre le reste". Les résultats sont présentés dans le tableau 4 avec la moyenne de la précision et du rappel (breakeven point) pour les 10 plus grandes catégories.
On remarque que nos algorithmes donnent toujours des résultats un peu meilleurs en ce qui concerne le taux de précision (jusqu'à 5,32% d'amélioration) au détriment, dans certains cas, de la rapidité de calcul.
Précision (  Reuters-21578 Pour l'ensemble de données RCV-1, nous avons utilisé le même prétraitement que (Chang et Lin, 2003), CCAT et ECAT formant la classe positive et GCAT et MCAT la négative (en enlevant les instances apparaissant simultanément dans les classes positive et négative). Les résultats montrent que nos algorithmes permettent une petite amélioration de la précision tout en étant très significativement plus rapide (jusqu'à 19 fois plus rapide). Ce facteur est encore plus significatif sur l'ensemble de données Adult (200 fois plus rapide pour une précision équivalente).
Pour l'ensemble de données Forest Cover Type, nos algorithmes ont effectué la classification des deux classes les plus nombreuses en moins de 30 secondes, LibSVM n'a donné aucun résultat après 21 jours de calcul. Cependant des travaux récents ont montré que l'algorithme SVM-Perf (Joachims, 2006) effectue la classification de cet ensemble de données en 171 secondes sur un Intel Xeon, 3,6GHz, 2Go RAM. En tenant compte des différences de rapidité des processeurs, nous pouvons estimer raisonnablement que nos algorithmes sont 8 fois plus rapides.
RNTI -X -
Enfin pour l'ensemble de données KDD-Cup'99, CB-SVM (Yu et al, 2003) a effectué la classification en 4750 secondes sur un P-III, 800MHz, 1Go RAM, avec 90% de précision. Nos algorithmes obtiennent 92% de précision en seulement 180 secondes soit à peu près 10 fois plus rapidement que CB-SVM.  Une première extension de ces travaux va consister à étendre ces algorithmes pour en faire des versions parallèles et distribuées sur un ensemble de machines. Cette extension permettra d'améliorer le temps de la tâche d'apprentissage. Une seconde sera de proposer une nouvelle approche pour la classification non linéaire.

Introduction
Les avancées technologiques en matière de haut débit favorisent l'apparition de nouveaux services de vente ou location en ligne de fichiers vidéos et musicaux. De tels services se veulent pro-actifs et proposent, en plus des actes promotionnels classiques, des choix personnalisés de films (ou de musique). Des méthodes de recommandation sont déjà utilisées sur certains sites Internet de vente par correspondance (Amazon, Fnac, Virgin, etc.) ou encore sur les platesformes musicales (Lastfm, Radioblog, Pandora, etc.). Candillier et al. (2007) fait un panorama des techniques de recommandation : qu'elles soient basées sur des notations d'internautes ou des descriptions de contenus (techniques user-and item-based utilisant le filtrage collaboratif) ou des rapprochements thématiques de profils d'internautes et de descriptions de contenus (filtrage de contenus), voire des techniques hybrides combinant les différentes approches, la problématique reste de gérer les matrices creuses. En effet, devant la variété d'un catalogue et le grand nombre d'utilisateurs, le faible nombre de notes qu'un utilisateur donne rend la comparaison avec d'autres utilisateurs risquée. Parmi les verrous bien connus du domaine de la recommandation figure la difficulté à recueillir des descriptions aussi bien des goûts des utilisateurs (notes, intérêts, usage) que des contenus (métadonnées). Ce problème est pourtant crucial au lancement d'un service (bootstrapping), et se repose pour tout nouvel utilisateur, car c'est un challenge d'attirer et fidéliser les clients dès la prise en main du service.
Afin de pallier ces problèmes, une nouvelle voie de recherche est ouverte : l'exploitation de la richesse de l'Internet ouvert au service d'un site web fermé. Internet est devenu plus que jamais une source de connaissances. Avec le web 2.0 et l'apparition de sites communautaires, les internautes partagent de plus en plus leurs photos, leurs signets, leurs nouvelles, leurs arnaques, leurs opinions. Considérer le web comme un catalogue permet de connaître les goûts d'un nombre considérable d'individus, et nous permet d'envisager à plus ou moins long terme, de décrire des profils type de fans, une typologie des films, ou encore de découvrir de nouveaux descripteurs de films décisifs dans le choix de films. L'objectif de l'étude exploratoire décrite dans ce papier est de comprendre ce que l'on peut dégager des commentaires de films publiés dans des sites communautaires 1 . Nous chercherons à identifier quel est le vocabulaire révélateur d'opinions, mais également découvrir si un vocabulaire particulier permet de regrouper des films. Nous montrerons que certains mots sont caractéristiques d'un groupe de films précis, tandis que d'autres mots sont beaucoup mieux répartis dans les commentaires de l'ensemble du corpus textuels.
Travaux connexes
De nombreux systèmes de mesure d'opinions dans les contenus textuels ont vu le jour ré-cemment. Deux grandes familles de méthodes existent, les méthodes basées sur des techniques de traitement automatique de la langue naturelle (TALN) et celles basées sur l'apprentissage, ces deux types de méthodes pouvant également être combinées. Liu et al. (2005) décrivent le système Opinion Observer qui analyse finement les commentaires d'usagers dans le but de produire automatiquement des comparatifs de produits commerciaux. Ils ont conçu une méthode de découverte de motifs linguistiques qui permet de trouver le vocabulaire décrivant des critères (comme la qualité des images pour un appareil photo) puis d'en calculer l'orientation. En comptabilisant les scores positifs et négatifs de chaque critère, pour chaque produit, le système produit un rapport détaillé comparant l'opinion générale qui se dégage sur ces produits et facilite ainsi l'achat d'un appareil photo parmi les modèles décrits par les usagers eux-mêmes. Opinion Observer est un exemple de système complet basé sur l'identification de mots "porteurs d'opinion" dans les phrases, suivie par un décompte de ces mots. Dave et al. (2003) présentent une méthode plus simple, sans détection de motifs, mais qui, à partir d'un dictionnaire, et selon une échelle d'intensité des mots d'opinions, attribue un score positif ou négatif à chacune des phrases. Grâce aux scores calculés, le système classifie automatiquement des commentaires textuels en 2 classes : positive et négative.
Méthodes linguistiques d'analyse d'opinions
Comme beaucoup d'autres systèmes (Morinaga et al., 2002;Turney, 2002;Wilson et al., 2004;Nasukawa et Yi, 2003), ils ont besoin de mots décrivant des opinions. Ce lexique peut être totalement construit à la main ; cependant devant le coût d'un tel procédé, beaucoup de systèmes décrivent des méthodes d'enrichissement plus ou moins automatiques d'une mouture minimale créée manuellement. La constitution de lexique peut se faire grâce à des techniques d'apprentissage. Par exemple, Hatzivassiloglou et McKeown (1997)  ou Turney et Littman (2004) utilisent un algorithme non supervisé pour associer de nouveaux mots à des mots pré-sélectionnés. Pereira et al. (1994) et Lin (1998) décrivent des méthodes permettant de découvrir des synonymes en analysant la collocation de mots. Des méthodes linguistiques exploitent l'analyse syntaxique et grammaticale de corpus afin d'étendre le lexique. Citons les travaux de Hatzivassiloglou et McKeown (1997) qui utilisent les conjonctions and et or pour déduire l'orientation sémantique de vocabulaire ainsi associé à des mots déjà connus. Turney (2002) utilise des motifs un peu plus complexes tels que adverbe + adjectif non suivis par un nom. Benamara et al. (2007) se basent sur une classification d'adverbes et attribue des scores à des adjectifs selon la catégorie de l'adverbe auquel ils sont associés.
Citons enfin les travaux de Google (Godbole et al., 2007) ou Hu et Liu (2004) qui utilisent le dictionnaire bien connu WordNet (Miller et al., 1993).
Méthodes d'apprentissage analysant les opinions
Les systèmes utilisant des méthodes d'apprentissage classifient des commentaires textuels en 2 classes (positive et négative), mais parfois cherchent à prédire des notes de 0 à 5. Ces méthodes de classification supervisées considèrent qu'un commentaire décrit un seul film et cherchent à prédire une note donnée par l'auteur du commentaire.
Beaucoup de méthodes utilisent une préparation linguistique du corpus. Wilson et Wiebe (2003) exposent comment étiqueter les mots porteurs d'opinions par une intensité, après quoi Wilson et al. (2004) testent 3 méthodes d'apprentissage différentes (fréquemment utilisées par les linguistes) : BoosTexter (Shapire et Singer, 2000), Ripper (Cohen, 1996) et SVMlight (la version légère de Support Vector Machine de Joachims (1998)). Cette dernière obtient les meilleurs résultats sur leur corpus annoté. De la même manière, pour caractériser ce qui est ou non apprécié dans chaque phrase, Nigam et Hurst (2004) combinent une technique de parsing avec un classifieur bayésien pour associer la polarité à des thématiques.
Pourtant Pang et al. (2002) et Dave et al. (2003) montrent que la préparation des corpus par des lemmatiseurs par exemple, ou encore par la prise en compte des négations, s'avère inutile. Afin de prédire l'opinion de commentaires sur des films, ces deux papiers explorent quelques méthodes d'apprentissage et démontrent qu'elles sont plus performantes que les méthodes de parsing suivies d'un décompte comme présentées ci-dessus, avec des résultats de l'ordre de 83% de bonnes prédictions. Les commentaires sont vus comme des sacs de mots. Pang et al. (2002) utilisent un classifieur bayésien naïf et un classifieur maximisant l'entropie.
Dans notre étude exploratoire, nous partons du principe que nous n'avons aucun a priori sur les données. Volontairement, nous n'avons procédé à aucun pré-traitement, et outre le fait que la technique peut de ce fait être utilisée sur toutes les langues, nous prenons l'hypothèse que le vocabulaire dédié aux opinions n'est pas le seul déterminant et utile pour faire des recommandations de films.
Technique utilisée
On présente dans cette section une extension au cas non supervisé des modèles en grilles introduits dans le cadre de l'évaluation bivariée pour la classification supervisée (Boullé, 2007c,a).
Après avoir formalisé l'évaluation d'une grille dans le cas de deux variables catégorielles à expliquer, on montre que ce type de grille peut s'interpréter comme un modèle non paramétrique de corrélation entre les valeurs de chaque variable. On décrit ensuite les algorithmes permettant d'optimiser ce type de modèles. On montre enfin qu'il peuvent s'appliquer à l'analyse exploratoire en utilisant un modèle de co-clustering des individus et des variables.
Groupement de valeurs bivarié non supervisé
On cherche à décrire conjointement les valeurs des deux variables catégorielles à expliquer Y 1 et Y 2 , comme illustré sur la figure 1.
FIG. 1 -Exemple de densité jointe pour deux variables catégorielles Y 1 ayant 4 valeurs a, b, c, d et Y 2 ayant 4 valeurs A, B, C, D. Le tableau de contingence sur la gauche ne contient des individus que sur la moitié des cases (marquées •), les autres cases étant vides. Suite au groupement de valeurs bivarié, le tableau de contingence sur la droite permet une description synthétique de la corrélation entre
On introduit en définition 1 une famille de modèles où chaque variable à expliquer est partitionnée en groupes de valeurs. On distribue les individus sur l'ensemble des cellules de la grille bidimensionnelle résultant du produit cartésien des partitions univariées ainsi définies. Cette distribution étant spécifiée, on en déduit par sommation sur les cellules la distribution des individus sur les groupes de valeurs pour chaque variable à expliquer. Il ne reste qu'à spécifier localement à chaque groupe la distribution des individus sur les valeurs du groupe pour obtenir une description complète de la distribution des individus sur les valeurs des deux variables conjointement. Définition 1. Un modèle de groupement de valeurs bivarié non supervisé est défini par : -un nombre de groupes pour chaque variable à expliquer, -la partition de chaque variable à expliquer en groupes de valeurs, -la distribution des individus sur les cellules de la grille de données ainsi définie, -la distribution des individus de chaque groupe sur les valeurs du groupe, pour chaque variable à expliquer.
Notations 1.
-N : nombre d'individus de l'échantillon -V 1 , V 2 : nombre de valeurs pour chaque variable (connu) -J 1 , J 2 : nombre de groupes pour chaque variable (inconnu)
j1 , m j2 : nombre de valeurs du groupe j 1 (resp. j 2 ) -n
j1 , N j2 : nombre d'individus du groupe j 1 (resp. j 2 ) -N j1j2 : nombre d'individus de la cellule (j 1 , j 2 ) de la grille Un modèle de groupement de valeurs bivarié non supervisé est entièrement caractérisé par le choix des paramètres de partition des valeurs en groupes
des paramètres de distribution des individus sur les cellules de la grille
et des paramètres de distribution des individus des groupes sur les valeurs des variables
Les nombres de valeurs par groupe sont déduits du choix des partitions des valeurs en groupes, et les effectifs des groupes par comptage des effectifs des cellules de la grille.
Afin de rechercher le meilleur modèle, on applique une approche Bayesienne visant à maximiser la probabilité P (M |D) = P (M )P (D|M )/P (D) du modèle sachant les données. À cet effet, on introduit en définition 2 une distribution a priori sur les paramètres des modèles. Définition 2. On appelle a priori hiérarchique l'a priori de modèle de densité par grille basé sur les hypothèses suivantes : -les nombres de groupes de valeurs J 1 (resp. J 2 ) des variables à expliquer sont indépen-dants entre eux, et compris entre 1 et V 1 (resp. V 2 ) de façon équiprobable, -pour un nombre de groupes donné J 1 de Y 1 , toutes les partitions des V 1 valeurs en J 1 groupes sont équiprobables, -pour un nombre de groupes donné J 2 de Y 2 , toutes les partitions des V 2 valeurs en J 2 groupes sont équiprobables, -pour une grille de taille donnée (J 1 , J 2 ), toutes les distributions multinômiales des N individus sur les G cellules de la grille sont équiprobables, -pour un groupe donné d'une variable à expliquer donnée, toutes les distributions multinômiales des individus sur les valeurs du groupe sont équiprobables.
Cette distribution a priori sur les paramètres des modèles est hiérarchique, uniforme à chaque étage de la hiérarchie. Pour les distributions multinômiales, les cases vides sont considérées dans la distribution a priori. En utilisant la définition formelle des modèles et leur distribution a priori hiérarchique, la formule de Bayes permet de calculer de manière exacte la probabilité d'un modèle connaissant les données, ce qui conduit au théorème 1. 
B(V, J) est le nombre de répartitions de V valeurs explicatives en J groupes (éventuel-lement vides). Pour J = V , B(V, J) correspond au nombre de Bell. Dans le cas général, B(V, J) peut s'écrire comme une somme de nombre de Stirling de deuxième espèce.
La première ligne de la formule (1) regroupe des termes d'a priori correspondant au choix des nombres de groupes J 1 et J 2 et à la spécification de la partition de chaque variable à expliquer en groupes de valeurs. La deuxième ligne représente la spécification de la distribution multinômiale des N individus de l'échantillon sur les G cellules de la grille, suivi de la spécification de la distribution des individus de chaque groupe sur les valeurs du groupe. La troisième ligne représente la vraisemblance de la distribution des individus dans les cellules de la grille, au moyen d'un terme du multinôme. La dernière ligne correspond à la vraisemblance des valeurs localement à chaque groupe pour chacune des variables à expliquer.
Interprétation
Dans le cas d'une grille comportant une seule cellule, la formule (1) se réduit à :
(2)
ce qui correspond à la probabilité a posteriori d'un modèle multinômial, pour chacune des variables catégorielles Y 1 et Y 2 à expliquer. On peut alors interpréter le modèle de groupement de valeurs bivarié non supervisé de la définition 1 comme un modèle de description de la corrélation entre les deux variables à expliquer. En cas d'indépendance entre les variables, la description des deux variables conjointement se réduit à la somme des descriptions de chaque variable individuellement. Le modèle en grille permet de capturer de façon non paramétrique des corrélations entre les valeurs des variables à expliquer. Le surcoût de description du modèle de corrélation en grille est alors compensé par une description plus concise des valeurs de chaque variable connaissant le modèle de corrélation. Le meilleur compromis est recherché suivant une approche Bayesienne de la sélection de modèles.
Exemple de deux variables catégorielles à expliquer corrélées. Prenons l'exemple de deux variables catégorielles identiques, et d'un modèle en grille M comportant autant de groupes que de valeurs (J 1 = V 1 ), comme illustré sur la figure 2. Le coût de description de la grille provenant de la formule (1) est alors égal à :
FIG. 2 -Grille de groupement de valeurs bivarié avec autant de groupes que de valeurs pour deux variables à expliquer identiques
En comparant les formules (2) dans le cas d'indépendance et (3) dans le cas d'égalité des variables à expliquer, on observe un surcoût de modélisation dans les termes d'a priori (spécification de chaque groupement de valeurs et spécification de la distribution des individus sur la grille bidimensionnelle). En revanche, le coût de vraisemblance est divisé par deux : les corrélations étant capturées dans le modèle en grille, la description des deux variables à expliquer se réduit à la description d'une seule variable.
Algorithme d'optimisation
Nous proposons une heuristique gloutonne ascendante d'optimisation, qui, partant d'une solution initiale de partitionnement bivarié aléatoire, procède par fusion itérative des groupes de valeurs tant qu'il y a amélioration du critère. Cet algorithme est précédé d'une étape de pré-optimisation qui consiste à déplacer les valeurs entre les groupes de façon à améliorer le critère. Afin d'améliorer la solution obtenue, une post-optimisation, basée sur le même algorithme de déplacement des valeurs, est également appliquée.
L'heuristique gloutonne a une complexité algorithmique de O(N 5 ) avec une implémenta-tion naïve. En effet, pour V ? N , il y a O(N ) étapes de fusions de groupes effectuée lors de l'heuristique gloutonne, et chaque étape repose sur l'évaluation de O(N 2 ) fusions de groupes potentielles impliquant des grilles de O(N 2 ) cellules. On montre dans (Boullé, 2007b) que cet algorithme peut être implémenté avec une complexité algorithmique de O(N ? N log N ) en partant d'une solution initiale aléatoire de taille O( ? N ). Pour atteindre cette complexité, on exploite l'additivité du critère d'évaluation des grilles bivariées, qui se décompose sur les caractéristiques de la grille, des variables, des groupes de valeurs et des cellules. On utilise également la nature intrinsèquement creuse des grilles qui comportent au plus N cellules non vides (une par individu) pour une taille de O(N 2 ) cellules potentielles. Comme cette heuristique gloutonne est efficace en temps de calcul, nous l'avons incorporée au sein de la méta-heuristique Variable Neighborhood Search (VNS) (Hansen et Mladenovic, 2001), qui consiste essentiellement à appeler l'heuristique principale en partant de solutions aléatoires générées dans le voisinage de la meilleure solution. On obtient ainsi un algorithme de type "anytime", qui permet d'améliorer la solution en fonction du temps de calcul disponible.
Co-clustering des individus et variables
Un co-clustering (Hartigan, 1972) est défini comme le regroupement simultané des lignes et des colonnes d'une matrice. Dans le cas des jeux de données de faible densité, ayant de nombreux 0 dans le tableau croisé individus x variables, le co-clustering est une technique attractive pour identifier des corrélations entre groupes d'individus et groupes de variables (Bock, 1979;Govaert et Nadif, 2006;Dhillon et al., 2003;Lechevallier et Verde, 2004).
Considérons un jeu de données binaire de faible densité avec N individus, K variables et V valeurs non nulles. Un tel jeu de données peut être représenté sous la forme d'un tableau à V lignes et deux colonnes. Cela correspond a un nouveau tableau de données avec deux variables nommées "ID Individu" et "ID Variable" où chaque individu est un couple de valeurs (ID Individu, ID Variable), comme illustré sur la figure 3.
FIG. 3 -Jeu de données binaire de faible densité : depuis la matrice creuse (individus x variables) au tableau bivarié dense.
En appliquant notre methode de groupement de valeurs bivarié non supervisée à ce tableau bivarié dense, on obtient une grille bivariée basée sur le groupement des individus d'une part, le groupement des variables d'autre part. Le critère d'évaluation du groupement bivarié conduit à maximiser la corrélation entre groupes d'individus et groupes de variables, ce qui correspond à l'objectif général du co-clustering. Il est à noter que les co-clusters sont ici les cellules de la grille, et qu'ils forment une partition sans recouvrement du jeu de données.
Résulats sur l'exemple d'analyse d'opinions
Nous avons effectué les premières analyses sur 50 000 commentaires portant indifférem-ment sur 7 114 films. Le vocabulaire est composé de 27 673 mots différents. En formatant les données sur le modèle de la figure 3, on obtient alors plus de 700 000 individus (film, mot).
Les commentaires de films ont subi un pré-traitement minimal, sans lemmatisation, stematisation ni filtrage sur les mots. Les seuls traitements effectués ont été de mettre tous les caractères en minuscule et la ponctuation a été supprimée. Les commentaires traités sont donc tous de la même forme que les exemples suivants :
-it was really good juss somethin u wouldnt expect from her 
Clustering de mots
Nous pouvons observer dans les résultats que les mots sont, en général, classés entre termes de même nature. On retrouve par exemple des groupes de mots :
- -etc. D'autres caractéristiques ressortent de certains groupes. On retrouve par exemple les mots n'appartenant pas à la langue anglaise classés dans des ensembles communs. On retrouve ainsi des groupes de mots français d'une part et des groupes de mots espagnols d'autre part. On retrouve aussi des groupes de mots ne contenant qu'un seul terme comme "it", "a", "of", "to", "movie", etc., termes qui n'apportent que peu de sens, voire pas du tout, et qui peuvent être employés quelque soit le contexte dans le langage courant. C'est pourquoi l'outil ne peut les rapprocher d'aucun autre mot.
Clustering de films
Parmi les résultats du clustering de films, on trouve des groupes séléctionnés selon diffé-rentes caractéristiques.
On 
Liens entre clusters de mots et clusters de films
Nous pouvons observer au moins trois tendances lorsque que l'on analyse les fréquences de chaque mot d'un même ensemble dans tous les clusters de films. Certain mots, comme ceux référents à un acteur ou personnage, ne sont présents que dans un très faible nombre de clusters de films (exemple avec l'ensemble de mots contenant "sean, connery, james, bond, etc.", voir figure 4). D'autres sont présents dans un plus grand nombre, comme les termes décrivant un type de films : comédie, action, policier, etc. (exemple avec l'ensemble de mots contenant "funny, hilarious, comedy, etc.", voir figure 5). Et enfin d'autres groupes de mots contiennent des termes présents quasi uniformément dans une grande majorité des clusters de films, c'est le cas pour les ensembles contenant des mots de liaison (in, film, with, from, he, his, etc.).
Conclusion, perspectives
L'avantage de notre technique de coclustering est qu'elle est fine et fiable, entièrement automatique en ne nécessitant ni paramètre utilisateur, ni connaissance a priori sur le domaine, interprétable et efficace en temps de calcul.
Ces premières analyses de co-clustering de films et de mots nous montrent que les textes produits par les internautes permettent de catégoriser les films selon différents critères. Cette méthode permet notamment de répertorier la plupart des genres existants dans le domaine du cinéma et d'associer ces genres à un vocabulaire précis employé par les amateurs de films. En plus du classement par genres, les films sont catégorisés en fonction des acteurs présents, du réalisateur et de l'appréciation des auteurs des commentaires analysés. Pour chacune de ces caractéristiques, cette méthode nous permet de connaître le vocabulaire s'y rapportant, vocabulaire qui ne paraît pas toujours informatif a priori et qu'il serait donc difficile à dé-terminer par des méthodes linguistiques. Ainsi notre méthode peut être complémentaire avec des analyses linguistiques, en proposant un enrichissement des dictionnaires répertoriant le vocabulaire d'opinion ou de genre cinématographique avec du vocabulaire fiable.
Outre l'exploration même des données, dans un contexte de service de recommandation, la méthode permet naturellement d'associer tout nouveau commentaire (un film qui vient de sor-

Summary
A method of automatically alignment of definitions is proposed in order to improve the fusion between specialized medical terminologies and a general one. An SVM classifier and a compact representation are used. The model trained on the nominal group of Noun-Adjectives reaches the best performances.

Introduction
Acquérir de la connaissance à partir de textes est une nécessité qui s'est accrue ces vingt dernières années, avec l'essor considérable de la masse de documents disponibles en format électronique, qu'il faut gérer afin d'extraire ou de filtrer les informations pertinentes parmi toutes celles contenues dans ces documents (Faiz, 2006). A titre d'exemple; les événements boursiers sont nombreux et diversifiés. Les experts de la bourse doivent analyser ces événe-ments en un temps relativement raisonnable pour prendre des décisions importantes. Il s'agit, donc, d'annoter les documents présentant des événements pour pouvoir extraire ceux qui sont pertinents. C'est dans ce cadre que s'inscrit notre travail dont l'objectif est de dévelop-per une approche qui annote automatiquement ces articles de Presse.
La suite du document est organisée comme suit : nous commençons, dans la section 2, par décrire les principaux systèmes d'annotations existants. Au cours de la section 3, nous présentons notre approche d'annotation, qui a été validée par le système AnnotEv lequel sera présenté et évalué dans la section 4. Enfin, dans la section 5, nous présentons quelques perspectives de notre travail.
Présentation de quelques systèmes d'annotation
Plusieurs méthodes et techniques sont utilisées par les systèmes d'annotations dédiés au Web sémantique telles que l'Exploration Contextuelle (Desclés, 1997), les graphes conceptuels (Roussey et al, 2002), les méta-thésaurus (Khelif et al., 2004) et les indicateurs linguistiques (Muller et al., 2004). Nous pouvons citer :
Le système EXCOM (Djaoua et al., 2006) utilise un ensemble d'outils linguistiques qui visent à annoter un document par un ensemble de connaissances aussi bien internes qu'externes. Le système prend en entrée des textes et procède à l'annotation sémantique et discursive de certains segments à partir des points de vue de fouille. L'annotation sémantique de ces segments fait appel à la technique linguistique et informatique d'exploration.
Annotea (Kahan et al., 2001) est un système client-serveur collaboratif pour l'annotation de documents. Les annotations, externes aux documents, sont écrites en RDF. Des utilisateurs, connectés à un serveur d'annotations, peuvent les ajouter, les modifier et les consulter. Cependant, nous constatons que l'affichage des annotations est séparé du document, ce qui devrait être résolu pour améliorer la compréhension et l'efficacité de telle annotation.
SyDoM (Roussey et al., 2002)  Setzer et Gaizauskas (2000) ont proposé un système d'annotation qui se base sur la dé-termination des événements et les relations entre eux en se basant sur des connaissances linguistiques. Cependant, nous constatons, le système ne prend pas compte de l'état et des questions de type Qui et Comment pour extraire un événement, et d'autre part, il se base uniquement sur les marqueurs temporels pour déterminer les relations entre les événements, hypothèse qui n'est pas tout à fait validée étant donné qu'il existe des relations implicites inter-événements qui sont exprimées sans utiliser des marqueurs temporels.
Approche proposée pour l'annotation des événements
Nous constatons que les approches proposées pour l'annotation de l'information temporelle sont principalement linguistiques et se basent sur les indices temporels. Nous nous sommes intéressés plutôt à l'annotation des événements sous forme de métadonnées liées aux documents. Notre travail ne se limite pas à la détection des événements, mais permet aussi de regrouper les événements similaires pour faciliter un traitement ultérieur (indexation, remplissage des formulaires, etc.). Le processus d'annotation automatique des documents que nous présentons, s'effectue en quatre étapes :
1. Prétraitement : qui consiste d'une part, à détecter les frontières des phrases dans un texte et d'autre part, à nommer les entités. 2. Annotation des événements : qui utilise un classificateur jouant le rôle d'un filtre pour les phrases non événementielles. 3. Clustering : qui consiste à regrouper les phrases se référant au même événement.
Nous avons proposé, pour cette étape, une nouvelle mesure de similarité entre les événements.
4. Annotation du document : qui prendra différentes formes (phrase, formulaire, concept, etc.) selon le domaine d'application.
Etape 1 : Prétraitement
Dans notre étude, le prétraitement est l'application de certains outils de Traitement Automatique des Langage Naturelles (TALN) au texte brut pour le segmenter en phrases et annoter les entités nommées.
Pour la tâche de segmentation, nous avons utilisé le système SegaTex développé par Mourad (2001) pour son aspect multilingue et sa disponibilité. Concernant la nomination des entités, nous avons utilisé le système GATE (Bontcheva et al., 2004).
Etape 2 : Annotation des événements
Un événement est un objet spécifique qui se produit à un instant spécifique et dans un endroit bien déterminé. Notre objectif est d'identifier tous les événements présents dans un document. Nous marquons par une balise chaque événement détecté. Pour cela, un modèle de classification est construit automatiquement, il permet de prédire si une phrase contient un événement ou non. Nous avons utilisé, dans un premier temps, les attributs qui se rapportent aux événements tels qu'ils sont définis par Naughton et al. (2006) Il s'agit donc de classer une phrase comme étant événementielle ou non. Plusieurs techniques d'apprentissage automatique peuvent être utilisées tels que les réseaux de neurones, l'arbre de décision, les réseaux bayésiens, etc. Nous avons choisi l'arbre de décision puisque la construction de l'arbre est moins paramétrable, comparé aux autres techniques, ce qui permet la réduction de la complexité du système. L'ensemble de l'apprentissage (training set) a été annoté par un expert. Pour chaque article de presse, les événements sont annotés comme suit : L'annotateur est amené à assigner des étiquettes à chaque phrase représentant un événement ; Si une phrase se rapporte à un événement, il lui assigne l'étiquette "Oui" sinon "Non".
Nous avons appliqué à ce même ensemble d'apprentissage, différents algorithmes de construction des arbres de décision. Puis, nous avons choisi le modèle qui a le plus grand PCC (Pourcentage de Classification Correcte). Le résultat de cette étape est l'ensemble des phrases se référant à des événements.
Etape 3 : Le Clustering
Au cours de cette troisième étape, nous regroupons les phrases se référant aux mêmes événements par l'application de l'algorithme « Hierarchical Clustering (HAC) », qui assigne initialement chaque objet à un cluster, puis fusionne, à plusieurs reprises, les clusters jusqu' à ce qu'un des critères d'arrêts soit satisfait (Manning et Schutze, 1999).
Dans ce cadre, nous avons proposé une nouvelle mesure de similarité sémantique entre les événements. Nous signalons qu'il existe plusieurs mesures de similarités entre les documents, telle que le Cosinus de Salton (1988), le Cosinus dans l'espace distributionnel et la distance de khi-deux (Lebart et Rajman, 2000). D'autres mesures, qui nous intéressent le RNTI -X -plus, portent sur la similarité entre les phrases, dont la plus récente est la mesure de Naughton (2006).
Ainsi, la nouvelle mesure de similarité que nous proposons, est inspirée de tf-idf (weight term frequency-inverse document frequency) et tient compte également de la position des clusters dans le document. Afin de pouvoir regrouper des phrases exprimant le même évé-nement par deux lexiques différents, nous avons utilisé une base de synonymes permettent le remplacement des instances par leurs classes. Par exemple, soient les deux phrases événe-mentielles suivantes, initialement considérées comme deux clusters C1  Avec Ctij une classe de la base des synonymes Par la suite nous présentons FSIM par la formule suivante : Faiz, 2006).
Etape 4 : Annotation du document
Nous continuons l'enrichissement du document par d'autres métadonnées qui seront très utiles pour d'autres applications (RI, Résumé Automatique, Question-Réponse, Indexation).
Une forme possible de métadonnée est le remplissage de formulaires, c'est-à-dire stocker les événements dans une base de données en répondant à des questions bien déterminées, par exemple : Keyword: Killed ; Location: Baghdad ; Time/date: 2 p.m ; Person: U.S. soldier.
Une autre forme de métadonnée est le résumé automatique, qui consiste à marquer, par des balises, les phrases qui forment le résumé d'un document. En général, le but d'un système de résumé automatique est de produire une représentation condensée du contenu dès son entrée, où les informations importantes du texte original sont préservées, il faut prendre en considération les besoins de l'utilisateur et de la tâche spécifiée (Minel et al., 2001).
Dans le cadre de notre étude, nous proposons un résumé informatif contenant les informations essentielles contenues dans l'article. Ce résumé est sélectif puisqu'il néglige les aspects généraux de l'article. Mais il est ciblé étant donné qu'il concerne les événements.
A partir de chaque cluster généré par la troisième étape, nous annotons l'article par les principaux événements qu'il contient. Nous utilisons une heuristique qui est : La phrase ayant les attributs maximaux est la meilleure pour annoter l'article. 
Conclusion et perspectives
Nous avons présenté une approche d'annotation automatique des événements qui se base sur l'apprentissage automatique, accompagnée d'une exploitation de l'annotation qui constitue un résumé automatique. Nous avons, également, proposé une nouvelle mesure de similarité sémantique : FSIM entre les événements.
Notre approche se compose de quatre étapes : en commençant, dans une première étape, par le prétraitement qui consiste à appliquer des outils de TALN pour préparer les données. Dans une deuxième étape, un classifieur qui permet de filtrer les phrases événementielles. Au cours de la troisième étape, les phrases sont regroupées dans des clusters selon leur degré de similarité (FSIM). Dans la dernière étape, un résumé est généré automatiquement et portant sur les principaux événements constituant l'article. Nous avons validé notre approche sur un corpus d'articles de presse. Une des perspectives que nous proposons est d'adopter AnnotEv à la langue arabe.

Introduction et contexte général
Savoir déterminer de manière à la fois efficace et exacte l'identité d'un individu est devenu un problème critique dans notre société. En matière de sécurité, la biométrie ne cesse d'apporter des solutions de plus en plus efficaces. Elle consiste à identifier une personne à partir de ses caractéristiques physiques ou comportementales. Le visage, les empreintes digitales, l'iris, etc, sont des exemples de caractéristiques physiques. La voix, l'écriture, le rythme de frappe sur un clavier, etc, sont des caractéristiques comportementales.
Dans la littérature récente, les recherches portent sur plusieurs problématiques de l'identification biométrique, et surtout sur la reconnaissance de visages qui s'avère une méthode, d'une part, simple pour l'utilisateur puisqu'une brève exposition devant une caméra permet de l'identifier ou de l'enregistrer dans le système et d'autre part, la reconnaissance de visages n'est pas encore un problème résolu comme l'ont montré les évaluations conduites par NIST 1 (Phillips et al., 2003).
NIST : National Institute of Standards and Technology
La recherche dans le domaine de la reconnaissance de visages profite des solutions obtenues dans le domaine de l'apprentissage automatique. En effet, le problème de classification de visages peut être considéré comme un problème d'apprentissage supervisé où les exemples d'apprentissage sont les visages étiquetés. Notre article introduit dans ce contexte une nouvelle approche hybride de classification qui utilise le paradigme d'apprentissage automatique supervisé. Il est organisé en cinq sections. Dans la deuxième section, nous présentons brièvement les principales approches de reconnaissance de visages. La section 3 présente un aperçu sur la classification supervisée et les treillis de Galois. En se basant sur le fondement mathéma-tique des treillis de Galois et leur utilisation pour la classification supervisée, nous décrivons un nouvel algorithme de classification baptisé CITREC dans la section 4. Son application pour la reconnaissance de visages et la validation expérimentale sont détaillés dans la section 5.
La reconnaissance de visages
Les travaux qui se sont intéressés à la reconnaissance de visages dans un environnement sous différentes conditions d'éclairage, d'expressions faciales et d'orientations, peuvent être classés en deux catégories suivant l'approche suivie. Nous allons décrire dans ce qui suit ces deux principales approches sachant qu'une revue détaillée des principales techniques de reconnaissance de visages est donnée dans (Enuganti, 2005).
L'approche géométrique
Elle englobe une famille de méthodes appelées méthodes analytiques ou à caractéristiques locales. Elles sont dites analytiques puisque, en vue de reconnaître un certain visage et le classer, elle procède par son analyse. En effet, il est possible d'attribuer une certaine description du visage humain, et ce, en rappelant ses parties et leurs relations. Nombreuses sont les dé-marches qui ont essayé de modéliser et de classer les visages en se basant sur des mesures de distances normalisées et d'angles entre des points caractéristiques du visage humain (Kamel et al., 1993). Ce qui rend ces méthodes intéressantes est qu'elles prennent en compte la spé-cificité du visage en tant que forme naturelle à reconnaître. Elles considèrent aussi un nombre réduit de paramètres (entre 9 et 14 distances (Kamel et al., 1993)).
L'approche globale
Les méthodes dites globales traitent les propriétés globales du visage. Elles sont fondées essentiellement sur l'information pixel, un point singulier dans une image. Le visage est traité comme un tout. Le système (ou classifieur) apprend ce qu'est un visage à partir d'un ensemble d'exemples. Ce type de systèmes de reconnaissance est très efficace, reste que la phase d'apprentissage s'avère lourde à mettre en oeuvre (Yang et al., 2002). Parmi les approches les plus utilisées, nous pouvons citer : l'approche de visages propres, l'approche stochastique, l'approche statistique et probabiliste et l'approche connexionniste.
Classification Supervisée et Treillis de Galois
Une des méthodes les plus utilisées de l'apprentissage automatique est celle de la classification qui consiste à répartir systématiquement de nouveaux objets selon des classes établies au préalable. Classer un objet consiste à lui faire correspondre une classe marquant sa parenté avec d'autres objets. Une classe désigne un ensemble défini de données et d'objets semblables. Ces données et objets, éléments d'une classe, sont des instances.
Les treillis de concepts formels (ou treillis de Galois) sont une structure mathématique permettant de représenter les classes non disjointes sous-jacentes à un ensemble d'objets (exemples, instances, tuples ou observations) décrits à partir d'un ensemble d'attributs (propriétés, descripteurs ou items). Ces classes non disjointes sont aussi appelées concepts formels, hyperrectangles ou ensembles fermés. Une classe matérialise un concept (à savoir une idée générale que l'on a d'un objet). Ce concept peut être défini formellement par une extension (exemples du concept) ou par une intension (abstraction du concept) (Nguifo et Njiwoua, 2005). Étant donné que, notre approche se base sur l'approche de Xie, nous présentons brièvement dans ce qui suit les systèmes CL N N et CL N B présentés dans (Xie et al., 2002).
Les systèmes CL
L'approche de Xie (Xie et al., 2002) consiste en deux algorithmes qui se nomment CL N B (Concept Lattices Naïve Bayes) et CL N N (Concept Lattices Nearest Neighbors). Ces deux systèmes incorporent respectivement un classifieur bayésien naïf (R. Duda, 1973) et un classifieur par plus proches voisins (Dasarathy, 1991) au sein des noeuds du treillis de concepts.
L'idée directrice de cette approche est le fait d'associer à chaque concept du treillis vérifiant certaines contraintes de sélection un classifieur contextuel. L'apprentissage de chacun de ces classifieurs est effectué seulement sur l'extension du concept en question. Ensuite, lors du classement, on vérifie si la nouvelle instance à classer correspond bien à l'intension du concept alors on utilise le classifieur correspondant pour déterminer sa classe. Il est à noter que les deux systèmes (CL N B & CL N N ) reposent sur le même principe, tant en apprentissage qu'au classement. L'unique différence réside dans le choix du type du classifieur contextuel utilisé par chacun de ces deux classifieurs. Le treillis de concepts est utilisé pour sélectionner le sousensemble d'instances qui sera utilisé par le classifieur contextuel lors de l'apprentissage et du classement (Nguifo et Njiwoua, 2005).
Il ressort des résultats en prédiction présentés par l'auteur dans (Xie et al., 2002), qu'en gé-néral, les méthodes CL N B et CL N N améliorent respectivement la prédiction des méthodes NB et NN prises individuellement. En outre, CL N B obtient des résultats en moyenne supérieurs à ceux des trois autres systèmes sur ces jeux de données. Il est à noter que nous distinguons trois limites non négligeables de cette approche, à savoir :
-Approche non incrémentale, ce qui signifie qu'à chaque nouvelle instance ajoutée dans la base, le processus d'apprentissage doit être relancé ; -Complexité élevée et lourdeur de la phase d'apprentissage ; -Problème de mise à l'échelle (Sclability), la construction du treillis dépend énormément du nombre d'instances. Pour pallier certaines de ces limites avec un minimum de compromis, nous proposons dans la section qui suit un nouvel algorithme de classification que nous nommons CITREC et que nous appliquerons dans la problématique de la reconnaissance de visages.
CITREC : Nouvelle approche hybride de classification à base de Treillis de Galois
Dans cette section nous allons introduire un nouvel algorithme, baptisé CITREC (Classification Indexée par le TREillis de Concepts), dont l'objectif principal est de pallier les principales limites de l'approche de Xie (Xie et al., 2002).
Principe général de CITREC
Tout comme l'approche de Xie (Xie et al., 2002), l'idée principale de notre approche demeure le fait d'utiliser le treillis de concepts pour sélectionner le sous-ensemble d'instances qui sera utilisé par un classifieur contextuel lors de l'apprentissage et du classement.
L'originalité de notre approche concerne essentiellement la phase d'apprentissage et plus précisément la construction du treillis qui consiste à le construire à partir d'un contexte réduit contenant seulement une instance représentative de chaque classe et non pas tout l'ensemble d'apprentissage. Ceci va nettement réduire le temps de construction en plus de l'amélioration de la mise à l'échelle (Scalability) sans perte d'informations. Pour chaque noeud du treillis qui satisfait certaines contraintes que nous présentons plus loin dans cette section, un classifieur de base serait entrainé sur les instances de la base d'apprentissage ayant la même classe que celles des instances représentatives dans l'extension du concept.
Quant à la phase de classement, des modifications ont été apportées à l'approche de Xie afin d'utiliser le treillis de concept comme étant un index.
Dans ce qui suit et afin d'illustrer l'explication de notre approche nous utilisons l'ensemble de données IRIS présentée dans (Fisher, 1936).
La phase d'apprentissage
La phase d'apprentissage est une phase cruciale pour tout système de classification. En effet, la phase de classement ne fera qu'utiliser les données préparées lors de cette phase. Par conséquent, les performances du système en dépendront d'une façon remarquable. Lors de la phase d'apprentissage, CITREC opère en quatre étapes successives que nous décrivons dans ce qui suit.
Prétraitement de la base d'apprentissage
Cette étape consiste à passer d'une base d'instances d'apprentissage ayant des attributs numériques et nominaux à un contexte avec des attributs binaires uniquement. Les attributs numériques sont plus difficiles à traiter, car leur espace de valeurs est infini, d'où la nécessité de discrétiser ces attributs pour aboutir à la fin à un espace de valeurs fini qui pourra ensuite être traité d'une manière semblable à celle des attributs nominaux. Pour notre approche, et suite à une série d'expérimentations, nous avons opté pour une méthode de discrétisation non supervisée générique qui consiste à diviser l'espace de valeurs de l'attribut sur k intervalles.
FIG. 1 -Treillis d'index relatif à la base illustrative IRIS
Réduction du contexte global
Après avoir généré le contexte général, l'idée clé de notre approche consiste à créer un nouveau contexte qui va servir à la construction du treillis d'index. Le contexte réduit, contiendra un nombre d'objets égal au nombre des classes des différentes instances de la base d'apprentissage. La taille de ce contexte est par conséquent insensible au nombre d'instances d'apprentissage, ce qui nous fera gagner en matière de temps d'exécution et de complexité lors de la construction du treillis et même lors du classement. L'idée est de sélectionner une instance représentative de chaque classe. Le choix de cette instance est assez délicat, car c'est elle qui servira à déterminer la classe de la nouvelle instance lors de la phase de classement. Nous notons que, pour mener à bien la phase de classement, cette instance devrait minimiser la somme des distances entre elle et le reste des instances de la même classe.
Construction du treillis d'index
Quant à la construction du treillis d'index, nous notons que ce treillis (voir figure 1) sera construit sur la base du contexte réduit généré lors de l'étape précédente. Nous avons opté pour un algorithme incrémental qui est une version améliorée de celui de Godin (Godin et al., 1995). Cet algorithme est implémenté au sein de la plateforme Galicia (Valtchev et al., 2003). Il est à noter que l'incrémentalité est un aspect très important de l'algorithme de construction du treillis utilisé au sein du système CITREC. Ce choix nous permet d'ajouter de nouvelles instances à la base d'apprentissage sans pour autant régénérer le treillis d'index.
Création et affectation des classifieurs contextuels
Une fois le treillis d'index construit, il ne reste qu'à sélectionner les noeuds pertinents et de leur affecter de nouveaux classifieurs. Par noeuds pertinents, nous désignons l'ensemble des noeuds qui satisfont deux contraintes de sélection, à savoir la contrainte de support et la contrainte de non-inclusion. Dans ce qui suit nous désignons par CLS un classifieur contextuel.
La contrainte de support : Cette contrainte peut être formalisée comme suit : POUR chaque règle contextuelle C ? CLS, nous avons :
Le paramètre ? désigne le pourcentage d'instances représentatives qui doit exister dans l'extension d'un concept pour le sélectionner et lui affecter un classifieur contextuel. Nous avons fixé ce paramètre à 10% comme valeur par défaut 2 .
La contrainte de non-inclusion : La contrainte de non-inclusion permet d'éviter de créer deux classifieurs dont le sous-ensemble d'apprentissage du premier est inclus dans celui du deuxième. Dans ce cas, le premier n'apportera pas de plus par rapport au deuxième. Ceci peut être formalisé comme suit : Étant donné deux règles contextuelles r 1 : C 1 ? CLS 1 et r 2 : C 2 ? CLS 2 , nous avons :
Algorithme 1: L'algorithme d'apprentissage de CITREC
La phase de classement
Lors de cette phase, CITREC reçoit en entrée une instance de classe inconnue et retourne la classe de cette instance après avoir effectué tout un processus de classification. Le processus de classification de CITREC peut être décomposé en trois étapes, à savoir : 1. Conversion de l'instance, qui consiste à transformer l'instance à classer de sa représen-tation d'origine (avec des attributs numériques et/ou nominaux) vers la représentation conceptuelle basée uniquement sur des attributs binaires, en respectant les mêmes attributs considérés lors de l'apprentissage. En d'autres termes, cette conversion devrait respecter les intervalles de chaque attribut défini lors de la discrétisation.
2. Recherche des classifieurs contextuels activés par la nouvelle instance. Cette étape consiste à ajouter l'instance dans notre treillis d'index et de marquer les noeuds auxquels cette nouvelle instance a été affectée. Sachant qu'on ne doit marquer que les noeuds incorporant un classifieur. La façon la plus simple de marquer ces noeuds consiste à parcourir l'ensemble des noeuds incorporant des classifieurs et de sélectionner les concepts dont l'intension de l'instance à classer est incluse dans l'intension du concept à marquer.
3. Vote majoritaire et décision, qui consiste à déclencher la classification de l'instance originale (avant la conversion) par les différents classifieurs contextuels concernés par l'instance à classer. Ensuite nous effectuons un vote majoritaire sur les résultats de chacun des classifieurs. La classe ayant eu plus de vote est celle de la nouvelle instance. Nous insistons sur le fait que la classification au sein des noeuds est effectuée avec l'instance brute avant le prétraitement et ceci pour surpasser la perte de précision engendrée par la discrétisation. Cette étape sous-entend la conversion de ces instances en une représentation à attributs binaires.
CITREC est incrémental
2. Mise à jour du treillis d'index. Cette étape nécessite un algorithme de construction du treillis de concept qui soit incrémental. Ce qui justifie notre choix pour l'algorithme de Godin (Godin et al., 1995) qui présente cet avantage. En considérant l'instance centrale déduite lors de la première étape, le treillis d'index sera mis à jour par l'ajout de cette instance.
3. Mise à jour des noeuds affectés par l'ajout.Il s'agit de parcourir les différents noeuds du treillis d'index affectés par l'ajout et de sélectionner les noeuds incorporant des classifieurs contextuels pour mettre à jour leurs classifieurs de base respectifs. Un point important est que, le fait de mettre à jour les classifieurs de base, sous-entend l'aspect incrémental de ceux-ci. Pour cela, nous avons opté pour des versions incrémentales de nos classifieurs de bases. 
Algorithme 2: L'algorithme de mise à jour de CITREC
Application de CITREC dans la reconnaissance de visages
Dans cette section, nous allons présenter l'application de notre approche de classification supervisée CITREC dans la problématique de reconnaissance de visages. Cette approche nous fera profiter de l'union de l'analyse formelle de concepts avec l'apprentissage à partir d'instances (classifieur à PPV (Dasarathy, 1991)) ou la classification bayésienne (classifieur bayé-sien naïf (R. Duda, 1973)) pour contribuer à la résolution de la problématique de reconnaissance de visages.
Présentation du Benchmark
Notre base de visages expérimentale est la base BioID qui consiste en 1521 images (384 x 288 pixels, niveaux de gris) de 23 personnes différentes et qui a été enregistrée durant plusieurs sessions dans différentes places et conditions d'illumination.
Le groupe de travail FGnet a procédé à l'annotation des visages de la base BioID. Cette annotation consiste à la localisation de 20 points caractéristiques du visage humain (position des yeux,de la bouche, etc.). Dans le cadre de cet article, nous avons étendu l'annotation fournie par FGnet de 20 points à 36 points caractéristiques. Les 16 points que nous avons ajoutés (voir figure 2) ont un impact remarquable sur les résultats, vu leurs emplacements stratégiques. Dans ce qui suit, nous désignons par annoter un visage le fait d'en localiser les points spécifiques sur sa photo.
Modélisation du visage humain
Formellement, le modèle du visage n'est autre qu'une traduction de la réalité qui nous permettra de l'utiliser dans le cadre de notre algorithme de classification CITREC. Ce modèle sera construit à partir des points caractéristiques du visage. Nous notons que ce modèle sera basé sur les distances entre les points et non sur leurs positions. Pour nos expérimentations, nous avons généré en utilisant le modèle du visage et l'annotation étendue que nous avons présenté dans la section précédente, un benchmark qui comporte 30 attributs numériques avec 1521 instances. Ces instances sont réparties sur 23 classes différentes.
Formalisation mathématique
Sur un visage, nous considérons qu'un point est représenté par deux coordonnées x et y, soit pt i un point, nous avons :
Comme nous l'avons déjà présenté dans la section précédente, le modèle du visage se base sur les distances entre les points. Ainsi, nous désignons par d a,b la distance euclidienne entre deux points pt a et pt b , nous avons :
Nous notons que toutes les distances sont normalisées par la distance entre les deux pupilles des yeux (distance entre pt 1 et pt 2 ) pour assurer l'invariance des mesures par rapport à la distance entre l'individu et le dispositif de capture de la photo lors de la prise de vue.
Nous désignons par n d a,b la distance euclidienne normalisée entre les points a et b. De ce fait, nous avons :
Ainsi, nous désignons par v la représentation d'un visage (voir figure 2) :
Nous passons dans ce qui suit à la validation expérimentale de CITREC dans la probléma-tique de la reconnaissance de visages.
Validation expérimentale de l'approche CITREC
En vu de valider notre approche hybride de classification supervisée et souligner son apport, nous présenterons une évaluation expérimentale de CITREC. Cette évaluation est enrichie par une suite de comparaisons avec d'autres algorithmes existants dans la littérature, à savoir C4.5 (Quinlan, 1993), CART (Breiman et Friedman, 1984) et BFTree (Haijian, 2007). Les différentes comparaisons sont réalisées par rapport à trois variantes de CITREC qui sont CITREC P P V , CITREC RB et CITREC BN . Ces variantes incorporent respectivement un classifieur à plus proche voisins (Dasarathy, 1991), un classifieur par réseau Bayésien (Pearl, 1985) et un classifieur Bayésien naïf (R. Duda, 1973). Nous avons aussi comparé chaque variante de CITREC avec son classifieur de base correspondant. Concernant la méthode de validation,
FIG. 2 -Modèle mathématique du visage
nous avons opté pour la validation croisée d'ordre 10 avec la technique leave-one-out pour la sélection des instances.
Afin de fournir des résultats vérifiables, nous avons adapté CITREC pour pouvoir l'intégrer dans l'environnement d'évaluation des algorithmes de datamining nommé WEKA (Witten et Frank, 2005).
Comparaison des résultats
La première étape de nos expérimentations porte sur la comparaison des résultats des différents algorithmes présentés précédemment par rapport à ceux des variantes de CITREC sur le benchmark des visages. Le tableau 1 présente ces résultats en termes de pourcentage de classifications correctes (PCC). Nous constatons un avantage des trois variantes de CITREC par rapport à leurs classifieurs de base respectifs, ainsi qu'un avantage par rapport aux autres algorithmes de la littérature. Ceci confirme expérimentalement l'adaptation de CITREC pour ce type de données, et par conséquent à la problématique de reconnaissance de visages. Nous constatons que la variante à plus proche voisins de CITREC l'emporte en matière de PCC. Ceci est due aux bons résultats de prédiction réalisé par le classifieur à plus proches voisins (Dasarathy, 1991) pour ce type de données.
Impact de l'extension de l'annotation sur les résultats des différents classifieurs
Nous avons présenté dans cet article, l'extension de l'annotation d'origine de la base BioID. Cette extension nous a permis de créer 15 attributs supplémentaires dans le modèle du visage (de c16 à c30 (voir figure 2)). Le tableau 1 présente les résultats relatifs à l'annotation de base et ceux de l'annotation étendue. En comparant l'annotation de base avec l'annotation étendue, nous remarquons une amélioration claire des résultats de tous les algorithmes de classification en utilisant le modèle issue de l'annotation étendue. Cette amélioration est due à la pertinence des attributs ajoutés, qui a augmenté la discrimination entre les personnes.
Conclusion et perspectives
Dans cet article, nous avons proposé une nouvelle méthode hybride de classification supervisée à base de treillis de Galois que nous avons nommé CITREC (Classification Indexée par le TREillis de Concepts) dont l'originalité provient de la combinaison de l'analyse formelle de concepts avec les approches de classification supervisée à inférence bayésienne (R. Duda, 1973;Pearl, 1985) ou à plus proches voisins (Dasarathy, 1991). Notre approche s'inspire de celle proposée par Xie dans (Xie et al., 2002) en essayant de pallier ses limites majeures tel que l'aspect non incrémental, la lourdeur de la phase d'apprentissage et le problème de mise à l'échelle (Scalability). Nous avons montré empiriquement sur un benchmark du domaine de la reconnaissance de visages que notre approche améliore les résultats de son classifieur de base et devance plusieurs algorithmes cités dans la littérature tels que : C4.5 (Quinlan, 1993), CART (Breiman et Friedman, 1984) et BFTree (Haijian, 2007).
Comme perspective, nous proposons d'étudier la possibilité de recourir à la logique floue pour la représentation des instances centrales des classes, et ce afin de minimiser la dégrada-tion de la précision engendrée par la discrétisation des attributs numériques. Ce choix devrait surtout influencer la phase de génération du treillis d'index.
Références Breiman, L. et J. Friedman (1984). Classification and regression tree. In Wadsworth International, California. 

Introduction
Existe-t-il des séries typiques d'événements qui structurent la vie familiale ? Est-ce que certaines séquences d'événements sont typiques d'une partie de la population ou d'une souspopulation ? Pour répondre à ces questions, les sciences sociales ont besoin de méthodes pour analyser les parcours de vie dans leur totalité. Mais comment décrire ou comparer des sé-quences d'événements ? Dans cet article, nous proposons de nous centrer sur les transitions dans les parcours de vie pour les décrire. Ainsi, l'approche proposée adopte un point de vue complémentaire à l'alignement de séquences, par exemple, qui se base sur des séquences d'états.
Les parcours de vie familiaux peuvent être compris comme des séries de transitions entre états de la vie familiale telles que fonder un nouveau foyer, l'arrivée d'un nouvel enfant ou le remariage d'un parent... 2 Ces transitions peuvent être caractérisées par plusieurs événements simultanés, par exemple, lorsqu'une personne fonde un foyer en quittant son domicile parental 1 Etude soutenue financièrement par le Fonds national suisse de la recherche (FNS) FN-100012-113998, et réalisée avec les données collectées dans le cadre du projet « Vivre en Suisse 1999-2020 », piloté par le Panel suisse de ménages et supporté par le FNS, l'Office fédéral de la statistique et l'Université de Neuchâtel.
2 Dans cet article, nous nous centrerons sur la vie familiale, mais nous pourrions inclure d'autres ensembles d'évé-nements tels que ceux affectant la vie professionnelle.
ou encore lorsqu'elle se marie en même temps que la naissance d'un enfant. Nous nous inté-resserons à caractériser les parcours de vie familiaux des individus vivant en suisse en prenant les données de l'enquête biographique rétrospective réalisée par le Panel suisse de ménages en 2002.
L'idée de départ de cet article est d'explorer l'applicabilité d'une approche de type ngrammes pour caractériser les parcours de vie et en extraire des connaissances. Les n-grammes ont été utilisés dans des contextes multiples pour caractériser des textes (Damashek, 1995;Mayfield et McNamee, 1998). Cette méthode se base sur le découpage en courtes sous-séquences de caractères d'un texte afin de pouvoir le caractériser et le comparer. Dès lors, il peut sembler logique d'utiliser cette méthode pour caractériser les parcours de vie représentés sous forme de séquences d'événements. Cependant, la méthode n'est pas directement applicable à cause des différences dans les types de données. En effet, les n-grammes sont construits sur une séquence continue qui ne connaît ni la simultanéité des événements (les caractères forment une séquence stricte) ni les trous (absence de caractère pendant une période indéfinie). De plus, il importe ici de tenir compte de la variabilité des durées qui séparent des événements consécutifs. Nous proposons d'utiliser la recherche de sous-séquences fréquente pour pallier ce manque.
Le reste de l'article est organisé de la manière suivante. Nous commençons par décrire plus précisément les données de type « parcours de vie » que nous avons à notre disposition. Nous passons ensuite en revue les principes de l'analyse n-grammes et discutons des limites de leurs applications au parcours de vie. Nous présentons ensuite la recherche de sous-séquences fréquentes avant de discuter de son adaptation pour caractériser les parcours de vie. Finalement, nous appliquons la méthode décrite avant de discuter de ses apports par rapport à d'autres méthodes plus classiques d'analyse des parcours de vie.
Présentation des données
Nous utilisons les données de l'enquête biographique rétrospective réalisée par le Panel suisse de ménages 3 en 2002. Nous utilisons les résultats du questionnaire sur les parcours de cohabitation. Pour chaque année, nous connaissons les personnes avec lesquelles habitaient le répondant. Ainsi, nous disposons, pour chaque individu, d'une histoire de sa vie familiale. Nous avons choisi de centrer notre analyse sur la transition vers l'âge adulte en prenant les parcours de vie depuis la naissance jusqu'à trente ans. Afin de comparer des séquences similaires, nous n'avons retenu que les répondants ayant trente ans lors de l'enquête, soit 3557 individus.
Notre base de données se présente sous la forme d'une liste « individu-transition » où chaque transition est décrite à l'aide d'un ensemble d'événements. Les événements considérés sont les arrivées et les départs de personnes qui compose le ménage dans lequel vit un individu donné. Le tableau 1 donne la liste des événements que nous avons utilisés dans notre analyse. Le codage des départs nous permet de capter les transitions en terme d'un état vers un autre. En effet, la transition n'est pas seulement caractérisée par son état de destination, mais également par son origine.
Nous présentons dans le tableau 2 un extrait de notre base de données. Dans cet exemple, nous avons représenté deux individus. Le premier n'a connu que deux transitions : il naît en 1973 (« apparition » des parents dans le parcours) et quitte ses parents (événements L et Cette représentation des données -considérée notamment par Agrawal et Srikant (1995) -n'est pas spécifique des données de type parcours de vie, mais peut représenter plusieurs types de séquence qui doivent prendre en considération la simultanéité des événements. Il en va de même pour le codage des événements que nous avons réalisé. En effet, dans une optique plus générale, on peut penser les événements comme l'apparition ou la disparition d'un attribut au cours de la vie d'un individu.
Après avoir présenté les données et leur particularité, nous présentons les principes des n-grammes et nous discutons des limites de leurs applications à ces données.
Principe des n-grammes
Nous reprenons ici la présentation des n-grammes de Damashek (1995). Un texte peut être représenté en utilisant un vecteur composé des fréquences relatives de chaque n-gramme distinct qui le compose. La liste exhaustive des n-grammes correspond à l'ensemble des séquences de n caractères obtenus en faisant coulisser une fenêtre de n caractères le long du texte, un caractère à la fois. Ainsi, si n = 3, le texte « abcde » sera décrit à l'aide des tri-grammes « abc », « bcd » et « cde ».
Le poids assigné à chaque n-gramme est égal à sa fréquence relative. Ainsi, si un texte comprend j n-grammes distincts, le poids x i du i-ième éléments est égal à :
avec m j , le nombre d'occurrences du j-ième n-gramme. Par construction, on a j j=1 x j = 1. Cette pondération donne un poids identique à chaque caractère de la séquence -chacun apparaissant dans n n-grammes 4 . D'autres types de pondération existent, tel que l'utilisation du T F/IDF (Mayfield et McNamee, 1998).
Afin de juger de la similarité de deux textes, Damashek (1995) propose d'utiliser le cosinus de l'angle entre deux vecteurs. Il note qu'il peut être intéressant de mesurer cette différence par rapport au centroïde. En effet, ceci permet de mesurer les dissimilarités à partir d'un point commun (qui pourrait former l'origine « réelle ») plutôt que par rapport à l'origine mathématique, nécessairement arbitraire.
A première vue, l'application des principes des n-grammes aux parcours de vie peut sembler directe. Il suffirait de coder les parcours de vie sous la forme de séquences de caractères. Cependant, ce codage n'est pas possible pour deux raisons. Premièrement, les textes sont caractérisés par de très longues séquences alors que les parcours de vie ne contiennent que peu de caractères (ou d'événements) en comparaison. Ainsi, l'information apportée par un caractère dans un texte est bien moindre que celle d'un événement dans un parcours de vie.
Deuxièmement, la notion de succession des caractères est clairement établie dans le cas des textes, où chacun est précédé et suivi d'un autre (hormis pour la fin du texte). Il n'en va pas de même pour les parcours de vie où les événements peuvent être simultanés. De plus, la notion de succession est également différente dans une séquence d'événements de vie. En effet, celle-ci n'est pas complètement définie : est-ce qu'un événement qui se produit vingt ans après un autre lui succède de la même manière que si l'événement suivant se produit 3 ans après ?
Pour pallier ces problèmes, nous proposons de nous baser sur la notion de k-séquence plutôt que celle de n-grammes pour décrire les séquences à l'aide de leurs sous-séquences. Cette notion a été introduite par Agrawal et Srikant (1995)  
K-séquence
Le concept de k-séquence et sa formalisation sont introduits par Agrawal et Srikant (1995). Nous utilisons ici la notation proposée par Zaki (2001) dans la présentation de son algorithme SPADE de recherche de sous-séquences fréquentes et de règles d'association entre celles-ci.
Nous reprenons ici sa formulation en adaptant les termes à la terminologie que nous avons déjà introduite.
Une séquence peut être comprise comme une liste ordonnée de transitions. Une séquence ? est notée (? 1 ? ? 2 ? ... ? ? q ), où chaque ? i désigne une transition. Les transitions sont des listes non ordonnées d'événements distincts (c'est-à-dire qu'un événement ne peut apparaître deux fois dans la même transition). Une séquence composée de k événements est appelée une k-séquence.
Agrawal et Srikant définissent alors la sous-séquence de la manière suivante : ? est une sous-séquence de ? si chaque transition de ? est un sous-ensemble de ? et que l'ordre des transitions est conservé. On le note ? ?. Ainsi, par exemple, (B ? AC) est une sousséquence de (AB ? E ? ACD) puisque B ? (AB) -l'événement B est contenu dans la transition (AB) -et que (AC) ? (ACD) -les événements A et C sont contenus dans la transition (ACD) .
Une sous-séquence est dite fréquente si on l'observe dans un nombre de séquences supé-rieures à un support minimum 5 défini au préalable. Une sous-séquence est dite maximale si elle n'est incluse dans aucune autre sous-séquence fréquente.
Dans un but d'interprétation des résultats, nous avons jugé utile de spécifier une fenêtre de temps maximale pour la recherche de sous-séquences (spécification qui existe également dans SPADE). Ainsi, il est possible de ne rechercher que les sous-séquences qui se déroulent dans un laps de temps donné. Cette spécification s'avère très utile dans le cas de longue séquence avec peu d'événements, car il est plus difficile d'assumer une liaison entre des événements très éloignés dans le temps.
La formalisation d'Agrawal et Srikant permet donc de distinguer l'ordre d'apparition des événements, mais également leurs simultanéités, en considérant les transitions comme des listes non ordonnées d'événements. Les k-séquences nous permettent de caractériser les parcours de vie de la même manière que les n-grammes permettaient de caractériser des textes. Les parcours de vie peuvent être considérés comme des séquences caractérisées par des sousséquences. C'est l'aspect que nous présentons à présent.
Caractérisation de parcours de vie à l'aide de k-séquences
En suivant le principe des n-grammes, on peut représenter chaque séquence à l'aide d'un vecteur composé des fréquences relatives de chacune des sous-séquences. Pour cela, nous devons encore définir un critère pour choisir les sous-séquences que nous utiliserons. Nous devons également choisir une méthode de comptage des sous-séquences afin de calculer les fréquences relatives.
Choix des sous-séquences
En suivant la méthode des n-grammes, on devrait utiliser un k fixé pour le calcul des sousséquences. Cependant, ceci n'est pas pertinent pour les parcours de vie. En effet, certaines dynamiques ne peuvent être captées. Dans notre cas, un individu qui resterait chez ses parents jusqu'à trente ans ne pourrait être décrit à l'aide d'une 3-séquence, car il n'aurait connu que deux événements. Il en va de même pour une personne ayant un enfant après dix ans de mariage, si notre fenêtre de temps est plus restreinte. Dès lors, il faudrait prendre des séquences d'ordre 1. Cette solution enlève toutes les notions de successions des événements et donc de séquence. Deux méthodes sont envisageables pour choisir les sous-séquences :
1. On retient l'ensemble des sous-séquences fréquentes qui sont applicables à une séquence donnée. Ainsi, si l'on considère la séquence (A ? B ? C), on utilisera les sousséquences (A ? C) (A ? B) et (B ? C) ainsi que les « sous-séquences événements » (A),(B) et (C) pour autant qu'elles satisfassent le support minimum.
2. On ne retient que les sous-séquences maximales d'une séquence donnée. Ainsi, la solution dépend du support minimum. Si l'on considère la séquence (A ? B), on ne retiendra que (A ? B) si cette sous-séquence est fréquente ou les « sous-séquences événements » (A) et (B), pour autant qu'elles soient fréquentes, sinon. Cette dernière méthode implique de recalculer plusieurs fois les sous-séquences fréquentes (puisque les fréquences dépendent de la sélection effectuée). Elle a cependant l'avantage de ne pas multiplier le comptage de chaque événement entre les sous-séquences.
Méthode de comptage et calcul des poids
Afin de calculer les poids de chaque sous-séquence, il est nécessaire de choisir une mé-thode de comptage. Il existe plusieurs méthodes pour compter le nombre de fois qu'une sousséquence apparaît dans un parcours de vie (Joshi et al., 2001). Nous proposons de compter le nombre d'occurrences d'une sous-séquence dans chaque séquence de vie, c'est-à-dire le nombre de fois où l'on observe la sous-séquence dans chaque parcours de vie. Par exemple, dans la séquence (A ? B ? B), on compte deux fois la sous-séquence (A ? B), une fois en liant (A) au premier (B) et une deuxième fois en le liant au second.
Si l'on s'en tient aux fréquences relatives des sous-séquences, chaque événement et chaque transition peuvent avoir des poids différents en fonction de la séquence. En effet, un événement peut être décrit à l'aide de plus ou moins de sous-séquences. Ce problème n'apparaît pas explicitement dans le cas des n-grammes. En effet, on considère une fenêtre coulissante, ce qui implique que chaque lettre est comptée n fois. Il est vrai que les n ? 1 premiers et derniers caractères sont compté moins souvent. Cependant, cette différence de poids est compensée par la longueur du texte. Ainsi, l'utilisation de la fréquence relative dans les n-grammes assure un poids plus ou moins équivalent à chaque caractère.
Plusieurs solutions peuvent être adoptées : -Un poids identique à chaque sous-séquence.
-Un poids établi de manière à ce que chaque événement ait un poids identique dans l'ensemble. -Un poids établi de manière à ce que chaque transition ait un poids identique dans l'ensemble. Il est nécessaire de fixer la base de calcul des poids accordés à chaque sous-séquence. Ils peuvent être calculés, par exemple, sur la base des événements concernés et du nombre de sous-séquences qui les décrivent. Ainsi si l'on prend un ensemble de parcours de vie composé de K événements distincts, on évaluera le poids w s associé à une sous-séquence s, composée de E événements en utilisant :
Où q s désigne le nombre d'occurrences de la sous-séquence s, S e l'ensemble des sous-séquences fréquentes contenant l'événement e et q se le nombre d'occurrences de la séquence s e qui exprime l'événement e. Ainsi, chaque événement repartit sa contribution (soit 1 K ) entre l'ensemble des sous-séquences qui le décrivent.
La caractérisation des séquences à l'aide de sous-séquences nécessite de spécifier les trois points suivants : la méthode de sélection des sous-séquences fréquentes, la méthode de comptage ainsi qu'une base permettant d'assigner des poids à chaque sous-séquence.
Application : Analyse en composantes principales
Nous avons caractérisé l'ensemble des séquences à l'aide des sous-séquences fréquentes en choisissant un support minimum de deux pour cent. Ce seuil nous fait retenir 326 sousséquences différentes pour décrire les parcours de vie. Nous ne présentons pas ici les résultats obtenus avec la caractérisation à l'aide des sous-séquences maximales. En effet, les résultats sont difficiles à interpréter, sans-doute à cause du manque de redondance de l'information qui permet de calculer les proximités entre événements et sous-séquences. Pour cette première analyse, nous avons choisi d'accorder un poids identique à chaque sous-séquence en suivant les principes utilisés dans le cas des n-grammes.
A l'aide de ces données, nous avons effectué une analyse en composante principale pour offrir une visualisation de l'espace formé par les parcours de vie. Dans les graphiques qui suivent, les sous-séquences contribuant le plus significativement à la construction des axes sont représentées à l'aide d'un rond bleu. Nous avons également ajouté plusieurs variables supplémentaires : le sexe (triangle, pointe en haut, rouge), la langue d'interview que nous considérerons comme un proxy pour aborder la culture d'origine (triangle, pointe en bas, rouge clair), la cohorte de naissance -la période dans laquelle on est né -(carré vert) ainsi que la confrontation à des problèmes d'argent dans la jeunesse (losange bleu). Ces différentes variables nous permettront de caractériser le contexte dans lequel ces séquences apparaissent.
Nous avons également ajouté comme variable supplémentaire (représentée à l'aide d'un pentagone vert) la classification des parcours de vie décrite dans Müller et al. (2007) obtenue à l'aide de la méthode « optimal matching ». Cette méthode permet de classer les séquences d'états en respectant les états présents ainsi que leurs temporalités. Cette classification est construite sur la base de quatre événements soit le départ du domicile parental, le mariage, l'arrivée du premier enfant et le divorce. La méthode décrite ici permet de prendre en considération un plus grand nombre d'événements, tel que la mise en couple, la cohabitation avec des amis, etc... Nous pourrons ainsi comparer la méthode présentée avec celle de l'optimal matching.
La figure 1 nous montre les deux premières dimensions de l'espace formé par les sé-quences. La première dimension oppose ceux qui sont restés chez leurs parents (à gauche) aux autres. Ainsi, on retrouve à gauche les séquences K, Q et KQ qui dénotent l'arrivée des deux parents. De par notre système de pondération, ces sous-séquences ont un poids très élevé La classification de l'optimal matching se conforme aux axes et à leur description. Ainsi, le premier axe oppose ceux qui ne partent pas ou tard de chez leurs parents aux autres, alors que le second distingue la temporalité du mariage. On remarque que cette temporalité est prise en compte à l'aide des sous-séquences du départ du domicile parental qui marque ainsi des transitions différentes pour le mariage tardif et le mariage précoce.
La troisième dimension (figure 2) oppose ceux qui ont un poids fort sur les événements de type départ de chez les parents aux autres. Ainsi, le point le plus à droite correspond à ceux qui ne partent pas selon la classification « optimal matching ». C'est le dernier axe à être corrélé FIG. 2 -Positionnement des séquences et des variables supplémentaires sur le plan des troisième (7.55%) et quatrième (5.45%) composantes principales.
avec le nombre d'événements ou de transition (r = 0.27). La quatrième dimension distingue ceux qui partent pour vivre en couple ou pour se marier, de ceux qui partent vivre seuls. Ainsi, les regroupements de trajectoires menant directement au mariage se situent dans les valeurs positives.
La cinquième dimension (figure 3) oppose ceux qui partent de chez leurs parents pour se mettre en couple aux autres formes de départ. Cette dimension montre une forte association avec la cohorte. On remarque que le départ du domicile parental pour se mettre en union sans mariage immédiat (ce qui ne signifie pas sans mariage) est une dynamique relativement récente. Cette dimension oppose les répondants originaires de Suisse romande (francophone) aux autres. Finalement, la sixième dimension distingue ceux qui se marient la même année où ils ont leur premier enfant des autres formes de départ. Ce modèle est relativement plus présent en Suisse italienne et dans les cohortes plus anciennes.
Ici encore, on trouve un lien avec la classification de l'optimal matching. Cependant, les liens ne sont pas directs. Ainsi, la catégorie « Mariage tardif » est située tout à gauche montrant qu'on se marie tard, mais qu'on est déjà en union. Il en va de même pour les trajectoires « Modernes ». Les « Mariages précoces » se situent à l'opposé et montre qu'on se marie directement en quittant ses parents et qu'on tend également à avoir des enfants à ce moment.
Nous ne présentons ici que les six premières dimensions. Cependant, il est possible d'associer un sens aux dix premières dimensions. Ceci nous montre qu'un grand nombre de dimensions sont nécessaires à la compréhension de l'espace des séquences. Ce n'est pas une surprise étant donné que nous avons regroupé un grand ensemble d'événements qui sont souvent analysés séparément, tel que le départ du foyer parental, la mise en union ou encore l'arrivée du premier enfant.
Les résultats obtenus sont relativement similaires à ceux de l'optimal matching, mais apportent un éclairage complémentaire en se centrant sur les transitions. Ainsi, on a pu voir des différences entre le modèle de mariage tardif et précoce en identifiant certaines transitions intermédiaires (notamment la mise en union). La méthode proposée a cependant l'avantage de pouvoir intégrer un plus grand nombre d'événements, surtout s'ils peuvent être simultanés. Avec l'optimal matching, cette situation implique un trop grand nombre d'états et les résultats deviennent difficilement interprétables. Sans spécifier explicitement une temporalité, celle-ci se retrouve dans les axes. Cependant, ceci est dû aux séquences qui se centrent sur la période allant de la naissance à 30 ans ainsi qu'à la présence de l'événement départ du domicile parental que presque l'ensemble des individus connaît pendant cette période.
Ainsi, la caractérisation à l'aide des sous-séquences fréquentes permet d'offrir une visualisation de l'espace formé par les séquences et de dégager quelques dynamiques intéressantes sur les différences entre types de trajectoire, notamment avec l'aide des variables supplémentaires. Toutefois, nous voyons deux limites à la méthode présentée qui nécessite des développements futurs. Le système de pondération des sous-séquences rend parfois l'interprétation des axes difficile, puisqu'elle dépend de la présence d'autres événements dans la séquence. Il s'agit d'un poids relatif à la séquence. Il est ainsi nécessaire de développer d'autres méthodes qui

Introduction
L'objectif de notre étude est de pouvoir semi-automatiser le processus de réponse aux plaintes exprimées en français, en langue naturelle et relatives à la pollution de l'air au sein des logements. Ces plaintes reflètent chacune un cas particulier, cependant elles abordent des problèmes communs que les experts aimeraient identifier de manière objective. Notre démarche est de construire de manière automatique des scénarii. Dans la première étape nous établissons un modèle de représentation et de recherche en ne négligeant pas l'aspect sémantique. Le choix de la ressource sémantique est guidé par l'étude du vocabulaire du corpus, il est présenté dans la partie suivante. Enfin, nous présentons l'évaluation de la qualité des partitions (scénarii) obtenues.
Modélisation de l'espace des plaintes
Par manque de place ici, nous ne pouvons rappeler de manière détaillée nos nombreuses positions pour formaliser les textes et pour définir les différentes mesures de similarité textuelle correspondantes. Néanmoins, nous pouvons noter que pour le traitement des textes longs, les modèles vectoriels sont les plus fréquemment utilisés, tandis que pour la comparaison des textes courts le modèle booléen flouifié est le mieux adapté. De manière générale, la plainte comporte des informations concernant les symptômes de l'occupant, elle décrit également le logement et son environnement extérieur, ..etc. Nous avons traduit ces champs de conversation sous forme de modèles de balise (unités sémantiques) dans un document XML pour enregistrer les plaintes. Les balises que nous avons retenues pour le formalisme XML des plaintes sont: symptômes, habitat et environnement Zargayouna et Salotti (2004) ont étendu le modèle vectoriel de Salton en définissant le nouveau poids des termes TF-ITDF adapté à la structure XML des documents. Un vecteur des poids des termes correspond à une unité sémantique (contenu d'une balise). La similarité entre deux unités sémantiques est calculée en fonction du cosinus de l'angle formé par les deux vecteurs correspondants.
Le modèle vectoriel étendu
Le modèle de recherche basé sur la proximité floue des termes
Le degré de proximité floue des termes de la requête permet d'évaluer le taux de densité des termes de la requête dans les textes. Il permet ainsi de classer les documents en fonction de leur pertinence par rapport à la requête. Mercier et Beigbeder (2004) calculent la pertinence relative µ des termes de la requête aux différentes positions x dans un document d comme suit:
Le paramètre k caractérise le degré d'influence d'une occurrence d'un terme. La pertinence d'une requête booléenne disjonctive et/ou conjonctive est calculée en prenant respectivement le maximum et/ou le minimum des pertinences locales. Le score d'un document par rapport à une requête est calculé en agrégeant les pertinences relatives locales.
Le modèle vectoriel sémantique
Le poids sémantique SemW du terme t au sein d'une balise b d'un document d au niveau du vecteur sémantique définit par Zargayouna et Salotti (2004) correspond à la somme de son poids TF_ITDF et les poids des termes qui lui sont proches sémantiquement.
Notre modèle de recherche
La mesure de Mercier-Beigbeder ne tient pas compte de la sémantique. Dans (Heddadji et al., 2007) nous augmentons cette mesure de manière à prendre en considération des liens de similarité latents entre documents et à l'adapter au formalisme XML.
Un seuil de similarité est nécessaire pour délimiter l'ensemble des termes sémantiquement pertinents par rapport à t. Nous fixons cette limite à l'ensemble des termes dont le degré de similarité avec t est au-delà du score de similarité de ce dernier avec le terme auquel est rattachée la balise correspondant à l'unité sémantique où son occurrence apparaît. Dans le cas d'un corpus annoté en XML, les similarités citées calculent des appariements locaux. Une agrégation des similarités locales est nécessaire pour généraliser ces mesures au niveau «document».
Génération de la sémantique
Il est généralement reconnu que l'emploi d'une ontologie apporte une solution élégante au problème de la gestion de la sémantique. Les plaintes sont formulées par des particuliers. De nombreuses marques de produits et autres sigles sont utilisés, le vocabulaire est très vivant, d'où l'interrogation de la possibilité de construire une ontologie.
Le vocabulaire des plaintes
Nous avons étudié le vocabulaire des plaintes dont nous disposons. L'expérimentation que nous avons menée a eu lieu sur un corpus de 655 documents formulant des plaintes provenant de 4 organismes différents. Pour étiqueter les textes des plaintes, nous avons utilisé l'outil Tree-tagger adapté au français. Les sigles, abréviations et autres acronymes en relation avec le domaine de la pollution inconnus de Tree-Tagger sont récapitulés dans un fichier dédié. Les termes retenus dans ce fichier sont substitués automatiquement par leurs synonymes ou par des termes plus génériques compréhensibles par l'étiqueteur. La compréhension de la langue naturelle est spécifiée formellement par la notion d'ontologie. Dans des domaines précis la communauté professionnelle s'accorde autour d'une «ontologie métier». Nous analysons ici l'évolution du vocabulaire de nos plaintes qui sont en rapport avec la pollution intérieure uniquement. L'allure asymptotique de la courbe de la FIG. 1 est une preuve de l'insuffisance d'une éventuelle ontologie gérant la sémantique car nous ne connaissons pas le vocabulaire utilisé qui reste ouvert en l'état actuel de la base des plaintes. Afin de permettre l'usage de la langue naturelle dans la description des plaintes, il est nécessaire de construire un réseau conceptuel de façon à comprendre la langue française. A date, il n'existe pas d'ontologie universelle en français dans laquelle on pourrait retrouver de manière exhaustive les termes du langage naturel et qui puisse servir de base à un système de recherche implémentant la sémantique. Le résultat de notre étude conduit à considérer l'utilité des dictionnaires électroniques des synonymes pour le contrôle de la sémantique tout en assurant une couverture la plus exhaustive possible du lexique des plaintes.
RNTI -X -
DICTIONNAIRE et codage des mots
Nous avons utilisé le dictionnaire électronique des synonymes du laboratoire CRISCO de l'université de CAEN baptisé DICTIONNAIRE et qui regroupe les synonymes de 48 881 mots (vedettes) (Manguin, 2004). En plus de la connaissance sémantique que nous offre DICTIONNAIRE, nous l'utilisons en tant que vocabulaire de base. Nous utilisons les vedettes de DICTIONNAIRE en tant que primitives vectorielles caractérisant les textes et permettant de les apparier à l'aide du module de recherche implémentant les modèles vectoriels. La proximité sémantique entre deux termes A et B du dictionnaire correspond au rapport entre le nombre de synonymes communs et le nombre total de ces synonymes (indice de Jaccard). En principe, les formes fléchies d'un terme partagent le même sens (combienmême elles ne sont pas de la même catégorie grammaticale). Le dictionnaire des synonymes est insuffisant (le taux de similarité entre « pollution » et « polluer » est de 0 dans la base sémantique inférée par DICTIONNAIRE ) et un système de codage des termes est nécessaire pour la gestion de la sémantique entre termes du même code. Pour coder les mots, nous avons choisi d'utiliser une heuristique, certes imparfaite, mais qui permet de rapprocher des termes ayant la même racine. L'heuristique d'Enguehard (Enguehard ,1992) qui définit le code d'un terme comme étant la sous-chaîne des premières lettres jusqu'à l'obtention de deux voyelles non consécutives nous a paru simple à mettre en oeuvre. Ayant fait ses preuves dans d'autres applications (serradura et al., 2002) nous avons appliqué ce principe pour apparier les termes issus du même code. Pour chaque paire de mots de DICIONNAIRE de même code on a effectué un échange de synonymes avec une influence de ½. Le degré de similarité entre les éléments de ces paires est de 0,5. Cette définition du lien sémantique entre termes nous permet maintenant de pouvoir comparer sémantiquement deux plaintes.
Comparaisons
Evaluation de la qualité des partitions obtenues
Pour définir les scénarii nous choisissons de réaliser une partition de l'ensemble des plaintes. Pour ceci nous utilisons l'algorithme des nuées dynamiques dans lequel nous avons choisi pour noyau itératif la plainte qui minimise la disparité autour d'elle. Ce travail a été réalisé parallèlement par 3 experts du CSTB (Centre Scientifique et Technique du Bâtiment) sur 100 documents d'entraînement. Les experts se sont entendus sur l'existence de 3 thématiques traitant chacune d'un phénomène de pollution isolé. Le but de cette étude est d'évaluer d'une part les performances des modèles de représentation et de recherche, et d'autre part la qualité de la synthèse automatique de la base de plaintes en un ensemble de scénarii type de pollution. In fine, la solution affectée au scénario dont appartient la plainte la plus pertinente sera assignée au problème courant. Pour évaluer la qualité de nos différentes classifications, nous avons calculé le rapport entre la distance inter-classes et la distante intra-classe. De plus, nous avons comparé les partitions automatiques avec les partitions des experts en utilisant l'indice de Rand-corrigé. 
RNTI -X -
Le modèle vectoriel sémantique et le modèle flou sémantique donne de meilleures classes quand l'espace de données est partitionné en 3 clusters. En effet, nous percevons dans FIG. 2 que les modèles sémantiques partitionnent moins strictement que les systèmes directs dans le cas où le nombre de classes est supérieur à 3. D'un autre coté, les experts ont constaté l'existence de 3 classes de plaintes disjointes. Ce qui explique que le meilleur score de partition soit au niveau du graphe à la position k=3 pour les modèles sémantiques. Ailleurs, la partition sous les systèmes sémantiques constituent des clusters moins denses, ce qui explique la flexion enregistrée aux partitions supérieures.
Conclusion
Nous souhaitions à partir de la série d'études présentée appuyer l'idée de la correspondance existante entre le raisonnement des experts basé sur des faits et le raisonnement de nos systèmes de recherche basé sur les termes et leur sémantique. Pour améliorer encore les résultats, il nous semble pertinent de réaliser une classification floue des plaintes pour mettre en évidence des cumuls de thématiques dans une plainte particulière.

Introduction
Bien que les méthodes d'analyse factorielle soient très puissantes et contribuent efficacement à la visualisation des données, les grands échantillons nécessitent de nouvelles méthodes mieux adaptées. En effet, les algorithmes de décomposition matricielle rencontrent leurs limites sur les grands tableaux numériques ; en outre, la construction de nombreux plans de projection, du fait des grandes dimensions, rend la tâche d'interprétation difficile pour recouper les informations disséminées sur ces plans. Finalement une grande quantité de données implique une grande quantité d'informations à synthétiser et des relations complexes entre individus et/ou variables étudiés. Il est alors possible, dans ce contexte, d'utiliser les cartes de Kohonen ou cartes auto-organisatrices (SOM) (Kohonen, 1997) qui sont des méthodes de classification automatique utilisant une contrainte de voisinage sur les classes pour conférer un sens topologique aux partitions obtenues. La carte auto-organisatrice originelle peut être vue comme une variante de l'algorithme des k-means (MacQueen, 1967) intégrant une contrainte d'ordre topologique sur les centres.
Lorsque la matrice des données x est définie sur un ensemble I d'objets (lignes, observations) et un ensemble J de variables (colonnes, attributs), différentes approches de classification automatique sont utilisées et la plupart des algorithmes proposés concerne généralement un des deux ensembles. Ces algorithmes peuvent être modélisés par différentes approches. Celle qui a suscité le plus d'intérêt ces dernières années est incontestablement l'approche modèle de mélange (McLachlan et Peel, 2000). Dans ce cadre, il a été proposé diverses versions probabilistes de SOM telles que dans (Lebbah et al., 2007;Verbeek et al., 2005;Luttrell, 1994).
Le papier (Govaert et Nadif, 2003) présente une extension du modèle de mélange pour ré-pondre à l'objectif de la classification croisée dite aussi classification par blocs qui permet de tenir compte de I et J simultanément. Différents modèles ont été proposés pour tenir compte de chaque type de données.
Ces méthodes (Dhillon, 2001) ont un grand intérêt en data mining car elles sont particuliè-rement appropriées pour les grands ensembles de données en grande dimension. Elles ne sont pourtant pas encore employées en visualisation alors qu'elles ont le potentiel pour fournir un outil très efficace et parcimonieux. En effet, elles utilisent beaucoup moins de paramètres que les modèles connus usuels tels que les modèles classiques de mélange.
Pour analyser le contenu d'un ensemble de données, la visualisation est une étape crucial pour laquelle les modèles génératifs sont devenus très utiles. En effet, la taille croissante des ensembles de données rencontrés permet une estimation pertinente de variables cachées synthétisant de manière interprétable l'information contenue dans les données. Pour toutes ces raisons, nous proposons dans ce papier de traiter la question de la visualisation par une approche basée sur le modèle de mélange croisé parcimonieux et l'algorithme GTM (Bishop et al., 1998), méthode de auto-organisatrice probabiliste basée sur un modèle gaussien.
Ce papier est organisé comme suit. Le deuxième paragraphe présente une brève introduction du modèle de mélange croisé et une description rapide de l'algorithme Block EM. Le troisième paragraphe est consacré au développement, dans le cas binaire, de l'algorithme Block Generative Topographic Model ou Block GTM. Cet algorithme peut être vu comme une extension efficace du GTM à un modèle de mélanges de Bernoulli par blocs. Un algorithme d'estimation y est présenté. Le quatrième paragraphe présente des expériences numériques à partir de deux matrices binaires textuelles. Enfin, le dernier paragraphe résume les principaux résultats du papier et les perspectives originales de cette approche.
Dans la suite, la matrice de données est notée x = {(x ij ); i ? I et ? J}, où I est un ensemble de n objets (lignes, observations) et J est un ensemble de d variables (colonnes, attributs). Une partition z en g classes de l'échantillon I sera représentée par la matrice de classification (z ik ; i = 1, . . . , n ; k = 1, . . . , g) où z ik = 1 si i appartient à la classe k et 0 sinon. Une notation similaire sera utilisée pour la partition w en m classes de l'ensemble J. Par souci de simplification des formules, les intervalles de variation des indices ne seront pas spécifiés, par exemple, nous noterons
.
L'algorithme Block EM
L'objectif de la classification par blocs est d'essayer de résumer cette matrice par des blocs homogènes. Le problème peut être étudié sous l'approche d'une partition simultanée des deux ensembles I et J en g et m classes respectivement. Dans (Govaert, 1983(Govaert, , 1995 plusieurs algorithmes ont été proposés pour obtenir une classification par blocs sur des tableaux de contingence ou plus généralement sur des tables qui ont les même propriétés : des données binaires, continues ou catégorielles.
Dans (Govaert et Nadif, 2003), ces méthodes ont été modélisées par une approche basée sur des mélanges de lois. Dans le contexte du problème de la classification par blocs, la formulation du modèle de mélange classique peut être étendue pour proposer un modèle en blocs latents défini par une distribution en sommant sur l'ensemble des affectations de I × J :
où Z et W dénote les ensembles de toutes les affectations possibles z de I et w de J. Comme pour l'analyse en classes latentes, les n × d variables aléatoires X ij générant les cellules x ij observées sont supposées être indépendantes lorsque z et w sont fixés ; nous avons alors
où ?(., ? k? ) est une distribution définie sur l'ensemble des réels R.
Par exemple, lorsque les données sont binaires, en notant ? = (p, q, ? 11 , . . . , ? gm ), où p = (p 1 , . . . , p g ) et q = (q 1 , . . . , q m ) sont les vecteurs de probabilités p k et q ? qu'une ligne et une colonne appartienne au k e composant et au ? e composant respectivement, nous obtenons le modèle par blocs latents de Bernoulli défini par la distribution suivante :
Utiliser ce modèle est nettement plus parcimonieux qu'utiliser un modèle classique de mélange sur chaque ensemble I et J. Par exemple, avec n = 1000 objets et d = 500 variables et des probabilités de classes égales p k = 1/g et q ? = 1/m, si on a besoin de faire la classification automatique d'une matrice binaire en g = 4 classes en lignes et m = 3 classes en colonnes, le modèle par blocs latents de Bernoulli impliquera l'estimation de 12 paramètres (? k? , k = 1, . . . , 4, ? = 1, . . . , 3) au lieu de (4 × 500 + 3 × 1000) pour les deux modèles de mélange de Bernoulli appliqués à I et J séparément. Maintenant nous nous intéressons à l'estimation d'une valeur optimale de ? par l'approche du maximum de vraisemblance associé à ce modèle de mélange par blocs. Pour ce modèle, les données complétées sont le vecteur (x, z, w) où les vecteurs non observés z et w sont les labels. La vraisemblance classifiante L(?; x, z, w) = log f (x, z, w; ?) est notée L C (z, w, ?). L'algorithme EM (Dempster et al., 1977) maximise la vraisemblance L M (?) par rapport à ? itérativement en maximisant l'espérance conditionnelle de la vraisemblance des données complétées L C (z, w, ?) par rapport à ?, étant donné une estimation précédente courante ? (t) et les données observées x :
j? , et e ikj? sont respectivement les probabilités a postériori sur les lignes, les colonnes, et les cellules, à l'itération t.
Malheureusement, la structure de dépendance des variables X ij du modèle entraine des difficultés pour la détermination de e (t) ikj? . Pour résoudre ce problème, une approximation variationnelle remplaçant e (t) (t) (t) ikj? par le produit c ik d j? permet de fournir une bonne solution (Govaert et Nadif, 2005).
Dans la section suivante, nous développons un algorithme d'apprentissage intégrant une contrainte d'ordre topologique sur les paramètres ? k? .
Modèle et estimation
Nous présentons le Binary Bock GTM, une carte auto-organisatrice générative par blocs pour une matrice binaire x dont chaque cellule x ij est un réel 0 ou 1. Pour induire une autoorganisation topologique des densités gaussiennes, une approche par le GTM considère des coordonnées 2d pour les noeuds d'une grille rectangulaire imaginaire qui représente l'espace de projection. Ce graphe planaire peut être vu comme une discrétisation d'une partie du plan sur lequel les données, les n lignes de la matrice, vont être projetées. Comme chaque noeud doit correspondre à une classe, chaque point 2d sur le plan est alors sujet à une transformation non linéaire afin d'être amené dans un espace de dimension h supérieure. Une projection linéaire permet alors d'obtenir des centres de même dimension que les individus vectoriels.
Plus formellement, afin d'obtenir une auto-organisation des probabilités ? k? , ces dernières sont paramétrées par les g coordonnées s k dessinant une grille rectangulaire régulière sur le plan. Ces coordonnées sont projetées dans un espace de plus grande dimension h, soit en prenant pour les applications ? des bases fonctionnelles de type noyau,
? où m ? est un centre posé ad'hoc et ? ? une variance bien choisie. Finalement, la paramétrisation nécessite l'estimation de m vecteurs de dimension h inconnus nommés w ? . On écrit les probabilités du BEM binaire original à l'aide de fonctions sigmoïdes ? k? = ?(w T ? ? k ) où ?(y) = e y /(1 + e y ), comme montré en figure 1. Le vecteur de paramètres devient ? = (p, q, w 1 , w 2 , · · · , w m ).
FIG. 1 -A gauche, la grille rectangulaire des s k , sur la droite, l'espace des distributions ?. Le graphique représente la paramétrisation non linéaire des sigmoïdes. Chaque coordonnée
g, de la grille est transformée de façon non linéaire pour se retrouver dans l'espace des distributions multivariées de Bernoulli par la transformation ?(w
La matrice g × m de probabilités est remplacée par la matrice h × m, et le modèle demeure parcimonieux puisque h est petit en pratique, quelques dizaines. Donc, en reprenant à nouveau l'exemple d'une matrice binaire de 1000 lignes et 500 colonnes de la section précédente, nous aboutissons à environ 100 paramètres, compte tenu du choix à effectuer sur la valeur de h, qui est toujours peu comparativement à une approche de mélange classique. Ensuite, les para-mètres inconnus sont estimés en trouvant un maximum local de la log-vraisemblance par un algorithme EM (Dempster et al., 1977).
Grâce à l'approximation variationnelle de e
ikj? , il peut être montré (Govaert et Nadif, 2005) que la maximisation de la log-vraisemblance du Block EM est réalisée en maximisant alternativement deux critères conditionnels Q(?,
, m).
A la convergence en une position stable de ?, le paramètre optimal est nomméˆ?nomméˆ nomméˆ?. Ici, considérant des paramètres w ? , ces deux critères prennent la forme suivante :
ik . La maximisation de ces deux espérances, effectuée à l'aide de la méthode du gradient, conduit aux relations suivantes :
En dérivant les deux critères, on obtient les vecteurs de gradient Q
v . Comme les hessiennes sont diagonales par blocs, la log-vraisemblance est augmentée à chaque pas EM par deux pas de montée de type Newton-Raphson, pour ? de 1 à m, ce qui correspond à un algorithme EM généralisé.
T la g × h matrice des bases fonctionnelles, nous obtenons :
, les valeurs consécutives courantes convergent vers un maximum d'une approximation de L M (?). Un biais bayésien (Bishop et al., 1998) peut éventuel-lement être ajouté pour améliorer la stabilité numérique des estimations. La forme matricielle obtenue par une approche de gradient du second ordre est analogue à une étape d'IRLS (McCullagh et Nelder, 1983). Une alternative serait un gradient au premier ordre sous optimal en pratique. On remarque enfin que la symétrie des formules du BEM originale est ici absente du fait que seules les lignes sont projetées par la méthode proposée.
Expériences numériques
Nous évaluons notre nouvelle méthode de projection à partir de deux matrice binaires de données textuelles. Les paramètres utilisés dans nos expériences sont m = 10, g = 81 et h = 28 pour les deux bases de textes.
La projection sur le plan d'un échantillon de données binaires par le modèle Block GTM peut s'effectuer de diverses manières, dont essentiellement : -La représentation matricielle qui place en s k * l'ensemble des individus affectés à la classe associée au noeud, tels quê z i = k * . Cette affectation obéit à la règle du maximum a postériori (MAP) doncˆzdoncˆ doncˆz i = argmax k ˆ c ik . Dans le cas du SOM, l'affectation utilise la distance euclidienne entre le vecteur centre et le vecteur donnée.
-La deuxième représentation, que nous avons utilisée dans la suite car celle-ci est plus fidèle à la classification floue obtenue, consiste en une projection par position moyenne sur le plan :
On remarque que la projection MAP correspond à la projection moyenne dans laquelle on remplace la matrice de classification floue de cellules (ˆ c ik ) par la matrice de classification dure de cellules (ˆ z ik ).
FIG. 2 -Projection par Binary Block GTM de la matrice textuelle 449 × 167 des données Classic 3.
La première matrice est projetée pour tester le modèle avec trois classes. Ces données correspondent à un échantillon de la matrice Classic 3 (Dhillon et al., 2003), qui est constituée de trois bases d'articles scientifiques : M edline, Cisi, Cranf ield. Par tirage au hasard, 450 documents ont été sélectionnés avec 150 documents dans chaque classe. Seuls les mots les plus fréquents (au dessus du seuil 30) ont été retenus. La matrice finale est de 449 lignes et 167 colonnes. La projection sur la figure 2 sépare les classes sans erreur quasiment. Les outliers peuvent s'expliquer par le fait que le tableau original est de contingence, et également que les classes ne sont pas exactement disjointes comme le révèle les benchmarks relatifs.
La seconde matrice textuelle présente quatre classes et compte 400 documents décrits par 100 termes (Girolami, 2001). Le vocabulaire a été choisi par tri selon l'information mutuelle évaluée grâce aux labels des classes. La projection de ces textes, à la figure 3, révèle 4 clusters facilement reconnaissables et correspondant aux quatre groupes de discussion "sci.crypt", "sci.space,"sci.med", et "soc.religion.christian" ; dans chacun, 100 news ont été tirés au hasard. Les classes sont bien séparées avec des frontières précises et les classes de mots obtenues peuvent être interprétées. Notons que la carte obtenue avec notre approche est assez similaire à la carte auto-organisatrice probabiliste basée sur un modèle multinomial asymétrique (Kabán et Girolami, 2001), qui est moins parcimonieux.
FIG. 3 -Projection par Binary
Block GTM de la matrice textuelle 400 × 100 des données newsgroups.
Conclusion et perspectives
Nous avons proposé une carte auto-organisatrice probabiliste. Celle-ci est obtenue par l'utilisation d'un modèle de mélange de Bernoulli croisé et de l'algorithme GTM. Notre méthode, appelée Binary Block GTM, est efficace et parcimonieuse. En effet, le nombre de paramètres inconnus est égal à h × m, ce qui est très peu comparativement à un modèle contraint de mélange de lois de Bernoulli (Girolami, 2001) ou une approche dyadique comme un pLSA binaire contraint (Priam et Nadif, 2006). Quelle que soit g la taille de la carte de projection, quel que soit n le nombre de lignes, le nombre de paramètres du modèle ne croît qu'avec le nombre de classes en colonnes. En conclusion, le modèle présenté apparait clairement comme un excellent candidat pour s'attaquer aux problèmes du data mining. Il serait intéressant d'étendre cette approche au tableau de contingence en proposant un Block GTM adapté. 

Introduction
Nautilus est un outil d'analyse de bases de données qui facilite la préparation de données agrégées et formatées mises à disposition des outils de datamining et de restitution.
Nautilus est bâti autour d'un système d'abstraction des données couplé à un moteur de requêtes. L'application modélise l'environnement de données en construisant une pile de métadonnées, permettant de visualiser, documenter, et de manipuler des données du SGBD sous la forme de concepts métiers (produits, trafic, revenus, segment, etc.). Ce système de paramétrage est relié à un générateur de requêtes SQL et un gestionnaire des tâches conçus pour l'agrégation de volumes importants de données. Nautilus permet ainsi de produire de manière rapide et fiable de grands volumes de données d'analyse « à l'intérieur » des SGBD, sans nécessiter d'extractions de données. De ce fait, Nautilus remplace avantageusement les scripts et les datamarts métiers créés ad hoc tant pour les performances que la sécurité.
Nautilus, disponible depuis mai 2007, est développé sous la forme de plug-in Eclipse. La construction de jeux de données avec Nautilus se déroule en deux étapes : -Modélisation des données du SGBD ; -Construction des indicateurs La première étape consiste à modéliser les concepts métiers sur les données du SGBD. Nautilus se connecte à la base de données via JDBC et effectue les opérations de rétro-ingénierie du SGBD en important ses métadonnées.
Une fois que le modèle de données est intégré, l'utilisateur crée le dictionnaire des objets métier dont il se servira pour spécifier les indicateurs : par ex. la notion "d'appel entrant".
FIG. 1 -Edition d'une règle métier dans le dictionnaire Nautilus.
Construction d'indicateurs
La définition des éléments métier du dictionnaire permet de spécifier les champs à calculer. Ces calculs se font avec les fonctions natives du SGBD : agrégations, comptages, statistiques ainsi que les fonctions de datamining de plus en plus intégrées aux SGBD.
Nautilus génère automatiquement le SQL pour créer les indicateurs au sein du SGBD. Afin de répondre aux contraintes des départements informatiques de grandes entreprises, l'application décompose les calculs en une série de tâches qui rendent plus lisibles les calculs et permettent d'optimiser les traitements.
L'application maintient les paramètres des calculs effectués pour faciliter leur partage et leu réutilisation dans l'entreprise.
Summary
Nautilus is a database analysis software. The purpose of the application is to generalize the usage of customer data within the enterprise. It helps business users access data by mapping business concepts onto the database. Nautilus includes a metadata management module and an optimized SQL generator to compute large volumes of data. Nautilus is developped as Eclipse plug-ins.

Introduction
Les systèmes de recherche d'information préconisent une fonctionnalité très intéressante voire indispensable lors de tout processus de recherche : il s'agit de la reformulation automatique de la requête. Cette fonctionnalité permet de rétablir les choix de l'utilisateur dans la perspective de retrouver plus de documents qui répondent à son besoin en information. Il est à noter à ce niveau que le besoin en information de l'utilisateur est très vague : l'utilisateur ne sait en général pas ce qu'il cherche. Par ailleurs, il peut tolérer un résultat initial imprécis sous réserve de l'améliorer par feedback Rocchio (1971).
Faire recours à de nouvelles méthodes d'apprentissage est alors devenu une nécessité. Plusieurs modèles qui ont été auparavant délaissés, tels que la classification, sont repris en vu d'améliorer l'apprentissage en recherche d'information. Nous proposons dans ce papier une méthode d'apprentissage en faisant appel aux réseaux petits mondes (small worlds en anglais, Watts (1999)).
Notre Approche
Les propriétés des réseaux petits mondes paraissent intéressantes dans les problèmes de classification. D'autant plus que ces propriétés sont valuées. Comme application à la recherche d'information, nous présumons qu'un ensemble de documents peut constituer des réseaux petits mondes pour moins qu'ils parlent du même sujet, et qu'une idée peut être transmise d'un document à un autre document si les auteurs partagent le même intérêt.
Nos objectifs pour l'intégration des small worlds en recherche d'information ont deux effets : un effet de construction des small worlds par le bais de la classification; et un effet d'estimation de pertinence sur d'autres documents.
En partant de l'hypothèse suivante : «une classe est raisonnable si elle admet certaines propriétés : celles des small worlds». Le premier effet va simplement faire une construction de small worlds de documents homogènes (pertinents ou non pertinents). Pour ce faire, nous proposons trois stratégies : une stratégie de construction de graphes de documents (1), une stratégie de propagation des liens (2), et une stratégie de construction des classes des documents (3). Pour la stratégie (3) nous utiliserons une méthode de classification hiérarchique, et l'identification du nombre de classes dépend de la qualité de classification et de la nature de classes construites. A chaque itération nous calculons une valeur d'inertie intra-classe qui permet de quantifier l'homogénéité de la classification. Pour des classes réellement construites les coefficients de clustérisation et les distances moyennes montrent que les classes construites admettent les propriétés des small worlds.
En partant de l'hypothèse suivante : «une classe est un small worlds, et qu'une classe homogène (constituée de documents pertinents ou non pertinents) peut être utilisée comme moyen efficace pour bien constituer l'estimation des scores d'autres documents», le deuxième effet consiste à estimer la pertinence pour d'autres documents. Pour traduire la pertinence pour un document il suffit d'identifier la classe à laquelle il appartient et de juger de sa pertinence en fonction de la nature de la classe. Ce document est jugé pertinent si la classe résultat contient plus de documents pertinents que de documents non pertinents et est jugé non pertinent si non,
Conclusion
Nous avons présenté dans cet article une approche statistique de classification des documents. L'approche consiste à définir un nouveau concept d'apprentissage. L'apprentissage consiste à construire des classes qui préservent les propriétés des réseaux petits mondes. Nous admettons que les classes préservant ces propriétés sont des estimateurs de pertinences d'autres documents. L'approche que nous avons proposée consiste à considérer tous les critères pouvant intervenir dans le jugement de l'utilisateur et de leur affecter les meilleurs poids pour que la pertinence utilisateur soit proche de la pertinence système.
Les poids des critères considérés sont ajustés par apprentissage. Chaque poids traduit l'intérêt porté par l'utilisateur à celui-ci. Les poids relatifs aux termes peuvent servir de moyen de construction de requête. Nous envisageons de tester l'approche sur une base réelle de documents afin de mesurer l'apport des réseaux petits mondes à la recherche d'information. Nous envisageons également de tester la reformulation de la requête en se basant sur les poids des critères. Avec l'effet petit monde, nous envisageons d'autres méthodes telles que les méthodes d'ordonnancement (ranking effect).

Introduction
Le clustering consiste à découvrir automatiquement des groupes ("clusters") présents dans le jeu de données. Une littérature abondante existe sur le sujet (une revue des principales mé-thodes peut être trouvée dans Rui et Wunsch (2005)). Nous nous plaçons ici dans le cadre des "cluster ensembles" (Strehl et Ghosh (2002)). Les "cluster ensembles" sont une sorte de méta-clustering : à partir de plusieurs clusterings du même jeu de données, on déduit un clustering "moyen" (Strehl et Ghosh (2002)). Plusieurs alternatives ont été proposées pour trouver le clustering moyen (méthodes agglomératives, ou basées sur des graphes). Indépendamment de la méthode de synthèse choisie, il est clair que le clustering moyen dépend fortement de la qualité et de la diversité de chaque clustering individuel (Fern et Brodley (2003)). Par exemple, agréger plusieurs clusterings issus de l'algorithme K-means avec des initialisations différentes atténuera les erreurs particulières dues à chaque clustering individuel ; cependant cela ne permettra pas de contourner les limitations fondamentales de l'algorithme (clusters de forme sphé-rique, sensibilité à la dimension...). La situation idéale pour les cluster ensembles est celle où les clusterings individuels sont variés, de bonne qualité et obtenus à faible coût.
L'idée explorée par Topchy et al. (2003) est d'obtenir ces clusterings individuels en projetant le jeu de données sur une direction aléatoire, et en faisant un clustering simple sur la projection (qui revient essentiellement à trouver les modes de la densité de la projection). L'intuition est qu'avec suffisamment de droites, les séparations entre clusters seront mises en évidence et permettront donc d'obtenir un clustering moyen de bonne qualité. Dans cet article, nous proposons d'améliorer cette méthode en construisant les clusterings individuels avec d'autres projections, toujours à faible coût mais avec de meilleures performances.
Le reste de cet article est structuré de la façon suivante : La section 2 explique notre approche, et décrit formellement les projections partielles des points, la recherche de modes et le clustering final. La section 3 compare les performances de la méthode avec celles d'autres méthodes sur des jeux de données réels. La section 4 conclut et évoque des pistes de travaux futurs.
Cluster ensembles et projections
Projections intéressantes pour le clustering
La question que nous nous posons est la suivante : comment trouver des projections intéressantes pour le clustering, qui soient variées et obtenues à faible coût ? On trouve dans la littérature de nombreux algorithmes de clustering qui utilisent des projections linéaires du jeu de données. Cela est le plus souvent justifié par la "malédiction de la dimensionalité" : quand la dimension augmente, les clusters deviennent épars (les points d'un cluster sont moins concentrés) et les distances entre les points tendent à perdre leur signification. Les clusters sont également dans des sous-espaces de dimension assez basse par rapport à la dimension de l'espace (Parsons et al. (2004)). Cette "malédiction" est un frein à l'efficacité de plusieurs méthodes de clustering basées sur les distances et/ou la densité. L'intérêt des projections est qu'elles permettent de faire apparaître plus clairement les clusters. Traditionnellement, c'est dans le domaine de la réduction dimensionnelle qu'on a d'abord cherché à trouver des directions "intéressantes" en fonction de certains critères (une direction est simplement une droite qui passe par l'origine). La plus utilisée est sans doute l'analyse en composantes principales (ACP), qui consiste à déterminer les k directions où le jeu de données présente la plus grande variance. La Projection Pursuit est une approche du même type, où un problème d'optimisation est résolu pour trouver les directions qui maximisent un "intérêt" lié à l'entropie. On peut aussi citer les projections aléatoires qui reviennent à projeter les données sur un ensemble de directions choisies uniformément sur la sphère unité.
Une démarche répandue consiste à d'abord réduire la dimension du jeu de données en projetant, puis d'appliquer un algorithme de clustering quelconque sur cette représentation. Cependant, la difficulté de paramétrer la réduction dimensionnelle pour obtenir un bon clustering a conduit certains auteurs à intégrer cette réduction directement dans l'algorithme de clustering. Par exemple, certains algorithmes de clustering divisif partitionnent récursivement le jeu de données en trouvant une direction "intéressante" et en coupant le jeu de données en deux par rapport à cette direction (Boley (1998) avec une ACP, Miasnikov et al. (2004) avec une Projection Pursuit). Les approches de Aggarwal et Yu (2000) et Ding et Li (2007) utilisent des ACP lors du clustering pour associer des sous-espaces "intéressants" à chaque cluster.
Dans l'article de Topchy et al. (2003) cité dans l'introduction, les droites de projection sont choisies au hasard et passent par l'origine. Le problème est que lorsque la dimension augmente, la projection sur une droite aléatoire tend à avoir une densité gaussienne unimodale, donc inintéressante pour le clustering. Une amélioration qui vient immédiatement à l'esprit est d'utiliser par exemple une ACP, ou plusieurs ACP locales. Mais cette alternative est coûteuse : la complexité du calcul d'une ACP est quadratique avec la dimension. La direction de plus forte variance n'est d'ailleurs pas forcément la meilleure pour séparer des clusters. Le même problème de complexité et de manque de garanties se pose pour les Projection Pursuit. De plus ces deux alternatives cherchent des directions, c'est-à-dire essentiellement des droites passant par l'origine. Il est probable cependant qu'une direction intéressante pour le clustering à un endroit donné de l'espace ne le soit pas à un autre.
Notre approche
FIG. 1 -Droites inter-points
Nous proposons deux idées simples pour obtenir des clusterings individuels de bonne qualité. La première est d'utiliser des droites inter-points comme droites de projection. Si les deux points définissant la droite sont dans deux clusters différents, chacun dans son sous-espace, (comme c'est le cas pour L 1?2 sur la Figure 1), les projections de ces deux clusters sur cette droite seront vraisemblablement bien séparés. Si en revanche les deux points définissant la droite sont dans le même sous-espace, comme L 1 ?1 par exemple, la droite pourra séparer des clusters appartenant au même sous-espace.
FIG. 2 -Clusters, droite de projection et densité résultante
La deuxième idée est de ne pas projeter tout le jeu de données sur toutes les droites, afin de mieux faire ressortir les clusters. Considérons par exemple le cas de la Figure 1, où les clusters C 1 et C 2 sont dans deux sous-espaces différents. Si une droite appartient à un sousespace seulement, par exemple L 1?1 , il n'est pas vraiment judicieux de projeter les points de C 2 dessus. Plus généralement, seuls les points proches de la droite (au sens de la projection orthogonale) sont intéressants pour une droite donnée. Ceci est illustré sur la Figure 2 : si on ne projette que les points des clusters C 1 et C 2 , la densité résultante permet de bien les séparer, alors que si on projette tous les points, la densité résultante (en pointillés) est unimodale et ne sépare plus rien.
Dans la suite, on note X = {x 1 , . . . , x N } ? R d le jeu de données. La sphère unité est notée S d . Le produit scalaire est noté ··. Le cardinal d'un ensemble E est noté |E|.
Projections partielles
Droites de projection Nous choisissons au hasard M droites inter-points comme droites de projection. Chaque droite
La distance d'un point x à la droite D k est la distance orthogonale du point à la droite. On peut décomposer le vecteur Ox comme étant la somme vectorielle (
Projections partielles Pour éviter les effets évoqués dans la sous-section 2.1, on ne projette réellement un point x ? X que sur les droites les plus proches de lui. Pour cela, on calcule les distances entre chaque point et les droites. On sélectionne pour x les m droites les plus proches de lui en triant ces distances par ordre croissant. A la fin de ce traitement, chaque droite D k est donc associée à un certain ensemble de points X k . On note X k la projection de X k sur D k .
Recherche de modes
Pour produire un clustering individuel à partir de X k , nous devons identifier les modes de la densité de X k . Pour effectuer cet étape, nous utilisons un estimateur de densité à noyaux :
k . La seconde étape consiste à identifier les modes de cette densité. Pour cela, nous devons identifier les minimums locaux dê f . Un minimum local est un point où la dérivéê f (t) est nulle et la dérivée secondê f (t) est positive. En pratique, il est suffisant de parcourir la droite et d'identifier les zones où la dérivée devient tour à tour négative, nulle puis positive. Nous faisons un parcours linéaire de la densité (en discrétisant avec un pas ?) des valeurs dê f . La dérivée en un point y = i · ? est approchée parˆf
. L'Algorithme 1 décrit en pseudocode la recherche de modes.
Algorithme 1 : Recherche de modes de la densité de X k
Clustering final
Après les étapes précédentes, on a identifié les modes sur chacune des droites D 1 , . . . , D M . On rassemble cette information de façon synthétique dans une matrice X modes (de N lignes et M colonnes) telle que
où mode k (x i ) est le numéro du mode contenant la projection de x sur D k . La i-ème ligne de X modes indique donc dans quels modes et sur quelles droites se projette x i ? X. Pour terminer le clustering, nous devons maintenant effectuer la synthèse des clusterings individuels. Nous avons choisi la méthode la plus simple : on effectue un clustering agglomé-ratif de type average-link (Duda et al. (2000)) sur les lignes de la matrice X modes . La mesure de distance entre deux lignes doit refléter l'intuition suivante : deux points sont "proches" si ils sont projetés dans les mêmes modes sur les mêmes droites. En comparant deux lignes, nous devons donc ignorer les colonnes où les deux lignes ont des composantes nulles, c'est-à-dire les droites où aucun des deux points ne sont projetés. Sur chacune des droites restantes, si les deux points sont dans des modes différents, cela contribue à les éloigner. Cela nous amène naturellement à la distance de Jaccard qui est simplement la proportion de coordonnées non nulles qui diffèrent : Il arrive bien souvent, en particulier avec des données réelles, que les clusters trouvés par l'algorithme contiennent des points venant de classes différentes. Un premier critère classique pour évaluer un clustering est la pureté. On la calcule de la façon suivante : on étiquette chaque cluster C ? Z par la classe dominante qu'il contient cldom
Complexité
La pureté PUR Y (Z) de tout le clustering Z est simplement la moyenne pondérée par la taille des clusters
Le second critère est l'information mutuelle normalisée (IMN) (voir Strehl et Ghosh (2002)
avec H(·) dénotant l'entropie discrète. Ce critère est au maximum de 1 quand les deux étique-tages sont parfaitement semblables. Si chaque classe de Y a un cluster correspondant dans Z, à part l'une d'elles divisée en deux clusters dans Z, alors l'IMN sera plus petite que 1. Ainsi, le critère pénalise un clustering qui n'a pas la même structure que la "vraie" information de classe. Ceci permet de juger à quel point les deux étiquetages ont la même structure.
Jeux de données
Nous avons pris trois jeux de données répandus pour l'évaluation des algorithmes. Le premier est CHART 1 : il s'agit de séries temporelles de contrôle. Il comporte N = 600 exemples en dimension d = 60. Il contient c = 6 sortes de séries temporelles différentes. Le second est 
Algorithmes et paramètres
Pour chacun des jeux de données, nous avons évalué la performance de quatre méthodes : -CLIP (Clustering par Lignes Inter-Points) : notre méthode ; -RP : la méthode de Topchy et al. (2003) ; -ACP+EM : mixture de gaussiennes, précédée d'une ACP gardant 85% de la variance des données ; -LAC : l'algorithme de subspace clustering de Domeniconi et al. (2007) qui est un Kmeans qui pondère les dimensions pour localiser le sous-espace particulier de chaque cluster. Pour chaque méthode, nous avons effectué le clustering en paramétrant un nombre de clusters k variable. Cela permet de mieux se rendre compte de l'efficacité de l'algorithme (voir Fern et Brodley (2003)), sachant qu'il est rare que le nombre de classes dans le jeu de données corresponde au nombre de clusters du point de vue géométrique du terme. Chaque expérience, pour un algorithme et un jeu de données fixés, a été effectuée 10 fois (avec des graines aléa-toires différentes pour CLIP et RP, et des initialisations différentes pour ACP+EM et LAC). Les tables de résultats affichent la moyenne et les écarts-types de la performance selon les deux critères de Pureté et d'Information Mutuelle Normalisée décrits plus haut. Les résultats de notre méthode CLIP ont été obtenus avec les paramètres suivants : pour CHART, nous avons pris M = 100 droites et projeté chaque point sur ses m = 10 droites les plus proches. Pour USPS, on a pris M = 400 et m = 10. Pour Coil20, on a pris M = 500 et m = 10. On a évalué la méthode RP avec le même nombre M de droites que CLIP.
Résultats
Pour chacun des jeux de données, les expériences montrent que notre approche CLIP donne des résultats sensiblement supérieurs aux autres approches. Ces bons résultats sont assez stables pour un nombre de clusters k différents, ce qui confirme la robustesse de la méthode. Les mauvais résultats de RP montrent quant à eux que des droites choisies complètement aléa-toirement et indépendamment des données ne permettent pas d'obtenir des clusterings individuels intéressants. (Nous avons également testé RP avec des droites aléatoires non contraintes à passer par l'origine, ce qui a donné des résultats légèrement moins bons que ceux de RP). L'algorithme classique de clustering par mixture de gaussiennes (précédée d'une ACP) est le deuxième meilleur après notre méthode. L'algorithme LAC vient ensuite. Les Figures 3, 4 et 5 illustrent graphiquement un clustering obtenu avec notre méthode pour chaque jeu de données.
Conclusion et perspectives
Nous avons proposé une nouvelle méthode dans le cadre des clustering ensembles. Chaque clustering individuel est réalisé à partir d'une projection partielle du jeu de données sur une droite reliant deux points de données pris au hasard. Nous avons évalué et comparé la méthode avec d'autres approches ; les expériences montrent que notre méthode donne de meilleurs ré-sultats. Les perspectives de travail futur portent sur deux aspects : le mécanisme de sélection des droites de projection pour chaque point, et le clustering global. Pour qu'un point influence les droites les plus proches de lui, on peut imaginer d'utiliser tous les points pour chaque droite, mais en pondérant l'influence de chaque point dans l'estimateur de densité en fonction de sa distance à la droite. Cela permettrait de s'affranchir du calcul des distances entre points et droites. Quant à la performance du clustering global, elle est très probablement due à la nouvelle représentation des données et non pas à la nature agglomérative de l'algorithme de synthèse des clusterings individuels. Une variante de K-means avec la distance de Jaccard devrait donner des résultats similaires pour une complexité moindre. 

Introduction
Obtenir un clustering efficace et de haute qualité sur des données de grande taille est un problème majeur pour l'extraction des connaissances. Il existe une demande de plus en plus importante pour des techniques flexibles et efficaces de clustering capables de s'adapter à des jeux de données de structure complexe. Un ensemble de données est typiquement représenté dans un tableau composé de N items (lignes) et d dimensions (colonnes). Un item représente un événement ou une observation, alors qu'une dimension peut-être un attribut ou une caractéristique de l'item. Dans un mode semi-supervisé ou supervisé, une partie ou tous les items peuvent être annotés par une classe. Les méthodes de clustering tentent de partitionner les items en groupes avec une mesure de similarité. Un ensemble de données peut être grand en termes de nombre de dimensions, nombre d'éléments, ou les deux.
L'approche classique est basée sur des algorithmes de clustering, comme les K-moyennes, le clustering spectral ou hiérarchique ainsi que leurs multiples variantes (Hastie et al., 2001). Il existe cependant plusieurs inconvénients connus à ces méthodes. Premièrement, il n'est pas toujours facile de déterminer, visualiser et valider les clusters de forme irrégulière. Plusieurs algorithmes sont efficaces pour trouver des clusters dans des formes elliptiques (donc convenant aux distributions normales multidimensionnelles), mais peuvent échouer à reconnaître des clusters de forme complexe. Deuxièmement, les algorithmes existant sont automatiques, ils excluent toute intervention de l'utilisateur dans le processus jusqu'à la fin de l'algorithme.
Il n'y a aucune manière commode d'incorporer la connaissance du domaine au sein de la phase d'analyse ou de permettre à l'utilisateur d'orienter un processus de clustering lorsque l'on utilise des algorithmes automatisés. Typiquement, l'analyse de cluster continue après la fin de l'algorithme, jusqu'à ce que les utilisateurs soient satisfaits et acceptent le résultat. Cependant, lorsque le résultat n'est pas satisfaisant, les utilisateurs veulent être étroitement impliqués dans le processus itératif de clustering et d'évaluation, en fournissant leurs impressions et intuitions.
Une alternative aux méthodes automatiques de clustering est le clustering visuel interactif (Chen et Liu, 2004;Kandogan, 2001;Seo et Shneiderman, 2002). Ici les clusters sont visualisés sur un plan 2D ou un espace 3D ; cependant, la réduction de dimension nécessaire à la visualisation n'est pas effectuée en sélectionnant les dimensions "les plus importantes", mais par le principe du "sweeping" (Kandogan, 2001). Selon ce principe, les n dimensions sont représentées par n axes disposés sur le plan 2D ou l'espace 3D. Les coordonnées cartésiennes de chaque point de données étant alors définies en fonction de la direction et de la longueur de chaque axe. Un exemple récent de clustering visuel est le système iVIBRATE (Chen et Liu, 2006) basé sur le principe des coordonnées en étoiles. Son composant principal est le rendu visuel du clustering qui aide l'utilisateur dans le processus itératif de clustering au travers d'une visualisation interactive. Il permet de produire des solutions guidées par la visualisation pour traiter efficacement les clusters de formes irrégulières. Les résultats montrent que iVIBRATE peut réellement impliquer l'utilisateur dans le processus de clustering et générer des résultats de haute qualité sur de grands ensembles de données (Chen et Liu, 2006). À la différence des méthodes automatiques, le clustering itératif dans iVIBRATE est accompli essentiellement par la manipulation des paramètres (longueur et direction des axes). Cependant le clustering manuel devient difficilement possible lorsque que les données sont de grande dimension.
Dans ce papier, nous proposons de combiner les avantages des deux approches ci-dessus, l'analyse avancée des données des méthodes de clustering automatique et la flexibilité et l'interactivité du clustering visuel. Notre idée principale est l'apprentissage semi-supervisé d'une métrique de distance permettant une projection optimale pour les systèmes de visualisation en coordonnées en étoiles. De plus, nous avons étendu le principe des coordonnées en étoile 2D en 3D grâce à une disposition sphérique des axes. Cette extension améliore grandement le rendu visuel et facilite donc le clustering.
Dans la section suivante, nous présentons le principe des coordonnées en étoiles 2D et 3D. Puis, nous décrivons l'interface Semi-Supervised Visual Clustering qui enrichie le clustering visuel de l'apprentissage semi-supervisé de la métrique de distance optimale. Nous décrivons trois modes possibles disponibles dans l'interface : manuel, automatique et hybride. Les modes manuels et automatiques sont inspirés des méthodes automatiques et du clustering visuel interactif mentionnés précédemment. Nous présentons aussi une approche hybride dans laquelle certains paramètres sont manuellement fixés par l'utilisateur tandis que les paramètres restant sont déterminés par un algorithme de distance optimale, utilisant l'ensemble des retours de l'utilisateur. Nous concluons avec une évaluation de la métrique optimale sur la collection standard UCI. données ne sont pas nécessairement orthogonaux entre eux. La valeur minimale d'une donnée pour une dimension est tracée à l'origine, et la valeur maximale est tracée à l'extrémité de cet axe. Ainsi les vecteurs unités de chacun des axes sont calculés de manière à permettre la correspondance des valeurs des données à la longueur des axes. Les systèmes de visualisation de clustering visuel (Chen et Liu, 2006;Kandogan, 2001;Seo et Shneiderman, 2002) fournissent un certain nombre de dispositifs d'interaction, dont les utilisateurs peuvent se servir pour amé-liorer leur compréhension des données. Les fonctionnalités de bases du clustering visuel sont les suivantes : Redimensionnement Le redimensionnement permet à des utilisateurs de changer la longueur d'un axe, en augmentant ou diminuant la contribution d'un attribut particulier sur la visualisation résultante. Rotation La transformation de rotation modifie la direction du vecteur unité d'un axe, rendant un attribut particulier plus ou moins corrélé avec d'autres attributs (voir figure 1). Lorsque plusieurs axes sont tournés dans une même direction, leurs contributions sont agrégées dans la visualisation. Annotation Les utilisateurs peuvent annoter des points soit par la sélection individuelle de points, soit par la sélection d'un ensemble de points au moyen d'une enveloppe convexe (voir figure 2). La couleur des points change selon l'annotation, ce qui permet de plus facilement les suivre dans la suite des transformations. Point de vue En 3D, les utilisateurs peuvent changer de point de vue et zoomer sur des données pour trouver une meilleure visualisation des clusters. Dans la suite, nous ferons principalement référence à l'extension 3D des coordonnées en étoiles.  
Q(x, y, z) est déterminé par la moyenne du vecteur somme des d vecteurs sc i · x i , où sc i sont les coordonnées sphériques qui représentent les d dimensions dans l'espace visuel 3D. Selon la transformation A, la projection 3D d'un point Q(x, y, z) est déterminée par :
, est issu des paramètres ajustables de redimensionnement, pour chacune des d dimensions. Ces ? i sont initialement fixés à 1. Les paramètres de rotation ? i et ? i sont initialement fixés à 2i?/d et peuvent être ajustés par la suite. Le point o=(x 0 , y 0 , z 0 ) fait référence au centre de l'espace de visualisation. La transformation A est une transformation linéaire avec un ensemble fixé de valeurs ?, ?, ?. Si nous fixons le centre o la transformation A ?,?,? (x 1 , . . . , x d ) peut être représentée par la transformation M x T , dans laquelle :
Elle ne casse pas les clusters dans la visualisation. Ceci étant, l'écart visuel entre des nuages de points reflète un réel écart entre les clusters dans l'espace original de grande dimension. Néanmoins, celle-ci peut-être la cause d'un chevauchement de clusters (Chen et Liu, 2006). La séparation de clusters imbriqués peut être accomplie grâce à la visualisation dynamique, au travers de la manipulation interactive. La transformation est ajustable par le redimensionnement des ? i et la rotation des ? i et ? i . Par la manipulation de ces paramètres, l'utilisateur peut voir l'influence de la ième dimension sur la distribution des clusters au travers d'une série de changements de vue, lesquels sont d'importants indices pour le clustering. La figure 1 montre l'exemple du changement de forme d'un cluster par la rotation d'un axe (en gras).
Les dimensions importantes pour le clustering vont être la cause de changements importants dans la visualisation puisque les valeurs des paramètres correspondant sont changées de manière continue. Les axes sont disposés autour du centre d'affichage et les objets graphiques sont conçus pour ajuster interactivement chaque paramètre. Malheureusement, la conception visuelle limite tout de même le nombre de dimensions qui peuvent être visualisées et manipulées. La visualisation dans des systèmes en coordonnées en étoiles comme iVIBRATE, permettent aux utilisateurs de manipuler facilement jusqu'à une cinquantaine de dimensions.
Clustering Visuel Interactif Semi-Supervisé
Lorsque les données deviennent trop difficiles à manipuler, nous proposons d'améliorer le clustering visuel interactif par l'automatisation de certaines sous-tâches, si l'utilisateur pense que cela peut l'aider. Nous avons développé l'interface Semi-Supervised Visual Clustering (SSVC) laquelle implémente à la fois les coordonnées en étoiles et leur extension sphérique pour une visualisation sur un plan 2D ou un espace 3D. L'interface SSVC offre un processus de clustering interactif au travers de la manipulation de paramètres et un processus d'optimisation basé sur le rendu visuel 2D ou 3D. Le système itère jusqu'à ce que les utilisateurs soient satisfaits. Les principaux composants et réglages du processus sont discutés ci-dessous en détails.
Sélection de dimensions. Si l'ensemble de données contient trop de dimensions, les utilisateurs peuvent vouloir préliminairement écarter les moins pertinentes. Le nombre de dimensions peut en effet influencer la facilité de manipulation des paramètres et réduire la complexité des algorithmes. Ci-dessous nous considérons trois options pour la sélection d'attributs :
Manuel. L'utilisateur peut sélectionner ou désélectionner manuellement une ou plusieurs dimensions. Mode non supervisé. L'analyse en composantes principales (ACP) (Hastie et al., 2001) peut être utilisée pour réduire la dimension des données par une approche linéaire. L'ACP ordonne les dimensions selon les valeurs propres de la matrice de covariance et permet de sélectionner une par une les d < d meilleures dimensions.
Mode semi-supervisé. Si une partie des données est déjà annotée, l'analyse discriminante de Fisher peut être utilisée pour réduire les dimensions par une approche linéaire. L'interaction basée sur l'entropie entre les attributs et les classes d'annotation (Yu et Liu, 2004) peut être une alternative non-linéaire, pour ordonner les dimensions et sélectionner les d < d meilleurs.
Manipulation et optimisation. Le processus de clustering interactif couvre essentiellement trois actions différentes. Le mode manuel est analogue aux systèmes de visualisation existant, lorsque l'utilisateur ajuste manuellement les valeurs ?, ? et ? pour modifier les clusters visibles. Le mode automatique fait référence au cas où les paramètres ?, ?, ? sont déterminés selon la métrique de distance optimale apprise dans le mode semi-supervisé à partir des annotations disponibles et des retours de l'utilisateur. Le mode hybride fait référence à divers cas intermédiaires lorsque l'utilisateur fixe quelques ? i , ? i , ? i et exige que les paramètres restant soient déterminés par une métrique de distance optimale.
Clustering semi-supervisé
Le clustering semi-supervisé suppose qu'une petite quantité de données annotées est disponible pour un meilleur clustering. Ces données souvent proviennent des retours de l'utilisateur, soit sous la forme d'annotations directes d'un item par une classe, soit d'indication "plus lé-gères" sur la similitude ou la dissimilitude de paires d'éléments. En utilisant ces indices, un meilleur clustering peut être réalisé par l'ajustement de la métrique de distance (Basu et al., 2004;Tang et al., 2007;Xing et al., 2003). Cette ajustement cherche à obtenir la vue utilisateur dans laquelle les items sont mis ensemble ou séparément. La représentation des données d'origine ne peut pas être incluse dans un espace où les clusters ne sont pas suffisamment sépa-rés. Modifier la métrique de distance transforme cette représentation de sorte que les distances entre des éléments de même cluster sont réduites au minimum, alors que les distances entre des éléments de clusters différents sont maximisées.
Cependant, à la différence de Basu et al. (2004) qui apprend la métrique de distance dans l'espace d-dimensionnel original, nous visons ici l'espace visualisé en CE3D. L'utilisateur peut alors exprimer au mieux ses impressions et intuitions car ce n'est pas l'espace d'origine qui est optimisé mais la vue qu'il en a. Nous unifions donc ainsi dans un même système le clustering semi-supervisé et le clustering visuel 3D. En d'autres termes, nous recherchons une métrique de distance de projection optimale donnée par la matrice M en (2). Dans la matrice, la projection 3D du point x avec la projection M est donnée par Q(x, y, z) = M x. Ci-dessous, nous considérons plusieurs alternatives pour la modélisation et l'évaluation de la métrique de distance de projection optimale.
Coordonnées sphérique vs analyse factorielle discriminante
Si un sous ensemble d'éléments est annoté, les coordonnées sphériques des axes pour la métrique de distance de projection optimale M peuvent-être obtenues par l'utilisation de l'analyse factorielle discriminante (Bishop, 1995) lesquelles sont une généralisation de l'analyse discriminante de Fisher à plus de 2 classes et 1 dimension projetée. Selon Bishop (1995), nous obtenons les variables discriminantes pour la projection 3D comme suit :
-Pour chaque classe nous formons la matrice de covariance V k et sa moyenne µ k . Puis nous définissons les poids de la matrice de covariance V = c k=1 N k V k , où N k est le nombre d'éléments dans la classe k, et c est le nombre totale de classes. -En utilisant la moyenne µ de l'ensemble des données et µ k , la moyenne pour chacune des classes k, nous formons la matrice
T . -Pour projeter dans l'espace 3D, nous construisons la matrice de projection optimale W 3 avec les 3 premiers vecteurs propres de V ?1 V B . Notons qu'il peut être coûteux sur un ensemble de grande dimension d'obtenir des vecteurs propres pour V ?1 V B .
Une fois la matrice de projection W 3 = {w ij }, i = 1, 2, 3, j = 1, . . . , d obtenue, nous résolvons la matrice équation M T = W 3 par la décomposition :
Si w i1 , w i2 , w i3 ne sont pas tous égaux à zéro, alors il existe une solution unique pour ? i , ? i et ? i dans (3). La solution est analogue pour la conversion des coordonnées cartésiennes aux coordonnées sphériques :
, ? i = arctan wi3 . En d'autres termes, si la matrice de projection W 3 n'a pas de colonne 0, alors il existe un ensemble unique de ?, ? et ? pour la visualisation en CE3D.
Apprendre la métrique de distance de projection
L'analyse discriminante de Fisher à partir des matrices de covariance, présentées précé-demment, fait l'hypothèse implicite que la distribution des données est multinomiale. Dans cette section, nous suivons (Xing et al., 2003;Basu et al., 2004) en ne faisant aucune hypothèse spécifique sur la distribution des données. La métrique de distance est obtenue à partir de l'ensemble des paires de similitude/dissimilitude par l'optimisation d'une fonction qui cherche à réduire les distances des éléments semblables tout en augmentant les distances entre éléments différents. Cependant, à la différence de (Xing et al., 2003;Basu et al., 2004), nous recherchons une métrique de projection de distance M qui sépare les données dans la projection 3D plutôt que dans l'espace initial. Nous définissons la métrique de distance de projection M comme la distance entre deux éléments x 1 et x 2 dans l'espace 3D projeté :
Nous supposons que nous disposons d'un ensemble S de pairs de similarité ou de dissimilarité D. Si nous possédons les classes d'annotation pour quelques items, alors les items ayant la même classe forme l'ensemble de similarité S et les items ayant des classes différentes forme l'ensemble de dissimilarité D.
Problème d'optimisation pour la métrique de distance de projection
Trouver la métrique de distance de projection peut être formulé comme un problème d'optimisation non contrainte. Nous avons deux termes sur les ensembles S et D. Ces deux termes sont le terme intra-groupe In(M ) pour toutes les paires de similitude données par l'ensemble S et le terme extra-groupe Ex(M ) pour toutes les paires de dissimilarité données par D :
Nous considérons ensuite une fonction d'optimisation J(M ) qui cherche à diminuer In(M ) et augmenter Ex(M ). Nous prenons en considération le fait que les ensembles S et D peuvent être de tailles différentes, en particulier dans le cas de l'annotation multi-classes. Ci-dessous nous considérons 4 variantes de la fonction d'optimisation J(M ) :
Dérivées partielles de J(M ). In(M ) et Ex(M ) ont tous deux des dérivées partielles. Pour développer les dérivées partielles pour le terme intra-groupe In(M ), nous représentons la matrice M comme M = {m kl }, k = 1, 2, 3, l = 1, . . . , d. Puis nous réécrivons In(M ) comme ci-dessous :
Pour obtenir les dérivées, nous utilisons la symétrie des valeurs de ? lk ? S S ln = ? nl , n, l = 1, . . . , d. Nous avons :
n=1
Les dérivées partielles pour Ex(M ) dans toutes les variantes de J(M ) sont développées de manière similaire. Nous pouvons ensuite utiliser la méthode de descente du gradient ou bien une de ses variantes pour trouver la valeur du minimum (local) J(M ) (Basu et al., 2004).
Optimisation basée sur les contraintes
L'optimisation non contrainte présentée dans la sous-section précédente semble être sensible au déséquilibre de taille des ensembles S et D. Pour remédier à ce problème, nous allons adapter l'optimisation basée sur les contraintes proposée dans Basu et al. (2004) au CE3D. Dans cette approche, la fonction J(M ) tente de minimiser le terme intra-groupe In(M ) tout en gardant le terme extra-groupe supérieur à 1 afin d'éviter la solution triviale M = 0 lorsque toutes les variables tombent à 0.
Le choix de la constante 1 est arbitraire mais n'a pas de réelle importance. La changer pour n'importe qu'elle constante m conduit à remplacer M par ? mM . En final, nous considérons une solution alternative, spécifique aux CE3D. Dans cette alternative, la fonction d'optimisation reste la même que dans (5). Néanmoins afin d'eviter les chevauchements de clusters, nous ajoutons des contraintes pour exclure les distributions de ? i qui tendent à ramener tous les axes vers une direction unique, d'où :
Où th est un seuil, 0 < th < 1.
Apprentissage partiel de la métrique de distance
Les problèmes d'optimisation discutés dans les sections précédentes font référence au mode entièrement automatique du clustering visuel interactif. Le dernier mode, hybride fait référence aux cas où les paramètres sont partiellement définis, c'est à dire lorsque quelques (? i , ? i ,? i ) sont fixés par l'utilisateur, et que l'optimisation de la métrique de distance de projection M est limitée aux axes libres. La mise en place de l'apprentissage partiel est directe. Dans toutes les méthodes d'optimisation, tous les (? i , ? i , ? i ) sélectionnés sont fixés et l'optimisation est alors réalisée sur l'ensemble des variables laissées libres.
La figure 3 montre un exemple de clustering visuel pour l'ensemble de données segment issu de la collection UCI. La figure 3.a montre le rendu visuel initial en CE3D. La figure 3.b est le résultat d'une réduction de dimensions réalisée grâce à l'analyse discriminante de 21 à 4 dimensions, suivie par l'application de la métrique de distance obtenue par l'optimisation de (9).  Il n'est pas surprenant de voir qu'en moyenne, les méthodes basées sur l'analyse discriminante se comportent mieux. Pour expliquer ce phénomène, nous avons étudié de plus près la collection elle-même ainsi que la fonction d'évaluation. En effet toutes les deux sont plus adaptées aux méthodes de clustering automatique : les groupes d'éléments dans les données ont une forme elliptique et la fonction d'évaluation favorise les résultats de clustering basés sur les centroïdes. Pour rendre l'évaluation plus correcte, nous recherchons actuellement des fonctions d'évaluation alternatives, soit directement définies sur les CE3D, soit personnalisées à partir de métriques génériques pour les contraintes sur les paires (Liu et al., 2007). Aussi nous cherchons à compléter notre évaluation sur une collection de documents techniques, qui est une tâche plus ambitieuse à cause de sa grande taille et de la forme complexe de ses clusters.
Conclusion
Nous avons décrit un système interactif dans lequel nous associons le clustering visuel dans un système 3D en coordonnées en étoiles avec le clustering semi-supervisé basé sur l'apprentissage d'une métrique de distance optimale à partir des retours de l'utilisateur. Le processus de clustering est guidé par l'utilisateur de façon intuitive et flexible ; ce dernier peut soit accomplir le clustering des données manuellement, soit le déléguer au système par l'annotation d'éléments ou l'indication de paires de similarité/dissimilarité. Différentes méthodes d'optimisation ont été comparées et une interface a été développée pour des tests en situation réelle. Il s'avère qu'associer le clustering visuel et les algorithmes automatiques offre un mécanisme puissant pour l'analyse intelligente des données complexes ou de grandes tailles.

Introduction
Dans de nombreux domaines applicatifs, l'analyste se trouve devant des jeux de données matriciels dans lesquels un certain nombre d'objets sont décrits par un certain nombre d'attributs qui prennent leurs valeurs dans un domaine numérique, éventuellement restreint au domaine 0/1. L'une des techniques phares pour l'étude exploratoire de tels jeux de données est la classification, i.e., le calcul de partitions, soit sur l'ensemble des objets, soit sur l'ensemble des attributs. On peut aussi vouloir faciliter l'interprétation des groupements calculés en dé-veloppant des méthodes de co-classification. Dans ce cas, les partionnements selon les deux dimensions sont couplés et les algorithmes comme ceux présentés dans Robardet et Feschet (2001); Dhillon et al. (2003); Ritschard et Zighed (2003); Jollois et al. (2003) produisent une bi-partition, i.e., une collection de co-clusters. Chacun des co-clusters est un groupe d'objets associé à un groupe d'attributs et la co-classification apparaît comme une méthode de classification conceptuelle. La co-classification a été particulièrement étudiée dans le contexte de l'analyse du transcriptome (voir, e.g., Cheng et Church (2000); Madeira et Oliveira (2004)). En effet, les technologies à haut débit permettent de construire des matrices d'expression de (tous les) gènes d'un organisme dans différentes situations expérimentales. Dans ce type de matrices Expériences × Gènes, un co-cluster apparaît comme un ensemble de gènes ayant des profils d'expression similaires dans un ensemble de situations expérimentales. Il est alors possible de considérer chaque co-cluster comme un module de transcription putatif, i.e., une hypothèse sur un mécanisme de régulation génique et donc une réponse (partielle) au but de l'analyse du transcriptome qui a motivé la collecte des données d'expression.
Nous nous intéressons à la pertinence des bi-partitions. Il nous semble important que les analystes puissent spécifier leurs attentes (intérêt subjectif dérivé de la connaissance du domaine) au moyen de contraintes et que les techniques de co-classification puissent produire des résultats cohérents vis-à-vis de ces spécifications. Une co-classification est alors vue comme le calcul de {? ? L | q(r, ?) est vrai} où r est une matrice, L désigne le language des bipartitions sur r, et le prédicat q spécifie des propriétés attendues sur ?. Une vision classique est que q va exprimer une contrainte d'optimisation sur une fonction objectif, e.g., la perte d'information mutuelle dans Dhillon et al. (2003). On peut également trouver d'autres contraintes comme la définition du nombre de co-clusters, le fait que certains objets (resp. attributs) doivent (resp. ne doivent pas) être ensembles (i.e., des contraintes habituellement désignées sous les noms de "must-link" et "cannot-link"). Combiner les méthodes d'optimisation sur la fonction objectif avec la satisfaction des autres contraintes est clairement difficile. L'introduction des contraintes comme "must-link" et "cannot-link" dans des processus de classification monodimensionnelle (e.g., K-Means, classification hiérarchique) a motivé de nombreux travaux ces 5 dernières années (Wagstaff et al., 2001;Klein et al., 2002;Bilenko et al., 2004;Davidson et Ravi, 2005a,b). Par contre, l'exploitation de contraintes pour la co-classification n'a été que très peu étudiée. Dans Pensa et al. (2006a,b), nous introduisions certains types de contraintes pour la co-classification. À coté des extensions de "must-link" et "cannot-link" pour qu'elles s'appliquent aux objets et/ou aux attributs, nous avons considéré le cas des dimensions ordonnées. Par exemple, dans le contexte de l'analyse de données d'expression, nous pouvons enregistrer l'évolution de l'expression des gènes au cours du temps. On peut alors spécifier des contraintes de contiguité (contrainte "Interval"). Dans nos travaux antérieurs, le calcul des bi-partitions était traité comme un post-traitement de collections de motifs qui capturent des associations localement fortes. Dans ce contexte, l'idée était de pouvoir "pousser" certaines contraintes sur la bi-partition jusque dans l'extraction des motifs locaux à post-traiter.
Nous présentons ici une approche de co-classification sous contraintes originale et alternative aux propositions décrites dans Pensa et al. (2006a,b). Ces dernières étaient réservées au traitement de données 0/1. Ici, nous travaillons sur des données numériques et sans passer par des collections de motifs locaux. Pour celà" nous étendons la technique de minimisation des résidus quadratiques proposée dans Cho et al. (2004) à un cadre de classification sous contraintes. La Section 2 formalise le problème. La Section 3 présente notre algorithme de coclassification sous contraintes. La Section 4 est dédiée à une validation expérimentale sur deux jeux de données réelles en analyse du transcriptome. La Section 5 est une brève conclusion. colonne j. Par exemple, x ij peut contenir le niveau d'expression du gène i dans la condition expérimentale j. Nous noterons x i. et x .j les vecteurs associés respectivement à la ligne i et à la colonne j. Une co-classification C k×l sur X produit simultanément un ensemble de k × l co-clusters (une partition C r en k groupes de lignes associée à une partition C c en l groupes de colonnes). Soit I r l'ensemble des indices des lignes appartenant à la classe de lignes r, et J c l'ensemble des indices des colonnes dans la classe de colonnes c. La sous-matrice de X déterminée par I r et J c est nommée co-cluster. Pour avoir un premier critère de qualité sur les co-classifications, nous cherchons toujours à optimiser une certaine fonction objectif.
ensemble de toutes les co-classifications possibles.
Des exemples de fonction objectif sont le coefficient ? de Goodman-Kruskal utilisé dans Robardet et Feschet (2001) ou la perte d'information mutuelle exploitée dans Dhillon et al. (2003). Dans cet article, nous utilisons la somme des résidus quadratiques introduite dans Cho et al. (2004). Pour des raisons de faisabilité calculatoire, les algorithmes de co-classification relaxent ces contraintes d'optimisation avec une mise en oeuvre d'heuristiques d'optimisations locales. En sus des contraintes d'optimisation qui sont souvent implicites, on veut pouvoir spécifier d'autres types de contraintes qui sont maintenant définis.
, elles ne peuvent pas être dans la même classe de
Ces formes de contraintes ont été très étudiées dans le cadre de la classification semisupervisée (Bilenko et al., 2004). Nous les généralisons ici pour qu'elles puissent s'appliquer aussi bien à l'ensemble des lignes qu'à l'ensemble des colonnes. Dans une matrice d'expression, on peut ainsi exploiter des connaissances sur les gènes et/ou sur les conditions expérimen-tales. Par exemple, si l'on sait que le gène i a et le gène i b ont la même fonction (disons F ) dans un processus biologique, on peut vouloir forcer une contrainte must-link entre ces deux gènes afin de privilégier la recherche d'un co-cluster associant des gènes ayant cette fonction F et donc d'identifier un module de transcription qui serait à l'origine de cette fonction. Nous pourrions également ajouter des contraintes cannot-link pour éviter d'associer dans les co-clusters des situations expérimentales que l'on souhaite séparer (e.g., séparer différents type de tissus ou de lignées cellulaires).
Nous faisons aussi l'hypothèse qu'une valeur réelle s c (j) (resp. s r (i)) est associée à chaque colonne j (resp. ligne i). Nous avons donc s r : {1, 2, . . . , m} ? R et s c : {1, 2, . . . , n} ? R. Par exemple, s c (j) (resp. s r (i)) pourrait être une mesure temporelle ou spatiale liée à j (resp. i). Dans des données issues de Puces ADN, où chaque colonne correspond à une puce (i.e., une expérience) et chaque ligne désigne un gène, s c (j) pourrait être le temps d'échantillonnage liée à la puce ADN j. Un second exemple serait de considérer s r (i) comme une mesure de la position (spatiale) absolue du gène i dans la séquence d'ADN de l'organisme étudié. Les deux fonctions s r et s c permettent de définir un ordre sur l'ensemble des colonnes et/ou des lignes. On dit alors que j a j b ssi s c (j a ) ? s c (j b ). Dans la suite, on considère que, si une fonction s c existe, alors tous les éléments j sont ordonnés, i.e., ?j a , j b tels que j a < j b , s c (j a ) ? s c (j b ) (idem pour les lignes). Il devient alors intéressant de rechercher des co-clusters qui soient cohérents avec les ordres définis par les fonctions s r et s c . Par exemple, si l'on s'intéresse aux différentes étapes du développement d'une cellule et si l'on veut découvrir les gènes majoritairement impliqués dans chaque étape, nous pouvons chercher des classes qui soient contiguës dans le temps. Pour cela, on peut forcer la contrainte d'intervalle introduite dans Pensa et al. (2006a,b). 
La satisfaction des contraintes must-link, cannot-link et interval entraîne en général une diminution de l'optimum théorique de la fonction objectif. Nous voulons un algorithme de coclassification qui puisse prendre en compte de telles contraintes tout en essayant d'optimiser la fonction objectif retenue. Notez que, la satisfaction d'une conjonction de contraintes c = , c = et c int n'est pas toujours faisable. Par exemple, pour trois objets i 1 , i 2 , i 3 tels que
ne peut jamais être satisfaite même si les sous-contraintes de cette conjonction ne posent aucun problème. Dans cet article, nous ferons l'hypothèse que les conjonctions de contraintes traitées par notre approche sont toujours faisables. Le problème de la faisabilité des contraintes pour les méthodes partitionnelles et hiérarchiques a été largement traité dans Davidson et Ravi (2005a,b).
Exploitation de la somme des résidus quadratiques
Nous présentons notre approche de co-classification sous contraintes en proposant un algorithme itératif qui minimise la somme des résidus quadratiques. Cette fonction objectif a été introduite dans Cho et al. (2004) pour la co-classification sans contraintes appliquée au contexte des matrices d'expression. Il s'agit d'une adaptation de la mesure introduite dans Cheng et Church (2000) pour la découverte de motifs locaux dans les matrices d'expression.
Soit X ? R m×n la matrice de données, nous cherchons une partition de X en k classes de lignes, et l classes de colonnes. Utilisons la définition de résidu de Cheng et Church (2000).
m×n la matrice des résidus calculés avec la définition précédente. La fonction objectif que l'on veut minimiser est la somme des résidus quadratiques de Cho et al. (2004) calculée de la manière suivante :
r,c i?Ir,j?Jc
On peut réécrire la matrice des résidus dans une forme plus compacte si l'on introduit les deux matrices
si i est dans la classe r (m r = |I r | étant le nombre de lignes dans la classe r), 0 autrement. Chaque élément (j, c) (1 ? c ? l) de la matrice C est égal à n ?1/2 c si j est dans la classe c (n c = |J c | étant le nombre de colonnes dans la classe c), 0 autrement. La matrice des résidus dévient alors :
La démonstration de la validité de cette équation est dans Cho et al. (2004). Les auteurs
IrJc , avant de montrer que (3) est vraie. Ils concluent que, si l'on considère la projection (I ? RR T )X de la matrice X, alors ||H|| 2 donne la fonction objectif de K-MEANS pour cette matrice modifiée.
Considérons maintenant notre contribution algorithmique. Notre approche utilise une technique dite "ping-pong" pour traiter de manière alternée (avec la méthode K-MEANS) les colonnes et les lignes. Ainsi, la matrice C n'est mise à jour qu'après que chaque colonne ait été affectée à la classe de colonnes la plus proche (similairement pour les lignes). Nous proposons donc de décomposer le calcul de la fonction objectif. Soit
T , nous obtenons la réécri-ture suivante :
De la même manière, en posant
nous obtenons comme décomposition en termes de lignes :  
L'algorithme 1, traite la co-classification en présence de conjonctions de contraintes mustlink et cannot-link (la partie concernant le traitement des lignes ( est omise). Il commence par initialiser (e.g., aléatoirement) les matrices C et R. À chaque itération, l'algorithme affecte chaque colonne (ligne) à la classe de colonnes (lignes) la plus proche qui n'introduit pas une violation des contraintes cannot-link. Si une colonne (ligne) est impliquée dans une contrainte must-link, nous affectons l'ensemble des colonnes (lignes) impliquées par la fermeture transitive de cette contrainte à la classe de colonnes (lignes) pour laquelle la moyenne des distances est minimum, en contrôlant toujours qu'il n'y ait pas de contrainte cannot-link violée par cette opération. Ensuite, l'algorithme met à jour la matrice C (R) selon le schéma d'affectation résultant des opérations décrites précédemment. Le processus est réitéré jusqu'à ce que la diminution de la fonction objectif devienne très petite (i.e., inférieure à un facteur de tolérance ? ). Notons que la phase d'initialisation peut ne pas prendre en compte les contraintes, car leur satisfaction est assurée par la première itération. Une amélioration possible consiste à utiliser un meilleur critère d'affectation pour les objets impliqués dans des contraintes cannot-link. On RNTI -X -6 sait d'ailleurs que la satisfaction d'un ensemble de contraintes cannot-link pour un nombre de classes donné est un problème NP-complet (Davidson et Ravi, 2005b).
Satisfaction de la contrainte "interval"
Algorithme 2 : IntCoClust(X,k,l)
Entrées : Une matrice de données X, k et l, int r , int c Sorties :
L'algorithme 2 permet de résoudre le problème de la satisfaction de la contrainte "interval" (la partie concernant le traitement des lignes ( est omise). Dans ce cas, l'initialisation ( des partitions concernées par cette contrainte doit produire un nombre l (resp. k) d'intervalles sur les colonnes (resp. les lignes). Ensuite, le processus d'affectation s'intéresse uniquement aux frontières entre les intervalles. Plus particulièrement, il traite itérativement d'abord la frontière gauche, puis la frontière droite. Une colonne (resp. ligne) peut être affectée à l'intervalle adjacent si la distance est inférieure à celle calculée sur l'intervalle de départ. Dans ce cas, on continue à traiter les colonnes (resp. lignes) restantes. Lorsque la frontière gauche et la frontière droite d'un intervalle correspondent à la même colonne (resp. ligne), l'algorithme passe à la frontière suivante. Dans le cas où il n'est pas nécessaire de réaffecter la colonne (resp. ligne), l'algorithme termine le traitement de cette frontière et passe à la frontière suivante. Notez que, contrairement à Pensa et al. (2006b), la satisfaction de la contrainte interval est bien garantie sur la co-classification résultat.
RNTI -X -7
Co-classification sous contraintes par la somme des résidus quadratiques L'intégration des deux algorithmes pour traiter une conjonction de contraintes must-link, cannot-link et interval n'a pas encore été traitée. Donnons cependant une piste de recherche. Il s'agit d'abord de faire en sorte que chaque ensemble M r ? M r (ou M c ? M c ) soit un intervalle. Par exemple, pour un ensemble d'objets {i 1 , i 2 , i 3 , i 4 , i 5 }, et un ensemble M r = {i 2 , i 4 }, nous serions obligés d'inclure l'objet i 3 dans M r par la définition même d'un intervalle. Ensuite, il faut que l'initialisation produise une partition qui prenne en compte l'ensemble des contraintes (notons que la satisfaction d'une conjonction de contraintes cannot-link est un problème NP-complet). Enfin, il est possible d'utiliser la stratégie de l'algorithme 1 uniquement sur les frontières, suivant le schéma présenté dans l'algorithme 2.
Complexité Concernant l'algorithme 1, notons que pour calculer (I ? RR T )X(I ? CC T ), le nombre d'opérations nécessaires est celui qu'il faut pour calculer R T XC, i.e., kn(m + l) opérations. Ce calcul peut donc être effectué dans un temps O(N ) (quand N = mn) dans l'hypothèse vraisemblable où k l << m n. La phase d'affectation des lignes et colonnes aux nouvelles classes, nécessite un temps O(N (k + l) à chaque itération. La complexité totale de l'algorithme est donc en O(N (k + l)t), où t est le nombre total d'itérations nécessaires à l'algorithme pour compléter la co-classification. La complexité de l'algorithme 2, est trivialement la même que celle de l'algorithme 1. Noter que, en général, le fait de travailler uniquement sur les frontières, se traduit par une meilleure efficacité dans la phase d'affectation.
Validation expérimentale
Nous avons étudié le comportement de nos algorithmes dans deux jeux de données "Puces ADN" nommés plasmodium et drosophila. Le premier décrit dans Bozdech et al. (2003) concerne le transcriptome du cycle de développement intraerythrocytique du Plasmodium Falciparum, i.e., un agent responsable de la malaria humaine. Les données fournissent le profil d'expression de 3 719 gènes dans 46 échantillons biologiques. Chaque échantillon correspond à un moment dans le cycle de développement : il commence avec l'invasion des globules rouges du sang par le mérozoïte, et il est divisé en trois phases : anneau, trophozoïte et schizonte (concernant respectivement le moustique, le foie, et le sang). Après 48 heures, le cellule se réplique et se divise. Aux instants marqués 17h et 29h, on observe deux transitions brusques. Le second jeu de données est décrit dans Arbeitman et al. (2002). Il concerne l'expression des gènes de la Drosophile melanogaster durant son cycle de vie. Les niveaux d'expression de 3 944 gènes sont mesurés pour 57 périodes séquentielles de temps divisés en stade embryonnaire, larvaire et pupaire.
Dans toutes nos expériences, la valeur du paramètre d'arrêt ? a été fixée à 10 ?4 ||X|| 2 . L'initialisation des partitions étant aléatoire, nous avons exécuté nos algorithmes 20 fois pour chaque groupe de contraintes.
Contraintes must-link et cannot-link
Nous avons d'abord étudié le traitement des contraintes must-link et cannot-link sur l'ensemble des gènes uniquement. Pour cela, nous avons utilisé le jeu de données plasmodium, pour lequel on dispose d'un certain nombre d'information sur les gènes impliqués dans les différentes étapes du développement. En particulier, nous avons considéré le groupe cytoplasmic RNTI -X -8 translation machinery (159 gènes), actif dans la première phase du cycle de vie de la bactérie, le groupe merozoite invasion (87 gènes) actif dans la seconde phase, et le groupe early ring transcripts (34 gènes), caractérisant la dernière phase de son développement. Tous ces groupes fonctionnels sont décrits dans Bozdech et al. (2003). Nous avons sélectionné aléatoirement 20 ensembles de contraintes sur la base des trois groupes de gènes précédemment décrits. Chaque ensemble contient un nombre variable de contraintes, et le nombre de gènes impliqués dans chaque ensemble varie entre le 20% et le 50% des gènes impliqués dans les trois groupes fonctionnels. Pour cette expérience, nous avons utilisé k = 3 and l = 3, afin de pouvoir identifier les trois étapes du développement de Plasmodium Falciparum. 
TAB. 1 -Pourcentage de gènes dans les trois classes pour l'approche sans contrainte (a) et avec contraintes (b).
Les résultats présentés dans le tableau 1 montrent, pour chaque groupe fonctionnel, le pourcentage de gènes impliqués dans chaque classe. L'amélioration est beaucoup plus sensible pour le second groupe de gènes (merozoite invasion). Le dernier groupe (early ring transcripts) ne semble pas bénéficier de l'exploitation des contraintes.
Pour mesurer l'impact de l'utilisation combinée de contraintes sur l'ensemble des lignes et l'ensemble des colonnes, nous avons choisi une co-classification cible parmi les résul-tats obtenus sans l'utilisation des contraintes. En particulier, nous avons sélectionné la coclassification avec la valeur minimale de la fonction objectif obtenue à la fin du processus itératif. Cette valeur était de 1.99×10
4 . Nous avons ensuite généré aléatoirement 20 ensembles de contraintes impliquant gènes et conditions expérimentales. Le nombre de gènes impliqués dans les contraintes varie entre 5% et 10% de la taille totale de l'ensemble de gènes. Pour les conditions expérimentales, ce nombre varie entre 15% et 25%. Pour évaluer la conformité entre la bi-partition choisie et les partitions découvertes par l'algorithme de co-classification, nous avons utilisé l'indice de Rand corrigé (Hubert et Arabie, 1985). Si C = {C 1 . . . C z } est la structure issue de la classification et que P = {P 1 . . . P z } est une partition prédéfinie, chaque paire de points peut être affecté à la même classe ou à deux classes différentes. Soit a le nombre de paires appartenant à la même classe de C et à la même classe de P, soit
RNTI -X -9 la valeur maximale de a. La conformité entre C et P peut être estimée au moyen de la formule :
Lorsque RC(C, P) = 1, les deux partitions sont identiques. Nous avons comparé les résultats obtenus avec l'utilisation des contraintes avec ceux qui ont été obtenus sans aucune spécification de contrainte. Le résumé de cette expérience est présenté dans le tableau 2. On voit que l'utilisation de contraintes produit une amélioration très nette de la conformité des deux partitions, en entraînant une légère diminution du nombre moyen d'itérations nécessaire pour compléter la co-classification. Dans le même temps, la spécification de contraintes entraîne une amélioration de la valeur finale de la fonction objectif (diminution d'environ 2%).
RC
Contrainte interval
Pour évaluer la valeur ajoutée de la contrainte interval, nous avons appliqué notre algorithme au jeu de données drosophila. Notre objectif est ici de redécouvrir les trois phases du cycle de vie de la drosophile en utilisant comme seule connaissance le nombre de classes (k = l = 3). 
TAB. 3 -Index de Rand corrigé, valeur finale de la fonction objectif et nombre d'itérations (valeurs moyennes).
Nous avons donc comparé l'index de Rand obtenu avec et sans l'utilisation de la contrainte interval pour une collection de 20 exécutions. Les résultats (voir tableau 3) montrent clairement que l'utilisation de la contrainte interval permet de retrouver de façon plus nette les trois phases du cycle de vie de la drosophile (l'amélioration mesurée sur l'index de Rand est d'environ 85%). De plus, le nombre d'itérations nécessaires pour compléter la co-classification est sensiblement inférieur à celui que l'on obtient si l'on n'utilise pas la contrainte interval. Notons que la valeur finale de la fonction objectif et meilleure dans la version non contrainte. Cela signifie que la structure découverte par notre algorithme est loin d'être l'optimum global pour ce jeu de données. Notons aussi que, dans aucun des cas, l'algorithme sans contrainte n'a pu retrouver des intervalles.
Conclusion
La co-classification permet une interprétation plus facile des groupements qu'une classification mono-dimensionnelle. Nous nous sommes posés le problème de l'exploitation de contraintes dans une co-classification. Permettre la définition de contraintes, c'est autoriser l'exploitation de connaissances du domaine pour obtenir des groupements plus pertinents. Encore faut-il être capable de combiner l'optimisation des fonctions objectifs (au coeur des algorithmes de classification) et la satisfaction de contraintes comme des contraintes must-link ou cannot-link étendues au contexte de la co-classification. Contrairement à la seule proposition de co-classification sous contraintes que nous connaissons (Pensa et al., 2006a,b), nous proposons ici une méthode qui travaille directement sur les données numériques et qui garantit le respect des contraintes spécifiées. À cette fin, nous nous appuyons sur la fonction objectif des résidus quadratiques introduite dans Cho et al. (2004). L'une des perspectives à court terme de ce travail consiste à valider les pistes identifiées pour l'intégration entre la résolution des contraintes d'intervalle et celle des autres types de contraintes.

Introduction
Dans le domaine des systèmes d'informations géographique et, plus généralement, des systèmes d'informations spatialisées, certaines recherches (Timpf, 2001, Laurini et Servigne, 2007, montrent depuis une dizaine d'années que ces applications doivent prendre en considération une variété plus large d'influences et de limitations que celles utilisées dans la cartographie conventionnelle (de nature matérielle, mais aussi de nature cognitive et sémantique), liées aux utilisateurs, aux dispositifs d'accès, à l'environnement, et de s'adapter afin d'être utilisables par leur différents utilisateurs finaux.
Ces préoccupations se sont multipliées avec l'émergence de l'informatique mobile et ubiquitaire. L'augmentation de l'autonomie des dispositifs mobiles, de leur taille mémoire et de leur puissance de calcul a donné la possibilité à la communauté de chercheurs en géomatique d'explorer de nouvelles applications des systèmes d'information géographique (Anegg, 2002. De nouvelles perspectives s'ouvrent pour les utilisateurs qui peuvent accéder à des informations spatialisées n'importe quand, n'importe où, à travers des dispositifs d'accès très variés en termes de puissance de calcul et capacité d'affichage (par exemple les services localisés et les systèmes de navigation). Cet accès accru et universel aux applications spatiales accentue les problèmes d'adaptation, liés tant aux limitations intrinsèques des dispositifs mobiles, qu'aux problèmes cognitifs soulevés par l'interaction entre l'utilisateur et ces types de dispositif dans un environnement mobile.
Initialement, les recherches se sont focalisées sur le contournement des limitations techniques des dispositifs mobiles (ou DM), comme les PDA (Personal Digital Assistant) et les smartphones (téléphones portables de dernière génération dotées d'écrans tactiles). Parmi les objectifs visés nous pouvons citer l'optimisation de l'utilisation de la bande passante dans des réseaux à bas débit (Follin et al., 2005), la généralisation du contenu pour réaliser l'adaptation à la capacité de calcul et à la capacité mémoire des DM (Lee et al., 2003), la généralisation dynamique pour adapter la présentation de cartes sur les DM à affichage réduit (Sester et Brenner, 2004).
Dans des recherches plus récentes, l'attention s'est portée vers les aspects cognitifs de l'adaptation à l'utilisateur (personnalisation) et à son contexte (sensibilité au contexte). Les travaux sur la personnalisation visent à éviter les phénomènes de fatigue et désorientation (Conklin, 1987). En transformant l'application de manière à prendre en compte le profil de l'utilisateur, il devrait avoir le sentiment que le système a été conçu spécialement pour lui. Parmi les caractéristiques des utilisateurs considérées pour l'adaptation, on trouve le plus couramment les préférences et intérêts de l'utilisateur (Malaka et Zipf, 2000), sa nationalité (Sarjakoski et al., 2002) ou son âge (Nivala et Sarjakoski, 2004).
D'autres recherches mettent en évidence l'influence que peut avoir le contexte de l'utilisateur sur l'adaptation (Zipf, 2005, Reichenbacher, 2001) en soulignant que les SIST doivent être capables de présenter à l'utilisateur une information pertinente, au moment opportun et au bon endroit, en fonction de son environnement. Plus précisément, la notion de contexte de l'utilisateur définit l'ensemble d'entités qui peuvent avoir une influence significative sur l'interaction entre l'utilisateur et l'application. Parmi les caractéristiques du contexte le plus souvent prises en compte pour l'adaptation, on peut citer la localisation (Anegg, 2002, l'activité en cours de réalisation par l'utilisateur , Petit et al., 2006, la saison (Nivala et Sarjakoski, 2004) ou encore la direction de mouvement de l'utilisateur (Roth, 2002).
Ces travaux présentent toutefois certains inconvénients. Ainsi, les applications existantes utilisent des mécanismes d'adaptation assez simplistes, qui ne tiennent pas compte des aspects plus complexes de la cartographie (Reichenbacher, 2001) comme, par exemple, les représentations multiples. De même, la dimension temporelle n'est généralement pas prise en compte, tant au niveau de la gestion de données qu'au niveau de l'adaptation (Schwinger et al., 2005). Enfin, les approches existantes n'offrent pas de support pour la conception de SIST adaptatifs. La mise en oeuvre des mécanismes d'adaptation se confond avec la logique interne de l'application, ce qui rend ces approches très difficilement réutilisables et extensibles.
En constatant ce manques d'outils et approches génériques pour l'adaptation dans les SIST, notre objectif est d'offrir aux concepteurs de SIST des outils de conception et des logiciels afin de leur faciliter les démarches de conception et de développement de SIST adaptables. Cet article présente un framework, appelé ASTIS, dédié à la conception et la -64 -RNTI-E-13 réalisation de systèmes d'information spatio-temporelle adaptatifs et interactifs. ASTIS adopte une approche de conception et de mise en oeuvre d'applications basée sur des modèles. Nous avons préalablement proposé une architecture modulaire appelée GenGHIS (Moisuc et al., 2005), séparant contenu et présentation et permettant aux concepteurs de SIST de générer des applications à partir de modèles de données conçus par eux. GenGHIS est composé de deux modules principaux : l'un assure la gestion du contenu, l'autre la gestion de la présentation, en s'appuyant sur un système de représentation de connaissances par objets spatio-temporel appelé AROM-ST (Moisuc et al., 2004 
L'architecture générale de la plate-forme
Notre approche pour la conception et le développement d'applications spatio-temporelles adaptées est basée sur la génération de code à partir de modèles. L'idée est de permettre aux concepteurs de créer des applications adaptatives en se basant sur une approche déclarative, de conception de modèles orientés objet. L'architecture est formée de composants paramétrables qui opèrent une suite de transformations et de conversions, depuis des formats et sources externes de données (aux formats relationnels les plus répandus : mif/mid, excel, mdb, etc.) jusqu'au format final, affichable à l'écran (documents SVG et XHTML avec une structure particulière). Le framework contient également des composants visuels génériques, capables d'afficher de manière interactive les documents structurés résultants du processus de transformation.
L'architecture est composée de trois modules principaux : un module de données, un module de présentation et un module d'adaptation. Le module de données est chargé du stockage, de l'interrogation et de l'analyse des données en conformité avec les modèles spécifiés par les concepteurs. Il permet aux concepteurs de décrire leurs modèles de données et de les instancier. Le module de présentation permet aux concepteurs de créer des modèles de présentation adaptés et de générer des présentations conformes à ces modèles. Il contient aussi les composants visuels nécessaires pour afficher ces présentations. Le module d'adaptation est chargé de la gestion du contexte, des utilisateurs, et de l'application dynamique des mécanismes d'adaptation choisis en fonction de ces cibles.
Le fonctionnement de l'architecture est décrit de manière simplifiée dans la FIG. 1. La partie gauche de la figure illustre la partie statique de l'architecture, c'est-à-dire les composants impliqués dans les processus de conception. La partie droite contient la partie -65 -RNTI-E-13
Conception de systèmes d'information spatio-temporelle adaptatifs avec ASTIS dynamique de l'architecture, c'est-à-dire les composants chargés de la génération à la volée et de l'affichage de contenus SVG adaptés. Les composants représentés par des rectangles à angles droits sont des composants paramètrisables via des modèles. Les composants représentés sous forme de rectangles à coins arrondis sont les composants chargés des conversions de format.
FIG. 1 Schéma du fonctionnement d'ASTIS.
Pour créer une nouvelle application, les concepteurs doivent d'abord spécifier le schéma de données de l'application. Ensuite, ils doivent spécifier différentes transformations de ce schéma afin de l'adapter aux caractéristiques des utilisateurs et du contexte. De la même manière, la troisième étape consiste à spécifier des adaptations de présentation, c'est-à-dire différentes options de style et d'affichage applicables. La dernière étape de conception permet de définir et faire la liaison entre les différentes caractéristiques du contexte de l'utilisateur et les adaptations correspondantes. Cela revient à spécifier un modèle d'adaptation comme celui de la FIG. 2. A présent, il n'existe pas un consensus sur une terminologie de l'adaptation. Nous avons adopté la terminologie de (Reichenbacher, 2003). Ainsi, la cible de l'adaptation (target) définit les caractéristiques significatives de l'utilisateur ou de son contexte auxquelles l'application doit s'adapter (par exemple, le niveau d'expertise de l'utilisateur ou sa localisation). Une composante à adapter (adaptee) représente une caractéristique du contenu ou de la présentation qui peut être ajustée en adéquation avec le contexte. Le mécanisme d'adaptation (adapter) est la fonctionnalité du système qui permet d'ajuster les caractéristiques des composantes à adapter en fonction des caractéristiques connues des cibles.
-66 -RNTI-E-13
FIG. 2 Modèle d'adaptation d'ASTIS.
Afin de finaliser la réalisation d'une nouvelle application les concepteurs doivent procéder à l'acquisition de données, c'est-à-dire peupler avec des instances la base de connaissances AROM-ST de l'application, en utilisant le composant de conversion de formats externes vers AROM-ST. Notre approche de conception s'inscrit donc dans le cadre de l'adaptativité (MacEachren, 1998), les concepteurs sont amenés à décrire des adaptations en fonction des caractéristiques de l'utilisateur et de son contexte, qui seront appliquées automatiquement, sans l'intervention des utilisateurs, au moment de l'exécution de l'application. Il est important de noter que, du point de vue technique il est possible de mettre en oeuvre de l'adaptabilité, c'est à dire de permettre aux utilisateurs de décrire et appliquer eux-mêmes ces adaptations, de manière dynamique, pendant l'exécution de l'application. Nous considérons que, pour que notre approche soit utilisable, il est nécessaire de créer de nouvelles interfaces et des manières simplifiées de contrôler les applications, parce qu'il est raisonnable d'imaginer que les utilisateurs finaux sont moins avisés en ce qui concerne la modélisation de données et de présentations spatio-temporelles. Une des perspectives de notre travail vise la mise en oeuvre d'interfaces et modalités de présentation simplifiées pour rendre notre architecture plus utilisable aussi dans une approche basée adaptabilité.
Au moment de l'exécution, le gestionnaire d'adaptation choisit, en fonction des caractéristiques concrètes du contexte, les modalités adéquates d'adaptation à appliquer, décrites par le modèle d'adaptation. Ces modalités sont appliquées au moment de la génération du code SVG de l'application, au niveau de la présentation (structure de la  
FIG. 4 Exemple de schéma de base dans une application dédiée aux risques d'éboulements.
Adaptation de la présentation
Les points de vue peuvent influencer non seulement les données à présenter aux utilisateurs, mais également la manière dont les données sont présentées. La plate-forme ASTIS permet de générer des interfaces de visualisation qui rendent compte des dimensions diverses de l'information (dimensions spatiale, temporelle et attributaire). Celles-ci sont affichées dans des fenêtres spatiales (cartes), temporelles (diagrammes temporels) ou attributaires (tableaux), dont les contenus sont synchronisés. Les mécanismes de synchronisation (ou linking, voir Buja et al., 1991)  Le modèle de présentation d'ASTIS étend le standard SLD sur deux aspects : la prise en compte des présentations temporelles et attributaires, et non pas seulement spatiales, ainsi que la prise en compte des éléments dynamiques de présentation, afin de permettre des présentations interactives. ASTIS permet de définir des visualisations complexes et multidimensionnelles (classe Vizualisation) contenant plusieurs fenêtres (classe Frame) de type spatial, temporel ou attributaire, chaque fenêtre affichant un certain nombre de couches (de type spatial, temporel ou attributaire). De nouveaux types de symboles graphiques temporels (1D, classe Temporal Symbol) et attributaires (0D, classe Plain Symbol) ont été ajoutés aux types existants en SLD (2D, classe Spatial Symbol). Nous pouvons apercevoir un exemple de visualisation multidimensionnelle en FIG. 6 à gauche, contenant une fenêtre temporelle, une fenêtre attributaire et deux fenêtres spatiales, chacune des fenêtres étant peuplée de symboles de type correspondant : symboles spatiaux (polygones colorées en aplats, cercles proportionnels, signes conventionnels, texte ancré, etc.), symboles temporels (histogrammes), symboles attributaires (texte simple).
Afin de permettre la génération de présentations dynamiques et interactives, le modèle de présentation d'ASTIS étend la notion de couche décrite en SLD en ajoutant des informations concernant les liens existants entre les différents types d'objet (classe Association Path). Par exemple, dans une application de gestion d'avalanches ( voir FIG. 7), en cliquant sur le contour d'une commune sur la carte (la commune avec un remplissage rouge) il est possible -72 -RNTI-E-13 de visualiser (toujours sur la carte) quels sont les sites avalancheux susceptibles de l'affecter (en violet), quelles sont les caractéristiques temporelles des événements avalancheux qui se sont produits sur ces sites (sur le diagramme temporel, en bas) ainsi que leurs caractéristiques attributaires (fenêtre attributaire, à gauche).
FIG. 7 Exemple de synchronisme interactif dans une application de visualisation de risques avalancheux.
FIG. 8 Exemple de visualisation pour le point de vue « touriste ».
Nous reprenons l'exemple de l'application dédiée au suivie des risques d'éboulements pour montrer comment peuvent être adaptées les présentations aux besoins de l'utilisateur pour un certain point de vue. Considérons la conception des présentations selon un point de vue « touristique » (voir la FIG. 8) et selon un point de vue « cyndinicien » (voir la FIG. 9). La visualisation pour le point de vue « cyndinicien » est conçue pour un groupe d'utilisateurs habitués aux outils de type SIST, alors que la visualisation selon le point de vue « touriste » s'adresse sans doute à des utilisateurs moins avisés.
Certaines différences de contenu entre les deux fragments de cartes sont visibles. Le point de vue « touristique » contient, par exemple, des informations sur des objectifs touristiques (musées, monuments, restaurants). Le point de vue « cyndinicien » contient plus d'informations liées à l'occupation des sols et au réseau hydrographique.
Au niveau de la structure, les deux visualisations contiennent des fenêtres attributaires, spatiales et temporelles. Pourtant, si pour le PDV « touriste » comporte plus de détails attributaires sur les points d'intérêt touristique, les tableaux attributaires pour les cyndiniciens apportent plus d'informations sur les risques mêmes. Au niveau de la fenêtre temporelle, les graphiques temporels permettent aux cyndiniciens d'avoir une perspective historique sur plusieurs décennies des phénomènes liés aux risques. La fenêtre temporelle de la visualisation dédiée aux touristes leur permet tout simplement d'accéder aux prévisions des dangers éventuels menaçant leurs points d'intérêt pour les prochains jours.
FIG. 9 Exemple de visualisation pour le point de vue « cyndinicien ».
La représentation du concept de risque est très simplifiée dans l'application dédiée aux touristes. Une zone rouge permet de mettre en évidence les différents types de zones d'éboulements qui sont à éviter par les touristes, permettant ainsi de les informer des risques existants lors de la préparation de leurs balades en montagne. Les différentes composantes d'une zone d'éboulements (pied de dépôt, niche de dépôt, niche composite, etc.) sont, au contraire, présentées en détail par des signes conventionnels familiers aux cyndiniciens.
En ce qui concerne les niveaux d'expertise, la visualisation s'adressant aux touristes est plus simple : la densité visuelle affichée (et, respectivement, la quantité totale d'information) est plus faible. Son style de présentation s'appuie sur l'utilisation de symboles visuels. Au contraire, la visualisation pour les cyndiniciens est plus dense et utilise des signes -74 -RNTI-E-13 conventionnels qui permettent d'afficher une quantité plus importante d'information, au détriment de la lisibilité.
Conclusions et perspectives
Nous avons présenté dans cet article une plate-forme pour la conception et la génération de SIST adaptables à l'utilisateur. Cette architecture implémente une approche conceptuelle de l'adaptation à l'utilisateur qui permet aux concepteurs de SIST de générer des applications adaptables en décrivant simplement des modèles conceptuels. Ces modèles doivent décrire la transformation applicable à la composante à adapter (les éléments du système qui sont transformés afin de répondre aux besoins de l'utilisateur) et la cible de l'adaptation (les caractéristiques de l'utilisateur ou de son environnement que le système doit prendre en compte pour l'adaptation). Afin d'aider les concepteurs dans la modélisation, la plate-forme offre des modèles génériques pour chaque type de cible et de composante à adapter, qu'il suffit aux concepteurs d'instancier. Cela permet aux mécanismes génériques d'adaptation implémentés au sein de notre plate-forme de réaliser l'adéquation aux types d'utilisateurs au niveau de l'application finale. Notre approche met en oeuvre tant l'adaptation du contenu (les données), que l'adaptation de la présentation (les interfaces de visualisation).
Plusieurs perspectives sont possibles, en vue d'améliorer notre proposition et de la rendre plus utile pour la conception et la réalisation d'applications pour des SIST mobiles. La première vise la réalisation d'un module d'acquisition du contexte. Ce module devrait utiliser un modèle compréhensif de contexte et devrait permettre aux applications d'acquérir un maximum de paramètres du contexte (localisation, temps, paramètres du dispositif d'accès, historique de l'utilisateur, agenda, etc.) sans aucune intervention de la part de l'utilisateur ou du concepteur. La deuxième perspective est de compléter la plateforme avec un module d'adaptation dynamique, chargé de détecter les changements de contexte et de déclencher (en utilisant un mécanisme de type ECA -Event Condition Action) les adaptations nécessaires, c'est-à-dire régénérer de manière adéquate les parties de l'application influencées par le changement de contexte.
Summary. The joint development of the Web and of ubiquitous computing increases constantly the potential diversity of the users, access devices and contexts of use of spatiotemporal information systems accessed via these means. Adapting these systems to their users becomes a necessity and a guarantee of usability and longevity. This paper presents a generic approach for the design and generation of adaptive spatio-temporal information systems. The presented framework integrates generic user adaptation mechanisms, aimed at adapting both the content and the presentation of the applications. It allows designers to integrate these mechanisms into adaptable applications. In order to define the needs and the adaptations of their applications, designers only have to create conceptual models, by instantiating generic models supplied by our framework.

Introduction
Pour faire face aux besoins des nouvelles applications (médicales, suivi de consommation, suivi des navigations sur un serveur Web, etc), de plus en plus de données sont stockées sous la forme de séquences. Pour traiter ces bases et en extraire des connaissances pertinentes, les motifs séquentiels ont été proposés Agrawal et Srikant (1995). Ils permettent, étant donnée une base de données de séquences, de trouver toutes les séquences maximales fréquentes au sens d'un support minimal défini par l'utilisateur.Si la découverte de corrélations dans les données séquentielles est primordiale pour le décideur, il n'en reste pourtant pas moins que certains problèmes ne peuvent être résolus par la recherche de tendances. De nouveaux motifs intéressent le décideur : les motifs inattendus qui contredisent les croyances acquises sur le domaine pour, par exemple, détecter des attaques sur un réseau.
Rappelons que notre objectif n'est pas de trouver les motifs rares, mais bien les motifs contredisant une connaissance, ce qui n'existe pas dans la littérature. La recherche de connaissance inattendue à partir d'une base de croyance a été introduite dans Silberschatz et Tuzhilin (1995) et Padmanabhan et Tuzhilin (2006) présentent une approche de découverte de règles d'association inattendues. Spiliopoulou (1999) propose un cadre basé sur la connaissance du domaine et des croyances pour trouver des règles séquentielles inattendues à partir de séquences fréquentes. Même si ces travaux considèrent des séquences inattendues, ils sont différents de notre problématique dans la mesure où la notion d'inattendue concerne des sé-quences fréquentes sur la base afin de trier les résultats obtenus. Notre objectif est d'extraire, à partir d'une base, toutes les séquences inattendues et d'obtenir des règles elles mêmes inattendues.
Dans cet article, nous définissons donc la notion de base de croyances et de contradiction dans le contexte des séquences. Nous parlons alors de séquence inattendue, et nous introduisons les méthodes de découverte de telles séquences. Etant donné que les motifs séquentiels traditionnels ne mettent pas en évidence des règles du type "antécédent-conséquent", nous étendons la notion de séquences inattendues à celles de règles inattendues. Pour extraire ces règles à partir d'une base de données de séquences et d'une base de croyances, nous proposons l'approche USER (Unexpected Sequence Extracted Rules). 
USER : Motifs séquentiels et règles inattendus
im . Dans un ensemble de séquences, si une séquence s n'est une sous-séquence d'aucune autre, elle est dite maximale ; sinon, si s est contenue dans s , on dit que s supporte la séquence s. Soit une séquence s = 1 I 2 . . . I k un segment g s est une sous-séquence qui contient des itemsets contigus i I i+1 . . . I i+n avec i ? 1 et i + n ? k. Le support d'une séquence est défini comme la proportion de séquences dans D qui supportent cette séquence.
Nous définissons, dans cet article, la longueur d'une séquence comme le nombre d'itemsets qu'elle contient, noté |s|. Nous considérons également la séquence vide et la concaténation de séquences. Une séquence vide est notée ?, avec s = ? ?? |s| = 0. Soient deux séquences s 1 et s 2 , la concaténation s 1 · s 2 de ces deux séquences correspond à s 1 complétée par s 2 en fin de séquence. Nous avons alors |s 1 · s 2 | = |s 1 | + |s 2 |.
Pour simplifier les notations et la lecture, dans la suite de cet article, nous utilisons les majuscules A, B, C . . . pour décrire des items, et la notation (ABC) pour désigner des itemsets. La notation désigne une séquence (A puis A et C puis B et C).
Nous notons n une contrainte sur la longueur de séquences, avec op ? {{ =, =, <, ? , >, ?} et n ? N. La notation |s | |= n signifie que la longueur de la séquence satisfait n Dans le cas où l'on a n = 0 on note * . Soit une séquence s, avec s 1 et s 2 deux sous-séquences de s, i.e. s 1 , s 2 s, telles que s 1 apparaisse avant s 2 dans s.
, avec g vérifie n Dans le cas où n = 0 (g = ?), nous notons s 1 ? s 2 le fait que s 1 soit directement suivi de s 2 dans la séquence s. Nous avons alors :
Dans le cas le moins contraint, l'écriture s 1 ? * s 2 désigne le fait que s 2 apparaît après s 1 dans s (s = s 1 · s · s 2 sans contrainte sur s ). Une croyance représente une connaissance décrite sous la forme d'une relation de causalité temporelle entre des occurrences d'éléments dans une séquence.
RNTI -G -
Dans notre approche, nous utilisons des règles pour décrire les relations de causalité entre séquences. De manière similaire à Spiliopoulou (1999), nous notons s ? la prémisse et s ? la conclusion. Ceci signifie que s ? ? s ? est satisfaite dans s, si l'occurrence de s ? s implique l'occurrence d'une sous-séquence s ? s telle que s ? · s ? s.
Dans notre approche, ce sont les experts qui définissent les croyances à prendre en compte.
Définition 1 (Croyance). Une croyance b sur une séquence est un couple (p, C) tel que : Notons qu'une séquence inattendue peut être liée à deux contraintes violées. Ainsi une séquence peut être à la fois ?-et ?-inattendue, ou être à la fois ?-et ?-inattendue.
Il est à présent intéressant de découvrir les tendances au sein de ces séquences (par exemple pour caractériser une attaque ou une fraude). Nous utilisons pour cela le cadre des motifs séquentiels. Afin de mieux identifier les segments de séquence correspondant à la partie "souche" et aux parties de contradiction, nous introduisons la notion de séquence inattendue bornée. 
L'approche USER
Nous supposons connue une base de croyances et cherchons les comportements inattendus dans une base de données de séquences. L'approche décrite ici s'articule autour de deux phases. Dans la première phase, l'algorithme USE (Unexpected Sequence Extraction) extrait toutes les séquences inattendues pour chaque type de violation de contrainte et pour chaque croyance. Dans une seconde phase, l'algorithme USR (Unexpected Sequence Rules) trouve tous les motifs séquentiels inattendus et les règles associées à partir des séquences inattendues trouvées par USE, à partir de seuils de support/confidence définis a priori.  
Conclusion
Dans cet article, nous avons introduit la problématique de la recherche de motifs séquen-tiels et règles séquentielles inattendus qui trouve de très nombreuses applications dans les bases de données réelles (détection de pannes, de fraudes, de niches commerciales, etc.) L'approche USER est proposée, décomposée en différentes étapes successives (USE/USR). for all u b do for all s a ? P 
RNTI -G -
Motifs séquentiels et règles inattendus

Résumé. Dans le contexte de la gestion de flux de données, les données entrent dans le système à leur rythme. Des mécanismes de délestage sont à mettre en place pour qu'un tel système puisse faire face aux situations où le débit des données dépasse ses capacités de traitement. Le lien entre réduction de la charge et dégradation de la qualité des résultats doit alors être quantifié. Dans cet article, nous nous plaçons dans le cas où le système est un cube de données, dont la structure est connue a priori, alimenté par un flux de données. Nous proposons un mécanisme de délestage pour les situations de surcharge et quantifions la dégradation de la qualité des résultats dans les cellules du cube. Nous exploitons l'inégalité de Hoeffding pour obtenir une borne probabiliste sur l'écart entre la valeur attendue et la valeur estimée.
La gestion de flux de données
Les avancées de l'électronique et de l'informatique enrichissent continuellement la pratique de la récolte et de la gestion des données. La constante est l'accroissement des capacités de traitement, tant au niveau de l'acquisition que du stockage et de l'accès aux données. Mais lorsque l'information doit être extraite instantanément de données récoltées continuellement, le modèle relationnel basé sur des tables atteint ses limites. C'est là qu'interviennent les flux de données.
Un flux de données est une suite de tuples ayant tous la même structure. Cette structure est représentée par un schéma, comprenant le nom des champs du tuple et leur type. La différence entre un flux et une table est le caractère ordonné des tuples. L'ordre est souvent déterminé par un champ d'agencement (typiquement la date, mais pas nécessairement). On entre dans le cadre de la gestion de flux dès lors que -les données du flux n'ont pas vocation à être stockées, -les données nécessitent un traitement immédiat, -les requêtes sont exécutées continuellement (i.e. les flux de données donnent naissance à d'autres flux de données). La gestion de flux de données repose sur un modèle "data push" : les données se présentent d'elles-mêmes, à leur propre rythme. En conséquence, le système ne maîtrise pas et ne connaît pas à l'avance le rythme et ses fluctuations. Des mécanismes d'adaptation sont à mettre en place pour faire face à toutes les situations, notamment lorsque le rythme s'accélère au point que le système ne peut plus traiter instantanément toutes les données. Dans ce cas, un mécanisme de réduction de la charge procède à une élimination des tuples en différents points du processus de traitement. On parle de mécanisme de délestage (ou : load shedding). Le système entre alors dans un mode "dégradé" et il convient de mesurer l'impact du délestage sur les critères de qualité.
Nous étudions ici un tel mécanisme dans le contexte suivant : le système est un cube de données alimenté par un flux de données. On parle à ce sujet de stream cubing (Han et al. (2005)). La structure du cube est supposée connue a priori et ne fluctue pas avec les données. Notre apport réside dans la proposition d'une mesure probabiliste de l'erreur commise sur le calcul de la valeur moyenne dans chacune des cellules du cube.
Liaison entre erreur et taux d'échantillonnage
Afin d'étudier l'impact de la réduction de la charge sur la qualité du résultat dans chaque cellule du cube, il nous faut lier taux d'échantillonnage et erreur sur le calcul de la moyenne. Nous exploitons pour cela l'inégalité de Hoeffding. Pour le confort du lecteur, nous présentons notre résultat indépendamment de son application. Notons que notre application de la borne de Hoeffding différe de celle de Babcock et al. (2004). Il s'agit ici de formuler une borne sur l'erreur d'estimation d'une moyenne à partir de données échantillonnées suivant des taux d'échantillonnage différents.
Soit N un entier et soient M 1 , . . . , M N des variables aléatoires indépendantes deux à deux. Soient B 1 , . . . , B N des variables de Bernoulli indépendantes deux à deux et indépendantes des M n . Ces variables sont réparties dans L paquets, les N l variables du l eme paquet suivant une loi de Bernoulli de paramètre p l .
Notons
Si on suppose les M n toutes de même espérance µ et b n ? a n = ? pour tout n, en notant
En utilisant l'inégalité triangulaire, l'additivité des probabilités et l'inégalité de Hoeffding par deux fois (pour les Y n et pour les M n ), on obtient les majorations
On déduit de cette inégalité que, sous les hypothèses données, on commet une erreur inférieure à ? = N ? p avec une probabilité supérieure à 1 ? ? en remplaçant
Nous proposons maintenant un mécanisme de délestage pour gérer la surcharge d'un système composé d'un cube de données alimenté par un flux de données. Nous montrons comment le résultat de la section 3 conduit à une caractérisation de la dégradation de la qualité du cube suite à l'échantillonnage de données. 
Autrement dit, on souhaite que le délai de traitement global des tuples qui seront conservés ne dépasse pas le délai d'acquisition des tuples du paquet. On obtient la probabilité d'échantillon-nage p l dans le buffer l : p l = ? l N l ? . Dès lors, chaque tuple du buffer l est indépendamment soumis à un tirage aléatoire avec probabilité p l .
Nous proposons deux stratégies pour fixer la taille des buffers : fixer la taille des buffers de manière explicite (i.e. en posant N l = N pour tout l) ou implicite (i.e. en posant ? l = ? pour tout l). La présentation qui suit est indifférente au choix de stratégie et nous adoptons arbitrairement ici la seconde.
Caractérisation de l'erreur. -Considérons une cellule c du cube à son niveau le plus fin. Cette cellule contient un ensemble de vecteurs de mesures (m 1 , . . . , m v ). Ces mesures sont relatives à un certain nombre de caractéristiques dimensionnelles, notamment la caractéris-tique temporelle : elles relèvent toutes de la même période de longueur T . Pour simplifier la description, supposons v = 1.
La valeur d'intérêt pour chaque cellule est la moyenne m 
Propriété. -Nous avons jusqu'ici considéré le niveau le plus fin du cube. On obtient plus gé-néralement une estimation de l'erreur sur toute agrégation de données contenues dans plusieurs cellules. Deux cas sont à distinguer successivement. Pour deux (ou plusieurs) cellules dont les données correspondent à une même période de longueur T , la borne se calcule à partir des nombres de tuples tombant dans le groupe de cellules avant et après échantillonnage. Pour une agrégation sur deux (ou plusieurs) périodes, la borne se calcule à partir de l'ensemble des buffers constitué au cours des différentes périodes.
Conclusion
De nombreuses applications nécessitent le traitement continuel de données qui se pré-sentent à leur propre rythme. Les systèmes ayant à gérer de tels flux de données doivent intégrer des mécanismes de gestion de la surcharge.
Dans cet article, nous avons proposé un mécanisme de délestage pour un système composé d'un cube de données alimenté par un flux de données. La nouveauté majeure consiste en l'exploitation de l'inégalité de Hoeffding pour quantifier l'erreur sur le résultat agrégé dans chaque cellule du cube.
Il reste à passer en phase expérimentale. Notamment, il faut résoudre la question de l'estimation du temps de traitement moyen d'un tuple. Plusieurs politiques existent : calculer un temps moyen absolu, considérer le temps de traitement moyen sur le paquet précédent, opérer une modélisation auto-régressive du temps de traitement, employer un modèle de régression liant le temps de traitement à différentes caractéristiques du système. Seule la pratique permettra de trancher entre ces différentes politiques. Babcock, B., S. Babu, M. Datar, R. Motwani, et J. Widom (2002). Models and issues in data stream systems. In PODS '02 : proceedings of the twenty-first ACM SIGMOD-SIGACT-SIGART symposium on principles of database systems, New York, NY, USA, pp. 1-16. ACM Press.

Introduction
Un problème important en analyse des données est la compréhension de comportements inattendus ou atypiques de groupes d'individus. Quelles sont les catégories d'individus qui gagnent de particulièrement forts salaires ou au contraire, quelles sont celles qui ont de très faibles salaires ?
Notre but est de détecter automatiquement tous les groupes d'individus ayant un comportement différent de celui de l'ensemble d'apprentissage pour une variable quantitative donnée et plus particulièrement pour les faibles et les fortes valeurs d'un intervalle déterminé par l'utilisateur. Nous recherchons donc les motifs ou conjonctions de variables dont la distribution diffère significativement de celle de l'ensemble d'apprentissage pour les faibles et fortes valeurs de l'intervalle de cette variable cible.
Un domaine d'étude proche de notre travail est l'extraction des règles d'association . Les règles d'association sont des relations entre les variables de la forme X?Y. Dans le cadre des bases de données transactionnelles, X et Y sont des items (articles) comme par exemple, cidre ou crêpes et dans le cadre des bases de données relationnelles, X et Y sont des paires d'attribut-valeur comme par exemple salaire>20K€ ou profession=?cadre?. Pour extraire des règles d'association, on doit tout d'abord rechercher les motifs (ou conjonctions de variables) ayant un support (ou taux de couverture) supérieur à une certaine valeur définie par l'utilisateur. Ces motifs vérifiant ce support minimum sont dits des motifs fréquents. Ensuite, à partir des motifs fréquents, on recherche les règles dont la confiance (ou probabilité conditionnelle) est supérieure à un autre seuil défini par l'utilisateur. Pour extraire ces groupes atypiques, nous recherchons non seulement les motifs fréquents (pour obtenir des groupes d'individus dignes d'intérêt) mais également les motifs dont le support est surprenant, c'est-à-dire étonnamment faible ou au contraire étonnamment élevé par rapport à ce qui est attendu. Cette extraction de motifs surprenants est réalisée grâce à l'adaptation d'une mesure statistique existante, l'intensité d'inclination (Guillaume, 2002). Cette mesure a également l'avantage de nous libérer de l'étape de transformation des variables quantitatives, à savoir l'étape de discrétisation suivie d'un codage disjonctif complet, puisqu'elle affecte un poids plus ou moins important aux individus répondant au critère recherché, comme par exemple les individus gagnant un fort salaire.  effectuent une telle transformation pour extraire des règles d'association quantitatives en réalisant une discrétisation automatique capable de maîtriser la perte d'information engendrée par cette transformation. Kuok et al. (1998), Zhang (1999) et Subramanyam et Goswami (2006 utilisent la technique des ensembles flous pour rechercher de telles règles. Notre approche s'apparente à la technique des ensembles flous avec l'attribution d'un poids aux individus, poids qui peut être rapproché d'un degré d'appartenance. La recherche des bons intervalles est un problème majeur pour extraire des règles pertinentes en présence de variables quantitatives. Ludl et Widmer (2000) ainsi que Bay (2001) ont montré qu'une discrétisa-tion de ces variables sans tenir compte du contexte, peut conduire à des solutions non optimales. Mehta et Parthasarathy (2005) ont donc proposé une discrétisation qui prend en compte la distribution de chacune des variables mises en jeu. D'autres auteurs ont opté pour l'optimisation des mesures : Fukuda et al. (1996) optimisent le support et la confiance alors que Brin et al. (2005)   Auman et Lindell (1999) proposent des règles qui s'appuient sur la distribution des valeurs des variables quantitatives. Ainsi, une règle est jugée intéressante pour ces auteurs si la catégorie d'individus vérifiant la prémisse a une moyenne pour la variable quantitative cible significativement différente du reste de l'ensemble d'apprentissage. Rückert et al. (2004) recherchent des règles à partir d'hyperplans et obtiennent, non plus des règles à partir des motifs (règles à partir d'hyperrectangles), mais des règles du type : si la somme pondérée d'un ensemble de variables quantitatives est supérieure à un seuil donné, alors une autre somme pondérée de variables sera plus grande qu'un autre seuil avec une confiance digne d'intérêt.
Cet article s'organise de la façon suivante. La section 2 définit la notion de groupes atypiques et la section 3 présente la mesure utilisée pour extraire de tels groupes. La section 4 expose deux critères qui vont permettre d'obtenir des groupes atypiques réellement intéres-sants, critères qui seront ensuite utilisés dans la section 5 lors de la présentation de l'algorithme pour réduire la complexité du problème. La section 6 évalue l'approche retenue sur une base de données standard : la base IPUMS du bureau de recensement américain. Nous terminons par une conclusion et des perspectives.
Groupes atypiques
Dans cette section, nous allons définir la notion de groupes atypiques, c'est-à-dire des groupes qui sont étonnamment sur-représentés ou au contraire étonnamment sous-représentés dans une zone de la variable cible définie par l'utilisateur.
Il nous a semblé intéressant de laisser à l'utilisateur la possibilité d'étudier cette variable quantitative cible dans une zone ou sur un intervalle particulier pour plusieurs raisons. 
Soit M un motif ou une conjonction de variables. Nous travaillons sur deux types de motifs : les motifs qualitatifs X, c'est-à-dire les motifs composés uniquement de variables qualitatives ayant subi un codage disjonctif complet (i.e. Dans le calcul du support d'un motif M , le même poids est attribué à chacun des individus vérifiant le motif. Nous nous intéressons aux associations ciblées ZM, dans lesquelles aucune transformation n'a été effectuée sur Z. Nous souhaitons connaître les catégories d'individus qui ont un support étonnamment élevé, ou au contraire étonnamment faible, pour les deux zones de l'intervalle de Z : les faibles et les fortes valeurs de cet intervalle. Pour cela, nous attribuons un poids aux individus, poids qui sera d'autant plus important que l'individu est en adéquation avec le critère recherché, comme par exemple ?toucher RNTI -X -un fort salaire?, donc attribution d'un poids plus important aux individus ayant de fortes valeurs pour la variable ?salaire?. Nous sommes donc amenés à définir deux nouveaux supports : le support positif qui se focalise sur les fortes valeurs de l'intervalle de Z et le support négatif qui se focalise sur les faibles valeurs de l'intervalle de Z.   Nous allons maintenant exposer la mesure utilisée pour extraire ces groupes atypiques. Pour cela, nous avons utilisé une mesure existante : l'intensité d'inclination.
? ?
? ?
Mesure d'extraction des groupes atypiques
Dans un premier temps, nous exposons la mesure existante, l'intensité d'inclination (Guillaume, 2002), et ensuite, nous allons voir comment nous l'avons utilisée afin d'extraire les groupes atypiques. 
Intensité d'inclination
Si la probabilité Pr(T ? t o ) d'avoir un nombre inférieur ou égal à t o est élevée, nous pouvons en conclure que t o n'est pas significativement faible car pouvant se produire assez fré-quemment.
Afin de mesurer la ?petitesse? de cet écart de façon croissante, l'indice ?(Z,Y) = Pr(T > t o ) est retenu. Ainsi, la faiblesse de cet écart est admissible au niveau de  Le tableau 1 résume l'utilisation de l'intensité d'inclination pour détecter les groupes atypiques positifs et négatifs.
Détection des groupes atypiques
Groupe atypique négatif
Groupe atypique positif supAbs(ZXY-,+)
TAB. 1 -Utilisation de l'intensité d'inclination pour détecter les groupes atypiques.
Groupes atypiques intéressants
négatifs) G n'est présent de façon exclusive (ou respectivement fortement absent) dans cette même zone, c'est-à-dire n'a de support supérieur (ou respectivement inférieur) à un certain seuil :
Groupe négatif :
Selon la zone étudiée, nous avons deux supports possibles : supAbs(ZM,-) pour la zone des faibles valeurs de l'intervalle cible (i.e. Après avoir défini ces deux critères qui vont nous permettre d'obtenir des groupes inté-ressants, nous allons voir comment nous les avons utilisés afin de réduire l'espace de recherche des motifs.
Algorithme
Dans cette section, nous présentons l'algorithme dans le cas de la détection des groupes atypiques intéressants positifs pour la zone des fortes valeurs de l'intervalle cible. Nous pouvons transposer sans difficulté cette recherche au cas des groupes négatifs pour toute zone de l'intervalle ainsi que pour la zone des faibles valeurs dans le cas de groupes positifs.
L'algorithme d'extraction des groupes atypiques positifs présenté en figure 1 est basé sur Apriori  et plus particulièrement sur la première partie de cet algorithme, à savoir l'extraction des motifs fréquents. Notre algorithme prend en entrée une table de données dans laquelle les variables qualitatives ont subi un codage disjonctif complet, une variable quantitative cible Z ainsi que la zone de l'intervalle d'étude [z 1 , z 2 ], le risque de première espèce, un support minimum et un support maximum, et retourne l'ensemble des groupes atypiques positifs intéressants pour la zone concernée.
Comme l'algorithme Apriori, notre algorithme effectue un parcours par niveau du treillis de l'ensemble des parties de l'ensemble des motifs de taille 1 (i.e. motifs composés d'une seule variable) et utilise l'anti-monotonie du support ainsi que la propriété 1 présentée cidessous afin d'effectuer des coupures. La complexité de notre algorithme est identique à celle de l'algorithme Apriori, c'est-à-dire linéaire en nombre de passes sur la table de données mais exponentielle par rapport au nombre de motifs de taille 1.
Cette propriété 1 utilise le critère 2 présenté dans la section 4, qui nous indique que si un motif M est très fréquent (ou respectivement très peu fréquent) pour une zone z donnée alors tout sur-motif M' conduira à l'obtention d'un groupe G' non informatif. Propriété 1.
L'algorithme commence par rechercher les groupes atypiques positifs les plus généraux c'est-à-dire les groupes dont le motif associé est composé d'une seule variable (étapes 1 à 3). L'étape 1 recherche les motifs qualitatifs fréquents. L'étape 2 recherche les groupes atypiques positifs parmi les motifs qualitatifs fréquents et les variables quantitatives. L'étape 3 détecte les motifs fortement fréquents (critère 2 de la section 4) qui seront ensuite éliminés de l'ensemble des motifs fréquents servant à générer les motifs candidats. 
FIG. 1 -Algorithme d'extraction des groupes atypiques positifs intéressants pour la zone des fortes valeurs de l'intervalle de la variable cible quantitative.
Ensuite, les étapes 4 à 7 vont extraire les sous-groupes atypiques positifs intéressants et ces quatre étapes vont être répétées pour chaque niveau k (un niveau k correspond à la recherche des motifs composés de k variables) jusqu'à ce que l'ensemble des motifs qualitatifs fréquents de niveau inférieur soit non vide (car nous ne pourrions pas générer de motifs candidats). L'étape 4 génère à la fois : (1) des motifs qualitatifs candidats (suivant le même principe que l'algorithme Apriori) à partir des motifs fréquents de niveau inférieur privé des motifs fortement présents, et (2) des motifs quantitatifs candidats composés d'une variable RNTI -X -quantitative non fortement présente et d'un motif qualitatif fréquent de niveau inférieur non fortement présent également. L'étape 5 calcule les motifs qualitatifs fréquents à partir des motifs candidats trouvés à l'étape 4. L'étape 6 extrait les groupes atypiques positifs intéres-sants à partir des motifs fréquents. L'étape 7 détecte les motifs fortement présents parmi les motifs qualitatifs afin de les supprimer de l'ensemble des motifs qualitatifs fréquents qui vont servir à générer les candidats de niveau supérieur.
L'algorithme a été implémenté en Java et intégré au logiciel libre d'extraction de connaissances WEKA (Waikato Environment for Knowledge Analysis) (Witten et Franck, 2005), logiciel développé par l'université de Waikato en Nouvelle-Zélande.
Expérimentations
Pour évaluer notre approche, la recherche des groupes atypiques a été effectuée sur la base de données IPUMS (Integrated Public Use Microdata Series), données du bureau de recensement américain. Ces données sont disponibles sur UCI KDD archive.
Cette quantitative en nous dispensant de l'étape de discrétisation des variables quantitatives. Cela élimine les erreurs liées à une discrétisation a priori et nous permet d'avoir une vue globale de l'association avec ces variables quantitatives et non plus un émiettement de la connaissance dû à cette transformation. La complexité du problème s'en trouve légèrement réduite, surtout pour les bases possédant de nombreuses variables quantitatives, puisqu'il n'y a pas multiplicité des variables. Cette extraction des groupes atypiques prend comme référence l'ensemble d'apprentissage. Il pourrait être intéressant d'introduire un autre paramètre : un ensemble de référence, ce qui nous permettrait de rechercher tous les groupes ayant un comportement identique ou différent de cet ensemble de référence.

Introduction
In the process of knowledge discovery from a raw data set, we first preprocess the data to remove noise and handle missing data fields. Then data transformation, such as the reduction of the number of variables and the discretization of attributes defined on a continuous domain, is often performed, which is later provided to a data mining algorithm. One of the most important and complex issues in data mining is related to the transformation process such as discretization which consists of converting numerical data into symbolic or discrete form. Kusiak [9] emphasized that the quality of knowledge discovery from a data set can be enhanced by discretization because many of the knowledge discovery techniques are very sensitive to size of data in terms of complexity. Thus, the choice of discretization technique has important consequences on the induction model used such as CART [2].
In addition, numerical value ranges are not easy enough for evaluation functions to handle in a nominal domain ; for example, the original versions of the popular machine learning algorithms ID3 could be used only for categorical data and Quinlan [11] had to transform continuous ones into discrete values in his C4.5 decision tree learner. Many real-world classification algorithms are hard to solve unless the continuous attributes are discretized. It is hard to determine the intervals for a discretization of numerical attributes that has an infinite number of candidates. A simple discretization procedure divides the range of a continuous variable into equal-width intervals or equal-frequency intervals. Fayyad et al. [6] suggested a class dependent algorithm which reduce the number of attributed values maintaining the relationship between the class and attribute values. Liu et al. [10] classified discretization methods from five different viewpoints : supervised vs. unsupervised, static vs dynamic, global vs local, topdown vs bottom-up, and direct vs incremental. Unsupervised methods do not make use of class information in the discretization process while supervised methods utilize it. If no class information is available, unsupervised discretization is the only method possible. Dynamic methods perform discretization of continuous values during classification process, while static methods preprocess discretization before classification process. Local methods use the local region of the instance space while global methods use the entire space. Top-down methods as FUSBIN, MDLPC and CONTRAST [5][6][7] start with one interval and split intervals in the process of discretization and are based mostly on binarization within a subset of training data. While, bottom-up methods like FUSINTER [5] and Chi-Merge [4] split completely all the continuous values of the attribute and merge intervals in the process of discretization. In this article, we focus on these two types of strategies in determining better discretization points and providing comparisons in terms of quality and prediction rates [1].
Our goal is find a way to produce better discretization points. Previously, various studies have been done to estimate the discretization points from samples. Significantly, in [1], a set of learning samples are used to approximate the best discretization points of the whole population, but also argue that the learning sample is an approximation of the whole population, so the optimal solution built on a single sample set is not necessarily the global one. This interpretation leads us to use a resampling approach [3] to determine better distributions of the discretization points, where each point has a probability to be the exact discretization point towards the whole population. By doing so, we attempt to improve on the quality of discretization and better estimation of the discretization points of the entire population, thus, treating the discretization problem in the statistical area with new results. In this paper, we show that by performing resampling using bootstrap [8] we determine a better estimate of discretization point distribution over the entire population, which is shown improving the prediction rate of the achieved discretization. Moreover, we further improve on the quality and mean prediction rate obtained from resampling by applying a discretization point selection protocol. This protocol selects the cut points according to some criteria (e.g. entropy) from the resampling bootstrap frequency point distribution obtained from resampling n times and improves further on the prediction rate. Furthermore, we compare the prediction rates of different top-down and bottom-up strategies by using resampling. In section 2, we lay out the framework for discretization and define the data sets used in our calculations. In 3, we give an illustration of our work and results by applying the methodology to an example data set and then to a much more detailed, Breiman's wave dataset [2]. We also compare several top-down and bottom-up strategy based criteria as in [1], such as Chi-merge based on ? 2 Statistical Law, FUSBIN and FUSIN-TER based on the uncertainty principle, MDLPC based on information gain and CONTRAST that takes into account the homogeneity of the classes and also the point density. In the end we conclude with observations, deductions and proposals for future work.
Definitions and Notations
Framework and Formulation : Let X(.) be an attribute value on the real line For each example ? of a learning set ?, X(?) is the value taken by the attribute X(.) at ?. The attribute C(.) is called the endogenous variable or class and is usually symbolic and if an example belongs to a class c, we have C(?) = c. We also suppose that C(?) is known for all ? of the learning sample set ?. Thus, we try to build a model, denoted by ?, such that ideally we have : C(.) = ?(X 1 (.), ..., X p (.))). The discretization of X(.) consists in splitting the domain D x of continuous attribute X(.), into k intervals I j , j = 1, ...., k, with k ? 1. We denote
.).
Prediction Rate : We measure the quality of discretization by taking into account the prediction rate, which is calculated as follows :
We denote by ? js the good prediction rate resulting from the discretization of X j obtained by applying the method q on the sample ? s or ? jt by applying on the test sample ? t .
Data Set : In this article, we use two different data sets. First, we use a small data set of 110 individuals corresponding to a two-class problem shown in figure 1. The second large data set used for comparisons and results is the Breiman's waveform dataset [2] having 4590 individuals and 21 attributes X(.), that correspond to a three class problem.
FIG. 1 -Runs R i and boundary points d j for a sample of 2 classes "x" and "o".
Results and Comparisons
Illustration using Example Data of Figure 1
Consider a data set of figure 1 of 110 individuals having two classes. We perform FUSBIN discretization with ? = 0.91 on each random and bootstrap sample of size 30 and generate 500 samples. Figure 2 gives us the discretization point distribution from 500 bootstrap and random samples. We see that the discretization achieved from bootstrap is seem to be a little more generalized and well defined over the four small intervals ; 4.5 to 6, 6.5 to 9, 12.5 to 14.5 and 22.5 to 27. While, in random sampling, the point distribution does seem to be poorly defined in a large region of values from 18 to 27. We further argue that this difference increases as the data set becomes larger which we shall see in with Breiman's data set. We also calculated the mean prediction rate i.e. Next, we improve the quality and prediction rate by introducing a notion of discretization point selection protocol. This protocol selects the discretization points from a given point frequency distribution, having higher probability of occurrence, and splits on those points if a certain criterion (e.g. entropy) is met. To illustrate, from figure 2 we see that the highest probable point is 25.5 ; so we take that point and split the population if a certain criterion (FUSBIN entropy) is met. We continue our process on the obtained splits in a top-down manner, until the criterion allows further splitting or all the points from the frequency distribution have been already chosen. We applied this protocol on both the bootstrap and random samples and it selected 6 out of 30 and 8 out of 36 discretization points from both the frequency point distributions respectively. We calculated the prediction rate as 22 for bootstrap and 19.5 for random sampling, demonstrating the better quality of discretization achieved by selection from bootstrap. We further argue that sampling gives us a lot of variation in prediction rates i.e. for bootstrap samples the prediction rate varies from 17 to 26 and thus, it is difficult to obtain a generalized estimate of the discretization points of original population. Here, our protocol achieves well defined discretization points and thus, give better estimate of the original discretization points.
Analysis and Results using Breiman's Waveform Data
For this section we use the Breiman's waves data set. We generated 100 bootstrap and random samples ? b and ? s ; s = 1,...,100 of 300 points each and ? t a test sample of 4590 points. For any ? taken from the sample, we have a vector of 21 components denoted as (X 1 (?), ..., X 2 1(?)) and a label C(?). We repeated the process described above with the waveform data set. We took each variable from the data set and generated bootstrap and random samples as above. Then, we performed FUSBIN on both the 100 bootstrap and random samples and obtained mean prediction rates of 196 and 180 respectively, showing a better performance with bootstrap sampling. Then, we applied our discretization point selection protocol on the point distribution obtained and selected the best points (using FUSBIN criterion) from both sampling methods. We found a prediction rate of 309 for points obtained from bootstrap distribution and a lesser value of 271 for random sampling showing a significant amount of improvement in the prediction rate by using resampling (or bootstrapping).
Finally we compare FUSINTER, FUSBIN, CONTRAST, MDLPC and Chi-Merge by resampling. This is done in two by two according to the following procedure ; Let u and v be the two methods to compare. First, we obtain discretization points from 100 bootstrap samples and create a frequency point distribution for each variable. Then, using our selection protocol, we select discretization points from those point distribution frequencies, by applying the criterion of the respective method (from which the initial discretization points were obtained). We then compute prediction rates ? j t of the selected discretization points from each method in relation to the whole test sample ? t . We form the difference ? uv of the two prediction rates obtained and conclude that u is better than v if ? uv is significantly superior to 0. Table 1 presents the comparison in terms of the difference of the means µ uv of prediction rates from all the variables. Positive values of µ uv indicate that the method in the row is better than the method in the column. Aside from Chi-Merge method whose results are relatively poor, all the other methods have relatively smaller differences. However, among those methods MDLPC seemed to be the best with a much lesser time complexity. FUSBIN and FUSINTER also had a smaller time complexity in comparison to CONTRAST which had a quadratic complexity which had to be taken into account when the number of examples becomes too high.
Conclusion
The learning sample is an approximation of the whole population, so the optimal discretization built on a single sample set is not necessarily the global optimal one. Resampling gives a better estimate of the discretization point distribution in terms of achieving a well-defined distribution. Applying our discretization point selection protocol on the frequency distribution achieved by resampling, significantly improves the quality of discretization and prediction rate and thus, nearing to a global optimal solution. Moreover, the same protocol when applied to the frequency point distribution of random samples, achieved much lesser improvements in the prediction rate as compared to bootstrap. We applied our protocol (after resampling) to various methods. Except for Chi-Merge, all the other methods provide small variations in terms of prediction rates. MDLPC performs the best and FUSBIN achieves the best time complexity, which is a key point when dealing with a lot of examples. As future work, we shall apply this discretization approach in the context of decision trees, to see whether it improves the global performance or not. But, at the same time carrying out this approach needs to answer some other questions such as time complexity. This may lead also to apply the potential discretization points in the context of fuzzy or soft discretization [12] in decision trees.

Introduction
La problématique de l'extraction de motifs séquentiels dans de grandes bases de données intéresse la communauté fouille de données depuis une dizaine d'années et différentes mé-thodes ont été développées pour extraire des séquences fréquentes. L'extraction de tels motifs est toutefois une tâche difficile car l'espace de recherche considéré est très grand. Afin de gérer au mieux cet espace de recherche, différentes stratégies ont été proposées. Les plus traditionnelles utilisent une approche à la Apriori Srikant et Agrawal (1996) et diffèrent principalement par les structures de données utilisées (vecteurs de bits, arbres préfixés, ...). Les approches les plus récentes considèrent, quant à elles, des projections multiples de la base de données selon le principe de pattern-growth proposé dans Pei et al. (2001) et évitent ainsi de générer des candidats. Outre ces différentes stratégies, les propositions les plus efficaces considèrent comme hypothèse que la base de données peut être chargée directement en mémoire centrale. Cependant, avec le développement des nouvelles technologies, ces dernières se trouvent de plus en plus mises en défaut dans la mesure où la quantité de données manipulées est trop volumineuse et qu'il devient irréaliste de stocker l'intégralité de la base en mémoire centrale.
Le développement des nouvelles technologies permet également de générer de très grands volumes de données issues de différentes sources : trafic TCP/IP, transactions financières, en-registrements médicaux, capteurs. Les données apparaissent alors sous la forme d'un flot (data stream) de manière continue, à un rythme rapide et éventuellement de manière infinie. L'extraction de connaissances à partir de tels flots a récemment donné lieu à de nombreux travaux de recherche qui se sont focalisés sur la découverte d'itemsets fréquents (e.g., Chi et al. (2004); Giannella et al. (2003); Manku et Motwani (2002)) en utilisant des méthodes telles que le landmark, la fenêtre glissante ou les modèles de pondérations temporelles et peu de travaux se sont intéressés à l'extraction de motifs séquentiels dans les flots de données Raissi et al. (2006). Il est vrai qu'outre l'espace de recherche, les approches traditionnelles nécessitent de faire plusieurs passes sur la base ou de stocker cette dernière en mémoire.
Le reste de l'article est organisé de la manière suivante. Les travaux liés à l'extraction de motifs séquentiels sont présentés dans la section 2. Dans la section 3, nous introduisons plus formellement le problème et les concepts préliminaires de l'extraction de motifs. La section 4 présente l'échantillonnage dans le cadre d'une base de données statique avec les résultats théo-riques sur la précision de l'échantillon et du seuil d'erreur. La section 5 étend ces résultats aux flots de données en présentant notre algorithme de maintien de synopsis. Les expérimentations sont décrites dans la section 6 et une conclusion est proposée dans la section 7.
Travaux antérieurs
Différentes approches efficaces d'extraction de séquences ont été proposées ces dernières années (e.g. PrefixSpan Pei et al. (2001), SPADE Zaki (2001), SPAM Ayres et al. (2002)). PrefixSpan est un algorithme basé sur la représentation de motifs au moyen d'un arbre préfixé qui dans son implémentation publique charge la base de données en mémoire centrale et utilise différentes projections afin d'éviter de générer des candidats. SPADE propose de transformer la base de données en une représentation verticale afin d'appliquer rapidement des opérations de jointure. Dans le cas de SPAM l'originalité réside dans la représentation des données sous la forme de bitmaps. Les expérimentations montrent que SPAM est plus efficace que PrefixSpan et SPADE sur de grandes bases de données. Cependant, ces trois algorithmes sont mis en défaut lorsqu'ils essayent de charger des bases trop volumineuses, de l'ordre de quelques gigaoctets.
Le volume de données important et croissant dans les bases de données statiques ou dans les flots de données impose de nouvelles contraintes à prendre en compte par les algorithmes d'extraction. Dans ce cas, il devient acceptable d'obtenir des réponses avec des approximations Aggarwal (2007). En d'autres termes, il devient indispensable de trouver un équilibre entre l'efficacité et la précision des résultats. De nombreuses structures de synopsis ont été développées ces dernières années comme les sketches, l'échantillonnage, les wavelets et les histogrammes. Toutes ces différentes structures possèdent les mêmes propriétés de grandes applicabilités (elles peuvent être utilisées pour répondre à divers problèmes), d'efficacité mé-moire (elles sont capables de résumer de manière significative de grandes quantité de données) et de robustesse. En outre, toutes ces méthodes ne nécessitent qu'une passe sur la base de données ce qui les rend particulièrement utiles dans le cas des flots de données. Dans les travaux que nous avons menés, nous nous sommes focalisés sur les classes d'échantillonnage par ré-servoir dans la mesure où elles sont aisées à mettre en oeuvre et garantissent un échantillon représentatif de bonne qualité. Cette approche a initialement été introduite dans Vitter (1985). Le principe est le suivant : nous maintenons un réservoir de taille n, les premiers n points dans l'ensemble des données sont stockés lors de l'étape d'initialisation. Lorsque le (t+1)
FIG. 1 -Une base de données exemple D point est traité, il remplace aléatoirement l'un des points du réservoir avec une probabilité de n t+1 . Ainsi, plus la taille de l'ensemble de données augmente plus la probabilité d'inclusion se réduit. Ceci est bien entendu un inconvénient majeur dans le cas des flots de données qui considèrent souvent que les informations les plus récentes dans le flot sont les plus pertinentes Giannella et al. (2003). Une solution pour répondre à ce problème est d'utiliser des fonctions biaisées pour réguler l'échantillon du flot de données Aggarwal (2006).
Concepts préliminaires
Dans cette section nous présentons la problématique de l'extraction des motifs séquentiels Srikant et Agrawal (1996); Srikant (1995)  
L'échantillonnage par réservoir biaisé fut introduit dans Aggarwal (2006). L'idée principale est de réguler l'introduction des points dans le réservoir et ce afin de maîtriser la fraîcheur de l'échantillon produit. En d'autres termes, la fonction de biais permet de moduler l'échantillon de manière à se focaliser sur des comportements récents ou anciens en fonction des contraintes de l'application. La fonction de biais est définie de la manière suivante : f (r, t) = e ?(t?r) où le paramètre ? correspond au taux de biais. Cette fonction est proportionnelle à p(r, t) avec r < t qui est la probabilité qu'un point introduit dans le réservoir à l'instant r soit encore présent à l'instant t. En outre, l'inclusion d'une fonction de biais exponentielle rend possible l'utilisation d'algorithmes de remplacement simples et surtout cette classe spéciale de fonctions de biais implique aussi une borne supérieure sur la taille du réservoir qui est indépendante de la longueur du flot. Pour un flot de longueur t, soit R(t) la taille maximale du réservoir qui satisfait la fonction de biais exponentielle, nous avons R(t) ? 1 ? .
Echantillonnage et base de données statique
Etant donné que l'un des facteurs clés pour l'extraction est la taille de la base considérée, l'intuition sous-jacente est que l'algorithme d'extraction pourrait être lancé sur un échantillon de la base de données originale afin d'avoir des résultats de manière plus rapide et plus facile.
La première question à laquelle nous devons répondre si nous voulons extraire des motifs à partir d'un échantillon est : à quel point l'échantillon est-il pertinent par rapport au jeu de données original ? Nous répondons à cette question en exhibant une garantie sur le taux d'erreur du support d'une séquence. Notons qu'une approche similaire a été proposée pour l'extraction d'itemsets fréquents dans Toivonen (1996). 
Posons X i,s une variable aléatoire indépendante définie telle que :
représente le nombre de clients supportant la séquence s présents dans l'échan-tillon S D . Ces clients peuvent être réécrits de la manière suivante :
Nous voulons estimer la probabilité que le taux d'erreur e(s,
Pour répondre à cette estimation, nous utilisons une méthode connue en statistiques : les inégalités de concentration (et plus précisément les inégalités de Hoeffding ;Hoeffding (1963)) qui permettent de borner la valeur réelle d'une variable aléatoire par rapport à son espérance et un terme d'erreur. Le théorème suivant exhibe une borne inférieure sur la taille de l'échantillon (ou réservoir) :
L'inégalité de Hoeffding énonce que pour n variables aléatoires indépendantes, X 1 , X 2 , . . . , X n tel que 
Motifs séquentiels, échantillonnage et flots de données
Un des problèmes majeurs qui rend la pratique de l'échantillonnage difficile sur les flots est que l'on ne sait pas à l'avance la taille du flot. Il faut donc développer des algorithmes d'échantillonnages dynamiques qui prennent en compte l'évolution et les changements dans la distribution des données transitant sur le flot. Dans cette section, nous étendons les résultats précédents et présentons un algorithme de maintien d'échantillon (biaisé ou non) de manière dynamique et qui prend en compte les différentes évolutions du flot de données. L'algorithme présenté peut être vu comme une étape de pré-traitement nécessaire afin de permettre l'extraction de séquences fréquentes. Afin que cette étape de pré-traitement soit pertinente, elle doit respecter les conditions suivantes : (i) L'échantillon doit avoir une borne inférieure sur sa taille afin de minimiser le taux d'erreur absolu en terme d'estimation du support. (ii) A cause de la nature même des séquences, les opérations d'insertions et d'enlèvements, nécessaires pour la mise à jour de l'échantillon, doivent se faire au niveau des clients, mais aussi de leurs itemsets. Pour s'en convaincre, il suffit de considérer un ensemble fini de clients avec pour chacun d'eux un grand nombre d'itemsets qui se rajoutent à chaque instant t. Cet ensemble ne peut bien sûr pas être considéré comme un échantillon ou un réservoir car il n'est pas borné et ne fait qu'augmenter avec le flot.
Dans notre modèle de flot de données, un point de données apparaissant à chaque instant t est défini comme un couple constitué d'un identifiant de client et d'une transaction.
Partant de ces contraintes et des résultats théoriques obtenus dans la section 4, nous proposons un algorithme simple de remplacement issu de l'approche de réservoir biaisé proposé dans Aggarwal (2006) qui permet de réguler l'échantillonnage des transactions des clients sur le flot grâce à une fonction de biaisage temporelle exponentielle. L'idée est la suivante : nous commençons avec un réservoir vide, de capacité maximale 1 ? (nous discutons un peu plus tard de la valeur du taux de biais ?) et chaque itemset d'un client apparaissant sur le flot est inséré de manière probabiliste dans le réservoir après une opération de lancer de pièce : soit par un remplacement des itemsets d'un client déjà présent dans le réservoir, soit par un ajout direct dans une des places encore vacantes. Comme discuté précédemment, nous devons aussi bien contrôler la taille du réservoir en terme de nombre de clients qu'en nombre d'itemsets. Cette opération de contrôle est appliquée grâce à une approche de fenêtre glissante (e.g. Aggarwal (2007), Babu et Widom (2001)) qui permet de garder uniquement les itemsets les plus récents pour un client donné dans le réservoir. Une fenêtre glissante peut être définie soit comme une fenêtre basée sur les séquences de taille k, contenant les k points les plus récents apparus sur le flot, soit comme une fenêtre basée sur un intervalle de temps de taille t contenant tous les points apparus sur le flot sur une durée de temps t. Dans notre approche nous utilisons des fenêtres glissantes basées sur des séquences afin de garder uniquement les transactions les plus récentes pour les clients présents dans l'échantillon. Ce type de fenêtre glissante permet l'extraction de séquences sur un horizon récent du flot. De plus, la fonction exponentielle de biais permet à l'utilisateur de choisir la taille de son réservoir (avec des contraintes sur l'(?, ?)-approximation) et ainsi, un échantillon représentatif du flot peut être construit et mis à jour en mémoire selon les besoins de l'application et de l'utilisateur. Le corollaire suivant, issu du théorème 1 exhibe le lien qui existe entre le taux de biais ? et les seuils d'erreurs ? et ? :
Corollaire 1 Soient ? le taux de bias, ? le seuil d'erreur et ? la probabilité maximale telle que e(s, S D ) > ?, alors :
Preuve 2 D'après Aggarwal (2006), pour un flot de taille t, supposons R(t) la taille maximale possible du réservoir qui satisfait la fonction de biaisage exponentielle, on a alors par définition : R(t) ? 1 ? . Nous considérons le réservoir entier comme l'échantillon cible pour l'extraction de motifs, on a donc : R(t) = |S D |. Par substitution dans le théorème 1, nous avons le résultat énoncé. 2
La table 2 montre quelques valeurs du taux de biaisage ? et la taille minimale du réservoir nécessaire pour la bonne approximation du support des séquences. 2. Voir si le client C i est déjà dans le réservoir, si oui, rajouter l'itemset dans la fenêtre et voir s'il y a un décalage nécessaire à faire sinon passer à l'étape 3.
3. Faire un lancer de pièce (tir aléatoire) en cas de succès remplacer un des clients déjà présent dans le réservoir de manière aléatoire (le client remplacé est alors mis dans la liste noire). En cas d'échec, rajouter le client et son itemset dans le réservoir sans rien remplacer.
La partie la plus importante dans l'algorithme est la gestion des décalages des fenêtres glissantes et la mise à jour de la liste noire. Un problème peut apparaître lors de l'étape de remplacement des clients présents dans le réservoir : nous devons détecter si un client était déjà présent dans le réservoir, car le réintroduire sans aucun test préalable rendrait les résultats de l'extraction des motifs séquentiels inconsistants avec la réalité du flot. Le problème d'inconsistance apparaît lorsqu'un client est remplacé par un autre et qu'il revient dans le réservoir à l'instant suivant. Au moment de l'extraction, ce client n'aura pas tous les itemsets qu'il aurait dû avoir dans la fenêtre glissante actuelle. Les points de certains clients doivent donc être ignorés. Mais d'un autre côté, ignorer des points sur le flot peut introduire un nouveau biais, puisque seuls les clients ayant remplacé d'autres clients seront présents dans l'échantillon. Afin de résoudre ce problème d'inconsistance entre les itemsets des différents clients nous introduisons un système de liste noire qui permet d'interdire l'échantillonnage à un certain nombre de clients indésirables. Cette liste noire n'est pas irréversible et est réactualisée à chaque glissement de la fenêtre. Notons que cet algorithme peut être très facilement implémenté. Cependant, dans la section 5 nous avons supposé implicitement que l'algorithme 1 construisait un réservoir biaisé respectant la fonction de biaisage exponentielle avec le paramètre ?. Nous allons maintenant présenter une preuve formelle de cette proposition. Comme dans Aggarwal (2006), nous allons démontrer que la politique de remplacement appliquée dans notre algorithme permet de construire un réservoir biaisé de taille
Proposition 1 L'algorithme 1 construit un réservoir biaisé respectant la fonction de biaisage temporel f (r, t) = e ??(t?r) avec ? = 1 SD .
Preuve 3 Soient K le nombre total de clients, q t le nombre de points déjà insérés dans le réservoir à l'instant t et b t la taille de la liste noire à l'instant t. Posons n = |S D |, le nombre possible de points dans l'échantillon. La probabilité que le client arrivant sur le flot soit dans la liste noire est bi K et la probabilité que le client soit déjà dans le réservoir est qi K . De plus, la probabilité que le lancer de pièce soit un succès après le test d'appartenance à la liste noire est qi n , la probabilité dans le cas d'un lancer fructueux est 1 qi donc la probabilité qu'un point dans le réservoir soit éjecté à l'instant donné i est qi 1 1 n × qi = n (la probabilité qu'un point soit encore à l'instant donné i dans le réservoir est alors (1 ? 1 n ), étape 2 de l'algorithme). De plus, sachant que r < t nous devons calculer toutes les combinaisons possibles (r, t) avec t fixé de la probabilité qu'un client inséré à l'instant r soit encore dans le réservoir à l'instant t :
Pour de grandes valeurs de n, 1 ? 1 n est approximativement égal à e ?1 . Par substitution dans l'équation 2, nous avons le résultat énoncé.  Pei et al. (2001), qui permet l'extraction des motifs séquentiels. Les tests ont été lancés sur plusieurs jeux de données synthétiques qui sont générés par le logiciel QUEST 1 . Les différents jeux de données utilisés dans ces expérimentations et leurs caractéristiques sont regroupés dans le tableau 3. Afin d'échantillonner les bases de données statiques nous avons implémenté l'algorithme d'échantillonnage par réservoir de Vitter (1985).
Expérimentations
Nous avons testé la pertinence de nos calculs théoriques pour l'échantillonnage des bases de données statiques (section 4). Nous avons utilisé dans nos expérimentations les jeux de données CL6MTR2.5SL10IT20K, CL1MTR10SL20IT10K et CL0.5MTR20SL20IT10K afin de comparer les résultats aux estimations de l'(?, ?)-approximation du théorème 1. Les résul-tats des différentes expérimentations sont listés dans les tableaux 4 et 5. Nous avons construit des échantillons de différentes tailles allant de 25000 à 500000 clients en utilisant l'approche d'échantillonnage par réservoir. Les expérimentations ont été répétées 5 fois pour chaque échantillon et les valeurs présentées sont les moyennes des résultats. La colonne Erreur dans les tableaux décrit le nombre de séquences extraites dont le support dépasse nos (?, ?)-approximations. On remarque que l'échantillonnage sur les bases de données statiques reste très précis, comme cela a été présenté dans les résultats théoriques de la section 4 et ce même avec de très petites tailles de résumés. Le temps d'extraction de motifs et l'utilisation mémoire pour ce processus est diminué de plusieurs ordres de magnitudes ce qui permet de pousser le processus global d'extraction vers des supports très bas. L'expérimentation sur les flots de données utilise le jeu de données CL1MTR2.5SL50IT10K en faisant varier différents paramètres : la valeurs de biais (?) et la taille de la fenêtre glissante. Dans ces expérimentations nous mettons en valeur l'efficacité de l'échantillonnage et montrons empiriquement que la liste noire reste bornée dans le temps. La figure 2 montre que la liste noire, qui est la garante de la consistence de l'échantillon pour le processus d'extraction de motifs reste assez limitée en terme d'espace mémoire utilisé et ce grâce aux différentes mises à jours faites à chaque décalage des fenêtres glissantes des clients présents dans le réservoir. De plus, si la taille de la fenêtre glissante est petite, la liste noire tend à être très petite aussi, ceci est du aux fréquents décalages qui permettent de mettre à jour les clients ignorés dans le réservoir. La figure 2 montre que le réservoir se remplit très vite, l'étape d'extraction de motifs séquentiels peut donc être lancé à partir de la 50 eme seconde. De plus, comme on peut le voir sur la figure 3, le réservoir reste borné et stable en terme d'occupation mémoire.
Conclusion
Dans cet article, nous nous sommes intéressés à de nouvelles techniques de résumés pour représenter des bases de données de motifs séquentiels. Nous avons montré qu'une approche basée sur des échantillons était tout à fait adaptée pour des bases de données statiques et que nous étions capables de maîtriser les taux d'erreur dans les résultats d'extraction de motifs séquentiels. A notre connaissance, ce travail est le premier à utiliser des techniques d'échan-tillonnage pour extraire des motifs séquentiels dans des bases de données. Nous avons égale-ment montré qu'une approche d'échantillonnage basée sur des réservoirs pouvait être adapté au contexte des flots de données et avons proposé un algorithme de remplacement des éléments du réservoir qui permet de réguler l'échantillonnage des transactions des clients sur le flot via une fonction de biaisage temporelle exponentielle.
Nous avons vu précédemment que les approches d'extraction de motifs sur les flots néces-sitent, pour des raisons de capacités mémoire, de supprimer des connaissances acquises préa-lablement (par exemple, une séquence n'est plus fréquente sur un petit intervalle de temps). Au travers de notre approche, il n'est plus besoin d'exécuter l'algorithme d'extraction en continu mais plutôt à la demande pour fournir au décideur toute la connaissance extraite du flot en lui garantissant une marge d'erreur. Les perspectives associées à ces travaux sont nombreuses. Dans un premier temps, nous souhaitons offrir la possibilité de stocker les évolutions des données dans le réservoir et lors des étapes d'extraction de connaissances afin de déterminer les tendances dans le flot. Pour cela, nous souhaitons étendre la notion de tilted-time window introduite dans Giannella et al. (2003) pour ne stocker que les variations de fréquences et non plus toutes les fréquences des motifs agrégées sur un intervalle de temps. Dans un second temps, nous souhaitons intégrer cette approche dans un outil complet de suivi de flots de données permettant ainsi de pouvoir effectuer non seulement de l'extraction mais aussi des requêtes sur les données du flot.

Introduction
La classification recouvrante (en anglais overlapping clustering) constitue un domaine de recherche étudié depuis les années 60 et relancé par des besoins applicatifs dans des domaines importants tels que la Recherche d'Information ou encore la Bioinformatique.
Le but recherché est alors d'extraire une collection de classes recouvrantes à partir d'une population d'individus de telle manière que : chaque individu appartienne à une ou plusieurs classes, les individus d'une même classe soient similaires, et deux individus n'appartenant pas au moins à une classe commune soient dissimilaires. Différentes directions ont été prospectées afin d'obtenir ce type de schéma de classification.
Des modèles hiérarchiques ont été proposés ; Jardine et Sibson (1971) ont permis, en introduisant les k-ultramétriques, d'envisager des structures hiérarchiques (ou pseudo-hiérarchiques) moins contraignantes que les arbres, par exemple des pyramides (Diday (1984)) ou encore des hiérarchies dites "faibles" étudiées par Bertrand et Janowitz (2003) notamment. L'un des avantages de ces modèles est de proposer une interprétation visuelle des classes et de leur organisation. En revanche, ces modèles ne permettent pas de prendre en compte la globalité des schémas de recouvrements possibles ; par exemple Bertrand et Janowitz (2003) montrent que dans une k-hiérarchie faible (le modèle hiérarchique le moins contraignant), "l'intersection de (k + 1) classes arbitraires peut être réduite à l'intersection de k de ces classes".
Les approches par partitionnement proposées ont consisté dans un premier temps à déter-miner des centres, des axes ou des représentants de classes auxquels les individus sont affectés relativement à un seuil d'appartenance. Il s'agit des travaux de Rocchio (1966) repris par Dattola (1968) et plus récemment de la méthode des k-moyennes axiales proposée par Lelu (1994). Ces approches, tout comme l'algorithme CBC (Pantel (2003)), sont motivés par le besoin de modèles spécifiques pour traiter les données textuelles mais souffrent d'un problème commun récurrent que constitue la détermination du seuil (similarité ou probabilité d'appartenance) qui décidera de l'affectation des individus aux classes extraites. Cleuziou et al. (2004) proposent alors l'algorithme POBOC pour se dégager de la contrainte du seuil ; l'affectation des individus est effectuée indépendamment d'un seuil fixé a priori, uniquement par l'étude de la distribution de leurs proximités avec l'ensemble des classes.
Les méthodologies de partitionnement mentionnées jusqu'ici s'appuient implicitement sur une hypothèse forte qui considère qu'un "bon" schéma de classification recouvrante (ou recouvrement) peut être obtenu par l'extension 1 d'un "bon" schéma de classification stricte (ou partition). D'autres pourront penser que de façon analogue, un "bon" recouvrement peut être obtenu par restriction 2 d'un "bon" schéma flou. La notion de "bon" schéma restant subjective à ce stade, Cleuziou (2007) propose un critère objectif générique de qualité d'un schéma de classification (recouvrant ou non) et montre que pour ce critère, il n'existe pas toujours une partition optimale qui, par extension, permettrait d'aboutir à un recouvrement optimal.
Les remarques précédentes nous amènent à considérer une nouvelle voie pour les mé-thodes de classification recouvrante : celle qui consiste à rechercher un "bon" schéma directement dans l'ensemble des recouvrements possibles. Cette démarche a été adoptée par Banerjee et al. (2005a) et par Cleuziou (2007) dans les algorithmes MOC et OKM respectivement. MOC (Model-Based Overlapping Clustering) peut être considéré de façon simplifiée comme une gé-néralisation de la méthode EM (Dempster et al. (1977)) pour la classification recouvrante ; cette approche que nous détaillerons s'appuie en effet sur les modèles de mélanges qui s'avèrent être très performants pour les problématiques de classification stricte. OKM (Overlapping-kMeans) est une généralisation de l'algorithme bien connu des k-moyennes (MacQueen (1967)) qui allie simplicité et rapidité pour traiter des problèmes concrets de manière efficace.
En notant que la variante classificatoire (CEM) de EM se ramène, sous certaines conditions restrictives (lois normales, variances sphériques et égales, proportions égales) à l'algorithme des k-moyennes (Celleux et Govaert (1992)), il nous a semblé indispensable d'étudier les analogies entre les deux approches recouvrantes MOC et OKM. L'objectif de cet article est alors de proposer une formulation de OKM en terme de modèles de mélanges puis de la comparer théoriquement et expérimentalement à MOC. L'article est organisé comme suit : les deux prochaines sections sont dédiées à la présen-tation des deux approches MOC et OKM respectivement ainsi qu'à la reformulation de OKM permettant une comparaison analytique des deux modélisations. La section 4 présente une discussion sur les modèles permettant d'identifier leurs principales différences et leurs consé-quences prévisibles sur les schémas de classification. Cette section sera illustrée par quelques expérimentations sur des données textuelles et biologiques et sera suivie d'une synthèse de l'étude permettant de dégager les principales pistes de réflexions à mener. Banerjee et al. (2005a) ont proposé récemment un modèle général pour le problème de classification recouvrante en utilisant les modèles de mélanges. Leur proposition s'appuie sur les modèles probabilistes relationnels ou PRM (Friedman et al. (1999)) d'une part, et sur des hypothèses de génération probabiliste des observations d'autre part. Nous décrivons l'essentiel de la méthode dans cette section en laissant le soin au lecteur de se reporter aux références citées pour en obtenir une présentation approfondie.
Le modèle MOC
Un modèle inspiré de la BioInfo
Le modèle MOC (Model-based Overlapping Clustering) peut être vu comme une instanciation du modèle PRM permettant de modéliser les relations entre des gènes, des processus et des valeurs d'expressions de ces gènes mesurées sur des puces ADN. Cette instanciation repose sur l'hypothèse que le niveau d'expression d'un gène (observé dans une certaine condition expérimentale) dépend des processus auxquels le gène participe et de leur niveau d'influence (dans cette même condition expérimentale).
FIG. 1 -Instanciation du modèle PRM.
La figure 1 illustre l'instanciation du modèle PRM permettant de modéliser le fait que l'expression X ij d'un gène X i dans une condition j dépend :
-des niveaux d'influence {A hj } h des processus {A h } (dans la condition j), -de la participation M ih (ou non) du gène i à un processus A h . Dans cet exemple on dispose de 2 gènes (données), 2 conditions (dimensions), 3 processus (classes).
Sous certaines hypothèses de distribution des observations {X ij }, la détermination des paramètres {A hj } et {M ih } s'apparente à un problème de classification où les processus s'identifient aux classes ; plus précisément à un problème de classification recouvrante puisque un gène peut participer naturellement à plusieurs processus différents (donc appartenir à plusieurs classes).
Hypothèses de distribution et modèles associés
Un premier modèle classique consiste à faire l'hypothèse que les observations X ij suivent des lois normales, de variances constantes ?. Selon le modèle général présenté précédemment, pour un nombre fixé de processus, les moyennes des gaussiennes associées sont déterminées par la somme des activités A hj des processus auxquels X i participe :
Le problème de classification sera alors résolu par la recherche des paramètres M et A maximisant la vraisemblance du modèle. Segal et al. (2003) montrent que sous certaines condi-
, le problème revient alors à minimiser l'expression (2) ci-dessous par un algorithme du type EM (Dempster et al. (1977)).
Banerjee et al. (2005a) généralisent ce modèle aux familles de distributions exponentielles en s'appuyant sur le fait qu'il existe une bijection entre les distributions exponentielles et les divergences de Bregman (Banerjee et al. (2005b)). Ainsi, quelque soit la loi exponentielle considérée, la distribution des observations peut s'exprimer par
avec d ? la divergence de Bregman associée à la densité exponentielle choisie, en particulier : -la distance euclidienne (élevée au carré) pour des densités Gaussiennes, -la I-divergence permettant d'explorer pour chaque M i un sous-ensemble des 2 k ?1 vecteurs M i possibles pour retenir la solution minimisant le critère ou à défaut conserver les anciennes appartenances.
La mise à jour de A peut être réalisée dans un cas général (pour toute divergence de Bregman) au moyen d'un algorithme de descente de gradient de la forme :
avec ? la fonction identifiant la divergence de Bregman utilisée et ? un coefficient d'apprentissage fixé. Dans les cas particulier de divergences simples qui nous intéresseront plus particulièrement dans les expérimentations à venir, le problème de minimisation peut être ré-solu plus directement.
-pour la distance euclidienne il s'agit d'une minimisation de type moindres carrés résolue par (6) 
-pour la I-divergence, les auteurs s'appuient sur des techniques de factorisation de matrices non-négatives pour aboutir à la règle de mise à jour suivante
Dans cette dernière variante, on peut observer que la règle de mise à jour proposée correspond à une simplification de la règle plus générale (8) réétudiée récemment par Finesso et Spreij (2006) :
Dans le cas particulier où chaque individu X i n'appartient qu'à une seule classe A h alors (M A) ij = M ih A hj ; en ajoutant à cela le fait que la I-divergence mesure l'écart entre deux distributions p et q telles que j p j = j q j = 1 la simplification (7) devient en effet possible ( j X ij = 1). Cependant la méthode MOC s'intéressant justement aux cas où chaque individu peut appartenir à plusieurs classes, cette simplification devient fausse ; c'est la raison pour laquelle nous proposerons de conserver la règle de mise à jour originelle (8) afin d'assurer la décroissance du critère (4).
Les trois étapes de mises à jour que nous venons de présenter permettent d'assurer la dé-croissance du critère (4) et par conséquent d'améliorer à chaque itération (et après chaque étape de mise à jour) la vraisemblance du modèle probabiliste. L'initialisation du modèle (matrices M et A) est effectuée au moyen d'une étape de partitionnement (k-moyennes). Enfin, de façon assez classique l'algorithme MOC itère le processus de mises à jour un nombre maximum de fois ou jusqu'à observer une variation epsilonesque du critère objectif.
Le modèle OKM
L'algorithme des k-moyennes présente un modèle théorique simple et intuitif, facilement appréhendé par les praticiens de domaines d'application variés qui, de surcroît, apprécient gé-néralement l'efficacité de cette méthode en terme de rapidité et de qualité des classes obtenues.
L'approche OKM, proposée par Cleuziou (2007), est le résultat d'une volonté de répondre de manière pragmatique aux besoins applicatifs actuels, en proposant d'étendre l'algorithme des k-moyennes à la recherche de recouvrements plutôt que de partitions.
Critère objectif et heuristique d'optimisation
Le critère des moindres carrés sur lequel repose l'algorithme des k-moyennes est une formalisation fidèle de l'objectif visé par les méthodes de partitionnement à savoir faire en sorte que deux individus d'une même classe soient similaires et deux individus de classes différentes soient dissimilaires. Comme nous l'avons mentionné en introduction, on peut assez naturellement considérer qu'un bon recouvrement sera caractérisé par :
-des individus similaires lorsqu'ils appartiennent plutôt aux mêmes classes, -des individus dissimilaires lorsqu'ils appartiennent plutôt à des classes différentes. Le critère utilisé dans OKM pour formaliser les caractéristiques précédentes introduit la notion d'image (que l'on notera ?(X i )) d'un individu X i dans une classification I 1 , . . . , I k . L'image de X i correspond à un point, dans l'espace de représentation initial, et représentatif des classes auxquelles X i appartient. Par exemple, en reprenant les notations utilisées précé-demment, on pourra définir l'image de X i dans un espace euclidien (R m , d) par
Dans (9), ? j (X i ) désigne la j ième composante de l'image, M ih ? {0, 1} l'appartenance de X i à la classe I h et A h correspond ici à la position du centre de la classe I h . Le critère objectif que l'on cherchera à minimiser, évalue la qualité d'un recouvrement par la variance entre les individus et leur image dans la classification :
i Pour ?(.) bien choisie, on peut noter que ce critère se ramène exactement au critère des moindres carrés lorsque l'on oblige chaque individu à n'appartenir qu'à une seule classe ( h M i,h = 1). L'heuristique de minimisation du critère objectif (10) dans OKM s'apparente à l'algorithme des k-moyennes. Après une étape d'initialisation aléatoire des centres de classes A, deux étapes de mises à jour (de M puis de A) sont itérées jusqu'à la vérification d'un critère d'arrêt portant sur le nombre d'itérations ou la variation du critère objectif. Mise à jour de M . Pour chaque individu X i , la mise à jour de M i est réalisée en considérant qu'un individu ne doit appartenir qu'aux classes dont il est le plus proche au sens de la métrique choisie. Ce principe guide la construction du nouveau vecteur d'appartenance : initialisation avec affectation au plus proche centre de classe puis ajout de nouvelles affectations dans l'ordre de proximité des centres {A h } h avec X i tant que le critère objectif s'en trouve amélioré ; le nouveau vecteur ainsi obtenu ne remplaçant le vecteur M i initial que s'il permet d'améliorer le critère objectif. Mise à jour de A. Cleuziou (2007) montre que pour la distance euclidienne on peut définir localement le nouveau centre A h de la classe I h de façon optimale au sens du critère objectif. 
Reformulation de OKM
En reprenant l'inspiration bioinformatique du modèle MOC, on considère qu'une observation X ij est le résultat d'une certaine combinaison des processus auxquels l'individu X i participe. Plutôt que de choisir comme combinaison l'addition des processus, nous en choisissons la moyenne. Ainsi sous l'hypothèse d'une distribution gaussienne (de variance constante ?) des valeurs X ij nous obtenons
avec µ un vecteur totalement défini par M tel que µ i = 1/ h M ih . Dans un processus de classification, l'objectif consiste par exemple à rechercher les paramètres M (matrice binaire) et A (matrice réelle) maximisant la (log-)vraisemblance des paramètres étant données les observations : log L(M, A|X) = log p(X|M, A). Le modèle MOC fait l'hypothèse qu'il y a indépendance des observations X ij conditionnellement aux M i et A .j . Sous ces mêmes hypothèses, la vraisemblance du modèle peut se décomposer ainsi
En considérant à présent la log-vraisemblance et en introduisant l'hypothèse de distribution gaussienne (11), on note que
En observant que µ i (M A) ij = ?(X i ) avec ?(X i ) l'image de X i telle que définie en (9), on montre que minimiser le terme ||X ? µ (10)). Nous avons donc démontré que l'approche OKM, dans sa version initiale utilisant la distance euclidienne, peut être réécrite comme un modèle de mélanges recouvrant faisant l'hypothèse que chacune des observations suit une lois normale dont la moyenne correspond à la moyenne des processus (ou classes) auxquelles l'individu participe.
Discussion et analyse comparative des modèles
Nous mènerons la discussion en deux temps : nous relèverons dans un premier temps les différences majeures et les limites théoriques des deux modèles MOC et OKM ; dans un second temps nous proposerons une étude comparative expérimentale des deux approches.
Discussion sur les modèles
Nous avons choisi de comparer analytiquement les deux modèles en les exprimant tous les deux en terme de modèles de mélanges recouvrants, plutôt que simplement comme des techniques de réallocation dynamique minimisant un critère d'inertie. On peut ainsi envisager plus facilement d'extraire ultérieurement des classes de formes, volumes et orientations variées. Cependant en l'état actuel des modèles, ces variations ne sont pas permises (hypothèse des variances toutes identiques) et les deux formalismes sont strictement équivalents.
Ce qui différencie les modèles MOC et OKM concerne la méthode de combinaison des "processus" déterminant les paramètres de la distribution d'une observation : pour une distribution exponentielle, c'est la moyenne de la distribution qui résulte de cette combinaison.
-Le modèle MOC propose une combinaison par addition, justifiée par le modèle bioinformatique sous-jacent qui suppose que l'expression observée d'un gène est le résultat de l"'addition" des processus dans lesquels ce gène intervient. -Le modèle OKM utilise la notion d'image qui correspond effectivement à une combinaison des représentants des classes auxquelles l'individu appartient. La définition de l'image dépend de la métrique considérée et s'exprime comme une moyenne plutôt qu'une somme : moyenne arithmétique ou quadratique par exemple (cf. tableau 1)). Ce choix de combinaison n'est pas anodin et peut avoir des conséquences notables sur la validité théorique du modèle d'une part, sur sa sensibilité aux données d'autre part.
La validité théorique du modèle peut être remise en cause lorsque la combinaison utilisée n'est pas un endomorphisme car le modèle implique de considérer la distance entre chaque individu et la combinaison associée. Par exemple, la I-divergence permet de comparer deux
La combinaison peut également jouer un rôle important dans la sensibilité du modèle, notamment dans la répartition des données pour certaines hypothèses de lois de mélange. Prenons un exemple jouet composé de quatre individus décrits dans R {X 1 = (1.0), X 2 = (4.0), X 3 = (5.0), X 6 = (6.0)} avec les hypothèses suivantes : distributions gaussiennes des observations résultant de deux processus/classes (k = 2). La figure 2 illustre la configuration des individus à classer et, pour chacune des deux approches MOC et OKM, la projection des paramètres A dans le même espace de description après optimisation.
FIG. 2 -Observation du paramètre
La classification finale retournée par OKM s'obtient simplement en affectant chaque individu X i à I 1 , I 2 ou aux deux classes selon que l'individu est plus proche de A 1 , A 2 ou (A 1 + A 2 )/2 respectivement ; sur l'exemple OKM retournera donc les classes I 1 = {X 1 , X 2 } et I 2 = {X 2 , X 3 , X 4 }. Dans l'approche MOC on obtient cette fois la classification finale en comparant les distances de X i avec A 1 , A 2 et A 1 + A 2 , aboutissant ainsi aux classes
Il suffirait sur cet exemple de recentrer les individus en zéro tout en conservant les distances entre individus inchangées pour obtenir avec MOC les mêmes classes que pour OKM.
Cet exemple nous a donc permis de mettre en évidence que MOC peut être sensible aux translations de données, en particulier sous des hypothèses de distributions gaussiennes observable, on remarque que l'intersection des deux classes (points violets) est cohérente sur les données initiales (figure 3) et de nouveau injustifiées sur les données translatées (figure 4). Il est donc important de préciser qu'en pratique, MOC produit des résultats intéressants sur des 5 On peut montrer que cette sensibilité est annihilée pour d'autres types de distributions telles que la I-divergence grâce aux contraintes vérifiées par les individus (e.g. P j X ij = 1). 6 Cette expérience a été réalisée dans le cadre du projet ANR/ARA Masse de données "Genomic data to Graph Structure" : http ://gd2gs.ibisc.univ-evry.fr/ données d'expressions de gènes, précisément parceque ces données sont par nature centrées globalement autour de zéro 7 .
Comparaisons expérimentales
Pour achever la comparaison des deux approches de classification recouvrante étudiées, nous présentons les résultats obtenus par MOC et OKM sur une expérience de classification de documents textuels qui correspond à un domaine d'applications cible. L'étude est menée sur un sous-ensemble des documents du corpus Reuters, utilisé et présenté de façon détaillée par Cleuziou (2007). Ce corpus présente l'intérêt que chaque document possède une ou plusieurs étiquettes de classes. Nous n'utilisons pas cette information dans le processus de classification mais seulement pour évaluer la qualité des schémas de classification recouvrants générés par les méthodes. L'évaluation opérée consiste à mesurer l'écart entre les associations de documents connues (pré-étiquetage) et les associations effectivement retrouvées dans la classification, en utilisant les indices classiques de précision, rappel et F-mesure Nous présentons les résultats de l'évaluation des classifications en utilisant la I-divergence d'une part (tableau 2) et des distributions gaussiennes ou métrique euclidienne d'autre part (tableau 3). Les valeurs reportées dans les tableaux correspondent à des moyennes de 10 exé-cutions de chaque méthode dans des conditions initiales identiques.
Tout d'abord, nous retrouvons un phénomène connu en Recherche d'Information qui est que la I-divergence donne de meilleurs résultats que la métrique euclidienne pour la classification de documents. Ceci est notamment dû au fait que la I-divergence compare des distributions de mots plutôt que des vecteurs de fréquences, réduisant ainsi les effets liés aux variations de tailles entre documents. Le second résultat attendu est de constater que la précision augmente et que le rappel diminue quand on augmente le nombre de classes ; ceci s'explique par le fait que lorsque le nombre de classes augmente, le nombre de paires de documents associés diminue automatiquement.
Enfin, en comparant les résultats obtenus par les approches MOC et OKM on observe que pour les deux métriques utilisées, OKM génère d'avantage de recouvrements que MOC 9 , ce qui se traduit par un taux de rappel plus élevé sans pour autant entraîner un fléchissement de la précision (qui reste d'ailleurs plutôt à l'avantage de OKM). Il semblerait donc, au regard de la globalité des résultats de cette expérience que : les deux approches de classification recouvrante étudiées permettent de générer des recouvrements pertinents (comparaison avec l'algorithme des k-moyennes) et que le gain obtenu soit plus net dans le cas de l'approche OKM notamment en utilisant la I-divergence. Cette dernière remarque corrobore les limites théoriques énoncées précédemment concernant le modèle MOC, et en particulier sous les hypothèses de distributions gaussiennes.
Conclusion et prespectives
Dans cet article nous avons étudié deux approches de classification recouvrante : l'approche MOC (Banerjee et al. (2005a)) et l'approche OKM (Cleuziou (2007)). En proposant une formalisation de OKM en terme de mélange de lois, nous avons pu constater les fortes analogies qui existent entre les deux approches. Nous avons détaillé leurs différences fondamentales et relevé quelques limites théoriques concernant le modèle MOC. Ces limites sont susceptibles de produire des résultats incohérents que nous avons observés expérimentalement.

Introduction
L'extraction de connaissances est un processus qui permet d'analyser une masses de données importante afin d'en extraire des connaissances nouvelles, valides et utiles. Ces connaissances sont ensuite présentées sous différentes formes notamment sous forme de règles d'association. Une règle d'association (RA) (Agrawal et al. (1993)) est une implication de la forme C 1 ? C 2 , où C 1 et C 2 sont des conditions C sur les attributs de la base. Soient minsup et minconf des seuils prédéfinis. Une RA est dite forte si elle satisfait deux contraintes :
-son support supp(C) ? minsup, avec supp(C) : nombre de transactions dans la base qui satisfont l'ensemble des conditions C tel que supp(C 1 ? C 2 ) = supp(C 1 ? C 2 ) ; -sa confiance conf (C 1 ? C 2 ) ? minconf , avec conf (C 1 ? C 2 ) = supp(C1?C2) supp(C1) .
Dans cet article, nous nous intéressons tout particulièrement à l'extraction de règles d'association quantitatives (RAQ). Ce type de règles prend en considération tout type de variables, quantitatives ou catégorielles. Un certain nombre de travaux relatifs à l'extraction de RAQ sont proposés dans (Srikant et Agrawal (1996), Hong et al. (1999), Aumann et Lindell (1999), Miller et Yang (1997), Lent et al. (1997), Fukuda et al. (1996), Rastogi et Shim (1999), Mata et al. (2002)). L'interprétation des résultats obtenus est basée sur la sémantique choisie. Cette sémantique exprime la nature des liens existants entre les différentes variables de la base. On peut alors exprimer le lien qui maximise le nombre d'éléments en commun entre deux variables. Ce type d'extraction est connu sous le nom d'extraction de règles d'association positives : une personne qui achète du lait achète aussi du café. De la même façon, on peut exprimer le lien qui unit deux variables et qui prend en considération le changement de comportement d'une variable spécifique lors de l'introduction d'une nouvelle variable. Cet aspect est particulièrement subtil dans le sens où il considère les variables qui ont un comportement opposé lors de leur union. Les règles extraites à partir de ce type d'union sont connus sous le nom de règles d'association négatives : une personne qui achète du Pepsi n'achète pas de Coca Cola, ; Teng et al. (2002); Antonie et Zaane (2004); Wu et al. (2004); Savasere et al. (1998);Yuan et al. (2002); Yan et al. (2004)). Une règle d'association négative (RAN ) est présentée sous l'une des formes suivantes :
Dans (Teng et al. (2002)), la distance du ? 2 permet d'évaluer l'indépendance des variables mais sans donner aucune indication sur la dépendance. Dans (Antonie et Zaane (2004)), la mesure d'inégalité de Cauchy Schwarz traduit, plus ou moins, la dépendance linéaire entre deux variables, mais reste cependant incapable de juger de la pertinence réelle de la règle d'association. De plus, elle ne permet pas d'exprimer l'effet d'influence recherché dans notre travail. ` A travers cet article, nous présentons une nouvelle méthode d'extraction de règles d'association quantitatives positives et négatives. Les règles visées ont une sémantique particulière qui permet, par ailleurs, de faire ressortir l'effet qu'une variable nommée influent pourrait avoir sur une autre nommée influé. Cet effet est exprimé par un changement significatif du comportement de l'influé. Plus précisément, par une chute importante du nombre d'individus dans l'intersection de l'association étudiée. L'évaluation du degré de changement de comportement est réalisée grâce à une nouvelle mesure nommée Influence. Cette évaluation vient affiner les résultats d'une autre étape exécutée en parallèle et qui par l'utilisation des tableaux de contingence permet une sélection et un regroupement tabulaire de zones qui constituent la source de règles potentiellement intéressantes. Grâce aux filtrages réalisés dans ces deux étapes, des règles que nous avons nommées règles d'influence sont extraites basées sur des contraintes spécifiées. Cet article s'organise de la façon suivante. Dans la Section 2, nous faisons état de quelques travaux sur l'extraction de RAQ positives et négatives. La section 3 expose notre stratégie de génération de règles d'influence. Enfin, nous présentons nos conclusions dans la section 4.
Les règles d'association quantitatives positives et négatives
Les RAQ positives prennent en considération les variables qui apparaissent simultané-ment. Intéressons nous à l'exemple suivant adapté de (Brin et al. (1997) = 0,875 > minconf . Dans ce cas la règle A ? ¬B serait valide pour la base de données. Toutefois, quelle que soit la nature de la RA, positive ou négative, le support et la confiance utilisés comme uniques contraintes sont insuffisants pour une extraction optimale des RA. Afin de remédier à l'extraction prohibitive des RAQ, plusieurs mesures évaluant la qualité des règles ont été proposées (Lif t IBM (1996), Conviction Brin et al. (1997)). Dans ce papier, nous élargissons l'extraction à ces règles qui sont complémentaires aux règles positives. Dans (Wu et al. (2004)), les RAQ négatives sont extraites à partir d'ensembles de motifs non fréquents. Leur objectif est d'identifier les caractères qui peuvent être ignorés. Dans (Antonie et Zaane (2004)), une nouvelle mesure statistique appelée l'inégalité de Cauchy Schwarz symbolisée par ? est utilisée à des fins d'organisation dans le rayonnage des supermar-
Un coefficient ? 0 signifie l'absence de relation linéaire entre A et B mais ne donne aucune indication sur l'indépendance. Si le coefficient ? +1 ou ? ?1, A et B sont fortement corrélées. Ceci dit, la corrélation ne doit aucunement être confondue avec la causalité. La corrélation entre deux caractères n'implique absolument pas que l'un cause l'autre. Des problèmes similaires sont apparus avec l'utilisation de certaines mesures statistiques. La distance du ? 2 (Teng et al. (2002)) détecte les variables qui expriment de fortes liaisons. Elle permet de déterminer si deux variables sont indépendantes. Malheureusement, elle est incapable de donner la direction de la dépendance : A ? B ou B ? A ? La section suivante introduit une nouvelle sémantique proche des règles d'impacte proposées par Webb dans (Webb (2001) 
Préparation des données
Les données présentes dans les bases de données sont sous forme brute. Antérieurement à leur exploitation, il est nécessaire de les traiter. Ce traitement, dans le cas des variables qualitatives, consiste à éclater une variable en un ensemble de variables correspondantes aux différentes valeurs distinctes de la variable traitée. Dans le cas des variables quantitatives, le traitement des données est plus complexe, et émane des nouveaux problèmes principalement liés au nombre important de valeurs distinctes que peut prendre une seule variable quantitative. La discrétisation fût une réponse naturelle aux problèmes rencontrés (Srikant et Agrawal (1996); Aumann et Lindell (1999)). Notre travail de préparation des données consiste à réaliser un codage disjonctif complet pré-cédé d'une étape de discrétisation sur les variables quantitatives. La discrétisation est réalisée en utilisant une méthode de clustering basée sur le calcul des plus proches voisins. Les clusters sont constitués d'individus dont les distances des uns par rapport aux autres n'excèdent pas un seuil limite prédéfini le M axDif . L'exemple suivant est extrait de la base de données W ages 1 . Les clusters de la variable âge sont calculés avec M axDif = 5 ans et sont listés dans la table 1. 
Évaluation des taux d'influence
Afin d'évaluer l'influence d'une variable sur une autre, le rejet de l'hypothèse d'indépen-dance doit être prouvé. Ceci peut être réalisé grâce à différentes mesures (? 2 , coefficient de corrélation linéaire . . .). Suite à cela, une analyse de la nature de la dépendance est réalisée grâce à notre mesure l'Inf luence. 
où |X | symbolise la cardinalité de l'ensemble des individus qui vérifient la condition X. Les résultats obtenus sont filtrés afin de ne retenir que les taux d'influence supérieur au seuil d'influence négative minimum fixé, symbolisé par S ? , tel que : 
FIG. 1 -i)Influence
ii). Les règles d'influence attendus sont de la forme
Les résultants obtenus, comme dans le cas négatif, sont filtrés par rapport à un seuil d'influence positive minimum fixé et symbolisé par S + , tel que :
n'est pas intéressante.
Consolidation de la mesure d'influence
La dépendance entre deux variables doit être évaluée objectivement. Les ensembles avec peu de représentants sont exclus pour n'inclure que ceux avec un taux d'influence excédant le seuil minimum fixé. Or, si nous fixons le seuil à S=70%, par exemple, alors que le taux d'influence calculé d'une règle est égal à 69%, ne devrions nous pas prendre cette dernière en considération ? Dans notre approche, nous nous sommes intéressées à la pertinence de ce genre de règles et au moyen de les inclure dans notre système. Nous avons, donc utilisé le théorème de la limite centrale (Macfie et Nufrio (2005)) qui répond parfaitement au problème moyennant des intervalles de confiance et une estimation du risque ? pour la proportion. Cette évaluation dote la mesure d'influence d'une meilleure sensibilité en permettant une étude dans des intervalles de la forme
avec ? suivant la loi normale N (0; 1), ? suivant la loi normale inverse et |n| la taille de la base de données. L'exemple qui suit est tiré de la base W ages. Cet exemple permet d'évaluer l'influence né-gative de la variable sexe représentée par deux clusters (masculin et féminin) sur la variable salaire qui est représentée par sept clusters listés dans la table (2), avec un risque ? = 0,05, ? ?/2 = 1.96 et S ? = 0,70. 
TAB. 3 -Exemple d'estimation des taux d'influence négatifs et des intervalles de confiance
Détermination des zones intéressantes
Au niveau de cette étape, le test d'indépendance entre les variables A et B est réalisé. Cette indépendance peut être évaluée en utilisant la table de contingence (T C) des variations . Cette table est calculée en comparant les effectifs observés et les effectifs théoriques, (voir table (4)) (|ef f ectif theorique| ? |ef f ectif observe |) Guillaume (2002). Si (|ef f ectif observe| = |ef f ectif theorique|) alors A et B sont indépendants, sinon, l'hypothèse d'indépendance est rejetée si cet écart est sigificatif. La T C des variations de l'association (sexe,salaire) est présentée table 5. Les cellules nulles dans T C des variations sont ignorées. Les résultats de cette table sont ensuite juxtaposés aux tables des taux d'influence positifs et négatifs pour constituer la source de nos RI. Cette opé-ration est réalisée dans l'étape de coordination des résultats.
Coordination des résultats
L'étape de coordination des résultats consiste à superposer la T C des variations, (voir exemple dans la table (5)   
Extraction des règles d'influence
En se basant sur les résultats des deux étapes précédentes, les RI positives et négatives sont extraites à partir de la table de coordination. Les règles sont de la forme A ? B ou A ? ¬B avec A l'influent et B l'influé. Les RI générées pour l'association (sexe,salaire) sont listées dans la table (7).
GenInf représente l'intervalle de confiance généralisé pour les taux d'influence [A; B] de la règle. GenInf est construit à partir des intervalles de confiance [A i ; B i ] spécifiques aux différentes valeurs des variables avec i la ième classe de l'association, tel que A = min{A i } et B = max{B i }. les RI peuvent alors être présentées sous une forme générale comme dans l'exemple présenté ci-après. sexe = masculin ? (¬salaire ? [1,00; 6,00]) ? (salaire ? [11,11; 16,  [0,53; 0,61]. Ces règles expriment que le fait d'être un homme augmenterait les chances d'obtenir un salaire entre 11,11K et 26,00K et de ce fait, les hommes ont peu de 'chance' d'avoir un salaire entre 1,00K et 6,00K. Inverssement, être une femme réduit les chances d'obtenir un salaire entre 11,11K et 26,00K, ainsi une femme à 'plus de chance' d'avoir un salaire entre 1,00K et 6,00K.
Conclusions et perspectives
Dans ce papier, nous proposons une approche pour l'extraction de règles d'influence quantitatives positives et négatives. La stratégie que nous adoptons combine une méthode pour l'identification des zones d'intérêt grâce aux tableaux de contingence des variations et une mesure d'élagage l'Inf luence qui analyse le comportement des variables et détermine la nature de leur dépendance. Ce genre d'analyse est très utile, notamment dans notre cadre de travail qui s'appuie sur une sémantique spécifique du type Inf luent ? Inf lué. Le système de règles obtenu peut servir de base à l'utilisateur afin d'établir des prédictions et éventuellement lancer

Introduction
L'apprentissage supervisé sur données déséquilibrées fait l'objet de nombreux travaux (Provost (2000)). Pour le cas des arbres de décision, différents auteurs ont proposé d'utiliser des mesures d'entropie prenant en compte l'asymétrie pour la recherche du meilleur éclate-ment. Nous avons ainsi proposé une axiomatique permettant de définir une famille de mesures asymétriques (Zighed et al. (2007)). Comment évaluer la qualité des arbres construits avec de telles mesures ? En effet, les critères de performances globaux (comme le taux d'erreur) ne prennent pas en compte l'asymétrie des classes. Ceux qui évaluent les performances du modèle sur une seule classe sont tributaires de la règle d'affectation d'une classe dans chaque feuille. Or, dans le cas de données déséquilibrées, la règle majoritaire utilisée habituellement ne convient pas. Nous proposons donc une méthodologie et une évaluation des arbres construits avec une entropie asymétrique.
Méthodes d'évaluation
Nous avons retenu deux méthodes pour évaluer les arbres de décisions asymétriques : les courbes ROC et les graphes rappel / précision. Les courbes ROC permettent d'évaluer la structure des arbres indépendamment du déséquilibre des classes (Provost et Fawcett (1997)). Les graphes rappel / précision permettent quant à eux d'évaluer les performances du modèle sur une classe, en faisant varier la règle d'affectation. Ces deux méthodes permettent ainsi de tenir compte des deux problèmes cités en introduction.
Nous avons mené des tests sur 11 jeux de données, dont 2 sont des données réelles issues du dépistage du cancer du sein. La proportion de la classe minoritaire varie de 4% à 35%. Nous avons construit deux modèles en 10-validation croisée : un arbre de décision utilisant l'entropie quadratique, et un arbre utilisant l'entropie asymétrique. Le critère d'arrêt a été fixé à 3% de gain d'information, et la distribution de référence de l'entropie asymétrique au déséquilibre du jeu de départ. Nous observons les résultats sur la classe minoritaire. Nous pouvons résumer les résultats obtenus en trois points. Premiérement, le critère AUC calculé à partir des courbes ROC est systématiquement supérieur en utilisant l'entropie asymétrique. Deuxièmement, la comparaison des courbes ROC entre les deux types d'arbres montre que la courbe ROC de l'entropie asymétrique est dominée sur la partie gauche du graphique (seuil d'acceptation élevé) mais domine sur la partie droite. Enfin et de la même manière, les graphes rappel / précision montrent qu'à rappel égal, l'entropie asymétrique est moins précise pour les forts seuils d'acceptation mais permet une meilleure précision sur les faibles seuils.
Conclusion et perspectives
Ainsi si on utilise un seuil d'acceptation bas pour classer les feuilles d'un arbre de décision, les courbes ROC comme le critère rappel / précision encouragent l'utilisation d'une entropie asymétrique lorsque les jeux de données sont déséquilibrés. Nous considérons quatre pistes pour étendre notre travail. D'une part, l'utilisation d'un critère adaptatif cherchant à s'écarter à chaque éclatement de la distribution du noeud parent. Deuxièmement, l'élaboration d'un critère d'arrêt adapté aux données déséquilibrées. Le présent travail nous permettra de proposer une méthode pour le choix de la règle d'affectation. Enfin, les outils que nous proposons pour adapter les arbres de décision aux données déséquilibrées seront étendus pour les cas à plus de deux modalités.
Références Provost, F. (2000). Learning with imbalanced data sets. 
Summary
To build decision trees on imbalanced datasets, authors proposed asymmetric entropies. Then the problem of evaluating those trees has to be solved. This paper proposes to evaluate the quality of decision trees based on asymmetric entropy measure.

Introduction
Le domaine de la classification de données textuelles se décline en de nombreux axes parmi lesquels la classification conceptuelle. Cette dernière consiste à regrouper des termes dans des concepts définis par un expert. Citons par exemple les termes pot d'échappement, pare-brise et essuie glace qui peuvent être classés dans le concept automobile. Afin d'établir une telle classification sémantique, la proximité de chacun des termes issus des textes doit être mesurée. Ces termes sont ensuite classés en fonction de leurs proximités sémantiques par un algorithme de fouille de données tels que les Kppv (K plus proches voisins) ou bien les K moyennes (Cornuéjols et Miclet (2002)).
Nous nous focalisons dans cet article sur la première étape de la réalisation d'une classification conceptuelle : l'étude de la proximité des termes. Afin de calculer une telle proximité, nous nous appuyons sur une méthode appelée Latent Semantic Analysis (LSA) développée par Landauer et Dumais (1997) 1 . La méthode LSA est uniquement fondée sur une approche statistique appliquée à des corpus de grande dimension consistant à regrouper les termes (classification conceptuelle) ou les contextes (classification de textes). Une fois l'analyse sémantique latente appliquée à un corpus, un espace sémantique associant chaque mot à un vecteur est retourné. La proximité de deux mots peut alors être obtenue par un calcul de similarité comme le cosinus entre deux vecteurs. L'objectif de nos travaux est d'améliorer les performances de LSA par une approche nommée ExpLSA (Expansion des contextes avec LSA).
L'approche ExpLSA consiste à enrichir le corpus qui constituera l'entrée d'une analyse sémantique latente classique. Cet enrichissement utilise les informations sémantiques obtenues grâce à la syntaxe, ce qui permet d'utiliser ExpLSA aussi bien avec des corpus spécialisés ou non. Il n'est en effet pas utile d'utiliser un corpus d'apprentissage et donc pas nécessaire de connaître le thème général du corpus.
Dans cet article, nous allons nous appuyer sur un corpus des Ressources Humaines de la société PerformanSe 2 écrit en français 3 . Notons que les premiers travaux sur ce corpus ont été initiés dans l'équipe IA du LRI (Roche et Kodratoff (2003)). Une caractéristique essentielle de ce corpus est qu'il utilise un vocabulaire spécialisé. Par ailleurs, il contient des tournures de phrases revenant souvent, ce qui peut influencer positivement le traitement avec LSA. Ce corpus a fait l'objet d'une expertise manuelle nous permettant ainsi de valider nos expérimen-tations.
Nous proposons dans la section suivante de détailler les caractéristiques théoriques de la méthode LSA ainsi que les limites d'une telle analyse. La section 3 propose un état de l'art dans le domaine de l'utilisation de connaissances syntaxiques associées à LSA. Nous présen-tons ensuite notre méthode en y développant ses différentes étapes (section 4). Nous décrirons également (section 5) le protocole expérimental utilisé pour finalement présenter les résultats obtenus.
LSA
La méthode LSA qui s'appuie sur l'hypothèse "harrissienne" est fondée sur le fait que des mots qui apparaissent dans un même contexte sont sémantiquement proches. Le corpus est représenté sous forme matricielle. Les lignes sont relatives aux mots et les colonnes représen-tent les différents contextes choisis (un document, un paragraphe, une phrase, etc.). Chaque cellule de la matrice représente le nombre d'occurrences des mots dans chacun des contextes du corpus. Deux mots proches au niveau sémantique sont représentés par des vecteurs proches. La mesure de proximité est généralement définie par le cosinus de l'angle entre les deux vecteurs.
Caractéristiques théoriques de LSA
La théorie sur laquelle s'appuie LSA est la décomposition en valeurs singulières (SVD). Une matrice A = [a ij ] où a ij est la fréquence d'apparition du mot i dans le contexte j, se décompose en un produit de trois matrices U SV T . U et V sont des matrices orthogonales et S une matrice diagonale.
Soit S k où k < r la matrice produite en enlevant de S les r ? k colonnes qui ont les plus petites valeurs singulières. Soit U k et V k les matrices obtenues en enlevant les colonnes correspondantes des matrices U et V . La matrice U k S k V T k peut alors être considérée comme une version compressée de la matrice originale A. Les expériences décrites dans la section 5 ont été menées avec un nombre de facteurs k égal à 100, facteur faible qui est davantage approprié à des contextes de taille réduite.
Il est coutume de dire que LSA est une méthode statistique ou numérique car elle s'appuie sur une théorie mathématique bien connue. Cependant, on peut également dire que LSA est une méthode géométrique car seuls des résultats d'algèbre linéaire sont utilisés.
Nous précisons qu'avant d'effectuer la décomposition en valeurs singulières, une première étape de normalisation de la matrice d'origine A est exécutée. Cette normalisation consiste à appliquer un logarithme et un calcul d'entropie sur la matrice A. Ainsi, plutôt que de se fonder directement sur le nombre d'occurrences de chacun des mots, une telle transformation permet de s'appuyer sur une estimation de l'importance de chacun des mots dans leur contexte. De manière similaire aux travaux de Turney (2001), cette étape de normalisation peut également s'appuyer sur la méthode du tf×idf, approche bien connue dans le domaine de la Recherche d'Information. Précisons de plus que nous ne prenons pas en compte les ponctuations ainsi qu'un certain nombre de mots non significatifs du point de vue sémantique tels que les mots "et", "à", "le", etc.
Les limites de LSA
LSA offre des avantages parmi lesquels, la notion d'indépendance par rapport à la langue du corpus étudié, le fait de se dispenser de connaissances linguistiques ainsi que de celles du domaine tels que des thésaurus. Bien que cette approche soit prometteuse, il n'en demeure pas moins que son utilisation soulève des contraintes.
Notons tout d'abord l'importance de la taille des contextes choisis. Rehder et al. (1998) ont montré lors de leurs expérimentations que si les contextes possèdent moins de 60 mots, les résultats s'avèrent être décevants. Il a également été mis en évidence par Roche et Chauché (2006) que l'efficacité de LSA est fortement influencée par la proximité du vocabulaire utilisé.
Pour résoudre de tels problèmes, une des solutions peut consister à ajouter des connaissances syntaxiques à LSA, comme cela est décrit dans la section suivante.
3 État de l'art sur l'ajout de connaissances syntaxiques à LSA  posent le problème du manque d'informations syntaxiques dans LSA en comparant cette méthode à une évaluation humaine. Il est question de proposer à des experts humains d'attribuer des notes à des essais sur le coeur humain de 250 mots rédigés par des étudiants. Un espace sémantique a été créé à partir de 27 articles écrits en anglais traitant du coeur humain "appris" par LSA. Les tests effectués concluent que la méthode LSA obtient des résultats satisfaisants comparativement à l'expertise humaine. Il en ressort que les mauvais résultats étaient dus à une absence de connaissances syntaxiques dans l'approche utilisée. Ainsi, les travaux qui sont décrits ci-dessous montrent de quelle manière de telles connaissances peuvent être ajoutées à LSA.
La première approche de Wiemer-Hastings et Zipitria (2001) utilise des étiquettes grammaticales (Brill (1994)) appliquées à l'ensemble du corpus étudié (corpus de textes d'étudi-ants). Les étiquettes étant rattachées à chaque mot avec un blanc souligné ("_"), l'analyse qui s'en suit via LSA considère le mot associé à son étiquette comme un seul terme. Les résul-tats de calculs de similarités obtenus avec une telle méthode restent décevants. Notons que de telles informations grammaticales ne sont pas des connaissances syntaxiques proprement dites contrairement à la seconde approche de Wiemer-Hastings et Zipitria (2001) décrite ci-dessous. Cette seconde approche se traduit par l'utilisation d'un analyseur syntaxique afin de segmenter le texte avant d'appliquer l'analyse sémantique latente. Cette approche est appelée "LSA structurée" (SLSA). Une décomposition syntaxique des phrases en différents composants (sujet, verbe, objet) est tout d'abord effectuée. La similarité est ensuite calculée en traitant séparé-ment par LSA les trois ensembles décrits précédemment. Les similarités (calcul du cosinus) entre les vecteurs des trois matrices formées sont alors évaluées. La moyenne des similarités est enfin calculée. Cette méthode a donné des résultats satisfaisants par rapport à "LSA classique" en augmentant la corrélation des scores obtenus avec les experts pour une tâche d'évaluation de réponses données par des étudiants à un test d'informatique. Kanejiya et al. (2003) proposent un modèle appelé SELSA. Au lieu de générer une matrice de co-occurrences mot/document, il est proposé une matrice dans laquelle chaque ligne contient toutes les combinaisons mot_étiquette et en colonne les documents. L'étiquette "préfixe" renseigne sur le type grammatical du voisinage du mot traité. Le sens d'un mot est en effet donné par le voisinage grammatical duquel il est issu. Cette approche est assez similaire à l'utilisation des étiquettes de Brill (1994) présentée dans les travaux de Wiemer-Hastings et Zipitria (2001). Mais SELSA étend ce travail vers un cadre plus général où un mot avec un contexte syntaxique spécifié par ses mots adjacents est considéré comme une unité de représentation de connaissances. L'évaluation de cette approche a montré que la méthode LSA était plus pertinente que SELSA dans un test de corrélation avec des experts. Cependant, SELSA se révèle plus précise pour ce qui est de tester les bonnes et mauvaises réponses (c.-à-d. SELSA fait moins de fautes que LSA mais en retourne de plus nuisibles).
L'approche ExpLSA que nous présentons dans cet article se place dans un contexte différent. En effet, dans notre cadre de travail, les contextes sont représentés par des phrases. Ceux-ci ont donc une taille réduite ce qui a tendance à donner des résultats décevants avec l'utilisation de LSA (Rehder et al. (1998); Roche et Chauché (2006)). Dans notre approche, nous proposons d'utiliser la régularité de certaines relations syntaxiques afin d'enrichir le contexte comme nous allons le monter dans la section suivante.
Notre Approche : ExpLSA
Le but final que nous nous fixons consiste à regrouper automatiquement des termes extraits grâce à des systèmes tels que ACABIT (Daille (1994)), LEXTER (Bourigault (1993)), SYNTEX (Bourigault et Fabre (2000)), EXIT (Roche et al. (2004)). Dans notre cas, nous nous proposons de regrouper les termes nominaux extraits avec EXIT. Les termes extraits avec ce système sont des groupes de mots respectant des patrons syntaxiques (nom-préposition-nom, adjectif-nom, nom-adjectif, etc.). Par ailleurs, EXIT s'appuie sur une méthode statistique afin de classer les termes extraits et utilise une approche itérative pour construire des termes complexes.
La première étape de ce regroupement pour finalement construire une classification conceptuelle sera effectuée par ExpLSA dont le principe est décrit dans la section suivante.
Principe général d'ExpLSA
Notre approche vise à enrichir le corpus initial lemmatisé en faisant une expansion des phrases (d'où le nom ExpLSA) fondée sur une méthode syntaxique. Il en ressort un contexte plus riche. Celui-ci est construit en complétant les mots du corpus par des mots jugés séman-tiquement proches.
Citons par exemple la phrase : "Vos interlocuteurs seront donc bien inspirés de placer les échanges ...". Nous la transformons tout d'abord en phrase lemmatisée via le système SYGFRAN (Chauché (1984) 
Avec log Asium (x) valant : -pour x = 0, log Asium (x) = 0 -sinon log Asium (x) = log(x) + 1
Une mesure d'Asium proche de 1 implique une importante proximité sémantique. L'exemple de la figure 1 illustre l'application de la mesure d'Asium pour les verbes écouter et convaincre. Nous considérerons par la suite plusieurs seuils de similarité, appelés SA, signifiant qu'au delà de ceux-ci, les verbes seront considérés comme proches par la mesure d'Asium. La méthode d'expansion utilisant la méthode d'Asium est décrite dans la section suivante.
Étapes d'ExpLSA
Après avoir explicité la mesure d'Asium permettant de mesurer la proximité des verbes du corpus, nous proposons de détailler les différentes étapes définissant ExpLSA afin d'étendre les contextes.
La première étape de l'approche ExpLSA identifie les différents termes extraits par EXIT. Cette identification consiste à représenter le terme par un seul mot (par exemple, le terme attitude profondément participative issu du corpus des Ressources Humaines devient nom234 qui représente le 234ème terme parmi une liste extraite par EXIT).
Après extraction des relations syntaxiques Verbe-Objet par le biais d'une analyse syntaxique, la phase suivante de notre approche consiste à étudier la proximité sémantique entre les L'étape suivante a pour but de regrouper tous les objets communs dont les verbes ont été jugés proches sémantiquement par le seuil de similarité le plus élevé parmi l'ensemble des couples de verbes.
Nous considérons deux possibilités de regroupement. La première consiste à considérer les objets communs aux deux verbes (interlocuteur et collaborateur dans l'exemple de la figure 1). La seconde considère les objets communs et les complémentaires aux deux verbes comme dans les travaux de Faure et Nedellec (1999)  Notons qu'une liste de noms non porteurs de sens ne sont pas pris en compte pour enrichir le contexte (par exemple, les mots "chose", "personne", etc.). Cette liste a été constituée manuellement.
L'évaluation, qui va être présentée dans la section suivante, consiste à comparer les résultats obtenus automatiquement en nous appuyant sur ExpLSA avec ceux d'un expert qui a associé manuellement les termes pertinents à des concepts.
Expérimentations
Pour discuter de la qualité des résultats retournés avec notre approche, nous nous appuyons sur le protocole expérimental décrit dans la section suivante.
Protocole expérimental
Dans nos expérimentations nous nous appuyons sur le corpus des Ressources Humaines expertisé manuellement. De cette expertise ressort une classification conceptuelle de l'ensemble des termes extraits par EXIT ; les concepts ayant été définis par l'expert. Par exemple, l'expert a défini le concept "Relationnel" dont les termes confrontation ouverte, contact superficiel et entourage compréhensif sont des instances. L'objectif de nos expérimentations est d'évaluer les similarités entre les termes retournées par les méthodes automatiques ci-dessous :
-M1 : LSA 4 -M2 : la méthode des intersections de ExpLSA -M3 : la méthode des complémentaires de ExpLSA -M4 : LSA + Tree-Tagger La méthode de LSA + Tree-Tagger consiste à utiliser un étiqueteur grammatical, le TreeTagger (Schmid (1995)) comme dans l'approche de Wiemer-Hastings et Zipitria (2001) présen-tée dans la section 3. Ainsi, nous appliquons LSA sur un corpus qui a été au préalable étiqueté par le Tree-Tagger.
Pour comparer ces méthodes, nous avons évalué deux à deux les termes des concepts. Pour cela, nous avons sélectionné parmi les concepts, ceux étant les plus représentés dans le corpus, c'est-à-dire les concepts regroupant un minimum de 200 termes distincts selon l'expertise manuelle. Cela nous laisse un total de quatre concepts. Nous proposons ainsi de comparer deux à deux chaque concept. L'objectif de nos expérimentations consiste à évaluer si les termes appartenant à un concept sont correctement associés aux termes de ce même concept par les quatre méthodes (M1 à M4) décrites ci-dessus.
Pour effectuer une telle comparaison, tous les termes d'un concept C1 sont pris en compte. La similarité (cosinus) est alors calculée entre l'ensemble des termes du concept C1 et les autres termes des concepts à comparer (par exemple, les termes de C1 + C2, C1 + C3 ou C1 + C4). Les couples de termes ainsi obtenus sont classés par valeur décroissante avec les quatre méthodes M1 à M4. Un système retourne des résultats de bonne qualité si les couples pertinents sont placés en début de liste. Un couple est pertinent si les deux termes appartiennent au même concept. Pour évaluer la qualité de la liste, nous calculons la précision des premiers couples retrournés, le rappel n'ayant pas été jugé adapté 5 . La précision permet d'évaluer la proportion de couples pertinents retrouvés par le système. Notons que la réalisation de ces expérimentations avec deux concepts est assez conséquente puisqu'elle produit plus de 60 000 calculs de similarité.
Comparaison des deux méthodes ExpLSA
Cette première évaluation propose de comparer les deux méthodes de ExpLSA utilisées pour l'enrichissement du corpus, la méthode des intersections (M2) et la méthode des complé-mentaires (M3). Le tableau 1 compare la moyenne des précisions des 100 6 premiers couples de termes avec un seuil pour la mesure d'Asium à 0,6. Avec une telle valeur de SA, nous faisons une large expansion du corpus. Ce tableau montre que les approches ExpLSA utilisant TAB. 1 -Précision en fonction des 100 premiers couples de termes. M1 : LSA, M2 : ExpLSA avec la méthode des intersections, M3 : ExpLSA avec la méthode des complémentaires. SA = 0,6. la méthode des intersections (M2) ou bien celle des complémentaires (M3) sont inférieures à LSA et n'améliorent que rarement la précision. Ces résultats s'expliquent par la quantité importante de données non pertinentes utilisées pour l'enrichissement. En effet, un seuil SA faible a tendance à ajouter du bruit comparativement à l'utilisation d'un seuil plus élevé qui permet une expansion quantitativement plus faible mais souvent plus pertinente. Ceci confirme les résultats préliminaires présentés dans (Béchet et al. (2007)). Le meilleur compromis entre la quantité de données ajoutées et la qualité de celles-ci a été experimentalement établi avec un seuil SA égal à 0,8 sur notre corpus. Nous constatons par ailleurs que la méthode M3 donne globalement des résultats plus faibles que la méthode M2 (bien que toutes les deux soient inférieures à M1 Nous proposons de comparer dans cette section deux méthodes utilisant des connaissances syntaxiques, ExpLSA (M2) et LSA + Tree-Tagger (M4). LSA + Tree-Tagger consiste à ajouter des connaissance grammaticales au corpus en complétant les mots par une étiquette grammaticale. Cette approche permet de lever les ambiguités de certains mots pouvant appartenir à des catégories grammaticales différentes. Par exemple, le mot bien peut-être un adverbe, un nom ou un adjectif. Avec la méthode M4, nous considérons dans cet exemple trois formes distinctes pour représenter ce mot : bien_ADV, bien_NOM et bien_ADJ.
La figure 3 montre que la méthode LSA + Tree-Tagger améliore les résultats de LSA pour les derniers couples ce qui ne correspond pas aux attentes de l'utilisateur. En effet, une fonction de rang est en général satisfaisante si un nombre important d'exemples positifs sont placés en tête de liste. Cette tendance se généralise pour les autres concepts comme le montre le tableau 3. La méthode LSA + Tree-Tagger (M4) reste cependant presque toujours inférieure à notre approche ExpLSA (M2). Ces résultats nous encouragent à envisager dans de futurs travaux une hybridation des méthodes M2 et M4 afin de conserver les résultats de ExpLSA et de bénéficier des améliorations de la méthode LSA + Tree-Tagger pour les derniers couples. LSA est une méthode statistique utilisée notamment pour regrouper des termes afin d'établir une classification conceptuelle. Néanmoins, cette méthode donne des résultats parfois déce-vants. Ceux-ci s'expliquent entre autres par l'absence de connaissances linguistiques. La qualité de ces résultats peut également être influencée par la taille des contextes utilisés, LSA obtenant de meilleurs résultats avec des contextes de grande taille.
C'est pourquoi nous nous sommes intéressés dans nos travaux à améliorer les performances de LSA avec des contextes assez courts (phrases) en proposant une approche, ExpLSA, consistant à effectuer une expansion des contextes avant d'appliquer LSA. Nous rendons de ce fait les contextes plus riches en utilisant des outils syntaxiques afin d'y parvenir.
Nous avons présenté deux expérimentations pour comparer l'approche ExpLSA. Nous avons conclu dans la première expérience qu'avec ExpLSA fondée sur la méthode des complémen-taires, l'expansion réalisée n'était pas pertinente et ajoutait une quantité importante de bruit. ExpLSA utilisant la méthode des intersections donne quant à elle des résultats satisfaisants pour les concepts discriminants d'un point de vue sémantique. Les termes des concepts pouvant générer des ambiguités se révelent plus ou moins difficiles à traiter automatiquement par notre approche. L'inconvénient de la méthode ExpLSA est qu'elle nécessite un temps d'exé-cution conséquent (environ deux heures pour traiter un corpus de 1,2 Mo). Ce temps important s'explique principalement par la durée d'execution de la tâche d'extraction des relations syntaxiques. La seconde expérimentation a montré que LSA + Tree-Tagger améliore rarement LSA et que notre approche ExpLSA donne de meilleurs résultats.
Nous envisageons comme futurs travaux d'approfondir les expérimentations en identifiant plus précisément dans quels cas ExpLSA donne de meilleurs résultats comparativement à LSA. Ceci permettra de mettre en place une approche hybride qui utilise LSA et/ou ExpLSA et/ou LSA + Tree-Tagger selon les situations les plus appropriées. De plus, nous souhaiterions valider le regroupement des mots avec ExpLSA en nous appuyant sur des mesures statistiques et des données numériques issues des moteurs de recherche du web (Turney (2001)). Par ailleurs, nous proposerons d'autres méthodes afin d'ajouter des connaissances syntaxiques à LSA. De plus, nous validerons notre méthode d'expansion des contextes en la confrontant à des problèmes de classification de textes. Nous envisageons enfin d'utiliser des vecteurs sé-mantiques avec SYGMART (Chauché (1984)) en considérant un terme comme produit d'un ensemble de concepts issus du thésaurus Larousse.

Introduction
Le problème de la recherche de règles d'association, introduit dans Agrawal et al. (1993), est basé sur l'extraction de corrélations fréquentes entre les enregistrements et connaît de nombreuses applications dans le marketing, la gestion financière ou l'analyse décisionnelle (par exemple). Au coeur de ce problème, la découverte d'itemsets fréquents représente un domaine de recherche très étudié. Dans l'analyse du panier de la ménagère, par exemple, les itemsets fréquents ont pour but de découvrir des ensembles d'items qui correspondent à un nombre significatif de clients. Si ce nombre est supérieur à un support défini (par l'utilisateur) alors cet itemset est considéré comme fréquent. Cependant, dans la définition initiale des itemsets fréquents, l'extraction est effectuée sur la base de données toute entière (i.e. soit min supp , le support minimum donné par l'utilisateur, les itemsets extraits doivent apparaître dans au moins |D| × min supp enregistrements de D). Toutefois, il est possible que des itemsets intéressants reste ignorés malgré des caractéristiques particulières (y compris de support). Effectivement, les itemsets intéressants sont souvent liés au moment qui correspond à leur observation. On pourrait prendre pour exemple le comportement des utilisateurs d'un site de commerce en ligne pendant une offre spéciale sur les DVD et les CD vierges pour laquelle une publicité est faite par mailing. De la même manière, le site Web d'une conférence peut voir le nombre de connexions augmenter dans une fenêtre de quelques heures avant la date limite de soumission. Une condition nécessaire à la découverte de ce type de données est liée à l'aspect temporel des données. Cet aspect a déjà été abordé pour les règles d'association dans Ale et Rossi (2000); Lee et al. (2001). Dans Ale et Rossi (2000), les auteurs proposent la notion de règles d'association temporelles. Leur idée consiste à extraire les itemsets qui sont fréquents sur des périodes définies par la durée de vie de chaque item (les périodes ne sont donc pas découvertes mais utilisées comme contrainte).
Dans cet article, nous proposons de découvrir les itemsets qui sont fréquents sur un sousensemble contigü de la base de données. Par exemple, les navigations sur les pages Web des DVD et CD vierges apparaissent de façon distribuée sur toute l'année. Cependant, la fréquence de ce comportement augmente très certainement pendant les quelques heures (ou jours) qui suivent le mailing. Ainsi, le défi consiste à trouver la fenêtre temporelle qui va optimiser le support de ce comportement. Considérons que le mailing soit envoyé le 3 mars et qu'il a influencé les clients pendant deux jours. Notre but est de découvrir que : "25% des clients, entre le 3 et le 5 mars, ont demandé la page des CD vierges, la page des DVD vierges et finalement la page des offres spéciales". Le support de ce comportement est sûrement trop faible pour permettre son extraction sur l'année toute entière mais cette connaissance (i.e. le comportement et la période sur laquelle il est fréquent) peut être très utile pour les décideurs qui veulent certainement découvrir ce comportement et sa période de fréquence pour finalement faire le lien avec le mailing.
Definitions
La définition 1 reprend le concept d'itemset fréquent de Agrawal et al. (1993). Nous y avons ajouté la notion d'estampille (donc une transaction peut couvrir plusieurs dates).
Définition 1 Soit
Définition 2 L'ensemble F des itemsets fréquents de D avec un support minimum ? est noté
Étant donnés un ensemble d'items I, une base de transactions D et un support minimum ?, le problème de l'extraction d'itemsets fréquents vise à trouver F (D, ?) ainsi que le support des itemsets de F . L'exemple 1 donne une illustration des concepts définis dans cette section. figure 1(a)   figure 1(a) 
Exemple 1 La
FIG. 1 -itemsets fréquents et itemsets compacts sur
Notre problème est basé sur les estampilles et vise à extraire des itemsets qui sont fréquents sur des périodes particulières de D. Nous présentons maintenant les notions d'itemset temporel et d'itemset compact, qui sont au coeur de cet article.
Définition 3
Une période P = (P s , P e ) est définie par une date de départ P s et une date de fin P e . L'ensemble des transactions qui appartiennent à une période P est défini par T r(P ) = {T /T ? D, ?i ? T, P s ? P i ? P e } avec P i l'estampille de l'item i dans la transaction T . Enfin, P R est l'ensemble des périodes possibles sur D.
En d'autres termes, l'ensemble des transactions qui appartiennent à P est l'ensemble des transactions dont tous les items sont estampillés dans les limites de P .
Définition 4 Un itemset temporel x est un tuple
Soit ?, le support minimum, nous présentons les caractéristiques d'un itemset compact dans la définition 5.
Définition 5 Soit x un itemset temporel. x est un itemset compact (IC) ssi les conditions suivantes sont respectées :
Soit k la taille de x i , alors x est un k-itemset compact. Enfin, SI k est l'ensemble de tous les k-itemsets compacts.
La première condition de la définition 5 assure que x représente un itemset qui est fréquent sur sa période. La seconde condition assure que la taille de x p est maximale. En fait, si une période plus grande existe, alors, sur cette période, x i n'est pas fréquent ou la couverture de x i reste identique (i.e. étendre la période de x p à p 2 n'apporte rien au support). Enfin, la troisième condition assure que la taille de x p est également minimale. En effet, si x i est supporté par la première et la dernière transaction de x p , alors si il existe un période plus petite sur laquelle x i est fréquent, la couverture sera plus faible (i.e. passer de x p à p 2 implique d'ignorer des transactions qui supportent x i et doivent donc être gardées). Une illustration de cette définition est donnée dans l'exemple 2.   
Définition 6 L'ensemble des Itemsets Compacts Maximaux (ICM) est défini comme suit : soit x un IC, x est un ICM si les conditions suivantes sont respectées :
Dans la suite de ce papier, nous proposons un algorithme optimisé pour la découverte de l'ensemble des ICM, comme décrits par la définition 6.
3 DeICo : principe général DEICO introduit un nouveau principe de comptage pour les itemsets candidats. Considérons un itemset temporel t qui n'est pas compact (i.e. t ? < ?). Tout surensemble u = (u x , u p , u ? )/t x ? u x ? u p ? t p de t ne peut pas être un itemset compact (i.e. u ? < ?). DEICO étend le principe d'apriori afin de générer des itemsets compacts candidats et compter leur support. Le principe de génération est modifié par l'ajout d'un filtre sur les intersections possibles entre candidats (i.e. si deux itemsets compacts de taille k ont un préfixe commun mais ne partagent pas la même période, alors leur croisement ne peut pas générer un itemset compact). Cependant, l'étape de comptage d'apriori ne peut pas s'appliquer directement dans notre cas. Considérons c, un itemset temporel candidat. Une solution consisterait à compter le nombre d'apparitions de c dans c p . Ce n'est pas une solution correcte. Considé-rons en effet le candidat c = ((a b), [1..10], c ? ) (généré à partir de x = ((a), [1..10], En effet, la façon dont une passe est effectuée (soit dans l'ordre séquentiel des transactions) implique de découvrir les kernels "à la volée".
Définition 7
Un kernel est une période. L'ensemble K(x, P, ?) des kernels de l'item x sur la période P pour un support ? est défini comme suit : Soit k ? P une période telle que x ? T r(k s ) ? T r(k s ) est la première apparition de x sur P . Si k n'existe pas, alors K = ?. Sinon, soit N l'ensemble des estampilles telles que ?n ? N, n ? P ? n > k s ? f requence(x, [k s ..n]) < ? (en d'autres termes, N est l'ensemble des estampilles de P telles que toute extension de k à une estampille de N implique la perte de fréquence pour x). Si N est vide, alors k e est défini comme la dernière apparition de x dans  
FIG. 2 -Kernels et période de l'itemset (b)
End while End Algorithm MERGEKERNELS Notre algorithme d'extraction se base sur le principe de la génération de candidats. Pendant la passe sur les données, le but de l'algorithme d'extraction est de mettre à jour les informations sur les kernels des itemsets temporels candidats dont la période englobe l'estampille de la transaction courante. A la fin de chaque passe de l'algorithme nous obtenons tous les kernels de chaque candidat pour cette passe. A la fin de chaque passe, les kernels obtenus pour chaque itemset temporel candidat sont fusionnés pour obtenir des itemsets compacts.
Conclusion
Nous avons proposé une nouvelle définition pour la découverte d'itemsets qui correspondent à des fréquences élevées sur des périodes précises sans connaissance préalable sur ces périodes. Cette découverte posait la difficulté de découvrir en même temps les itemsets et leurs périodes optimales de fréquence. De plus, le nombre de combinaisons possible devait être réduit et nous avons apporté les bases théoriques nécessaires à la résolution du problème. Notre algorithme, basé sur la découverte de 'kernels' et leurs fusions s'est révélé efficace et capable d'extraire ce nouveau type de connaissance de manière précise et exhaustive. D'après nos expérimentations, les itemsets compacts constituent un résultat lisible et instructif pour mieux comprendre les données étudiées.
Références
Agrawal, R., T. Imielinski, et A. N. Swami (1993). Mining association rules between sets of items in large databases. In SIGMOD, Washington, D.C., USA, pp. 207-216. Ale, J. M. et G. H. Rossi (2000). An approach to discovering temporal association rules. pp. 294-300. Lee, C.-H., C.-R. Lin, et M.-S. Chen (2001). On mining general temporal association rules in a publication database. pp. 337-344.
Summary
Frequent pattern mining is a very important topic of knowledge discovery, intended to extract correlations between items recorded in databases. However, those databases are usually considered as a whole and hence, itemsets are extracted over the entire set of records. In this paper, we introduce the definition of solid itemsets, which represent a coherent and compact behavior over a specific period.

Introduction
Suivre et modéliser les ravines d'érosion en montagne est un défi nouveau, nécessaire pour développer des modélisations distribuées précises basées sur les processus hydrologiques élémentaires. Les avancées récentes en modélisation distribuée exigent de plus en plus une connaissance fine des processus élémentaires associée à une description physique précise des objets d'intérêt (Hessel et al., 2003). Le mode de transport liquide et solide est à la base de la réaction des bassins versants pour ces modélisations, elle se décline de plus en plus en interrogations sur les « chemins de l'eau », point qui apparaît à la fois fondamental et mal connu, et souvent très simplifié dans les modèles. En particulier en zones de badlands, avec un relief très tourmenté, il s'agit de la caractérisation physique des ravines et du modelé du relief à différentes échelles.
Pour atteindre ce but, les techniques de télédétection semblent appropriées, mais des questions de résolutions spatiales et temporelles se posent. La précision exigée par la thématique varie de quelques mètres pour la description numérique des très grosses ravines, mais elle devient de quelques décimètres quand on s'intéresse aux pentes locales ou aux -79 -RNTI-E-13 ravines et rigoles élémentaires. La précision peut encore atteindre quelques centimètres, voire des millimètres pour les analyses des dépôts et ablations de versants. Les photographies aériennes classiques prises par avion donnent des informations métriques. Afin d'obtenir une résolution au sol de l'ordre de quelques centimètres, notre choix s'est porté vers un vecteur lent (drone avion) pour éviter les effets de filés sur les photos, volant à très base altitude, et utilisant un appareillage photographique « grand public » donc facilement reproductible. L'utilisation de drone semble tout indiquée pour cela, bien qu'elle présente deux difficultés : une connaissance de la position exacte de l'appareil en vol, et une limitation de la charge utile. La facilité initiale de prise de vue est alors compensée par une nécessité de travaux associés : repérage de mires in situ pour effectuer une « aérotriangulation » correcte, rectifications de lentille et corrections diverses, que nous allons détailler.
En outre, la validation des modèles numériques de terrain (MNT) très détaillée n'est pas une tâche facile. La quantité et la précision des données « vérité terrain » requises ainsi que la souplesse d'utilisation de l'appareillage sur place sont souvent hors de portée pour les outils couramment disponibles : GPS différentiel (DGPS) ou tachéométrie. Ces méthodes pourraient être assez précises mais elles ne produisent pas assez de données pour une validation approfondie. Néanmoins, le LiDAR terrestre a la potentialité pour satisfaire les conditions de validation d'un MNT très détaillés.
L'objectif de cet article est de quantifier à chaque étape de la chaîne de traitement stéréophotogrammétrique, le gain de précision obtenu dans la construction d'un MNT, pour un vecteur et un milieu particuliers et peu favorables.
Site d'étude et données disponibles
Le Cemagref suit depuis 1975 des bassins versants expérimentaux d'érosion dans des zones de badlands à Draix, dans les Alpes de haute Provence, récemment transformés en Observatoire de Recherche en Environnement -ORE Draix (Mathys et al., 2003). Les bassins sont situés sur des marnes noires très érodables, l'érosion y est particulièrement importante : les valeurs d'ablation sont supérieures à 100 t/ha/an (Oostwoud et Ergenzinger, 1998). Dans cette zone les chemins de l'eau prédominants se présentent sous forme de ravines d'érosion, voire de rigoles ou griffures élémentaires. Notre site porte sur une partie du bassin du Moulin (figure 1) avec des dénivelées très fortes : 40 m entre les points haut et bas, à une distance de 100 m (ravines de 10 à 20 m de large et 5 à 10 m de profondeur).
Les données utilisées sont essentiellement 1) des photographies dans le visible prises par drone « Avion jaune » (www.lavionjaune.fr), avec un appareil photo numérique Sony DSC-P150 (à 7.2 méga pixels) ; 2) des repères topographiques au sol ou points de contrôle terrestre (PCT) et des points de vérité terrain (PV) sur les thalwegs et les crêtes, par une campagne mixte DGPS-tachéométrie ; 3) un scan LiDAR terrestre (ILRIS-3D) dans une ravine du site d'étude (mai-juin 2007), avec une direction de balayage vers le Nord-Est. Dans la figure 1 nous pouvons observer la direction du scan (ligne noire pointillée en haut), ainsi que le nuage de points LiDAR (en bas de l'image) et la ravine sélectionné pour le contrôle de qualité du MNT drone (polygone noir) à partir des données LiDAR.
-80 -RNTI-E-13 -81 -RNTI-E-13
Méthodologie
Les grandes étapes de la méthode stéréophotogrammétrie numérique utilisée sont celles des techniques habituelles (Kraus & Waldhäusl, 1998) : 1) préparation du vol et prise de vues stéréoscopiques, 2) géométrie (orientation interne et externe), 3) aérotriangulation, et 4) restitution numérique du relief.
La première étape est le point du départ du développement d'un MNT et comprend la sélection du vecteur, d'après les objectifs de la mission, des dates et des heures de vol, du type de chambre photo, de la hauteur et la vitesse de vol, ainsi que le nombre et la précision des points de contrôle terrestre nécessaires. L'étape suivante consiste à définir les caractéristiques de la chambre photo de prise de vue, ainsi que les conditions de prise de vues (position et orientation de l'appareil photo). Ces paramètres d'orientation interne et externe, généralement inconnus pour les appareils photo grand public et les vecteurs légers ou drones, sont à la base des calculs à développer dans la prochaine étape d'aérotriangulation, où un lien entre l'appareil photo, les photos et le terrain doit s'établir (colinéarité). La dernière étape consiste à restituer le relief à partir des couples stéréoscopiques par corrélation locale des points homologues.
Les paramètres d'orientation inconnus entraînent des erreurs géométriques importantes pendant la restitution du relief. Il est alors évident que la souplesse de la méthode entraîne des approximations et des incertitudes à résoudre. Notre but est alors de préciser les points où la méthode devient particulière, et de vérifier jusqu'où les incertitudes citées sont surmontables et à quel compromis « qualité -souplesse d'utilisation » il est possible d'accéder.
L'ensemble des éléments présentés correspond à des schémas classiques de constitution de MNT par couple stéréoscopique. C'est l'application et l'adaptation de ces méthodes à des zones érodées avec de fortes pentes et avec des appareils photo grand public qui constituent le point original de notre contribution, avec un grand effort de validation de la précision sur le terrain.
Préparation du vol et prise de vue stéréoscopique
Un véhicule aérien non-piloté (UAV, Unmanned Aerial Vehicle, ou drone avion), est un vecteur piloté au sol. Il supporte une charge utile de l'ordre de 3 à 4 kg, ce qui permet d'embarquer un ou deux appareils photos « grand public », ainsi que des éléments de vol comme un GPS légers notamment (Raclot et al., 2005).
Pour une utilisation en stéréoscopie, la précision requise pour la position du capteur à l'instant de la prise de vue est toutefois nettement plus forte que celle permise par ces GPS légers mono-fréquence et non-différentiels. Une qualité de positionnement requise de quelques centimètres impose la prise de points d'appui au sol, afin de pouvoir faire un calcul a posteriori plus précis de la position du vecteur en fonction des éléments de l'image.
Les éléments de stéréophotographie sont particuliers dans les zones de ravines. En effet, dans la littérature on cite couramment le rapport B/h comme élément de qualité stéréoscopique (Kraus & Waldhäusl, 1998). Dans le triangle formé par le point photographié au sol et les deux positions de l'appareil photo au moment des prises de vues, h représente -82 -RNTI-E-13 l'altitude de la prise de vue et B est la base, c'est-à-dire la distance entre les deux positions de prise de vue (figure 2).
Classiquement un rapport B/h proche de 1 est préconisé pour une stéréoscopie « optimale ». Néanmoins, dans le cas de zones très tourmentées où les pentes des versants analysés sont excessivement fortes, les lignes d'incidence de la prise de vue doivent être nettement plus verticales pour éviter des parties cachées. Dans ces conditions nous préconisons un rapport B/h plus petit (de l'ordre de 0,2 à 0,3), ce qui permet un bon compromis entre altitude, distance focale, et recouvrement entre clichés.
Par rapport au calcul des coordonnées de PCT, plusieurs solutions sont possibles : DGPS en mode RTK, qui présente des avantages en terme de rapidité de mesure et de contrôle de qualité en temps réel (Mora et al., 2003) ; tachéomètre avec une qualité millimétrique. 
Géométrie
La géométrie interne de l'appareil photo est définie par la distance focale, le point principal de symétrie, ainsi que les déformations de lentilles. L'orientation interne est fondamentalement utilisée pour transformer système de coordonnées pixel en système de coordonnées spatiales de l'image (x, y, et z).
Afin de calibrer la chambre photo en ce qui concerne l'orientation interne, nous analysons des photographies particulières représentant des zones dont la géométrie est parfaitement connue. Ces prises de vue particulières peuvent être réalisées sur des mires planes parfaitement quadrillées. Néanmoins, il existe aussi des méthodes qui s'appuient sur des couples de prises de vue réalisées dans des bâtiments particuliers munies des repères (mires 3D, parking sous-sol à l'IGN, Paris), ce qui conduit à une expression plus stable et rapide de la déformation de lentille.
Cette méthode permet d'accéder aux déformations de l'image liées aux imperfections de la lentille. Ce résultat peut être modélisé par une fonction de déformation généralement polynomiale et exprimée en fonction de la distance au centre de l'image, que l'on peut ensuite appliquer à l'image brute pour obtenir une image « théoriquement » corrigée des déformations. L'équation du polynôme de distorsion radial est de la forme : F(r) = a*r3+b*r5+c*r7 (1) où r est la distance radiale en pixels, a, b et c sont des coefficients.
L'orientation externe définit la position et l'orientation angulaire de l'appareil photo au moment de la prise de vue. Les paramètres de position de l'orientation externe incluent Xo, Yo, et Zo, position du centre de perspective (O) par rapport au système de coordonnées terrestre (X, Y, Z). Zo est généralement égal à la hauteur de l'appareil photo au-dessus du niveau de la mer.
En utilisant les trois angles de rotation : omega (?), phi ( Le vecteur image et le vecteur terrain sont seulement situés sur la même ligne droite si l'un est le multiple scalaire de l'autre (k) :
Une expression matricielle de cette même relation dans le système de coordonnées image va alors s'écrire sous la forme :
L'équation finale qui définit le rapport entre le centre de perspective O de l'appareil photo au moment de la prise de vue et le point P sur le terrain (point p sur l'image) est la suivante :
où xp et yp sont les coordonnées image du point terrain, xo et yo sont les coordonnées images du point principal et f est la distance focale, Xp, Yp et Zp sont les coordonnées terrain du point terrain, Xo, Yo et Zo sont les coordonnées terrain du centre de perspective (figue 3). Cette équation est à la base de la colinéarité qui est employée dans la plupart des opérations photogrammétriques (ERDAS OrthoBASE user's guide, 2002).
-84 -RNTI-E-13
Des points additionnels communs aux deux images stéréo (dits points homologues ou PH), connus uniquement dans le système de coordonnées image, sont intégrés afin d'améliorer l'ajustement du modèle d'aérotriangulation. Ces PH peuvent être repérés manuellement ou de façon automatique par corrélation d'image.
FIGURE 3 -Paramètres d'orientation externe (d'après OrthoBASE user's guide, 2002).
Aérotriangulation
Afin d'accomplir la restitution d'un couple stéréo, une relation entre les paramètres de l'appareil photo, des photos et du terrain doit être établie. Pour définir cette relation on utilise les paramètres d'orientation interne et externe de chaque photo et une représentation précise du terrain par un ensemble de points de contrôle terrestres (PCT).
Les paramètres d'orientation externe d'une photo aérienne sont normalement inconnus et le calcul de ces paramètres est l'obstacle les plus important de la stéréorestitution. Selon les données d'entrée disponibles, plusieurs techniques sont employées pour définir les variables exigées dans la stéréophotogrammétrie.
Parmi eux, l'ajustement par paquet de blocs est le plus rigoureux considérant la minimisation d'erreurs et la distribution des résidus. Casson (2002) signale que -85 -RNTI-E-13 l'aérotriangulation en bloc présente les meilleures précisions et que ce résultat peut s'expliquer par la condition de coplanéité et la présence des PH distribués de manière homogène sur la zone de recouvrement des clichés stéréoscopiques (ce qui compense une répartition des PCT parfois hétérogène). La condition de coplanéité établit que les deux positions du capteur d'un couple stéréoscopique, ainsi qu'un point au sol quelconque et sa position image correspondante sur les deux images, doivent se situer dans un plan commun.
Une fois que chaque équation d'observation est formulée, la condition de colinéarité peut être résolue en utilisant l'ajustement par moindres carrés. Cette technique statistique est employée pour estimer les paramètres inconnus associés à une solution, tout en minimisant l'erreur. En ce qui concerne l'aérotriangulation, cette technique est employée pour : a) estimer ou ajuster les paramètres d'orientation interne et externe ; b) estimer les coordonnées X, Y, et Z des PH ; c) minimiser l'erreur et distribuer les résidus dans l'ensemble des PCT.
Des erreurs systématiques subsistent à l'issue de l'aérotriangulation et se répercutent sur le MNT et les images associées. En mode numérique il s'agit des effets de la non isotropie de la déformation de lentille ainsi que des effets d'une mauvaise disposition des mires dans la zone couverte par les clichés. Maatouk (2004) a réalisé des essais par rapport à la disposition des mires sur modèle réduit. Il montre des conséquences fortes sur la stabilité de l'aérotriangulation (basculement par exemple). Une dissymétrie dans la disposition des mires a entraîné un déplacement en X jusqu'à 5,2 cm dans l'estimation de la position d'un appareil photo situé à 1,60 m d'altitude.
Il existe plusieurs manières de réduire les erreurs systématiques, comme la compensation a posteriori, le calibrage par données complémentaires de terrain, et l'approche par autocalibrage. Tandis que les deux premières méthodes sont coûteuses en temps, les approches par auto-calibrage emploient des paramètres additionnels dans le processus de triangulation pour minimiser les erreurs systématiques.
Les paramètres additionnels sont un moyen extrêmement efficace pour compenser les erreurs systématiques, quelle que soit leur provenance, qu'il s'agisse d'une distorsion résiduelle de l'objectif, de déformations du film (en cas de photo argentique), d'anomalies de réfraction, ou de toute autre source d'erreur (Kraus & Waldhäusl, 1998).
La recherche et le développement intensif des modèles « auto-calibrage » dans la stéréophotogrammétrie se sont intensifiés pendant les années 1970 et 1980, et plusieurs de ces résultats sont opérationnels. Parmi eux, la solution d 'Ebner (1976) consiste en une recherche systématique de déformations types sur la restitution finale des images, en utilisant 12 paramètres additionnels. L'efficacité de cette correction dépend de l'importance relative entre erreur systématique et erreur aléatoire, donc exige une bonne qualité des mesures des PCT. Un nombre plus important de PCT et de PH est rendu nécessaire par l'introduction de ces paramètres additionnels.
Restitution numérique et création du MNT
La restitution numérique cherche à retrouver de manière automatique la position de chaque pixel de l'image 1 représentant le même objet sur l'image 2, dans la zone de recouvrement d'un couple stéréoscopique. La corrélation d'images permet l'identification et la mesure automatiques des points homologues situés dans les zones de recouvrement des deux images.
-86 -RNTI-E-13 La méthode de corrélation d'images à une dimension utilise la géométrie épipolaire comme contrainte géométrique. Pour chaque point p1 dans une image, les différentes positions possibles du point homologue p2 dans l'image associée varient selon l'altitude et le lieu de ces points constitue la courbe épipolaire. La géométrie épipolaire est associée à la condition de coplanéité. La recherche de l'optimum du coefficient de corrélation d'un pixel est donc orientée : chaque point est cherché sur sa courbe épipolaire dans l'autre image.
La corrélation automatique d'images peut s'effectuer sur une large fenêtre (par exemple 7*7 pixels) pour une solution rapide mais qui implique un adoucissement local du relief (Casson et al, 2002). Elle est implémentée dans les logiciels de stéréophotogrammétrie disponibles ce qui fournit une solution valable pour la plupart des applications.
Le MNT final est obtenu au moyen d'une compensation par moindre carré qui s'appuie sur la relation de coplanéité. Les résidus sont minimisés en ajustant chaque position sol. A chaque pixel ne correspond pas forcément un couple de points homologues corrélés.. Le résultat se présente sous la forme d'un ensemble de points irrégulièrement distribués (X, Y et Z). L'efficacité de la corrélation dépend du pas de la corrélation et de la taille de la fenêtre de corrélation, et est également fonction du nombre de pixels effectivement corrélés.
Afin d'obtenir un MNT en surface continue il est nécessaire de faire une interpolation. Ceci peut être effectué dans une grille régulière, mais il est possible aussi de convertir le nuage de point 3D en format TIN par triangulation.
Application à Draix
Le vol
Une mission UAV drone a été effectuée sur la zone du Moulin à Draix le 15 juin 2005 à 11h30, avec des photographies numériques en couleurs naturelles. Aucun système de guidage précis (GPS embarqué) n'était disponible à l'époque des prises de vue. Le guidage du drone a été opéré de façon manuelle. La répartition des mires dans les images n'a donc pu être optimisée « a priori ».
Nous avons préconisé finalement une altitude de vol entre 100 et 200 m selon la résolution spatiale au sol recherchée, une focale courte (40 à 80 mm) et un recouvrement entre clichés de 60%. La vitesse de vol du drone est de l'ordre de 30 à 40 Km/h, ce qui permet d'obtenir des photos nettes (sans effet de filé). 39 photos ont été acquises en plusieurs passages avec une hauteur de vol d'environ 150 m. Nous avons sélectionné le couple photo (figure 1) qui remplit au mieux la demande de recouvrement établie précédemment. La résolution au sol ou taille de pixel sur les images est d'environ 6 cm.
Les points de terrain
Le DGPS ne travaille pas avec la même précision dans toutes les conditions de relief, notamment en fond des ravines profondes. Etant donné cette limitation du système DGPS, nous avons choisi une solution mixte DGPS-Tachéométrie pour une précision satisfaisante (? 3 cm) dans toutes les configurations de terrain.
-87 -RNTI-E-13
A l'intérieur de la zone de recouvrement on a 8 mires-PCT et 388 PV sur sol nu, obtenus par une campagne mixte DGPS-Tachéométrie. La qualité de la donnée de terrain a été analysée en utilisant le levé tachéométrique comme référence. Les résultats exprimés en erreur quadratique moyenne (RMSE) montrent un décalage des positionnements acquis par DGPS en mode RTK de 0,025 m selon les axes Est-Ouest et Nord-Sud, et de 0,030 m en Altitude.
Géométrie
Les paramètres d'orientation interne ont été calculés au préalable (avant aérotriangulation), en utilisant le logiciel Poivilliers E (Etalon, IGN). Une série de photos a été prise dans un bâtiment muni de 65 cibles disposées sur différents plans de façade, avec la chambre Sony DSC-P150 utilisée pendant le vol.
Les distorsions obtenues sur le bord de l'image sont beaucoup plus importantes qu'au centre. L'impact du modèle de déformation de lentille calculé ci-dessus sur la qualité finale du MNT sera évalué suivant le calcul des erreurs par rapport aux données de vérité terrain.
Les paramètres d'orientation externe sont inconnus et ils ont été calculés pendant l'aérotriangulation.
Aérotriangulation
Pour l'aérotriangulation, nous disposons de 8 PCT (en 3D) et 326 PH détectés automatiquement par corrélation d'images à l'intérieur de la zone de recouvrement du couple choisi (ainsi que 4 PCT à l'extérieur, figure 4). La distribution des PH n'est pas homogène : le coté Nord-Ouest des images, plus végétalisé, a un faible potentiel de corrélation ce qui se traduit par une densité plus faible de PH que la zone non végétalisé. Nous avons utilisé le modèle orthogonal d 'Ebner (1976)  Deux PCT ont été rejetés pendant l'ajustement du modèle d'aérotriangulation, sur un total de 8 disponibles, ainsi que 8 PH sur un total de 326 disponibles. Dans le tableau 2 nous pouvons observer les résidus des PCT non rejetés après l'aérotriangulation.
Le PCT mire45 présente les résidus les plus importants, ce qui est cohérent avec la position de la mire (figure 4) au bord de la zone de recouvrement de deux images, où les effets de la déformation de lentille sont plus forts. La mire32, encore plus au bord de l'image, a été rejetée pendant l'aérotriangulation.
MNT par corrélation d'images
Etant donné son meilleur contraste par rapport aux autres canaux disponibles, le canal rouge a été utilisé pour la corrélation automatique des images. La taille finale du pixel MNT -89 -RNTI-E-13 a été fixée à 15 centimètres (soit trois fois la taille originale moyenne du pixel dans le couple stéréoscopique).
ID PCT rX (m) rY ( 
-Résidus des PCT non rejetés après l'ajustement du modèle d'aérotriangulation (en mètres).
Afin d'apprécier le détail atteint dans la restitution du relief par stéréoscopie drone, nous comparons qualitativement sur une ravine un MNT issu de restitution manuelle de photos argentiques au 1/4000°, avec notre résultat. Les courbes de niveau du MNT existant sont à espacement 2 m, celles issues du couple drone sont à espacement 20 cm (figures 5a et 5b, respectivement). La finesse de détail et le plan des courbes de niveau démontrent les potentialités de la restitution détaillée du relief à partir d'images drone.
3 915 pixels ont été effectivement corrélés dans une fenêtre de 285 m 2 (points noirs, figure 5b). Cette densité représente environ 14 points par mètre carré, mais elle n'est pas homogène sur la surface. Ceci correspond à une grille régulière de 20 cm approximativement, mais dans les zones densément corrélées la distance entre points peut atteindre la taille du pixel des images (autour de 6 cm). Il est important de rappeler qu'un couple de pixels effectivement corrélés n'exclut pas une fausse corrélation, ce qui peut être à l'origine d'une anomalie du MNT (un pic ou un fossé).
FIGURE 5 -(a) courbes de niveau sur une ravine à partir de photos aériennes 1/4000 ; (b) courbes de niveau et points effectivement corrélés (en noir), à partir d'images drone.
Les points effectivement corrélés suivent les zones de rupture radiométrique dans les images, telles que les lignes d'ombres. En absence de végétation, les zones de rupture radiométrique sont les lignes de contraintes classiques d'un MNT : les thalwegs et les crêtes.
-90 -RNTI-E-13
En présence de végétation les points effectivement corrélés sont bien contrôlés par les bordures de la végétation et ses ombres. La dépendance de la corrélation sur les points de rupture abrupte de la radiométrie laisse une grande partie de la surface des photos inexploitée dans la restitution du relief (environ 2% du total des pixels disponibles dans la zone de recouvrement ont été effectivement corrélés). La figure 6 montre une vue 3D du MNT calculé.
FIGURE 6 -Vue 3D du MNT drone, démarquée en noir la ravine analysée dans la figure 5 (en rouge les points de validation).
Evaluation de qualité
Pour tester l'effet des différents traitements pendant l'aérotriangulation dans la qualité de la restitution du relief, trois MNT ont été construits à partir du même couple stéréoscopique d'images drone, avec le même jeu de PCT et de PH : a) MNT sans correction de lentille et sans auto calibrage (MNT.o) ; b) MNT avec correction de lentille mais sans auto calibrage (MNT.L) ; et c) MNT avec correction de lentille et avec auto calibrage (MNT.LA). Les résultats de l'évaluation de qualité altimétrique des MNT par rapport aux données de vérité terrain DGPS-tachéométrie, sont présentés dans la figure 7.
FIGURE 7 -Statistiques de contrôle de qualité (a) et histogrammes de fréquence des erreurs en Z (b), pour chaque MNT analysés (unités en mètres).
L'amélioration la plus importante dans la qualité du MNT provient de la correction de la déformation de lentille (cf. figure 7a et 7b). Avec l'auto calibrage (toutes corrections),
-92 -RNTI-E-13 l'erreur quadratique moyenne (RMSE) en Z pour le MNT.LA diminue d'environ 80% par rapport à la RMSE du MNT.o (aucune correction)..
En effet l'application de ces corrections permet de :
• ramener la moyenne de l'erreur pour le MNT.LA à 8 cm, ce qui est à peine supérieur à la taille initiale du pixel de l'image drone (6 cm).
• Améliorer fortement la précision du MNT (écart-type), ce qui est garant d'une meilleure cohérence de la représentation du relief.
• Réduire fortement la plage de variation de l'erreur (5,11 m pour le MNT.o contre 1,85 m pour le MNT.LA). La mise en oeuvre de l'auto calibrage sur les images (MNT LA) nous permet d'améliorer encore de manière significative, notamment en ce qui concerne les erreurs systématiques. Ces affirmations sont bien évidentes en observant les histogrammes de fréquence des erreurs pour chaque MNT analysé (figure 7b). L'histogramme des erreurs pour le MNT.o (ligne bleue) est plat et étendu avec une plage de variation très importante (figure 7a). Avec l'application de corrections (MNT.L et MNT.LA, lignes verte et rouge respectivement) il se déplace vers l'axe des ordonnées et il se resserre, les valeurs des erreurs sont plus concentrées.
Dans la figure 8 nous pouvons observer les séries des erreurs (résidus) correspondants à un profil de crête et un profil de thalweg. La tendance générale des séries des erreurs montre l'effet de déformation radiale de lentille surtout pour le MNT sans aucune correction (MNT.o, carré clair dans la figure 8), dans chaque profil. Les résidus du profil thalweg sont d'une ampleur plus importante que ceux de la crête.
Après la correction de la déformation de lentille, les résidus qui correspondent au MNT.L (cercle foncé, figure 8) montrent déjà une amélioration significative, surtout pour la crête. Par rapport au thalweg, nous pouvons observer toujours des erreurs systématiques même si elles ont été réduites en ampleur. L'utilisation de l'approche par auto calibrage (MNT.LA, triangles clairs dans la figure 8) ne représente pas une amélioration significative pour le profil qui correspond à la crête. Toutefois, ce n'est pas le cas du profil thalweg où le profil MNT.LA semble plus affecté par l'erreur systématique.
Analyse selon la distance au centre de l'image
Afin d'avoir une vision générale de la distribution spatiale des erreurs, nous avons calculé la distance au centre de l'image (rayon) de chaque point de validation (figure 9), pour les trois niveaux de traitement étudiés. L'effet de la déformation de la lentille dans la distribution des erreurs est bien évident, l'erreur est plus forte dès qu'on s'éloigne du centre de l'image (figure 9a). L'application du modèle de déformation de lentille (figure 9b) montre une amélioration significative de la qualité du MNT. L'effet d'erreur systématique du MNT est encore visible (forme de la ligne de tendance), mais cet effet a été significativement diminué.
La figure 9c correspond au MNT.LA avec toutes corrections. La distribution des erreurs est plus resserrée autour de l'axe des X, correspondant à la baisse de la moyenne des erreurs en Z de 0,19 à 0,08 cm (figure 7). L'effet des erreurs systématiques a été aussi diminué, la pente de la ligne de tendance (0,0033, figure 9c) est plus faible que dans le cas sans auto calibrage (0,0069, figure 9b).
FIGURE 8 -Séries des erreurs « vérité terrain (DGPS-tachéométrie) -différents MNT » selon un profil de crête et un profil de thalweg.
La RMSE totale finale obtenue (0,22 m), et la présence de résidus systématiques laisse encore la possibilité d'améliorations à faire, surtout autour d'une estimation plus performante de paramètres d'orientation interne de l'appareil photo (modèle de déformation de lentille, distance focale).
Analyse d'un versant (validation par LiDAR terrestre)
Pour avoir une évaluation de qualité « surface continue » du MNT.LA, nous avons utilisé un scan LiDAR terrestre comme vérité terrain. Le balayage au sol du faisceau laser a produit un nuage dense de données, même s'il y a beaucoup d'espaces vides dus aux effets d'ombre portée. Nous avons sélectionné une ravine où les données LiDAR étaient plus denses, en enlevant aussi la végétation de l'information originale. Cette ravine est délimitée par un polygone ( figure 1 en bas, et figure 10).
-94 -RNTI-E-13 -95 -RNTI-E-13
Une nouvelle grille régulière a été calculée à l'intérieur du polygone LiDAR sélectionné. Cette grille a été ré-échantillonnée avec une maille de 5 cm en utilisant la valeur moyenne comme filtre de bruit dans l'information brute et puis une interpolation, afin de créer un MNT-LiDAR.
Une évaluation de qualité a été faite dans le polygone sélectionné (figure 10), en comparant le MNT.LA drone aux deux sources de données de vérité terrain : 1) 30 points de contrôle de qualité DGPS-Tachéométrie, et 2) 101 912 pixels du MNT-LiDAR terrestre (pixel 5 cm). Les résultats (tableau 4) montrent que même si l'écart type en Z est similaire pour les deux jeux de données de vérité terrain, la RMSE est plus importante pour le LiDAR terrestre. Malgré une évaluation un peu moins précise avec le LiDAR terrestre, celui-ci reste très intéressant parce qu'il propose un très grand nombre de points de validation, y compris sur des zones d'accès difficile où on ne peut aller avec le GPS.
DGPS-
Nous avons établi une différence entre le MNT.LA drone et le MNT-LiDAR de référence, afin d'effectuer une évaluation de qualité en « surface continue » (MNT.LA moins MNT-LiDAR, figure 10).
Les valeurs positives de cette différence sont principalement situées en tête de ravine, et moins évidemment le long de son côté droit, dénotant une sur estimation de Z pendant la corrélation d'images drone. Nous pouvons trouver une situation inverse au côté gauche de la même ravine, où la sous estimation est plus évidente.
Les nombreuses taches bleues et rouges intenses sont associées à des anomalies locales de type « pics et fossés », respectivement, dus aux fausses corrélations pendant la restitution numérique du relief. Ces nombreuses taches isolées affectent la qualité finale du MNT mais aussi sa continuité et sa cohérence. Elles pourraient être éliminées dans notre cas par un post traitement de type algorithme de lissage. D'autres solutions, en développement, intègrent ces régularisations directement dans l'algorithme de corrélation d'image, par exemple le logiciel MicMac (Pierrot D., 2007).
Ces appréciations n'auraient pas été possibles en utilisant seulement les points de validation pris par DGPS-Tachéométrie.
L'évaluation d'erreur en Z dépend non seulement du décalage en Z lui-même mais également des décalages selon les axes X et Y, particulièrement dans un relief très accidenté comme Draix. Une optimisation 3D (MNT.LA au MNT-LiDAR) a été effectuée pour calculer le décalage moyen global dans chaque axe : -0,004, 0,014, et -0,272 (tous en mètres) pour X, Y et Z respectivement. Ces résultats montrent un décalage significativement plus important en Z, par rapport aux axes X et Y.
-96 -RNTI-E-13 
Conclusion
Le problème traité dans l'article concerne l'utilisation de drone dans l'acquisition d'images stéréoscopiques pour le développement de MNT précis et cohérents par stéréophotogrammétrie. Les vecteurs légers ne permettent pas d'utiliser des appareils photo professionnels, certes plus précis mais trop lourds pour être placé sur des drones. Les prises de vue par appareil « grand public » permettent une grande souplesse d'utilisation. Notre objectif était de préciser les points où la méthode devient particulière, et de vérifier jusqu'où les incertitudes citées sont surmontables et à quel compromis « qualité -souplesse d'utilisation » on peut arriver. L'application de la méthode IGN (Poivilliers E) pour le calcul des paramètres d'orientation interne a été traduite par une amélioration très significative dans la précision du MNT, en passant d'une RMSE totale de 1,09 à 0,31 mètre. A ce stade, il reste toujours des erreurs systématiques résiduelles. Elles sont probablement dues à plusieurs effets : instabilité de vol et disposition non régulière des mires dans les images ; déformations de lentille non parfaitement corrigées liées au modèle (polynomial) de correction utilisé. L'approche par auto calibrage (Ebner, 1976) compense en partie ces erreurs systématiques. La moyenne finale de l'erreur est de 8 cm, proche de la taille du pixel de l'image drone (6 cm). La RMSE totale obtenue est alors de 22 cm, soit trois fois et demie la taille de pixel initial de l'image.
Toutefois, il faut remarquer que l'analyse de distribution d'erreurs par rapport à la position dans le paysage, reflète que les observations sur les crêtes atteignent une précision significativement meilleure que sur les fonds des ravines. Ceci est dû au contraste entre les images et les effets d'ombres portées, toujours plus présentes au fond des ravines très profondes.
Afin de bien développer le potentiel des images drone pour la restitution du relief par stéréophotogrammétrie dans des zones à fortes pentes (sans sacrifier la souplesse d'utilisation), on doit surmonter plusieurs contraintes comme l'instabilité de vol et les déformations de lentille. Un autre facteur perfectible est l'efficacité de la corrélation d'images, qui contrôle la qualité de la restitution de relief. La distribution de points effectivement corrélés montre en effet la dépendance pratiquement exclusive de la corrélation sur les points de rupture abrupte de la radiométrie, en laissant ainsi une grande partie de la surface des photos inexploitée. L'algorithme de corrélation d'images disponible sur ERDAS Imagine LPS ne dispose pas de mécanismes pour contrôler les fausses corrélations entre pixels d'un couple stéréoscopique, résultant en nombreuses irrégularités dans la surface du MNT (pics et fossés). Les données « vérité terrain » par LiDAR terrestre ont permis de détecter ces irrégularités grâce à l'énorme densité de l'information disponible, et elles constituent un excellent complément aux données DGPS-Tachéométrie dans la validation de MNT très détaillés.
Le développement de routines plus performantes destinées à augmenter la densité de points effectivement corrélés en modifiant le critère de sélection des points, seraient certainement une avancée dans ce domaine : par exemple le corrélateur MEDICIS (CNES), qui recherche un optimum local en analysant le rapport signal à bruit des images (Casson et al., 2002).
Nos résultats montrent une amélioration significative par rapport aux résultats de Henry et al. (2002), Maatouk (2004) et Raclot et al. (2005), dans les mêmes conditions de relief. Les résultats révèlent le potentiel du vecteur drone pour le développement des MNT peu coûteux, avec une très haute résolution spatiale, malgré de nombreuses contraintes.

Introduction
Les motifs séquentiels sont étudiés depuis plus de 10 ans (Agrawal et Srikant (1995)). Ils ont donné lieu à de nombreuses applications.Des algorithmes ont été proposés, basés sur le principe d'Apriori (Masseglia et al. (1998); Zaki (2001); Ayres et al. (2002)) ou sur d'autres propositions (Pei et al. (2004)). Récemment, les motifs séquentiels ont été étendus aux motifs séquentiels multidimensionnels par Pinto et al. (2001), Plantevit et al. (2005), et Yu et Chen (2005) dans l'objectif de prendre en compte plusieurs dimensions d'analyse. Par exemple, dans Plantevit et al. (2005), les règles telles que Un client qui achète une planche de surf avec un sac à NY achète plus tard une combinaison à SF sont découvertes. Toutefois, le nombre de motifs extraits dans une base de données peut être très important. C'est pourquoi des représen-tations condensées telles que les motifs clos ont été proposées pour l'extraction des itemsets (Pasquier et al. (1999); Pei et al. (2000); Zaki et Hsiao (2002); El-Hajj et Zaïane (2005)) et des séquences (Yan et al. (2003); ). Les clos permettent de disposer à la fois d'une représentation condensée des connaissances extraites et d'un mécanisme d'extraction plus efficace afin d'élaguer significativement l'espace de recherche. Néanmoins, ces propositions ne peuvent pas être directement appliquées aux motifs séquentiels multidimensionnels pour la raison suivante : une super séquence peut être obtenue de deux façons (1) une plus longue séquence (plus d'items) ou (2) une séquence plus générale (plus de valeurs non spécifiées) ce qui modifie les définitions des méthodes précédemment introduites.
Notre contribution majeure est la définition d'un cadre théorique pour l'extraction de motifs séquentiels multidimensionnels clos ainsi qu'un algorithme permettant de rechercher de tels motifs. Nous adoptons une méthode basée sur le paradigme "pattern growth" (Pei et al. (2004)) afin de proposer une solution d'extraction de motifs séquentiels multidimensionnels clos efficace. De plus, nous souhaitons définir un algorithme qui se dispense de gérer un ensemble de clos candidats, seules les séquences closes étant ajoutées à l'ensembles des clos.
Motivations et Problématique
Nous définissons ici le cadre théorique de l'extraction des motifs séquentiels multidimensionnels clos à partir de l'approche définie par Plantevit et al. (2005).
Défini par Pasquier et al. (1999), un motif clos est un motif qui n'a pas le même support que tous ses super-motifs. Les motifs clos permettent de représenter les connaissances extraites de manière compacte sans perte d'information et sont généralement associés à des propriétés qui permettent de réduire sensiblement l'espace de recherche à l'aide d'opérations d'élagage autres que l'élagage élémentaire des motifs infréquents. Dans un contexte multidimensionnel, une séquence peut être plus spécifique qu'une autre si elle contient plus d'items (séquence plus longue), ou si elle contient des items plus spécifiques (moins de valeurs *).
Si ? est plus spécifique que ?, nous notons ? ?S ? où ?S représente la relation de spécialisation. Soient s1 = b1, c1), (a2, * , c1)}{( * , b2, c2)} s2 = * , * ), (a2, * , c1)}{( * , b2, c2)} et s3 = b1, c1)}{( * , b2, c2)} trois séquences multidimensionnelles. On a s2 ?S s1 et s3 ?S s1. A partir de cette définition, nous pouvons définir une séquence multidimensionnelle close. Le contexte multidimensionnel rend l'approche générer-élaguer très difficile en raison du très grand nombre de combinaisons d'items possibles. Nous utilisons donc le paradigme "pattern growth" introduit par Pei et al. (2004) s'appuyant sur un parcours en profondeur de l'espace de recherche. L'extraction des motifs se fait en concaténant à la séquence traitée (préfixe) les items fréquents sur la base de données projetée par rapport à cette séquence préfixe. Le terme de g-k-séquence désigne les séquences composées de k items au sein de g itemsets. 
}, {e
1 , e 2 , . . . , e
Lorsqu'on considère des séquences d'itemsets, l'opération de concaténation peut s'effectuer de deux façons différentes : (1) concaténation inter itemset où l'item est inséré dans un nouvel itemset (le (g + 1)
` eme itemset de la séquence) :
(2) concaté-nation intra itemset où l'item est inséré dans le dernier itemset de la séquence (le g ` eme itemset de la séquence) : S ? = s 1 , s 2 , . . . s g ? {e ? } . Ordonner les items au sein des itemsets est un des moyens d'améliorer le processus d'extraction en éliminant de façon efficace des cas déjà examinés. La valeur joker * n'existe pas comme valeur réelle dans la base de données. Ainsi, les solutions proposées dans un contexte classique par Yan et al. (2003) (CloSpan) et  (BIDE) ne sont pas directement applicables au contexte multidimentionnel avec valeur joker.
TAB. 1 -Contre exemple : ordre dans les itemsets
Le Tab. 1 illustre le fait que la valeur joker n'est pas explicitement présente dans les nuplets, il n'est pas possible de définir un ordre lexicographique total. Ainsi, il n'est pas possible d'obtenir la séquence 1 , b 2 ), ( * , b 1 )} CloSpan extrait l'item (a 1 , b 2 ) avec un support de 2 et construit ensuite la base projetée à partir de la séquence 1 , b 2 )} qui contient les sé-quences et 2 , b 1 )} L'item ( * , b 1 ) n'apparaîtra donc pas comme fréquent dans cette base projetée alors qu'il l'est dans la base initiale. Il est donc nécessaire d'ordonner les sé-quences en prenant en compte le caractère joker (*) comme valeur de dimension possible.
Un pré-traitement sur la base de données par extension à l'ensemble des n-uplets contenant la valeur joker étant trop coûteux, nous souhaitons traiter cette particularité à la volée pendant le processus d'extraction de motifs séquentiels multidimensionnels clos. C'est pourquoi nous introduisons un ordre lexico-graphico-specifique(LGS) qui est un ordre alpha-numérique par rapport au degré de précision des items (nombre de * dans l'item). La priorité est ainsi donnée aux items les plus spécifiques.Nous tentons de matérialiser localement cet ordre au sein de chaque transaction à l'aide d'une fonction LGS-Closure qui est une application d'un itemset i vers la fermeture de i en respectant l'ordre LGS < lgs .
Extensions et clos
Actuellement, la plupart des algorithmes d'extraction de motifs clos ont besoin de maintenir l'ensemble des clos (ou juste candidats) en mémoire et vérifier en post traitement si un motif peut être absorbé ou non par un autre motif. Mais la maintenance d'un tel ensemble est très coûteuse, c'est pourquoi notre objectif est d'éviter une telle gestion. D'après la définition d'un motif séquentiel multidimensionnel clos, si une g-k-séquence S = s 1 , . . . , s g n'est pas close alors il existe une séquence S ? de même support telle que S ? S S ? . La définition 4 présente les cinq différents types de construction d'une séquence plus spécifique à partir d'une séquence préfixe. 
Extraction de Motifs Séquentiels Multidimensionnels Clos
Nous verrons que le dernier point peut être facilement détecté grâce à l'ordre de parcours dès lors que les précédents le sont.
Théorème 1 (Extension bi-directionnelle). Une séquence S est close si et seulement si elle n'accepte aucune extension vers l'avant, ni extension vers l'arrière, ni spécialisation.
Pour déterminer si une séquence préfixe est close, nous devons donc vérifier si elle ne peut pas avoir d'extension vers l'avant ou vers l'arrière ainsi que de spécialisation d'item. Le lemme suivant facilite l'étude des extensions vers l'avant.
Lemme 1. Pour une séquence S, l'ensemble complet des extensions vers l'avant est équi-valent à l'ensemble des items localement fréquents sur la base projetée par rapport à S ayant un support égal à support(S).
Pour les extensions vers l'arrière, la recherche d'extension est moins triviale. En effet, une extension vers l'arrière peut être réalisée de deux façons différentes :
. . , s g . Soit un item s'insère dans un nouvel itemset, entre deux itemset s i et s i+1 existants (inter-itemsets), soit il s'insère dans un itemset existant (intra itemset). Comme une séquence peut se répéter plusieurs fois à l'intérieur d'une séquence de données, on peut identifier g intervalles pour localiser les possibles insertions vers l'arrière d'une g-k-séquence. Il faut maximiser ces intervalles afin de détecter toutes les extensions possibles vers l'arrière.
Définition 5.
Etant données une g-k-séquence préfixe S p = 1 , s 2 , . . . , s g et une séquence de données S, le i ` eme intervalle maximal se définit de la façon suivante : pour i = 1 : la sous-séquence du début de S jusqu'à strictement avant da(s 1 ) la dernière apparition de s 1 dans S telle que da(s 1 ) < da(s 2 ) < . . . < da(g) pour 1 < i ? g : la sousséquence entre la première apparition de la séquence 1 , s 2 , . . . , s i?1 notée pa( 1 , s 2 ,-. . . , Une séquence préfixe ne peut pas être close s'il existe une spécialisation d'un item de la sé-quence préfixe. L'ordre LGS, que nous adoptons, nous permet d'extraire les séquences closes en commençant par celles qui contiennent les items les plus spécifiques (le moins de valeurs * ). Ainsi, s'il existe une spécialisation possible d'une séquence préfixe considérée, alors la "séquence spécialisée", qui contient au moins un item plus spécifique, sera déjà présente dans l'ensemble des clos déjà extraits. Ainsi, si une séquence est potentiellement close (pas d'extensions vers l'avant ou l'arrière), il suffit de vérifier qu'il n'existe pas de séquence plus spécifique dans l'ensemble des séquences closes déjà extraites.
Elagage de l'espace de recherche
Tout en recherchant les nouvelles séquences fréquentes avec l'algorithme d'énumération des séquences, nous pouvons utiliser la propriété de fermeture bidirectionnelle pour vérifier si la séquence est close dans le but de générer un ensemble non redondant de connaissances. Bien que la propriété de fermeture retourne un ensemble plus compact, cela ne permet pas d'extraire les séquences plus efficacement. Par exemple, il peut n'y avoir aucun clos au delà d'un certain noeud dans l'arbre des préfixes, il faudrait donc éviter de parcourir inutilement la branche et réduire ainsi significativement l'espace de recherche.
Comme nous l'avons dit précédemment, une séquence peut apparaître plusieurs fois dans une séquence de données. Dans la définition 5, nous avons introduit la notion d'intervalle maximal afin de pouvoir détecter toutes les extensions vers l'arrière. Nous désirons minimiser ces intervalles afin de détecter les séquences "non-prometteuses". Nous définissons ainsi la notion d'i ` eme intervalle minimal. 
Travaux Connexes
Nos travaux sont au carrefour de plusieurs problématiques : (1) l'extraction de séquences multidimensionnelles, (2) l'extraction de séquences closes. Pinto et al. (2001) sont les premiers à aborder le problème de l'extraction de motifs séquen-tiels dans un contexte multidimensionnel. Les séquences extraites ne contiennent pas plusieurs dimensions puisque la relation d'ordre (temps) concerne uniquement la dimension produits. Les autres dimensions sont "statiques" et seulement utilisées pour caractériser le profil des utilisateurs. Yu et Chen (2005) proposent d'extraire des séquences dans un contexte de web usage mining en considérant trois dimensions (pages, sessions, jours) qui appartiennent à une même hiérarchie. Ainsi, les séquences extraites décrivent des corrélations temporelles entre objets en considérant une seule dimension (pages). Plantevit et al. (2005) proposent des règles définies sur plusieurs dimensions d'analyse non "statiques".
Même s'il existe de nombreux travaux pour l'extraction d'itemsets clos (Pasquier et al. (1999); Pei et al. (2000); Zaki et Hsiao (2002);El-Hajj et Zaïane (2005)), il n'y a, à notre connaissance, que deux propositions pour les motifs séquentiels clos : BIDE de  et CloSpan de Yan et al. (2003). CloSpan et BIDE ne peuvent pas être directement adaptés dans notre contexte multidimensionnel à cause de la valeur joker. De plus CloSpan gère un ensemble de séquences closes candidates et effectue un post-traitement coûteux (quadratique en la taille de l'ensemble).
Nous pouvons également citer les travaux de Songram et al. (2006) qui abordent le problème des motifs séquentiels clos dans un contexte multidimensionnel en proposant une repré-sentation condensée des motifs définis par Pinto et al. (2001). Cependant, il s'agit de séquences définies sur une seule dimension où les autres dimensions sont "statiques".
Conclusion
Dans cet article, nous avons proposé une approche complète (définitions et algorithmes) pour l'extraction de motifs séquentiels multidimensionnels clos. Ces motifs permettent d'obtenir une représentation condensée de l'ensemble des motifs séquentiels multidimensionnels sans aucune perte d'information. De plus, ceci permet de calculer différentes mesures (e.g. la confiance pour les règles séquentielles) sans passe supplémentaire sur la base de données puisque tous les supports sont connus. Outre leur puissance représentative, les motifs multidimensionnels clos permettent d'utiliser des propriétés supplémentaires d'élagage, ce qui est prépondérant pour assurer le passage à l'échelle de telles techniques d'extraction. Notre approche adopte le paradigme pattern growth et permet l'apparition de valeurs joker * dans les motifs pour une extraction plus pertinente.
Les perspectives associées aux motifs séquentiels multidimensionnels clos sont nombreuses : prise en compte des hiérarchies, autres représentations condensées (non-dérivables Calders et Goethals (2002), k-libres Boulicaut et al. (2003)) et extraction de motifs séquentiels multidimensionnels sous contraintes (top k).

Introduction
L'annotation sémantique est devenue l'une des approches privilégiées par les travaux sur le web sémantique. Les travaux visant à extraire semi-automatiquement ces annotations, plus particulièrement à partir de textes, ont connu ces dernières années une avancée importante. Dans ce contexte, des outils de traitement automatique de la langue naturelle (TALN) sont proposés. Ces outils reposent en général sur des méthodes linguistiques telles que la projection de patrons morpho-syntaxiques ou des méthodes statistiques (fréquence d'apparition). Les méthodes de TALN peuvent être semi-automatiques (l'intervention de l'expert du domaine est alors requise) ou automatiques (dans ce cas, les approches proposées requièrent une certaine spécialisation dans un domaine particulier (Aussenac-Gilles et al., 2006)). Les approches utilisées jusqu'à présent reposent en général sur l'extraction de termes, certaines permettent également l'extraction de relations entre ces termes, mais en ignorant en général le contexte de leur apparition.
Dans le cadre de cette problématique, nous proposons une approche de modélisation, d'extraction et d'exploitation des annotations, qui prenne en compte leurs contextes. La limite observée, concernant les approches d'extraction des termes pour l'annotation, a été notre principale motivation pour offrir des annotations qui représentent au mieux le contenu d'un document. Nous considérons l'annotation sémantique d'un document comme une image par un annotateur (humain ou programme) du contenu de ce document. Cette annotation sémantique doit être exploitable par la machine et de la qualité de cette image dépend son exploitation par l'application visée. Ce travail s'inscrit dans le cadre du projet SEVENPRO qui a comme objectif de développer, en reposant sur des technologies et des outils qui aident à la fouille de connaissances sur un produit, des corpus de textes multimédia et sur la réalité virtuelle 3D enrichie sémantiquement.
Tout d'abord, dans la section 2, nous allons analyser quelques travaux sur l'extraction des annotations à partir du texte. Puis dans la section 3, nous aborderons notre proposition sur la modélisation de la notion du contexte. Dans la section 4, nous proposerons notre approche d'extraction automatique des annotations contextuelles illustrée par des exemples issus du projet SEVENPRO et nous étudierons les possibilités pour inférer/exploiter les annotations contextuelles. La section 5 présentera nos conclusions.
Etat de l'art
Nous nous intéressons plus particulièrement aux travaux qui permettent de produire des annotations d'une manière automatique ou semi-automatique et qui concernent la notion de contexte. Cependant, pour une vue plus approfondie sur l'annotation sémantique, citons (Prié Y. et al., 2004) ou encore (Amardeilh F., 2007).
Par rapport à notre problématique d'utilisation de la notion de contexte lors de l'extraction des annotations, nous avons classé les travaux en deux grandes catégories : extraction d'annotation par le contenu du document et extraction d'annotation à partir de sources externes au document.
Extraction d'annotation par le contenu du document
On distingue deux types de techniques d'annotation de documents par le contenu (ou indexation) : la technique classique, qui consiste en général à attribuer un ensemble de mots clés (ou termes) à chaque document, et la technique sémantique qui attribue une annotation basée sur des concepts (et non de simples mots-clés) et éventuellement sur les relations entre eux. Les travaux visant à extraire des annotations par le contenu se focalisent généralement sur l'extraction des termes. (Guarino et al., 1999)  (Khelif et al., 2005) prennent également en compte les relations sémantiques entre termes. Dans (Guarino et al., 1999), les auteurs décri-vent OntoSeek un système de recherche documentaire en ligne pour les « pages jaunes ». Afin de construire automatiquement des résumés, (Berri J., 1996) utilise la méthode d'exploration contextuelle (Desclés J. P. et al., 1994) qui repose sur des critères linguistiques et qui consiste à affecter des étiquettes sémantiques aux phrases contenant des indicateurs pertinents. Dans (Desmontils et al., 2002), est proposée une approche supervisée pour indexer des ressources web en reposant sur le contenu des pages web à l'aide d'une ontologie ; les termes sont pondérés par rapport à leur importance dans la page (titre, paragraphe,…). D'autres travaux utilisent les techniques d'extraction d'information pour l'annotation de textes dans un domaine particulier : par exemple, la génomique (Nédellec C., 2004).
Extraction d'annotation à partir de sources externes au document
Les travaux cités dans cette partie font appel à des ressources externes au document. (Njmogue, et al. 2004) propose une approche basée sur un référentiel métier. L'idée principale est que l'indexation d'un document dépend des activités de l'entreprise et non pas des mots clés du document. Cette approche utilise à la fois une analyse linguistique et statistique du document et un traitement sémantique. Nous pouvons souligner que les activités de l'entreprise peuvent être considérées comme un contexte d'utilisation des documents. Dans (Abrouk, 2006), l'auteur propose une approche pour l'annotation semi-automatique de ressources selon les liens de référencement et ce, sans connaissance préalable du contenu du document. D'autres travaux se sont intéressés à modéliser le processus de recherche d'information et la modélisation de l'utilisateur. Nous citons à titre d'exemple (Hernandez, 2005)  
Type d'objets manipulés
Nous nous intéressons à l'extraction d'annotations contextuelles à partir de textes : par conséquent, les objets que nous manipulons sont de type textuel. Un « Objet Textuel » (OT) est défini comme un élément du texte (mot, terme, phrase, titre, texte mis entre parenthèses, paragraphe, section, partie de phrase,…) qui transmet une sémantique. Nous définissons aussi un « Objet Sémantique » (OS) comme la sémantique transmise par un objet textuel et dont nous cherchons à déterminer le contexte. D'une manière plus simple, un objet sémanti-que est la représentation sémantique associée à un objet textuel. Nous avons adopté dans notre approche, la représentation de la sémantique par des concepts et des relations provenant d'une ontologie. L'utilisation de la structure du texte, à travers les objets textuels, nous permet de choisir le niveau de détail à étudier ou « granularité ». En effet, nous pouvons étudier, à titre d'exemple, d'une part les « paragraphes » et les RC entre eux, et d'autre part les RC entre « phrases ». La difficulté réside principalement dans le choix de la bonne granularité à étudier. En outre, la notion de granularité vient conforter notre vision sur la récursivi-té entre les contextes et ainsi assurer la présence du contexte à tous les niveaux d'abstraction.
Par conséquent, nous pouvons étudier les RC, non seulement entre les objets textuels du même niveau d'abstraction, mais entre les objets appartenant à des niveaux différents.
La portée de validité d'une sémantique
Définir une portée de validité d'un objet sémantique donné revient à chercher un ensemble d'objets sémantiques dans lequel il est utilisable pour raisonner sans produire des incohé-rences sémantiques. Du point de vue des objets sémantiques, la notion de validité permet de distinguer : les objets sémantiques valides quel que soit le contexte et les objets sémantiques valides dans un (ou plusieurs) contexte(s) particulier(s). Pour le contexte, la notion de validité offre la possibilité d'étudier la portabilité entre contextes, c'est-à-dire la validité dans un autre contexte, des objets sémantiques valides dans un contexte donné.
Jusqu'à présent, nous avons montré notre vision du contexte et sa modélisation avec les annotations contextuelles. Reste à élaborer une approche permettant d'extraire ces annotations contextuelles.
Extraction et exploitation des annotations contextuelles
Avant de proposer l'approche d'extraction des annotations contextuelles, définissons les différentes relations contextuelles structurelles/sémantiques que nous cherchons à extraire.
Relations contextuelles structurelles/sémantiques
Comme souligné précédemment, plusieurs niveaux de granularité existent, pour les annotations contextuelles, selon les objets textuels que nous voulons étudier (phrase, paragraphe,…). Nous allons proposer une approche basée sur un niveau de granularité particulier que nous jugeons riche sémantiquement. Les objets textuels, que nous manipulerons à ce niveau, ont les « liens logiques » comme délimiteurs. Les liens logiques (Asher et al., 2003) (relation rhétorique, connecteurs logiques ou relations de discours,…) sont définis comme les liens qu'entretiennent les idées entre elles dans un texte argumentatif et servent aussi à assurer les articulations et la progression d'un texte.
Des travaux se sont déjà intéressés à identifier ces relations dans un texte, nous citons (Saito et al., 2006) qui décrit un système permettant d'identifier les relations de discours entre deux phrases qui se suivent en langue japonaise. Aussi, (Marcu et al., 2002) propose une approche non supervisée pour identifier les catégories des relations de discours (CONTRAST,…). D'autres (Teufel et al., 2000) proposent une approche qui détecte les actions et les affecte à leurs types en exploitant la structure des documents scientifiques (Aim, Own,…). Une autre approche (Desclés J. P., 2006)  
Relations contextuelles spatiales
Nous définissons une relation contextuelle spatiale comme toute relation exprimant la position d'un objet (textuel ou sémantique) par rapport aux autres objets de même granularité ou non. Pour les objets textuels, nous pouvons prendre à titre d'exemple les relations de succession, appartenance,…. Dans le cas des objets sémantiques nous aurons par exemple des relations exprimant un rang (devant, derrière, après...) ou un lieu (dans, chez, sous...).
Relations contextuelles temporelles
Nous définissons une relation contextuelle temporelle comme toute relation exprimant la notion de temps entre un objet (textuel ou sémantique) et d'autres objets du même niveau de granularité ou non. Pour les objets sémantiques, nous pouvons avoir à titre d'exemple comme liens logiques (avant, depuis, pendant,...). Dans le cas des objets textuels, nous pouvons considérer le temps des verbes (présent, futur,…) comme une relation temporelle qui exprime le moment où la sémantique de ces objets sera manifestée.
Relations contextuelles diverses
Nous appelons relations contextuelles diverses des relations exprimant une notion séman-tique quelconque (ni spatiale ni temporelle) d'un objet (textuel ou sémantique) par rapport aux autres objets du même niveau de granularité ou non.
Dans le cas d'un objet textuel, un exemple de ce type de relation contextuelle peut être : le degré d'importance entre un paragraphe et son titre. Pour les objets sémantiques, toutes les relations de lien logique qui n'expriment pas l'espace ou le temps peuvent être prises en compte : une addition (de plus, d'ailleurs,…), une illustration (ainsi, comme,…).
Approche d'extraction des annotations contextuelles
Nous proposons une approche d'extraction d'annotations contextuelles répartie en deux grandes parties : manipulation textuelle et manipulation sémantique ( voir FIG. 1).
FIG. 1 -Etapes d'extraction des annotations contextuelles
Extraction et exploitation des annotations contextuelles
Manipulation textuelle
Cette partie a pour objectif d'obtenir un ensemble d'OT et les RC structurelles entre eux. Identification des objets textuels. Identifier ces objets revient à identifier les titres, phrases, lien logique, mots,… ainsi que les arguments de chaque lien logique présent dans le texte. Nous avons utilisé dans cette étape la bibliothèque de développement d'ingénierie linguistique GATE (Cunningham al., 2002) qui repose sur l'application successive (chaîne de traitement) de transducteurs 1 aux textes. Suivant une liste de liens logiques, des règles JAPE 2 sont gérées automatiquement pour obtenir la position dans le texte des liens logiques. D'autres règles sont construites manuellement pour identifier les marqueurs des indices numériques qui précèdent certaines phrases. Les règles JAPE sont appliquées en tant que transducteur dans la chaîne de traitement. D'autres transducteurs fournis par défaut dans GATE nous ont permis d'identifier les phrases et les paragraphes dans le texte. Nous avons exploité, à ce niveau, les marqueurs de position tels que le début et la fin dans le texte (d'une phrase, d'un lien logique,…) pour identifier les arguments des liens logiques dans le texte. Identification des relations contextuelles structurelles. Afin d'identifier les RC structurelles citées dans (4.1.1), nous avons (a) identifié les titres à l'aide des marqueurs numériques, qui précèdent certaines phrases tels que « les numérotations de type '6.2.1'», ainsi que des heuristiques telles que « une seule phrase existante dans le paragraphe qui contient le titre (cette phrase représente le titre lui même) ». Nous soulignons que le transducteur fourni par défaut dans GATE pour identifier les titres, donne une faible précision, ce qui nous a menés à introduire ces heuristiques pour les identifier automatiquement; (b) calculé la portée des titres et construit les imbrications entre eux (nous aurons ainsi défini quel paragraphe et quel sous-titre appartiennent à quel titre); (c) construit les imbrications entre les paragraphes, les phrases et les arguments en utilisant les marqueurs de position dans le texte. Une fois la structure hiérarchique du texte construite, les RC structurelles peuvent être déduites.
Manipulation sémantique
Nous nous intéressons dans cette partie aux OS et aux RC sémantiques qui les relient. Identification des objets sémantiques. Identifier ces objets signifie représenter la sémanti-que des OT, par un formalisme de représentation des connaissances. Nous avons opté pour une représentation avec le standard RDF(S) reposant sur la notion des triplets (ressource, propriété, valeur). Dans notre approche, nous supposons qu'une ontologie est déjà construite. Pour associer les OT à des triplets RDF en se référant à l'ontologie, nous proposons d'identifier les ressources (ou concepts), les propriétés et les valeurs (instances) dans le texte. Pour cela, nous proposons de construire des règles JAPE d'une manière automatique. En effet, l'idée principale est de bénéficier de la propriété rdfs:label, dans un schéma RDFS, pour construire des règles JAPE qui détectent les différentes manifestations d'un concept (ou propriété) dans le texte. La propriété rdfs:label représente le nom lisible par un humain d'un concept (ou propriété) dans le texte. Par ailleurs, les règles JAPE qui détectent les instances sont construites en se référant à un document RDF qui contient la liste d'instances pour cha-1 Un transducteur est un automate à états finis qui, pour chaque état parcouru, produit une ou plusieurs informations. 2 JAPE (Java Annotation Patterns Engine) est un langage d'expression de grammaires pour le TALN (un exemple est donné dans 4.2.3). que concept. Souvent, la détection des instances est difficile puisque nous ne connaissons pas auparavant les instances et à quel concept elles réfèrent. Le document RDF qui contient les listes des instances est une particularité propre aux données que nous manipulons dans notre expérimentation. Par la suite, les règles JAPE sont introduites dans la chaîne de TALN pour produire des marqueurs, qui permettent de connaître la position des concepts, propriétés et instances dans le texte. Enfin, il reste la construction des triplets RDF associés à chaque OT. Des problèmes d'ambiguïté peuvent surgir dans cette étape. Ils sont dus à l'identification de plusieurs concepts, propriétés ou instances dans un même OT. Nous proposons d'ajouter la prise en compte des contraintes range et domain pour déterminer quelle propriété correspond à quel concept. Néanmoins, nous avons remarqué que l'ambiguïté est généralement inexistante si la fréquence des liens logiques est grande dans le texte. En effet, plus la fréquence des liens logiques est grande et plus la taille 3 des arguments est relativement petite. Par conséquent, le même argument correspond moins souvent à plusieurs concepts, propriétés ou instances. Quant à l'objet textuel de type « titre » sa taille est généralement petite. Identification des relations contextuelles sémantiques. Cette étape consiste à attribuer le rôle sémantique aux liens logiques déjà détectés. Des travaux (Sporleder et al., 2005)  (Marcu et al., 2002) identifient ces rôles d'une manière automatique : par exemple, une addition pour les liens logiques (de plus, d'ailleurs,…). Néanmoins, des problèmes d'ambiguïté persistent dans certains liens logiques plus complexes. Nous nous sommes contentés, à ce stade de notre travail, des liens logiques qui ne représentent pas des ambiguïtés sémantiques tels que les liens logiques «because, compared to, except,…».
Description of the mill internal elements
Inlet Headliners
This new design is composed of 3 thicker bolted rings, compared to the original design of 2 rings. The liners have a thickness of 70mm. except for the area of most wear (R/2-R/2+R/3), where the thickness is 85mm.
<title Id="OT1"> Description of the mill internal elements <title Id="OT12"> Inlet Headliners <paragraph> <sentence> <contextualRelation Id="OT121" type="comparedTo"> <argument1 Id="OT1211"> This new design is composed of 3 thicker bolted rings, </argument1> <argument2 Id="OT1212">the original design of 2 rings </argument2> </contextualRelation> </sentence> <sentence> <contextualRelation Id="OT122" type="except"> <argument1 Id="OT1221"> The liners have a thickness of70mm. </argument1> <argument2 Id="OT1222"> for the area of most wear (R/2-R/2+R/3), <contextualRelation Id="OT12221" type="where"> <argument Id="OT122211"> the thickness is 85mm. </argument> </contextualRelation> </argument2> </contextualRelation> </sentence> </paragraph> </title> </title>
FIG. 2 -Exemple de partie de texte dans un document FIG. 3 -Arborescence des objets textuels
Déroulement de l'approche proposée
Nous soulignons que la langue que nous prenons en compte actuellement est l'anglais. L'approche que nous proposons est expérimentée sur un document texte (3768 mots et 1862 autres unités linguistiques tel que les : chiffres, virgules, parenthèses,…) issu des partenaires industriels dans le cadre du projet SEVENPRO.. Nous exposons dans ce qui suit le déroule-ment des étapes de l'approche sur une partie de ce document FIG. 2. phase: first options: control = appelt Rule:JRuleRingOfDiaphragmMill ( ({Token.lemma =="Ring"}({SpaceToken})?{Token.lemma =="of"}({SpaceToken})?{Token.lemma =="mill"}({SpaceToken})? {Token.lemma=="diaphragm"}({SpaceToken})?)| ({Token.lemma=="Ring"}({SpaceToken})? {Token.lemma=="of"} ({SpaceToken})?{Token.lemma=="diaphragm"} ({SpaceToken})? )| ({Token.lemma =="Ring"}({SpaceToken})? ) ):RingOfDiaphragmMill -->: RingOfDiaphragmMill.Concept = {kind ="RingOfDiaphragmMill", rule=JRuleRingOfDiaphragmMill } Rule: JRulehasPart ( ({Token.lemma=="has"}({SpaceToken})? {Token.lemma =="part"})| ({Token.lemma=="is"}({SpaceToken})?
{Token.lemma=="composed"} ({SpaceToken})?{Token.lemma=="of"} ({SpaceToken})?)| ):hasPart -->:hasPart.property = {kind ="hasPart", rule=JRulehasPart} <rdfs:Class rdf:ID="RingOfDiaphragmMill"> <rdfs:subClassOf rdf:resource="#Item"/> <rdfs:label xml:lang="en">Ring of mill diaphragm </rdfs:label> <rdfs:label xml:lang="en">Ring of diaphragm </rdfs:label> <rdfs:label xml:lang="en">Ring</rdfs:label> <rdfs:label xml:lang="fr">Anneau diaphragme </rdfs:label> <rdfs:comment xml:lang="en">denotes a part of a diaphragm mill (i.e. ring). </rdfs:comment> </rdfs:Class> <rdf:Property rdf:ID="hasPart"> <rdf:type rdf:resource="http://www.w3.org/ 2002/07/owl#TransitiveProperty"/> <owl:inverseOf rdf:resource ="http://www.sevenpro.org/ ontologies/2006/estanda#partOf"/> <rdfs:domain rdf:resource="#Item"/> <rdfs:range rdf:resource="#Item"/> <rdfs:label xml:lang="en">has part</rdfs:label> <rdfs:label xml:lang="en">is composed of </rdfs:label> <rdfs:comment xml:lang="en"> hasPart is transitive and also reflexive, and anti-symmetrical. </rdfs:comment> </rdf:Property>
FIG. 4 -Représentation rdfs d'un Concept et d'une relation dans l'ontologie et les règles JAPE permettant de les identifier dans le texte
En appliquant l'étape « manipulation textuelle » et ses différentes identifications et constructions en utilisant les marqueurs extraits à partir des règles JAPE, nous obtenons un ensemble d'OT, selon une structure arborescente sous format XML (FIG. 2).
Les exemples de RC structurelles pouvant être extraites à ce niveau sont : OT1 imbrique OT12; OT122 succède OT121; OT12 plus Important que 4 OT121; … Les étapes de la manipulation textuelle sont implémentées sous forme de requêtes XQuery 5 . Les objets textuels étant identifiés, il faut leur associer des objets sémantiques gérés à l'étape de manipulation sémantique. Selon l'ontologie associée 6 , un ensemble de règles JAPE est construit automatiquement. La FIG. 4 montre une description RDFS du concept «RingOfDiaphragmMill» et la relation «hasPart» dans l'ontologie ainsi que les règles JAPE associées permettant de les identifier dans le texte. Nous avons obtenu 65 règles gérées automatiquement et qui correspondent à 65 concepts dans l'ontologie. Cependant, comme l'ontologie est amenée à évoluer, il faudra relancer l'opération de génération pour avoir des règles qui reflètent l'état de l'ontologie. L'ontologie étant en cours de construction, le nom-bre de propriétés/instances construit n'est pas conséquent. De ce fait, les règles associées aux propriétés et instances ne sont pas générées dans l'expérimentation. L'utilisation de ces règles dans la chaîne de TALN permet de marquer tous les concepts et propriétés. Nous utilisons « Token.lemma » pour faire référence à toute variante d'un mot donné. Les concepts, propriétés et instances identifiés dans l'exemple de la FIG. 2  
-Evaluation des étapes d'extraction
Les résultats d'évaluation partielle des étapes de l'extraction (TAB. 1) sont très satisfaisants. Cependant la construction des triplets RDF (FIG. 5) n'a pas été expérimentée encore vu que l'ontologie est en cours de construction (pas assez de propriétés et instances).
A la différence des simples relations prises en compte par le formalisme RDF reliant des concepts, les RC que nous proposons sont des relations entre OS. Par conséquent, cela se traduit en relations entre triplets RDF (c'est-à-dire relations entre des annotations sémanti-ques), or RDF ne permet pas la représentation de telles relations. Afin de pallier à ce problème de représentation, nous attribuons des identifiants aux objets sémantiques. Ces derniers sont mis dans un document RDF à part. Nous remplaçons les OT dans le fichier XML, qui représente la structure du document, par une référence (avec les identifiants) vers les OS.
Exploitation des annotations contextuelles
Dans cette partie, nous allons donner quelques exemples d'utilisation d'annotations contextuelles ainsi que des orientations de nos travaux futurs sur leur exploitation. L'exploitation des annotations contextuelles repose essentiellement sur la notion de portée de validité. La portée de validité d'un objet sémantique donné est calculée suivant un contexte donné modélisé par les RC structurelles et sémantiques. Prenant l'exemple de la FIG. 3, la portée de validité de l'objet sémantique associé à l'objet textuel «OT1» est l'ensemble d'objets sémantiques associés aux objets textuels «OT12, OT121, OT122, OT1211, OT1212, OT1221, OT1222, OT12221, OT122211». Les conséquences de cette portée sont que : nous pouvons déduire, par exemple, que le concept «newDesign» cité dans l'objet «OT1211» correspond à la nouvelle conception du concept «mill» citée dans l'objet «OT1». Par ailleurs, la notion de portée de validité nous permettra d'étudier la portabilité entre contextes, ce qui conduit à une complexité supplémentaire dans l'inférence.
Le challenge qui reste à surmonter réside dans la difficulté du raisonnement sur les RC spatiales et temporelles. Nous proposons de nous inspirer des travaux sur les SIG (systèmes d'informations géographiques) pour raisonner sur l'aspect spatial et temporel. En effet, nous pouvons utiliser le raisonnement dit « de propagation de contraintes », et plus particulière-ment en utilisant les relations (ou intervalles) d'Allen (Allen, 1984). Les relations d'Allen (FIG. 6) peuvent être utilisées pour représenter les liens logiques exprimant des RC temporelles. Par exemple les liens logiques avant, et durant peuvent être représentés respectivement par les relations (A before B ou A meets B), et (A during B). Ainsi nous pouvons faciliter l'inférence sur l'aspect temporel. Nous pouvons exploiter les ontologies déjà construites et qui modélisent l'aspect temporel (Santos et al., 2003). Cependant, nous proposons de simplifier ces propositions et de les adapter à la manipulation du texte. Les limites de RDF concernant la représentation des relations entre triplets, ouvrent des perspectives de recherche sur une extension de ce formalisme et ceci nous pousse à utiliser un formalisme de description plus puissant (OWL), notamment pour représenter le rôle sémantique des RC. La  FIG. 7 donne un exemple d'une représentation OWL de la relation temporelle before.
<rdf:Property rdf:ID="Before"> <rdfs:subPropertyOf rdf:resource="#TemporalProperty"/> <rdf:type rdf:resource="&owl;TransitiveProperty"/> <owl:inverseOf rdf:resource="&TempSchema;After"/> <rdfs:domain rdf:resource="#SemanticObject"/> <rdfs:range rdf:resource="#SemanticObject"/> </rdf:Property>
FIG. 6 -Les relations d'Allen FIG. 7 -La relation temporelle «Before»
De la même manière, nous proposons d'exploiter les relations d'Egenhofer (Egenhofer et al., 1991) pour représenter les RC spatiales (4.1.1). Les annotations contextuelles que nous proposons ouvrent des perspectives de raisonnement sur le texte qui auparavant n'étaient pas envisageables.
Conclusions
Nous avons proposé, dans cet article, une approche qui modélise le contexte à partir de sources textuelles, en prenant en compte les différents types de relations structurelles/sémantiques (spatiale, temporelle, et diverse). Dans l'approche d'extraction des annotations contextuelles que nous proposons, les étapes automatisées sont : la détection des liens logiques, des titres et leurs portées, des imbrications entre « titre, paragraphe, phrase et argument », des concepts. Les étapes qui restent à automatiser sont : l'identification des propriétés et des instances ainsi que la construction des triplets. Un prototype est implémenté pour évaluer partiellement les différentes étapes de l'extraction. Les résultats de l'évaluation sont très satisfaisants. Cependant, l'ontologie à laquelle nous nous référons est incomplète et a besoin d'être enrichie. Aussi l'attribution des rôles pour les RC reste à généraliser sur des liens logiques plus complexes. Par conséquent, nous envisageons d'introduire l'inférence spatiale et temporelle pour mieux exploiter la richesse sémantique considérable des liens logiques. Afin de valoriser ce travail, un outil qui regroupe toutes les étapes d'extraction est en cours d'élaboration. Aussi une ontologie regroupant les relations contextuelles (temporelles, spatiales et autres) est en cours de construction, pour faciliter leur réutilisation. L'approche d'extraction et d'exploitation des annotations contextuelles sémantiques offre des perspectives prometteuses dans des domaines d'extraction de connaissances à partir du texte. Néanmoins, i) l'utilisation des relations contextuelles pour déduire les dépendances d'inférence (portée de validité) pourra peser lourd sur le temps d'exécution de l'inférence ; ii) aussi, nous avons constaté des redondances d'annotations contextuelles dans un même contenu textuel. Pour ces deux problèmes, nous envisageons de proposer des solutions techniques pour optimiser l'inférence et réduire la taille des annotations.

Introduction
Face à de grandes quantités de documents web, notre objectif est d'extraire et de valider semi-automatiquement des relations d'un domaine. Dans l'état de l'art, l'extraction des relations a été faite soit par une approche statistique, une approche linguistique ou une approche hybride. De plus, l'intérêt a été toujours porté sur un voire deux types de relations. A contrario, notre objectif est d'extraire des relations de différents types en combinant des analyses de textes et en considérant les caractéristiques des mots. Dans cet article, nous avons défini un algorithme contextuel de découverte de relations qui combine différentes analyses (lexicale, syntaxique et statistique) pour définir des processus complémentaires qui assurent l'extraction de relations variées et pertinentes. Notre algorithme établit des opérations de croisements entre analyses afin de pouvoir valider certaines relations. Les relations valides, comme celles invalides, seront présentées à l'expert du domaine mais séparément.
La découverte des relations
La notion de contexte. Pour l'extraction des relations, nous souhaitons trouver les mots qui sont reliés au mot étudié. Donc, nous cherchons des contextes qui contiennent ces mots reliés. Pour cela, nous avons défini différents contextes et nous les avons catégorisés en quatre types: le contexte structurel, le contexte linguistique (centré autour du verbe, globalement syntaxique et lexical), le contexte documentaire (paragraphe) et le contexte fenêtre (avec un degré de proximité). Notre approche utilise toutes ces analyses afin d'extraire de nouvelles relations (en plus de celles existantes dans la hiérarchie) et de les valider automatiquement. L'algorithme contextuel de découverte des relations. Il applique différents types d'analyses pour extraire et évaluer les relations. Il dépend de certains paramètres comme le degré de confiance (DC), NO est le pourcentage d'occurrences de mots dans le corpus (NO) et FN est la fréquence normalisée des mots dans le corpus (FN). Ces paramètres sont utilisés lors du filtre statistique ainsi que la validation. Le DC doit être défini par l'utilisateur vu qu'il explique sa confiance en l'application. Par contre, NO et FN peuvent être définis soit par l'expert du domaine, soit par le système en les déduisant de la valeur de DC ou par défaut (valeur définie par le concepteur du système). Dans le cas où le système est utilisé pour calculer les valeurs de NO et FN, si la valeur de DC est supérieure à 50% leurs valeurs (par défaut) seront maintenues, sinon elles seront multipliées par deux. Notre algorithme catégorise quatre types de relations extraites : valides, invalides, déduites et étiquetées. Une relation valide est celle qui est récupérée après une opération de croisement entre analyses. Une relation invalide est celle qui n'a pas été retrouvé dans deux analyses.
Notre algorithme est composé de cinq étapes. Une première étape applique les différentes analyses pour extraire les relations. Une seconde étape applique un filtre interne pour élimi-ner les relations qui représentent les liaisons des mots à l'intérieur des classes validées. L'étape trois applique un filtre par croisement des relations résultantes des différentes analyses. Nous proposons deux types de croisements complets (qui nécessitent que la relation existe dans les deux analyses pour qu'elle soit retenue) pour la première étape de validation : un croisement au sein de l'analyse statistique. Ce croisement est fait entre les relations structurées et les relations paragraphes vu qu'une structure telle que définie dans notre démarche (contexte structurel) n'est pas systématiquement incluse dans un paragraphe. D'où l'intérêt de recueillir ces relations qui se trouvent dans les deux résultats de nos contextes de même nature ; un croisement hybride réservé pour les relations provenant de l'analyse fenêtre par proximité et celles des analyses syntaxiques et lexicales. La quatrième étape prend en compte l'ensemble des relations invalides et applique un filtre statistique. Ce dernier est fait en défi-nissant la valeur de deux paramètres à savoir le nombre d'occurrences NO et la fréquence normalisée FN. L'étape 5 et 6 s'occupent respectivement d'établir les validations par degré de confiance et les déductions de nouvelles relations à partir de l'existant et d'étiqueter ces relations qu'elles soient valides, invalides ou déduites.
Expérimentations. Après avoir appliqué notre algorithme sur un corpus de 565 documents HTML en langue française relatif au domaine du tourisme, nous avons pu extraire: relations centrées autour du verbe (2251) ; relations globalement syntaxiques (34439) ; relations lexicales (5793) ; relations paragraphe (72476) ; relations structurelles (16966) ; relations fenêtres (206010). Par la suite, nous avons établit deux types de croisements à savoir un croisement entre les relations structurelles et paragraphes, et un second entre les relations fenêtres et lexicales. Le premier croisement nous a permis de retenir 372 relations (Hôtelle-rie/ hébergement, Réservation/hébergement, Camping/dormir). Quant au second croisement, nous avons pu avoir 268 relations (Catholicisme/christianisme, Ethnographie/paléontologie), sachant que dans les deux croisements nous avons supprimé certaines relations contenant des noms propres afin de minimiser le bruit. Après l'étape de filtre statistique, nous n'avons pas pu retenir des relations valides sur celles lexicales, globalement syntaxique et centrée autour du verbe vu que la relation la plus récurrente ne dépasse pas les 20 fois ; ce qui est largement loin de nos critères définis. Par contre, selon notre algorithme, pour les relations fenêtres (Activité/sport, Nautique/sport, Patrimoine/histoire, Plonger/sport) et structurelles (Casino/divertissement, Festival/musique, Vigne/vignoble), nous avons obtenu respectivement 24818 et 15257 relations validées. Pour les relations paragraphe, le résultat des validations a été négatif. Les relations qui n'ont pas été validées tout au long de notre démarche seront les relations invalides. Celles-ci seront présentées à l'expert en cas de besoin.
Summary
In this research, we focus on extracting relations among concepts in order to build a domain ontology. For this, we define a contextual relation discovery algorithm that applies different textual analyses in order to extract, deduce, label and validate the domain relations. Our algorithm is based on a rich contextual modelling that takes into account the document structure and strengthens the term co-occurrence selection, a use of the existent relations in the concept hierarchy and a stepping between the various extracted relations to facilitate the evaluation made by the domain experts. Our main perspective is using these relations for the concept hierarchy evaluation and enhancement.

Introduction
Le FIA ? est un nouvel automate qui permet de traiter de façon efficace la problématique de l'extraction des itemsets fréquents dans les flots de données. FIASCO est l'algorithme qui permet de construire et de mettre à jour le FIA ? en effectuant un seul passage sur les données. Notre objectif dans cet article est de présenter et d'illustrer par l'expérimentation l'applicabilité et le passage à l'échelle de FIASCO dans le cas des flots de données.
FIASCO (Frequent Itemset Automaton Stepwise Construction Operator)
Le FIA ? est un automate déterministe et acyclique, ce qui nous permet d'établir une relation d'ordre sur ses états (notée De par cette relation d'ordre, nous introduisons un algorithme en deux passes pour la construction de cet automate, en utilisant des bits positions : FIASCO2. Cet algorithme utilise les propriétés d'Apriori afin d'optimiser sa construction, ce qui le rend efficace dans le cas d'une base de données (cf. section 3). Nous proposons aussi un algorithme en une passe (FIASCO1), pour les flots de données, permettant de mettre à jour incrémentale-ment le FIA ? , item par item, avec une phase d'élagage en utilisant un support statistique.
Expérimentations
Les expérimentations ont été réalisées sur les jeux de données 2 kosarak et T10I4D100K, sur une machine munie d'un bi-processeur AMD ATHLON 3600+ 64 bits, avec 1Go de RAM. 
Conclusion
Nous présentons dans cet article un nouvel algorithme, FIASCO, qui permet de construire et de mettre à jour incrémentalement le FIA ? appliqué aux flots de données. Cet algorithme est en une passe, avec une granularité par item. Les expérimentations, avec une analyse en temps et en espace, montrent l'applicabilité et le passage à l'échelle de l'algorithme.
Summary
We present in this paper a new algorithm for constructing and incrementally updating the FIA ? : FIASCO. Our algorithm only needs one scan over the data and takes into account the new batches, itemset per itemset and for each itemset, item per item.

Introduction
En transcription automatique de la parole, de grands corpus audio (incluant généralement des centaines d'heures de parole) servent à estimer des modèles acoustiques précis de phonèmes contextuels. Ces modèles de sons élémentaires sont ensuite concaténés pour aboutir à des modèles de mots en s'appuyant sur la connaissance de leur prononciation. Cette connaissance est incomplète à l'heure actuelle et une partie importante de l'information caractérisant les variantes de prononciations se trouve encodée implicitement dans les modèles acoustiques. L'objectif de ce travail est de s'appuyer sur les techniques de fouille de données afin d'extraire des connaissances relatives aux spécificités acoustiques et prosodiques caractéri-sant les prononciations. Cette approche a déjà pu montrer son intérêt pour la caractérisation des accents étrangers (Vieru-Dimulescu et al., 2007). Nous nous intéresserons ici aux mots considérés comme homophones, i.e. phonémiquement pareils, et qui sont de ce fait sujets à de nombreuses erreurs de confusion lors de la transcription automatique. Partant de ces constats, nous nous sommes interrogés si les mots homophones ne déploieraient pas de particularités acoustiques/prosodiques qui n'ont été prises en compte ni par les paramètres acoustiques classiques (vecteurs de cepstres), ni par les modèles acoustiques (Modèles de Markov Cachés à trois états) et qui permettrait leur discrimination. Nous faisons ainsi l'hypothèse que des informations prosodiques (concernant durée, fréquence fondamentale notée f0, cooccurrence avec des pauses, etc.) puissent contribuer à lever certains types d'homophonie, en particulier s'il s'agit d'homophones issus de classes syntaxiques différentes (hétéro-syntaxiques). Nous avons fait appel aux techniques de fouille de données afin de classer automatiquement ces mots grâce à un ensemble d'attributs acoustiques/prosodiques spécifiques développés pour ce travail.
Comme déjà évoqué ci-dessus, de nombreuses erreurs de transcription concernent des mots, considérés comme homophones, par exemple une confusion entre un nom au singulier et un nom au pluriel (table, tables), un verbe au participe passé ou à l'infinitif (allé, aller) ou des mots-outils homophones (à, a). Ces derniers, par leur fréquence d'occurrences dans la langue participent de manière significative aux erreurs de transcription automatique.
Dans la section 2, nous allons présenter les corpus utilisés et les analyses menées concernant : la fréquence des mots, la durée, la f0, en nous limitant à deux paires de mots homophones choisies pour cette étude préliminaire, i.e. à (préposition) vs. a (verbe) et et (conjonction) vs. est (verbe). Ces mesures visent à étudier les réalisations acoustiques de ces mots homophones hétéro-syntaxiques dans le but d'identifier d'ores et déjà des attributs potentiellement pertinents lors de la classification automatique. Nous allons ensuite essayer de mettre en évidence des traits spécifiques différenciant ces mots fréquents en utilisant la classification automatique et la fouille de données (section 3), avant de conclure et d'ouvrir quelques perspectives (section 4).
Analyses acoustiques de mots (quasi-)homophones
Lors de la campagne ESTER (Evaluation des Systèmes de Transcription enrichie d'Emissions Radiophoniques) (Galliano et al., 2005), financée par le programme interministériel français TECHNOLANGUE et organisée par l'AFCP (Association Francophone de la Communication Parlée), la DGA (Délégation Générale pour l'Armement) et ELDA (European Language Resources Distribution Agency), le système de transcription automatique du LIMSI a obtenu environ 11% de taux d'erreur de mots (Galliano et al., 2005). Cependant de nombreuses erreurs portent sur les mots outils, qui sont les mots les plus fréquents et qui sont de plus souvent monosyllabiques comme et, est, a, à, un, que, qui, il, y, etc. (Adda-Decker, 2006). Dans l'étude présentée ici, deux paires de mots et (conjonction)/est (verbe être) et à (préposition)/a (verbe avoir), qui sont parmi les mots les plus fréquents du français et qui sont souvent confondues lors de la transcription automatique, sont choisies pour définir et examiner des descripteurs ou attributs acoustiques permettant potentiellement de distinguer ces paires. Il faut souligner que la paire et/est n'est pas vraiment homophone au sens phonologique, car le /E/ y correspond à deux degrés d'aperture différents: la réalisation /e/ (e fermé) caractérise le mot et, tandis que la prononciation canonique du verbe est est /E/ (e ouvert). Cependant, dans la parole spontanée, la réalisation acoustique en tant que [e] (fermé) du verbe est fréquente, ce qui entraîne l'homophonie des deux mots dans ce cas.
Corpus utilisés
Cette étude part de deux types de corpus en français : l'un est composé de journaux radiodiffusés d'environ 55 heures et appelé BN (Broadcast News), provenant de différentes stations de radios (France Inter, Radio France International (RFI), France Info et Radio Television du Maroc (RTM)). Le style de ces corpus est (semi-) préparé. L'autre corpus, appelé PFC (Phonologie du Français Contemporain) (Durand et al., 2003) contient des enregistrements de variétés de français de régions différentes et en différents styles de parole : parole spontanée (entretiens) et lecture (liste de mots et texte). La partie du corpus PFC utilisé contient environ 32 heures, dont le style « entretien » couvre 20h. Nous avons retenu ce sous-ensemble d'entretiens correspondant à de l'oral spontané.
Alignement automatique
L'alignement automatique vise à localiser dans le signal acoustique les mots prononcés, de déterminer leur prononciation et de segmenter le flux audio en phones. Pour ce faire le système d'alignement utilise des transcriptions manuelles, un dictionnaire de prononciation et des modèles acoustiques de phones indépendants du contexte. Le système de transcription automatique de parole du LIMSI (Gauvain et al., 2005)  
Extraction automatique des paramètres acoustiques
Le logiciel PRAAT (Boersma et Weenink, 1999) a été utilisé afin d'extraire un nombre de paramètres acoustiques intervenant par la suite dans la définition d'attributs pour la classification des mots homophones. Ces paramètres concernent le mot même (mot cible) et le contexte immédiat gauche/droite (i.e. la voyelle ou la pause précédant/suivant le mot cible). Nous avons ainsi extrait les trois premiers formants (F1, F2, F3) et la fréquence fondamentale (f0). Les formants (F1, F2 et F3) apportent des informations relatives à la qualité vocalique des segments: globalement le premier formant (F1) correspond à l'articulation sur un axe ouvert/fermé du segment, le second (F2) à la réalisation sur un axe antérieur/postérieur et le troisième (F3) au caractère étiré/arrondi de la voyelle. La fréquence fondamentale (f0), c'est-à-dire la fréquence de vibration des cordes vocales, détermine la hauteur du son. Elle permet essentiellement de distinguer les grandes classes de sons (voisés/non-voisés), mais donne aussi des informations quant au genre du locuteur, à la structuration temporelle du discours et à l'accentuation. Quant à la durée, il a été montré qu'elle peut être associée au style de parole ou bien à l'appartenance des mots à une classe lexicale ou fonctionnelle (Adda-Decker, 2006 
RNTI -X -
Pour chaque segment aligné, correspondant à l'un des 4 mots cibles et leur contexte, les mesures sont effectuées toutes les 5ms. Un segment comprend ainsi au moins 6 points de mesures, étant donné que la durée minimale d'un segment est de 30ms. Pour chaque segment un taux de voisement peut être calculé, correspondant au rapport donné par le nombre de points de mesure avec f0>0 sur le nombre total de points de mesure. La détermination automatique des valeurs de formant est sujette à erreurs. Afin d'éliminer des erreurs de mesure de formants, un filtrage a été mené: un segment est retenu à condition que son taux de voisement n'y soit pas nul.
Pour le mot est, il y a la prononciation canonique /?/ (e ouvert) et sa variante /e/ (e fermé) dans le dictionnaire de prononciation utilisé pour la transcription automatique. Ces deux phonèmes sont perceptivement ainsi qu'acoustiquement différents (cf. Fig. 1). Ci-dessous, les courbes détaillent les distributions des mots analysés selon leurs durées respectives. La distribution de la paire de mots et/est pour des durées allant de 30ms jusqu'à 200ms est montrée dans la Figure 2 (corpus BN à gauche et corpus PFC-entretien à droite). Pour le mot est, trois courbes sont présentées : la première courbe (losanges) illustre la prononciation majoritaire /e/ (variante de prononciation), la deuxième (carrés) -le phonème /?/ (prononciation canonique), tandis que la troisième courbe (triangles) réunit les distributions des deux prononciations possibles. Chacune des courbes représentées somme à 100%.
Analyse des paramètres
Une première tendance qui se dégage est que la durée peut être mise en relation avec le style de parole : les mots sont plus courts dans le corpus PFC (maximum à 30ms) que dans le corpus BN (maximum autour de 60-70ms). Pour ce qui est des différences entre les homophones et et est, on observe que la conjonction et (courbes rouges à croix) a tendance à avoir une durée plus importante que le verbe. L'intersection des deux mots et/est est à 80ms pour les deux styles de corpus. Après 80ms, le taux de et est plus important que celui du mot est. L'évolution des trois courbes du mot est est similaire dans les deux corpus.
La Figure 3 montre des décomptes similaires pour les mots à (préposition) et a (verbe avoir) pour le corpus BN (à gauche) et le corpus PFC-entretien (à droite). Par rapport à la paire de mots et/est (cf. Fig. 2), la paire à/a observe la même tendance de durée plus courte pour le style de parole spontané (PFC-entretien). La préposition à est légèrement plus longue que le verbe a. Par exemple on peut remarquer dans la Figure 3 à droite (PFC-entretien), qu'un écart d'environ 8% existe pour les deux items à/a pour la durée minimale de 30ms.
Pour conclure sur cette partie, nous avons analysé les durées de deux paires de mots (quasi-)homophones. Une différence qui semble s'imposer pour les deux paires de mots et les deux styles de corpus est que la durée de mots-outil de types conjonction ou préposition (surtout et, dans une moindre mesure à) est plus longue que celle des verbes auxiliaires est et a. Cette information contribuera éventuellement à différencier nos paires de mots.  
FIG. 3 -Distributions de durée de « à (préposition) » et « a (verbe) » du corpus BN (gauche) et PFC (droite).
Fréquence fondamentale (f0)
Les paramètres prosodiques généralement considérés pour caractériser un mot sont la f0, la durée et l'intensité. D'après Selkirk (1996), les mots peuvent être distingués selon la caté-gorie grammaticale à laquelle ils appartiennent, e.g. fonctionnelle (le déterminant, la préposi-tion, l'auxiliaire, le complément, la conjonction et la particule, etc.) ou lexicale (le nom, le verbe et l'adjectif). Ce statut (fonctionnel vs. lexical) peut être mis en relation avec le paramètre appelé fréquence fondamentale. En effet, on pourrait émettre l'hypothèse qu'un verbe se trouvant à l'intérieur d'un mot prosodique réalise un f0 moyen différent de celui d'une pré-position ou d'une conjonction se trouvant en début de mot prosodique. De la même manière, on pourrait penser qu'en début de mot prosodique on peut observer des zones de voisement partielles, précédées éventuellement de césures ou de pauses. Nous avons donc analysé la f0 en tant qu'information prosodique pour nos paires de mots et/est et à/a. La question s'est posée si le taux de voisement (i.e. la proportion de valeurs non nulles de f0) joue un rôle important dans l'articulation des mots homophones analysés. Ainsi, selon le degré de voisement des segments vocaliques correspondant aux mots cibles, les données ont été divisées en trois classes :
1.  
FIG. 5 -Distribution de pourcentage d'occurrences et valeurs moyennes de f0 de « à (prépo-sition) » et « a (verbe) » selon le taux de voisement du corpus BN (gauche) et PFC (droite).
La comparaison des mots à/a est montrée dans la Figure 5 pour le corpus BN (à gauche) et pour le corpus PFC-entretien (à droite). De gauche à droite, le mot à est illustré en prune, le verbe a en bleu. Comme pour la conjonction et, la préposition à est plus susceptible d'apparaître dans les groupes à voisement partiel. De manière réciproque, le verbe a est mieux représenté dans la classe Voisé (les paquets d'au moins 80% de voisement), même si cette tendance est moins forte que pour la paire et/est. Cela nous informe donc que dans les deux types de corpus et pour les deux paires de mots, le verbe est plus fréquemment complètement voisé que la préposition ou la conjonction. Le taux de mots dans la classe Voisé est plus faible dans le corpus PFC (entretien) que dans le corpus BN, ce qui peut être à nouveau mis en lien avec le style de parole spontanée.
À partir de l'analyse de la f0 et du taux de voisement, on peut remarquer que les verbes auxiliaires sont en général plus voisés que la préposition et la conjonction faisant partie de la catégorie fonctionnelle. Ces résultats indiquent qu'en français, les mots dans la catégorie lexicale pourraient être plus accentués que ceux dans la catégorie fonctionnelle, confirmant ainsi ce qui a été mis en évidence en anglais (Selkirk 1996). Ces mesures permettent d'envisager des attributs permettant de distinguer nos paires de mots homophones. Dans la section suivante nous nous intéressons à ces paires de mots en contexte, afin de trouver des descripteurs distinctifs supplémentaires.
Cooccurrence de pauses (gauche/droite)
Selon Beaugendre et Lacheret-Dujour (1999), les pauses jouent un rôle très important dans le processus d'extraction automatique d'informations prosodiques, et ceci est plus particulièrement vrai pour ce qui est de la parole spontanée. Quant à la perception humaine, les pauses permettent entre autre, de repérer différents contours dans la substance verbale. Nous nous sommes intéressées au rapport qui existe entre les pauses au sens large (qu'il s'agisse d'un silence, d'une pause remplie, i.e. une hésitation, ou d'une respiration) et les paires de mots homophones analysés ici. On examine leurs cooccurrences à gauche et à droite du mot cible.
Mots On peut remarquer que les taux de la catégorie « pause gauche » pour la conjonction et sont plus importants que ceux pour le verbe est, et ceci dans les deux styles de corpus. Cela suggère que de manière générale, les locuteurs introduisent une césure plus souvent devant la conjonction que devant le verbe et ceci d'autant plus lorsqu'il s'agit de parole conversationnelle (PFC). Pour ce qui est de la paire de mots à vs. a, des différences comparables sont observées: les pauses sont plus nombreuses devant le mot outil que devant le mot lexical mais dans un degré moindre.
Pour finir, notons donc que la principale différence entre mots fonctionnels (à, et) et mots lexicaux (a, est) concerne l'occurrence de pauses surtout à gauche (et plus légèrement à droite) du mot cible. Ainsi, de manière générale, les verbes est et a sont rarement précédés d'une pause, au contraire des mots fonctionnels.
Discussion
À l'issue de cette section, on peut observer que les caractéristiques acoustiques et prosodiques des deux paires de mots homophones établies à partir de dizaines d'heures de parole et quelques milliers d'occurrences pour chaque mot examiné, présentent quelques différen-ces. Des paramètres tels que la durée et le taux de voisement permettent de les distinguer au moins partiellement. La cooccurrence de pauses à gauche et à droite des mots cibles change aussi selon la nature fonctionnelle ou lexicale des homophones considérés. Ainsi, on peut s'interroger si ce type d'attributs acoustiques ne pourrait pas être utilement exploité dans la discrimination de telles paires de mots.
Partant de là, nous nous sommes posé comme objectif de définir des attributs qui pourraient caractériser les mots à travers des techniques de fouille de données. Dans la section suivante, nous décrivons d'abord les descripteurs acoustiques et prosodiques mis en oeuvre, avant d'aborder la méthode adoptée pour discriminer les paires d'homophones analysées ici.
Classification des mots homophones par fouille de données
Un ensemble de tests de classification automatique a été mené, visant à déterminer à la fois l'algorithme de classification et les attributs acoustiques les mieux adaptés à distinguer les deux paires de mots homophones et/est et à/a. Lors de cette étude préliminaire, nous avons fait appel au logiciel Weka (développé à l'université de Waikato, en NouvelleZélande) pour classifier automatiquement ces mots grâce à quelques descripteurs acoustiques et prosodiques pressentis. Le logiciel Weka est destiné à résoudre une variété de problèmes rencontrés dans le cadre de la fouille de données, et le système est équipé d'une large gamme d'algorithmes d'apprentissage et de classification.
Définition d'attributs
Pour la classification automatique, 41 attributs acoustico-prosodiques ont été définis. Ils ont été choisis pour modéliser à la fois le mot cible (attributs intra-phonème) et sa relation au contexte (attributs inter-phonème). Ces attributs sont :
Attributs intra-phonème (33) : durée, f0 (moyenne/segment, début, milieu, fin), les formants (F1, F2 et F3) (valeurs moyennes par segment ainsi qu'en début, milieu et fin de segment), taux de voisement (moyennes par segment ainsi qu'en début, milieu et fin de segment). Nous avons également calculé les différences (notées ?) début-milieu, milieu-fin et début-fin pour les formants et pour la f0.
Attributs inter-phonème (8) : durée, f0, pause. Le paramètre durée est mesuré ici comme suit : la différence entre la durée au centre du segment correspondant au mot cible et le centre de la voyelle précédente/suivante, même s'il y a des consonnes ou des pauses entre ces phonèmes. Pour la f0 au niveau inter-phonémique, ?f0 a été calculée comme différence entre la valeur moyenne de f0 du phonème du mot cible et celle de la voyelle précédente et suivante, et entre ces deux voyelles précédant et suivant le mot cible. Les paramètres « pause gauche » et « pause droite » ont été également rajoutés.
Expériences de classification
Pour classifier automatiquement les mots à partir de ces attributs, nous avons testé 17 algorithmes implémentés dans le logiciel Weka (classification bayésienne, arbres, règles et fonction etc.), avec le but de trouver l'algorithme le plus performant. Les expériences de classification sont effectuées à l'aide de la méthode de validation croisée, comprenant la division du corpus dans une partie « entraînement » et une partie « test ». Les résultats obtenus sur la partie « test » sont évalués en termes de classification correcte et de coefficient kappa 1 . En fonction des scores de classification et du coefficient kappa, les meilleurs algorithmes sont sélectionnés pour chaque paire de mots. Le tableau 2 ci-dessous montre l'algorithme ayant permis la meilleure discrimination de chaque paire et la moyenne des 5 meilleurs algorithmes/paire de mots. Les résultats montrent que la paire et/est est nettement mieux classifiée que la paire à/a. Cela va dans le sens des résultats de la section précédente où l'on observait que la paire et/est se distinguait mieux que la paire à/a. Cela s'explique également en partie par le fait qu'un tiers environ des occurrences du verbe est ne sont pas de vrais homophones (prononciation /?/ pour est) de la conjonction et ce qui engendre des attributs plus discriminants. Les résultats pour et/est sont particulièrement prometteurs pour le corpus PFC. La parole spontanée présente en général plus d'erreurs lors de la transcription automatique, ces résultats montrent cependant que les homophones analysés sont en grande partie distinguables dans ce type de parole. 
Sélection d'attributs
41 attributs ont été utilisés pour classifier les paires de mots. Nous pourrions faire l'hypothèse que parmi ces attributs, certains sont plus pertinents que d'autres. Les 10 meilleurs attributs ont été ainsi retenus (cf. tableau 4) à partir des résultats données par l'algorithme le plus performant, i.e. l'algorithme LMT (Logistic Model Trees). À partir de ces 10 attributs sélectionnés, nous avons recalculé les pourcentages de classification correcte avec l'algorithme LMT en utilisant la méthode de la validation croisée. Les attributs sélectionnés et leurs résultats sont présentés dans le tableau 3. Les résultats à partir de 10 attributs sont légèrement moins performants que l'utilisation de tous les attributs, surtout pour la paire et/est dans le corpus PFC. Le résultat du corpus BN montre que la différence entre 10 attributs et 41 attributs reste marginale (1%). Ceci montre que seuls 10 attributs suffisent pour 1 Le coefficient kappa mesure ici la concordance entre la classification automatique par Weka et les deux classes réelles de paramètres caractérisant les mots et/est, à/a. Il varie entre -1 (désaccord total) et 1 (accord total), en passant par 0 (classification au hasard). En fonction des résultats de la classification, les seuils correspondant aux meilleurs algorithmes sont : k>0.50 pour et/est et k>0.25 pour à/a. Cela montre d'emblée une différence entre les deux paires : le kappa de et/est est très bon tandis que les résultats moins bons pour à/a montrent que cette paire est beaucoup plus difficile à discriminer.
RNTI -X -produire des résultats pratiquement équivalents à ceux obtenus avec 41 attributs. Il y a donc des traits acoustiques et prosodiques particulièrement significatifs qui permettent de distinguer les mots homophones et ces traits sont encodés en grande partie dans les 10 attributs. 
-10 attributs (intra-segmentaux en italiquer et inter-segmentaux en gras) mieux classés par l'algorithme LMT.
En regardant de plus près les attributs les plus discriminants, un certain nombre de tendances se dégage. En ce qui concerne la paire et/est, on remarque que les paramètres concernant les durées (durée du phonème, ?durée inter-phonèmes) et l'existence d'une pause devant le mot cible s'avèrent particulièrement discriminants. Les paramètres liés au voisement (taux de voisement du phonème, ?f0 inter-phonèmes) pour le corpus BN et aux formants (F2, F2 milieu, F3 début, ?F1, ?F2) pour le corpus PFC-entretien sont aussi discriminants. Les paramètres inter-phonémiques sont surtout pertinents pour le corpus BN.
Pour la paire à/a, on peut observer d'abord que les paramètres inter-phonémiques sont importants : ceux liés aux durées (?durées inter-phonèmes) et au voisement (?f0 interphonémes). Ensuite, le paramètre « pause gauche » est également utile. Enfin, les paramètres concernant le deuxième formant (F2 début, F2 milieu, F2 fin, ?F2), ainsi que le voisement jouent un rôle non négligeable. Il est aussi à noter qu'il y a beaucoup de paramètres communs aux deux corpus qui permettent de distinguer les deux mots.
Cette analyse, bien que préliminaire, a permis de mettre en évidence que les mots homophones peuvent être différenciés grâce à certains paramètres acoustiques et prosodiques. Ce fait est valable pour les deux paires et surtout pour et/est dans les deux corpus. Les résultats obtenus suggèrent des pistes intéressantes pour mieux traiter ces mots lors de la transcription automatique de la parole. Afin de valider cette approche et les paramètres retenus, l'analyse devrait s'étendre à d'autres mots homophones et d'autres types de corpus.
Conclusion et perspectives
Dans ce travail nous avons cherché à étudier les réalisations acoustiques des mots homophones dans des grands corpus oraux de différents styles de parole (parole préparée, parole conversationnelle) avec le but à plus long terme de pouvoir contribuer à la modélisation acoustique des variantes de prononciation dans les systèmes de transcription automatique de la parole. Notre objectif était ensuite d'examiner les réalisations acoustiques de mots homophones fréquents, sources de nombreuses erreurs de transcription, de définir et d'évaluer des attributs acoustiques, permettant éventuellement de classifier correctement ces homophones. À cet effet nous avons utilisé différents outils (STK-LIMSI pour l'alignement automatique, Praat pour l'extraction d'attributs acoustiques, Weka pour la classification et la fouille de données).
Nous nous sommes servies de l'expérience acquise sur ces grands corpus de parole variés, pour définir des attributs acoustiques et prosodiques potentiellement utiles pour la discrimination des mots, en particulier des mots (quasi-)homophones. Ainsi nous avons pu étu-dier plus particulièrement les paires de mots outils homophones, et/est et à/a, qui sont les couples de mots qui produisent le plus d'erreurs lors de la transcription automatique. La comparaison de durées entre et (conjonction) /est (verbe être) montre que le mot est a tendance à être réalisé avec une durée beaucoup plus faible, alors que la conjonction et se trouve souvent allongée. Cette simple mesure suggère que les homophones, réalisés a priori avec les mêmes phonèmes (par exemple, les mêmes valeurs de formants pour les voyelles), peuvent différer dans leur réalisation prosodique. Par la « réalisation prosodique » nous entendons tout ce qui concerne durée, fréquence fondamentale, voisement des segments, ainsi que leur articulation avec les contextes droite et gauche, incluant des mesures de pauses. On aboutit ainsi à un ensemble de mesures intra-et inter-phonèmes, servant à définir les 41 attributs acoustiques utilisés pour la classification. Pour les expériences de classification, nous avons testé les différents algorithmes proposés dans Weka : classification par arbres de décision, classification par règles, classification par MVS (machines à vecteurs support), classification bayésienne. Le résultat de la classification automatique par les 17 algorithmes testés dans le logiciel de fouille de données montre que les attributs impliquant les durées intra-et intersegmentales de la paire et/est sont parmi les plus importants pour la classification. La classification automatique s'est avérée particulièrement prometteuse pour la paire et/est : les résultats sont supérieurs à 70% (BN) voire 90% pour le corpus PFC. Les mots à/a sont discriminables aussi, même si de manière moins aisée que le paire et/est. Ces résultats montrent qu'il existe des informations acoustiques pertinentes pour la discrimination des homophones. Leur utilisation explicite dans les systèmes de transcription devrait contribuer dans le futur à réduire le taux de confusions observées.
Nous rappelons ici les attributs qui se sont avérés les plus utiles pour la classification: les durées inter-segmentales (en lien avec un phonème vocalique précédent ou/et suivant) sont plus efficaces que la durée du segment propre. La même remarque vaut également pour la f0.
Dans des travaux futurs, nous allons essayer de mieux caractériser les homophones en rajoutant de nouveaux attributs pour la discrimination, et ensuite tenter d'implémenter efficacement les attributs identifiés (en particulier mesures de durée et de f0 inter-segmentales) pour améliorer la transcription automatique. Nous comptons également, dans le futur, éten-dre ce type d'études à plus de mots, et intégrer alors des informations morpho-syntaxiques, afin de mieux factoriser les variantes observées dans la parole.

La génération des séquences résumées proposée
La génération de résumé vidéo est une technique alternative prometteuse utilisée dans l'indexation et la recherche vidéo (Truong et Venkatesh, 2007). L'objectif de cet article est de proposer une approche de génération des séquences résumées utilisant le soft computing vue son efficacité dans les systèmes tolérant l'imprécision et l'incertitude (Zadeh, 1956). Etant donné une longueur cible du condensé vidéo, on veut calculer les segments vidéo qui couvrent le maximum du visuel en respectant la longueur du condensé. Les segments vidéo sont représentés par des images clés et décrites par un histogramme de couleur. L'histogramme est un outil très ordinaire pour résumer visuellement la distribution d'un échantillon de données. Pour maximiser le visuel on doit maximiser la présence de l'information de la couleur et sa distribution. Ce pendant, le problème revient à maximiser l'occurrence et la distribution du visuel représenté par l'information de couleur du contenu vidéo. En effet, l'occurrence et la distribution de ce contenu visuel représente des variables linguistiques, c.-à-d. "l'occurrence est maximale" et "la distribution est maximale". L'occurrence et la distribution représentent deux ensembles flous qui nécessitent une détermination de leur fonction d'appartenance.
Dans notre proposition, nous segmentons les séquences vidéo en plans (shots) comme unité de base en utilisant une transformation de couleur RGB réversible comme une représentation du contenu vidéo (Hadi et al., 2006b). Après, nous sélectionnons les images représentatives (keyframes) à partir des plans vidéo on se basant sur l'estimation du mouvement local (Essannouni et al., 2006). L'algorithme d'extraction des images représentatives utilisé est basé sur un Séquence résumé par le Soft Computing algorithme de classification k-medoid pour rechercher les images les plus représentatives pour chaque plan Hadi et al. (2006a). Comme résultat, les images extraites représentent le centre des sous segments dans un plan.
Dans l'optique de créer des condensés vidéo à partir des segments résultats, nous représen-tons les images clés calculées par leur histogramme de couleur utilisé dans (Hadi et al., 2006b). La création est réalisée en maximisant l'occurrence et la distribution de la coleur considéraient comme des ensembles flous. Afin déterminer leurs fonctions d'appartenance, on cherche les instances d'histogrammes liées aux occurrences maximales et aux distributions maximales en utilisant les algorithmes génétiques (AG). Avec ces deux instances fictives d'histogrammes, nous calculons leurs distances avec les histogrammes des images clés calculées dans la première étape d'analyse vidéo. On obtient ainsi deux vecteurs de distance pour l'ensemble des segments à sélectionner pour la création du condensé vidéo. Afin de chercher la valeur d'appartenance du visuel maximal, nous utilisons les théories des ensembles flous. Le condensé final sera généré à partir de la courbe des valeurs d'appartenance du visuel maximal en sélec-tionnant les segments ayant les plus grandes valeurs (visuel maximal) en respectant la longueur du condensé qui représente la sommes des longueurs des segments sélectionnés.

Introduction
D'un point de vue linguistique, la pragmatique s'intéresse aux éléments du langage dont la signification ne peut être comprise qu'en fonction d'un contexte d'interprétation donné. Dans le cadre des ontologie de domaine (qui sont des spécifications formelles de conceptualisations partagées Gruber (1993)), il s'agit d'enrichir la sémantique formelle intrinsèque à une ontologie de domaine (OD) à l'aide d'éléments caractéristiques d'un contexte de création ou d'usage comme la culture, le mode d'apprentissage, ou encore l'état émotionnel. Pour ce faire, nous prenons comme point de départ les ontologies telles quelles sont définies aujourd'hui au moyen du langage OWL. Nous y ajoutons deux gradients, dont le but est de modéliser une différence intuitive de degré de vérité dans le processus de catégorisation : (1) une pondération des liens « is-a » dans la hiérarchie des concepts, les gradients de prototypicalité conceptuelle, (2) une pondération des synonymes du label utilisé pour dénoter un concept donné, le gradient de prototypicalité lexicale. Ces gradients permettent de prendre en considération différents points de vue pour une même ontologie, et par la-même d'enrichir la sémantique formelle initiale intrinsèque à l'ontologie d'une pragmatique inhérente au contexte d'usage considéré (fonction de la culture, des modes d'apprentissage ou encore du contexte émotionnel). Après avoir exposé notre approche 1 dans les sections 2.1 et 2.2, les gradients de prototypicalité conceptuelle et de prototypicalité lexicale, fondés sur les processus cognitifs de catégorisation, sont explicités dans les sections 2.3 et 2.4.
Gradients de prototypicalité
Nos travaux reposent sur l'idée fondamentale que tous les concepts ne sont pas constitués de membres équidistants par rapport à la catégorie qui les subsume, mais qu'ils comportent des membres qui sont de meilleurs représentants que d'autres (Kleiber, 2004). Ce phénomène est également applicable pour l'ensemble des termes 2 désignant un concept. C'est sous cette hypothèse, validée par des travaux de psychologie cognitive tels que Cordier (1985), que nous proposons deux mesures : (1) le gradient de prototypicalité conceptuelle, qui correspond à une pondération des liens « is-a » dans la hiérarchie des concepts, et qui permet de mesurer les différentes représentativités des sous-concepts ; (2) le gradient de prototypicalité lexicale, qui, pour un concept donné, correspond à une pondération des synonymes du terme saillant.
Approche objective, fondée sur les propriétés
La composante objective de notre gradient vise à prendre en compte l'aspect intensionnel d'une conceptualisation aux travers des propriétés de concepts. Le rôle des attributs d'une caté-gorie en vue d'un rattachement à une autre catégorie a été développé dans Smith et al. (1974). Nous appelons cette pondération objective, car elle est issue du courant objectiviste selon lequel la catégorisation s'opère sur la base de propriétés communes (Kleiber, 2004). Il s'agit d'un courant classique qui se fonde sur une vue catégorielle de l'univers tel qu'a pu le définir Aristote. Cette objectivité provient uniquement de la conceptualisation élaborée par l'endogroupe, conceptualisation capturée dans l'ontologie via la hiérarchie des concepts et leurs propriétés.
L'idée directrice est que plus un concept va ajouter d'attributs à ceux hérités de son père, plus il sera sur la voie de la spécialisation et moins il sera prototypique, i.e. représentatif de sa catégorie. Nous considérons cette valeur comme étant le rapport entre le nombre d'attributs du fils et le nombre d'attributs du père. De manière formelle, la fonction objectif : C ×C ? [0, 1] est définie comme suit :
Où (1) attributs(c p ) est le nombre d'attributs du concept parent, (2) attributs(c f ) le nombre d'attributs du concept fils, et (3) n le nombre de concepts fils du concept c p .
Nous élevons le rapport entre le nombre d'attributs des deux concepts à la puissance n de manière à pouvoir tenir compte de la structure de l'héritage et ainsi favoriser davantage les élé-ments dont la valeur objectif est forte. Le concept le plus prototypique d'une décomposition se trouve ainsi renforcé proportionnellement au nombre d'éléments de cette décomposition.
Approche subjective, fondée sur les fréquences
La composante subjective de notre gradient vise à prendre en compte l'aspect extensionnel d'une conceptualisation à travers les termes utilisés pour dénoter les concepts. Cette approche est fondée sur la fréquence d'apparition d'un concept appartenant à un domaine D dans l'univers d'un endogroupe G. De la sorte, plus un élément sera fréquent dans cet univers, plus il sera jugé comme représentatif / typique de sa catégorie. Cette notion de typicalité fait l'oeuvre des travaux d'Eleanor Rosch (Rosch, 1973). Dans notre contexte, l'univers de l'endogroupe va être constitué par l'ensemble des documents contenus dans ? (D,G) .
De manière formelle, la fonction subjectif G,
Avec : -f requence(terme) retourne la fréquence d'apparition du terme dans les documents appartenant à ? (D,G) ; -count(document, terme) retourne le nombre de documents appartenant à ? (D,G) où le terme apparaît ; -count(document) retourne le nombre de documents appartenant à ? ( le rapport entre le nombre de documents où le terme est présent et le nombre total de document. Une idée présentée fréquemment dans peu de documents est moins influente en terme de connaissance qu'une idée défendue et citée peu de fois dans chacun des documents mais de façon uniforme dans tout le corpus 4
Gradients de prototypicalité conceptuelle
Les gradients de prototypicalité conceptuelle prennent en compte, dans le processus de catégorisation et de classement, deux aspects des concepts : intensionnel d'un côté par leur structure, extensionnel de l'autre par leur fréquence d'évocation et d'utilisation dans l'univers de l'endogroupe. 
Gradient de prototypicalité conceptuelle locale (GPCL). Soit protconloc
.5] une pondération de la composante objective, (2) ? ? [0, 0.5] une pondération de la composante subjective, et (3) ? ? 0 une pondération de l'état mental de l'endogroupe G.
Les valeurs de ? et de ? peuvent varier suivant le domaine traité, la volonté des experts lors de la création de l'ontologie, le contexte d'usage de l'ontologie considérée, etc. Ils permettent ainsi d'accorder plus ou moins d'importance à l'aspect structurel de la conceptualisation par rapport à l'évocation des concepts dans les documents. Quant au facteur ?, il a pour objectif de tenir compte de l'état mental de l'endogroupe. Selon Mikulinger et al. (1990), un état mental négatif favorise la diminution de la valeur de représentation, et inversement pour un état mental positif. Nous caractériserons (1) un état mental négatif par une valeur de ? ?]1, +?[, (2) un état mental positif par une valeur de ? ?]0, 1[, et (3) un état mental neutre par une valeur de 1.
Ainsi, une très faible valeur de ? va avoir pour effet d'augmenter considérablement la valeur de ce gradient pour des concepts initialement placés comme peu représentatifs (un état positif facilite l'ouverture d'esprit, la valorisation etc.) A l'inverse, une très forte valeur de ?, pour un état mental fortement négatif, va avoir pour effet de sélectionner uniquement les concepts à forte typicalité, éliminant de facto les autres concepts. Une ontologie pragmatisée vernaculaire de domaine est par conséquent une ontologie vernaculaire de domaine placée dans un contexte particulier, défini par les trois paramètres ? pour la composante objective, ? pour la composante subjective et ? pour l'état mental.
Gradient de prototypicalité lexicale (GPL).
Ce gradient a pour but de valuer le fait que tous les synonymes d'un terme considéré comme le terme saillant, i.e. le label préconisé pour un concept, n'ont pas forcément la même repré-sentativité au sein de l'endogroupe. La question est en effet « pourquoi nommons-nous plus le concept x avec le vocable y plutôt que z ? » Pour définir ces variations de représentativité lexicale, nous reprenons le gradient calculé précédemment, à la différence près que nous n'allons pas utiliser la composante objective liée aux propriétés (du fait que nous sommes sur la même catégorie). Nous allons une fois encore nous inspirer de la formule calculant le contenu en information d'un concept, en prenant pour évaluer ce gradient le rapport entre la fréquence d'utilisation de ce terme et la somme de fréquences de tous les termes du concept dans ? (D,G) . 
.
Conclusion
La finalité de nos travaux, focalisés sur la notion d'ontologie pragmatisée vernaculaire de domaine, est de prendre en compte la subjectivité de la connaissance via sa spécificité à un endogroupe et un domaine, son aspect écologique, et l'importance de son contexte émotion-nel. Cet objectif nous conduit à étudier la dimension pragmatique d'une ontologie. En nous inspirant des travaux d'E. Rosch sur la prototypicalité, nous avons développé deux mesures identifiant deux gradients de prototypicalité, l'un conceptuel et l'autre lexical, de manière à pouvoir -entres autres -pondérer les liens « is-a » des hiérarchies catégorielles. Ces pondéra-tions permettent, dans un premier temps et au niveau local, d'ordonner les sous-concepts d'une catégorie donnée pour le premier gradient, la liste des synonymes d'un terme saillant pour le second. Bien évidemment, ces gradients ne modifient en rien la sémantique formelle inhérente à l'ontologie considérée ; les liens de subsomption restent valides. Ces gradients ne sont que le reflet de la prise en compte de la pragmatique, à savoir une sur-couche pragmatique sur la sémantique.
A partir de ces informations, plusieurs applications peuvent être envisagées :
-évaluation / validation d'ontologies. Les gradients de prototypicalité peuvent représenter des indicateurs de qualité de catégorisation, et par là même de qualité de la modélisation inhérente à une ontologie de domaine formalisée (au moyen d'OWL par exemple).Par exemple, il peut se poser le problème des concepts jugés très peu typiques. Que faut-il en faire ? Sont-ils à la bonne place dans la hiérarchie catégorielle ? -recherche d'information. Les gradients de prototypicalité peuvent permettre de classer les résultats d'une requête (sous-entendue d'une requête dite étendue) suivant un ordre de pertinence, en plaçant au premier plan les éléments les plus représentatifs d'une caté-gorie ou d'un terme donné.

Introduction
Les avancées dans le domaine du Web ont ouvert de nouvelles perspectives dans le domaine de la cartographie interactive et dynamique (Koben, 2001, Josselin et Fabrikant, 2003. Aujourd'hui le web propose de multiples cartographies qui s'adaptent aux besoins d'un utilisateur, qui peut être, tour à tour, décideur, citoyen, voyageur… Les services d'itinéraires routiers en sont un exemple typique. Dans ce contexte, le groupe de recherche pluridisciplinaire HyperCarte 1 s'est donné pour objectif de concevoir et implémenter une collection cohérente de plates-formes interactives d'analyse spatiale et de représentations cartographiques de phénomènes sociaux, économiques, environnementaux, etc. (Grasland et al., 2005-b). Les applications visées se situent principalement dans le domaine socio-économique ou 1 http://www-lsr.imag.fr/HyperCarte/ -19 -RNTI-E-13 environnemental et l'aide à la décision en matière de prospective territoriale pour un public large : chercheurs en géographie, en sciences sociales et humaines, mais aussi décideurs politiques et grand public. L'enjeu est de proposer une cartographie s'adressant à des utilisateurs aux compétences diverses, néophytes ou spécialistes pour une cartographie exploratoire (Antoni et al, 2004). L'évolution des technologies associées au Web offre d'énormes possibilités à ce type d'approche, en particulier grâce à la souplesse qu'apporte une interactivité de haut niveau.
L'exposé de cet article concerne plus précisément la réalisation d'un environnement, basé sur l'infrastructure du Web, capable de générer dynamiquement des cartes continues, c'est-à-dire affranchies d'éventuels maillages d'observation (administratifs, grilles de collecte, etc.). L'objectif est de visualiser la distribution spatiale des phénomènes analysés à un niveau macroscopique, c'est-à-dire largement supérieur aux maillages d'observation. La méthode que nous proposons, appelée méthode de transformation par potentiel, conserve la masse totale des données et en donne une représentation non biaisée. Définie empiriquement dans les années 1990 par les travaux de l'UMR Géographie-cités sur les conséquences de la réunification allemande (Grasland, 1991) et de la chute du mur de Berlin en Europe (Boursier-Mougenot et al. 1993), l'étude de cette méthode en vue de la création d'un outil d'analyse spatiale multiscalaire a abouti à une publication méthodologique de référence (Grasland et al., 2000). Le lissage est un cas particulier d'application de cette méthode globale, lorsque la portée du potentiel est petite.
Une des difficultés de diffusion de cette méthode d'analyse spatiale et de représentation cartographique vient, d'une part du coût élevé du calcul 2 qui empêche de répondre à l'exigence d'interactivité et, d'autre part de la contrainte sur la sécurité et la confidentialité des données analysées.
Cet article présente la solution conçue pour répondre à ces besoins. Elle s'est développée suivant trois axes : (i) distribution des calculs sur un serveur multiprocesseur détenant les données, (ii) parallélisation des tâches de calcul, (iii) visualisation des cartes sur un client web interactif avec connexion sécurisée.
Cette présentation s'organise autour du plan suivant : dans un premier temps, les fondements théoriques et les applications possibles de la méthode du potentiel sont rappelés, et cette méthode est située en regard de ce qui existe. Suivent ensuite la justification de l'architecture retenue et la description de la stratégie d'optimisation des calculs. Enfin, les résultats de cette réalisation sont présentés ainsi que les perspectives d'amélioration qu'elle offre. 2 Il faut plusieurs heures pour calculer une carte de résolution moyenne visualisant la densité de population en France, à partir du recensement sur les 36000 communes.
-20 -RNTI-E-13 2 La méthode de transformation par potentiel
Principe
Les représentations cartographiques continues de phénomènes spatiaux discrets sont nécessaires lorsque l'on souhaite s'abstraire d'un maillage spatial, parce que le maillage est hétérogène ou qu'il n'est pas signifiant pour le phénomène étudié, afin de ne garder que l'organisation spatiale du phénomène, sans référence au découpage sous-jacent du territoire.
La méthode considère un certain espace géographique, sur lequel est plaqué un maillage constitué d'unités territoriales auxquelles sont associés des stocks, c'est-à-dire des variables statistiques résultant de dénombrement, comme la population, le nombre d'actifs, etc. Le maillage territorial peut être, par exemple, l'ensemble des communes françaises et leur équivalent européen dans la nomenclature NUTS de niveau 5, adoptée par l'European Spatial Planning Observation Network. Ce type de maillage s'emboîte généralement dans un maillage de plus haut niveau, par exemple le maillage départemental (NUTS-3) : un département est composé d'un sousensemble de communes. Les indicateurs considérés pour cette méthode possèdent une propriété additive, c'est-à-dire que la valeur d'un stock d'une maille de niveau supérieur est égale à la somme des valeurs des mailles qui la composent au niveau inférieur. L'objectif est de calculer en tout point de l'espace discrétisé la valeur du potentiel de chaque stock. La discrétisation est une division de l'espace en parcelles régulières, formant une grille, par exemple. En tout point de cet espace géométrique, la valeur du potentiel doit être comprise comme la valeur probable de l'indicateur considéré, qui dépend de la contribution de chaque maille de l'espace géographique, pondérée par sa distance au point calculé.
Si on note A l'ensemble des unités territoriales, a un élément de cet ensemble, Sa la valeur du stock sur cette unité, alors sachant que les effets des stocks s'additionnent et sont liés à la distance ? entre a et le point M, le potentiel ?(M) est défini en tout point M de l'espace géométrique par :
[ 1]
A a " # Par exemple, considérons A, l'ensemble des communes européennes, et S le nombre d'habitants centenaires par commune, et supposons que l'on veuille calculer le potentiel de centenaires en tout point de l'espace. Pour chaque commune a, Sa est le nombre de personnes centenaires, et g a le centre de la commune.
On spécifie alors la distance ?(a,M) en mesurant la distance d entre M et g a, un point représentatif de a (qui peut être son centre de géométrie, son centre administratif ou industriel, etc.). La contribution au potentiel de chaque élément a est pondérée par une fonction f de la distance d, car l'effet d'un stock diminue usuellement avec la distance : il est maximal à une distance nulle et nul à une distance infinie. Pour que le potentiel ait du sens, en particulier lorsqu'il dépendra d'un paramètre, on normalise celui-ci par l'équation [2], en tout point O de l'espace:
La somme totale des stocks est égale à l'intégrale du potentiel, on obtient ainsi une redistribution de la masse sur l'espace considéré.
Dans une métaphore du modèle de gravité, ?(M) s'interprète comme l'attraction exercée par l'environnement sur un mobile placé en M, dont le vecteur de déplacement serait alors -grad ?. Une interprétation duale serait aussi que ?(g a ) mesure l'influence d'une masse placée en g a sur l'ensemble des points M de son voisinage. Par exemple, le nombre de centenaires vivant à Nuoro, une ville de la Sardaigne, contribuera beaucoup plus à l'estimation d'un point M situé dans son voisinage que les centenaires habitant Rome. Du point de vue méthodologique, la méthode s'apparente aux méthodes de traitement du signal par déconvolution du signal échantillonné (Grasland, Vincent, 2006).
De l'équation [1], il découle que la complexité du calcul dépend à la fois de la taille de l'espace administratif (le nombre n d'éléments a), et de la résolution de l'image à produire (le nombre m de points M que l'on estime).
Le calcul dépend principalement de la fonction f, fonction d'interaction spatiale. Elle intègre les hypothèses concernant les lois de diffusion dans l'espace associées au phénomène étudié. Trois modèles de fonction paramétrée sont proposés : un modèle à support limité (disque et disque amorti), un modèle exponentiel (Gaussienne) pour des interactions proches -la décroissance de f se fait alors selon une exponentielle négative -; et enfin un modèle à interaction de longue portée (Pareto) -la décroissance suit une puissance inverse. Cette méthode permet par exemple de modéliser et d'étudier la propagation d'épidémies : leur diffusion pourra se faire soit sur de longues distances, soit sur de courtes distances, suivant le rayon d'action de l'élément contaminant (Grasland et al., 2005-a). L'utilisateur peut tester différents modèles en choisissant la fonction d'interaction à appliquer.
L'analyse du phénomène dépend aussi de la portée p de la fonction d'interaction. La portée est définie comme la distance moyenne d'action d'une masse sur son voisinage. Elle est reliée à la forme de la fonction d'interaction par l'équation suivante en tout point O de l'espace (après passage en coordonnées paramétriques, r représente le rayon) :
La portée peut être interprétée comme l'échelle spatiale de représentation choisie. Le tandem (fonction, portée) traduit concrètement les hypothèses économiques et sociologiques associées aux interactions entre les acteurs sur le territoire. À portées identiques, le calcul du ratio de deux potentiels de stocks différents s'interprète comme une densité. Par exemple, le potentiel ? P de population peut se rapporter au potentiel ? S de superficie, ce qui nous donne une densité de population sur une portée donnée et uniforme. (Voir figure 1). D'autre part, il faut choisir le type de distance utilisée. Celle-ci dépend de la taille de l'espace couvert par la carte. Par exemple, à l'échelle d'un continent, on ne peut pas utiliser la distance euclidienne sans introduire des déformations. La - 
Deux exemples d'application empirique
Nous avons déjà dressé un récapitulatif concernant des usages relativement classiques de la méthode, (Plumejeaud et al., 2007). Nous présentons ici des usages plus complexes, mettant en oeuvre une modélisation de flux associés aux différentiels locaux, ou des distances non-isotropiques avec des effets de barrière.
Le premier exemple concerne la recherche d'une mesure des polarisations locales en Europe à partir d'une information très pauvre (population et superficie des communes en 1999) mais très détaillée spatialement, correspondant au niveau communal, (Dubois, Gloersen & al. 2007  (Dubois, 2007).
-24 -RNTI-E-13
Le calcul des potentiels de population et de superficie pour des voisinages gaussiens de portée croissante (10,20,40,80, 160 km) a permis de résoudre le premier problème (élimination de l'hétérogénéité liée au découpage initial) tandis que la comparaison des densités de population dans des voisinages de portées successives a permis de résoudre le second problème en repérant les zones localement plus denses que les espaces environnants. La série de cartes ainsi obtenues (voir figure 2) permet de mettre en valeur les pics relatifs de concentration de population à différents niveaux de généralisation et d'observer comment des pics locaux (différentiels de densité entre les lissages à 10 et 20 km) fusionnent progressivement dans des pics régionaux ou globaux. Ce résultat est fondamental en terme d'aménagement du territoire européen car il permet de donner une formalisation objective d'un concept politique central qui est celui du polycentrisme. Même si celui-ci n'est envisagé ici que sous une forme morphologique (l'approche devant également être menée en termes de flux et de réseau) la méthodologie proposée ouvre de très importantes perspectives pour l'identification de réseaux de villes et de coopérations transfrontalières.
Le second exemple concerne l'étude de la vulnérabilité des régions européennes face à la mondialisation 3 . La méthode des potentiels a été utilisée en tenant compte de l'anisotropie de l'espace puisque les distances ont été mesurées en temps routier et que d'éventuels effets de barrières liés au franchissement des frontières internationales ont été introduits. A partir d'un indicateur de vulnérabilité connu pour l'ensemble des régions européennes (part des actifs dans les secteurs industriels menacés tels que le textile, l'électronique et la mécanique), la vulnérabilité de chaque région a été comparée à celle des régions voisines en tenant compte d'une part, de leur accessibilité routière, (matrice de distance temps), et d'autre part, du fait que ces régions voisines étaient situées dans le même pays ou dans un pays voisin. Nous avons ainsi défini 7 niveaux de vulnérabilité correspondant à une discrétisation de l'espace autour de la région elle-même selon différent temps de trajets (moins de 2h, entre 2 et 4h, entre 4 et 8h), y compris le franchissement d'une frontière. Comme l'illustre la figure 3, on identifie ainsi la région elle-même (Reg), une auréole de régions situées à moins de 2h du même pays (N1nat) ou d'un pays voisin (N1int), puis une deuxième zone de régions situées entre 2 et 4h (N2nat, N2int) et enfin une troisième zone de régions situées entre 4 et 8 h (N3nat, N3int). Les seuils de 2h, 4h et 8h ont été choisis pour tenir compte des différents points de vue des voyageurs considérés :
-pour les travailleurs : 2h représente le temps maximal acceptable pour se rendre à son travail -pour les entreprises : 4h représente le temps maximal acceptable pour effectuer un aller-retour dans une journée -pour les fournisseurs : 8h représente le seuil maximum de transport franchissable en une journée.
La distinction national/international est quant à elle fondamentale en raison des politiques plus ou moins protectionnistes de chaque pays en matière de compétition industrielle et d'ouverture du marché de l'emploi.
FIG. 3 -Définition de voisinages tenant compte des temps de trajets et des franchissements de frontières.
Les figures 4 et 5 présentent les résultats pour deux régions typiques permettent de saisir l'intérêt de l'approche ainsi proposée par rapport à la thématique de la vulnérabilité régionale face à la mondialisation. La région française des Ardennes et la région italienne de Modène présentent grosso modo des niveaux voisins de vulnérabilité, chacune ayant environ 15-16% de ses emplois dans des secteurs industriels menacés. Toutefois, elles s'inscrivent dans des contextes radicalement différents si on examine la situation dans leurs différents voisinages fonctionnels comme définis précédemment.
FIG. 4 -Situation de la région des Ardennes (France).
-26 -RNTI-E-13
La région des Ardennes (voir figure 4) est un pic isolé de vulnérabilité, entouré de régions qui ont très peu d'emplois dans les secteurs vulnérables, aussi bien à l'intérieur du territoire français que dans les espaces voisins de Belgique, Luxembourg, Belgique ou Pays-Bas. Une crise des secteurs industriels vulnérables pourra donc être compensé par la recherche d'emplois dans les régions voisines. Mais les entreprises touchées par la crise ne pourront pas compter sur une solidarité des territoires voisins.
FIG. 5 -Situation de la région de Modène (Italie).
La région de Modène (voir figure 5) se situe au contraire à l'intérieur d'un véritable bastion de régions vulnérables dans un cadre qui est essentiellement national (aucune région étrangère à moins de 2h de route). Les régions les plus proches (moins de 2h de route) sont d'ailleurs encore plus vulnérables que celle de Modène. Plus on s'éloigne, plus la vulnérabilité diminue. À l'inverse de la région des Ardennes, les employés des secteurs vulnérables risquent d'avoir des difficultés à trouver un emploi dans les régions voisines si la crise frappe l'ensemble du secteur. D'un autre côté, les entreprises pourront plus facilement faire bloc à l'échelle de plusieurs régions voisines…
Situation par rapport à d'autres méthodes
Les vertus de la méthode ne s'expriment pas sur un maillage régulier (taille des unités quasi-égales, et distance relative des centres toujours identique), mais au contraire lorsque l'on traite des données issues de recensements sur des maillages irréguliers. En effet, la forme et l'échelle du support des données jouent un rôle très important sur l'estimation produite : ces facteurs sont la source de problèmes identifiés comme le Modifiable Areal Unit Problem (MAUP) ou plus généralement le Change Of Support Problem (COSP), qui ont donné lieu à divers développements méthodologiques et discussions sur leurs apports et contraintes respectives [Gotway et al., 2002], et qui sont liés à la nature des données traitées.
Le premier facteur d'erreur est un effet de l'agrégation : les résultats de l'estimation de la répartition spatiale des données varient en fonction de la taille des unités. Par exemple, le traitement d'un jeu de données communales au niveau de son découpage initial ne donne pas des résultats identiques à celui qui serait fait en regroupant les données au niveau départemental. Ce qu'on appelle communément -27 -RNTI-E-13 l'« effet d'échelle » (« scale effect ») s'explique par le fait que la variance de l'échantillon initial (communal dans l'exemple) est mathématiquement plus élevée que la variance de la moyenne de ces échantillons. Ce qui signifie que sur des maillages fins, on perçoit une hétérogénéité des valeurs plus forte que sur des maillages plus grossiers, effets du pour partie à un simple effet mécanique, et pour partie à l'organisation du phénomène à cet échelon. Le second facteur, dénommé « effet du découpage » (« zoning effect »), dérive du mode de regroupement des données à une échelle fixée : on prouve que la corrélation entre des données spatiales varie en fonction du découpage territorial (comparaison faite entre entités de surfaces comparables) (Openshaw et al., 1979). Les méthodes classiques d'interpolation telles que triangulation, moyenne locale, interpolation polynomiale (splines, Bezier) ou méthode de Shepard subissent les effets du MAUP : elles sont bien adaptées à l'estimation de variables continues dans l'espace pour lesquelles il existe des points de mesures (comme la température, ou l'altitude), mais ne le sont pas pour des variables résultant d'un comptage sur une zone délimitée à l'intérieur de laquelle la répartition effective de la population est inconnue. Notre démonstration (Grasland, et al, 2006) s'appuie sur la comparaison de cartes obtenues à partir d'un maillage de niveau NUTS 2 ou NUTS 3 en employant la méthode de Shepard. Elles ne font pas disparaître le maillage sousjacent, et au contraire, elles pourraient faire croire que les disparités observées sont le fait du phénomène étudié, alors qu'en réalité, elles sont la résultante de l'hétérogénéité du maillage. Ces méthodes sont les plus usuellement implémentées dans les SIG.
Pour solutionner le problème du COSP, des auteurs ont discuté les avantages des méthodes géostatistiques (krigeage) qui modélisent la variabilité spatiale des observations, et utilisent ces résultats pour inférer une surface continue, (Journel et al., 1978). Ces méthodes ont l'avantage de fournir une mesure sur l'incertitude des résultats. Elles s'implémentent via un filtre de Kalman, et sont implantées dans de nombreux SIG. A contrario, la méthode du potentiel propose un modèle de diffusion a priori défini par un couple (fonction, portée) proposé par l'utilisateur, et la structure spatiale du phénomène n'entre pas en compte. Cependant si les méthodes géostatistiques ont un caractère prédictif efficace, elles restent des méthodes locales qui contrairement à la méthode du potentiel ne permettent pas de prendre en compte l'impact de toutes les mesures sur un point donné de l'espace géographique. D'autre part, les modèles géostatistiques doivent êtres simplifiés lorsque la quantité de données à traiter est importante.
Les méthodes basées sur les modèles bayésiens hiérarchiques sont aussi des solutions envisageables pour l'estimation de variables dans le cadre du MAUP. Les hypothèses sous-jacentes sont que la variable à estimer dépend (et ceci s'exprime sous forme de probabilité conditionnelle) d'autres variables dont on fournit une loi de dispersion spatiale (loi de Poisson, ou Gaussienne par exemple), et dont les données de recensement sont fournies. Il est ainsi possible de modéliser des interactions complexes entre variables pour affiner l'estimation (Wickel, 2002). Ce type de méthodes s'inscrit donc dans des démarches plus explicatives que celle que nous proposons. Le cadre proposé pour la méthode du potentiel ne nécessite pas de statistiques auxiliaires pour l'estimation des variables d'intérêt.
-28 -RNTI-E-13
La méthode la plus similaire est la méthode pycnophylactique (Tobler, 1979) qui, elle aussi, conserve la masse des données. Elle réalloue les stocks mesurés à l'intérieur de chaque maille de façon à :
(i) garantir la continuité avec une maille voisine, (ii) conserver sur chaque maille la masse mesurée. La courbe continue de la figure 6 montre comment peut se passer le réajustement de la répartition des stocks à l'intérieur de chaque unité. C'est un cas simple, on peut employer des méthodes d'ajustement plus complexes où les surfaces obtenues sont au moins dérivables, voire de classe supérieure. Rase donne des développements intéressants de cette méthode (Rase, 2001). Mais en raison du prédicat (ii), la visualisation dépend alors du maillage utilisé pour l'analyse : les résultats sont différents selon le niveau d'agrégation des données, alors que ce n'est pas le cas avec le potentiel. La méthode des potentiels s'apparente aux méthodes de traitement du signal par transformation de Fourier, visant à découvrir la structure du signal échantillonné et ceci indépendamment de la fréquence d'échantillonnage. Dans le cadre de l'analyse spatiale, cette fréquence doit être comprise comme la mesure de la régularité des recensements dans l'espace, évaluée via la distance entre chacun des centres administratifs des unités spatiales du maillage support de l'étude. Pour cette raison, la méthode des potentiels s'inscrit dans la lignée des méthodes proposées pour solutionner les problèmes d'instabilité des résultats, notamment les corrélations, en fonction de la résolution spatiale choisie.
Cependant, en dessous d'une certaine portée, la méthode devient imprécise ; la portée minimale peut être calculée par l'analogue du théorème de Nyquist : elle vaut deux fois la taille maximale des mailles (Nyquist, 1928). Enfin, un maillage trop hétérogène conduit à faire un compromis entre les portées minimum associées à chaque classe de taille d'unité.
-29 -RNTI-E-13
Par ailleurs, la méthode de transformation par potentiel n'intègre pas encore les effets de barrières (cet aspect fait partie de nos futurs travaux de recherche). Par exemple, les montagnes sont des barrières géographiques qui jouent un rôle dans la dispersion spatiale des phénomènes étudiés. De même, la mer est un espace géographique non constructible, et en ce sens, les côtes maritimes pourraient être modélisées comme des barrières infranchissables lorsque on étudie la densité d'habitat.
Réalisation d'un prototype
Un prototype a été réalisé pour des utilisateurs disposant de données socioéconomiques géo-référencées, des géographes par exemple, et désireux de tester leurs hypothèses de diffusion spatiale des phénomènes étudiés. Pour l'instant, ces analyses utilisent la distance orthodromique, qui permet de travailler sur de vastes espaces d'étude, et requiert un format de données suffisamment simple. Le prototype se base sur une architecture distribuée client serveur.
En effet, le volume de données à traiter et les ressources exigées pour les calculs sont, de fait, importants. Par conséquent, la partie graphique (qui inclue la visualisation et la configuration de l'analyse) est déportée sur un client Web Java, tandis que les calculs et le traitement lourd de données sont effectués sur un serveur accessible à distance, via le protocole SOAP (Simple Object Access Protocol) avec sécurisation des échanges via SSL (Secure Socket Layer). Le traitement des données coté serveur est optimisé de façon à produire des résultats intermédiaires dans des temps n'excédant pas quelques secondes. La figure 7 donne une vue globale de cette architecture.
FIG. 7 -Vue générale de l'infrastructure distribuée.
L'infrastructure de gestion de données, d'authentification et de gestion d'erreurs restant encore à développer, l'installation des fichiers de données dans un répertoire dédié à l'utilisateur se fait manuellement du côté du serveur. Le format du fichier de données est le suivant : il contient une liste des points de mesure connus par leur latitude et leur longitude, et la valeur du stock associé, et il est nommé d'après le nom du stock. Le client Web est fourni à l'utilisateur avec le certificat de sécurité adéquat, avec le fond de carte approprié qu'aura fourni l'utilisateur au format -30 -RNTI-E-13 MIF/MID. Ainsi, il peut utiliser librement et à tout moment l'infrastructure, sur l'espace d'étude qui l'intéresse. Le serveur de calcul est installé sur une machine de type SMP, dont les ressources sont partagées par d'autres projets de recherche, et qui est accessible à travers cette connexion Web sécurisée. Le serveur est prévu pour le partage de charge dynamique, il n'y a donc pas en théorie de limitations sur le nombre d'utilisateurs, même si dans les faits, il est encore restreint à la communauté des chercheurs en géographie, analyse spatiale et aménagement du territoire qui doivent s'adresser au projet HyperCarte pour son utilisation.
Communication entre le serveur et le client
Le format des données renvoyées par le serveur est contraint par les fonctionnalités attendues sur le client. Outre l'interactivité sur le choix des palettes, du nombre de classes et du type de progression dans la distribution des couleurs, on souhaite que le client puisse générer à la demande un rapport numérique (sous forme de fichier texte ou bien HTML) contenant les coordonnées géographiques de chaque point M, avec la valeur de son potentiel. Le client a donc tout avantage à récupérer et sauver la grille matricielle des valeurs de potentiel calculées plutôt qu'une image au format PNG, JPEG, ou GIF. Il peut ainsi redessiner une image rapidement en cas de changement de préférences graphiques. 
Stratégie d'optimisation
Remarquons, en premier lieu, que les stratégies classiques de gestion de cache ne sont pas adaptées à notre cas car chaque requête doit générer un résultat global qui ne peut être pré-calculé puisqu'il dépend des paramètres de l'analyse. Cependant un examen approfondi des tâches de calcul sur le serveur montre qu'il existe des redondances que nous pourrions exploiter pour optimiser certaines parties du calcul. (Grasland, 2005 -32 -RNTI-E-13
La visite de chaque branche de niveau n-1 dépend de la réussite du test suivant, qui vérifie si le poids des enfants du noeud n-1 est négligeable ou non :
[ 4]
noeud n"1 ! Le point délicat est d'ajuster la valeur de l'epsilon de manière à ne pas négliger trop de points. Par défaut, la valeur est fixée à un 1/1000 de la somme totale des stocks. Des études plus approfondies restent à mener sur l'influence d'epsilon sur, d'une part la précision des calculs obtenus, et, d'autre part, la durée du calcul. Intuitivement, on suppose qu'une petite valeur d'epsilon allongerait le temps de calcul, mais aussi augmenterait la précision des résultats. Inversement, un grand epsilon provoquerait une forte simplification du calcul, qui serait alors ramené à une moyenne mobile sur moments gaussiens, et diminuerait l'intérêt de la méthode.
Le second facteur de lenteur de calcul (l'évaluation d'expressions contenant des termes en arccosinus, cosinus et sinus d'angles) peut être contourné en tabulant de façon fine ces fonctions. C'est-à-dire que les valeurs des fonctions sur des angles correspondant à une division régulière et fine d'un intervalle I donné sont pré-calculées, et pour tout angle la valeur de la fonction est approximée par la borne inférieure de la division à laquelle il appartient. Le grain de cette tabulation est fixé à l'avance par un paramètre du programme.
Visualisation interactive
La méthode prend tout son intérêt dans un contexte d'utilisation interactive, qui permet une exploration des structures spatiales révélées selon un certain nombre de paramètres :
-la fonction d'interaction : une liste déroulante présente la liste des fonctions implantées sur le serveur : disk, amortized_disk, gaussian, pareto, exponential.
-la portée moyenne en kilomètres de l'analyse, sélectionnée via une barre de glissement, ou par saisie numérique. Ce choix n'est pas borné.
-la résolution de visualisation définie en nombre de points en largeur et hauteur désirés pour la grille, et qui se rapporte au pas de discrétisation de l'espace étudié.
-le cadrage (le territoire ciblé) pour l'analyse, correspondant actuellement à l'espace visualisé. Un zoom ou un déplacement modifie donc le cadrage.
-les stocks proposés dans deux listes déroulantes séparées, une pour le numérateur, l'autre pour le dénominateur, afin de pouvoir ensuite calculer un ratio de potentiel. Le client interroge le serveur lors de l'authentification de l'utilisateur pour connaître la liste des stocks disponibles.
Une collection d'onglets est créée, chacun contenant une carte spécifique : le premier présente simplement l'aire d'étude et le maillage administratif. Les trois suivants présentent pour une portée V 1 donnée le potentiel du numérateur, du dénominateur et leur ratio Z 1 . Les trois autres suivants de même mais sur une portée différente V 2 . Enfin le dernier onglet compare les densités entre les deux voisinages en visualisant leur différence : Z 2 -Z 1 .
L'utilisateur dispose en plus des fonctions de base de l'interactivité : zoom, déplacement dans une carte, navigation dans l'atlas de cartes ainsi produit. La visualisation d'une carte de potentiel se fait en superposition avec un fond vectoriel présentant les limites administratives de l'espace analysé. Ces cartes présentent une gradation de couleurs, suivant l'intensité du phénomène, paramétrable indépendamment pour chaque onglet, via une proposition à gauche de la zone de visualisation pour le choix de palette, le type de progression, et le nombre de classes dans la distribution. La figure 10 donne un aperçu de l'interface du client.
FIG. 10 -Aperçu de l'interface, avec un ratio du PNB par habitant sur un calcul gaussien de portée 50 km.
Bilan de la réalisation
Performances et expérimentations pour un usage local
Le serveur peut s'utiliser de façon autonome par rapport au client. Les expérimentations suivantes ont été réalisées sur un serveur bi-processeurs sous Linux (Pentium 4 à 2,6 Ghz, avec 1 Go de mémoire), une machine de travail pour un utilisateur ordinaire, avec comme jeu de test la population en Europe recensée au niveau communal, ce qui représente 116203 entités géographiques. Par exemple, -34 -RNTI-E-13 une carte de population est calculée pour une fonction gaussienne de portée 80 km avec la résolution forte de (800 x 600) en 1 minute 38 secondes.
La complexité de l'algorithme est vérifiée lors des mesures sur les temps de calcul, comme le montre les mesures du tableau 1. Mathématiquement, elle est linéairement proportionnelle à la résolution (nombre de points n à calculer sur l'image). Elle est aussi linéairement proportionnelle à la portée p. La connaissance de complexité en O(np) permet d'estimer le temps de calcul d'une carte en fonction de la résolution demandée n et de la portée p requise. Cette durée reste raisonnable pour un usage sur des machines classiques : 2 minutes environ pour une longue portée (100 km) et une résolution fine de (800 x 600).
Résolution
Portée ( 
Performances et expérimentations pour un usage serveur
Nous avons implanté des options pour l'exécution parallèle du calcul sur un ensemble de processeurs, dans le cas où le serveur de calcul est partagé par plusieurs usagers, et où il se déploie sur une machine multi-processeurs. L'algorithme de calculs des potentiels est une itération sur l'ensemble des points de la grille qui sont arrangés dans un tableau, dont la taille n égale la valeur de la résolution. Chaque point peut-être calculé indépendamment des autres : pour cette raison la parallélisation du programme est très aisée. Elle est fondée sur la distribution de tâches (le calcul d'une entrée du tableau) à un nombre k de processeurs. Le nombre de points dévolus à un processeur est donc de n/k, et le calcul se termine lorsque chaque processeur a terminé le calcul de sa portion de tableau. Cette distribution naïve des tâches montre cependant deux limitations majeures. En premier lieu, un tel algorithme n'est pas résistant aux perturbations : si les capacités d'un processeur tombent à 50% brusquement, le temps de calcul est alors rallongé du temps équivalent à la moitié de sa tâche, puisqu'il n'y a pas de procédé de rééquilibrage des charges de travail automatique entre processeurs. D'autre part, notre grille est très hétérogène : les points situés dans des zones à faible densité de mesure sont calculés bien plus rapidement grâce au procédé d'élagage, et certains processeurs terminent donc leur travail plus vite.
Une solution pour la répartition de charge dynamique a été imaginée, par un mécanisme de transmission de charge adaptatif (Roch, 2006) : chaque processeur « libre » vole la moitié de la tâche de travail restante p au processeur occupé. Cette redistribution n'a lieu que lorsque le processeur occupé achève la portion de travail -35 -RNTI-E-13
indivisible qui lui est attribuée ? log(p). ? est un paramètre configurable, qui est ajusté de manière à ce que le temps de contention (lock pour accéder au tableau de données) soit largement inférieur au temps de calcul de la portion du tableau. Cette notion de quantité minimale de travail est introduite pour éviter les problèmes de contention lors de l'accès au tableau de points : lorsque qu'un processeur prend une tâche, il actionne un sémaphore sur le tableau et bloque l'accès des autres processeurs au tableau. Ce temps d'inter-blocage est court, et la définition d'une charge de travail minimale optimisée évite la répétition fréquente d'inter-blocages. Cet algorithme a été mis en oeuvre avec PTHREAD dans notre implémentation et, testé sur une machine multi-processeurs de type SMP, huit coeurs avec mémoire partagée sur la population européenne, à une résolution fixée de 800*600 et une portée gaussienne de 100 km. Il donne les résultats confirmant l'efficacité de la méthode, (voir la figure 11). En outre, nous signalons que cet algorithme a été implémenté avec deux librairies différentes pour l'ordonnancement de tâches, TBB 6 (Intel Threading Building Blocks, http://threadingbuildingblocks.org/) et Kaapi (Danjean, 2007). Quelque soit la librairie, les performances sont améliorées de façon similaires : le gain en temps de calcul est linéaire avec le nombre de processeurs mobilisés, tant que la redistribution n'est pas saturée (observée à partir de 5 processeurs avec PTHREAD, elle est repoussée avec Kaapi car l'algorithme de répartition de tâches est mieux optimisé).
Saturation de la redistribution FIG 11 -Accélération du calcul des potentiels en fonction du nombre de processeurs
Le mode de répartition dynamique de charge décrit ne tient pas compte de la dimension spatiale des données : une répartition de charge basée sur un découpage -36 -RNTI-E-13 spatial statique paraît plus efficace. Afin de vérifier cette hypothèse, une seconde version de l'algorithme de répartition de charge a été implémentée et testée : l'espace d'étude est découpé en tuiles contiguës de tailles égales, et chaque tuile est traitée en parallèle par un des processeurs mobilisés pour le calcul. La figure 12 illustre les résultats comparés des deux modes de répartition de charge sur un banc d'essai de 16 processeurs. Les points des courbes situent le taux d'occupation des processeurs au cours du calcul, en fonction du temps. On constate que l'algorithme de répartition dynamique de charge termine plus tôt (une centaine de secondes d'écart), et occupe avec un meilleur rendement les processeurs que l'algorithme de répartition statique. Une première explication est suggérée par l'interprétation du palier observé au niveau du taux d'occupation des processeurs pour l'algorithme statique : certains blocs de calculs avec une densité de données moindre auront terminés plutôt, laissant des processeurs inactifs, dans l'attente de la fin du calcul.
FIG. 12 -Comparaison des performances de deux modes de répartition de charge pour le calcul d'une carte de potentiel sur un ensemble de 16 processeurs.
La version statique pourrait être améliorée en découpant les tuiles proportionnellement à la densité de données présente, de manière à équilibrer la charge sur les différents processeurs. Cependant, un partitionnement statique, même prenant en compte la répartition spatiale des données, ne permet pas de s'adapter dynamiquement aux variations de charges CPU induites par les autres utilisateurs de l'environnement de calculs intensifs, contrairement à l'algorithme adaptatif. Par contre, la combinaison d'un partitionnement statique pour la première partie du -37 -RNTI-E-13 calcul et d'un équilibrage de charge dynamique pour la fin du calcul est une stratégie qui porte souvent ses fruits lorsqu'on travaille sur des volumes importants de données (Jafar, 2006).
Les temps de latence réseau sont bons, malgré le fait que nous n'avons pas encore mis en oeuvre la compression des données échangées via gzip. En fait, on mesure un temps correspondant à l'emballage de la réponse, l'encryptage et le déballage de 4s localement avec une résolution de 300 x 400. Côté client, la reconstruction de l'image à partir d'un tableau de flottants prend très peu de temps (128ms). Cette matrice est re-calculée dès lors que l'utilisateur change un des paramètres d'analyse, agrandit, réduit ou se déplace dans la zone de visualisation. Elle est cachée avec les paramètres de la requête correspondante afin de pouvoir l'exploiter ultérieurement. La résolution demandée étant le plus souvent inférieure à celle de l'image vectorielle (1027 x 688), une interpolation bi-quadratique est appliquée sur l'image produite afin de la caler sur le fond vectoriel.
Perspectives d'amélioration
Plusieurs pistes s'offrent pour améliorer les performances du serveur de calcul. Par exemple, le calcul de la distance orthodromique peut bénéficier d'un pré-calcul supplémentaire. Sans entrer dans les détails, il faut encore exploiter les travaux de recherche concernant la quête du compromis idéal entre consommation d'espace mémoire et gain de temps obtenu par tabulation. De même, la troncature des flottants transmis dans la grille matricielle réduirait le volume des données échangées, et accorderait donc un gain de temps sur la transmission des données. Un algorithme qui automatise la détermination du niveau de troncature est en cours d'élaboration.
D'autres pistes, moins classiques, sont liées à l'usage de notre algorithme de cutoff. Il s'agit de préparer une stratégie de sous-échantillonage des données : par exemple, adapter la valeur du seuil d'élagage dynamiquement au lieu de fixer arbitrairement sa valeur, ou bien limiter la visite de l'arbre à un certain niveau, sans aller jusqu'aux feuilles, lorsque que le niveau d'information requis est très grossier. Dans ce but, l'usage de cubes de données organisés en fonction de la hiérarchie spatiale des maillages territoriaux, tels ceux développés dans le domaine de l'analyse de données en ligne (OLAP) serait d'un très grand intérêt (Rigaux, 2005).
Enfin, la sélection de la distance pourrait se faire de manière adaptative, selon l'échelle d'étude retenue. Par exemple, employer la distance euclidienne sur la région Rhône-Alpes n'introduirait pas de biais notables.
De façon identique, quelques améliorations pour le client sur le plan de l'ergonomie sont prévues. Comme, par exemple, décorréler la zone de visualisation et la zone de traitement, en utilisant un cadre de sélection de la surface à traiter. Nous souhaitons donner une interprétation plus directe de la résolution : indiquer un pas en nombre de kilomètres, plutôt qu'un nombre de points sur une grille. Enfin, le calcul et la représentation de courbes de niveau à partir de l'image matricielle amélioreraient sensiblement le fondu avec le fond vectoriel représentant le maillage administratif.
-38 -RNTI-E-13
Le client offre aussi un champ de réflexions en ce qui concerne l'usage de caches autorisant un raffinement progressif des images, grâce à l'émission de requêtes successives. En effet, tant que l'utilisateur ne modifie pas ses souhaits, il serait judicieux de construire une image dont la résolution augmente dans le temps, et d'anticiper un zoom sur la zone lissée.
Conclusion
Cet article présente HyperSmooth, une mise en oeuvre du calcul et de la visualisation de potentiels dans un contexte de cartographie interactive. L'objectif étant de fournir un outil accessible à des utilisateurs via le Web, tout en assurant la sécurité et la confidentialité de ces données. Nous montrons que la méthode est adaptée pour étudier la propagation spatiale de phénomènes sociaux, environnementaux, ou économiques. Nous soulignons que le paramétrage d'une telle analyse peut s'avérer délicat, et nécessite donc de développer un outil adapté afin de rendre les choix plus compréhensibles pour un utilisateur néophyte.
La réalisation d'un tel prototype nécessite une réflexion avancée et coordonnée sur l'architecture, les modalités d'optimisation des calculs, et de paramétrage de l'analyse pour offrir une visualisation interactive. En effet, le coût du calcul, et les contraintes de confidentialité sur les données nous placent face à un défi technique.
Le bilan de notre réalisation s'avère positif quant au choix de l'architecture distribuée : répartition des calculs sur un serveur parallèle d'un coté, visualisation et paramétrage de l'analyse sur un client Java intelligent de l'autre, les deux parties étant connectées via un protocole de plus en plus répandu et présentant une accessibilité et sécurité maximale : SOAP, couplé avec un cryptage SSL. De plus, les calculs sont accélérés grâce à l'utilisation d'une méthode de cut-off algébrique, et une tabulation des distances orthodromiques. L'ensemble du code et un manuel du serveur sont accessibles librement sur Internet à l'adresse suivante : http://hyantes.gforge.inria.fr/ Une question d'ordre théorique et algorithmique porte sur l'introduction d'autres distances, et sur l'extension de la méthode de façon à illustrer les propriétés anisotropiques de l'espace géographique réel. Comme l'a montré une étude des phénomènes de tempêtes, aux flux très orientés (Boulier, 2003), il est intéressant de développer sur la base de la méthode générale une vision anisotropique de l'espace. Pour quantifier le potentiel en un lieu, il s'agit de tenir compte à la fois de la distance de la source d'information au point à estimer, mais aussi de l'orientation du flux d'information qui relie les deux points. Dans le premier exemple, appliqué à la forêt, les corrélations entre les potentiels ainsi déterminés et les dégâts constatés sont très nettes. Cette modification de la méthode générale permet d'intégrer de nouvelles formalisations des mouvements dans l'espace (vents dominants, courant marin, etc.) complétant celles qui sont associées à la fonction d'interaction spatiale. Cette extension du modèle peut également alors rendre compte des barrières géographiques naturelles comme la mer ou bien les montagnes pour l'étude de la dispersion de phénomènes dans l'espace, ou de phénomènes de polarisation.

Introduction
La prise en compte des connaissances additionnelles constitue un problème essentiel et un vrai défi pour la recherche actuelle dans le domaine de la classification automatique. Il s'agit à la fois de l'expression, de la structuration et de la formalisation des connaissances (appelées aussi connaissances a priori) pour les intégrer dans le processus de la classification automatique. Les premiers travaux dans ce domaine ont été réalisés par (Wagstaff et Cardie, 2000) en modifiant l'algorithme COBWEB proposé par (Fisher, 1987). Les auteurs ont montré, à partir de résultats expérimentaux, une amélioration claire de la précision de la classification. Les mêmes auteurs ont proposé une autre approche qui intègre les contraintes dans l'algorithme K-means (MacQueen, 1967). L'algorithme proposée est appelé COP-Kmeans (Wagstaff et al., 2001). Son principe consiste à contrôler la violation des contraintes dans la phase de mise à jour des classes. Les auteurs arrivent à démontrer qu'il est possible d'améliorer sensiblement la précision du partitionnement même avec un nombre réduit de contraintes. Les auteurs dans (Davidson et Ravi, 2005) ont étudié le problème de la faisabilité de la classification en présence de plusieurs combinaisons de contraintes dans une approche de type K-means. Récemment, nous avons proposé dans (Elghazel et al., 2007)  Les contraintes sur les observations définissent une relation binaire, positive et transitive entre les observations. Ce qui génere des contraintes sous-entendues apportées par le calcul de la fermeture transitive. Ces contraintes sont présentées sous la forme suivante :
L'algorithme proposé
Notre contribution se situe dans les deux phases de l'étape itérative de l'algorithme de Kohonen (Kohonen, 1994) : la phase d'affectation (compétition) au cours de laquelle une forme z i est affectée au neurone de la carte le plus proche au sens de la distance euclidienne et qui n'entraîne pas une violation de contraintes (L'ensemble de ces neurones est noté C Cond ), et la phase de minimisation (adaptation) qui consiste à mettre à jour les poids des neurones du voisinage du neurone vainqueur dans l'étape de compétition. Ces neurones ne doivent pas entraîner une violation de contraintes. La modification majeure apportée à l'algorithme de Kohonen est essentiellement dans la phase de compétition par l'incorporation de la fonction ViolateCon(.) (Algorithme 2). Cette fonction prend en entrée le neurone courant c, l'observation z i présentée à l'apprentissage, l'ensemble des contraintes positives (Con = ) et l'ensemble des contraintes négatives (Con = ). Elle retourne en sortie l'ensemble des neurones C viol qui entraînent une violation de contraintes avec l'observation z i . D'une part, CrTM (Algorithme 1)parcourt les paires (z i , z j ) satisfaisant les contraintes positives Con = et vérifie si z j est affecté à un neurone différent de c, dans le cas échéant, il y a violation de contraintes et donc c ? C viol qui n'entrera pas en jeu dans le choix du neurone vainqueur. D'autre part, l'algorithme parcourt les paires (z i , z k ) qui satisfont les contraintes négatives(Con = ) et vérifie si z k est affecté au neurone courant c, si oui, il y aura une violation de contraintes et donc C viol = {c, V (c)} tel que V (c) représente les voisins de c. Cette vérification est nécessaire pour garantir que z i sera affecté loin du neurone violant c et aussi pour que l'ensemble des neurones C viol ne se rapprochent pas de z i et donc ils ne l'apprennent pas en conséquence. 
Adaptation :mise à jour des poids des neurones C Cond 
un paramètre d'adaptation , appelé pas d'apprentissage et T = T (t) représente le rayon de voisinage. Ces deux paramètres décroissent en fonction du temps t. K T (?(c, r)) = exp( ?0.5?(c,r) T (t)
).
Algorithme 2 ViolateCon(c, z i , Con = , Con =) ENTRÉES: c : un neurone donné, z i : une observation donnée, Con = , Con = : Ensembles des contraintes positives et négatives, respectivement. SORTIES: C viol : Ensemble des neurones entraînant une violation avec l'observation présen-tée z i . 1: pour tout z j tel que Con = (z i , z j ) faire 2:
C viol = {c} 4:
si z k ? c alors  (Rand, 1971). Cet indice représente une mesure d'accord entre deux partitions P 1 , P 2 d'un même ensemble de données A. P 1 représente la partition correcte produite par les étiquettes des classes prédéfinies. P 2 représente la partition produite par l'algorithme CrTM. Chaque partition est vue comme un ensemble de N (N ? 1)/2 paires de décisions où N est la taille de A. Pour chaque paire d'observations z i , z j dans A, P 1 et P 2 les assignent à la même classe ou à deux classes différentes. Nous montrons aussi pour tester notre proposition que les informations apportées par les contraintes peuvent améliorer la performance de la classification même sur les observations qui n'ont pas été concernées par les contraintes ('Held-Out'). Cette dernière mesure représente le taux de bonne classification en ne considérant que les observations qui ne sont pas directement (ou par transitivité) concernées par les contraintes.
Résultats utilisant les contraintes artificielles
La validation expérimentale est réalisée par la génération des contraintes artificielles, c'est-à-dire pour générer une contrainte, nous prélevons aléatoirement deux observations de la base étiquetée et nous comparons leurs étiquettes : si elles ont la même étiquette, une contrainte positive est générée sinon une contrainte négative. Nous avons choisi trois bases de données de la banque UCI (Blake et Merz., 1998) : "Soybean", "Tic-tac-toe" and "Heart Disease". L'application de CrTM sur la première base a fournit une performance de 100% sans aucune contrainte. Ce résultat parfait est dû à la séparation linéaire des 4 classes dans l'espace des données, ce qui n'a pas été un souci pour l'apprentissage de la carte. Nous rappelons que COB-COBWEB et COP-Kmeans atteignent, respectivement, 96% et 98% après l'intégration de 100 contraintes artificielles. COP-b-coloring atteint la performance de 100% avec 30 contraintes. Vue la nature symbolique des variables de la deuxième base, nous avons procédé par codage disjonctif pour générer une base d'apprentissage numérique. CrTM commence avec un taux de 66.6 % sans contraintes. Il atteint une performance totale de 96,70% ("Overall Accuracy") et 91.40% sur le "Held-Out" avec 500 contraintes aléatoires (les résultats du "Held-Out" sont : 49% par COP-COBWEB, 56% par COP-Kmeans et 82% par COP-b-coloring avec le même nombre de contraintes). CrTM a donc permis une amélioration de 24% (Figure 1.b) En l'absence de contraintes, l'algorithme de Kohonen atteint une performance de 78% sur la troisième base (Figure 1.a). Après l'intégration de 240 contraintes aléatoires, notre algorithme améliore de 13% la performance totale, avec un "Overall Accuraccy" de 91%. Le "HeldOut" quant à lui atteint les 85% avec seulement 150 contraintes intégrées, soit 7% d'améliora-tion avec seulement 5% d'information a priori. L'algorithme COP-b-coloring atteint une performance de 50% sans contraintes puis de 89% avec 500 contraintes et le "Held-Out" avoisine les 66%.
Application aux données de mélanomes chez l'humain
Nous avons testé l'algorithme CrTM sur une base médicale contenant 226 images de grain de beauté. Chaque image est caractérisée selon la règle ABCD (Stolz, 1994). l'Asymétrie de formes, de textures et de couleurs, les Bords abrupts des structures pigmentées, la diversité des Couleurs, et différentes structures Dermatoscopiques. Les contraintes sont représentées par l'information a priori apportée par les diagnostics des dermatologues sous forme d'étiquettes associées aux images. Nous avons constaté que CrTM atteint une performance de 78,2% sans contraintes. "Overall Accuracy" augmente jusqu'à 84%, après l'intégration de 210 contraintes, soit un total de 3200 paires de décisions (y compris celles obtenues par transitivité). Ces 17% d'information a priori nous ont amélioré la performance de la classification, calculée par "Held Out", pour atteindre 81,2% (voir Figure 2).
FIG. 2 -Evaluation de la performance de CrTM sur la base des mélanomes
Conclusion
Nous avons proposé dans cet article une extension de l'algorithme des cartes topologiques pour intégrer les contraintes liées aux données. Ces contraintes sont binaires (entre chaque

Introduction
En fouille de texte comme en recherche d'information (RI), plusieurs modèles sont utilisés pour représenter un document. Ces modèles, de type probabiliste, booléen ou vectoriel, se sont révélés bien adaptés pour représenter des documents textuels. Cependant, ils présentent l'inconvénient de ne pas tenir compte de la structure du document. Or, la plupart des informations disponibles aujourd'hui sur Internet ou dans des bases documentaires sont fortement structurées. C'est la raison pour laquelle des travaux récents, en RI comme en fouille de données se sont intéressés à la structure des documents. Ceci a notamment conduit à l'émergence de la recherche d'information XML orientée contenu dont l'objectif est justement d'exploiter l'information structurelle contenue dans les documents pour concevoir des systèmes de RI plus efficaces. La compétition INEX 2 (INitiative for Evaluation of XML Retrieval) produit d'ailleurs depuis 2002 de larges collections de documents utilisables pour l'évaluation de tels systèmes. L'exploitation de la structure a aussi été étudiée dans des tâches de classement, supervisé ou non, de documents . Dans ce contexte, plusieurs voies ont été envisagées, parmi lesquelles on citera l'extension des modèles usuels de représentation de documents textuels [Doucet et Ahonen-Myka (2002)] ou l'exploitation de la structure arborescente des documents XML [Yi et Sundaresan (2000); Marteau et al. (2005); Vercoustre et al. (2006)]. Enfin, dans le contexte de la détection d'information nouvelle (Novelty Detection), d'autres travaux ont pris en compte la structure logique des documents en estimant le poids à accorder chacune des parties qui le composent [Jacquenet et Largeron (2006)].
Dans cet article, nous proposons d'étendre le modèle probabiliste de façon à tenir compte du rôle joué par les éléments de structure et de mise en forme pour mettre en évidence des informations importantes. Notre approche nécessite une phase d'apprentissage sur une partie de la collection considérée. Au cours de cet apprentissage, un poids est calculé pour chacune des balises, basé sur la probabilité pour que cette balise distingue les termes pertinents. Dans une seconde phase, le modèle que nous avons développé permet d'estimer, en tenant compte de ce poids, la probabilité qu'un document de la collection soit pertinent pour une requête donnée. Ce modèle est décrit dans la prochaine section tandis que les résultats d'expérimentations obtenus sur la collection INEX 2006 sont présentés dans la troisième section.
2 Un modèle probabiliste de représentation de documents structurés
Principe d'intégration de la structure dans un modèle probabiliste de documents
En Recherche d'Information, le modèle probabiliste de documents [Robertson et Jones (1976)] aspire à estimer la pertinence d'un document pour une requête à partir de deux probabilités : celle de trouver une information pertinente et celle de trouver une information non pertinente. Ces estimations sont basées sur la probabilité de chacun des termes contenus dans le document d'apparaître dans un document pertinent ou dans un document non pertinent de la collection. Pour ce faire, on utilise une collection de test, composée de documents, de requêtes et de la connaissance des documents pertinents pour chaque requête. Cette collection permet, dans une phase d'apprentissage, d'estimer la probabilité de pertinence de chaque terme en fonction de ses distributions respectivement dans les documents pertinents et les documents non pertinents.
Notre objectif est d'intégrer la structure des documents dans ce modèle afin de parvenir à une recherche d'information structurée. Dans notre modèle, seront considérés des éléments de structure logiques (titre, section, paragraphe, etc.) et de mise en forme (souligné, en gras, centré, etc.). L'intégration de la structure dans le modèle probabiliste s'effectue ensuite à deux niveaux. Dans le premier, la structure logique est utilisée pour identifier les éléments XML qui seront susceptibles d'être indexés par notre système : les sections, paragraphes, tableaux, etc. Dans le second, les balises de structure logique et de mise en forme sont intégrées au modèle probabiliste classique. Cette intégration nécessite une étape préliminaire qui consiste à estimer un poids pour chacune des balises. Ce poids est basé sur la probabilité pour qu'une balise distingue les termes pertinents. Dans la seconde étape d'intégration des balises, le modèle que nous avons développé permet de déterminer la probabilité qu'un document de la collection soit pertinent pour une requête donnée en tenant compte non seulement de la pondération classique des termes du modèle probabiliste, mais aussi de la pondération de chacune des balises qui englobent ces termes. La section suivante présente plus formellement ce modèle probabiliste de représentation de documents structurés.
Notations
On dispose d'un ensemble D de documents structurés. En pratique, il s'agira le plus souvent de documents XML. Chaque élément logique (i.e. section, paragraphe,...) e j d'un document XML représente donc un ensemble de termes, délimité par une balise structurelle logique, qui sera utilisée pour indexer l'élément.
On note : -E = {e j , j = 1, ..., l}, l'ensemble des éléments structurés considérés dans la collection, par exemple des sections, des paragraphes, etc. -T = (t 1 , ..., t i , ...t n ), un index de termes construit sur E. -B = {b 1 , ..., b k , ..., b m }, l'ensemble des balises logiques et de mise en forme considé-rées. Soit E j , un vecteur de variables aléatoires T ij à valeur dans {0, 1} :
T ik = 1 si le terme t i apparaît étiqueté par la balise b k T ik = 0 sinon T i0 = 1 si le terme t i apparaît sans être étiqueté par une des balises de mise en évidence de B T i0 = 0 si le terme t i n'apparaît pas sans étiquette On notera e j = (t 10 , ..., t 1k , ..., t 1m , t i0 , ..., t ik , .., t im , t n0 , ..., t nk , .., t nm ) une réalisation de la variable aléatoire E j . À partir de cette représentation, l'objectif est maintenant d'étendre le modèle probabiliste pour prendre en compte la structure de mise en forme des documents.
Probabilité de pertinence d'un élément XML, basée sur les balises
La fonction de pondération BM25, introduite par [Robertson et Jones (1976)] auquel nous renvoyons le lecteur pour plus de précision, est très largement utilisée dans les systèmes de recherche d'information probabilistes pour estimer le poids d'un terme t i dans un élément XML e j . Dans notre modèle, cette première pondération, notée w ij , est enrichie de manière à prendre en compte la structure logique et de mise en forme des documents. Dans un contexte de recherche d'information, on désire en effet estimer la pertinence d'un élément XML e j relativement à une requête. Ce qui revient à estimer P (R|e j ) (respectivement P (N R|e j )), la probabilité de trouver une information pertinente (respectivement non pertinente) quand on observe l'élément e j pour une requête donnée.
On introduit une fonction de classement f c 1 (e j ) qui permettra, en comparant ces deux probabilités, d'ordonner les documents en fonction de leur pertinence par rapport à la requête :
Plus f c1(e j ) est élevée, plus pertinentes sont les informations contenues dans e j . Par la formule de Bayes et en éliminant le terme P (R) P (N R) constant sur la collection pour une requête donnée qui n'interviendra donc pas dans le classement des documents, on obtient f c 2 , proportionnelle à f c 1 : f c 2 (e j ) = P (e j |R) P (e j |N R)
Moyennant l'hypothèse d'indépendance des termes (Binary Independance Model) :
Pour simplifier les notations, on pose :
probabilité de ne pas avoir t i sachant que l'élément est pertinent
probabilité de ne pas avoir t i sachant que l'élément est non pertinent
En reportant dans la fonction de classement f c 2 (e j ) :
La fonction logarithmique étant croissante, en prenant le logarithme de f c 2 (e j ) le classement produit par la fonction f c 3 (e j ) = log(f c 2 ) sera le même que celui produit par f c 2 :
Le terme t ik ?ej log(
) est une constante relativement à la collection (i.e. indépen-dant de t ik ), ne pas le considérer ne change pas le classement produit par la fonction. On en déduit la fonction de pertinence, tirée de f c 3 :
Ainsi, dans ce modèle probabiliste tenant compte de la structure du document, la pertinence d'un élément e j par rapport aux balises est mesurée par le score f c balises (e j ) :
En pratique, pour mesurer cette pertinence, il convient d'estimer les probabilités p ik et q ik , i ? {1, .., n} et k ? {0, .., m} à partir d'un échantillon d'apprentissage EA constitué d'élé-ments déjà jugés pour une requête. À partir des ensembles R et NR contenant respectivement les éléments pertinents et non pertinents, on obtient :
-r ik : nombre de termes t i étiquetés par b k parmi les éléments pertinents de EA ; -n ik : nombre de termes t i étiquetés par b k parmi les éléments de EA ; -r ? ik = n ik ? r ik : nombre de termes t i étiquetés par b k parmi les éléments non pertinents de EA ; -R = ik r ik : somme des occurrences des termes figurant parmi les éléments pertinents de EA ; -N ? R = ik r ? ik : somme des occurrences des termes figurant parmi les éléments non pertinents de EA.
Ayant construit des estimateurs sans biais de pik et de q ik , les probabilités d'avoir le terme t i étiqueté par b k sachant que l'élément est respectivement pertinent et non pertinent, on en déduit p.k, la probabilité d'avoir la balise b k sachant que l'élément est pertinent et q.k, la probabilité d'avoir la balise b k sachant que l'élément n'est pas pertinent :
Combinaison des pertinences basées sur les termes et sur les balises
Pour obtenir un score global de classement f c(e j ) d'un élément e j , en fonction des termes et des balises, qui permette d'estimer sa pertinence par rapport à une requête, nous avons proposé une une première approche qui consiste à multiplier le poids w ij de chaque terme de e j par la moyenne des poids w ? ik correspondant à toutes les balises qui englobent le terme. Ainsi, nous calculons f c(e j ) = ti?ej w ij * k/t ik =1 w ? ik 3 Expérimentation sur la collection INEX
Présentation de la collection
Nous avons évalué notre modèle sur la collection d'INEX 2006 (Initiative for Evaluation of XML Retrieval) composée de 659.388 articles en anglais issus de l'encyclopédie Wikipedia. Le modèle vectoriel basé sur la fonction de pondération BM25, a été utilisé comme modèle de référence et comparé au modèle probabiliste structurel décrit précédemment, qui utilise lui aussi la fonction de pondération BM25, mais en intégrant les balises. Les résultats ont été évalués en utilisant les taux de précision et de rappel ainsi que la mesure de performance globale interpolated mean average precision (iMAP) permettant de les combiner et définie dans [Pehcevski et al. (2007)] Sur les 114 requêtes de la collection, l'indice iMAP est égal à 2,34% dans le cas du modèle de référence sans utilisation des balises. Il est égal à 1,80% quand toutes les balises sont considérées. Cette tendance est confirmée lorsqu'on considère le rappel et la précision indé-pendamment l'un de l'autre [Géry et al. (2007)]. Ces résultats préliminaires peu convaincants, ne remettent pas nécessairement en cause l'intérêt d'exploiter l'information structurelle en plus de l'information textuelle mais plutôt les modalités de combinaison des poids des termes avec ceux des éléments structurels.
Conclusion
Dans cet article, nous avons proposé d'étendre le modèle probabiliste de représentation des documents structurés de façon à tenir compte du poids des balises représentant la structure logique et la structure de mise en forme. Ces poids sont estimés par apprentissage puis inté-grés dans le calcul de la probabilité qu'un document de la collection soit pertinent pour une requête donnée. Bien que les résultats préliminaires obtenus sur la collection test de la campagne d'évaluation INEX 2006 soient peu convaincants, nous pensons qu'ils ne remettent pas en cause l'intérêt d'exploiter l'information structurelle en plus de l'information textuelle, mais que la combinaison des poids des termes avec ceux des balises doit être étudiée de manière plus approfondie.

Introduction
L'ensemble des propositions présentées dans cet article est élaboré dans le cadre du projet ?R 1 , un des projets clefs d'une action de recherche appliquée initiée depuis trois ans 2 .
Du point de vue approche adoptée, elle est résolument descendante (top-down). Par l'étude et la compréhension des usages cibles, elle débouche sur la formalisation du fonctionnement de l'information nécessaire à cet usage. Cette information étant contenue dans un corpus documentaire source. La formalisation obtenue permet alors de concevoir un modèle de l'information attendue, puis d'opérationaliser des outils de repérage et de représentations symboliques afin d'alimenter de manière automatique ce modèle.
Comme annoncé dans Casenave et al. (2004) et formalisé dans Nodenot et al. (2006) et Loustau et al. (2008), de plus en plus d'activités pédagogiques sont construites autour des documents patrimoniaux. C'est notamment le cas en géographie : localisation des principaux lieux constitutifs d'un itinéraire de voyage, positionnement sur une carte des lieux visités, lecture d'un itinéraire à différentes échelles, etc. L'enjeu ici est de se servir des documents analysés automatiquement pour ne concevoir que les activités que l'on va pouvoir encadrer de manière automatisée (ou assistée) par un tuteur informatique. Des expérimentations avec des enseignants et des apprenants sont menées depuis 2 ans pour définir les contextes d'utilisation opérationnels qui ont du sens pour de vrais enseignants.
Concernant le traitement automatique, la réponse à l'usage cible retenu nécessite une phase de marquage de l'information mettant en oeuvre des méthodes s'apparentant à de l'extraction d'information (IE) et une phase de Recherche d'Information Géographique (GIR), concept proposé en 2004 et précisé en 2005 par Jones et Purves (2006) les co-fondateurs du premier workshop de ce nom. Selon le niveau de la représentation symbolique souhaité par la suite, l'interprétation à réaliser dans la phase de GIR peut s'appuyer sur une granularité plus ou moins locale de l'information géographique. Compte tenu de l'approche adoptée, cette granularité variera du niveau syntagmatique 3 (souvent considéré comme atomique pour l'information géographique) à des agglomérats de ce premier niveau, de plus en plus importants (portés par une proposition, une phrase, un paragraphe ou plus) et ceci selon les besoins exprimés par l'usage.
Dès que l'on veut obtenir une représentation symbolique d'une information, les traitements en GIR doivent se baser sur des méthodes de calcul du sens. Dans ce cas il faut alors se questionner sur le type de représentation sémantique le plus adapté pour capturer automatiquement le sens de telle ou telle granularité textuelle. Comme le précise Blackburn et Bos (2003), il n'y a actuellement pas de réponse unique à ce type de questionnement : cela dépend essentiellement du niveau de finesse attendu et du type de « phénomène »linguistique auquel on s'intéresse.
1 ?R : Prototype pour l'Interprétation d'Itinéraires dans des Récits de voyage 2 Cette action de recherche est portée par diverses formes de collaborations réalisées entre chercheurs du LIUPPA (équipe-projet DeSI), chercheurs de laboratoires tels que le LRI, le COGIT (IGN), L'IRIT (grâce au projet ANR-GEONTO) et différents partenaires d'organisations privées ou publiques : les entreprises DIS et Géocime, la média-thèque de Pau (MIDR), la communauté d'agglo. (CDAPP), le conseil général (CG64). Cette action a comme objectif de créer un ensemble de ressources et de traitements afin de raisonner, dans le cadre d'usages ciblés, sur de l'information textuelle à contenu géographique, après l'avoir recherchée, marquée et annotée.
3 c'est-à-dire composé de syntagme(s). Le syntagme est un groupe de termes dont la succession a un sens qui forme une unité fonctionnelle.
État de l'art
Différentes actions de recherche ont été menées autour de la problématique de l'itinéraire, elles ont donné naissance à des plateformes comme nous allons le constater au travers des projets comme Egges et al. (2001), Coyne et Sproat (2001) ou encore Maaßet al. (1993). Certains de ces projets s'intéressent comme nous au lien entre expression textuelle en langue naturelle et concepts géographiques liés aux itinéraires.
De nombreux travaux ont d'autre part été effectués autour de la problématique de modé-lisation du concept d'itinéraire du point de vue cognitif. Le modèle des attendus que nous proposerons, bien qu'émanant des attentes pour un usage pédagogique, s'inspire de ces diffé-rents travaux.
Enfin, dans le domaine du TALN et chez les linguistes, divers travaux vont nous permettre de proposer un modèle d'extraction (c'est-à-dire de repérage pour une interprétation) afin de rendre possible un traitement automatique. Les uns concentrent leurs efforts sur la caractérisa-tion d'expressions dont la granularité est inférieure à la proposition. Les autres s'intéressent à la manière dont l'itinéraire est exprimé dans la langue (orale ou écrite) et enfin d'autres encore focalisent leurs recherches sur les verbes de déplacements. Ces verbes occupent en effet une place prépondérante dans une description d'itinéraire.
-179 -RNTI-E-13
Des projets autour de l'itinéraire
Près de nos préoccupations, à savoir établir un lien entre le langage écrit et un concept géographique, de nombreux auteurs se sont intéressés à des systèmes permettant de passer automatiquement d'un texte à une scène (text-to-scene conversion). Coyne et Sproat (2001) avec WordsEye en est un exemple : ce système permet de visualiser en trois dimensions des descriptions du type : The cat is on the large chair. A dog is facing the chair. WordsEye utilise une base lexicale de plus de 10000 objets (et leur représentation associée), l'ontologie Wordnet, un analyseur de dépendances et des grammaires de cas pour interpréter les textes soumis. Cependant, WordsEye ne semble pas prendre en compte des textes relatant des histoires réelles et les exemples donnés ne sont que des cas d'école relativement courts dans lesquels le système cherche à positionner correctement les objets les uns par rapport aux autres de manière statique.
Carsim (Egges et al. (2001)) est un projet qui cherche à offrir une visualisation en 3D de descriptions d'accidents de la route à partir de constats textuels. L'approche est quelque peu semblable à celle que nous proposons : les auteurs cherchent à décrire formellement l'accident à partir d'une analyse linguistique puis, dans un deuxième temps à générer la scène virtuelle associée. Maaßet al. (1993) avec le projet VITRA (VIsual TRAnslator) s'intéresse à la connexion entre langage et perception visuelle. L'élaboration d'un système de représentation des connaissances qui permette un accès en langage naturel aux données visuelles est un objectif de ce projet. VITRA vise plus particulièrement la génération automatique de descriptions d'itinéraires. Nous sommes là en présence de travaux qui adoptent une démarche exactement inverse à la nôtre : à partir d'un modèle de données de l'itinéraire, on cherche à produire une description d'itinéraire. Dans ce projet, l'accent est mis sur l'apport de la bi-modalité dans la description de l'itinéraire (description verbale et description graphique).
Les spécificités de notre approche par rapport à ces projets sont triples. Tout d'abord dans notre projet nous travaillons sur des documents d'un contenu bien plus volumineux que celui des constats d'accidents de la route ou des descriptions du type the cat is on a large chair. Cela a pour conséquence non seulement un nombre de données spatio-temporelles à manipuler plus important mais également des échelles variables dans les références spatiales. De plus, les formes d'expression de l'espace et du temps sont différentes : si dans les constats d'accidents les objets, leur trajectoire, les chocs sont importants, dans les documents patrimoniaux à composante géographique, les lieux, les époques, et les déplacements le sont plus. D'autre part, nous souhaitons intégrer dans notre approche les usages qui seront faits des informations extraites : dans le cas d'étude que nous proposons, celui de l'extraction des itinéraires, nous souhaitons aller plus loin que la visualisation des informations extraites. L'objectif est en effet à terme de fournir des textes interprétés à des applications à vocation pédagogique.
Dans les années quarante, des théories concernant l'espace et son modèle cognitif font leur apparition. Les travaux de Tolman (1948) sur la notion de carte cognitive chez l'animal et les recherches de Piaget qui s'est intéressé en 1948 au développement de l'apprentissage de l'espace chez les enfants en sont les principaux exemples. B. Kuipers est également à l'origine de travaux largement cités dans ce domaine. Kuipers (1977) propose le modèle TOUR dans lequel il décrit les fonctions de la carte cognitive qui sont d'assimiler les informations concernant l'environnement, de représenter la position actuelle, et de répondre à des questions de localisation et de recherche d'itinéraires. Le modèle TOUR est un modèle computationnel psychologique du sens commun spatial pour la description d'itinéraire. Wunderlich et Reinelt (1982) s'intéressent, dans un dialogue, au processus de la description d'un chemin répondant à la question How to get there from here ? Ils décomposent ce processus en quatre phases : initiation (question initale, confirmation, reconfirmation), description de la route (par l'informateur), confirmation (confirmation et répétition éventuelle) et fermeture (remerciements, etc.). La phase 2 nous intéresse ici plus particulièrement puisqu'il s'agit de décrire un itinéraire. Dans ces travaux, les auteurs décomposent l'itinéraire ainsi : un point de départ, des destinations intermédiaires et un point d'arrivée. Ces destinations intermédiaires sont des landmarks, un landmark a des caractéristiques particulières qui le rendent facilement reconnaissable. Les auteurs introduisent également les extended landmarks. Au contraire des landmarks classiques qui s'apparentent à des points (un monument, un carrefour, etc.), les extended landmarks sont des portions de route que l'on doit suivre (une route, un tunnel, le bord d'un cours d'eau, etc.). Denis (1994) puis Przytula-Machrouh et al. (2004) s'intéressent aux connaissances utilisées par les personnes décrivant un itinéraire, aux modes de représentation de ces connaissances et à leur utilisation. Ces travaux sont à cheval entre le concept et son évocation de manière verbale. Concernant le concept d'itinéraire, les auteurs se placent dans un contexte de description a priori d'un itinéraire en ville, dans le but d'apporter une information, de la même manière que Wunderlich et Reinelt (1982). Dans ces travaux, le concept de scène élémentaire est utilisé comme unité de base de la description d'itinéraire. Une scène élémentaire est fondamentalement constituée de repères et d'actions. Les auteurs se sont également intéressés au moyen de verbaliser ces descriptions d'itinéraire et actent que les repères sont représentés par des noms ou des groupes nominaux alors que les actions sont verbalisées grâce à des verbes de déplacement. Fraczak et Lapalme (1999) se placent également dans le contexte de la description d'itinéraire a priori, ils décrivent la structure conceptuelle de l'itinéraire comme une succession d'entités spatio-temporelles appelées segments et relais. Le segment est un fragment de l'itiné-raire durant lequel une ou plusieurs caractéristique(s) reste(nt) constante(s) tandis qu'un relais marque un changement de caractéristique(s). Ces caractéristiques pouvant être une orientation, une direction, un type de chemin, etc.
Si la modélisation de l'itinéraire a donné lieu à de nombreux travaux, ces travaux se placent en amont de la réalisation de l'itinéraire. Il s'agit d'une description a priori qui a un objectif bien particulier : conduire celui qui va réaliser l'itinéraire d'un point de départ à un point d'arrivée. Le référentiel de la description est donc celui qui effectue l'itinéraire. Dans nos travaux, l'itinéraire considéré est un itinéraire a posteriori. Le narrateur le décrit une fois qu'il a été réalisé non pas pour qu'un autre le réalise à son tour mais dans un but narratif, pour que la majorité des personnes puisse comprendre son voyage. Cela implique une description parfois moins fine de repères visuels d'orientation et l'utilisation d'un référentiel intrinsèque. Autrement dit un référentiel peu dépendant de l'objet de référence et faisant appel à des relations ternaires impliquant un observateur, l'objet à situer et l'objet de référence. Ce référentiel est le seul qui puisse être à la fois partagé par celui qui a fait l'itinéraire et celui qui le lit. En effet, le lecteur n'est pas physiquement sur les lieux racontés, et, sauf exception, il n'a pas non plus en tête la configuration exacte des lieux traversés et racontés dans le récit.
D'autre part, nous pensons qu'un modèle conceptuel ne peut s'élaborer de manière purement objective : il adopte forcément un point de vue. Nous verrons (cf section 3.1) que notre point de vue est celui du pédagogue qui souhaite faire intervenir ces descriptions d'itinéraires dans ses activités pédagogiques.
Enfin, si l'itinéraire global est certes perçu avant les éléments qui le composent, il n'en est pas moins indissociable. En effet, dans un texte, il n'y a pas d'itinéraire sans l'évocation des emplacements successifs et des déplacements du narrateur. C'est le sujet de la section suivante.
L'itinéraire : son évocation dans la langue par le déplacement
Dans le développement de la méthode que nous proposons pour interpréter les itinéraires, nous nous appuyons sur les travaux de linguistes et du TALN. Ces travaux montrent que le déplacement est primordial dans la compréhension de l'itinéraire, ceci sera confirmé par des observations faites sur un échantillon de notre corpus. Nous donnons ici un aperçu des principaux travaux linguistiques qui ont été menés autour de l'expression du déplacement dans la langue. Boons (1987) constate que les critères pour déterminer les compléments locatifs de la phrase sont insuffisants. Le plus souvent, la question où ? que l'on nous apprend à nous poser à l'école primaire pour trouver le complément locatif ne suffit pas. Boons propose alors de classer les verbes de mouvement selon la phase spatio-temporelle sur laquelle ils sont focalisés. Les verbes de déplacement peuvent donc avoir une polarité aspectuelle initiale (comme pour le verbe sortir), médiane (comme pour le verbe passer) et finale (comme pour le verbe arriver). Laur (1991)  Plus tard, Sablayrolles (1995) reprend ces travaux et introduit le changement d'emplacement. Le troisième critère de Laur montre une large sous-spécification du déplacement. En effet, s'il n'y a effectivement pas de changement de LRV dans la phrase Paul s'approche du mur, il y a tout de même un changement d'état. Paul est effectivement plus près de la cible mur au temps t+1 qu'au temps t. Les lieux sont donc insuffisants pour décrire le déplacement dans la langue. C'est pour palier à ce problème que Sablayrolles introduit les emplacements, en plus des lieux. A la différence d'un lieu, un emplacement est une portion de surface, sans aucune fonctionalité ni élément lexical associé. Il est uniquement défini géométriquement par l'enveloppe pragmatique associée à l'entité concernée. De notre point de vue, l'approche de Sablayrolles nous semble être relativement semblable à celle de Laur, si ce n'est qu'il introduit un grain plus fin que le lieu : l'emplacement. Sarda (2000) s'est penchée sur le cas particulier des verbes de déplacement transitifs directs et en propose une typologie. Elle cherche à raffiner la catégorie des verbes médians qui selon elle est plutôt définie par défaut, c'est-à-dire rassemblant les verbes qui ne sont ni initiaux, ni finaux. Sarda propose de catégoriser les verbes en fonction de la nature des relations de localisation qu'ils impliquent. Elle distingue en premier lieu les verbes relationnels (qui dénotent un déplacement quel que soit l'objet) des verbes référentiels (qui dénotent un déplacement seulement lorsque l'objet est un lieu). Les verbes relationnels sont catégorisés selon des relations de distance (approcher, fuir, etc.), d'orientation (monter, descendre, etc.) ou de passage (traverser, sauter, etc.) et les verbes référentiels sont soit neutres initiaux (quitter, déserter, etc.) soit neutres finaux (atteindre, regagner, etc.) soit de contact (heurter, taper, etc.). Une classe de verbes moins clairement définie subsiste : celle des verbes médians (arpenter, sillonner, parcourir, etc.) qui sont référentiels par rapport à un domaine topologique (un intérieur par exemple).
La représentation de la sémantique des verbes de déplacement a donné lieu a de très nombreux travaux, notamment chez les linguistes et dans la communauté TALN. Elle pose encore de nombreux problèmes et est sans cesse remise en question comme dans la thèse de Mathet (2000). Cependant, le type de documents sur lesquels nous travaillons (des récits de voyage) ainsi que les objectifs que nous souhaitons atteindre quant à la finesse de l'interprétation du déplacement dans la langue nous permettent de simplifier le problème. Le sous-ensemble des verbes de déplacement auxquels nous nous intéressons ici sont ceux qui entrent dans une construction verbe,préposition (facultative),entité_spatiale, que nous noterons triplet (V,P ?,E). Cette construction lève une grande partie des problèmes d'ambiguïté que l'on peut trouver dans des propositions comme quitter son mari, traverser une mauvaise période, etc.. De plus, nous souhaitons simplement inférer qu'à un moment donné, le sujet est localisé sur l'objet entité spatiale. Les préoccupations plus fines sont pour le moment écartées de nos attendus. En effet, dans un but de traitement automatique de corpus volumineux à des fins d'interprétation pour le domaine pédagogique, une première approximation du déplacement par la polarité aspectuelle des verbes de déplacement nous semble suffisante. Pour prendre un exemple, dans la compré-hension globale de l'itinéraire décrit dans un récit, que le narrateur s'éloigne de Pau ou quitte Pau est relativement semblable.
Modélisation du concept d'itinéraire et son expression dans des récits de voyage : deux modèles computationnels
Les enseignants proposent de nombreuses activités pédagogiques à partir de documents à contenu géographique. Les origines de ces documents peuvent être multiples : des extraits de publications universitaires, des sélections obtenues dans des publications plus grand public (par exemple des articles de quotidiens, d'hebdomadaires, de guides touristiques, . . .) ou encore des passages choisis dans des oeuvres littéraires. Pour l'ensemble de ces « documents sources » se posent des problèmes pédagogiques importants : l'adéquation du matériau aux objectifs d'apprentissage visés. En effet, souvent l'exploitation directe du contenu de ce type de document reste une tâche difficile pour les élèves (du secondaire en particulier). Il s'ensuit que le péda-gogue, auteur de manuel ou professeur, adapte le document afin de le rendre plus accessible aux élèves. Il s'agit très fréquemment d'une adaptation dans le sens de la simplification, d'où la mention fréquente dans les manuels : « d'après tel auteur ».
Pour les travaux présentés ici, nous avons retenu comme activité cible : la mobilisation chez l'élève des repères spatio-chronologiques. Elle est une composante incontournable de plusieurs objectifs pédagogiques. Étant données les diverses formes qu'une telle activité peut revêtir et afin de modéliser un processus de complexité maîtrisable, nous nous sommes restreints à l'explicitation de trois tâches :
1. localisation des principaux lieux constitutifs d'un itinéraire, 2. interprétation des déplacements plausibles entre les différents lieux empruntés ou traversés au sein l'itinéraire, 3. cartographie du parcours interprété. Ces tâches devront être appliquées à une sous catégorie de « documents sources » des documents de type oeuvre littéraire et plus spécifiquement du genre littéraire récit de voyage.
Dans la démarche du pédagogue de mise en adéquation de la source documentaire aux finalités pédagogiques, il lui est nécessaire de passer par trois grandes étapes :
1. une étape de recherche d'information et d'extraction de passages qui peut lui retourner plusieurs documents, 2. une étape de comparaison du matériau retourné en étape 1 qui lui fait choisir un document, 3. enfin une étape de reconstruction du « document source pédagogique », à partir du document choisi en étape 2. Le corpus de « documents sources » sur lequel nous travaillons est constitué de monographies datant d'une même époque (fin du XIXème siècle) et évoquant des récits de voyage réalisés sur un même territoire : les Pyrénées.
Remarquons que les textes du corpus sont spécifiques par rapport aux nombreux travaux du domaine de la description d'itinéraire car leur but n'est pas de décrire uniquement un itinéraire. L'objectif premier des auteurs de ces documents est de relater l'expérience personnelle de l'auteur. Néanmoins, on peut observer que le genre descriptif est d'avantage utilisé que le genre narratif et que la description de l'itinéraire y occupe une place prépondérante. On peut constater, d'autre part, qu'il s'agit dans notre cas d'une description a posteriori alors que la plupart des travaux existants qui traitent de la description d'itinéraire se placent a priori. Le récit de voyage ne contient donc pas des énoncés tels que ceux qui apparaissent dans les descriptions a priori comme continuer tout droit, tourner à droite, etc.. On peut remarquer également une certaine régularité dans l'évocation du territoire et des déplacements au sein de l'itinéraire, comme évoqué par Boons (1987), Laur (1991), Mathet (2000). Il est le plus souvent évoqué par des verbes de déplacement (j'ai quitté Bordeaux à 8h00, je suis arrivé à Pau à 11h, j'ai gravi le Pic du Midi d'Ossau le lendemain, etc.), et ce quel que soit l'auteur. Enfin, le récit de voyage obéit à des règles particulières concernant la chronologie des évènements relatés : il y a dans ce genre de document une synchronisation quasi parfaite entre la chronologie du texte et la chronologie du voyage.
Compte tenu de l'état actuel de la formalisation des connaissances dans le domaine de la compréhension automatique de texte concernant les concepts spatiaux, temporels et spatiotemporels, il n'est pas encore possible de concevoir un système permettant d'assurer de manière autonome les trois grandes étapes de préparation d'un tel matériau pédagogique. Par contre, comme la suite de ce papier tente de le montrer, il est possible de concevoir un outil accompagnant le pédagogue dans ces tâches.
Dans un premier temps, nous présenterons une formalisation de l'information attendue, puis après avoir discuté les résultats d'une étude réalisée sur un échantillon du corpus, nous détaillerons le modèle d'extraction. Sera enfin relatée la phase d'interprétation, autrement dit la transformation de l'information exprimée dans le modèle d'extraction pour sa ré-expression dans le modèle des attendus.
Le modèle des attendus
Dans le cas particulier de l'exemple de récit de voyage donné en figure 1, les éléments constituant une description d'itinéraire sont :
1. des lieux et des « temps calendaires » 4 , 2. des déplacements, 3. des faits (activités ou situations) qui modifient la dynamique par défaut du déplacement.
Les lieux sont mentionnés lorsque l'auteur les a simplement traversés ou qu'il y situe des faits marquants. Tous les lieux traversés lors d'un itinéraire ne sont pas relatés ce qui nous rappelle les propriétés de saillance de Przytula-Machrouh et al. (2004); Denis (1994) ;Wunderlich et Reinelt (1982).
D'autre part, seuls les lieux associés à une description de faits (arrêt pour visiter, pour manger, pour changer de moyen de transport, etc.) peuvent éventuellement donner naissance à un itinéraire d'échelle inférieure. Nous les apparentons à la notion d'étape intermédiaire de Fraczak et Lapalme (1999). Nous proposons également de reprendre les notions de relais et de segments (figure 2) telles que décrites par Fraczak et Lapalme (1999) Pour donner une représentation semi-formelle du modèle obtenu, nous avons utilisé le langage MADS de Parent et al. (2006). MADS est un modèle de données qui permet d'ajouter des 5 j u i l l e t . Dimanche . J e me s u i s b a i g n é . J e s u i s p a r t i à 11 h1 / 4 a v e c un g r o u p e de g e n s p o u r R o c h e f o r t d a n s une p i n a s s e . Vent c o n t r a i r e . En l o u v o y a n t , en r a m a n t e t a v e c l ' a i d e de l a marée , n o u s a r r i v â m e s à R o c h e f o r t v e r s t r o i s h e u r e s ; c e p e n d a n t l a j o u r n é e f u t b e l l e e t l a c o m p a g n i e p l u t ô t a g r é a b l e . J ' a i vu t o u t e s l e s p a r t i e s du p h a r e ; l a b a s e f u t c o n s t r u i t e au t e m p s de L o u i s XIV , l a p a r t i e s u p é r i e u r e en 1 7 8 9 . Le d i s p o s i t i f l e n t i c u l a i r e a c t u e l s e r t d e p u i s d o u z e a n s . Le g a r d i e n s e p l a i n t s e u l e m e n t de l a lampe q u i l u i c a u s e d e s e n n u i s . Nous a v o n s q u i t t é R o c h e f o r t v e r s 5 h e u r e s . P r e s q u e c a l m e ; n o u s a v o n s mis deux h e u r e s p o u r a r r i v e r à Royan . J ' y a i p a s s é l a n u i t .
6 j u i l l e t . J e s u i s p a r t i p a r l e b a t e a u à v a p e u r à 6 h e u r e s du m a t i n e t j ' a i a t t e i n t B o r d e a u x à m i d i a p r è s une b r è v e t r a v e r s é e . Après?m i d i p l u v i e u s e . J ' a i d î n é a v e c M. G u e s t i e r l . J ' a i é c r i t à E l i s a .
7 j u i l l e t . J ' a i q u i t t é B o r d e a u x à 7 h e u r e s en d i l i g e n c e p o u r Pau . J ' a i é t é a g r é a b l e m e n t s u r p r i s p a r l a b e a u t é de l a campagne [ . . . ] Nous a v o n s t r a v e r s é Langon .
FIG. 1 -Extrait d'un récit de voyage.
types de données propres aux données spatio-temporelles mais aussi d'ajouter des relations spécifiques entre ces données. MADS se veut également un modèle de données résolument tourné vers une modélisation des concepts lors de la phase de modélisation. Les concepteurs doivent en effet s'abstraire totalement des contraintes d'implémentation pour être le plus près possible du monde réel et de ses représentations. Cela n'était pas le cas dans la gestion des données géographiques où les concepteurs ont longtemps été influencés par des préoccupa-tions d'implémentation internes aux outils qui gèrent ces données. Avant de décrire un à un les objets qui composent le modèle que nous proposons, il est bon de préciser que les quatre principaux objets sont tous des Entités Géographiques (EG). De manière abstraite une EG est définie par trois composantes : une spatiale, une temporelle et une thématique ou phénomène. Cette définition explicitée dans Usery (2003)  Le segment (ou fragment d'itinéraire) : il est le chemin qui relie deux relais consé-cutifs. Il est du type spatial MADS OrientedLine ( ) et du type temporel MADS Interval ( ). Il est important de noter que ce chemin n'est qu'un chemin virtuel, c'est-à-dire une des représentations possibles de l'espace. Dans la carte cognitive -définie par Kuipers (1977) que se construit le lecteur, le chemin qui relie deux relais n'est pas forcément celui qui a été véritablement emprunté par l'acteur de l'itinéraire. Ce chemin virtuel approche cependant le chemin véritable de manière plus ou moins juste selon les indications dont dispose le lecteur. Ces indications peuvent être aussi nombreuses que variées : la modalité du transport, la vitesse de parcours, la topologie du terrain qui sépare les deux relais, etc. Ces indices, qui aident le lecteur à approcher le chemin véritablement parcouru par l'acteur, peuvent cependant être classés en deux catégories. D'une part, les précisions données par l'acteur de l'itinéraire dans son récit (modalité du transport, vitesse de parcours, etc.), d'autre part, des connaissances dont dispose le lecteur sur la région traversée (topologie du terrain, difficulté de parcours, existence de telle voie de communication, etc.).
Tout segment est associé à un relais de départ et àun relais d'arrivée par les associations depart et arrivee. Elles sont du type MADS TopoTouch (( )), c'est-à-dire que le segment est adjacent du point de vue topologique aux relais (de départ comme d'arrivée). L'activité : l'activité correspond aux occupations du narrateur lors de ses étapes (visites, repas, etc.).
Nous avons ajouté au modèle des attendus une contrainte OCL qui spécifie que dans le cas où un itinéraire I ? naît à une étape de départ (respectivement d'arrivée), cet itinéraire I ? est différent de l'itinéraire I pour lequel cette étape joue le rôle d'étape de départ (respectivement d'arrivée). Ceci pourrait être le cas dans des configurations telles que celles de la figure 3.
FIG. 3 -Des exemples de configurations où plusieurs itinéraires émergent.
Le modèle des attendus que nous proposons se veut compact de manière à pouvoir être manipulé dans une chaîne de traitement informatique permettant de reconstruire un itinéraire à partir de son évocation dans un texte. Le second modèle, le modèle d'entrée ou d'extraction, a pour but de modéliser la manière dont les auteurs expriment leur itinéraire dans leur récit de voyage. Pour sa mise au point, nous avons observé sur un échantillon du corpus, les différentes formes des expressions supportant l'information sur les itinéraires et leur organisation dans le discours.
Un modèle d'extraction
Observations sur un échantillon du corpus
Parmi les documents mis à notre disposition par la médiathèque, trois ont été retenus 5 afin d'y entreprendre plusieurs études pour valider plus finement les premières observations (description a posteriori, régularité dans l'évocation du territoire et des déplacements au sein de l'itinéraire, synchronisation entre la description et le déroulement du voyage).
Ces trois textes racontent le voyage d'un explorateur qui part d'une grande ville (Bordeaux, Paris) à la découverte des Pyrénées sur plusieurs jours.
Étude 1, importance des Entités Spatiales : cette première étude consiste à relever les propositions qui évoquent la position (PP) de l'auteur dans son voyage et à comptabiliser combien parmi elles utilisent des Entités Spatiales (ES). Ces PP qui utilisent des ES seront appelées -188 -RNTI-E-13 P. Loustau et al.
PPES. Le concept d'ES a fait l'objet de travaux antérieurs
6 , succinctement, une ES est une zone géolocalisable. On définit une ES par rapport à un point d'ancrage dans l'espace : l'Entité Géographique Nommée (EGN). L'EGN est un objet dont on peut obtenir la géolocalisation à l'aide d'une ressource, grâce à son nom. Les résultats sont donnés dans la figure 4. L'emploi des ES pour évoquer le déplacement y apparaît assez nettement. On atteint des taux de l'ordre de 80% selon les auteurs, mais aussi selon la nature du voyage qui est relaté. En effet, le déficit d'emploi d'ES dans l'évocation du déplacement apparaît le plus souvent lorsque l'auteur évoque des déplacements de plus petites tailles (je suis allé au parc, j'ai fait une promenade sur le port, j'ai quitté l'hôtel, etc.), le plus souvent lorsqu'il est à une étape intermédiaire. C'est le cas pour le texte de Ann Lister, dans lequel l'auteur évoque largement ses occupations lors des étapes le long de son voyage.
Étude 2, importance des formes verbales : cette seconde étude a pour but d'évaluer le poids des verbes. Elle consiste à comptabiliser dans les propositions (PP) qui évoquent le déplacement le nombre de propositions qui utilisent des formes verbales à cette fin (PPV). Avec une moyenne de 87,7% (les résultats par document sont donnés en figure 4) cette étude montre leur prédominance.
Si ces deux études doivent encore être approfondies en augmentant le nombre de récits étudiés, cela donne d'ores et déjà une bonne idée du poids qu'occupent les ES et les verbes de déplacement dans notre corpus. Ces observations corroborent également les travaux autour de l'évocation du déplacement dans la langue tels que ceux de Laur (1991); Sarda (1992); Muller et Sarda (1999 
FIG. 4 -Études 1 et 2 : le poids des propositions utilisant les ES (PPES) et des verbes (PPV) dans les propositions évoquant la position (PP)
Étude 3, synchronisation entre le récit et le voyage : cette étude a pour but de montrer le parallèle qui existe entre la chronologie du voyage et celle du texte le décrivant. Nous avons relevé les dates et heures qui peuvent être attachées aux déplacements mentionnés dans les textes utilisés dans les études précédentes.
Les résultats sont donnés en figure 5. Ils montrent clairement la synchronisation entre le voyage et le récit qui en est fait. Les rares exceptions qui font que des déplacements ne sont pas évoqués dans l'ordre chronologique sont issus d'exemples du type : avant d'arriver à ES x , j'ai traversé ES y . 6 Dans nos précédents travaux (Lesbeguerries et Loustau (2006) 
Le modèle
Les études réalisées sur le corpus documentaire ont montré l'importance des ES et des verbes de déplacement dans l'évocation du déplacement des acteurs lors du récit de leur voyage (cf figure 4). Ces évocations de déplacements permettent au lecteur de se construire une repré-sentation mentale de l'itinéraire parcouru au sein de sa carte cognitive, concept introduit par Kuipers (1977). Nous présentons dans ce paragraphe la manière dont les déplacements apparaissent dans la langue.
FIG. 6 -Un modèle pour l'extraction des déplacements.
Nous reprenons ici principalement le critère de polarité aspectuelle des verbes introduits par Boons (1987) et repris par Laur (1991). Nous modélisons donc les déplacements dans la langue en prenant en compte les verbes de déplacement obligatoirement associés à un acteur, des ES, et une ET (entité temporelle). De cette manière, nous résolvons en partie le difficile problème de la polysémie de certains verbes (cf. section 2.3 pour quelques exemples) qui renferment ou non, selon le contexte, un sens spatial.
-190 -RNTI-E-13 Ainsi, conformément à la notion de polarité aspectuelle, les déplacements extraits seront initiaux (quitter), médians (traverser) ou finaux (arriver).
Les notions d'origine, de destination et de position intermédiaire interviennent également. Les constructions verbales du déplacement font en effet émerger une (quitter Pau), deux (quitter Pau pour Bordeaux) ou trois (quitter Pau pour Bordeaux par la RN 134) de ces propriétés. De même, les notions temporelles qui permettent de situer le déplacement dans le temps jouent également un rôle important. Cependant, le lien entre le verbe de déplacement et l'entité temporelle est moins fort syntaxiquement parlant que celui entre le verbe et la ou les entité(s) spatiale(s), notamment dans les textes que nous considérons.
Pour résumer (cf figure 6), un verbe de déplacement est spécialisé en verbe initial, médian ou final. Dans la langue, il est associé à un Acteur et à une Entite_Spatiale au moins (qu'elle soit d'origine, intermédiaire ou de destination). Il est également associé à une Entite_Temporelle, ce qui permet d'horodater le déplacement.
La phase d'interprétation : transformation du modèle d'extraction vers le modèle des attendus
Nous donnons ici, de manière abstraite, le parallèle qui peut être fait entre le modèle d'extraction et le modèle d'interprétation. Nous reviendrons par la suite sur la méthode, les outils et les ressources nécessaires au passage de l'un à l'autre. Attardons-nous donc exclusivement sur la manière dont les principaux objets du modèle des attendus naissent à partir du modèle d'extraction. Naissance du relais : un relais naît lorsqu'un déplacement est évoqué par un verbe de polarité aspectuelle donnée associé à une entité spatiale compatible avec cette polarité (entité origine compatible avec verbe initial, entité indermédiaire avec verbe médian, entité destination avec verbe final).
Naissance du segment : un segment naît lorsque deux relais consécutifs sont identifiés (cela pré-suppose que les relais ont été ordonnés dans le temps selon leur propriété temporelle).
Naissance d'une étape : par défaut, le premier et le dernier relais deviennent étapes. D'autres relais peuvent devenir étapes : lorsqu'une activité peut être identifiée et rattachée à un relais (qui devient étape de ce fait).
Naissance d'une activité : les activités peuvent être variées (nuit dans un hôtel, repas, changement de modalité, etc.). Nous verrons plus loin comment procéder afin de détecter de manière automatique ces activités grâce à la notion de rupture.
Naissance d'un itinéraire : la naissance de l'itinéraire est directement liée à la naissance des étapes. Dès que deux étapes sont identifiées, un itinéraire est identifié.
Nous donnons dans la figure 7 un exemple de ce passage dans un cas simple : celui dans lequel un seul itinéraire est décrit dans un document. Les exemples d'instances du modèle d'extraction (du haut vers le bas) sont les résultats de l'interprétation des déplacements de l'extrait de document (Le journal de James David Forbes) donné en figure 1 :
-  Lesbegueries et al. (2006). Extraction des déplacements : nous proposons ici de rendre opératoire la modélisation des déplacements donnée en section 3.2.2 grâce à des transducteurs. Nous rappelons qu'un transducteur est un dispositif qui transforme un langage donné en un autre. Les transducteurs sont basés sur des machines à états finis mettant en correspondance deux langages réguliers. Compte tenu des observations faites sur notre corpus documentaire, la construction des verbes de déplacements peut être apparentée à un langage régulier, modélisé par des machines à états finis, tel que présenté dans la figure 8. Cette modélisation du déplacement sous forme de transducteurs est générique aux documents du type récits de voyage et aux documents dans lesquels le déplacement est exprimé principalement sous forme verbale. Les transducteurs sont traduits en règles de grammaire dans lesquelles nous retrouvons les principaux objets du modèle : le verbe, la préposition et l'ES. Cette analyse à base de règles s'appuie sur les résultats d'analyses plus en amont. La première est une analyse morphosyntaxique capable de retourner deux étiquettes concernant la forme (@tag et @stag dans la figure 8) et le lemme 8 de chaque unité lexicale (@lemme dans la figure 8). Elle permet de s'abstraire des formes fléchies des mots, notamment celles des verbes conjugués. La deuxième analyse est l'extraction des ES (@sem dans la figure 8).
Prenons par exemple l'interprétation du déplacement précédemment étudié : nous arrivâmes à Rochefort. Nous considérons à ce stade que des analyses en amont ont déjà été réa-lisées. De ce fait, nous avons accès à diverses informations pour chaque mot de cette phrase. Ces informations sont les suivantes.
-nous : @texte=nous / @lemme=nous / @tag=pro / @stag=null / @sem=null -arrivâmes : @texte=arrivâmes / @lemme=arriver / @tag=ver / @stag=ppa / @sem=null -à : @texte=suis / @lemme=être / @tag=pre / @stag=null / @sem=null -Rochefort : @texte=Rochefort / @lemme=Rochefort / @tag=nom / @stag=prp / @sem=es Nous pouvons alors analyser la phrase à l'aide du transducteur de la figure 8 comme ceci : -en début d'analyse, le transducteur est en état 0 et le pointeur d'analyse positionné sur le mot nous. -il n'y a pas de transition par nous (ou une de ses propriétés), on avance donc le pointeur sur le deuxième mot arrivâmes, le transducteur reste en état 0. -il existe une transition par arrivâmes (concernant son lemme, @lemme=arriver), le transducteur passe en état 7 et nous avons détecté un déplacement final. Le pointeur est avancé sur le mot à. -il existe une transition par à (concernant son texte, @texte=à), le transducteur passe en état 8. Le pointeur est avancé sur le mot Rochefort.
FIG. 8 -Extrait simplifié de la base des transducteurs spatiaux. Le symbole % représente le « joker ». Les transitions en pointillé montrent que d'autres états existent.
-il existe une transition par Rochefort (concernant sa propriété sem, @sem=es), le transducteur passe en état 9 et nous avons détecté un déplacement final qui a pour destination Rochefort. Le pointeur est en fin de phrase et le transducteur sur un état final, l'analyse est validée : nous avons reconnu un déplacement de polarité aspectuelle finale ayant pour destination Rochefort. Dans la section suivante nous chercherons à répondre à cette question : comment s'élever au niveau du discours à partir de ces éléments extraits localement ?
Des déplacements à l'itinéraire
Tout comme l'humain doit avoir une connaissance géographique du monde pour pouvoir raisonner sur un itinéraire Kuipers (1977), un système doit également avoir des capacités équi-valentes. Les principales ressources envisagées sont des ressources de type géographique et elles peuvent être classées en deux catégories : les données et les raisonnements.
Données factuelles : ce sont des données brutes, que l'on pourrait qualifier de données universelles. Elles s'apparentent aux connaissances du monde que peut avoir l'humain. Elles représentent des faits, au sens logique du terme, c'est-à-dire qu'elles sont considérées comme vraies. Les couches de données SIG 9 (comme celles des communes de France, du réseau rou-
FIG. 9 -Quitter Saint-Jean-de-Luz : les zones probables selon la modalité du transport. (1) en bateau,(2) en voiture
tier, ou du réseau fluvial), les Gazeteers 10 , les bases toponymiques, sont des données factuelles envisageables.
Raisonnements : ils sont de deux types. Les plus génériques, s'effectuent directement sur les données factuelles et peuvent être ainsi qualifiés de raisonnement bas niveau. Ces raisonnements sont très souvent offerts par les outils traitant l'Information Géographique, ils permettent notamment d'utiliser des opérateurs et des fonctions afin de déduire de nouvelles informations à partir des données factuelles. Prenons un exemple simple et considérons que dans nos données factuelles nous ayons les coordonnées géographiques des villes de Pau et Bordeaux. Une fonction élémentaire d'un SIG nous permettra de connaître la distance entre Pau et Bordeaux ou de construire la ligne correspondant au segment reliant Pau et Bordeaux. Nous considérons ces fonctions comme prédéfinies dans notre système. Des raisonnements plus spécifiques sur ces données tentent d'apporter au système des règles de bon sens ou règles de sens commun dans le domaine des itinéraires.
-Règles situant l'acteur par rapport à l'entité et mettant en relation la polarité aspectuelle du verbe avec les ES d'origine, intermédiaires et de destination. Un déplacement de polarité aspectuelle initiale P i ayant pour origine ES o situe l'acteur du déplacement en ES o . Un déplacement de polarité aspectuelle médiane P m ayant pour entité intermé-diaire ES i situe l'acteur en ES i . Un déplacement de polarité aspectuelle finale P f ayant pour destination ES d situe l'acteur en ES d . -Règles concernant l'étendue des zones probables de localisation de l'acteur en fonction de la polarité aspectuelle des verbes et de la modalité de déplacement. Un déplacement évoqué avec un verbe initial dont l'origine est l'ES ES o situe l'acteur dans une région limitrophe de la frontière de l'entité ES o , la largeur de cette zone est différente selon la modalité de déplacement : elle sera plus large sur un déplacement en voiture que sur un déplacement à pied et cela est principalement dû à la vitesse du déplacement. -Règles concernant les modalités de transport et leur localisation probable (ex : voiture sur route, bateau sur mer/océan/fleuve, vélo sur route mais pas autoroute, etc.).
Ces deux dernières règles concernant les modalités du transport sont illustrées sur la figure 9. L'utilisation de fonctions comme Boundary, Intersection et Buffer proposées dans les spé-cifications Open GIS 11 et implémentées dans la plupart des SIG permettent de construire ces zones. Par exemple, on obtient les zones hachurées de la figure 9.2 en appliquant une extension (Buffer) d'un facteur ?, dépendant de la vitesse de déplacement sur l'intersection (Intersection) entre la frontière du polygone de St-Jean-De-Luz (Boundary(Geom stjean )) et les géométries des routes du département des Pyrénées Atlantiques (Geom routes64 ). Une dernière intersection du résultat obtenu avec les routes donne les zones hachurées. Ceci se résume par une requête de la forme :
Enfin des règles de détection de ruptures permettent de différencier les étapes des relais, avec comme ruptures possibles (notons que c'est la combinaison de ces ruptures qui permet de déceler une activité) :
-rupture dans la modalité : détection simple dès lors que les modalités peuvent être repé-rées ; -rupture dans l'amplitude dans le déplacement et/ou le temps (ie changement d'échelle) : détection possible par inclusion spatiale et/ou temporelle ; -rupture dans la structure logique : dépend de la qualité de la phase ROC 12 (détection des changements de paragraphe possible avec une ROC basique, détection de titre de chapitre, de section voire plus avec des ROC de meilleure qualité) ; -rupture dans la linéarité spatiale de l'itinéraire : apparition de boucles ; -rupture dans la continuité spatiale de l'itinéraire : apparition de sauts spatiaux ; -rupture dans la continuité temporelle de l'itinéraire : apparition de sauts temporels.
?R : un Prototype pour l'Interprétation d'Itinéraires dans des Récits
La démarche générale consiste à construire une chaîne de traitement linguistico-géographique (figure 10), capable d'extraire les déplacements de manière locale au niveau phrastique dans les textes puis de reconstruire l'itinéraire en utilisant des ressources comme nous le décrivions dans la section précédente. Cette chaîne de traitement utilise le langage XML qui permet de facilement enchaîner différents traitements en ajoutant à l'information extraite dans une phase n l'information extraite à une phase n+1. Nous décrivons ici les outils utilisés par chaque phase du traitement.
Extraction des déplacements : correspond à la partie supérieure de la figure 10. Comme précédement évoqué, l'extraction des ES est effectuée par le prototype PIV de Lesbegueries et al. (2006). L'extraction des déplacements est une chaîne de traitement linguistique à part entière. Elle est constituée des grandes phases que nous décrivions dans la section précédente et a été implé-mentée grâce à la plate-forme de traitement linguistique  (2005). Au sein de celle-ci, l'analyse morpho-syntaxique est effectuée par Tree-Tagger, l'analyseur morphosyntaxique éprouvé de Schmid (1994). Les transducteurs des verbes de déplace-ments sont traduits en grammaires DCG 13 et l'analyse basée sur ces grammaires est confiée à Prolog afin de profiter des mécanismes de déduction et d'unification de ce langage.
On obtient ainsi la première partie de notre chaîne de traitement, celle qui est capable d'extraire les déplacements des textes (cf figure 10).
Reconstruction de l'itinéraire : ce module nous permet de passer des déplacements extraits à un itinéraire. Il prend en entrée un fichier XML dans lequel les déplacements sont représentés selon les critères que nous avons évoqués dans la section 3.2.2. Il utilise le SIG PostGIS 14 afin de mettre en application les règles de raisonnement spatial précédemment évo-quées. Diverses ressources sont également nécessaires afin de reconstruire l'itinéraire. Dans ?R, nous utilisons différentes couches de données pour la géolocalisation des relais (communes de France, toponymes, etc.) et des segments (réseau autoroutier, routier, sentier, etc.). Ces couches de données sont également rendues disponibles dans PostGIS.
Pour chaque déplacement extrait, on cherche tout d'abord à produire une zone probable de localisation de l'acteur. Pour cela nous faisons appel aux règles concernant la polarité aspectuelle et la modalité du transport. Les fonctions boundary, buffer, intersection du SIG PostGIS ont été utilisées à ces fins. Considérons le déplacement de l'exemple : « Nous avons quitté Bordeaux pour Langon en voiture. ». Celui-ci a pour origine Bordeaux et pour destination Langon. Il fait donc émerger un segment reliant les deux relais que sont Bordeaux et Langon. La modalité du déplacement étant voiture, on va faire appel à un algorithme de calcul de trajet dans le réseau routier pour construire la route probablement empruntée par l'acteur du déplacement. Cette fonctionnalité est apportée par le module pgRouting 15 . Rappelons qu'il ne s'agit pas forcément de la réalité ; l'acteur a peut-être pris un autre chemin. Par cette méthode, on cherche à 13 DCG : Definite Clause grammar 14 PostGIS : extension GIS du système de gestion de base de donnée libre PostgreSQL 15 pgRouting est un module qui donne au SIG PostGIS des fonctions de calcul de plus court chemin (algorithme de Dijkstra, A-étoile, etc.) FIG. 11 -Sortie simplifiée du prototype ?R : un fichier XML contenant l'interprétation de l'itinéraire. Visualisation effectuée avec MapServer approcher la représentation mentale de l'itinéraire que pourrait se faire le lecteur dans sa carte cognitive.
Une fois que chaque segment de l'itinéraire est ainsi construit, on stocke les segments dans le SIG PostGIS.
Les détections de ruptures qui donnent naissance à des activités ne sont pas encore implé-mentées. De ce fait, nous appliquons la règle par défaut pour la détection des étapes de départ et d'arrivée : le premier relais devient étape de départ, le dernier relais devient étape d'arrivée.
En sortie du prototype ?R, nous obtenons donc une instance du modèle des attendus au format XML. Ce fichier XML contient l'ensemble des objets qui permettent de décrire un itinéraire : des segments, des relais et des étapes. Tous ces objets sont des EG, ils contiennent donc un géocodage au format GML, et une marque temporelle. Ces interprétations d'itinéraires peuvent être alors visualisées par des outils de cartographie tels que MapServer 16 comme montré dans la figure 11. Ils peuvent également intervenir dans la phase de conception d'activités pédagogiques : c'est ce que nous montrons dans la section suivante par des exemples de requêtes multi-niveaux de l'information géographique.
FIG. 12 -Prise en compte de la complexité croissante des informations géographiques sur lesquelles portent les requêtes.
-199 -RNTI-E-13 figure 12 présente quatre exemples de requêtes de complexité croissante ; la première colonne montre un exemple de texte brut pour lequel l'information ciblée est soulignée. La deuxième colonne décrit la structure sémantique de l'information ciblée alors que la troisième colonne propose une projection de cette information afin de pouvoir la traiter avec un SIG par exemple.
De nombreux SIG permettent de résoudre la requête 0 (et même des systèmes d'information classiques se basant sur des appariements de type fulltext). Les autres requêtes sont beaucoup plus spécifiques à la problématique étudiée dans cet article puisqu'elles prennent en compte les entités géographiques associées aux verbes de déplacement qui apparaissent dans le texte (si nous avions proposé des modèles computationnels prenant en compte les descriptions de lieux associées à des verbes de perception, le système aurait également dû s'appuyer sur les requêtes de niveau 0).
Pour chacune des requêtes 0 à 3, nous présentons dans les paragraphes qui suivent les traitements effectués (niveau de granularité des informations recherchées) et nous indiquons quels modèles computationnels sont les plus appropriés pour répondre à ces requêtes.
Requête 0 : Trouver les documents qui parlent de Pau et de Nay Ici, il s'agit de traiter une information géographique de base, donc de faire une simple comparaison entre les entités géographiques nommées (EG) du corpus documentaire et de la requête. Traitements sur le corpus : extraction des EG, géocodage des EG, indexation des EG. Traitements sur la requête : si elle est exprimée en langage naturel, des traitements similaires à ceux effectués sur le corpus sont nécessaires c'est-à-dire extraction des EG, géocodage des EG, indexation des EG. Si la requête est formulée via une interface-usager (via une carte par exemple), le géocodage est obtenu directement. Croisement de la requête et du corpus : il s'agit d'une comparaison géométrique au sens strict entre le géocodage de la requête et le géocodage des EG des documents.
Requête 1 : Trouver les documents qui parlent de la banlieue de Pau et de Nay Dans cette requête, le niveau d'agrégation des informations géographiques à exploiter est plus élevé car une simple utilisation des EG ne permet pas de traiter la requête. Le système doit être en mesure d'interpréter les EG Pau et Nay mais aussi le concept de banlieue c'est-à-dire d'interpréter des Entités spatiales (ES). Traitements sur le corpus : extraction des ES, géocodage des ES, indexation des ES. Traitements sur la requête : si elle est exprimée en langage naturel, des traitements similaires à ceux effectués sur le corpus sont nécessaires c'est-à-dire extraction des ES, géocodage des ES, indexation des ES. Si la requête est formulée via une interface-usager (via une carte par exemple), le géocodage est obtenu directement. Croisement de la requête et du corpus : il s'agit d'une comparaison géométrique au sens strict entre le géocodage de la requête et le géocodage des ES issues des documents.
Requête 2 : Trouver les documents dans lesquels le narrateur part de Pau Ici encore, le niveau d'agrégation de l'information géographique est augmenté. Le système doit non seulement interpréter les ES mais aussi les relations spatiales entretenues par le narrateur avec ces ES. En d'autres mots, le système doit être capable de déterminer si l'ES Nay apparaît dans un déplacement du narrateur de polarité finale. Traitements sur le corpus : extraction, interprétation, géocodage et indexation des ES, extraction, interprétation, géocodage et indexation des déplacements. Traitements sur la requête : si elle est exprimée en langage naturel, des traitements similaires à ceux effectués sur le corpus sont nécessaires. Si la requête est formulée via une interface-usager (via une carte par exemple), le géocodage est obtenu directement. Croisement de la requête et du corpus : il ne s'agit plus de faire une simple comparaison géométrique comme dans les cas précédents. Le système doit également interroger la structure sémantique (codée dans un arbre XML) pour déterminer la sémantique du déplacement effectué pour une ES donnée. Dans notre exemple, l'ES Pau est associée à un déplacement de polarité initiale.
Requête 3 : Trouver les documents dans lesquels le narrateur part de la banlieue de Pau, traverse Bizanos et arrive à Nay Dans cette requête, le niveau d'agrégation est encore augmenté puisque le système doit être en mesure de capter l'itinéraire complet effectué entre la banlieue de Pau et Nay. Traitements sur le corpus : extraction des ES, géocodage des ES, indexation des ES, extraction et interprétation des déplacements, reconstruction et indexation de l'itinéraire. Traitements sur la requête : si elle est exprimée en langage naturel, des traitements similaires à ceux effectués sur le corpus sont nécessaires. Si la requête est formulée via une interface-usager (via une carte par exemple), le géocodage est obtenu directement. Croisement de la requête et du corpus : il s'agit d'effectuer une comparaison géométrique entre le géocodage de la requête et le géocodage des itinéraires extraits des documents du corpus.
Bilan et perspectives
Dans cet article, nous avons présenté une approche computationnelle et un outillage pour capter et exploiter, à différents niveaux de granularité, la sémantique des informations géogra-phiques contenues dans un corpus de récits de voyage. Les informations particulières visées par ces travaux sont (des plus simples aux plus agrégées) des entités géographiques nommées, des entités spatiales, des déplacements, des relais, des étapes et des segments constitutifs des itinéraires relatés par les auteurs.
Nous nous situons ici dans un courant très profond qui est celui du passage du web actuel (pour lequel on affiche des documents avec quelques fonctionnalités de recherche, y compris dans des projets comme Google Library) à un web de la connaissance et des services (le web 2.0).
Des niches très diverses se constituent des outillages qui sont capables d'exploiter les contenus textuels que l'on peut récupérer via les bibliothèques numériques en ligne. Cela justifie une démarche de traitements totalement automatisés en amont des usages (notre approche consiste à fournir des services d'interrogation / visualisation sur une clé d'accès au document très particulière qui est celle de l'itinéraire). Les traitements automatisés que nous proposons dans ce papier sont transparents pour les utilisateurs puisque les documents sont traités avant que l'utilisateur ne s'y intéresse via un traitement back-office (comme lors d'une indexation classique d'un moteur de recherche). Cela justifie également que l'on propose des services à valeur ajoutée utilisant le potentiel des documents ainsi sélectionnés dans des applications finalisées (la sélection de documents sur des critères géo-spatiaux n'étant pas une fin en soi). D'où la conception d'applications pédagogiques et les usages faits de ces applications par des apprenants.
L'intérêt des usages pédagogiques et l'utilisabilité des documents analysés par notre outillage est de se servir de leur sémantique pour guider l'activité de conception. D'une part pour définir les types d'interaction avec l'apprenant qui pourront être évalués par le système tuteur (diagnostic des connaissances de l'apprenant, diagnostic des erreurs de compréhension sur la base de la connaissance qu'a ce tuteur du texte mis à disposition de l'apprenant). D'autre part pour éviter au concepteur de définir des activités que la machine ne pourrait pas interpréter parce que ce qui a été capté dans le texte de manière automatisée est trop pauvre par rapport à la compréhension que peut avoir l'apprenant de cette activité par sa lecture "humaine" du document. L'enjeu ici est de se servir des documents analysés automatiquement pour ne concevoir que les activités que l'on va pouvoir encadrer de manière automatisée (ou assistée) par un tuteur informatique.
Compte tenu de la taille du corpus documentaire mis à notre disposition, nous avons proposé une approche totalement automatisée d'extraction des itinéraires de ce corpus. Cette approche se base sur deux modèles computationnels et un processus automatisé de mise en relation de ces deux modèles. Le premier modèle est un modèle des attendus (cf. section 3.1). Il a pour but de donner une représentation au concept d'itinéraire et a été piloté par les usages que nous souhaitons en faire (à des fins de RI et de conception d'activités pédagogiques). Le modèle des attendus proposé s'inspire de travaux des nombreux auteurs (cf. section 2.2) ayant nourri les recherches dans ce domaine. Rappelons enfin que la formalisation MADS que nous avons proposée pour ce modèle des attendus est purement conceptuelle (cf figure 2) et ne pré-juge pas de la façon dont le modèle est ensuite formalisé pour son exploitation par un Système d'Information de type Base de Données. Le second modèle (cf. section 3.2.2) est un modèle d'entrée ou d'extraction, il suit une logique du domaine de l'Extraction d'Information. Il permet par sa mise en application au sein de notre chaîne de traitement de capter environ 75% (cf figure 13) des déplacements importants d'un récit de voyage. Cette capacité varie en fonction de la précision avec laquelle l'auteur évoque ses déplacements : les déplacements de petite ampleur (le plus souvent ceux qui ne font pas intervenir d'ES) sont plus difficiles à capter. Comme dans la plupart des travaux, les capacités d'extraction dépendent fortement de la base lexicale utilisée. Ici, elles s'appuient sur l'existence de transducteurs de tous les verbes susceptibles d'évoquer un déplacement (cf. section 4.1). Des expérimentations en cours semblent toutefois montrer qu'il est possible d'augmenter automatiquement ces ressources lexicales par synonymie, par similarité de construction, etc. Le processus automatisé que nous avons proposé permet de reconstruire l'itinéraire (conformément au modèle des attendus) à partir des déplacements extraits (conformément au modèle d'entrée). La mise en oeuvre de ce processus repose sur une hypothèse relativement forte concernant la chronologie des déplacements. Cette hypothèse est acceptable car les récits de voyage constituent un domaine à part entière : ce qui est dit doit être fidèle à ce qui a été vu, le locuteur rendant compte de ses découvertes avec la plus grande exactitude (ce qui est visé, c'est la parfaite concordance des mots et des choses vues). Cependant, nous tentons actuellement de lever une partie de cette hypothèse en prenant en compte les aspects temporels et thématiques des relais mais aussi en tenant compte du temps (conjugaison) des verbes.
Du point de vue qualitatif, les résultats actuels sont très encourageants. Nous avons évalué les capacités de notre outillage en comparant les itinéraires annotés automatiquement à une annotation manuelle effectuée sur trois textes. Nous donnons en figure 13 les résultats de cette évaluation : Dans l'évaluation 1, nous avons comparé les déplacements captés automatiquement à tous les déplacements annotés manuellement. Les résultats sont mitigés car certains dé-placements ne sont pas évoqués sous la forme attendu, c'est-à-dire avec le triplet (V,P ?,E). En comparant l'extraction automatique avec les déplacements évoqués grâce au triplet (V, P ?,E), nous avons obtenu de bien meilleurs résultats (évaluation 2). Dans une grande majorité des cas, c'est l'absence de l'ES qui met en échec notre processus d'extraction des déplacements. 

Introduction
L'interprétation automatique d'images devient un processus de fouille de données de plus en plus complexe. Pour les images à très haute résolution, l'utilisation de l'approche dite orientée objet consiste à identifier dans l'image, souvent à l'aide d'une segmentation de l'image, des objets composés de plusieurs pixels connexes et ayant un intérêt pour l'expert du domaine.
Il existe de nombreux algorithmes de segmentation. Néanmoins, ces techniques nécessi-tent souvent une paramétrisation complexe telle que le choix de seuils ou de pondérations. Le nombre de paramètres augmente bien souvent avec la complexité des algorithmes. Ainsi, l'utilisateur amené à définir ces paramètres a souvent du mal à faire le lien entre sa connaissance sur les objets présents dans l'image et les paramètres adéquats pour les construire et les identifier dans une segmentation.
L'utilisation des algorithmes génétiques (Goldberg, 1989) est une solution à ce problème de recherche des paramètres optimaux. Ils peuvent être utilisés pour optimiser un ensemble d'attributs si une fonction d'évaluation des paramètres est disponible. Les méthodes existantes d'optimisation de segmentation par approche génétique (Pignalberi et al., 2003;Bhanu et al., 1995;Song et Ciesielski, 2003;Feitosa et al., 2006) se basent sur des fonctions d'évalua-tions demandant des exemples d'objets segmentés fournis par l'expert. Si aucun exemple n'est disponible, il est possible d'utiliser des critères non supervisés (Bhanu et al., 1995;Feitosa et al., 2006), c'est à dire jugeant la qualité intrinsèque que doit avoir une segmentation (e.g. homogénéité des régions). Néanmoins ces critères non supervisés sont souvent insuffisants pour obtenir une segmentation de bonne qualité notamment pour l'analyse d'images complexes.
Dans cet article, nous proposons d'utiliser des connaissances du domaine afin d'évaluer la qualité d'une segmentation. En effet, l'approche orientée objet permet à l'expert d'exprimer ses connaissances sur les objets de l'image. Ce nouveau cadre de discernement permet de raisonner sur des régions et non sur des pixels ce qui permet une description intuitive et naturelle des objets pouvant être présents dans une image. Une ontologie, ou base de connaissance, peut alors être utilisée pour définir les différents types d'objets du domaine (i.e. concepts) de l'image ainsi que leurs caractéristiques. Il devient alors possible d'évaluer la cohérence d'une segmentation par rapport aux concepts définis dans cette ontologie. Cette approche a l'avantage de ne pas nécessiter d'exemples et utilise la connaissance définie dans l'ontologie.
Le plan de cet article est le suivant. Tout d'abord nous introduisons l'algorithme de segmentation ainsi que ses paramètres. Nous présentons ensuite la modélisation de la connaissance sous la forme d'une ontologie. Nous étudions ensuite comment un algorithme génétique est utilisé pour choisir les paramètres de la segmentation grâce à une évaluation utilisant l'ontologie. Finalement nous présentons des expérimentations dans le cadre de l'interprétation d'images pour l'observation de la Terre.
Segmentation d'image
Dans cet article, nous utilisons l'algorithme de segmentation par ligne de partage des eaux (Vincent et Soille, 1991) ainsi que les méthodes de seuillage du gradient, de la réduction de dynamique et de fusion de régions. Ces différentes techniques sont utilisées simultanément pour réduire la sur-segmentation provoquée par la ligne de partage des eaux et nécessitent donc de définir 3 paramètres (le seuil du gradient, le seuil de dynamique et le seuil de fusion). Les valeurs optimales de ces paramètres sont difficiles à trouver car la valeur pour un paramètre donné dépend des valeurs choisies pour les autres paramètres. De plus, il existe de nombreux optima locaux, ce qui accroît la difficulté de trouver la meilleure solution.
Ontologie d'objets géographiques
Nous présentons ici les principes de l'ontologie d'objets géographiques définie dans (Brisson et al., 2007) puis étendue dans . Cette ontologie se présente comme une hiérarchie de concepts ainsi que des relations entre ces concepts. Un mécanisme d'appariement permet de comparer une région construite lors d'une segmentation et les différents concepts définis dans l'ontologie.
L'ontologie est formée d'une hiérarchie de concepts dont la figure 1 présente un extrait. Dans cette hiérarchie chaque noeud correspond à un concept. Chaque concept a une éti-quette (e.g. pavillon) et est défini par des attributs. Chaque attribut est associé à un intervalle de valeurs acceptées pour cet attribut (e.g. [50; 60]) ainsi qu'une pondération (dans [0; 1]) représentant son importance pour reconnaître l'objet géographique correspondant à ce concept (1 indiquant que cet attribut est très pertinent). Les valeurs de ces concepts ont été renseignées par les experts géographes grâce à leur connaissance de la morphologie des objets urbains. Certaines informations ont également été extraites de bases de données topographiques ou de connaissances sur les réponses spectrales de certains types de matériaux (tuile, bitume . . .).
Un mécanisme d'appariement de région permet d'évaluer la similarité entre une région construite lors d'une segmentation et les concepts définis dans la hiérarchie de l'ontologie. Ce mécanisme permet d'obtenir la signification sémantique d'une région si les caractéristiques de celle-ci se rapprochent de la description d'un des concepts définis dans l'ontologie. L'appariement d'une région consiste à vérifier la validité des caractéristiques extraites de celle-ci (réponse spectrale, taille, élongation . . .) en fonction des propriétés et des contraintes définies dans les concepts de l'ontologie.
Algorithme génétique
Les algorithmes génétiques (Goldberg, 1989) font partie des méthodes d'optimisations. Ils sont reconnus pour être efficaces même lorsque l'espace de recherche est vaste et contient de nombreux maxima locaux. Ces algorithmes sont d'inspiration biologique avec des notions d'individus (i.e solutions) et de population d'individus qui évoluent pour s'améliorer à travers des opérations de sélection, croisement et mutation. Dans cet article, un individu représente le vecteur des paramètres de la méthode de segmentation. On considère que les valeurs de ces paramètres sont définies entre zéro et un, on obtient donc un espace de recherche à 3 dimensions pour la ligne de partage des eaux (trois seuils). Nous utilisons un taux de mutation de 1% et un nombre de générations de 14, des tests ayant montré que plus de générations n'amélioraient pas les résultats.
La définition de la fonction d'évaluation est une des étapes les plus importantes dans un système d'évolution génétique. Nous cherchons ici à utiliser les connaissances de l'ontologie pour guider le processus évolutif et trouver les paramètres qui permettent de maximiser la découverte d'objets dans l'image. Nous allons donc utiliser comme fonction d'évaluation, le pourcentage de la surface de l'image qui est identifié par l'ontologie. Chaque individu i va donc produire un vecteur de paramètres produisant une segmentation de l'image. Soit R i les régions de la segmentation issue de l'individu i et R avec Aire(r) une fonction renvoyant la surface en pixel de la région r. La surface des régions identifiées a été préférée à leur nombre pour évaluer le résultat comme nous cherchons ici à maximiser la surface de l'image reconnue par l'ontologie.
Expérimentations
La méthode proposée a été évaluée sur une image de Strasbourg prise par le satellite Quickbird. La figure 2 présente des extraits de segmentations avec les paramètres obtenus au cours d'une évolution génétique aux générations 1, 3, 5 et 11. On observe une réduction de la sous--segmentation (trop peu de régions) au cours des générations et une amélioration de la construction des objets, l'image étant de mieux en mieux interprétée par l'ontologie. Pour valider ces résultats nous avons évalué la qualité des segmentations obtenues avec des vérités terrains de trois classes, pavillon, végétation et route. Les évaluations sont effectuées sur des objets géographiques construits et labellisés manuellement par un expert. Trois indices de qualité ont été utilisés pour évaluer la qualité des segmentations : le rappel (en considérant la reconnaissance de l'ontologie comme une classification pixel), l'indice de Janssen (Janssen et Molenaar, 1995) et l'indice de Feitosa (Feitosa et al., 2006) qui évaluent la qualité des régions construites.
Nous avons évalué notre critère d'évaluation (surface reconnue par l'ontologie) par rapport à ces trois critères. Pour chacun des critères une moyenne est calculée sur l'ensemble des objets  TAB. 1 -Résultat de l'évaluation de la méthode sur les objets experts pour 4 générations.
fournis par l'expert. Le but de cette analyse est de vérifier que l'amélioration de notre critère conduit bien à une amélioration de la segmentation. Au cours d'une évolution génétique nous avons évalué chaque individu en fonction de ces différents critères. Ainsi, 200 paramétrages possibles ont été évalués. Ils ont ensuite été ordonnés en fonction de notre critère d'évaluation. La figure 3 présente les courbes pour les différents indices. Nous pouvons constater que dans les trois cas, les deux courbes semblent avoir même comportement et être corrélées. Ces résul-tats montrent qu'optimiser notre critère est pertinent et permet d'effectuer des segmentations de qualité sans contraindre l'expert à fournir des exemples. Enfin, le tableau 1 présente les valeurs de ces indices avec les paramètres trouvés au cours des générations 1, 3, 5 et 11 (la qualité n'évoluant plus après). On constate que les différentes mesures d'évaluation sont optimisées au cours de l'évolution (sauf une légère augmentation de l'indice Feitosa sur la dernière génération due à une sur-segmentation de la végétation mais qui n'influe pas sur la reconnaissance de l'ontologie, la taille d'une région n'étant pas discriminante pour caractériser la végétation).
Conclusion
Dans cet article nous avons présenté un mécanisme permettant de guider un processus automatique d'interprétation d'images grâce à une ontologie. Cette ontologie représente la connaissance des experts sous la forme d'une hiérarchie de concepts, d'une caractérisation de ces concepts et des relations entre ces concepts. Un processus évolutif va alors chercher les paramètres de segmentation produisant des régions qui seront bien identifiées par l'ontologie. Ainsi nous disposons d'une méthode d'interprétation automatique d'images de télédétection basée sur la connaissance des experts à propos des objets à identifier.
Des résultats intéressants ont montré la validité du système. Il est à noter que n'importe quel algorithme de segmentation nécessitant des paramètres peut remplacer celui présenté dans cet article. Dans le futur nous souhaitons intégrer des connaissances contextuelles (position des objets entre eux) nécessaire à l'identification de certains objets.

Introduction
La gestion et la maîtrise du développement territorial imposent le recours à une vision globale et objective des caractéristiques et des dynamiques d'un espace. C'est pourquoi, avant d'engager des politiques d'action, il est nécessaire de fonder la réflexion sur une dé-marche de diagnostic de territoire. Ce processus de diagnostic territorial permet non seulement d'effectuer une sorte « d'état des lieux » du territoire -considéré comme une construction sociale résultant des interactions entre les acteurs et les activités et s'analysant en tant que réseau de relations (Lardon et al., 2001) -, des relations qui le construisent, mais aussi de -43 -RNTI-E-13
GEOdoc proposer des solutions aux éventuelles difficultés soulevées par le diagnostic. Il est bien souvent le siège d'une concertation entre acteurs et experts, au sein de laquelle des opinions parfois divergentes doivent pouvoir s'exprimer. La solution retenue consiste alors en l'acceptation d'un consensus par les parties en présence.
La mise en oeuvre d'un diagnostic de territoire est un processus complexe. Cette complexité est en partie du à la grande variété d'acteurs impliqués et à la mobilisation importante de représentations spatiales. C'est ce qui explique qu'un certain nombre de méthodes aient été développées pour faciliter le dialogue entre acteurs et experts et favoriser l'usage des représentations spatiales comme base de réflexion et de travail. Citons par exemple l'approche de diagnostic Structure-Dynamique-Projet développée par le laboratoire POP'TER (Politiques publiques et développement des territoires) de l'ENGREF ou encore la méthode du Zonage À Dire d'Acteurs (ZADA) développée par CIRAD-TERA (Brau et al. 2005). Ces deux méthodes, bien que fondées sur des approches différentes, possèdent des similarités en ce qui concerne l'organisation des étapes du diagnostic et le positionnement des représentations spatiales.
Cette démarche suppose donc que les acteurs impliqués cheminent dans le processus vers une conception « commune » du territoire et des objectifs « individuels » compatibles avec les objectifs « communs » (Lardon et Mainguenaud 2006). L'exercice de diagnostic requiert des outils, une méthodologie et des représentations spatiales adaptées, de manière à prendre en compte tous les avis et à aboutir à des résultats acceptés par le plus grand nombre. Certains outils comme les tableaux de bord, les systèmes d'aide à la décision ou les Spatial OLAP, permettent déjà d'aider aux décisions à partir de règles et de paramètres prédéfinis. Mais bien souvent, les décideurs de niveau stratégique (élus locaux par exemple) n'ont pas les connaissances nécessaires indispensables à la manipulation de ces outils même les plus simples. Ils n'ont pas non plus toujours à leur disposition les documents de référence pertinents sur lesquels ils pourraient baser leur jugement (Ciobanu et al. 2006). Il devient alors très difficile dans ces conditions de trouver les informations pertinentes, de les mettre en relation les unes avec les autres, de les contextualiser et finalement de les utiliser à bon escient, notamment à cause de la masse toujours plus importante de documents diffusés.
Partant de ce constat, nous nous sommes engagés dans la conception d'un outil de visualisation qui permettrait de naviguer de manière simple à travers des documents géographi-ques mis en réseaux. En effet, dans la pratique, le diagnostic de territoire nécessite la conception, la mobilisation et l'étude de nombreux documents géographiques ; des représentations spatiales variées, mais aussi d'autres documents multiformes comme les articles de journaux, les photos, les tableaux de statistiques, etc. Un document géographique, dans la suite de cet article, est défini comme un « support physique ou numérique incluant des données géomé-triques, descriptives ou graphiques, porteurs de représentations, d'informations et de connaissances, en lien direct avec la description et la caractérisation d'un territoire ou d'un espace géographique donné ».
Nous débutons cet article par une synthèse rapide des caractéristiques des processus de diagnostic de territoire, de manière à poser les contraintes auxquelles la solution GEOdoc devra répondre. Nous proposons dans la foulée un bref état de l'art des solutions et outils déjà existants. Nous passons ensuite en revue les techniques et approches existantes pour -44 -RNTI-E-13 répertorier, organiser et visualiser des documents géographiques. Enfin, nous proposons une première conceptualisation de cet outil de visualisation appuyée sur un cas d'usage type, et présentons brièvement quelques perspectives d'évolutions possibles.
2 Des outils pour appuyer le diagnostic de territoire
Documents géographiques et diagnostic de territoire
La dimension spatiale du diagnostic de territoire est à l'origine d'une importante complexité. La difficulté est liée au fait que chaque acteur impliqué possède « sa » représentation de l'espace et des enjeux associés. C'est l'une des raisons qui expliquent pourquoi l'intégration sociale des technologies géospatiales dans ces processus de diagnostic est encore faible. Pourtant dans le contexte de la société de l'information, il apparaît plus que jamais nécessaire de concevoir de nouveaux outils qui permettraient de favoriser l'usage de ces technologies au sein des systèmes sociaux de décision et de concertation (Roche et Hodel 2004). C'est un peu ce que prône H. Campbell (1999) dans son approche « d'interactionnisme social ». Elle propose d'intégrer les acteurs sociaux dans le développe-ment des solutions géomatiques ; de ne pas les reléguer au rang de simples spectateurs ou utilisateurs passifs. Elle met ainsi l'accent sur le lien très fort qui associe les avancées technologiques dans un domaine et leurs impacts sociétaux. Les deux notions interagissent et s'influencent à des degrés différents suivant les époques. Ainsi, comme le rappelle C. Bucher (2002), la représentation de l'espace géographique dépend du contexte social de la personne qui en fait la retranscription.
L'un des fondamentaux du diagnostic de territoire repose précisément sur l'élaboration de représentations spatiales traduisant les problèmes, les forces, les faiblesses, voir les élé-ments de solution. Ceci conduit à produire des formes de représentations différentes dont l'interprétation peut varier en fonction des objectifs visés. Ainsi, outre le format de la repré-sentation (cartes, photos, schémas, chorèmes...), les objets, les phénomènes ou encore les dynamiques représentés sont dépendants de la volonté de leurs auteurs et des priorités qu'ils ont souhaité mettre de l'avant. Ces acteurs concepteurs de représentations ne sont pourtant pas les seuls à fournir des renseignements sur un territoire. D'autres documents viennent préciser le contexte d'analyse, apporter une information chiffrée ou encore synthétiser des points de vue. Ils sont bien souvent présentés sous des formes différentes (textes, tableaux, graphiques, vidéos…) et ne sont pas toujours pris en compte lors d'un diagnostic de territoire, par méconnaissance ou inaccessibilité. Pourtant, ces documents doivent, au même titre que les représentations spatiales, être inclus dans les analyses et les réflexions sur lesquelles se construit le diagnostic. De notre point de vue, ces documents sont fondamentaux dans la mesure où ils offrent bien souvent aux acteurs des clés de lecture complémentaires pour la compréhension des enjeux. C'est précisément l'un des objectifs de GEOdoc que de permettre une visualisation de l'ensemble des documents pertinents pour un diagnostic de territoire, et de rendre explicite le réseau de relations qui lient les documents entre eux, les documents et les acteurs, liens géo-graphiquement contextualisés.
-45 -RNTI-E-13
GEOdoc
Les limites des outils et solutions existants
Les technologies géospatiales offrent des solutions à géométrie variable pour appuyer les diagnostics de territoire. Des SIG bâtis pour l'analyse, aux SIG participatifs -PPGISconçus et développés pour supporter la participation du public, la panoplie est large. Pour autant, ils ne sont pas particulièrement efficaces dès lors que les données à mobiliser sont de nature hétérogène, ou pour gérer et diffuser des documents multimédia à forme variable (cartes, graphiques, film, vidéo, etc.). D'autres solutions technologiques existent, dont l'objet consiste précisément à gérer des documents sous forme numérique (GED -gestion électro-nique des documents). Mais dans ce cas, la gestion de la composante géospatiale des documents est rarement considérée. Aussi pour prendre la pleine mesure de la solution proposée ici, nous avons jugé pertinent et utile de dresser un rapide portrait des solutions existantes dans le domaine de la gestion documentaire, de la géomatique décisionnelle et de l'intelligence territoriale. Il s'agit là de secteurs fortement marqués par des phénomènes de convergences technologiques (Clemens 2004, Plante et al. 2007, Leblond 2007. GEOdoc vise spécifiquement à appuyer la prise de connaissance, la concertation, puis la prise de déci-sion tactique et stratégique ancrée sur le territoire, objet d'un diagnostic. Bien qu'il s'agisse là d'un type de système décisionnel novateur, GEOdoc possède des affinités fonctionnelles et de contenu avec certains types de systèmes d'information actuels: (1) tableaux de bord, (2) outils de GED et (3) systèmes d'information géographique (Figure 1).
FIG. 1 -Position du GEOdoc par rapport aux autres solutions existantes
GEOdoc possède des similitudes avec les outils utilisés en intelligence d'affaires (« Business Intelligence »), domaine décisionnel en pleine effervescence (Dresner et al. 2003  (EMC 2008-b). À l'instar de ces solutions, GEOdoc vise à faciliter l'accès à l'information en constituant plus spécifiquement un "guichet" intégré numérique pour accéder à de multiples informations gérées par les parties prenantes. Néanmoins, il est important de comprendre que le GEOdoc ne vise pas à se substituer dans sa finalité aux logiciels de gestion documentaire pouvant déjà être en place dans les organisations, mais vise plutôt à venir les compléter sur le plan géospatial en proposant une interface cartographique novatrice, simple d'utilisation, favorisant (accélérant) ainsi la réflexion et la prise de décision territoriale.
Finalement, GEOdoc emprunte certaines fonctionnalités aux bases de données géospatia-les, aux systèmes d'information géographique (SIG) et aux logiciels de dessin assisté par ordinateur (DAO). Il repose en effet sur une interface à l'utilisateur, basée sur la localisation et la représentation cartographique. La différence principale réside dans la forte simplification de l'interface de l'outil par rapport à des SIG offrant habituellement des fonctionnalités complexes de navigation géographique et d'analyse spatiale, tels que ArcGIS (ESRI 2008) et MapInfo (Mapinfo 2008). Sur le plan du contenu, GEOdoc diffère aussi des SIG, tout en se rapprochant davantage des géorépertoires ou des catalogues cartographiques de métadon-nées. Il offre ainsi moins un accès aux données de base qu'aux représentations spatialesplus ou moins fermées -déjà conçues à partir des données. Pourtant, certaines déclinaisons récentes particulières des SIG vont dans le même sens que GEOdoc à certains égards. Ainsi, Google Earth et Google Map (Google Earth 2008) proposent une interface cartographique simplifiée pour des non-spécialistes de la géomatique, facilitant ainsi grandement l'accès à l'information géospatiale. D'autre part, le SOLAP (SOLAP 2008) constitue un système inté-ressant, combinant à la fois des fonctions de forage de données géographiques, une structure multidimensionnelle de données, ainsi qu'une interface géographique. Finalement, soulignons le logiciel MetaCarta (MetaCarta 2008), très intéressant dans sa capacité d'identifier une localisation générale attribuable à de multiples textes (documents descriptifs) et de les localiser visuellement dans une interface géographique.
Cette analyse de l'existant nous a permis de mettre en évidence le caractère novateur de GEOdoc. En effet, aucun outil existant ne permet de rendre explicite, navigable et interrogeable le réseau de relations caractéristiques d'un processus de réflexion et de décision collectives spatialisées (diagnostic de territoires) : relations entre les documents géographiques eux-mêmes, relations entre les documents et les acteurs ; l'ensemble de ces relations étant géographiquement contextualisées.
-47 -RNTI-E-13
GEOdoc 3 Les concepts à la base de GEOdoc
« médium humain » et « objets frontières »
L'un des objectifs principaux du diagnostic territorial consiste à favoriser l'appropriation des représentations spatiales par les décideurs locaux, de manière à faciliter la compréhen-sion des concepts spatiaux qui sont développés. Dans ce sens, deux concepts fondamentaux permettent de renouveler l'approche des systèmes décisionnels géospatiaux dans le diagnostic territorial : le « médium-humain » et les « objets frontières ».
Le « médium-humain » est en quelque sorte la personne qui assure le lien entre les données produites par les experts et professionnels en particulier (les représentations spatiales par exemple) et les usagers finaux (les décideurs, le public…). Plus exactement, ce médium favorise l'accessibilité et l'assimilation de l'information géographique par des utilisateurs indirects (Roche 2000, Roche et Hodel 2004. Il est le garant de l'interactivité entre des domaines variés comme la géomatique et l'aménagement. Il permet d'éviter certains blocages dus à la difficulté de compréhension des données géographiques par des non spécialistes et ainsi contribue à enrichir le débat en favorisant la communication.
Les « objets frontières » constituent des éléments fondamentaux dans le partage des connaissances. Ce sont des « objets marquant une frontière, tout en facilitant le dialogue entre des spécialistes dans la réalisation d'un projet commun pluridisciplinaire » (définition issue du Réseau d'Activité à Distance : http://rad2000.free.fr). Il peut s'agir de concepts définis dans un schéma d'aménagement, de règles imposées sur un espace, de cartes ou de modèles… De manière générale, les individus issus d'un même « groupe social » s'organisent en réseau pour transmettre de l'information. Au sein d'un même groupe, la compréhension des informations qui circulent est favorisée par la proximité des profils. Lorsque la communication et les échanges doivent s'opérer entre des groupes différents, c'est presque systématiquement le cas dans un diagnostic de territoire, la compréhension est beaucoup moins évidente. Et c'est là précisément que les objets frontières jouent un rôle central. Dans le domaine des sciences géomatiques par exemple, les « objets frontières » participent, tout comme le concept de « médium-humain », de l'amélioration des interactions entre les TIG -technologies de l'information géographique -et le contexte social d'un projet. Harvey et Chrisman (1998) apportent un éclairage intéressant, qui montre la pertinence des « objets frontières » pour comprendre la problématique d'adoption des TIG dans les contextes de diagnostic de territoire : « The part of social-constructivist thinking we specifically mobilize to examine GIS technology is a concept known as boundary objects. This concept articulates the process through which technology becomes part of different social groups, and how technology successfully connects multiple, even opposing, perspectives. Boundary objects provide coherence by linking multiple social groups through the stabilizations of facts and artefacts ».
Ces deux concepts se situent donc à la croisée des exigences qu'impose la production d'informations sur un territoire donné, pour appuyer une opération de diagnostic d'une part, et l'assimilation de ces connaissances par les acteurs impliqués dans le diagnostic d'autre part. Ces deux concepts posent les bases de GEOdoc : faciliter l'accès à des documents géo--48 -RNTI-E-13 graphiques multiformes par des acteurs locaux aux profils variés, mais aussi organiser et situer ces documents à l'intérieur d'un réseau complexe d'acteurs. En effet, un document géographique n'est assimilable que s'il est « positionné » dans son contexte et mis en perspective par rapport aux autres documents produits. Ainsi, GEOdoc s'inscrit dans une démar-che globale. Il peut être consulté à toutes les étapes d'un diagnostic, de manière à trouver des indications sur le contexte d'élaboration d'un diagnostic de territoire.
Documents géographiques et réseaux d'acteurs
Difficile d'envisager un diagnostic de territoire sans porter une attention particulière au rôle des réseaux d'acteurs. Une démarche de diagnostic de territoire doit se fonder sur une approche systémique du territoire, de manière à ce que soit pris en compte l'ensemble des relations et des interactions. Il est capital d'identifier les stratégies de chaque acteur, ainsi que les contraintes (règles et normes) auxquelles ils sont soumis (Bion 2003). Cette approche globale permet de prendre en compte le système de jeux de pouvoirs qui s'exerce au sein d'un territoire et qui modifie soit directement, par des actions politiques, soit indirectement par une communication des acteurs, le point de vue de la population. Si les interactions entre acteurs sont en quelque sorte le carburant d'un diagnostic, elles constituent aussi une source de blocage dans les négociations et d'échec dans l'aboutissement du diagnostic. Cet ensemble complexe d'interactions a fait l'objet de nombreuses études en sociologie. Dans cet article, nous nous intéressons plus particulièrement à l'approche, connue sous le nom de « Actor Network Theory -ANT », développée en particulier par Bruno Latour. Martin (2000) décrit l'ANT comme: « a dynamic and unusual approach towards technology and the varied roles it plays in influencing society ». Cette approche s'appuie sur des notions comme les « objets frontières » et propose de traduire l'impact des technologies sur des groupes sociaux. Cette théorie renouvelle la conception des réseaux d'acteurs, en présentant les acteurs et les ré-seaux comme deux entités indissociables (Latour 1999). Elle introduit les notions d'acteurs humains et d'acteurs non-humains. En effet, les interactions ne se font pas seulement entre des personnes. Elles sont reliées également à des objets dans un système complexe de dépen-dance, c'est la notion « d'hétérogénéité » des réseaux. Dans le domaine du diagnostic de territoire, on imagine immédiatement une transposition qui permettrait ainsi de repositionner les documents géographiques (les représentations spatiales par exemple) produits et mobilisés dans le réseau d'acteurs impliqués. Dans l'approche ANT, tous les acteurs (humains et non-humains) sont équivalents, du point de vue fonctionnel. D'ailleurs, ce n'est pas tant la structure et la forme des relations qui est étudiée, mais leurs natures -les interactions et les effets entre acteurs (Martin 2000). L'approche ANT permet donc une certaine liberté dans la conception de réseaux et n'impose pas une conceptualisation de référence.
Nous l'avons précisé plus haut, les documents géographiques sont le carburant essentiel d'un diagnostic de territoire, en même temps qu'ils en constituent des éléments matériels de production. La variété et la diversité des documents mobilisés et produits au cours d'un diagnostic imposent, dans la perspective de concevoir un outil comme GEOdoc, de les structurer mais aussi d'en garantir un accès facilité par la mise à disposition des renseignements pertinents à leur évaluation (métadonnées). Ces deux conditions constituent deux autres contraintes de spécification auquel GEOdoc doit répondre.
-49 -RNTI-E-13 GEOdoc Une autre contrainte de spécification est imposée par la prise en compte de l'approche ANT. En effet, les interrelations étroites qui lient différents documents géographiques porteurs de connaissances sur un territoire donné (les non-humains), mais aussi les liens qui existent entre ces documents et les acteurs impliqués (humains) dans les dynamiques territoriales forment le réseau d'acteurs. C'est précisément ce réseau complexe que GEOdoc doit donner à voir et à comprendre. C'est dans ce réseau qu'il doit permettre de naviguer. C'est en référence à ce réseau, qu'il doit permettre de consulter et de visualiser les documents pertinents. La consultation et la visualisation sont en effet deux dimensions fondamentales de GEOdoc, mais sans une mise en perspective des documents, sans leur référencement au ré-seau d'acteurs (ANT), ces fonctions se révèlent insuffisantes pour répondre aux exigences des processus de diagnostic de territoire. Car c'est bien la compréhension des relations qui lient les documents géographiques aux acteurs moteurs d'un territoire, qui permet de caracté-riser et d'identifier le territoire et ses dynamiques.
Cette exigence impose en particulier que les documents géographiques soient caractérisés aussi bien par des données (géométriques, descriptives et graphiques) que par des métadon-nées. Compte tenu de l'importance des métadonnées pour formaliser le réseau d'acteurs d'un territoire selon l'approche ANT, c'est-à-dire de pouvoir y inclure les acteurs non-humains (les documents géographiques en particulier), il nous a paru fondamental que le prototype de GEOdoc prenne en compte les normes de métadonnées qui existent dans les domaines de l'information géographique et de la gestion des connaissances. Nous avons ainsi étudié les normes Dublin Core, le Content Standard for Digital Geospatial Metadata, la norme Global Information Locator Service ou encore la norme de l'International Standard Organization TC/211. Toutes ces normes fournissent de nombreuses informations sur les données géogra-phiques qui peuvent être échangées ou consultées. Une partie seulement de tous ces renseignements a été retenue pour GEOdoc. En effet dans le contexte du support au diagnostic de territoire, les métadonnées doivent principalement aider les acteurs locaux à identifier la pertinence d'un document et les liens qui le caractérisent (liens avec les autres documents, mais avec les acteurs également). Les normes précédemment citées ont donc servi de réfé-rence pour la conception du modèle de base de données (qui structure les documents géogra-phiques) sur lequel s'appuie GEOdoc (cf. la section suivante).
Une autre composante fondamentale de GEOdoc est la définition d'un thésaurus -une ontologie très simplifiée -permettant de classer les documents par thèmes appropriés et d'en définir les modalités de manipulation. Sur le plan pratique on peut considérer qu'une ontologie est un ensemble structuré de concepts. Les concepts sont organisés dans un graphe dont les relations peuvent être : des relations sémantiques ou des relations de composition et d'héritage (au sens objet) ». Il s'agit donc d'une manière élaborée de manipuler un vocabulaire défini par avance. Dans le domaine géospatial, « An ontology of geographic kinds is designed to yield a better understanding of the structure of the geographic world, and to support the development of geographic information systems that are conceptually sound » (Smith and Mark 1998). Un thésaurus fait référence quant à lui à un classement en concepts définis dans lequel les relations entre concepts ne sont pas nécessairement spécifiées. Ontologie et thesaurus constituent deux moyens très performants de tri des connaissances et permettent par ailleurs de fournir à un utilisateur un cadre de référence lors de la recherche ou consultation d'un document.
-50 -RNTI-E-13
Intelligence collective et réseau ouvert
La navigation dans le réseau d'acteurs (ANT) et la visualisation des documents et des relations, ainsi que leur mise à jour dans la base de données constitue un autre enjeu pour GEOdoc. Plusieurs concepts intéressants dans le contexte du diagnostic de territoire, ont été développés au cours des dernières années, même s'ils sont encore peu utilisés dans les outils de recherche ou de consultation de l'information.  
Conception de la maquette de GEOdoc
Les concepts et notions développés plus haut constituent les bases de fonctionnement de GEOdoc. Nous présentons ici les caractéristiques principales. Nous avons en effet réalisé une première maquette (comme preuve de concept) de manière à illustrer ce que pourrait être un outil intégrant ces concepts novateurs comme support du diagnostic de territoire.
Le travail réalisé à propos des métadonnées et des ontologies constitue les guides à partir desquels la structure de la base de données de GEOdoc est conçue. Cette structure vise principalement à faciliter la recherche d'un document ou d'un acteur. Des options de recherche sont proposées de façon à pouvoir accéder le plus rapidement possible à l'information recherchée. Le modèle de base de données proposé permet également de structurer les documents et les acteurs en un réseau d'objets (au sens de l'ANT) dans lequel l'utilisateur peut aisément naviguer. Il favorise ainsi la compréhension des interactions entre les différentes composantes des dynamiques du territoire sur lequel porte le diagnostic. Enfin, avant de consulter un document, il est possible d'accéder à certaines métadonnées du document (date de création, format, description, utilisateur…).
Il est important de rappeler que GEODoc est destiné à des utilisateurs peu habitués à manipuler de l'information géographique. La conception a donc été guidée par un souci constant de ne pas surcharger le modèle avec des données trop complexes à interpréter. La majorité des attributs de la base de données sont par exemple inspirés de la norme Dublin Core car ils sont universels et simples d'utilisation. Pour les documents spécifiquement géogra-phiques (représentations spatiales), les attributs (description spatiale des données) sont issus des normes spécifiques du domaine (ISO en particulier). L'utilisateur peut par exemple obtenir la localisation d'un document par ses coordonnées géographiques ou par un index géo-graphique (« gazetteer »). Des informations génériques sur la projection cartographique sont également disponibles (pas de donnée détaillée sur l'ellipsoïde de référence par exemple).
Concernant le design d'interaction, les documents et les concepts ont été organisés selon un réseau favorisant la navigation. Les liens qui relient par exemple les entités documents, concepts et acteurs doivent être générés à la volée lors de l'affichage du réseau, ce qui suppose qu'ils soient déjà implémentés dans la base de données. La conception de la base de données s'appuie également sur la définition d'un thésaurus qui permet de classer les docu--52 -RNTI-E-13 ments et de caractériser les relations entre les objets. Ainsi, l'architecture de la base de données facilite l'organisation et la recherche des documents ou des acteurs. Dans une perspective opérationnelle, il serait souhaitable que la création du thésaurus permettant de répertorier les documents soit réalisée conjointement par des experts et utilisateurs (décideurs locaux) afin d'être adaptée aux besoins et au contexte spécifique.
Le modèle de données est conçu en UML (cf. les extraits du diagramme de classe de la Figure 2), il a été conçu à l'aide de Perceptory (http://sirs.scg.ulaval.ca/perceptory/). La classe « Document géographique » est la classe centrale du modèle conceptuel. Elle regroupe les informations générales sur les documents comme le titre, le sous-titre, la description, le ré-sumé… mais aussi l'index géographique. Toutes les autres classes s'articulent autour de celle-là. Par exemple, la classe « Acteur » lui est reliée avec une classe d'association afin de caractériser plus précisément la relation entre le document et l'acteur.
Il est ainsi possible de savoir si un acteur est producteur, utilisateur ou éditeur de la donnée (ou plusieurs choses à la fois). La liste n'est d'ailleurs pas exhaustive et peut être complétée selon les besoins. On remarque également que les deux classes précédemment citées sont reliées à elles-mêmes afin de densifier le réseau et de faciliter la navigation entre les documents. Il est donc possible de consulter tous les documents qui ont un lien avec le document visualisé ou encore de voir tous les acteurs qui interagissent avec un acteur donné. Il est également possible de consulter le lien qui unit les acteurs et de savoir ainsi quels sont les acteurs qui travaillent ensemble sur un projet, quels sont ceux qui sont localisés dans la mê-me région, quels sont ceux qui entretiennent des relations privilégiées (figure 2).
Ainsi l'organisation graphique du réseau d'acteurs (ANT) est supportée par l'architecture de la base de données. Les documents sont classés selon trois classes : « Domaine », « Thème » et « Sous-thèmes ». Avec les deux classes « Document géographique » et « Acteur », ces trois classes constituent le squelette de l'organisation du réseau. À travers ces trois concepts, il est ainsi possible de naviguer dans l'espace des documents. Enfin de manière à optimiser la recherche et la navigation dans les documents, nous avons procédé à la spéciali-sation de la classe « Document géographique ». Cette classe regroupe des documents qui se différencient à la fois par leur forme et par leur fond. Nous avons choisi de la spécialiser en créant sept « sous-classes » qui décrivent chacune un type particulier de document géogra-phique. On peut ainsi associer des attributs spécifiques aux sous-classes qui caractérisent plus précisément certains types de documents. L'un des avantages de GEOdoc repose sur la structure évolutive de sa base de données. À l'image des réseaux ouverts, les utilisateurs pourront proposer les informations supplémentaires qu'ils souhaitent voir apparaître lors de la navigation dans le réseau. Un premier pas vers une forme d'intelligence collective au service du diagnostic de territoire.
-53 -RNTI-E-13
GEOdoc
FIG. 2 -Extraits du modèle de données de GEOdoc
Fonctionnalités de GEOdoc : cas d'usage type
Les fonctionnalités de GEOdoc se déclinent ainsi selon trois catégories principales : (1) la recherche de documents ou d'acteurs (les objets) à l'intérieur du réseau (ouvert), (2) la navigation dans ce réseau, et (3) l'enrichissement du réseau sur le principe de l'intelligence collective. De manière à illustrer le fonctionnement de GEOdoc, nous proposons dans les sections suivantes d'explorer un cas d'utilisation représentatif et de l'expliciter en parcourant ces familles de fonctionnalités. À ce stade du projet, ce cas n'a pas fait l'objet d'un test formel. Ces étapes de tests fonctionnels fera l'objet d'une deuxième phase du projet. 
Plaçons nous dans
La recherche spatialisée de documents
FIG. 3 -Ecran de recherche de la maquette de GEOdoc
La première fonctionnalité dépend principalement de l'organisation de la base de données. Elle doit néanmoins permettre à l'utilisateur d'effectuer une recherche sur des critères géographiques et/ou sémantiques (figure 3). La recherche sur des critères géographiques permet de sélectionner le territoire d'intérêt grâce à des outils simples de navigation cartographique ou bien à l'aide d'une requête tout aussi simple sur un localisant (un nom de lieu ou les coordonnées d'une zone) -dans notre cas, le chargé d'étude choisi de sélectionner la ville de Québec et plus spécifiquement la basse ville de Québec. Lors de la première utilisa--55 -RNTI-E-13 GEOdoc tion, nous faisons l'hypothèse que la base de données a été peuplée, dans une version minimale, par le chargé d'études impliqué dans le projet. Il est intéressant en effet de pouvoir restreindre le réseau aux documents ou aux acteurs pertinents pour le diagnostic à réaliser. L'utilisateur accède alors à une information ciblée, ce qui limite les pertes de temps liées à la lecture de résultats non pertinents. Les utilisateurs ont le choix entre différents types de recherche et peuvent, au besoin, accéder à un document ou un acteur en particulier.
La navigation dans le réseau « géo-localisé » (documents et acteurs)
La navigation dans le réseau repose sur les concepts étudiés plus haut dans l'article. Elle est accessible après avoir réalisé une recherche sur des documents ou des acteurs -dans notre cas d'utilisation, le chargé d'études a lancé une recherche de documents cartographiques relatifs à la répartition spatiale de la population par âge (figure 4). L'affichage du réseau dépend de l'option de recherche choisie par l'usager. On peut décomposer le réseau en trois composantes (figures 4 et 5) :
-la première composante renvoie à une formalisation de l'organisation du territoire sur lequel porte le diagnostic autour de thèmes et de sous-thèmes, permettant ainsi de classer les documents (basé sur un principe d'ontologie), -la deuxième composante renvoie aux documents géographiques qui sont rattachés aux différents sous-thèmes, -la troisième composante représente les acteurs qui sont en interaction avec le territoire d'étude par l'intermédiaire des documents géographiques.
FIG. 4 -Ecran de navigation (1) de GEOdoc
-56 -RNTI-E-13
Il n'y a aucune discontinuité entre chacune des composantes, mais une symbolisation graphique adaptée permet de distinguer les objets. Ces objets peuvent être des concepts, des documents géographiques ou bien des acteurs. Les concepts sont rattachés à la notion de thésaurus ou d'ontologie. Ils permettent à un utilisateur d'accéder à une classification des documents selon une structure du type : Domaines Thèmes Sous-thèmes. Il s'agit là des balises qui forment le cadre de référence et aident un utilisateur à se situer dans le réseau (qui peut être complexe selon le nombre de documents). Cette classification a été choisie de façon à fournir une première organisation. Pourtant, rien ne dit que c'est la plus adaptée à l'utilisation que souhaitent en faire les acteurs. Il conviendrait sans doute d'établir une structure en accord avec les acteurs et décideurs locaux pour construire une ontologie plus adaptée à des projets de territoire qui regroupent de plus en plus de domaines.
Chaque fois que l'utilisateur se déplace dans le réseau, ses actions sont sauvegardées dans un historique. Ainsi, il lui est possible de reconstruire son cheminement (un peu à l'idée des itinéraires méthodologiques développés par Lardon et al. (2001)), et de revenir sur un document ou un acteur qui aurait attiré son attention. L'objectif consiste ici à fournir à l'utilisateur les conditions qui lui permettront de se construire une vision claire du territoire sur lequel porte le diagnostic. L'affichage des relations entre les concepts, les documents et les acteurs permet de mettre en perspective les connaissances disponibles.
L'intérêt fondamental de la navigation dans le réseau repose sur la possibilité de consulter les informations sur les documents et les acteurs -ainsi dans notre cas, le chargé d'études peut immédiatement basculer depuis le document issu de la recherche initiale vers les acteurs reliés à ce document. Ces renseignements sont affichés sur la fenêtre de droite (figures 4 et 5), dès que l'utilisateur clique sur un document ou un acteur dans la fenêtre centrale. Il peut alors avoir une idée du contenu du document avant même d'y accéder physiquement. L'utilisateur peut également choisir les données qu'il souhaite voir présentées ; par exemple s'il souhaite savoir avec quel logiciel il peut lire un document donné, il choisit d'afficher le champ dans le cadre texte de droite. Il en est de même avec les informations sur les acteurs. Cette option permet de personnaliser l'interface et d'adapter GEOdoc aux besoins de l'utilisateur.
Lors de l'affichage des noms des documents, chaque objet est caractérisé par un pictogramme qui indique le type auquel il est rattaché (multimédia, image, représentation spatiale géoréférencée, etc.). Cette option visuelle permet de connaître rapidement les caractéristiques du document (figure 5). Lors de l'affichage des relations entre un document et les acteurs ou des relations entre les acteurs eux-mêmes, un symbole graphique (points de couleurs) indique le type de relation. L'usager doit pour cela cliquer sur le symbole. Ces symboles permettent non seulement d'alléger l'affichage graphique mais aussi de renseigner sur l'importance de la relation. Ainsi, plus un acteur entretient de relations avec un autre acteur, plus le symbole est foncé. Il est alors facile de voir, avec cette pondération, comment s'organise et se structure le réseau d'acteurs. Il est aussi possible pour un même acteur de visualiser soit les acteurs qui interagissent avec lui, soit les documents géographiques qui lui sont liés (figure 5).
-57 -RNTI-E-13
GEOdoc
FIG. 5 -Ecran de navigation (2) de GEOdoc
Enrichissement du réseau par l'utilisateur
Outre l'attention accordée à la simplicité et à l'ergonomie de l'interface de GEOdoc, les recherches sur les nouveaux concepts d'intelligence collective et de réseaux ouverts nous ont poussés à réfléchir à la problématique d'appropriation de cet outil par les utilisateurs potentiels. Ainsi, des possibilités sont offertes aux usagers de concevoir éventuellement l'organisation même de l'outil, ou pour le moins de contribuer explicitement à son évolution et à la spécification de certaines fonctionnalités. Il est possible par exemple d'ajouter des commentaires à une donnée afin d'apporter des renseignements qui ne figureraient pas dans la base de données. Chaque utilisateur peut ainsi faire un commentaire qui sera ensuite réfé-rencé et consultable par l'ensemble de la communauté d'usagers -dans notre cas, après exploration, le chargé d'étude se rend compte que les documents qu'il a trouvé relativement à la répartition de la population ne sont pas compatibles avec le niveau de granularité des questions soulevées par l'assemblée de quartier, il peut ainsi le noter directement par le biais de l'interface. Cette option peut constituer une forme de forum de discussion qui permettrait à chacun de s'exprimer sur les éléments de diagnostic.
La possibilité de faire évoluer la base de données est aussi une caractéristique fondamentale de GEOdoc. Les informations qu'elle contient sur les documents et les acteurs ne sont pas exhaustives et d'autres attributs de classes peuvent sans doute être proposés. Cela permettrait d'enrichir, grâce à l'expérience de chacun, une base de données collective et d'en améliorer à la fois le contenu et la structure. Il serait alors intéressant que des utilisateurs -58 -RNTI-E-13 S. Roche et al. puissent eux-mêmes ajouter, modifier ou supprimer des documents. Cette possibilité pose la question de la gestion de ces droits, un problème technique, en particulier, sur lequel nous n'avons pas travaillé dans le cadre de cette recherche. Sur ce point en particulier, l'implantation d'un moteur du type WikiSIG tel que proposé par Ciobanu et al. (2007) est la solution que nous envisageons de tester dans la phase 2.
GEOdoc sur ce point en particulier, se situe en marge des outils existants. Il n'a pas pour objectif de proposer une aide directe à la réalisation de nouvelles analyses, ni même de proposer des solutions à un problème territorial à partir de critères prédéfinis. Il vise plutôt à faciliter deux aspects très importants du diagnostic de territoire : l'accès à des documents géographiques de formes très diverses d'une part, la consultation de ces documents et leur mise en perspective dans le réseau qui forme le territoire d'autre part. Le simple fait de visualiser une liste de documents n'est pas suffisant pour comprendre les caractéristiques et les dynamiques d'un territoire. Il faut mettre les documents en relation et les inscrire dans un réseau d'acteurs complexe. C'est donc à partir de la structure de la base de données et des fonctionnalités de l'outil que ces buts peuvent être atteints. Il ne faut pas perdre de vue ici que l'objectif principal de GEOdoc est de faire participer les utilisateurs. Il s'agit de leur permettre d'ajouter, de modifier ou de supprimer un document ; de les faire acteurs de la construction d'une intelligence territoriale collective. Cette solution est assimilable aux concepts de Peer to Peer utilisés par d'autres logiciels. Elle a l'énorme avantage de fonctionner sur une base de données mise à jour constamment et une richesse de documents très importante. Cette solution pour être parfaitement efficace, suppose que GEOdoc fonctionne sur Internet -dans notre cas, le chargé d'étude peut informer en temps réel le conseiller de ce qu'il a trouvé en l'invitant à se connecter. Tous les acteurs et plus généralement toutes les personnes intéressées pourraient alors avoir accès à des informations diverses et variées sur un territoire, disposant ainsi des supports nécessaires pour contribuer au diagnostic.
Conclusion
Le diagnostic de territoire repose sur un processus de concertation entre acteurs rattachés à des groupes sociaux divers et variés. Ce processus permet de préparer une action de déve-loppement. Il met en jeu de nombreux documents géographiques dont l'analyse et la synthèse débouchent sur des prises de décisions. Il nécessite également une méthodologie éprouvée qui associe des concepts d'analyse à l'expérience pratique. La difficulté de la dé-marche réside dans la nécessité d'aboutir à un consensus. Il faut en effet concilier d'une part les visions de chaque acteur sur le territoire, et d'autre part leurs objectifs dans le projet de territoire. Une discussion sur les atouts et les faiblesses d'un territoire nécessite en outre une connaissance approfondie de ce dernier. Or, c'est à ce niveau-là que les acteurs impliqués connaissent parfois quelques difficultés : méconnaissance ou inaccessibilité des documents pertinents produits sur un (ou à propos d'un) un territoire donné.
En réponse à cette problématique, nous avons conçu une maquette d'outil de navigation et de visualisation de documents géographiques d'un nouveau type. GEOdoc n'a pas pour but de proposer des outils d'analyses de données géographiques ou un support direct à la prise de décision. Son objectif central est de rendre accessible et lisible aux acteurs impliqués dans un diagnostic de territoire le réseau dont ils font partie. GEOdoc offre des fonctionnali--59 -RNTI-E-13 6 Références

Introduction
L'étude de la migration des termes, en particulier de l'évolution des données relationnelles issues de la synthèse de grands corpus d'information est un aspect majeur dans l'ingénierie de la connaissance et en particulier dans le cadre de la veille. Dans ce contexte, le recours à la visualisation de données par des graphes apporte un réel confort aux utilisateurs, qui, de façon intuitive, peuvent s'approprier une forme de connaissance difficile à décrire autrement. Bien souvent, ces graphes sont trop complexes pour être étudiés dans leur globalité, il faut alors les décomposer de manière à faciliter la lecture et l'analyse des données. Une première simplification du graphe est réalisé par le biais de la classification en un graphe réduit dont les sommets représentent chacun un groupe distinct d'acteurs ou de termes du domaine. D'autre part, la décomposition en graphes de périodes simplifie la structure de la représentation, en prenant en compte la dimension temporelle.
Dans ce contexte, le prototype VisuGraph, module de la plate-forme de veille stratégique Tétralogie, permet déjà la visualisation de données évolutives. Basée sur une visualisation globale, toutes périodes confondues, puis individuelle, les données sont représentées sous forme de sommet dont les coordonnées traduisent les caractéristiques temporelles.
Ce prototype est aussi doté de la classification interactive de données relationnelles, basée sur une technique de Markov Clustering, qui permet à la fois d'obtenir des classes homogènes et le graphe réduit dont les sommets sont les classes obtenues. Nous l'avons aménagée pour pouvoir intervenir sur le nombre de classes (augmentation, diminution), mais celui-ci reste assez aléatoire. Nous proposons alors de prendre en compte la dimension temporelle afin d'analyser l'évolution des différentes classes de données obtenues par l'algorithme MCL au cours du temps. Cette technique prend en compte la topologie du graphe (diamètre, centralité, adjacence, flux, …) dans un contexte évolutif, tout en s'appliquant à une métrique algébrique classique : la distance.
Dans la première partie de cet article, la plate forme de veille Tétralogie est présentée, en mettant l'accent sur l'extraction et le traitement des données. Puis, le module de représenta-tion graphique VisuGraph est proposé. Dans la section 3, nous détaillons notre proposition, en développant l'analyse évolutive des classes de données, que nous expérimentons. Enfin, dans la dernière section, nous concluons sur nos travaux et présentons nos perspectives.
Présentation de Tétralogie
Tétralogie (Dousset et al., 1988) est un logiciel de macro-analyse de données textuelles semi-structurées intégrant la dimension temporelle. Les données analysées par la plateforme Tétralogie sont issues de bases de données, de revues, de journaux, de périodiques, de revues de veille technologique, des thèses et de brevets ou encore de CDROMS. Les informations extraites de ces sources sont synthétisées sous forme de matrices de cooccurrence, exploitables dans les différents modules proposés par Tétralogie.
Les unités de base de toute analyse sont le terme, le champ (auteur, mots-clefs, adresse, date, ...) et le document. Un champ est une balise prédéfinie de la base de donnée semistructurée, par exemple auteur, date, adresse, organisme. Un champ, peut être mono-valué (journal) ou multi-valué (auteur, mot-clef,…). Un terme est une unité textuelle correspondant au contenu d'un champ mono-valué ou une partie d'un champ multi-valué délimité par des séparateurs. Les données analysées peuvent être croisées entre deux champs, sous champs ou groupes de champs afin d'obtenir des matrices de fréquence, de présence/absence ou encore de co-occurrence (une des variables peut contenir plusieurs champs : auteurs, mots, clés…) sur lesquelles porteront ensuite les analyses. Dans le cas de données évolutives, un champ relatif au temps est pris en compte. Dans un contexte temporel, on aura autant de matrices de croisement que de périodes (appelées aussi « instances ») étudiées. Chaque croisement au sein d'une matrice est appelé « valeur de métrique » d'un ou plusieurs champs. Certaines informations sont sémantiquement équivalentes ou hiérarchisées, aussi est-il très utile de disposer de la fonctionnalité de Tétralogie permettant de radicaliser les données en les transformant en données simples, de les nettoyer mais aussi de normaliser ou d'homogénéiser les termes (adresse, organisme, date), tout en contrôlant qu'il n'y ait pas d'ajout ou encore d'oubli de caractère au sein de chaque terme .
3 La visualisation évolutive de données relationnelles 3.1 La prise en compte de la dimension temporelle Le prototype VisuGraph est un module de visualisation, offrant à la plateforme Tétralogie la possibilité de représenter les données matricielles sous forme de graphe. Dans cet article, nous ne traitons que des graphes non orientés. Considérons le graphe non orienté G = (S,A) où S est l'ensemble fini des éléments appelés sommets ou encore noeuds. A est l'ensemble fini des liens appelés arêtes, liant les sommets. A ? S x S = ?(s,t) ? s,t ? S? Chaque sommet est assimilé à la valeur de la métrique d'un seul champ alors que chaque lien correspond à la valeur du croisement de deux champs. Afin d'obtenir un graphe planaire, sans qu'aucune arête n'en croise une autre, nous avons recours à l'algorithme « force-directed placement » (FDP) (Eades, 1984), assimilant les sommets à des masses et chaque arête d'un graphe à un ressort reliant les sommets. Un tel système engendre des forces entre les sommets, ce qui entraine des déplacements respectifs. Après une phase de transition le système se stabilise. La condition d'arrêt est un nombre maximum d'itérations.
Dans ce cas d'analyse temporelle, un graphe global représente toutes les données, toutes périodes comprises, puis chaque graphe de période est visualisé individuellement, réalisant ainsi une animation. Dans ce contexte évolutif, nous attribuons un sommet virtuel (non visible dans le dessin mais dont la présence est prise en compte dans le graphe) qui servira de repère pour chaque période considérée. Ces sommets virtuels sont fixés dans un ordre chronologique et de façon équidistante sur le contour de la fenêtre de visualisation (comme les heures sur un cadran) (Loubier2, 2007). Le dessin de graphe est influencé par l'attribution de nouveaux arcs reliant chacun des sommets aux repères temporels, qui le concernent, en leur attribuant un poids plus important que la valeur de la métrique d'arête la plus grande. Ceci engendre un déplacement, vers certains repères, en fonction de la plus ou moins forte pré-sence d'un sommet dans chaque période.
Simplification sous forme de graphe réduit
Afin de faciliter l'analyse, les données les plus fortement liées doivent être regroupées en classes homogènes. Parmi les travaux effectués sur le partitionnement de graphe, (Alpert et al. 1995, Kuntz et al. 2000, Jouve et al. 2001) se basent sur des approches spectrales alors que les algorithmes de la famille METIS (Karypis et al. 1998) se basent sur le partitionnement multi niveaux. La méthode de partitionnement utilisée dans VisuGraph est inspirée du Markov Clustering (Van Dongen 2000) que nous avons aménagée pour pouvoir influencer le nombre de classes proposées (Karouach, 2003). Cette approche calcule des probabilités de transition entre tous les sommets du graphe en partant de la matrice de transition des marches aléatoires. Deux simples opérations matricielles sont successivement itérées. La première calcule les probabilités de transition par des marches aléatoires de longueur fixée r et correspond à une élévation de la matrice à la puissance r. La seconde consiste à amplifier les diffé-rences en augmentant les transitions les plus probables et en diminuant les transitions les moins probables. Les transitions entre sommets d'une même communauté sont alors favorisées et les itérations successives des deux opérations conduisent à une situation limite dans laquelle seules les transitions entre sommets d'une même communauté sont possibles.
Soient un graphe G = (V,w) où w : V x V AE R + et M G la matrice associée à G. T G est la matrice normalisée (somme des poids des arcs sortants = 1). Soit T pq la probabilité d'effectuer la transition p AE q(T = T G ). L'expansion s'effectue par multiplication des matrices, visant à élargir la capacité de l'arc entre deux noeuds. L'inflation d'une colonne vise la promotion des voisins favoris au détriment de ceux moins favoris.
L'évaluation de la méthode MCL a montré la rapidité et la qualité de ses résultats (Enright et al. 2002). Le graphe obtenu est alors un graphe de classe, pour lequel chaque sommet représente une classe. Les liens entre les sommets sont assimilés aux liaisons interclasses. Dans un second temps, l'attribution d'une couleur spécifique à chaque classe permet de visualiser le graphe complet, en figeant un représentant par classe et en distribuant les autres sommets sur une couronne centrée sur ce dernier, permettant ainsi une vue intra classe.
FIG. 1 : Visualisation du graphe global des classes, puis visualisation par animation des graphes de classe par période (« Rep n » ÅAE repère temporel de la nième période).
Conclusion
Davantage qu'une simple recherche, la veille consiste à recueillir l'information, à la synthétiser et à tirer des conclusions aidant à la prise de décision en anticipant les tendances. L'outil VisuGraph permet le prétraitement des données, en particulier dans la gestion de la synonymie, et la synthèse des données relationnelles sous forme de matrices de cooccurrences décomposées en périodes homogènes. En se basant sur ces matrices, les données peuvent être représentées dans un premier temps, sous forme de graphe de classes global, toutes tranches de temps confondues, puis dans un second temps successivement sous forme de graphe de période. De par la prise en compte de la dimension temporelle, la position des sommets du graphe est stratégique et spécifiques aux caractéristiques liées au temps, telles que l'appartenance à certaines périodes et non à d'autres. Ainsi, chaque portion de la fenêtre de représentation correspond à une typologie relative au temps particulière. Cependant les limites de cette proposition concerne la méthode de classification choisie, le Markov Clustering MCL, qui ne prend pas en compte le point de vue de l'utilisateur, et, dans certains cas, le RNTI -X -partitionnement est soit trop fin soit trop grossier (à la limite une seule classe est trouvée). Il conviendrait d'offrir à l'utilisateur la possibilité d'intervenir dans la classification par une analyse visuelle des regroupements qui s'opère lorsqu'on joue sur les paramètres permettant de dessiner au mieux le graphe.
Références

Introduction
L'extraction d'itemsets fréquents est une problématique de recherche qui intéresse la communauté fouille de données depuis plus d'une dizaine d'années et intervient pour la recherche de règles d'association, de motifs séquentiels ou encore d'itemsets maximaux. Les premiers à traiter cette question furent Agrawal et Srikant (1994), ils ont été suivis en ce sens par Han et al. (2000). Traditionnellement, les différents algorithmes proposés dans la littérature reposent sur des structures de données de type arbre ou encore treillis (e.g. : A-priori (Agrawal et Srikant, 1994), F P-growth (Han et al., 2000), . . . ). La problématique de recherche de motifs (i.e., une généralisation des itemsets) apparaît dans des domaines aussi variés que la bioinformatique ou la fouille de textes. En ce qui concerne ce dernier, de nouvelles structures de données, basées sur des automates sont apparues afin d'extraire les sous-séquences communes à une ensemble de textes (Troní?ek, 2002). Par exemple, Hoshino et al. (2000) ont introduit, un nouvel automate déterministe et acyclique : le SA (Subsequence Automaton) qui permet de reconnaître toutes les sous-séquences d'un ensemble de textes. L'un des problèmes principaux auxquels doit faire face une approche d'extraction de motifs est de disposer de structures qui soient suffisamment compactes et informatives afin de minimiser l'explosion combinatoire liée à d'importants espaces de recherche. En effet, l'applicabilité des algorithmes proposés peut être remise en question en raison des coûts trop prohibitifs en temps de calcul et en espace mémoire utilisé. Le premier objectif de cet article est d'apporter une réponse à la question suivante : est-il possible de trouver de nouvelles structures de données, suffisamment informatives et compactes, pour extraire de façon efficace les itemsets fréquents ?
Récemment, pour faire face au fait que les données peuvent être disponibles sous la forme de flots qui arrivent de manière continue et sont éventuellement en quantités infinies, les chercheurs de la communauté fouille de données se sont intéressés à l'extraction de connaissance dans de telles conditions. De nombreuses applications (e.g. transactions financières, navigation sur le Web, téléphonie mobile, . . . ) rentrent dans ce cadre et nécessitent d'obtenir des résul-tats rapidement. Dans le cas de la problématique d'extraction d'itemsets fréquents, la prise en compte de données disponibles sous la forme de flots engendre de nouvelles problématiques. En premier lieu, il est indispensable de considérer les aspects liés à la mise à jour. En effet, étant donné que les données arrivent de manière continue, la base de données est sujette à des mises à jour régulières et fréquentes. Ainsi, la connaissance obtenue pour une base à un moment donné n'est plus forcément valable lorsque de nouvelles données arrivent et il n'est pas envisageable de relancer l'algorithme sur toute la base mise à jour. La maintenance de connaissance incré-mentale a, par exemple, été étudiée dans Masséglia et al. (2003) où les auteurs proposent de maintenir la connaissance au fur et à mesure des mises à jour successives. Malheureusement, tous les travaux de recherche basés sur une approche incrémentale ne sont pas adaptés aux flots dans la mesure où nous ne pouvons pas disposer de la base dans son intégralité. Il est donc né-cessaire, en second lieu, de disposer de nouveaux algorithmes qui effectuent une seule passe sur la base (i.e., des algorithmes une-passe). Les travaux récents sur les flots ont montré qu'il n'était plus envisageable d'obtenir une réponse exacte (i.e., les itemsets réellement fréquents sur le flot) et qu'il fallait accepter une approximation quant à l'estimation de la fréquence des motifs : les itemsets obtenus ne sont en fait que des itemsets fréquents observés. Il faut donc prendre en compte l'incertitude engendrée par la connaissance toujours incomplète du flot de données. Il est important de noter que dans les flots, des itemsets classés comme non fréquents peuvent le devenir sur une plus longue période d'observation et inversement, des itemsets observés fréquents peuvent ne plus l'être après un certain temps. Dans Vapnik (1998), l'auteur a montré qu'il est statistiquement impossible de s'affranchir de ces deux sources d'erreurs à partir de la connaissance d'une partie (même très grande) du flot. On peut toutefois chercher à minimiser l'une des sources d'erreurs tout en maintenant l'autre en dessous d'un seuil raisonnable. La seconde question à laquelle nous nous intéressons dans cet article est la suivante : est-il possible de trouver une structure de données ayant des propriétés incrémentales satisfaisantes et qui permette de construire et de la maintenir de façon efficace l'ensemble des itemsets fréquents du flot de données tout en minimisant l'une ou l'autre des sources d'erreurs ?
La suite de l'article est organisée de la manière suivante. La Section 2 présente plus formellement la problématique. Dans la Section 3, nous proposons un aperçu d'autres travaux abordant cette problématique. La Section 4 présente notre approche et nos solutions. Les expé-rimentations sont décrites dans la Section 5 et une conclusion est proposée dans la Section 6.
Problématique
Soit I = {i 1 , i 2 , . . . , i m } un ensemble d'items muni d'une relation d'ordre utilisés dans une base de données DB de transactions, où chaque transaction t r identifiée de manière unique est associée à un ensemble d'items de I. Un ensemble X ? I est appelé un p-itemset et est RNTI -E -2 représenté par {x 1 , x 2 , . . . , x p }. L'entier p = |X |, est la longueur de X et Sub(X ) l'ensemble de tous les sous-itemsets de X , c'est à dire les itemsets obtenus en supprimant zéro ou plusieurs items de X . Le support d'un itemset X , noté supp(X ), correspond au nombre de transactions dans lesquelles l'itemset apparaît. Un itemset est dit ?-fréquent si supp(X ) ? ?, où ? = ?? × |DB|? correspond au support minimal (généralement spécifié par l'utilisateur) avec ? ?]0; 1] et |DB| la taille de la base de données. Le problème de la recherche des itemsets fréquents consiste à rechercher tous les itemsets dont le support est supérieur ou égal à ? dans DB. Cette problématique, étendue au cas des flots de données, peut s'exprimer comme suit. Soit un flot de données DS = B La fréquence d'un itemset X à un instant donné t est défini comme étant le ratio du nombre de transactions qui contiennent X dans les différents batches sur le nombre total de transactions connu à l'instant t. Ainsi, pour un support minimal fixé par l'utilisateur, le problème de la recherche des itemsets fréquents dans un flot de données consiste à rechercher tous les itemsets X qui vérifient bi ai supp(X ) ? ?? × k? dans le flot. Dans l'exemple de la Table 1  
Travaux antérieurs
Les différents travaux portant sur la problématique d'extraction d'itemsets fréquents dans les flots de données se déclinent selon trois axes en fonction du modèle de traitement des itemsets du flot. Le premier utilise des fenêtres à point fixe où sont conservés tous les itemsets acquis du flot (cf. Manku et Motwani, 2002;Li et al., 2004). Le second axe est différent du pré-cédent simplement par le fait que l'on introduit une distinction entre les itemsets récemment et moins récemment acquis. Chang et Lee (2004b) attribuent un poids décroissant aux transactions en fonction de l'ancienneté de leur acquisition. Autrement dit, les anciennes transactions contribuent moins que les nouvelles au calcul de la fréquence des itemsets. Par exemple, Giannella et al. (2004) utilisent une structure de type FP-tree pour rechercher des itemsets fré-quents à différents niveaux de granularité temporelle. Le dernier axe concerne l'extraction à partir de fenêtres glissantes où l'on ne considère plus seulement l'acquisition mais aussi le retrait d'itemsets (cf. travaux de Chang et Lee, 2004a). L'approche que nous développons dans cet article, s'inscrit dans le premier axe. Aussi, nous préciserons dans la suite de ce para-RNTI -E -3 graphe, les caractéristiques des algorithmes ainsi que l'erreur et les types d'approximation sur les résultats relatifs à cet axe. Manku et Motwani (2002) ont développé un algorithme : Lossy counting, basé sur la propriété d'antimonotonie du support. Cet algorithme effectue un seul passage sur les données et utilise une structure à base d'arbres pour représenter les itemsets. Les auteurs introduisent un paramètre d'erreur fixé par l'utilisateur et voulu très inférieur au support afin de minimiser le nombre de résultats faux positifs et afin d'améliorer la valeur de la fréquence obtenue des itemsets. Ils donnent les garanties suivantes sur leurs résultats : tous les itemsets réellement fréquents sont trouvés ; il n'y a pas de faux négatifs ; tous les itemsets considérés fréquents à tort (i.e., les faux positifs) ont une fréquence proche de la fréquence voulue ; l'incertitude sur la fréquence des itemsets est fonction du paramètre d'erreur. Li et al. (2004) proposent d'extraire les itemsets fréquents en partant des plus grands aux plus petits et utilisent une structure très compacte qui résulte d'une extension d'une représentation basée sur des arbres préfixés : le C F I-tree (Candidate Frequent Itemset tree). Toutefois, l'algorithme développé : D S M-F I, bien qu'effectuant un seul passage sur les données, comprend une phase d'élagage du C F I-tree et nécessite plusieurs parcours de la structure pour obtenir l'information sur la fréquence des itemsets. Les garanties apportées, quant aux résultats, indiquent qu'il n'y a pas de faux négatifs et que l'erreur sur la fréquence des itemsets est bornée.
Notre Approche
Dans un premier temps, nous nous intéressons à l'extraction des itemsets fréquents dans une base de données. Nous introduisons un nouvel automate : le FIA (Frequent Itemset Automaton), qui constitue une structure de données très compacte et informative permettant d'extraire de façon efficace tous les itemsets fréquents d'une base de données. Dans un second temps, nous étendons cette approche à la prise en compte des flots de données et nous montrons comment mettre à jour incrémentalement le FIA lors de l'ajout de nouveaux batches issus du flot. Cependant, pour tenir compte de l'incertitude engendrée par la connaissance toujours incomplète du flot, nous étudions la représentation de la bordure statistique à l'aide du FIA afin de développer une approche prédictive (Laur et al., 2007). En effet, plutôt que d'extraire des itemsets observés fréquents sur la partie connue du flot, nous considérons qu'il est préférable de prédire les itemsets véritablement fréquents sur tout le flot à partir des itemsets connus.
Rappels sur la théorie des automates
Nous présentons dans cette section, les principes fondamentaux de la théorie sur les automates finis (cf. Hopcroft et Ullman, 1990) qui seront utilisés dans la suite.
Définition 1. Un automate à états finis A est un quintuple tel que
Un mot est reconnu s'il est l'étiquette d'un chemin réussi. Le langage reconnu par l'automate A est l'ensemble des mots reconnus par A, soit L(A) = {w ? ?|?c : i w ? ? f, i ? I, l ? F }. Un état q ? Q d'un automate RNTI -E -4 A = (Q, ?, ?, I, F ) est accessible s'il existe un chemin c : i ? q avec i ? I. De même, l'état q est coaccessible s'il existe un chemin c : q ? f avec f ? F. Un automate est émondé si tous ses états sont accessibles et coaccessibles. Soit P l'ensemble des états accessibles et coaccessibles et soit A 0 = (P, ?, ? ? (P × ? × P), I ? P, F ? P), l'automate A 0 est émondé par construction. Comme tout chemin réussi de A ne passe que par des états accessibles et coaccessibles, on a L(A 0 ) = L(A). Les automates A 0 et A sont dits équivalents.
Définition 2. Un automate à états finis
Nous adaptons les Définitions 3 et 4 proposées initialement par Hoshino et al. (2000) pour l'automate des sous-séquences (SA), au cas des itemsets. 
Définition 3. Étant donnés un ensemble S d'itemsets tel que
L'automate des itemsets fréquents : le FIA
Dans cette section, nous introduisons la définition d'un nouvel automate : le FIA ? qui intègre la notion de fréquence et qui permet de reconnaître l'ensemble des itemsets fréquents d'une base de données.  
Définition 5. Étant donnés un ensemble
Il est aisé de constater que les états qui ne sont pas ?-satisfaisants ne sont pas coaccessibles (le contraire signifierait qu'un itemset non fréquent est inclus dans un itemset fréquent). Ainsi, le FIA ? émondé s'obtient en ne construisant que les états ?-satisfaisants accessibles. Un simple algorithme glouton permet de le construire et ne requiert en aucun cas une phase d'élagage. Le support d'un itemset étiquetant un chemin de l'état initial à un état q donné du FIA ? (S) s'obtient en calculant le nombre de valeurs qui ne sont pas égales à ? dans le ppos associé à q. Ainsi, un itemset étiquetant q 
S) correspondant, pour tout état q tel qu'il n'existe pas de transition vers un état q ? de même support, le plus long itemset reconnu en q est un itemset fermé (il est unique). Réciproquement, tous les itemsets fermés sont reconnus dans des états q tels qu'il n'existe pas de transition menant à un état q ? de même support.
En considérant le batch B 1 0 de la Table 1, nous montrons, sur la Figure 1, le FIA ? émondé pour ? = 0, 4 et donc ? = 2. Les états finaux sont repérés par un double cercle et l'état initial est représenté avec une flèche entrante sans label. L'itemset vide est reconnu à l'état initial (avec le support 5). Par ailleurs, nous observons que les itemsets abd, ad et bd avec une même valeur de support à 2, sont reconnus à l'état q 7 . Il en est de même pour les itemsets bce et ce de support 2 reconnus en q 10 et pour be et e de support 3 en q 5 . Le FIA ? , du fait de la Propriété 2 est une structure très compacte. En effet, seuls les états q 1 , q 6 et q 8 n'identifient pas un itemset fermé (la Propriété 2 donne les équivalences suivantes pour les itemsets fermés : q 0 ? ?, q 2 ? b, q 3 ? c, q 4 ? d, q 5 ? be, q 7 ? abd, q 9 ? cd et q 10 ? bce). Enfin, si l'on choisit la fréquence décroissante comme relation d'ordre ? sur les items, à l'instar de l'algorithme F P-growth, le F P-tree résultant compte 11 noeuds mais 10 états pour le FIA dans ce cas. 
Intégration des bordures statistiques dans le FIA
Appliquée au cas des flots de données la mise à jour du FIA requiert de connaître l'ensemble des états accessibles, y compris des états non ?-satisfaisants. En effet, mettre à jour le FIA émondé reviendrait à considérer que les itemsets non reconnus par l'automate ont tous un support à 0 ; ce qui engendrerait nécessairement un grand nombre de faux négatifs sur la totalité du flot. À l'inverse, en considérant les états non ?-satisfaisants, un grand nombre d'itemsets vrais négatifs seraient analysés inutilement. Afin d'illustrer cet aspect, considérons d'une part la représentation du FIA 40% émondé de la Figure 1 et d'autre part la représentation de la Figure 2 du FIA 40% non émondé avec tous ses états accessibles. Les états q 11 , q 12 , q 13 , q 14 ne sont pas 2-satisfaisants mais 1-satisfaisants et ne sont donc pas finaux. La question revient à savoir quel est l'automate qu'il convient de considérer pour effectuer la mise à jour du FIA. Il est donc nécessaire de trouver un compromis entre ne conserver aucun état non ?-satisfaisant 1 En pratique, les ppos ne sont pas construits, seul le support est calculé et mis à jour.
RNTI -E -7
Automate des Itemsets Fréquents: le FIA et tous les états accessibles. Idéalement, seuls les états qui correspondent à des itemsets vrais ?-fréquents du flot devraient être construits, quand bien même ils ne satisfont pas la contrainte de support à un instant donné. La solution que nous avons adoptée est d'utiliser la bordure statistique supérieure présentée par Symphor et Laur (2006), dans laquelle est maximisé le rappel (cf. Définition 7 et Théorème 1 ci-après). Le Théorème 1 ci-dessous permet d'établir la valeur du support statistique qui permet la construction de la bordure statistique supérieure.
Théorème 1. Étant donné un flot de données observé DS
les supports statistiques ?
Les fréquences obtenues (? ? = ?±?) sont statistiquement presque optimales (cf. Laur et al., 2007). Celle-ci repose sur l'utilisation d'inégalités de concentration de variables aléatoires, qui, dans ce cas précis, permettent d'obtenir un résultat statistiquement presque optimal. Par optimalité, nous entendons que toute technique d'estimation obtenant de meilleures bornes RNTI -E -8 est condamnée à se tromper (le critère à maximiser n'est plus égal à un) quel que soit son temps de calcul. Dans le cas de la bordure statistique supérieure correspondant à ? ? = ? ? ?, il s'agit de réduire autant que possible le nombre de faux négatifs à savoir les itemsets véritablement fréquents du flot et qui ne sont pas retenus comme tels pour la partie observée du flot. Lorsque seuls les états de la bordure statistique supérieure ont été conservés (i.e., les états ?(? ? ?) × k?-satisfaisants), l'automate obtenu après une mise à jour incrémentale est une approximation du FIA ? pour le flot observé (nous le noterons  
L'intérêt du calcul des P -valeurs est clairement illustré dans Denise et al. (2001). Ici, la P -valeur traduit littéralement que la probabilité qu'il existe un faux positif (i.e., un itemset de fréquence observée ? ? qui ne soit pas ?-fréquent sur tout le flot), est inférieure à ?.
RNTI -E -9
Automate Notre algorithme de construction du FIA est en une passe, mais pour la comparaison avec notamment F P-growth, nous avons utilisé une version deux-passes, le premier passage permettant uniquement de réordon-ner les items par ordre de fréquences décroissantes. Les résultats obtenus avec le FIA sont meilleurs sur une large plage de fréquence tant pour le temps pris que pour la mémoire requise. On observe effectivement, au-delà des valeurs de fréquence à 5%, un écart en temps de l'ordre de 3 sec. et de mémoire de 1M o en faveur du FIA, qui prend un temps total de 4 sec. et consomme 3M o par rapport à F P-growth. L'algorithme A-priori, qui effectue plusieurs passages sur les données, donne de meilleurs résultats seulement à partir des fréquences dépas-sant 45%. Toutefois, les performances du FIA sont dépendantes des données indexées. En effet, cette structure est d'autant plus avantageuse que les itemsets fréquents sont grands. En contrepartie, lorsque les itemsets fréquents sont, majoritairement des singletons, ce qui est le cas pour de très faibles fréquences, le FIA tend à ressembler à un arbre lexicographique et l'algorithme de construction devient inadapté au regard des méthodes classiques de construction de tels arbres. Le FIA est une structure d'autant plus efficace que les données sont denses en informations à extraire. Sur les Figures 6 et 7, nous illustrons les résultats obtenus avec l'algorithme du FIA incrémental en représentant le temps pris et la mémoire requise en fonction de l'insertion de nouveaux batches (nombre constant de transactions) pour T10I4D100K. Le temps pris et la mémoire consommée demeurent stables. Cela montre l'applicabilité de l'algorithme du FIA incrémental dans le cas des flots de données.
Conclusion
Dans cet article, nous apportons une contribution originale en élaborant un nouvel automate : le FIA qui permet de traiter de façon efficace la problématique de l'extraction des itemsets fréquents dans les flots de données. À notre connaissance, les automates en tant que structure de données n'ont pas du tout été utilisés pour aborder cette question. Nous montrons que le FIA est une structure très compacte et informative car plusieurs itemsets fréquents ayant la même valeur de support sont reconnus à un même état. Par ailleurs, la structure indexe directement tous les itemsets fréquents sans qu'il soit nécessaire de lui associer un tableau pour finalement obtenir les résultats. Le FIA présente également des propriétés incrémentales qui facilitent grandement la mise à jour dans le cas des flots de données avec une granularité très fine par batch. Utilisé dans le cadre d'une approche prédictive, le FIA permet d'indexer les itemsets véritablement ?-fréquents du flot en maximisant le rappel et en fournissant à tout moment une information sur leur pertinence statistique avec la P -valeur. Ces deux avantages permettent notamment de construire le FIA par incrément à partir d'une base initiale vide. L'algorithme développé, pour mettre à jour le FIA, ne requiert qu'un seul passage sur les données qui sont prises en compte par batch, itemset par itemset et pour chaque itemset, item par item. Lors de l'acquisition de nouveaux batches, la connaissance du flot augmentant, il est possible de mettre à jour la bordure supérieure. Celle-ci tend à diminuer (la valeur de ? tends vers 0, donc ? ? tends vers ?) au fur et à mesure des mises à jour. Il devient donc possible de construire le FIA ? à partir de l'automate vide, est les premiers résultats obtenus (non présentés dans cet article, faute de place essentiellement) montrent clairement la robustesse de notre approche. Ceci est également étayé par les expérimentations présentées, avec une analyse en temps de calcul et en mémoire consommée, qui donnent des résultats satisfaisants et qui prouvent l'applicabilité et le passage à l'échelle de l'algorithme. Notre contribution ouvre donc une voie prometteuse avec le FIA, quant à l'utilisation de nouvelles structures de données de type automate, dans les problématiques d'extraction de motifs fréquents dans les flots de données.
Références Agrawal, R. et R. Srikant (1994 

FIG. 1 -Variation des histogrammes de la variable « DIVERS »
FIG. 2 -Caractérisation de la classe 7 FIG. 3 -Représentation croisée des variables histogramme tarification et vague
FIG. 4 -Une nouvelle visualisation 2D et 3D pour les pyramides et hiérarchies

Introduction
Une base de données de grande taille est difficile à appréhender dans sa totalité. Pour palier ce problème, diverses techniques ont été créées afin de fournir des vues partielles ou d'effectuer des regroupements de données par thèmes. De façon similaire il est difficile de comprendre une base de connaissances. Plus une base de connaissance est grande, plus le nombre de connaissances utilisables afin d'effectuer une déduction est important. A partir d'un certain nombre l'humain ne peut plus évaluer toutes les connaissances mises en jeu dans une déduction. Il est donc nécessaire de diviser l'ensemble des étapes d'une déduction par paquets et de fournir à l'humain une évaluation de chaque paquet. Cette évaluation peut être imprécise mais facilite la compréhension en donnant l'idée générale. Pour notre étude nous nous intéressons à un modèle graphique de gestion de connaissances appelé cartes cognitives (Tolman, 1948).
Une carte cognitive représente un réseau d'influences entre concepts. Une influence est une relation de causalité entre deux concepts. L'effet de l'influence d'un concept sur un autre peut être représenté de manière numérique ou symbolique. Ce type de représentation fournit un bon support à la communication entre humains dans le but d'effectuer une analyse d'un système complexe. Les cartes cognitives ont été utilisées dans de nombreux domaines tels que la biologie (Tolman, 1948) (Touretzky et Redish, 1995), l'écologie (Celik et al., 2005) (Poignonec, 2006), la sociologie (Poignonec, 2006). Un mécanisme d'inférence des influences dans une carte cognitive peut être défini, ce qui en fait un outil d'aide à la décision. Ce type d'outils a été utilisé par exemple en politique et en économie (Axelrod, 1976) (Cossette, 1994). La repré-sentation informatique d'une carte cognitive et la mise en oeuvre d'un calcul automatique de l'inférence est relativement simple. L'objectif de ce travail est de faciliter la compréhension et l'exploitation de cartes cognitives de grandes tailles. Pour cela nous présentons un modèle de cartes cognitives permettant à l'utilisateur d'obtenir des vues partielles et synthétiques d'une carte.
Les vues partielles sont calculées à l'aide de regroupements de concepts préalablement défi-nis par le concepteur de la carte sous la forme d'une hiérarchie. Un regroupement de concepts est considéré comme une nouveau concept. Ce concept peut être choisi par l'utilisateur afin qu'il soit présent dans la vue partielle de la carte et ainsi remplacer l'ensemble des concepts qu'il regroupe. Lorsqu'il existe une influence entre deux concepts appartenant à deux regroupements différents et que les regroupements sont remplacés par leurs concepts correspondants, il est nécessaire d'ajouter une influence entre ces derniers dans la vue partielle. La représen-tation symbolique ou numérique de l'effet de cette influence doit fournir à l'utilisateur une idée de l'effet des influences qu'il existe entre des concepts de ces regroupements. Pour dé-terminer cet effet il est nécessaire de préalablement définir un mécanisme d'inférence entre deux regroupements de concepts. Ce mécanisme est inspiré d'un travail précédemment effectué (Genest et Loiseau, 2007) ainsi que de travaux ayant pour objectif de fusionner plusieurs cartes cognitives en utilisant des mécanismes de fusion de plusieurs influences en une seule (Axelrod, 1976)(Chaib-draa, 2002 (Fabiola Mata Avila, 2002) (Jung et al., 2003).
La première section de cet article présente un modèle standard de carte cognitive, dit modèle de carte cognitive simple et son mécanisme d'inférence permettant de l'exploiter. La seconde partie présente un mécanisme permettant à l'utilisateur d'interroger la carte afin de connaître l'influence d'un regroupement de concepts sur un autre. La troisième section pré-sente notre modèle de carte cognitive hiérarchique s'appuyant sur une hiérarchie de concepts qui détermine des regroupements de concepts. La quatrième section fournit le mécanisme d'inférence adapté aux cartes cognitives hiérarchiques. La dernière section décrit un mécanisme de visualisation partielle d'une carte cognitive hiérarchique.
Un modèle simple de cartes cognitives
Une carte cognitive simple est un graphe orienté dont les noeuds sont étiquetés par des concepts. Une concept est représenté par un texte. Un arc du graphe représente une influence, c'est à dire une relation de causalité possible entre deux concepts. Un arc porte un symbole caractérisant l'effet de l'influence qu'il représente. Typiquement une influence est positive ou négative.
Définition (Carte cognitive simple): Soit S un ensemble de symboles. Soit C un ensemble de concepts. Une carte cognitive simple définie sur C et S, est un multigraphe orienté étiqueté Un mécanisme de propagation de l'influence dans une carte cognitive permet de détermi-ner l'effet d'un concept sur un autre (Axelrod, 1976). L'influence propagée d'un concept sur un autre est calculée en fonction des chemins qu'il existe entre les noeuds étiquetés par ces concepts. On appelle ces chemins des chemins d'influence.
, une carte cognitive simple définie sur un ensemble de concepts C et un ensemble de symboles S. Soit c 1 , c 2 deux concepts de C. On appelle un chemin d'influence P entre c 1 et c 2 une liste de
Exemple: Sur la carte cognitive simple de la figure 1, ((´ etiq
Afin de connaître l'effet d'un concept sur un autre il est nécessaire de déterminer l'ensemble des chemins d'influences entre ces concepts. Il faut ensuite évaluer l'influence propagée par chaque chemin d'influence. Pour cela il est nécessaire de cumuler les symboles de chaque influence présente dans un chemin d'influence. Classiquement l'ensemble des symboles utilisés pour décrire l'effet d'une influence est {+, ?}. Nous nous limitons dorénavant à ces symboles afin de pouvoir fournir des mécanismes de calculs qui les utilisent.
Définition (Influence propagée dans un chemin d'influence):
, une carte cognitive simple définie sur un ensemble de concepts C et l'ensemble de symboles {+, ?}. L'influence propagée dans un chemin d'influence P est définie de la façon suivante:
avec est une application définie sur {+, ?} × {+, ?} ? {+, ?} représentée par la matrice:
L'influence propagée dans le seul chemin d'influence entre Autoroute et Erreur de trajectoire est négative.
On remarque que l'ensemble des chemins d'influence entre deux notions peut être de dimension infinie car la carte cognitive peut contenir des cycles. Nous definirons donc l'influence propagée entre deux concepts à l'aide d'un sous-ensemble fini de cet ensemble. Ce sous-ensemble est appelé l'ensemble des chemins d'influence minimaux.
, une carte cognitive simple définie sur un ensemble de concepts C et un ensemble de symboles S. Soit P un chemin d'influence entre deux concepts c 1 et c 2 de C. P est un chemin d'influence minimal si et seulement si il n'existe pas de chemin d'influence P entre c 1 et c 2 tel que P est une sous-liste de P . On note P c1,c2 l'ensemble des chemins d'influence minimaux entre c 1 et c 2 .
Le mécanisme de propagation de l'influence entre deux concepts effectue des comparaisons des influences propagées dans les différents chemins d'influence minimaux existants entre ces deux concepts. La valeur retournée par ce mécanisme peut être positive (notée +), négative (-), nulle (0) ou ambiguë ( ?). Cette valeur est positive (respectivement négative) quand tous les chemins d'influence minimaux entre ces concepts ont une influence propagée positive (respectivement négative). Cette valeur est nulle quand il n'y a pas de chemin d'influence minimal entre ces concepts. Cette valeur est ambiguë lorsque deux chemins d'influences minimaux ont une influence propagée de symboles différents.
Définition (Propagation de l'influence entre deux concepts):
, une carte cognitive simple définie sur un ensemble de concepts C et l'ensemble de symboles {+, ?}. Soit c u , c v deux concepts de C. L'influence propagée entre c u et c v est définie de la façon suivante: 
De nombreux travaux formalisent des mécanismes de propagation dans des cartes cognitives. Certains travaux représentent une carte cognitive sous la forme d'une matrice d'adjacence (Fabiola Mata Avila, 2002). Le mécanisme de propagation étant effectué à l'aide d'opé-rations matricielles. Définir ainsi le mécanisme de propagation a l'avantage de fournir une évolution du système au cours du temps. D'autres travaux quantifient de manière différente les influences. Leur principal objectif étant de lever automatiquement les ambiguités obtenues au cours de la propagation de l'influence. Ces travaux s'appuient souvent sur la logique floue (Zadeh, 1965) et définissent des cartes cognitives floues (Kosko, 1992) (Taber, 1991) (Perusich, 1996) où les influences sont pondérées par des valeurs numériques allant de ?1 à 1. Des modèles de cartes cognitives floues (Carvalho et Tomé, 1999) plus complexes associent des règles de logique floue pour chaque concept de la carte. Ces travaux sont intéressants pour la conception d'outils capables d'effectuer des raisonnements de façon autonome mais les ré-sultats fournis étant plus difficiles à interpréter par l'utilisateur, nous avons choisi de garder un formalisme plus simple.
Mécanisme d'inférence entre deux regroupements de concepts
On souhaite pouvoir interroger une carte cognitive simple afin de connaître l'influence d'un ensemble de concepts sur un autre ensemble de concepts. Nous définissons donc un regroupement de concepts comme étant un sous-ensemble des concepts.
Définition (Regroupement de concepts):
Soit C un ensemble de concepts. Un regroupement de concepts est un sous-ensemble de concepts de C.
Exemple:
Les concepts Brouillard, Pluie, Neige, Givre forment un regroupement de concepts (figure 1) appelé Mauvais temps.
Définissons un mécanisme d'influence entre deux regroupements de concepts C 1 et C 2 . L'influence entre C 1 et C 2 est nulle (notée 0) lorsqu'il n'y a pas de chemins d'influence entre un concept de C 1 et un concept de C 2 . Elle est positive, notée + (resp. négative, notée ?), lorsque chaque concept de C 1 influence positivement (resp. négativement) tous les concepts de C 2 . Elle est "positive ou nulle", noté ? (resp. "négative ou nulle", notée lorsque un concept de C 1 n'influence pas un concept de C 2 et qu'un concept de C 1 influence positivement (resp. négativement) un concept de C 2 mais aucun concept de C 1 influence négativement (resp. positivement) un concept de C 2 . Elle est ambiguë, notée ?, lorsqu'un concept de C 1 influence positivement (ou de manière ambiguë) un concept de C 2 et un concept de C 1 influence négativement (ou de manière ambiguë) un concept de C 2 .
Définition (Propagation de l'influence entre deux regroupements de concepts):
Soit M = (V, ´ etiq V , I, ´ etiq I ), une carte cognitive simple définie sur un ensemble de concepts C et l'ensemble de symboles {+, ?}. Soit C 1 ,C 2 deux regroupements de concepts. La propagation de l'influence entre deux regroupements est l'application I C définie sur C × C ? {0, +, ?, ?, ?} telle que: 
Hiérarchie et carte cognitive
On souhaite permettre à l'expert qui conçoit la carte de prédéfinir des regroupements de concepts qui ont un sens à être ensemble. Ces regroupements prédéfinis facilitent l'emploi par l'utilisateur du mécanisme de propagation entre deux regroupements de concepts. Nous considérons ici que les seuls concepts qui peuvent être regroupés ensemble sont ceux qui sont une spécialisation de la même chose. Nous proposons de définir un regroupement de concepts comme un nouveau concept appelé par le nom choisi. Ce concept peut être à son tour groupé avec d'autres concepts. Nous considérons ici qu'un concept est regroupé par un autre concept signifie que le premier est une spécialisation du second. L'ensemble des concepts forme alors un ensemble partiellement ordonné par une relation de spécialisation. Nous appelons hiérar-chie cet ensemble partiellement ordonné de concepts.
Définition (Hiérarchie):
Une hiérarchie est un ensemble de concepts C muni d'une relation d'ordre partiel notée Cette relation d'ordre est une relation de spécialisation.
Exemple:
La figure 2 est un diagramme de Hasse (Skiena, 1990) représentant l'ensemble partiellement ordonné des concepts. Un diagramme de Hasse est un graphe acyclique orienté qui ne contient pas d'arc de transitivité. Les arcs dans la hiérarchie symbolisent des relations de spécialisation. Par exemple, Ville et Lieu sont reliés car une Ville est une sorte de Lieu.
Un ensemble partiellement ordonné par une relation contient des éléments minimum, c'est à dire des éléments pour lesquels il n'existe pas d'éléments inférieurs. Nous appelons les élé-ments minimum d'une hiérarchie des concepts-minimum.
Définition (Ensemble des concepts-minimum d'une hiérarchie):
Soit (C, une hiérarchie de concepts. On appelle l'ensemble des concepts-minimum le sous ensemble des concepts de C défini de la façon suivante:
Exemple: Les concepts-minimum de la hiérarchie de la figure 2 sont ceux utilisés dans la carte cognitive simple de la figure 1.
Une carte cognitive hiérarchique est une carte cognitive simple pour laquelle des regroupements de concepts ont été prédéfinis par le concepteur.
FIG. 2 -Hiérarchie de concepts
Définition (Carte cognitive hiérarchique): Soit (C, une hiérarchie de concepts. Soit S un ensemble de symboles. Une carte cognitive simple définie sur M in(C), S est une carte cognitive hiérarchique définie sur (C, S.
Exemple:
La carte cognitive simple de la figure 1 devient une carte cognitive hiérarchique définie sur la hiérarchie de la figure 2. Les noeuds de cette carte cognitive hiérarchique sont étiquetés par les concepts-minimum de cette hiérarchie.
Mécanisme d'inférence dans une carte cognitive hiérar-chique
Définissons un mécanisme d'inférence permettant à l'utilisateur d'interroger une carte cognitive hiérarchique afin de déterminer l'influence entre deux concepts de n'importe quel type, que ce soit des concepts-minimum ou non. Ce mécanisme d'inférence entre deux concepts c et c détermine dans un premier temps deux regroupements de concepts-minimum : le regroupement de concepts-minimum qui sont inférieurs ou égaux à c et le regroupement de concepts-minimum qui sont inférieurs ou égaux à c .
Définition (Ensemble des concepts-minimum inférieurs ou égaux à un concept):
Soit (C, une hiérarchie de concepts. L'ensemble des concepts-minimum inférieurs ou égaux à un concept c de C est {c ? M in(C)|c c}. On note cet ensemble M inInf (c).
Exemple:
Déterminons l'ensemble des concepts-minimum inférieurs ou égaux à Brouillard : M inInf (Brouillard) = {Brouillard} et l'ensemble des concepts-minimum inférieurs ou égaux à Mauvaises conditions de circulation : M inInf (Mauvaises conditions de circulation) = {Mauvaise visibilité, Densité de circulation, Route glissante, Route sinueuse}
Une fois que les deux regroupements de concepts-minimum sont déterminés, le mécanisme d'inférence dans une carte cognitive hiérarchique calcule l'influence entre ces deux regroupements de concepts à l'aide du mécanisme d'inférence entre deux regroupements vu précédem-ment.
Définition (Mécanisme d'inférence dans une carte cognitive hiérarchique):
Soit H = (V, ´ etiq V , I, ´ etiq I ) une carte cognitive hiérarchique définie sur une hiérarchie (C, ) et l'ensemble de symboles {+, ?}. L'inférence dans une carte cognitive hiérarchique H est une application définie sur C × C ? {0, +, ?, ?, ?} telle que : 
Mécanisme de visualisation partielle d'une carte cognitive hiérarchique
Cette section présente un mécanisme qui, à partir d'une carte cognitive hiérarchique et d'un ensemble de concepts sélectionnés par l'utilisateur dans la hiérarchie, calcule une carte cognitive simple représentant une vue synthétique du système. On appelle vue partielle la carte cognitive simple obtenue par ce mécanisme. La vue partielle est définie sur un sous-ensemble de concepts de la hiérarchie (pas seulement les concepts-minimum). Ces concepts sont reliés par des influences dont le symbole est déterminé utilisant le mécanisme d'inférence des cartes cognitives hiérarchiques. La vue partielle est une représentation synthétique du système, les influences qui la composent portent donc des symboles moins précis que ceux employés dans une carte cognitive classique. Les symboles que nous employons sont +, ?, ?, et ? (Axelrod, 1976)(Chaib-draa, 2002 (Fabiola Mata Avila, 2002).
Notation (Ensemble de concepts sélectionnés dans la hiérarchie):
Soit H = (V, ´ etiq V , I, ´ etiq I ) une carte cognitive hiérarchique définie sur (C, et S. Notons C s le sous ensemble des concepts de C sélectionnés par l'utilisateur.
L'ensemble des concepts de la vue partielle contient les concepts sélectionnés par l'utilisateur, les concepts-minimum qui ne sont pas strictement inférieurs aux concepts sélectionnés. Si un concept sélectionné est inférieur à une autre concept sélectionné, il ne fait pas parti de l'ensemble des concepts de la vue partielle.
Définition (Ensemble de concepts d'une vue partielle):
Soit (C, une hiérarchie de concepts. Soit C s l'ensemble des concepts sélectionnés par l'utilisateur. Soit StrictInf (c) = {c ? C|c c ? c = c} l'ensemble des concepts strictement inférieurs à un concept. L'ensemble des concepts de la vue partielle est défini tel que:
Exemple: Les concepts présentent dans la figure 3 sont les concepts obtenus lorsque l'utilisateur sélec-tionne les concepts : Erreurs, Accident, Lieu, Puissance, Mauvaises conditions de circulation et Type de véhicule Une vue partielle d'une carte cognitive hiérarchique est une carte cognitive simple. L'ensemble des noeuds de la vue partielle sont étiquetés par des concepts de la hiérarchie déterminés en fonction des concepts sélectionnés dans la hiérarchie par l'utilisateur. Deux noeuds de la vue partielle v 1 et v 2 associés à deux concepts c 1 et c 2 sont reliés par une influence si dans la carte cognitive hiérarchique il existe une influence entre un noeud associé à un concept inférieur à c 1 et un noeud associé à un concept inférieur à c 2 . Le symbole de cette influence est déterminé à l'aide du mécanisme d'inférence dans une carte cognitive hiérarchique appliqué aux concepts c 1 et c 2 .
Définition (Vue partielle sur une carte cognitive hiérarchique): Soit H = (V, ´ etiq V , I, ´ etiq I ) une carte cognitive hiérarchique définie sur une hiérarchie (C, ) et l'ensemble de symboles {+, ?}. Soit C s l'ensemble des concepts sélectionnés par l'utilisateur. Une vue partielle de H en fonction de C s est la carte cognitive simple 
Conclusion
Cet article présente un nouveau modèle de carte cognitive, dite hiérarchique, qui permet à un utilisateur d'obtenir une vue partielle et synthétique de celle-ci. Cet article présente d'abord un modèle simple de carte cognitive qui formalise les nombreux travaux portant sur les cartes cognitives. Un mécanisme d'inférence de l'influence entre deux concepts et un mécanisme d'inférence entre deux regroupements de concepts sont définis. Ensuite ce modèle est étendu en lui associant une hiérarchie. Une hiérarchie déterminée par le concepteur de la carte représente des regroupements de concepts dans une carte cognitive simple. Le mécanisme d'inférence de l'influence entre deux regroupements est utilisé afin de définir l'influence de n'importe quel concept de la hiérarchie sur un autre. Ces mécanismes ont permis de fournir à l'utilisateur une visualisation partielle d'une carte cognitive hiérarchique. Nous avons développé un prototype 1 en java qui permet de construire et d'utiliser des cartes cognitives hiérarchiques. Les différents composants nécessaires à la visualisation des cartes sont implémentés en utilisant JGraph 2 , une bibliothèque de visualisation de graphes. Les figures de cartes cognitives présentées dans cet article ont été obtenues à l'aide de ce prototype.

Introduction
Aujourd'hui de nombreux génomes séquencés sont disponibles du fait du développement continu des technologies à haut débit et des procédures expérimentales 1 . Les experts biologistes jouent un rôle central dans l'analyse et l'annotation de cette quantité massive de données brutes. Pour annoter un nouveau génome, ils doivent intégrer plusieurs types d'informations en provenance de sources variées, ce qui prend entre 12 et 18 mois à une équipe de 2 à 4 personnes pour un petit génome bactérien contenant environ 2000 gènes. Pour faire face au déluge des nouvelles données génomiques, le processus d'annotation doit être le plus automatisé possible. Dans le contexte du projet RAFALE 2 , nous proposons aux biologistes utilisant la plate-forme AGMIAL 3 , un système semi-automatique d'annotation fonctionnelle de protéines. Nous proposons un système semi-automatique car le processus est collaboratif : pour chaque protéine, une annotation est suggérée par le système et les biologistes décident de l'annotation finale.
Les annotations sont issues d'arbres de décision obtenus par deux approches différentes : TILDE (Blockeel et Raedt, 1998) et Clus-HMC (Blockeel et al., 2006). Ces arbres sont appris sur les données d'un génome puis testés sur un autre génome (tous deux disponibles dans AGMIAL). Les classes fonctionelles que l'on propose (appelées prédictions) et les classes fonctionnelles données par l'expert aux protéines (appelées annotations) sont classées dans une hiérarchie fonctionnelle dérivée de celle de Subtilist (Moszer et al., 2002).
Mesures d'évaluation hiérarchiques
Les prédictions obtenues par les arbres de décision sont pondérées par un indice de confiance qui est égal au pourcentage d'exemples arrivant dans la feuille de l'arbre permettant d'effectuer la prédiction, arbre appris sur l'ensemble d'apprentissage.
Cet indice de confiance est utilisé pour contrôler la qualité des prédictions via un seuil d'élagage défini par l'expert.
Afin de mieux évaluer la pertinence des prédictions effectuées, nous avons exhaustivement recensé les types de couples annotation/prédiction possibles, en fonction des différents niveaux de la hiérarchie. Ce classement peut être comparé à celui de TABS (Iliopoulos et al., 2003) qui associe un score à chaque type de différences observées entre deux annotations d'un même génome. Comme nous ne travaillons pas dans le même contexte d'étude, certains types de différences de TABS ne s'appliquent pas à notre système (erreurs de typographies, annotations non répertoriées, erreurs de domaines, prédictions sans annotations) 4 . Nous définissons les types de différences, entre une annotation et une prédiction, réperto-riés dans TABS et utilisés dans notre système à l'aide des notations suivantes :
Notons E l'ensemble des exemples étudiés (ici les protéines). Soit x ? E une protéine et f (x, i) (resp. f (x, i)) l'annotation (resp. la prédiction) de x, si elle existe, au niveau i de la hié-rarchie utilisée. Soit
ensemble des protéines pour lesquelles annotation et prédic-tion sont en adéquation jusqu'au niveau k de la hiérarchie mais pas au niveau k + 1, et il existe une annotation (resp. une prédiction) jusqu'au niveau i (resp. j).
Les types de différences de TABS pertinents pour notre approche sont les suivants : l'annotation et la prédiction existent et sont identiques jusqu'au niveau i (cas d'une protéine dans A i i,i ) ; l'annotation est plus précise que la prédiction : elles sont identiques jusqu'au niveau j (cas d'une protéine dans A j i,j avec i > j) et l'annotation est moins précise que la prédiction : elles sont identiques jusqu'au niveau i (cas d'une protéine dans A i i,j avec i < j). De plus, nous ajoutons pour notre système le cas où i est le premier niveau où la prédiction diffère de l'annotation (cas d'une protéine dans A i?1 i,i ). L'évaluation de la qualité des prédictions est effectuée avec différentes mesures hiérar-chiques telles que : la précision, le rappel, la spécificité ou le Fscore.
Etude des prédictions plus précises que l'annotation
Le cas d'une protéine annotée 3.5.2 et prédite 3.5.1 (incluse dans l'ensemble A Or, si le premier cas correspond clairement à une erreur de pr édiction, le second cas correspond à une sous-prédiction supplémentaire par rapport à l'existant qui pourrait permettre de raffiner une annotation, ce qui est potentiellement intéressant pour le biologiste.
La figure 1 présente différentes configurations d'intérêt pour notre étude.
FIG. 1 -Exemples de différences entre annotations (encadrées) et prédictions (grisées) dans le cadre de mesures hiérarchiques.
-(e 1 ) correspond à l'adéquation parfaite entre l'annotation et la prédiction (e 1 ? A 3 3,3 ).
-(e 2 ) correspond au cas où l'annotation et la prédiction sont connues jusqu'au niveau 3 de la hiérarchie et ne sont en accord que jusqu'au niveau 2. Les prédictions de niveau 1 et 2 sont considérées comme justes, la prédiction de niveau 3 est considérée comme erronée par rapport à l'annotation (e 2 ? A 2 3,3 ). -(e 3 ) correspond au cas où l'annotation est plus précise que la prédiction. Le système, bien que n'ayant pas commis d'erreur de prédiction, n'a pas su prédire suffisament finement la classe fonctionnelle (e 3 ? A 2 3,2 ). -Enfin, le cas (e 4 ) illustre une prédiction plus précise que l'annotation courante. Le système a prédit la classe fonctionnelle 3.5.2 alors que l'annotation est 3.5. Ce cas est traditionnellement considéré comme érroné (e 4 ? A 2 2,3 ). Les protéines possédant une prédiction plus précise ( comme e 4 )peuvent être classées de deux manières selon le sens que l'on décide de leur attribuer : -elles interviennent dans le calcul du décompte des sous-prédictions erronées quand une sous-prédiction supplémentaire est comptée comme une erreur. Cette signification est couramment utilisée afin d'éviter de propager des erreurs en cas d'annotation d'un gé-nome par rapport à un autre. -elles interviennent à la fois dans le calcul du décompte des sous-prédictions justes et erronées pour moitié, en considérant qu'une sous-prédiction supplémentaire peut être aussi bien une erreur qu'une spécialisation justifiée (par exemple, dans le cas où de nouvelles informations sont disponibles depuis l'annotation).
Nous nous sommes focalisés sur l'analyse des protéines correspondant au cas (e 4 ). Les résultats des expérimentations que nous avons réalisées sur les génomes L.bacillus et L.sakei annotés avec la plateforme AGMIAL sont présentés dans la section suivante.
Dans la suite de cet article, nous choisissons la deuxième signification et considérons qu'une sous-prédiction supplémentaire est à la fois une erreur et une spécification justifiée.
RNTI -X -3
Résultats expérimentaux
Les résultats sont analysés en fonction du seuil d'élagage appliqué aux arbres de décision (évoqués à la section 2).
Les courbes de la figure 2 présentent la proportion de prédictions plus précises entre les niveaux 1 et 2 (courbe A) et les niveaux 2 et 3 (courbe B). Dans l'approche usuelle, ces protéines sont indifférenciables des protéines ayant une prédiction réellement erronée.
D'après ces courbes, nous pouvons remarquer que nous identifions des prédictions plus précises que l'annotation, essentiellement au troisième niveau de la hiérarchie (jusqu'à 100% pour L.sakei à partir d'un seuil d'élagage 0.90, courbe B, correspondant aux cas des différences entre les niveaux 2 et 3). De manière similaire, nous observons (courbe A) pour le même seuil d'élagage, uniquement 4 sous-prédictions plus fines que les annotations au niveau 2. Cela est potentiellement dû à la qualité des annotations. En effet, la plupart des protéines des génomes étudiés sont annotées aux niveaux 2 ou 3 de la hiérarchie et rares sont celles qui sont uniquement annotées au niveau 1.
Voici par exemple, les règles concluant à la prédiction de la classe 1.2.5 de la protéine 157 de L.sakei qui est annotée en 1.2 : Ces règles sont des chemins issus de plusieurs arbres appris par TILDE à partir d'informations utilisées sur les protéines de L.bulgaricus. La hiérarchie que nous utilisons est organisée en trois niveaux, nous prédisons donc une classe fonctionnelle en trois étapes correspondant aux niveaux successifs.
1. Au premier niveau, comme plus de 60% des protéines qui ressemblent à la protéine considérée (sous certaines conditions), sont annotées avec le terme GO :0006810 transport, la protéine esa157 est classée dans la classe 1 (Cell envelope and cellular processes) avec un indice de confiance de 97%. L'indice de confiance, (acc97), correspond au nombre de protéines dont l'annotation et la prédiction sont en adéquation lors de la phase d'apprentissage des arbres.
2. Au second niveau, comme il existe des protéines qui ressemblent à la protéine considérée, annotées avec le terme GO :0006810 transport et comme il n'y a pas plus de 62.5% des protéines qui ressemblent à la protéine esa157, et qui sont annotées avec le terme GO :0016469 proton-transporting two-sector ATPase complex, la protéine esa157 est classée en 1.2 Transport/binding proteins and lipoproteins (indice de confiance de 95%).
3. Au troisième niveau comme il n'existe pas de protéine qui ressemble à la protéine considérée et qui soit annotée avec le terme GO :0009401 phosphoenolpyruvate-dependent sugar phosphotransferase system ou GO :0003824 catalytic activity ou le mot clé SwissProt Lipoprotein, et comme il existe des protéines qui ressemblent à la protéine considé-rée, qui sont annotées avec le terme GO :0006865 amino acid transport alors la protéine esa157 est classée en 1.2.5 Transport/binding of amino-acids (indice de confiance de 91%).
L'identification explicite des protéines ayant des prédictions plus précises que l'annotation permettra à l'expert de les traiter séparément, surtout dans le cadre de réannotation de génomes où les informations complémentaires apportées par l'annotation automatique sur ces protéines pourront être analysées plus rapidement. Ici, pour la protéine esa157, l'annotation initiale 1.2 peut être corrigée en 1.2.5, comme prédit (après consultation de l'annotateur).
Conclusion et perspectives
Les mesures hiérarchiques usuelles prennent en compte la hiérarchie mais elles ne traduisent pas bien la statégie d'annotation des experts biologistes qui évitent de propager des erreurs.
De plus le cas de prédiction plus fine que l'annotation existante est traité comme une erreur par les mesures usuelles alors que dans un cadre d'annotation fonctionnelle, cette information doit impérativement être différenciée.
Nous avons répertorié les types de différences qui peuvent exister entre une annotation et une prédiction dans notre problème d'annotation fonctionnelle, et proposé une formalisation pour les représenter, valable quelque soit le niveau de profondeur de la hiérarchie.
Nous travaillons actuellement sur de nouvelles mesures hiérarchiques permettant de traiter ces cas à l'aide de cette formalisation. Nous pourrons ainsi traiter différemment les protéines ayant des prédictions plus précises et les protéines ayant une prédiction erronée.
RNTI -X -5

Introduction
L'évaluation d'une ontologie est une tâche difficile. Ceci explique l'absence de méthodes d'évaluation standard ou de mesures d'évaluation servant à valider l'ontologie. Dans cet article, nous focalisons notre intérêt sur l'évaluation des concepts de l'ontologie de domaine (appelés concepts ontologiques) qui sont extraits des pages Web. Nous travaillons sur les documents HTML écrits en français, dans le domaine du tourisme. Dans un travail précé-dent, nous avons défini un contexte structurel qui tient compte du document HTML et déve-loppé un algorithme de clustering afin de bien rassembler les mots sémantiquement proches (Karoui et al, 2006). Le résultat de ce travail était constitué de classes de mots pour lesquelles les experts ont du réaliser une lourde tâche d'évaluation et d'étiquetage. Pour aider ces derniers et faciliter l'interprétation sémantique de ces classes (concepts), nous avons défini une méthode d'évaluation basée sur trois critères révélateurs. Ces derniers sont le degré de crédibilité, le degré de cohésion et le degré d'éligibilité. Le degré de crédibilité exploite deux types de contextes : un contexte linguistique et un contexte documentaire. En se basant sur ces deux types de contextes, nous calculons le degré de crédibilité associé à chaque classe de mots et à chaque contexte. Le degré de cohésion calcule le degré de rapprochement des mots d'une classe en utilisant les documents du web. Le degré d'éligibilité sélectionne ou suggère le mot de la classe qui peut être son concept ou qui peut orienter le raisonnement vers le futur concept approprié. Dans ce qui suit, nous détaillerons ces critères ainsi que leur rôle.
Les critères révélateurs de l'évaluation intelligente des concepts
Les critères révélateurs assistent l'expert du domaine durant la tâche d'évaluation. Ces critères sont : le degré de crédibilité: le caractère de ce qu'on croit ; le degré de cohésion: le caractère d'une chose dont toutes ses parties sont réunies avec une relation logique entre ses éléments et sans aucune contradiction ; le degré d'éligibilité : le caractère d'un mot qui com-bine les conditions nécessaires pour être élu comme concept puisqu'il est le mot le plus représentatif de la classe ou qu'il peut orienter le raisonnement, l'interprétation et la tâche d'étiquetage.
Degré de crédibilité. Notre objectif est de pouvoir modifier les classes en cas de nécessi-té, de les étiqueter et d'estimer le rapprochement sémantique de leurs mots. Pour cela, nous effectuons une « contextualisation progressive » lors du processus d'évaluation, qui exprime la diversité des contextes et se concentre sur comment différents objets (dans notre cas les mots) opèrent et s'adaptent à leurs contextes à travers différentes structures de documents, d'organisations de mots, de méthodes de conception, d'intensions de concepteur, etc. La contextualisation progressive rejette l'idée de l'utilisation d'un unique contexte pour comprendre un objet. Au contraire, elle affirme qu'un discours ou une rédaction ordinaire révè-lent plusieurs contextes pour chaque objet étudié. L'interaction de ces contextes permet une interprétation sémantique correcte et écarte les mauvaises évaluations. Dans notre recherche, les différents contextes déduits à partir des documents sont donnés par deux analyses diffé-rentes. La première est une analyse linguistique qui permet de fournir les groupes nominaux, groupes verbaux, les associations de mots par une préposition (de, à, etc.) et celles par une conjonction (et, ou, etc. randonneur, gorge TAB. 1 -Exemples de classes de mots En fournissant tous ces contextes à l'expert, il lui sera difficile d'exploiter les associations entre les mots pour évaluer et étiqueter les classes (au vu de la quantité d'information importante qu'il devra analyser). Afin de lui faciliter la tâche, nous définissons un indice sémanti-que qui représente la crédibilité de l'association des mots étudiés en considérant les diffé-rents contextes. Cet indice est appelé « degré de crédibilité ». Il est calculé automatiquement pour chaque classe de mot et pour chaque contexte.
Notre algorithme, intitulé « Calcul de Degré de Crédibilité » et noté CDC, est exécuté sur un ensemble de classes de mots afin de calculer leurs degrés de crédibilité (DC). Ces derniers sont de la forme suivante : DC= {nombre de mots} nombre d'associations /Exemple : 3 2 (deux associations de trois mots).
Par exemple, en prenant la classe 1 (TAB.1) et en choisissant la phrase comme contexte, l'algorithme génère toutes les combinaisons possibles dans ce contexte c.-à-d. il essaye de trouver les 4 mots {académie, golf, golfeur, club}, puis les associations possibles de combinaisons de trois mots, etc. Pour chaque association trouvée, il présente les mots associés ainsi que le degré de crédibilité qui représente le nombre de fois que cette association a été trouvée. Par exemple, l'algorithme détecte deux associations de trois mots qui sont {académie, golf, golfeur} et {golf, golfeur, club}. Dans ce cas, le degré de crédibilité est 3 2 c.-à-d. deux associations de trois mots. Cet algorithme possède plusieurs fonctionnalités à savoir :
-Trouver les associations disponibles dans les contextes ainsi que le concept de la classe de mots. Par exemple, pour la Classe 2 (TAB.1), le concept est « civilisation » car ce terme de la classe est le plus fréquent dans les associations.
-Détecter les éléments générant du bruit dans une classe pour les supprimer ou les transférer dans leurs classes appropriées. Par exemple pour la classe 6 (TAB.1), le mot 'randonneur' a été trouvé dans de nombreuses associations contextuelles relatives à la classe 5 et pas dans celles de la classe 6. Dans ce cas, l'algorithme peut proposer de changer la place de ce mot de la classe 6 vers la classe 5. Lorsqu'un mot d'une classe apparaît souvent dans les associations de deux classes différentes (par exemple sa classe et une autre), l'algorithme écrit le mot en rouge pour souligner une situation ambiguë.
-Enrichir une classe par d'autres mots provenant des associations produites par les contextes. Pour la classe 3, nous pouvons enrichir la classe par les mots 'naturel' et 'espace' et lui associer le nom du concept suivant : 'espace naturel'.
Cette méthode d'évaluation permet à un utilisateur (expert, ingénieur de connaissances, étudiant, etc.) de commencer par analyser les associations produites par les contextes linguistiques. A défaut de ne pas retrouver l'information utile, il pourra étendre son analyse aux résultats des contextes documentaires. Grâce au degré de crédibilité calculé pour chaque classe et pour chaque contexte, l'utilisateur peut obtenir des informations utiles et dans certains cas suffisantes pour manipuler les classes de mots (supprimer, ajouter, etc.), les évaluer et les étiqueter. Pour une classe, si l'utilisateur trouve les degrés (5 1 , 4 3 , 3 8 , 2 15 ) qui sont des indications quantitatives, il devrait commencer par analyser l'association de 5 mots. Si ceci s'avère insuffisant pour lui, il pourra poursuivre avec les associations suivantes (de 4 mots, etc.). Autrement dit, l'algorithme génère un nombre d'informations complémentaires, mais l'utilisateur ne sera pas amené à toutes les utiliser mais plutôt à suivre l'ordre (des contextes linguistiques vers les contextes documentaires et au sein d'un même contexte des associations les plus larges (association de cinq mots 5 1 ) vers les plus réduites (les associations de deux mots 2 15 )) pour décider de s'arrêter dès qu'il atteindra son objectif. Degré de cohésion. Notre idée est basée sur le fait qu'il existe une grande quantité de pages web indexées par Google (8 058 044 651). Le web représente des connaissances fournies par différentes personnes ou organismes qui ont des expériences différentes mais certainement complémentaires dans le domaine. Nous sommes intéressées par le calcul du degré de cohé-sion de chaque classe de mots. Pour cela, nous définissons une distance sémantique entre les mots d'une classe en se basant sur les documents indexés par Google. Notre mesure intitulée 'degré de cohésion', notée Coh-D, utilise le nombre de documents dans lesquels les mots de la classe apparaissent ensemble. Ce critère est défini après une adaptation de la distance RNTI -X -normalisée de Google (Cilibrasi etVitanyi, 2006) de manière à pouvoir calculer le rapprochement sémantique pour un nombre de mots supérieur ou égal à deux (la distance de Google peut estimer le rapprochement sémantique entre uniquement deux mots). Notre formule est la suivante:  (karoui et al, 2006). Une classe acceptable est une classe que l'expert est capable de labelliser et dans laquelle les termes appartenant au même groupe sont proches sémantiquement. Une classe incorrecte est une classe qui présente l'un des deux cas suivants : soit elle contient certains termes qui n'ont pas de relations avec le concept extrait de cette classe, soit elle contient plusieurs concepts clairement identifiés par l'expert. Une classe inconnue est une classe dont les termes n'ont aucune relation sémantique ; l'expert ne peut pas en donner une interprétation sémantique.
Par exemple, les classes 1 et 6 (TAB.2) sont deux classes acceptables puisque tous les mots sont associés au même concept. Les classes 3 et 5 (TAB.2) contiennent des éléments de bruit (mots qui ne doivent pas appartenir à la classe en question). C'est pour cela, que les experts du domaine suppriment plusieurs mots afin de trouver le concept ou ne peuvent pas définir de concept pour la classe. Dans les autres cas, la classe peut contenir des mots de bruit mais qui sont facilement repérables ('feuille' dans la classe 8). La question qui se pose est comment peut-on détecter le mot qui doit être supprimé de la classe. En utilisant le moteur de recherche Google et sa base de données, nous calculons le nombre de réponses de chaque mot de la classe en l'associant au mot représentant le domaine (dans notre cas "tourisme"). Le mot qui apparaît le moins est supprimé et nous maintenons cette modification au niveau du calcul du degré de cohésion pour observer son effet. D'autres exemples sont pré-sents dans la TAB.2. Les mots en gras sont ceux qu'une fois supprimés, améliorent le degré de cohésion.
Par exemple, dans la classe 7 (TAB.2), en supprimant le mot "cervidés", nous obtenons un degré de cohésion plus représentatif (un impact réel puisque le degré de cohésion est passé de 223.52 à 81433.26) et par conséquence une tâche d'évaluation plus facile. D'après nos expérimentations et l'avis des experts, quand le degré de cohésion est faible (<1000), cela indique que le contenu de la classe devra être modifié.
Le degré de cohésion est un critère quantitatif qui aide l'expert du domaine ou un utilisateur novice à voir si la classe est sémantiquement cohérente ou non. Mais l'évaluateur ne peut pas établir un jugement uniquement en se basant sur ce critère. Ce dernier est plus inté-ressant quand il est considéré avec les autres critères (crédibilité et éligibilité). Degré d'éligibilité. Le degré d'éligibilité représente le nombre de voix qui indique à l'utilisateur quel est le mot candidat qui peut probablement représenter la classe ou du moins initier le processus de raisonnement qui mène à la définition du concept approprié de la classe. Il est calculé en appliquant la formule suivante :
NT(x) est le nombre d'occurrences de x ; n est le nombre de mots dans la classe. Le mot ayant le degré d'éligibilité le plus proche de la moyenne de la classe désigne le candidat représentant la classe. Il est noté Mot-E-D. Ce degré est calculé uniquement pour les classes acceptables. Vu que nous opérons dans le domaine de la sémantique, ces critères révélateurs ainsi que les constatations résultantes restent des propositions à approuver ou à rejeter par l'expert. Dans les deux cas, nous en tirons profit car même en cas de proposition incorrecte, l'expert raisonnera plus rapidement pour définir le 'bon' concept approprié à la classe. 
N°
TAB. 3 -Des résultats
Pour la classe 1 (TAB.3), notre critère trouve le bon terme "golf" qui représente la classe. Dans le cas ou le mot désigné par le degré d'éligibilité et celui mentionné par le degré de crédibilité en tant que mot de référence sont identiques, l'algorithme présente le mot en question en un style gras afin d'exprimer une forte suggestion. La classe 1 est une classe acceptable. La classe 3 (TAB.3) contient un mot de bruit qui est "mètre". En le supprimant, l'algorithme détecte le bon terme représentatif de la classe qui est "parc". Pour la classe 2, l'algorithme propose "caravane" en tant que concept. Mais le mot de référence (le mot qui est donné par l'algorithme de calcul de degré de crédibilité et qui est le plus présent dans tous les contextes) de cette classe est "camping". Puisque les deux mots sont différents, l'algorithme les mentionne avec une couleur rouge pour signaler une situation d'ambigüité. Dans ce cas, l'évaluateur doit faire le choix entre les deux mots ou définir un autre concept soit sur la base des deux ou complètement nouveau. Mais dans les deux premiers cas (les classes 1 and 3 (TAB.3)), l'évaluateur peut décider facilement.
Discussion. Notre méthode d'évaluation des concepts ontologiques, que nous avons caracté-risée comme étant 'intelligente', du fait qu'elle pourra à terme s'appliquer automatiquement, donne deux types d'informations révélatrices à savoir :: une indication qualitative basée sur les associations des mots déduites à partir de la projection des mots d'une classe dans un contexte défini ; une indication quantitative représentée par le degré de crédibilité calculé pour chaque classe et pour chaque contexte, le degré de cohésion et celui d'éligibilité. L'évaluation qualitative permet d'aider l'utilisateur au niveau de l'interprétation sémantique. Notre méthode permet à un utilisateur ordinaire d'aider l'expert en manipulant en amont les classes de mots et en suggérant des étiquettes sémantiques. Par conséquent, l'expert peut décider rapidement si le label est convenable ou si la classe est sémantiquement correcte. Ce type d'évaluation permet, également, la réutilisation et l'évolution de l'ontologie puisqu'elle est faite à partir des pages web et tiendra compte de leurs changements possibles. Ainsi, si les documents du web changent, les contextes extraits changent et les résultats de l'algorithme CDC seront différents. D'où l'effet de mise à jour qui sera appliqué. Ces informations présentées à l'expert serviront à l'évaluation en ligne et seront stockées pour une éventuelle utilisation ultérieure. Notre méthode a été appliquée après (pourra être pendant) la construction de la hiérarchie de concepts (pas de l'ontologie complète). Dans ce travail, le choix des contextes a été défini manuellement, par contre l'application de ce choix sur le corpus, l'extraction des contextes et des associations ainsi que les calculs des degrés d'évaluation sont faits automatiquement.
Conclusion
Dans cet article, nous avons proposé une méthode d'évaluation conduite par le web qui guide l'interprétation, évite les cas ambigus et aide l'expert ou l'utilisateur. Afin d'atteindre cet objectif, notre méthode se base sur trois critères révélateurs qui sont le degré de crédibili-té, le degré de cohésion et le degré d'éligibilité. Chaque critère est calculé en appliquant un algorithme séparé. L'algorithme de crédibilité essaye d'éliminer ou de déplacer les éléments bruits, propose des labels sémantiques et produit des associations de mots par type de contexte et pour chaque classe. Les degrés de cohésion et d'éligibilité informent l'utilisateur de la relation entre les mots de chaque classe et quel serait le possible candidat représentant le concept. Comme perspective, nous travaillons sur la visualisation de ces critères.

Introduction
Le monde bouge et bien des phénomènes ne peuvent être compris sans étudier leurs mouvements. Heureusement, de nouvelles technologies (GPS et autres transmetteurs, capteurs et réseaux de capteurs, marqueurs de type RFID) permettent désormais de saisir des positions spatio-temporelles et ainsi de suivre un grand nombre d'objets en mouvement. Cette voie correspond aux besoins réels d'applications dans de nombreux domaines, parmi lesquels l'analyse des déplacements des habitants d'une ville ou d'un état afin d'optimiser les infrastructures de transport, le suivi d'une flotte de véhicules de livraison, ou encore les études comportementales d'animaux migratoires.
Malheureusement les systèmes de gestion de base de données (SGBD) actuels n'offrent au mieux que la possibilité de traiter de positions fixes. Toute la logique de modélisation et de traitement du mouvement est à la charge des programmes d'applications. Le monde de la recherche a été plus loin, en concrétisant au niveau du prototypage les concepts et les opérations liés à la gestion d'objets en mouvement, dits objets mobiles (Almeida et al., 2006, Pelekis et al., 2006. Toutefois, ces travaux n'ont abordé que la dimension spatio-temporelle du mouvement, à savoir la trace géométrique du parcours de l'objet dans l'espace au cours du temps.
Cela est insuffisant pour les applications qui utilisent une vision plus structurée, plus sémantique, du mouvement. Par exemple, de nombreuses applications perçoivent le mouvement d'un objet non pas comme un déplacement unique qui commencerait au début de la vie de l'objet et ne se terminerait qu'avec elle, mais comme une séquence de déplacements, chaque déplacement ayant son propre but, un lieu et un instant de départ bien définis, et un lieu et un instant d'arrivée bien définis. C'est, entre autres, le cas des applications qui analysent les déplacements quotidiens des employés entre leur domicile et leur lieu de travail, les tournées hebdomadaires de camions qui livrent des marchandises dans une région donnée, ou les migrations annuelles d'oiseaux à la recherche d'une nourriture plus abondante. Dans ces cas, la perception d'un mouvement unique pour chaque objet n'est pas adéquate, et une perception sémantique verra plutôt une succession de déplacements nettement différenciés les uns des autres. C'est chacun de ces déplacements successifs, effectués au cours de la vie d'un objet mobile, que nous appelons trajectoire. L'analyse de ces trajectoires permet de construire des modèles de mobilité servant soit à une prise de décision (par exemple récolter des trajectoires urbaines afin d'en dériver des connaissances utiles pour optimiser le trafic), soit à l'acquisition de connaissances concernant l'objet mobile (par exemple analyser les trajectoires des oiseaux pour mieux comprendre leur comportement), soit encore au contrôle de stratégies, notamment en logistique (par exemple surveiller le bon fonctionnement et l'efficacité du système de livraison de colis par une entreprise postale).
Du point de vue des applications, l'étude de ces trajectoires demande de récolter de nombreuses autres informations en plus du chemin parcouru. Par exemple pour une application étudiant les déplacements quotidiens pour aller au travail, le moyen de transport est une information essentielle. Pour l'étude des migrations d'oiseaux, ce sont les conditions météorologiques et les arrêts des oiseaux en cours de parcours (quand, dans quel type de lieu, pourquoi et pendant combien de temps). Il est aussi important de pouvoir décrire les contraintes liées à l'application, comme le fait que certains oiseaux ne volent jamais durant la nuit, tandis que d'autres ne volent que de nuit. Ces applications ont besoin de nouvelles structures de représentation qui aillent au-delà de la simple description du mouvement. Ces nouvelles structures doivent comprendre une partie fixe pour les caractéristiques communes à toutes les trajectoires, et une partie variable pour les caractéristiques spécifiques aux trajectoires de chaque application.
Cet article étudie le problème de modélisation des trajectoires au niveau conceptuel, ce qui nous permet d'établir les bases de modélisation sans être influencés par des considérations liées à des implémentations particulières. Nous proposons un patron de modélisation des trajectoires qui peut être adapté à toute application qui utilise des trajectoires. Cette approche par patron semble particulièrement adaptée parce qu'elle offre la flexibilité indispensable pour prendre en compte la diversité de la sémantique des
État de l'art
Depuis longtemps différents types de mouvements ont fait l'objet d'études dans des domaines divers. C'est le cas en sciences sociales pour l'étude des migrations, des déplacements quotidiens ou des comportements humains en voyage (Kwan et Lee 2003, Thériault et al. 2002. C'est le cas en biologie pour l'étude du déplacement des cellules ou en médecine pour l'étude de la propagation de maladies... Par ailleurs en géomatique, le mouvement effectué par un objet mobile a été décrit de façon générique. Il est appelé "ligne de vie geospatiale" par Mark et al. (1999) et est décrit comme une ligne dans un espace spatio-temporel à trois dimensions (x,y,t), les coordonnées géographiques (x,y) et le temps, t. Hornsby et Egenhofer (2002) en ont proposé une définition formelle qui permet d'appréhender les problèmes des granularités multiples. Laube et al. (2005) ont fait appel aux techniques de fouille des données pour analyser les lignes de vie géospatiales afin d'en extraire une typologie des déplacements. Suivant une approche base de données, Güting et al. (Güting et al. 2000, Güting et Schneider 2005 ont apporté une contribution essentielle en proposant une théorie formelle pour les objets mobiles (points et surfaces) et un prototype qui implémente cette théorie en utilisant un SGBD propriétaire extensible, Secondo (Güting et al. 2004). Le coeur de cette théorie est un ensemble de types de données qui comprend des types spatiaux, des types temporels et des types "mobiles" (des types qui représentent une valeur qui varie dans le temps). Plus récemment, les auteurs ont étendu leur approche à la modélisation et manipulation de trajectoires contraintes par un réseau prédéfini, par exemple, un réseau ferré ou routier (Güting et al. 2006). La plupart des -153 -RNTI-E-13 travaux sur le mouvement contraint par un réseau décrivent, d'un côté, le réseau avec sa structure et sa localisation et, d'un autre côté, l'objet mobile avec ses coordonnées. Puis ils contraignent les coordonnées de l'objet à se situer sur le réseau. L'approche de Güting et al. (2006) est différente : ils décrivent la position de l'objet via une référence aux routes du réseau, par exemple "sur l'autoroute A9, 10 km après Montreux en direction du Valais". Cette technique de spécification implique que l'objet mobile est nécessairement positionné à l'intérieur du réseau, ce qui rend inutile le recours à des contraintes spatiales. Wolfson et al. (1998Wolfson et al. ( , 1999 ont élaboré une autre approche des objets mobiles dans l'objectif de réduire le coût des mises à jour. Ceci les conduit à stocker des vecteurs de mouvement (direction, vitesse, instant) plutôt que des positions spatio-temporelles (point, instant), ce qui a l'avantage de permettre de prédire le mouvement futur en fonction du dernier vecteur de mouvement. Le vecteur courant reste actif tant que la différence entre la position prédite et la position relevée est inférieure à un seuil donné. Quand le seuil est dépassé un nouveau vecteur est calculé et stocké. Ce travail ne considère que les points mobiles, pas les régions mobiles. Les mêmes auteurs se sont aussi intéressés aux déplacements contraints par un réseau. Notons que ce thème des déplacements contraints a aussi été étudié par Speicys et al. (2003). Noyon et al. (2005) ont étudié comment les déplacements de deux objets mobiles ponctuels ou surfaciques peuvent être corrélés via des mesures de distance, ou comment analyser l'évolution de la position relative d'un objet par rapport à celle d'un autre objet.
L'étude des mouvements périodiques (se répétant régulièrement, par exemple le mouvement de planètes, de trains ou d'animaux migrateurs) a intéressé plusieurs équipes. Behr et al. (2006) ont proposé un modèle formel pour représenter les mouvements périodiques qui peuvent contenir des répétitions imbriquées comme, par exemple, le mouvement d'un métro qui réalise la même trajectoire chaque jour de la semaine. Suivant l'approche initiale à base de types abstraits de données proposée par Güting et al. (2000), les auteurs ont défini et implémenté sur Secondo un ensemble de types de données apte à décrire les mouvements périodiques, en particulier, le type pmpoint (periodic moving point).
Du coté des implémentations, on trouve le système Hermès (Pelekis et al. 2006), dont les spécifications ont été construites en se basant sur les approches de Güting et de Wolfson. Hermès est une nouvelle cartouche de données spatio-temporelles pour le SGBD relationnelobjet Oracle 10. La cartouche utilise les types spatiaux statiques d'Oracle (Oracle 2007) et les types temporels proposés par la cartouche temporelle de TAU-TLL de Pelekis et Theodoulidis (2002). Hermès offre aussi différents types de fonctions d'interpolation pour reconstituer d'éventuelles parties manquantes du mouvement. Par ailleurs, certains travaux ont élaboré des techniques d'indexation spécifiques aux objets mobiles pour améliorer les performances du système (voir Chakka et al. 2003et Pelanis et al. 2006.
A contrario, la prise en compte d'une vision applicative des objets mobiles, c'est-à-dire une vision qui tienne compte des aspects sémantiques du mouvement, ne semble pas avoir motivé les chercheurs. Ainsi, nous n'avons pas trouvé d'approche conceptuelle du mouvement qui soit basée sur les besoins des applications. Les modèles conceptuels pour bases de données spatio-temporelles proposent bien le concept de point mobile (voir par exemple Khatri et al. 2004ou Parent et al. 2006a, mais elles ne vont pas au-delà. À notre connaissance un seul travail, Brakatsoulas et al. (2004), s'est donné pour objectif d'enrichir la sémantique d'un modèle pour objets mobiles. Malheureusement les trajectoires de ces auteurs restent au niveau géométrique. Ce sont des polylignes qui connectent les différentes positions spatio-temporelles relevées; elles représentent le mouvement par une séquence de -154 -RNTI-E-13 segments spatio-temporels plutôt que par une séquence d'étapes sémantiques. De même, les propriétés des trajectoires sont uniquement celles qui sont dérivables des positions spatiotemporelles, comme la vitesse ou le chemin parcouru, et non d'autres propriétés sémantiques qu'une application pourrait souhaiter conserver à propos de ses trajectoires, comme, par exemple, le but de la trajectoire.
Se démarquant des approches purement géométriques, cet article perçoit les trajectoires comme des déplacements qui ont une signification sémantique (trajectoires de personnes allant au travail, d'animaux en migration, de véhicules en route pour une destination, de phénomènes naturels régis par les lois de la nature, …). La liste des positions spatiotemporelles d'un objet tout au long de sa vie ne fournit qu'un des éléments de la trajectoire. D'autres éléments décrivent, entre autres, quand la trajectoire débute, se termine ou quand elle est temporairement suspendue. Ces éléments sont déterminés par l'application qui, seule, peut définir la sémantique de la trajectoire. Notre approche est basée sur ce principe.
Les applications ont aussi souvent besoin de pouvoir décrire des trajectoires selon différentes granularités spatiales et temporelles. Quelques travaux se sont intéressés à la multi-granularité dans le cadre des objets mobiles (Friis-Christensen et al. 2005, Camossi et al. 2003. Nous prévoyons de réutiliser un travail précédent sur la multi-représentation dans les bases de données spatio-temporelles (Balley et al. 2004), Parent et al. 2006b) et de proposer une approche similaire pour la multi-représentation des trajectoires. Cet aspect n'est cependant pas présenté dans cet article.
Une application
Ce paragraphe présente un exemple typique d'application utilisant des trajectoires. Ce type d'application a pour objectif d'analyser les déplacements d'animaux migrateurs. Les données sur leurs mouvements sont soit obtenues automatiquement grâce à des émetteurs portés par les animaux et qui envoient régulièrement leur position, soit obtenues par observation directe en capturant, temporairement, les animaux.
L'exemple qui nous sert d'illustration concerne les migrations annuelles des cigognes blanches (Ciconia ciconia), auxquelles s'intéressent plusieurs groupes de recherche 1 . Comme plusieurs espèces d'oiseaux migrateurs, les cigognes migrent tous les ans. Chaque automne, elles quittent l'hémisphère nord, notamment l'Europe, pauvre en nourriture en hiver, et migrent vers l'hémisphère sud, où la nourriture est abondante tout au long de l'année. Mais pour se reproduire et nourrir leurs petits, les cigognes ont besoin des longues journées qu'offre l'hémisphère nord en été. Elles reviennent donc vers le nord au printemps.
L'analyse des migrations animales a pour objectif général d'améliorer les connaissances concernant le comportement animal. Est-ce que la migration est innée et en quelque sorte programmée génétiquement ? Comment les oiseaux s'orientent-ils lors de la migration ? En effet, ils reviennent souvent dans les mêmes endroits pour se reproduire. Comment font-ils pour voler sur de si longues distances ? Volent-ils en groupe ? Où et quand s'arrêtent-ils pour se reposer pendant la migration ? Quels sont les facteurs environnementaux qui influencent leur migration (topographie, conditions météorologiques, prédateurs, disponibilité de nourriture) ? Quels sont les dangers rencontrés ? Pourquoi certains oiseaux ne survivent-ils pas à la migration ? … Pratiquement les données que nous avons utilisées proviennent d'un ensemble de cigognes blanches. Elles ont été équipées chacune d'un petit émetteur qui émet régulièrement des signaux qui sont captés par des satellites. Les cigognes blanches volent uniquement durant la journée parce qu'elles utilisent les courants thermiques créés par le soleil quand il réchauffe la terre. Elles interrompent donc leur vol la nuit et en profitent pour se reposer et se nourrir. Elles peuvent parcourir plusieurs centaines de kilomètres par jour selon les conditions météorologiques. Elles migrent en groupes dont la composition peut changer lors des arrêts. Pour détecter les groupes de cigognes volant ensemble on analyse les positions spatio-temporelles des cigognes. Cependant, comme seul un petit pourcentage des cigognes sont équipées d'un émetteur, le groupe dans lequel vole un oiseau peut rester inconnu.
Afin d'analyser le comportement des cigognes blanches durant leur migration, les chercheurs ont besoin de connaître deux types d'information :
(1) Informations propres aux trajectoires des oiseaux : Durant le vol des cigognes, les informations suivantes sont enregistrées à intervalles réguliers :
-leur position spatio-temporelle.
-l'altitude à laquelle la cigogne vole. Cette information, de même que les conditions météorologiques et les objets naturels ou artificiels rencontrés, change le long de la trajectoire de l'oiseau. -le groupe avec lequel la cigogne vole. Lorsque les cigognes blanches sont à l'arrêt, les informations suivantes sont enregistrées :
-le type d'arrêt : arrêt pour la nuit ou arrêt plus long (généralement quelques jours ou semaines) pour se reposer et se nourrir. -si possible, les activités de l'oiseau durant l'arrêt : alimentation, repos, … -si la cigogne a pu être attrapée durant l'arrêt, son poids, le pourcentage de graisse, la température de son corps et sa condition générale.
(2) Informations sur les conditions environnementales des trajectoires : Ici sont principalement enregistrés : -Les conditions météorologiques : vent (direction, force), température, pression, état du ciel (soleil, pluie, nuages, brouillard, ciel clair). -Les objets naturels (montagnes, grandes étendues d'eau, déserts…), et les objets artificiels (lignes électriques, antennes, gratte-ciels, éoliennes…) qui peuvent constituer des obstacles pour les cigognes et influencent donc leur vol, notamment leur direction de vol. La description de ces obstacles est particulièrement importante parce qu'ils constituent une menace qui peut entraîner la mort des migrateurs.
Trajectoires : Définitions de base
Nous présentons maintenant les définitions de base sur lesquelles repose notre travail, et l'approche de modélisation conceptuelle des trajectoires que nous avons élaborée à partir de l'étude des besoins. Notre objectif est de permettre aux applications gérant des objets mobiles de décrire leur vision du mouvement d'un objet mobile (c'est-à-dire l'évolution de sa position au cours de sa vie) par un ensemble de trajectoires identifiables et significatives pour l'application. Par exemple, une entreprise gérant une flotte de camions de livraison pourra décrire les déplacements des camions par un ensemble de trajectoires, chacune correspondant à une tournée de livraison qui part du siège de l'entreprise et qui y retourne. À une autre granularité temporelle, le déplacement d'un oiseau migrateur pourra être décrit par deux trajectoires annuelles : l'une en automne de l'Europe vers l'Afrique, l'autre au printemps suivant de l'Afrique vers l'Europe. La figure 1 illustre l'idée de la désignation d'un ensemble de trajectoires dans le mouvement d'un objet mobile. La figure montre aussi que certains segments du mouvement peuvent n'appartenir à aucune trajectoire (par exemple, le segment entre t 1 et t 2 ). Cette non-appartenance traduit le fait que ces segments correspondent à des déplacements qui n'intéressent pas l'application.
FIG. 1 -Parcours spatio-temporel d'un objet en mouvement, support de trajectoires définies par segmentation sémantique
Étant donné qu'un objet ne peut être qu'à un seul endroit à un instant donné, chaque trajectoire d'un objet est repérable par l'intervalle de temps qui la délimite, depuis l'instant (appelé tdébut) où l'objet commence un déplacement dans un but donné jusqu'à l'instant (appelé tfin) où le déplacement pour ce but se termine. Identifier (soit directement soit par une règle de calcul) le début et la fin de chaque trajectoire est de la responsabilité de l'application, en fonction de ses propres règles de gestion.
Nous pouvons maintenant définir, au niveau conceptuel, la notion de trajectoire pour un objet ponctuel ou indéformable.
Définition 1 (Trajectoire) : Une trajectoire est la description, du point de vue utilisateur, de l'évolution de la position (perçue comme un point) d'un objet qui se déplace pendant un intervalle de temps donné pour un but particulier.
Trajectoire : [t début , t fin ] ? espace
Les trajectoires sont ainsi introduites comme un concept sémantique, qui interprète un segment du parcours d'un objet mobile, identifié par le choix de l'intervalle de temps [t début , t fin ], comme une unité significative par rapport à l'application. Par exemple, dans l'application de suivi des oiseaux, les ornithologistes doivent définir s'ils voient l'aller-retour annuel d'un oiseaux comme une seule trajectoire ou comme deux trajectoires différentes. Dans certains domaines d'application, les trajectoires sont facilement identifiables: par exemple, le parcours des camions de livraison peut être aisément segmenté en trajectoires journalières. Dans d'autres domaines d'application, par exemple le suivi de chimpanzés, il n'est pas toujours évident de déterminer quand l'objet (le chimpanzé) commence une nouvelle trajectoire et quand il continue simplement la trajectoire précédente. Dans ce cas, la segmentation peut être induite par l'observateur plutôt que par l'observé : par exemple, en définissant une trajectoire par période d'observation. Quel que soit le critère choisi par l'application, l'intervalle de temps qui définit la trajectoire, [t début ,t fin ], est nécessairement inclus dans le cycle de vie de l'objet et est nécessairement disjoint (ou adjacent) aux intervalles de temps des autres trajectoires du même objet. Si la segmentation en trajectoires couvre l'intégralité du parcours, les intervalles de temps définissant deux trajectoires successives se touchent toujours. De nouveau, ce choix dépend de l'application.
La trajectoire ainsi définie ne constitue que la trace matérielle d'un voyage de l'objet. Cette description minimale peut être enrichie, par exemple en décrivant également, avec plus ou moins de richesse, l'objectif assigné à la trajectoire, les conditions dans lesquelles elle a été réalisée, ou les résultats auxquels elle a abouti.
La fonction temps ? espace qui définit une trajectoire n'est pas nécessairement identique à celle produite par le processus d'acquisition des données. Ce dernier fournit les données brutes du déplacement sous la forme d'une suite de couples (point d'échantillonnage, instant). Ces données brutes sont souvent d'abord modifiées (nettoyées), afin de corriger les erreurs de saisie et les approximations réalisées lors de l'acquisition, puis retravaillées pour constituer une fonction qui produise une image du déplacement ayant la qualité souhaitée. Enfin, les besoins applicatifs déterminent comment la fonction-trajectoire doit être définie. L'application pourrait n'être intéressée que par un sous-ensemble des points connus : par exemple, elle peut supprimer les points acquis durant la nuit et ne conserver que les déplacements réalisés durant la journée. Elle peut aussi avoir ses propres règles pour simplifier la fonction en remplaçant certaines séquences de points par un point unique synthétisant la séquence.
Les objets ne se déplacent pas nécessairement tout le temps pendant une trajectoire (c'est le cas dans l'exemple des oiseaux). Ils peuvent marquer des temps d'arrêt. Les trajectoires peuvent donc être elles-mêmes segmentées en périodes de déplacement et périodes d'absence de déplacement (où l'objet est vu comme immobile). Nous appelons ces premières des déplacements et ces dernières des arrêts. Une trajectoire peut donc être perçue comme une séquence de déplacements allant d'un arrêt à l'arrêt suivant. Par exemple, un oiseau en migration va faire un arrêt quelque part pour un intervalle de temps donné pour se nourrir, un autre arrêt pour se reposer et ainsi de suite jusqu'à ce qu'il atteigne sa destination finale, la fin de sa trajectoire. Des représentants de commerce en déplacement vont faire des arrêts à tous les endroits où ils doivent rencontrer un client.
Comme pour la détermination du début et de la fin des trajectoires, l'identification des arrêts et des déplacements dans une trajectoire est de la responsabilité de l'application. Les arrêts physiques (c'est-à-dire le fait que la position de l'objet est la même durant au moins deux instants consécutifs) ne sont pas systématiquement des arrêts conceptuels, parce qu'ils peuvent être la conséquence d'évènements qui ne sont pas pertinents pour l'application. Par exemple, l'arrêt fait par le représentant de commerce pour boire un café n'est pas pertinent pour l'application. Par contre, l'arrêt réalisé pour rencontrer un client l'est. L'application peut vouloir compter le nombre d'arrêts par trajectoire et ici seuls les arrêts significatifs doivent être comptés. Dans la suite, nous supposons que les arrêts et les déplacements couvrent complètement la trajectoire (c'est-à-dire qu'il n'existe pas d'instant inclus dans Nous ne considérons pas le début et la fin de la trajectoire comme des arrêts. Leur emprise temporelle est en effet un instant, et non pas un intervalle non vide.
Étude des besoins pour la modélisation de trajectoires
Les définitions précédentes sont valables quel que soit le type de trajectoire. Cependant, les besoins en termes de modélisation peuvent varier suivant le type de trajectoire considéré. Nous différencions dans cet article trois types de trajectoires : les trajectoires métaphoriques, les trajectoires géographiques et les trajectoires spatio-temporelles.
Les trajectoires métaphoriques
Le terme de trajectoire est quelquefois utilisé dans un sens métaphorique afin de décrire une évolution qui n'est pas un mouvement physique. Par exemple, il n'est pas rare de parler de la trajectoire professionnelle d'une personne pour décrire une succession d'états ou de changements, comme dans "Je suis passée du milieu académique à l'industrie où j'ai travaillé dans une grande compagnie, puis je suis retournée dans le milieu académique". Cette utilisation métaphorique de la notion de trajectoire repose sur l'image d'un objet (ici une personne) se déplaçant dans un espace abstrait dont les points sont les différentes valeurs d'un attribut thématique (ici un attribut décrivant le secteur professionnel).
Du point de vue de la modélisation, ce type de trajectoire peut être décrit en définissant l'attribut thématique comme variable dans le temps, c'est-à-dire un attribut dont la valeur est définie par une fonction du temps vers le domaine de valeurs de l'attribut. Réciproquement, tout attribut thématique variable dans le temps peut être vu comme définissant une trajectoire métaphorique pour les objets qui ont cet attribut. Les modèles de base de données spatiotemporelles existants, tels ceux de Khatri et al. (2004) ou Parent et al. (2006a), proposent ce concept d'attribut variable dans le temps. La variabilité peut être :
-continue : les valeurs de l'attribut changent de façon continue. C'est le cas, par exemple, de la température mesurée en un point particulier ou de la valeur d'une action en bourse. -par paliers : les changements de valeur sont instantanés et chaque valeur est valable durant un intervalle de temps. C'est le cas, par exemple, du salaire d'un employé ou du secteur professionnel de la personne dans l'exemple ci-dessus. -discrète : les valeurs de l'attribut existent uniquement à certains instants. C'est le cas, par exemple, de l'attribut prime d'un employé qui est un évènement ponctuel. Un attribut variable en continu a nécessairement un domaine de valeurs continu à la granularité près. Par contre, un attribut à domaine de valeurs continu peut être variable selon l'un quelconque des trois types. Le choix entre ces trois types dépend de comment change la valeur de l'attribut dans le temps : évolution continue, remplacement d'une valeur par une autre au bout d'un certain temps, valeur significative seulement à certains instants.
Tant que l'application n'a besoin que de mémoriser l'évolution de la valeur des attributs, modéliser ce type de trajectoire métaphorique ne requiert pas de nouveaux concepts. Cependant, si la description d'une trajectoire métaphorique implique des liens entre la trajectoire et d'autres objets de l'application, la trajectoire doit alors être modélisée comme un objet et non comme un attribut. En conséquence, ces trajectoires, dont la sémantique est plus riche, doivent être modélisées de manière similaire aux trajectoires spatio-temporelles (voir le paragraphe 5.3). Dans ce cas, les relations topologiques et de synchronisation fréquemment utilisées pour modéliser les trajectoires spatio-temporelles seront de simples relations thématiques.
Les trajectoires à connotation géographique naïve
Les voyages sont fréquemment décrits comme un déplacement d'un endroit à un autre, par exemple d'une ville à une autre ville: "Je suis allée à Paris, puis à Bruxelles, puis je me suis rendue à Amsterdam et Berlin avant de revenir à Lausanne". De façon similaire, les déplacements d'une trajectoire peuvent être définis par référence à des objets à connotation géographique de nature linéaire mais dont les coordonnées spatiales ne sont pas définies ou pas utiles pour l'application. Par exemple : "J'ai pris le TGV de Lausanne à Paris, puis le Thalys de Paris à Bruxelles...". Le voyage a dans ces exemples une connotation géographique forte -ce pourquoi nous qualifions ces trajectoires de géographiques -mais ils n'est pas défini en termes de coordonnées spatiales: il est défini par référence à des objets géographiques (comme dans les approches dites "Géographie Naïve" Egenhofer et al. (1995)). De fait, les trajectoires géographiques naïves sont un cas particulier de trajectoires métaphoriques. Comme ces dernières, elles peuvent être décrites en utilisant des attributs thématiques variables dans le temps (dans les exemples ci-dessus villeVisitée et trainUtilisé) ou d'une manière similaire aux trajectoires spatio-temporelles.
Les trajectoires spatio-temporelles
Une trajectoire est dite spatio-temporelle si des cordonnées spatiales -dans l'espace vu comme ayant deux ou trois dimensions -sont utilisées pour décrire la position de l'objet qui se déplace. Une trajectoire spatio-temporelle comprend deux composantes :
Une composante géo-temporelle qui décrit physiquement le déplacement de l'objet. Une composante sémantique qui décrit les informations de la trajectoire liées à l'application telles que des annotations et des liens vers les objets décrits dans la base de données. Cette composante sémantique est analysée dans le paragraphe suivant.
La sémantique des trajectoires
Les caractéristiques sémantiques pertinentes des trajectoires varient d'une application à une autre. Cependant, on peut identifier des constantes qui peuvent faire l'objet d'une approche de modélisation générique. Ce sont ces constantes qui nous intéressent ici et qui nous conduisent à proposer un patron de modélisation.
Comme nous l'avons déjà dit, une première préoccupation consiste à définir, s'il en existe, les arrêts qui décomposent la trajectoire en une séquence de déplacements.
Les arrêts peuvent être définis directement par l'objet mobile. C'est le cas, par exemple, de la trajectoire d'un représentant de commerce qui définira lui-même quand il est en arrêt pour visiter des clients, de même qu'il définira lui-même quand il commence et termine sa tournée, c'est-à-dire une trajectoire. Par contre, d'autres applications ne peuvent pas compter sur des interactions avec l'objet mobile. Elles doivent elles-mêmes repérer et définir les segments du parcours spatio-temporel qui peuvent constituer des arrêts, ainsi que le début et la fin de chaque trajectoire. C'est le cas du suivi des cigognes. Dans ce cas, c'est la connaissance que l'on a du comportement des animaux qui permet de fixer des règles qui, lors de l'analyse de leur déplacement, vont déterminer les composants : début, fin, arrêts et déplacements. Les règles ci-dessous en sont des exemples.
-Les cigognes ne volent jamais de nuit. -Les cigognes s'arrêtent pour manger et pour se reposer.
-Le début d'une nouvelle trajectoire est détecté en observant dans la suite des positions spatio-temporelles de l'oiseau un changement entre une période de résidence stable de l'oiseau (plusieurs mois pendant lesquels sa position nocturne reste dans une même zone) et une période de déplacement (une suite de journées où l'oiseau se déplace en s'éloignant de sa zone de résidence). Le déplacement constaté doit se diriger approximativement vers le Sud s'il a lieu en automne, vers le Nord s'il a lieu en fin d'hiver. Les paramètres exacts de ce calcul (taille de la zone de résidence, durée du séjour, durée minimale permettant de conclure à un début de déplacement) sont à fixer en fonction des caractéristiques du comportement des cigognes. -La fin d'une trajectoire est détectée de façon similaire mais inverse, en observant un changement entre une période de déplacement et une période de résidence stable de l'oiseau. -Une suite de positions spatio-temporelles situées entre le début et la fin d'une trajectoire constituent un arrêt si elle satisfait les trois conditions suivantes. 1/ Les positions forment un nuage de points dont le rayon est inférieur à un certain seuil (par exemple 10 Km). 2/ L'intervalle de temps défini par l'instant de la première position et celui de la dernière position a une durée supérieure à un certain seuil (par exemple 30 minutes). 3/ Cette suite est maximale, c'est-à-dire que la position spatiotemporelle précédant la première position de la suite et celle suivant la dernière position ne satisfont pas les conditions. Pour l'arrêt ainsi défini, son point sera le centre du nuage de points et son intervalle sera formé des instants de la première à la dernière position de la suite. -On appellera arrêt de nuit un arrêt d'une durée supérieure à 6h, ayant lieu approximativement du coucher du soleil au lever du soleil le lendemain. -On appellera arrêt de repos tout autre arrêt.
Une autre caractéristique des trajectoires est qu'une partie, importante, de leur sémantique est traduite par des liens entre la trajectoire et les autres objets de la base de données (en particulier, les objets géographiques comme les rues, bâtiments, villes, régions, pays, lacs...). Lorsque l'objet lié est un objet spatial, doté de sa géométrie, ces liens sont habituellement porteurs de contraintes topologiques (par exemple, pour exprimer que les arrêts de la trajectoire se situent nécessairement dans une ville). Lorsque ce n'est pas le cas, le lien est une simple relation classique d'association. Certains liens sont au niveau de la trajectoire entière, d'autres sont au niveau d'une partie de trajectoire ou de ses composants. Un lien à contrainte spatiale au niveau trajectoire exprime que chaque point de la trajectoire doit satisfaire la contrainte spatiale. Par exemple, telle tournée d'un camion de livraison (c'est-à-dire telle trajectoire du camion) est tout entière incluse dans telle région. Comme exemple de lien à contrainte spatiale au niveau composant, citons de nouveau le cas des trajectoires de migration des cigognes: les ornithologues peuvent lier chacun des arrêts à l'objet géographique surfacique qui est situé à cet endroit et qui présente un intérêt du point de vue ornithologique, tel qu'un marais ou une roselière au bord d'un cours d'eau. La relation sera une relation topologique de type inclusion, ce qui signifie que le point représentant la cigogne doit être dans la surface associée à l'objet géographique. Les ornithologues peuvent aussi vouloir mémoriser quels obstacles la cigogne a survolé, par exemple une ligne à haute tension. Ils pourront le faire en reliant à l'aide d'une relation topologique chaque déplacement de la trajectoire aux objets qui décrivent les obstacles surmontés.
En plus de liens vers des objets de la base de données, la description sémantique d'une trajectoire peut conduire à associer à la trajectoire ou à ses composants des attributs et des contraintes d'intégrité, et plus généralement toute caractéristique offerte par un modèle conceptuel. Le processus qui consiste à associer une information thématique aux instances de trajectoire est généralement appelé annotation sémantique. Des exemples d'annotation sont: le but de la trajectoire, le moyen de locomotion, le nom du lieu où se trouve l'objet mobile à chaque instant, le type d'activité de l'objet mobile durant ses déplacements ou durant ses arrêts, les rencontres effectuées lors des arrêts, et pour les migrations d'oiseaux l'altitude et -162 -RNTI-E-13 les conditions météorologiques lors des vols. Suivant le cas, ces annotations prennent la forme de valeurs d'attribut (par exemple, pour le but de la trajectoire) ou d'instanciation d'un lien vers un objet particulier de la base de données (par exemple, pour noter le nom du lieu où se trouve un point de la trajectoire, en supposant que ce lieu est représenté comme un objet dans la base de données).
Ces annotations peuvent avoir la même valeur pour toute la trajectoire (par exemple le but de la trajectoire) ou une valeur différente pour chaque composant de la trajectoire (par exemple la vitesse moyenne par déplacement, l'activité principale pendant chaque arrêt). Ainsi, selon le cas, l'attribut sera un attribut de la trajectoire, du déplacement, ou de l'arrêt. De plus, la valeur de l'attribut peut varier tout au long de la trajectoire ou du déplacement ou de l'arrêt. Dans ce dernier cas l'attribut sera variable dans le temps. Par exemple pour les cigognes, la valeur de l'attribut altitude varie pendant chaque déplacement. Quand il y a plusieurs annotations variables dans le temps, il faut déterminer quelles sont les règles d'échantillonnage utilisées par l'application. Si plusieurs annotations variables sont mesurées aux mêmes instants, alors elles seront décrites par un seul attribut complexe qui regroupera ces différentes valeurs et qui sera variable dans le temps. Il y aura ainsi une seule et même suite de points d'échantillon pour toutes les valeurs. Par contre si chaque annotation variable est mesurée selon une séquence d'instants qui est indépendante de la séquence des autres, alors chaque annotation sera décrite par un attribut simple variable dans le temps.
Des contraintes d'intégrité thématique, spatiale et temporelle peuvent aussi être associées aux trajectoires. Par exemple, un avion ne peut pas voler s'il n'y a pas une équipe de pilotes et d'hôtesses, les cigognes ne volent pas durant la nuit, les arrêts des représentants doivent inclure au moins un rendez-vous avec un client, un nombre limité d'arrêts est autorisé, ou encore la distance ou la durée entre deux arrêts ne doit pas être inférieure ou supérieure à un certain seuil. Très fréquemment les trajectoires des humains sont contraintes de suivre un réseau spécifique, par exemple les voitures et les camions ne peuvent se déplacer que sur le réseau routier. Une trajectoire contrainte par un réseau doit respecter deux types de contraintes : une contrainte topologique d'inclusion (le point mobile doit toujours être dans la géométrie du réseau), et les contraintes associées aux noeuds du réseau. En effet, le réseau routier décrit non seulement les voies, mais aussi les intersections avec les changements de direction autorisés (tourner à droite ou à gauche, faire demi-tour...). La position des objets qui se déplacent dans un réseau peut être décrite par rapport à la géométrie du réseau : chaque point mobile est alors défini par l'identifiant de la voie et sa position relative sur cette voie (par exemple au 12 ème km), Güting et al. (2006). Un type particulier de trajectoire contrainte est celui des trajectoires fixes et récurrentes, comme celle des trains, métros et bus, qui font toujours le même trajet avec les mêmes arrêts, avec des horaires prédéfinis. D'autres trajectoires ont des contraintes spatiales ou temporelles particulières, qui doivent être calculées dynamiquement. C'est le cas, par exemple, des trajectoires des oiseaux migrateurs qui font du vol plané grâce aux thermiques, qui ne peuvent pas traverser de grandes étendues d'eaux (les cigognes contournent la Méditerranée) ou s'effectuer de nuit à cause de l'absence de thermiques.
Les applications peuvent aussi souhaiter conserver des caractéristiques dérivées du mouvement, comme la direction, la vitesse, l'accélération (instantanée ou moyenne). Les informations de ce type sont calculables à partir de la fonction qui définit la trajectoire ou de la séquence de points d'échantillon et sont généralement obtenues en appelant des méthodes du type de données point mobile.
Enfin, les applications doivent avoir la possibilité de spécifier plusieurs représentations de la même trajectoire selon le point de vue, la résolution et les objectifs de l'application. Tous les composants et toutes les informations sémantiques de la trajectoire peuvent avoir plusieurs représentations. Par exemple, si on considère la trajectoire d'une cigogne, une application aura besoin de différencier les différents types d'arrêts le long de la trajectoire (de repos, de nuit, etc...) et décrira chaque type avec des informations spécifiques. Une autre application aura simplement besoin de savoir que l'oiseau s'est arrêté et à tel endroit pendant tant de temps, sans s'intéresser à d'autres informations relatives à cet arrêt. Enfin, une troisième application ne prendra en compte que les longs arrêts de repos et ignorera les arrêts qui ne durent qu'une nuit. Un mécanisme qui permet cette représentation multiple pour les objets spatio-temporels a été présenté dans MADS, Parent et al. (2006b).
En résumé, une trajectoire est composée d'un début, d'une fin, de déplacements et d'arrêts. Un modèle conceptuel pour trajectoires doit permettre la définition de trajectoires et de leurs composants avec des attributs, des liens vers les objets de la base de données, des contraintes thématiques, spatiales et temporelles. Chaque élément a une géométrie, qui est ponctuelle pour le début, la fin et les arrêts, et qui est linéaire pour les déplacements. Chaque élément a une extension temporelle qui est un instant pour le début et la fin, et un intervalle de temps pour les arrêts et les déplacements. Ces derniers peuvent donc être annotés avec des attributs variables ou non dans le temps.
Modélisation des trajectoires
Certains modèles conceptuels pour bases de données incluent la description des aspects spatiaux et temporels, par exemple Parent et al. (2006a), Tryfona et al. (2003), Bédard et al. (2004). Ces modèles permettent de décrire des objets en mouvement et répondent donc en partie aux besoins de modélisation des trajectoires : la description de la composante géo-temporelle. Mais ils n'offrent pas de constructeurs spécifiques aux trajectoires; les notions de trajectoire, de début, de fin, d'arrêt et de déplacement ne font pas partie du modèle et seront donc ignorés du SGBD sur lequel la base de données sera implantée. Ces notions restent alors du domaine exclusif des utilisateurs et des programmes d'application. Nous avons donc étudié différentes solutions pour compléter les services offerts par les SGBD spatiotemporels, et en avons développé deux. La première est basée sur la définition d'un patron de modélisation, la seconde sur celle de nouveaux types de données. Dans cet article, nous présentons la première solution, la seconde est décrite dans Spaccapietra et al. (2008).
Le concept de patron de modélisation -de l'anglais "design pattern" Gamma et al. (1995) -vient du génie logiciel. Appliqué aux bases de données, il dénote un schéma prédéfini qui décrit une solution simple à un problème de modélisation récurrent. Il peut être utilisé par le concepteur de bases de données dans l'étape de modélisation. Le patron est en effet prévu pour être facilement intégré dans un schéma de bases de données. Notre patron de modélisation pour trajectoires vise à représenter explicitement les trajectoires et leurs composants, début, fin, arrêts et déplacements. Cette solution utilise un modèle conceptuel spatio-temporel sans le modifier, ce qui permet de l'implémenter facilement sur tout SGBD qui offre un support pour le spatial (par exemple Oracle, DB2 ou SQL Server) ou sur un système d'information géographique. Cette solution réifie les composants des trajectoires : le début, la fin, les arrêts et les déplacements sont représentés par des objets. Cette réification a pour but de satisfaire un des besoins vus aux paragraphes précédents : le début, la fin, les arrêts et les déplacements doivent pouvoir être reliés à des objets de la base de données. Pour cela, ils doivent être représentés eux aussi comme des objets. En effet, les modèles de données permettent de relier entre eux des éléments de premier rang (les entités en entité-relation, les objets en orienté-objets, les tables en relationnel), mais pas les attributs. Cette solution a l'avantage de donner des schémas clairs, lisibles et qui permettent facilement de compléter la description des trajectoires avec l'information sémantique spécifique à l'application. De plus, ces schémas peuvent évoluer facilement.
Le paragraphe 6.1 décrit dans ses grandes lignes le modèle conceptuel spatio-temporel, MADS, dans lequel est décrit le patron pour trajectoires. Le patron est présenté en détail au paragraphe 6.2, et un exemple de schéma utilisant le patron est décrit au paragraphe 6.3.
Le modèle conceptuel spatio-temporel MADS
Le modèle MADS est un modèle de données de type entité-relation étendu au spatial, au temporel et à la multi-représentation (possibilité d'avoir plusieurs représentations différentes pour une même information). Pour la dimension thématique, on notera simplement que MADS permet de décrire des attributs simples et des attributs complexes (composés d'autres attributs), des attributs monovalués (qui prennent une valeur) et des attributs multivalués (qui peuvent prendre plusieurs valeurs).
Pour la dimension spatiale, MADS offre un jeu de types de données spatiales, organisé en une hiérarchie. Les types les plus courants sont Point, Line, Surface, et le type générique Geo qui contient toutes les valeurs spatiales qu'elles soient simples ou complexes. De même il y a une hiérarchie de types de données temporels dont Time est la racine et Instant, TimeInterval, InstantSet sont les plus courants. En MADS, les types d'objet, tout comme les types de relation, peuvent être spatiaux et/ou temporels. Un type d'objet (ou de relation) spatial possède un attribut particulier, appelé geometry, qui décrit son emprise spatiale et dont le domaine de valeurs est l'un des types spatiaux. Un type d'objet (ou de relation) temporel possède un attribut particulier, appelé lifecycle, qui décrit sa durée de vie et les états par lesquels il passe au cours de sa vie : actif, suspendu, invalidé. Certains types de relations contraignent les objets qu'ils lient. Ce sont d'une part les relations topologiques qui relient deux objets spatiaux et contraignent leurs attributs geometry à satisfaire un prédicat topologique, par exemple adjacence, intersection ou inclusion, et d'autre part les relations de synchronisation qui relient deux objets temporels et contraignent leurs attributs lifecycle à satisfaire un prédicat temporel, par exemple l'un doit succéder à l'autre. Les attributs, quelles que soient leurs caractéristiques, peuvent varier dans le temps et/ou dans l'espace. Dans ce cas, leur valeur est définie au niveau conceptuel par une fonction qui associe à chaque instant et/ou chaque point un élément du domaine de définition de l'attribut. Si l'attribut est spatial (c'est-à-dire qu'il a pour domaine un des types de données spatiales) et variable dans le temps, alors cet attribut décrit un déplacement et/ou une déformation. Par exemple, un attribut de domaine Point et variable dans le temps a pour valeur un point mobile. Il existe trois catégories d'attribut variable : attribut variable en continu, par paliers ou attribut variable discret. Chacune correspond à une façon de varier des phénomènes décrits. Par exemple, la température varie de façon continue, le salaire d'un employé varie par paliers. Selon la catégorie d'attribut variable, les valeurs sont relevées de façon différente. Les valeurs d'un attribut variable en continu ou discret sont fournies sous la forme d'une suite de couples (valeur-instantanée, instant), et celles d'un attribut variable par paliers sous la forme d'une suite de couples (valeur-instantanée, intervalle-de-temps).
Le lecteur qui voudrait avoir plus de détails sur le modèle de données MADS est invité à consulter Parent et al. (2006a).
Patron de conception pour les trajectoires
Comme nous l'avons vu, un patron de conception pour les trajectoires doit comprendre un type d'objet pour représenter chacun des concepts : trajectoire, début, fin, arrêt, et déplacement. La figure 2  Les deux types d'objet Noeud et Déplacement sont reliés par deux types de relation, De et Vers, représentant le fait que chaque déplacement commence et finit par un arrêt. Ces deux relations sont à la fois topologiques et de synchronisation (à contrainte de type adjacence temporelle avant/après). Les deux contraignent chaque déplacement à être lié à des arrêts qui leur sont adjacents dans l'espace et dans le temps. Dans les modèles de données spatiotemporels, la sémantique des relations spatiales qui contraignent les géométries de deux objets doit être étendue au cas où l'un des deux objets (voire les deux) bouge ou se déforme. À cette fin en MADS, les relations topologiques qui lient des objets qui bougent ont été étendues en demandant au concepteur de définir la portée temporelle de la contrainte : la contrainte doit être satisfaite soit au moins pendant un instant, soit pendant toute la durée de vie de l'objet, ce qui dans notre cas revient à toute la durée de la trajectoire ou toute celle d'un arrêt ou d'un déplacement. Par exemple, le fait qu'une cigogne passe au-dessus d'une ligne à haute tension pendant un certain déplacement, sera représenté en deux dimensions par une relation topologique d'inclusion du point mobile (la cigogne) dans la ligne (haute tension) pendant au moins un instant du déplacement. Dans le patron de la figure 2, la contrainte topologique de la relation De (Vers), qui lie un déplacement à son noeud de départ (fin), est une contrainte d'égalité qui doit être satisfaite au premier (dernier) instant du déplacement.
FIG. 2 -Un patron de modélisation de trajectoires
En plus de la description de la structure interne de la trajectoire, le patron inclut des relations-crochets qui seront utilisées par le concepteur pour relier la trajectoire aux objets de l'application. Dans la figure 2, les noms des crochets sont écrits en italiques. Le type d'objet Trajectoire est relié à un type d'objet-crochet ObjetATrajectoires qui représente l'objet du monde réel qui effectue les trajectoires. Début, fin et arrêt (Noeud) peuvent être reliés à des types d'objets spatiaux qui sont représentés par le type d'objet-crochet spatial générique, ObjetGéographique1. La relation EstDans impose une contrainte topologique d'inclusion : le noeud doit être situé dans l'objet géographique. Comme ce crochet est optionnel, il est dessiné en pointillés. De façon similaire, les déplacements peuvent être reliés à un type d'objet-crochet spatial ObjetGéographique2 par une autre relation topologique d'inclusion, EstSur. Cette dernière peut être utilisée pour modéliser les trajectoires contraintes par un réseau.
-167 -RNTI-E-13
Modélisation conceptuelle des trajectoires
FIG. 3 -Le patron de trajectoires utilisé pour l'application des Cigognes
Un exemple d'emploi du patron de trajectoires
Quand un patron est utilisé, les concepteurs doivent l'adapter à leur application. Ils peuvent, par exemple, supprimer des éléments sans intérêt pour l'application, ajouter de nouveaux éléments répondant à des besoins supplémentaires ou modifier la structure du patron pour l'adapter à leurs besoins. Par exemple, des types d'objet Début, Fin et Arrêt peuvent être ajoutés comme sous-types de Noeud. Afin de connecter le patron au reste du schéma, les concepteurs doivent remplacer les types d'objet-crochet par des types d'objet de l'application ou rajouter d'autres relations-crochets spécifiques à l'application.
La figure 3 montre un exemple d'utilisation du patron de trajectoires pour l'application de migration des cigognes. Les concepteurs ont personnalisé le patron de la façon suivante. Le type d'objet-crochet ObjetATrajectoires a été mis en correspondance avec le type d'objet Cigogne. Le type d'objet-crochet ObjetGéographique1 et sa relation ont été dédoublés et sont devenus Pays et Zone (ce dernier avec sa hiérarchie de sous-types). Zone et ses sous-types décrivent les zones d'intérêt pour les oiseaux. De la même façon, le type d'objet-crochet ObjetGéographique2 et sa relation ont été dédoublés. La relation PasseSur, qui décrit le fait que l'oiseau est passé au-dessus d'une zone dangereuse pour lui, est une relation topologique de type inclusion pour un instant au moins : Pour qu'une instance de Déplacement puisse être liée à une instance de DangerNaturel le point mobile du déplacement doit être au moins un instant dans la zone du danger naturel. Quant à la relation PassePrès son type a été modifié. C'est une relation métrique qui contraint la distance entre les objets liés. Un nouveau lien, non spatial, Avec, a été ajouté au type d'objet Déplacement; il décrit le groupe -s'il est connu -avec lequel l'oiseau a fait ce vol (ce déplacement).
Finalement, des attributs spécifiques à l'application ont été ajoutés aux types d'objets du patron. Par exemple, la direction (Nord/Sud), l'année de la trajectoire, la météo durant la trajectoire ont été ajoutés à Trajectoire; l'altitude est enregistrée le long des déplacements; pour chaque arrêt où l'oiseau a été observé, le type de l'arrêt, les activités de l'oiseau, son poids et son pourcentage de graisse sont enregistrés. Les attributs météo et altitude sont des attributs variables dans le temps, c'est-à-dire que l'historique de la météo est enregistré durant la totalité de la trajectoire et l'historique de l'altitude de l'oiseau est enregistré durant chaque déplacement. En pratique, la fonction spécifiant un attribut continu variable dans le temps est définie par une liste de couples (valeur, instant), appelées valeurs d'échantillon (ou points d'échantillon dans le cas de points variables dans le temps) et par des fonctions d'interpolation entre les points de l'échantillon. Quand un objet comporte plusieurs attributs variables dans le temps, a priori leurs fonctions sont indépendantes, c'est-à-dire que leurs valeurs d'échantillon sont définies pour différents instants. Cependant, si les valeurs de plusieurs attributs variables sont toujours enregistrées en même temps, ces attributs sont temporellement liés. Et cela doit être explicitement spécifié par une contrainte d'intégrité ou en regroupant les attributs dans un attribut complexe qui sera, lui, variable dans le temps. Par exemple, dans la figure 3, météo est un attribut complexe variable dans le temps dont les valeurs d'échantillon possèdent le format suivant: (instant, vent, température, pression, ciel).
D'un autre côté, le type d'objet Déplacement possède deux attributs variables dans le temps, geometry et altitude qui ne peuvent pas être regroupés dans un attribut complexe parce que geometry est un attribut prédéfini par le modèle de données MADS. Une contrainte d'intégrité est donc nécessaire afin d'exprimer la contrainte suivante : "Les listes de points d'échantillon des attributs geometry et altitude sont basées sur la même liste d'instants". Enfin, afin de faciliter l'accès à l'information pour les utilisateurs, les concepteurs peuvent choisir de stocker de l'information redondante et de gérer la cohérence de cette redondance via des contraintes d'intégrité. Par exemple, l'attribut naissance.année du type d'objet Cigogne est défini comme dérivé de l'instant de début du cycle de vie de l'objet et l'attribut Nord/Sud de Trajectoire comme dérivé des positions spatiales du début et de la fin de la trajectoire (c'est-à-dire les positions des premier et dernier objets Noeud).
Implémentation de l'approche
Afin de valider nos approches de modélisation des trajectoires, nous sommes en train de recueillir les descriptions et les données de plusieurs applications portant sur des trajectoires. Nous modélisons les bases de données des applications, puis les implémentons sur uun SGBD relationnel étendu au spatial (Oracle 10). Nous utilisons ces bases de données de plusieurs façons : pour répondre à des requêtes ponctuelles de type classique (SQL), pour faire de la fouille des trajectoires à l'aide de programmes spécifiques, et enfin nous montons un entrepôt spécialisé pour les trajectoires.
Dans ce paragraphe, nous présentons pour l'application des cigognes le schéma relationnel de la base de données qui correspond au schéma conceptuel de la figure 3 et, à titre d'exemples, quelques requêtes SQL. Le schéma relationnel (voir la figure 4)  
FIG. 4 -Schéma relationnel correspondant à la figure 3
Une autre spécificité des schémas MADS est la présence de types de relation topologique ou de synchronisation temporelle qui contraignent les géométries ou les cycles de vie des objets liés. Ces types de relation à contraintes sont traduits de la même façon que des relations classiques, mais avec en plus un trigger qui vérifie lors des insertions et des mises à jour des objets liés que leurs géométries ou leurs cycles de vie satisfont bien la contrainte. Par exemple, le type de relation topologique EstDans, qui lie Noeud à Pays dans la figure 3, est traduit, dans la À titre d'exemple, nous donnons deux requêtes SQL qui peuvent être posées à la base de données. La requête "Combien de fois la cigogne Max s'est-elle arrêtée durant chacune de ses trajectoires ?" peut être exprimée comme suit : SELECT t.N°Traj, t.Debut, COUNT(n.N°Noeud) FROM Trajectoire AS t, Noeud AS n WHERE t.N°Traj=n.N°Traj AND t.nomCigogne="Max" GROUP BY t.N°Traj, t.Debut Quant à la requête : "Quelles activités a eu la cigogne pendant chaque arrêt de la trajectoire 133 ?", elle s'écrit très simplement :
SELECT N°Noeud, activité FROM NoeudActivité WHERE N°Traj=133
Conclusion
Dans les applications qui s'intéressent à des objets mobiles, une connaissance approfondie des trajectoires spatio-temporelles de ces objets est très souvent indispensable à la réalisation des objectifs de l'application. Par exemple, l'optimisation d'un réseau de transport demande que l'on recueille les données sur les déplacements de la population concernée et que l'on modélise ces déplacements. Le plus souvent la connaissance recherchée ne se limite pas au tracé spatio-temporel des déplacements. Elle peut inclure d'autres informations, par exemple le moyen de transport utilisé, la classe d'âge de la personne ou sa capacité de mouvement (invalides, handicapés), ou encore des informations sur les lieux visités. La description des informations pertinentes relatives à une trajectoire peut donc conduire à des constructions complexes qui combinent des caractéristiques relatives au mouvement brut (où est l'objet et quand) avec une variété d'annotations sémantiques décrivant les connaissances spécifiques nécessaires à chaque application.
La contribution de cet article est essentiellement de définir précisément ce concept de trajectoire et d'en donner une caractérisation qui, tout en s'appuyant sur une étude des besoins, est indépendante de toute application particulière. En particulier, nous proposons de structurer les trajectoires grâce aux concepts complémentaires d'arrêt et de déplacement, ce qui permet d'enrichir le contenu sémantique d'une trajectoire. Ensuite, l'article propose une approche de modélisation des trajectoires à l'aide d'un patron de conception (design pattern), autrement dit une construction standard (ici exprimée dans les termes d'un modèle entité-relation) que les concepteurs peuvent réutiliser quelle que soit la nature de leur application. L'article détaille les composantes du patron proposé et la manière de l'intégrer dans le schéma de la base de données de l'application. Enfin, l'implémentation du patron sur un SGBD relationnel-objet est montrée.
À notre connaissance, ce travail est le premier à proposer une approche conceptuelle pour modéliser la sémantique des objets mobiles. Il ajoute ainsi une couche sémantique par rapport aux approches habituelles de modélisation qui représentent uniquement les déplacements bruts des objets mobiles.
Le travail présenté peut être étendu dans plusieurs directions. Par exemple, il conviendrait d'explorer les contraintes particulières des trajectoires liées à un réseau (par exemple, les voitures circulant sur un réseau routier) afin de déterminer l'influence de ces contraintes en termes de modélisation. Dans le cadre du projet GeoPKDD (http:/geopkdd.it) nous explorons une stratégie similaire de modélisation pour un entrepôt de trajectoires, première étape pour réaliser des fouilles de données destinées à extraire des trajectoires des connaissances plus synthétiques pour les décideurs du domaine étudié (voir, par exemple, Alvarez et al. 2007et Giannotti et Nanni 2006.
Le travail pour le développement d'un entrepôt de trajectoires doit notamment conduire à la spécification d'opérateurs d'agrégation de trajectoires (voir Braz et al. 2007). Différents types d'agrégation sont possibles et peuvent être réalisés à différents niveaux en distinguant par exemple les opérateurs appliqués aux composants d'une trajectoire des opérateurs appliqués sur la trajectoire entière. Un exemple d'opération sur les composants est l'agrégation qui remplace un ensemble d'arrêts dans une région par un arrêt unique représentant l'intégralité du temps passé dans cette région. Un exemple d'opération réalisée sur la trajectoire entière est l'agrégation qui remplace un ensemble de trajectoires par la trajectoire moyenne calculée à partir de l'ensemble.

Introduction
L'utilisation des descripteurs locaux permet d'obtenir de bons résultats pour la reconnaissance d'images, la classification d'images et la recherche d'images par le contenu. Ces descripteurs sont robustes aux changements de contenu. Cette méthode a été proposée en 1997 par C. Schmid dans (Schmid et Mohr, 1997). Récemment, les méthodes développées originellement pour l'analyse des données textuelles (ADT) comme pLSA (probabilistic Latent Semantic Analysis) (Hofmann, 1999a), LDA (Latent Dirichlet Allocation) (Blei, 2003) sont appliquées en analyse d'images, par exemple pour la classification des images (Willamowski, 2004), la découverte des thèmes dans l'image (Sivic et al., 2005), la classifications des scènes (Bosch et al., 2006), et la recherche d'images (Lienhart et Slaney (2007)).
Dans ce travail, nous utilisons l'analyse factorielle des correspondances (AFC) pour la recherche d'images. Etant donné une image requête, le système doit retourner les images (dans la collection des images) les plus similaires à la requête. L'AFC permet de réduire l'espace pour représenter les images et calculer la similarité entre les images dans cet espace réduit. Les deux contributions principales de cet article sont l'utilisation de l'AFC pour la recherche d'images, et la proposition d'un prototype de recherche d'images utilisant des fichiers inversés basés sur la qualité de représentation des facteurs de l'AFC.
L'article est organisé de la façon suivante : nous décrivons brièvement les méthodes pLSA et l'AFC dans la section 2. La section 3 présente la recherche d'images par l'AFC. La section 4 est consacrée aux résultats expérimentaux. Dans la conclusion, nous présentons les perspectives de ce travail.
Méthodes
Représentation des images
Qu'il s'agisse des méthodes comme le pLSA (probabilistic semantic latent analysis) ou l'AFC, il faut d'abord représenter le corpus sous forme d'une matrice d'occurrences F (un tableau de contingence) de dimension M x N où M désigne le nombre de documents et N indique le nombre de différents mots apparaissant dans le corpus. Chaque case de la matrice F ij décrit le nombre de fois où le mot j (indice de colonne) est observé dans le document i (indice de ligne). Une telle représentation ignore l'ordre des mots dans un document et est appelée modèle sac-de-mots (bag-of-words).
Il n'y pas de mots au sens littéral du terme dans les images. Il faut donc les construire.
Construction des mots visuels
Les mots dans les images, appelés mots visuels, doivent être calculés pour constituer un vocabulaire de N mots. Chaque image sera donc représentée enfin par un histogramme de mots. La construction des mots visuels se fait en deux étapes : (i) calcul des descripteurs locaux pour un ensemble d'images, (ii) classification (clustering) des descripteurs obtenus. Chaque cluster correspondra à un mot visuel. Il y aura donc autant de mots que de clusters obtenus à l'issue de l'étape (ii).
Le calcul des descripteurs locaux dans une image se fait aussi en deux étapes : il faut d'abord détecter des points d'intérêt dans l'image. Ces points d'intérêt sont, soit des maximums du Laplacien de Gaussien (Lindeberg, 1998), soit des extremums locaux 3D de la différence de Gaussien (Lowe, 1999), soit des points extraits par un détecteur Hessian-affine (Mikolajczyk, 2004). Ensuite, le descripteur de ce point d'intérêt est calculé sur le gradient des niveaux de gris dans la région autour du point. On a sélectionné des descripteurs invariants à la rotation et au changement d'échelle, les descripteurs SIFT (Lowe, 2004). Chaque descripteur SIFT est un vecteur à 128 dimensions. La seconde étape consiste à former des mots visuels à partir des descripteurs locaux calculés à l'étape précédente. La plupart des travaux effectue un k-means sur les descripteurs locaux et prend les moyennes de chaque cluster comme mots visuels (Willamowski, 2004, Sivic, 2005, Bosch et al., 2006.
Après avoir construit le vocabulaire visuel, chaque descripteur est affecté au cluster le plus proche. Pour cela, on calcule dans R 128 les distances de chaque descripteur aux représen-tants des clusters définis précédemment. Une image est ensuite caractérisée par la fréquence de ses descripteurs dans chaque cluster. On obtient ainsi un tableau de contingence croisant les images et les clusters.
Dans nos expérimentations, nous avons utilisé la méthode décrite dans (Mikolajczyk, 2004) pour détecter des points d'intérêt. Le vocabulaire est construit en appliquant un kmeans sur environ 300000 descripteurs tirés aléatoirement (un tiers pour chaque catégorie : faces, motorbikes, airplanes, background et cars). Le vocabulaire obtenu est de 2224 mots pour 4090 images. Ce choix de 2224 mots a été effectué par Sivic (Sivic et al., 2005).
PLSA
Introduite par Thomas Hofmann (1999a), le pLSA est une technique statistique qui s'inspire du LSA (Latent Semantic Analysis) (Deerwester, 1990) pour l'analyse des tableaux de contingence. LSA est une méthode purement géométrique qui ressemble beaucoup aux méthodes factorielles ; dans pLSA, on introduit un modèle probabiliste : la distribution des mots dans une image est considérée comme multinomiale. La méthode se base sur une dé-composition des mélanges dérivée d'un modèle de variables latentes.
pLSA introduit une variable latente z ? Z = {z 1 , z 2 , …, z K } et modélise la probabilité jointe par :
Le logarithme de la vraisemblance du corpus est défini par : 
Analyse factorielle des correspondances
L'AFC est une méthode exploratoire classique pour l'analyse des tableaux de contingence. Elle a été proposée par J. P. Benzécri (1973) dans le contexte de la linguistique, c'est-à-dire pour l'analyse de données textuelles. La première étude a été réalisée sur les tragédies de Racine. L'AFC sur un tableau croisant des mots et des documents permet de répondre aux questions suivantes : y a-t-il des proximités entre certains mots ? Y a-t-il des proximités entre certains documents ? Y a-t-il des liens entre certains mots et certains documents ? L'AFC comme la plupart des méthodes factorielles utilise une décomposition en valeurs singulières d'une matrice particulière et permet la visualisation des mots, et des documents dans un espace de dimension réduit. Cet espace de dimension réduit a la particularité d'avoir un nuage de points projetés (mots et/ou documents) d'inertie maximale. Par ailleurs, l'AFC fournit des indicateurs pertinents pour l'interprétation des axes comme la contribution d'un mot ou d'un document à l'inertie de l'axe ou la qualité d'un mot et/ou d'un document sur un axe (Morin, 2004).
Pour déterminer le meilleur sous-espace de projection des données, on calcule les valeurs propres et les vecteurs propres de la matrice de taille N x N, où
X est la transposée de X . On obtient alors les valeurs propres ? et les vecteurs propres u :
On ne garde que les K (K < N) premières valeurs propres les plus grandes et les vecteurs propres associés. Ces K vecteurs propres constituent une base orthonormée de l'espace réduit (appelé aussi espace des facteurs). Le nombre de dimensions du problème passe de N à K. Les documents (images) sont projetés dans le nouvel espace réduit :
Dans cette formule, X P 1 ? représente les profils lignes et A est la matrice de transition associée à l'AFC. La projection des mots dans le sous-espace de dimension K est fournie par la formule suivante:
Un nouveau document (i.e. 
Mesure de similarité des images
Après avoir calculé les coordonnées des images dans le nouvel espace (i.e. représentation probabiliste dans le cas de pLSA, sous-espace pour l'AFC), nous devons définir la similarité entre une requête et les images. Plusieurs métriques pour cette mesure de similarité sont disponibles, parmi les quelles les 4 mesures suivantes :
Distance Manhattan (norme 1):
Divergence de Jensen-Shannon :
Dans les expérimentations, nous avons utilisé la distance euclidienne et celle de cosinus.
Recherche d'images par l'AFC
AFC pour réduction de dimension
Un des avantages de l'AFC est de réduire la dimension du problème. Pour tenir compte du nombre d'images, il est préférable d'utiliser une structure d'indexation sous forme d'arbre comme un arbre k-d (Bentley, 1975). Cependant, une telle structure deviendra inefficace quand le nombre de dimensions est supérieur à 16 à cause de la malédiction de la dimension (Bellman, 1961). Enfin, l'indexation par l'arbre utilise la distance euclidienne la plupart du temps car on rencontre des difficultés lorsqu'on travaille avec d'autres distances.
Passage à l'échelle
Il y a deux indicateurs importants pour l'interprétation et l'évaluation en AFC. Ce sont la contribution des images à l'inertie d'un axe (facteur) d'une part et la qualité de représenta-tion des images sur un axe d'autre part. Dans les expérimentations, nous avons constaté que la distance cosinus donnait de meilleurs résultats que la distance euclidienne. La distance cosinus est directement liée à la qualité de représentation des images sur les axes. Pour cette raison, nous avons proposé un nouveau prototype de recherche d'images utilisant des fichiers inversés basés sur la qualité de représentation. Cela permettra de réduire le nombre d'images à considérer lors du calcul de leur similarité avec la requête. 
RNTI -X -
Après avoir réduit la dimension des données à K, on construit, pour chaque axe, 2 fichiers inversés (un pour la partie positive et un autre pour la partie négative)
1 . Chaque fichier contient des images ayant une bonne qualité de représentation sur la partie (positive, néga-tive) de l'axe associé. Le seuil ? est un paramètre qui contrôle la qualité de résultat et le temps de recherche. Dans les expérimentations, nous avons choisi un seuil égal à la moyenne de la qualité de représentation, à la moitié, et à un quart de la moyenne. Plus le seuil est grand, plus le nombre d'images dans un fichier inversé est petit, moins il faut de temps de recherche mais la qualité de résultat diminue.
Prototype de recherche
Algorithme pour rechercher une image requête q est donné dans le tableau 1.
Entrée : un vecteur q représentant l'image requête 1. Projeter q dans l'espace des facteurs suivant la formule [3] et trier les facteurs par leur qualité de représentation. 2. Prendre NF fichiers inversés associés à NF premiers facteurs. 3. Faire l'union des fichiers inversés et filtrer les images par ces fichiers inversés : une image sera conservée si elle apparaît dans au moins NF/2 fichiers. 4. Calculer la distance entre la requête et les images conservées dans l'étape 3. Sortie : la liste des images les plus similaires à l'image requête TAB. 1 -Algorithme de recherche utilisant des fichiers inversés.
Le nombre d'images retrouvées après le filtrage sera beaucoup plus petit que le nombre d'images dans la base. Donc, le temps de recherche sera considérablement réduit.
Le nombre NF est choisi empiriquement. Cependant, il est préférable que NF soit impair et inférieur à K/2 (pour le vote majoritaire dans l'étape de filtrage). Nous proposons une heuristique qui peut être utilisée pour déterminer NF en fonction de l'image requête. Cette heuristique se base sur l'observation suivante : si un point est bien représenté sur quelques axes, le cosinus carré sur ces axes sera grand et le cosinus carré des autres axes sera petit car la somme des cosinus carrés est égale à 1. Donc, on peut prendre NF axes tels que la somme des cosinus carrés sur ces axes est supérieure à ? (ex : ? = 0.75).
Résultats
Nous avons implémenté l'AFC en GNU Octave (Eaton, 1995). Sur un PC Pentium 4, 512MO avec système d'exploitation Linux, le temps d'exécution pour l'AFC sur un tableau de contingence de 4090 x 2224 est d'environ 3 minutes.
Ensemble de tests
Les tests ont été réalisés sur la base Caltech4 (Sivic et al., 2005) tirée de la base Caltech101 (Fergus et al., 2003 La base est divisée en 10 sous-ensembles, et l'évaluation est faite par validation croisée.
RNTI -X -
AFC vs d'autres méthodes
Nous avons fait une AFC sur les données de la base Caltech4 et avons gardé les 7 premiers axes (pour la comparaison au pLSA, avec 7 modalités). Lorsqu'on augmente le nombre d'axes conservés, on constate que les premières images retournées (i.e. les 10 premières images) sont bonnes mais que la qualité se dégrade lorsqu'on retient un plus grand nombre d'images (voir le tableau 3). La distance euclidienne et la distance cosinus sont utilisées pour calculer les similarités dans les espaces de dimension réduite. Nous avons utilisé la courbe de précision -rappel pour comparer la performance des différentes méthodes.
FIG. 2 -Projection de la base Caltech4 sur les axes : à gauche, projection des images (sans background) sur les axes 1 et 2 ; à droite, projection des images (avec catégorie background) sur les axes 1, 2 et 3.
TF*IDF
Avec cette méthode, chaque élément F(i,j) dans le tableau de contingence est normalisé à TF(i,j) et pondéré par IDF(j) où TF(i, j) est le nombre de mots j qui apparaît dans l'images i divisé par le nombre de mots dans l'image i et IDF(j) = ln(N/N j ) où N i est le nombre d'images qui contiennent le mot j et N le nombre d'images dans la base. La distance euclidienne et la distance cosinus sont calculées sur les données pondérées.
pLSA
Nous avons appliqué un modèle pLSA avec 7 modalités (ce modèle donne les meilleurs résultats sur cette base (cf. Sivic et al., 2005)). Chaque image dans la base est représentée par sa distribution P(z|d). La dimension du problème est réduite à 7. Les nouvelles coordonnées servent à calculer la similarité entre la requête et les images dans la base (Hofmann, 1999b).
Discussion
Le nombre de thèmes (modalités de la multinomiale) dans pLSA et le nombre d'axes conservés en AFC sont des paramètres à régler. Le nombre d'axes conservés en AFC est difficile à choisir car les valeurs propres en AFC décroissent très lentement. Nous avons pris 7 axes en AFC pour la comparaison avec pLSA. La figure 3 montre les résultats quand on fait des tests sur 4 catégories (on ne tient pas compte de la catégorie « background ») et sur 5 catégories (avec la catégorie « background »). Le meilleur résultat est obtenu avec l'AFC quelle que soit la distance, euclidienne ou cosinus. Nous avons également testé TF*IDF avec la distance de Manhattan. Dans ce cas, on améliore le résultat mais ce n'est pas significativement supérieur aux résultats obtenus avec d'autres distances, ni avec pLSA et l'AFC.
FIG. 3 -Courbe de précision -rappel : à gauche, résultat pour 4 catégories (sans background) et à droite, résultat pour 5 catégories (avec background).
AFC avec fichiers inversés
Pour comparer la performance de la nouvelle méthode de recherche avec la méthode exhaustive, nous avons calculé la précision sur les 5, 10, 50 et 100 premières images retournées. Le paramètre K est pris égal à 7, 15 et à 30 ; le seuil pour des fichiers inversés est mis à 1/(4*K); NF est calé à 3. Les résultats figurent dans le tableau 3. La nouvelle méthode limite le nombre d'images pour lesquelles on calcule la similarité avec la requête. Cela diminue le temps de réponse. En général, la nouvelle méthode est 4 fois plus rapide que la méthode exhaustive (on gagne 75% du temps) avec une perte inférieure à 1% du taux de précision. Par ailleurs, dans certains cas (ex. K = 15, et avec 5 premières images retournées), elle fait mieux que la méthode exhaustive. Enfin, dans tous les cas, la nou-RNTI -X -velle méthode est meilleure que TF*IDF en qualité de résultat (plus de 10%) ainsi que le temps de recherche (13 fois plus rapide).
Les Quand NF est petit (ex. NF = 1), un seul axe ne suffit pas en terme d'information récupé-ré et donc le résultat n'est pas bon. Quand on prend plus d'axes (ex. NF = 3) la qualité augmente et le filtre devient également trop contraignant (dans le cas NF = 7, une image sera gardée si elle apparaît au moins 4 fois dans les fichiers inversés). Par conséquent, cela dé-grade la qualité du résultat. C'est pour cela qu'il faut choisir le paramètre NF en se basant sur la somme des cosinus carrés des NF premiers axes (cf. Section 3.1.2). Sur la base Caltech4, NF moyen est égale à 4.6 et notre proposition est justifiée a posteriori.
Conclusion et perspectives
Nous avons présenté dans cet article une nouvelle approche pour la recherche d'images par le contenu en utilisant l'AFC. Cette méthode est testée sur la base Caltech4 et comparée aux méthodes classiques comme : TF*IDF et pLSA. Les expérimentations ont montré que dans tous les cas, l'AFC donne le meilleur résultat. Nous avons aussi proposé une nouvelle approche utilisant des fichiers inversés basés sur la qualité de représentation des images sur les axes. La nouvelle méthode réduit le temps d'exécution et dans certains cas, améliore le taux de précision par rapport à la méthode exhaustive. Comme la plupart des méthodes de réduction de dimensions, une question concerne le nombre de dimensions à conserver. Dans la littérature, on conserve souvent 100 dimensions pour LSA et 30 pour l'AFC. On peut suggérer l'utilisation de la méthode Bayésienne de (Teh et al., 2004) pour déterminer ce nombre.
Pour le passage à l'échelle, les méthodes basées sur la décomposition de la base d'images en sous-bases sont prometteuses. La base peut être décomposée en clusters et l'AFC est appliquée sur les clusters. Un algorithme heuristique sera utilisé pour sélectionner les clusters dans lesquels il faudra chercher la réponse à la requête. Une autre amélioration des résultats est de combiner l'AFC avec d'autres méthodes comme CDM (Contextual Dissimilarity Measure) de (Jegou et al., 2007) et/ou des méthodes d'apprentissage pour les rangs (Chu, 2005 ;Burges, 2005).

Introduction
Les Machines à Vecteurs de Support (SVM) sont une méthode très populaire d'apprentissage supervisé pour la classification et la régression. Dans sa forme la plus simple pour la classification bi-classes, cette méthode est basée sur un classificateur linéaire séparant deux ensembles de points par un hyperplan. L'idée originale est de trouver un hyperplan séparant "au mieux" les points par la maximisation de la marge entre l'hyperplan séparateur et les points dans la base d'apprentissage. Cette formulation conduit à un problème d'optimisation d'une fonction convexe sous des contraintes linéaires. Récemment des extensions de cette technique de base et de l'approche de maximisation de la marge ont été proposées pour le traitement de données structurées comme les séquences, les arbres etc (Tsochantaridis et al., 2004).
La méthode originale de Vladimir Vapnik pour résoudre le problème d'optimisation avec contraintes des SVMs consiste à introduire des multiplicateurs de Lagrange pour chaque contrainte, et d'optimiser le problème dual équivalent. Cet algorithme est coûteux en temps et en mémoire. Par exemple, l'espace mémoire nécessaire (la matrice noyau est de taille N au carré, si N est le nombre d'exemples). Ces caractéristiques de complexité rendent difficile l'emploi de machines à vecteurs support et plus généralement de méthodes de maximisation de la marge dans certaines situations, lorsque l'on traite des données structurées ou bien lorsque l'on dispose de très grandes quantités de données d'apprentissage. Plusieurs voies ont été suivies pour dépasser les problèmes posés par l'optimisation dans ce cadre.
Certains travaux ont porté sur l'optimisation efficace du dual, par le contrôle du nombre de contraintes actives (Joachims, 2006), ou par la décomposition du problème d'apprentissage (Osuna et al., 1997). Dans ce dernier cas, l'algorithme SMO ou SVMLight par exemple, on ne s'intéresse à une itération donnée qu'à un nombre limité de variables actives.
Des travaux plus récents ont porté sur l'optimisation directe de la forme primale par l'usage de la fonction hinge(z)=max(0,z). Cela permet de se ramener à un problème d'optimisation sans contraintes où la fonction objectif est convexe. La difficulté de ces dernières approches vient du fait que la fonction hinge n'est pas dérivable en 0. Divers travaux ont alors proposé d'utiliser une version dérivable partout de cette fonction hinge par lissage ou bien en utilisant un coût quadratique car dans ce cas des méthodes d'optimisation standard peuvent être appliquées. Par exemple, (Chapelle, 2007) a montré qu'il était possible d'utiliser la méthode de Newton ou bien du gradient conjugué.
Une autre approche pour optimiser le primal consiste à utiliser la méthode du sous-gradient directement sur le problème d'optimisation de la fonction objectif non différentiable (Zhang, 2004). L'avantage de cette approche est sa simplicité, mais la vitesse de convergence est très dépendante du réglage du pas de gradient. Une exception est l'algorithme Pegasos (ShalevShwartz et al., 2007) qui est le seul algorithme de ce type ne nécessitant pas le réglage d'un hyperparamètre pour le pas de gradient.
Dans ce travail, nous nous plaçons dans le cadre de l'optimisation de fonction continue non partout dérivable. L'idée principale est que la minimisation du primal peut être attaquée comme un problème minimax sur un ensemble de fonctions quadratiques convexes définies sur des sous-espaces de l'espace des paramètres. Nous montrons que la fonction objectif n'est pas dérivable sur les frontières entre ces sous-espaces de définition et que ces frontières correspondent à des hyperplans associés à chaque exemple d'apprentissage. En analysant ces hyperplans dans l'espace des paramètres, nous décrivons une méthode efficace pour calculer la direction de plus grande pente et estimer le pas optimal. En exploitant ces résultats, nous proposons un nouvel algorithme d'optimisation qui se compare favorablement aux algorithmes de type Pegasos en mode batch et en mode on-line.
Nous décrivons tout d'abord le cadre d'optimisation dans lequel nous nous plaçons et les outils que nous allons utiliser. Ensuite nous décrivons notre algorithme en détails puis le validons expérimentalement en le comparant à des algorithmes de référence.
Préliminaires
Nous formalisons tout d'abord le problème que nous attaquons ici et qui consiste à optimiser une fonction objectif non partout différentiable. Nous rappelons ensuite quelques résultats sur ce type de fonctions et décrivons brièvement une méthode classique pour leur optimisation.
Formalisation
Considérons un problème de classification de données d'entrée x en 2 classes y ? {?1, +1}. Considérons une base d'apprentissage
Nous nous intéressons à l'apprentissage du classifieur linéaire : h w (x) = sign( w où w ? R d est l'ensemble des paramètres du classifieur à apprendre. Notons que w divise R d en deux sous-espaces, la frontière est un hyperplan d'équation H w : w = 0. L'idée principale de l'apprentissage vaste marge est de trouver un hyperplan séparant "au mieux" les points par maximisation de la marge entre l'hyperplan séparateur et les points de la base d'apprentissage. Cette formulation conduit à un problème d'optimisation d'une fonction convexe sous des contraintes linéaires :
où ? est un hyper-paramètre de l'algorithme qui permet de régler l'importance des deux termes de la fonction objectif. Les ? i sont des variables non-négatives qui représentent des pénalités pour les cas où la contrainte de marge n'est pas respectée pour l'exemple x i .
En introduisant la fonction hinge(z) = max(0, z), on obtient un problème équivalent sans contraintes :
Notons que cette fonction objectif est quadratique par morceau et convexe mais qu'elle n'est pas différentiable (par rapport à w) en certains points, sur les hyperplans H i : 1 ? y i i , w = 0. Il y a un hyperplan par exemple d'apprentissage. En des points qui n'appartiennent à aucun de ces hyperplans la fonction objectif est localement quadratique.
Chaque hyperplan H i divise l'espace des w en deux sous-espaces :
Dans chaque hypercube C k , la fonction objectif a une forme quadratique :
ensemble des indices des exemples qui violent la contrainte de la marge pour w ? C k . Par construction cet ensemble est identique pour tous les w d'un même hypercube C k , c'est pourquoi nous ne marquons pas la dépendance de I k à w. Avant de présenter notre algorithme en détail, nous commençons par quelques résultats théoriques sur l'optimisation de fonctions non-différentiables.
Généralités sur l'optimisation de fonctions non différentiables
Dans cette section, nous présentons quelque résultats concernant l'optimisation et l'analyse des fonctions convexes non-différentiables. Ces résultats, et leurs dérivations, peuvent être trouvés dans (Bertsekas et al., 2003;Demyanov et Vasilev, 1985). Nous introduisons tout d'abord la définition du sous-gradient et de la sous-différentielle d'une fonction convexe.
Soit f :
L'ensemble de tous les sous-gradients en x 0 est appelé la sous-différentielle en x 0 , et est noté par ?f (x 0 ). La sous-différentielle est un ensemble compact, non vide et convexe.
Théorème 1 (Demyanov and Vasilev) : Une condition nécessaire pour une fonction continue non partout dérivable, éventuellement non convexe, f (x) :
. Pour une fonction convexe, la condition est également suffisante. Si 0 / ? ?f (x * ) alors la direction ? = ?arg min d??f (x) est la direction du sous-gradient de plus grande pente.
.., m} un ensemble de fonctions convexes différentiables, alors la sous
Méthode du sous-gradient
Un façon d'optimiser le problème de l'équation (2) est d'utiliser la méthode du sousgradient, qui converge vers le minimum global (Bertsekas et al., 2003). Nous comparerons notre algorithme à une méthode de référence de ce type (Shalev-Shwartz et al., 2007). A chaque itération, le nouveau vecteur de paramètres w t est calculé par :
où ? t est le pas de sous-gradient et ? t ? ?f (w t ) est un sous-gradient quelconque de la fonction f à w t .
3 Descente de sous-gradient pour l'optimisation du primal
Dans ce travail, nous proposons d'appliquer un algorithme du type descente de sousgradient. La différence avec la méthode du sous-gradient est que nous choisissons la direction du sous-gradient selon la plus grande pente (plutôt que de prendre un sous-gradient quelconque dans la sous-différentielle), et que nous proposons une méthode optimale pour déterminer le pas de gradient. L'algorithme est résumé par le pseudo code décrit ce-dessous.
Cet algorithme a comme nous le verrons une complexité linéaire dans le nombre d'exemples d'apprentissage à chaque itération. En effet, il passe en revue tous les exemples d'apprentissage pour déterminer la direction de descente de plus grande pente et pour ensuite calculer le pas de gradient optimal dans cette direction. Nous décrivons maintenant ces étapes de calcul de la direction de plus grande pente du gradient et de calcul du pas de gradient optimal.
Entrées : BA = {(x 1 , y 1 ), ..., (x N , y N )} Sorties : w Initialization : w 1 = 0; t = 1 ; tant que vrai faire
Calculer la direction du sous-gradient de plus grande pente ? t ; si t = 0 alors return w t ; ; Calculer le pas de gradient optimal ? t ; Mise à jour : w t+1 = w t + ? t ? t ; t = t + 1; fin 3.1 Direction du sous-gradient de plus grande pente Rappelons que nous souhaitons minimiser la fonction objectif sous sa forme primale donnée dans l'Eq. (2). Nous cherchons dans un premier temps à déterminer la sous-différentielle de f (w) en un "point" w. Pour cela nous commençons par nous intéresser à la sous-différentielle de la fonction élémentaire max(0,
1?yi N
). Cette fonction est différentiable sauf pour les points w tel que y i i , w = 1. Pour un point de ce type on obtient en appliquant le théorème 2 :
Par ailleurs en un point w tel que y i i , w = 1 la fonction est dérivable et sa sousdifférentielle est réduite à sa dérivée. La sous-différentielle de la fonction élémentaire s'exprime donc suivant les cas par :
Finalement, en utilisant la proposition 1 de la section précédente, on obtient :
Il est intéressant de noter que cette formulation est proche de celle utilisée dans (Eizinger et Plach, 2003). Cela vient de la présence de fonctions hinge dans la formalisation de l'apprentisage du perceptron, qui peut s'écrire comme la minimisation d'une fonction linéaire par morceaux, en fait la somme des fonctions hinge(?y i i , w A noter que dans ce cas la fonction objectif est non strictement convexe et qu'il peut exister, dans le cas séparable, une infinité de solutions. Dans notre formalisation la fonction objectif comporte un terme quadratique 2 et des termes hinge(1 ? y i i , w ce qui rend le problème strictement convexe. Par ailleurs, notre fonction objectif vise à maximiser la marge ce qui doit conduire à de meilleures propriétés de généralisation.
La recherche de la direction du sous-gradient de plus grande pente peut être vu comme le problème des moindres carrés suivant :
Ce problème peut être résolu par des procédures standard (voir (Boyd et Vandenberghe, 2004)). Une fois ? * calculé on obtient la direction du gradient de plus grande pente par la direction donnée dans le Théorème 1 avec les valeurs de ? * données dans l'Eq. (11). Notons pour terminer que si K = 0 alors la fonction est différentiable en w et la direction du gradient de plus grande pente est simplement ? t = ?d 0 .
Pas de gradient optimal
FIG. 1 -L'espace des w.
Une fois la direction de recherche ? t déterminée comme décrit à la section précédente, nous proposons ici une méthode pour déterminer le pas de gradient de façon optimale. Cela revient à résoudre le problème d'optimisation unidimensionnel suivant :
Nous nous intéressons à la droite D t = {w = w t + ?? t |? ? R} et au comportement de f (w) sur cette droite. Si l'on "avance" sur la droite D t dans la direction de ? t , c'est à dire qu'on examine les w = w t + ?? t pour ? croissant, alors w va successivement croiser des hyperplans H i frontières entre hypercubes et traverser d'autres hypercubes. Au passage, la fonction f (w) change d'une fonction quadratique d'un hypercube à une autre (voir figure 1). La fonction de ? que nous cherchons à optimiser, g(?) = f (w t + ?? t ) est une fonction quadratique par morceaux (voir figure 2). Les points non-différentiables correspondent aux intersections entre w t + ?? et les hyperplan H i séparateurs. On peut caractériser l'intersection, si elle existe, entre la droite D t et un hyperplan H i : 1 ? y i i , w = 0 par une valeur particulière de ?, que nous notons ? i . Elle est déterminée par :
Une valeur positive de ? i signifie que l'hyperplan H i est "devant" w t (dans la direction ? t ) et une valeur négative de ? i signifie le contraire. Imaginons, sans perte de généralité, que la droite décrite par w = w t + ?? t en faisant croître ? de zéro vers l'infini traverse successivement les hyperplans H 1 , H 2 , H 3 ... (voir Figure  2a). Sur le segment ? n , ? n+1 , g(?) vaut :
i?I k(n) où k(n) est l'indice de l'hypercube correspondant au n ieme segment. Sa dérivée par rapport à la variable ? est :
A l'optimum cette dérivée, si elle existe, est nulle. Dans ce cas :
Si ? n opt appartient effectivement au n ieme segment, et que g(?) est dérivable en cette valeur alors le pas optimal vaut ? n opt . Cependant, g(?) peut ne pas être dérivable à l'optimum (cf. figure 2b), cela signifie que l'optimum correspond à un ? sur une frontière entre segments (i.e. le w optimal est sur une frontière, un hyperplan séparateur, entre hypercubes). Dans ce cas, pour tout n l'optimum de g n (?) est "en dehors" du n ieme segment. Une méthode simple pour vérifier si le n ieme point d'intersection ? n est la solution est de calculer ?
opt alors ? n est la solution, sinon ? n n'est pas la solution. Au final, notre algorithme pour trouver le pas de gradient optimal est décrit par le pseudo code ci-dessous. Dans ce code, nous notons L le nombre d'hyperplans à envisager (a priori inconnu mais nécessairement L ? N ), c'est à dire ceux correspondant à des exemples pour lesquels il existe une intersection.
Entrées : BA, w t , ? t Sorties : ? t Initialization; Estimer I 0 ? {i|y i i , w ? 1?w ? [w t , w t + ? 1 ?]}; Estimer les ? corespondant aux intersections de D t avec les hyperplans H i , sélectioner des ? non-negatives et les trier dans l'ordre croissant :
.., i L les indices des hyperplans (i.e. exemples d'apprentissage) correspondents; n = 0; ? 0 = 0; tant que vrai faire
Shrinking et Complexité
L'algorithme itératif présenté dans la section précédente se compare avantageusement à d'autres algorithmes d'optimisation batch proposés récemment, en termes de performance et de complexité algorithmique. Nous proposons ici une variante beaucoup plus rapide de cet algorithme. Rappelons que l'algorithme passe en revue tous les exemples d'apprentissage (ou plutôt les hyperplans correspondants) pour déterminer la direction de descente de plus grande pente et pour ensuite calculer le pas de gradient optimal dans cette direction. Or en pratique, il y a le plus souvent très peu d'exemples qui contribuent à l'estimation de la direction de recherche et à l'estimation du pas de gradient optimal. Ce sont des exemples que nous appelons des exemples actifs, les autres étant passifs. On peut donc espérer casser la complexité de l'algorithme en réduisant le problème d'estimation à chaque itération en ne considérant que les exemples actifs. Nous proposons ici d'utiliser une méthode de shrinking similaire à ce qui est utilisé dans les techniques d'optimisation du dual pour sélectionner à une itération donnée un nombre restreint de variables actives (Joachims, 1999).
Pour minimiser la fonction objectif de l'Eq. (2), nous n'allons considérer à chaque itération qu'un nombre limité d'hyperplans actifs, en cherchant à optimiser la fonction :
où L A représente la liste des exemples actifs et L I représente la liste des exemples inactifs violant la marge. L'itération d'optimisation est réalisée comme décrit à la Section 3, la seule différence venant du fait que certains hyperplans ne sont pas considérés (les termes correspondants sont rajoutés à d 0 , cf. Eq. (11)) et que l'on cherche une solution w t+1 dans l'espace délimité par les hyperplans actifs. La procédure de sélection des exemples/hyperplans est heuristique. A l'initialisation tous les exemples sont actifs. A une itération t on considère comme actifs les K t hyperplans les plus proches de la solution courante. Afin d'obtenir une optimisation très rapide, on réduit de moitié le nombre K t d'hyperplans actifs à chaque itération.
Pour garantir que la solution est correcte vis à vis des hyperplans sélectionnés, on restreint de plus l'espace de recherche à une boule B(w t , R t ) centrée autour de la solution courante et de rayon R t , qui est calculée comme la distance maximale entre la solution courante et les hyperplans actifs sélectionnés. Il s'agit d'une heuristique qui permet d'éviter la plupart du temps de traverser un hyperplan inactif lors de la mise à jour de w. Si c'est le cas la solution trouvée est identique à celle que l'on trouverait avec tous les hyperplans actifs. Si ce n'est pas le cas on n'a plus cette garantie. C'est la raison pour laquelle nous avons choisi, régulièrement, de tout réinitialiser en réactivant tous les hyperplans, soit lorsque le nombre K t est inférieur à un seuil (e.g. 10) soit lorsque le rayon R t tombe à zéro. Cette procédure permet de garantir la convergence comme pour l'algorithme originel de la Section 3.
.N }; L I = ?; R t = inf tant que vrai faire 1. Estimer la direction de recherche optimale ? t en considérant la fonction objectif de l'Eq. (11), pour les hyperplans actifs de L A 2. si |? t | = 0 alors Arrêt fin 3. Estimer le pas optimal ? t pour la fonction objectif de l'Eq. (12), dans la direction ? t et pour les hyperplans actifs de L A 4. Contraindre la nouvelle solution à appartenir à B(w t , R t ) : ? t = min(? t , Rt?1 ) 5. Mise à jour de w : 
Expériences
Nous décrivons ici des résultats expérimentaux obtenus en classification d'images de chiffres manuscrits sur la base MNIST 1 . Elle contient 60000 exemples d'apprentissage et 10000 exemples de test, les images sont en dimension 28x28. Nous avons prétraité les données via une analyse en composantes principales (ACP) afin de réduire la dimension des données à 50 dimensions (on ne garde que les 50 composantes des images sur les 50 axes principaux d'inertie). Il s'agit d'un prétraitement standard sur ces données, décrit par exemple dans (LeCun et al., 1998).
Nous avons comparé notre algorithme noté HyperPass (Hyperplane Passenger) avec l'algorithme Pegasos, basé sur la méthode de sous-gradient. Ce dernier s'est montré expérimentale-ment très efficace par rapport aux méthodes d'optimisation du dual pour les bases de données de grand dimension. Tout d'abord nous comparons la vitesse de convergence de Pegasos en mode batch et de notre algorithme sans la stratégie de shrinking. La figure 3 montre l'évolu-tion du primal en fonction du nombre de passages sur la base de données. On voit que notre algorithme HyperPass converge beaucoup plus rapidement que Pegasos. Notons toutefois que chaque passage de la base de données correspond à une itération de HyperPass avec complexité 2 × O(N d), tandis que la complexité d'une itération de Pegasos est en O(N d). On voit égale-ment que la courbe de Pegasos est large car la valeur primal oscille beaucoup entre itérations successives.
FIG. 3 -MNIST '0' vs all -Primal objectif.
Nous avons également étudié sur le même problème la version HyperPass avec shrinking et observé que cet algorithme converge après quelques cycles d'itérations (l'algorithme se termine au 7 ieme cycle, ce qui correspond à une complexité de 28 × O(N d)). Nous comparons les solutions de HyperPass Shrinking et de la version on-line de Pegasos (beaucoup plus rapide que la version batch) pour une même complexité algorithmique (tableau 1). Le paramètre K de Pegasos représente le nombre d'exemples utilisés pour faire un pas de sous-gradient, Enfin, nous comparons la solution de HyperPass Shrinking à celles obtenues avec l'algorithme Pegasos en le faisant tourner jusqu'à arriver à une valeur du primal seuil égale à celle obtenue par HyperPass + pour différentes valeurs de Le tableau 2 montre que Pegasos descend très vite au debut lorsque K est petit, mais que la vitesse de convergence est plus faible ensuite. Par exemple, dans le cas K = 10, il faut 51 passages pour arriver à une solution avec = 0.001 tandis qu'il faut 246 passages pour arriver à une solution avec = 0.0001. 
Conclusions
Nous avons proposé dans ce travail un nouvel algorithme d'apprentissage vaste marge pour des machines de type SVM. Cet algorithme optimise la fonction objectif sous sa forme primale en combinant la méthode du sous-gradient, des résultats sur l'optimisation de fonctions non

Introduction
Les réseaux de neurones supervisés sont d'excellents régresseurs permettant à la fois de classer des données ou de trouver des relations entre des entrées et des sorties (régression). Cependant ils sont bien souvent durs à mettre en oeuvre de part le nombre important de paramètres. L'utilisation d'algorithmes évolutionnaires (en particulier les algorithmes génétiques) permet de faciliter la mise en oeuvre de ces réseaux, en définissant leur structure ou les poids des connexions. Grâce à une inspiration fortement biologique, nous proposons un nouvel algorithme, RBF-Gene, qui permet une optimisation incrémentale d'un réseau RBF (structure et connexions) de manière efficace.
Algorithmes évolutionnaires et réseaux de neurones
Dans un réseau de neurones chaque neurone réalise un traitement simple et ce sont le nombre de neurones et leur connectivité qui vont faire toute la puissance du réseau. Dans les réseaux de neurones dits "en couche", le réseau est constitué de trois sous-ensembles de neurones : les neurones d'entrée, les neurones cachés (complètement connectés aux neurones d'entrée ou, si le réseau possède plus d'une couche cachée, à la couche précédente) et les neurones de sortie connectés à la dernière couche de neurones cachés.
Les réseaux RBF (Poggio et Girosi, 1989), pour "Radial Basis Function", sont des réseaux à une couche cachée dont les neurones cachés utilisent une fonction de transfert gaussienne tandis que les neurones de sortie réalisent une "simple" somme pondérée des réponses des neurones cachés. Ces réseaux sont des approximateurs universels (Park et Sandberg, 1991) et sont très efficaces pour des tâches de classification ou de régression. Leur utilisation passe par le choix d'un grand nombre de paramètres libres (le nombre de neurones cachés, leurs paramètres -moyenne et écart-type -et les poids de leur combinaison linéaire). Or ces paramètres sont tous interdépendants ce qui rend leur détermination difficile.
Les algorithmes évolutionnaires (AE) ont souvent été employés pour paramétrer des ré-seaux de neurones. Ils recouvrent différentes techniques inspirées de l'évolution biologique : algorithmes génétiques, programmation génétique, stratégies d'évolution, évolution grammaticale, etc. . . Cependant, le principe général est toujours le même : une population de solutions est générée aléatoirement puis va successivement subir une phase de sélection puis de reproduction jusqu'à un critère de terminaison.
On peut classer les AEs permettant l'optimisation de réseaux de neurones en trois caté-gories. Les optimiseurs de poids (Blanco et al., 2001) permettent d'optimiser les poids d'un réseau dont la structure a été fixée préalablement. Les optimiseurs de structure (MacLeod et Maxwell, 2001;Barrios et al., 2001) permettent de tester différentes structures pour le réseau de neurones (un second algorithme étant utilisé pour déterminer les poids). Enfin, les optimiseurs de structure et de poids (Arotaritei et Negoita, 2002;Ku¸sçuKu¸sçu et Thornton, 1994) permettent d'optimiser simultanément la structure du réseau et les poids des connexions.
Dans ces derniers algorithmes, la représentation choisie est souvent de taille variable, une partie correspondant à la structure du réseau (nombre de neurones cachés et leur placement) et une autre aux poids des connexions entre les neurones cachés et les sorties. Or la structure d'un réseau a une forte influence sur le nombre de connexions et donc de poids. Toute mutation dans la partie "structure" doit entraîner une modification de la taille de la partie "connexions". L'évolution va alors se faire en deux temps : tout d'abord l'évolution de la première partie et son décodage, puis l'évolution de la seconde en fonction des informations de la première. Des processus spécifiques doivent être mis en oeuvre pour maintenir la cohérence des génomes, en particulier pour les opérations de croisement (crossover).
Nos individus possèdent donc trois niveaux d'organisation : un génome composé d'un ensemble de gènes détectés localement, un protéome composé de l'ensemble de nos protéines virtuelles (des neurones cachés entièrement définis), et un phénotype qui est le réseau RBF construit à partir du protéome. Le passage du protéome au phénotype est trivial : le réseau construit est simplement l'assemblage des différents neurones cachés. Nous nous intéresserons donc surtout au passage du génome au protéome, et aux différents opérateurs génétiques. Une présentation détaillée de l'algorithme peut être trouvée dans (Lefort, 2007).
Code génétique et mapping génotype-protéome
Chacun de nos gènes code pour un neurone caché et la séquence de ce gène doit définir totalement ce neurone de la même façon qu'un gène détermine totalement la séquence d'une protéine. Ceci est rendu possible par l'utilisation d'un code génétique qui va transformer le contenu du gène en une suite d'acides aminés, celle-ci étant ensuite décodée pour calculer les paramètres du neurone : la moyenne (qui est un vecteur dont la dimension est celle des entrées), l'écart-type et le vecteur de poids de sortie.
Nous avons choisi de coder chacun de ces paramètres avec un code binaire dont la longueur pourra évoluer, de manière à pouvoir avoir des valeurs grossières en début d'évolution (gènes courts) et, si nécessaire, précises en fin d'évolution (gènes longs).  Notre génome est constitué d'une suite de lettres (ou bases), deux lettres particulières (A pour "start" et B pour "stop") permettant de délimiter les gènes, les autres étant utilisées par paires pour coder les différents paramètres. Le nombre de lettres nécessaires dépend du problème : outre le "start" et "stop", il faut deux lettres pour chaque caractéristique du neurone (une pour représenter un 0 binaire, et une pour le 1). Ainsi, sur un problème avec une entrée et une sortie, nous avons besoin de 8 lettres.
Pour reconstituer la valeur d'un paramètre, il suffira de rechercher les deux bases correspondantes dans le gène, de les extraire (les différents paramètre pouvant être mixés) et de les transformer en suite de 0 et de 1. Il suffit ensuite de normaliser la valeur binaire pour obtenir une valeur réelle (figure 1).
Opérateurs
Notre algorithme conserve la boucle générationnelle des algorithmes évolutionnaires. Cependant la structure génétique nous autorise à élargir le répertoire des opérateurs de mutation puisque ceux-ci peuvent modifier les gènes et/ou la structure du génome.
Nous avons sept opérateurs de mutations. Trois opérateurs locaux n'agissent que sur une seule base à la fois et vont principalement modifier les valeurs des paramètres : le switch (qui remplace une base par une autre), la délétion ponctuelle (qui supprime une base) et l'insertion (qui en rajoute une). Trois opérateurs globaux agissant sur des segment génétiques (qu'ils contienne des gènes ou non) permettent des remaniements de la structure génétique. Il s'agit de la translocation (qui déplace une zone à un autre endroit du génome), de la duplication (qui insère une copie d'une zone dans le génome) et de la délétion large (qui supprime une zone du génome). Enfin, nous avons un crossover à un point, qui sera effectué après un alignement à gauche des génomes pour permettre un échange de gènes.
Étude en régression
Nous avons étudié notre algorithme sur des benchmarks de régression, afin de vérifier tout d'abord la facilité de paramétrage, puis les résultats en convergence et enfin le déroulement de l'évolution. Le benchmark utilisé ici est Boston Housing (Automatic Knowledge Miner (AKM) Server, 2003), qui possède 13 entrées pour une sortie. Il est constitué de 506 points (405 pour l'apprentissage et 101 pour la validation). L'algorithme a été testé sur d'autres benchmarks (1D et 2D Sine Wave, Abalone), les résultats en convergence pouvant être trouvés dans (Lefort, 2007).
Le paramétrage n'est pas détaillé ici, mais nous utilisons les réglages que l'on peut trouver dans (Lefort, 2007). On notera que notre algorithme ne possède que deux paramètres qui ne soient pas classiques aux algorithmes évolutionnaires (taux des mutations larges et taille initiale des génomes).
Même si la comparaison de notre algorithme avec d'autres algorithmes de régression n'est pas le coeur de ce travail, notre algorithme a montré qu'il était compétitif avec les principaux algorithmes de régression utilisés dans (Madigan et Ridgeway, 2004). Les détails des comparaisons se trouvent dans (Lefort, 2007).
Nos solutions au terme de l'évolution sont compétitives avec d'autres algorithmes de ré-gression. Notre algorithme possède cependant de nombreux degrés de liberté (nombre de neurones, taille des gènes, taille des génomes. . .) qu'il est intéressant d'étudier. Nous allons nous intéresser à deux indicateurs : la taille du génome et le taille moyenne des gènes. Les évolutions de ces indicateurs sont représentées figure 2.
On remarque que dans une toute première phase, la taille du génome augmente de manière exponentielle. Cependant, et malgré l'absence de processus limitant la taille, celle-ci se stabilise à 15000 bases (la valeur de stabilisation dépendant du problème et du taux de mutation). Contrairement à d'autres algorithmes (de type programmation génétique) nous n'avons donc pas une explosion de la taille.
Cette augmentation de la taille est due à deux facteurs. Le premier est l'augmentation du nombre de gènes (de 2 à 80 par génome) ce qui permet de résoudre efficacement le problème Boston Housing. Le deuxième correspond à la taille des gènes qui reste stable dans les toutes premières générations puis augmente fortement pour se stabiliser vers 150 bases. Cette augmentation correspond à une amélioration de la précision du codage : à la convergence, chaque paramètre est codé en moyenne sur dix bits.
Notre algorithme gère donc de façon dynamique le nombre de gènes comme leur taille (et donc le nombre de neurones cachés et la précision des paramètres), notre précision tout comme notre nombre de neurones restant cohérents avec le problème posé.
Conclusion
En nous inspirant de la biologie, nous avons proposé un algorithme évolutionnaire qui permet d'optimiser des réseaux RBF pour la régression. Notre génome est homogène et composé de gènes indépendants les uns des autres et pouvant se déplacer, se dupliquer ou changer de taille. Chacun de ces gènes représente un neurone caché complètement défini, dont les paramètres seront issus uniquement du contenu du gène, transformé via un code génétique artificiel en un code binaire de taille variable.
L'évolution artificielle va donc pouvoir optimiser les valeurs des différents paramètres, mais aussi la précision de chacun d'entre eux, le nombre de gènes et leur disposition sur le génome. Nos résultats sont compétitifs avec d'autres algorithmes dédiés à la régression, mais surtout notre algorithme permet un choix dynamique de la taille du génome ou de celle des gènes, qui se stabilisent toutes les deux. Il serait maintenant intéressant de l'appliquer à des problèmes réels plus complexes pour tester ses limites.

Introduction
La taille des données peut être mesurée selon deux dimensions, le nombre de variables et le nombre d'observations. Ces deux dimensions peuvent prendre des valeurs très élevées, ce qui peut poser un problème lors de l'exploration et l'analyse de ces données. Pour cela, il est fondamental de mettre en place des outils de traitement de données permettant une meilleure compréhension des données. La réduction des dimensions est l'une des plus vieilles approches permettant d'apporter des éléments de réponse à ce problème. Les méthodes qui nous intéressent dans ce papier sont celles qui permettent de faire à la fois de la réduction de dimension et la classification non supervisée de données en utilisant les cartes autoorganisatrices (SOM : Self-organizing Map). Celles-ci sont souvent utilisées parce qu'elles sont considérées à la fois comme outils de visualisation et de partitionnement non supervisé de différents types de données. Elles permettent de projeter les données sur des espaces discrets qui sont généralement en deux dimensions. Plusieurs extensions des cartes autoorganisées ont été dérivées du premier modèle original proposé par Kohonen (  (Guérif et Bennani, 2007), BeSOM (Lebbah et al., 2007). Ces modè-les sont différents les uns des autres, mais partagent la même idée de présenter les données de différents types de grande dimension en une simple carte à deux dimensions. Un intérêt majeur sera donné à l'algorithme w-SOM qui est une extension de l'algorithme w-kmoyennes. Ce modèle permet en même temps de construire une carte topologique et d'estimer des pondérations globales de chacune des variables constituant la base d'apprentissage.
La pondération de variable consiste à associer des valeurs numériques (poids de pondé-ration) à chaque variable. Elle permet de nous donner une information sur l'importance de la variable. Ainsi une variable possédant une pondération forte explicite le fait qu'elle est pertinente et qu'elle a participé activement au processus de classification.
Dans ce papier, nous proposons une méthode de pondération locale des variables basée sur l'approche w-SOM (Guérif et Bennani, 2007). Ces pondérations seront utilisées pour la segmentation de la carte topologique. En effet, contrairement à la méthode de pondération globale w-SOM qui estime un seul vecteur de pondérations pour tout l'ensemble des réfé-rents, la pondération locale associe un vecteur de pondérations des variables à chaque réfé-rent de la carte. Par conséquent nous pouvons utiliser ces pondérations pour regrouper les prototypes qui ont les mêmes variations du vecteur de pondérations.
La suite de cet article est organisée comme suit : nous présentons notre approche de pondération locale des variables dans la section 2, après l'introduction de l'algorithme w-SOM. Dans la section 3, nous présentons les différents résultats obtenus. Finalement on termine par la conclusion et les perspectives de la méthode proposée. Dans la version globale de w-SOM, le poids w j associé à la j-eme variable est le même pour toutes les autres j-eme variables associées à l'ensemble des référents Z de la carte. En étendant ce modèle au cas d'une pondération locale, chaque référent k de la carte a alors son propre vecteur de poids w k . En Notant jk w le poids de la j-ème variable pour le référent k, la fonction de coût s'écrit de la manière suivante :
Pondération des variables
où C est le nombre de référents, N le nombre d'exemples, et n la dimension de l'espace.
En notant U la matrice de partitionnement ou d'affectation des exemples x sur la carte topologique, l'optimisation de
W) Z, P(U,
, s'effectue en itérant l'optimisation suivante :
en fixant Z et W ; chaque individu x est affecté au référent dont il est le plus proche au sens de la distance euclidienne pondérée :
en fixant U et W ; chacun des référents est remplacé par le barycentre des individus qui lui sont affectés et de ceux qui sont affectés à ces voisins détermi-nés par la fonction de voisinage h: 
Le poids jk w de la variable j pour le référent k est défini de la manière suivante :
RNTI -XPondération locale des variables en apprentissage numérique non-supervisé
Ainsi nous obtenons un vecteur des pondérations des variables pour tous les sous ensembles associés aux référents de la carte. La pertinence des variables dépend de ces pondéra-tions, ainsi, si elles sont plus petites, on pourra les éliminer. Par conséquent, nous pourrons utiliser ces pondérations pour regrouper les référents qui ont des vecteurs de pondération les plus proches. 
Fin de boucle
Généralement l'utilisation des cartes topologiques est suivie d'une segmentation des réfé-rents de la carte. Souvent ces méthodes de segmentation se résument à l'utilisation de l'algorithme de classification hiérarchique ou des K-moyennes combinés avec un indice de qualité pour déterminer la taille de la partition idéale de la carte. Ainsi dans ce qui suit nous proposons une application de lw-SOM consistant à utiliser ces pondérations locales, pour segmenter la carte en utilisant l'algorithme K-moyennes combiné avec l'indice de qualité interne de Davies-Bouldin, (Vesanto et Alhoniemi (2000) 
Segmentation de la carte topologique
L'algorithme lw-SOM décrit dans la section précédente permet d'obtenir d'une part, une projection en deux dimensions des données et d'autre part, une pondération des variables spécifiques à chaque région de l'espace. Vesanto et Alhoniemi (2000) ont proposé de segmenter une carte topologique en combinant l'algorithme des k-moyennes à l'indice de Davies-Bouldin qui permet de déterminer automatiquement la taille de la partition après segmentation. Nous avons appliqué cette approche sur les référents et sur les pondérations.
En utilisant le jeu de données Iris, nous avons évalué le découpage par l'approche classique et en utilisant les k-moyennes sur les vecteurs de pondérations. La figure 1 indique une segmentation de la carte 6?4 en deux sous ensembles, utilisant seulement les référents de la carte. La figure 2 indique une segmentation de la même carte en utilisant dans ce cas les vecteurs de pondérations qui prennent en considération l'importance locale de chacune des 
FIG. 4 -Segmentation de la carte avec lw-SOM, en appliquant les kmoyennes sur les pondérations des neurones. Valeur de l'indice de Davies
Bouldin = 0,007.
En observant les résultats obtenus avec les deux bases, il est clair que l'utilisation des pondérations des variables locales permet de mieux segmenter la carte topologique.
Discussion
Les figures 5 et 6 représentent les différentes pondérations des variables associées aux ré-férents de la carte topologique obtenue avec lw-SOM, sous forme de signal. En observant les cartes, il est clair que visuellement, on peut segmenter la carte par rapport aux différentes pondérations en regroupant les référents qui ont des pondérations proches. Contrairement à w-SOM globale, l'algorithme lw-SOM, permet de caractériser chaque sous ensemble associé à un référent de la carte, par un vecteur de pondérations indiquant la pertinence de chacune des variables. Par conséquent, les variables qui ont des pondérations proches permettent de distinguer les référents proches, ainsi d'obtenir la segmentation.
En observant la figure 5 représentant les pondérations locales de la base des Iris, on peut voir un sous ensemble de prototype sur le coin haut à gauche de la carte qui est caractérisé par les deuxièmes et quatrièmes variables associées à des pondération très forte. En observant aussi la figure 6 représentant les pondérations locales de la base waveform, on peut voir trois sous ensembles de référent avec des pondérations variables. On rappelle que les variables du bruit sont représentées dans la partie droite du signal. On distingue par exemple que dans le coin droit en bas de la carte ces pondérations représentent plus le bruit que les variables.
RNTI -XPondération locale des variables en apprentissage numérique non-supervisé
FIG. 5 -Les pondérations des variables de la base Iris
FIG. 6 -Les pondérations de variables de la base Waveform
Conclusions
Dans cette étude, nous avons introduit une nouvelle méthode de pondération des variables. L'algorithme lw-SOM propose d'apprendre les vecteurs de pondération associés à chaque référent durant le processus d'apprentissage en se basant sur l'algorithme batch de Kohonen. Contrairement à w-SOM, notre approche permet de caractériser chaque sous ensemble « cluster » par les variables les plus pertinentes. En obtenant un vecteur des pondérations pour chaque référent de la carte, on peut regrouper ceux qui ont des pondérations similaires et éliminer ainsi les autres. Nous obtenons donc un découpage de la carte plus fin en comparaison aux autres algorithmes classiques. Comme perspectives, nous envisageons poursuivre ce travail pour un objectif de sélection de variables et d'étendre ce modèle aux variables binaires.
Références

Introduction
Les chimistes mettent au point de nouveaux procédés de synthèse de molécules en consultant de très grandes bases de données recensant les réactions chimiques disponibles. Les chimistes aimeraient pouvoir fouiller les graphes moléculaires contenus dans ces données pour en extraire des schémas de réactions fréquents qui serviront de candidats privilégiés lors de nouveaux problèmes de synthèse. Deux obstacles s'opposent à cela. D'une part la manière dont les chimistes représentent les réactions par des graphes ne permet pas aux techniques de fouille de graphes d'extraire les schémas de réactions fréquents. Il existe des algorithmes efficaces (Yan et Han, 2002, 2003Nijssen et Kok, 2004) pour extraire d'un ensemble E de graphes étiquetés l'ensemble des sous-graphes G connexes fréquents dont le support, défini comme le nombre de graphes de E qui contiennent au moins un sous-graphe isomorphe à G, est supé-rieur à un certain seuil. Si ces méthodes peuvent s'appliquer avec succès à la fouille de graphes moléculaires (Fischer et Meinl, 2004), leur application directe aux graphes d'une base de ré-actions ne conduirait à aucun résultat pertinent : tout au plus pourrait-on mettre en évidence les fragments de graphes moléculaires qui sont fréquemment détruits ou au contraire fréquem-ment créés lors des réactions sans qu'aucun schéma de réaction, c'est à dire, aucun schéma de transformation entre graphes moléculaires, ne puisse s'en déduire. D'autre part les bases de données contiennent des descriptions de réactions souvent incomplètes, ambiguës ou erronées. Leur fouille sans autre filtrage conduirait à des résultats trop bruités donc inexploitables.
Une étape de prétraitement s'avère donc indispensable pour améliorer la qualité des données fouillées et pour exprimer les données au sein d'un modèle répondant au problème posé. Considérant qu'un problème d'extraction de connaissance ne peut être résolu efficacement que si les connaissances du domaine d'application sont prises en compte à tous les niveaux, que ce soit lors du prétraitement des données, de leur fouille proprement dite ou de l'analyse des résultats, le présent article décrit comment les connaissances du domaine, c'est à dire certains principes établis de chimie organique, ont aidé à la conception d'un prétraitement original des bases de réactions, conçu spécifiquement pour extraire les schémas de réactions fréquents à l'aide des algorithmes existants de recherche de sous-graphes fréquents. L'article se restreint essentiellement à présenter les détails de ce prétraitement, qui n'est qu'une étape d'un processus d'extraction de connaissances plus vaste, dont les principes généraux ont été introduits dans Pennerath et Napoli (2006) et dont les premiers résultats sont exposés dans Pennerath et Napoli (2008).
Définitions formelles des données et du problème
Les molécules sont des groupes d'atomes maintenus solidaires par des forces de covalence. Toute molécule se modélise donc naturellement par un graphe moléculaire g étiqueté et connexe dont les ensembles de sommets S(g) et d'arêtes A(g) représentent respectivement les atomes et les liaisons de covalence de la molécule. Chaque sommet s ? S(g) est étiqueté par l'élément chimique e(s) de son atome (H pour l'hydrogène, ..., et par défaut C pour le carbone) et chaque arête a ? A(g) est étiquetée par la multiplicité m(a) de la liaison associée (simple, double, etc). Une réaction chimique modifie la structure de certaines molécules et se modélise en première approximation par une transformation de graphes moléculaires. Les chimistes représentent cette transformation par une équation chimique (cf figure 1) dont les membres de gauche et de droite représentent respectivement l'ensemble des graphes moléculaires des molécules de départ, appelées réactants, et des molécules d'arrivée appelées produits. Les bases FIG. 1 -Exemple d'équation de réaction à deux réactants et deux produits.
de réactions décrivent principalement chaque réaction par son équation chimique, c'est à dire par la donnée de deux graphes moléculaires R et P représentant respectivement les membres gauche et droit de l'équation. Les composantes connexes de R (resp. P) correspondent aux graphes moléculaires des différents réactants (resp. produits). Deux fonctions d'annotation ? R : D ? R ? S(R) ? N et ? P : D ? P ? S(P) ? N des sommets de R et de P complètent les données des graphes R et P. Un sommet s est apparié si ? R ou ? P lui associe un indice d'appariement (i.e. s ? D ? R ?D ? P ). Une équation (cf figure 1) annote chaque sommet apparié par son indice. Deux sommets s 1 ? R et s 2 ? P sont appariés l'un à l'autre s'ils sont annotés par le même entier. Ils sont alors censés représenter un et un seul même atome. Une équation (R, P) est dite totalement appariée si tout sommet de P est apparié. Une base de réactions est donc équivalente à un ensemble de 4-uplets {(R i , P i , ? Ri , ? Pi }) 1?i?n .
Un schéma de réaction est une équation de réaction incomplète qui permet de représen-ter un schéma de transformation commun à plusieurs réactions. Le schéma de la figure 2 est un des nombreux schémas généralisant l'équation de la figure 1. Formellement le schéma de réaction S 1 = (R 1 , P 1 , ? R1 , ? P1 ) généralise l'équation ou le schéma de réaction S 2 = (R 2 , P 2 , ? R2 , ? P2 ) (et on note S 1 ? S S 2 ) si les graphes R 1 et P 1 sont isomorphes à des sousgraphes de R 2 et P 2 et si les injections correspondantes de sommets ? R :
Le support d'un schéma de réaction S relativement à un ensemble E de réactions est le nombre de réactions de E généralisées par S. Le problème de la recherche des schémas de réactions fréquents dans une base de réactions consiste à déterminer le support de tous les schémas de réactions fréquents dont le support est supérieur ou égal à un seuil f min .
L'axiomatisation des connaissances du domaine
Les équations présentes dans les bases de réactions comportent souvent des erreurs. Les chimistes prennent par exemple rarement la peine de décrire tous les appariements entre sommets et certains produits jugés inintéressants sont tout simplement omis de l'équation. Ces né-gligences sont tolérées dans la mesure où les chimistes n'ont aucune difficulté à réinterpréter correctement les données l'aide des connaissances qu'ils ont du domaine. Dans le cadre d'un processus automatisé d'extraction de connaissance, il devient nécessaire d'identifier les propriétés particulières que présentent les graphes (R, P) et que le chimiste utilise implicitement. La démarche adoptée consiste à reformuler ces propriétés en axiomes exprimés exclusivement à partir de concepts propres à l'informatique et à la théorie des graphes de manière à ce que les algorithmes de prétraitement puissent s'en déduire indépendamment des connaissances du domaine :
Conservation des sommets Tout atome se conservant au cours d'une réaction, il existe une bijection ? des sommets du graphe P vers ceux de R. Cela implique en particulier que les fonctions
Valence des sommets Le nombre de liaisons simples auxquelles un atome stable participe étant défini par l'élément chimique de l'atome, tout sommet s de R ou P et d'étiquette l(s) a un degré pondéré deg(s) (i.e. la somme des multiplicités m(a) des arêtes a incidentes à s) égal à l'image de l(s) par une fonction appelée valence :
Réagencement des arêtes Du fait de la propriété de conservation des sommets, les réactions consistent uniquement à briser, créer ou changer la multiplicité des liaisons. Le graphe produit P s'obtient donc du graphe de départ R en ajoutant à la multiplicité m(a) de chaque arête a = {s 1 ;
La propriété de valence des sommets implique alors :
Minimalité de la distance d'édition Une réaction transforme ses réactants en ses produits en suivant statistiquement la séquence (t j ) de transformations élémentaires qui minimise l'énergie thermodynamique nécessaire. Cette énergie est proportionnelle en première approximation à la distance d'édition d(R, P) = c(t j ) pour passer de R à P en supposant que le coût c(t j ) soit l'énergie nécessaire à la transformation t j . D'après l'axiome de réagencement des arêtes, les transformations élémentaires consistent uniquement à diminuer ou augmenter la multiplicité des arêtes. La distance peut donc se réécrire comme d(R, P) = A(R)?A(P) c(a) où le coût c(a) correspond à l'énergie nécessaire pour modifier de r(a) unités, la multiplicité m(a) de l'arête a. Ce coût est nul si r(a) ? 0 puisque la formation d'une liaison libère de l'énergie, et peut être supposé proportionnel au nombre d'arêtes élémentaires brisées lorsque r(a) < 0. Finalement :
4 Le processus de fouille des schémas de réactions La figure 3 présente les différentes étapes du processus de fouille de schémas de réactions à partir de bases de réactions. Ce processus entièrement opérationnel comprend toutes les étapes du processus d'extraction de connaissances tel que décrit par Fayyad et al. (1996)  réactions à l'aide d'un langage de requêtes spécifique puis sauvegarde la réponse de sa requête dans un fichier Reaction Data File ou RDF 1 . L'étape de prétraitement développée en section 6, filtre et corrige l'ensemble {E i } des équations de départ en un ensemble {E j } d'équations totalement appariées. Cet ensemble peut alors être transformé en l'ensemble des graphes de réactions équivalents {G r (E j )} dont le modèle est développé en section 5. Les descriptions de ces graphes étiquetés sont sauvegardées dans un fichier au format gbdm afin d'être exploités par un algorithme de fouille de graphes, comme Gaston (Nijssen et Kok, 2004) ou gSpan (Yan et Han, 2002) pour la recherche des sous-graphes fréquents ou Forage (Pennerath et Napoli, 2007) pour l'extraction des schémas de réactions les plus informatifs. Dans tous les cas, l'algorithme produit un fichier gbdm contenant un ensemble {(g k , f k , ...)} de motifs g k associés à leur fréquence f k plus éventuellement d'autres propriétés (score, information, etc). L'étape de post-traitement permet de convertir l'ensemble des graphes de réactions partiels g k en l'ensemble {S(g k )} des schémas de réactions équivalents, qui sont ensuite triés selon les valeurs décroissantes d'une des propriétés spécifiée par l'expert (par exemple le score ou la fréquence), avant d'être sauvegardés dans un fichier RDF. L'expert peut alors analyser les schémas obtenus et leurs propriétés à l'aide d'un logiciel de visualisation d'équations de réac-tions.
La transformation des données : les graphes de réactions
Tout algorithme de recherche de motifs fréquents exploite la propriété de monotonicité de la relation de subsomption entre motifs qui correspond en l'occurrence à la relation d'inclusion ? S . Les méthodes existantes de fouille de graphes sont incapables de s'adapter à cette relation d'inclusion pour deux raisons essentielles : d'une part ces méthodes ne génèrent, pour des raisons de réduction combinatoire, que des motifs de graphes connexes, ce qui n'est pas le cas des schémas de réactions. D'autre part ces méthodes n'intègrent pas naturellement la relation d'ordre ? S entre schéma de réactions puisque cette relation repose sur la notion étran-gère d'appariement entre sommets (i.e. les fonctions ? R et ? P ). Le modèle des graphes de réactions introduit initialement dans Vladutz (1986) pour produire une représentation connexe d'une réaction, a également l'avantage de résoudre le second problème Pennerath et Napoli (2006) : tirant parti des axiomes de conservation des sommets et de réagencement des arêtes, le graphe de réaction G r (S) associé à un schéma de réaction S = (R, P, ? R , ? P ) totalement apparié revient à confondre le graphe des produits P avec celui des réactants R en fusionnant les sommets appariés afin d'identifier les arêtes inchangées, détruites et créées lors de la réaction (par des étiquettes d'arêtes 0, ? et +). Le graphe de réaction de l'équation de la figure 1 est représenté sur la figure 4. Cette transformation S ? G r (S) est bijective : les FIG. 4 -Graphe de réaction équivalent à l'équation totalement appariée de la figure 1.
graphes R et P de S s'obtiennent de G r (S) en supprimant respectivement les arêtes créées (marquées +) et brisées (marquées ?) puis en remplaçant dans R et P tout ensemble A d'arêtes multiples par une seule arête a de multiplicité m(a) = |A|. Un graphe de réaction G r (S) est donc un graphe connexe rigoureusement équivalent à un schéma de réaction S totalement apparié. On démontre que la relation d'ordre ? g de sous-graphe isomorphe définie sur l'ensemble des graphes de réactions est isomorphe à la relation d'inclusion entre schémas de réactions : S 1 ? S S 2 ? G r (S 1 ) ? g G r (S 2 ). La figure 5 illustre l'équivalence des deux relations d'ordre sur l'exemple du schéma de réaction de la figure 2 inclus dans l'équation de la figure 1. Le problème de recherche des schémas de réactions fréquents dans un ensemble (E i ) 1?i?n d'équations de réactions est donc équivalent à celui de la recherche des graphes fré-quents dans l'ensemble des graphes de réactions équivalents (G r (E i )) 1?i?n . Les algorithmes existants de fouille de graphes peuvent donc résoudre le problème de la recherche de schémas de réactions fréquents (du moins si on se restreint aux motifs contenant au moins une arête de type ? ou +).
FIG. 5 -Equivalence des relations d'ordre
6 Le prétraitement des données
Les données du domaine et leurs imperfections
Une base de réactions décrit une équation chimique par un 4-uplet (R, P, ? R , ? P ). Ce 4-uplet respecte très rarement tous les axiomes de la section 3. Les équations telles que celles des figures 6 et 7 peuvent être qualifiées différemment selon la nature de leurs non conformités (Berasaluce, 2002). Une équation est ainsi :
Non saturée quand les atomes d'hydrogène ne sont pas explicités et que le principe de la valence n'est pas respecté (i.e. quand {s ? S(R)?S(P)|deg(s) < valence(l(s))} = ?, cf figure 6).
FIG. 6 -Exemple d'équation non saturée, non déterministe, incomplète et ambiguë.
Non déterministe quand les produits d'une équation ne sont pas produits simultanément mais concurremment selon des rendements statistiques respectifs (i.e. quand il existe au moins deux sommets s 1 et s 2 de deux composantes connexes distinctes de P portant le même indice d'appariement ? P (s 1 ) = ? P (s 2 ), cf figure 6).
Non équilibrée quand certains réactants ou produits doivent être dupliqués pour que le principe de conservation des atomes soit respecté (i.e quand il existe une combinaison linéaire H R × C R = H P × C P telle que les vecteurs de pondération C R et C P aient des coefficients strictement positifs non tous égaux et que la matrice H R (resp. H P ) ait pour coefficients h ij les nombres de sommets d'étiquette i dans le j ème réactant R j (resp. j ème produit P j )). Erronée quand l'équation ne peut être équilibrée et que certains éléments chimiques sont plus présents dans les produits que dans les réactants (i.e. quand il existe une étiquette de sommet d'avantage présente dans P que dans R, cf figure 7).
FIG. 7 -Exemple d'équation erronée.
Incomplète quand l'équation n'est ni équilibrable ni erronée parce que certains produits secondaires sont omis de l'équation (i.e. quand il existe une étiquette de sommet plus présente dans R que dans P, cf figure 6) Ambiguë quand l'équation n'est pas totalement appariée parce que plusieurs appariements sont envisageables (i.e. quand D ? P = P, cf figure 6).
Les étapes du prétraitement
Le prétraitement des données se décompose en une succession d'étapes. Chaque étape peut être perçue comme une fonction qui transforme tout 4-uplet qu'elle reçoit en entrée en un ensemble éventuellement vide de 4-uplets. Tout 4-uplet en sortie de e i devient ensuite un 4-uplet en entrée de e i+1 . L'ordre des étapes est défini de telle sorte qu'une étape résout une catégorie particulière de défauts sans jamais introduire un type de défaut résolu par une étape précédente. La succession des étapes garantit que les graphes de réactions obtenus en sortie de la dernière étape correspondent aux équations des réactions les plus plausibles. Les étapes sont dans l'ordre : La saturation des molécules qui consiste simplement à connecter à chaque sommet s de R et de P, valence(l(s)) ? deg(s) sommets d'hydrogène par des arêtes simples pour satisfaire l'axiome de valence des sommets. La scission des équations non déterministes : si l'équation (R, P, ? R , ? P ) est non détermi-niste, toute composante connexe P k de P dont le rendement associé dépasse un seuil configurable donne lieu à une équation déterministe (R,
élimination des équations erronées : si il existe une étiquette de sommet (i.e. un élément chimique) qui est présente dans P sans l'être dans R, l'équation candidate est erronée et est éliminée. La pondération des équations non équilibrées est un problème théorique complexe de programmation linéaire en nombres entiers (Sena et al., 2006). En pratique les équations comptent rarement plus de trois réactants et trois produits. L'étape de pondération se contente donc pour toute équation non équilibrée de tester toutes les duplications évi-dentes de réactants et/ou de produits jusqu'à obtenir une équation équilibrée (en testant la condition H R × C R = H P × C P ). Si toutes les tentatives de pondération échouent, on distingue deux cas : si il existe une étiquette de sommet plus représentée dans P que dans R, l'équation candidate est considérée erronée et est éliminée. Dans le cas contraire l'équation est incomplète mais peut passer à l'étape suivante.
La complétion des appariements : A ce stade du prétraitement, l'équation obtenue est presque toujours ambiguë. Un rejet systématique des équations ambiguës est donc impossible. La solution adoptée consiste à produire l'ensemble des équations totalement appariées les plus plausibles qui se déduisent de l'équation ambiguë. Cet ensemble contient vraisemblablement l'unique réaction se produisant réellement en plus d'éventuelles équations factices. L'effet néfaste introduit par la présence de ces artefacts est toutefois limité par le filtrage statistique qu'opère la recherche des motifs fréquents en ignorant les schémas de réactions non représentatifs. Pour désambiguïser une équation, il est nécessaire d'apparier tous les sommets non appariés de P à des sommets non appariés de R. Compte tenu du nombre exponentiel d'appariements possibles, il est nécessaire de restreindre la procédure aux appariements les plus plausibles. On introduit à cette fin la notion de compatibilité entre sommets : deux sommets s 1 ? S(P) et s 2 ? S(R) sont compatibles si aucun des deux n'est déjà apparié, si ils ont la même étiquette, si ils sont tous deux adjacents à un sommet apparié et si ils partagent le même ensemble maximal de sommets voisins appariés : s 1 est incompatible avec s 2 si il existe un sommet s 3 ? S(R) compatible avec s 1 qui a plus de voisins appariés avec des voisins de s 1 que s 2 n'en a avec s 1 . En notant V(s) l'ensemble des sommets voisins du sommet s, cette dernière condition équivaut à :
L'appariement de deux sommets n'est plausible que si ces derniers sont compatibles, selon le principe de minimalité de la distance d'édition. L'élimination des appariements incompatibles permet d'élaguer l'arbre de recherche dans l'espace d'état des appariements possibles. La procédure consiste alors en un algorithme de backtracking qui effectue à chaque étape de sa progression l'appariement d'un sommet de P non encore apparié avec un sommet de R qui lui est compatible puis met à jour les fonctions ? R et ? P . Un retour arrière se produit soit dès qu'un sommet non apparié de P n'est plus compatible avec aucun sommet de R soit dès que tout sommet de P est apparié, l'équation associée étant alors passée à l'étape de traitement suivante.
La construction du graphe de réaction : A ce stade du prétraitement, toutes les équations sont totalement appariées. Chaque équation E est donc remplacée par son graphe de réaction G r (E), conformément à la section 5.
L'élimination des graphes de réactions non réalistes : L'étape de complétion des appariements transforme une équation de départ en un nombre limité d'équations E i totalement appariées qui n'ont pas nécessairement le même degré de plausibilité. De ce fait, le nombre n i d'arêtes brisées (étiquetées ?) de chaque graphe de réaction G r (E i ) est calculé. Tout graphe G r (E i ) dont le nombre n i n'est pas égal à n min = min(n i ) peut alors être éliminé au nom du principe de minimalité de la distance d'édition. Après élimina-tion, toute équation initiale E aboutit à un ensemble d'équations totalement appariées de même n i minimal. Le nombre de ces équations est le plus souvent égal à 1, parfois nul lorsque E s'avère erronée, parfois égal à 2 ou 3 (mais rarement plus) lorsque E peut valablement être interprétée selon différents appariements de sommets. Lorsque le nombre d'appariements possibles est supérieur à un seuil (fixé à 4), on estime que l'appariement de l'équation initiale est insuffisant et que les équations qui en découlent sont trop incertaines et doivent être éliminées.  7 Résultats des tests L'algorithme de prétraitement a été implémenté et testé sur plus de 95000 réactions monoétapes issues de l'intégralité des deux bases de réactions Orgsyn et JSM 2 particulièrement importantes pour la variété des méthodes de synthèse qu'elles proposent. Les résultats des tests résumés dans le tableau de la figure 9 reposent sur la définition de différents taux indicateurs. Soit ainsi I l'ensemble initial des équations de la base de données. Soit E ? I l'ensemble des  une diminution régulière, probablement due à une sélection de plus en plus stricte des données. Le taux d'ambiguïté t a observe une décroissance plus récente, traduisant une qualité croissante des appariements.
Conclusions
Les résultats obtenus prouvent qu'il est possible d'extraire les schémas de réactions fré-quents des bases de réactions à l'aide des algorithmes existants de fouille de graphes. L'outil Forage a ainsi pu extraire de différents ensembles de réactions les schémas de réactions fré-quents, fermés fréquents et les plus informatifs fréquents (Pennerath et Napoli, 2007). Si ce résultat devrait à terme permettre aux chimistes d'identifier des schémas de réactions intéres-sants, il pourrait également inciter les spécialistes de la fouille de données à s'intéresser de plus près à ces objets d'étude riches et complexes que sont les réactions chimiques.

Introduction
On appelle « concept », une entité qui se définit par un croisement de catégories. L'objet de l'ADS est d'analyser des ensembles de concepts décrits par des variables symboliques. Ces variables sont non seulement à valeur numérique ou qualitative mais aussi à valeur intervalle, histogramme, loi de probabilité, fonction, ensemble de valeurs etc., afin de tenir compte de la variation des valeurs prises par les individus de l'extension de chaque concept. L'ADS et son logiciel SODAS comportent deux étapes : la première consiste à construire la description des concepts à partir de celle des individus, la seconde consiste à analyser le tableau de données symboliques ainsi créé en étendant les méthodes de la Statistique ou du Data Mining aux concepts considérés comme unités statistiques de plus haut niveau. Nous illustrons ces deux étapes en montrant trois avantages de l'ADS : i) on peut étudier les bonnes unités statistiques à un niveau de généralisation voulu par l'utilisateur ; ii) on réduit la taille des données en considérant comme unités d'étude, des classes plutôt que les individus ; iii) on réduit le nombre de variables du fait qu'elles sont à valeur symbolique (par exemple, à valeur « histogramme » plutôt qu'à valeur «fréquence d'une catégorie» ou à valeur intervalle plutôt qu'à valeur « borne d'intervalle »). On utilise pour cela le logiciel SODAS (voir l'ouvrage collectif issu du projet européen ASSO d' EUROSTAT : Diday, Noirhomme (2007)).
Description
Les données fournies par le LCPC (Laboratoire Central des Ponts et Chaussées) sont constituées d'un ensemble de 14 TGV qui en passant à une température donnée sur un pont déclenchent des signaux de 9 capteurs répartis à différents endroits du pont (voir la figure 1). En entrée, on dispose d'un tableau de données symboliques qui contient dans la case (i, j) le RNTI-E-11 -211 -

Introduction
De nombreux domaines comme la biologie ou la médecine voient naître chaque jour de nouveaux termes et abréviations, notamment des sigles. Un sigle est un ensemble de lettres initiales servant d'abréviation, par exemple "RATP" peut être associé à la définition (aussi appelée expansion) "Régie Autonome des Transports Parisiens". Nos travaux ont consisté à développer un logiciel afin de faciliter l'acquisition ou l'enrichissement de dictionnaires en extrayant automatiquement, à partir de diverses sources, les sigles et leur(s) définition(s). Une fois ces dictionnaires constitués, l'approche AcroDef que nous avons proposée dans (Roche et Prince (2007)) consiste à établir la définition pertinente d'un sigle présent dans un document. Dans ces documents, la définition n'est pas toujours présente d'où la difficulté du traitement. Dans ce contexte, il est donc essentiel d'avoir à disposition un dictionnaire adapté, ce qui justifie les travaux présentés dans cet article.
De nombreuses méthodes pour extraire les sigles et leur(s) définition(s) ont été développées (Larkey et al. (2000); Okazaki et Ananiadou (2006)). La plupart des approches de détection de sigles dans les textes s'appuient sur l'utilisation de marqueurs spécifiques associés à des heuristiques adaptées. Certains travaux récents (Okazaki et Ananiadou (2006)) consistent à associer ces approches à des mesures statistiques spécifiques pour améliorer la qualité des méthodes d'acquisition de dictionnaires. L'approche que nous avons développée se compose de deux étapes successives qui sont détaillées dans la section 2.
La seconde étape de notre application utilise les résultats obtenus lors de la première phase afin de filtrer les candidats pertinents. Les résultats sont triés afin de (1) supprimer les paires sigle/définition non pertinentes, (2) extraire précisément les définitions présentes dans les dé-finitions potentielles (ces dernières pouvant être trop longues puisque coupées arbitrairement lors du second cas de la recherche des candidats). Pour permettre un tel filtrage, nous effectuons un alignement des lettres contenues dans le sigle avec les mots de la définition. Cet alignement consiste à vérifier la correspondance entre les lettres des sigles avec les premières lettres de chacun des mots des définitions. Dans notre méthode, si le premier caractère des mots de la définition candidate ne peut être aligné, les caractères qui suivent au sein des mots sont considérés. Par exemple, cette méthode permet de reconnaître "Extraction Itérative de la Terminologie" comme la définition du sigle EXIT dans lequel la lettre "X" a pu être alignée. Nous présentons ici une évaluation de notre système d'alignement des sigles avec les défi-nitions candidates. Pour cette évaluation, nous nous appuyons sur les données issues du site http ://www.sigles.net/. L'évaluation consiste à extraire aléatoirement de ce site des sigles de 2, 3 et 4 caractères et d'évaluer le taux de réussite de l'alignement (nombre de sigles alignés avec les définitions du site en utilisant la version actuelle de notre logiciel développé en Javaversion 1.0). Le tableau ci-dessous présente les résultats de plus de 800 alignements qui sont globalement très satisfaisants (taux de réussite de 78% à 98%). 
Conclusion et perspectives
L'application présentée dans cet article consiste à acquérir ou enrichir de manière automatique un dictionnaire de sigles/définitions à partir d'un corpus. Notre approche n'utilise aucune connaissance linguistique, elle peut donc s'appliquer à des textes en différentes langues. Dans nos futurs travaux, nous proposons d'associer de manière automatique le domaine de chaque sigle/définition en utilisant des méthodes fondées sur le contexte (Roche et Prince (2007)).

Introduction
Dans ce papier, nous proposons un nouveau système de détection d'intrusions, visant la diminution de génération de fausses alarmes et l'augmentation de détection de vraies intrusions. Nous montrons que l'utilisation des règles associatives génériques, de taille très compacte, permet d'atteindre ce double objectif. Les expérimentations que nous avons menées, montrent que l'approche proposée permet d'obtenir un SDI robuste avec un taux très élevé de détection de vraies intrusions.
Le système de détection d'intrusions IDS-GARC
Peu de travaux ont fait appel au concept des règles associatives dans le cadre de détection d'intrusions. Pour améliorer la qualité de détection d'intrusions, nous proposons un nouveau SDI appelé IDS-GARC (Intrusion Detection System based on Generic Association Rule with Classifier), dont l'objectif est de minimiser la génération de fausses alarmes et surtout l'augmentation de détection de vraies intrusions.
Le nouveau système IDS-GARC, dont l'architecture du IDS-GARC est décrite par la figure 1, dérive de l'application d'un processus, qui peut être résumé dans les quatre étapes suivantes :
-Pré-traitement des données : Nous discrétisons automatiquement des données de détec-tion d'intrusions identifiées par des experts en sécurité informatique -Génération de la base générique des règles associatives : En particulier, nous utilisons les règles génériques extraites de la base IGB (Gasmi et al., 2006) -Sélection des règles associatives génériques de détection : Pour se faire, nous avons recours à la classification associative. -Construction d'un classifieur : Pour détecter les nouvelles attaques, nous utilisons un classifieur appelé GARIDC (Generic Association Rule for Intrusion Detection based Classifier).
Evaluation expérimentale
Afin d'évaluer les performances du IDS-GARC, nous avons mené une série d'expérimen-tations sur une base de données orientée détection d'intrusions DARPA 98. Le choix de cette base s'explique par le fait puisqu'elle est fréquemment utilisée pour évaluer les performances des SDIs. Le tableau 1 présente les résultats obtenus en termes de taux de détection et de
FIG. 1 -Architecture du système IDS-GARC
fausses alarmes. L'analyse du tableau 1 permet de conclure que la technique de règles gé-nériques de classification permet d'obtenir le meilleur taux de détection de vraies intrusions (97, 86%). De plus, elle engendre peu de fausses alarmes, soit 2, 50%. Ainsi, les deux taux TD et TF permettent de montrer la robustesse et l'efficacité du SDI que nous avons proposé dans ce papier.
Approche
Taux de détection (TD) Taux de fausses alarmes (TF) Arbre de décision 92,05% 2, 50% Algorithmes génétiques 92, 60% 3, 62% Règles génériques de classification 97, 86% 2, 50%
TAB. 1 -Comparaison des taux de détection de vraies intrusions et de fausses alarmes
Le tableau 2 montre les taux de Pourcentage de Classification Correcte (PCC) obtenus avec notre approche par rapport à celles trouvées avec des approches de la littérature de la fouille de données, pour la base d'audit DARPA 98. 
TAB. 2 -Comparaison de la précision de classification correcte
En comparant les résultats du tableau 2, pour la catégorie normale, nous constatons que les règles génériques de classification donnent le meilleur taux i.e., 100%. De plus, cette stratégie présente le taux de PCC le plus élevé, pour la catégorie U2R, avec 91,41%.
Summary
Intrusion Detection Systems mainly aim to guarantee high immunity to networks from external attacks. In this paper, we introduce a novel approach for intrusion detection based on the use of generic basis of association rules. Preliminary carried out results showed the efficiency of such an approach. Keywords : Intrusion Detection Systems, Association rules, generic basis

Introduction
Un entrepôt de données est une structure informatique dans laquelle est centralisé un volume important de données. L'organisation des données permet l'analyse multidimensionnelle qui consiste à explorer tout ou partie des données à un niveau détaillé et/ou agrégé grâce aux outils OLAP (On-Line Analytical Processing). L'information géo-référencée, souvent contenue dans les données, est intégrée sous forme textuelle dans les modèles multidimensionnels classiques. Des modèles plus récents, de type "OLAP Spatial" (SOLAP), visent à intégrer la donnée spatiale dans l'OLAP et à enrichir les systèmes OLAP classiques grâce, par exemple, à la visualisation cartographique, permettant ainsi d'expliciter la distribution géographique d'une information et/ou de mettre en relation des informations à diverses granularités géographiques.
L'information géographique est la représentation d'objets, ou de phénomènes, localisés dans l'espace ; les modèles SOLAP se concentrent généralement sur cette composante spatiale de l'information géographique. Or celle-ci est aussi caractérisée par un ensemble d'aspects sémantiques (ses attributs descriptifs alphanumériques et ses relations avec d'autres objets) qui sont pertinents à la fois dans la modélisation de la donnée géographique multidimensionnelle et de ses hiérarchies, et aussi dans l'analyse. L'analyse spatiale offerte par les Systèmes d'Information Géographique est par nature flexible et itérative : les données géographiques peuvent être modifiées ou remplacées grâce aux méthodes de transformations spatiales tout au long du processus d'analyse. Or les opérateurs spatiaux fournis par les différents systèmes SOLAP sont souvent des opérateurs orthogonaux aux opérateurs et mesures détaillées) et/ou pré-calculées (mesures agrégées). Chaque résultat d'analyse est la conséquence des résultats précédents. Chaque étape du processus d'analyse est effectuée par une navigation dans l'hypercube, ou par une requête multidimensionnelle. Ces requêtes utilisent les opérateurs OLAP (Roll-up, Drill-down, Slice, etc…) permettant de calculer les mesures pour des ensembles de membres à des niveaux de granularité et selon des prédicats sélectionnés par l'utilisateur.
Les architectures des systèmes OLAP sont des architectures trois tiers. Le premier tiers est un serveur d'entrepôt de données. Les données, sélectionnées pour leur pertinence, sont extraites des bases de données transactionnelles, nettoyées, transformées puis intégrées dans l'entrepôt. Le deuxième tiers est un serveur OLAP qui, pour optimiser le temps de réponse, pré-calcule les différents agrégats en utilisant des fonctions d'agrégation classiques de l'algèbre relationnelle (SUM, MIN, MAX, AVG ou COUNT). Le dernier tiers est un client OLAP qui offre une interface utilisateur avec des outils de reporting, d'analyse interactive, et parfois de fouille de données. Le paradigme de visualisation le plus adopté par les clients OLAP est la table de pivot. Il s'agit d'un tableau multidimensionnel auquel sont associés les totaux et les sous-totaux, et qui offre une vue de données imbriquées sur plusieurs niveaux. Les tables de pivot sont généralement couplées avec des affichages graphiques (histogrammes, courbes, etc.). Les actions de l'utilisateur sur ces différentes composantes (clic de souris, sélection graphique, drag-and-drop) se traduisent par l'appel aux opérateurs OLAP.
L'uniformisation de données hétérogènes, la présentation de l'information à différents niveaux de détail et sur différents axes d'analyse, la rapidité d'accès aux données et l'interface interactive, simple et intuitive font des entrepôts de données associées aux outils OLAP de véritables SAD.
Systèmes d'Information Géographique et analyse spatiale
L'information géographique est la représentation d'un objet ou d'un phénomène réel localisé dans l'espace à un moment donné. L'information géographique est caractérisée par une composante purement spatiale et une composante sémantique (Degréne et Salgé, 1997). La composante spatiale représente la position sur la surface terrestre et la forme d'un objet du monde réel. Une position est décrite dans un système de référence explicite comme par exemple un système de coordonnées. Cette composante permet de représenter la forme de l'objet lui-même et de positionner celui-ci par rapport aux autres phénomènes ou objets du monde réel. La composante sémantique représente l'information relative à la nature, l'aspect et les propriétés descriptives d'un objet ou d'un phénomène du monde terrestre. Cette information peut aussi inclure des relations spatiales, descriptives et de généralisation cartographique avec d'autres objets ou phénomènes. En effet, un des aspects sémantiques qui distingue l'information géographique des données classiques est sa représentation multiple à différentes échelles ou selon différents thèmes secondaires (Weibel et Dutton, 2001). Les cartes généralisées sont obtenues grâce aux opérateurs de généralisation (Regnauld et McMaster, 2007).
Les Systèmes d'Information Géographique (SIG) permettent de générer, mémoriser et visualiser les données géographiques. Les SIG offrent des fonctions d'analyse spatiale, qui les transforment en véritables outils pour l'analyse spatiale. Ainsi, les utilisateurs de SIG disposent d'un ensemble d'outils statistiques (par exemple des méthodes de Point Pattern Analysis) et non statistiques (par exemple le buffer), qui peuvent créer ou modifier les données géographiques (Longley et al., 2001). Le processus d'analyse spatiale est itératif et comprend les étapes suivantes : (1) identification du problème et des buts de l'analyse, (2) identification des problématiques spatiales et des outils pour les résoudre, (3) identification des données et leur préparation pour les opérations spatiales, (4) création d'un plan d'analyse, (5) exécution du plan et visualisation des résultats (Mitchel, 2005). Chaque étape du processus peut être modifiée, itérée ou éliminée en fonction des résultats obtenus. En d'autres termes, les données, les sujets et les outils du processus d'analyse spatiale ne sont pas fixés a priori. Dans ce processus, la visualisation des résultats joue évidement un rôle très important car elle stimule l'utilisateur dans son processus de découverte de patterns, relations et tendances. La «géo-visualisation» intègre ainsi les techniques de visualisation scientifique, de cartographie, d'analyse des images, d'exploration de données, pour fournir une théorie, des méthodes et des outils pour la représentation et la découverte de la connaissance spatiale (MacEachren et al., 2001).
L'OLAP Spatial
Yvan Bédard définit l'OLAP Spatial (SOLAP) comme «une plate-forme visuelle spécialement conçue pour supporter l'analyse et l'exploration spatio-temporelles rapides et faciles des données multidimensionnelles composées de plusieurs niveaux d'agrégation à l'aide d'affichages cartographiques aussi bien qu'à l'aide de tableaux et diagrammes statistiques» (Bédard, 1997). Le SOLAP se propose comme un SAD dans lequel les fonctionnalités OLAP sont associées à des fonctionnalités SIG et à des techniques de géo-visualisation (Rivest et al., 2005). La prise en compte de la composante spatiale améliore l'analyse OLAP classique, la représentation cartographique permettant de mettre en évidence des relations spatiales entre différents faits et/ou dimensions qu'une simple étiquette textuelle ou un affichage graphique n'aurait pas, ou mal, montrées. Les modèles SOLAP redéfinissent les concepts principaux de l'OLAP : mesure, dimension et opérateurs de navigation multidimensionnelle.
Concepts principaux
Le terme de dimension spatiale désigne la prise en compte de l'information spatiale dans un axe d'analyse d'une application décisionnelle. Plusieurs travaux s'intéressent à la modélisation et la prise en compte des dimensions spatiales, par exemples (Bédard, et al., 2001), (Fidalgo et al., 2004) et (Malinowski et Zimányi, 2005). L'introduction explicite de la composante spatiale dans les niveaux des hiérarchies de dimensions permet de bénéficier d'une représentation cartographique des membres de dimensions, d'utiliser de prédicats spatiaux dans les opérations de coupe, et de prendre en compte les relations topologiques pendant les processus d'agrégation (Jensen et al., 2004), (Malinowski et Zimányi, 2005).
L'introduction des données spatiales dans les dimensions d'entrepôts de données a mené différents auteurs à la définition d'opérateurs d'analyse spatio-multidimensionnelle. (Rivest et al., 2005), (Sampaio et al., 2006), (Matias et Moura-Pires, 2007), (Scotch et Parmanto, 2005), (Hernandez et al., 2005) reformulent les opérateurs de forage définissant les opérateurs "spatial-drill down" et "spatial roll-up" qui permettent de naviguer dans une dimension spatiale. (Sampaio, et al., 2006), (Colonnese et al., 2005) et (Matias et MouraPires, 2007) appellent "spatial slice" une opération de coupe qui utilise un prédicat spatial.
-130 -RNTI-E-13 (Scotch et Parmanto, 2005) introduisent deux nouveaux opérateurs de coupe : le "buffer" et le "spatial drill-out". Le "buffer" utilise l'opérateur d'analyse spatiale de buffer, qui crée une zone tampon autours d'un membre, pour sélectionner des membres de la dimension spatiale. Le "spatial drill-out" sélectionne tous les membres adjacents au membre sur lequel cet opérateur est appliqué. Ces opérateurs spatio-multidimensionnels permettent de naviguer dans un hypercube spatial en utilisant les attributs géométriques des dimensions spatiales.
Les outils SOLAP
Un outil SOLAP repose sur l'intégration des fonctionnalités SIG et OLAP (Kouba, et al., 2000), (Rivest et al., 2005  (Silva et al., 2006) et (Sampaio et al., 2006). (Silva et al., 2006) présentent un outil web SOLAP dont la caractéristique principale est l'usage des services web géographiques pour la définition de GeoMDQL, un langage de requêtes qui étend le langage MDX de Microsoft pour les entrepôts de données spatiales. Le prototype est basé sur le serveur OLAP Mondrian modifié pour gérer les requêtes GeoMDQL et le client OLAP JPivot couplé avec une carte interactive pour la représentation des dimensions spatiales. L'outil utilise la modélisation logique présentée en (Fidalgo et al., 2004) et ne gère pas les mesures spatiales. Dans (Sampaio et al., 2006), les auteurs décrivent un système web SOLAP qui permet d'interroger les entrepôts de données spatiales avec des opérateurs de forage et de sélection sur la dimension spatiale et gérer les mesures spatiales. Dans cette solution, l'interface web est composée d'une interface cartographique, un navigateur pour sélectionner les membres de dimensions, et une zone de texte pour éditer les requêtes multidimensionnelles. Dans (Rivest et al., 2005)  
Limites des solutions SOLAP
Dimension spatiale
Les modèles proposés en littérature (Bédard et al. 2001), (Fidalgo et al. 2004) (Malinowski et Zimányi, 2005) qui introduisent l'information géographique comme axe d'analyse, sont caractérisés par la présence de l'attribut géométrique dans les membres des différents niveaux. Les hiérarchies spatiales sont définies en utilisant les attributs spatiaux ou alphanumériques des dimensions. Ces relations définissent, comme pour les hiérarchies classiques, une relation d'inclusion entre les membres de niveaux différents. Les hiérarchies spatiales sont donc des hiérarchies classiques de classification ou de spécialisation (LujianMora et al. 2002) avec des attributs géométriques. Ainsi, entre deux membres spatiaux de -132 -RNTI-E-13 S. Bimonte et al. deux niveaux différents, il existe toujours une relation topologique d'inclusion ou intersection (Malinowsky et Zimányi, 2005). Ces hiérarchies représentent différentes granularités de l'information géographique, chaque niveau géographique représentant une information géographique différente.
Les dimensions spatiales, basées exclusivement sur les relations spatiales, ne reflètent pas la sémantique des relations entre les membres de différents niveaux. Par exemple, un des aspects sémantiques qui caractérise l'information géographique est sa représentation à différentes échelles ou selon différents thèmes secondaires. Or, les relations hiérarchiques de généralisation cartographique ne représentent pas toujours de relations d'intersection ou inclusion. Grâce à ce type de hiérarchie, le décideur pourrait, par exemple, naviguer à travers différentes représentations à diverses échelles de la même information géographique, et visualiser les mesures à différents degrés de précision spatiale ; les méthodes d'agrégation devraient alors être différentes de celles utilisées pour les hiérarchies spatiales classiques car il n'est pas possible de quantifier l'apport d'un membre par rapport à son ancêtre dans le cas d'une hiérarchie de généralisation.
Selon nous, la prise en compte de la sémantique des relations entre les différents membres des dimensions spatiales est fondamentale pour le processus décisionnel, car elle peut être utilisée pour caractériser les processus d'agrégation, comme nous le montrons dans la section suivante.
Opérateurs multidimensionnels
Dans le processus d'analyse multidimensionnelle, l'utilisateur navigue dans l'hypercube à travers les hiérarchies des dimensions, en comparant les mesures, qui sont agrégées à différentes granularités avec des fonctions d'agrégation. Les chemins d'analyse sont donc imprédictibles, mais le contexte d'analyse, i.e. les données et le modèle multidimensionnel, est défini dès la phase de conception. Le processus d'analyse spatiale, lui, est un processus itératif dans lequel les données sont modifiées ou remplacées à chaque itération grâce aux méthodes d'analyse spatiale de transformation (i.e., le buffer, l'overlay, etc.). Ainsi, il apparait souhaitable d'introduire et d'adapter les opérateurs d'analyse spatiale au contexte OLAP afin de profiter pleinement du caractère itératif de l'analyse spatiale. Ainsi les dimensions qui incluent l'information géographique doivent pouvoir être reformulées selon les exigences de l'utilisateur et les mesures doivent alors être recalculées. Dans la majorité des cas, les outils SOLAP offrent d'une part les fonctionnalités OLAP de navigation, et d'autre part les fonctionnalités SIG de visualisation et d'analyse spatiale. Les cartes interactives sont couplées avec un ensemble d'outils d'analyse spatiale (i.e. ajout des cartes en format matriciel ou vectoriel, personnalisation des affichages, opérateurs métriques, requêtes spatiales, opérateurs d'analyse spatiale ou de fouille de données, etc.) qui ne sont pas intégrées au processus de navigation multidimensionnel. En d'autres termes, ces outils ne modifient pas la structure de l'hypercube ce qui limite le caractère itératif du processus d'analyse.
Un cadre conceptuel pour l'OLAP géographique
Dans cette section, nous introduisons les concepts de dimension géographique et les opérateurs multidimensionnels associés ainsi que les principales définitions de notre modèle formel multidimensionnel.
Pour présenter nos contributions, nous utiliserons comme cas d'étude un projet concernant l'étude de la pollution de la lagune de Venise réalisé dans le cadre d'une collaboration avec l'organisation internationale CORILA (Consorzio per la Gestione del Centro di Coordinamento delle Attività di Ricerca inerenti il Sistema Lagunare di Venezia), qui a comme but la sauvegarde environnementale, architecturale et économique de la lagune de Venise.
Dimension Géographique
Nous appelons objet complexe une entité du monde réel (un patient, un produit, etc.) décrite par un ensemble d'attributs descriptifs alphanumériques (âge, nom, type, etc.). Un objet géographique est un objet complexe (une ville, un bâtiment, etc.) qui présente un attribut spatial (i.e. une géométrie) en plus de ses attributs descriptifs. Un exemple est une unité environnementale de la lagune de Venise décrite par les attributs suivants : son nom ("Murano", "Chioggia", etc.), la liste de plantes qui la recouvrent ("Spartima Marittima", etc.), son type (industrielle, agricole, etc.), un index de salinité (valeur numérique), un attribut spatial décrivant sa géométrie et sa surface.
Définition 1. Une dimension est dite géographique si les membres d'au moins un niveau sont des objets géographiques.
Une dimension géographique structure l'information à différentes granularités représentées par ses niveaux. Les membres de ces niveaux peuvent être liés par des relations spatiales, et/ou des relations de généralisation cartographique et/ou des relations descriptives. Une dimension géographique peut présenter une ou plusieurs de ces hiérarchies, l'opération d'agrégation appliquée à la mesure dépend alors de la hiérarchie sur laquelle on navigue. On peut ainsi imaginer appliquer une opération additive lorsqu'on navigue sur une hiérarchie spatiale, et aucune agrégation sur une hiérarchie de généralisation cartographique. La modélisation des hiérarchies non-strictes, non-couvrantes et non-onto est une de caractéristiques avancées des modèles multidimensionnels classiques. Une hiérarchie est non-couvrante si une branche de la hiérarchie peut "sauter" un ou plusieurs niveaux, nononto si les feuilles ne sont pas toutes au même niveau, et non-stricte si une relation n-n peut exister entre les membres de différentes niveaux. Notre modèle doit permettre de modéliser ces hiérarchies complexes afin de prendre en compte les caractéristiques spécifiques des relations entre objets géographiques. Nous introduisons maintenant les différents types de hiérarchies applicables à une dimension géographique.
Hiérarchie descriptive
Une hiérarchie descriptive organise l'information géographique à différentes granularités thématiques.
Définition 2. Une hiérarchie descriptive d'une dimension géographique est une hiérarchie OLAP de classification ou de spécialisation. Elle est définie en utilisant les attributs descriptifs des objets géographiques.
Un exemple de hiérarchie descriptive pour la lagune de Venise est présenté dans la Figure 1. Cette hiérarchie classe les unités ("Unit") de la lagune par rapport à leur type ("Type").  Les dimensions sont le temps, les polluants et une dimension géographique avec la hiérarchie descriptive. Notons que certains niveaux de la dimension géographique peuvent être alphanumériques (eg. le niveau "Type"). Cette application permet d'analyser la pollution des eaux de la lagune en fonction du temps, des polluants et des unités de la lagune. La valeur de pollution est agrégée en utilisant la moyenne. Une requête multidimensionnelle peut être : "Quelle est la valeur moyenne de la pollution pour les unités de type industriel pour chaque année et pour les polluants de type organique ? ". Ce modèle multidimensionnel utilise la moyenne comme fonction d'agrégation sans contraintes particulières.
Hiérarchie spatiale
Définition 3. Une hiérarchie spatiale d'une dimension géographique est une hiérarchie où les membres de différents niveaux sont liés par des relations topologiques d'inclusion et/ou d'intersection.
Une hiérarchie spatiale peut éventuellement être calculée grâce à l'attribut géométrique des membres de la dimension. Si les mesures peuvent être redistribuées sur la surface des membres, les relations topologiques qui caractérisent cette hiérarchie peuvent permettre de quantifier l'apport d'un membre par rapport à son ancêtre dans le calcul de l'agrégation. Un exemple de schéma et d'instance d'une hiérarchie spatiale est montré dans Figure 3 où plusieurs unités de la lagune de Venise "Unit" sont regroupées de façon topologique en régions "Zone". Une relation d'inclusion existe entre les unités et les zones. Le calcul de la mesure, ici la moyenne de la valeur de pollution, peut prendre en compte ces relations topologiques.  
Hiérarchie de généralisation cartographique
Une hiérarchie de généralisation cartographique représente une information géographique à différentes échelles ou selon différents thèmes secondaires.
Définition 4. Une hiérarchie de généralisation cartographique d'une dimension géographique est une hiérarchie où les membres des niveaux représentent l'information géographique à différentes échelles/ ou selon différents thèmes secondaires et dont les membres d'un niveau sont les résultats de la généralisation des membres du niveau directement inférieur.
Dans la même application multidimensionnelle que celle de la Figure 4, nous substituons à la hiérarchie spatiale une hiérarchie de généralisation cartographique qui représente les unités de lagune de Venise à deux différentes échelles 1:1000 et 1:500 ( Figure 5). Cette application multidimensionnelle permet de répondre aux questions telles que : "Quelle est la pollution moyenne par polluant, année et par unité de la lagune à l'échelle 1:500 ?"
FIG 5. Modèle multidimensionnel comportant une dimension géographique représentant la lagune de Venise décrite par une hiérarchie de généralisation cartographique.
La Figure 6 montre un exemple de cette hiérarchie et les deux cartes associées. La carte généralisée est obtenue en utilisant les opérateurs de généralisation, d'agrégation, de simplification et de sélection (Regnauld et McMaster, 2007) : la forme de "Palude Maggiore" est simplifiée, "Campo" et "Ruzolo" sont fusionnées dans une seule grande zone dont la forme est simplifiée et "Colpo" est éliminée ("Colpo" est associé à la racine de la hiérarchie). Notons que cette relation est multi-valuée.
La représentation des mesures à travers des cartes à différentes échelles ou selon différents thèmes secondaires permet à l'utilisateur d'avoir un aperçu visuel global et simplifié du phénomène, en excluant les informations n'étant pas primordiales pour la compréhension de ses caractéristiques générales. Le processus d'agrégation pour les hiérarchies de généralisation cartographique doit tenir compte des relations de multiassociation (Spaccapietra et al., 2007)   
Opérateurs spatio-multidimensionnels portant sur la dimension géographique
Nous classifions les opérateurs spatio-multidimensionnels qui s'appliquent aux dimensions géographiques en trois catégories :
• Les opérateurs de forage permettent la navigation dans les dimensions géographiques. Dans le cas où plusieurs hiérarchies co-existent, ils doivent permettre de préciser la hiérarchie de navigation utilisée.
• Les opérateurs de coupe permettent de sélectionner une partie de l'hypercube en utilisant l'interaction avec la carte et/ou des relations topologiques, métriques et/ou directionnelles entre les membres géographiques.
• Les opérateurs de modification dynamique de l'hypercube permettent à l'utilisateur de créer de nouveaux membres géographiques à la volée grâce à des opérateurs d'analyse spatiale.
Les opérateurs de modification dynamique de l'hypercube permettent d'introduire de la dynamique dans la structure de l'hypercube. Au contraire des opérateurs de forage et de coupe, ils représentent une nouvelle approche dans l'analyse spatio-multidimensionnelle. En effet, comme nous l'avons montré dans la section précédente, les opérateurs d'analyse spatiale des solutions SOLAP gardent une définition "orientée" SIG. Or, fournir une vision multidimensionnelle de ces opérateurs est indispensable pour que les processus d'analyse multidimensionnelle et d'analyse spatiale s'enrichissent l'un l'autre. L'adaptation des opérateurs d'analyse spatiale au modèle multidimensionnel implique que les dimensions géographiques et les mesures associées puissent être calculées à la volée. La connaissance du phénomène étudié est bien sûr fondamentale pour définir le mode de calcul des mesures après insertion et/ou transformation d'un membre dans la dimension géographique.
Nous détaillons ici un de ces opérateurs. L'overlay est un opérateur d'analyse spatiale de transformation qui permet de mettre en relation des informations de nature différente. A partir de deux cartes, il génère une carte dans laquelle les géométries des objets géographiques sont recalculées grâce à l'opération topologique d'intersection. L'ensemble des objets géographiques de la carte résultat dépend de l'opérateur logique utilisé (AND ou OR). Un exemple d'overlay qui utilise l'opérateur logique AND est montré dans la Figure 7. Dans un contexte spatio-multidimensionnel, il est possible d'appliquer l'overlay entre la carte qui représente un niveau d'une dimension géographique et une autre couche choisie par l'utilisateur. Le résultat de cette opération crée de nouveaux membres de la dimension géographique. Pour ces nouveaux membres, les attributs des mesures doivent être recalculés -138 -RNTI-E-13 en utilisant les parties de membres issues de l'opération d'overlay, les membres originaux des deux couches et les valeurs des attributs des mesures dans la table de faits. De la même façon, les opérateurs spatiaux de buffer, clipping, etc peuvent être utilisés pour modifier la structure et le contenu (i.e. membres et mesures) de l'hypercube.
Le modèle GeoCube
Les concepts de l'OLAP Géographique sont formalisés dans le modèle conceptuel GeoCube (Bimonte et al., 2005), (Bimonte, 2007). GeoCube prend en compte les composantes spatiale et sémantique de l'information géographique en dimension et en mesure et introduit la dynamique de l'analyse spatiale dans le processus d'analyse multidimensionnelle.
Ainsi, parallèlement aux concepts de dimension géographique et des opérateurs multidimensionnels géographiques associés, l'OLAP Géographique définit le concept de mesure géographique. Dimension et mesure sont traitées de façon symétrique, ce qui permet d'étendre le type de requêtes effectuées par l'utilisateur. Le concept de mesure géographique étant partie intégrante du modèle et de son algèbre, nous en synthétisons ci-après les grandes lignes. Le lecteur pourra trouver une présentation plus détaillée dans (Bimonte et al., 2006).
La mesure géographique est un objet géographique, décrit par un ensemble d'attributs descriptifs et un attribut spatial, et appartenant à une ou plusieurs hiérarchies géographiques, ceci dans une définition parfaitement symétrique avec celle des dimensions. L'information géographique peut donc être utilisée en dimension comme en mesure. L'agrégation d'une mesure géographique (lors d'une opération de forage) consiste en l'agrégation différenciée de ses attributs : par exemple, sur une mesure "unit", on appliquera la fusion des géométries, aucune agrégation aux noms, une liste pour les plantes, la moyenne pour la salinité et, pour le type, une moyenne pondérée par la surface. Ces attributs descriptifs pourront être utiles au processus décisionnel, par exemple un index de salinité très élevé peut se révéler toujours associé à une forte pollution en fer. D'autre part, la mesure géographique appartient à une structure hiérarchique et doit donc pouvoir être analysée à différentes granularités qui correspondent aux niveaux des hiérarchies. Ainsi, le passage d'une mesure détaillée à une mesure agrégée s'effectue lors d'une opération de forage ; le passage d'une mesure à une mesure de granularité différente s'effectue lors d'une opération de navigation dans la mesure. Cette vision complètement symétrique entre mesures et dimensions, associée à des opérateurs de navigation dans la mesure augmente encore la dynamique de l'analyse OLAP géographique.
Dans le reste de cette section nous présentons les concepts principaux du modèle de données et de l'algèbre de GeoCube. -Llagoon_spatial = {Szone}, Sunit, Sall_unit sont les niveaux de la hiérarchie, -lagoon_spatial est la relation d'ordre du treillis telle que (Sunit lagoon_spatial Szone) et (Szone lagoon_spatial Sall_unit) Le Schéma du Cube de Base représente le schéma conceptuel de l'application multidimensionnelle et l'Instance de Cube de Base représente le cuboïde de base, i.e. une table de faits où toutes les dimensions sont aux niveaux les plus détaillés. Le Schéma du Cube de Base est défini par un ensemble de Schémas de Hiérarchie et une fonction booléenne. Cette fonction peut être représentée dans un espace multidimensionnel où les instances des membres des niveaux les plus détaillés des dimensions sont projetées sur les axes, et les points sont des valeurs booléennes représentant l'existence du fait. Grâce à cette modélisation de l'espace multidimensionnel, les Schémas de Hiérarchie utilisés dans la définition du Cube de Base jouent tous le rôle de dimensions. La distinction entre les Schémas jouant le rôle de dimensions et le Schéma jouant le rôle de la mesure se fait de façon dynamique, au moment de l'énoncé de la requête multidimensionnelle.
Modèle de données
Le Cube de Base pour l'application de la figure 4 est BCcorila = ?Hpollutants, Htime, Hlagoon_spatial, Hrate, ?? où :
-H pollutants est la hiérarchie qui décrit les polluants, -H time est la hiérarchie temporelle, -H rate est la hiérarchie des intervalles de valeurs de pollution, -H lagoon_spatial est la hiérarchie spatiale -? une fonction booléenne définie sur I(S pollutant )×I(S day )×I(S unit )×I(S rate5 ) soit le produit cartésien des instances des niveaux les plus détaillés des hiérarchies du Cube.
L'algèbre
Le  Il est aussi possible de définir, à partir du même Cube de Base, une Vue qui utilise les unités de la lagune comme mesures : Vcorila-unit-day = ?BCcorila, ?Spollutant, Sday, Srate?, ?unit-fusion, ?? représentant les unités polluées par polluant, valeur de pollution et jour. où H h est la hiérarchie contenant le membre géographique résultant de l'opérateur de buffer et f est une fonction qui calcule les mesures pour ce nouveau membre géographique. L'opérateur ? utilise les mêmes paramètres. -Deux opérateurs qui permettent de changer la granularité de la mesure. Ces opérateurs permettent de monter dans la hiérarchie de la mesure en remplaçant un ensemble de mesures détaillées par des mesures appartenant à un niveau moins détaillé. L'opérateur Classify, noté ?, nécessite que tous les descendants des nouvelles mesures soient présents dans la   Figure 8A), les graphiques, et la carte avec la GIS Toolbar et la GeWOlap Toolbar. La GIS Toolbar ( Figure 8B) propose des fonctionnalités purement SIG (zoom, pan, etc.). La GeWOlap Toolbar ( Figure 8C) présente les opérateurs tels que définis dans la section précédente, une fonctionnalité (Properties Member) qui affiche les propriétés d'un membre de la dimension géographique et une autre (Measure displays) qui permet de paramétrer le type de diagramme représentant les mesures.
Présentation générale de l'interface utilisateur
Navigation avec GeWOlap
Dans cette section, nous illustrons le processus de navigation dans l'hypercube de la Figure 4. Nous montrons les opérateurs de forage, les opérateurs de coupe et les opérateurs de modification dynamique de l'hypercube. La navigation dans un hypercube avec des mesures géographiques a été décrite en détail dans (Bimonte et al., 2006).
GIS Toolbar
GeWOlap Toolbar
FIG 8. Interface GeWOlap ; A) OLAP Toolbar, B)GIS Toolbar C) GeWOlap Toolbar
Les opérateurs de forage (GeWOlap Roll-up, GeWOlap Drill-down Position, etc.) sont disponibles dans GeWOlap à travers la table de pivot et la composante cartographique. Lors de ces opérations, GeWOlap synchronise les composantes tabulaire et cartographique, en évitant que des membres cachent (recouvrent) leur fils sur la carte. On peut ainsi obtenir une carte dont les zones géographiques sont de grains différents. Un exemple d'utilisation d'un opérateur de forage est montré dans la Figure 9. La carte initiale (Figure 9.1) montre 3 mesures qui représentent la pollution moyenne pour tous les polluants, les polluants inorganiques et organiques sur la région sud. Après forage sur le membre géographique "North Lagoon", la carte de la Figure 9.2 présente la pollution moyenne sur des zones détaillées de la région nord de la lagune (North Swam, South Venice, North Venice, Bocca Lido et Litorale de Cavallino).
Cette opération réalise l'opération ?(Vcorila-rate)[Szone] = Vcorila-rate-zone où Vcorila-rate est la Vue qui représente la valeur de la pollution pour chaque unité, tous les polluants et tous les jours, et Vcorila-rate-zone représente la valeur de la pollution pour chaque zone, tous les polluants et tous les jours. Il est important de souligner le fait que puisque la table de pivot mélange plusieurs granularités d'une même dimension, sa représentation formelle correspond à un ensemble de Vues. Les opérateurs de modification dynamique de l'hypercube (GeWOlap Buffer et GeWOlap Overlay) créent de nouveaux membres de dimensions géographiques et permettent de recalculer les mesures pour ces nouveaux membres. GeWolap Buffer permet d'exprimer des requêtes sur des membres de la dimension géographique définis à la volée par l'utilisateur, grâce à la désignation d'une zone tampon. Pour ces nouveaux membres, les mesures doivent être recalculées grâce à une fonction fournie par l'utilisateur. Le calcul de la mesure prend en compte les parties de membres qui sont recouverts totalement et/ou partialement par la région tampon, les membres originaux et les valeurs de mesures dans la table de faits associés. Supposons ainsi que la concentration des polluants est calculée comme une moyenne pondérée sur les surfaces. La Figure 10.2 montre la pollution moyenne pour "Bocca Chiogia", "Chioggia", "Litorale Pellestrino", "South Swam" et une zone tampon de 3 km autour de "Bocca Malmocco". Cette opération implémente l'opérateur ?(Vcorila-rate) [Hlagoon_spatial_zoneb,favgbuffer]. Il est appliqué à Vcorila-rate en utilisant comme paramètres une hiérarchie qui contient le résultat de l'opération spatiale de buffer et la fonction pour le re-calcul de la mesure. On note que la modification issue de l'application de cet opérateur se répercute à la fois sur la représentation tabulaire et sur la cartographie. 
FIG 10. GeWOlap Buffer 1) Données initiales 2) Résultat
L'opérateur GeWOlap Overlay permet de faire des requêtes sur des membres calculés en utilisant une autre information géographique issue d'une autre couche et une fonction qui est fournie par l'utilisateur. Le calcul de la mesure prend en compte les parties de membres issus de l'opération d'overlay, les membres originaux des deux couches et les valeurs de mesures dans la table de faits associés. Supposons que l'utilisateur veuille ajouter aux informations de la pollution une autre source de données, par exemple une carte thématique de la lagune définissant les zones soumises à épuration. Supposons que la valeur de la nouvelle mesure peut être estimée comme une moyenne pondérée sur la surface des zones non épurées. La Figure 11.2 montre les valeurs modifiées de la pollution pour les zones de la lagune "North Venice" soumises à épuration. Nous notons que la pollution dans "Isola delle Tresse" avant l'overlay était de 2631.4 (Figure 11.1). GeWOlap overlay nous permet de mettre en évidence une zone d'épuration ("Isola delle Tresse-ZoneRD") où la valeur de la pollution a pu être ré-estimée. Cette opération implémente l'opérateur ?(Vcorila-rate) [Hlagoon_spatial_zoneov, favgover]. Il utilise comme paramètres une hiérarchie qui contient le résultat de l'opération spatiale d'overlay et la fonction pour le re-calcul de la mesure. 
FIG. 11. 1) Données initiales 2) Résultats du GeWOlap Overlay
Conclusion
L'OLAP spatial vise à intégrer la donnée spatiale dans les systèmes OLAP pour offrir un processus de décision spatial et multidimensionnel. En partant d'une réflexion sur l'analyse multidimensionnelle et spatiale, nous montrons que les modèles SOLAP ne prennent pas complètement en compte la composante sémantique de l'information géographique et le caractère itératif de l'analyse spatiale. Nous redéfinissons les concepts de dimension spatiale et les opérateurs spatio-multidimensionnels afin de pallier cette lacune. Nous introduisons le concept de dimension géographique et spécifions les différents types de hiérarchies possibles en considérant la sémantique des relations entre les membres. Nous identifions également de nouveaux opérateurs spatio-multidimensionnels qui intègrent l'analyse spatiale dans le paradigme multidimensionnel. Nous présentons notre prototype GeWOlap, une solution web qui intègre des fonctionnalités OLAP géographiques dans un environnement unique et synchronisé en supportant les dimensions et les mesures géographiques. Ce prototype propose des opérateurs qui intègrent l'analyse multidimensionnelle et l'analyse spatiale. L'introduction de ces opérateurs présente une double difficulté : la prise en compte d'une gestion dynamique de la structure de l'hypercube (en termes de membres et de hiérarchie) et le calcul des mesures associées aux nouveaux membres. GeWOlap vise à combiner effectivement l'analyse spatiale et multidimensionnelle dans le même paradigme d'exploration et d'analyse. Actuellement, nous travaillons sur l'intégration des hiérarchies de généralisation cartographique dans GeoCube et sur des politiques d'agrégation ad-hoc pour ce type de hiérarchies. De plus, un cadre global qui détermine automatiquement les variables visuelles à utiliser selon des règles sémiologiques parait nécessaire. En effet, GeWOlap

Introduction
Un des principaux objectifs de la biologie moléculaire consiste à comprendre la régulation des gènes d'un organisme vivant dans des contextes biologiques spécifiques. Les facteurs de transcription sont les régulateurs de la transcription qui vont réagir avec les promoteurs de la transcription des gènes cibles. Les techniques récentes d'analyse du transcriptome, telles que les puces à ADN permettent de mesurer simultanément les niveaux d'expression de plusieurs milliers de gènes. Nous avons déjà décrit le système LICORN (Elati et al., 2007a) qui se fonde sur un modèle de régulation locale coopérative : chaque gène peut être régulé par un ensemble des coactivateurs et/ou un ensemble de coinhibiteurs, ces corégulateurs agissent collectivement pour influencer leur(s) gène(s) cible(s). LICORN met en oeuvre une approche originale de Fouille de Données afin d'inférer des relations de régulation coopérative à partir de données d'expression. Cet algorithme a été évalué avec succès sur des données publiques de transcriptome de levure. L'application de LICORN sur des données de transcriptome humaines est plus complexe, car le nombre de régulateurs connus est plus important, et nécessite un temps de calcul considérable. En effet, les gènes de faible support vont avoir un nombre très élevé de régulateurs candidats. Nous proposons dans ce travail d'étendre LICORN pour qu'il puisse traiter une contrainte de sélection de corégulateurs candidats adaptative pour chaque gène, prenant en compte le support du gène cible et bornant le nombre de corégulateurs candidats possibles. La suite de cet article est organisée comme suit. Dans la section 2, nous introduisons briève-ment le principe de LICORN. Dans la section 3, nous détaillons l'extension de LICORN à la recherche adaptative. Nous exposons des résultats préliminaires concernant l'application de cette approche à des données de cancer de vessie (section 4) et enfin, nous concluons sur les perspectives ouvertes par ce travail.
LICORN : Algorithme d'apprentissage
Étant donnés : -un ensemble de régulateurs R et un autre de gènes cibles G -deux matrices d'expression discrétisées (MG, MR) à valeurs dans ? = {?1, 0, 1}, codant les états de R et G pour un échantillon S de n conditions : S = s 1 , . . . , s n -un programme de régulation RP : ? × ? ? ?, qui associe à un état d'un ensemble d'activateurs ? R et à un état d'un ensemble d'inhibiteurs ? R un état du gène cible 1 -une fonction de score local h : ? n × ? n ? R, permettant d'évaluer un réseau local de régulation, notre but est de trouver, pour chaque gène cible, l'ensemble de régulateurs qui explique au mieux son niveau d'expression, i.e. le graphe qui minimise la différence entre la valeur d'expression prédite et la valeur d'expression observée pour un gène cible. Pour le résoudre, nous avons formalisé le problème d'inférence de réseaux de régulation comme un problème de recherche dans un espace d'états muni d'une relation de spécialisation, des motifs satisfaisant un ensemble de contraintes anti-monotones (Agrawal et al., 1993). Ceci nous a permis d'optimiser le parcours de l'espace de recherche et de proposer une méthode originale LICORN, capable d'apprendre des réseaux de régulation coopérative. Cet algorithme décompose la tâche d'apprentissage de structures en trois étapes indépendantes. Nous présentons par la suite ces trois étapes, déjà décrites dans Elati et al. (2007a). Dans la suite, chaque gène ou régulateur g est représenté par deux ensembles supports ? S, son 1-support S 1 (g) et son ?1-support S ?1 (g). LICORN construit d'abord, en utilisant une extension de l'algorithme Apriori (Agrawal et al., 1993) qui manipule en parallèle les deux supports (1 et ?1-supports), le treillis CL des ensembles de corégulateurs fréquents. Un corégulateur est fréquent s'il est fréquent pour au moins une des valeurs d'intérêt, ici 1 ou -1.
Puis, LICORN calcule pour chaque gène cible g un ensemble de co-régulateurs candidats, de taille réduite par rapport à celle de tous les sous-ensembles possibles de régulateurs. Le critère pour qu'un ensemble de corégulateurs puisse participer au programme de régulation d'un gène cible est d'observer dans les données d'expression une covariation significative de leurs niveaux d'expression. La contrainte de corégulation teste la taille de l'intersection entre les supports du gène et ceux du corégulateur.
Définition 1 (Contrainte de corégulation) Soit une combinaison de co-régulateurs C, un gène cible g, et leurs supports respectifs
Nous distinguons les complexes de régulateurs satisfaisant C coreg pour chaque gène cible selon leur mode de régulation : les complexes d'activateurs candidats A(g) co-varient positivement avec le gène g, en revanche, les complexes d'inhibiteurs candidats I(g) co-varient négative-ment avec le gène cible :
Nous avons décrit dans (Elati et al., 2007a) comment nous exploitons l'anti-monotonie de C coreg pour effectuer une recherche efficace dans le treillis des régulateurs candidats. Ainsi, nous pouvons calculer l'ensemble de tous les graphes de régulation candidats pour chaque gène cible g, à partir des complexes d'activateurs et d'inhibiteurs extraits comme suit :
Un GRN candidat pour un gène cible g, ou tout simplement un GRN, est un élement de C(g).
Enfin, à partir de chaque C(g), LICORN extrait le réseau de régulation (GRN) pour chaque gène cible qui explique au mieux les données d'expression, à l'aide d'une fonction de score (l'erreur absolue moyenne) qui mesure la capacité des régulateurs à prédire le niveau d'expression du gène cible g. Ensuite, LICORN sélectionne les GRNs statistiquement significatifs en se fondant sur une méthode (non décrite ici) à base de permutations.
Génération adaptative de corégulateurs candidats
L'algorithme précédent de génération de corégulateurs candidats ne fournit pas de méca-nisme pour limiter le nombre de corégulateurs candidats pour un gène cible. Ceci mène parfois à obtenir trop ou trop peu de corégulateurs candidats, et ainsi à un temps excessif de calcul ou à une faible performance. La plupart des algorithmes classiques d'extraction de motifs fréquents ou contraints ne permettent pas d'encadrer a priori le nombre souhaité de résultats. L'extraction des k-meilleurs motifs fréquents a été introduite dans (Fu et al., 2000). Notre objectif ici, est d'extraire pour chaque gène cible, s'il existe, un ensemble de taille modeste de meilleurs corégulateurs au sens de la contrainte de corégulation. Nous proposons, étant donné un intervalle sur le nombre de corégulateurs candidats souhaités, de concevoir un algorithme itératif qui effectue une recherche adaptative des meilleurs corégulateurs candidats.
Cet algorithme, noté AdapSearch (algorithme 1), ajuste le seuil de la contrainte de coré-gulation pendant la phase d'extraction des corégulateurs candidats, afin d'obtenir un nombre approprié de corégulateurs candidats pour chaque gène cible. Etant donné le treillis de corégulateurs fréquents (voir section 2), un gène cible g et un intervalle [minReg, maxReg] encadrant le nombre de corégulateurs candidats pour g, AdapSearch initialise le seuil de coré-gulation S coreg à 50% et appelle GenCoreg (lignes 1 à 3 de l'algorithme 1).
La procédure GenCoreg (non décrite) effectue une recherche par niveau sur ce treillis de corégulateurs, à partir des corégulateurs les plus généraux jusqu'à atteindre les corégulateurs les plus spécifiques (i.e., maximaux au sens de l'inclusion) qui satisfont la contrainte de corégulation. Notons que GenCoreg termine le processus de génération dès que le nombre de corégulateurs candidats dépasse maxReg. Quand le résultat de GenCoreg est retourné, AdapSearch vérifie d'abord si le nombre des corégulateurs candidats dépasse maxReg. Si c'est le cas (ligne 6), c'est-à-dire que le seuil de corégulation est trop bas, AdapSearch augmente  Si (minReg > |R(g)|) Alors 10:
R(g) := AdapMin(g,R(g),CL, maxReg, S coreg ) 12:
FinSi 13: Fin TantQue 14: Retourner R(g) 15: Fin le seuil de corégulation (algorithme 2, ligne 1) : S coreg est incrémenté d'un pas fixe ? et la procédure GenCoreg est appellée itérativement jusqu'à ce que le nombre de corégulateurs soit inférieur à maxReg ou que S coreg dépasse la valeur maximum, i.e., 1. Si l'augmentation du seuil engendre un ensemble de corégulateurs candidats dont la taille est inférieure à minReg (ligne 7 de l'algorithme 2), alors qu'il était supérieur à minReg dans l'itération précédente (ligne 5 de l'algorithme 2), alors l'algorithme oscille entre deux valeurs de S coreg et AdapSearch retourne alors les meilleurs maxReg corégulateurs candidats parmi les deux ensembles de régulateurs résultats. R(g) := maxReg meilleurs corégulateurs de (R(g) ? tmp) 5: FinSi 6: Retourner R(g) Dualement, AdapSearch vérifie si le nombre de corégulateurs est inférieur à minReg. Si c'est le cas, on décrémente le seuil de corégulation de ? jusqu'à ce que le nombre de corégu-lateurs candidats soit supérieur ou égal à minReg ou que S coreg atteigne un seuil minimum (par exemple 30%). Dans ce dernier cas, on garantit que les corégulateurs ont un minimum de recouvrement avec le gène cible.
Expérimentations : LICORN versus LICORN adaptatif
Dans cette étude, nous utilisons des données humaines de 85 échantillons. Ces échantillons se divisent en cinq échantillons normaux (sujets dépourvus de cancer), et quatre-vingts échan-tillons de carcinomes de vessie à différents stades de la maladie. Nous nous intéressons à trois tendances de variation du niveau d'expression : dans un échantillon tumoral donné, le niveau d'expression d'un gène peut augmenter par rapport à son niveau normal (noté par 1 dans la matrice discrétisée) ; il peut diminuer par rapport à son niveau normal (noté par -1) ; ou encore être stable (noté par 0). Nous renvoyons à (Elati et al., 2007b) pour une description de la technique de discrétisation utilisée. Nous obtenons une matrice discrétisée de 8000 gènes et 699 facteurs de transcription.
Dans la partie gauche de la figure 1, nous présentons la distribution de nombre de corégu-lateurs candidats générés par LICORN avec S coreg fixé à 60%. La valeur médiane du nombre de corégulateurs candidats est 1024 et la valeur maximale est 35000. Il y aura donc un sousensemble des gènes, souvent de faible fréquence, pour lequel LICORN génère plusieurs milliers de corégulateurs candidats. Ces résultats nous montrent que l'utilisation de LICORN adaptatif avec un intervalle de nombre de corégulateurs candidats autour de la médiane va réduire nettement le volume de corégulateurs candidats engendré, ce qui représente un gain considérable en terme de temps de calcul. Dans la deuxième phase de validation, nous avons lancé LICORN adaptatif avec les valeurs suivantes de paramètres : ? = 0.1, valeur minimum de S coreg = 30% et un intervalle de coré-gulateurs candidats de [200, 2000]. LICORN adaptatif a obtenu des résultats équivalents voire meilleurs que LICORN (voir figure 1, partie droite) : i) 85% de GRNs sont détectés comme significatifs avec un FDR de 5% (voir Elati et al. (2007a)) par les deux méthodes avec 92% d'interactions régulateur/gène en commun. ii) 5% de GRNs détectés par LICORN seulement contre 10% de GRNs détectés par LICORN adaptatif seulement.
Conclusion
Afin d'améliorer notre algorithme en vue de l'appliquer sur des données humaines de cancer, nous l'avons étendu pour qu'il puisse traiter une contrainte de corégulation adaptative pour chaque gène, prenant en compte le support du gène cible et en bornant le nombre de corégula-teurs candidats possibles. Cette recherche adaptative a rendu le calcul plus efficace, tout en engendrant des réseaux dont les performances sont dans une grande majorité des cas équivalentes voire meilleures. Enfin, des structures plus adéquates peuvent être utilisées ou imaginées pour améliorer les performances de la recherche adaptative, par exemple la structure de COFI-tree récemment utilisée par Ngan et al. (2005) pour optimiser l'extraction de k-meilleurs motifs.
Références
Agrawal, R., T. Imielinski, et A. Swami (1993) 
Summary
We have previously proposed an original Data Mining algorithm LICORN, that infers cooperative regulation relations from expression datasets. LICORN has been successfully applied to public Yeast datasets, but running it on more complex datasets (e.g., human) is problematic. In order to overcome this limitation, we propose here an adaptive coregulation constraint that takes into account the support of the gene to look for a bound number of candidate regulators. Preliminary experiments on human bladder cancer data have shown that the adaptive constraint allows LICORN to learn much more efficiently regulation relations that show a similar (if not better) prediction performance.

Introduction
Le problème de la comparaison de graphes est un sujet qui a été largement étudié dans la littérature depuis plusieurs décennies. S'il existe des algorithmes pour la recherche d'isomorphisme entre deux graphes, c'est-à-dire dans le cas où les deux graphes ont la même structure, même nombre de noeuds et même nombre d'arêtes, le cas plus général de comparaison entre deux graphes de tailles différentes est un problème NP-complet. Le problème est encore plus difficile lorsque les graphes sont valués et que l'on recherche une mesure de similarité entre graphes, afin de pouvoir les ordonner, les classer, etc.
On est confronté à ce problème dans certaines approches de la reconnaissance des formes où on cherche à construire des classes d'objets représentés par des ensembles structurés de ré-gions, lignes, points, etc. Une des problématiques de la recherche d'image par le contenu est de retrouver dans une base, les images contenant un objet particulier ou un type d'objet, d'animal ou de personne, pouvant prendre des aspects très variables dans des environnements eux aussi variables. Les signatures globales ne permettent pas toujours de résoudre ce problème et les approches par points d'intérêt ne sont pas bien adaptées aux changements d'aspect d'un animal ou d'une personne, selon la prise de vue. Une approche prometteuse semble donc être de représenter un objet par un ensemble de régions adjacentes valuées à la fois par des caractéris-tiques intrinsèques de couleur, texture et forme, mais aussi par leurs dispositions relatives (cf. Philipp-Foliguet et Gony (2006)). Le graphe d'adjacence de régions constitue donc la structure adaptée pour représenter des objets dans leur infinie variabilité. Cependant l'obtention des ré-gions est un problème extrêmement difficile, qui ne possède en aucun cas une solution unique, car dépendant du niveau de détail souhaité, et qui est peu robuste aux changements d'éclairage, de résolution et d'aspect de l'objet. Le nombre et les caractéristiques des régions sont donc très variables d'une image à l'autre pour représenter un même objet.
Des approches récentes ont proposé de considérer les graphes comme des ensembles de chaînes (Kashima et Tsuboi (2004)) et les similarités employant des noyaux sur des graphes se ramènent alors à des noyaux sur des chaînes. Nous avons adopté cette approche par noyau de chaînes et proposons de comparer différents types de chaînes et différentes fonctions noyau sur ces chaînes.
Pour effectuer l'appariement entre graphes ou sous-graphes d'images en un temps compatible avec une utilisation en temps réel, nous avons opté pour l'algorithme de "branch and bound". La classification ou la fouille d'une base à partir d'un ou plusieurs exemples se fait ensuite par apprentissage interactif à l'aide de SVM (Support Vector Machine) pour la classification et de techniques d'apprentissage actif pour la sélection des images à faire annoter par l'utilisateur.
Mesures de similarité de graphes
Chaque image de la base est représentée par un graphe G ? G défini par un couple G = (V, E), où V est un ensemble de sommets, et E ? V × V un ensemble d'arêtes. Par exemple, lorsqu'une image est segmentée en régions, on peut construire un tel graphe en considérant que chaque sommet v ? V est une région, et chaque arête e = (v 1 , v 2 ) ? V × V représente une adjacence entre deux régions. On considère aussi les chaînes h ? H(G) présentes dans le graphe, à savoir les suites (v 1 , e 1 , v 2 , e 2 , ...) de sommets v i ? V reliées par des arêtes e i = (v i?1 , v i ) ? V × V . Différentes fonctions H(.) qui à un graphe G font correspondre un ensemble de chaînes peuvent être considérées, comme celles qui renvoient les chaînes avec ou sans cycles par exemple. Dans cette partie, nous nous intéressons aux fonctions de similarité qui peuvent être utilisées avec n'importe quel ensemble de chaînes, nous abordons le choix des fonctions H(.) dans la partie suivante.
Dans de nombreuses mesures de similarité
qui ont été proposées, l'idée est généralement de trouver les meilleurs appariements entre les sommets et les arêtes. Par exemple, Sorlin et al. (2006) propose une mesure de similarité qui renvoie la valeur moyenne des meilleures similarités en fonction des appariements entre sommets et arêtes. FReBIR (cf. Philipp-Foliguet et Gony (2006)) calcule la valeur du meilleur appariement entre une chaîne requête et les chaînes de l'autre image. Cependant cette mesure de similarité ne respecte aucune propriété mathématique usuelle telle que la symétrie ou l'inégalité triangulaire, ce qui la rend difficilement utilisable par certains outils puissants utilisés en classification ou en "browsing" par exemple.
Certaines mesures de similarité s'efforcent de respecter un ensemble de contraintes mathématiques permettant une utilisation facile par les moteurs de recherche, par exemple les fonctions noyaux au sens de Mercer. Dans ce cas, la mesure de similarité que nous noterons K(G, G ? ) sera le produit scalaire K(G, G ? ) = ?(G ? ) dans un certain espace Hilbertien H, avec ? : G ? H une fonction d'injection qui à un graphe fait correspondre un vecteur de H.
Certaines approches de construction de fonctions noyaux s'intéressent au calcul explicite du vecteur ?(G). Par exemple, Jurie et Triggs (2005) proposent de calculer un dictionnaire des prototypes de sommets du graphe (des points d'intérêt) les plus répandus dans la base, puis projettent les sommets sur ce dictionnaire pour former des histogrammes -histogrammes qui seront les vecteurs ?(G) de l'espace Hilbertien. Grauman et Darell (2005) intègrent les contraintes spatiales de manière implicite via une approche pyramidale. L'inconvénient de ces méthodes basées sur des prototypes est leur faible capacité à généraliser.
Une autre technique pour construire des fonctions noyaux sur graphes à laquelle nous nous intéressons plus particulièrement dans cet article, est le calcul implicite des images dans l'espace Hilbertien via la fonction noyau. Le processus de construction repose principalement sur un ensemble de propriétés concernant les fonctions noyaux, comme le fait que la somme ou le produit de deux fonctions noyaux est encore une fonction noyau. Par exemple, dans le cas où la fonction noyau ne porte que sur les sommets du graphe, la fonction suivante est une fonction noyau sous réserve que la fonction K V (v, v ? ) l'est aussi :
v?G,v ? ?G ? Kashima et Tsuboi (2004) ont proposé de comparer deux graphes en comparant tous les parcours possibles de ces deux graphes le long des arêtes. La fonction noyau porte alors sur des ensembles (ou sacs) de chaînes, lesquels intègrent les similarités entre sommets et entre arêtes. Ils ont défini un modèle assez général pour le calcul d'une fonction noyau sur graphe, en considérant l'ensemble des chaînes h = (v 1 , e 1 , v 2 , e 2 , ...) du graphe, puis en calculant la valeur moyenne de la similarité entre les chaînes de G et G ? de même longueur. Si on note |h| la longueur de la chaîne h, i.e. son nombre d'arêtes, cette fonction noyau peut s'exprimer :
avec p(h|G) la probabilité de trouver la chaîne h dans le graphe G. La fonction noyau K C (h, h ? ) mesure la similarité entre deux chaînes :
Les noyaux mineurs qui interviennent dans cette équation sont K V , noyau sur les sommets et K E , noyau sur les arêtes. Pour K V , nous utilisons classiquement un noyau Gaussien, qui retourne des valeurs comprises entre 0 et 1. Le noyau K E permet de prendre en compte les similarité entre arêtes (cf. Suard et al. (2005)), ou plus simplement il peut prendre une valeur fixe.
Dans le contexte de graphes sur des molécules utilisé par Kashima, la similarité entre sommets est binaire, un sommet (un atome) est ou n'est pas le même qu'un autre. Cependant, dans notre contexte où la similarité entre deux sommets est continue, cette fonction a tendance à noyer dans la somme les similarités entre chaînes. Par exemple s'il existe 3 appariements (valeur de similarité a élevée) parmi 10000 appariements possibles (9997 valeurs de similarité b faibles), alors la similarité globale vaudra 3a + 9997b. Les 3 appariements forts ne faisant pas le poids face aux 9997 appariements faibles. Dans le contexte d'application aux images, les probabilités (connues pour des molécules) de l'équation ? sont toutes mises à 1.
L'autre problème relatif au modèle de Kashima concerne sa complexité très élevée en terme de calculs. Afin de palier ces problèmes Suard et al. (2005) a proposé une fonction noyau aux propriétés plus intéressantes pour notre contexte. Il utilise une recherche du maximum, ce qui permet d'une part d'utiliser des algorithmes rapides, mais aussi d'avoir une meilleure discrimination :
Notons que cette fonction ne respecte pas les conditions de Mercer, cependant elles ne sont violées que dans des cas très particuliers, et s'avèrent toujours vérifiées sur les bases de données que nous avons utilisées. Ce type de fonctions a aussi été utilisé par Eichhorn et Chapelle (2004) et Wallraven et al. (2003) qui en tirent les mêmes conclusions.
D'autres fonctions permettent d'augmenter encore plus la discrimination ainsi que la vitesse de calcul, avec un processus de mise en correspondance proche du moteur FReBIR (cf. Philipp-Foliguet et Gony (2006)), qui effectue la recherche du meilleur appariement :
De même, cette fonction n'est pas une fonction noyau au sens strict, cependant en pratique elle respecte aussi les conditions sur les bases expérimentales.
Appariement de graphes
Algorithmes d'optimisation
Une fois définie la mesure de similarité entre deux graphes, le problème de trouver l'appariement qui maximise cette similarité demeure très complexe, surtout si on ne se limite pas aux isomorphismes entre les deux graphes. Il y a souvent un compromis à faire entre solution optimale et temps de calcul. Les algorithmes par colonie de fourmis ou par recherche taboue (cf. Sorlin et al. (2006)) trouvent des solutions optimales mais sont trop longs pour les utilisations "temps réel" que nous envisageons. Une autre approche très répandue utilise des arbres de recherche. Chaque noeud de cet arbre représente un couple de sommets (v, v?) ? V × V ? candidats à l'appariement. On construit une arborescence de proche en proche à partir d'un noeud racine vide et en développant chaque noeud par les couples candidats. Les noeuds candidats sont les couples (v, v?) compatibles avec les noeuds déjà présents dans le chemin menant de la racine au noeud courant. L'avantage de cette représentation par arborescence est que la similarité d'un chemin se calcule au fur et à mesure de la construction du chemin. Dans le cas d'une fonction de similarité qui utilise un max (comme K Suard ou K max ), l'algorithme "branch and bound" permet de trouver une solution optimale sans explorer toutes les solutions possibles. On obtient d'abord la solution la plus prometteuse qui fournit une borne inférieure de la similarité K(G, G?). Puis on construit les autres branches de l'arbre seulement si elles sont susceptibles d'améliorer la valeur de similarité. La solution la plus prometteuse est obtenue en explorant d'abord le chemin construit avec les noeuds dont les valeurs de similarité sont les plus grandes.
Ensembles des chaînes
Puisque nous avons choisi de mesurer la similarité entre graphes par la similarité entre ensembles de chaînes, nous décrivons ici différentes propriétés que peuvent posséder les ensembles de chaînes et qui correspondent à des configurations d'appariements.
On notera pour simplifier h = abc... une chaîne du graphe  
Expérimentations
Nous avons utilisé deux bases d'images pour nos tests : Columbia modifiée et Caltech, où l'objectif est de retrouver les images qui contiennent un objet particulier ou un objet appartenant à une catégorie. La première nous sert à tester et comprendre le comportement des différents noyaux selon la longueur des chaînes employées. La deuxième permet de comparer sur une base réelle différents ensembles de chaînes et aussi de comparer les méthodes par graphe d'adjacence de régions à d'autres méthodes soit globales, soit employant des ensembles de point d'intérêt.
Les images sont segmentées en régions floues (cf. Philipp-Foliguet et Gony (2006)), ce qui permet de segmenter une base entière avec un réglage de paramètres global sur la base. Les régions sont des ensembles flous, qui se chevauchent plus ou moins (cf Fig 3). Le nombre de régions formant l'objet est très variable d'une image à l'autre, un visage par exemple peut être constitué par une seule région floue sur une image ou par 5 comme sur la 
Protocole expérimental
Nous possédons une vérité terrain pour chacune des bases, ce qui nous permet d'une part de simuler la recherche interactive d'un objet ou d'une classe d'objets, et d'autre part d'éva-luer les résultats renvoyés par le système. Muni d'un noyau sur graphe, nous entraînons un classifieur SVM dans le but de classer les images par ordre de pertinence. Les noyaux nous permettent aussi d'utiliser des techniques d'apprentissage actif qui permettent de sélectionner les meilleures images à faire annoter par l'utilisateur (cf. Gosselin et Cord (2006)). Nous éva-luons chaque méthode par la simulation d'un grand nombre de sessions de recherche. Pour chaque session de recherche, une catégorie est choisie. Au sein de cette catégorie, une image choisie au hasard est annotée positivement, ce qui permet d'obtenir un premier classement en se basant uniquement sur la similarité. Puis, les images sélectionnées par la technique d'apprentissage actif sont annotées en fonction de la catégorie choisie au début de la session simulée. Ce premier jeu d'annotation permet d'entraîner le classifieur et ainsi d'obtenir un meilleur classement. Le processus est ensuite répété en suivant le même principe de sélection et de classification. Nous répétons la simulation de session de recherche une centaine de fois pour chaque catégorie. Ainsi, pour chaque catégorie nous pouvons mesurer la qualité moyenne du classement à chaque étape d'annotation à l'aide de la Précision Moyenne, un critère d'évalua-tion souvent utilisé dans les campagnes d'évaluation telle que TRECVID 1 . Puis, dans le but d'obtenir une mesure de la qualité globale du système, nous calculons la valeur moyenne des Précisions Moyennes (MAP, Mean Average Precision) sur toutes les catégories. 
Columbia modifiée
Comportement des noyaux en fonction de la longueur des chemins considérés
La Fig. 2(a) montre les résultats des simulations sur la base Columbia modifiée avec des ensembles de chaînes sans boucle ni cycles pour différentes longueurs de chaînes avec le noyau de Kashima (Eq. 2). Lorsque des chaînes sont de longueur fixe (|h| = k), les performances sont d'autant meilleures que la longueur est petite, ce qui s'explique par le fait que les objets sont représentés par 1 à 3 régions. En utilisant des longueurs de chaînes variables (|h| ? k), les performances ne sont pas améliorées bien que l'on considère davantage de possibilités. Cela peut s'expliquer par le fait que le noyau de Kashima noie les appariements intéressants dans la somme(cf section 2).
La Fig. 2(b) montre les résultats de simulations similaires aux précédentes, mais cette fois ci avec un noyau K max (Eq. 5). On retrouve la même évolution avec des chaînes de longueur fixe (|h| = k), mais les différences sont plus grandes. Par contre, lorsque des chaînes de longueur variables sont utilisées (|h| ? k), les résultats sont bien meilleurs. En effet la similarité entre les graphes avec ce noyau se résume à la similarité d'un seul couple de chaînes appariées. On augmente les chances de trouver ce meilleur appariement si on le recherche dans un ensemble de chaînes de plusieurs longueurs et non limité à une seule longueur. image avec toutes les autres de la base. 
FIG. 2 -Performance (%) sur la base Columbia modifiée avec le noyau
Comparaison des deux noyaux
Si on compare le meilleur résultat obtenu avec le noyau K max (|h| ? 2) et le meilleur résultat avec le noyau K Kashima (|h| = 0) (Fig. 2(c)), on constate qu'au début de l'apprentissage (jusqu'à une vingtaine d'images annotées), le noyau K max conduit à des résultats lé-gèrement meilleurs que le noyau K Kashima . Ensuite les deux courbes sont très proches. Par contre le temps de recherche du meilleur appariement avec l'algorithme de "branch and bound" est beaucoup plus rapide avec le noyau K max qu'avec le noyau K Kashima (cf Fig. 2). Nous concluons que sur cette base où les objets d'intérêt sont représentés par au plus 3 régions, le noyau K max calculé sur toutes les chaînes de longueur inférieure ou égale à 2 est préférable aux autres noyaux. 
Caltech
Nous utilisons la base Caltech avec la vérité terrain de la campagne d'évaluation PASCAL 4 . Cette base contient 5775 images réparties en 5 catégories de 450 à 1370 images. Les graphes issus de la segmentation en régions floues comprennent entre 1 et 23 sommets et le nombre moyen de sommets est d'environ 9 par image (cf. figure 3). Un exemple de session de recherche est donné sur la figure 4. Nous avons mené les expériences suivantes sur cette base :
-Attributs de régions floues, avec le noyau de Kashima (Eq. 2) et des ensembles de chaînes H 0 . Nous n'avons pas considéré de chaînes plus longues pour des raisons de complexité. -Attributs de régions floues, avec le noyau K max (Eq. 5) et différents ensembles de chaînes. -Attributs globaux sous la forme d'un histogramme de couleurs et de textures. Les histogrammes sont calculés sur un dictionnaire adapté à la base suivant un processus de quantification vectorielle décrit dans Gosselin (2005). Un noyau Gaussien avec une distance du ? 2 est utilisé. -Attributs type points d'intérêt avec régions MSER (cf. Matas et al. (2002)) décrites par des SIFT (cf. Lowe (2004) Les résultats de ces expériences sont présentées par catégories dans la table 2, et en fonction du nombre d'annotations dans la figure 5. Tout d'abord, si on s'intéresse au noyau Kashima et max avec le meilleur paramétrage sur la base Columbia modifiée, on peut remarquer que nous avons ici un comportement inverse. Sur la base Caltech, le noyau de Kashima avec H 0 est plus performant quelle que soit la catégorie. Cela peut s'expliquer par le fait que, sur cette base, le contexte joue un rôle important, contrairement à la base Columbia modifiée où il n'existe pas de relation particulière entre un objet et son fond. En effet, étant donné que ce noyau somme toutes les similarités entre les différents sommets, et que l'appariement entre les régions de fond, apporte une information pertinente. Par exemple, les voitures sont souvent en ville, dont les couleurs et les textures sont proches.
Puis, si on s'intéresse aux autres ensembles de chaînes pour le noyau max, nous pouvons remarquer qu'il n'y a pas de différence notable entre les différents ensembles. Cela peut s'expliquer par le fait que, sur la base Caltech, il existe un sous-ensemble de chaînes communes aux chaînes Eulérienne et sans boucle ni cycles qui sont suffisantes pour pouvoir discriminer les images. Ce résultat est intéressant en terme de complexité étant donné que, plus l'ensemble des chaînes considérées est petit, plus les calculs sont rapides.
Enfin, si l'on compare les résultats avec d'autres attributs que les régions floues, nous pouvons constater que les régions floues sont particulièrement intéressantes étant donné qu'elles permettent d'obtenir les meilleurs résultats, sauf la catégorie "background". Néanmoins, dans ce cas a priori défavorable (la catégorie "background" correspond à une recherche de type globale), les régions floues arrivent toutefois à donner de très bon résultats. Ceci tend à montrer la capacité des régions floues à pouvoir s'adapter plus facilement aux différents types de catégories, de la recherche globale à la recherche purement locale, en passant par les cas où le contexte peut jouer un rôle important. 
Conclusion
Nous avons montré que l'utilisation de noyau de graphe dans le cadre de la recherche ité-rative d'objet était possible et efficace. D'après nos expériences, il semble que la description d'une image à l'aide de régions soit plus efficace qu'une description globale ou une description basée sur des points caractéristiques. En effet une primitive région, même si elle ne colle pas parfaitement à la sémantique de l'image porte une information locale plus robuste aux variations.
Les imprécisions de la segmentation et les différences d'aspect d'un objet d'une image à l'autre sont compensées par la mise en correspondance de graphes qui soit la plus générale possible. Les noyaux de graphes calculés à partir de noyaux sur des chaînes dans ces graphes permettent de trouver une solution optimale au problème de l'appariement des graphes avec un algorithme de branch and bound. Cependant, l'emploi des noyaux de Kashima, qui recherchent le couple de chaînes les plus semblables parmi tous les couples de chaînes possibles est incompatible avec l'utilisation en temps réel. Il faut réduire la longueur des chaînes à au plus deux sommets (pour des graphes qui comportent 10 à 20 régions). A longueur de chaînes égale, il vaut mieux prendre un noyau K max , qui, associé à l'algorithme de « branch and bound » trouve le meilleur appariement beaucoup plus rapidement que le noyau de Kashima. Sur les deux bases utilisées il s'est avéré que l'utilisation de petites chaînes suffisait à une bonne reconnaissance des objets. Et pour l'instant nous n'avons pas pu démontrer l'intérêt d'utiliser un ensemble particulier de chaînes.
Le choix de la longueur maximale des chaînes reste le problème principal, il y a un compromis à faire entre temps de calcul et efficacité. Il semble que cette longueur soit liée d'une part au nombre de régions formant l'objet et d'autre part à l'importance du fond dans la reconnaissance de l'objet. Dans le cas où le fond est important pour reconnaître un objet, un noyau basé sur les seules régions suffit. Par contre si l'objet doit être trouvé quel que soit le contexte, la structure du graphe est importante. Nous avons dans cette première étude, fait intervenir que l'adjacence entre les régions. L'emploi d'attributs plus précis de position relative, dans le noyau sur les arêtes, devraient améliorer considérablement la recherche, en la contraignant.

Introduction
La recherche d'information dans les bibliothèques numériques est souvent une tâche ennuyeuse et fastidieuse. Les utilisateurs doivent répéter le processus d'envoyer les requêtes, regarder les résultats et modifier les requêtes jusqu'à ce qu'ils trouvent les informations pertinentes. Une des raisons principales est que les requêtes des utilisateurs sont souvent courtes et donc ambiguës. Par exemple, la même requête «java» peut être formulée par une personne qui s'intéresse au langage de programmation «java», et par une autre qui veut chercher des informations concernant une île en Indonésie. Cependant les moteurs de recherche renvoient le même résultat pour ces deux personnes. Même avec une plus longue requête comme «langage programmation java» ; nous ne savons pas quels types de document cet utilisateur veut chercher. Si c'est un(e) programmeur(e), peut-être il/elle s'intéresse aux documents techniques sur le langage Java, si c'est un(e) enseignant(e), peut-être il/elle s'intéresse aux tutoriels de Java pour ses cours.
Le problème que nous avons mentionné peut être résolu en utilisant des techniques de personnalisation avec des profils utilisateurs. D'une manière générale, nous pouvons définir un profil d'utilisateur comme un ensemble structuré d'informations qui décrit les intérêts et/ou les préférences de cet utilisateur.
Le travail de Amato et Straccia (1999) est parmi les premiers travaux consacrés à définir un modèle de représentation de profil utilisateur dans les bibliothèques numériques, leur modèle est un modèle multidimensionnel dans lequel le profil utilisateur se compose de plusieurs catégories (ou dimensions) différentes : catégorie de données personnelles, catégorie de données de la source, catégorie de données de livraison, catégorie de données de comportement et catégorie de données de sécurité. Dans le système CiteSeer (Bollacker et al. (1999)), un profil hétérogène a été utilisé pour représenter des intérêts des utilisateurs. Lorsqu'un nouvel article est disponible, CiteSeer va décider si cet article sera recommandé à l'utilisateur ou non. CiteSeer utilise deux méthodes pour déterminer si l'article est intéressant pour l'utilisateur : i) jeu de contrainte (constraint matching) et ii) similarité de propriétés (feature relatedness).
Notre travail se concentre sur la recherche d'information personnalisée dans les bibliothèques numériques contenant des articles scientifiques. Cependant, tandis que la plupart des systèmes personnalisés utilisent des approches basées sur le contenu textuel pour construire les profils et représenter les documents de façon à calculer la similarité entre eux ; nous utilisons aussi des approches basées sur les citations des articles et des approches hybrides (contenu textuel et citations) pour ce but.
Méthode des co-citations sur le Web
Dans cette section nous présentons la méthode des co-citations pour trouver la similarité entre les articles scientifiques, méthode proposée par Small (1973). Dans cette méthode, la similarité entre deux articles est basée sur leur nombre de co-citations : c'est-à-dire le nombre de fois où ils sont cités ensemble par un autre article.
Cette méthode a été utilisée depuis longtemps. Cependant, elle a ses limites. Pour avoir les information de citation (nombre des co-citations), il faut avoir accès au graphe de citation de la collection actuelle ; ou il faut utiliser une base de données de citations 1 . Ces deux sources sont souvent limitées par la couverture soit de la bibliothèque numérique, soit de la base de données de citations par rapport aux publications qu'elles ont collectées. Plusieurs travaux ont montré que si cette couverture n'est pas suffisante, l'efficacité de la méthode des co-citations sera diminuée (par exemple Couto et al. (2006)). C'est pourquoi nous avons proposé une mé-thode (Van et Beigbeder (2007)) qui peut surmonter cette limite. Cette méthode est appelée la méthode des co-citations sur le Web. Dans cette méthode, nous calculons la similarité de co-citations entre deux articles scientifiques par le nombre de fois où ils sont «co-cités» sur le Web en utilisant le moteur de recherche Google.
Pour trouver la fréquence à laquelle un article est «cité» par Google, nous envoyons le titre de cet article (recherche d'une expression exacte en utilisant des guillemets) à Google et notons le nombre de documents retournés. Similairement, pour trouver le nombre de fois où deux article sont «co-cités», nous envoyons les titres de ces deux articles à Google et notons le nombre de documents retournés. Dans la méthode des co-citations, la similarité entre deux Nos expérimentations sont des simulations de la recherche d'information personnalisée en utilisant des profils utilisateurs. Dans ce cas, les topics représentent les besoins d'information de personnes différentes. Pour chaque topic, nous sélectionnons manuellement quelques documents pertinents (5 en moyen) pour former un «pseudo profil utilisateur» de ce topic. Les articles qui sont inclus dans les profils seront exclus de la collection pour éviter l'influence sur les résultats finaux. Chaque fois qu'une requête est envoyée au moteur de recherche zettair 4 (le modèle par défaut utilisé dans zettair est le modèle Dirichlet-smoothed, Pehcevski et al. (2005)), les 300 premiers documents sont sélectionnés, puis on calcule la similarité entre chaque document dans cette liste avec le profil utilisateur. La similarité entre un document d et un profil utilisateur p est calculée par : 
Enfin, le score original calculé par zettair sera combiné avec la similarité document-profil pour donner le score final d'un document. Les documents dans cette liste sont re-triés en utilisant ce nouveau score et puis présentés à l'utilisateur.
Les nouveaux travaux
Dans cette partie nous présentons nos nouveaux travaux. Dans les travaux antérieurs, pour chaque topic nous avons sélectionné manuellement quelques documents pertinents pour former un «profil utilisateur» de ce topic. Maintenant nous utilisons une autre méthode d'éva-luation qui est basée sur le principe de la méthode de validation croisée à k blocs (k-fold cross-validation en anglais, Kohavi (1995)). Selon cette approche, pour chaque topic, nous partitionnons aléatoirement l'ensemble des documents pertinents en k blocs (dans nos expéri-mentations, k = 5). Les documents dans un bloc sont utilisés comme documents de test et les documents dans les autres k -1 blocs sont utilisés comme le «profil utilisateur» de ce topic. L'expérimentation est répétée k fois, chaque fois avec un bloc différent contenant des documents de test. Avec cette approche, chaque document pertinent sera utilisé comme document de test 1 fois et dans le «profil» k -1 fois. Le résultat final sera une valeur moyenne de k résul-tats correspondant avec k blocs. La fiabilité des résultats est augmentée avec cette méthode de validation.
De plus, dans les expérimentations précédentes, la similarité document-profil est calculée avec plusieurs approches basées sur les citations (co-citations sur le Web, co-citations avec Web of Science, couplage bibliographique). Dans les nouvelles expérimentations, on utilise seulement l'approche des co-citations sur le Web qui a donné la meilleure performance dans les expérimentations précédentes comme approche basée sur les citations. Par ailleurs, on ajoute l'approche basée sur le contenu textuel et l'approche hybride citation-texte. Dans l'approche basée sur le contenu textuel, la similarité document-profil (cf. formule 2) est calculée par le modèle vectoriel (Pehcevski et al. (2005)) en utilisant le logiciel zettair. Le score final d'un document sera une combinaison entre deux ou trois des scores suivantes : i) score original calculé par zettair (score_zettair) ii) similarité document-profil calculée par la méthode des co-citations sur le Web (sim_cocitations) et iii) similarité document-profil basée sur le contenu textuel (sim_texte). Les scores sont normalisés (divisés par le maximum des valeurs correspondantes) pour avoir des valeurs dans l'intervalle de 0 à 1. Actuellement, nous considérons les combinaisons suivantes : i) score_zettair avec sim_cocitations ii) score_zettair avec sim_texte et iii) ces trois scores. Nous avons utilisé deux formule de combinaison : une formule linéaire et une formule produit. Dans la formule linéaire, nous avons essayé différents coefficients pour trouver la meilleure combinaison possible.
Résultats et discussions
Pour évaluer la performance des différentes méthodes, nous utilisons la métrique précision à n document (avec n = 5, 10,15,20,30). Le logiciel trec_eval 5 est utilisé pour l'évaluation. La précision à n est la fraction des documents pertinents parmi les n premiers document. Puisque nous utilisons l'approche de validation à k blocs, nous obtenons k valeurs de précision que nous moyennons :
Les résultats sont montrés dans le tableau 1. La deuxième colonne représente le résultat original de zettair. La troisième colonne correspond à la méthode des co-citations sur le Web (score_zettair combiné avec sim_cocitations). La quatrième colonne représente le résultat avec la méthode basée seulement sur le contenu textuel des document (score_zettair combiné avec sim_texte). La cinquième colonne représente le résultat de la méthode hybride : score_zettair combiné avec sim_cocitations et sim_texte. Dans chaque méthode, p signifie d'autres méthodes de citations avec les méthodes actuelles pour pouvoir gagner une meilleure performance. De plus, sachant qu'il y a des points similaires entre citations des articles scientifiques et hyperliens des pages Web, nous avons l'intention de faire des expérimentations similaires sur une collection des pages Web pour pouvoir comparer les performances de ces méthodes quand on les utilise dans l'environnement Web.

Introduction
Durant les dernières décennies, l'utilisation de réseaux de capteurs a été largement déve-loppée pour mesurer et observer l'évolution de systèmes complexes à forte dynamique. Les applications sont par exemple le trafic routier, le transport d'énergie, les processus d'entreprise et la météorologie. Dégager des liens de corrélations dans un tel réseau à travers le temps permet, par exemple, d'établir des prévisions probabilistes à court ou moyen terme. Dans ce qui suit, on suppose que les capteurs, effectuant des mesures sur le trafic routier urbain, sont fixes et géoréférencés. Un graphe de connexion logique représente les échanges ou les causalités directes possibles entre ces différents lieux géographiques. Le graphe est supposé connu. A l'aide d'un outil d'estimation efficace, on peut prédire le comportement usuel du trafic devant chaque capteur. Cependant, lorsque la circulation est atypique, au sens de l'occurrence, la qualité des prévisions s'en retrouve considérablement affectée. Nous proposons d'identifier des motifs spatio-temporels de propagation de ces cas atypiques ayant pour objectif d'aider à prévoir les conséquences d'un évènement inhabituel sur l'intégralité du réseau. Les motifs se réfèrent généralement à des structures répétitives sur le graphe sous-jacent dans l'espace et le temps. Des motifs décrivant des changements dans l'espace et le temps sont qualifiés de motifs spatio-temporels (spatiotemporal patterns). La notion de motifs spatio-temporels apparaît dans différents secteurs scientifiques tels que les géosciences (Knopoff et al. (2001)), la météorologie (Tourre et al. (1999), Imfeld (2000)) ou l'écologie (Weigand et al. (1998)). Généralement, on utilise des techniques d'analyse et de fouilles de données spatio-temporelles discrètes pour identifier ces motifs dans de grands ensembles (Tsoukatos et Gunopulos (2001), Bittner (2001)). Dans cet article, nous proposons dans un premier temps un outil permettant de détecter les comportements atypiques. Nous introduisons ensuite une méthode fondée sur la combinaison de l'information mutuelle (Shannon et Weaver (1963)) et de l'algorithme Isomap (Tenenbaum et Langford (2000)) calculant une première version des motifs de propagation que nous tentons d'améliorer par la suite. Les tests expérimentaux sont effectués sur des données réelles de trafic routier intra-urbain. Ces données nous ont été fournie par l'INRETS dans le cadre du projet CADDY (http: //norma.ecp.fr/wikimas/Caddy) de l'ACI Masse de données 2003. Dans Joliveau et De Vuyst (2007), nous avons proposé une adaptation de la méthode SpaceTime Principal Component Analysis (STPCA) à un ensemble de données incomplètes calculant des estimations de séries temporelles définies pour chaque instant de mesure. L'utilisation de cette méthode sur nos données de trafic intra-urbain nous a permis de dégager un ensemble complet de données de débit et de taux d'occupation provenant d'un réseau de capteur. Le débit moyen de véhicule (nombre de véhicules/instant) correspond à la quantité de véhicules étant passés dans la zone d'activité du capteur lors du dernier intervalle de mesure. Le taux d'occupation, exprimé en pourcentage symbolise quant à lui la densité de la circulation. Plus le taux d'occupation est élevé, plus la circulation est dense. Une première difficulté est de combiner ces deux informations afin de posséder une variable ayant un sens pour l'état du trafic. La loi fondamentale du trafic provenant de la théorie du transport indique la relation entre le flot et la densité des véhicules sur une route. A partir du diagramme représentatif de cette loi, nous proposons une nouvelle variable 
Détection de cas atypiques
Définition d'une variable d'état de circulation continue
Estimation du comportement moyen et détection de situations inhabituelles
La méthode STPCA définie dans  offre un moyen de résumer efficacement des séries temporelles tout en conservant la majeure partie de l'information au  5  10  15  20  25  0  200  400  600  800  1000  1200  1400  1600  1800  2000   0  5  10  15  20   figure. sens de l'énergie. L'énergie est obtenue par la trace des matrices de corrélation définie par :  de l'énergie). Pour détecter un comportement atypique, il suffit de comparer la valeur réelle à son estimée par STPCA. Si l'écart entre ces deux valeurs est élevé, cela signifie que l'activité actuelle au capteur n'est pas représentative de la situation usuelle. On détecte alors un comportement atypique au sens de l'occurrence. Notre raisonnement est illustré sur la figure 3. On remarque que, les deux courbes étant très ). Or, les mesures réelles relèvent une activité plus fluide que d'habitude. Dans ce cas, le comportement atypique détecté correspond à une fluidité inhabituelle du trafic. Dans le cadre où on cherche à établir des prévisions sur le trafic à court ou moyen terme, l'utilisation de la STPCA sur la variable ¡ est un outil très pratique. Si l'écart entre la valeur mesurée par un capteur et son approximation est faible, on se réfère à la série estimée par l'algorithme STPCA pour établir notre prévision. Dans le cas contraire, on détecte un comportement atypique. On s'appuiera alors sur des motifs spatio-temporels de propagation pour prédire la circulation.
Recherche de motifs spatio-temporels
Les motifs intraday que nous cherchons à identifier ont pour but de nous aider à faire des prévisions sur le trafic. Ceux-ci se focalisent plus particulièrement sur l'anticipation de la propagation d'une situation atypique à travers le réseau. La propagation d'une situation atypique sur un réseau est une notion de voisinage locale en espace et en temps indiquant sur quels points du réseau on observe un comportement atypique suite à la réalisation d'un évènement inhabituel à l'instant précédent. Deux outils principaux sont utilisés pour identifier les motifs spatio-temporels de propagation : l'information mutuelle (Shannon et Weaver (1963)) et l'algorithme Isomap (Tenenbaum et Langford (2000)).
L'information mutuelle
L'information mutuelle est tirée de la théorie des probabilités et de la théorie de l'information. Cette quantité mesure la dépendance statistique entre deux variables. L'information mutuelle mesurant la quantité d'information apportée en moyenne par une réalisation d'un évè-nement X sur les probabilités de réalisation d'un évènement Y , et, en considérant qu'une distribution de probabilité représente notre connaissance d'un phénomène aléatoire, on peut mesurer l'absence d'information en utilisant l'entropie de Shannon (Shannon et Weaver (1963)) de cette distribution. Ainsi, l'information mutuelle est donnée par : 
Isomap
Isomap est une méthode introduite par Tenenbaum et Langford (2000) dont l'objectif est d'identifier la structure cachée dans des observations multivariées de grandes dimensions. Le principe de cette méthode est de projeter les points d'un ensemble de données dans un espace de plus faible dimension. L'avantage d'Isomap est que contrairement aux méthodes classiques telles que l'analyse en composantes principales (ACP) ou l'échelonnement multidimensionnel (multidimensional scaling -MDS), l'algorithme est capable de découvrir des relations non linéraires régissant certaines observations complexes. Une fois le plongement (ou embedding) calculé, la distance entre deux points sur celui-ci est représentative de la similitude globale entre les évènements mesurés par ces points. Isomap se décompose en trois étapes définies de manière détaillée dans le tableau 1. Lors de la première étape, on cherche à déterminer pour chaque élément les points qui lui sont le plus proche en fonction d'une distance 
Calcul des plus courts chemins
Initialiser
. La matrice des valeurs finales  
Construction des motifs spatio-temporels
Calcul de motifs d'origine
Afin de construire une première version de motifs spatio-temporels de propagation de comportement atypique dans un réseau, nous appliquons l'algorithme Isomap en utilisant l'information mutuelle comme mesure de distance  et on relie ces deux couples capteur-instant par un arc. De cette manière, on obtient un graphe du même type que sur la figure 4 représentant les premiers motifs de propagation de cas atypiques. La notion de capteur le plus proche peut être définie par un rayon de distance minimale ? ou par l'algorithme des K plus proches voisins. Dans nos expériences, nous avons combiné les deux approches en choisissant les K plus proches voisins dans un rayon de ? .
Amélioration des motifs
Suite à la combinaison d'Isomap et de l'information mutuelle, on dispose de premiers motifs spatio-temporels de propagation pouvant être représentés par une chaîne reliant les capteurs d'un réseau entre les périodes de mesure. Nous proposons de pondérer les arcs de cette chaîne par la tendance de propagation du caractère atypique. Pour cela, on simule le déroulement du temps en détectant les cas atypiques par le procédé expliqué dans la section 2 et on calcule les probabilités  . La colonne "voisins directs" correspond aux résultats obtenus à l'aide de la première version des motifs tandis que la colonne "voisins indirect" se réfère à la méthode avec les améliorations. On remarque que la solution basée sur l'utilisation exclusive des voisins directs, déjà de bonne qualité, permet de prévoir jusqu'à
Expériences sur l'ensemble des données
des situations atypiques avec un taux de faux positifs acceptable. Bien que notre but soit de prévoir la propagation de cas atypiques au sens de l'occurrence, il nous faut distinguer les éléments atypiques reproductibles des éléments atypiques trop ponctuels. Le réglage du seuil
permet de gérer ce compromis. De son coté, l'ajout des voisins indirects augmente la précision
Voisins directs
Voisins Indirects ).
Voisins directs Voisins Indirects
). Il faut cependant le paramétrer de manière à ne pas trop augmenter le nombre de faux positifs engendrés. 
Apprentissage sur un échantillon
).
Le but de notre travail étant de déterminer des motifs spatio-temporels afin de réaliser des prévisions en temps réel, nous aimerions estimer les facultés d'apprentissage de notre méthode. Pour cela, nous sélectionnons aléatoirement un échantillon de nos données et nous procédons aux différentes étapes de notre méthode à partir de cet échantillon (détermination des modes principaux, calcul de l'information mutuelle, détermination des premiers motifs avec Isomap, amélioration des motifs). Finalement, on teste les capacités de prédiction des motifs extraits de l'échantillon sur l'intégralité des données. Le tableau 4 illustre les résultats obtenus en utilisant un échantillon de V £ jours et en fixant les valeurs des paramètres à représente la topologie du réseau, les capteurs étant représentés par des carrés, les routes par des traits gris clairs et les cours d'eau par des traits fins noirs. La partie droite illustre un morceau de la chaîne de motifs correspondant aux instants entre
. L'importance de la tendance de propagation est symbolisée par l'épaisseur des flèches.
Prévisions à moyen terme
En modifiant l'échantillonnage temporel, on peut appliquer notre méthode sur des épisodes plus longs. Le tableau 5 illustre les résultats de la méthode à moyen terme (sur des épisodes de faux positifs similaire aux prévisions à court terme.
Conclusion
Dans ce papier, nous avons proposé une méthode de génération de motifs spatio-temporels de situations atypiques. Cette méthode s'applique principalement à des données issues d'un réseau de capteurs fixes géoréférencés. Nous utilisons l'algorithme STPCA comme outil de prévision du comportement usuel de séries temporelles issues d'un réseau de capteurs, nous permettant également de détecter les situations atypiques. Nous avons également introduit une méthode d'identification de motifs spatio-temporels de propagation de situations atypiques fondée sur la combinaison d'Isomap et de l'information mutuelle. Ces motifs ont pour fonction

Introduction
Le problème de la classification de données est identifié comme une des problématiques majeures en extraction des connaissances à partir de données. Depuis des décennies, de nombreux sous-problèmes ont été identifiés, comme par exemple la sélection des données ou des variables, la variété des espaces de représentation (numérique, symbolique, etc), l'incrémen-talité, la nécessité de découvrir des concepts, ou d'obtenir une hiérarchie, etc. La popularité, la complexité et toutes ces variantes du problème de la classification de données, (Jain et al. (1999)), ont donné naissance à une multitude de méthodes de résolution. Ces méthodes peuvent faire appel à des principes heuristiques ou encore mathématiques.
Les méthodes qui nous intéressent dans ce travail, sont celles qui permettent de faire de la classification non supervisée de données en utilisant les cartes topologiques (appelées aussi SOM :Self-organizing Map). Celles-ci sont souvent utilisées parce qu'elles sont considérées à la fois comme outils de visualisation et de partitionnement non supervisé de différents types de données (quantitatives et qualitatives). Elles permettent de projeter les données sur des espaces discrets qui sont généralement en deux dimensions. Le modèle de base, proposé par Kohonen (Kohonen (2001)), est uniquement dédié aux données numériques. Des extensions et des reformulations du modèle de Kohonen ont été proposées dans la littérature, (Bishop et al. (1998); Pour l'apprentissage des cartes topologiques, les critères de qualité sont plus difficiles à dé-finir ; ils s'articulent autour de l'interprétation des regroupements ou des partitions obtenues. Par conséquent un premier problème se pose : celui de la segmentation (partitionnement) de la carte. On retrouve dans la littérature plusieurs méthodes ou propositions de segmentation de la carte qui utilisent des critères de similarité standard qui ne tiennent pas compte du voisinage introduit par la carte, (Vesanto et Alhoniemi (2000)). Elles se résument, souvent, à l'utilisation d'un algorithme de regroupement (classification hiérarchique ou les K-moyennes) combiné à un indice de qualité pour déterminer la partition idéale. Le second problème qui nous intéresse dans cet article est celui du choix de l'algorithme de segmentation de la carte. Ainsi, nous avons introduit une nouvelle classification hiérarchique que l'on va appliquer sur les référents (représentants) de la carte. Cette nouvelle méthode nommée AntTree introduite par (Azzag et al.) s'inspire du comportement d'auto-assemblage observé chez une population de fourmis réelles et leurs capacités à s'accrocher entre elles pour construire des structures vivantes.
La suite de notre article est organisée comme suit : dans la section 2, nous présentons les principes généraux des cartes SOM avec la nouvelle mesure proposée, ainsi que la nouvelle méthode de classification hiérarchique utilisée pour la segmentation de la carte topologique. La section 3, quant à elle, est consacrée aux résultats et à l'étude comparative réalisée sur des bases de données numériques. La dernière section rassemble les conclusions faites au cours des expérimentations et présente des perspectives.
Segmentation topologique et hiérarchique
Les cartes auto-organisatrices présentées par Kohonen ont été utilisées pour la classification et la visualisation des bases de données multidimensionnelles. Une grande variété d'algorithmes des cartes topologiques est dérivée du premier modèle original proposé par Kohonen. Ces modèles sont différents les uns des autres, mais partagent la même idée de présenter les données de grande dimension en une simple relation géométrique sur une topologie réduite.
Dans cette section, nous décrivons la version originale des cartes auto-organisatrices. Ce modèle consiste en la recherche d'une classification non supervisée d'une base d'apprentissage
Ce modèle classique se présente sous forme d'une carte possédant un ordre topologique de N c cellules. Les cellules sont réparties aux noeuds d'un maillage. La prise en compte dans la carte C de la notion de proximité impose de définir une relation de voisinage topologique. Ainsi, la topologie de la carte est définie à l'aide d'un graphe non orienté et la distance ?(c, r) entre deux cellules c et r étant la longueur du chemin le plus court qui sépare les deux cellules c et r. 
Où ? affecte chaque observation z à une cellule unique de la carte C. Dans cette expression ||z ? w r || 2 représente le carré de la distance euclidienne.
A la fin de l'apprentissage, la carte auto-organisatrice détermine une partition des données en p sous ensembles. Cette partition et les sous-ensembles associés seront notés par P = {P 1 , . . . , P c , . . . , P p }. A chaque sous ensemble P c on associe un vecteur référent w c ? R d qui sera le représentant ou la "moyenne" de l'ensemble des observations de P c .
Souvent l'utilisation des cartes topologiques est suivie par une segmentation des cartes ou un regroupement des référents. Cette tâche, est réalisé à l'aide d'algorithmes de partitionnement tel que K-moyennes, ou la classification ascendante hiérarchique CAH (Jain et Dubes (1988)). Le choix des deux sous-ensembles qui vont être regroupés est réalisé à l'aide d'une mesure de similitude définie entre deux sous-ensembles. Différents critères de similitude sont proposés dans la littérature, (Jain et Dubes (1988); Ambroise et al. (1998)). Souvent ces critères ne tiennent pas compte de la topologie ou de l'auto-organisation des référents obtenue avec la carte topologique. La mesure de similitude la plus connue est celle du critère de Ward définie comme suit :
tel que n c et n r indiquent le nombre d'observations affectées pour le sous-ensemble P c et P r .
En considère deux partitions : P t?1 comme la partition avant regroupement des deux sousensembles P c et P r associés aux deux référents c et r, et P t la partition obtenue en regroupant les sous ensembles P c et P r . On peut montrer que la différence entre l'inertie des deux partitions est égale au critère de regroupement de Ward (2). Ainsi à chaque étape de la classification on calcule une matrice de similarité associée à la nouvelle partition. Par conséquent, à chaque étape de l'algorithme, on choisit une nouvelle partition qui limite l'augmentation de l'inertie intra-classe. Notons que cette propriété ne garantit pas l'optimisation globale du critère. L'algorithme peut être décrit en 5 étapes :
1. Initialiser la matrice de similarité avec la partition obtenue avec la carte.
2. Trouver les deux sous ensembles les plus proches selon le critère de Ward.
3. Regrouper les deux sous ensembles P c et P r en un seul sous ensemble.
4. Mettre à jour la matrice de similarité de la nouvelle partition.
Répéter 2
Nous rappelons que la classification hiérarchique ou tout autre méthode de segmentation des cartes topologiques sont couplées à un indice de qualité qui permet de choisir la taille de la partition optimale. Afin d'optimiser l'algorithme de segmentation de la carte, nous proposons dans ce papier deux modifications. La première consiste à utiliser un algorithme de classification hiérarchique qui supprime l'étape 4 et fournit automatiquement la taille de la partition "idéale". Ceci se résume à utiliser un algorithme de classification hiérarchique qui utilise une seule et unique matrice de similarité, qui est celle de la partition à l'instant t = 0 (P 0 ). Cet algorithme sera détaillé par la suite dans la section 2.1. Notre deuxième proposition consiste à modifier la mesure de similarité de regroupement, afin de prendre en compte le voisinage de la topologie fournie par la carte.
Le critère de Ward mesure la perte d'inertie après chaque fusion de deux sous ensembles, il est donc nécessaire de considérer la modification de la topologie de la carte après fusion en pondérant l'indice de Ward par un paramètre quantifiant ce changement. Nous proposons de quantifier le changement topologique par une pondération du critère de Ward avec une mesure qui prend en compte la nouvelle structure de la carte, cette dernière est définie comme suit :
Cette pondération permet donc de quantifier ce changement topologique de la carte. Afin de de prendre en compte la proximité des référents sur la carte, nous proposons de soustraire une quantité à cette mesure de façon à diminuer la perte d'inertie mesurée par le critère de Ward, selon la proximité des sous-ensembles sur la carte topologique. Finalement, la nouvelle mesure devient :
Cette mesure heuristique est constituée de deux termes. Le premier terme correspond à la perte d'inertie des observations après fusion des deux sous ensembles P c et P r . le deuxième terme permet de rapprocher les sous-ensembles correspondants à deux référents voisins sur la carte, afin de conserver l'ordre topologique entre les différentes partitions. En effet, si c et r sont voisins sur la carte, la valeur de ?(c, r) sera alors basse et dans ce cas celle de K(?(c, r)) sera élevé ; le second terme a donc comme effet de réduire davantage le premier terme. Il est évident que pour un voisinage nul, notre mesure se réduit à calculer le critère de Ward. Il est donc possible de dire que notre mesure permet d'obtenir une solution régularisée du critère de Ward : la régularisation étant obtenue grâce à la contrainte d'ordre topologique introduit dans notre proposition de mesure.
Finalement l'utilisation de notre mesure permet de définir une matrice de similarité qui tient compte à la fois de la perte d'inertie et de la topologie de la carte. Pour traiter cette matrice nous allons présenter dans la section suivante l'algorithme de classification hiérarchique basé sur les fourmis artificielles.
Classification hiérarchique
Pour segmenter la carte nous avons utilisé une approche biomimétique inspirée du comportement d'auto-assemblage observé chez les fourmis réelles. Ces dernières construisent des structures vivantes en se connectant progressivement entre elles, (Anderson et al. (2002)).
Le modèle développé utilise des règles comportementales afin de construire des heuristiques pour la classification non supervisée hiérarchique. Dans notre modèle chaque fourmi artificielle représente une donnée z à classer. Ces fourmis artificielles vont construire de manière similaire un arbre en appliquant certaines règles où les déplacements et les assemblages des données sur cet arbre dépendent de leurs similarités.
Le principe d'AntTree est le suivant : chaque donnée (fourmi) à classer z i , i ? [1, N ] (N est le nombre de données initiales) représente un noeud de l'arbre à assembler. 
FIG. 1 -Construction de l'arbre par des fourmis : les fourmis qui sont en déplacement sont représen-tées en gris et les fourmis connectées en blanc.
Initialement toutes les fourmis artificielles sont positionnées sur un support noté f 0 (voir figure 1). A chaque itération, une donnée z i est choisie dans la liste des données triée au départ. On notera par la suite par f i chaque fourmi représentant une donnée z i à classer dans l'arbre. Cette fourmi va chercher alors à se connecter sur sa position courante, sur le support (f 0 ) ou sur une autre fourmi (donnée) déjà connectée (noté f pos ). Cette opération ne peut aboutir que dans le cas où elle est suffisamment dissimilaire à f + (la fourmi connectée au support f 0 ou à f pos et dont la donnée est la plus similaire à z i ). Dans le cas contraire, la fourmi f i associé à la donnée z i se déplacera de manière déterministe dans l'arbre suivant le chemin le plus similaire indiqué par f + . Le seuil permettant de prendre ces décisions ne va dépendre que du voisinage local. Pour étiqueter les données nous allons ensuite considérer que chaque sous arbre va représenter une classe trouvée. Dans (Azzag et al.) l'auteur détaille de manière plus complète les règles comportementales définissant les différents algorithmes de cette approche.
Notons qu'AntTree a l'avantage d'avoir une complexité proche du n log(n). Une étude dé-taillée a été réalisé dans (Azzag et al.), elle confirme que par rapport à d'autres algorithmes en n 2 les temps nécessaires par AntTree peuvent être jusqu'à 1000 fois inférieur à ceux de la CAH sur de grandes bases de données et ceci pour une qualité égale.
Ces temps vont encore être réduits puisque AntTree s'applique sur l'ensemble des référents fournit par la carte topologique, W = {w 1 , ..., w p }. Ceci réduit considérablement la complexité de la segmentation de la carte. Ainsi la structure d'arbre recherchée est celle qui représente le mieux l'ensemble des référent W (avec la nouvelle mesure de similarité (3)). Chaque noeud parent de l'arbre est plus représentatif du noeud fils. Ainsi l'algorithme, AntTree-SOM-Neigh-W, résumant les étapes élémentaires pour la segmentation de la carte topologique (SOM) peut être présenté comme suit :
-Entrée : W = {w 1 , ..., w Nc }, l'ensemble des référents constituant la carte topologique à la fin de l'apprentissage.
-Calcul de la matrice de similarité en utilisant la formule (3) -Construction de l'arbre avec l'algorithme AntTree. -Sortie : structure des référents sous forme d'arbre.
L'arbre fournit une partition de la carte topologique P = {P 1 , ..., P s } où la valeur de l'indice s représente le nombre de sous arbres connectés au support fournit par AntTree. Ainsi, dans le même processus nous proposons de segmenter la carte et de fournir le nombre de sous ensembles constituant la partition sans aucun indice de qualité.
Validation
Afin de pouvoir évaluer la qualité de la classification obtenue, nous avons utilisé des bases de données comportant un nombre variable d'observations (Blake et Merz (1998)). Nous avons également testé notre approche sur des données artificielles (Azzag et al.), engendrées par des lois gaussiennes avec des difficultés diverses (recouvrement des classes, variables non pertinentes, etc.) ainsi que des données réelles. Le tableau 1 présente pour chaque base, le nombre de classes réelles (Cl R ), la dimension de l'espace des données (d) et le nombre de données total (N ). La figure 2 montre quelques exemples avec des difficultés variables utilisés pour tester notre modèle.
Nous avons comparé notre modèle avec la segmentation de la carte utilisant l'algorithme AntTree combiné à l'indice de Ward (formule2), sans aucune information du voisinage (modèle appelé SOM-AntTree-W). Dans ces expérimentations, la comparaison des différents résultats est mesurée à l'aide de trois critères externes. On peut utiliser ces indices lorsque la segmentation souhaitée est connue, en particulier sur nos jeux de données. Il s'agit de la comparaison entre la segmentation proposée et une segmentation souhaitée. Ainsi nous avons utilisé, le taux de bonne classification (appelé aussi pureté) en utilisant l'étiquette connue de chaque donnée ; l'indice de Rand, qui calcule le pourcentage du nombre de couples d'observations ayant la même classe et se retrouvant dans le même sous ensemble après segmentation de la carte, et le troisième indice est celui de Jaccard qui est similaire à l'indice Rand sans prendre en considération les couples d'observations correctement classées dans des sous ensembles différents, (Saporta (2006)).
Atom (2) TwoDiamonds (2) Lsun (3) Demicercle (2) 
FIG. 2 -Quelques exemples de jeux de données (Atom (2), TwoDiamon (2), Lsun (3), demi-cercle (2)).
Le numéro qui suit le nom de la base indique le nombre de classe  Le tableau 2 indique les performances atteintes avec notre modèle AntTree-SOM-Neigh-W en comparaison avec le modèle utilisant simplement le critère de Ward. La figure 2 montre la variation des trois indices utilisés pour comparer ces deux segmentations de la carte.
Sur les graphiques de la figure 2, nous pouvons observer que les courbes obtenues par notre modèle sont de plus grandes amplitudes sur l'indice de Rand ainsi que l'indice de Jaccard. Ceci confirme l'avantage de considérer le voisinage dans AntTree-SOM-Neigh-W. Cependant notre modèle obtient de moins bon résultats sur la base Anneaux (le pic représenté sur les deux premières courbes). Ceci s'explique par le fait que notre modèle retrouve beaucoup plus de classes sur cette base. En effet AntTree-SOM-Neigh-W utilise l'information du voisinage et cette dernière lui permet de détecter des sous-ensembles de manière plus précise que le modèle classique.
Nous observons également sur le tableau 2 un résultat global qui indique que les puretés sont améliorées à chaque fois que l'on introduit le voisinage dans la segmentation de la carte. Par exemple, avec la base pima de 67% à 72.4%, avec le même nombre de classes trouvées. Avec la base Hepta on obtient des résultats identiques de l'ordre de 43.4%. Pour la base Anneaux on constate une baisse de pureté, on passe de 97.8% à 81.5%.
Finalement, on peut constater, globalement, une claire amélioration de la pureté lorsqu'on utilise la nouvelle mesure de regroupement proposée (SOM-AntTree-Neigh-W). La prise en compte de la topologie de la carte améliore nettement les résultats de la segmentation. Nous rappelons ici, qu'il existe d'autres algorithmes de segmentation de la carte n'utilisant que la
85.87 (5) 99.9 (7) Anneaux (2) 97.8 (6) 81.5 (5) Demi-cercle (2) 58.833 (2) 72.67 (4) Engytime (2) 74.14 (5) 88.04 (7) Glass (7) 38.32 (5) 59.81 (6) GolfBall (1) 100 (3) 100 (4) Hepta (7) 43.4 (4) 43.4 (4) Lsun (3) 55 (3) 93 (5) Pima (2) 67 (5) 72.4 (5) Target (6) 83.25 (5) 94.42 (6) Tetra (4) 62.5 (3) 81.75 (5) Twodiamonds (2) 100 (4) 100 (5) WingNut (2) 95.67 (3) 87.11 (5) ART1 (4) 50.5 (4) 84.75 (4) ART2 (2) 94.9 (4) 97.7 (4) ART4 (2) 100 (3) 100 (5)   Ambroise et al. (1998)). Avec notre modèle couplé à la nouvelle mesure 3 aucun indice n'est nécessaire pour obtenir la partition optimale. La figure 4 montre sur deux jeux de données le résultats de la segmentation de la carte. La figure à gauche indique comment la carte apprend la topologie du nuage suivi à droite de la structure de l'arbre fourni par l'algorithme de classification hiérarchique AntTree. Chaque classe est représentée par un sous arbre connecté au support. Aussi, chaque noeud de la carte et de l'arbre représente un référent qui est associé à un sous-ensemble de données.
Conclusions et perspectives
Dans ce travail nous avons développé une nouvelle mesure de similarité pour la segmentation des cartes auto-organisatrices afin de prendre en compte le voisinage entre les référents d'une carte. Nous avons également introduit une nouvelle approche de segmentation utilisant un algorithme de classification hiérarchique basé sur le principe des fourmis artificielles. En effet, ce dernier a l'avantage d'être très rapide tout en fournissant la taille de la partition en une seule étape. Par conséquent, le temps total de la segmentation de la carte est amélioré en comparaison aux autres méthodes qui parcourent l'ensemble des partitions possibles en choisissant une avec un l'indice de qualité optimal, (Vesanto et Alhoniemi (2000)). Lors de la comparaison avec la distance euclidienne simple nous avons pu remarquer que notre nouvelle approche apporte des résultats compétitifs sur plusieurs bases de données.
Nous avons pu constater, qu'il existe des bases pour lesquelles notre méthode n'est pas efficace. Pour améliorer ces résultats, des perspectives peuvent être déduites. La première consiste à améliorer la mesure de similarité qui constitue un critère important dans la segmentation de la carte. En effet dans cette mesure nous devons tenir compte plus du voisinage et de l'inertie intra-classe, (Yacoub et al. (2001)). Par conséquent, il serait intéressant de vérifier si on retrouve les deux termes de notre mesure en calculant la perte d'inertie de la carte topologique. La seconde perspective concerne la méthode de segmentation qui a été utilisée, Il serait intéressant de penser à améliorer l'algorithme AntTree en l'hybridant avec une heuristique qui supprime les sous ensembles (classes) de petit effectif. 

Introduction
Data stream management systems (DSMS) have emerged to meet the needs of processing continuous changing, unbounded data and real-time responses. The applications include stock quoting, auction processing, network flow monitoring, moving objects monitoring [Abdessalem et al. (2007), Moreira et al. (2000)], etc. In these cases, the common features consist in : 1-the data sources are infinite and real-time changing, 2-queries over data have to produce continuous responses. To cope with the first feature, the window concept is proposed. The idea consists in transforming unbounded data stream into bounded data tables, then queries can be processed as in a traditional database system. For the second feature, query evaluation methods should be executed continuously resulting in a real-time changing of the response. As we mentioned above, window techniques are proposed for solving two issues in data stream processing : infinite data sources and continuous query. In current DSMS, the windowing operation is done using the timestamps of the input data (i.e. temporal attributes). For example, in a network traffic monitoring application it is not possible to store and analyze online the whole input data. We can just continuously monitor the situation for a bounded time interval, for instance the latest 1 hour. In this case, we have an infinite stream of traffic data and a temporal window of 1 hour. The set of data belonging to the specified window is finite. Queries are then evaluated periodically (for instance, every 2 minutes) on the set of data belonging to the window. So, the result of the queries will change continuously (every 2 minutes). DSMS also propose windowed operators such as aggregate operators, join operator, select operator, etc. to enable answering complex queries. In addition to initial applications of data streams, the development of location-sensitive devices has also spurred the on-line location-aware services and real-time moving objects monitoring applications. Besides the common characteristics of DSMS, these applications further need to process spatial and spatio-temporal data for location services. In this case, the input data is supposed to be composed of at least two attributes : a spatial attribute and a timestamp. In the previous literatures, two main approaches are considered for continuous queries processing over spatio-temporal data streams. In the first approach [Patroumpas et Sellis (2004)], the windowing operation is done using only the timestamp attribute. Then, spatial restrictions are evaluated on each set of data composing the obtained windows "temporal windows". The problem with this approach is that the fixed order of operations(temporal/spatial restrictions) in a query evaluation plan can have a important impact on the performance of the system [Elmongui et al. (2006)]. In other terms, it may be inefficient in some cases when the temporal restriction is always done before the spatial one. In addition, many location services don't involve temporal attribute at all and only care about spatial information, e.g. Range or kNN query ] etc. In the second approach, as proposed in the PLACE project ], the windowing operation is done using the spatial attribute. Then, temporal restrictions are evaluated on each set of data composing the obtained Spatial Windows.
To point out the necessity of spatial windows for spatio-temporal applications, we take the following example. Given the situation of a vehicle moving on the road. The vehicle sends the updates of its position periodically to the DSMS, and we would like to know its average speed in the last 50 kms. Temporal windowing is not efficient to answer this kind of query, because the size of the temporal window can't be decided in advance. The time spent by the observed object to cross the 50 kms may change significantly along the trajectory of the object.
A spatial windowing is more suitable to answer this kind of query. We may fall back on the spatial window (specified by the criteria "in the last 50 kms") to catch a finite data relation from the unbounded data stream. Only the semantics of the windowing operation changes because we use the spatial dimension instead of the temporal one. When processing the incoming streams, spatial window maintains its contents according to the spatial attribute of the input data (tuples). Only data within a certain area scope are of interest and are kept in the window.
In this paper, we focus on the extension of DSMS to the management of spatio-temporal data. We propose two kinds of spatial windows : a static spatial window and a moving spatial window. These operations are presented as a possible extension to the query language CQL (Continuous Query Language) [Arasu et al. (2006); Arasu et Widom (2004)]. Their semantics are defined based on the abstract semantics of CQL, and their expressiveness is demonstrated using examples of queries. This paper is organized as follows. Section 2 presents the CQL query language. Section 3 describes our extension of CQL to support spatial windowing, and section 4 concludes the paper.
Preliminaries
In this section, we present the data model and the basic operations proposed in CQL. This query language was defined in the STREAM project [Arasu et al. (2006); Arasu et Widom (2004)]. Its syntax is close to the SQL-99 syntax and it is especially designed for continuous data processing over data streams. CQL assumes that the input tuples come in a time ordered sequence and the windowing operation is processed on the timestamp of each tuple. First, we present the basic functions and data domains, then we give the definitions of stream and relation and their related operators.
T is a discrete, ordered time domain. A time instant is any value from T. Time attributes belong to T.
A tuple is a finite sequence of atomic values.
This is the domain of finite, but unbounded, bags of tuples.
This is the domain of multi-sets over TP × T (see Definition 2.1) -R : Relation domain ; R : T ? ?, this is the domain of functions that map time instants to bags of tuples (see Definition 2.2).
This the domain of functions that produce a bag of tuples from one or more bags of tuples. For example, the relational algebra operators (e.g. ?, ?, etc.).
This is the domain of functions that converse a stream to bags of tuples. For example, the CQL operators RANGE and SLIDE.
This is the domain of functions that converse finite bags of tuples to a stream. Such functions in CQL are IStream, DSream and RStream.
Definition 2.1 :
A stream S is a (possibly infinite) bag (multi-sets) of elements t where s is a tuple belonging to the schema of S and t ? T is the timestamp of the element.
Definition 2.2 :
A relation R is a mapping from each time instant t ? T to a finite but unbounded bag of tuples, denoted R(t), belonging to the schema of R.
CQL queries are composed from operators belonging to the three classes : Relation-toRelation operators (from R2ROp domain), Stream-to-Relation operators (from S2ROp domain), and Relation-to-Stream operators (from R2SOp domain).
Let us rebuild the example given in the Introduction, and consider that we look for the average speed of a car in the past 60 minutes every 10 minutes. In this situation, sliding temporal window can be used to extract tuples of latest 60 minutes with the sliding value of 10 minutes. By using CQL, query sentence can be expressed as follows :
minutes', SLIDE '10 minutes'] WHERE R.ID=mycar
This query is constructed from three classes of operators : RANGE '60 minutes' and SLIDE '10 minutes' respectively design the window size and moving step of the spatial window. They belong to the S 2ROp domain. A relational restriction operator restricts tuples to have the ID value equal to 'mycar' after the conversion of stream to relation. An aggregate operator 'AvgSpd()' calculates the average speed of 'mycar' in the last 60 minutes by using tuples meeting the previous restriction. These two operators belong to the R2ROp domain. Finally, an RStream operator converts the content of result relation(object ID and average speed after previous calculation) to stream and transmits it to users. This operator belongs to the R2SOp domain. The three classes of operators in this example can be expressed by the semantics of CQL.
The semantics of CQL is specified using a meaning function M [Arasu et Widom (2004)]. The meaning function takes any query Q belonging to CQL and returns an "input-output" function M s, t) after computation by Q. M s, t) takes the streams and relations referenced in Q and a time instant t (e.g. now) as the input. Then it specifies the output produced by Q at time instant t. Let us consisder the example of query Q 1 given above. In the first stage, a Stream-to-Relation conversion is done on the data stream S tream_car. The meaning function of this operation is the following :
}, where t high = minutes × 10 minutes, and t low = max{t high ? 60 minutes, 0}
The expression t high = minutes minutes computes the largest time instant multiple of 10 minutes and smaller then t. Intuitively, the Stream-to-Relation conversion defines its output tuples each 10 minutes, only the tuples within the last 60 minutes are contained in the output. Based on the obtained output relation, the evaluation of the where operation is done according the the following meaning function :
The meaning function of the operator AvgSpd(R) is :
In this case, distance represents the path that the monitored car has passed in the last 60 minutes. It is calculated using the position information that must contain each tuples in the window.
Finally, operator RStream of R2SOp domain converts its input relation to a stream. Its meaning function is :
3 Adding a spatial window operation to CQL
In this section, we propose an extension to CQL in order to support spatial windowing. We consider two categories of spatial windows : stationary windows and moving windows. The spatial coordinates of a stationary window do not change over time. However, the spatial coordinates of a moving window can change over time. To illustrate this, let's consider the following two spatial queries :
• Stationary Spatial Windows Suppose that we are interested in monitoring the trajectories of fishing ships in the east China sea. In this case, the interest area could be regarded as a stationary spatial window and the trajectory of each fishing ship consists in its continuous positions in that area.
• Moving Spatial Windows
Recall the example of section 1 and expand it :"Query the average speed of a vehicle in the last 50 kms every time after it moves 10 kms". The spatial window here has the window size of 50 kms and the sliding step of 10 kms, which indicates that the spatial window is moving.
Based on this informal description of spatial windows, we extend in 3.1 the CQL data model and its basic operations presented in section 2. Then, we define in 3.2 the syntax and the semantics of the spatial window operation.
Extending the Data model
To be able to deal with spatial data, we add the following domains to CQL data model.
-G : Space domain ; This is the domain of spatial values. The spatial attributes of spatio-temporal data stream belong to this domain.
this is the domain of functions that map spatial restriction to bags of tuples (see Definition 3.2). R t : T ? ?, this is the domain of functions that map temporal restriction to bags of tuples (this domain is denoted R in the CQL data model, see section 2). -S : Stream domain ;
This is the domain of multi-sets over TP ×G × T (see Definition 3.1)
This is the domain of windowing functions that converse a spatio-temporal stream to bags of tuples. This will correspond to the new windowing operators RANGE BY ...
This is the domain of functions that converse finite bags of tuples to a spatio-temporal stream, by adding a spatial stamp and a temporal stamp to each tuple. This will correspond to the new CQL functions IStream, RStream and DStream.
A Spatio-temporal data stream consists of a stream of tuples, each one is stamped with a temporal attribute and a spatial attribute (i.e. each tuple have two stamps). Temporal windowing operators are executed on the basis of the temporal stamps, and spatial windowing operators are executed according to the spatial stamps. We define the stream and relation model in the case of spatio-temporal data as follows.
Definition 3.1 :
A stream S in the spatio-temporal case is a (possible infinite) bag of elements g, t where s is a tuple belonging to the schema of S , g ? G is a spatial stamp, corresponding for example to the spatial coordinates of a moving object, and t ? T is the temporal stamp of the element.
Definition 3.2 :
A relation R(g,t) is a mapping from each location g ? G and each time instant t ? T to a finite but unbounded bag of tuples, belonging to the schema of R. The content of a relation R changes over space and time.
In a data stream S, we denote by g the spatial stamp, and we denote by t the temporal stamp, of each tuple. These stamps are necessary for spatial and temporal windowing operations. Updates from different sources (for instance, moving objects) can flow in separate streams or may be incorporated into the same stream. When employing a spatial windowing operation, tuples will be filtered into bags of tuples according to their spatial stamps. When employing a temporal windowing operation, they will be filtered according to their temporal stamps. A continuous query on a spatio-temporal stream may be composed of only spatial or temporal windowing operations. It can also be composed at the same time of both temporal and spatial windowing operations.
Semantics and Syntax of Spatial Window
Since a data stream is infinite and the memory size is limited, the windowing approach is fundamental for the processing of continuous queries in DSMS [Patroumpas et Sellis (2006)]. In CQL syntax [Arasu et al. (2006); Arasu et Widom (2004)], the window operation is denoted by the keywords RANGE, ROW and SLIDE. In TelegraphCQ [Chandrasekaran et al. (2003)], the window operation is denoted by the expression RANGE BY ... SLIDE BY and in ; Maier et al. (2005)] the same CQL keywords RANGE, SLIDE are used to denote the window operation. In this paper, we use a syntax similar to TelegraphCQ to illustrate our spatial window operations.
The syntax we consider for the windowing operation is as follows :
Where v 1 denotes the window size (range attribute) and v 2 denotes a step between two successive windows (slide attribute). The range and the slide attributes may be temporal or spatial values. This is indicated by the keywords RATTR SPACE and RATTR TIME for the range attribute, and by the keywords SATTR SPACE and SATTR TIME for the slide attribute. The range and the slide attribute may belong to the same domain (T or G) or not. In the following, we only consider the case where these two attributes are spatial values (v 1 ? G and v 2 ? G).
Stationary Spatial Window
Let us take the example of the stationary spatial window given above at the beginning of Section 3. Since we only care about the trajectories of fishing ships in a certain region (east China sea), we can use a spatial window corresponding to the east China sea in order to only catch from the stream the tuples of interest. The trajectories consist of sets of position information. For a given object o 1 , the following query gives its trajectory in the east China sea.
In this case, Moving_Objects_Stream denotes a spatio-temporal stream. The schema of this stream is composed of the ID attribute and some other attributes that indicate the speed and the orientation of the monitored moving objects. Function Stamps(*) returns the spatial and temporal stamps (g and t) of each tuple, which indicate the trajectory of the observed object. This query contains a RANGE BY value without a SLIDE BY. It means that the spatial window is stationary and do not change over time or space.
Formally, the semantics of the spatial window used in query Q 2 is specified by the following meaning function :
While processing query Q 2 , only the tuples within the spatial range East_China_Sea are preserved in the window. The content of the window is updated when a new tuple comes in. The fact that a tuple is qualified for the window or not is determined by the spatial operator inside, which determines if the spatial location g (spatial stamp) is inside the spatial area denoted East_China_Sea.
After the Stream-to-Relation operation (windowing operation), further processing on the window contents will be done. In this example, the Where clause and the Stamps operator will be performed. The meaning function of the where clause can be deduced easily from the semantics of CQL presented in section 2. The meaning function of the Stamps operator is as follows.
At the last stage, the result of query Q 2 is returned to the user in a stream format. This is done by the RStream operation. For each tuple added to its input relation, the RStream operator will re-evaluate completely its output and return all the tuples composing the output stream. Formally, the semantics of RStream is as follows :
F??. 1 -Stream-to-Relation operation
F??. 2 -Relation-to-Relation opertation for Window Content
M R2S = ?R.{e : e ? R(g, t)} Figure 1 shows an example of static spatial window over a spatio-temporal stream (an S2ROp). Three moving objects (o 1 , o 2 and o 3 ) are observed. Each new tuple represents an observation and contains the ID of the observed object, its velocity and its orientation. The temporal stamp of the tuple indicates the instant of the observation, and the spatial stamp indicates the location of the object at that instant. The trajectories shown in figure 1 represent the successive locations of the monitored objects. Figure 1.a represents the observations received up to now in the stream, and figure 1.b shows the subset of observations that are located inside the spatial window area East_China_Sea. Figure 2 shows the result of the R2ROp filtering operation. The Where clause restricts the observations to only those corresponding to object o 1 . Figure 3 illustrates the conversion from relation to stream by R2SOp. In this example, RStream will output all the tuples of its input relation. In the case of an IStream operation, the result relation will be compared to the previous one and only the new tuples will compose the output.
F??. 3 -Relation-to-Stream opertation for Window Content
F??. 4 -Stream-to-Relation operation for Moving Window
Moving Spatial Window
Let us take the example of the moving spatial window given at the beginning of Section 3. We add additional restrictions to this example, supposing that the car moves in a straight line and keeps the same direction. This query may be expressed as follows :
SELECT The tuples in the relation are updated according to the parameter RANGE BY and SLIDE BY. Every time the monitored car moves 10 km ahead, the output result is re-evaluation and only the tuples having a spatial stamp g located inside the last 50 kms will be preserved in the window. Figure 4 illustrates the moving window operation in this example. The monitored car moves along a road. At time T 1 , the car completed the first 50 kms and the tuples received in the stream up to T 1 compose the first window. At time instant T 2 , the query window moves 10 kms ahead, and only the tuples corresponding to the last 50 kms are kept in the window. Note that the time needed by the car to cross the "slide by" distance is not constant : T 3 ? T 2 my be not equal to T 2 ? T 1 .
Similarly to query Q 1 , the Relation-to-Relation operator AvgSpd(R) and the Relation-toStream operator RStream are used here to calculate the average speed and to output the the result in a stream format. Their meaning functions can be deduced easily from the example of Q 1 .
The purpose of this paper was to define the semantics and general language syntax for a spatial windowing operation over data streams. Spatial windowing operation is useful for the querying of spatio-temporal data streams. Based on the continuous query language CQL, we proposed a syntax to express spatial windows and defined the semantics of this operation. this is the main contribution of this paper. Next steps will be the implementation of the windowing operation presented in this paper, its performance analysis on a real world application, and the analysis of more complex spatial windowing cases.

Introduction
Cet article présente une extension à la méthode de construction d'ontologie à partir de textes Terminae Aussenac-Gilles et al. (2008). Lors de la création d'une nouvelle ontologie, nous proposons de réutiliser une ontologie générique de référence afin de faciliter la phase de conceptualisation des termes d'un corpus. Une ontologie générique de référence (traduction du terme core ontology) couvre un domaine composite (par exemple le droit) comportant de nombreux sous domaines (droit public, privé, européen, etc.). A ce titre, une telle ontologie constitue un cadre unifié pour la construction d'ontologies de domaine composite puisqu'elle décrit les concepts communs à l'ensemble des sous-domaines.
La réutilisation constitue actuellement un point central de l'ingénierie des ontologies soulevant des questions complexes. De nombreux travaux sont en cours dans ce domaine Euzenat et al. (2004), Noy (2004b), Shvaiko et Euzenat (2005), Predoiu et al. (2005), Bach (2006), Safar et al. (2007). Toutefois, peu de travaux exploitent la distinction entre les différents types d'ontologies et leur articulation. En effet, parmi les concepts de l'ontologie générique, certains jouent un rôle de pivot entre les ontologies des sous-domaines et permettent d'ancrer l'ontologie en cours de construction. Le processus d'alignement proposé exploite également des informations lexicales et sémantiques de l'ontologie de référence.
Dans le paragraphe 2, nous situons notre approche de la réutilisation d'ontologies et son intégration dans la méthode Terminae. Le paragraphe 3 détaille l'algorithme d'alignement sé-mantique. Puis quelques exemples illustrent les premières expérimentations faites dans le domaine juridique. Enfin, nous concluons en discutant les apports et les limites de la méthode adoptée.
Méthode
2.1 Positionnement par rapport à alignement Lors de la construction d'une nouvelle ontologie relative à un domaine de connaissances pour supporter une activité particulière, deux stratégies sont envisageables : la construction ex nihilo ou une réutilisation des ressources existantes. L'existence d'ontologies génériques disponibles, accessibles via les moteurs de recherche, conduit à définir des méthodes pour leur réutilisation. Les travaux relatifs à l'intégration d'ontologies cités supra concernent l'éla-boration de méthodes d'alignement semi-automatique ou automatique. Elles conduisent à la définition de mesures de similarité lexicales ou structurelles, dites sémantiques. Si les résultats obtenus sont indéniables, ils sont relativement peu nombreux dans le domaine de la construction d'ontologies à partir de textes. Une des raisons vient sans doute de la difficulté à maîtriser le passage du niveau linguistique au conceptuel. Certaines des techniques utilisées pour l'alignement ont recours à l'exploitation structurelle des entités ou à des informations lexicales. Dans ce travail, l'alignement sémantique utilise des informations linguistiques contenues dans la définition associée à chacune des entités considérées. L'alignement est réalisé au fil de l'éla-boration avec l'ontologie générique de référence et il est dirigé de l'ontologie en cours de construction vers l'ontologie générique de référence.
Intégration dans Terminae
Les concepts de l'ontologie en cours de construction sont définis et organisés à partir des connaissances exprimées dans les textes et en fonction des besoins de l'application. L'alignement intervient dans l'étape de conceptualisation qui se décompose en deux phases, l'amorçage et la consolidation. La phase d'amorçage consiste à identifier, dénoter et définir les concepts terminologiques (en lien avec le corpus) du domaine puis à les organiser dans des hiérarchies locales. La dénotation et la définition en langue naturelle du concept terminologique sont éla-borées à partir des occurrences du terme étudié. Le repérage de propriétés structurelles et fonctionnelles liant ces concepts est obtenu à l'aide de patrons lexico-syntaxiques. Les propriétés structurelles servent de support à leur organisation hiérarchique et les propriétés fonctionnelles qui sont propres au domaine permettent d'établir des liens autres que hiérarchiques. La phase de consolidation a pour objectif de relier les hiérarchies locales obtenues et d'enrichir le modèle. Trois processus y contribuent : la généralisation, la spécialisation et le regroupement. La généralisation selon un axe ascendant permet de déterminer les nouveaux concepts ancêtres et de définir des concepts plus abstraits ou de faire référence à des catégories de plus haut niveau dans la hiérarchie. La spécialisation intervient pour chaque concept existant afin de s'assurer que ses sous-concepts ont bien été définis. Le regroupement qui permet de créer de nouveaux concepts partageant des propriétés identiques peut conduire à la définition de concepts non terminologiques. giques en cours de conceptualisation avec les entités de l'ontologie générique. Cet appariement nécessite une comparaison lexicale et sémantique de ces entités. L'aspect lexical vient accentuer les problèmes associés à la désignation des concepts terminologiques. L'aspect sémantique conduit à comparer le sens des entités de l'ontologie et des concepts terminologiques en s'appuyant à la fois sur les définitions établies à partir des occurrences des termes dans le corpus et les commentaires décrivant les entités de l'ontologie.
L'objectif poursuivi est de semi-automatiser cette mise en correspondance en exploitant la langue utilisée pour décrire les entités (dénotation de concepts, commentaires associés) de l'ontologie générique, ce qui suppose une similitude avec le vocabulaire utilisé dans le corpus. L'évaluation de la ressemblance entre les entités de deux ontologies conduit à utiliser des techniques qui permettent de comparer les concepts en mesurant la proximité lexicale Euzenat et Shvaiko (2007) des termes qui les dénotent, des propriétés structurelles et fonctionnelles qui les lient, de leur voisinage et de leurs extensions. La comparaison des propriétés peut être obtenue en comparant l'intersection et l'union des ensembles auxquelles elles appartiennent Staab et Maedche (2001), les domaine et codomaine des relations peuvent également être utilisés Cullot et al. (2003), Noy (2004a), ou encore les propriétés définissant les relations entre les concepts telles que la symétrie et la transitivité peuvent également être étudiées Ehrig et Sure (2004). Dans ce travail, la mise en correspondance repose sur une comparaison des chaînes de caractères des formes canoniques des entités (dénotation de concepts, de rôles et commentaires, les termes extraits du corpus).
L'algorithme d'alignement sémantique
On suppose que les deux ontologies sont exprimées dans le même langage et que l'ontologie générique est commentée en langue naturelle. On dispose de trois listes constituées à partir des entités de l'ontologie : la liste des dénotations des concepts (LCO) ; la liste des dénotations des rôles (LRO) ; la liste des termes utilisés dans les commentaires (LCom). La dénotation du concept terminologique est désignée par CT. 
Pseudo-Algorithme
/ / r e c h e r c h e de c d a n s LCO t e l que Sim ( CT , c )
i f ( sim ( CT , c ) ) { / / Sim ( CT , c ) s i m i l i t u d e l e x i c a l e de CT e t c 4 i f ( sem ( CT , c ) ) { / / Sem ( CT , c ) s i m i l i t u d e s é m a n t i q u e de CT e t c 5 i f ( f e u i l l e ( c ) ) {
6
/ / c r é a t i o n a n c r a g e
7 } e l s e { 8
/ / d é c i s i o n e x t e r n e p o u r a n c r a g e de CT
9 
/ / v e r s c ou un de s e s d e s c e n d a n t s
s e { / / r e c h e r c h e d a n s l e s c o m m e n t a i r e s
13
/ / r e c h e r c h e d a n s l e s c o m p o s a n t s du s y n t a g m e n o m i n a l
/ / r e c h e r c h e d a n s l e s c o m p o s a n t s du s y n t a g m e n o m i n a l 12
e n s M o t s = l e s s y n t a g m e s nominaux i n c l u s d a n s CT Si aucun ancrage n'est trouvé, le processus ci-dessus est réitéré sur un hyperonyme jusqu'à rencontrer un terme dénotant un concept de l'ontologie générique. Au pire, l'itération se poursuit jusqu'aux concepts d'ancrage de l'ontologie de haut niveau tels que processus, objet abstrait, etc.
Illustration
Le domaine juridique sert de support à cette présentation et l'ontologie à construire relève du droit européen. L'ontologie générique LKIF-Core 1 notée LKIF (Legal Knowledge Interchange) exprimée en anglais est utilisée pour l'alignement. Le corpus est constitué de directives, en langue anglaise, relatives au droit des travailleurs.
Nous présentons un exemple de mise en oeuvre de ce pseudo-algorithme dans les cas suivants : Terme : directive Le concept terminologique directive correspond à un concept de l'ontologie générique qui a pour : -commentaire associé : Les termes de LRO dénotant les noms des rôles dans l'ontologie générique sont comparés aux relations lexicales extraites du corpus. Seuls quelques termes de LRO apparaissent dans le corpus comme observe qui intervient dans la définition de NATURAL_PERSON. L'étude des rôles fait apparaître au moins deux difficultés : au niveau conceptuel, la restriction des rôles de l'ontologie générique, au niveau de la normalisation, la mise en correspondance des relations lexicales avec les termes de LRO.
Conclusion et perspectives
Dans ce papier, nous avons proposé un algorithme d'alignement sémantique prenant en compte les informations linguistiques et sémantiques contenues dans une ontologie générique de référence (dénotation des concepts, des rôles et les commentaires associés). L'avantage de cette approche semi-automatique est d'affranchir l'ingénieur de la connaissance d'une exploration systématique de l'ontologie générique et de lui permettre de juger du sens après une comparaison lexicale des entités étudiées. Néanmoins, les premières expérimentations soulèvent des difficultés relatives à la représentation des entités contenues dans les ressources exploitées comme l'interprétation de la sémantique des commentaires. Des mesures plus précises relatives à l'amélioration apportée par cette approche sont en cours d'expérimentation.

Introduction
Mettre en oeuvre l'une des méthodes de classification non supervisée consiste en premier lieu à choisir une manière de représenter les documents (Sebastiani, 2002) ; dans un second temps il faut choisir une mesure de similarité, et en dernier lieu choisir un algorithme de classification que l'on va mettre au point à partir des descripteurs et de la métrique choisis. Tout document d j sera transformé en un vecteur de poids w kj des termes t k . La majorité des méthodes, pour calculer le poids w kj , sont axées sur une représentation vectorielle des textes de type TF-IDF (Sebastiani, 2002), qui attribue un poids d'autant plus fort que le terme apparaît souvent dans le document et rarement dans le corpus complet. Il existe différentes approches pour la représentation des documents. Typiquement, la similarité entre documents est estimée par une fonction calculant la distance entre les vecteurs de ces documents. Plusieurs mesures de similarité ont été proposées (Jones & Furnas, 1987). Parmi ces mesures on peut citer la distance du cosinus. L'algorithme SOM (Kohonen & al, 2000) a été depuis longtemps proposé et appliqué dans le domaine de la classification des documents textuels. Cependant, les combinaisons entre SOM et représentation conceptuelle de textes d'une part, SOM et représentation basée sur les n-grammes d'autre part n'ont pas été beaucoup étudiées.
Expérimentations, résultats et évaluation
Les données utilisées dans nos expérimentations sont issues des textes du corpus Reuters21578. Dans l'approche basée sur les n-grammes, on compte les fréquences des ngrammes trouvés. Dans l'approche conceptuelle, on remplace les termes par les concepts qui leur sont associés dans l'ontologie de références lexicales Wordnet (Miller, 1990). Cette représentation nécessitera deux étapes : la première est le « mapping » des termes dans des concepts et le choix de la stratégie de « merging », la deuxième est l'application d'une stratégie de désambiguïsation. On choisit la stratégie « Concept seulement », où il s'agit de SOM pour la Classification Automatique Non supervisée de Textes basés sur Wordnet remplacer le vecteur des termes par le vecteur des concepts en excluant tous les termes de la nouvelle représentation, y compris les termes qui n'apparaissent pas dans Wordnet. Pour la désambiguïsation nous utilisons la stratégie du « Premier concept » et la fonction TFIDF pour le calcul des poids de chaque terme pour les deux approches. Nous avons utilisé une carte de Kohonen 7x7. Pour chaque approche, quatre mesures de similarité ont été testées: les distances du cosinus, euclidienne, euclidienne au carré, et la distance de Manhattan. Nous avons calculé, pour chaque cas, le nombre de classes, le temps et le taux d'apprentissage. Nous avons pu observer que malgré les bons résultats obtenus par la méthode des n-grammes particulièrement pour n=3 et n=4, ceux obtenus par la méthode conceptuelle, avec la distance du cosinus, sont plus performant. Pour évaluer la qualité des classifications obtenues nous avons utilisé la f-mesure et l'entropie. La partition P considérée comme la plus pertinente et qui correspond le mieux à la solution externe attendue est celle qui maximise la F-mesure associée ou minimise l'entropie associée. La plus grande valeur de la f-mesure est 62,5 et la plus petite valeur de l'entropie est 37,5. Ces deux valeurs correspondent à l'approche conceptuelle (Wordnet) avec la distance du cosinus, ce qui confirme les conclusions tirées.
Conclusion et perspectives
Dans cet article nous avons proposé deux nouvelles approches pour la classification non supervisée de textes, l'une basée sur l'utilisation des n-grammes et l'autre sur WordNet. Les résultats obtenus montrent que malgré les bons résultats obtenus par la méthode des ngrammes, le fait d'ajouter des connaissances lexicales dans la phase représentation permet de construire une classification de meilleure qualité. Nous projetons dans un premier temps d'utiliser d'autres stratégies de désambiguïsation et voir leur influence sur la classification, et dans un second temps utiliser d'autres approches conceptuelles de références syntaxiques pour la classification par la méthode SOM des textes multilingues.

Introduction
Dans cet article, nous proposons trois stratégies de classification non supervisée appliquées sur fenêtres superposées. Notre objectif est de pouvoir repérer les changements de la distribution sous-jacente d'un flux de donnés sur le temps. Notre approche consiste donc à fixer a priori la taille de la fenêtre et appliquer un algorithme de classification non supervisée sur les données contenues à l'intérieur de la fenêtre. Nous définissons deux types de partitionnement de données sur les fenêtres : partitionnement par nombre d'effectifs (fenêtre logique) et partitionnement par intervalle de temps (fenêtre de temps).
L'idée principale est de faire glisser la fenêtre sur le temps de telle façon que des nouvelles données soient rajoutées dans la fenêtre et par conséquence, les données les plus anciennes en soient éliminées. L'action de glissement de la fenêtre sur les données est fait de telle manière à ce qu'il y ait toujours une zone de chevauchement entre les deux ensembles de données contenues dans la fenêtre avant et après son glissement. Chaque fois qu'une nouvelle fenêtre est définie, l'algorithme de classification non supervisé est appliqué sur les données contenues dans la fenêtre, ce qui définit une partition et un ensemble de prototypes. La détection des possibles changements est faite par la comparaison de deux partitions obtenues sur le même ensemble d'individus. Dans ce contexte, nous proposons trois types de comparaisons de partitions : comparaison sur les données de l'intersection, comparaison sur les données de l'union et comparaison sur la totalité des données.
Pour la classification de données dans notre approche, nous avons utilisé l'algorithme Kmeans (MacQueen, 1967)  
Conclusion
Comme résultat de toutes expérimentations, nous avons des valeurs très élevées (compris entre 0.8 et 1) pour les deux critères d'évaluation, ce qui nous montre un jeu de données assez stable et sans changements remarquables. Cela peut être dû à la courte période de temps disponible pour l'analyse : pas plus de 22 jours, les premiers de l'année. En conclusion, il est évident la nécessité d'établissement d'un compromis entre la taille de la fenêtre et le temps de réponse désiré, mais aussi en considérant la périodicité des changements.
Comme prolongement de ces expérimentations, nous citons l'application de cette approche sur d'autres jeux de données -aussi bien réelles qu'artificielles -et aussi l'exécution d'autres simulations afin d'analyser l'influence des différentes valeurs des paramètres d'entrée (tels que le pourcentage de chevauchement et le nombre de clusters). De plus, nous envisageons la mise en place de dispositifs permettant la découverte automatique de la périodicité présente dans le flux de données.
Références
Hubert, L. et P. Arabie (1985). Comparing partitions. Journal of Classification 2, 193-218. MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. In 5th Berkley Symposium on Mathematics and Probability, Volume 1, pp. 281-297.
van Rijsbergen, C. J. (1979). Information Retrieval (second ed.). London : Butterworths.
Summary
A major difficulty is present in the data stream domain: the underlying data distribution may change over time. In this article, we propose three strategies of unsupervised classification based on overlapping windows. Our aim is to detect these changes over time. Our approach is applied on a benchmark of real data. The conclusions obtained are based on statistical analysis.

Introduction
Etant donné un ensemble d'objets et un ensemble d'étiquettes de classes, le problème de classification est de chercher une fonction pour attribuer à chaque objet une étiquette de classe. Une telle fonction est appelée un classifieur. Les constructions de ces classifieurs sont en géné-ral basées sur les données d'exemples (d'entraînement). Il existe plusieurs méthodes de classification, telles que l'arbre de décision Quinlan (1993), la méthode naïve-Bayes Duda et Hart (1973), les méthodes basées sur les règles Clark et Niblett (1995); Cohen (1995). Ce papier présente une approche à la construction de classifieurs basée sur les règles classe-associations Lent et al. (1997); Liu et al. (1998); Li et al. (2001), en utilisant une structure d'arbre de pré-fixes pour l'extraction des itemsets fréquents et les règles d'association Agrawal et al. (1993).
Dans les approches telles que CMAR Li et al. (2001), HARMONY Wang et Karypis (2005), par optimisations, les règles d'association sont essentiellement construites sur les itemsets clés Bastide et al. (2000). Ce présent travail montre que parmi ces itemsets clés, on peut s'intéresser seulement à ceux de petites tailles. Ensuite, via un test de ? 2 , il montre que parmi ces derniers, il existe encore ceux qui ne sont pas significatifs pour la classification. Ces itemsets clés sont dits non essentiels. Les résultats d'expérimentations sur les grands jeux de données de UCI Coenen (2004) montrent que l'optimisation par la suppression de ces itemsets est correcte et efficace.
Préliminaires
Un jeu de données est un triplet D = (O, I, R), où O, I, R sont des ensembles finis et non vides. Un élément de I est appelé un item, un élément de O est appelé un objet (ou une transaction) représenté par un identifiant, et R est une relation binaire entre O et I. Une paire (o, i) de R signifie que l'item i est une valeur attribuée de l'objet o. Un itemset est un sousensemble de I. Un k-itemset est un itemset avec k items ; k est la taille (ou la longueur) de l'itemset.
La connexion de Galois Ganter et Wille (1999) est une paire de fonctions (f, g), où g(I) = {o ? O | ?i ? I, (o, i) ? R} et f (O) = {i ? I | ?o ? O, (o, i) ? R}. En fait, g(I) est l'ensemble des objets de O qui ont en commun tous les items de I, et f est la fonction duale de g. La fonction g est anti-monotone : pour tout I 1 , I 2 ? I, si I 1 ? I 2 alors g(I 2 ) ? g(I 1 ).
Soit un jeu de données D = (O, I, R) et sa connexion de Galois (f, g). Les opérateurs de fermetures de Galois sont les fonctions suivantes : h = f o g et h = g o f , où o dénote la composition de fonctions. Les fonctions h et h sont monotones. Soit I un itemset. Alors h(I) = f (g(I)) est appelée la fermeture de I. En effet, pour tout I ? I, I ? h(I) (Extension) et h(h(I)) = h(I) (Idempotence). Un itemset I est dit fermé si I = h(I). I est appelé un itemset clé (ou un générateur minimal Bastide et al. (2000)) si ?I ? I, h(I ) = h(I) implique I = I . Le support de I est sup(I) = card(g(I)), où card dénote la cardinalité. Il est clair que si I ? I alors sup(I) ? sup(I ). Soit minsup un seuil de support. I est dit fréquent si sup(I) ? minsup. I est un itemset clé (respectivement, fermé) fréquent si I est fréquent et I est aussi un itemset clé (respectivement, fermé).
Une règle d'association (RA) est une expression de la forme X ? Y , où X et Y sont des itemsets disjoints. Soit r = X ? Y une RA. Alors LHS(r) et RHS(r) dénotent respectivement la partie gauche et la partie droite de r. Le support de r est : sup(r) = sup(X ? Y ). La confiance de r est : conf (r) = sup(r)/sup(X).
En classification, on considère un ensemble C des éléments appelés les étiquettes de classes. Une règle classe-association (RCA) est une expression de la forme X ? c, où X ? I ? C et c ? C. Soit r = X ? c une RCA. Un objet o est couvert par r si o a tous les items de X ; on dit que o satisfait r. Un objet o est classifié correctement par r si o satisfait r et o est effectivement de la classe d'étiquette c. Le support de r, sup(r) = sup(X ? {c}), est le nombre d'objets de D qui sont classifiés correctement par r. La confiance de r, conf (r) = sup(r)/sup(X), représente la fréquence d'applications correctes de r dans D.
Exemple 1 Le tableau 1 représente un jeu de données d'entraînement dans lequel
Sur les RCAs un ordre partiel, appelé l'ordre de précédence et noté , est défini comme suit : Soient r et r des RCAs, r r (lire r précède r  Cohen (1995); Yin et Han (2003) calculent à chaque fois une règle, en utilisant des heuristiques basées sur l'analyse statistique. En contraste, les algorithmes basés sur l'extraction des règles classe-associations, comme CBA Liu et al. (1998), CMAR Li et al. (2001), cherchent un ensemble de règles de confiances élevées construites sur les itemsets fréquents.
CBA adapte Apriori pour extraire les RCAs. Les règles sont triées dans l'ordre de précé-dence avant d'être sélectionnées pour le classifieur. Une règle est sélectionnée si elle classifie correctement au moins un des objets d'entraînement. Dans ce cas, tous les objets couverts par la règle sont écartés du processus de sélection. Par l'ordre de précédence, le classifieur préfère les règles formées sur les itemsets clés.
Basé sur l'idée de CBA, CMAR Li et al. (2001) adapte FP-growth pour extraire les RCAs. En plus de l'ordre de précédence, CMAR considère la corrélation entre la partie gauche de règle et l'étiquette de classe. D'ailleurs, CMAR permet à chaque objet d'être couvert par plusieurs règles et propose un schéma de classification basé sur de multiples règles. HARMONY Wang et Karypis (2005) utilise la même stratégie que FP-growth pour extraire les RCAs. Par défaut, les items d'un jeu de données restreint sont triés dans l'ordre croissant des coefficients de corrélation entre le préfixe correspondant et ces items. Les items et les jeux de données restreints non prometteurs sont exclus de l'espace de recherche. Par ces exclusions, le classifieur préfère les règles formées sur les itemsets clés.
A la différence de CBA et CMAR, pendant l'extraction des règles, HARMONY maintient pour chaque objet une liste de règles de confiance la plus élevée qui classifient correctement l'objet. A la fin du processus de l'extraction, HARMONY regroupe les règles sélectionnées selon leurs étiquettes de classes et les trie dans l'ordre décroissant des confiances et des supports. Pour classifier un objet de test ti, HARMONY calcule, pour chaque groupe du classifieur, la somme de confiances de k premières règles de confiance la plus élevée qui couvrent ti. La classe avec la somme la plus grande est sélectionnée pour prédire la classe de ti.
Contribution : La contribution de ce travail consiste en :
-L'étude d'une relation entre les supports des itemsets qui s'incluent, la généralisation d'une propriété importante des itemsets non clés, et la notion d'itemset clé non essentiel.
-L'application de la notion d'itemset clé non essentiel pour optimiser la construction de classifieurs basée sur les RCAs.
-L'approche n'explore pas les itemsets de toutes tailles. Elle se limite aux itemsets clés de petites tailles (? 5). Par conséquent, elle peut exploiter les règles de supports très bas, mais avec de meilleures confiances.
Les résultats expérimentaux sur les grands jeux de données catégoriels montrent qu'en moyenne l'approche est efficace, en comparaison avec les approches importantes, et que l'optimisation sur les itemsets clés non essentiels est correcte et efficace.
Itemsets clés non essentiels
Itemsets non clés
Il est facile de voir que si X est un itemset clé, alors pour tout itemset I = X, si X ? I ? h(X), alors sup(X) = sup(I) = sup(h(X)). Cette propriété permet de définir une relation d'équivalence sur les RAs : une règle X ? Y , X ?Y = ?, avec X et Y étant des itemsets clés, représente une classe d'équivalence de règles, par rapport au support et à la confiance. Ces RAs représentatives sont très intéressantes : leur nombre est plus réduit, et avec les parties gauches réduites (au sens de clés), elles peuvent être appliquées à un plus large nombre d'objets, par rapport aux règles aux itemsets non clés.
Proposition 1 Soient
L'application de la proposition 1 aux RCAs résulte en :
D'après le corollaire 1, en classification on peut construire les classifieurs avec seulement les RCAs aux itemsets clés. La fermeture de X, i.e. h(X), et tout I tel que X ? I ? h(X), sont des itemsets non clés. Ces itemsets ne sont pas intéressants pour les classifieurs. La propriété suivante est utile pour la suppression de ces itemsets Phan-Luong (2002).
Bastide et al. (2000) ont montré que, avec les conditions de la proposition 2, on a
Une conséquence directe de la proposition 2 est la suivante :
On peut en déduire que si Y n'est pas une clé, alors tout super ensemble de Y ne l'est pas. Donc, pour la construction de classifieurs, il est plus intéressant de commencer par les itemsets de petites tailles. Quand on ajoute un nouvel item dans un itemset courant, si le résultat n'est pas un itemset clé, alors il n'est plus intéressant de continuer la recherche avec ce résultat. Dans la suite, nous étudions une généralisation de cette propriété pour pousser encore l'optimisation sur les itemsets clés.
Itemsets clés non essentiels
A cette étape on peut voir que la proposition 2 est une directe conséquence de la proposition 4. En effet, lorsque
Parmi les itemsets clés, il existe ceux qui ne sont pas très différents entre eux, aux niveaux de supports et d'étiquettes de classes. Ces itemsets peuvent être considérés comme la même chose, dans le sens où les RCAs construites avec eux portent les informations très similaires sur le support et la confiance. Ces itemsets sont définis via un test ? comme suit. 
Définition 1 Soient X, Y des itemsets tels que
Pour les itemsets X et Y satisfaisant la définition 1, en construction d'un classifieur, on peut s'intéresser seulement aux itemsets clés comme X avec la taille la plus petite et oublier les itemsets comme Y . Les itemsets clés comme Y sont appelés itemsets clés non essentiels.
Pour élaguer les itemsets comme Y , on peut espérer une propriété similaire à celle spécifiée en proposition 2. C'est-à-dire, commençant par les itemsets clés de petites tailles, et lors de l'ajout d'un nouvel item dans l'itemset courant, si le résultat est une clé et, en comparaison avec l'itemset courant, la définition 1 est satisfaite, alors on peut arrêter la recherche sur cet itemset courant. Précisément, si X ? Y ? Z et X CX Y , alors on attend que
Pour cette conjecture, on peut avoir trois arguments importants. D'abord, bien que
la différence entre ? |C (Z?Y )?X |,? et ? |CX |,? ne soit pas importante, surtout quand on considère les itemsets de petites tailles. Ensuite, par le corollaire 3, on a :
Et dernièrement, d'après la définition 1 :
Nous n'avons pas prouvé formellement cette conjecture. Cependant, nous l'appliquerons dans la méthode de construction de classifieurs, basée sur une structure d'arbre de préfixes pour l'extraction des itemsets.
Extraction de classifieurs
Extraction de règles de classe-association
L'approche utilise une technique d'énumération de sous-ensembles Rymon (1992) sur un arbre de préfixes pour l'extraction de RCAs. Commençant par un arbre de préfixes p vide, on lit successivement les objets d'un jeu de données f pour mettre à jour p. Pour chaque objet o = (l : c), où l est la liste d'items de o, et c son étiquette de classe, les sous-itemsets de l sont énumérés dans l'ordre lexicographique et stockés avec l'étiquette c, dans l'arbre p (fonction Build). Leurs supports ainsi que les occurrences de leurs étiquettes de classes sont mis à jour le long de la lecture du jeu de données. L'approche peut adopter le calcul des itemsets fréquents par niveau comme Apriori. La fonction LevelBuild est un exemple spécifique de ce calcul. Par contraste avec Apriori : (i) l'approche ne génère pas de candidats, (ii) le calcul de supports se fait pendant la construction de l'arbre, et (iii) l'approche peut commencer par les i-itemsets et passer de k-itemsets aux (k + j)-itemsets, avec i, j ? 1.
// Construire l'arbre p avec les sous-itemsets de taille maximale max du jeu de données f . function LevelBuild(f, p, max) { for (i = 1 ; i ? max ; i++) { Build(f, p, i) ; // énumérer les itemsets de taille maximale i PruneInfrq(p, minsup) ; // élaguer les itemsets non fréquents } }
L'approche applique la contrainte de support aux i-itemsets, seulement pour i ? 2. Cependant, quand elle sélectionne les règles pour le classifieur, seules les règles dont la confiance et le support sont maximaux, par rapport à chaque objet d'entraînement, sont retenues.  
Suppression des Itemsets Clés Non Essentiels en Classification
Réduction de l'arbre de préfixes
Nous avons vu dans la section 4 que les RCAs formées sur les itemsets clés sont suffisantes pour les classifieurs (Corollaire 1). Nous avons aussi défini la notion d'itemset clé non essentiel et pensé que ces itemsets se comporteront comme des itemsets non clés (Corollaire 3) et auront la même propriété spécifiée en proposition 2.
La suppression des itemsets non clés et les itemsets clés non essentiels se fait en deux étapes. La première étape s'applique à l'arbre de préfixes (fonction T reeReduction). Si un noeud N a le même support que son prédécesseur ou si l'inéquation 1 de la définition 1 pour N et son prédécesseur est satisfaite, alors le sous-arbre avec N à la racine sera coupé. Pour tout noeud restant N , on réduit la liste lc(N ) en enlevant les paires (x, kx) telles que la valeur de kx n'est pas maximale dans lc(N ) (fonction RedCls).
Les paramètres de T reeReduction sont les suivants : N est un noeud de l'arbre, k et precls représentent respectivement le support du prédécesseur de N et la liste des étiquettes de classes associées au prédécesseur de N avec leurs nombres d'occurrences, et chi[ ] est un tableau de valeurs ? 2 pour le test de clés non essentiels.
Construction de Classifieurs
La deuxième étape de réduction est appliquée pendant la construction de classifieurs. Après la réduction utilisant T reeReduction, le jeu de données est lu à nouveau. Pour chaque objet, la fonction M atch cherche dans l'arbre les noeuds correspondants aux RCAs qui classifient correctement l'objet. Ces noeuds sont mis dans une liste temporaire nommée lnd (initialisée à vide pour chaque objet), dans l'ordre de précédence des règles correspondantes. Quand l'arbre est entièrement visité, les règles formées des noeuds de lnd sont rangées (Fonction AddRule) dans le classifieur, noté lrc, d'après leurs étiquettes de classes. Cependant, soient r une règle en cours de considération et rc la règle de lrc en cours d'être comparée avec r, si r et rc ont la même étiquette de classe et LHS(rc) ? LHS(r) et conf (r) ? conf (rc) alors r est rejetée.
La méthode pour classifier un objet suit le schéma de test de HARMONY : pour chaque étiquette de classe c, chercher dans lrc les règles qui couvrent l'objet. La somme de confiances de ces règles est calculée. L'objet est prédit de la classe dont la somme est maximale. Le tableau 2 rappelle les caractéristiques des 10 jeux de données de UCI Coenen (2004)  Maintenant nous présentons les résultats des expérimentations menées par ce travail (Tableau 3). Le programme exécutable de HARMONY est fourni gracieusement par les auteurs de Wang et Karypis (2005). Les paramètres sont configurés d'après la description par ces auteurs : minsup = 50 et les items sont ordonnés d'après l'ordre des coefficients de corrélation (l'ordre avec lequel HARMONY atteint la meilleure performance). En particulier, pour connect, seuls les items de supports < 20000 sont considérés. Les mêmes considérations sont appliquées pour SIM. La configuration des paramètres pour SIM :
(i) L'extraction des itemsets fréquents commence directement par les 2-itemsets, et tous k-itemsets considérés satisfont k ? 5.
(ii) L'élagage des itemsets non fréquents est effectué seulement pour les i-itemsets pour i ? 2, et avec minsup = 50. Les étapes suivantes sont développées sur les 2-itemsets fréquents, mais l'élagage n'est plus effectué, sauf pour les itemsets de support 1.
(iii) Les tests ? 2 sont effectués avec le risque d'erreurs de 0, 5%. Dans le tableau 3, on utilise les notations suivantes. -Ts : le temps d'exécution total en secondes des dix exécutions du test "10-fold cross validation" par jeu de données, pour construire les dix classifieurs et pour les tester.
-Acc. : La précision moyenne de prédiction, en pourcentage.
-L : la longueur maximale des itemsets considérés pendant la construction de l'arbre.
-#Rules : le nombre moyen de règles dans les classifieurs. Notons que dans les expérimentations de SIM, les objets sont lus toujours du disque. D'ailleurs, bien que l'on limite la longueur des itemsets en construction de l'arbre de préfixes, on ne limite pas la longueur des objets dans les jeux de données. waveform HARMONY est environ 130 fois plus lent que SIM. La raison est que HARMONY peut considérer les itemsets de toute taille (pour waveform, la plupart des itemsets sont de tailles de 5 à 9, tandis que SIM ne considère que des itemsets de taille maximale 3).
-En précision de classification : les deux approches sont comparables, sauf pour les jeux de données chess, nursery et waveform, SIM est plus précis. Ceci peut s'expliquer par la sélection de RCAs de confiances et de supports maximaux parmi celles ayant de petits supports.
Bien que SIM soit similaire à HARMONY sur plusieurs points, les résultats expérimentaux des deux approches sont très différents. En général, SIM est meilleur en temps d'exécution et en précision de classification. Avec un temps d'exécution total environ quatre fois plus court, la prédiction par SIM est environ 2, 5% plus précise que celle de HARMONY, en moyenne.
Quand SIM implémente la suppression des itemsets clés non essentiels, en moyenne, le nombre de RCAs diminue d'environ 5%, le temps d'exécution est amélioré d'environ 3%, et la précision baisse de 0, 53%. Cependant, par rapport à HARMONY, cette précision est encore de 1, 98% plus grande.
Ces résultats valident l'idée d'utilisation des RCAs construites sur les itemsets clés de petites tailles et la possibilité de sélection des RCAs de confiances et de supports maximaux parmi celles ayant de petits supports. D'ailleurs, ils montrent que la notion d'itemset clé non essentiel est applicable et utile. Pour perspective, on peut penser que cette notion peut être développée pour l'apprentissage dans le contexte d'existence de bruits.
Summary
In classification based on class-association rules, key itemsets (minimal generators) are essential in the built classifiers: non key itemsets can be pruned without affecting the accuracy of the classifiers. This work studies the generalization of a property of non key itemsets and shows that among the small size key itemsets, there still exist those which are not significant to the built classifiers, and can also be pruned. Those key itemsets are defined based on a ? 2 test. We apply this pruning to a method for building classifiers based on class-association rules, using a prefix tree structure for mining the frequent itemsets. Experiences on large datasets show that the pruning method is actually efficient and sound.

Introduction
L'objectif de la gestion des connaissances dans une entreprise est de favoriser la croissance, la transmission et la conservation des connaissances. Saad (2005) s'intéresse au repérage des connaissances cruciales pour justifier le choix d'investissement dans des opérations de capitalisation sur les connaissances. Dans la revue de la littérature, nous constatons qu'il existe peu de travaux, s'intéressant à la délimitation du champ des connaissances sur lesquelles il faut capitaliser. Les auteurs Dieng et al. (1998) ; Grundstein et al. (2003) ;B. Tseng et Huang (2005), précisent que le processus de détermination des connaissances cruciales est une action difficile à mener.
Dans cet article, nous proposons une approche multi-agents argumentative permettant de résoudre des conflits dans un système d'aide à l'identification des connaissances cruciales nommé K-DSS Saad (2005), Saad et Chakhar (pear). Les connaissances cruciales sont des savoirs et des savoir-faire nécessaires aux processus essentiels qui constituent le coeur des activités de l'entreprise. Le système proposé est basé sur une méthode composée de trois phases. La première phase consiste à déterminer l'ensemble d'apprentissage que nous appelons les "connaissances cruciales de référence". La deuxième phase consiste à évaluer les "connaissances cruciales de références" sur une famille de critères et à inférer des règles de décision.
La troisième phase consiste à exploiter l'ensemble des règles de décision inféré dans la phase précédente pour classifier des nouvelles connaissances que nous appelons "connaissances potentiellement cruciales".
Durant la deuxième phase, des divergences concernant la crucialité des connaissances peuvent apparaître entre les décideurs et aboutir ainsi à des incohérences dans la base commune de connaissances la rendant inexploitable. La construction d'un ensemble de règles de décision collectivement acceptées est basée sur une approche constructive qui s'appuie sur les travaux de Belton et Pictet (1997). Ces auteurs s'intéressent au problème de la prise en compte de l'information individuelle dans un modèle multicritère.
Notre objectif à travers ce travail est de proposer une approche multi-agent argumentative permettant d'automatiser la résolution des conflits entre décideurs. Afin de concevoir cette approche, nous nous appuyons sur la théorie multi-agents pour représenter les acteurs humains par des agents logiciels connaissant leurs préférences et leurs règles de décision et pouvant ainsi argumenter leurs choix ou mettre à jour leurs croyances en fonction des arguments qu'ils reçoivent des autres agents décideurs.
L'article est organisé comme suit. Dans la section 2, nous présentons des travaux qui traitent la problématique d'évaluation des connaissances cruciales. Dans le section 3, nous décrivons la procédure d'inférence des règles de décision collective. Dans le section 4, nous présentons le système multi-agent. Enfin, dans la section 5, nous décrivons les expérimentations et les résultats.
Travaux antérieurs
La notion de besoin en connaissances pertinentes, en amont de toute opération de capitalisation, a été définie par plusieurs chercheurs Dieng et al. (1998) ;Grundstein (2000) ; Noh et al. (2000) ; B. Tseng et Huang (2005). Les travaux théoriques et empiriques proposés dans la littérature sont peu nombreux. Nous distinguons des méthodes centrées sur les domaines de connaissances, d'autres centrées sur les processus. En analysant les démarches au niveau de la construction des critères et de l'évaluation des connaissances, nous constatons que les auteurs Ermine (2003) ; Grundstein (2000) proposent des critères construits d'une façon intuitive. En effet, ils n'expliquent pas comment ils ont construit et validé les critères, ni comment ils gèrent les multiples points de vue des acteurs de terrain impliqués dans le processus d'évaluation des connaissances. Or, il est déterminant de repérer les acteurs pertinents et s'assurer qu'ils adhérents aux critères retenus.
Comme nous l'avons précédemment mentionné, la classification des connaissances se fait par plusieurs décideurs ne partageant pas forcément les mêmes points de vue. Ceci peut engendrer des problèmes lors de la classification des connaissances. En effet, dans un processus de prise de décision collective, des conflits peuvent être constatés principalement à cause de divergences de points de vue. Plusieurs travaux théoriques ont montré l'apport d'une approche argumentative dans plusieurs domaines tels que la négociation Kraus et al. (1998) et la ré-solution de conflits Sycara (1989). Plusieurs travaux traitent du problème de la résolution de conflits en ayant recours à l'argumentation Elvang- Goransson et al. (1992), Simari et Loui (1992)) et particulièrement dans le domaine de la gestion de connaissances Chesñevar et al. (2006b), Chesñevar et al. (2006a).
A travers cet article, notre objectif est double. D'abord, démontrer l'apport de la théorie multi-agent dans notre contexte et évaluer l'impact d'une approche argumentative sur la qualité de classification et sur le nombre de conflits entre décideurs.
3 Inférence d'une base de règles de décision collective La méthode proposée est composée de trois phases Saad (2005) : une première phase pour construire l'ensemble d'apprentissage " connaissances cruciales de référence ". La deuxième phase consiste à construire un modèle de préférences des décideurs, puis une troisième phase de classification des nouvelles connaissances à évaluer. Durant la deuxième phase, des divergences concernant la crucialité des connaissances peuvent apparaître entre les décideurs et aboutir ainsi à des incohérences dans la base commune de connaissances la rendant inexploitable. Notre objectif à travers ce travail est de proposer une approche argumentative permettant d'automatiser la résolution des conflits entre décideurs. Afin de concevoir cette approche, nous nous appuyons sur la théorie multi-agents pour représenter les acteurs humains par des agents logiciels connaissant leurs préférences et leurs règles de décision et pouvant ainsi argumenter leurs choix ou mettre à jour leurs croyances en fonction des arguments qu'ils reçoivent des autres agents décideurs.
Construction d'un modèle de préférences des décideurs
Cette phase consiste à déterminer des règles de décision à partir des informations préfé-rentielles des décideurs sur un ensemble de connaissances qui constituent des exemples d'apprentissage et que nous nommons " connaissances cruciales de référence ". Les informations préférentielles sont liées à la décision d'affecter ces connaissances soit à la classe de déci-sion des " connaissances non cruciales ", c'est-à-dire des connaissances qui ne nécessitent pas une opération de capitalisation, soit dans la classe des " connaissances cruciales ", c'est-à-dire celles qui nécessitent une telle opération. Ce modèle de préférences du (des) décideur(s) se traduit sous forme de règles de décision de type " Si conditions, alors conclusion ". Cette phase est composée de trois étapes présentées ci-dessous.
Nous proposons une procédure itérative permettant d'inférer des règles de décision collectivement acceptées par les décideurs. Les différentes étapes de la procédure sont présentées dans la Figure 1. En s'appuyant sur l'ensemble des n " connaissances de référence " et les deux classes de décisions définies, la première étape consiste à déterminer avec chaque décideur des exemples d'affectation de ces " connaissances de référence " dans les deux classes de déci-sion " connaissances non cruciales " et " connaissances cruciales ". La deuxième étape permet alors d'inférer un ensemble de règles pour chacun des exemples d'affectation déterminé dans l'étape précédente. La troisième étape consiste à modifier les exemples d'affectation ou bien les critères avec le décideur concerné, si des incohérences sont détectées dans l'ensemble des règles relatif à chaque décideur. La dernière étape consiste à déterminer, après concertation avec les décideurs, une base de règles collectivement acceptée. Nous présentons ci-dessous, la détermination des exemples d'affectation des " connaissances de référence " dans les deux classes de décision, puis l'inférence des règles de décision correspondant à chaque décideur. Nous déterminons enfin les règles collectivement acceptées par les différents décideurs.
3.2 Détermination des exemples d'affectation des " connaissances de ré-férence " dans les deux classes de décision
Au cours de cette étape, en fonction des évaluations des connaissances sur les différents critères, l'homme d'étude demande à chaque décideur d'affecter les " connaissances de référence " à l'une des deux classes de décision ordonnées : -Cl1 : classe de décision " connaissances non cruciales " correspondant à des connaissances qui ne se révèlent pas nécessaires à capitaliser ; -Cl2 : classe de décision " connaissances cruciales " correspondant à des connaissances qui se révèlent nécessaires à capitaliser. -L' homme d'étude est celui qui aide à modéliser les préférences des acteurs impliqués dans un processus de décision, en faisant émerger les diffé-rents points de vue qu'il faut prendre en compte, sans pour autant influencer la décision prise par le décideur.
Chaque décideur affecte les " connaissances de référence " à l'une des deux classes de déci-sion, classe des " connaissances non cruciales " ou classe des " connaissances cruciales ". Nous obtenons un nombre de tableaux de décision égal au nombre des décideurs. Chaque tableau de décision contient les valeurs f (k i , g j ) correspondant à l'évaluation de chaque connaissance k i sur chaque critère g j ainsi que son affectation dans l'une des deux classes de décision .
Inférence des règles de décision correspondant à chaque décideur
A partir des tables de décision, l'homme d'étude utilise un des algorithme d'inférence (DOMLEM, Explore) proposé dans la méthode DRSA Greco et al. (2000)afin d'inférer, pour chaque décideur, les règles de décision correspondant à ses exemples d'affectations. Le choix de l'algorithme est lié à la présence ou pas de données manquantes dans la table de décision, à la volonté de déterminer des règles non redondantes ou bien la liste de toutes les règles. L'homme d'étude analyse, avec chaque décideur, l'ensemble des règles inférées à partir des exemples d'affectation donnés par chacun d'entre eux. Il vérifie tout d'abord s'il existe des incohérences dans la base des règles. L'origine des incohérences peut provenir : -de l'hésita-tion du décideur au moment de l'affectation de la connaissance dans une classe de décision ; -du changement de point de vue du décideur au cours du processus de décision. Lors des expérimentations sur le terrain, nous avons constaté que le décideur pouvait changer d'avis concernant l'évaluation d'une connaissance sur un critère donné ; -de l'incohérence de la famille de critères : un critère manquant, un critère en trop.
Une fois l'origine de l'incohérence déterminée, l'homme d'étude la corrige avec le déci-deur. Il procède de manière itérative tant que des incohérences sont identifiées dans la base de règles, et tant que le décideur a l'intention de modifier les exemples d'affectation et/ou les critères. Ce processus itératif permet alors d'obtenir une meilleure compréhension des règles de décision choisies par chaque décideur. Pour chaque décideur, deux types de règles de décision sont déterminés, les règles couvrant les connaissances appartenant avec certitude à la classe de décision " connaissance cruciales " et les règles couvrant les connaissances qui peuvent appartenir à la classe " connaissances cruciales ". Parmi ces règles, l'homme d'étude ne doit retenir que celles couvrant les " connaissances de référence " appartenant avec certitude à la classe de décision " connaissances cruciales ". Après une analyse des différentes règles inférées, un ensemble de règles est retenu pour chaque décideur. Une règle de décision a la forme suivante :
Avec g 1 , · · · , g m est une famille de critères, (r g1 , · · · , r gm ) Vgm sont les valeurs d'évalua-tion d'une connaissance sur les critères.
Détermination des règles collectivement acceptées
L'homme d'étude définit un ensemble de règles unique correspondant à l'ensemble des règles collectivement acceptées par les décideurs à partir des règles de décision retenues pour chacun d'eux. Dans le système K-DSS Saad (2005) nous avons suggéré que l'homme d'étude utilise la technique de " comparaison " proposée par Belton et Pictet (1997). Ainsi, il aide les décideurs à se concerter pour déterminer un ensemble collectivement accepté, à partir des différentes règles retenues par chacun d'eux. La qualité de l'ensemble de ces règles doit être vérifiée en les testant sur des exemples d'affectation des nouvelles connaissances par les mêmes décideurs. Cette technique de concertation n'est pas efficace pour évaluer un nombre important de connaissances.
Le système multi-agent
Dans notre contexte, les systèmes multi-agents s'avèrent d'un grand intérêt. D'une part, grâce à leur caractère autonome, les agents sont capables de représenter fidèlement les acteurs humains. D'autre part, nous considérons qu'un processus automatisé est adéquat à notre problématique vu le nombre important de connaissances à analyser et la difficulté de rassembler les décideurs humains pour argumenter sur toutes les affectations qui sont sources de conflit. Notre système multi-agents se compose d'un agent médiateur et de N agents décideurs :
1. l'agent médiateur m responsable de la gestion de la base de connaissances. Son but consiste à aboutir à une base de connaissances cohérente, il a pour rôle de détecter les conflits d'opinion, de mettre en contact les agents décideurs qui sont à l'origine de ces conflits. Si un accord ne peut être atteint entre agents décideurs, l'agent médiateur se charge grâce à ses méta-règles de prendre une décision objective quant à la classification en question. L'agent médiateur repose sur deux modules : un module de communication et un module de décision. Le module de communication se charge de l'échange de messages avec les autres agents du système. Le module de décision a pour rôle la résolution de conflits entre agents décideurs en ayant recours à une base de méta-règles. Notons que seul l'agent médiateur a le droit d'accès en mise à jour à la base de connaissances collective. La notion de méta-règle sera approfondie dans la section 4.4.
2. les agents décideurs a i qui sont responsables de l'affectation des connaissances en fonction de leurs croyances. Chaque agent décideur représente un décideur humain et détient une base de règles individuelle lui permettant de procéder à la classification et à l'argumentation. Les agents décideurs impliqués dans le processus de classification des connaissances ont un même objectif final : partager une base de connaissances cohérente. Les agents décideurs reposent chacun sur trois modules interdépendants : un module de communication permettant l'échange de messages avec les autres agents du système, un module d'inférence chargé d'inférer les règles de la base de règles individuelle et de déduire la classification à effectuer sur chaque connaissance et un module d'argumentation capable de construire les arguments en fonction des affectations conflictuelles. Le module de communication est en relation avec le module d'argumentation afin de construire les messages à envoyer aux autres agents décideurs.
Le module d'argumentation est en relation avec le module d'inférence capable de lui fournir les arguments qui appuient une affectation donnée.
FIG. 1 -Architecture multi-Agent
Processus et Protocole de résolution de conflits
Notations préliminaires -Notons a 1 , a 2 . . . a n les agents décideurs impliqués dans le processus de classification des connaissances. 
Protocole de communication
Le protocole de communication spécifie les actions que les agents sont autorisés à entreprendre, leur syntaxe et leur conditions. Les primitives de dialogue relatives au processus de classification des connaissances sont fournies dans la figure 2. Le processus d'argumentation est initié par l'agent médiateur qui, dès détection d'un conflit (cf. Définition 2), envoie un message call ? f or ? arguments aux agents concernés les appelant à argumenter leurs dé-cisions de classifications respectives. Après avoir reçu cet appel, les deux agents commencent le processus d'argumentation proprement dit qui peut être vu comme un échange de messages justif y achevé par un message accept ou un message reject.
Un message d'acceptation établit qu'un accord a été atteint par les agents décideurs en question. Un message de rejet établit qu'un accord n'a pas pu être atteint, ce qui implique que l'agent médiateur doit prendre une décision objective en tenant en compte sa propre base de méta-règles.
Algorithme de l'agent médiateur
L'agent médiateur a pour rôle de résoudre les conflits entre classifications en se basant sur les méta-règles qu'il détient et qui permettent d'établir des classifications objectives.
La figure 3 montre le graphe d'états de l'agent médiateur. A la détection d'un conflit, l'agent médiateur envoie un message call-for-arguments aux agents concernés et reste en attente. A l'issue du processus argumentatif, les agents décideurs concernés informent l'agent médiateur de leur décision. S'il reçoit un message accept alors le processus est terminé et l'affectation convenue est établie par l'agent médiateur. En revanche, si un message reject est envoyé, l'agent médiateur est appelé à prendre une décision objective quant à l'affectation de la connaissance et ce en se référant à ses méta-règles tel que détaillé dans la section suivante.
Méta-règles
La figure 4 représente les critères de classification des connaissances ainsi que les règles qui leur sont associées. Une méta-règle consiste ainsi à déterminer un poids ? i associé à chaque 
U i étant une fonction de scores qui réduit les critères à une même échelle [O,100].
x ? i étant la valeur de l'affectation ? sur le critère i. 
FIG. 4 -Critères de classification et règles associées
Expérimentations et résultats
Dans cette section, nous évaluons l'apport d'un système multi-agents automatisé dans un processus de classification collaborative des connaissances. Pour ce faire, nous avons implé-menté une plateforme sous Java composée de cinq modules : -un module de représentation des connaissances : responsable de la représentation des connaissances par leur nom, leur type ainsi que leur contenu ; -un module agents : responsable de la représentation des agents par plusieurs informations : le nom, la force, la base de règles associée, les affectations qu'il a faites, . . .
-un module d'argumentation : responsable de la représentation, de l'évaluation et de la construction des arguments ; -un module de génération aléatoire : offre des outils de génération de données aléatoires avec respect aux domaines de définition (discret ou continu) ; -un module de test : permet le paramétrage et la conduite des expérimentations. Nous avons conduit des expérimentations basées sur des données générées aléatoirement afin d'évaluer l'impact d'un nombre croissant de connaissances à classer sur le nombre de conflits. Notre approche expérimentale a pour objectif de comparer une approche multi-agent argumentative à une approche non argumentative. Les entrées de nos simulations peuvent être résumées en ce qui suit :
-10 agents décideurs représentés par une qualité d'approximation générée aléatoirement ; -100 connaissances à classer ; -1000 affectations de connaissances en deux classes de décision. Afin de collecter les résultats préliminaires issus de nos premières expérimentations, nous avons basé le processus argumentatif sur la base unique de la qualité d'approximation des agents décideurs. Ainsi, un agent accepte un argument uniquement lorsque l'agent émetteur a une qualité d'approximation meilleure que la sienne. Cette approche, bien que réductrice, nous permet d'avoir une première appréciation de l'apport d'une approche multi-agents dans notre contexte d'étude. Les premiers résultats montrent que pour tous les tests conduits, le nombre de conflits décroit en utilisant une approche argumentative. La figure 5 montre une comparaison entre une approche argumentative et une approche non argumentative. Nous avons noté que, dans la majorité des cas, le nombre de conflits a significativement baissé. Par exemple, nous pouvons baisser le nombre de conflits, en utilisant une approche argumentative, de 162 à 17. Par ailleurs, il convient de signaler que le nombre de conflits n'a pas baissé dans tous les cas, ce qui est dû à une égalité des qualités d'approximation des agents concernés.
FIG. 5 -Comparaison entre une approche argumentative et une approche non argumentative
Pour évaluer l'impact d'un nombre croissant de connaissances sur le nombre de conflits, nous avons conduit des séries d'expérimentations en faisant varier le nombre de connaissances à classer. nous observons sur la figure 6, au delà du fait que le nombre de conflits est nettement inférieur en utilisant une approche argumentative, qu'une approche non argumentative est plus sensible à une variation du nombre de connaissances à classer qu'une approche argumentative.
FIG. 6 -Impact du nombre de connaissances sur le nombre de conflits
Conclusion
Nous avons présenté dans cet article une approche multi-agent argumentative pour la classification collaborative des connaissances cruciales. Après avoir présenté le système multiagents ainsi que l'approche de classification des connaissances, nous avons conduit une approche expérimentale qui a permis de mettre en lumière l'apport d'une approche multi-agent argumentative dans un contexte de classification collaborative des connaissances cruciales.
Dans de travaux futurs, nous visons à proposer des expérimentations plus approfondies qui visent à valider l'approche proposée sur des données issues d'un contexte réel et s'appuyant sur plusieurs critères.

Introduction
La classification non supervisée, ou clustering, est un outil très performant pour la détec-tion automatique de sous-groupes pertinents (ou clusters) dans un jeu de données, lorsqu'on n'a pas de connaissances a priori sur la structure interne de ces données. Les membres d'un même cluster doivent êtres similaires entre eux, contrairement aux membres de groupes diffé-rents (homogénéité interne et séparation externe). La classification non supervisée joue un rôle indispensable pour la compréhension de phénomènes variés décrits par des bases de données. Un problème de regroupement peut être défini comme une tâche de partitionnement d'un ensemble d'items en un ensemble de sous-ensembles mutuellement disjoints. La classification est un problème de regroupement qui peux être considéré comme un des plus compétitifs en apprentissage non-supervisé. De nombreuses approches ont été proposées (Jain et Dubes, 1988). Les approches les plus classiques sont les méthodes hiérarchiques et les méthodes partitives.
Les méthodes de classification hiérarchiques agglomératives (CAH) utilisent un arbre hiérar-chique (dendrogramme) construit à partir des ensembles à classifier. Dans ce cas, les noeuds de l'arbre issus d'un même parent forment un groupe homogène (Ward, 1963), alors que les méthodes partitives regroupent des données sans structure hiérarchique.
Une méthode efficace utilisée pour la classification est la carte auto-organisatrice ou Self Organizing Map (SOM : Kohonen, 1984Kohonen, , 2001. Une SOM est un algorithme neuro-inspiré qui, par un processus non-supervisé compétitif, est capable de projeter des données de grandes dimensions dans un espace à deux dimensions. Cet algorithme d'apprentissage non supervisé est une technique non linéaire très populaire pour la réduction de dimensions et la visualisation des données. Dans cette approche, la détection des regroupements est en général obtenue en utilisant d'autres techniques de classification telles que K-Moyennes ou des méthodes hié-rarchiques. Dans la première phase du processus, une SOM standard est utilisée pour estimer les référents (moyennes locales). Dans la deuxième phase, les partitions associées à chaque référent sont utilisées pour former la classification finale des données en utilisant une méthode de classification traditionnelle. Une telle approche est appelée méthode à deux niveaux. Dans cet article, nous nous intéressons particulièrement aux algorithmes de classification à deux niveaux.
Une des questions les plus importantes pour la plupart des applications réelles, aussi connue comme le "problème de sélection de modèle", est de déterminer un nombre approprié de groupes. Sans connaissances a priori il n'y a pas de moyen simple pour déterminer ce nombre. L'objectif de ce travail est de fournir une approche de classification à deux niveaux simultanés utilisant une SOM, qui peut être appliquée à de grandes bases de données. La méthode proposée regroupe automatiquement les données, c'est-à-dire que le nombre de groupes est dé-terminé automatiquement pendant le processus d'apprentissage, i.e. aucune hypothèse a priori sur le nombre de groupes n'est exigée. Cette approche a été évaluée sur un jeu de problèmes fondamentaux pour la classification et montre d'excellents résultats comparés aux approches classiques.
Le reste de cet article est organisé comme suit. La Section 2 présente l'algorithme de classification à deux niveaux simultanés. La Section 3 décrit les bases de données utilisées pour la validation ainsi que le protocole expérimental. Dans la section 4 nous présentons les résultats de la validation et leur interprétation. Une conclusion et des perspectives sont données dans la section 5.
2 Algorithme de classification à deux niveaux simultanés basé sur une carte auto-organisatrice Dans un espace de grande dimension les données peuvent être fortement dispersées, ce qui rend difficile pour un algorithme de classification la recherche de structures dans les données. En réponse à ce problème, un grand nombre d'approches basées sur une réduction de dimensions ont été développées et testées pour différents domaines d'application. L'idée principale de ces approches est de projeter les données dans un espace de faible dimension tout en conservant leur topologie. Les nouvelles coordonnées des données ainsi projetées peuvent alors être efficacement utilisées par un algorithme de classification. C'est ce qu'on appelle la classification à deux niveaux. De nombreuses approches ont été proposées pour résoudre des problèmes de classification à deux niveaux (Bohez, 1998;Hussin et al., 2004;Ultsch, 2005;Guérif et Bennani, 2006;Korkmaz, 2006). Les approches basées sur l'apprentissage d'une carte autoorganisatrice sont particulièrement efficaces du fait de la vitesse d'apprentissage de SOM et de ses performances en réduction de dimensions non linéaire (Hussin et al., 2004;Ultsch, 2005;Guérif et Bennani, 2006).
Bien que les méthodes à deux niveaux soient plus intéressantes que les méthodes classiques (en particulier en réduisant le temps de calcul et en permettant une interprétation visuelle de l'analyse, Vesanto et Alhoniemi (2000)), la classification obtenue à partir des référents n'est pas optimale, puisqu'une partie de l'information à été perdue lors de la première étape. De plus, cette séparation en deux étapes n'est pas adaptée à une classification dynamique de données qui évoluent dans le temps, malgré des besoins importants d'outils pour l'analyse de ce type de données. Nous proposons donc ici un nouvel algorithme de classification non-supervisée, S2L-SOM (Simultaneous Two Level -SOM), qui apprend simultanément les prototypes (référents) d'une carte auto-organisatrice et sa segmentation.
Principe
Une SOM est un algorithme d'apprentissage compétitif non-supervisé à partir d'un réseau de neurones artificiels (Kohonen, 1984(Kohonen, , 2001. Lorsqu'une observation est reconnue, l'activation d'un neurone du réseau, sélectionné par une compétition entre les neurones, a pour effet le renforcement de ce neurone et l'inhibition des autres (c'est la règle du "Winner Takes All"). Chaque neurone se spécialise donc au cours de l'apprentissage dans la reconnaissance d'un certain type d'observations. La carte auto-organisatrice est composée d'un ensemble de neurones connectés entre eux par des liens topologiques qui forment une grille bi-dimensionnelle. Chaque neurone est connecté à n entrées (correspondant aux n dimensions de l'espace de représentation) selon n pondérations w (qui forment le vecteur prototype du neurone). Les neurones sont aussi connectés à leurs voisins par des liens topologiques. Le jeu de données est utilisé pour organiser la carte selon les contraintes topologiques de l'espace d'entrée. Ainsi, une configuration entre l'espace d'entrée et l'espace du réseau est construite ; deux observations proches dans l'espace d'entrée activent deux unités proches sur la carte. Une organisation spatiale optimale est déterminée par la SOM à partir des données et quand la dimension de l'espace d'entrée est inférieure à trois, aussi bien la position des vecteurs de poids que des relations de voisinage directes entre les neurones peuvent être représentées visuellement. Le neurone gagnant met à jour son vecteur prototype, de façon à devenir plus sensible à une pré-sentation future de ce type de donnée. Cela permet à différents neurones d'être entrainés pour différents types de données. De façon à assurer la conservation de la topologie de la carte, les voisins du neurone gagnant peuvent aussi ajuster leur vecteur prototype vers le vecteur pré-senté, mais dans un degré moindre, en fonction de leurs distances au prototype gagnant. Ainsi, les prototypes les plus proches d'une donnée correspondent à des neurones voisins sur la carte. En général, on utilise pour cela une fonction de voisinage gaussienne à symétrie radiale K ij .
Dans l'algorithme S2L-SOM, nous proposons d'associer à chaque connexion de voisinage une valeur réelle qui indique la pertinence des neurones connectés. Étant donné la contrainte d'organisation de la carte, les deux meilleurs représentants de chaque donnée sont reliés par une connexion topologique. Cette connexion sera "récompensée" par une augmentation de sa valeur, alors que toutes les autres connexions issues du meilleur représentant seront "punies" par une diminution de leurs valeurs. Les récompenses et les punitions sont d'autant plus importantes que l'apprentissage est bien avancé et donc que la structure de la carte est bien représentative de la structure des donnés.
Ainsi, à la fin de l'apprentissage, un ensemble de prototypes interconnectés sera représen-tatif d'un sous-groupe pertinent de l'ensemble des données : un cluster.
Algorithme S2L-SOM
L'apprentissage connexionniste est souvent présenté comme la minimisation d'une fonction de coût. Dans notre cas, cela correspond à la minimisation de la distance entre les données et les prototypes de la carte, pondérée par une fonction de voisinage K ij (Kohonen, 2001). Pour ce faire, nous utilisons un algorithme de gradient. La fonction de coût à minimiser est définie par :
Avec N le nombre de données, M le nombre de neurones de la carte, w .j = (w 0j , w 1j , ..., w nj ), N (x (k) ) est le neurone dont le vecteur prototype est le plus proche de la donnée x (k) . K ij est une fonction symétrique positive à noyau : la fonction de voisinage. L'importance relative d'un neurone i comparé à un neurone j est pondérée par la valeur de K ij , qui peut être définie ainsi :
?(t) est une fonction de température qui contrôle l'étendue du voisinage qui diminue avec le temps t de ? i à ? f (par exemple ? i = 2 à ? f = 0,5) :
t max est le nombre maximum d'itérations autorisé pour l'apprentissage. d 1 (i, j) est la distance de Manhattan définie entre deux neurones i (de coordonnée (k, m)) et j (de coordonnée (r, s)) sur la grille de la carte :
Le processus d'apprentissage de S2L-SOM est proche d'un "Competitive Hebbian Learning" (CHL, Martinetz, 1993). La différence essentielle est qu'un CHL ne change pas les ré-férences des prototypes en cours d'apprentissage. De plus, avec S2L-SOM, seuls les neurones voisins sur la carte peuvent être connectés, ce qui conserve la topologie en deux dimensions de la carte et permet une réduction de dimensions et une visualisation simple de la structure des données. Par ailleurs, l'utilisation d'une valeur de récompense associée aux connexions donne une information sur la représentativité locale des deux neurones connectés, ce qui n'est pas le cas avec un CHL. Martinetz (1993) a montré que le graphe généré de cette manière préserve la topologie de façon optimale. En particulier chaque arc de ce graphe suit la triangulation de Delaunay correspondant aux vecteurs de référence.
L'algorithme S2L-SOM procède essentiellement en trois phases :
-Définir la topologie de la carte.
-Initialiser aléatoirement tous les prototypes w .j = (w 0j , ..., w nj ) pour chaque neurone j. -Initialiser les connexions ? entre chaque couple de neurones i et j :
) pour repré-senter cette donnée :
-Augmenter la valeur de la connexion entre
les valeurs des autres connexions issues de
Avec : -Mettre à jour les prototypes w .j de chaque neurone j selon la règle d'adaptation suivante :
où ?(t) est le pas du gradient 4. Répéter les phases 2 et 3 jusqu'à ce que t = t max .
À la fin de l'apprentissage, chaque ensemble de neurones connectés entre eux par des connexions de valeurs positives est representatif d'un groupe homogène de données. L'algorithme attribue un numero à chacun de ces ensemble. Le nombre de groupe est ainsi obtenu automatiquement.
Validation
Description des bases de données utilisées
De façon à démontrer les performances de l'algorithme de classification proposé, huit bases de données présentant différentes difficultés de classification ont été utilisées.
Les bases de données "Hepta", "Chainlink", "Atom" et "TwoDiamonds" proviennent du Fundamental Clustering Problem Suite (FCPS Ultsch, 2005). Nous avons généré aussi quatre autres bases de données intéressantes ( "Rings", "Spirals", "HighDim" et "Random"). "Rings" est composée de 3 groupes en 2 dimensions non linéairement séparables et de densité et variance différentes : un anneau de rayon 1 pour 700 points (forte densité), un anneau de rayon 3 pour 300 points (faible densité) et un anneau de rayon 5 pour 1500 point (densité moyenne). "HighDim" est constitué de 9 groupes de 100 points chacun bien séparés dans un espace à 15 dimensions. "Random" est un tirage aléatoire de 1000 points dans un espace à 8 dimensions. Enfin "Spirals" est constitué de deux spirales parallèles de 1000 points chacune dans un anneaux de 3000 points. La densité des points dans les spirales diminue avec le rayon.  
Protocole expérimental
Nous avons comparé les performances de S2L-SOM en terme de qualité de la segmentation et de stabilité par rapport aux performances des méthodes classiques à un ou deux niveaux. Les algorithmes de comparaison choisis sont K-Moyennes, SingleLinkage et Ward appliqués sur les données ou sur les prototypes de la carte après apprentissage. L'indice de Davies-Bouldin (Davies et Bouldin, 1979) est utilisé pour déterminer le meilleur découpage des arbres (SingleLink et Ward) ou le nombre optimal K de centroïdes pour K-Moyennes. S2L-SOM détermine automatiquement le nombre de classes et n'a pas besoin d'utiliser cet indice.
Dans cet article la qualité de la segmentation à été évaluée à partir d'indices externes (indices de Rand et Jaccard) fréquemment utilisés (Halkidi et al., 2001(Halkidi et al., , 2002. En effet, si des catégo-ries indépendantes des données sont connues, on peut demander si la classification obtenue correspond à ces catégories. Rand = a 00 + a 11 a 00 + a 01 + a 10 + a 11 Jaccard = a 11 a 01 + a 10 + a 11
Ici a 00 est le nombre de paires d'objets dont les deux éléments sont dans la même catégorie et le même cluster. a 01 est le nombre de paires d'objets dont les deux éléments sont dans la même catégorie mais pas le même cluster, alors que a 10 est le nombre de paires d'objets dont les deux éléments sont dans le même cluster mais pas la même catégorie. Pour finir, a 11 est le nombre de paires d'objets dont les deux éléments ne sont ni dans le même cluster, ni dans la même catégorie.
Nous avons aussi utilisé pour ces travaux les indices internes de Davies-Bouldin (1979) et Calinski-Harabasz (1974). Ici la principale question est d'estimer à quel point une segmentation des données correspond à sa structure interne. En l'absence de connaissances a priori cette segmentation peut toujours être évaluée par des critères internes comme l'homogénéité intra-groupes et la séparation entre groupes. Les valeurs de ces indices ont été normalisés entre 0 et 1 pour chaque test pour une meilleure lisibilité.
L'indice suggéré par Davies et Bouldin (1979) pour différentes valeurs de K (nombre de cluster) est typiquement introduit comme suit pour évaluer les concepts de séparation entre groupes (le dénominateur) et l'homogénéité intra-groupes (le numérateur). Soit s i la racine carrée de l'erreur standard (variance intra-groupes) du groupe i de centroïde c i : de Calinski et Harabasz ((1974) est le plus largement utilisé dans les méthodes de classification classiques. Il peut être défini comme suit :
Avec N le nombre de points de données et K le nombre de groupes. trace(W ) est la somme des distances carrées inter-groupes alors que trace(B) est la somme des distances carrées intra-groupes. Cet indice présente une valeur élevée lorsque le nombre de groupes est optimum.
Le concept de stabilité est aussi utilisé pour estimer la validité de la segmentation. Pour éva-luer la stabilité des différents algorithmes, nous utilisons une méthode de sous-échantillonnage (Ben-Hur et al., 2002). Pour chaque base de donnée, chaque sous-échantillon est segmenté par un algorithme de classification, et nous comparons deux à deux avec l'indice de Jaccard les différences entre les segmentations obtenues. Ce processus est répété un grand nombre de fois et la moyenne de l'indice est considérée comme une estimation fiable de la stabilité de la classification.
Les résultats pour les indices externes montrent que pour ces données S2L-SOM est capable de retrouver sans aucune erreur la segmentation attendue. Ce n'est pas le cas des autres algorithmes, en particulier lorsque les groupes sont de formes arbitraires ou lorsqu'il n'y a pas de structure dans les données (voir figure 4(a)). Les segmentations obtenues par S2L-SOM sont d'excellentes qualités selon les indices internes lorsque les données sont regroupées en amas compacts et plus ou moins hypersphériques ("Hepta" ou "HighDim" par exemple). Par contre ces indices ne sont pas du tout adaptés à des groupes de formes arbitraires ("Rings", "Spirals", "Chainlink" ou "Atom", figure 4(b)), ce qui explique les mauvaises performances de S2L-SOM pour ces données. Il faut noter aussi que nous ne pouvons pas évaluer de cette façon une segmentation en un seul groupe, comme c'est le cas pour la segmentation des données "Random" par S2L-SOM. En ce qui concerne la stabilité (figure 5), S2L-SOM montre des résultats excellents pour les données regroupées en hypersphères, quelle que soit la dimension ("Hepta" et "HighDim"), mais aussi dans les cas où les groupes sont de formes arbitraires en deux dimensions ("Rings" et "Spirals") et lorsque les données ne sont pas structurées ("Random"). On remarque que dans ce dernier cas la segmentation obtenue par les méthodes classiques est extrêmement instable. Lorsque les données ne sont pas linéairement séparables dans des dimensions supérieures à deux ("Atom" et "Chainlink"), l'algorithme est limité par la contrainte topologique en deux dimensions de la carte auto-organisatrice et la stabilité de la segmentation n'est pas maximale. On peut cependant noter que même dans ce cas S2L-SOM reste plus stable que la quasi totalité des méthodes classiques. Par contre, tout en présentant une stabilité relativement élevée, S2L-SOM est moins stable que la plupart des méthodes classiques lorsque les groupes pressentent un point de contact ("Diamonds"). En effet, ce point de contact favorise la création et l'augmentation par l'algorithme de la valeur des connexions entre les deux groupes. La visualisation des groupes obtenus confirme ces résultats. En effet, l'algorithme S2L-SOM est un puissant outil pour la visualisation en deux dimensions de la segmentation obtenue. Les groupes sont aisément et clairement identifiables, ainsi que les zones sans données. Tel qu'on peut le voir sur la figure 6, les résultats obtenus avec l'algorithme S2L-SOM sont plus proches de la réalité que ceux obtenus par des méthodes classiques. Dans cet article, nous proposons une méthode de classification à deux niveaux simultanés. On utilise une SOM comme technique de réduction de dimensions et effectue en parallèle une classification optimisée. Les performances de cette méthode ont été évaluées à partir de tests sur une série de problèmes fondamentaux pour la classification, et comparées aux méthodes à deux niveaux classiques s'appuyant sur CAH ou K-Moyennes. Les résultats expérimentaux démontrent que l'algorithme proposé produit une classification de meilleure qualité que les approches classiques. Ils montrent aussi que le grand avantage de l'algorithme S2L-SOM est qu'il n'est pas limité aux groupes de formes convexes, mais est capable d'identifier des groupes de formes arbitraires. Pour finir, le nombre de groupes est déterminé automatiquement dans notre approche pendant l'apprentissage, c'est-à-dire qu'aucun a priori sur ce nombre n'est requis.
Cependant, cette méthode ne peut fonctionner que si les clusters sont suffisamment séparés dans l'espace de données. En effet, des groupes qui se touchent ne sont définis que par une diminution de la densité dans la zone de contact, ce qui ne peut pas être détecté par S2L-SOM. Dans le futur, nous prévoyons donc d'utiliser des informations sur la densité des données pour améliorer les performances de l'algorithme. Nous prévoyons aussi d'incorporer de la plasticité à l'algorithme S2L-SOM, pour rendre le modèle incrémental et évolutif.

Introduction
Le succès récent des représentations géographiques technologiques réalistes illustré par l'omniprésence des globles virtuels de type Google Earth dans notre quotidien, ne doit pas pour autant nous faire oublier l'importance des représentations abstraites pour l'analyse et la compréhension de phénomènes spatiotemporels complexes. L'intérêt des représentations abstraites réside souvent dans leur capacité à se détacher partiellement des contraintes liées à l'espace euclidien, en favorisant ainsi la prise en compte de dimensions non spatiales mais néanmoins fondamentales. Ces formes abstraites offrent en effet plus de flexibilité que les représentations réalistes pour communiquer simultanément les trois aspects du schéma « triad spatio-temporel » (Peuquet 1994) que sont les dimensions spatiale (où), temporelle (quand) et thématique (quoi).
Géovisualisation et spatio-temporalité
Géovisualisation : présentation
La Visualization in Scientific Computing (ViSC), qui est apparue à la fin des années 80 (McCormick et al. 1987), correspond à la création de représentations visuelles facilitant la réflexion et la résolution de problèmes à l'aide de technologies sophistiquées (Hearnshow and Urwin 1994). Cette approche s'est avérée particulièrement bien adaptée dans différents domaines tels que : la représentation d'objets physiques sous des angles ne pouvant être visualisés dans la réalité (ex. les couches géologiques) ; l'exploration de bases de données complexes à l'aide de métaphores visuelles (Peuquet 2002) ; la production d'hypothèses dans les démarches exploratoires (Goodchild and Janelle 2004) ; ou encore pour pallier certaines limites inhérentes aux méthodes d'inférence statistique ou d'analyse multivariée (Gahegan 2000 ;Koua and Kraak 2004). Stan Openshaw et al. (1994) estiment même que les résultats obtenus à l'aide de la visualisation peuvent servir de base pour le développement de modèles ou théories, voire même pour produire des conclusions suffisantes qui ne requièrent pas nécessairement d'autres formes d'analyse. Ces différents éléments contribuent à faire de l'analyse visuelle de larges bases de données un axe de recherche majeur des sciences de l'information géographique (Koua and Kraak 2004), généralement regroupé sous le terme de géovisualisation.
La géovisualisation se rapporte plus particulièrement à la visualisation de données ayant une dimension géographique. La géovisualisation cherche à rendre visible les contextes et problèmes spatiaux. Elle se base sur les capacités de l'oeil et du cerveau à détecter des structures et des anomalies ainsi qu'à se remémorer des informations spatiales de manière à favo-riser l'analyse scientifique (Peterson 1994 ;Goodchild and Janelle 2004). Elle remplace ou complète la « visualisation mentale » lorsque celle-ci est rendue impossible par le volume et la complexité des données (McEachren 1995). Son principal intérêt réside dans sa capacité à combiner la puissance informatique analytique avec l'aptitude de l'être humain pour l'interprétation des représentations graphiques de manière à favoriser l'émergence de phé-nomènes non anticipés (Peuquet and Kraak 2002).
Même si sa dimension visuelle confère à la géovisualisation un caractère quasi universel qui la destine à un public large et diversifié, il a été mis en évidence que les utilisateurs novices ont généralement plus de difficultés pour utiliser et comprendre des représentations abstraites (Bishop 1994;Peuquet 2002). Les formes abstraites de visualisation de l'information géographique sont donc principalement destinées à un public d'experts dans le domaine étudié pour lesquels elles se révèlent généralement plus efficaces que les formes réalistes (DiBiase et al. 1992;Dorling 1992).
D'après Ronald Finke et al. (1992) les images qui stimulent le plus l'imagination sont celles qui sont nouvelles, incongrues, abstraites et/ou ambiguës. La création de formes de géovisualisations abstraites et originales peut donc être envisagée comme un moyen de stimuler la réflexion et de favoriser la découverte. D'autre part, les formes abstraites offrent souvent plus de flexibilité pour représenter simultanément les trois aspects fondamentaux du schéma « triad spatio-temporel » que sont les dimensions spatiale (où), temporelle (quand) et thématique (quoi) (Peuquet 1994). La représentation simultanée de ces trois composantes est cruciale pour l'analyse des phénomènes spatio-temporels. Les réponses potentielles aux trois principales questions qu'elle implique -quoi ? quand ? où ? -participent de la puissance et de l'intérêt réel de la géovisualisation (McEachren 1995). Plus que l'une ou l'autre de ces composantes, c'est véritablement leur combinaison qui peut permettre une meilleure analyse des données existantes ainsi qu'une meilleure compréhension des phénomènes géospatiaux qu'elles sous-tendent. Cette représentation simultanée demeure complexe et par conséquent marginale.
Géovisualisation et analyse spatiotemporelle : exemples d'applications
Depuis le XVIIIe siècle et l'apparition des premières isochrones (cf. Husson 2004), les cartographes et ingénieurs ont développé des trésors d'ingéniosité pour intégrer le temps dans les cartes statiques (cf. Monmonier 1990 ;Kraak 2005) à l'image de la fameuse « Carte figurative des pertes successives en hommes de l'armée française dans la campagne de Russie, 1812-1813 » publiée en 1869 par Joseph Minard. Cette carte représente simultanément les dimensions temporelles, spatiales et quantitatives de l'évolution de l'armée napoléo-nienne au cours de la campagne de Russie. Elle privilégie la simplification du message au détriment de son exactitude géographique et préfigure ainsi le potentiel des cartogrammes pour la représentation et l'analyse simultanée des trois dimensions de l'information géogra-phique.
Le format numérique offre désormais de multiples potentialités pour représenter des phénomènes spatio-temporels sous des formes animées et/ou dynamiques (cf. Andrienko and Andrienko 2005). Ces représentations privilégient généralement la dimension spatiale au détriment de la dimension temporelle ou inversement. Il est donc possible de différencier ces formes visuelles en fonction de leur position le long d'un continuum espace / temps. À une extrémité de ce continuum se situent des représentations à forte dominance spatiale telles que Google Earth -qui offre désormais la possibilité de visualiser des séries temporelles -ou encore TimeMap qui permet de représenter des séries temporelles à partir de cartes conventionnelles (Johnson 2004). Le développement de ces applications augure d'une généralisation de l'intégration du temps dans les représentations cartographiques conventionnelles, malgré la complexité que cette intégration implique au niveau de la structure des bases de données géographiques (Peuquet 1994 ;Sellis 1999 ;Beard 2004).
Ces applications privilégiant la dimension spatiale sont particulièrement bien adaptées pour l'étude de phénomènes pour lesquels la distance physique est primordiale tels que ceux caractérisés par une auto corrélation spatiale forte (ex. hauteurs de précipitations). En revanche ils sont beaucoup moins pertinents dans les contextes nombreux pour lesquels les relations géographiques ne sont que partiellement influencées par la proximité spatiale tels que les échanges commerciaux de matières premières.
Les applications situées à l'autre extrémité du continuum espace / temps, c'est-à-dire celles privilégiant la dimension temporelle, fournissent ici des solutions intéressantes à l'image de l'application Gapminder (Gapminder 2007) qui présente sous une forme innovante, interactive, animée, ludique et extrêmement sophistiquée des séries statistiques temporelles à l'échelle du monde. Cette application qui utilise le logiciel Trendalyzer couplé à une interface développée en flash privilégie l'étude des changements temporels et des similarités entre entités géographiques distantes spatialement (Figure 1 3 Un cyber cartogramme gravitationnel : présentation
Contexte général de la cybercartographie
Un cartogramme est généralement considéré comme une abstraction de la réalité dans laquelle la surface des entités est proportionnelle a des données autres que géographiques (Dent 1990). Dans le cas du cartogramme présenté dans cet article, ni la taille, ni la forme, ni la localisation latitudinale des objets géographiques représentés -en l'occurrence les pays du monde -n'ont été conservées. Ce cartogramme a été conçu dans le cadre d'un large projet de recherche intitulé « Cybercartographie et la nouvelle économie » mené à l'université Carleton (Ottawa, Canada). Ce projet explore le concept de cybercartographie introduit et déve-loppé par Fraser Taylor (1997Taylor ( , 2003Taylor ( , 2005Taylor and Caquard 2006). Ce concept vise à fournir des éléments de réponse aux multiples défis contemporains auxquels fait face la cartographie, que ce soit du point de vue technologique, représentationnel, conceptuel, ou socié-tal. Ces défis sont notamment étudiés à travers le développement de différents atlas cybercartographiques. Ces atlas ont été conçus pour aborder les questions relatives à la mise à disposition d'informations géospatiales sur Internet sous forme multimédia, multi sensorielle, interactive et animée. Ils sont conçus pour être évolutifs et pour être développés de manière collaborative. Ils offrent en effet la possibilité à des individus et communautés de concevoir, créer et gérer leur propre atlas dédié aux thématiques et lieux de leurs choix.
Le cyber cartogramme a été initialement développé pour l'atlas cybercartographique du commerce canadien (cf. Eddy and Taylor 2005). L'objectif principal de cet atlas est de faire émerger des tendances et changements en termes d'échanges commerciaux à partir de l'analyse d'une base de données complexe et volumineuse. Cette base de données, compilée au fil des ans par Statistiques Canada, comprend la valeur en dollars canadiens des exportations et importations du Canada en direction de 120 pays du monde, pour une cinquantaine de critères sur une période de 25 ans , ce qui correspond à une base de données volumineuse.
Les représentations cartographiques conventionnelles se sont rapidement révélées limitées pour la représentation simultanée des dimensions spatiales, temporelles et thématiques de ces données. En effet, la prégnance visuelle des formes géographiques et des distances physiques inhérentes aux cartes conventionnelles est apparue surdimensionnée par rapport à leur importance réelle dans les échanges commerciaux. Par exemple, malgré sa proximité relative, la Russie ne constitue pas pour autant un partenaire commercial privilégié du Canada. Si la distance physique reste un des éléments affectant les échanges commerciaux, ceuxci sont aussi largement influencés par d'autres critères non spatiaux tels que les coûts énergé-tiques, les choix politiques ou stratégiques. La notion de « proximité économique » s'est donc avérée beaucoup plus pertinente que celle de proximité spatiale pour l'analyse des échanges commerciaux. Cette notion de proximité économique privilégie les caractéristiques commerciales au détriment de la distance géographique. Deux pays distants géographique-ment peuvent être proches économiquement du fait de leurs similarités commerciales. C'est cette volonté de représenter visuellement cette proximité économique qui a inspiré la conception du cyber cartogramme.
Ce cyber cartogramme a été conçu à l'aide de Nunaliit (http://nunaliit.org), un logiciel source ouverte développé spécifiquement pour la création d'atlas cybercartographiques. En Inuktitut, qui est le nom donné par les canadiens au dialecte Inuit, Nunaliit signifie « communauté », « implantation », « habitat ». Le choix de ce nom pour l'infrastructure cybercartographique illustre d'une part la dimension canadienne du projet et d'autre part sa finalité communautaire. Nunaliit est un logiciel qui aspire à être développé par une communauté d'informaticiens afin de permettre à des communautés d'auteurs de créer leurs propres atlas et de les mettre à la disposition de l'ensemble des communautés d'utilisateurs intéressés. Nunaliit a été conçu à partir de concepts et modèles développés tout au long du projet Cybercartographie et la Nouvelle Economie. Il intègre une partie des résultats obtenus au cours de ce projet. Ce logiciel vise à permettre à des auteurs sans connaissances informatiques poussées, de combiner relativement aisément des données géospatiales disponibles en ligne avec différents médias (ex. son, texte, narration, vidéo), de manière à produire de nouvelles formes d'exploration et d'expression géospatiale. Ces formes d'expression sont généralement regroupées au sein de « modules » qui correspondent à des composantes de l'atlas dédiées à l'étude d'un lieu et/ou d'un thème spécifique et qui combinent pour cela des éléments cartographiques, narratifs et multimédias. Nunaliit permet la création, la mise à jour et l'interconnexion des modules correspondants à autant de chapitres ou sections d'un même atlas.
De manière un peu plus spécifique, Nunaliit se présente comme un kit de développement logiciel composé d'un schéma, d'une librairie d'outils source ouverte et d'un compilateur. Le schéma dicte la structure des documents XML au sein desquels l'auteur de l'atlas définit les cartes, les géovisualisations, le texte et les éléments multimédias nécessaires à la création de son module. Chaque document XML comporte les liens et actions associés à chaque module. Ces liens et actions sont générés à l'aide d'une librairie d'outils incluant aussi bien des outils courants des SIG (ex. le zoom), que des outils beaucoup plus novateurs tels que la géosonori-sation (cf. Brauen 2006 ; Caquard et al. à paraître) ou le cyber cartogramme. C'est donc au sein du document XML que le contenu, la structure et la représentation du module sont défi-nis par l'auteur. Une fois créé ou modifié, le fichier XML est ensuite compilé par le compilateur qui le transforme en fichiers HTML dynamiques (DHTML) et SVG, produisant ainsi des documents hautement interactifs générés à partir de différentes applications géomatiques en ligne (cf. Figure 2)  
Les composantes du cyber cartogramme
Le cyber cartogramme correspond donc à un des outils de la librairie cybercartographique. Il est constitué de trois composantes principales que sont : (1) un espace géographique unidimensionnel représenté sous forme semi-circulaire et centré sur une origine servant de pôle (ex. le Canada) autour duquel viennent graviter (2) des entités géographiques (ex. pays). Ces entités se répartissent autour du demi-cercle en fonction de leur position géographique par rapport à l'origine. Elles se déplacent ensuite le long du rayon du cercle en direction de l'origine en fonction de valeurs attributaires (ex. le volume des échanges commerciaux). Enfin, (3) une ligne de temps interactive permet d'explorer la dimension temporelle de l'information. Chacune de ces entités est générée automatiquement à la volée par le logiciel Nunaliit.
L'espace géographique est donc matérialisé sous une forme circulaire correspondant à une représentation unidimensionnelle (1D) du monde centrée sur une entité géographique définie. Dans le cas de l'atlas du commerce canadien, le cartogramme est centré sur le Canada et le monde est représenté de manière semi-circulaire (cf. figure 3). Le choix d'une forme semi-circulaire, plutôt que circulaire, s'explique par la position septentrionale du Canada. Le Canada n'ayant pas de partenaires commerciaux au nord, il devenait inutile de représenter cette direction sur le cartogramme. Les partenaires commerciaux du Canada sont représentés par des points qui se répartis-sent autour de l'hémicycle en fonction de la position longitudinale de leurs centroïdes par rapport à celle du Canada (cf. Tab. 1). Ces points correspondent à la deuxième composante du cartogramme. Les pays situés à l'ouest du Canada (Asie) se retrouvent à gauche, ceux situés au Sud (Amérique) se retrouvent en bas et ceux situés à l'est (Europe, Afrique, Moyen Orient) se retrouvent à droite. Seule la dimension longitudinale est utilisée pour représenter l'espace géographique, ce qui permet de réserver la deuxième dimension du plan pour repré-senter des valeurs attributaires. Chaque point se positionne donc sur le rayon du demi-cercle en fonction d'une valeur attributaire : plus cette valeur est élevée et plus le point se rapproche du centre, c'est-à-dire plus le pays qu'il représente est proche économiquement et visuellement du Canada. Dans notre exemple la position de chaque pays est définie par le pourcentage de ses échanges commerciaux avec le Canada dans un domaine spécifique (ex. l'énergie) par rapport à l'ensemble des autres pays pour une année donnée. 
TAB. 1 -Méthode de calcul du positionnement des objets sur le cartogramme.
Différentes variables visuelles telles que la taille, la couleur, la transparence ou la valeur peuvent aussi être attribuées à chaque point multipliant ainsi les combinaisons possibles de données représentées simultanément. Dans notre exemple la taille est utilisée pour représen-ter le montant en dollars canadiens des échanges commerciaux de chaque pays avec le Canada (cf. Figure 4). Plus la taille des symboles s'accroît et plus les échanges commerciaux qu'ils représentent sont importants ; plus ces symboles se rapprochent du centre du demicercle et plus le rôle des pays qu'ils représentent est proportionnellement important par rapport aux autres pays. L'auteur peut aussi décider du seuil au-delà duquel les pays apparaissent sur le cartogramme. Ici, les pays totalisant moins de 1% des échanges commerciaux avec le Canada pour chaque année n'apparaissent pas pour des raisons de clarté et de lisibilité.
FIG. 4 -Chaque pays se positionne sur le rayon du demi-cercle en fonction d'une valeur attributaire (ici le pourcentage du commerce de chaque pays par rapport aux autres pays). La taille du point varie en fonction d'une autre valeur attributaire (ici le volume total des échanges en dollars canadiens). L'utilisateur peut sélectionner les données temporelles à l'aide d'une ligne de temps interactive générée automatiquement à la volée à partir du champ de la base de données sélectionné.
La troisième composante du cartogramme est la ligne de temps interactive. Tout comme les deux composantes précédentes cette ligne de temps est générée à la volée à partir de la sélection d'un champ de la base de données comportant des données temporelles. Cette ligne de temps se présente sous la forme d'un rectangle allongé de la longueur de la fenêtre de l'atlas subdivisé en autant de rectangles qu'il y a de valeurs temporelles dans le champ sélec-tionné. Dans notre exemple, la base de données comporte une série temporelle composée de 25 années. La ligne de temps est donc subdivisée automatiquement en 25 rectangles identiques. Chaque fois qu'une nouvelle année est ajoutée dans la base de données, un nouveau rectangle vient s'ajouter automatiquement à la ligne, réduisant ainsi proportionnellement la taille de chacun. Lorsque l'utilisateur clique sur un des rectangles matérialisant une année, les données relatives à cette année apparaissent sur le cartogramme. Lorsque les données temporelles sont trop nombreuses ou correspondent à des échelles temporelles différentes (ex. annuel Vs. mensuel) la ligne de temps peut être dédoublée afin de faciliter la visualisa-tion et l'analyse. La création automatisée de cette ligne de temps à partir de bases de données géographiques diverses, pouvant être distantes, ainsi que son haut niveau d'interactivité, en font un outil extrêmement utile pour l'intégration de la dimension temporelle dans l'analyse géographique.
La combinaison de ces trois composantes offre de multiples possibilités et présente diffé-rents avantages. D'un point de vue spatial elle favorise la comparaison intercontinentale (Asie, Etats-Unis, Europe) qui est un élément clé de l'analyse des relations commerciales du Canada. Chaque continent est matérialisé par une agrégation visuelle de différents points représentant chacun un pays. Chaque pays participe donc au regroupement visuel (clustering) nécessaire à la comparaison intercontinentale. À une échelle plus fine, chaque pays peut rapidement être identifié et comparé visuellement et quantitativement à tout autre pays.
La ligne de temps interactive permet à l'utilisateur de sélectionner une date de référence et de comparer visuellement la situation à d'autres dates. Le fait que les points se déplacent de manière linéaire le long d'un axe qui leur est propre (le rayon du cercle) accroît leur pré-gnance visuelle et favorise la mise en évidence des changements. En effet, comme le souligne Alan MacEachren (1995) sur une carte dynamique, les éléments qui bougent attirent plus l'attention que ceux qui ne bougent pas et ceux qui se déplacent attirent plus l'attention que ceux qui bougent sur place, c'est-à-dire que ceux qui se dilatent ou se rétractent. L'impression d'animation qui découle du déplacement des points le long des rayons du demi-cercle génère une variable cartographique supplémentaire représentant de manière intuitive les processus dynamiques et stimulant notre capacité à identifier les changements (MacEachren 1994). Enfin, le fait que les composantes du cartogramme (hémicycle, points et ligne de temps) soient indépendantes les unes des autres, tout en étant interconnectées, multiplie les possibilités combinatoires et étend ses domaines potentiels d'application.
Le cyber cartogramme : applications et discussion
Les échanges commerciaux canadiens relatifs aux produits de la mer
Une première phase de l'analyse exploratoire des échanges commerciaux du Canada a été de mettre en évidence les principales tendances spatio-temporelles pour chacune des théma-tiques étudiées. Pour cela, une première version du cartogramme a été utilisée. Elle comportait deux hémicycles (importations et exportations) sur lesquels se répartissaient les pays agrégés par grandes régions géographiques (Etats-Unis, Amérique du Sud, Afrique, Europe, Moyen Orient, Asie, Pacifique). Ce cartogramme a été utilisé pour analyser les 15 principales thématiques de la base de données de Statistiques Canada. Ce cartogramme exploratoire a permis de faire rapidement émerger différentes tendances spatio-temporelles du commerce canadien comme les variations interannuelles importantes dans l'origine de l'approvisionnement énergétique du Canada, comparées à la stabilité de ses exportations dans ce domaine en direction des Etats-Unis. Cette première version a donc permis d'identifier certaines thématiques pour lesquels un approfondissement de l'analyse pourrait s'avérer pertinent, ce qui a été fait de manière plus spécifique pour le commerce des produits de la mer.
Au Canada, l'industrie de la pêche commerciale représente annuellement plus de 5 milliards de dollars canadiens, ce qui en fait un des principaux producteurs mondiaux. Le Cana--10 -RNTI-E-13 da exporte près de 85% de ses produits de la pêche et importe annuellement environ 2 milliards de dollars ce qui contribue à faire de ce secteur un domaine économique stratégique (Agriculture et Agroalimentaire Canada 2007). Ce secteur est aussi fondamental d'un point de vue social puisqu'il emploie plus de 130 000 personnes et que 207 communautés à travers le pays étaient encore dépendantes de la pêche en 2001 (Atlas du Canada 2007). Un cyber cartogramme dédié à l'analyse des échanges commerciaux par pays a donc été conçu et inté-gré dans un module spécifique de l'atlas du commerce canadien dédié à cette thématique. Ce cartogramme a permis de faire apparaître de nombreuses tendances intéressantes telles que la réduction relative des exportations en direction de l'Europe au profit principalement du Japon à la fin des années 90 ou encore l'accroissement constant des importations en provenance de la Thaïlande durant la période 1985-1994 (cf. Figure 5).
FIG. 5 -Copie d'écran du cartogramme intégré dans le module de l'atlas du commerce canadien traitant de l'industrie de la pêche (http://gcrc.carleton.ca/graphomap ). Dans cet exemple, l'année sélectionnée est 1976 (points de couleur foncée) et elle est comparée à 2000 (points plus clairs) pour les exportations (en haut) et les importations (en bas). La taille des points représente les montants (en dollars canadiens) alors que la proximité du centre représente le pourcentage de chaque pays dans la valeur commerciale annuelle du Canada. Dans la partie droite de la fenêtre, apparaissent notamment les valeurs numériques des entités sélectionnées.
Au-delà de ces résultats, la capacité du cartogramme à faire émerger ces tendances a suscité l'intérêt de nombreux utilisateurs. Ce cartogramme a ainsi été utilisé de manière quasi exclusive par l'experte en commerce international chargée d'analyser les données et de mettre en évidence des structures et tendances. Ce cartogramme a par ailleurs fait l'objet d'une évaluation informelle qui s'est déroulée le 27 novembre 2007 dans les locaux de Statistiques -11 -RNTI-E-13 En effet, si ce cartogramme a été conçu pour l'analyse géographique de proximités éco-nomiques, il peut tout aussi bien être utilisé pour l'analyse de proximités culturelles, linguistiques, démographiques, sociales, économiques, environnementales ou écologiques. Celles-ci peuvent concerner des pays, aussi bien que des communautés (ex. communautés autochtones), des entités géographiques cohérentes (ex. agglomérations urbaines), administratives (ex. régions), environnementales (ex. parcs naturels), ou écologiques (ex. biotopes). La multiplicité des formes de proximité pouvant ainsi être générées favorise l'émergence de nouvelles perspectives sur les relations existantes entre des entités spatiales diverses et distantes.
Les limites du cyber cartogramme
Même si les possibilités combinatoires des dimensions thématiques, temporelles et spatiales qu'offre ce cartogramme apparaissent illimitées, cette application n'en est pas pour autant dénuée de limites. La première limite de ce cartogramme concerne la représentation d'objets géographiques de manière unidimensionnelle. Cette approche comporte en effet un risque évident de superposition d'entités éloignées latitudinalement mais situées sur la même longitude. C'est par exemple le cas entre l'Europe et l'Afrique et plus particulièrement entre des pays aussi distants géographiquement, économiquement, historiquement et culturellement que la Finlande et la République Démocratique du Congo. Ce problème reste néan-moins marginal dans notre application étant donnée la faible part des échanges commerciaux du Canada avec l'Afrique et par conséquent la présence limitée des pays Africains sur le cartogramme. Ce problème pourrait par ailleurs facilement être résolu par l'utilisation de couleurs ou formes distinctes pour différencier des entités géographiques se situant à des latitudes éloignées, ou sur des continents différents.
Une deuxième limite concerne les changements subits par les entités géographiques au cours du temps. Pendant les 25 années étudiées, certains pays ont disparu (ex. Yougoslavie), d'autres sont apparus (ex. Bosnie Herzégovine) et d'autres enfin se sont transformés (ex. l'URSS est devenue la Russie). Ces évolutions ont affecté les objets géographiques (ex. déplacement des centroïdes), et par conséquent leur position sur le cartogramme. Par exemple, au cours de la transformation de l'URSS en Russie le centroïde de ce pays s'est légère-ment déplacé vers l'est du fait de ses modifications territoriales, ce qui a affecté radicalement sa position sur le cartogramme : de manière quelque peu paradoxale, la Russie s'est retrouvée à l'extrême gauche du cartogramme alors que l'URSS était positionnée à l'extrême droite. Ce type de problème peut aussi être aisément réglé (ex. déplacement manuel du centroïde ou affectation d'un coefficient spatial) et demeure de toute façon inhérent à toute représentation spatiale de séries temporelles.
-12 -RNTI-E-13
Enfin une dernière limite concerne la complexité relative de la mise à jour des représenta-tions. Pour l'instant, les efforts concernant le développement du logiciel Nunaliit ont été principalement dévolus à assurer sa stabilité et son efficacité. Si l'objectif final est d'offrir aux auteurs d'atlas cybercartographiques une interface facile d'utilisation, il reste encore beaucoup de chemin à parcourir dans ce domaine. En effet, la création de modules se fait actuellement à partir d'un fichier XML qu'il faut modifier et compiler (cf. Figure 6). Par conséquent, la mise à jour des modules, et donc du cartogramme, nécessite une connaissance de base du langage XML. Même si l'utilisation de ce langage s'avère de plus en plus courante dans les applications géomatiques destinées à l'Internet, il n'en reste pas moins que son usage peut rebuter nombre de créateurs potentiels d'applications cybercartographiques. Ce problème devrait être résolu sous peu grâce au développement d'une interface de type wiki, matérialisant ainsi la véritable finalité communautaire de Nunaliit et des atlas cybercartographiques en général.
FIG. 6 -Exemple du fichier XML utilisé pour générer le cartogramme représentant les échanges commerciaux canadiens appliqués aux produits de la mer (cf. Figure 5).
Conclusion
Malgré les quelques limites qui viennent d'être évoquées, le cyber cartogramme présenté dans cet article constitue un véritable outil de géovisualisation, original et innovant permettant de représenter de manière simple, intuitive et automatisée, les trois composantes interreliées du schéma « triad spatio-temporel » -où, quand, quoi -à partir de données variées et interopérables. De plus, ce cartogramme développé en source ouverte offre la possibilité de décliner ces combinaisons sur de nombreux modes, permettant ainsi d'explorer de multiples formes de proximités appliquées à divers domaines. Cette diversité des applications potentielles de ce cartogramme est illustrée par son utilisation au sein de l'atlas cybercartographique du cinéma canadien en cours de développement (www.atlascine.org). Dans le contexte de cet atlas, le cartogramme est utilisé pour analyser l'évolution spatio-temporelle des recettes générées par différents films canadiens lors de leur sortie en salle. Ce cartogramme se caractérise donc par sa simplicité d'utilisation, son évolutivité et son adaptabilité aux besoins de ses utilisateurs potentiels, mais aussi par la multiplicité de ses domaines d'application.
Enfin, il est important de rappeler que la conception de ce cartogramme participe d'une logique de diversification des modes de représentation de l'information géographique de manière à favoriser : (1) l'exploration des structures et phénomènes géographiques sous différents angles à travers différentes formes de proximité ; (2) la revalorisation de l'abstraction comme moyen de stimuler l'intérêt et l'imagination des utilisateurs d'informations géographiques. Cette logique vise à fournir une alternative aux modèles ré-alistes dominants dans les sciences de l'information géographique et stimuler ainsi l'émergence de nouveaux modes d'expression géospatiale ; et (3) une approche communautaire de la construction du savoir géographique et des outils de géovisualisation permettant cette construction et sa diffusion. 
Références
Summary
The cartogram introduced in this paper has been designed to improve visual analysis of complex spatio-temporal data. It provides the means for representing simultaneously the three dimensions intrinsic to geospatial information: space (or location), theme (or event) and time (specific moments). The cartogram does so using three major components: (1) a onedimensional (1D) geographical space represented by a semi-circle centered on a specific location (e.g. Canada); (2) certain geographical entities (e.g. countries) that gravitate around this location according to specific attributes; and (3) an interactive timeline offering the possibility to explore the temporal dimension of the information. Together these three components provides a range of possibilities for analyzing different forms of spatio-temporal proximity, including proximity expressed in economic, cultural, social and demographic terms.
The potential this open source cartogram offers is illustrated by examples from the Cybercartographic Atlas of Canada Trade with the World. This article is based on a paper given at the SAGEO 2007 conference.

Introduction
Les mesures de similarité sont des éléments clés dans les algorithmes de traitement automatique des langues. Elles sont utilisées pour orienter le processus d'extraction de connaissance. Ainsi, elles sont les principales responsables des performances d'un algorithme. Si une mesure de similarité pertinente améliorera les performances, une mauvaise mesure risque de mener à des résultats incohérents. La définition d'une bonne mesure n'est pas un processus aisé. En effet, la mesure doit donner une bonne indication sur le degré de similarité entre deux documents. La notion de sémantique n'est pas clairement définie. Bien que nous essayons d'imiter la perception humaine, l'information sémantique peut prendre différente forme selon l'approche adoptée. Il existe deux grandes approches : l'une basée sur l'information statistique tel que la fréquence de co-occurrence des termes et l'autre basée sur des sources de connaissances externes telles que les ontologies.
Dans la communauté de l'apprentissage, les noyaux (Shawe-Taylor et Cristianini, 2004) sont utilisés depuis une décennie comme fonctions de similarité basées sur le cosinus formé par deux vecteurs. Les noyaux sont, en réalité, des produits scalaires définis dans un espace de Hilbert. Ils ont la spécificité de plonger, implicitement, les données dans un espace de Hilbert, dit espace de description, avant de calculer le produit scalaire. Les noyaux peuvent être intégrés dans tout algorithme d'apprentissage basé sur le produit scalaire tel que les Séparateurs à Vaste Marge (SVM) (Vapnik, 1995). Ainsi, ils étendent l'utilisation de l'apprentissage numérique aux tâches du TAL (Traitement Automatique des Langues). En effet, aucune contrainte n'étant imposée sur l'espace des données, les noyaux peuvent être définis pour tout type de données tel que les données textuelles. Le modèle d'espace vectoriel (VSM) (Salton et al., 1975), repré-sentant un document sous la forme d'un vecteur de fréquences de termes, est largement utilisé. Les noyaux basés sur ce modèle ont permis d'obtenir des résultats très prometteurs dans le domaine de la catégorisation de texte (Joachims, 2002(Joachims, , 1998.
Dans cet article, nous présentons un modèle d'espace vectoriel de concepts (CVSM) pour la représentation des documents textuels. Cette représentation, induite par des connaissances a priori, est présentée, ici, comme une alternative au modèle classique d'espace vectoriel. Le VSM se base uniquement sur la fréquence d'occurrence des termes. Pour le CVSM, une taxonomie de concepts linguistiques est utilisée comme source de connaissance pour définir l'espace vectoriel. De plus, nous proposons deux noyaux basés sur les concepts. Le premier noyau, le noyau CVSM linéaire, est défini dans le CVSM où chaque document est représenté par un vecteur de concept intégrant l'information sur la taxonomie des concepts. Le second noyau, le noyau CVSM latent, mélange l'approche agnostique basée sur l'information statistique et l'approche a priori utilisant des connaissances externes propres au domaine. Basé sur le noyau LSA (Cristianini et al., 2002), le noyau CVSM latent utilise une décomposition en valeurs singulières (SVD) pour découvrir des structures latentes entre les concepts linguistiques du CVSM.
L'utilisation des noyaux CVSM est illustrée par une tâche de catégorisation de texte dans le domaine biomédical. L'Unified Medical Language System (UMLS) est utilisé tant que source a priori de connaissances biomédicales pour l'extraction de concepts à partir documents textuels. Les performances de ces noyaux sont évaluées sur cette tâche en utilisant un classifieur SVM. Le corpus Ohsumed qui est connu pour être un corpus difficile, est utilisé pour l'évaluation expérimentale.
Pour raffiner le VSM, l'Analyse Sémantique Latente (LSA) (Deerwester et al., 1990) a été proposée. L'idée principale est de représenter un document dans le VSM non pas par ses termes mais par ces concepts. L'hypothèse est que les concepts sont plus adaptés pour modéliser le sens des documents que les termes. Dans la LSA, un espace sémantique de faible dimension est défini en appliquant une Décomposition en Valeurs Singulières (SVD) sur la matrice de termes par document. Les vecteurs documents du VSM sont alors projetés dans l'espace sémantique. En utilisant ce nouvel espace, un Noyau Sémantique Latent a été proposé dans (Cristianini et al., 2002). Ce noyau a été utilisé pour projeter les documents du VSM vers l'espace sémantique et calculer le produit scalaire. Le noyau a été utilisé avec succès sur une tâche de catégorisation de texte. Il a été montré que le noyau LSA peut atteindre les mêmes performances, dans un espace de très faible dimension, que le noyau du VSM utilisé dans (Joachims, 2002(Joachims, , 1998).
Bien que l'espace sémantique de la LSA permet d'obtenir de très bonnes performances, l'espace étant défini par des concepts statistiques, il est très difficile, linguistiquement, d'interpréter cet espace et les concepts.
Noyau linéaire du modèle d'espace vectoriel de concepts
Pré-traitement des documents
Les documents textuels sont formatés pour une utilisation humaine. Ainsi, ils contiennent une riche variété de symboles et de protocoles tels que les règles typographiques. Ces informations sont ajoutées pour rendre la lecture et la compréhension plus aisées. Toutefois, le traitement automatique de ces données est rendu compliqué. En effet, si les éléments d'un document ne sont pas correctement gérés, ils peuvent être une source de bruits et d'ambiguïtés qui entraînera inexorablement une baisse des performances du système. Une façon d'éviter ce problème est de pré-traiter le document pour avoir une représentation adaptée au système de traitement.
Dans le cadre de ce travail, nous utiliserons le pré-traitement suivant : 1. Nettoyage du document :Tous les éléments qui peuvent introduire du bruit sont éliminés. Ainsi, tous les nombres, aussi bien sous un format numérique que sous un format littéral et les données non-textuelles sont retirés. Il est à noter que nous utilisons le terme "bruit" dans un sens général. 2. Segmentation du texte : Le document est segmenté en unité lexicale composée d'un ou plusieurs mots. Pour cela, un lexique doit être utilisé. Dans notre, cas, le lexique médical "The Specialist Lexicon" de l'UMLS a été utilisé.
Suppression des mots vides :
Les unités lexicales ne contenant que des mots vides, à savoir, sans signification sont éliminés. 4. Normalisation des termes : Pour éviter les différentes formes fléchies des mots, un processus de normalisation est effectué à l'aide d'un lexique. La normalisation consiste à lemmatiser chaque mot d'une unité lexicale et à les ordonner alphabétiquement au sein de cette unité. Les unités lexicales absentes du lexique sont décomposées en unité lexicale composée d'un seul mot. Un processus de stemming est ensuite appliqué à chacun de ces mots. 5. Annotation Sémantique : Chaque unité lexicale est associée à un groupe de concepts (composé d'un ou plusieurs concepts). Cette étape nécessite l'utilisation d'un thésaurus. En outre, une taxonomie de relation "est-un" entre les concepts sera utilisée lors de la phase de traitement. Dans le cadre de notre travail, le Metathesaurus de l'UMLS a été utilisé. Ce thésaurus intègre une ontologie de concepts complexe. L'ontologie a été transformée en taxonomie en ne conservant que les relations "est-un" et en supprimant les cycles.
Le modèle d'espace vectoriel de concepts
Le modèle d'espace vectoriel (VSM) est basé sur la forme morphologique des termes. Il est, ainsi, hautement dépendant de la langue du texte et de la fréquence d'occurrence des termes. En effet, le VSM utilise simplement la fréquence des termes pour capturer l'information d'un document. Il est, ainsi, limité par le fait qu'il ne peut correctement gérer les termes synonymes et les termes polysémiques. De plus, les liens entre les termes sémantiquement proches ne peuvent être modélisés. Toutefois, l'espace à haute dimension induit par le VSM permet aux systèmes, utilisant le VSM, d'obtenir des performances qui sont parmi les meilleures (Joachims, 1998). Dans cette section, nous présentons le modèle d'espace vectoriel de concepts (CVSM), basé sur le VSM. Ce modèle devrait permettre de gérer les problèmes, rencontrés par le VSM, énumé-rés ci-dessus. Le CVSM est un espace vectoriel dans lequel chaque axe représente un concept défini dans un dictionnaire de concepts. Le dictionnaire est constitué des concepts définis dans un thésaurus et des mots racines, obtenus par stemming, lorsque ces mots ne peuvent être associés à des concepts. Nous prenons, alors, l'hypothèse que ces mots expriment un concept à part entière. Dans le CVSM, les documents sont pré-traités selon la méthode décrite plus haut. Une fois qu'un document d a été pré-traité, chaque unité lexicale l du document d est associée à un vecteur de concepts local ?(l). Le i ème composant ? i (l) de ?(l) associé au concept c i est alors donné par :
où N (t) est l'ensemble de termes normalisés d'une unité lexicale l et ? n i (t) et le i ème composant, associé au concept c i , du vecteur de concept ? n (t) pour le terme normalisé t. ? n i (t) est défini par :
C(t) est l'ensemble des concepts associés à t, P(c) est l'ensemble des chemins allant du concept c au concept racine dans la taxonomie, c'est à dire les chemins allant du concept spéci-fique c au concept le plus général, d p (c i , c j ) est la distance entre le concept c i et le concept c j sur le chemin p et ? ? [0, 1] est une valeur exprimant la puissance de décroissance. ? est utilisé pour décroître l'influence des concepts généraux sur la représentation d'un terme. En effet, étant donné un terme t, un concept c associé à t, et p un chemin allant de c au concept le plus général (le concept racine), les concepts proches, en terme de distance, de c fourniront le sens principal de t comparés aux concepts éloignés de c. Les concepts généraux n'expriment pas clairement le sens pertinent de t et peuvent même mener à des ambiguïtés voire du bruit. Par conséquent, il peut être intéressant de diminuer le pouvoir expressif des concepts selon leurs distances par rapport au concept spécifique c. En fixant ? à un, nous pouvons considérable-ment réduire l'expressivité des concepts généraux et en fixant ? à zéro, nous pouvons donner le même pouvoir d'expression à tous les concepts en ne faisant aucune différence entre les concepts généraux et spécifiques. ? doit être fixé de manière empirique en fonction du corpus.
Étant donné un ensemble de vecteurs de concepts locaux {?(l 1 ), . . . , ?(l n )} pour un document d, le vecteur de concept global ?(d) pour d est donné par la formule suivante :
La normalisation dans l'équation 4 est utilisée pour représenter des documents de longueur différente avec une même échelle. De plus, les concepts qui apparaissent dans beaucoup de documents du corpus, ne fourniront pas d'information utile pour la discrimination de documents alors que les concepts rares dans un corpus peuvent être significatifs. Par conséquent, nous proposons d'utiliser un vecteur pondéré ? IDF (d) en utilisant la pondération IDF (Inverse Document Frequency). Ce vecteur est défini tel que :
où N est le nombre de document dans le corpus et D(c i ) est l'ensemble des documents du corpus contenant le concept c i . La figure 1 illustre la modélisation d'un document textuel dans le CVSM.
Le noyau linéaire
Nous définissons le noyau linéaire dans le modèle d'espace vectoriel de concept comme ceci :
4 Le noyau CVSM latent L'analyse sémantique latente permet de mettre à jour des relations entre des termes. Ces relations sont extraites par une décomposition linéaire en valeurs singulières de la matrice des fréquences de termes par document. Elles sont par conséquent des relations de co-occurrences des termes dans les documents. Les relations extraites sont appelés "concepts latents". La LSA permet, ainsi, non seulement de diminuer la dimension de l'espace en ne conservant que les relations les plus importantes mais aussi de gérer la polysémie et la synonymie.
Dans un même esprit, nous proposons, dans ce papier, l'utilisation de la LSA dans le CVSM pour capturer les structures latentes entre les concepts. Les concepts LSA peuvent être perçus
FIG. 1 -La représentation d'un document texte selon le modèle d'espace vectoriel de concepts.
comme des concepts abstraits de haut niveau. Il peut être intéressant d'analyser expérimenta-lement l'effet du mélange entre concepts linguistiques et concepts statistiques. Nous définissons la matrice M de concept par document dans le CVSM comme ceci :
avec m la dimension du CVSM (le nombre total de concepts) et n le nombre de documents. La LSA étant une méthode non-supervisée, à savoir que la méthode n'utilise pas l'information sur les étiquettes des documents, nous utiliserons les documents étiquetés (documents d'apprentissage) et les documents non-étiquetés (documents de test) pour M. La SVD de M donne :
Nous dénotons par U k , les k vecteurs singuliers associés aux k valeurs singulières les plus élevées et ? k la matrice diagonale contenant les valeurs singulières. La projection du vecteur ? IDF (d) du CVSM dans l'espace sémantique latente est donnée par :
Le noyau CVSM latent est alors défini par :
Évaluation expérimentale
Nous avons mené plusieurs expérimentations sur un corpus médical pour évaluer la représentation CVSM. Nous avons utilisé le modèle d'espace vectoriel standard, VSM, comme base de comparaison. Dans cette section, nous présentons le corpus médical utilisé, le mode opératoire pour les expérimentations et un résumé des résultats expérimentaux.
Le corpus Ohsumed
Les expérimentations ont été menées sur le corpus Ohsumed qui est un corpus médical contenant 6286 documents d'apprentissage et 7643 documents de test (Hersh et al., 1994). Les documents sont des résumés d'articles de médecine issus de la base bibliographique médicale MEDLINE. Chaque document est, ou doit être dans le cas des documents de test, étiqueté avec une ou plusieurs des 23 étiquettes. Les étiquettes correspondent à des catégories cardiovasculaires. La tâche de catégorisation sur ce corpus est connue pour être difficile. En effet, les systèmes de catégorisation qui fonctionnent relativement bien sur les corpus tels que le Reuters-21578 et le 20-NewsGroups voient leurs performances diminuées. Par exemple, dans (Joachims, 1998), le classifieur linéaire SVM sur la VSM atteint une performance de 65.9% alors qu'il atteint une performance de 86% sur le corpus Reuters 21578. Les difficultés de catégorisation sont dues au fait que les données sont bruitées avec des termes médicaux très spécifiques et que les catégories ont un haut degré de corrélation.
La préparation des expérimentations
Dans toutes nos expérimentations, les documents ont été pré-traités pour la représentation selon le modèle d'espace vectoriel de concepts (CVSM). Pour le modèle d'espace vectoriel, VSM, nous avons utilisé le stemming pour réduire les mots fléchis à leurs bases communes. Nous avons, en outre, éliminés les mots vides. Pour le stemming, nous avons utilisé l'algorithme de Porter (Porter, 1980). Pour la gestion du problème à multi-catégories, nous avons utilisé la stratégie "un-contretous". Cette stratégie a mené à la décomposition du problème principal en 23 sous-problèmes de catégorisation binaire. La librairie libSVM (Chang et Lin, 2001) a été utilisée pour l'apprentissage des classifieurs SVM.
Afin d'évaluer le CVSM, nous avons utilisé le VSM comme base de comparaison. Nous avons utilisé un noyau linéaire, dans le VSM, avec une pondération TF-IDF et une normalisation des vecteurs selon le mode opératoire defini dans (Joachims, 1998). Ce noyau est nommé "noyau Sac-de-mots"(Bag Of Words -BOW -kernel). De plus, nous avons aussi utilisé un noyau LSA dans le VSM en utilisant l'équation 10. Ce noyau LSA est utilisé comme base de comparaison pour le noyau CVSM latent. Nous utiliserons la dénomination "BOW SVD" pour désigner le noyau LSA dans le VSM.
La mesure utilisée pour évaluer les performances des classifieurs est, principalement, la mesure F1 (Sebastiani, 2002). La mesure F1 est la moyenne harmonique de la précision et du rappel d'un système de catégorisation.
Évaluation de la puissance de décroissance ?
Dans la première expérimentation, nous cherchons, empiriquement, la valeur optimale de la puissance de décroissance ? pour le corpus Ohsumed. Nous rappelons que ? contrôle la manière dont les concepts généraux sont pris en compte dans l'équation 3. Une valeur de zéro donnera un poids égal aux concepts généraux et spécifiques. Pour cette expérimentation, nous utilisons uniquement 10% des données d'apprentissage et 10% des données de test pour éva-luer la performance. De plus, nous utilisons un échantillonnage stratifié pour conserver les proportions. La figure 2 montre les scores micro-F1 pour différentes valeurs de ? ? [0, 1]. Le meilleur score est obtenu pour ? = 0.1 avec une valeur micro-F1 de 53.9%. En outre, une meilleure performance est obtenue pour ? = 0 que pour ? = 1. Ce point signifie que les concepts généraux jouent un rôle important dans la tâche de catégorisation pour le corpus Ohsumed. C'est, effectivement, le cas lorsque plusieurs mots, avec un sens general commun, sont utilisés dans différents documents d'une même catégorie.
FIG. 2 -La variation de la micro-F1 en fonction du pouvoir de décroissance ?. Les résultats sont obtenus en utilisant 10% des données d'apprentissage et 10% des données de test.
Évaluation du nombre de concepts latents
Pour les noyaux basés sur la LSA, la dimension de l'espace sémantique doit être fixée empiriquement. Par conséquent, nous avons mené un ensemble d'expérimentations sur le corpus entier en faisant varier le nombre de concepts latents, à savoir le nombre de vecteurs singuliers associés aux valeurs singulières les plus élevées. La figure 3 montre la variation du score micro-F1. Les noyaux CVSM latent et BOW SVD atteignent leurs performances quasioptimales pour approximativement 2000 concepts latents. En fait, il y a une croissance rapide des performances pour un nombre de concepts compris allant de 0 à 1000. Ceci montre que les 1000 premiers vecteurs singuliers fournissent l'information principale pour décrire les documents. Puis, la croissance des performances diminue pour un nombre de concepts de 1000 à 2000, indiquant ainsi la présence d'une faible quantité d'information. Les performances du noyau CVSM latent sont meilleures de près de 2% par rapport au noyau BOW SVD. De plus, les différences sont plus prononcées pour un nombre de dimensions faible. Ceci signifie que le noyau CVSM latent est capable de capturer et d'exprimer l'information principale dans un espace de faible dimension, c'est à dire que l'information principale est résumée par un faible nombre de vecteurs singuliers. Pour le reste des expérimentations, nous avons fixés le nombre de concepts latents à 3000.
FIG. 3 -La variation de la micro-F1 en fonction du nombre de concepts latents.
Évaluation des noyaux
Les performances des noyaux sur le corpus Ohsumed sont reportées dans les tableaux 1 et 2. Les expérimentations ont été réalisées sur l'ensemble des données sans sélection de termes. Les noyaux dans le CVSM obtiennent de meilleurs résultats, jusqu'à 2% de plus, que les noyaux dans le VSM. Comme attendu, les noyaux LSA ont de meilleurs performances que les noyaux linéaires. Ceci est dû au fait que les concepts abstraits de haut niveau de l'espace sémantique résument l'information principale et réduisent le bruit. En outre, le noyau CVSM linéaire est plus performant que le noyau BOW SVD. Nous en déduisons que les concepts basés sur l'ontologie (les concepts linguistiques) sont plus adaptés pour exprimer le sens des documents médicaux que les concepts abstraits (les concepts statistiques) obtenus par la dé-composition linéaire de la LSA. 
Noyau
Réduction de la quantité de données d'apprentissage
La figure 4 montre l'impact de la quantité de données utilisées pour l'apprentissage sur les performances des noyaux. Comme précédemment, les noyaux LSA sont plus performants que les noyaux linéaires. Néanmoins, il existe deux points intéressants dans ces résultats. Premiè-rement, le noyau BOW SVD est plus performant que le noyau CVSM linéaire lorsque 30% ou moins des données d'apprentissage sont utilisées. Au delà des 30% le noyau CVSM linéaire est plus performant. Ceci signifie que les concepts statistiques ont un pouvoir discriminatoire supé-rieur au concepts du CVSM pour des petits échantillons de données d'apprentissage. Toutefois, quand la base d'apprentissage est suffisamment importante, les concepts CVSM deviennent plus expressifs. Deuxièmement, tous les noyaux ne réussissent pas à capturer l'information principale avec une faible quantité de données. En effet, les performances ne cessent de s'amé-liorer en fonction de la quantité de données d'apprentissage. Par conséquent, nous pouvons en déduire que chaque document d'apprentissage fournit une nouvelle information qui améliore la performance de catégorisation. Ceci est principalement dû au fait que ce corpus contient des termes spécifiques avec une faible fréquence d'occurrence.
Conclusion
Dans cet article, nous avons présenté un modèle d'espace vectoriel de concepts pour la représentation de documents. Ce modèle utilise des connaissances a priori pour capturer le sens des documents textuels. Nous avons montré une façon simple d'utiliser efficacement les ontologies pour les intégrer au modèle d'espace vectoriel, VSM, standard. Nous avons, aussi, adaptés le noyau linéaire et le noyau LSA pour le CVSM. Nous avons illustré l'utilisation du CVSM par une application au domaine biomédical. Le Metathesaurus de l'UMLS a été utilisé comme source a priori de connaissance pour définir le CVSM.
Les performances des noyaux CVSM ont été, expérimentalement, évaluées sur une tâche de catégorisation de documents biomédicaux. Les noyaux ont été comparés au noyau sac-demots (BOW) et au noyau LSA. Les résultats ont montré que les noyaux CVSM améliorent
FIG. 4 -La variation de la micro-F1 en fonction du pourcentage de documents d'apprentissage utilisés.
les performances de catégorisation de près de 2%. De plus, les expérimentations ont montré que l'utilisation des concepts latents, extraits à partir des concepts linguistiques par une SVD, permettent d'améliorer les résultats.
Pour un travail futur, nous pensons améliorer la représentation CVSM en intégrant une pondération d'attributs plus adaptée que la pondération IDF. Il a été montré dans (Debole et Sebastiani, 2003) que la pondération supervisée des termes pouvait améliorer la catégorisation de documents. En effet, des recherches récentes, en matière de pondération des termes, ont donné des résultats prometteurs (Soucy et Mineau, 2005;Lan et al., 2006Lan et al., , 2007.

Introduction
L'interconnexion croissante des systèmes d'information, de même que des initiatives telles que le Web sémantique requièrent la création de nombreuses ontologies pour assurer la cohérence sémantique des opérations. Il devient donc nécessaire de développer des systèmes de gestion qui permettent non seulement de les stocker mais également de les aligner et de les combiner pour créer de nouvelles ontologies adaptées à des besoins particuliers, favorisant ainsi la réutilisation.
Contrairement à une démarche d'intégration où l'on ne cherche à obtenir qu'une seule ontologie homogénéisée, notre approche s'attache à conserver au sein d'une même base les différents points de vue (c'est-à-dire les différentes ontologies), mettant ainsi en évidence les apports de chaque contributeur. Cependant, les outils utilisant des ontologies ont besoin d'ontologies "normales" (mono-point de vue) pour fonctionner. Nous proposons donc un ensemble d'opérations et laissons le soin aux utilisateurs de les utiliser pour extraire de la base une ontologie "sur mesure", dans un contexte et un but spécifiques. Axiomes. Un axiome établit une relation de subsomption entre deux expressions de concepts E 1 et E 2 . Dans le cas où E 1 est un concept atomique on parle de définition.
Un modèle de bases d'ontologies
Une expression de concept (ici nous prendrons le langage SHOIN qui est celui de OWL, Patel-Schneider et al. (2003), mais la démarche s'applique à toute logique descriptive) obéit à la syntaxe
Le graphe d'une telle expression est son arbre syntaxique étiqueté de manière à distinguer les descendants de chaque noeud. Pour un noeud and, or ou not, les arcs vers les descendants sont étiquetés par Arg. Pour les noeuds some et all on utilisera les étiquettes P rop et V al pour distinguer la propriété sur laquelle porte la restriction et le concept dans lequel la propriété prend ses valeurs. Pour atLeast et atM ost on ajoute des arcs N um pour le nombre et pour oneOf et hasV alue des arcs Ind pour indiquer les individus. Annotations. Les annotations constituent la partie non formalisée de l'ontologie. On distingue des annotations d'ordre terminologique, qui lient un concept atomique à un ou plusieurs termes, éventuellement dans plusieurs langues, ainsi qu'à des définitions ou commentaires en langue naturelle. Les annotations peuvent également être d'ordre argumentatif, par exemple pour soutenir ou au contraire critiquer une définition ou une partie de définition, Falquet et Mottaz Jiang (2000). Dans notre modèle, les annotations sont des objets possédant une valeur (en général un texte) qui sont liés aux concepts atomiques où aux expressions de concepts par des arcs étiquetés. 
Opérations sur les ontologies
Les opérations que nous présentons ci-dessous ont été conçues pour réaliser de manière simple des tâches typiques d'adaptation ou de combinaison d'ontologies que nous avons observées lors de la réalisation de différents projets basés sur des ontologies.
Projection. On définit la projection d'une ontologie sur un ensemble X de propriétés en distinguant ce qui se passe pour les arbres d'expression de concepts et pour le reste du graphe de l'ontologie.
La projection sur X d'un arbre d'expression de concept de racine e est l'arbre obtenu en supprimant tous sous-arbres de racine f tels que le chemin de f à e contient un noeud all, some, atLeast ou atM ost avec un arc V al qui fait référence à une propriété n'appartenant pas à X.
Par exemple, la projection sur {R, S} de l'expression and(all(R, C), some(T, all(R, D)), all(S, or(A, all(T, A)))
donnera and(all(R, C), all(S, or(A)))
Il est parfois utile de limiter la projection à un seul niveau. Dans ce cas, tous les sous-arbres d'un noeud faisant référence à une propriété appartenant à X seront conservés. La projection sur {R, S} limitée au premier niveau de l'expression 1 sera
and(all(R, C), all(S, or(A, all(T, A))))
La projection d'une ontologie est obtenue en remplaçant chaque arbre d'expression par sa projection. Pour le reste de l'ontologie (les annotations et arcs d'axiomes) on élimine les arcs dont l'étiquette n'appartient pas à X et on supprime les éventuelles composantes connexes ainsi créées qui ne contiennent aucun axiome (et ne participent donc plus à l'ontologie).
Il est clair qu'en général les expressions obtenues par projection ne seront pas équivalentes, par contre les alignements sont conservés car on considère qu'il s'agit de nouvelles définitions (moins précises) des mêmes concepts.
Sélection. La sélection a pour but d'extraire un sous-ensemble de l'ensemble des concepts définis dans une ontologie. Pour spécifier une sélection on définit une expression logique portant sur (le graphe de) l'ontologie, c'est-à-dire sur les définitions de concepts, leurs annotations et éventuellement leurs instances.
Un expression de sélection est une expression de logique du premier ordre possédant une variable libre et définie sur le vocabulaire constitué de prédicats unaires correspondant aux types de noeuds du graphe d'une ontololgie (And, Or, Some, . . . ) de prédicats binaires correspondant aux types d'arcs (Arg, P ro, V al, N um, plus tous les noms de propriétés d'annotation) et de constantes les noms de propriétés et les littéraux.
Par exemple, pour sélectionner dans O 1 tous les concepts qui ont une restriction some sur la propriété S au premier niveau on écrira :
Etant donné un prédicat de sélection S(x), la sélection ? S O de l'ontologie O par S a pour graphe le sous-graphe de O qui contient tous les concepts atomiques (noeuds) satisfaisant S, les arcs d'alignement et d'annotation de ces concepts, leurs expressions de définition, les concepts atomiques référencés (à travers des arcs Arg et V al depuis ces définitions et les noeuds.
Les concepts non-sélectionnés mais référencés doivent être conservés, mais sans leur défi-nition, pour maintenir la consistance des définitions des concepts sélectionnés. Ils passent donc du statut de concept défini à concept de base.
Etant donné que les ontologies sont interconnectées, l'expression de sélection peut utiliser d'autres ontologies. Par exemple, si l'on veut sélectionner les concepts de O 1 qui sont alignés avec un concept de O 2 , on écrira ? ?(y:$1)Aligned(x,y) O 1 (O 2 ) où y : $1 indique que la variable y doit être un objet de la première ontologie passée en paramètre (ici O 2 ) Jointure. Contrairement à l'union, la jointure combine les définitions de concepts par une opération ensembliste (union, inter). L'idée est que pour toute paire d'axiomes
. Il en va de même pour les axiomes de la forme A 1 def E 1 . Si l'un des axiomes est de la forme A i def E 1 , l'axiome obtenu sera A def (E 1 E 2 ). On procède de la même manière s'il y a plus de deux concepts alignés sur le même objet d'alignement. On remplace également tous les arcs de ou vers l'un des A i par un arc de ou vers A.
Si l'on élimine les concepts qui ne sont pas alignés avec au moins un autre concept, on obtient la jointure stricte. Si on les garde on peut obtenir la jointure externe à gauche ou à droite.
Exemples d'utilisation
Dans le projet UNL (Universal Networking Language www.undl.org), des équipes dispersées dans le monde développent indépendamment des parties de l'UNLKB (ontologie linguistique du langage UNL). Ces parties, qui portent sur des domaines disjoints ou non, sont ensuite importées dans la base d'ontologies UNLPLAZA. Chaque concept est associé à un "mot universel" qui joue le rôle d'objet d'alignement entre ontologies. L'une des fonctions de l'UNLPLAZA est de produire (exporter) des ontologies construites par union, dans le cas où il existe plusieurs définitions de concepts pour le même mot universel, un ordre de priorité déter-mine laquelle choisir. Si l'ordre de priorité des ontologies est
Dans le cas où la base contient des ontologies portant sur un même domaine, mais déve-loppées par des personnes qui s'intéressent à des caractéristiques différentes, on peut utiliser la jointure pour regrouper les caractéristiques en des définitions plus complètes. Par exemple : une ontologie des aliments selon le point de vue des cuisiniers (propriétés : saveur, temps de cuisson, prix, saison) pourra être jointe avec une ontologie selon le point de vue des diététiciens (valeur énergétique, vitamines, graisse, hydrate de carbone, protéines) Pour importer dans O 1 uniquement les parties de définition concernant une propriété P on projette O 2 sur P puis on joint (à gauche) le résultat à O 1 ) :
Le remplacement de définitions s'effectue de la manière suivante. Soit E(x) l'expression permettant de sélectionner les concepts de O 1 dont on veut remplacer la définition par celles existant dans O 2 . l'expression de remplacement est O 1 ? x.?(y:$1)Aligned(x,y) O 2 (? E(x) O 1 ) où l'on commence par sélectionner les concepts de O 2 alignés avec les concepts sélection-nés de O 1 , puis on fait une jointure externe gauche des deux ontologies.

Contexte
Dans le but de résoudre des problèmes complexes du monde réel dans des domaines différents tels que l'optimisation, la détection d'anomalies ou la robotique, des heuristiques inspirées de mécanismes naturels ont été exploitées avec succès. Plusieurs chercheurs se sont intéressés aux systèmes immunitaires biologiques (SIB) comme un nouveau paradigme de l'intelligence artificielle et ont développé des applications industrielles en ordonnancement, en robotique, ou en détection d'intrusion. Néanmoins, peu de travaux ont traité la probléma-tique de la détection de fraude de comportement en télécommunications.
Dans ce papier, on propose un nouveau système immunitaire artificiel (SIA) pour la dé-tection du comportement du soi non soi avec une approche non supervisée basée sur le mé-canisme SIB dit inné de cellule NK. Un tel système diffère des SIA existants qui se basent sur le mécanisme supervisé adaptatif de SIB des cellules T et B (Garrett 2005).
Présentation du système NK proposé
L'algorithme de notre système NK, décrit dans le tableau TAB1, comporte quatre phases qui concernent la reconnaissance et l'extraction de modèles d'instances puis leur transformation en signaux d'inhibition et d'activation. La dernière phase concerne la détection de la présence de comportements anormaux sur la base de l'analyse des densités spectrales ou de filtrage des signaux. Notons ici que la terminologie signal utilisée correspond à un signal discret à temps discret et que l'entrée de l'algorithme est une série chronologique vectorielle.
L'algorithme du système NK élaboré a été testé sur des données simulées de 10100 instances de télécommunication, relatives aux trafics de certains usagers chez un intermédiaire, et qui sont infectées par un comportement frauduleux pour les instances entre 10000 et 10100 et dont la proportion représente 0.01% de l'échantillon. Les résultats obtenus sont satisfaisants car, malgré la proportion très faible des opérations frauduleuses dans l'échantil-lon, notre système NK a réussi à les détecter (cf . FIG. 3) et à identifier les instances de comportements frauduleux (cf . FIG. 1 et FIG. 2 
Summary
We propose a new artificial immune system called NK system for the detection of self non self behaviour with an unsupervised approach based on the mechanism of NK cell. In this paper, the NK system is applied to the detection of fraud in mobile telephony.

Introduction
Les données dans un SIG (Faïz, 1999), sont souvent recueillies pour les besoins propres d'une institution, voire d'un service. Face à cette réalité, il devient judicieux de déployer de nouvelles sources pour répondre aux besoins d'un nombre plus important d'utilisateurs. Ceci est qualifié d'enrichissement de bases de données géographiques (BDG). C'est dans ce contexte que s'inscrit notre approche b , Faïz et Mahmoudi, 2005. Cette dernière utilise la technique de résumé de documents multiples (Barzilay et McKeown, 2005) permettant d'extraire l'information pertinente sous une forme abrégée. Pour assurer l'extraction dans des temps raisonnables et conformément au paradigme multi-agents (Ferber, 1999), nous adoptons trois classes d'agents: agent interface, agent géographique et agent tâche. L'interaction entre les agents est achevée par envoi de messages. L'enrichissement est réalisé en trois phases : une identification de segments et de thèmes, une délégation et enfin, un filtrage textuel. S'ajoute à ces étapes de base, une approche, exercée à la demande, pour un raffinement du processus.
La section 2 présente, certains travaux d'enrichissement des BDG dans les SIG ainsi que notre approche pour cet enrichissement. La section 3 est dédiée à la mise en oeuvre et l'évaluation de notre système.  (Plazanet, 1996), par exemple, l'enrichissement procure les BDG d'informations en terme de structure des formes, des connaissances se rapportant à l'ordre des opérations et aux algorithmes appropriés. Un autre flot de travaux se rapporte à l'aspect sémantique (dit aussi factuel ou descriptif) des BDG. Dans cette catégorie, nous pouvons citer Metacarta (MetaCarta, 2005) et GeoNode (Hyland et al., 1999).
Le projet Metacarta accompli l'enrichissement par son produit Geographic Text Search (GTS). GTS permet de relier des documents textuels à des entités géographiques localisées sur la carte venant enrichir les données de la BDG. MetaCarta GTS est offert comme une extension au système d'information géographique ArcGIS.
GeoNode (Geographic News On Demand Environment) exploite la technique d'extraction d'informations pour aboutir à l'enrichissement, via le système Alembic. GeoNode permet d'extraire les entités nommées et les évènements associés qui seront visualisées d'une manière geospatiale. Le SIG ArcView supporte GeoNode.
Ce qui marque notre approche est qu'à l'opposition des travaux existants, elle va au-delà de la simple localisation de l'information, pour permettre la synthèse de ces informations. De même, à l'opposé des travaux existants dont la source de données aurait été déjà pré-établies sous forme de librairies (données traitées et classées) par exemple, notre approche préconise la génération en temps réels (information toujours récente et mise à jour) des documents requis peu importe le type d'information réclamée (nous ne pouvons jamais prédire tous les besoins des utilisateurs). Egalement, et outre l'aspect sémantique, nous exploitons la composante spatiale des BDG afin d'améliorer les résultats de l'enrichissement.
Notre approche pour l'enrichissement des BDG
Notre processus d'enrichissement émane d'un besoin informationnel réclamé par les utilisateurs des SIG. Un utilisateur soumettant une requête à la BDG, mais, se trouvant en situation d'insatisfaction, peut lancer notre processus d'enrichissement. En fait, les documents relatifs à une ou plusieurs entités géographiques sont confiés à l'agent interface qui va les distribuer entre les agents tâche. Chaque agent tâche procède à la segmentation de son document en parties thématiquement cohérentes. Notre proposition est une adaptation de l'algorithme de TextTiling (Hearst, 1997). Le texte est initialement découpé en blocs de taille fixe. En fait, il arrive que lors du découpage, deux phrases hétérogènes soient classées sous le même bloc, ainsi, lors du calcul de la similarité, nous obtenons forcément des valeurs élevées faisant preuve de l'homogénéité des deux blocs. Pour résoudre le problème de localisation non précise des frontières, nous intervenons lors du découpage initial en appliquant la procédure C99 (Choi, 2000), reconnue pour être la plus appropriée pour les textes courts. Cette procédure remplace la similarité entre phrases par le rang dans un contexte local. Un score de cohésion (métrique du cosinus) est attribué aux blocs adjacents. Les similarités sont présentées sous forme de graphe et aplanie. Les vallées observées sur le graphe correspondent à des scores faibles indiquant une potentielle rupture thématique.
Les segments précédemment détectés, subissent une annotation (étiquetage) par les agents tâche moyennant l'attribution de thème. Le mot le plus fréquent est assigné comme le thème du texte en cas de distribution hétérogène des fréquences des mots. Pour une distribution homogène, nous partons de l'idée que le texte est un ensemble de termes contribuant à développer un thème donné. Pour déterminer ce thème, nous exploitations les relations sémantiques du thesaurus WordNet (Miller, 1990 Les thèmes décelés sont assignés aux agents tâche par une délégation entreprise par l'agent interface (cas d'une seule entité géographique) ou un des agents géographique (en cas d'une multitude d'entités). Cette délégation consiste à distribuer les différents thèmes entre les agents tâche, et ce tenant compte d'un coût qui reflète le coût de communication (en terme de messages échangés) et la charge du travail (en terme de volume textuel à prendre en charge). Dorénavant, chaque délégué détient à sa disposition des documents générés qui sont l'ensemble des segments textuels relatifs aux thèmes qui lui sont affecté.
Par la suite, chaque délégué entame une phase de filtrage visant la condensation dans un format de résumés de ses documents générés pour ne retenir que l'essentiel de l'information (Mann et Thompson, 1988). Ainsi, pour une phrase formée de deux unités lexicales et reliée par une relation d'exemplification (par la présence de cues ; for instance, for example), nous ne retenons que la partie (appelée nucléus) qui ne contient pas ce cue, obligatoire pour la compréhension du texte. L'autre contenant le cue est subsidiaire (appelée satellite). En cas d'absence de cues, nous calculons la similarité entre les unités et nous ne retenons que les unités les plus dissimilaires.
A l'issue des étapes susmentionnées, il se trouve que parfois l'utilisateur reste insatisfait par les résultats de l'enrichissement. Ceci peut provenir d'une des raisons suivantes : (i) la recherche de l'information n'a pas ciblé les documents les plus pertinents dans le web, (ii) l'utilisateur est submergé par une masse colossale de documents qui ne sont pas tous pertinents par rapport à l'entité en question, (iii) L'information visée n'est pas disponible dans les documents du corpus. L'idée est d'examiner le voisinage de l'entité sujette de la recherche pour dégager les entités avec les quelles, elle détient des relations spatiales (Mahmoudi et Faïz, 2006 b ). Notre thèse est argumentée par la première loi de la géographie décrivant la nature des systèmes géographiques dans lesquels "everything is related to everything else, but near things are more related than distant things" (Bin et Itzhak, 2006). Dans ce cas, l'utilisateur peut lancer un processus de raffinement que nous avons intégré au processus de base. Il exploite les relations spatiales (adjacence, superpositions…) afin de mieux décrire les entités sujettes de la recherche et ainsi mieux cibler les informations pertinentes.
langage Java ce qui a permis une implantation d'un système distribué transposant les concepts du système multi-agents. Cette mise en oeuvre a donné naissance à un outil que nous avons baptisé Semantic Data Enrichment Tool ou SDET (Mahmoudi et Faiz, 2007, Mahmoudi et Faiz, 2006 ). SDET offre un ensemble de fonctionnalités visant l'enrichissement des données initialement stockées dans la BDG allant de la création de documents générés relatifs aux thèmes décelés du corpus textuel jusqu'à la condensation des idées essentielles incarnées dedans sous forme de résumés. En plus, SDET offre le moyen d'exploiter les relations spatiales que les entités géographiques y maintiennent pour un éventuel raffinement de la recherche. Ce système à été intégré au SIG OpenSource : OpenJump.
Pour l'évaluation des résumés générés, nous avons utilisé l'outil ROUGE (RecallOriented Understudy for Gisting Evaluation) adopté par la campagne d'évaluation de systèmes de résumé DUC (Document understanding conference). ROUGE calcule le taux de chevauchement entre les résumés produits par un système de résumé et les résumés produits par des humains (appelé modèle ou référence) en utilisant des n-grammes. Formellement, ce score s'exprime par la formule suivante :
ROUGE-N, dénote un score basé sur le nombre de n-grammes (1<=n<= 4) communs entre les résumés automatiques et les résumés modèles. Count match (n-gram) est le nombre de n-grammes partagés entre le résumé produit par le système et le résumé modèle et Count (ngram) est le nombre de n-grammes dans le résumé modèle.
Rouge-2, Rouge-SU-4 et BE se sont imposés. ROUGE-2 calcule le nombre de paires de mots successifs (nombre de bigrammes) en commun entre les résumés automatiques et les résumés modèles. Rouge-SU-4 correspond au rappel en "bigrammes à trous" (skip units) de taille maximum 4. Pour, BE (Basic Elements) elle a est définie comme unité sémantique minimale qui consiste en deux éléments et une relation (entités-relation) entre ces éléments. Par exemple, à partir de la phrase "two Libyans were indicted for the Lockerbie bombing in 1991" nous détectons la BE suivante : indicted|libyans|obj, tel que obj est la relation objet entre indicted et libyans. La similarité entre le résumé référence et le résumé système en terme de BEs, permet de juger que le résumé système est un bon résumé.
Comme pour la conférence DUC, nous avons défini deux types de références basses, appelées aussi baseline. Ces résumés sont créés automatiquement en fonction des règles suivantes : une baseline sélectionne les 150 premiers mots du document le plus récent, une autre baseline sélectionne la première phrase dans les (1, 2,…, n) documents de l'ensemble à résumer trié par ordre chronologique jusqu'à atteindre 150 mots. Ces règles découlent du fait que d'après des études de différents documents textuels, l'importance du commencement du document par rapport à sa fin a été approuvée. Ainsi, les premières lignes du texte couvrent en général une partie importante de l'essentiel du texte. Pour notre évaluation, nous avons généré des résumés automatiques avec les systèmes suivants : le système MEAD (Radev et al., 2003) et les méthodes baselines (baseline1 et baseline2). Nous avons utilisé l'outil ROUGE pour comparer les résumés obtenus avec notre système SDET, MEAD, baseline1 et baseline2 avec les résumés humains comme le montre le Tableau 1.
Le score le plus élevé étant le meilleur, il indique le système le plus performant. SDET est classé au premier rang avec les meilleures notes d'évaluation. Les deux baselines donnent les résultats les plus faibles. Notons que si l'hypothèse de base qui stipule que les premières lignes du texte couvrent en général une partie importante de l'essentiel, cela reste insuffisant dans le cas de résumé multi-documents qui nécessite une prise en charge particulière des similarités et des différences informationnelles à travers tous les documents.
Système
Rouge 
Conclusion
La disponibilité des informations pertinentes répondants aux besoins de la quasi totalité des utilisateurs SIG est devenue un objectif visé par les concepteurs de tels systèmes. En effet, les informations manipulées sont fournies par des BDG qui deviennent très vite insuffisantes et ne répondent plus à l'ensemble des besoins des utilisateurs. Face à cette situation, nous avons proposé et mis en oeuvre un processus d'enrichissement pour les BDG. Il s'agit d'une identification de segments et de leurs thèmes, une délégation puis un filtrage. Une phase supplémentaire peut être invoquée en cas de résultats insatisfaisants, il s'agit d'un raffinement visant un ciblage plus pertinent des informations requises. La mise en oeuvre du processus d'enrichissement a donné lieu à un outil que nous avons baptisé SDET. Les résultats générés par notre système ont été évalués en utilisant le package ROUGE. Les résultats de cette évaluation sont très convaincants faisant preuve de l'intérêt de notre démarche pour l'enrichissement des BDG.

Introduction
La classification de textes a pour objectif le regroupement de documents selon différents critères. Dans les travaux présentés dans cet article, nous nous intéressons à la classification de textes d'opinion qui consiste à classer les textes selon un jugement tel que l'aspect positif ou négatif d'une critique, l'aspect favorable ou défavorable donné par un expert, etc. Nous proposons dans cet article une approche fondée sur plusieurs classifieurs combinés à un système de vote. Dans un premier temps, nous présentons les corpus du défi DEFT'07 (Grouin et al., 2007) sur lesquels nous avons mené nos expérimentations ainsi que les représentations des textes utilisées. La section 3 décrit les classifieurs et les systèmes de vote proposés. Enfin, la partie 4 présente les résultats obtenus.
Représentation des données textuelles
La troisième édition du défi francophone DEFT'07 consistait à déterminer des catégories de jugements à partir de quatre corpus français très différents en terme de thème, taille, tournures de phrases, richesse du vocabulaire, représentation des catégories de jugement : ? Corpus 1 : Critiques de films, livres, spectacles et bandes dessinées. La première étape de notre approche consiste à appliquer un certain nombre de prétraitements linguistiques. Ceux-ci consistent à extraire du corpus toutes les unités linguistiques (mots lemmatisés ou lemmes) utilisées pour la représentation des textes. En effet, le prétraitement consistant à lemmatiser les données textuelles a globalement tendance à améliorer les tâches de classification (Plantié, 2006). Par ailleurs, ces prétraitements consistent également à éliminer certains mots ayant des types grammaticaux peu discriminants pour classifier les textes d'opinion : articles, ponctuations. Dans notre approche nous avons souhaité conserver les lemmes associés à tous les autres types grammaticaux. En effet, le fait de traiter spécifiquement des textes d'opinion nous encourage à conserver un maximum de types grammaticaux susceptibles d'exprimer des nuances d'opinions ou des contributions à des opinions (comme les adverbes). Les expériences que nous avons menées sur les textes d'opinion propres aux corpus DEFT'07 ont en effet montré que la suppression de types grammaticaux diminuait les performances. Dans la suite de cet article, nous appellerons « index » la liste de lemmes constitués pour chacun des corpus.
Chaque corpus est représenté sous forme matricielle en utilisant l'approche classique dite de Salton (Salton et al., 1975). Dans cette représentation, les lignes sont associées aux différents textes du corpus et les colonnes sont relatives aux lemmes. Chaque cellule de la matrice représente le nombre d'occurrences du lemme dans chaque texte du corpus.
L'ensemble des textes d'un corpus et donc les vecteurs associés constituent dans notre approche l'ensemble d'apprentissage qui permet d'identifier un classifieur associé. L'espace vectoriel défini par l'ensemble des lemmes du corpus d'apprentissage et dans lequel sont définis ces vecteurs comporte un nombre important de dimensions. Nous avons choisi d'effectuer une réduction de l'index. Nous utilisons la méthode présentée par Cover qui mesure l'information mutuelle associée à chaque dimension de l'espace vectoriel (Cover & Thomas, 1991). Cette méthode expliquée en détail dans (Plantié, 2006) permet de mesurer l'interdépendance entre les mots et les catégories de classement des textes par la différence d'entropie entre celle de la catégorie et celle de la dimension en cours d'étude de l'espace vectoriel. Notons que plus la différence est grande, plus la quantité d'information de discrimination est importante et plus le mot est important pour la tâche de catégorisation. Dans notre approche, nous avons appliqué un seuil de zéro pour effectuer cette réduction. Un tel seuil signifie que les mots retenus sont réellement discriminants. Cette opération diminue de manière très importante l'ensemble des corpus avec un pourcentage de réduction de plus de 90% de tous les corpus de DEFT'07. Cette étape améliore de façon sensible les résultats de classification (environ 10% sur la « F-mesure » comme indiqué ci-après).
Dans le cadre de DEFT'07, nous avons appliqué un prétraitement supplémentaire. Ainsi, avant d'effectuer la classification des textes, nous avons cherché à améliorer les traitements « linguistiques » des textes. Dans ce but, les termes (groupes de mots respectant des patrons syntaxiques spécifiques tels que « Nom Adjectif », « Adjectif Nom », etc.) ont été extraits avec EXIT (Roche, et al., 2004). Ainsi, nous avons considéré la liste des termes extraits comme l'index du corpus à partir duquel tous les textes ont été vectorisés. Puis la procédure classique a été appliquée : réduction d'index et classification. Le nombre de termes extraits peut se révéler assez faible pour certains textes ce qui réduit significativement la taille de l'index. Ceci met alors en défaut les méthodes statistiques qui ont été mises en place dans le cadre du défi. Par ailleurs, les termes sélectionnés après réduction d'index ne sont pas suffisamment significatifs pour représenter la diversité des textes. Ceci peut expliquer que notre approche uniquement fondée sur les termes dégrade nos résultats. Ainsi, nous proposons ci-dessous une méthode plus générale fondée sur l'utilisation de bigrammes de mots.
Dans l'approche développée, outre les mots qui sont pris en compte, les vecteurs sont aussi constitués de bigrammes de mots. Seuls les bigrammes contenant des caractères spéciaux sont rejetés (caractères mathématiques, ponctuations, etc.). Cette représentation plus riche des textes permet d'obtenir des informations plus adaptées aux textes d'opinion. A titre d'exemple, dans le corpus de relectures d'articles les bigrammes tels que pas convainquant, mieux motiver, pas assez sont des groupes de mots beaucoup plus porteurs d'opinion comparativement à chacun des mots constituant ces bigrammes. Deux différences majeures sont à relever par rapport à la méthode fondée sur la terminologie : (1) L'approche prend en compte les mots ainsi que les bigrammes pour constituer l'index, (2) Le nombre de bigrammes retournés est beaucoup plus important que le nombre de termes respectant des patrons syntaxiques définis.
Les résultats en utilisant cette représentation enrichie des textes par la prise en compte des bigrammes améliore les résultats comme nous allons le montrer dans la section 4. Outre la qualité de la représentation des textes qui améliore les tâches de classification, la prise en compte de différents classifieurs (voir section suivante) reste déterminante pour retourner un résultat de bonne qualité.
Processus de classification
Afin d'améliorer la méthode générale de classification des textes d'opinion, nous avons mis en oeuvre un système de vote fondé sur différents classifieurs. Le même processus de classification a été appliqué en nous appuyant sur les représentations de textes présentées dans la section précédente. Dans la suite de ce papier, nous appellerons Copivote (Classification de textes d'OPInion par un système de VOTE) le système de classification appliqué à la représentation par lemmes uniquement et CopivoteBi (Classification de textes d'OPInion par un système de VOTE avec Bigrammes) le système de classification appliqué à la représentation par lemmes et bigrammes.
Le système de vote mis en place s'appuie sur différentes approches de classification. Ces dernières sont fondées sur trois méthodes principales : -Vote à la majorité simple : choix des classes à la majorité des résultats des classifieurs.
-Vote par choix du maximum (respectivement, minimum) : choix de la classe allouée par le classifieur qui a donné la probabilité la plus élevée (respectivement, faible).
d'appartenance. Dans ce cas, il y a nécessité que les probabilités exprimées par les différents classifieurs soient comparables. -Vote par somme pondérée : pour chaque document et pour chaque classe la moyenne des probabilités de tous les classifieurs est calculée et le choix de la classe attribuée au document est alors fondé sur la plus forte moyenne. Notons que plusieurs travaux s'appuient également sur un système de vote de classifieurs (Kuncheva, 2004 ;Kittler et al., 1998).
Le choix de la procédure de classification a été appliqué sur chaque ensemble d'apprentissage. Nous avons conservé la méthode de classification la plus performante pour un corpus donné. Les données sont évaluées par validation croisée sur l'ensemble du corpus en s'appuyant sur les mesures de précision, rappel et F-mesure. La précision d'une classe i correspond à la proportion de documents correctement attribués à leur classe i par rapport aux documents attribués à la classe i. Le rappel calcule la proportion de documents correctement attribués à leur classe i par rapport aux documents appartenant à la classe i. La précision et le rappel moyens peuvent alors être calculés par rapport à l'ensemble des classes. Un compromis entre la précision et le rappel est alors calculé en mesurant la Fmesure (la F-mesure est la moyenne harmonique du rappel et de la précision).
Le système de vote mis en place étant décrit, les différents classifieurs utilisés par Copivote sont succinctement présentés ci-dessous. Une description plus précise de ces derniers est donnée dans (Plantié, 2006). ? Bayes Multinomial. La méthode de Bayes Multinomial (Wang et al., 2003) qui est une approche classiquement utilisée pour la catégorisation de textes combine l'utilisation de la loi de Bayes bien connue en probabilités et la loi multinomiale.
? Les Machines à Vecteurs Support (Support Vector Machine -S.V.M.). La méthode
Machines à Vecteurs Support (Joachims, 1998;Platt, 1998) consiste à délimiter par la frontière la plus large possible les différentes catégories des échantillons (ici les textes) de l'espace vectoriel du corpus d'apprentissage. Les vecteurs supports constituent les éléments délimitant cette frontière : plus la frontière est large, plus les risques d'erreurs de classification sont rares. ? Les réseaux RBF (Radial Basis Function). Les réseaux RBF sont fondés sur l'utilisation d'un réseau de neurones à fonctions radiales de base. Cette méthode utilise un algorithme de « clustering » de type « k-means » (MacQueen, 1967) avec application d'une méthode de régression linéaire. Cette technique est présentée dans (Parks & Sandberg, 1991).
Résultats
Le tableau 1 montre que la procédure de vote que nous avons mise en place améliore globalement les résultats. Notons que toutes les procédures de vote permettent une amélioration du même ordre même si des résultats légèrement meilleurs sont retournés avec le « vote par somme pondérée ». Dans un deuxième temps, nous pouvons noter que l'utilisation des bigrammes (CopivoteBi) améliore globalement les résultats par rapport au traitement sans les utiliser (Copivote).
Le tableau 1 présente les méthodes de classification prises en compte dans le système de vote. Nous constatons que le classifieur de Bayes Multinomial est très performant avec un temps de calcul très faible. Les meilleurs résultats sont dans la grande majorité des cas obtenus par le classifieur SVM. Le classifieur RBF Network donne quant à lui des résultats décevants. 
Corpus
Conclusion et perspectives
La classification de textes d'opinion est un thème porteur. La mise en place du défi DEFT'07 montre l'intérêt de l'étude de ce type de textes par la « communauté fouille de textes ». Cet article présente une approche consistant en une combinaison de représentations mots-clés et bigrammes tout en utilisant un système de vote de plusieurs classifieurs. Les résultats obtenus se sont révélés particulièrement intéressants avec une valeur de F-mesure légèrement supérieure à la meilleure soumission de DEFT'07.
Dans nos futurs travaux, nous comptons utiliser des représentations combinées motsclés, bigrammes et trigrammes qui pourraient encore améliorer les résultats. Nous souhaitons également utiliser des procédures de vote avec un nombre plus important de classifieurs. Enfin, une étude plus globale en utilisant d'autres corpus et surtout des données textuelles de langues différentes pourraient être menée.

Introduction
Nous nous intéressons à la découverte de correspondances, ou mappings, entre ontologies distribuées modélisant les connaissances de pairs du système de gestion de données P2P (PDMS) SomeRDFS. Un PDMS est un système constitué de pairs autonomes qui communiquent pour répondre collectivement à une requête. Les communications entre pairs s'éta-blissent grâce à des mappings qui définissent des relations sémantiques entre leurs connaissances. Un PDMS est sollicité via l'interrogation d'un des pairs qui pourra ensuite faire appel aux autres pour répondre. Une spécificité des PDMS est que chaque pair ne connaît que ses propres connaissances et les mappings le connectant à d'autres pairs. Dans ce cadre, nous cherchons à augmenter le nombre de mappings de chaque pair afin d'améliorer les réponses fournies globalement par le système, en quantité et en qualité.
Nous travaillons, dans le cadre du projet MediaD (projet financé par France Telecom R&D), dont l'objectif est la création d'un environnement déclaratif de construction de systèmes de gestion de données P2P. Ces travaux ont conduit au développement de la plate-forme SomeRDFS (Adjiman et al., 2006) au sein de laquelle nous situons notre travail.
Nous présenterons dans un premier temps le contexte de notre travail. Nous montrerons ensuite comment les requêtes des utilisateurs peuvent être exploitées pour identifier des raccourcis de mappings ainsi que des relations cibles à partir desquelles des mises en correspondances intéressantes peuvent être trouvées. Étant données ces relations cibles, nous proposerons alors des techniques basées sur l'interrogation du système pour construire des ensembles de candidats à un mapping. Nous présenterons ensuite quelques travaux proches. Enfin, nous conclurons et présenterons quelques perspectives.
Contexte de travail
Dans le contexte de SomeRDFS, les ontologies et les mappings sont exprimés en RDF(S) tandis que les données sont représentées en RDF. Il est ainsi possible de définir des classes, des sous-classes, des propriétés, des sous-propriétés, de typer le domaine et le co-domaine des propriétés. Les constructeurs autorisés sont l'inclusion de classes, l'inclusion de propriétés, le typage de domaine et de co-domaine. Ce langage est basé sur des relations unaires qui représentent les classes et des relations binaires qui représentent des propriétés.
Les ontologies des pairs sont représentées à l'aide d'expressions RDFS composées uniquement de relations appartenant au vocabulaire du pair. Nous notons P:R la relation R (classe ou propriété) de l'ontologie du pair P.
Les données d'un pair sont associées à des relations faisant partie de son vocabulaire. Un mapping correspond à une inclusion entre classes ou propriétés de 2 pairs différents ou au typage d'une propriété d'un pair donné avec une classe d'un autre pair (cf. TAB. 1). Ainsi les mappings sont également des expressions RDF(S). Leur spécificité est d'être construites à partir du vocabulaire des ontologies de pairs différents qui établissent ainsi des correspondances sémantiques entre eux. Chaque pair P peut être sollicité à l'aide de requêtes exprimées avec son propre vocabulaire.
Mappings
Notation LD Traduction en logique du premier ordre Inclusion de classes P1:C1 P2:C2 ?X, P1:C1(X) ? P2:C2(X) Inclusion de propriétés
TAB. 1 -Mappings
Le calcul des réponses aux requêtes se fait en deux temps. Les requêtes sont d'abord ré-écrites en un ensemble de requêtes les subsumant. Le calcul des réécritures maximales de chaque atome d'une requête fournit un ensemble de conjonctions de relations (classes ou propriétés). Les requêtes sont ensuite propagées du fait de l'existence de mappings avec des relations d'autres pairs. Ces derniers transmettront les réécritures obtenues. Ces réécritures seront ensuite évaluées afin d'obtenir les données associées.
Exploitation des réponses aux requêtes utilisateurs 3.1 Découverte de raccourcis de mappings
Un raccourci de mappings est un mapping résultant de la composition de mappings existants. Les raccourcis de mappings renforcent le réseau en créant des liens directs entre des relations de deux pairs différents là où jusqu'alors n'existaient, dans le PDMS, que des liens indirects. L'objectif n'est pas de rajouter ce type de mapping systématiquement mais de façon sélective. En effet ces mappings peuvent être intéressants à représenter car, bien qu'ils ne permettent pas d'aboutir à des réponses plus riches, ils peuvent être très utiles en cas de disparition de pairs du PDMS. La découverte des raccourcis de mappings à repésenter peut être automatisée. Nous proposons, pour cela, d'exploiter le mécanisme de réponse aux requêtes des utilisateurs et d'appliquer ensuite des techniques de sélection des raccourcis pertinents à représenter.
Concernant le traitement des requêtes utilisateurs aboutissant à l'obtention des données, nous proposons de dissocier les deux étapes du traitement qui sont : (1) le calcul des réécritures maximales de chaque atome de la requête posée, (2) l'évaluation des réécritures. Cette dissociation est intéressante lorsqu'un utilisateur ne trouve pas, au sein du vocabulaire du pair auquel il s'adresse, la relation lui permettant de définir précisément les données qu'il recherche, et qu'il est dans l'obligation d'indiquer une autre relation. Cette autre relation peut être plus spécifique. Dans ce cas, toutes les réponses attendues ne seront pas obtenues. Elle peut être plus générale. Dans cet autre cas, elle permettra d'obtenir toutes les réponses attendues par l'utilisateur mais contiendra, en revanche, également des réponses inutiles. Ainsi, par exemple, si l'utilisateur s'adresse au pair P 1 dans l'espoir d'obtenir les données de la classe SteelSculptor, il peut, en l'absence de la classe SteelSculptor dans P 1 , s'intéresser aux données d'une classe plus générale en posant la requête Q 2 (X) ? P 1 :Artist(X). Il obtiendra, parmi les réécritures, P 2 :SteelSculptor(X) indiquant que les données instanciant SteelSculptor(X) sont des artistes mais également P 2 :W oodSculptor(X) ou P 2 :GlassSculptor(X) dont il n'est pas utile de rechercher les données, compte tenu du besoin précis de l'utilisateur. Le fait de dissocier le calcul des réécritures de leur évaluation est, dans ce cas, intéressant. Il permet à l'utilisateur de sélectionner les réécritures pour lesquelles il demande l'évaluation.
Nous proposons de stocker les raccourcis de mappings correspondant à des liens directs avec des relations qualifiées d'intéressantes pour les utilisateurs. Ces relations, dites intéres-santes, appartiennent au vocabulaire de pairs distants. Elles sont plus spécifiques que celles du pair étudié. Elles n'apparaissent pas, pour l'instant, dans ses mappings, mais sont apparues dans des réécritures et les utilisateurs ont, à plusieurs reprises, demandé leur évaluation. L'ensemble des mappings potentiels ainsi stockés seront traités globalement ultérieurement pour sélectionner ceux qui seront effectivement ajoutés.
Identification de relations cibles
L'intérêt principal des mappings est de permettre de propager des requêtes à des pairs distants pour qu'ils contribuent aux réponses. Lorsqu'un utilisateur s'adresse à un pair, il arrive toutefois que les réponses proviennent toutes de ce pair. Cette situation révèle un manque de mappings de spécialisation entre les relations de ce pair et celles de pairs distants. L'identification de ce phénomène est possible par analyse des réponses obtenues aux requêtes utilisateurs, ou, plus précisément, par observation de la localisation des éléments qui composent les ré-ponses.
Une étude nous a permis de définir deux cas pour lesquels le calcul de réécritures est un indice pour trouver des relations appartenant à des pairs distants qui peuvent être rapprochées de relations du pair interrogé. Nous présentons successivement ces deux cas en nous limitant aux classes, le fonctionnement étant similaire pour les propriétés.
Cas 1 (cf . FIG 1) : Soient les pairs P 1 , P 2 et P 3 contenant respectivement les classes C 1 , C 2 et C 3 et les mappings suivants : P 1 :C1(X) ? P 2 :C 2 (X) et P 3 :C 3 (X) ? P 2 :C 2 (X) représentés chacun dans les deux pairs concernés.
1. L'utilisateur s'adresse à P 3 et pose la requête Q 4 (X) ? P 3 :C 3 (X) 2. SomeRDFS ne fournit aucune réécriture.
La découverte de mappings dans SomeRDFS
Cas 2 (cf . FIG 1) : Soient le pair P 1 contenant la classe C1 et le pair P 2 contenant les classes C 2 et C 3 telles que P 2 :C 2 (X) ? P 2 :C 3 (X). Nous supposerons que le mapping P 2 :C2(X) ? P 1 :C1(X) est représenté à la fois dans P 1 et dans P 2 .
1. L'utilisateur s'adresse à P 2 et pose la requête Q 5 (X) ? P 2 :C 3 (X). 2. SomeRDFS fournit la réécriture suivante : P 2 :C 2 (X). Aucune réécriture composée de relations de pairs distants n'est donnée.
Cas 1
Cas 2
FIG. 1 -Cas 1 et Cas 2
C 3 dans le cas 1 et C 2 dans le cas 2 sont respectivement appelées relations cibles car c'est à partir d'elles que des mises en correspondance intéressantes peuvent être trouvées comme nous le montrons dans la section qui suit. Les deux cas présentés ci-dessus sont des cas élémentaires qui, s'ils sont combinés, peuvent permettre de traiter des cas plus complexes. La notion de relation cible pourra être étendue à toute relation locale apparaissant dans une réécriture et généralisant une relation cible.
Identification de candidats pour la mise en correspondance
Nous proposons une procédure d'identification de candidats à un mapping, qui s'appuie sur les relations cibles, notées R CIBLE , supposées appartenir au pair P CIBLE . Sans liens de spécialisation avec d'autres pairs via des mappings, ces relations ne permettent pas de propager le raisonnement vers un autre pair. Pour chaque relation cible, nous nous proposons alors de déterminer, dans un second temps, un ensemble des relations entre lesquelles il serait pertinent de rechercher des mises en correspondance. Cet ensemble de relations sera appelé candidats au mapping et noté CM . Le processus de découverte de mappings nouveaux est donc un processus en trois étapes : (1) recherche des relations cibles, (2) recherche d'ensembles de candidats au mapping, (3) alignement des éléments de l'ensemble de candidats au mapping.
Notre approche pour la recherche d'ensembles de candidats au mapping est basée sur l'idée selon laquelle il est pertinent de rechercher des mises en correspondance entre des relations ayant des points communs. Dans le cadre de SomeRDFS, les points communs considérés seront (1) l'existence d'une relation commune plus générale, ou bien (2) l'existence d'une relation commune plus spécifique. Nous proposons un processus de recherche de candidats au mapping pour chacune de ces situations. Nous nous basons sur l'existence d'une relation commune plus générale lorsque R CIBLE n'a qu'une relation R g (au sein du même pair ou dans la définition d'un mapping) qui la généralise. Dans ce cas, nous proposons de poser une requête sur cette relation (Q(X) ? R g (X)) et d'en calculer les réécritures. Ces dernières fournissent un ensemble de relations plus spécifiques que R g entre lesquelles il est pertinent de rechercher l'existence de correspondances. Cet ensemble constitue CM . Si ?!R g /R CIBLE ? R g et Q(X) ? R g (X) alors CM = l'ensemble des relations correspondant à des réécritures de Q(X). Nous nous basons sur l'existence d'une relation commune plus spécifique lorsque R CIBLE a plusieurs relations R g (au sein du même pair ou dans la définition d'un mapping) qui la généralisent. Dans ce cas, l'ensemble de ces généralisants constitue un ensemble de candidats au mapping
Ces deux situations correspondent aux cas 1 et 2 décrits dans la section précédente.
La recherche de mappings devra ensuite être effectuée entre les éléments de l'ensemble CM . Cet ensemble comprend des relations de différents pairs, celui à qui la requête a été posée pour obtenir cet ensemble, et les pairs distants ayant contribué à la réponse à la requête via des réécritures. Le pair interrogé a une certaine compréhension des relations de CM faisant partie de son vocabulaire puisque celles-ci font partie de son ontologie. En revanche, les relations de CM appartenant au vocabulaire de pairs distants sont inconnues du pair interrogé. Pour lui, il s'agit de noms de relations isolées. Le problème d'alignement qui se pose consiste alors à mettre en correspondance des relations prises isolément avec des relations appartenant à une ontologie. Le travail que nous avons réalisé jusqu'alors permet d'isoler les ensembles de relations à aligner. Il doit être complété par l'application de techniques d'alignement appropriées. L'étude des techniques les plus adaptées fait partie de nos perspectives.
Travaux proches
Appliqué aux systèmes P2P, le problème d'alignement peut être résolu de différentes façons. Certains travaux comme Piazza (Halevy et al., 2004) proposent l'application de techniques éprouvées testées dans le cadre de systèmes d'intégration mais supposent, pour l'alignement, que les ontologies de tous les pairs sont connues de tous, ou du moins accessibles dans leur totalité par tous. D'autres travaux font intervenir des ontologies dites de référence avec lesquelles l'ontologie de chaque pair peut être liée, ce qui évite la mise en relation directe des ontologies des pairs les unes avec les autres (Herschel et Heese, 2005). Enfin, une troisième catégorie de travaux consiste à étudier le problème de l'alignement d'ontologies lorsque toutes les ontologies composant le système P2P sont distribuées et qu'il en existe aucune qui soit connue de tous et qui puisse servir d'ontologie de référence. Ce problème a été étudié dans le système Helios (Castano et al., 2003) où la découverte de mappings est basée sur l'interrogation des pairs du réseau.
Par rapport à cet état de l'art, l'approche que nous proposons dans ce papier est spécifique au contexte distribué, tous les pairs étant considérés de la même façon et les ontologies étant réparties entre tous les pairs du système. En ce sens, elle se rapproche de la 3ème catégorie de travaux décrite ci-dessus. Nous exploitons toute la richesse du raisonnement pouvant être mis en oeuvre au sein d'un PDMS, suite à l'envoi de requêtes. Toutefois, contrairement aux travaux de (Castano et al., 2003), les requêtes ne sont propagées qu'à un nombre réduit de pairs, elles ne sont pas envoyées à l'ensemble des pairs du PDMS. Par ailleurs, les requêtes exploitées pour la découverte de mappings ont toutes la forme de requêtes utilisateurs. Elles ne néces-sitent donc pas de concevoir des modules de traitement de requêtes spécifiques. L'originalité de notre approche consiste donc à réutiliser les mécanismes de raisonnement mis en oeuvre dans SomeRDFS de façon à cibler les éléments à rapprocher puis à réutiliser ou adapter les techniques d'alignement qui ont été expérimentées dans d'autres contextes et dont les résultats se sont avérés être de qualité.
Conclusion
Au travers du travail décrit dans ce papier, qui s'inscrit dans le cadre de systèmes P2P, nous avons étudié comment tirer parti du processus de raisonnement logique mis en oeuvre dans SomeRDFS, dans le but d'aider à la découverte de correspondances entre ontologies. Nos perspectives portent sur l'étude des techniques usuelles capables, en particulier, d'aligner des éléments considérés de façon isolée avec d'autres dont on connait précisément l'ontologie à laquelle ils appartiennent (Reynaud et Safar, 2007;Kefi et al., 2006). Nous étudierons égale-ment la découverte de mappings prenant appui sur des requêtes ayant la forme de conjonction de relations. Enfin, des tests à plus grande échelle seront réalisés, mettant en jeu un nombre de pairs importants dotés d'ontologies de taille significative.

Introduction
Il est courant de séparer le domaine de l'apprentissage automatique en deux domaines distincts. D'un coté, l'apprentissage supervisé désigne un cadre où les exemples sont reliés à une information relative à leur classe, à un concept. Les méthodes supervisées produisent par la suite, à partir d'une base d'exemples d'apprentissage pour lesquels la classe est connue, une règle de décision visant à prédire la classe de nouvelles observations. Cette règle de décision, appellée aussi classifieur ou hypothèse, peut être considérée géométriquement comme une hypersurface séparant les exemples représentés dans un espace multidimensionnel.
A contrario, cette notion de classe ou de concept est absente dans le cadre de l'apprentissage non supervisé. Aucune information a priori n'étant disponible, les techniques non supervisées visent à détecter des structures de groupes fondées sur des notions de distance ou de similarité entre les exemples (Lerman (1970)). C'est précisement dans ce cadre que se place ce travail de recherche qui part d'un constat simple : en classification supervisée, des méthodes dites ensemblistes ont au cours des dernières années montré des performances tout à fait intéressantes. Dans le cas particulier du boosting, il est question d'un processus itératif qui va repondérer les exemples en insistant sur ceux mal classés par la méthode d'apprentissage à une itération donnée (Freund et Schapire (1997)).
Dès lors, il est tentant de vouloir s'inspirer d'un processus comme le boosting dans le domaine de la classification non supervisée. Cette transposition n'est pas du tout immédiate et soulève un certain nombre de problèmes : tout d'abord, le boosting repose sur des justifications théoriques solides et il serait présomptueux de prétendre appliquer rigoureusement une telle méthode. C'est pourquoi la contribution de ce travail doit simplement être considérée comme une approche ensembliste inspirée des grands principes du boosting. Dans un premier temps, il faut donc se confronter aux problèmes classiques liés aux ensembles de regroupeurs, présentés dans la section 2. D'autre part, la notion d'exemple "difficile", intuitive en apprentissage supervisé (les exemple difficiles sont par essence les exemples mal classés par la méthode d'apprentissage de base), reste, dans le domaine non supervisé, à définir. Si l'on veut, par analogie avec le boosting, insister à chaque itération sur les exemples difficiles, il faut précisement pouvoir les détecter, c'est-à-dire évaluer la qualité individuelle de bonne classification d'un exemple. Dans cet article, l'approche UBLA (Unsupervised Boosting-Like Approach) est proposée, qui détecte et repondère les exemples difficiles à regrouper à partir d'une partition floue, pour construire itérativement une matrice de co-association qui permettra de former finalement une partition dure plus pertinente au sens de certains critères de qualité.
Les ensembles de regroupeurs
La problématique des ensembles de regroupeurs consiste à combiner les résultats de plusieurs algorithmes de partitionnement (ex : centres mobiles) afin de former une partition plus pertinente des différentes instances. On trouvera une présentation des méthodes les plus populaires dans Hornik (2004). Les applications dans les domaines du rassemblement et de la réutilisation des connaissances sont nombreuses mais les ensembles de regroupeurs peuvent aussi permettre de combiner des partitionnements obtenus à partir de sous-ensembles d'individus ou d'attributs différents. La formation d'un ensemble de regroupeurs se heurte à certaines difficultés. D'une part, l'absence d'information sur la classe des instances pose le réel problème de la correspondance entre les classes construites par les différents partitionnements. D'autre part, la construction de la partition finale doit s'appuyer sur une méthode de consensus efficace, qui tient aussi compte de la qualité des différents partitionnements.
En ce qui concerne la correspondance des classes, Fred (2001) a proposé un index de cohérence qui va calculer la similarité entre deux classes de deux partitions différentes au sens du plus grand nombre de points partagés. La procédure, connue aussi sous le nom de high matching score, va donc désigner itérativement les deux classes possédant le plus grand score de correspondance. Strehl et Ghosh (2002) ont quant à eux proposé une fonction objectif qui pourrait assurer de trouver le partitionnement idéal, partageant le plus d'information possible avec les partitionnements de base. Ils font appel à la notion d'information mutuelle normalisée. Dans Dimitriadou et al. (2001), il est question d'une méthode pour laquelle un ensemble de partitionnements durs ou flous peut être combiné afin de former une partition floue du jeu de données. La partition finale est formée par un vote qui minimise une fonction de dissimilarité entre les partitions initiales. Cette approche ne résiste pas à l'obstacle de la correspondance des groupes en considérant toutes les permutations possibles des matrices d'appartenance. Fred (2001) propose de contourner le problème de la correspondance des groupes en utilisant une matrice de co-association individus-individus qui ne considère que la fréquence avec laquelle deux exemples se retrouvent dans un même groupe.
Certaines méthodes ensemblistes supervisées ont déjà été l'objet de transpositions au domaine non supervisé. Leisch (1999) a par exemple appliqué le bagging en contexte non supervisé pour proposer l'algorithme bagged clustering, que nous utiliserons comme sous-procédure de notre contribution. L'application du boosting à la classification non supervisée a déjà été abordée dans Frossyniotis et al. (2004), où la méthode boost-clust repondère itérativement les exemples difficiles et forme une partition floue optimale. Toutefois, cet algorithme transpose par analogie et sans réelles justifications des concepts justifiés en contexte supervisé. L'apport le plus significatif des auteurs réside dans l'utilisation des vecteurs d'appartenance des partitions floues pour détecter les exemples sensibles. Les expérimentations proposées, trop succintes, ne permettent malheureusement pas d'évaluer de façon satisfaisante l'efficacité de la méthode et de ses différentes variantes.
L'algorithme UBLA
L'algorithme UBLA (Unsupervised Boosting-Like Approach) constitue une nouvelle approche inspirée du boosting, qui va construire itérativement une partition dure des données. L'algorithme se déroule en quatre phases, trois phases sont répétées à chaque itération de la procédure tandis que la quatrième et dernière phase établit la partition finale :
1. Chacune des t itérations de la procédure "boostée" commence par une phase d'évalua-tion qui va permettre la sélection des exemples difficiles. Au cours de cette évaluation les c-moyennes floues (Bezdek (1981)) sont effectuées une seule fois. Le caractère flou est utilisé pour calculer les critères de qualité à partir des degrés d'appartenance et les exemples vont être repondérés directement à la fin de cette phase d'évaluation.
2. Une deuxième phase commence alors, appelée phase de regroupement/stabilisation, qui constitue la sous-procédure de bagged clustering, où les c-moyennes floues classiques sont appliquées sur dix échantillons bootstrap obtenus à partir de la nouvelle distribution des poids. Il est important de noter que le fait d'opérer la repondération à la fin de la première phase entraîne la non utilisation du jeu de données initial pendant tout l'algorithme UBLA. Ainsi, même à la première itération, la procédure de bagging est effectuée à partir d'une nouvelle distribution des poids. Cet aspect implique que toutes les partitions sont obtenues à partir de distributions de poids non uniformes.
3. Le problème de correspondance entre les différents groupes est ici contourné par l'utilisation d'une matrice de co-association (Fred (2001)) qui intervient lors de la phase 3 de l'algorithme. Ainsi, les points sont considérés deux à deux et la matrice A, individus-individus, est mise à jour lorsque deux points sont dans une même classe pour une partition d'une itération donnée. 4. A la fin des itérations dites de "boosting", la partition finale est formée par un vote majoritaire sur la matrice A construite itérativement (phase 4). Avant de détailler le processus (algorithme 1), il semble indispensable de s'attarder dans les sous-sections suivantes sur quelques points essentiels de la méthode UBLA.
Le critère local de qualité d'un exemple
Une approche inspirée du boosting doit mettre l'accent par itérations successives sur les exemples dits difficiles à classer. Comment détecter de tels exemples ? Là où cette notion est très intuitive en apprentissage supervisé (un exemple difficile est par essence mal classé par le classifieur choisi), il convient en apprentissage non supervisé de quantifier la qualité du positionnement de chaque exemple, du fait -dans la grande majorité des cas-de la non connaissance a priori de la structure des données. Il est judicieux, tout comme Dans ce cas, l'entropie de Shannon de U 1 est maximale, soit E 1 = 2. Nous sommes dans l'incertitude la plus totale, l'exemple pouvant appartenir avec la même croyance à n'importe quelle classe. Au contraire, un exemple très bien classé, dont l'appartenance à une des quatre classes apparaît clairement, possédera un vecteur U de la forme suivante :
Ici, l'entropie de Shannon est minimale, soit E 2 = 0. Le cas succinctement exposé traduit le fait qu'un exemple devra être d'autant plus repondéré que son critère de qualité local, à savoir l'entropie de ses degrés d'appartenance, est fort. Ainsi, les exemples semblant difficiles à classer seront favorisés (aucun dégré d'appartenance à une classe ne se détache). Ce type de critère n'est d'ailleurs pas sans rappeler la notion de rejet en contexte supervisé (Leray et al. (2000)).
Le critère global de qualité
Parallèlement à la qualité locale d'un exemple, la repondération doit tenir compte conjointement de la qualité globale du partitionnement. D'aucuns y verront une analogie avec l'erreur en apprentissage utilisée par Freund et Schapire (1997) dans le boosting pour construire un coefficient qui entre en compte dans la favorisation exponentielle des exemples mal classés. Cette qualité globale est aussi et surtout envisagée par souci de logique. En effet, pourquoi repondérer un exemple apparemment difficile si la qualité de la partition dans laquelle il se trouve est médiocre ?
Pour conserver une certaine cohérence dans l'approche, il est logique que le critère global de qualité du partitionnement en cours soit la moyenne des critères locaux. Ainsi, à la b ieme itération, le critère de qualité C b est la moyenne des E i courants. Plus précisement, ce critère offre la possibilité de repondérer un exemple en tenant compte non seulement de la qualité de classement intrinsèque au partitionnement (c'est le critère local E i ) mais aussi de la qualité globale du regroupement dans lequel il se trouve (critère C b ).
La repondération des exemples difficiles
Les performances du boosting reposent en partie sur la construction d'un ensemble d'hypothèses faibles construites à partir de distributions statistiques dans lesquelles sont favorisés les exemples dits difficiles. C'est ce principe qui est transposé dans notre proposition en prenant pour hypothèse de base l'algorithme classique des c-moyennes floues. L'intérêt du choix d'une partition floue réside bien sûr dans la possibilité de raisonner en terme de degré d'appartenance. L'erreur en apprentissage est donc remplacée ici par deux critères de qualité, local et global, définis ci-dessus. Il s'agit par la suite de mettre à jour les poids de chaque exemple en tenant compte de la qualité globale de la partition puis de la qualité locale de classification d'un point. A chaque itération b, le poids d'un exemple p est ajusté de la façon décrite en (1) (cf algorithme 1). Par conséquent, les poids des exemples dont l'entropie des degrés d'appartenance est supérieure à la moyenne de l'échantillon sont augmentés (E i > C b ), tandis que ceux des autres se voient diminuer par normalisation. L'argument de l'exponentielle comporte deux termes. Le premier terme (log 2 K ? C b ) se rapproche de zéro lorsque la qualité globale de la partition est mauvaise (log 2 K est la borne supérieure du coefficient d'entropie à minimiser). Le deuxième terme (E i ) représente la qualité individuelle de classification d'un exemple i et est d'autant plus élévé que l'exemple est difficile. Logiquement, les exemples qui prendront exponentiellement le plus d'importance seront les points mal classés dans une partition de bonne qualité.
Utilisation d'une sous-procédure : le bagged clustering
Une autre approche inspirée des méthodes supervisées, en l'occurence le bagging (Breiman (1996)), peut être transposée en contexte non supervisé. Cette méthode, appelée bagged clustering a été proposée par Leisch (1999). La procédure combine une méthode de regroupement (centres-mobiles, c-moyennes floues) avec une classification hiérarchique. La nouveauté réside dans l'application de la méthode de clustering à plusieurs échantillons bootstrap du jeu de données de départ, puis dans la transposition du problème initial dans l'espace des centres formés par la méthode de base sur ces échantillons bootstrap. Le nombre de classes a priori est donc donné suffisament grand, le regroupement (centres-mobiles ou c-moyennes floues) est appliqué sur chaque échantillon bootstrap puis tous les centres finaux sont regroupés dans une matrice. Une classification hiérarchique sur les centres est ensuite effectuée puis chaque point est assigné à la classe qui contient le centre dont il est le plus proche. La figure 1 résume la procédure.
ETAPE 1 : A partir de la distribution des poids W, tirage de 10 échantillons Bootstrap du jeu de données.
-MEANS -MEANS -MEANS -MEANS FUZZY-C-MEANS FUZZY-C-MEANS FUZZY-C-MEANS FUZZY-C FUZZY-C FUZZY-C FUZZY-C-MEANS -MEANS FUZZY-C-MEANS -MEANS FUZZY-C FUZZY-C FUZZY-C-MEANS -MEANS FUZZY-C FUZZY-C-MEANS -MEANS FUZZY-C FUZZY-C FUZZY-C-MEANS -MEANS FUZZY-C-MEANS
ETAPE 2 : Application des c-moyennes floues sur chacun des échantillons avec un nombre élévé de centres. Par exemple, 15 centres. Récupération de la matrice finale des centres de taille (10 * 15 lignes, P colonnes), où P est le nombre d'attributs. ETAPE 3 : Classification hiérarchique ascendante sur la nouvelle matrice des centres. 1500   1000   500   Height   0   65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  97  98  99  100  89  90  91  92  93  94  95  96  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 ETAPE 4 : La partition dure finale est obtenue en coupant le dendrogramme au nombre de classes K désiré. Ensuite, parmi les K classes obtenues, chaque exemple est affecté à la classe qui contient le centre dont il était le plus proche parmi les 10*15 centres.
Cluster Dendrogram
FIG. 1 -La procédure de bagged clustering.
Il a semblé intéressant d'introduire ce processus comme une sous-procédure de l'algorithme, qui stabilisera les résultats et donc la partition formée, sans pour autant dénaturer l'idée originale de repondérations successives.
Complexité de l'algorithme
A l'intérieur des phases 1 et 2, la complexité algorithmique reste linéaire en fonction de N . Lors de la phase 3, la complexité est quadratique (O(N 2 )), à une constante près liée au nombre d'itérations T de boosting. On remarquera que l'utilisation d'une classification hié-rarchique n'augmente pas la complexité car celle-ci est effectuée sur la matrice des centres, dont la taille est liée à des constantes. Enfin, la formation de la partition finale (phase 4) s'effectue en O(N 2 ) par parcours de la matrice A. Finalement, c'est l'utilisation d'une matrice de co-association qui borne de façon générale la complexité de la procédure UBLA en O(N 2 ). , regrouper les deux exemples dans la même classe. Les éventuels exemples "seuls" formeront des singletons. fin
Expérimentations
Choix du critère de qualité externe de la partition finale
Un problème majeur de l'apprentissage non supervisé réside dans l'absence de juge de paix pour départager les différentes méthodes. En classification supervisée, ce juge existe et est aussi bien local (erreur de prédiction) que global (taux d'erreur). Nous avons donc arbitrairement choisi un juge de paix qui n'est ici que global et qui compare la qualité des partitions finales. Les différentes mesures de validation des partitions ont été bien résumées dans Halkidi et al. (2001). Pour valider expérimentalement les performances de l'approche UBLA, à savoir la qualité de la partition dure proposée, des indices fondés sur des mesures de compacité (dispersion intra-classe) et de séparation (dispersion inter-classe) semblent bien adaptés car les algorithmes de regroupement classiques cherchent également à optimiser des critères liés aux mêmes notions. Ainsi, des indices comme l'indice de Dunn, la silhouette moyenne et le ratio intra/inter (wb) sont tout à fait pertinents. Une combinaison de ces trois indices peut être intéressante dans les cas où les valeurs seront très serrées. Rappelons que l'indice de Dunn et la silhouette moyenne d'une partition sont à maximiser tandis que le ratio intra/inter est à minimiser. L'indice de qualité suivant : Ind = Dunn + silhouette ? ratiowb combinera donc simplement les trois indices et sera lui aussi à maximiser. La comparaison s'est effectuée entre la méthode UBLA, les centres-mobiles de MacQueen (1967), les c-moyennes floues de Bezdek (1981) et la procédure de bagged clustering classique de Leisch (1999), qui constitue rappelons-le une sous-procédure de notre proposition. Dans la grande majorité des cas, la meilleure partition domine les trois autres au sens des trois indices (maximisation de Dunn et de la silhouette moyenne, minimisation du ratio wb par rapport aux deux autres). Mais pour les rares cas plus incertains, la valeur finale de Ind est prise pour sélectionner la méthode formant la meilleure partition pour une expérimentation précise.
Résultats
La méthode UBLA a été testée dans l'environnement statistique R sur onze jeux de données, disponibles dans les bases de données bien connues du web. Comme le rappelait Diday (1974), les algorithmes de regroupement peuvent fournir des solutions satisfaisantes, mais qui ne sont pas forcément optimales. Ainsi, plusieurs répétitions des centres-mobiles sur un même jeu de données peuvent donner des résultats différents. Par conséquent, nous avons pour nos expérimentations rélancé les différents algorithmes dix fois et sélectionné le meilleur résultat pour chaque méthode. Il serait d'ailleurs intéressant d'analyser la stabilité des partitions des algorithmes de regroupement comme dans Bertrand et Bel Mufti (2006). Les caractéristiques des jeux de données (nombre d'individus et de variables) sont rappellées dans le tableau suivant. La valeur de l'indice de qualité Ind, le nombre d'itérations T et le nombre final de groupes formé par notre méthode (qui peut rappelons-le être différent de K), sont également introduits dans le tableau 1. Dans le cas où le nombre K final de classes formées par U BLA était différent du K initial, c'est avec ce K final que le calcul de l'indice de qualité, qui dépend du nombre de classes, s'effectue pour les centres-mobiles et les c-moyennes floues (ceci assure la cohérence des comparaisons des partitions formées).
Les résultats obtenus sont assez intéressants. Sur un total de 55 expérimentations, la mé-thode UBLA surpasse les centres mobiles et les c-moyennes floues dans 47 cas. (Il y a en plus un ex-aequo). En considérant les comparaisons de UBLA aux deux méthodes de manière indé-pendante, il est observé que, toujours au sens de l'indice de qualité Ind, notre approche domine d'une part les centres-mobiles dans 39 cas sur 45, de l'autre améliore les c-moyennes floues dans 41 cas sur 45. La méthode UBLA est supérieure au bagged clustering dans 37 cas sur 55 et égale dans 8 cas sur 55. Nous pouvons constater que le nombre d'itérations est généralement assez faible, contrairement à ce qui pouvait être attendu d'une procédure de boosting. Un trop grand nombre d'itérations peut même dans certains cas entraîner des chutes de performances, les répondérations successives des exemples déformant trop la structure initiale du jeu de données. Pour conclure, nous pouvons dire que les résultats montrent qu'un processus ensembliste 
Illustration graphique
La figure 2 compare les partitions formées par l'approche UBLA et des c-moyennes floues pour le jeu de données des IRIS de Fisher, bien connu des statisticiens. Il s'agit de 150 individus (des plantes) décrits par 4 variables. Il existe donc six plans de représentation possibles pour ces données. Pour cet exemple, le nombre de classes donné a priori était K = 2. La figure 3 illustre quant à elle la classification en trois classes du jeu de données DNase (deux dimensions).
classification des iris par les c?moyennes classiques c l assification des iris par la méthode UBLA 
Conclusion
Dans cet article, nous avons proposé une nouvelle approche ensembliste en apprentissage non supervisé, qui s'inspire des principes du boosting. A chaque itération, l'algorithme repère au sens de certains critères les exemples difficiles pour leur donner plus d'importance à l'itéra-tion suivante. Cette approche se sert ainsi dans un premier temps d'une partition floue, formée par la méthode des c-moyennes floues pondérées, pour glisser ensuite vers une partition finale dure. 
FIG. 3 -Visualisation de la classification du jeu de données DNase (2D) par les c-moyennes classiques (à gauche) puis UBLA (à droite). Nous remarquons une zone sensible au milieugauche de la figure ou deux classes ne sont pas nettement séparées par les c-moyennes floues. A droite, l'incertitude est corrigée.
Les premiers résultats sont prometteurs et laissent penser qu'une telle approche peut amé-liorer la qualité des partitions finales formées en termes de compacité et de séparation. A présent, il serait pertinent de comparer les performances de la méthode UBLA avec plusieurs autres approches ensemblistes proposées dans le domaine non supervisé (ex : Frossyniotis et al. (2004)). Parmi les perspectives de ce travail, il faudrait trouver une alternative à la matrice de co-association qui est coûteuse et proposer de nouveaux critères de qualité. Par exemple en implémentant des critères qui ne soient pas essentiellement fondés sur des mesures de dispersion intra-et inter-classe. Il serait de plus envisageable de s'affranchir de l'utilisation de la procé-dure de bagged clustering pour implémenter notre propre procédure de stabilité, fondée sur des tirages bootstrap ou des modifications de l'échantillon par rapport à la nouvelle pondération.
Pour conclure, nous pouvons dire que la transposition du boosting à l'apprentissage non supervisé doit se faire avec la plus grande prudence. En effet, des concepts justifiés théo-riquement en contexte supervisé ne s'appliquent pas de façon directe en classification non supervisée. Notre travail se veut donc simplement inspiré de certains concepts du boosting, principalement la repondération des exemples difficiles. Il reste maintenant à approfondir le cadre théorique de la méthode proposée.
Références Bertrand, P. et G. Bel Mufti (2006 

Introduction
Dans le domaine du bâtiment, une masse croissante de normes régissent l'exécution des projets de construction (e.g. bâtiments publics, maisons individuelles) et de nombreuses initiatives 1 sont lancées pour fournir des services électroniques de régulation. Un des objectifs généraux en est l'automatisation du contrôle de la conformité d'un projet de construction par rapport à un ensemble de normes techniques du bâtiment en vigueur. Cela constitue le cadre de notre travail au CSTB 2 et nous proposons ici un modèle de contrôle de conformité. Les projets de construction sont maintenant communément décrits dans le modèle IFC 3 , un modèle orienté objet développé par l'IAI 4 pour faciliter l'interopérabilité dans le domaine de la construction. Il est pourvu d'une syntaxe ifcXML 5 ; des données ifcXML peuvent être automatiquement générées par les outils de COA dédiés à l'architecture ou par les convertis- www.iai-international.org/Model/IFC(ifcXML)Specs.html seurs de données EXPRESS 6 . Cependant, le langage de représentation ifcXML ne permet pas de capturer toute la sémantique des données d'un projet de construction indispensables pour vérifier la conformité de ce projet (Lima et al. 2006). Pour ce faire, nous proposons de construire une ontologie dédiée au contrôle de conformité à partir des classes du modèle IFC dont le choix est guidé par les concepts retrouvés dans les normes techniques. Ensuite, nous extrayons des données IFC des représentations de projets de construction qui reposent sur cette ontologie. Dans la perspective d'un service électronique de régulation disponible sur le web, nous adoptons les langages du web sémantique pour représenter formellement projets de construction, normes techniques et ontologie de contrôle de conformité.
Le coeur de notre modèle du contrôle de conformité consiste en l'appariement des repré-sentations des normes avec celles des projets de construction. Son efficacité repose sur l'acquisition de connaissances ontologiques et sur le processus d'extraction de la représenta-tion d'un projet orienté contrôle de conformité, qui prend en compte les connaissances ontologiques acquises. Elle repose ensuite sur des méta-connaissances acquises auprès des experts du CSTB qui permettent de guider le contrôle lui-même. Enfin, nous extrayons des annotations des normes et organisons leurs représentations selon ces méta-annotations. Cela permet de définir un ordonnancement des appariements à réaliser pour valider un projet par rapport à ces normes. Ces annotations sont également utilisées pour expliquer à l'utilisateur les résultats du processus de validation, en particulier en cas d'échec.
Dans la section 2 nous décrivons le processus d'acquisition de la description d'un projet de construction, guidé par l'ontologie de contrôle de conformité. La section 3 est dédiée au modèle de contrôle de conformité proprement dit et la section 4 présente l'ensemble de la méthode et du système C3R.
Acquisition de la représentation utile d'un projet
Guidés par le but spécifique du contrôle de conformité, nous définissons un modèle de représentation d'un projet de construction qui ne contient que les éléments utiles au processus de contrôle.
Notre méthode d'acquisition de connaissances requiert dans une première étape d'expliciter des représentations formelles des normes techniques. Nous ne nous intéressons pas ici au problème d'extraction de connaissances à partir de textes qui dépasse le cadre de notre travail. Dans notre cas, nos avons explicité manuellement auprès d'experts du CSTB et à partir de documentations techniques une base de contraintes représentant des normes relatives à l'accessibilité des bâtiments. Nous les représentons dans le langage SPARQL. En effet, une norme de construction peut être représentée par un ensemble de couples des requê-tes. Dans chaque couple, la première requête exprime une condition d'application (e.g. D'autre part, nous exploitons le CD REEF 7 , afin de développer des annotations RDF des requêtes de conformité contenant des méta-connaissances sur les normes de conformité, à partir desquelles nous avons formalisé ces requêtes. Ce second type d'annotation est relatif à l'origine de la requête de conformité (e.g. référence à un texte légal d'où elle a été extraite, article d'extraction, date de publication, etc.). Elles sont utilisées pour organiser la base de requêtes et pour expliquer à l'utilisateur les résultats du contrôle de conformité d'un projet.
La seconde étape de notre méthode d'acquisition est dédiée à la construction automatique d'une ontologie de contrôle de conformité, à partir des entités du modèle IFC intervenant dans les représentations des normes. Précisément, nous construisons une hiérarchie de concepts dans le langage RDFS et nous l'enrichissons ensuite par des concepts non IFC qui apparaissent dans la description des normes. La place de ces nouveaux concepts dans l'ontologie est identifiée par un expert du domaine.
Vient alors l'étape d'acquisition de la représentation d'un projet de construction utile au contrôle de conformité. Cette représentation est extraite des données IFC relatives au projet. Le processus d'acquisition est guidé par l'ontologie de contrôle construite lors de l'étape précédente. Nous extrayons une représentation RDF d'un projet de construction par transformation XSLT de données ifcXML qui ne conserve que la partie de la description des entités/relations IFC utile au contrôle de conformité.
Les requêtes SPARQL qui représentent les normes techniques que les projets de construction doivent satisfaire jouent ainsi le rôle de design pattern dans la construction des représentations de projets. De fait, nous ne conservons de la transformation XSLT des données ifcXML que les triplets RDF qui font intervenir des concepts présents dans l'ontologie de contrôle de conformité.
La représentation RDF d'un projet de construction extraite par transformation XSLT de données ifcXML est ensuite enrichie automatiquement par l'application en chaînage avant des règles ontologiques définies par des experts. C'est ce qui pourra rendre possible l'appariement de la représentation d'un projet avec celle d'une règle lorsque celle-ci fait intervenir des concepts non IFC.
Modèle de contrôle de conformité
Notre modèle du contrôle de conformité repose sur l'appariement des représentations des normes avec celles des projets de construction : l'appariement d'une requête SPARQL de conformité à une représentation RDF d'un projet de construction. Ces opérations d'homomorphisme de graphes sont maintenant bien connues et nous nous reposons sur les travaux de (Baget, 2005) et (Corby et al., 2006).
Afin de rendre efficace le processus de contrôle, la base des requêtes de conformité représentant les normes est organisée selon leurs méta-annotations (extraites du CD-REEF) de sorte qu'un ordre des appariements à effectuer peut être défini qui capture l'expérience experte acquise. Nous avons ainsi explicité auprès d'experts du CSTB des critères de classification des requêtes correspondant a une classification des normes que représentent les requê-tes : thématique (e.g. accessibilité, acoustique), domaine d'application (e.g. établissements recevant du public), sujet d'une règle (e.g. elle s'applique à un « hall d'entrée »), etc. D'autre part, nous classons les requêtes de conformité sur la base des relations de spécialisa-tion/généralisation qui existent entre leurs patterns de graphes. Ainsi, s'il n'existe pas d'appariement d'une requête donnée à l'annotation d'un projet de construction, nous évite-rons de chercher inutilement des appariements des requêtes dont les graphes patterns seraient des spécialisations de celui de la première : il n'en existe pas.
La formalisation du raisonnement expert passe premièrement par l'analyse de l'ensemble des requêtes de conformité choisies par l'utilisateur, afin de vérifier la non existence de contradiction entre ces requêtes. Ensuite le choix par l'utilisateur de certaines requêtes peut suggérer la prise en compte d'autres requêtes complémentaires qu'il s'agit alors de lui proposer d'intégrer à sa sélection. Le traitement de requêtes dépend de l'organisation de la base de requêtes. Un algorithme d'application des requêtes de conformité modélisant un raisonnement expert prend en compte les principes suivants : (i) l'ordre de traitement des requêtes dépend des priorités relatives entre les classes de requêtes (e.g. sur une classification des requêtes selon leur niveau hiérarchique, celles représentant des lois sont prioritaires à celles représentant des décrets) ; (ii) au sein d'une même classe de requêtes, celles relatives aux connaissances les plus spécialisées sont prioritaires, c'est-à-dire celles dont le graphe pattern est une spécialisation de celui d'une autre (e.g. une requête relative à une porte d'entrée est prioritaire par rapport la même requête relative à une porte car dans le cas de non-conformité à la première, on en déduit la non-conformité à la seconde).
Les résultats de l'appariement sont ensuite analysés pour interpréter les causes d'une éventuelle non validation d'un projet de construction. Il s'agit tout d'abord de lister les requêtes de conformité qui échouent : (i) qui ont effectivement échoué ; (ii) pour lesquelles on peut déduire automatiquement l'échec (celles dont le graphe pattern est plus général que d'autres qui ont échoué et celles dont l'annotation représentant la condition d'application est plus générale que celles d'autres pour lesquelles l'appariement de leur annotation avec l'annotation du projet a échoué) ; (iii) qui ont échoué car l'annotation du projet de construction ne contient pas les connaissances nécessaires à la réalisation un appariement.
L'aboutissement du processus de contrôle de conformité est la génération d'un rapport de conformité. Pour chaque requête, le rapport indique si elle a réussi ou échoué ; dans le cas d'un échec, il indique les causes de non-conformité ou les insuffisances de l'annotation du projet (pour chaque requête qui échoue, quels éléments de l'annotation du projet de construction sont responsables de l'échec, c'est-à-dire empêchent l'appariement).
C3R : un modèle et un système
Nous avons baptisé le modèle de contrôle de conformité que nous proposons C3R ( fig.1). Les principales étapes de C3R sont les suivantes : (1) Acquisition des requêtes de conformité ; (2) Acquisition des connaissances ontologiques ; (3) Organisation hiérarchique des requêtes ; (4) Acquisition de l'annotation d'un projet de construction ; (5) Validation par appariement et méta-raisonnement ; (6) Génération d'un bilan de conformité. L'implémentation de C3R est en cours ; elle repose sur le moteur sémantique CORESE 8 développé à l'INRIA (Corby et al., 2006). Il offre une implémentation des langages RDF,8 Conceptual Resource Search Engine, http://www-sop.inria.fr/acacia/soft/corese RDFS et SPARQL. Les mécanismes de raisonnement reposent sur une représentation interne des connaissances ontologiques et assertionnelles dans le modèle des graphes conceptuels (Sowa, 1984) (Berners-Lee, 2001). Il permet de chercher les réponses à une requête SPARQL relativement à une base RDF, en prenant en compte des connaissances ontologiques représentées en RDFS (Corby et Faron-Zucker, 2007). Il est en outre muni d'un moteur de règles d'inférence qui permet en chaînage avant de compléter une base de faits RDF à l'aide de règles ontologiques représentées sous la forme de couples de graphes RDF (Baget et Mugnier, 2002).
FIG. 1 -C3R in a nutshell
Travaux connexes
D'autres travaux de recherche sont en cours visant à la standardisation des représenta-tions de projets de construction -sans s'intéresser cependant au problème particulier du contrôle de conformité. Parmi eux, certains adoptent comme nous une approche ontologique, (i) buildingSMART (Bell et Bjorkhaus, 2006)  Le problème du contrôle de conformité se pose dans d'autres domaines d'applications et nos travaux se rapprochent en ce sens de ceux sur la validation, parmi lesquels nous pouvons citer (Dibie-Barthélemy et al., 2004). Leur travail est consacré au problème de la validation de bases de connaissances construites sur le modèle des graphes conceptuels (Sowa, 1984), des contraintes étant exprimées sous la forme de graphes conceptuels certifiés fiables.
Conclusion et perspectives
Nous avons présenté le modèle C3R pour le contrôle semi-automatique de la conformité d'un projet de construction aux normes techniques du bâtiment en vigueur. Nous adoptons une approche ontologique pour résoudre ce problème et le système qui implémente C3R repose sur les langages et techniques du web sémantique. Un projet est valide par rapport à une norme si la représentation RDF du projet est une réponse à la requête SPARQL de conformité ; une norme s'applique à un projet si l'annotation de la requête de conformité peut être RNTI -X -appariée à celle du projet. L'acquisition de connaissances ontologiques est la clé de voûte de notre approche. L'extraction de la représentation d'un projet de construction à partir de données IFC est guidée par une ontologie de contrôle de conformité que nous construisons à partir du modèle IFC et par explicitation de connaissances expertes. Les requêtes de conformité et leurs annotations sont acquises à partir de données techniques et par explicitation de connaissances expertes au CSTB. L'organisation de la base de requêtes repose sur les annotations de celles-ci ; elle rend plus efficace et explicable à l'utilisateur le processus de contrôle de conformité d'un projet dans son ensemble. L'implémentation de C3R est en cours ; un premier prototype simple existe et nous pré-parons l'organisation de la base de requêtes et son évaluation auprès d'experts du CSTB.

Introduction
Les systèmes de supervision de la plupart des applications industrielles génèrent une très grande quantité d'informations et les collectent dans des bases de données. Ce papier concerne la découverte de modèles de chroniques à partir de séquences d'événements. Chaque événe-ment appartient à une certaine classe. Selon l'approche stochastique (Le Goc et al. (2005)), un ensemble de séquences est représenté sous la forme d'une chaîne de Markov afin de l'utiliser par la suite pour générer un modèle de chroniques (Le Goc et al. (2005)) sous forme de relations binaires entre classes d'événements C i ? C o . Le nombre des relations binaires peut être très grand, par conséquent une réduction de ce nombre est nécessaire. Pour cela, nous proposons une adaptation de la J-Measure de la théorie de l'information aux chaînes de Markov, la BJ-Measure, pour formuler des heuristiques d'élimination d'hypothèses.
Élagage d'un modèle de chroniques
Considérant la propriété d'absence de mémoire de la chaîne de Markov, la relation C i ? C o entre deux classes C i et C o peut être considérée comme l'une des quatre relations entre deux variables aléatoires binaires
Nous mesurons cet écart par la formule suivante :
Soit S = {C i ? C o } un ensemble de relations binaires construites à partir de la séquence ?. Selon la propriété d'absence de mémoire de la chaîne de Markov, les relations binaires contenues dans S sont indépendantes. L'ensemble S est vu comme une succession de plusieurs canals binaires de transmissions sans mémoire. La BJ-Measure d'un chemin M = {C i ? C i+1 } i=0...n?1 est le produit de nombre de relations binaires et la somme des BJ-Measure de chaque relation binaire
i=0,...,n?1 
FIG. 1 -Expertise (1995)
FIG. 2 -Relations observées en 2007
des occurrences associées à la variable appelée omega. La séquence étudiée contient 7682 occurrences de classes d'événements. Le nombre des relations binaires générées est 3199999. L'application de l'heuristique L(M ) permet d'élaguer l'ensemble des relations afin de garder que 195 ? 1 relations binaires. Grâce à la définition de la notion de classe, nous avons construit un modèle fonctionnel en substituant chacun des identifiants de classe par la variable associée. Le graphe de la figure 2 indique les variables ayant un impact sur la variable omega. Ce graphe peut être comparé avec les connaissances a priori formulées par les experts en 1995 (cf Figure  1). Le graphe (Figure 1) donné par les connaissances des experts est inclus dans celui donné par l'Approche Stochastique (figure 2) sauf en ce qui concerne le sens de la relation entre les variables F T et BD.
Références
Le Goc, M., P. Bouché, et N. Giambiasi (2005) 
Summary
In this paper, we propose to adapt the Information Theory J-Measure to Markov chains, the BJ-Measure, to define heuristics to prune the set of binary relations generated by the stochastic approach.

Introduction
L'objectif de nos travaux est de faciliter la recherche d'information dans des pages Web par l'utilisation conjointe de treillis de Galois et d'ontologies, qui constitue ce que nous appelons un « contexte conceptuel ». Les regroupements conceptuels fournis par les treillis, associés aux liens sémantiques de l'ontologie, permettent d'améliorer la recherche d'information en fournissant des niveaux de navigation plus abstraits et complémentaires.
Les treillis de Galois restent néanmoins complexes du fait du nombre élevé de concepts qu'ils sont susceptibles de contenir et l'objectif de ce papier est de proposer une mesure de similarité entre ces concepts pour trouver les plus pertinents à conseiller à un utilisateur durant sa navigation dans un treillis.
Ce papier est organisé comme suit : dans un premier temps, la section 2 présente les outils mis en oeuvre dans nos travaux, à savoir les treillis de Galois et les ontologies, ainsi que la manière dont ils peuvent être associés pour la recherche d'information. Dans la section 3, nous présentons un état de l'art des mesures de similarité définies pour l'appariement d'ontologies. Nous étendons l'une de ces mesures pour proposer, dans la section 4, une mesure de similarité adaptée aux concepts d'un treillis de Galois ; nous présentons également la manière dont nous la mettons en oeuvre pour faciliter la navigation en indiquant les concepts les plus pertinents pour une requête donnée ou à partir de la position courante dans le treillis. Finalement, nous décrivons dans la section 5 une petite expérimentation menée sur un ensemble de pages Web dédiées au tourisme, avant de conclure et de présenter les perspectives de ce travail.
Existant
Les travaux présentés ici s'inscrivent dans une méthodologie plus large décrite dans  dont le but est de faciliter la recherche d'information dans des systèmes d'information complexes. Cette méthodologie est fondée sur l'utilisation conjointe de treillis de Galois et de structures sémantiques, par exemple des thesaurus ou des ontologies, pour fournir trois niveaux de navigation dans les données explorées. Ces trois niveaux de navigation proposés sont illustrés sur la figure 1 et décrits dans la section suivante.
Architecture
La complexité des systèmes d'information peut se traduire de différentes manières, qu'il s'agisse d'un volume important, d'un nombre élevé de dimensions, d'un manque de structure ou des relations et corrélations entre les données du système. Concernant ces deux derniers points en particulier, la construction de treillis de Galois à partir des données initiales permet de construire une structure sur les données et de montrer leurs relations en regroupant, sous forme de classes recouvrantes, les données présentant des caractéristiques communes. Le treillis nous fournit ainsi un niveau d'abstraction au-dessus des données brutes. Ce niveau est appelé niveau conceptuel. A ce niveau, plusieurs treillis peuvent être construits sur différents ensembles de données. Chaque treillis représente un contexte conceptuel et constitue un espace de recherche pour l'utilisateur. Nous distinguons le contexte conceptuel global, défini par le treillis, et le contexte conceptuel instantané, qui dépend du treillis mais aussi de la requête de l'utilisateur ou de sa navigation. Ces deux notions sont présentées dans  et définies formellement dans (Polaillon et al., 2007). Les ensembles de données sur lesquels les différents treillis sont calculés peuvent posséder des intersections non vides. Dans ce cas, une même donnée pourra appartenir à plusieurs contextes conceptuels globaux permettant ainsi une navigation entre différents treillis. Le niveau sémantique est construit au-dessus du niveau conceptuel. Il est peuplé d'ontologies dont les concepts sont reliés à des concepts des treillis du niveau conceptuel. L'objectif du niveau sémantique est double : il permet à l'utilisateur de naviguer vers des concepts dont la généralisation n'existe pas dans RNTI -X -les contextes conceptuels globaux ainsi que de passer d'un contexte global à un autre lorsqu'ils ne possèdent pas d'intersection.
FIG.1 -Trois niveaux de navigation
Mise en oeuvre de l'architecture
La mise en oeuvre de l'architecture consiste à sélectionner les ensembles de données à explorer, choisir les ontologies du niveau sémantique, construire les treillis de Galois sur les ensembles de données sélectionnés puis à labelliser les concepts des treillis avec un vocabulaire contrôlé issu des ontologies choisies au niveau sémantique. C'est grâce à cette labellisation que sont construits les liens entre les contextes globaux et les ontologies. Cette mise en oeuvre est effectuée hors ligne du fait des temps de calcul engendrés par la construction des treillis et par la labellisation.
Une fois cette étape terminée, l'utilisateur peut explorer les ensembles de données au travers des concepts des treillis du niveau conceptuel ou du niveau sémantique. A un instant t, l'utilisateur se trouve sur un concept du niveau conceptuel ou sémantique. A partir du concept courant où se il trouve, l'utilisateur peut naviguer vers un autre concept plus général ou plus spécialisé que le concept courant.
Outils conceptuels et sémantiques
Dans cette section, nous nous positionnons et rappelons rapidement les outils nous permettant l'utilisation conjointe de treillis de Galois et d'ontologies.
Treillis de Galois
Les treillis de Galois (Godin et al 1993)   Les treillis de Galois regroupent les données sous forme de concept en fonction de leurs caractéristiques communes et permettent d'exprimer de manière explicite toutes les relations entre les données.
Treillis de Galois et ontologies
Pour améliorer la recherche d'information, nous utilisons les treillis conjointement à des informations sémantiques « extérieures », qui peuvent prendre la forme de taxonomies, de thesaurus ou d'ontologies, selon leur expressivité .
Plusieurs travaux se sont intéressés à l'extraction des données et à la représentation des informations sous la forme d'un treillis de Galois.
L'enrichissement des treillis par des ontologies a fait l'objet de nombreux travaux dans le cadre de la recherche d'informations permettant une analyse plus efficace des différentes relations existantes entre les données : enrichissement d'ontologies pour l'organisation et la recherche d'information (Delteil et al 2002) ; les ontologies de domaine ont été utilisées pour guider la construction du treillis selon les préférences de l'utilisateur (Safar et al 2004) et pour enrichir l'indexation du treillis par un thesaurus (Priss 2000).
Ce processus d'extraction a été employé pour divers types de données : gestion de publications (Szathmary et Napoli 2004); aéronautique (Zenou et Samuelides 2004) ; biologie (Messai et al 2005), pages Web (Carpineto et Romano 2004), etc.
Par ailleurs, des travaux ont été réalisés pour que l'utilisateur, face à des treillis de grande taille lors d'une navigation, puisse focaliser son attention sur seulement une partie du treillis par décomposition ou par effet de zoom sur des vues abstraites et plus détaillées (Carpineto et Romano 1995)  (Godin et al 1993).
Discussion
L'architecture à trois niveaux que nous avons présentée permet d'apporter une aide pertinente et contextualisée à la recherche d'information par l'utilisateur. Ce dernier se déplace de manière transparente parmi ces niveaux et passe aisément de l'un à l'autre selon ses besoins en termes de recherche d'information. Nous avons également présenté les qualités qui nous ont conduits à utiliser les treillis de Galois dans notre architecture. Néanmoins, la navigation dans un treillis de Galois présente des difficultés. En effet, le nombre de concepts créés augmente avec le nombre d'individus et le nombre de propriétés, ce qui en fait des structures complexes et très difficiles à interpréter au-delà d'une certaine taille. Compte tenu de cette complexité, une aide est nécessaire pour conseiller l'utilisateur dans le choix du concept de départ de sa navigation et du prochain concept à explorer en termes de pertinence par rapport à sa position actuelle. Pour apporter cette aide, nous avons besoin d'une mesure de similarité entre les concepts d'un treillis ; il nous faut pour cela une mesure de similarité adéquate.
Dans la section 3, nous présentons un état de l'art des mesures de similarité définies dans le cadre de l'appariement d'ontologies et dans la section 4 nous proposons l'extension de l'une d'entre elles pour l'adapter aux treillis de Galois RNTI -X -3 Etat de l'art sur les mesures de similarité pour l'appariement d'ontologies
Approche basée sur les arcs
Parmi les solutions classiques pour les mesures de similarité, on peut trouver les approches basées sur les arcs qui reposent uniquement sur la structure de l'ontologie. Les deux mesures les plus utilisées sont la mesure de Rada (Rada et al 1989) et celle de Wu & Palmer (Wu et Palmer 1994) décrites ci-dessous. Elles se basent sur la distance en termes de nombre d'arcs séparant un concept d'un autre. (Rada et al 1989) suggèrent que, pour mesurer la distance entre deux concepts ontologiques, notée dist (c1, c2), on se base sur le nombre d'arcs minimum à parcourir pour aller du concept c1 au concept c2. La mesure de similarité est ainsi de la forme :
Dans le même ordre d'idée, (Wu et Palmer 1994) définissent la similarité en fonction de la distance qui sépare deux concepts ontologiques dans la hiérarchie et également par leur position par rapport à la racine. On a ainsi :
depth(c) / depth (c1) +depth (c2).
avec : -depth(c) le nombre d'arcs qui séparent le plus petit généralisant de c1 et c2 de la racine. -depth (ci) le nombre d'arcs qui séparent le concept ci de la racine en passant par c. Ces mesures ont l'avantage d'être faciles à implémenter et peuvent donner une idée sur le lien sémantique entre les concepts. Cependant, elles ne prennent pas en compte le contenu du concept lui-même, ce qui peut conduire, dans certains cas, à une marginalisation de l'apport du concept en terme d'information.
Approche utilisant le contenu informationnel
Les mesures de similarité suivant cette approche sont fondées sur la notion de Contenu Informationnel (CI) qui utilise conjointement l'ontologie et le corpus. Le Contenu Informationnel d'un concept traduit sa pertinence dans le corpus en tenant compte de sa spécificité ou de sa généralité. Pour ce faire, la fréquence des concepts dans le corpus est calculée et elle regroupe la fréquence d'apparition du concept lui-même ainsi que les concepts qu'il subsume (concepts fils). Les deux mesures les plus connues dans cette catégorie sont celles de Resnik (Resnik 1995) et Jiang-Conrath (Jiang et Conrath 1997.
Resnik (Resnik 1995) définit la similarité sémantique entre deux concepts par la quantité d'information qu'ils partagent : elle est égale au contenu informationnel du concept le plus spécifique (plus petit généralisant ppg) qui subsume les deux concepts dans l'ontologie. Elle est définie comme suit :
Sim (c1, c2) = CI (ppg (c1, c2)) avec : CI= -log (P(c)) où : P(c) est la probabilité de retrouver une instance du concept. Ces probabilités sont calculées par la fréquence de c sur le nombre total des concepts.
RNTI -X -La mesure de (Jiang et Conrath 1997) prend en compte à la fois le contenu informationnel du ppg et celui des concepts concernés. Par conséquent, elle peut pallier les limites de la mesure de Resnik et est définie de la manière suivante : 
Mesure de similarité adaptée aux treillis de Galois
Nous proposons une mesure de similarité entre des concepts d'un treillis de Galois, construit ici pour la classification de pages Web. Cette mesure est une extension d'une mesure de similarité entre les concepts d'une ontologie, que nous avons adaptée à nos besoins, afin de permettre une meilleure étude du treillis. Cette mesure tient compte à la fois de la sémantique et de la topologie, à travers le voisinage d'un concept ainsi que sa profondeur.
L'objectif de cette mesure de similarité est de permettre une meilleure navigation dans le treillis et une restriction du champ des concepts visités. En particulier, nous cherchons à quantifier l'apport d'information des éléments de l'intention d'un concept par rapport à tout le corpus. Nous nous intéressons tout particulièrement à la fréquence des éléments communs à deux concepts, afin de pouvoir avoir une idée sur le contexte partagé.
Chaque concept du treillis de Galois est constitué de deux sous-ensembles (extensionintension). La fréquence des termes de l'intention dans les pages Web constituant le corpus est calculée pour mesurer l'Information Moyenne (IM) de chaque concept, qui intervient, par la suite, dans le calcul de la mesure de similarité entre les concepts. L'IM est dérivée du Contenu Informationnel (CI) défini dans la section 3.2. L'intérêt de l'IM est d'évaluer le poids des termes dans les pages des sites web.
Notre mesure de similarité tient également compte de la topologie du treillis de Galois en faisant intervenir la profondeur de chaque concept c (notée depth(c) et correspondant au nombre d'arcs qui séparent c du concept le plus spécifique du treillis) ainsi que l'information moyenne du plus petit généralisant des deux concepts dont on mesure la similarité.
On définit l'information moyenne d'un concept c constitué d'une extension E et d'une intention I comme le Contenu Informationnel de l'intention, défini de la manière suivante :
RNTI -X -
IM(c)= -log (P(I))
où P(I) est la probabilité de retrouver les termes de l'intention (i.e. les termes fréquents) simultanément dans le corpus (i.e. les pages Web). Cette probabilité correspond au rapport entre le nombre de pages Web possédant les termes de I et le nombre total de pages Web du corpus.
Nous nous intéressons ici au calcul de la similarité entre deux concepts du treillis de Galois. Si l'on considère deux concepts c1 et c2 du treillis de Galois, on note ppg (c1,c2) le plus petit généralisant de c1 et de c2, qui est le « plus proche ancêtre » commun à c1 et c2.
La mesure de similarité entre c1 et c2 est alors définie de la manière suivante :
où depth(c) est la profondeur du concept c.
Notre mesure est une adaptation de la mesure de Jiang & Conrath (cf. section 3.2). Notre apport consiste à prendre en compte, non seulement l'information moyenne de chaque concept mis en jeu, mais aussi sa profondeur afin de préserver la particularité des treillis : plus on descend au niveau de la hiérarchie et plus on se spécialise. Un autre apport de cette mesure est de tenir compte de la structure du treillis puisqu'elle fait intervenir l'information moyenne du plus proche ancêtre commun aux deux concepts dont on mesure la similarité.
La mise au point d'une telle mesure de similarité au niveau du treillis de Galois permet de retrouver le voisinage du concept requête favorisant la redirection de l'utilisateur au cours du processus de recherche de l'information pertinente, comme nous l'illustrons dans la section 5.
Méthodologie
La mesure de similarité présentée dans ce papier permet de faciliter la navigation de l'utilisateur à l'intérieur d'un treillis de Galois. Nous distinguons la phase d'initialisation de la navigation et la phase de navigation elle-même.
Initialisation de la navigation
Cette phase a pour objectif de trouver le point d'entrée le plus adapté dans le treillis, c'est-à-dire le concept qui servira de point de départ à la navigation, que nous appelons le concept de départ et notons CD. Ce concept est constitué d'une extension E CD et d'une intention I CD . La recherche d'information peut être précise, dans le cas où l'utilisateur sait la formuler sous forme de requête, ou non ; nous envisageons ces deux scénarios dans cet article.
A partir d'une requête de l'utilisateur Dans le cas d'une recherche d'information précise, nous proposons de formuler la requête par R termes de l'ontologie, afin d'utiliser un vocabulaire contrôlé et éviter ainsi les ambiguïtés. Par exemple, si le treillis est construit à partir d'un ensemble de pages Web, les requêtes des utilisateurs pourront être formulées sous la forme d'un ensemble de mots-clés appartenant à ce vocabulaire contrôlé. L'ensemble des mots-clés choisis par l'utilisateur constitue l'intention d'un concept cible noté CT. Au sein du treillis, le concept de départ,CD RNTI -X -choisi pour démarrer la navigation sera le concept dont l'intention possèdera le maximum d'éléments contenus dans l'intention du concept CT. Cette intention est constituée de tous les termes de la requête et notée : I CT = {p 1 , p 2 , …, p R }. Si plusieurs concepts du treillis possèdent le même nombre de propriétés recherchées, il faut choisir le concept initial parmi ces concepts dits candidats. Pour cela, on calcule leur similarité deux à deux et le concept retenu comme concept de départ est celui dont la valeur moyenne de similarité avec les autres concepts candidats est la plus élevée :
On note c 1 , c 2 , …, c G les concepts du treillis, G étant le cardinal du treillis. Soient cc 1 , cc 2 , …, cc N , N concepts candidats. On note Sim (cc i , cc j ) la similarité entre le concept cc i et le concept cc j , avec i et j appartenant à [1-N]. La similarité moyenne entre un concept candidat cc k et les autres concepts candidats et dite partielle, notée Sim ck mp est égale à la moyenne des valeurs de similarité entre cc k et les autres concepts candidats. Le concept de départ est celui dont la similarité moyenne avec les autres concepts candidats est la plus élevée, soit :
En sélectionnant le concept de départ comme étant, en moyenne, le plus proche des autres candidats, on choisit en quelque sorte le concept le plus « central » parmi ceux qui correspondent le mieux à la requête de l'utilisateur. Si M concepts répondent au critère de similarité moyenne maximale, le concept de départ retenu est celui dont la similarité moyenne avec l'ensemble des concepts du treillis (y compris les concepts non candidats) est la plus élevée. Pour le concept candidat cc k , cette similarité moyenne dite totale est notée Sim ck mt et correspond à la moyenne des valeurs de similarité entre cc k et tous les autres concepts du treillis. Si l'on note cc 1 ', cc 2 ', …, cc M ' les M concepts candidats résiduels, on a : ( cc i ', c k ) ) pour k allant de 1 à G, G étant le cardinal du treillis et k?i.
Sans requête de l'utilisateur
Si l'utilisateur n'a pas formulé de requête et souhaite simplement naviguer pour explorer un ensemble de pages Web, le concept de départ est celui dont la similarité moyenne totale avec tous les autres objets du treillis est la plus élevée, soit :
En sélectionnant le concept de départ comme étant, en moyenne, le plus proche des autres concepts du treillis, on choisit en quelque sorte le concept le plus « central », ce qui semble être un point de départ pertinent et représentatif du treillis.
Navigation
Lorsque l'utilisateur se trouve dans cette phase, il est déjà positionné sur l'un des concepts du treillis : le concept courant qu'il est en train de visiter, noté CP. Quand RNTI -X -l'utilisateur désire quitter CP pour explorer d'autres concepts, l'objectif est alors de lui proposer le concept le plus proche -sémantiquement et conceptuellement parlant-du concept CP. Le concept le plus similaire au concept CP semble, en effet, être le choix le plus pertinent pour la poursuite de sa navigation. La similarité entre chaque paire de concepts du treillis de Galois peut être pré-calculée, et il suffit alors d'indiquer à l'utilisateur la valeur de la similarité entre son concept courant et tous les autres concepts du treillis, par exemple par ordre décroissant. En phase de navigation, l'unicité du concept proposé n'est pas une nécessité comme dans la phase d'initialisation et, sous réserve de contraintes ergonomiques, plusieurs concepts candidats peuvent être proposés l'utilisateur.
Expérimentation
Dans cette section, nous illustrons la méthodologie présentée précédemment à partir d'un corpus constitué de pages Web relatives au domaine du tourisme.
La première étape consiste à construire les objets et les propriétés qui serviront d'entrée au calcul du treillis de Galois. Dans notre exemple, chaque page Web représente un objet et ses propriétés correspondent aux termes les plus fréquents qu'elle contient. L'extraction des termes les plus fréquents est effectuée à partir d'un thesaurus sur le domaine du tourisme.
Les objets et propriétés ainsi extraits sont rassemblés dans une base de données servant à la construction d'un treillis de Galois. Nous avons implémenté un algorithme incrémental reposant sur celui de (Godin et al 1991)   {vacance,voyage,location,Hôtel,annuaire}) et ({1},{croisière,vacance,voyage,port,bateau,location,destination,promo}). Plusieurs questions se posent à l'utilisateur : comment choisir entre plusieurs pères sans devoir faire un choix a priori sur les propriétés retenues ? Comment choisir entre plusieurs fils sans devoir faire un choix a priori sur les pages retenues ? Par ailleurs, n'est-il pas possible que le concept le plus pertinent pour poursuivre sa navigation ne soit ni un père, ni un fils du concept courant, c'est-à-dire un concept qui n'est pas relié à celui-ci par un arc du treillis (par exemple un frère tel que le concept ({1, 4), {croisière, voyage, location})) ?
Afin de répondre à toutes ces questions, on regarde la valeur de la mesure de similarité entre le concept courant et tous les autres concepts du treillis, afin de choisir le concept le plus pertinent pour poursuivre la navigation à partir du concept que l'utilisateur est en train de visiter. Toutes ces valeurs sont présentées à l'utilisateur par ordre décroissant comme le montre la figure 3. En l'occurrence, le concept le plus similaire est le concept ({2), {vacance, voyage, location, hôtel, annuaire}). Il est à noter que toutes les valeurs de similarité entre chaque paire de concepts peuvent être pré-calculées afin d'optimiser le temps de réponse.
Conclusion et perspectives
L'extraction d'informations pour en faciliter la recherche peut être réalisée de différentes manières : par des techniques de clustering numériques, basées sur la fréquence d'apparition des termes dans un document, ou par des techniques de clustering conceptuel, permettant d'effectuer des regroupements d'objets partageant les mêmes propriétés. L'avantage de ces dernières est de permettre une structuration des données, et d'offrir des mécanismes de généralisation/spécialisation bien adaptés à l'utilisateur final. Un niveau sémantique, ontologie ou thesaurus, a été proposé au dessus du niveau conceptuel dont les avantages ont été détaillés en section 2.
La problématique abordée dans cet article est de trouver un moyen d'indiquer à l'utilisateur les concepts les plus pertinents par rapport à sa requête, ou de pouvoir lui proposer un point d'entrée dans la structure conceptuelle (treillis de Galois) qui peut être de grande taille. Nous avons donc proposé une mesure de similarité tenant compte à la fois de la sémantique et de la topologie du treillis (position du concept dans le treillis, prise en compte du voisinage). Cette mesure permet donc d'ordonner l'ensemble des concepts se trouvant dans le voisinage d'un concept donné dans le treillis, et ainsi de guider la navigation de l'utilisateur.
Cette mesure a été expérimentée sur des treillis construits à partir de pages Web, les propriétés communes étant des termes extraits de ces pages. Les résultats ont été comparés avec d'autres mesures comme celle de Wu&Palmer, décrite en section 3. Les résultats obtenus montrent un meilleur ordonnancement du voisinage avec notre mesure de similarité.
RNTI -X -Les perspectives de ce travail consistent dans un premier temps à développer une interface permettant à des utilisateurs de valider cette approche de manière expérimentale. Nous travaillons également sur la visualisation spatiale des corpus de documents à la base de la construction des treillis, ainsi que sur la manière de mettre en correspondance ces régions spatiales avec les concepts du treillis, pour une meilleure interprétation du voisinage des concepts. Dans la perspective de traiter de gros volumes de données, on pourra aussi s'intéresser à la manipulation de gros volumes de données avec les treillis de Galois grâce à l'utilisation d'algorithmes de construction de treillis récents tel que proposé par (Diday et Emilion, 2003).

Introduction Générale
L'émergence des bases de données modernes qui présentent d'énormes capacités de stockage et de gestion, associée à l'évolution des systèmes de transmission et des techniques d'acquisition automatique des données contribuent à la construction d'une masse de données qui dépasse de loin les capacités humaines à les traiter. Ces données sont des sources d'informations pertinentes qui nécessitent des outils de synthèse et d'interprétation. Les recherches se sont orientées vers des systèmes d'intelligence artificielle puissants permettant l'extraction des informations utiles et aidant à la prise des décisions. Pour une meilleure synthèse et interprétation, la fouille de données ou data mining est née en puisant ses outils au sein de la statistique, de l'intelligence artificielle et des bases de données. La méthodologie du data mining offre la possibilité de construire un modèle de prédiction d'un phénomène à partir d'autres phénomènes plus facilement accessibles, qui lui sont liés, en se basant sur le processus d'extraction des connaissances à partir des données qui n'est qu'un processus de classification intelligente des données. Cependant, le modèle construit peut parfois engendrer des erreurs de classification que même une classification aléatoire n'aurait pas engendrée. Pour réduire ces erreurs, de nombreux travaux en data mining et spécifiquement en apprentissage automatique ont porté sur les méthodes d'agrégation de classifieurs. Leur but suprême est d'améliorer, par des techniques de vote, les performances d'un classifieur unique. Ces méthodes d'agréga-tion sont efficaces d'un point de vue compromis Biais-variance, mais aussi grâce aux trois raisons fondamentales (raison statistique, raison informatique et raison de représentation) expliquées dans l'étude menée par (Dietterich, 2000). Ces méthodes d'agrégation de classifieurs peuvent être regroupées en deux catégories : celles qui fusionnent des classifieurs prédéfinis, on trouve par exemple : le vote simple (Bauer et Kohavi, 1999), le vote pondéré (Bauer et Kohavi, 1999) et le vote à la majorité pondérée (Littlestone et Warmuth, 1994) et celles qui fusionnent des classifieurs selon les données durant l'apprentissage, on trouve des stratégies adaptatives (boosting) et son algorithme de base AdaBoost (Shapire, 1990) ou des stratégies aléatoires (bagging) (Breiman, 1996). Nous nous intéresserons, par la suite, au boosting, dans la mesure où l'étude comparative menée dans (Dietterich, 1999) montre bien que dans le cas où les données sont faiblement bruitées, AdaBoost est plus performant que le bagging, tout en semblant être immunisé contre le sur-apprentissage. En effet, AdaBoost essaye directement d'optimiser les votes pondérés. Cette constatation s'est traduite non seulement par le fait que l'erreur empirique sur l'échantillon d'apprentissage décroît exponentiellement avec le nombre d'itérations, mais également par le fait que l'erreur en généralisation baisse elle aussi. Cependant, cette même étude montre que, sur des données fortement bruitées, AdaBoost présente un taux d'erreur parfois plus important que le bagging. Une raison à ces mauvaises performances, mise en évidence par Dietterich, vient du fait que le boosting tend à augmenter le poids des exemples bruités à travers les itérations. La conséquence immédiate est le sur-apprentissage des exemples bruités. La vitesse de convergence du boosting se trouve également pénalisée sur ce type de données. Pour surmonter les difficultés rencontrées par le boosting face aux données bruitées, nous proposons une nouvelle approche qui associe les hypothèses déjà construites à l'hypothèse courante pour définir les erreurs de l'itération courante. De par sa nature, cette approche est appelée approche Hybride. La suite de cet article est organisée comme suit : la section 1 est consacrée à un état de l'art synthétique des principaux travaux visant à amélio-rer le boosting. En section 2, nous présentons l'amélioration du boosting que nous proposons. Dans la section 3, nous effectuons une large étude expérimentale visant à comparer, sur de nombreuses bases de données réelles, les performances d'AdaBoost et AdaBoost Hybride, en termes d'erreur, de rappel et de vitesse de convergence. Enfin, en section 4, nous terminons par une conclusion et des perspectives.
Etat de l'art
En présence de données bruitées, le boosting présente différentes faiblesses, telles que le sur-apprentissage et la dégradation de la vitesse d'apprentissage. Diverses améliorations ont été proposées qui opèrent sur la mise à jour du poids des exemples ou parfois sur le principe même du boosting. De ce fait, nous allons présenter les principales méthodes ayant comme objectif l'amélioration du boosting par rapport à ces deux faiblesses. Cet état de l'art est effectué selon deux axes de recherches. Le premier axe regroupe les approches abordant le problème de la gestion des données bruitées, sans laquelle des phénomènes de sur-apprentissage peuvent survenir. Le deuxième axe regroupe les approches portant sur les problèmes de vitesse de convergence.
Le sur-apprentissage
L'émergence et l'évolution des bases de données modernes contraignent aujourd'hui les chercheurs à étudier et à améliorer les capacités de tolérance au bruit du boosting. En effet, ces bases de données sont fortement bruitées en raison des nouvelles technologies d'acquisition de données telles que le web. En parallèle, des études telles que celles de (Dietterich, 1999), (Rätsch, 1998) et (Schapire et Singer, 1999) montrent bien qu' AdaBoost tend à surapprendre les données lorsqu'elles sont bruitées. De ce fait, un certain nombre de travaux récents ont tenté de limiter ces risques de sur-apprentissage. Les améliorations proposées se fondent essentiellement sur le fait qu'AdaBoost tend à augmenter le poids des exemples bruités de manière exponentielle. Deux solutions se présentent pour réduire ces données bruitées. Soit ces données sont détectées et supprimées avant l'apprentissage à l'aide d'heuristiques efficaces (Brodley et Friedl, 1996), (Wilson et Martinez, 2000). Soit ces données sont détectées tout au long du processus de boosting, on parle alors d'une bonne gestion de bruit. Pour ce faire, les chercheurs se sont orientés vers l'amélioration des points forts du boosting tels que la mise à jour des exemples mal classés, la maximisation de la marge, la signification des poids qu'Adaboost associe aux hypothèses et enfin le choix de l'apprenant faible.
-Modification des poids des exemples : la mise à jour adaptative de la distribution des exemples, visant à augmenter le poids de ceux mal appris par le classifieur précédent, permet d'améliorer les performances de n'importe quel algorithme d'apprentissage. En effet, à chaque itération, la distribution courante favorise les exemples ayant été mal classés par l'hypothèse précédente. De ce fait, plusieurs chercheurs ont proposé des stratégies portant sur une modification de la mise à jour des poids des exemples, pour évi-ter le sur-apprentissage. Madaboost (Domingo et Watanabe, 2000) a pour principe de borner le poids des exemples suspects par leur probabilité initiale. Il agit ainsi sur la croissance incontrôlée du poids des exemples bruités qui est à l'origine des problèmes d'AdaBoost. Une autre approche qui rend l'algorithme du boosting résistant au bruit est BrownBoost (McDonald et al., 2003), un algorithme basé sur le Boost-by-Majority en incorporant un paramètre temporel. Ainsi par une bonne évaluation de ce paramètre, BrownBoost est-il capable d'éviter le sur-apprentissage. On citera encore Logitboost (Schapire et Singer, 1999), qui adapte au principe d'AdaBoost un modèle de régression logistique. LogitBoost réduit au minimum son critère en employant les étapes de Newton-like pour adapter un modèle de régression logistique en optimisant le logarithme de la vraisemblance. Une autre approche, qui produit moins d'erreur en généralisation comparée à l'approche classique mais au prix d'une erreur d'apprentissage légèrement plus élevée, est celle de (Vladimir et Vezhnevets, 2002). En fait, sa mise à jour se base sur la diminution de la contribution des classifieurs, si cela fonctionne "trop bien" sur les données qui ont déjà été correctement classifiées. C'est pourquoi la méthode est appelée Modest AdaBoost. Elle force les classifieurs à être "modestes" et travaille seulement dans le domaine défini par une distribution bien dé-terminée et un critère d'arrêt normal. SmoothBoost (Servedio, 2001) essaye de réduire l'effet de sur-apprentissage par des limites imposées à la distribution produite pendant le processus de boosting. En particulier, lors de chaque itération, on fixe une limite de poids. Un exemple dont le poids dépasse cette limite est considéré comme bruité et retiré de l'ensemble d'apprentissage. Une dernière approche, IAdaBoost (Sebban et Suchier, 2003), se base sur l'idée de construire autour de chacun des exemples une mesure d'information locale permettant d'évaluer les risques de surapprentissage, en utilisant un graphe de voisinage qui permet de mesurer l'information autour de chaque exemple. Grâce à ces mesures, on calcule une fonction qui évalue la nécessité de mettre à jour l'exemple avec AdaBoost. Cette fonction permet de gérer à la fois les outliers, les recoupements de classes et les centres de clusters.
-Modification de la marge : certaines études, après une observation des algorithmes de boosting, ont montré que l'erreur en généralisation décroît encore une fois que l'erreur en apprentissage est stable ou même nulle. L'explication est que même si tous les exemples d'apprentissage sont déjà bien classés, le boosting tend à maximiser davantage les marges (Servedio, 2001), d'où les performances du boosting. À la suite de cette explication, certains ont cherché à modifier la marge explicitement soit en la maximisant soit en la minimisant dans le but d'améliorer les performances de boosting contre le sur-apprentissage. Plusieurs approches se sont succédées telles que AdaBoostReg (Rätsch et al., 2001) qui essaye d'identifier et d'enlever les exemples mal étiquetés de l'ensemble d'apprentissage, ou d'appliquer la contrainte de la marge maximale sur des exemples supposés mal étiquetés, en utilisant la Soft Margin qui est moins sensible au sur-apprentissage par rapport à la marge d' Adaboost. Dans l'algorithme proposé par (Friedman et al., 1998), les auteurs utilisent un schéma de pondération qui exploite une fonction des marges qui croît moins vite que la fonction exponentielle.
-Modification de poids des classifieurs : lors de l'évaluation des performances du boosting, des chercheurs se sont également interrogés sur la signification des poids ?(t) qu'AdaBoost associe aux hypothèses produites. Ce poids est une valeur déterminée en fonction des échecs et des réussites de classification sur un échantillon bien déterminé. Cependant, ils ont noté lors d'expériences sur des données très simples que l'erreur en généralisation diminuait encore alors que l'apprenant faible avait déjà fourni toutes les hypothèses possibles. Autrement dit, lorsqu'une hypothèse apparaît plusieurs fois, elle vote finalement avec un poids, cumul de tous ses ?(t), qui a peut-être un caractère plus absolu. De ce fait, plusieurs auteurs ont espéré approcher ces valeurs par un processus non adaptatif, tel que Locboost (Meir et al., 2000), une alternative à la construction de l'ensemble de représentation des hypothèses, qui permet aux coefficients ?(t) de dé-pendre des données. On aura donc des poids locaux attribués à chaque exemple.
-Choix de l'apprenant faible : Plusieurs auteurs se sont intéressés au choix du classifieur de base du boosting. GloBoost (Torre, 2004) utilise un apprenant faible qui produit des hypothèses correctes. Celles-ci peuvent donc s'abstenir sur une partie des exemples mais en aucun cas se tromper sur un exemple. Il s'agit de moindres généralisés maximalement corrects. RankBoost (Dietterich, 1999) se base sur un apprenant faible qui accepte comme données d'entrées des attributs de préférences (rank) qui ne sont que des fonctions.
Certes, ces méthodes ont pu améliorer d'une façon ou d'une autre la performance du boosting contre le bruit. Toutefois, d'autres paramètres du boosting se trouvent pénalisés, tels que l'erreur d'apprentissage, le temps de détection du bruit et la vitesse de convergence.
La vitesse de convergence
En plus du problème de sur-apprentissage rencontré par le boosting dans les bases de données modernes déjà évoqué précédemment, il existe un autre problème qui est celui de la vitesse de convergence des algorithmes de boosting (spécialement Adaboost). En effet, en cas de présence de données fortement bruitées, l'erreur optimale de l'algorithme d'apprentissage utilisé est atteinte tardivement. En d'autres termes, Adaboost "perd" du temps, et donc des itérations à pondérer ces exemples qui ne méritent aucune attention, puisqu'il s'agit de bruit. Des recherches ont été menées pour détecter les données bruitées et améliorer ainsi les performances du boosting en termes de convergence tel que iBoost (Kwek et Nguyen, 2002), qui vise à spécialiser les hypothèses faibles sur les exemples qu'elles sont supposées correctement classer. iAdaboost est aussi une approche qui contribue à améliorer Adaboost contre sa convergence. En fait, l'idée de base de l'amélioration est la modification du théorème de (Schapire et Singer, 1999). Cette modification est réalisée afin d'intégrer le risque de Bayes et de mettre en exergue les situations où certains exemples de classes différentes partagent la même représen-tation. Les effets de cette modification sont une convergence plus rapide vers le risque optimal et une réduction du nombre d'hypothèses faibles à construire. Enfin, RegionBoost (Maclin, 1998) est une nouvelle stratégie de pondération des classifieurs. Cette pondération est évaluée au moment du vote par une technique basée sur les k plus proches voisins de l'exemple à étiqueter. Cette approche permet de spécialiser chaque classifieur sur des régions de l'ensemble d'apprentissage.
Amélioration proposée : Adaboost Hybride
Pour améliorer les performances d'Adaboost et éviter de le forcer à apprendre des exemples a priori bruités ou des exemples qui deviendraient trop difficiles à apprendre durant le processus du boosting, nous proposons une nouvelle approche qui s'inspire du fait qu'Adaboost construit, à chaque itération, des hypothèses sur un échantillon bien défini. La mise à jour et le calcul de l'erreur d'apprentissage sont faits à partir des résultats de ces seules hypothèses et n'exploitent pas les résultats fournis par les hypothèses construites aux itérations antérieures sur d'autres échantillons. Cette approche est appelée approche Hybride au sens où elle se base sur les hypothèses antérieures : la mise à jour des exemples à l'itération courante tiendra compte non seulement des résultats de l'itération courante mais aussi de ceux des itérations antérieures.
Pseudo code de Adaboost Hybride
Soit X 0 la classe à prévoir et S = (x 1 , y 1 ), ....., (x n , y n ) un échantillon -Pour i = 1, 2...n, faire -Initialiser les poids p 0 (x i ) = 1/n ; -Fin pour -t ? 0 -Tant que t ? T faire -Tirer un échantillon d'apprentissage S t dans S selon les probabilités p t . -Construire une hypothèse h t sur S t par un algorithme d'apprentissage A. -Soit t l'erreur apparente de h t sur S avec t = poids des exemples
H(x) = argmax y ? Y T t=1 ? t La modification de l'algorithme porte sur la prise en compte de l'ensemble des itérations passées pour effectuer la prédiction courante, ce qui modifie notamment le poids des exemples et le calcul de l'erreur.
-Modification des poids des exemples : à chaque itération, on fait appel aux avis des experts déjà utilisés (hypothèses des itérations antérieures) pour mettre à jour les poids des exemples. En effet, on ne compare pas seulement la classe prédite par l'hypothèse à l'itération courante avec la classe réelle mais la somme des hypothèses pondérées depuis la première itération jusqu'à l'itération courante. Si cette somme vote pour une classe différente de la classe réelle alors une mise à jour exponentielle semblable à celle effectuée par Adaboost est appliquée à l'exemple mal classé. Ainsi, cette modification ne concerne-t-elle que les exemples qui sont soit mal classés soit pas encore classés. Il est donc logique de s'attendre à une amélioration de la vitesse de convergence, de même pour la réduction de l'erreur en généralisation étant donnée le lissage des hypothèses à chaque itération. -Modification du calcul de l'erreur de l'hypothèse à l'itération t : la méthode que nous proposons, prend en considération les hypothèses antérieures à l'itération courante pour former l'hypothèse courante. De ce fait, à chaque itération, l'erreur apparente est le poids des exemples prédits de façon erronée par la moyenne pondérée des hypothèses des itérations antérieures. Du coup, le coefficient attribué à l'hypothèse courante, ?(t), est lui aussi modifié, puisque ce coefficient dépend du calcul de l'erreur apparente Cette modification a un effet de lissage et laisse l'algorithme à chaque itération très dépendant des autres itérations. Des résultats améliorant surtout l'erreur en généralisa-tion sont attendus puisque le vote de chaque hypothèse (coefficient ?(t)) est calculé à partir des autres hypothèses.
Expérimentations
Dans cette section, nous allons comparer les résultats de notre algorithme AdaBoostHyb avec ceux fournis par AdaBoost, l'algorithme de référence et par BrownBoost, un algorithme connu pour être résistant face aux données bruitées. En fait, BrownBoost utilise la fonction de pondération suivante, qui n'est rien d'autre que la loi de probabilité d'une variable binomiale, qui dépend du nombre d'itérations finales k (temps total d'exécution), de l'itération courante i, du nombre de fois où l'exemple a déjà été correctement étiqueté r, et enfin de la probabilité de succès 1 ? ? imposée à toute hypothèse faible. ? i r = (
L'avantage de cette approche est que les données bruitées seront probablement détectées à un certain moment, et leur poids cessera d'augmenter. Au cours des expérimentations, la comparaison se fait à travers l'erreur en généralisation, le rappel et la vitesse de convergence. L'apprenant faible utilisé est l'algorithme C4.5 choisi suite à l'étude de (Dietterich, 1999) qui a montré que C4.5 est très sensible au bruit. Pour estimer sans biais le taux de succès théorique, nous avons fait appel à une procédure de validation croisée en 10 parties. Afin d'évaluer le comportement d'AdaBoostHyb vis à vis tant des performances que de la vitesse de convergence, nous avons séparé nos expérimentations en plusieurs parties. Dans la première, où nous avons travaillé sur 15 bases de l'UCI (D.J. Newman et Merz, 1998), nous rapportons la valeur de l'erreur en généralisation et le rappel, choisis comme critère de performance. Dans la deuxième partie, nous avons bruité aléatoirement ces bases de données avec un taux de bruit de 20%, pour analyser le comportement des trois algorithmes retenus. Ce taux est choisi en référence à l'étude de (Dietterich, 1999)   
Comparaison en termes d'erreur en généralisation
Le tableau 2 présente les résultats obtenus pour cette partie en ayant choisi pour chacun des algorithmes d'effectuer 20 itérations. Le choix du nombre d'itérations sera expliqué dans la dernière partie des expériences. Nous avons indiqué le taux d'erreur en généralisation es-timé pour chacun des algorithmes AdaBoostM1, BrownBoost et AdaBoostHyb. Nous avons par ailleurs utilisé les mêmes échantillons pour la validation croisée des différents algorithmes afin d'avoir une comparaison plus fine.
L'observation des résultats montre déjà les effets positifs de l'approche hybride. En effet, pour 14 bases sur 15, l'algorithme AdaBoostHyb présente un taux d'erreur inférieur ou égal à celui d'AdaBoostM1. C'est seulement pour la base LYMPH que notre approche donne une erreur de généralisation plus élevée que l'approche classique. Nous remarquons, aussi, des améliorations significatives de l'erreur en généralisation correspondant aux bases de données NHL, CONTACT-LENS et BREAST-CANCER. Par exemple l'erreur en généralisation de la base BREAST-CANCER passe de 45.81% à 30.41%.
De même, si nous comparons l'approche proposée avec BrownBoost, nous remarquons que pour 11 bases de données sur 15 l'algorithme AdaBoostHyb présente un taux d'erreur inférieur ou égal à BrownBoost. Ce gain en faveur de AdaBoostHyb nous montre bien qu'en exploitant les hypothèses générées aux itérations antérieures pour corriger le poids des exemples, il est possible d'améliorer les performances du boosting. Ceci peut être expliqué par le mode de calcul de l'erreur apparente et par conséquent le calcul du coefficient du classifieur ?(t) ainsi que par l'hybridation de l'hypothèse courante et des hypothèses antérieures. 
Comparaison en terme de rappel
Les résultats encourageants auxquels nous sommes parvenus, nous mènent à approfondir l'étude de cette nouvelle approche. Dans cette partie, nous essayons de connaître l'impact de notre approche sur le taux de rappel, puisque celle-ci n'améliore effectivement le boosting que si elle agit positivement sur le rappel. Le tableau 3 présente les résultats obtenus pour cette partie, ayant choisi pour chacun des algorithmes d'effectuer 20 itérations comme précédemment. Nous avons indiqué le rappel pour chacun des algorithmes AdaBoostM1, BrownBoost et AdaBoostHyb. Les résultats obtenus ici confirment les précédents. En effet, AdaBoostHyb augmente le rappel des bases de données ayant des taux d'erreur moins important. Le rappel des deux algorithmes est le même dans le cas où les taux d'erreur des bases de données sont égaux. Si nous considérons BrownBoost, nous remarquons que ce dernier améliore le rappel d'AdaBoostM1, pour chaque base de données (sauf la base de données TITANIC). Cependant, le rappel obtenu par notre approche est meilleur que celui obtenu par BrownBoost, sauf pour la base de données ZOO.
Nous constatons aussi que notre approche améliore le rappel dans le cas de la base LYMPH où l'erreur était plus importante. Nous notons alors que la nouvelle approche n'agit pas néga-tivement sur le rappel mais elle l'améliore même lorsque l'on a une erreur de généralisation plus importante. 
Comparaison sur des données bruitées
Dans cette partie, on s'est basé sur l'étude déjà faite par Dietterich (Dietterich, 1999) en ajoutant du bruit aléatoire aux données. Cet ajout de bruit de 20% est effectué, pour chacune de ces bases, en changeant aléatoirement la valeur de la classe prédite à l'aide d'un programme par une autre valeur possible de cette classe. Le tableau 4 nous montre le comportement des algorithmes vis-à-vis du bruit. Nous remarquons bien que l'approche hybride est sensible elle aussi au bruit puisque le taux d'erreur en généralisation est augmenté pour toute les bases des données. Cependant cette augmentation reste toujours inférieure à celle de l'approche classique sauf pour les bases de données telles que CREDIT-A, HEPATITIS et HYPOTHYROID. Nous avons donc étudié de près ces bases de données et nous avons noté un point commun, les valeurs manquantes. En fait, CREDIT-A, HEPATITIS et HYPOTHYROID possèdent respectivement 5%, 6%et 5,4% de valeurs manquantes.
Nous constatons alors que notre amélioration perd son effet avec l'accumulation de deux types de bruit : les valeurs manquantes et le bruit artificiel, bien que l'algorithme AdaBoostHyb améliore les performances d'AdaBoost contre le bruit sur le reste des bases de données. Considérant BrownBoost, nous remarquons qu'il améliore l'erreur en généralisation de toute les bases de données en comparaison avec AdaBoostM1. Cependant, BrownBoost donne des résultats meilleurs que l'approche hybride sur seulement 6 bases de données. Notre approche donne les meilleurs résultats avec les 9 autres bases de données. Ces résultats nous encouragent à étudier en détails le comportement de notre approche vis-à-vis du bruit. 

Problématique
L'Extraction d'Information consiste à identifier de l'information bien précise d'un texte en langue naturelle et à la représenter sous forme structurée. Les composantes de l'information recherchée sont généralement prédéfinies et circonscrites à un domaine spécifique, et les principaux travaux réalisés en matière d'identification de relations sémantiques ont essentiellement concerné les relations portées par une structure de type prédicats-arguments. Les principales approches d'identification de ces relations ont été basées sur l'analyse syntaxique (identification du verbe et ses arguments) (Khélif, 2006), ou sur la définition de patrons lexico-syntaxiques (Aussenac et al., 2000). L'étude de corpus de domaines différents montre que bonne partie de l'information pertinente peut aussi être distribuée sur plusieurs phrases, par le biais de relations exprimées à l'aide de variations linguistiques, comme la coréférence, l'anaphore ou l'ellipse. Les métho-des classiques d'extraction de relations ne sont alors plus adaptées.
La résolution de relations non prédicatives, et plus particulièrement d'une certaine forme elliptique (formes passives où l'argument agent est effacé) utilisée fréquemment, nous a conduits à proposer une représentation des connaissances du domaine considéré, à l'aide du modèle des graphes conceptuels, car ce modèle est doté d'opérations et offre des procédures de raisonnement (Salvat, 1997).
Identification de relations non prédicatives
Nous avons étudié un corpus de résumés d'articles scientifiques décrivant des expérien-ces génétiques menées par des chercheurs sur un ensemble de patients porteurs d'une même maladie génétique, le but étant de localiser les régions chromosomiques affectées. Une des relations pertinentes identifiées est Conditions Expérimentales qui relie l'ensemble des patients observés au type d'analyse subie. Dans l'exemple "A study was conducted on 22 MM patients. The authors used G-banding", la relation Conditions Expérimentales ne peut être détectée par une approche classique. La mise en oeuvre d'une procédure de raisonnement qui établit un lien entre les auteurs (authors) et l'étude (study) menée par les auteurs (authors) Extraction de relations non prédicatives permet d'inférer le lien existant entre l'échantillon (22 MM patients) et l'analyse (Gbanding).
Chacune des phrases est traduite à l'aide d'un graphe dit de référence (GRef) qui modé-lise le domaine de l'étude et auquel des règles d'inférence ont été associées :
Pour cet exemple, l'opération de jointure entre les deux graphes fournit un graphe résul-tat, auquel sont appliquées les règles de déduction.
Conclusion
Ce travail, qui dans un premier temps est dédié à une forme d'ellipse particulière, doit être étendu aux relations exprimant des liens de cohésion et de cohérence entre différents éléments du texte, pour une meilleure compréhension du texte. Aussenac N. et P. Séguéla (2000). 
Références
Summary
Semantic relations generally recognized by information extraction tasks are of a predicative form. The useful information is often distributed over several sentences. To detect this kind of complex relations, we propose a knowledge representation based on the conceptual graph model.

Introduction
L'extraction d'informations à partir de messages électroniques (mails) n'a pas été très étu-diée dans la communauté du TAL 1 . Ceci est dû principalement à la présentation informelle des mails et à leurs faibles apports d'informations. Cependant, les mails peuvent être parfois la principale source de connaissances pour une organisation ou une communauté de pratique (CoP). C'est le cas d'@pretic 2 qui est une association ouverte à tous les enseignants exploitant les TIC 3 en Belgique durant leurs interactions avec les apprenants pour préparer leurs leçons. La communication dans cette CoP se fait essentiellement par échanges de mails sur une liste de diffusion décrivant des problèmes rencontrés. Dans le but de faciliter la navigation dans cette liste de diffusion et la recherche de solutions pour des problèmes déjà posés, nous proposons une approche de création d'annotations séman-tiques pour cette liste, ces annotations reposant sur une ontologie qui est elle-même extraite en partie à partir du corpus de mails. La base d'annotations créée servira pour la navigation guidée par l'ontologie en s'appuyant sur le moteur de recherche sémantique CORESE (Corby et al., 2004). Dans ce qui suit, nous présentons l'ontologie @pretic puis nous présentons un scénario d'utilisation de cette ontologie avant de conclure.
Construction de l'ontologie @pretic
Afin de construire l'ontologie d'@pretic, nous optons pour une approche modulaire composée de quatre ontologies, chacune dédiée à une tâche particulière : (i) une ontologie pour les 1 Traitement Automatique des Langues 2 Association des professeurs exploitant les TIC en Belgique francophone : http://www.apretic.be/ 3 Technologies de l'information et de la communication composants informatiques, (ii) une ontologie pour la description des mails, (iii) une ontologie qui décrit les membres de la CoP, et (iv)  
Nettoyage du corpus
La première phase de nettoyage consiste à extraire les mails sous un format XML, supprimer les « spams », restaurer les liens entre les messages d'origines avec leurs réponses et filtrer les signatures en concevant un algorithme de détection de signature par comparaison de messages. Bien que notre corpus provienne d'une communauté francophone il était trilingue (quelques messages en anglais et en flamand). Dans une seconde phase, nous avons utilisé l'outil TEXTCAT (Canvar et Trenkle., 1994) permettant de détecter la langue. Une des origines majeures de la dégradation des textes des mails est la non accentuation. Cette erreur orthographique a un impact négatif sur l'extraction des candidats termes. Pour résoudre ce problème, nous avons utilisé l'outil REACC 5 .
À l'opposé de corpus formels où le nettoyage fournit un corps de texte non bruité, les corps des mails contiennent beaucoup de bruit même après le nettoyage. Ce bruit n'a pas de forme générique : formes de salutations, remerciements, signatures non filtrées, etc. L'écriture de filtres pour éliminer ce bruit nécessite le parcours du corpus, la localisation des zones de dégradation et l'ajout d'un filtre pour chaque zone détectée. Or cette méthode manuelle est lente et les filtres écrits peuvent être valables seulement pour les zones examinées. Nous avons adopté une méthode de nettoyage semi-automatique dont le but est d'accélérer la détection du bruit. Dans cette approche, la chaîne de TAL est fermée par un retour sur le texte original.
Extraction des candidats termes
L'extraction des candidats termes a pour objectif d'extraire un maximum de termes significatifs afin de construire une ontologie assez riche et couvrir la majorité des problèmes informatiques. Pour cela, nous utilisons deux approches de TAL, à savoir : syntaxique par le biais de l'outil FASTR (Jacquemin, 1997) et syntaxico-statistique implémentée par l'outil ACABIT (Daille, 1994).
Amorçage et enrichissement de l'ontologie
Pour l'amorçage de l'ontologie des problèmes, nous considérons les candidats termes provenant de messages initiaux, c'est-à-dire les messages qui ouvrent une discussion et qui sont susceptibles de soulever un problème. Ces messages partagent une régularité syntaxique par rapport aux termes utilisés pour exprimer un problème. Cette régularité consiste à utiliser le mot « problème » suivi du composant informatique concerné par ce problème. Cette étude nous a menés à amorcer la construction de l'ontologie par la sélection des candidats termes ayant comme tête le mot « problème ».
Problème de réception -Problème de port -Problème de réseau Problème de câblage -Problème de connexion -Problème de rapidité Problème avec hotmail -Problème sur le disque -etc.
La formalisation d'une partie de ces termes nous a permis d'avoir un premier schéma de l'ontologie qui a été validé par les formateurs de la CoP @pretic. Cependant, cet embryon d'ontologie, quoique intéressant en couvrant en grande partie des problèmes rencontrés, est assez générique et risque d'induire une ambiguïté lors de la génération des annotations. Afin d'enrichir notre ontologie et la rendre de plus en plus spécifique, nous avons effectué une analyse manuelle de tous les candidats termes générés par les outils de TAL. La liste suivante montre des exemples de termes extraits par les deux outils et utilisés pour l'enrichissement de l'ontologie.
"lenteur de connexion", "manque de mémoire", "perte de donnée", "retard dans la réponse","ordinateur contaminé", "mémoire insuffisante", "manque d'efficacité", etc.
L'étude de cette liste de termes nous permet, dans une première étape, de : -Détecter de nouveau termes significatifs permettant d'enrichir directement l'ontologie ("manque de mémoire", "lenteur de connexion", etc.). -Détecter des relations de synonymie entre certains termes significatifs ("mémoire insuffisante" = "insuffisance de mémoire" ou "message infecté" = "infection de message"). Ces termes synonymes se traduiront par un même concept de l'ontologie. -Faire émerger des régularités structurelles (i.e. patrons syntaxiques) dans une grande partie des termes ("lenteur de X", "perte de X", "difficulté de X", "retard de X", "manque de X", etc.), X étant un terme de l'ontologie des Composants.
Dans une deuxième étape, nous nous sommes inspirés du travail réalisé dans SAMO-VAR 6 (Golebiowska et al., 2001), afin de proposer des règles heuristiques qui permettent d'alimenter de façon semi-automatique l'ontologie. Ces règles détectent des structures prédéfinies dans le texte et enrichissent l'ontologie par des candidats termes qui n'ont pas été nécessai-rement détectés par les outils de TAL. Ces règles sont écrites en syntaxe JAPE (Cunningham, 2002) et greffées dans le processus d'annotation par l'ontologie des composants. 
Rattachement semi-automatique des concepts
À l'issue de ces phases de détection de termes révélateurs de problèmes, l'ontologie des problèmes informatiques ne montre pas de relations hiérarchiques entre les concepts. Par conséquent, nous avons conçu un algorithme de rattachement automatique pour lier chaque terme à un concept générique de l'ontologie Problème (Problème Matériel, Problème Logiciel, etc.). Pour chaque concept de l'ontologie Problème, nous générons une liste de concepts voisins (dans le même message) annotés par l'ontologie Composants. Nous avons choisi ensuite un ensemble de concepts pivots de l'ontologie Composants utilisés dans la majorité des discussions. Pour chaque liste obtenue, nous calculons la somme des distances sémantiques entre les concepts de cette liste et les concepts pivots. Nous calculons ces distances grâce au moteur de recherche sémantique CORESE. La catégorie retenue pour un terme est celle qui a la distance sémantique globale la plus petite. Par exemple le terme « lenteur du réseau » est rattaché à un « problème de modems » et le terme « cas d'infraction » est rattaché à un « problème de sécurité ». La méthode adoptée pour la construction de l'ontologie des problèmes est géné-ralisable pour d'autres listes de diffusion, ainsi qu'à d'autres domaines. En effet :
-La première phase de nettoyage des messages est indépendante du domaine.
-Nous supposons que les termes utilisés pour poser un problème ne diffèrent pas d'un domaine à l'autre, ce qui rend la phase d'amorçage réutilisable. -Des régularités structurelles dans les termes utilisés existent ou apparaissent au fur et à mesure au sein de la même communauté (i.e. notre cas et celui de SAMOVAR), ce qui facilite la tâche d'enrichissement de l'ontologie.
Exploitation des ontologies
Annotation sémantique
Nous avons développé un module d'annotation sémantique qui interroge le serveur d'annotation de KIM (Popov et al., 2004). Nous avons enrichi la plate-forme KIM par nos ontologies OntoPedia et l'ontologie des problèmes pour fournir pour chaque message les entités nommées détectés que nous sauvegardons en RDF 7 .
Navigation guidée par l'ontologie
L'ontologie @pretic a pour but de guider la recherche dans les questions fréquemment posées pour résoudre les problèmes rencontrés dans la CoP. Nous avons ainsi développé une interface Web (cf. figure 1)  
Conclusion et perspectives
Dans cet article, nous avons présenté une approche pour la génération semi-automatique d'une ontologie et d'annotations sémantiques à partir d'une liste de diffusion de support informatique. La finalité de ce travail est la proposition d'une FAQ sémantique utilisant des ontologies, des annotations sémantiques et des raisonnements offerts par un moteur de recherche sémantique (i.e. CORESE). L'approche proposée s'inspire en partie de celle de (Golebiowska et al., 2001) et plus généralement de celle de (Aussenac-Gilles et al., 2000), proposées pour la construction d'ontologies à partir des textes. Deux des originalités de ce travail consistent en (i) l'utilisation d'un corpus très dégradé, à savoir le corpus de mails, et (ii) l'utilisation effective de deux approches et outils correspondants de TAL, ce qui à notre avis rend les résultats plus riches.
Le caractère informel des messages échangés sur la liste de diffusion a rendu les tâches de nettoyage et d'extraction très prenantes et différentes de celles présentées dans (Even et Enguehard, 2002) où les connaissances sont extraites à partir de textes dégradés mais plus au moins formels. Le nettoyage de mails a été aussi traité dans (Tang et al., 2006). Enfin, notons que l'ontologie des problèmes a été validée par les membres de la CoP @pretic ; ils seront aussi impliqués dans la génération semi-automatique de la FAQ.

Introduction
Durant la dernière décade, les machines à vecteurs de support (ou Séparateurs à Vaste Marge : SVM) ont connu un immense succès, principalement comme puissants classifieurs. Cependant, une des principales limitations des SVM est le manque d'intelligibilité des résul-tats. En effet, les SVM ne produisent pas d'explications ni d'indices quant aux raisons d'une classification et les résultats produits doivent être pris tels quels, en faisant confiance au système. Nous proposons de rendre les SVM actionnables en classant (ordonnant) les exemples, pas seulement en les classifiant. En effet, les moyens d'action sont la plupart du temps limités, ce qui ne permet d'agir que sur une petite partie des exemples de la population. De plus, le classement peut être très utile pour "tamiser" les exemples d'apprentissage afin de ne garder que les exemples réellement importants, représentatifs des classes. L'idée sous-jacente de notre méthodologie consiste à contraster les résultats ordonnés des SVM afin de découvrir les principales propriétés caractéristiques discriminantes entre la partie haute (désignée Top dans la suite) et la partie basse (désignée par Bottom) du classement produit par SVM. Nous sommes concernés par le problème d'intelligibilité, car de notre expérience pratique, les experts du domaine sont clairement beaucoup plus confiants quand il s'agit d'agir sur des exemples hautement classés, si les raisons d'un tel classement sont fournies avec la liste ordonnée des exemples. De plus, il peut être aussi important de comprendre la partie basse du classement. Ceci peut être d'un grand intérêt pour l'expert du domaine pour diriger des actions et comprendre le système.
Le schéma général de notre approche est comme suit : -Ordonner les exemples en utilisant les SVM.
-Créer deux sous-ensembles des données ordonnées : les n exemples du haut (Top n) et les m exemples du bas (Bottom m) de l'ordonnancement. Typiquement, n = m, et ||n + m|| = 5 ? 20% du total des exemples. -Extraire l'ensemble des propriétés les plus importantes en analysant les motifs d'attributs dans les sous-ensembles Top et Bottom. Notons qu'ignorer la partie milieu de l'ordonnancement et se concentrer seulement sur les extrémités, permet de faciliter l'extraction de motifs intéressants. Nous voulons identifier les caractéristiques des exemples du haut de la liste en les contrastant aux exemples du bas. En effet, il est possible d'analyser la fréquence relative des différentes propriétés (attribut=valeur) et de calculer l'importance de telles propriétés en utilisant des indices statistiques tels que le "Leverage". Nous présentons une application de notre méthode sur diverse données dont des données médicales concernant des patients de l'athérosclérose. Nos résultats empiriques semblent très prometteurs et montrent l'utilité de notre approche quant à l'intelligibilité et l'actionnabilité des résultats produits par SVM.
État de l'Art
Un certain nombre de travaux récents (Barakat et Diederich (2004); Barakat et Bradley (2006) Nunez et al. (Nunez et al. (2002)) ont suggéré une méthode géométrique permettant de transformer un classifieur SVM en règles de classification. Pour ce faire, les auteurs utilisent les k-moyennes pour déterminer un ensemble de vecteurs prototypes. Lorsqu'ils sont combinés avec les vecteurs de support se trouvant sur la marge SVM, ces vecteurs aident à construire les frontières d'ellipsoïdes ou d'hyper-rectangles. Ces derniers sont traduits en équations Si-Alors ou à des règles d'intervalles respectivement. Une approche similaire est proposée par Zhang et al. (Zhang et al. (2005)) et suggère un algorithme pour l'extraction de règles d'hyper-rectangles (HRE) pour SVM. La principale différence avec l'approche précédente est que le regroupement des vecteurs de support (Ben-Hur et al. (2002)) est utilisé pour trouver les vecteurs prototypes de chaque classe au lieu des k-moyennes. Ceci évite de choisir le nombre de groupes à priori.
Barakat et Bradley (Barakat et Diederich (2004)) combinent les arbres de décision aux SVM pour produire les explications. Ceci est réalisé comme suit : d'abord, construire un classifieur SVM. Ensuite, sélectionner les vecteurs de support générés par le modèle et écarter leurs étiquettes de classes. Le modèle SVM est ensuite utilisé pour prédire la classe des vecteurs de support ce qui conduit à une nouvel ensemble de données. Enfin, construire un arbre de décision en utilisant les nouvelles données pour produire des règles symboliques. Les règles de décision ainsi produites sont alors évaluées sur un ensemble test afin de vérifier que les nouveaux exemples sont classés correctement par l'arbre de décision. Les mesures utilisées ici, principalement exactitude (accuracy) et fidélité, ont été étendues dans (Barakat et Bradley (2006)) à l'aire sous la courbe ROC.
Fung et al. (Fung et al. (2005)) proposent une approche pour convertir les SVM linéaires en un ensemble de règles ne se chevauchant pas de la forme : ? n i=1 l i ? x i < u i équivalentes au classifieur linéaire. Ceci est réalisé en résolvant un simple problème de programmation linéaire a 2n variables, n étant le nombre d'attributs. Chaque règle représente un hyper-cube dans un espace de dimension n avec des surfaces à axes parallèles. L'ensemble des règles optimales est calculé soit en utilisant un critère de maximisation de volume de l'hyper-cube ou un critère de couverture qui maximise le nombre de points dans le demi-espace. Les règles sont exprimées sous forme de disjonctions de conjonctions. L'approche que nous proposons diffère fondamentalement des approches décrites ci-dessus dans le sens où nous n'utilisons pas les vecteurs de support pour extraire les explications des SVM. A la place, nous focalisons plutôt sur les exemples se trouvant en haut (Top) et en bas (Bottom) des exemples ordonnés par SVM, ce qui est moins couteux que d'utiliser un ensemble de vecteurs de support potentiellement grand. Notre principal argument est que l'utilisation des vecteurs de support n'est pas nécessairement un bon choix. En effet, la zone autour de la marge SVM est très bruitée et le fait que les vecteurs de support séparent les classes ne veut nullement dire qu'ils sont représentatifs de ces classes.
Notre Approche
Notre approche se décompose en deux étapes. La première consiste à classer (ordonner) l'ensemble des exemples en utilisant les SVM. Cette étape est décrite en Section 3.1. La seconde étape concerne l'extraction des explications proprement dites et ce à partir de l'ensemble ordonné d'exemples. Nous décrivons cette étape dans 3.2 et donnons l'algorithme YSVM.
Classement via les SVM
Nous nous intéressons au problème de classement des données. Le terme classement dé-signe le processus qui consiste à considérer un ensemble de données et les classer dans un ordre significatif et utile. Le classement supervisé permet d'atteindre tel objectif en utilisant les attributs des exemples ainsi que leurs étiquettes de classe. Plus formellement, nous voudrions classer des exemples (x 1 , y 1 ), . . . , (x n , y n ), où x 1 , . . . , x n sont des vecteurs attributs décrivant les objets o 1 , . . . , o n , et chaque objet o i est étiqueté par la classe y i ? {+1, ?1}. Même si le but est de produire un classement, les données du problème sont similaires à ceux d'un problème de classification. C'est pourquoi, nous utilisons une méthode de classification (SVM) et convertissons les résultats fournis en classement. Plus précisément, nous classons les objets en triant les objets par leur valeurs de décision de SVM linéaires (Vapnik (1995)) :
où les ? sont des variables dites ressort qui pénalisent l'erreur commise et le paramètre C détermine le compromis entre régularisation et pénalisation des erreurs de classification. Le paramètre R ajuste la pénalité pour la classe positive. Puisque nous voudrions pénaliser les erreurs d'étiquetage d'un exemple par la proportion de la popultation de la classe, nous pouvons fixer le paramètre R par : R = nombre de vrais négatifs nombre de vrais positifs (2) Typiquement, SVM produit un classifieur qui étiquette les exemples x par y = sign(w T x+b), mais nous ne mettons pas de seuil à nos résultats de telle manière à pouvoir ordonner nos exemples selon la fermeté avec laquelle le classifieur prédit la classe de chaque exemple. Autrement dit, nous gardons plutôt le score donné par SVM à l'exemple et non pas seulement son signe. Nous utilisons les courbes ROC (Receiver Operating Characteristic) (Bradley (1997)) pour évaluer la qualité de notre classement. Ils procurent une bonne façon de mesurer la qualité du classement lorsque la seule vérité dont nous disposons est si un exemple appartient au haut du classement (étiqueté +1) ou plutôt en bas (étiqueté ?1). ROC est essentiellement normalisé par la cardinalité de la classe ce qui est similaire à la normalisation de la fonction de perte (loss) que nous avons pris pour apprendre le SVM. La qualité d'une courbe ROC est facilement mesurée à l'aide de l'aire sous la courbe (Area Under the Curve : AUC), qui se trouve dans l'intervalle [0, 1]. Une aire de 0.5 peut être atteinte avec un classement aléatoire des données alors qu'une aire 1.0 est atteinte en ordonnant parfaitement les exemples positifs en haut et les négatifs en bas.
Exemple Considérons un ensemble de 100 composants électriques. Chaque composant est décrit par son numéro de série, âge, taille et fabriquant et est étiqueté par son statut de panne (label=1 pour en panne, -1 autrement). Un classement SVM permet d'ordonner les composants selon leur susceptibilité aux pannes. Le haut du classement aurait dans ce cas les composants les plus sensibles aux pannes alors que les composants du bas du classement ont une moindre tendance aux pannes. Ainsi, l'expert du domaine peut focaliser sur les n composants du haut du classement pour agir, par exemple en planifiant des inspections/remplacements.
Extraction d'Explications
La phase d'extraction d'explications consiste à sélectionner et comparer d'abord les exemples du haut et du bas du classement. Ainsi, nous groupons les exemples en trois ensembles, où les exemples les plus "purs", i.e. les "très positifs" et les "très négatifs" se trouvent en haut et bas du classement respectivement. Les exemples du milieu du classement, autour de la marge SVM, sont plutôt les exemples bruités. Une question que l'on peut se poser est pourquoi considérer le haut et le bas du classement au lieu de comparer les exemples de la classe positive et ceux de la classe négative. La raison pour laquelle nous ne comparons pas simplement les deux classes est que certains exemples sont considérés comme étant négatifs alors que leur classe est en vérité inconnue. Ceci peut arriver fréquemment dans les applications réelles. Dans l'exemple précédent, nous ne sommes pas surs si les exemples négatifs le sont réelle-ment : même si ces composants ne sont pas en panne, ils peuvent l'être dans un avenir proche. Les SVM ne devraient pas classer ces exemples profondément dans le demi-espace de la classe négative, mais plutôt autour de la marge séparant les deux classes. L'approche que nous suggé-rons ici écarterait ces exemples en focalisant seulement sur le haut et le bas du classement, là ou les étiquettes des exemples sont les plus fiables. Une fois que l'on a focalisé sur les extrémités du classement, nous recherchons l'ensemble des règles intéressantes de la forme :
où P ropriete est une paire attribut-valeur et Concept est soit le concept "être en haut" ou "être en bas". Nous évaluons l'importance des propriétés en utilisant l'indice statistique de Leverage (Piatetsky-Shapiro (1991)). La raison pour laquelle nous avons choisi cette mesure est le fait qu'elle combine bien un haut pouvoir discriminant avec la capture des propriétés associées les plus fréquentes (support élevé). La mesure de Leverage a été utlisé dans d'autres tâches d'apprentissage telles que la caractérisation ( voir par exemple Turmeaux et al. (2003)), On la trouvera aussi dans la littérature avec d'autres noms tels que Nouveauté. Le Leverage de la règle ci-dessus est donné par :
La mesure de Leverage évalue la proportion d'exemples additionnels couverts par la partie gauche et droite de la règle au dessus de ceux attendus si les deux cotés de la règle sont indépendants l'un de l'autre. Clairement, nous avons ?0.25 ? Leverage(R) ? +0.25. Une propriété est dite intéressante pour un concept donné si la valeur de son Leverage est fortement positive. Ceci indique une forte association entre la propriété et le concept, alors qu'une forte valeur négative indique une forte association entre la propriété et la négation du concept. Dans notre approche, le Leverage d'une règle peut etre estimé empiriquement par :
où Concept est soit T (l'ensemble Top) ou B (l'ensemble Bottom), p est une propriété et la notation V p (x) est une fonction booléenne telle que pour un exemple x, nous avons V p (x) = vrai ou f aux ce qui veut dire que la propriété p peut être satisfaite par x ou non. 
L'algorithme YSVM (donné en Algorithme 1) explore l'espace de recherche des proprié-tés possibles pour découvrir celles qui sont les plus importantes (parties gauches de règles) qui ont conduit SVM à classer des exemples avant d'autres. Pour une meilleure visualisation, l'algorithme extrait aussi un histogramme pour chaque attribut donnant la fréquence relative de ses valeurs dans Top et Bottom. Nous avons étendu YSM pour traiter des conjonctions de propriétés. Nous l'avons également étendu pour essayer différentes valeurs de Top et Bottom afin de sélectionner les tailles qui conduisent au plus grand nombre de propriétés intéressantes.
Exemple Considérons la liste ordonnée de composants électriques illustrée dans la Table 1. Extraire les principales propriétés comme le montre le Tableau 2 aide à identifier les facteurs de pannes. Par exemple, il peut être important de trouver des motifs dans les attributs des exemples ordonnés, comme savoir que des composants particuliers d'un certain fabriquant sont disproportionnellement responsables de failles. Le but ultime est d'aider l'expert dans son choix quant à l'achat de composants fiables, planification des inspections, etc. 
Tests Empiriques
Nous avons implémenté YSVM en Python et avons conduit des tests empiriques sur divers benchmarks. Nous avons utilisé SVMLight 1 pour obtenir les classements SVM des différentes bases d'exemples.
Données Synthéthiques
Nous avons d'abord vérifié si YSVM capturait les "bons attributs". Pour cela, nous avons généré une base d'exemples aléatoires synthétiques de 1000 exemples. Chaque exemple est décrit par 50 attributs tels que X ? {?1, 1} 50 . Les étiquettes de classes sont assignées comme suit : Y = sign( k=11 k=1 X k ). En d'autres termes, l'étiquette de classe est une combinaison linéaire des 11 premiers attributs parmi les 50 attributs. YSVM a réussi à re-découvrir ces attributs en focalisant seulement sur le top 5% et bottom 5% du classement avec Leverage minimum de 0.08. Il n'a pas été possible de découvrir ces attributs en utilisant tous les exemples jusqu'á ce qu'on ait réduit le Leverage minimum à une valeur très faible. On en conclut qu'il y a plus de pouvoir discriminant dans Top+Bottom que dans toute la base d'exemples.
Données de l'Atherosclerose
Nous décrivons dans ce qui suit les tests
que nous avons effectué sur des données mé-dicales dans le contexte du projet Stulong 2 . Les données concernent une étude qui s'est étalée sur 20 ans concernant les facteurs de risque de l'athérosclérose dans une population de 1 419 hommes. Nous avons utilisé un ensemble de données préparé par (Lucas et al. (2002)) en se fixant le but d'identifier les principaux facteurs de risque de cette maladie. Les attributs utilisés sont donnés en Annexe 1 Tableau 4. Les patients ont été classés en 3 groupes : un groupe normal, un groupe à risque et enfin un groupe ayant la pathologie. Alors que cet attribut n'a pas été utilisé durant l'aprentissage du classement avec les SVM, les patients ayant la maladie et ceux qui sont à risques ont été classés avant les patients normaux. La cible d'apprentissage est l'attribut "death". La Figure  1 montre la courbe ROC pour les résultats s'apprentissage avec les parties Top 10% et Bottom 10% mises en évidence où Top regroupe les patients les plus malades et Bottom ceux en meilleure santé par rapport à la maladie. Nous avons utilisé YSVM avec différentes valeurs de Top et Bottom et avons retenu les valeurs Top=5% et Bottom=5% donnant le plus grand nombre de propriétés intéressantes. Les résul-tats sont reportés en Figure 2 et les histogrammes associés en Annexe 1, Figure 5. Nos tests avec différents Top et Bottom ont montré que plus on augmentait ||T+B||, moins on obtenait de propriétés intéressantes. Conernant le temps d'éxecution, il faut compter quelques secondes pour générer des propriétés de taille ? 2. (par manque de place, les détails ne sont pas fournis ici). Les données de Stulong ont déjà fait l'objet de nombreuses publications (voir par exemple Lucas et al. (2002)). Les facteurs de l'athérosclérose sont connus pour être principalement la consommation et durée de consommation de tabac, le surpoids, l'activité physique alors qu'il n'y pas d'évidence quant à l'impact de la consommation d'alcool comme facteur de la maladie. Tous ces facteurs ont bien été découverts par YSVM comme le montre la Figure 3 et la Figure 5. Enfin, la 
Summary
Support Vector Machines (SVMs) have attracted a great deal of attention and achieved huge success mainly as powerful classifiers. However, one of the main drawbacks of SVMs is the lack of intelligibility of the results. SVMs are "black box" systems that do not provide insights on the reasons of a classification or explanations -the results produced must be taken on faith. We are concerned about the problem of intelligibility because from our practical experience, domain experts strongly prefer Machine Learning with explanations. In that context, we have developed a new approach to provide explanations and make SVMs results more actionable. The underlying idea is to produce explanations by applying symbolic Machine Learning models to SVM-produced ranking results. More precisely, we are contrasting SVM results from the top and bottom of rankings to detect the main characteristic properties of the classes which can be useful for the practitioner to direct actions and understand the system. We applied our approach on several datasets. Our empirical results seem promising and show the utility of our methodology with regard to the intelligibility and actionability of an SVM output. Key words: Support Vector Machines (SVMs); Ranking, Rule Extraction; Actionability. 

Introduction
, le Data Mining est un processus non-trivial d'identification de structures inconnues, valides et potentiellement exploitables dans les bases de données. Plusieurs intervenants industriels ont proposé une formalisation de ce processus, sous la forme d'un guide méthodologique nommé CRISP-DM pour CRoss Industry Standard Process for Data Mining, voir Chapman et al (2000). Le modèle CRISP-DM (FIG 1) propose de découper tout processus Data Mining en six phases:
1. La phase de recueil des besoins fixe les objectifs industriels et les critères de succès, évalue les ressources, les contraintes et les hypothèses nécessaires à la réalisation des objectifs, traduit les objectifs et critères industriels en objectifs et critères techniques, et décrit un plan de résolution afin d'atteindre les objectifs techniques. 2. La phase de compréhension des données réalise la collecte initiale des données, en produit une description, étudie éventuellement quelques hypothèses à l'aide de visualisations et vérifie le niveau de qualité des données. 3. La phase de préparation des données consiste en la construction d'une table de données pour modélisation (Pyle, 1999;Chapman et al, 2000). Nous nous y intéressons plus particulièrement par la suite. 4. La phase de modélisation procède à la sélection de techniques de modélisation, met en place un protocole de test de la qualité des modèles obtenus, construit les modèles et les évalue selon le protocole de test. 5. La phase d'évaluation estime si les objectifs industriels ont été atteints, s'assure que le processus a bien suivi le déroulement escompté et détermine la phase suivante.
6. La phase de déploiement industrialise l'utilisation du modèle en situation opérationnelle, définit un plan de contrôle et de maintenance, produit un rapport final et effectue une revue de projet.
FIG 1: Processus Data Mining CRISP DM
Le modèle CRISP-DM est essentiellement un guide méthodologique pour la conduite d'un projet Data Mining. La plupart des praticiens du Data Mining s'accordent pour dire que les phases de préparation de données et de déploiement consomment à elles seules 80 % des ressources des projets. L'explication est simple. L'utilisation des méthodes statistiques nécessite de représenter les données sous la forme d'un tableau croisé : en ligne les instances et en colonnes les variables caractérisant ces instances. Or, afin d'optimiser le stockage, les données sont stockées dans des bases de données relationnelles, et ce quel que soit le phénomène étudié : les gènes, les transactions de cartes bancaires, les sessions IP, les informations sur les clients… Lors de la phase de préparation de données, la première tâche de l'analyste est donc d'extraire un tableau croisé du système d'information. Cette étape n'est pas anodine car le nombre de représentations potentielles des données relationnelles dans un tableau croisé est gigantesque (FIG 2). En pratique, l'analyste doit faire un choix a priori de l'ensemble des variables explicatives sur lesquelles se fera l'étude. La conséquence est que la perte d'information due à la mise à plat des données relationnelles est très importante.
Lors de la phase de déploiement, le modèle construit préalablement doit être appliqué à toute la population concernée, afin de produire un score pour chaque instance. Toutes les variables explicatives pour toutes les instances doivent être construites. Cette étape est potentiellement très couteuse lorsque le nombre d'instances et de variables explicatives est important.
Les principaux produits commerciaux de Data Mining, comme SAS, ou SPSS, proposent des plateformes permettant de construire et de déployer des modèles prédictifs. Néanmoins, ils n'offrent pas de solution satisfaisante pour exploiter tout le potentiel de l'information contenue dans la base de données source. Dans les applications industrielles construites sur ces plateformes logicielles, le nombre de variables explicatives à partir desquelles sont construits les modèles reste limité à quelques centaines. Or le potentiel est tout simplement d'un autre ordre. Dans l'exemple illustratif présenté (FIG 2), la base de données ne contient que deux tables. L'étude du nombre d'usages par type de service, par mois, et par jour de la semaine pourrait conduire à elle seule à construire 10000 variables explicatives ! RNTI -X - Avec ces technologies, un difficile compromis doit être trouvé entre le coût de mise en oeuvre et la qualité de l'information produite.
Parmi les produits commerciaux, KXEN propose un module permettant de construire automatiquement des agrégats à partir de données temporelles. L'avantage est de pouvoir explorer un plus grand nombre de variables explicatives. Nous avons très largement généralisé et systématisé cette approche pour automatiser entièrement le processus de préparation de données. La construction de variables explicatives est entièrement pilotée par un algorithme de sélection de représentation très performant (Boullé, 2007 2 L'automatisation de la préparation de données 2.1 Généralités L'objectif de la sélection de représentation est triple: améliorer la performance prédictive des modèles, le temps d'apprentissage et de déploiement des modèles, et permettre leur interprétation (Guyon et Elisseeff, 2003). La sélection de variables est un sujet bien couvert dans la recherche en fouille de données, si bien qu'aujourd'hui, les méthodes de sélection de variables sont suffisamment robustes pour permettre la construction de modèle en très grande dimension (Guyon et al, 2006) 
L'architecture de traitements
Contrairement à l'architecture actuelle de la fouille de données, les variables explicatives ne sont pas calculées à l'avance dans un datamart. Dans notre architecture de traitements, les données permettant de construire les variables explicatives sont stockées dans une base de données relationnelle simple, le data folder (FIG 3). Les variables explicatives sont construites et sélectionnées automatiquement en fonction de l'étude menée. Le modèle de données du data folder permet d'assurer une normalisation des différentes sources de données qui seront toujours présentées sous la forme d'un schéma en étoile : 
Le pilotage des extractions
Les extractions entre le data folder et les tableaux croisés sont paramétrées par trois types de dictionnaires :
-le dictionnaire de sélection pour filtrer les instances, -le dictionnaire de requêtes pour spécifier les mises à plat des données du data folder,
-le dictionnaire de préparation pour spécifier le recodage des variables. Ces dictionnaires d'extraction permettent de définir des requêtes suffisamment simples pour être pilotées automatiquement par les processus de sélection de représentation et suffisamment expressives pour produire une très grande variété de variables explicatives.
Quelque soit son objet, une requête portera toujours sur deux tables au plus : la 
La sélection de représentation
L'architecture de traitements permet à un algorithme de piloter efficacement des extractions de tableaux croisés pouvant compter des dizaines de milliers de variables. Pour sélectionner la meilleure représentation possible, nous avons besoin d'une méthode de sélection de variable particulièrement robuste et rapide. Deux approches principales, filtre et enveloppe (Kohavi et John, 1997), ont été proposées dans la littérature pour sélectionner les variables. Les méthodes enveloppes sont très coûteuses en temps de calcul (Féraud et Clérot, 2001, Lemaire et Féraud, 2006. C'est pourquoi nous avons retenu une approche de type filtre pour déterminer et construire les variables explicatives pertinentes. L'approche filtre la plus fréquemment utilisée repose sur la mise en oeuvre de tests statistiques (Saporta, 1990), comme par exemple le test du Khi2 pour les variables explicatives catégorielles, ou les tests de Student ou de Fisher-Snedecor pour les variables explicatives numériques. Ces tests d'indépendance sont simples à mettre en oeuvre, mais présentent de nombreux inconvénients.
RNTI -X -Ils se limitent à une discrimination entre variables dépendantes et indépendantes, sans permettre un ordonnancement précis des variables explicatives, et sont contraints par des hypothèses d'applicabilité fortes (effectifs minimaux, hypothèse de distribution gaussienne dans le cas numérique…). De nombreux autres critères d'évaluation de la dépendance entre deux variables ont été étudiés dans le contexte des arbres de décision (Zighed et Rakotomalala, 2000). Ces critères sont basés sur une partition de la variable explicative, en intervalles dans le cas numérique et en groupe de valeurs dans le cas catégoriel. En recherchant de façon non paramétrique un modèle de dépendance entre variables explicatives et cible, ils permettent une évaluation fine de l'importance prédictive des variables explicatives. Dans le cas où tous les modèles de partitionnement de la variable explicative sont envisagés, un compromis doit être trouvé entre finesse de la partition et fiabilité statistique. Ce compromis est réalisé dans l'approche MODL (Minimum Optimized Description Length, voir Boullé, 2005Boullé, , 2006   Les trois premiers termes représentent l'a priori du modèle: choix du nombre d'intervalles, des bornes des intervalles, et de la distribution des valeurs cibles dans chaque intervalle. Le dernier terme représente la vraisemblance d'observer les valeurs de la variable cible connaissant le modèle de discrétisation. A titre illustratif (FIG 4), nous donnons le modèle de discrétisation de la variable SepalWidth obtenu par l'optimisation de ce critère pour séparer les différents iris (Blake et Merz, 1996). Le même type de critère est construit pour le groupement de valeurs. Le critère possède une structure similaire à celle du critère de discrétisation, en remplaçant dans les deux premiers termes la probabilité a priori d'une partition en intervalles par celle d'une partition en groupes de valeurs.
( )  La FIG 5 illustre le résultat du groupement des valeurs de la variable couleur de chapeau grâce à l'optimisation du critère pour la classification des champignons comestibles et vénéneux (Blake et Merz, 1996).
La discrétisation et le groupement de valeurs optimaux sont recherchés en optimisant les critères d'évaluation, au moyen de l'heuristique gloutonne ascendante décrite dans (Boullé, 2005). La complexité algorithmique en O (JN log (N)) de cette heuristique associée à l'excellente fiabilité de la méthode nous permet de traiter un grand nombre de variables, de l'ordre de 50 000. Un modèle bayesien naïf sélectif (Boulle 2007), basé sur une sélection régularisée des variables et une moyenne de modèles, est ensuite construit pour supprimer les variables redondantes et pour produire les scores. 
RNTI -X -
La sélection des parangons
La table des parangons est déterminante pour la performance finale du système. Une table de parangons peu représentative pourrait conduire à la construction de scores inefficace sur l'ensemble de la population. A contrario, une base de parangons de très grande taille diminuerait sensiblement l'intérêt de l'utilisation des parangons. Nous devrons donc gérer au mieux le compromis entre la réduction de volumétrie et la représentativité de la base.
Pour sélectionner efficacement les instances, notre approche est d'utiliser un algorithme d'échantillonnage en une seule passe optimisant un critère de représentativité de l'échantillon. L'algorithme Reservoir Sampling permet de construire un échantillon grâce à un réservoir maintenu en ligne en ajoutant et supprimant aléatoirement des instances du réservoir (Vitter 1985). Cet algorithme stochastique est très bien adapté au cas des flux de données de taille infinie et pas nécessairement stationnaires. Néanmoins le problème que nous cherchons à résoudre est de nature différente : nous connaissons la table de données que nous voulons échantillonner. Dans ce cas un algorithme déterministe est plus adapté. Il permet d'optimiser un critère de représentativité de l'échantillon (Li 2002) en un nombre connu à l'avance d'itérations. Des versions déterministes de Reservoir Sampling existent. L'algorithme Deterministic Reservoir Sampling (Akcan et al 2006) minimise sur le réservoir un critère local inspiré de l'algorithme FAST (Chen et al 2002) : la distance L2 entre l'échantillon et l'ensemble total est minimisée en ajoutant et supprimant en ligne des instances au réservoir.
Afin de réduire le temps de traitement, pour optimiser le critère local nous nous contentons de remplir au fur et à mesure un réservoir jusqu'à ce qu'il atteigne la taille P désirée sans supprimer d'instances. Le risque théorique est de tomber dans un minimum local. En pratique, la taille de notre échantillon est suffisamment importante tant en termes de proportion que de nombre d'instances pour que ce risque soit faible (de l'ordre de 1% pour une taille de 10000). L'algorithme utilisé est le suivant :
1. Le réservoir est initialisé par les K premières instances rencontrées. 2. Pour p allant de K à P : -une instance est choisie dans une fenêtre de recherche de taille M de manière à minimiser C(p) le critère de qualité de l'échantillon, RNTI -X --la fenêtre est ensuite décalée de L instances de manière à obtenir un échantillon de taille P lorsque la table complète de taille N sera parcourue, avec L = (N-M)/P. La taille de la fenêtre de recherche permet de régler un compromis entre le coût de traitement et la précision de l'algorithme : plus M est grand, mois l'algorithme est rapide, mais plus il est précis. Le paramètre K permet d'initialiser l'algorithme d'optimisation. Une trop petite valeur de K risque de rendre totalement inefficace les premières itérations de l'optimisation du Khi².
Nous utilisons le critère du Khi² pour mesurer la proximité entre l'échantillon et la table complète. L'algorithme de discrétisation et groupement de valeurs, décrit à la section précédente, nous permet d'extraire une représentation binaire : chaque variable binaire i correspond à la fréquence d'une modalité d'une variable discrétisée. Le critère à minimiser s'écrit alors :
Où i S est la fréquence théorique de la variable binaire i donnée par l'algorithme de discrétisation et groupement de valeurs, et p i S la fréquence observée dans l'échantillon de taille p.
L'indexation
Le problème qu'on cherche à résoudre est simple à énoncer: étant donné un individu, trouver son plus proche voisin parmi la base des parangons; on souhaite faire cela pour tous les individus de l'entrepôt.
La recherche du plus proche voisin est une opération coûteuse. Son implémentation naïve implique une recherche exhaustive parmi les parangons, donc une complexité en O(n.m.p), n étant le nombre d'individus dont on cherche les voisins, m le nombre de variables dans l'espace de représentation et p le nombre de parangons. Afin d'accélérer la recherche du plus proche voisin, on peut être amené à préférer un compromis entre vitesse et performance plutôt que de viser la performance maximale (trouver le plus proche voisin). C'est précisément ce que permet l'algorithme Locality Sensitive Hashing (Gionis et al 1999). Il repose sur une technique de hachage pour sélectionner de bons candidats parmi les parangons pour être proche voisin de l'instance considérée. On applique ensuite une recherche exhaustive parmi ces candidats. Notre implémentation de cette technique permet de ramener la complexité de la recherche du plus proche voisin à ( ) p m n o . . soit un gain d'un facteur 300 pour 100 000 parangons, et laisse à l'utilisateur le contrôle du compromis vitesse / performance.
Expérimentations
Pour mesurer la fiabilité des scores produits par notre approche, nous avons comparé des scores construits pour les campagnes marketing de France Télécom avec ou sans notre technologie. Nous avons alimenté la plateforme avec différents jeux de données provenant des applications décisionnelles du groupe France Télécom. Nous avons consolidé des informations de 1 000 000 de clients du groupe sur un passé récent entre janvier et juin 2005.
RNTI -X -
Les quatre premiers mois ont été utilisés pour construire les profils des clients et les deux derniers pour calculer la variable cible. 20% des clients sont réservés pour l'évaluation les modèles. Pour évaluer la qualité d'un modèle, nous utilisons une courbe de gain (FIG 6). Ce type de courbe permet de sélectionner un modèle par rapport à son efficacité économique. L'axe des abscisses correspond à la proportion de la population visée par les courriers et donc au coût de la campagne. L'axe des ordonnées identifie le pourcentage de la population cible touchée, et donc le gain de la campagne marketing. Le modèle utilisé actuellement par les services marketing de France Télécom se base sur 200 variables explicatives. Lorsque 20% de la population est contactée, 45% des clients fragiles sont contactés. Le gain est de + 25% par rapport au ciblage aléatoire. L'automatisation de la recherche de représentation, nous a conduit à sélectionner un modèle se basant sur 191 variables explicatives choisies dans un ensemble de 50 000 variables. Le modèle est ensuite déployé sur toutes les instances en utilisant un nombre variable de parangons : 500, 5 000, 15 000, et déploiement direct sur la population. Avec un déploiement direct sur toutes les instances, lorsque 20% de la population est contactée en se basant sur ce ciblage, 65% des clients qui vont rendre leur abonnement dans les deux prochains mois sont touchés. Par rapport à la technique actuelle, pour le même nombre de courriers, 20% supplémentaire de la population cible est touchée. Cette amélioration du ciblage est vraie sur toute la courbe de gain.
Lorsque la technique de déploiement des scores par les parangons est utilisée, il y a une perte potentielle de fiabilité qui dépend du nombre de parangons utilisés. Plus ce nombre est important, plus le ciblage est proche du meilleur possible, mais plus il est couteux à utiliser. Lorsque 5 000 parangons sont utilisés pour représenter 1 000 000 clients, à 20% de la population, 60% des clients fragiles sont touchés. Le gain reste de + 40% par rapport à RNTI -X -l'aléatoire et de + 15% par rapport à la technique actuelle. Avec 15 000 parangons, les performances obtenues sont quasiment similaires à celles obtenues avec un déploiement direct. Pour évaluer la qualité de l'algorithme de sélection de parangons, nous avons comparé les performances obtenues lorsque les parangons sont sélectionnés de manière aléatoire, avec les performances obtenues lorsque les parangons sont sélectionnés en optimisant le Khi² entre la distribution théorique des variables et celle obtenue dans l'échantillon. Avec 500 parangons obtenus avec K=200 et M=1000, à 20 % de la population, 50 % de la cible est atteinte pour la sélection aléatoire contre 55 % pour l'optimisation locale du Khi² (FIG 6).
Le processus complet d'extraction de la table des parangons à partir d'un million de clients et d'un espace de recherche de 50 000 variables correspond à 20 heures de calcul sur un serveur comprenant quatre processeurs à 3 Ghz muni chacun de 3 Go de mémoire. Deux tiers du temps de traitement correspond à la sélection de la représentation et un tiers par la recherche et l'indexation des parangons. Une fois les parangons obtenus, la production des scores à partir de la table des parangons est faite en moins d'une minute, alors que 2 heures de traitements sont nécessaires avec la méthode habituelle pour générer une table d'un million d'instances caractérisées par 191 variables explicatives et appliquer le modèle sur cette table. L'utilisation de parangons est très efficace pour déployer un score récurrent. : en utilisant de l'ordre de 1 % de parangons pour représenter la population totale, la perte de précision du modèle est négligeable et le coût de déploiement est divisé par 100.
Conclusion
Nous avons décrit une plateforme de fouille de données permettant de construire des modèles de prévision basés sur un nombre de variables explicatives de deux ordres de grandeurs au-dessus de ce qui se fait actuellement. La conséquence est une nette augmentation de la qualité des modèles. Cette plateforme repose sur une architecture novatrice permettant d'automatiser les traitements couplée avec des méthodes performantes de construction, sélection, et indexation de variables et / ou d'instances. Le temps de traitement du à la mise à plat des données reste la principale limite à l'exploration d'un espace de recherche plus grand. Pour aller plus loin dans l'exploration des grandes masses d'information, nous devrons élaborer une méthode de parcours de l'espace des variables permettant de se diriger plus rapidement vers les zones contenant les variables pertinentes.

Introduction
Contexte
Le traitement des documents de propriété intellectuelle, tels que les brevets, est important pour l'industrie, les affaires et les communautés juridiques. Récemment, les communautés de recherche académiques et en particulier, les chercheurs de traitement automatique de la langue naturelle et de la recherche documentaire ont reconnu l'importance du traitement des brevets. En fouillant les brevets scientifiques, nous pouvons remarquer un volume important d'informations sur la biologie, les substances et les procédures médicales. En effet, l'extraction des informations de ces brevets permet de donner une idée précise sur : (i) par exemple les interactions biomédicales et l'effet pharmacologique résultant, et (ii) la propriété intellectuelle dans un certain contexte biologique.
Durant ces dernières années, de grands efforts ont été exercés pour mettre les données relatives aux brevets sous une forme électronique et les présenter au public via les services en ligne. De nos jours, nous remarquons que ces services présentent et fournissent des structures de données hétérogènes, ce qui rend difficile à mettre en oeuvre une analyse automatique des brevets.
Dans ce papier, nous présentons l'approche PatAnnot fondée sur les principes du web sémantique et qui se réfère aux notions de métadonnée et ontologies pour faciliter l'extraction des connaissances et la recherche d'informations relatives aux brevets.
Ce travail rentre dans le cadre du projet européen Sealife (Schroeder et al, 2006) qui a pour objectif la réalisation d'un navigateur Web sémantique pour le domaine des sciences de la vie, qui exploitera les ressources du Web en les rendant partageables, accessibles et manipulables par plusieurs utilisateurs dans différents domaines biomédicaux et ce afin de favoriser le partage des connaissances.
Annotations sémantiques sur les brevets
Notre travail vise à faciliter la génération automatique des annotations sémantiques à base d'ontologies sur les brevets accessibles en ligne et repose sur une approche basée sur les principes et les technologies du web sémantique. Ces annotations peuvent être utilisées par les moteurs de recherche sémantiques afin d'extraire les connaissances incluses dans les brevets et les présenter selon le profil de l'utilisateur.
Une annotation est une description permettant d'avoir une information du type métadon-née facilitant l'exploitation, l'accès, la recherche et la reconnaissance d'une ressource. L'annotation peut se baser sur un modèle conceptuel comme par exemple une ontologie afin d'avoir un aspect sémantique lui permettant d'être utilisable, accessible et reconnue par un ensemble d'acteurs ou d'agents. Ainsi une annotation sémantique permet d'établir un lien entre une entité d'une ressource donnée et sa représentation sémantique décrite dans le modèle qui est en général une ontologie relative au domaine où la ressource évolue.
La formalisation du modèle d'annotation se base sur des ontologies ; l'utilisation de la hiérarchie de l'ontologie peut (i) permettre aux annotateurs de choisir le niveau approprié de détail de l'annotation, (ii) diminuer l'ambiguïté de la connaissance et (iii) aider à réduire les erreurs au cours du processus d'annotation. Dans le contexte du web sémantique, l'utilisation des formalismes standards tels que RDF (Lassila et Swick, 2001)  
L'approche PatAnnot
L'idée capitale est de pouvoir prendre en considération la structure des brevets afin de retrouver un lien entre, d'une part, les connaissances contenues dans les documents et, d'autre part, les concepts de l'ontologie utilisée. Nous avons commencé par parcourir la littérature des brevets existante en ligne à travers les sites officiels des offices nationaux et internationaux des brevets et quelques moteurs de recherche spécifiques aux brevets afin de déterminer les caractéristiques de ces documents en termes de structure, contenu et typologie. Ainsi l'annotation sémantique générée portera sur trois aspects principaux : la structure, les méta-données et le contenu textuel des documents de brevets. Vers une fouille sémantique des brevets Nous pouvons remarquer que l'ontologie de brevet intervient dans différents niveaux de l'architecture du système grâce à sa modularité, ce qui permet de raffiner l'annotation sé-mantique et de l'enrichir à chaque étape.
Ontologie de brevet : PatOnto
La connaissance de la structure du document de brevet fournit non seulement une meilleure image sur la morphologie du document mais peut aussi réellement mener le procédé d'analyse. Ainsi la représentation de ces documents est une tâche importante. Jusqu'ici la recherche et le développement dans le domaine de l'analyse des brevets ont été limités à des approches qui ne tiennent pas compte du contenu sémantique des brevets. Ainsi les tâches orientées contenu sont assurées manuellement. De ce fait, une représentation sémantique des documents de brevets ne doit pas porter sur un seul aspect de ces documents. Nous avons donc conçu une ontologie modulaire PatOnto qui modélise une représentation sémantique des brevets. La figure 3 décrit les différentes composantes de cette ontologie. 2. Les métadonnées du document de brevet sont modélisées par l'ontologie ''Auxiliary-Data''. C'est une modélisation des informations explicites qui décrivent directement des documents de brevets, et des informations implicites qui exigent un traitement linguistique avancé. Les métadonnées explicites dans ces documents sont utiles pour la description des ressources documentaires puisqu'elles concernent le titre de l'invention, le nom de l'inventeur, la classification du document et toute autre information bibliographique. Ainsi les métadonnées explicites sont bien définies, elles suivent des règles spécifiques. Les métadonnées implicites se composent des concepts qui doivent être extraits en appliquant des niveaux plus élevés d'association entre les documents de brevets avec leur contenu textuel. Ce genre d'informations peut être extrait de la section des références située au niveau de la section de page de garde. La figure 5 décrit un extrait de cette ontologie que nous avons développé en OWL et qui est composée de 109 concepts et 54 relations. 3. L'ontologie ''Patent Media-Content'' décrit le contenu non textuel du document de brevet, en effet les illustrations d'une invention font croître la complexité du document car ce ne sont pas des images ordinaires mais divers genres de schémas qui n'ont pas la même interprétation. Le but d'une telle ontologie est de modéliser les objets multimédias qui constituent une partie du document de brevet. Cette ontologie est constituée de 59 concepts et 18 relations, modélisés en OWL. Les concepts que nous avons conçus sont disjoints, par exemple, un objet multimédia ne peut pas être classé en même temps comme étant une formule et une figure. La figure 6 pré-sente un extrait de cette ontologie. ChemicalFormula Figure   FIG. 6 -Extrait de l'ontologie « Patent Media-Content ».
En conclusion, l'ontologie de brevets PatOnto que nous avons conçue et développée est constituée de trois sous-ontologies décrites précédemment, cette ontologie servira comme référence dans le processus de génération des annotations sémantiques sur les brevets.
L'annotation sémantique est basée aussi sur une ontologie de domaine, de ce fait comme nous traitons les brevets du domaine biomédical nous avons choisi le vocabulaire UMLS (Humphreys et Lindberg, 1993)   (Khelif et al., 2007).
La section suivante reprend le processus de génération des annotations sémantiques dé-cris par le schéma d'architecture de la figure 2.
Génération des annotations sémantiques
La génération des annotations sémantiques sur les brevets est un processus qui suit les étapes suivantes que nous allons détailler par la suite :
1. Structurer le document de brevet ; 2. Générer les annotations sémantiques correspondant aux métadonnées utilisant l'ontologie de brevet. 3. Générer les annotations sémantiques sur le contenu en se basant sur l'ontologie de brevet et l'ontologie de domaine et fusionner les annotations. Nous comptons sur la structure quasi uniforme des brevets fournis par les organismes principaux de la propriété intellectuelle dans le monde qui éditent plus de 80% 2 des brevets accordés dans le monde (OMPI, USPTO 3 ,…). Nous supposons que prendre en considération la structure et le contenu textuel des documents de brevets va nous permettre de générer des annotations sémantiques plus riches.
Structuration des documents de brevets
Les documents de brevets accessibles en ligne sont en majorité en format HTML. En analysant progressivement cette représentation, nous avons proposé une méthode de transforma-tion de ce format initial vers le format standard XML. L'utilisation de XML résout le problème en assurant une description structurée du document de brevet, mais il n'y a aucune manière générique simple qui permet de convertir un document HTML vers XML en tenant compte de l'arborescence de ses métadonnées. Ainsi nous avons proposé un processus basé sur les transformations XSLT qui permet de (i) charger le document de brevet en le représen-tant sous forme d'une arborescence et (ii) le transformer en XML.
Phase 1 : Construction de l'arbre HTML du document de brevet Les balises HTML sont utilisées par les navigateurs pour gérer l'affichage d'une page HTML. En effet ces balises sont conçues pour permettre le repérage des structures logiques ayant une influence sur la présentation physique.
Le format HTML tolère la non fermeture de certaines balises comme <br>, <hr>, <p>, <li> etc. ainsi que la fermeture d'une balise avant la fermeture de ses fils, par exemple, <strong><font>toto</strong></font>. Par conséquent, la construction d'un arbre de repré-sentation du document est impossible puisque nous avons une ambiguïté dans la classification des noeuds. Comme solution, nous avons appliqué une API existante appelée ''HTMLCleaner'', cette API permet de corriger le code HTML d'un document et d'extraire son arborescence. Un tel arbre se compose d'une racine, des noeuds internes et des feuilles ; un noeud correspond à une étiquette HTML, une feuille peut être un texte ou une étiquette. Phase 1 : Annotation basée sur l'ontologie « Patent Content-Form » Cette annotation est la principale qui sera raffinée pendant les étapes suivantes du processus de génération des annotations sémantiques. En utilisant un fichier de configuration que nous avons proposé, la correspondance consiste à parcourir le document XML du brevet et l'ontologie et à construire un fichier de correspondance (figure 9).
Si le noeud courant correspond à un concept C de l'ontologie, alors ce concept C est ajouté au document de correspondance et une recherche dans l'ontologie est effectuée afin de trouver les propriétés appropriées qui peuvent relier C à d'autres concepts de l'ontologie :
-Pour chaque propriété P trouvée, le processus explore le document XML de brevet afin de trouver un autre concept C' pouvant être lié à C par cette propriété. Si un tel concept C' existe, alors la propriété P et le concept C' sont ajoutés au document de correspondance; -Si aucune propriété n'est trouvée, alors le concept C est considéré comme un noeud isolé; Le document de correspondance résultant est de la forme suivante : <ResultMapping> <Concept> <Cname>PatentDocument</Cname> <OnProperty> <Property>hasPatentType</Property> <AppliedOn>PatentType</AppliedOn> </OnProperty> ……………….
FIG. 8 -Extrait du fichier de correspondance.
Parcourant ce fichier de correspondance et le document de brevet, un processus récursif permet la génération de l'annotation selon les étapes suivantes :
-Construire en mémoire un objet « annotation » de type DOM qu'une méthode spéci-fique se charge de l'initialiser avec tous les « espaces de noms » requis ; -Pour un concept C du document de correspondance : -Construire une instance de C dans l'objet « annotation » avec la description formelle de la syntaxe RDF ; -Pour chaque propriété P figurant dans la description « OnProperty » du document de correspondance comme noeud fils du C, construire une instance de P suivant la syntaxe RDF et trouver le concept C' qui est associé à l'étiquette XML figurant dans le noeud fils « AppliedOn » : o si C' se trouve dans le document de configuration comme concept qui a des propriétés comme noeuds fils, alors le processus refait le même traitement en gardant la référence du noeud XML ; o Sinon, instancier le concept et mettre la valeur du noeud XML correspondant comme attribut qui donne la valeur de cette ressource en RDF ; L'annotation générée est une première version de l'annotation sémantique de brevet qui va être enrichie tout au long du processus d'annotation.
Phase2 : Annotation basée sur l'ontologie « Auxiliary-Data » Des parties textuelles telles que les références, les revendications et la description sont employées pour raffiner l'annotation de métadonnées. Par exemple, au niveau des références, nous classons ces références en nous basant sur les concepts de cette ontologie (références de brevets américains, références étrangères et citations, …) puisque les brevets sont liés et une analyse de plusieurs brevets fournit des résultats plus consistants que ceux obtenus en étu-diant chaque brevet à part. Ainsi, l'annotation sur la bibliographie peut être enrichie. Pour cela nous avons conçu une méthode qui permet lors de traitement d'une partie appropriée du document, de générer une annotation sémantique en se fondant sur l'ontologie ''AuxiliaryData'', ensuite cette annotation est ajoutée à son emplacement dans l'annotation principale.
Grâce à cette ontologie, nous pouvons décomposer les parties textuelles du brevet en plusieurs petites parties significatives ayant chacune un lien avec tout le document pour préparer la phase de traitement automatique de la langue et l'annotation de contenu basée sur l'ontologie de domaine. 
Génération de l'annotation du contenu basée sur l'ontologie de domaine
Les documents de brevet ont une terminologie spécifique et concrète qui affecte n'importe quel genre de traitement linguistique. La terminologie biologique semble fréquente et importante de ce fait, nous avons utilisé UMLS (Humphreys et Lindberg, 1993) comme ontologie de domaine pour traiter les brevets biomédicaux. Si au cours de l'annotation le processus rencontre une partie textuelle, il la sauvegarde en mémoire et garde une trace du noeud approprié de l'objet « annotation », ce contenu textuel va être annoté par un autre processus. Ainsi nous avons conçu un ensemble de méthodes qui interrogent le module Mea-tAnnot (Khelif et al, 2007). MeatAnnot fait partie du projet MEAT (Memoire d'Expériences pour l'Analyse du Trascriptome) élaboré au sein de l'équipe edelweiss/Acacia, il est généri-que et indépendant de toute ontologie ou plateforme et assure la détection des concepts existant dans le texte en se basant sur l'ontologie du domaine en question. MeatAnnot repose sur des outils TALN (Traitement Automatique de la Langue Naturelle) tel que TreeTagger (Helmut, 1994), GATE (Cunningham et al, 2002) ainsi que d'autres extensions propres à edelweiss dédiées à la détection des relations sémantiques et l'extraction des concepts UMLS.
Après avoir collecté toutes ces informations linguistiques, MeatAnnot permet de générer une annotation RDF décrivant le texte proposé en entrée.
Grâce à l'utilisation des ontologies, nous avons pu générer des annotations sémantiques sur les documents de brevets décrivant leur contenu sémantique, ces annotations sont regroupées dans une base d'annotations qui est chargée dans le moteur de recherche sémantique CORESE (Corby et al, 2004). Deux services Web ont été également développés pour encapsuler les différents processus, le premier prend en entrée un document brevet et fournit son annotation sémantique, et le deuxième permet de répondre aux requêtes en interrogeant Corese via SPARQL 4 .
Conclusion
Discussion
L'approche ''PatAnnot'' que nous venons de présenter dans cet article vise à fournir un support méthodologique et technique pour faciliter la fouille des documents de brevets, considérés comme une source inestimable d'informations scientifiques. En effet, le système implémentant l'approche est un système modulaire développé en java et utilise les technologies standards du web sémantique. Notre système est générique et se compose de modules réutilisables ; (i) l'ontologie modulaire ''PatOnto'' que nous avons conçue et construite est indépendante du domaine et couvre toute information structurelle dans un document de brevet, (ii) l'intégration des feuilles de style XSLT qui permettent de générer les documents XML est facile et flexible, (iii) l'outil MeatAnnot est générique et indépendant des ontologies utilisées, (iv) le générateur permet de rassembler toutes les parties des annotations relatives à plusieurs ontologies en une seule annotation sémantique d'un document de brevet qui porte sur plusieurs aspects de cette ressource. Enfin, PatAnnot a été testé et validé sur les deux plus grandes bases des brevets : USPTO et EPO, et ce en annotant automatiquement plus de 1000 brevets.
Travaux connexes
Dans de nombreux domaines l'analyse des brevets est une tâche très importante dont le but est de décrire l'état de l'art des inventions et préserver la propriété intellectuelle. Parmi les outils aidant à effectuer cette tâche nous citons :
Le système Vigitext qui repose sur la méthode d'exploration contextuelle pour la fouille des documents techniques (en particulier les brevets) à des fins de veille technologique (Gougon, 2000). Patent Cafe 5 , qui aide à mener des recherches professionnelles dans diffé-rentes bases de brevets et exploite l'information sur la propriété intellectuelle provenant des différents offices. BioPatentMiner (Mukherjea et al, 2005) facilite la recherche d'information dans les brevets biomédicaux, en identifiant les termes biologiques et les relations entre les brevets dans le but de fournir une approche basée sur le web sémantique. PATExpert (Mark et al, 2006) est un nouveau projet visant à fournir une approche web sémantique et des techniques avancées de traitement de brevets. Notre approche fondée sur les ontologies et les annotations sémantiques est originale par rapport à ces outils. En outre, notre correspondance entre les documents XML et les ontologies qui diffère de (Amann et al, 2001) et (Xiao et al, 2006) par sa généricité et sa manipulation des technologies du Web Sémantique.
Perspectives
Ce travail sur l'exploitation de brevets illustre une application intéressante de Web sé-mantique, très utile pour la gestion de connaissances des compagnies et des communautés collaborant par le Web. Comme perspective nous pouvons ajouter un module basé sur des outils de TALN qui permet d'extraire à partir de la partie revendications le type du brevet (modèle d'utilité, brevet de conception ou brevet d'usine). L'approche peut être appliquée à d'autres domaines que le domaine biomédical : par exemple les domaines techniques. D'ailleurs, les principes de notre approche (c.-à-d. l'exploitation de la structure des documents et des techniques de TALN sur leur contenu textuel pour produire des annotations sémantiques) pourraient être généralisés à d'autres genres de documents structurés (par exemple fiches patients, fiches d'incident, etc.).
Remerciements
Nous remercions la commission européenne pour le financement de ce travail dans le cadre du projet européen Sealife (IST-2006-027269).

Introduction
Les travaux présentés dans cet article répondent aux besoins d'une experte médiéviste souhaitant découvrir des connaissances nouvelles dans un corpus de textes écrits en Ancien Français. Les connaissances extraites à partir de ce corpus sont sous forme de motifs séquentiels. Dans notre contexte, un motif séquentiel est une suite ordonnée d'itemsets (phrases). Un itemset est un ensemble d'items (mots). Par exemple, le motif <(chevalier dam)(roi)> extrait à partir de notre corpus signifie que, souvent, les mots "chevalier" et "dam" apparaissent ensemble au sein d'une même phrase avant l'apparition de "roi" dans une phrase suivante. Ceci permet aux experts d'analyser, sans a priori, les mots et enchaînements de mots qui apparaissent dans un même contexte, mettant ainsi en relief des associations susceptibles d'apporter des connaissances nouvelles à un expert. Notons que dans l'étude actuellement menée, l'experte médiéviste souhaite plus particulièrement découvrir des motifs séquentiels faisant intervenir des mots propres à la parenté. Les différentes étapes et fonctionnalités de notre logiciel sont décrites dans la section suivante.
Processus d'extraction des motifs séquentiels
La première étape du prétraitement des données textuelles consiste à appliquer le Tree Tagger de Schmid (1994) qui possède des règles et des lexiques adaptés à l'Ancien Français. Ce système apporte des informations grammaticales aux différents mots du texte (par exemple, étiquettes "adjectif", "nom", etc). Les mots qui sont davantage porteurs de sens tels que les noms peuvent alors être filtrés. Par ailleurs, l'utilisation du Tree Tagger permet de lemmatiser les mots du corpus. Après ce prétraitement, l'extraction des motifs séquentiels à partir des données textuelles peut s'effectuer à l'aide de la méthode SPaC (Sequential PAtterns for Text Classification) qui est décrite dans (Jaillet et al. (2006)).
Un thème pouvant être privilégié par l'utilisateur (dans notre cas la parenté), notre logiciel permet de n'extraire que des motifs relatifs à cette thématique au travers d'une liste de
FIG. 1 -Le moteur de recherche de l'interface de visualisation des résultats et la consultation de l'origine du motif séquentiel (chevalier)(enfant).
mots pertinents du domaine. Les motifs séquentiels extraits du corpus ne seront alors que ceux dont au moins un item est un mot de la liste manuellement établie pas l'utilisateur. L'ajout de connaissances permet donc de filtrer les motifs offrant ainsi à l'utilisateur des informations à la fois complètes (recherche sur l'ensemble du corpus) et pertinentes (adaptées à la thématique).
Afin de répondre à la difficulté liée au nombre de motifs qui peut être élevé, notre application s'accompagne d'un moteur de recherche permettant de mettre en relief les motifs contenant un ou plusieurs mots spécifiés par l'utilisateur (figure 1). Les résultats d'une recherche peuvent également être triés selon plusieurs critères (support, nombre d'itemsets). Par ailleurs, notre logiciel permet de visualiser les phrases qui valident un motif séquentiel donné.
Conclusion
Nous proposons dans cet article un ensemble de méthodes implantées au sein d'une interface dédiée aux utilisateurs non informaticiens mais experts des données. Une perspective envisageable pourrait exploiter le fait que l'approche présentée s'avère adaptée à une classification des paragraphes du corpus en fonction des thématiques présentes dans ce dernier. 
Summary
This paper introduces a tool to visualize sequential patterns extracted from textual data in Old French.

Résumé. Cet article propose une méthodologie pour la visualisation et la classification des parcours de vie. Plus spécifiquement, nous considérons les parcours de vie d'individus suisses nés durant la première moitié du XXème siècle en utilisant les données provenant de l'enquête biographique rétrospective menée en 2002 par le Panel suisse de ménages. Nous nous sommes concentrés sur ces évé-nements du parcours de vie : le départ du foyer parental, la naissance du premier enfant, le premier mariage et le premier divorce. A partir des données de base sur ces événements, nous discutons de leur transformation en séquences d'états. Nous présentons ensuite notre méthodologie pour extraire de la connaissance des parcours de vie. Cette méthodologie repose sur des distances calculées par un algorithme d'optimal matching. Ces distances sont ensuite utilisées pour la classification des parcours de vie et leur visualisation à l'aide de techniques de « Multi Dimensional Scaling ». Cet article s'intéresse en particulier aux problé-matiques entourant l'application de ces méthodes aux données de parcours de vie.
Introduction
Nous proposons dans ce travail d'étudier et de comparer diverses techniques de visualisation et de classification de parcours de vie 1 . Plus spécifiquement, nous considérons les parcours de vie familiale d'individus suisses nés durant la première moitié du XXème siècle à partir de données récoltées par le Panel suisse de ménages. Les parcours de vie familiale sont composés d'événements constitutifs de la vie familiale, comme le départ du foyer parental, le premier enfant, le premier mariage ou le premier divorce. Il est possible, à partir de ces événements, de considérer des parcours de vie individuels sous la forme de séquences d'états, chaque événement survenant dans la vie de l'individu correspondant à un changement d'état. Une méthodologie ad hoc destinée à créer une typologie des parcours de vie et à visualiser les comportements individuels et l'évolution des normes sociales les régulant est présentée ici. La méthode principale consiste à calculer une distance entre chaque séquence à l'aide d'un algorithme d'optimal matching ; on obtient ainsi une distance qui respecte le caractère temporel des séquences de parcours de vie. Les résultats sont ensuite visualisés à l'aide de méthodes de type « Multi Dimensional Scaling ».
Cet article est construit de la manière suivante. La première partie présente les données utilisées ainsi que les transformations nécessaires pour construire des séquences d'états à partir d'événements. La deuxième partie présente la méthode d'optimal matching, son fonctionnement et la problématique de la définition du coût des opérations. La troisième partie concerne les méthodes de visualisation de type Multi Dimensional Scaling et leur principe de fonctionnement. La quatrième partie présente les résultats de l'application de notre méthodologie aux données du Panel suisse de ménages. Les résultats sont interprétés à l'aide de graphiques et de modèles de régression logistiques. Nous concluons finalement sur les possibilités que nous permettent d'envisager l'application de cette méthodologie aux données en sciences sociales.
Données
A partir des réponses à un questionnaire, nous extrayons des données sous la forme d'un tableau où chaque ligne est un individu et chaque colonne une variable (tableau 1).
ind. naissance départ mariage enfant divorce 1 1974 1992 1994 1996 n/a TAB. 1 -Exemple de données sous la forme d'événements
Le passage à une représentation sous forme de séquences d'états n'est pas trivial. La difficulté consiste à représenter sous la forme d'un unique état une combinaison d'événements qui se sont déjà produits ou non à chaque âge. De manière plus formelle, nous définissons l'état qui définit un individu à un âge précis comme une information sur les événements réalisés. On peut dire, à partir d'un état, quels événements se sont déjà produits. La réalisation d'un ou de plusieurs événement durant une année t entraîne le passage de l'état dans lequel se trouvait l'individu à t ? 1 à un nouvel état. La définition des états à partir des événements est un problème propre au type de données et à la problématique de recherche. Une manière simple de procéder consisterait à créer un état pour chaque combinaison d'événements. Avec cette solution, le nombre d'états s'élèverait à 2 n pour n événements, ce qui rend l'interprétation difficile dès lors qu'on prend en considération beaucoup d'événements. Nous avons donc choisi d'agglomérer certaines combinaisons en accord avec les objectifs de recherche.
Dans le cadre de cette étude, nous avons décidé de retenir quatre événements constitutifs de la vie familiale : le départ du foyer parental, le premier mariage, le premier divorce et la naissance du premier enfant. Le tableau 2 présente le codage des états que nous avons établi par rapport aux quatre événements retenus. Le nombre d'états a été réduit de 16 à 8, notamment en supprimant des états impossibles (tous ceux qui contiennent un divorce sans un mariage préalable) ou en combinant deux états (par exemple l'état 2 concerne les individus mariés qui ne sont pas partis du foyer parental, qu'ils aient eu des enfants ou non  1974 ... 1991 1992 1993 1994 1995 1996 1997 1998  
Méthode
Nous reprenons ici la formulation de Rohwer et Pötter (2002). Prenons ?, l'ensemble des opérations possibles, et a[w] le résultat de l'application des opérations w ? ? sur la séquence a. Nous considérons trois types d'opérations : l'insertion d'un élément, la suppression d'un élé-ment, ou la substitution d'un élément par un autre. Si l'on attribue un coût c(w) qui correspond au coût d'appliquer l'opération w ? ?, la distance entre une séquence a et une séquence b peut être formalisée de la manière suivante :
Autrement dit, pour chaque paire de séquences, on cherche la combinaison d'opérations pour rendre les séquences identiques dont la somme des coûts est la plus petite. L'algorithme utilisé pour trouver cette distance minimale utilise une méthode de programmation dynamique qui est décrite dans (Deonier et al., 2005). L'implémentation de l'algorithme que nous avons utilisée est celle présente dans le logiciel TDA ; son fonctionnement est détaillé dans son manuel d'utilisation (Rohwer et Pötter, 2002).
Définition des coûts
Comme nous l'avons vu précédemment, un coût c peut être attribué aux opérations w ? ?. Les coûts de substitution, auxquels nous nous sommes intéressés en particulier, peuvent être représentés sous la forme d'une matrice symétrique qui définit une valeur pour chaque paire d'état. L'attribution de ces valeurs en se basant sur un modèle théorique est particulièrement difficile dans le cadre d'une utilisation en sciences sociales, ce qui fait l'objet d'un débat (Wu, 2000). Il est en effet délicat de décider du coût du passage d'un état à un autre, mais il est pourtant intéressant et parfois capital de pouvoir différencier ces coûts. Pour cela, deux méthodes disponibles ont été essayées sur notre jeu de données. La première est implémentée dans le logiciel TDA (Rohwer et Pötter, 2002) et définit le coût de chaque substitution en fonction des taux de transition observés dans les données. Le coût du passage d'un état i à un état j est donc calculé de la manière suivante : c i,j = c j,i = 2 ? P (i t |j t?1 ) ? P (j t |i t?1 ). Le coût de base est fixé à 2, et plus la probabilité P (i t |j t?1 ) de passer de l'état i à l'état j, et inversement, est grande, plus ce coût baisse. Ainsi, les substitutions correspondantes aux transitions observées fréquemment seront moins coûteuse que celles qui n'arrivent jamais. Une autre méthode, proposée dans le logiciel T-COFFEE/SALTT (Notredame et al., 2005), consiste à calculer une matrice des coûts de substitution optimale par un processus itératif (Gauthier et al., 2007). Les tableaux 4 et 5 contiennent les résultats de l'application de ces deux méthodes de définition des coûts de substitutions sur nos données. Une analyse visuelle du tableau 4 permet d'observer qu'un passage de l'état 0 (aucun événement) à l'état 7 (divorce) ne s'observe jamais dans nos données, puisque son coût est de 2 dans les coûts tirés des taux de substitution (en gras). Cette transition correspondrait à un individu qui dans l'espace d'une année se marie puis divorce. Le passage de l'état 3 (départ et mariage) à l'état 6 (départ, mariage et enfant) est quant à lui beaucoup plus fréquent, et par conséquent moins coûteux. Le tableau 5 semble cohérent avec les coûts définis, même si la comparaison est difficile en raison de la plus grande variabilité des valeurs.
Le coût des opérations d'insertion et de suppression a quant à lui été fixé à une valeur unique de 3 dans la solution basée sur les taux de transition . Ce choix a pour but de favoriser au maximum les opérations de substitution (qui ont un coup maximum de 2) afin d'éviter les phénomènes de distorsion du temps qu'engendrent les opérations d'insertion. solution, les seules situations où sont utilisées les insertions/suppressions sont en cas de léger décalage (p.ex. 0-1-2-3-4-4 à aligner avec 0-0-1-2-3-4). Dans le cas de la solution basée sur la matrice des coûts optimaux, le coût d'insertion/suppression a été fixé selon les recommandations de Gauthier et al. (2007), c'est-à-dire égal à la moyenne des coûts de substitution. La figure 1 donne une vision graphique de la disparité entre les matrices de distances calculées avec les différentes solutions de coût. Le graphique de gauche confronte les distances calculées avec les coûts de substitution fixés en fonction des taux de transition aux distances calculées avec un coût de substitution fixé à 2. Il apparaît très nettement que les résultats fournis par ces deux solutions sont quasiment identiques ( fig. 1 partie gauche). La comparaison de la solution des taux de substitution avec la solution des coûts optimaux montre une plus grande disparité des distances et un effet d'échelle dû à la plus grande variabilité des coûts optimaux ( fig. 1 partie droite). On peut en conclure qu'avec ce jeu de données, l'utilisation des taux de transition plutôt qu'un coût fixe n'a que peu d'influence sur les distances. En revanche, la différence entre la solution des taux de transition et la solution des coûts optimaux est plus marquée.
Classification
Nous sommes maintenant capables de produire une matrice de distances mesurant les différences entre les parcours de vie des individus. Celle-ci peut être utilisée dans une procédure de classification hiérarchique ascendante selon la méthode de Ward. Le tableau 6 croise les FIG. 1 -La partie gauche présente les distances obtenues par la méthode avec les coûts substitutions basés sur les taux de transition selon les valeurs des distances obtenues avec des coûts de substitution fixés à 2 (le fait que les valeurs soient sur la diagonale indique que les distances obtenues par ces deux distances sont égales). La partie droite présente ces mêmes distances fondées sur les coûts de transition en fonction de celles calculées avec les coûts optimaux. Ces vues sont des graphiques en densité (plus la quantité de points associé à une unité de surface est grande, plus l'unité de surface est foncée), ainsi ces figures restent lisibles malgré la grande quantité de points présentés (environ 2000 2 /2). 
Multi Dimensional Scaling
Avant de procéder à une classification hiérarchique ascendante, la matrice de distances apporte peu d'information aux experts. Ainsi, pour leur permettre d'appréhender les résultats, nous proposons de générer des « cartes » exprimant les relations de proximité entre les parcours des individus. Une telle représentation intuitive des données peut être obtenue par des méthodes de type « Multi Dimensional Scaling ». De cette manière, on dispose d'un outil qui permet de visualiser graphiquement les distances et d'aider à la décision du nombre de groupes à retenir dans une classification hiérarchique.
DD-HDS
Nous constatons que les représentations bidimensionnelles et tridimensionnelles obtenues à partir de ces données par Classical Multi Dimensional Scaling (Torgerson, 1952) sont peu efficaces (résultats non présentés). Nous formulons donc l'hypothèse que l'inefficacité de cette méthode pourrait être due à des relations non linéaires, puisqu'elle fait implicitement appel à des projections linéaires. Dans ce cas, l'utilisation d'une méthode de réduction de dimension non-linéaire est recommandée (on peut citer par exemple dans ce cadre les SOM (Kohonen, 1997), Isomap (Tenenbaum et al., 2000) ou l'analyse en composantes curvilignes (Desmartines et Hérault, 1997). Leur but commun est d'offrir une configuration de points sur un espace de faible dimension qui préserve les distances entre les données (avec un effort particulier pour la conservation des distances courtes). Parmi elles, nous avons choisi DD-HDS (Data-Driven High Dimensional Scaling, (Lespinats et al., 2007b) Nous constatons en effet qu'une représentation sur un espace bidimensionnel permet de rapprocher les individus dont les parcours de vie sont proches. Par exemple on peut observer que les individus divorcés se rassemblent sur la droite de la représentation ( fig. 3). Notons que plus le divorce est précoce, plus l'individu s'écarte vers la droite. La même analyse peut bien sûr être menée pour les 7 états, ce qui permet d'appréhender facilement l'organisation spatiale des individus (données non présentées).
FIG. 3 -Représentation bidimensionnelle des parcours de vie. Le code couleur permet de visualiser l'âge des divorces. Les points noirs de taille réduite correspondent aux individus qui n'ont pas divorcé. Le niveau de gris des autres points exprime l'âge de l'individu au moment du divorce. Plus l'individu est jeune au moment de son divorce, plus le point associé est clair.
Ce type de représentation permet également de visualiser d'autres types d'information. Par exemple, la figure 4 montre la répartition des dates de naissance des individus sur la représen-tation. Ainsi, nous observons que certains comportements ont eu tendance à disparaître comme le fait de rester chez ses parents (en haut au centre) et que des nouveaux comportements apparaissent comme les mariages tardifs (zone sur la gauche de la partie centrale).
FIG. 4 -Organisation des dates de naissance dans la représentation. La représentation est divisée en unité de surface, le niveau de gris de chaque zone dépend de la moyenne des dates de naissance (plus la date moyenne est ancienne, plus l'unité de surface associée est foncée). 
RankVisu
En termes de réduction de dimension, on cherche classiquement à préserver les distances entre données. RankVisu propose un nouveau point de vue sur les données en cherchant à conserver les rangs de voisinages (Lespinats et al., 2007a). Cette méthode renforce les groupes de données et permettra ainsi de valider notre clustering. La représentation obtenue à l'aide de RankVisu est mise en relation avec le résultat d'une classification hiérarchique (critère de Ward).
Notons que ces deux méthodes se basent sur des informations relativement différentes : la classification s'appuie sur les distances tandis que RankVisu utilise les rangs de voisinage entre données. La figure 5 présente la représentation obtenue par RankVisu, en distinguant les groupes identifiés par la classification en cinq classes. Chaque classe forme sur le graphique un groupe bien défini, ce qui renforce le crédit de notre classification ( fig. 5). En effet, les deux méthodes aboutissent à des conclusions comparables.
Interprétations
Nous analysons maintenant les caractéristiques de chacun des groupes. L'interprétation peut se faire de plusieurs manières ; nous privilégions ici une méthode visuelle pour la distinction des groupes. Nous disposons de deux types de graphique pour représenter la forme des séquences individuelles. Le premier type consiste à représenter, pour chaque âge entre 15 et 45 ans, la proportion d'individus se trouvant dans chaque état. La figure fig. 7 donne les représentations pour les groupes 2 et 4. Le deuxième type de graphique représente quant à lui chaque séquence individuelle. Ainsi, on lit sur l'abscisse l'âge de l'individu, et les séquences sont dessinées horizontalement. L'ordre dans lequel les séquences apparaissent est définie par la distance qui les sépare d'une séquence de référence choisie au hasard parmi toutes les sé-quences du groupe ( fig. 8)   16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45   15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  

Introduction
Le Web représente aujourd'hui la principale source d'information. Ce gisement contenant une grande quantité de données non-structurées, distribuées et multi-medias a besoin d'être maintenu, filtré et organisé pour permettre un usage efficace. Cette tâche s'avère difficile à réaliser avec la large distribution, l'ouverture et la forte dynamicité du Web. Par conséquent, plusieurs travaux de recherche ont tenté d'analyser le contenu des sites Web et comprendre le comportement des utilisateurs de ces sites. L'approche que nous proposons dans cet article se situe dans ce cadre. Notre objectif est d'analyser un site Web en se basant sur le contenu et indépendamment de l'usage. En d'autres termes, nous cherchons à réduire la quantité d'information contenue dans le site Web en un groupe de thèmes qui pourraient susciter l'intérêt des internautes. Il sera par la suite possible d'analyser le comportement des utilisateurs vis-à-vis de ces thèmes.
Approche du Web Content Data Mining
Le Web Content Data Mining (WCDM) est l'application des techniques de Data mining au contenu du Web (textes, images, hyperliens…). Il est défini comme étant une analyse textuelle avancée intégrant l'étude des liens hypertextes et la structure sémantique des pages Web. L'approche que nous proposons pour le WCDM est présentée par le schéma suivant.
Pages Web
Typage des pages
Pages hybrides Pages de contenu Pages de navigation Conversion des pages Web en fichiers Textes
Fichiers Textes
Lemmatisation et étiquetage à l'aide de TreeTagger
Prétraitement des textes
Choix des descripteurs : sélection et extraction
Base des attributs
Catégorisation des textes : classification simultanée
Base des descripteurs
Block clustering
Textes catégorisés par thèmes
FIG. 1 -Approche proposée pour le Web Content Data Mining.
RNTI
Cette approche se déroule en trois étapes principales. La première est celle de la classification des pages Web en pages de navigation et pages de contenu. L'objectif de cette étape est de limiter le travail postérieur aux pages de contenu. La deuxième est celle du prétraitement des textes et préparation des données et la dernière est celle de la classification simultanée ou block clustering.
Typage des pages
L'objectif de cette étape est de distinguer les pages qui servent à faciliter la navigation sur le site, appelées pages de navigation ou pages auxiliaires, des pages contenant de l'information qui pourrait intéresser l'internaute. Ces pages sont appelés « pages de contenu ». Certaines pages Web sont à la fois des pages de contenu et des pages de navigation. Il s'agit de pages hybrides.
Description des variables
Le typage des pages est effectué à l'aide d'une classification appliquée aux pages du site caractérisées par un ensemble de variables. Dans (Charrad et al. 2006) les variables utilisées sont : le nombre de liens entrants (inlinks), le nombre de liens sortants (outlinks), la durée moyenne de consultation de chaque page et le nombre de visites à chaque page. Ces variables sont déterminées à partir des fichiers Logs résultant de la navigation sur le site. Comme nous nous intéressons à l'analyse textuelle du site indépendamment de l'usage, et que le site étudié est un site de tourisme (site de Metz) 1 , nous utilisons les variables suivantes: le nombre de liens entrants (inlinks), le nombre de liens sortants (outlinks), la taille du fichier (size), le nombre d'images par page, la présence d'un contenu textuel (texte).
Comme la variable « texte » est binaire, une première classification est effectuée sur les pages du site pour différencier les pages présentant un contenu textuel des pages présentant seulement des liens ou des images. Ainsi, la première hypothèse utilisée est que les pages ne contenant pas du texte sont des pages de navigation. Par contre, les pages présentant un contenu textuel peuvent servir de pages de navigation ou de pages de contenu. La détermination du type de la page est effectuée par la méthode de K-means.
Analyse des résultats
La méthode K-means (l'algorithme de Forgy (1965)) est appliquée aux pages présentant un contenu textuel c.à.d appartenant à la première classe. Les variables utilisées pour caractériser ces pages sont le « Nombre Outlinks », le « Nombre Inlinks », le « Size » et le « Nombre Images ». Les caractéristiques des sous-classes obtenues permettent d'attribuer une étiquette à chacune d'entre elles. En effet, la sous-classe C1 comporte environ 9% de pages caractérisées par la présence d'un nombre élevé de liens entrants et sortants utilisés pour passer d'une page à une autre, une taille importante et un nombre faible d'images. Ces pages sont destinés principalement pour passer d'une page à une autre. Elles correspondent alors à des pages de navigation bien qu'elles présentent un contenu textuel. Par contre, la sous-classe C2 comporte environ 27% de pages caractérisées par un nombre élevé de liens entrants et d'images, en plus du contenu textuel, et un nombre faible de liens sortants. Ainsi, plusieurs pages pointent vers les pages de la sous-classe C2 mais elles pointent vers un nombre faible de pages. Ces pages correspondent à des pages de contenu. La sous-classe C3 présente les caractéristiques de pages de contenu (C2) et de pages de navigation (C3) à la fois, on les considère comme des pages hybrides. En résumé, trois classes sont obtenues à la fin de la classification : les pages auxiliaires résultant de la première et la seconde phase de classification, les pages de contenu et les pages hybrides.   
Variables
Comparaison entre typage manuel et typage par Kmeans
Le typage manuel consiste à analyser manuellement le site Web et attribuer une étiquette à chaque page selon qu'elle soit une page de contenu, une page hybride ou une page de navigation. L'attribution de l'étiquette dépend seulement du motif de la visite que pourrait faire l'internaute au site. En effet, une page de contenu pour un visiteur peut être considérée comme étant une page de navigation pour un autre. Le typage manuel du site de Metz a montré que 12% des pages du site sont considérées comme des pages de navigation, 11% comme des pages de contenu et 77% des pages sont des pages hybrides. Nous remarquons que les résultats de la classification sont proches des résultats du typage manuel. Ces résultats montrent que le site de Metz présente peu de pages auxiliaires et de pages de contenu. La majorité de pages sont de type hybride.
Prétraitement des textes
L'objectif du prétraitement est de représenter chaque page du site par un vecteur de descripteurs qui donne une idée sur son contenu. Le prétraitement est réalisé en deux étapes. La première étape est celle de représentation des documents. La seconde est celle du choix des descripteurs.
Représentation des textes
Plusieurs méthodes ont été proposées dans la littérature pour la représentation des textes. La méthode la plus utilisée est la représentation vectorielle, appelée «bag-of-words » ou « sac de mots » dans laquelle chaque texte est représenté par un vecteur de n termes pondérés (Salton et Buckley, 1998). À la base, les n termes sont les n différents mots apparaissant dans les textes de l'ensemble d'entraînement. Scott et Matwin (1999) ont fait des nombreux essais pour représenter les textes pour des fins de classification. Ils ont utilisé les groupes nominaux (des suites de noms et d'adjectifs) pour construire les termes de l'espace vectoriel et une application pour l'analyse de la nature grammaticale des mots du texte. Ils ont aussi évalué l'impact de regrouper les mots synonymes en un même méta-descripteur. La notion d'hyperonymes a été aussi mise à l'épreuve pour regrouper des mots. Aucune de ces méthodes n'a produit de résultats équivalents ou supérieurs à l'approche «bag-of-words». Lewis (1992) a également représenté les textes à l'aide de groupes nominaux mais les résultats n'étaient pas satisfaisants.
En adoptant l'approche « bag of words », le poids associé à chaque terme peut être une valeur binaire, indiquant la présence ou l'absence du terme dans le document (1 si le mot est présent dans le texte, 0 sinon), ou un entier positif représentant le nombre d'occurrences du terme dans le document. L'inconvénient d'introduire ce comptage est qu'il accorde un poids important aux termes qui apparaissent très souvent à travers toutes les classes des documents et qui sont peu représentatifs d'une classe en particulier. Une autre méthode largement utilisée pour calculer le poids d'un terme est la fonction TFIDF (acronyme de Term Frequency Inverse Document Frequency») (Salton et Buckley, 1998).
Choix des descripteurs
L'inconvénient de l'utilisation directe du vocabulaire contenu dans les textes d'entraînement est la dimension très élevée de l'espace vectoriel. L'utilisation de tous les mots contenus dans les textes peut influencer négativement la précision de la classification. D'autre part, un mot présent dans un nombre élevé de textes ne permet pas de décider de l'appartenance d'un texte qui le contient à l'une ou l'autre des catégories car son pouvoir de discrimination est faible.
Pour pallier ces problèmes, certaines techniques de réduction de la dimension du vocabulaire ont été mises en place. Ces techniques se divisent en deux grandes familles.
-Les techniques basées sur la sélection de descripteurs («feature selection») : Ces techniques conservent seulement les descripteurs jugés utiles à la classification, selon une certaine fonction d'évaluation. L'avantage de la sélection de descripteurs consiste à éliminer les descripteurs réellement inutiles ou les descripteurs erronés («noisy») (mots mal orthographiés par exemple). Plusieurs techniques de sélection de descripteurs ont été développées en vue de réduire la dimension de l'espace vectoriel. Chacune de ces techniques utilise des critères lui permettant de rejeter les descripteurs jugés inutiles à la tâche de classification. Le processus de sélection de descripteurs débute généralement par la suppression de mots très fréquents tels que les mots grammaticaux ou les mots de liaisons. La recherche de radical («stemming») et la lemmatisation sont d'autres méthodes utilisées pour créer un vocabulaire réduit. Leur but est de regrouper en un seul descripteur les multiples formes morphologiques des mots qui ont une sémantique commune. Les mots très peu fréquents, qui n'apparaissent qu'une ou deux fois dans l'ensemble de documents, sont également supprimés, car il n'est pas possible de construire des statistiques fiables à partir d'une ou deux occurrences (Stricker, 2000). -Les techniques basées sur l'extraction de descripteurs («feature extraction») : Ces techniques créent des nouveaux descripteurs à partir des descripteurs de départ, en faisant des regroupements ou des transformations afin de réduire le nombre de descripteurs redondants. Le processus d'extraction de descripteurs consiste à créer à partir des descripteurs originaux un ensemble de descripteurs synthétiques en effectuant des regroupements de termes « term clustering » ayant une sémantique commune (Stricker, 2000). Les classes obtenues deviennent les descripteurs d'un nouvel espace vectoriel. Blum et Mitchell (1998) rapportent des résultats intéressants à propos de ce regroupement.
Résultats du prétraitement
Dans notre cas, le prétraitement nécessite tout d'abord la conversion des pages Web en fichiers Textes, et le remplacement des images qu'ils contiennent par leurs légendes. Ces textes sont par la suite traités par l'algorithme TreeTagger 2 qui a été développé à l'Institut de Linguistique Computationelle de l'Université de Stuttgart (Schmid, 1994).
L'étiquetage et la lemmatisation à l'aide de TreeTagger permettent de remplacer les verbes par leur forme infinitive, les noms par leur forme au singulier et certaines formes des verbes tels que les participes présents et les participes passés par leurs racines. Afin de réduire la dimension de l'espace vectoriel des vecteurs représentant les textes, il s'avère nécessaire de supprimer : -Les formes de ponctuation, -Les mots vides tels que les prépositions, les déterminants, les numéros, les conjonctions, les pronoms et les abréviations, -les mots inutiles à la classification tels que les adverbes et les adjectifs, -Les termes de type non reconnu par TreeTagger sont examinés manuellement afin de ne garder que les noms et les verbes. D'autre part, les termes auxquels TreeTagger attribue l'étiquette « Nom » sont examinés afin de supprimer les noms propres que TreeTagger n'arrive pas à identifier. Ainsi, seuls les noms et les verbes sont conservés dans la base des descripteurs. -Les mots très fréquents : Nous avons adopté la méthode proposée par Stricker (2000). En effet, le rapport )
, tel que TF(m,t) est l'occurrence du mot m dans un texte t et CF(m) est l'occurrence du mot m dans l'ensemble des documents, permet de classer les mots par ordre décroissant. Plus le mot m est fréquent, plus le ratio est faible et, inversement, plus un mot est rare, plus le ratio est élevé. Dans le cas limite où un mot n'apparaît qu'une seule fois dans l'ensemble de documents, ce ratio vaut 1 et le mot est classé en tête de liste.
-Les mots très peu fréquents : ce sont les mots -dont le nombre de documents dans lesquels ils apparaissent est inférieur à un certain seuil. Dans notre cas, nous supprimons les mots qui apparaissent dans une seule page du site Web. Ceci réduit la base des descripteurs à 652 descripteurs au lieu de 1500. -dont le nombre d'occurrences dans la base est égal à 1 i.e les mots qui apparaissent une seule fois dans toute la base. Le prétraitement des textes aboutit à la construction d'une matrice croisant 418 descripteurs à 125 pages avec le nombre d'occurrences du descripteur dans une page du site comme poids. Les méthodes de classification automatique appliquées à des tableaux mettant en jeu deux ensembles de données agissent de façon dissymétrique et privilégient un des deux ensembles en ne faisant porter la structure recherchée que sur un seul ensemble. Ainsi, la détermination des liens entre les deux partitions est difficile. La recherche simultanée de partitions sur les deux ensembles a donné naissance à des méthodes de classification simultanée (block clustering) telles que les méthodes de classification directe de Hartigan (1972de Hartigan ( , 1975, les méthodes de classification croisée de Govaert (1983) et les méthodes de biclustering utilisée généralement en bioinformatique. Ces méthodes de classification simultanée fournissent des blocs homogènes à partir d'une partition des instances et une partition des attributs recherchées simultanément.
Application de Croki2
Comme notre tableau des données est un tableau de contingence, nous avons appliqué l'algorithme CROKI2 (classification CROisée optimisant le Khi2 du tableau de contingence) proposé par Govaert (1983) pour les tableaux de contingence. L'objectif de cet algorithme est de trouver une partition P de I en K classes et une partition Q de J en L classes telle que 
marginales définies sur le tableau T1 s'écrivent :
L'algorithme CROKI2 consiste à déterminer une série de couples de partitions (P n ,Q n ) optimisant le 2 ? du tableau de contingence en appliquant alternativement sur I et sur J une variante de la méthode des nuées dynamiques. Les entrées de l'algorithme sont : le tableau de contingence, le nombre de classes en ligne et en colonne et le nombre de tirages de départ. Les sorties de l'algorithme sont : la valeur du Khi2 du tableau initial, la valeur du Khi2 du tableau (P,Q), le pourcentage d'inertie (ou d'information) conservée et le tableau de contingence initial réordonné en ligne et en colonne suivant les classes de deux partitions.
Analyse des résultats
L'application de l'algorithme CROKI2 du logiciel SICLA permet d'obtenir la nouvelle matrice suivante (  Comme aucun critère n'est proposé pour choisir le nombre de classes en lignes et en colonnes dans l'algorithme CROKI2, nous utilisons le coefficient T de Tschuprow 
TAB. 3 -Informations sur les biclasses.
L'examen de ces biclasses a pour objectif d'attribuer un thème à chaque groupe de pages. Par exemple, la biclasse (2, 3) composée de la classe 2 de descripteurs et la classe 3 des pages a pour thème « spécialités de cuisine » sachant que presque tous les descripteurs sont en relation avec l'alimentation (amande, crème, eau de vie, flamber, fruit, glacer, mirabelle, recette, purée…etc). Les biclasses (8,4) et (8,5) présentent le même thème « hébergement » puisque il s'agit de la même classe en ligne. Par conséquent, il est possible de regrouper les pages de la classe 4 et de la classe 5 dans une même classe (4+5). La nouvelle biclasse obtenue après fusion des biclasses (8,4) et (8,5) a pour thème « informations sur les hôtels ou hébergement». Par contre, les biclasses (6,7) et (7,7) présentent en commun la même classe en colonne. Ainsi, les pages de la classe 7 auront deux thèmes différents « horaires et tarifs des lieux à visiter » et « réservation ».
FIG. 4 -Exemple de biclasses.
Le tableau suivant présente les meilleures biclasses et les thèmes qui leur sont associés. 
Analyse factorielle des correspondances
Dans cette section, nous appliquons au tableau de contingence l'analyse factorielle des correspondances. La projection des pages et des descripteurs sur le même plan factoriel (figure 5) permet d'associer des groupes de pages à des groupes de descripteurs. A titre d'exemple, Le nuage de points situé à gauche est composé des descripteurs (points bleus) appartenant à la classe 8 de descripteurs et des pages (triangles rouges) appartenant aux classes 4 et 5 de pages. Ce nuage de points correspond aux deux biclasses (8,4) et (8,5) déterminées à l'aide de CROKI2. Le nuage de points en bas comporte les descripteurs appartenant à la classe 5 de descripteurs et les pages appartenant à la classe 8 de pages. Ce nuage correspond à la biclasse (5,8). L'ensemble des points au centre est composé des descripteurs appartenant à la classe 1 et la classe 3 et des pages appartenant à la classe 1 et la RNTI classe 2 de pages. Ce nuage correspond aux deux biclasses (1,1) et (3,2). Le nuage situé en haut est un mélange de descripteurs et de pages appartenant aux biclasses (6,7), (7,7), (4,6) et (2,3). Ainsi, on trouve dans les résultats de l'analyse factorielle des correspondances certains résultats obtenus dans la section précédente.
Projection des pages (a)
Projection des descripteurs (b)
Projection des pages et des descripteurs ©
FIG. 5-Projection des descripteurs et des pages sur les axes factoriels.
Conclusion
Dans cet article nous avons proposé une approche d'analyse du contenu textuel d'un site Web basée sur la catégorisation simultanée des pages et des descripteurs de pages et indépendamment de l'usage du site. Les thèmes découverts vont servir par la suite à l'analyse du site du point de vue de ses usagers. Les résultats obtenus par cette approche prouvent son applicabilité sur des sites Web non volumineux.

Introduction
Avec l'émergence du web sémantique, l'exploitation des ressources sémantiques pour annoter des documents, personnaliser des services ou décrire des ressources disponibles sur le web est devenue essentielle. Créées par des communautés distinctes, existant déjà en grand nombre, les ressources sémantiques se différencient par des niveaux différents de formalisation et de conceptualisation engendrant un certain degré d'hétérogénéité. Ainsi, l'utilisation conjointe des éléments (documents, services web) décrits par des ressources distinctes est soumise à leur mise en correspondance. Un nouveau problème émerge, qui concerne la mise en correspondance des ressources sémantiques. Parmi les techniques proposées pour apporter des solutions à ce problème, l'alignement sera considéré dans ce papier. L'alignement établit des appariements entre les entités appartenant à deux ressources distinctes. Au-delà d'une certaine complexité, taille, ou nombre de ressources il est impossible d'établir manuellement ces appariements. La nécessite des méthodes automatiques (ou semi automatiques) pour l'alignement des ressources sémantiques est évidente.
Dans le cadre de ce travail, une ressource sémantique représente un modèle de connaissance spécifique à un domaine. Elle est constituée de concepts, qui modélisent les objets spécifiques au domaine et de rôles, qui correspondent à des relations entre ces objets. Les concepts et les rôles peuvent être structurés dans une hiérarchie. L'alignement est réalisé automatiquement et met en correspondance des entités ayant la même nature. Ainsi, soient S s et S c deux ressources qui décrivent le même domaine. L'alignement associe à chaque entité de e s de S s une entité e c appartenant à S c , si et seulement si les deux entités ont la même nature et modélisent des objets ou des relations qui sont similaires, voir identiques. Un ensemble de règles d'appariement est utilisé pour établir ces correspondances. Les règles sont déduites empiriquement et interviennent à plusieurs niveaux : formel, conceptuel, etc.. Des algorithmes sont ensuite implémentés qui combinent ces règles afin de mettre en oeuvre l'alignement.
Le papier est structuré en trois parties : la première définit l'alignement et introduit l'ensemble des règles d'appariement utilisées. La deuxième présente l'alignement de deux ressources du domaine de l'accidentologie et met en évidence les problèmes spécifiques à cette étude de cas. Des travaux connexes sont présentés dans la troisième partie.
Alignement de ressources sémantiques
Nous avons proposé une approche pour aligner deux ressources d'un même domaine qui est fondée sur le cadre proposé par Ehrig et Sure (2004). Ce cadre définit des règles empiriques pour estimer le degré de ressemblance entre les entités (concepts ou rôles) des deux ressources. Il a été choisi car : les règles définies prennent en compte des éléments situés à différents niveaux conceptuels ; il n'existe pas de contrainte concernant la modélisation et la formalisation des ressources ; il est possible d'enrichir le cadre initial en ajoutant de nouvelles règles d'appariement. Par la suite on introduit l'alignement des ressources, tel qu'il est défini pour ce travail, le cadre proposé par Ehrig et Sure (2004) ainsi que les règles d'appariement ajoutées pour l'enrichir.
Définition
L'alignement des ressources sémantiques peut être défini formellement comme suit : soient S s (source) et S c (cible) deux ressources sémantiques du même domaine ; E Ss , E S c les ensembles d'entités (concepts et rôles), appartenant à la ressource S s , respectivement S c . L'alignement s'exprime par une fonction : 
Dans le cadre de ce travail, l'alignement associe une seule entité de S c à une entité de e s j . Ceci représente un choix de modélisation, déterminé par l'utilisation ultérieure des résultats issus de cet alignement.
Règles pour l'alignement des ressources sémantiques
Des règles empiriques pour supporter l'alignement des ressources sémantiques ont été identifiées par Ehrig et Sure (2004). Les règles définies estiment la similarité entre deux entités en prenant en compte des éléments qui se trouvent à différents niveaux conceptuels. Ces niveaux sont repartis sur une échelle, présentée dans la fig. 1. Ehrig et Sure (2004) Les éléments situés sur chaque niveau ainsi que les règles définies à partir de ces éléments sont présentés infra.
FIG. 1 -Niveaux conceptuels, d'après
Au niveau des entités Les entités constituent le premier niveau de l'échelle. Elles repré-sentent les concepts et les rôles modélisés dans la ressource. Les entités s'identifient à l'aide des termes (étiquettes) qui sont attribués par les humains lors de la construction de la ressource. L'approche d'alignement étant proposée pour des ressources décrivant le même domaine, une première règle peut être définie. Elle s'énonce : Le niveau des logiques de description Le troisième niveau concerne les ressources exprimées dans le formalisme des logiques de description, voir Baader et al. (2003). Dans ce cas, les concepts sont structurés dans une hiérarchie, dont la racine est un concept générique, le topConcept. Les liens hiérarchiques correspondent à des relations de généralisation/spécialisation entre concepts. Par conséquent, tout concept, excepté la racine, a des concepts pères (ascendants) et des concepts fils (descendants). Un concept de l'ontologie est plus spécifique que ses concepts ascendants et plus générique que ses concepts descendants. Les règles qui peuvent être définies à ce niveau sont : Le niveau des axiomes Si des relations entre des entités ont été représentées sous la forme d'axiomes, ces relations peuvent être utilisées pour estimer le degré de ressemblance entre les entités. Néanmoins, dans la pratique, de telles modélisations sont presque inexistantes.
Nous avons proposé une approche d'alignement qui enrichit l'ensemble de règles définies par ce cadre. Deux règles ont été ajoutées, qui seront appelées transversales, car elles font ap-pel à des éléments qui se trouvent à deux niveaux conceptuels. Ces règles s'énoncent comme suit : R 8 : tout concept de la ressource S s qui n'a pas été assigné par la règle R 1 à un concept de la ressource S c sera assigné au concept auquel son concept père a été assigné par la règle R 1 . Si les rôles de la ressource sont organisés dans une taxinomie, on déduit :
R 9 : tout rôle de la ressource S s qui n'a pas été assigné par la règle R 1 à un rôle de la ressource S c sera assigné au rôle auquel son rôle père a été assigné par la règle R 1 . Ces règles prennent en compte à la fois les étiquettes des entités, car elles exploitent les ré-sultats fournis par la règle R 1 et la structuration hiérarchique des entités. Elles sont utilisées si la règle R 1 ne réussit pas à associer à un concept (rôle) de S s un concept (rôle) similaire, voire identique, appartenant à S c . Dans ce cas, les règles transversales essayent d'assigner un concept (rôle) de S s à un concept (rôle) de S c qui le généralise, en utilisant le concept (rôle) auquel son concept père (rôle père) a été assigné.
3 Aligner des ressources sémantiques de l'accidentologie L'approche proposée a été employée pour aligner une ontologie et une ressource terminoontologique (RTO) de l'accidentologie. L'ontologie de l'accidentologie modélise des connaissances expertes. La RTO a été construite à partir d'un corpus constitué de procès verbaux (PV) d'accidents de la route. Les deux ressources modélisent des connaissances propres à des communautés distinctes et ont été construites par des approches différentes. Ce paragraphe présente les ressources utilisées, la manière dont l'approche proposée a été mise en oeuvre, les résultats obtenus et l'évaluation de ces résultats.
3.1 Ressources sémantiques de l'accidentologie L'ontologie de l'accidentologie, voir Desprès (2002) modélise les connaissances expertes du domaine. Elle a été construite ex nihilo (from scratch en anglais), et est fondée sur des entretiens réalisés avec des chercheurs en sécurité routière et en utilisant comme principale ressource textuelle les scénarios d'accidents. L'éditeur Protégé, Noy et al. (2000) a été utilisé pour construire cette ontologie et OWL est le langage de représentation choisi. Les connaissances sont modélisées selon un point de vue systémique, les principaux concepts du domaine (l'Humain, le Véhicule et l'Environnement) étant mis en évidence ainsi que les relations qui les lient. Les concepts sont dénommés par des termes du domaine, (conducteur, piéton). Des attributs sont définit pour chaque concept (le concept Humain a l'attribut âge), qui sont implémentés à l'aide du type DataTypeProperty. Les relations entre concepts sont modélisées par des rôles à partir des verbes du domaine. Les rôles sont implémentés à l'aide du type ObjectProperty et sont organisés hiérarchiquement.
La ressource termino-ontologique, voir Ceausu et Desprès (2005), a été construite à partir de procès verbaux (PV) d'accidents de la route rédigés par les gendarmes ou les policiers. Elle décrit les accidents de la route en mettant en évidence les particularités du vocabulaire employé par les forces d'ordre pour décrire les accidents de la route. L'éditeur Terminae, Aussenac-Gilles et al. (2002) qui offre des facilités pour la construction des ressources sé-mantiques à partir de textes et leur gestion a été choisi pour structurer les connaissances. Un modèle du domaine a été élaboré utilisant deux types d'entités : les concepts et les rôles. Les concepts sont structurés dans une hiérarchie. Un concept est dénommé par un terme du domaine, et il ne possède pas d'attributs. Les concepts sont liés par des rôles créés à partir de verbes du domaine. La RTO est représentée en OWL. Le nombre de concepts et de rôles de la RTO construite à partir des PV est plus important que celui de l'ontologie. L'explication tient au fait que la RTO est construite à partir de textes rédi-gés en langage courant par des communautés de personne différentes dont l'objectif n'est pas la rigueur dans la présentation mais la description de l'accident dans lequel ils sont impliqués. Tandis que les chercheurs utilisent un langage de spécialité et se contraignent à la concision pour écrire leurs textes.
Sélection des règles pour aligner les ressources
Pour aligner les deux ressources, des règles d'appariement introduites dans le paragraphe 2.2 sont utilisées. L'alignement réalisé a comme ressource source la RTO construite à partir de PV et comme ressource cible l'ontologie de l'accidentologie. Chaque concept (respectivement rôle) appartenant à la RTO est assigné à un concept (respectivement rôle) de l'ontologie. Orienté de cette manière, l'alignement permet de mettre en évidence la manière dont les connaissances expertes modélisées dans l'ontologie de l'accidentologie sont exprimées dans le langage commun employé par les forces d'ordre. Le choix des règles utilisées est guidé par les particularités des deux ressources et par le sens de l'alignement. Les deux ressources modélisent des entités (concept et rôles) du domaine, par conséquent il est possible d'utiliser les règles définies au premier niveau. Ces règles prennent en compte les étiquettes des entités pour estimer leur degré de ressemblance. Les deux ressources adoptent le formalisme des logiques de description et sont représentées en OWL. Cependant, les règles définies au niveaux 2 (réseau sémantique) ne sont pas utilisées, car utilisées conjointement elles engendrent des calculs circulaires. Les rôles modélisés par la RTO ne sont pas structurés hiérarchiquement, par conséquent les règles définies au niveau de la logique de description ne sont pas appliquées. Les règles des deux derniers niveaux sont ignorées, car les ressources ne font pas appel à des directives du langage OWL tels que OWL :sameClassAs et elles ne contient pas d'axiomes. Le nombre d'entités modélisées par la RTO étant plus important que le nombre d'entités de l'ontologie, les règles transversales (R 8 et R 9 ) sont choisies pour mettre en oeuvre l'alignement. L'ensemble des règles utilisé pour aligner les deux ressources est constitué de R 1 , R 8 et R 9 .Ces règles sont utilisées par deux algorithmes développés pour aligner les concepts, respectivement les rôles des deux ressources, qui sont présentés dans le paragraphe suivant.
Aligner les concepts des ressources L'alignement des concepts est fondé sur la règle R 1 et R 8 . L'application de la première (R 1 ) identifie pour chaque concept de la RTO un concept de l'ontologie ayant la même étiquette. Un coefficient égal à 1 sera assigné à chaque couple de concepts. La règle R 8 s'applique successivement en prenant en compte les assignations ainsi obtenues. Chaque application de cette règle entraîne une diminution de la valeur du coefficient qui caractérise le degré de similarité des entités. Un concept modélisé dans la RTO sera assigné à un concept de l'ontologie d'autant plus général que le nombre d'applications de la règle R 8 est important. L'alignement est terminé si tout concept de la RTO est assigné à un concept de l'ontologie. Un extrait des résultats obtenus en alignant les concepts est présenté dans le tab. 3.
Aligner les rôles des ressources Les rôles sont structurés hiérarchiquement dans l'ontologie de l'accidentologie, mais ils se trouvent au même niveau dans la RTO. Par conséquent, les règles d'alignement fondées sur la hiérarchie ne peuvent pas être appliquées. Pour pallier cet inconvénient, la règle R 1 a été adaptée. Ainsi, elle est implémentée en utilisant comme mesure de similarité lexicale la mesure de Monge-Elkan, voir par exemple Ceausu et Desprès (2006). Cette mesure fait des comparaisons récursives au niveau des sous-chaînes et fournit la valeur maximale si la chaîne s 1 est une sous-chaîne de la chaîne s 2 . La règle devient : les rôles dénommés par des étiquettes dont la similarité (calculée par le coefficient MongeElkan) est supérieure à une valeur seuil donnée seront considérés similaires (voir identiques). Les valeurs calculées par Monge-Elkan représentent les coefficients caractérisant le degré de ressemblance des rôles. Le tab. 4 montre un extrait des résultats obtenus en alignant les rôles des deux ressources. 
Concept
Evaluation des résultats
Les résultats de l'alignement de chaque type d'entité sont évalués indépendamment. Cette évaluation distincte est nécessaire car les concepts et les rôles sont structurés différemment et les algorithmes utilisés pour les aligner sont distincts. Les résultats de l'alignement ne peuvent pas être évalués globalement car ils sont issus de deux approches différentes. L'évaluation réalisée concerne seulement l'étude de cas présentée, car la qualité des résultats de l'alignement est influencée par la complexité des ressources alignées. Le scénario d'éva-luation proposé compare, pour chaque type d'entité, les résultats fournis par l'alignement des ressources avec des résultats obtenus en établissant, manuellement, des correspondances entre des entités. L'évaluation fait appel à des mesures proposées dans le domaine de la recherche de l'information, qui reposent sur les notions classiques de Rappel et de P recision. Ces mesures ont été adaptées au nouveau contexte de l'alignement des ressources sémantiques. Ainsi, on peut définir le Rappel comme suit :
où N oCorrects représente le nombre d'alignements corrects et N oRef représente le nombre d'alignements de référence. La P recision est définie par :
où N oCorrects représente le nombre d'alignements corrects et N oRef représente le nombre d'alignements proposés. L'évaluation des résultats peut aussi être exprimée en terme de Bruit ou Silence, comme suit :
F-measure est une mesure d'efficacité globale qui combine Precision et Rappel en une mesure unique donnée par :
On suppose qu'un alignement de référence a été établit manuellement, et que c'est par rapport à cet alignement que seront évalués les résultats obtenus. La comparaison par rapport à l'alignement de référence sera effectuée en comparant les couples de concepts, respectivement rôles, engendrés par l'alignement. La valeur du coefficient qui exprime le degré de ressemblance est ignorée lors de l'évaluation. Ainsi, un alignement (e s , e c , coef f icient) sera considéré correct si, dans le set de référence il existe un couple (e s , e c ), quelque soit la valeur du coefficient coef f icient. Les valeurs obtenues sont présentées dans le tab. 
TAB. 5 -Evaluation des résultats
peut observer que l'algorithme utilisé pour aligner les concepts est plus performant. Cela s'explique par la structuration hiérarchique des concepts dans les deux ressources, qui fait possible l'utilisation de la règle transversale R 8 . Par conséquent, l'alignement des concepts est réalisé par un algorithme combinant deux règles d'appariement. Malgré l'adaptation de la règle R 1 (qui utilise une mesure lexicale particulière pour apparier des rôles), l'algorithme proposé pour aligner les rôles est moins performant. D'un point de vue pratique, un nombre important de rôles de la ressource source (la RTO construite à partir de PV) sont assignés au rôle générique de la ressource cible (l'ontologie de l'accidentologie).
Travaux connexes
Un recueil des méthodes et des outils permettant la mise en correspondance de sémantiques est réalisé dans : Kalfoglou et Schorlemmer (2003) et Euzenat (2004. Des comparaisons entre les outils sont présentées dans Do et al. (2002), Rahm et Bernstein (2001). Parmi les outils proposés, on retrouve :
Anchor Prompt , Fridman Noy et Musen (2001) est un outil permettant l'alignement et l'intégration des ontologies. Il reçoit en entrée deux ontologies et une liste de paires de termes. Les paires de termes sont fournies par l'utilisateur ou identifiées en utilisant des métriques lexicales. Un processus semi-automatique permet d'identifier des concepts qui sont similaires en utilisant ces paires de termes, les structures des ontologies et les choix de l'utilisateur.
Chimaera , McGuinness et al. (2000) est un outil qui permet d'aligner des ontologies de grande taille. Un algorithme engendre des paires de concepts similaires en comparant : les termes utilisés pour designer les concepts, les définitions des concepts, et, selon le cas, les acronymes ou les expansions de ces noms. Chimaera est aussi capable d'identifier des concepts qui sont corrélés par d'autres types de relations, telles que la subsomption, ou des termes qui sont disjoints.
Cupid, Madhavan et al. (2000) est un système qui implémente un algorithme d'alignement fondé sur les similarités lexicales et structurelles. Des coefficients de similarité sont calculés et trois étapes sont exécutées pour générer des paires de concepts similaires. Une première étape calcule des similarités au niveau lexical en utilisant les noms des entités, des mesures de similarité lexicale et en faisant appel à un thesaurus ; la deuxième étape estime la similarité d'un point de vue structurel, en considérant les contextes d'apparition des concepts dans les ontologies. La dernière phase engendre les paires des concepts similaires, en choisissant, parmi les paires générées, celles ayant un coefficient de similarité supérieur à une valeur seuil donnée.
Asco est un système développé à INRIA Sophia Antipolis, qui peut identifier des correspondances entre : deux concepts appartenant à deux ontologies distinctes ; deux relations modéli-sées dans deux ontologies distinctes ; un concept et une relation appartennant à deux ontologies distinctes. Asco implémente un algorithme qui met en correspondance les entités en exploitant le maximum d'éléments disponibles : les noms des entités ; les structures des ontologies ; les structures des entités,des concepts ou des rôles. Cet algorithme est implémenté en Java et est fondé sur le moteur de recherche sémantique CORESE, décrit dans Corby et Faron (2002).
Des méthodes ont été proposées qui estiment la similarité en prenant en compte les instances des concepts, voir par exemple les systèmes Glue, Doan et al. (2002) et FCA Merge, Stumme et Maedche (2002) ou les axiomes présentes dans une ontologie, voir Furst (2002).
L'approche que nous avons proposée pour aligner deux ressources sémantiques est fondée sur les travaux de Ehrig et Sure (2004). Ce choix a été guidé par les particularités des ressources à aligner qui sont constituées de concepts et de rôles. Elles ne contiennent pas d'axiomes ou des individus, par conséquent les méthodes faisant appel à ces éléments ne peuvent pas être appliquées. L'alignement doit mettre en évidence des similarités entre les concepts et les rôles des ressources. Des outils tels que Anchor Prompt, Chimaera ou Cupid s'avèrent inappropriés, car ils établissent des correspondances seulement entre les concepts des ressources. Asco est un outil récent qui n'a pas pu être considéré dans le cadre de ce travail. L'approche proposée par Ehrig et Sure (2004) à l'avantage de mettre en correspondance les concepts et les rôles appartenant à deux ressources. Nous avons adaptée cette approche en enrichissant l'ensemble de règles d'appariement utilisées.
Conclusion
Ce papier présente une approche pour aligner deux ressources sémantiques. L'alignement est fondé sur des règles d'appariement entre les entités des deux ressources et se traduit par des correspondances entre ces entités. Une première étude a été réalisée qui emploie cette approche pour aligner une ontologie et une ressource termino-ontologique de l'accidentologie. L'évaluation des résultats issus de cette expérimentation montre qu'elle est sensible à la manière dont les entités sont modélisées. En perspective, la méthode d'alignement des ressources peut être améliorée en enrichissant l'ensemble de règles utilisées. De nouveaux algorithmes faisant appel à ces règles peuvent être proposés. Une méthode d'évaluation prenant en compte à la fois les couples d'entités obtenus ainsi que les coefficient caractérisant leur degré de similarité est également envisageable.
Références
Aussenac-Gilles, N., B. Biébow, et S. Szulman (2002) 

Introduction
La fouille archéologique est un processus technique visant à recueillir toutes les informations pertinentes sur les manifestations présentes dans un site archéologique [1]. Le processus de fouille d'un site archéologique passe par les étapes suivantes 
Annotation XML des objets archéologiques
Les données archéologiques sont décrites en utilisant les informations recueillies et enregistrées. Les informations de description concernent : les aspects matériaux, le contexte de fouille et la sémantique des oeuvres (c.-à-d. ce que les objets représentent). Un standard de description appelé "CIDOC-ICOM" est développé par le groupe de travail CIDOC-IDOC [3] sur les sites archéologiques. Ce dernier définit les catégories minimales d'informations à enregistrer sur des objets archéologiques afin d'en faciliter la recherche dans un cadre international. Notre modélisation des oeuvres archéologiques s'appuie sur ce standard avec une structure XML qui permette de générer des associations de façon dynamique.
L'architecture générale de notre application «musée virtuel TARCHNA» est illustrée dans la Figure 1. Les différents composants qui constituent notre application sont : le moteur de présentation, le gestionnaire de profiles utilisateurs et le processeur de sémantique.
• Le moteur de présentation ("Presentation Engine") : il gère la présentation. Son but est de supporter un maximum de technologies clients : support des différents navigateurs, type d'interfaces tout en assurant l'adaptation des structures de données renvoyées au client.  
Summary:
In this paper, we propose an annotation framework and a navigation tool of archaeological data. Our goal is to structure the annotation to allow incremental navigation. By using the answer set of a query, users can discover approximate links with other objects in the database. This approach has been implemented and is in the process of validation.

Introduction
Dans le monde scientifique, de nombreuses données sont produites en continu : il est difficile de se maintenir à jour avec le flot d'informations, et de synthétiser les données venant de sources diverses au moment où on en a besoin. Notre but est la construction d'un entrepôt de données XML sur un domaine d'application précis, où différentes données collectées sur le Web seront annotées avec une ontologie du domaine, de manière à être facilement interrogeables. Notre travail se concentre sur l'annotation des tableaux de données, qui sont un moyen de présenter l'information de façon synthétique, très utilisé dans les domaines scientifiques et économiques.
La structure des tableaux de données que l'on trouve dans les rapports et publications scientifiques collectés sur le Web est très hétérogène : elle varie d'un auteur à l'autre, et on observe même souvent différentes formes de tableaux dans un même article scientifique. De plus, le fait que l'on s'intéresse à des tableaux nous prive de l'utilisation d'un contexte linguistique : les techniques de wrapper induction basées sur la structure (Baumgartner et al., 2001) ou le contexte linguistique (Freitag et Kushmerick, 2000) ne sont donc pas adaptées à notre problème d'annotation. Notre but est de construire un outil d'annotation sans phase d'apprentissage, reposant uniquement sur une ontologie. Nous ne cherchons pas, comme présenté par Pivk et al. (2004), à découvrir des relations à partir de tableaux de données et d'outils linguis-tiques généraux tels que WordNet et GoogleSets, mais nous voulons au contraire reconnaître des relations prédéfinies dans une ontologie spécifique au domaine d'application.
Notre approche utilise les idées développées par Gagliardi et al. (2005) concernant l'annotation de tableaux guidée par une ontologie, sur la base d'égalités de mots entre les termes de l'ontologie et ceux du Web. Cependant, nous allons plus loin dans le sens où nous distinguons deux méthodes de traitement selon que les données sont numériques ou symboliques, et que nous proposons une annotation floue pour les données symboliques.
La section 2 décrit l'ontologie, élément central de notre système. Nous présentons ensuite notre système d'annotation dans l'ordre d'application des différentes étapes : distinction entre données numériques et symboliques en section 3, annotation des données symboliques en section 4 et annotation des données numériques en section 5. Chaque étape de ce travail est validée expérimentalement.
L'ontologie dans le système MIEL++
Le domaine d'application de notre entrepôt de données est défini dans une ontologie, et tout notre système est guidé par cette ontologie : pour changer de domaine d'application, il suffit de changer d'ontologie.
Le travail présenté ici est appliqué au domaine de la microbiologie alimentaire, et l'entrepôt de données construit s'intègre dans un système existant appelé MIEL 1 (Buche et al., 2005). Dans le système MIEL, les données de microbiologie alimentaire sont entrées manuellement dans une base de données relationnelle, et les utilisateurs interrogent cette base via une interface de requêtes où ils sélectionnent dans l'ontologie les microorganismes, produits alimentaires et facteurs expérimentaux qui les intéressent, avec la possibilité de définir des pré-férences : la base de données est interrogée selon ces critères, qui sont cependant élargis pour ramener plus de données, et les résultats sont ordonnés suivant leur proximité avec la requête de l'utilisateur. Dans le système MIEL++ (MIEL élargi à l'entrepôt de données XML, voir Buche et al., 2006), on souhaite conserver le même mode d'interrogation, la base de données et l'entrepôt étant interrogés simultanément, de façon transparente pour l'utilisateur. Pour cela, les données de l'entrepôt XML doivent être annotées avec la même ontologie que celle déjà utilisée dans le système MIEL.
L'ontologie utilisée dans le système MIEL++ décrit les relations sémantiques intéressantes pour le domaine de la microbiologie alimentaire, et les types de données impliqués dans ces relations. Par exemple, la relation Growth kinetics est composée des types Food product, Microorganism, Temperature, Time, Colony count. Les types sont décrits dans l'ontologie de deux manières, suivant qu'ils sont symboliques (Food product, Microorganism) ou numériques (Temperature, Time, Colony count). Les types symboliques sont décrits par une taxonomie des valeurs possibles (par exemple, taxonomie des microorganismes). Les valeurs qui peuvent être prises par un type symbolique sont appelées termes. Les types numériques sont décrits par les unités dans lesquelles on peut les exprimer (par exemple,
• C ou • F pour Temperature) ainsi que, le cas échéant, par un intervalle de valeurs possibles (par exemple un pH est compris entre 0 et 14). Cette ontologie, construite manuellement lors de la création du système MIEL, est représentée dans un format spécifique. Nous étudions la possibilité de la représenter dans un des langages standards du web sémantique.
Distinction entre colonnes numériques et symboliques
On suppose dans cette section qu'un prétraitement permet de mettre les tableaux issus du Web dans un format standard, avec des entêtes de colonnes, puis des lignes composées d'un ensemble de cellules : chaque ligne est une instance de la relation sémantique présentée par le tableau. Notre objectif est de reconnaître le type des colonnes du tableau, pour en déduire la relation sémantique représentée par le tableau. Un traitement différent est appliqué suivant qu'une colonne contient des données numériques ou symboliques : notre premier travail est donc de faire la distinction entre les colonnes numériques et symboliques.
Classification par règles des colonnes numériques et symboliques
Faire la différence entre des colonnes numériques et symboliques dans un tableau n'est pas si simple qu'il y paraît, surtout dans le domaine de la microbiologie alimentaire où de nombreuses données symboliques comportent des chiffres (par exemple la souche de microorganisme "E. coli O 157 : H7") alors que les données numériques comportent souvent des mots (unités, précision d'un intervalle de confiance...). La solution que nous proposons pour distinguer les colonnes numériques des colonnes symboliques tient compte des unités définies dans l'ontologie pour les types numériques.
Tout d'abord, les nombres sont reconnus selon l'expression régulière (digit)+((','|'.')(digit)+) * , avec digit correspondant à l'un des dix chiffres ('0'|'1'|...|'9'). Les nombres en notation scientifique sont reconnus suivant l'expression régulière digit '.' (digit)+ 'x 10 ' (digit)+. On recherche égale-ment dans le tableau les occurrences des unités définies dans l'ontologie, et des indicateurs de résultat absent, qui sont des chaînes de caractères prédéfinies (par exemple, "No result", ou "NS" pour Not Specified 
Résultats expérimentaux
Notre méthode de classification a été expérimentée sur 60 tableaux intéressants pour le domaine de la microbiologie alimentaire. Une classe symbolique ou numérique a été manuellement assignée à chacune des colonnes de ces tableaux, résultant en 264 colonnes numériques et 85 colonnes symboliques. Les résultats de notre classification ont été comparés à ceux d'un classifieur « naïf », dans lequel toute cellule contenant un chiffre est considérée comme numé-rique et une colonne est considérée comme numérique si plus de la moitié de ses cellules sont numériques. Les résultats de cette classification sont donnés dans le tableau 1.
classification utilisant les unités classification naïve P P P P P P P P La précision globale de la classification (proportion de colonnes bien classifiées par rapport au nombre total de colonnes classifiées) atteint 98% pour la classification utilisant les unités définies dans l'ontologie, contre 86% pour le classifieur naïf. On voit ici à quel point il est intéressant d'utiliser l'ontologie dès ce stade de l'annotation.
Annotation des données symboliques
Lorsqu'on a affaire à une colonne de données symboliques, on cherche d'une part à annoter le contenu de chaque cellule avec les termes de l'ontologie, et d'autre part à reconnaître le type de la colonne. La première étape consiste en l'annotation du contenu des cellules par les termes de l'ontologie qui en sont lexicalement les plus proches, comme présenté en section 4.1. En deuxième étape, les résultats de cette annotation sont utilisés pour déduire le type de la colonne, comme présenté en section 4.2. Enfin, l'annotation des cellules obtenue en première étape est modifiée afin de ne conserver que les termes correspondant bien au type trouvé pour la colonne.
Annotation des cellules au sein d'une colonne de valeurs symboliques
Dans notre système, le contenu d'une cellule symbolique, ci-après appelé « terme du Web », est annoté non pas uniquement avec un terme de l'ontologie, mais avec plusieurs termes possibles. Contrairement aux travaux de Gagliardi et al. (2005), les différents termes de l'ontologie proposés pour l'annotation n'ont pas tous la même importance, mais sont ordonnés selon leur similarité avec le terme du Web. Nous utilisons pour représenter notre annotation le modèle des sous-ensembles flous.
Les sous-ensembles flous
Le système MIEL, que nous souhaitons étendre pour l'interrogation des données annotées, utilise en effet le formalisme des sous-ensembles flous (Zadeh, 1965(Zadeh, , 1978 pour l'expression des requêtes. Nous utilisons ce même formalisme pour représenter nos annotations.
La notion de sous-ensemble flou est un assouplissement de la notion de sous-ensemble classique d'un ensemble de référence X. Dans le cas classique, les éléments de X qui possèdent une certaine propriété constituent un sous-ensemble A de X, les éléments de X qui ne possèdent pas cette propriété appartiennent au complémentaire de A dans X. Dans le cas d'un sous-ensemble flou, les éléments peuvent appartenir partiellement à un sous-ensemble, avec un degré d'appartenance compris entre 0 (élément n'appartenant pas au sous-ensemble) et 1 (élément appartenant totalement au sous-ensemble). Définition. Un sous-ensemble flou A d'un ensemble de référence X est défini par une fonction d'appartenance µ A de X dans [0, 1] qui associe à chaque élément x de X le degré µ A (x) avec lequel x appartient à A.
FIG. 1 -Exemple de sous-ensemble flou sur un ensemble de définition à valeurs symboliques.
Dans notre système d'annotation, nous utilisons les sous-ensembles flous pour décrire la similarité d'un terme du Web avec différents termes de l'ontologie. L'ensemble de définition du sous-ensemble flou est l'ensemble de tous les termes de l'ontologie, la fonction d'appartenance est une mesure de similarité entre le terme du Web et chacun des termes de l'ontologie. Par exemple, la figure 1 représente l'annotation du terme "minced beef" trouvé sur le Web. Ce terme n'existe pas tel quel dans l'ontologie, mais est similaire à divers termes de l'ontologie : "ground beef", et dans une moindre mesure "minced meat" ou "minced poultry". Les termes de l'ontologie dont le degré de similarité avec "minced beef" est nul ne sont pas représentés.
Degré de similarité d'un terme du Web avec un terme de l'ontologie
Il nous faut maintenant définir quelle mesure de similarité nous utilisons comme fonction d'appartenance pour nos sous-ensembles flous servant à l'annotation des termes du Web. Différentes mesures de similarité sémantique entre deux termes ont été présentées par Lin (1998);Resnik (1999); Seco et al. (2004) : ces mesures ont en commun qu'elles nécessitent l'utilisation d'une ontologie tierce comprenant les deux termes à comparer. Dans notre cas, une telle ontologie n'existe pas. Des essais que nous avons menés avec WordNet ont montré que cette ontologie était trop généraliste pour comprendre tous les noms d'aliments trouvés dans les publications issues du Web ; le thésaurus AgroVoc, utilisé par la FAO 2 et spécialisé dans l'agriculture et l'agro-alimentaire, ne contient pas lui non plus les noms d'aliments ré-pertoriés dans les publications scientifiques en microbiologie alimentaire que nous cherchons à exploiter. Une mesure de similarité lexicale fondée sur les n-grammes est présentée par Lin (1998), mais cette mesure cherche à retrouver des mots de même racine plutôt que de même signification.
Nous proposons une mesure de similarité lexicale entre deux termes, fondée sur des égali-tés de mots. Chaque terme de l'ontologie, ainsi que chaque terme du Web, est décomposé en un ensemble de mots, qui sont lemmatisés (par exemple, "carrot cuts" et "cut carrots" donnent tous deux le même ensemble {carrot, cut}). Tous les mots d'un terme n'ont pas la même importance dans la signification du terme, et ce de façon différente suivant le domaine d'application : dans le terme "minced poultry", c'est "minced" qui est le plus important si on se concentre sur les procédés alimentaires, tandis que c'est "poultry" si on s'intéresse plutôt à l'origine des produits. La distinction entre mots importants et moins importants est donc une affaire d'experts, qui ajoute de la connaissance sur un domaine. Dans l'ontologie, chaque mot de chaque terme se voit attribuer manuellement un poids entre 0 et 1 correspondant à son importance dans la signification du terme. Pour simplifier le travail d'attribution de poids dans les termes de l'ontologie, on conserve trois niveaux de poids :
-poids de 0 pour les mots n'apportant pas de sens, tels qu'articles et conjonctions, listés dans une stop-list ; -poids de 1 pour les mots d'importance majeure pour la signification du terme ; -poids de 0, 2 pour les mots d'importance moyenne (la valeur 0, 2 a été définie lors d'expériences préliminaires qui ont montré une annotation de meilleure qualité avec ce poids qu'avec le poids intermédiaire de 0, 5). Comme on ne dispose pas de connaissances d'expert pour les termes du Web, tous les mots des termes du Web se voient attribuer un poids de 1 (sauf les mots de la stop-list qui gardent un poids de 0).
Lors de l'annotation d'un terme du Web, tous les termes de l'ontologie et celui du Web sont représentés comme des vecteurs, dont les coordonnées représentent l'ensemble de tous les mots lemmatisés possibles (i.e. tous les mots présents dans l'ontologie et les mots du terme du Web), les valeurs de ces coordonnées correspondant au poids du mot dans le terme, ou 0 si le mot n'est pas présent dans le terme. Un exemple de représentation vectorielle de termes est donné dans le tableau 2. Dans cet exemple, nous ne montrons que les termes de l'ontologie ayant au moins un mot en commun avec le terme du Web. Une fois les termes représentés en tant que vecteurs, on définit la similarité entre un terme du Web et un terme de l'ontologie comme la mesure de similarité par cosinus (Van Rijsbergen, 1979) entre les deux vecteurs. Cette mesure a été choisie car c'est l'une des plus répandues pour la comparaison de vecteurs pondérés, et qu'une comparaison expérimentale avec d'autres mesures ne nous a pas apporté de meilleurs résulats.
Définition. La similarité entre un terme w du Web et un terme o de l'ontologie, représentés comme des vecteurs pondérés w = {w 1 , ..., w n } et o = {o 1 , ..., o n } est définie par la formule suivante :
Exemple. Selon les poids donnés dans le tableau 2, on calcule les degrés de similarité utilisés dans la figure 1 :
Résultats expérimentaux
La validation de notre méthode d'annotation floue des termes du Web a été faite sur la partie aliments de l'ontologie : 185 termes distincts ont été manuellement annotés par leur terme le plus proche (ci-après appelé best match) dans la taxonomie des aliments. Pour valider la gé-néralité de notre approche, nous avons également fait des tests d'annotation avec la taxonomie du Codex Alimentarius (taxonomie d'aliments utilisée par l'OMS 3 ). Les deux taxonomies ont été retravaillées pour donner des poids aux mots de tous les termes. Chaque terme du Web a été annoté selon la méthode présentée en section 4.1.2 dans chacune des deux taxonomies, une fois avec les poids des mots définis manuellement, une fois avec des poids de mots de 1, comme si tous les mots avaient la même importance (mis à part les mots de la stop-list qui conservent un poids de 0). Les termes de la taxonomie proposés pour l'annotation d'un terme du Web sont ordonnés selon leur degré de similarité avec le terme du Web, et l'on regarde en quelle position se trouve le best match. Cette position est évaluée « au pire », c'est à dire que s'il y a plusieurs termes ayant le même score de similarité avec le terme du Web, le best match est considéré comme étant en dernière position. Cette méthode d'évaluation est due à notre souhait de proposer une annotation semi-automatique, où une liste des n meilleurs termes sera proposée à un utilisateur pour l'annotation du terme du Web : si plusieurs termes ont le même score, on ne maîtrise pas si le « bon » sera affiché dans la liste ou non. Les résultats de l'annotation sont présentés dans le tableau 3.
taxonomie Codex Alimentarius aliments dans MIEL++ termes dont le best match a un score non nul 60% 78%
h TAB. 3 -Résultats de l'annotation de 185 noms d'aliments.
h h h h h h h h h h h h h h h
Tout d'abord, on s'aperçoit que les résultats d'annotation sont meilleurs avec l'ontologie de MIEL++ qu'avec le Codex Alimentarius. Ceci s'explique par le fait que l'ontologie de MIEL++ a été construite spécialement pour le domaine de la microbiologie alimentaire, contenant notamment des noms d'aliments transformés, alors que le Codex Alimentarius a été construit à d'autres fins et est essentiellement tourné vers les matières premières (par exemple, on n'y trouve pas "butter" mais "cow milk fat", "goat milk fat",...). Cela plaide en faveur de l'utilisation d'ontologies de domaine vraiment adaptées au centre d'intérêt applicatif.
Pour qu'un terme de l'ontologie ait un score de similarité non nul avec un terme du Web, il faut et il suffit que ces termes aient un mot en commun : utiliser une méthode fondée sur l'égalité de mots n'est pas dénuée de sens, puisque 78% des best match ont effectivement un mot commun avec le terme du Web dans le cas de l'ontologie de MIEL++. L'utilisation de poids sur les mots dans les taxonomies ne modifie pas les termes qui seront retenus pour l'annotation, mais modifie l'ordre dans lequel ils sont présentés : on voit que l'utilisation de poids apporte une amélioration, faible mais systématique. En revanche, que l'on utilise ou non les poids sur les mots, l'utilisation d'une annotation floue avec termes ordonnés selon un degré de similarité est un gain important, puisqu'il y a en moyenne 16 termes de la taxonomie des aliments de MIEL++ ayant au moins un mot commun avec le terme du Web (avec un maximum à 94 termes pour le terme du Web "raw milk cheese") : une présentation non ordonnée de tous les termes possibles pour l'annotation est donc à proscrire. En utilisant le score de similarité, on arrive à obtenir 66% des best match dans les 5 premières positions pour l'ontologie de MIEL++, ce qui est bien plus intéressant dans le cadre d'une annotation semi-automatique, ou pour une interrogation où les résultats sont ordonnés selon leur similarité à la requête.
Détermination du type d'une colonne de valeurs symboliques
Une fois les termes des cellules d'une colonne annotés en utilisant le degré de similarité avec les termes de l'ontologie qu'on vient de présenter, on peut déterminer le type de la colonne symbolique.
Utilisation des degrés de similarité avec les termes de l'ontologie
On détermine le type de chaque terme de la colonne d'après ses scores de similarité avec les différents termes de l'ontologie. Les types de tous les termes d'une colonne sont ensuite utilisés pour déterminer le type de la colonne. Soit col une colonne d'un tableau, type un type symbolique défini dans l'ontologie. Soit T type l'ensemble de tous les termes de l'ontologie appartenant au type type. Soit t ext le terme du Web contenu dans une cellule de la colonne col. Alors le score du type type pour le terme t ext est le suivant :
t?Ttype Pour chaque terme de la colonne, on calcule le score de chaque type de l'ontologie. Soit bestT ype(t ext ) le type qui a le meilleur score pour le terme t ext . Si ce score est supérieur à un seuil ? défini par l'utilisateur, alors le terme t ext est considéré comme étant du type bestT ype(t ext ). Si par contre score(t ext , bestT ype(t ext )) < ?, alors t ext est considéré comme de type inconnu. Le score du type type pour la colonne col est la proportion de termes de la colonne ayant le type type. Soit T col l'ensemble de tous les termes de la colonne et T type col l'ensemble de tous les termes de la colonne ayant le type type, alors
Considérons le type bestT ype(col) qui a le meilleur score pour la colonne col. Si ce score est supérieur à un seuil ? défini par l'utilisateur, avec ? ? [0, 1], alors la colonne est classifiée comme étant du type type. Sinon le type de la colonne est considéré comme non reconnu.
Lorsque le type de la colonne est reconnu, on restreint le domaine de définition des sousensembles flous servant à l'annotation des termes au sein de la colonne : le nouveau domaine de définition est l'ensemble de tous les termes de l'ontologie correspondant au type de la colonne.
Résultats expérimentaux
Les 80 colonnes ayant bien été reconnues comme symboliques lors de la classification numérique/symbolique (section 3.2) ont été utilisées pour cette expérience. Les colonnes ont été manuellement classées en trois types : aliment(46 colonnes), microorganisme(16 colonnes) et autre(18 colonnes). Tous les termes de ces colonnes ont ensuite été automatiquement annotés avec les termes de l'ontologie correspondant aux types aliment et microorganisme. Les types des colonnes ont été calculés selon la méthode présentée ci-dessus, avec pour paramètres ? = 0, 2 et ? = 0, 5, les colonnes de type non reconnu étant classées comme de type autre.
La qualité de cette classification a été comparée avec une classification automatique par apprentissage : on a utilisé la méthode de classification SMO, une optimisation des SVM (voir Platt, 1999), implémentée dans Weka 4 , en conservant les paramètres par défaut, les colonnes étant transformées en vecteurs pondérés de tous les mots qu'elles contiennent. La classification par SMO a été évaluée en validation croisée par leave one out (chaque colonne est classifiée en utilisant un classifieur entraîné avec l'ensemble des 79 autres colonnes). Les résultats obtenus sont présentés dans le tableau 4. utilisation de l'ontologie SMO P P P P P P P P Avec notre méthode sans apprentissage utilisant l'ontologie, on obtient une précision de 94% et une couverture de 74% pour les aliments : la classification par apprentissage suivant la méthode SMO donne certes une couverture de 100%, mais avec une précision plus faible à 81%. Pour les microorganismes, notre méthode donne une précision de 100% et une couverture de 75%, alors que SMO permet aussi une précision de 100% mais avec une couverture plus basse (69%). Notre méthode donne donc des résultats tout à fait comparables (voire meilleurs dans le cadre de notre application où l'on cherche avant tout une bonne précision) aux mé-thodes classiques de classification par apprentissage. L'avantage de notre méthode est qu'elle ne nécessite pas de phase d'apprentissage, en utilisant une ontologie déjà existante. Nous avons également testé la sensibilité de notre méthode au choix des paramètres. La sensibilité pour ? est faible : on obtient les résultats présentés dans le tableau 4 pour tout ? entre 0, 01 et 0.4 ; cependant pour des valeurs de ? plus élevées, on perd en couverture plus vite qu'on ne gagne en précision. On atteint une précision de 100% pour les aliments et microorganismes avec ? = 1 : on a alors une couverture de 65% pour les aliments et 69% pour les microorganismes. Notre méthode est un peu plus sensible pour le choix du paramètre ? : plus ? est grand, plus la précision est grande, avec une moindre couverture. Cependant les variations ne sont que de quelques points par tranche de 0, 1 ajoutée ou enlevée à ?.
Annotation des données numériques
De même que nous avons recherché le type des colonnes symboliques, nous recherchons le type de colonnes numériques afin de pouvoir ultérieurement déterminer la signature de la relation représentée dans chaque tableau.
Reconnaissance du type d'une colonne numérique
Afin de déterminer le type d'une colonne numérique, on combine deux scores : le score de similarité du titre de la colonne avec les noms des différents types numériques, et un score déduit des unités utilisées dans la colonne.
On considère tout d'abord le titre de la colonne : on ne conserve que les mots qui ne correspondent ni à une unité, ni à un mot « sans intérêt » de la stop-list et on leur attribue un poids de 1. On calcule ensuite le score de similarité entre le titre de la colonne et chacun des types numériques de l'ontologie, selon la formule du score de similarité donnée en section 4.1.2, avec comme terme du Web le titre de la colonne et comme terme de l'ontologie le nom du type numérique dans l'ontologie (par exemple, "Samples tested" pour le nombre d'échantillons sur lesquels l'expérience porte). Soit t titre le titre de la colonne col et t type le nom du type type, alors le score de similarité du titre de la colonne col avec le type type est :
Examinons maintenant les unités utilisées dans la colonne. Soit u une unité et T u l'ensemble de tous les types numériques pouvant s'exprimer dans cette unité. Le score du type type pour l'unité u est score(u, type) = 1 Tu si u est une unité valable pour type, et score(u, type) = 0 si le type type ne s'exprime pas dans l'unité u. Soit U col l'ensemble de toutes les unités présentes dans la colonne : on considère également les unités présentes dans le titre de la colonne à condition qu'elles ne fassent pas partie d'un couple nombre-unité, qui représente généralement une précision de condition expérimentale (par exemple "at 37
• C"). Le score sur les unités de la colonne col avec le type type est :
Ainsi le score de la colonne col avec le type type est : -si toutes les valeurs numériques contenues dans les cellules de la colonne sont compatibles avec l'intervalle de valeurs possibles associé au type type, alors score f inal (col, type) = 1?(1?score titre (col, type))×(1?score unit (col, type)) (6) Ce score est inspiré de Yangarber et al. (2002), où une mesure similaire est utilisée pour combiner les confiances que l'on a en différentes règles de reconnaissance d'une entité nommée. Les deux scores se renforcent ainsi mutuellement, mais il suffit que l'un des deux scores soit bon pour que le score final soit bon.
-s'il existe une valeur dans les cellules de la colonne qui est en dehors de l'intervalle de valeurs défini dans l'ontologie, alors score f inal (col, type) = 0
Le type retenu pour la colonne est celui qui a le meilleur score f inal . Si tous les scores sont nuls, le type de la colonne est considéré comme non reconnu.
Résultats expérimentaux
Les 263 colonnes numériques reconnues lors de la classification numérique/symbolique (section 3.2) ont été utilisées pour la validation de notre approche. Ces colonnes ont été manuellement classées suivant 19 types numériques définis dans l'ontologie. Les résultats de notre classification utilisant l'ontologie ont été comparés à ceux de la méthode de classification par apprentissage SMO, les colonnes étant représentées par des vecteurs pondérés de tous les mots contenus dans les cellules et le titre de colonne, toutes les valeurs numériques étant remplacées par le mot-clef #NUM. La classification par SMO a été évaluée en validation croisée par leave one out.
Avec notre méthode de classification sans apprentissage utilisant l'ontologie, on obtient une précision globale de 96% et une couverture de 95%, sur l'ensemble des 19 types. La méthode SMO classe toutes les instances, avec une précision globale et une couverture globale de 96%. Notre méthode de classification, qui ne nécessite pas de données d'entraînement, donne donc des résultats de classification tout à fait comparables à une méthode classique de classification par apprentissage, plus gourmande en temps d'expert si l'on part du postulat que l'ontologie existe de toute manière (ce qui est le cas puisque l'objet de la classification est de reconnaître les types définis dans l'ontologie).
Conclusion et perspectives
Nous avons présenté une méthode d'annotation de tableaux de données guidée par une ontologie, sans phase d'apprentissage. Nous distinguons tout d'abord les données numériques et symboliques, pour les traiter différemment. Les données symboliques sont annotées avec les termes de l'ontologie, et ces annotations permettent de déduire le type de chaque colonne symbolique. Pour les colonnes numériques, on utilise à la fois le titre de la colonne et les valeurs et unités contenues dans la colonne pour déterminer le type de la colonne : là encore nous utilisons l'ontologie, dans laquelle sont définis les unités et intervalles de valeur valables pour chaque type numérique. Notre approche donne des résultats comparables à une méthode classique de classification par apprentissage, mais sans nécessiter la construction d'un jeu d'entraînement.
A partir des types de colonnes ainsi identifiés, il nous reste maintenant à reconnaître les relations représentées par le tableau de données. Ensuite nous travaillerons sur les techniques d'interrogation de l'entrepôt de données, en tenant compte du fait que les critères d'interrogation permettent d'exprimer des préférences, que l'annotation des données symboliques est floue et que le type de certaines colonnes n'est pas reconnu.

Introduction
L'apprentissage de la structure des réseaux bayésiens (RB) à partir de données est un problème ardu ; la taille de l'espace des graphes orientés sans circuits (DAG en anglais) est super-exponentielle en fonction du nombre de variables et le problème combinatoire associé est NP-difficile (Chickering et al., 2004). Deux grandes familles de méthodes existent pour l'apprentissage de la structure des RB : celles fondées sur la satisfaction de contraintes d'indé-pendance conditionnelle entre variables et celles à base de score fondées sur la maximisation d'un score (BIC, MDL, BDe, etc.). Les deux méthodes ont leurs avantages et leurs inconvé-nients. Les méthodes sous contraintes sont déterministes, relativement rapides et bénéficient des critères d'arrêt clairement définis. Les contraintes imposées à la structure du graphe proviennent des informations statistiques sur les dépendances et indépendances conditionnelles observées dans les données. Elles reposent cependant sur un niveau de signification arbitraire du test d'indépendance employé. En outre, les erreurs commises au début peuvent se répercu-ter en cascade dans la suite de l'execution de l'algorithme, et conduire à un graphe erroné. Les méthodes à base de score ont, quant à elles, l'avantage d'incorporer des probabilités a priori sur la structure du graphe et de traiter plus facilement les données manquantes. En revanche, elles sont facilement piégées dans les nombreux minima locaux et le graphe final obtenu dépend fortement des conditions initiales.
Plusieurs méthodes ont été proposées durant ces quinze dernières années mais quelques avancées prometteuses ont été réalisées très récemment. Dans un article paru en 2006, Tsamardinos et al. montrent par des simulations exhaustives sur une vingtaine de bancs d'essais (Child, Insurance, Alarm, Hailfinder etc.) l'avantage significatif d'un algorithme sous contraintes, dénommé Min-Max Hill-Climbing (MMHC), au regard des algorithmes majeurs (score et contraintes) en fonction de plusieurs métriques de performance (Tsamardinos et al., 2006). Son inconvénient, toutefois, est sa complexité au pire cas en O(n2 n ) où n désigne le nombre de noeuds. En réalité, aucun algorithme exact n'échappe à une complexité exponentielle car les méthodes exactes reposent toutes sur une recherche exhaustive des indépendances entre deux variables X et Y conditionnellement à un ensemble Z. Cette recherche nécessite O(2 |Z| ) opérations au pire cas. C'est pourquoi toutes les approches polynômiales reposent sur une heuristique particulière pour parcourir les ensembles Z.
Brown et al. ont donc proposé une version polynomiale en O(n 4 ) de MMHC dénommée Polynomial Min-Max Skeleton (PMMS) (Brown et al., 2005) en adoptant une heuristique ingénieuse. L'algorithme séduit par ses nombreux attraits : outre sa grande simplicité, les auteurs ont montré empiriquement l'excellent compromis entre faible complexité et qualité de reconstruction comparé aux autres algorithmes, surtout en présence de faibles jeux de données. Cet avantage est décisif à nos yeux car nous disposons d'une base de données d'une étude épi-démiologique cas-témoins du cancer du nasopharynx (NPC) de seulement 1289 observations. L'idée est donc d'utiliser PMMS afin de construire par apprentissage la structure du RB associé aux données. Néanmoins, comme tous les algorithmes sous contraintes, PMMS échoue lorsque des dépendances fonctionnelles (DF) déterministes existent entre des groupes de variables. Une DF, notée X ? Y , est une contrainte entre un ensemble de variables, telle que tout ensemble de valeurs prises par les X j ? X determine la valeur de Y de façon univoque. Or rien n'exclue cette éventualité compte tenu du faible nombre d'observations. En outre, PPMS ne s'applique qu'aux données complètes, ce qui n'est pas notre cas. Aussi, dans cet article, nous apportons quelques modifications algorithmique à PMMS pour remédier à ces deux problèmes : le traitement des DF et son adaptation aux données manquantes.
Après un rappel indispensable de la problématique et des principes de l'algorithme, ces modification sont présentées en détail et validées sur deux bancs d'essai Asia et Asia8 avec une DF entre trois variables, 1289 données et 5% de données manquantes pour nous ramener au cas du NPC. La nouvelle version a été développée sous Matlab à l'aide de la Toolbox BNT de (Murphy, 2001) et de la Toolbox BNT-SLP de (Leray et Francois, 2004). Ensuite, nous appliquons la méthode aux données du NPC de 1289 observations. A la différence des travaux préliminaires menés dans (Aussem et al., 2006) avec seulement 10 variables binaires synthé-tiques et sans données manquantes, une base plus vaste de 61 variables qualitatives ordinales et 5% de données manquantes est analysée. L'objectif est de dresser un profil statistique type de la population étudiée et d'apporter un éclairage utile sur les différents facteurs impliqués dans le NPC.
Préliminaires
Notons l'indépendance conditionnelle entre X et Y sachant l'ensemble Z dans une loi de probabilité P par Ind P (X; Y |Z) et la dépendance par Dep P (X; Y |Z). Les lettres majuscules en gras, Z, désignent des ensembles de variables aléatoires, les autres majuscules, X, désignent des variables uniques, les minuscules (X = x, X = x) désignent les attributs ou modalités des variables. Soit P , une loi de probabilité conjointe sur un ensemble de variables aléatoires V, et G =< V, E > un graphe orienté sans circuit (DAG en anglais). On dira que le tuple < G, P > est un réseau bayésien si < G, P > vérifie la condition dite de Markov : chaque variable, X ? V, doit être indépendante de ses non descendantes (N D X ) dans G conditionnellement à ses parents (Neapolitan, 2004;Pearl, 2000). Cette condition se note Ind P (X; N D X |Pa G i ) où Pa G i désigne l'ensemble des parents de X i dans G. La condition de Markov implique la factorisation de la loi jointe :
Cette propriété importante montre qu'il suffit de stocker les valeurs de  Naim et al. (2004);Neapolitan (2004)). La propriété de Markov impose en revanche une condition forte aux lois de probabilité P qui peuvent être représentées par le même graphe G. Contraintes du graphe -La d-séparation est un critère important qui permet de caractériser graphiquement toutes les contraintes d'indépendance des lois P qui peuvent être représentées par un même DAG. Pour éclaircir son rôle, il faut introduire la notion de chaîne d'information bruité. Par chaîne, on entend une succession d'arcs orientés entre X et Y mais vus comme des arêtes non orientés (Neapolitan, 2004). Pour comprendre la d-séparation, il faut symboliser les noeuds sur ces chaînes par des vannes d'information, ouvertes ou fermées selon le cas. Un chemin est dit ouvert si toutes les vannes sont ouvertes auquel cas il laisse passer l'information. A l'inverse, si l'une des vanne est bloquée, la chaîne est dite bloquée. En extrapolant, l'information qu'apporte X sur Y peut se voir comme la somme des flots sur tous les chaînes ouvertes reliant X à Y . Il reste à spécifier le mécanisme d'ouverture et de fermeture des vannes. Il existe 3 types de connexions : les connexions en série 
Condition de fidélité -G et P sont dits fidèles (faithful) l'un à l'autre ssi toutes les indépen-dances conditionnelles sont strictement identifiées par les d-séparations, i.e., Dsep G (X; Y |Z) ? Ind(X; Y |Z). On parle alors de réseau bayésien < G, P > fidèle. PMMS repose sur l'hypothèse de fidélité ; l'algorithme construit un DAG sensé être fidèle à la loi de probabilité P sous-jacente aux données. Cela pose un problème car toutes les distributions ne sont pas fidèles à un DAG, c'est le cas notamment du réseau Asia en raison de la variable O. O est un OU logique entre T et L, du coup Ind P (O; X|{T, L}) est vérifié sans pour autant avoir Dep P (O; X|{T, L}). L'existence de dépendances fonctionnelles (DF), accidentelles ou non, entre des variables, est hélas fréquent. C'est souvent le cas dans les données sont issues de questionnaires pour de multiples raisons (e.g. questions redondantes ou mal comprises, ré-ponses groupées etc.).
Polynomial Min-Max Skeleton revisité
Dans ce paragraphe, nous rappelons le principe de PMMS avant de présenter les modifications que nous avons opérées pour traiter l'existence des DF et les données incomplètes. PMMS exploite ingénieusement le critère de d-séparation. Il construit itérativement le voisinage, parents et enfants, de chaque variable cible T , en observant que ce sont les seuls noeuds qui ne peuvent être d-séparés de T . Notons par PC T les noeuds parents et enfants du noeud T dans G. PC T est unique à tous les DAG tels que < G, P > soient fidèles, il ne dépend donc pas de G. PMMS emploie une mesure d'association probabiliste conditionnelle notée
C'est-à-dire les plus petites associations entre X et Y pour tous les sous-ensembles S de Z. PMMS appelle successivement la procédure Polynômial Min-Max Parents and Children PMMPC pour chaque variable de G (voir algorithme 1). PMMPC identifie PC T étant donné une variable cible T . Ainsi, connaissant le voisinage direct de chaque variable cible, il suffit de connecter les noeuds pour obtenir le squelette du graphe (non connecté). La version originale de PMMPC opère en 2 phases. Nous y avons adjoint une troisième phase pour traiter les DF (voir algorithme 2).
Phase I -Les variables entrent séquentiellement dans un ensemble de candidats noté CPC à l'aide d'une heuristique Max-Min. L'idée est de sélectionner itérativement les variables qui ne peuvent pas être d-séparées par l'ensemble CPC courant. Celle qui rentre est celle qui présente l'association résiduelle la plus forte avec la cible T malgré notre effort pour bloquer toutes les chaînes les reliant. Le calcul de M inAssoc(X; Y |CP C) requiert normalement un nombre d'appels exponentiel à la fonction Assoc. Dans PMMS, ce calcul est réduit à l'aide de l'heuristique gloutonne GreedyM inAssoc(X; T |CPC, minval, MinSet) ; minval est l'estimation courante du minimum d'association et MinSet est l'estimation courante de S ? Z qui réalise ce minimum. Initialement MinSet = ? et minval = Assoc(X; Y |?) ; MinSet croît itérativement par adjonction d'une variable de CPC après l'autre jusqu'à ne plus pouvoir décroître l'association résiduelle. La notation min{x = désigne le plus petit x différent de s'il existe. Si les données sont en nombre insuffisant, alors Assoc = la phase de croissance est stoppée et la dépendance est supposée. Néanmoins la valeur de l'association est fixée à la plus petite valeur possible, comme discuté au chapitre 3.1.
Phase II -Dans cette phase backward, les faux-positifs entrés par erreur dans la phase I (voir Tsamardinos et al. (2006)) sont itérativement éliminés de CPC. Pour ce faire, on teste pour chaque
Phase III -Cette étape est nouvelle. Si CPC ? T est une DF, le graphe G n'est pas fidèle avec P , auquel cas T sera d-séparé de tous ses enfants par CPC alors que c'est faux. Dans le cas d'Asia par exemple, PPMPC lancé sur la cible O conduit à CPC = {T, L} car l'association de T et L avec O est la plus forte, et par suite, ni X ni D ne pourront plus entrer dans CPC. Aussi, pour y ajouter les enfants de la cible T , nous testons si la relation CPC ? T est un DF par un appel à IsF uncDep(T ; CPC; D) (simple parcours de l'hypercube de contingences). Si la DF est observée, un nouvel appel récursif à PMMPC est fait en retirant CPC à l'ensemble des variables V jusqu'aucune DF ne subsiste. On récupère au final un ensemble CPC qui contient non seulement les enfants de T mais aussi ses grands-parents. Dans Asia, le CP C de O sera au final l'ensemble {T, L, X, D, A, S} mais PMMS ne connectera pas O avec ses grands-parents A et S car réciproquement O ne sera ni dans le CP C de A, ni dans celui de S. Ainsi, la phase 3 permet de détecter les DF susceptibles de tromper PMMPC (et uniquement celles-ci) pour ensuite trouver tous les parents et enfants de la variable cible.
Au final, PMMS construit un graphe non orienté (le squelette) qui est sensé, une fois les arêtes dirigées, représenter la structure du réseau bayésien. Comment diriger les arêtes du graphe du squelette ? Il faut garder à l'esprit que plusieurs DAG peuvent encoder la même loi de probabilité conjointe, seule importe la position des V-structures (i.e., X ? Y ? Z tel que X et Z ne soit pas connectés). Cet article ne porte que sur l'apprentissage du squelette, la partie de loin la plus difficile. Pour la recherche des V-structures et la direction des arcs, le lecteur est invité à consulter (Naim et al., 2004;Neapolitan, 2004) pour de plus amples informations. 
Mesure d'association
finsi 8: fin pour 9: return E au risque ? du test, et zéro sinon. Le ? 2 XY |Z suit une loi ? 2 à ? = (n X ? 1)(n Y ? 1)c degrés de liberté où c = Zj ?Z n Z j est le produit du nombre de modalités de chaque variable dans Z. Intuitivement, plus la valeur de p est petite, plus l'association entre X et Y sachant Z est forte. Aussi, une valeur de p supérieure au seuil ? indiquera une association nulle. En pratique, le test n'est utilisé que si n est suffisamment grande devant ?. Dans le cas contraire, il faudrait idéalement procéder à des regroupements de modalités voisines. Des méthodes heuristiques existent pour évaluer empiriquement le nombre effectif de degrés de liberté (voir Tsamardinos et al. (2006)). Dans notre cas, le test est appliqué dès lors que n > 10? comme dans l'heuristique PC Spirtes et al. (2000), sinon Assoc(X; Y |Z) retourne une constante inférieur au risque du test 0 < < 1 ? ? et l'on suppose la dépendance, faute de pouvoir statuer.
La présence de données manquantes dans les données du cancer pose une difficulté supplémentaire. L'apprentissage de RB en présence de données manquantes est en soi un domaine de recherche actif dans lequel plusieurs solutions ont été proposées, en majorité pour les algorithmes bayésiens (à base de score) François (2006). Nous optons pour une solution simple connue sous le nom "available case analysis" : le ? 2 XY |Z est calculé uniquement sur les observations pour lesquelles les n variables X, Y et Z j ? Z sont présentes. Les autres sont ignorées. Ce faisant, le risque est toutefois d'introduire un biais dans les estimateurs (Ramoni et Sebastiani, 2001;Friedman, 1998;Dash et Druzdzel, 2003) en particulier si l'hypothèse Missing Completely at Random (MCAR) n'est pas vérifiée, i.e., le processus de perte est indépendant de la valeur des données complétées D (observées et manquantes).
On distingue donc trois cas : 1) le cas où l'indépendance est supposée, 2) le cas du manque de données n ? 10? et enfin 3) le cas l'hypothèse d'indépendance est rejetée : Les résultats obtenus avec la nouvelle version de PMMS sont consignés dans la Table 1 en fonction du risque ? du test. L'objectif est de montrer la qualité de la reconstruction du squelette (Figure 1) malgré le faible nombre de données, partiellement manquantes, et de choisir empiriquement le meilleur seuil ?. Des tests avec ? > 0.1 ne sont pas affichés car ils se sont avérés décevants. Dix jeux de données ont été synthétisés à partir du RB original. La Table 1  
Application au cancer
Les données
Nous appliquons la nouvelle version de PMMS aux données d'une étude épidémiologique cas-témoins du cancer du nasopharynx (NPC) dans la lignée des travaux de recherche récents (Aussem et al., 2006;Antal et al., 2004;Getoor et al., 2004 
Résultats
La base est contient 5% de données manquantes mais on ignore si l'hypothèse M CAR est valide. Le nombre de modalité varie de 2 (binaire) à 11 pour la classe d'âge. La majorité des variables sont codées en trois classe "pas du tout", "un peu" et "beaucoup". La variable à expliquer est "cas NPC", les autres sont les variables explicatives, voir le lexique de la Figure  2. Elles portent sur les conditions socio-professionnelles, l'habitat, l'exposition aux produits toxiques, la nourriture industrielle ou faite maison, les maladies, allergies, les drogues locales etc. Le squelette du RB obtenu avec la nouvelle version de PMMS est représenté sur la Figure  2 avec ? = 0.1. Il s'agit d'une visualisation graphique des interactions entres les variables qui dresse le profil statistique de la population considérée. En observant uniquement la structure du graphe, des groupes thématiques cohérents de variables de A à P ont été exhibés avec notre expert. Leur homogénéité est frappante. A est la le seul groupe lié au NPC (variable 1), il est lié à l'aération de l'habitat (variables 30, 31 et 32 mais pas 33 car indépendamment de l'orientation des arcs, 1 peut être d-séparée de 33 par un sous-ensemble de 30, 31 et 32) ; B, conditions socio-professionnelles liées à l'âge. C, lieu d'habitat ; D, catégorie de logement ; E, produits toxiques et fumées ; F , drogues ; G, animaux domestiques ; H, encens, parfums et feu de bois ; I, maladies ; J, graisse rance ; L, protéines maison ; M , piment et harrissa ; N , nourriture industrielle ; O, conserves ; P légumes et fruits. On retrouve donc des résultats de bon sens : les hommes (3) sont plus enclins à fumer, à consommer des drogues (F ) et être exposés à des produits toxiques au travail (E), que l'exposition aux fumées (encens, parfums, feu de bois etc.) (H) est plus fréquente dans les gourbis que les appartements en villes (C) ; que la nourriture industrielle (N ) se consomme en conserve (O) ; que d'une manière géné-rale, les habitudes acquises à l'enfance se conservent à l'âge adulte ; que les produits toxiques (E) provoquent des maladies (I) etc. Nous avons de plus testé les qualités du clasifieur selon le principe du 10-fold cross validation. Après chaque apprentissage, on retrouve toujours la V-structure 1 ? 30 ? 31, l'inférence est alors immédiate et seules 30 et 31 doivent être renseignées. Le taux de réussite moyen est de 74% sur les données en test à comparer aux 51% d'individus atteints par le NPC, ce qui semble a priori un bon résultat. Toutefois, le NPC semble être la cause de la mauvaise aération cuisine à l'enfance et non l'inverse d'après l'orientation des arcs ! Après discussion avec l'expert, ces résultats curieux confirment surtout un biais dit de classement : les individus atteints du NPC (cancer des voies respiratoires) sont plus enclins à chercher les causes de leur maladie dans la mauvaise aération de leur habitat. Souffrant de difficultés respiratoires, ils ont une tendance à imputer à tort la cause de leur cancer à l'aération. Ce résultat curieux n'est donc pas à imputer à PMMS mais aux données elles-mêmes.
En conclusion, ce graphe nous renseigne, certes, sur le mode de vie des sujets maghrébins mais révèle par ailleurs des biais propres aux comportements psychologique des individus, à la façon dont ils comprennent (ou non) les questions. Malgré la pertinence des groupes de variables obtenus et leurs associations, il ne dit rien en revanche sur les "causes potentielles" du NPC, en gardant à l'esprit que, même en supposant l'hypothèse de suffisance causale et la fiabilité de la mesure d'association, il est impossible de déceler les relations causales à partir de données sans mener des expérimentations supplémentaires.

Introduction
Les moteurs de recherche classiques sur le web ont des caractéristiques étonnantes : ils possèdent des milliards de documents dans leur index, ils peuvent traiter des millions de requêtes quotidiennement, ils donnent des réponses très volumineuses quasiment en temps réel et ils nécessitent des ressources informatiques et humaines considérables. On peut dire aujourd'hui qu'il est pratiquement impossible de concevoir une approche alternative pour un moteur de recherche sans passer par l'un de ces « géants ». Même si les points forts de ces moteurs sont nombreux, ils ont aussi des faiblesses, comme des requêtes très simples, une présentation des résultats souvent pauvre en information, ou encore la nécessité pour l'utilisateur d'explorer un à un les nombreux liens qu'ils donnent en sortie.
En effet, lorsque l'on recherche une information précise ou spécialisée, comme par exemple trouver une entreprise répondant parfaitement à notre préoccupation du moment, les moteurs classiques se révèlent difficile d'usage et peu pertinents à la fois. Pour arriver à ses fins, l'internaute a alors le choix entre plusieurs types d'outils destinés à l'aider dans sa tâche. D'un côté, les méta-moteurs de recherche spécialisés, bien que de plus en plus sophistiqués, se heurtent à deux écueils principaux : le Web invisible et la masse gigantesque d'informations gérées. D'autre part, les annuaires professionnels représentent une alternative intéressante par une meilleure exhaustivité des données managées. Cet avantage est cependant tempéré par le cloisonnement des entités à l'intérieur de rubriques préétablies, manquant de discernement. Ces différentes solutions montrent leurs limites dans le manque d'efficacité en terme de pertinence.
Nous proposons dans cet article un outil de recherche spécialisé dans la recherche d'entreprises mêlant efficacité de formulation de la demande initiale et pertinence des résultats affichés. Afin d'assister le mieux possible l'internaute, ce nouveau moteur de recherche dispose d'une fonctionnalité de géo-localisation autorisant la restriction géographique de la requête et le repérage graphique des entreprises atteintes.
L'élaboration d'un moteur de recherche, à usage des entreprises, bâti sur une infrastructure Web sémantique constitue une nouvelle voie qu'il convenait d'exploiter. L'idée maîtresse est de donner une signification au contenu des documents présents sur le Web. De cette façon, les machines sont en mesure de comprendre le sens des documents et d'effectuer des raisonnements automatisés. La réalisation de ce moteur de recherche « intelligent » passe par la modélisation d'une ontologie. Celle-ci est nécessaire à la formalisation des connaissances du domaine, permettant leur partage et leur interprétation opérationnelle.
La suite de cet article est organisée comme suit ; la section 2 présente un état de l'art de différents systèmes de recherche d'information se basant sur le principe du web sémantique et mettant en oeuvre des ontologies. La section 3 détaille l'architecture retenue pour l'élaboration de notre moteur de recherche ainsi que la description des différentes ontologies permettant d'organiser l'ensemble des données accessible. La section 4 donne des résultats expérimentaux obtenus par comparaison au principal outil du genre : les Pages Jaunes. La section 5 conclut sur les nombreuses perspectives qui découlent de ce travail.
Systèmes de recherche d'information et ontologie
Le Web sémantique, proposé par le W3C (World Wide Web Consortium), est une nouvelle approche qui vise, à partir de la structure actuelle du Web, à donner un sens au contenu des pages. Selon Tim Berners-Lee, inventeur du Web et directeur du W3C, "The semantic Web is not a separate Web but an extension of the current one, in which information is given welldefined meaning, better enabling computers and people to work in cooperation" Burners-Lee et al. (2001). C'est une manière de donner un sens bien défini aux informations permettant une interprétation aussi bien par les machines, que par les humains.
Par ce biais là, l'objectif du Web sémantique est de décharger les utilisateurs d'une grande partie de leurs tâches de recherche et d'exploitation des résultats. Mais pour que le Web sé-mantique fonctionne il est nécessaire que les machines aient accès à des collections structurées d'informations et de règles d'inférence qu'elles peuvent utiliser pour parvenir à un raisonnement automatisé. Cette modélisation des données, appelée ontologie, est destinée à jouer un rôle clé car dispensant une connaissance commune, et partagée, du domaine. Le monde des sciences de l'information s'est approprié ce terme pour désigner "une spécification formelle et explicite des termes d'un domaine ainsi que des relations que ces termes entretiennent entre eux" Gruber (1993). Il est important de compléter cette définition en précisant qu'une "ontologie est indépendante des considérations d'exécution, son objectif principal étant de spécifier la conceptualisation du domaine sous-jacent à l'application" Welty et Guarino (2001).
Dans le cadre du Web sémantique, l'ontologie a ainsi pour enjeu de "proposer une compréhension partagée et commune, pour un domaine donné, qui peut être transmise aussi bien aux personnes qu'aux applications" Davies et al. (2002). Elle doit traduire un certain consensus, explicite, de manière à être partagée par la communauté l'ayant construite et acceptée. Ceci est vital pour permettre l'exploitation des ressources présentes sur le Web par différentes applications ou autres agents logiciels. Le Web sémantique doit ensuite ajouter de la logique, c'est-à-dire lui donner la possibilité d'utiliser les règles pour faire des inférences. Par l'exploitation de ces règles, les moteurs d'inférence peuvent raisonner intelligemment et offrir des réponses automatiques à des questions posées par une personne. Ils doivent être en mesure de déduire de nouvelles informations à partir d'informations et de ressources déjà existantes dans l'environnement.
Nous voyons apparaître depuis quelques années des outils exploitant ces données dans des cadres divers et notamment concernant les systèmes de recherche d'information. Les informations contenues dans les ontologies permettent de déterminer un sens non ambigu aux différents éléments rencontrés. Les résultats produits sont par conséquent plus pertinent que ceux issus des moteurs de recherche traditionnels se basant sur des considérations purement statistiques pour effectuer leurs recherches Lawrence et Giles (1999) ;Brin et Page (1998); Baeza-Yates et Ribeiro-Neto (1999).
Un des premiers domaine à avoir bénéficié des avancées des ontologies sur le Web est certainement celui des annuaires, tel Yahoo !. Pour ces applications, la localisation d'une information est assurée par un système de catalogues thématiques hiérarchiques (classification) consultables à l'aide de mots clés ou par navigation de thèmes en sous thèmes. Par exemple, Labrou et Finin (1999) utilise cette classification afin de décrire les documents à la manière d'une ontologie. L'annotation de chaque document permet de rendre le sens compréhensible par des outils automatique. En utilisant un mécanisme d'inférence, il devient alors possible d'améliorer la qualité des résultats produits par les moteurs de recherche sémantiques Mayfield et Finin (2003). En particulier Shah et al. (2002) utilise le texte sémantiquement enrichi afin de lever certaines ambiguïtés du texte libre et procède ensuite par inférence pour améliorer la qualité de l'indexation. Guha et al. (2003) a, quand à lui, développé un système de recherche d'information utilisant une ontologie afin d'améliorer les résultats des moteurs de recherche classiques en ajoutant des sources issus des concepts de l'ontologie associés aux résultats originaux.
Cette information sémantique peut aussi être utilisée afin d'affiner des mesures utilisés dans les algorithmes de scoring des moteurs de recherche. L'indexeur Swoogle Ding et al. (2004), Ding et al. (2005), découvre, indexe et analyse les ontologies des documents du web. Il utilise les informations sémantiques contenus dans les méta-données des documents afin de produire une mesure de similarité la plus pertinente possible. OntoSearch, Gao et al. (2005), analyse les méta-données afin de déterminer des poids dans un vecteur de concept pour chaque document. La pertinence des documents à la requête de l'utilisateur est alors mesurée en calculant une similarité entre vecteur de document et vecteur de requête.
Un moteur de recherche d'activités géo-localisées
Le domaine de la recherche d'informations est, en partie, lié aux langues que ce soit lors de l'interprétation d'une requête ou de l'analyse des documents traités. Il apparaît selon plusieurs auteurs (de Loupy (2000)), qu'un système de recherche d'informations devra, pour être efficace, conjuguer l'approche statistique avec un traitement linguistique. De nombreux problèmes de polysémie et de synonymie limitent en effet l'efficacité d'une recherche purement statistique par mots clés. Les relations sémantiques précédentes sont génératrices de non-conformité des résultats produits par une recherche. Une des solutions destinée à lever ces ambiguïtés séman-tiques est d'utiliser des liens thématiques de Loupy et Crestan (2004). La démarche consiste à regrouper les termes par affinités : par exemple, le domaine services informatiques pourrait regrouper les termes infogérance, développement et maintenance logiciels.
Afin de cataloguer et de gérer les ambiguïtés pouvant intervenir dans la formulation des activités, nous avons décidé de créer une ontologie de description de ce domaine de connaissance. A chaque activité recensée, nous associerons plusieurs termes synonymes nous permettant de lever une partie des problèmes de polysémie et de synonymie. Et de manière analogue nous organiserons la connaissance des entreprises dans une ontologie particulière.
La recherche d'une entreprise par son activité consistera donc à rechercher, dans un premier temps, l'activité recensée la plus proche de ce que recherche l'utilisateur en utilisant les termes synonymes introduits précédemment, puis de lister les références des entreprises dont au moins une activité y est associée.
Ontologie de description d'activités géolocalisée
Pour produire une ontologie relativement complète des différentes activités d'entreprises pouvant exister, nous nous sommes appuyés sur les données recueillies auprès des organismes officiels chargés d'enregistrer les déclarations de toute entreprise française : l'INSEE. Une initiative similaire à déjà été utilisée dans Marquet et al. (2003) afin d'unifier l'accès à des ressources médicales en se référant aux terminologies standard du domaine. Ces différentes structures, constituent une source capitale, dans le cadre du présent projet. Les nomenclatures économiques présentent l'avantage d'être maintenues par des experts reconnus du domaine ce qui certifie exhaustivité et qualité du vocabulaire employé. Elles comprennent aussi bien la liste complète des activités pouvant être exercées par les entreprises que les produits développés. En outre, toute unité économique exerçant en France est rattachée à la NAF, via le code APE.
La NAF et la CPF ont été élaborées dans un cadre européen, harmonisé, afin de clarifier l'information sur le marché unique européen. Elles sont organisées sur plusieurs niveaux hié-rarchiques : sections et sous-sections comme le montre la figure 1.
Les nomenclatures permettent le classement de toutes les activités économiques et de tous les produits (biens et services). Elles constituent un outil pour ordonner l'information éco-nomique mais proposent, aussi, un langage commun présentant un intérêt dans de nombreux domaines.
Le code APE représente l'activité principale exercée par l'entreprise ce qui correspond au code de la classe issue de la nomenclature française des activités. Dans l'hypothèse d'une entreprise exerçant plusieurs types d'activités, l'INSEE, par le biais d'une estimation statistique, détermine celle qui demeure prédominante. Il est important de souligner que, dans le cas d'une entreprise disposant de plusieurs établissements, chacun d'eux dispose d'un code APE.
Cette notion d'activité principale de l'entreprise est importante, notamment en droit, pour déterminer, par exemple, les champs d'application des conventions collectives.
Géo-localisation et description des entreprises
Les attributs caractéristiques d'une entreprise, sur le Web, se matérialisent par les pages constituant son site. En pratique, une entreprise peut être spécialisée dans plusieurs activités et plusieurs produits localisés sur plusieurs lieux. Il en résulte qu'une page donnée peut être liée à un ou plusieurs attributs du répertoire des activités. En outre, la construction d'un moteur de recherche à usage des entreprises implique que les sociétés retenues dans l'index ne possèdent pas nécessairement de site internet. Dans ce cas précis, une adresse Email de contact sera prise en compte pour référencer l'activité ou le produit concerné. On peut imaginer une entité commerciale exploitant plusieurs activités avec, pour chacune d'elles, un responsable possédant une adresse Email. L'entité de l'entreprise prise en compte dans notre organisation suit alors l'organisation du répertoire SIRENE de l'INSEE. Ce répertoire se base sur la notion d'unité administrative dans laquelle un établissement est localisé géographiquement et rattaché à une unité légale (entité juridique déclarée aux administrations compétentes) qui est elle-même rattachée à un groupe financier. Le numéro de SIREN identifie alors de manière unique une unité légale tandis que le numéro SIRET l'établissement en tant qu'unité géographiquement localisée.
La structure du répertoire SIRENE par son organisation spécifique va répondre de façon satisfaisante à notre problématique de géo-localisation. En effet, la notion d'établissement, vue par l'INSEE comme une unité géographiquement localisée, via le code SIRET, nous garantit le référencement géographique, sans ambiguïté, de ce type d'unité administrative.
Organisation de l'ontologie
Les différentes classes constituant l'ontologie (décrite dans la figure 2) répondent, chacune d'elles, à un ou des services spécifiques mis en évidence durant la modélisation. Ceux-ci peuvent être classés à l'intérieur de quatre catégories principales : 1) la classification des produits, 2) la nomenclature des activités, 3) l'organisation interne de l'entreprise et 4) l'organisation géographique. Si les deux premières catégories s'imposent de façon triviale, car issues directement du modèle INSEE, les deux suivantes ont été constituées pour mieux structurer l'ontologie définitive. Ces quatre ensembles peuvent être considérés comme des sous-ontologies que nous utiliserons respectivement dans le but de 1. rechercher les concepts liés à un mot clé ; généraliser, spécialiser un concept lié à une activité ou un produit ; récupérer les produits associés à une activité particulière ; 
Architecture du moteur de recherche
L'architecture retenue en vue de la réalisation du démonstrateur envisagé s'appuiera sur la plate-forme Sesame Broekstra (2005) associée à une base de données MySql. Cette solution présente l'avantage d'être robuste en terme de volumétrie ce qui convient pour la partie moteur de recherche. Le langage de requêtage exploité sera RDQL qui permet de traiter la représenta-tion de l'ontologie comme une base de données. Il autorise l'exécution de requêtes complexes, utilisant des modèles et des contraintes sur les triplets RDF, tout en permettant des jointures.
Pour permettre à l'utilisateur de retrouver facilement l'information qu'il recherche, notre système doit être capable 1) de récupérer rapidement les concepts liés aux mots clés de l'utilisateur, 2) d'offrir un module de désambiguïsation sémantique, 3) de permettre la navigation dans l'ontologie et 4) d'afficher des résultats sous la forme définitive (raison sociale, descriptif entreprise, référence professionnelle).
Le système propose à l'utilisateur de saisir sa requête à l'aide de 2 champs : le Quoi, obligatoire permet de préciser les mots clés d'activité à rechercher, et le Où, facultatif, définissant la limitation géographique concernée. Les mots-clés saisis par l'utilisateur feront l'objet d'une désambiguïsation sémantique. La composante géographique sera limitée à la saisie d'un ou plusieurs codes postaux.
La récupération des concepts, à partir des mots clés saisis par l'utilisateur, demeure une phase fondamentale du processus. Celle-ci, outre le fait de rechercher les activités et produits rattachés à une liste de mots clés, se charge de supprimer, de factoriser et de définir un ordre de pertinence sur les concepts.
FIG. 3 -Interface de l'application présentant l'écran de sélection des concepts pour la désa-mbiguïsation.
Suppression des concepts généraux Ce traitement consiste à ne considérer que le concept le plus spécialisé en cas de concurrence sur une même branche de l'arbre de concepts. Cela permet préciser au mieux la requête de l'utilisateur en éliminant les termes généraux.
Factorisation des concepts
La factorisation a pour objectif de ne sélectionner que les concepts les plus généraux en cas de concurrence sur deux branches d'un même sous-arbre. Lorsque la distance hiérarchique entre le concept général et les deux concepts fils est très proche (1 à 2 branches traversées), il peut être judicieux de résumer la pensée de l'utilisateur par le concept général.
Ordre de pertinence des concepts Pour donner un ordre de pertinence aux concepts sélec-tionnés, nous déterminons un poids représentatif du nombre de mots clés issus de la requête utilisateur associés à chaque concept. Ceux dont le poids est le plus important apparaissent alors en tête de la liste proposée par l'écran de désambiguïsation sémantique.
Lorsqu'un mot clé saisi par l'utilisateur correspond à plusieurs concepts dans l'ontologie, nous proposons de manière interactive un module de désambiguïsation sémantique. Il appartient alors à l'utilisateur de déterminer dans quel champ d'application il souhaite réaliser sa recherche. La liste des concepts associée sera affichée en tenant compte de leur pertinence respective (taux de mots clés associés). Une sélection multiple sera admise.
De plus, nous permettons dans ce cas à l'utilisateur de naviguer par spécialisation / généra-lisation dans la hiérarchie des concepts afin de préciser au mieux sa pensée. La figure 3 illustre l'interface de notre application comprenant les champs Quoi et Où, la zone de résultat et la fenêtre de désambiguïsation.
Premiers résultats
Méthodologie de test
Pour valider notre méthode, nous avons entrepris de construire un jeu d'essaie réduit à une région particulière. Nous avons ensuite étudié les réponses obtenues aux différentes questions proposées dans la Lors de cette phase de test, nous n'avons pas mis en oeuvre le module de désambigüa-tion afin de ne pas avantager notre modèle en guidant plus précisément l'utilisateur dans sa formulation de mots clés.
Analyse des résultats
L'ensemble des résultats obtenus par notre prototype et les Pages Jaunes sont donnés en terme de précision/rappel dans le tableau 2. Afin de valider la pertinence du classement des résultats retournés par chaque moteur, nous avons évalué la précision et le rappel pour chaque requête en ne considérant dans un premier temps que les 5, puis les 10, puis 20, puis 30 premiers résultats.
Il est évident que dans les résultats obtenus, notre approche apporte systématiquement une meilleure précision et un meilleur rappel que les Pages Jaunes. Ceci est du en particulier aux opérations de suppression et de factorisation de concepts permettant de cibler d'avantage le souhait d'un utilisateur. On peut toutefois noter que l'annuaire testé fait apparaître une qualité hétérogène des réponses obtenues. Dans certains cas, les résultats pertinents du corpus sont pris en compte mais noyés dans l'ensemble des réponses. Dans d'autres exemples, certains éléments corrects ne sont pas considérés dans les résultats. À travers les exemples traités, deux grandes familles de mots-clés se dégagent : les motsclés directement rattachés à une activité ou un produit, ce qui induit une forte adhérence avec les nomenclatures de type NAF (ex : restaurant, logiciel) ; les autres mots-clés qualifiant un terme commun généraliste (ex : agent, dimanche). Il apparaît, clairement, que l'annuaire est performant tant qu'il s'agit d'interpréter des mots-clés liés à une activité ou un produit. Son architecture repose, manifestement, sur les nomenclatures de type NAF et les mots-clés associés. En revanche, l'annuaire est dans l'incapacité de traduire des mots-clés de la deuxième famille. Cette lacune se traduit par un manque de précision pouvant conduire à des excès en terme de bruit et de silence. Par opposition, le prototype, construit sur une ontologie, est capable d'interpréter tout type de mots-clés. Ceci suppose que les concepts, toutefois en partie basés sur les nomenclatures activités et produits, soit sémantiquement affectés aux mots-clés indépendamment de la famille d'appartenance.
Il est également important de souligner que le prototype permet une sélection géographique régionale ce que ne permet pas l'annuaire testé. Pour parvenir à ce résultat, il a été nécessaire d'implémenter plusieurs stratégies. Tout d'abord, il était important que la modélisation de l'ontologie tienne compte des trois caractéristiques suivantes : dualité des mots-clés traités, sépa-ration concepts concernés / classement des résultats et relation entreprise / lieu géographique. Et il a été également essentiel de mettre à jour l'ontologie en associant les mots-clés aussi bien du côté des nomenclatures activités et produits que du côté des entreprises indexées.
Conclusion
Nous avons décrit dans cet article un nouveau moteur de recherche géo-localisé à usage des entreprises. La méthodologie que nous avons adoptés, issue du web sémantique, nous a permis d'améliorer significativement l'efficacité de solutions du domaine largement répandu comme les Pages Jaunes, en intégrant une ontologie. La phase de modélisation de l'ontologie a mise en évidence l'intérêt des structures économiques maintenues par l'INSEE. Les nomenclatures économiques ont été retenues pour gérer la classification des activités et produits pouvant être dispensés par les entreprises tandis que la structure des unités administratives, telle que gérée au sein du fichier SIRENE, s'est avérée judicieuse pour réponse à la problématique de géo-localisation des entreprises.
Nous avons élaboré un démonstrateur mettant en oeuvre différentes stratégies se servant de l'ontologie afin de guider l'utilisateur dans la formulation de sa requête. Nous envisageons de poursuivre dans ce sens en expérimentant notre prototype sur des bases de données plus importantes et en menant des études afin de déterminer plus précisément l'amélioration qu'apporte ce système de feed-back. Actuellement, la base de travail est alimentée manuellement et il serait également intéressant de profiter de la connaissance présente dans l'ontologie afin d'introduire une indexation automatique plus pertinente que celle réalisée par les moteurs de recherche classiques. Le prototype réalisé dans ce papier sert de base à l'élaboration d'un outil de recherche géo-localisé plus complet (Géoternet) développé par la société IP&moteur.

Introduction
Grâce à des techniques récentes de traitement de la parole, de nombreux centres d'appels téléphoniques automatisés voient le jour. Ces serveurs vocaux permettent aux utilisateurs d'exécuter diverses tâches en dialoguant avec une machine. Les entreprises cherchent à amé-liorer la satisfaction de leurs clients en les redirigeant en cas de difficulté vers un opérateur humain. L'aiguillage des utilisateurs mécontents revient à détecter les émotions négatives dans leurs dialogues avec la machine, sous l'hypothèse qu'un problème de dialogue génère un état émotionnel particulier chez le sujet.
La détection d'émotions dans la parole est généralement traitée comme un problème d'apprentissage supervisé. Cela s'explique par le fait que les descripteurs utilisés sont relativement éloignés du concept d'émotion, dans la pratique l'étiquetage d'exemples s'avère nécessaire. La détection d'émotions se limite généralement à une classification binaire, la prise en compte de labels plus fins pose le problème de l'objectivité de l'étiquetage (Liscombe et al., 2005). Dans ce cadre, les données sont coûteuses à acquérir et à étiqueter. L'apprentissage actif peut diminuer ce coût en étiquetant uniquement les exemples jugés informatifs pour le modèle.
Cet article propose une approche d'apprentissage actif pour la redirection automatique d'appels. La première section présente le contexte de l'étude ainsi que les données utilisées. Les différentes stratégies d'apprentissage envisagées ainsi que le modèle utilisé sont traités dans la section 2. La dernière section est consacrée aux résultats obtenus et à leur discussion.
Classification d'émotions : caractérisation des données
Cet article se base sur des travaux antérieurs (Poulain, 2006) cherchant à caractériser au mieux des échanges vocaux en vue d'une classification d'émotions. Le but est de réguler le dialogue entre des utilisateurs et un serveur vocal. Cette étude porte plus particulièrement sur la pertinence des variables décrivant les données par rapport à la détection d'émotions.
Les données utilisées sont issues d'une expérience mettant en jeu 32 utilisateurs qui testent un service boursier fonctionnant sur un serveur vocal. Du point de vue de l'utilisateur, le test consiste à gérer un portefeuille fictif d'actions, le but étant de réaliser la plus forte plus value. Les traces vocales obtenues constituent le corpus de cette étude, soit 5496 "tours de parole" échangés avec la machine. Les tours de parole sont caractérisés par 200 variables acoustiques, décrivant notamment la variation du volume sonore, la variation de la hauteur de voix, le rythme d'élocution. Les données sont également caractérisées par 8 variables dialogiques décrivant notamment l'âge du locuteur, le rang du dialogue, la durée du dialogue. Chaque tour de parole est étiqueté manuellement comme étant porteur d'émotions positives ou négatives.
Le sous-ensemble des variables les plus informatives vis-à-vis de la détection d'émotions est déterminé grâce à un sélecteur bayésien naïf (Boullé, 2006). Au début de ce procédé, l'ensemble des attributs est vide, à chaque itération on ajoute l'attribut qui améliore au plus la qualité prédictive du modèle. L'algorithme s'arrête lorsque l'ajout d'attributs n'améliore plus la qualité du modèle. Finalement, 20 variables ont été sélectionnées pour caractériser les échanges vocaux 1 . Dans le cadre de cet article, les données utilisées sont issues du même corpus et de cette étude antérieure. Chaque tour de parole est donc caractérisé par 20 variables. • L'ensemble d'apprentissage T constitué de couples "instance, étiquette" notés (u, f (u)).
• La fonction Utile : X × M ? ? qui estime l'utilité d'une instance pour l'apprentissage.
Algorithme 1: échantillonnage sélectif, Muslea (Muslea, 2002) 3 Classification active d'émotions
Introduction
La mise en oeuvre d'un système automatique de détection d'émotions requiert générale-ment l'entraînement d'un classifieur. Ici, le modèle qui va classifier les émotions est réalisé grâce à un processus d'apprentissage actif. A la différence de l'apprentissage passif, qui utilise un ensemble de données déjà étiquetées, l'apprentissage actif permet au modèle de construire lui-même son ensemble d'apprentissage au cours de son entraînement. Parmi les stratégies d'apprentissage actif existantes (Castro et Nowak, 2005), on se place dans le cadre de l'échan-tillonnage sélectif où le modèle dispose d'un "sac" d'instances non étiquetées dont il peut demander les labels.
Muslea (Muslea, 2002) a formalisé de manière générique l'échantillonnage sélectif à travers l'Algorithme (1). Celui-ci met en jeu la fonction Utile(u, M) qui estime l'intérêt d'une instance u ? U x pour l'apprentissage du modèle M. La problématique centrale de l'apprentissage actif est de "préjuger efficacement" de l'intérêt des exemples avant de les étiqueter.
Le choix du modèle
La grande variété des modèles capables de résoudre des problèmes de classification et parfois le grand nombre de paramètres nécessaires à leur utilisation rend souvent l'apport d'une stratégie d'apprentissage difficile à mesurer. On choisit d'utiliser une fenêtre de Parzen à noyau gaussien et de norme L2 (Parzen, 1962) car ce modèle prédictif n'utilise qu'un seul paramètre et est capable de fonctionner naturellement avec peu d'exemples. La "sortie" de ce modèle est une estimation de la probabilité d'observer l'étiquette y i conditionnellement à l'instance u n :
La valeur optimale (? 2 =0.24) du paramètre du noyau a été déterminée grâce à une crossvalidation sur l'erreur quadratique moyenne (Chappelle, 2005). Cette valeur est utilisée par la suite pour fixer le paramètre de la fenêtre de parzen.
Pour que le modèle puisse affecter une étiquettê f (u n ) à l'instance u n , un seuil de décision noté Seuil(L x ) est calculé. Ce seuil minimise l'erreur de prédiction 2 sur l'ensemble d'apprentissage. L'étiquette attribuée estˆfestˆ estˆf (u n ) = 1 si { ˆ P (y 1 |u n ) > Seuil(L x )}, etˆfetˆ etˆf (u n ) = 0 sinon. Puisque le seul paramètre de la fenêtre de Parzen est fixé, l'apprentissage du modèle se réduit au "comptage" des instances (au sens du noyau gaussien). Cela permet de comparer uniquement les stratégies de sélection d'exemples sans être influencé par l'apprentissage du modèle.
Deux stratégies d'apprentissage actif
La première stratégie d'apprentissage actif proposée a pour but de réduire l'erreur de géné-ralisation du modèle, cette erreur peut être estimée par le risque empirique (Zhu et al., 2003).
Ici, le risque R(M)est définit comme étant la somme des probabilités que le modèle prenne une mauvaise décision sur l'ensemble d'apprentissage. On note P (y i |l n ) la probabilité réelle d'observer la classe y i pour l'instance l n ? L x . Le risque empirique s'écrit alors selon l'équa-tion 2, avec ½ la fonction indicatrice égale à 1 si f (l n ) = y i et égale à 0 sinon. La fenêtre de parzen estime P (y i |l n ), on peut donc approximer le risque empirique en adoptant un apriori uniforme sur les P (l n ) (voir équation 3). Le but de cette stratégie est de sélectionner l'instance non étiquetée u i ? U x qui minimisera le risque à l'itération t + 1. On estime R(M +un ) le risque "attendu" après l'étiquetage de l'instance u n . Pour se faire, on se base sur les données étiquetées dont on dispose et on suppose que
. L'équation 4 montre comment agréger les estimations de risque selon les probabilités d'observer chacune des classes. Pour exprimer la stratégie de réduction du risque sous forme algorithmique, il suffit de remplacer l'étape (B) de l'algorithme 1 par : "Rechercher l'instance q = argmin u?UxˆRu?Uxˆ u?UxˆR(M +un )".
La deuxième stratégie d'apprentissage consiste à choisir l'instance pour laquelle la prédic-tion du modèle est la plus incertaine possible. On considère que l'incertitude d'une prédiction est maximale quand la probabilité de sortie du modèle se rapproche du seuil de décision (voir équation 5). L'algorithme correspondant à cette stratégie s'obtient en remplaçant l'étape (B) de l'algorithme 1 par : "Rechercher l'instance q = argmax u?Ux Incertain(u n )".
En dehors de ces deux stratégies actives, une approche "stochastique" sélectionne uniformément les exemples selon leur distribution de probabilité. Cette dernière approche est notre juge de paix et tient lieu de référence pour mesurer l'apport des stratégies actives. 4 . Selon les résultats de la figure 1, la "réduction du risque" est la stratégie qui maximise la qualité du modèle. En étiquetant 60 exemples grâce à cette approche, la performance du modèle est très proche du BER asymptotique 5 (l'écart n'est que de 0.04). La stratégie de "maximisation de l'incertitude" n'est pas performante au début de l'apprentissage. Il faut étiqueter 180 exemples pour que cette stratégie donne de meilleurs résultats que l'approche "stochastique". Cette approche à l'avantage d'être rapide, pour 300 exemples étiquetés on observe une performance comparable à la "réduction du risque", avec un temps de calcul 5 fois moins important.
Résultats
Résultats et Discussion
La figure 2 compare les performances d'un modèle passif entraîné sur la totalité de l'ensemble d'apprentissage et de deux modèles actifs entraînés sur 100 exemples. Les résultats sont présentés sous la forme de courbes de lift réalisées sur l'ensemble de test. Ces courbes montrent la proportion d'émotions "négatives" détectées par le modèle, en considérant une certaine proportion de la population totale. Par exemple, le modèle entraîné grâce à la minimisation du risque détecte 74% des émotions négatives en utilisant 20% de la population totale (voir point "A" de la figure 2). La "maximisation de l'incertitude" permet de détecter plus efficacement les émotions négatives que la "réduction du risque" (ce résultat ne prend pas en compte le taux de fausses alertes). Les deux modèles actifs offrent des performances proches de celle du modèle passif, en utilisant 37 fois moins d'exemples d'apprentissage. 
Conclusion et perspectives
Cet article montre l'intérêt de l'apprentissage actif pour un domaine où l'acquisition et l'étiquetage des données sont particulièrement coûteux. Au vu des résultats obtenus lors de nos expériences, l'apprentissage actif est pertinent pour la détection d'émotions dans la parole.

Introduction
Avec le développement des bibliothèques électroniques, il est devenu nécessaire de concevoir des méthodes automatiques pour la recherche de données pertinentes par rapport à une requête donnée. Pour de telles applications, il s'agit plus d'ordonner les exemples que de les discriminer.
La communauté d'apprentissage a formulé cette problématique à travers le nouveau paradigme d'apprentissage supervisé de fonctions d'ordonnancement. Dans ce cas, il s'agit d'apprendre une correspondance entre un ensemble d'instances et un ensemble d'alternatives capable d'ordonner les alternatives par rapport à une instance donnée. Par exemple, dans le cas de la recherche documentaire (RD), une instance représente une requête et les alternatives sont les documents concernés par cette requête et le but est d'inférer un ordre partiel sur l'ensemble des alternatives de façon à ce que les documents pertinents par rapport à la requête soient mieux ordonnés que les documents non-pertinents.
Dans ce papier, nous nous plaçons dans le cadre de l'ordonnancement bipartite dans lequel les instances sont soit positives soit négatives et où il s'agit d'ordonner les instances positives au-dessus des instances négatives. Ce cadre restreint englobe de nombreuses applications de la recherche d'information telle que le résumé automatique  ou la recherche de passages pertinents dans les systèmes de questions/réponses  et a ré-cemment fait l'objet de plusieurs études aussi bien sur un plan pratique que théorique (Agarwal et Roth (2005); Rudin et al. (2005); Freund et al. (2003)).
Le principal inconvénient de l'apprentissage supervisé de fonctions d'ordonnancement est que l'étiquetage des instances nécessite l'intervention d'un expert qui doit examiner manuellement une grande quantité de données. Dans le cadre de la classification, la communauté d'apprentissage s'est intéressée depuis la fin des années 90 au problème d'apprentissage semisupervisé qui consiste à prendre en compte les données étiquetées et non-étiquetées dans le processus d'apprentissage. L'originalité de notre approche est que nous proposons un algorithme d'apprentissage semi-supervisé pour la tâche d'ordonnancement bipartite. La plupart des algorithmes d'ordonnancement semi-supervisés sont des techniques transductives à base de graphes, qui permettent d'étiqueter les exemples non-étiquetés d'une base test fixe. Nous préconisons une approche inductive à ce problème où il s'agit d'apprendre une fonction d'ordonnancement à partir de deux bases d'apprentissage, étiquetée et non-étiquetée, et qui est capable d'ordonner de nouveaux exemples qui n'ont pas été utilisés pour entraîner le modèle. Notre algorithme adopte une approche itérative en initialisant d'abord une fonction d'ordonnancement à partir des exemples étiquetés de la base d'apprentissage et en apprenant la structure des données non-étiquetées de la base d'apprentissage par une méthode transductive. Il répète ensuite deux étapes jusqu'à ce que les critères de convergence ou d'arrêt soient atteints. La première étape consiste à ordonner un sous-ensemble d'exemples non-étiquetés avec la sortie de la fonction d'ordonnancement et ensuite à calculer une dissimilarité entre cet ordre et celui inféré par la méthode transductive sur ce sous-ensemble. Dans la deuxième étape, l'algorithme apprend une nouvelle fonction d'ordonnancement à partir de l'ensemble des données étiquetées et du sous-ensemble d'exemples non-étiquetés trouvé à l'étape précédente. Nous montrons l'efficacité de cette approche pour la tâche RD.
Dans ce qui suit, nous reviendrons en section 2 à la tâche d'ordonnancement bipartite dans le cas supervisé. Dans la section 3 nous présenterons notre algorithme d'ordonnancement semi-supervisé et dans la section 4, nous présenterons les résultats obtenus sur la base CACM 1 constituée des titres et des résumés du journal Communications of the Association for Computer Machinery. Finalement nous discuterons des résultats obtenus en section 4.2.  
Où [[pr]] est la fonction indicatrice valant 1 si le prédicat pr est vrai et 0 sinon. L'erreur moyenne d'ordonnancement R D1,D?1 (h) est la probabilité qu'un exemple positif échantillonné aléatoirement suivant D 1 ait un score plus faible qu'un exemple négatif échantillonné aléatoi-rement suivant D ?1 (Cortes et Mohri, 2003). L'erreur empirique d'ordonnancement correspondante de h sur une base d'apprentissage S = (S 1 , S ?1 ) est (Freund et al. (2003)) 
Il a été démontré que dans le cas où lim (x,x )?(0,0) C g (x, x , h) = 1, minimiser (2) revient à minimiser (1) (Bartlett et Long, 1998;Clémençon et al., 2005). Nous allons présenter dans la section suivante l'algorithme supervisé LinearRank optimisant le critère (2). Cet algorithme a été appliqué avec succès à la tâche de résumé automatique de textes .
L'algorithme supervisé LinearRank
Nous cherchons ici à apprendre les poids B = (
. L'apprentissage de la fonction score h revient alors à trouver les poids B = (? i ) qui optimisent le critère
Un avantage d'utiliser un coût exponentiel pour C g (x, x , h) et une fonction score linéaire est que le critère (3) peut se calculer avec une complexité linéaire par rapport au nombre Apprentissage semi-supervisé de fonctions d'ordonnancement d'exemples. En effet, le critère L exp peut s'écrire dans ce cas comme suit :
Un autre intérêt de la fonction de coût exponentiel est que des algorithmes d'optimisation standard permettent d'effectuer sa minimisation. Dans notre cas nous avons utilisé l'algorithme LinearRank  qui est une adaptation de l'algorithme iterative scaling déve-loppé pour la classification par (Lebanon et Lafferty, 2001). 
Méthode d'ordonnancement transductive
Les méthodes semi-supervisées qui ont été proposées en ordonnancement bipartite sont basées sur une hypothèse de variétés (Zhou et al., 2004b;Chu et Ghahramani, 2005;Agarwal, 2006). Une variété peut être définie comme un espace topologique qui est localement euclidien. Par exemple, toute ligne dans un espace euclidien est une variété de dimension 1 et toute surface constitue une variété de dimension 2. Les applications des variétés sont nombreuses en mathématiques et en physiques et ont été récemment introduites en apprentissage, principalement pour la tâche de discrimination. Les méthodes utilisant la notion de variété supposent que les exemples se trouvent sur une variété de dimension inférieure à l'espace de départ et que les scores des exemples proches sur la variété sont assez similaires.
Ces algorithmes cherchent alors à exploiter la nature intrinsèque des données (c-à-d une variété) pour améliorer l'apprentissage de la fonction de décision. Par exemple, les méthodes semi-supervisées faisant l'hypothèse de variétés utilisent la grande quantité d'exemples nonétiquetés pour pouvoir estimer cette structure. Pour ce faire, un graphe incorporant l'information de voisinage local est construit avec une méthode telle que les K plus proches voisins. Les noeuds sont alors constitués des exemples étiquetés et non-étiquetés de la base d'apprentissage et les poids reflètent la similarité entre les exemples voisins. La définition de cette similarité dépend des algorithmes proposés.
Après avoir estimé la variété, la plupart de ces méthodes s'attachent à trouver les étiquettes des exemples non-étiquetés en exploitant directement le graphe en propageant par exemple les étiquettes des données étiquetées à leurs voisins non-étiquetés (Zhou et al., 2004a). Ces algorithmes ne peuvent ainsi pas étiqueter les exemples absents de la phase d'apprentissage puisqu'ils ne font pas parti des noeuds du graphe. Ces méthodes sont dites transductives par opposition aux méthodes inductives, qui sont capables d'ordonner d'autres exemples que ceux qui ont été utilisés pour apprendre.
Récemment, des méthodes transductives à base de graphes ont été adaptées à la tâche d'ordonnancement bipartite. Par exemple, (Zhou et al., 2004b) a adapté ses travaux de classification semi-supervisée (Zhou et al., 2004a) au cas d'ordonnancement transdutif. Son algorithme construit d'abord un graphe valué et non orienté en connectant petit à petit les points les plus proches jusqu'à ce que le graphe devienne connexe. Il affecte ensuite un score pour chacune des instances, 1 pour les instances positives et 0 pour les autres. Les scores sont alors propagés à travers le graphe jusqu'à la convergence. À la fin, les scores obtenus permettent d'induire un ordre sur l'ensemble des instances non-étiquetées. L'algorithme proposé dans ce papier étant en partie basé sur cette méthode, nous allons le décrire plus en détails :
Soit d : X × X ? R une métrique sur X et soit f la fonction score qui donne à chaque instance x i un score noté f i . f peut alors être vue comme un vecteur 
2 avec D la matrice diagonale telle que d ii est égal à la somme des éléments de la i ème ligne de W . -Construire le vecteur y telle que y i = 1 si x i est une instance positive, 0 sinon.
Comme préconisé par (He et al., 2004), nous avons utilisé la méthode des K plus proches voisins dans la construction du graphe pour avoir plus de connexions entre les instances. En effet, la base CACM utilisée dans nos expériences comporte peu d'exemples positifs pour chaque requête. Augmenter le nombre de connexions permet ainsi d'augmenter l'influence des exemples étiquetés positifs sur les scores des exemples non-étiquetés.
Le modèle semi-supervisé inductif
La méthode supervisée LinearRank est une technique inductive dans le sens où, une fois le critère (3) optimisé, elle est capable d'ordonner les instances non vues durant la phase d'apprentissage. La méthode transductive quant à elle exploite la structure des données pour ordonner les instances en se basant sur leur similarité par rapport aux exemples positifs. Dans ce papier, nous nous intéressons à combiner les deux méthodes pour profiter de chacun de leurs avantages. Notre approche consiste à trouver un compromis entre optimiser le coût exponentiel (3) et respecter l'ordre trouvé à partir de la variété sur un sous-ensemble de données étiquetées.
Notre approche consiste dans un premier temps à apprendre (1) une fonction score h avec l'algorithme LinearRank en minimisant le coût exponentiel sur l'ensemble des données étique-tées et (2) un ordre total sur les données non-étiquetées avec la méthode transductive décrite dans la section précédente. La partie itérative de notre algorithme répète alors deux étapes jusqu'à ce que le critère de convergence ou qu'un nombre maximum d'itérations soit atteint (algorithme 2) : la première étape consiste à sélectionner les n instances non-étiquetées les mieux ordonnées par la sortie de la fonction h. Suite à cela, nous calculons une dissimilarité entre l'ordre trouvé par la fonction h et celui trouvé par la méthode transductive. À cette étape, nous faisons l'hypothèse que l'ordre sur ce sous-ensemble trouvé par la méthode transductive est plus pertinente que celle trouvée par la fonction h : la méthode transductive exploite en effet la structure des données. Nous définissons la dissimilarité entre les deux ordres par le nombre de paires de préférence différentes :
j=1 avec ?(j) une fonction qui retourne l'index de l'instance ordonnée au rang j par la méthode transductive. Dans une deuxième étape nous cherchons à trouver une nouvelle fonction score qui minimise le coût exponentiel régularisé L exp (S, h) + ?? exp (h, ?), où :
j=1 avec ? exp qui est la borne supérieure de (5). Le terme de régularisation ? permet de pondérer l'apport des données non-étiquetées dans l'apprentissage et il est fixé pour toutes les itérations de notre algorithme. Nous essayons ainsi de trouver une fonction d'ordonnancement qui minimise le nombre de couples de préférence mal ordonnées et qui donne un ordre sur les instances non-étiquetées les mieux ordonnées le plus proche de celui trouvé par la méthode transductive.
L'algorithme général peut ainsi résumer par ce qui suit :
) -Apprendre un ordre total sur les exemples non-étiquetés avec la méthode transductive de l'algorithme 1 -t ? 0 répéter -Sélectionner les exemples non-étiquetés S (t)
unl les mieux ordonnés par h (t) . -Produire la fonction index ? (t) à partir de la méthode transductive. -Apprendre une nouvelle fonction score en optimisant le coût exponentiel régularisé
Dans ce papier, nous avons utilisé la fonction coût exponentielle ainsi qu'une mesure de dissimilarité de même nature. D'autres fonctions de coût et de dissimilarité sont néanmoins envisageables en utilisant d'autres fonctions convexes qui bornent la fonction indicatrice ( le logit par exemple). Un travail similaire au nôtre est celui de (Agarwal, 2006) qui a récemment proposé d'étendre les travaux de (Belkin et Niyogi, 2004) au cadre de l'ordonnancement bipartite semi-supervisé. L'étude de (Agarwal, 2006) concerne plus un cadre d'ordonnancement transductif mais l'auteur propose d'utiliser la technique développée dans (Sindhwani et al., 2005) pour rendre l'algorithme inductif. Notre algorithme est inductif dans sa construction itérative ce qui présente l'avantage d'être plus rapide à l'exécution.
Expériences
Base utilisée
Pour montrer de façon empirique que les instances non-étiquetées peuvent être utiles pour l'ordonnancement bipartite, nous avons comparé notre algorithme à l'algorithme supervisé LinearRank . Pour évaluer ces méthodes, nous nous sommes basés sur deux critères couramment utilisés dans la communauté de recherche d'information : l'aire sous la Courbe ROC (AUC) et la précision moyenne. Les expériences ont été menées sur la base CACM qui rassemble les titres et les résumés provenant du journal Communications of the Association for Computer Machinery (CACM).
Nous avons dans un premier temps prétraité les données pour chaque requête. Nous avons ainsi retiré pour chaque requête les documents pertinents ne contenant aucun mot de la requête. À partir de l'ensemble des documents obtenu, un ensemble de test a été crée aléatoire-ment en sélectionnant la moitié des documents. Nous avons gardé uniquement les requêtes qui contenaient suffisamment de documents pertinents dans la base d'apprentissage et de test. 12 requêtes ont été alors retenues.
Pour chaque requête, nous avons évalué les méthodes en formant cinq bases d'apprentissage et de test différentes. La méthode semi-supervisée a été appliquée sur toute la base d'apprentissage (étiquetée et non-étiquetée) en fixant un taux de données étiquetées permettant d'avoir au moins une instance positive dans la partie étiquetée. Pour connaître l'apport des données non-étiquetées, nous avons entraîné le modèle supervisé uniquement sur cet ensemble étiqueté. Les résultats obtenus en fixant les paramètres ? à 1, n et K à 10 ont été reportés dans le tableau 1. Les valeurs de test correspondent aux moyennes des résultats en AUC et en pré-cision moyenne sur les 5 bases d'apprentissage et test considérées. Le tableau 2 donne les résultats moyennés par rapport aux requêtes.
Résultats et discussion
Les résultats obtenus sur la précision moyenne montrent que le modèle supervisé obtient de meilleurs performances que celui semi-supervisé. Ce résultat peut s'expliquer par le fait que notre algorithme n'optimise pas ce critère. Cependant le critère AUC a été de nombreuses fois utilisé en Recherche d'Information. En effet, ce critère est plus facile à optimiser que la précision moyenne et plusieurs études ont montré que ces deux critères étaient en général fortement corrélés (Caruana et Niculescu-Mizil, 2004). Optimiser le critère AUC permet ainsi d'optimiser la précision moyenne. Dans notre cas, le déséquilibre pourrait expliquer en partie les résultats que l'on obtient. En effet, pour chaque requête, il existe un nombre très limité d'exemples positifs. Pour améliorer ce critère, nous aurions pu aussi optimiser directement la précision moyenne (Metzler, 2005) ou un critère dérivé de l'AUC (Rudin et al., 2005), qui permet à l'algorithme de se concentrer sur les instances ordonnées de la liste.
Par contre, les résultats obtenus sur la mesure AUC montrent que notre méthode semisupervisée obtient clairement de meilleures performances sur l'ensemble des requêtes. Nous notons néanmoins une baisse conséquente pour la requête 17. En regardant de plus près les ré-sultats, nous avons remarqué que cette requête contient très peu de mots, ce qui pourrait ainsi biaiser la dissimilarité basée sur la variété. Néanmoins, nous obtenons des gains importants pour plus de la moitié des requêtes. En moyenne, l'approche semi-supervisée permet ainsi un gain d'environ 4, 2%.
Les résultats empiriques obtenus sur l'AUC montrent des résultats plus qu'encourageant. En effet, notre méthode cherche à améliorer ce critère en utilisant des exemples non-étiquetés. Cependant, les résultats sur la précision moyenne sont plus surprenants. La baisse des performances sur ce critère que l'on observe en moyenne et pour une grande partie des requêtes tempère les résultats obtenus sur l'AUC. 
Conclusion
La principale contribution de ce papier est une méthode d'ordonnancement bipartite semisupervisée inductive. Notre approche est une combinaison d'une méthode supervisée et d'une méthode transductive à base de graphe. Elle est générale dans le sens où d'autres fonctions coût que le coût exponentiel et d'autres méthodes transductives peuvent être utilisées. Les résultats obtenus sur le critère AUC montrent une amélioration significative par rapport à la méthode supervisée, tendant à montrer ainsi l'apport possible des exemples non étiquetés. Cependant, les résultats obtenus sur la précision moyenne montrent une dégradation des performances. Les deux critères étant généralement corrélés, ce résultat est assez surprenant. Pour la suite des travaux, nous allons ainsi tester notre algorithme sur d'autres bases pour confirmer les performances obtenues. Ce papier étant une première étude pour la tâche d'ordonnancement semisupervisée, nous avons uniquement fourni une partie pratique pour la tâche plus restreinte de l'ordonnancement bipartite. Il est néanmoins à noter que les techniques inductives d'apprentissage semi-supervisé ont été exclusivement développées dans le cadre de la classification (Amini et Gallinari, 2003) et que les résultats obtenus dans ce papier sont un bon présage quant à l'utilisation des données non-étiquetées dans d'autres cadres d'apprentissage supervisé. Une direction intéressante à explorer serait d'apprendre conjointement avec les données étiquetées et non-étiquetées pour la tâche de l'extraction d'information (Amini et al., 2000).

Introduction
L'algorithme des cartes auto-organisatrices de Kohonen, Kohonen (1994) représente un véritable outil de visualisation des données multidimensionnelles. Il permet de convertir des relations statistiques complexes et non-linéaires entre les données de grande dimension en une simple relation géométrique sur une topologie réduite. Cet algorithme permet de compresser l'information tout en préservant les relations topologiques et métriques les plus importantes à partir de l'espace des données primaires. De plus, l'algorithme de Kohonen définit un niveau d'abstraction par la possibilité d'interprétation qui devient plus facile avec une carte bi-dimensionnelle, à la fois simple et significative comparée à l'espace initial des données. Bien que l'algorithme de Kohonen décrit une méthode connexionniste qui appartient à la famille des algorithmes neuronaux, il peut être formulé par une méthode de classification statistique type : nuées dynamiques. Ce formalisme transforme le problème d'auto-organisation en un problème d'optimisation. Dans ce contexte, nous décrivons une variante de classification neuronale non-supervisée proposée par Lebbah et al. (2000). Cette variante, appelée carte topologique binaires, est dédiée aux données qualitatives et consiste en la recherche d'une classification automatique d'un nuage de points App = {(z i , p i ), i = 1..N } où l'individu z i = (z1 i , z2 i , ..., z d i ) muni de la pondération p i appartient à l'ensemble des données binaires
Cette variante s'inspire de la version nuées dynamiques de Kohonen, Anouar (1998), mais utilise un critère spécifique pour déterminer l'ordre topologique. Dans cet article, nous étudions le comportement des cartes topologiques binaires face à des données qualitatives présentant des valeurs manquantes. Ces données sont issues d'une étude épidémiologique castémoins du cancer du nasopharynx (NPC). Cette base est constituée d'une population divisée, de manière équitable, en deux cas : cancer et non-cancer. D'une part, notre étude consiste à extraire des profils de gens atteints du NPC et des gens qui ne le sont pas et d'autre part, elle consiste à déterminer pour chaque profil extrait, l'ensemble des variables explicatives de la population qui lui est associée. L'objectif de cette étude vise donc à détecter dans le profil statistique général du NPC, des profils "types" sous formes de groupes de population homogènes. Chaque groupe, étant un représentant d'un cas particulier de la population globale, est muni d'un ensemble de caractéristiques résultants de la classification non-supervisée faite par les cartes topologiques binaires et optimisée par une classification statistique automatique.
Données qualitatives et codage
Il existe de nombreuse variables, dites discrètes, ne pouvant prendre par nature qu'un nombre restreint de valeurs Marchetti (1989). Citons par exemple les variables associées aux caractéristiques physiques tel que la taille (grande, moyenne, petite) ou encore à la situation familiale (célibataire, veuf, divorcé, marié). Les variables ainsi définies sont appelées variables qualitatives. Elle se répartissent en deux groupes : les variables qualitatives ordinales et les variables qualitatives nominales. Si l'on utilise un codage adapté, les données qualitatives deviendront des données binaires. Les codages utilisés le plus souvent sont : (a)Le codage binaire additif : Ce codage permet essentiellement de rester cohérent avec la notion d'ordre entre les modalités d'une variable.(b) Le codage disjonctif complet : Ce tableau résulte de la transformation, par le codage disjonctif complet, d'un tableau de variables qualitatives nominales encore appelé questionnaire multiple. Une seule modalité est choisie pour chaque variable, TAB.1.
Que les données initiales soient dans l'espace des données avec modalités ou après transformation dans l'espace des données binaires, nous aboutissons à des caractéristiques identiques Leich et al. (1998). L'espace des données binaires peut être muni de la distance euclidienne, il est souvent beaucoup plus intéressant de le munir de distances adaptées permettant de mieux traduire ses particularités. Dans ce papier nous utilisons la distance de Hamming appelée H.  
mise :
Les données étant binaires, ? j est la médiane binaire de l'ensemble des valeurs prises par la variable j sur l'ensemble des individus. La médiane est la valeur 1 ou 0 correspondant à la plus grande sommation des pondérations de la valeur 1 et 0. Dans le cas particulier où z i sont munis d'une même pondération (p i = 1, ?i) la règle fournit une médiane ayant une interprétation particulièrement simple, ? j est alors la valeur 0 ou 1 la plus souvent choisie par les individus sur la variable j.
Valeurs manquantes
Le problème des valeurs manquantes est un véritable problème de recherche. Ceci étant, il existe un certain nombre de façons pour détourner ce problème, par exemple en les remplaçant par la médiane ou par l'apprentissage d'un prédicteur automatique,...etc. Cependant, dans notre cas, les variables sont qualitatives. Nous proposons, pour une raison de simplicité, de définir une modalité supplémentaire pour les valeurs manquantes. Cette pseudo-solution, ne pose aucun problème dans le cas d'un codage disjonctif. Cependant, le problème se pose pour le codage additif où l'ordre entre les modalités est important. Nous proposons donc de définir la modalité des valeurs manquantes de telle façon à ce qu'elle soit la plus proche de la médiane entre toutes les autres valeurs de la variable.
La carte topologique binaire
Nous rappelons ici comment l'utilisation de la médiane peut permettre de définir un modèle de carte auto-organisatrice adapté aux données binaires. Comme pour le modèle classique des cartes topologiques, nous utilisons un réseau de neurones avec une couche d'entrée pour les entrées et une carte possédant un ordre topologique de k cellules. La prise en compte dans la carte C de la notion de proximité impose de définir une relation de voisinage topologique. Les neurones sont répartis aux noeuds d'un maillage. Comme dans le cas de l'algorithme de Kohonen nous définissons la topologie de la carte à l'aide d'un graphe non orienté et la distance ?(c, r) entre deux cellules c et r étant la longueur du chemin le plus court qui sépare la cellule c et r. Afin de modéliser la notion d'influence d'un neurone r sur un neurone c, qui dépend de leur proximité, on utilise une fonction à la fonction noyau
L'influence mutuelle entre deux cellules c et r est définie par la fonction K(?(c, r)). A chaque cellule c de la grille est associé un vecteur de poids binaire w c de dimension d. L'ensemble des poids associés constitue l'ensemble des référents noté W. L'auto-organisation de la carte va maintenant se faire à l'aide du formalisme des nuées dynamiques et donc par l'intermédiaire de la minimisation d'une fonction de coût.
Pour utiliser l'algorithme des nuées dynamiques, Diday et C.Simon. (1976), nous avons utilisé la fonction de coût E(?, W) déjà définit dans Lebbah et al. (2000), adaptées aux traitements des données binaires . Donc la fonction de coût à minimiser est alors :
zi?App r?C
Où ? affecte chaque observation z à une cellule unique de la carte C. La minimisation de la fonction de coût est réalisée à l'aide d'une procédure itérative en deux phases :
1. Phase d'affectation : mise à jour de la fonction d'affectation ? associée à l'ensemble W fixé. On affecte chaque observation z au référent défini à partir de l'expression suivante :
2. Phase d'optimisation : La fonction d'affectation étant fixée à sa valeur courante, choisir le système de référents qui minimise la fonction E(?, W) dans l'espace ? m . ce point n'est autre que le centre médian de App lorsque chaque observation z i est pondérée
La minimisation de E(?, W) s'effectue par itérations successives jusqu'à stabilisation des deux phases.
Dans la pratique nous avons utilisé une fonction noyaux
) en faisant varier le paramètre T entre deux valeurs T max et T min . On obtient alors pour décrire la carte un ensemble de référents binaires W. Ces référents sont du même genre que les données initiales : Le décodage (additif ou exclusif ) de différents vecteurs permet l'interprétation symbolique des référents trouvés.
Résultats
Nous appliquons la méthode aux données d'une étude épidémiologique cas-témoins du cancer du nasopharynx (NPC). Pour clarifier le rôle de l'environnement dans l'étiologie du NPC, le CIRC a mené en 2004 une étude cas-témoins multicentrique dans la région endé-mique du Maghreb. Le NPC présente une incidence très variable selon les régions du monde. C'est un cancer relativement rare sauf en Chine, en Asie du sud-est et au Maghreb, où les taux d'incidence sont élevés. Dans ces régions, le NPC est un problème majeur de santé publique. Les études ont suggéré l'existence d'un grand nombre de facteurs de risques environnementaux incluant habitudes alimentaires et environnement domestique et professionnel. Afin de focaliser la présente analyse sur la forme adulte du NPC seuls les individus âgés de plus de 35 ans recrutés dans lŠ étude ont été sélectionnés, soit un total de 986 individus, dont 499 sont atteint par le cancer (les cas), et 487 ne le sont pas (les témoins). Chaque individu est décrit par 61 caractères.
Dans les 61 caractères, il y a la variable à expliquer 1-" NPC " (c'est la variable du cancer) ; les autres sont les variables explicatives. Lexique : 2-âge, 3-sexe, 4-niveau dŠinstruc-tion, 5-catégorie professionnelle, 6-habitat dans lŠenfance, 7-habitat a lŠage adulte, 8-parents consanguins, 9-fréquentes otites, 10-fréquentes angines, 11-fréquentes rhume, 12-asthme, 13-eczéma, 14-allergie, 15-exposition aux engrais chimiques et pesticides, 16-exposition aux produits chimiques, 17-exposition aux fumées, 18-exposition aux poussières, 19-exposition aux formaldéhyde, 20-consommation dŠalcool, 21-consommation de tabac, 22-consommation de neffa, 23-consommation de cannabis, 24-25-type de logement enfant. et adulte., 26-27-lits séparés enfant. et adulte., 28-29-animaux dans la maison enfant. et adulte., 30-31-aération cuisine enfant. et adulte., 32-33-aération maison enfant. et adulte., 34-35-exposition aux fumées d'encens enfant. et adulte., 36-37-exposition aux fumées de kanoun et tabouna enfant. et adulte., 38-39-exposition aux fumées de feu de bois enfant. et adulte., 40-41-42-allaité et age au sevrage et modalité de sevrage, 43-contact avec la salive adulte par le sol ou les aliments, 44-traitements traditionnels dans lŠenfance, 45-consommation de piment, 46-47-consommation de smen et graisse enfant. et adulte., 48-49-légumes fruits agrumes enfant. et adulte., 50-51-harrissa maison enfant. adulte., 52-53-harrissa industrielle enfant. adulte., 54-55-protéines maison enfant. adulte., 56-57-protéines industrielles enfant. adulte., 58-59-conserves légumes industrielles enfant. adulte., 60-61-conserves légumes maison enfant. adulte.
Nous appliquons la variante des cartes topologiques binaires sur les données décrites cidessus avec une architecture de (10 × 10) cellules. l'apprentissage de cette carte fournit pour chaque cellule un référent w c prenant en compte les deux codages : disjonctif et additif. Fig.1 représente la distribution de la population sur la carte. Fig.2 représente la répartition en distinguant les cas cancer et cas non-cancer. Grâce à la cartographie obtenue, nous pouvons déjà effectuer quelques analyses sur la répartition des individus. En effet, il existe 5 neurones vides, i.e. des neurones qui n'incluent aucun individu. Ces neurones représentent la propriété de lis-
FIG. 1 -Carte topologique 10 × 10 avec les cardinalites des neurones
sage, une des propriétés fortes qui caractérisent les cartes auto-organisatrices. Les neurones sont mélangés, ce qui veut dire que quelques individus n'ayant pas le cancer, même s'ils sont minoritaires dans un neurone, peuvent avoir les mêmes caractéristiques que les individus ayant le cancer ! Pour optimiser le nombre de neurones obtenu dans la carte, nous avons appliqué la méthode des K-means sur les référents avec différentes partitions et nous avons choisi celle qui minimise le mieux l'indice de boulding. Le nombre de classes optimal trouvé étant égal à 6 (Fig.3). A partir de l'indice optimal indiqué dans cette dernière figure, nous avons partitionné la carte en 6 grandes classes (Fig.4). Dans cette figure, nous avons constaté les statistiques suivantes : Classe 1 : la zone bleue qui regroupe 13% de la population dont 67% ayant le cancer. Classe 2 : La zone mauve, regroupant 13% de la population dont 53% ayant la cancer. Classe 3 : la zone verte représentant 25% de la population dont 65% de Non-cancer. Classe 4 : zone rouge, représentant 18% dont 60% ayant le cancer. Classe 5 et 6 (31% de la population) : zones jaune et orange, représentent des zones de conflit, que nous ne pouvons interpréter.
Au vu de ces résultats, deux classes attirent notre attention : la classe 1 et la classe 3 en raison de : 1) la proportion anormale d'individus atteints du cancer au regard des 50% dans la population d'origine, et 2) le nombre significatif d'individus dans ces classes. Pour caractériser les profils des individus dans ces classes, nous avons étudié la distribution des variables explicatives. La divergence de Kullback-Leibler Bishop (2006) notée KL(p||q) (i.e., ? x p(x) ln(q(x)/p(x)) dans le cas discret) est classiquement utilisée pour mesurer la dissimilarité entre la distribution d'origine p(x) d'une variable et celle observée dans la classe considérée, q(x). Aussi, nous avons classé les 60 variables explicatives dans chaque classe dans l'ordre décroissant de la divergence KL. Le principe sous-jacent est de dire qu'une variable est d'autant plus discriminante que sa distribution dans la classe considérée est significativement modifiée par rapport à la distribution d'origine.
Dans la classe 1 pour laquelle il y a une sur-représentation des individus atteints du NPC, Plus précisément, on observe dans cette classe que 60% des individus ont consommé de l'harrissa à l'enfance alors que ce taux n'est que de 20% dans la population étudiée. Dans la classe 3 caractérisé par un sous-représentation des individus atteints du NPC, on observe que la variable 26 a une valeur de KL élévée (0.8) suivie de 32, 30 et 24 (0.38, 0.32, 0.31) au regard des autres (<0.25). La variable 26 est associée au lits séparés à l'enfance, les autres portent sur l'aération de la maison, de la cuisine et de la catégorie du logement à l'enfance. On observe dans cette classe que 90% des individus avaient des lits séparés à l'enfance alors que ce taux n'est que de 40% dans la population étudiée. De même, 22% ont vécu dans une maison bien aérée durant l'enfance alors que ce taux n'est que de 2% dans la population étudiée.
A titre d'exemple, nous illustrons pour la classe 1, la distribution de la variable 50 (Harissa maison enfant) sur la carte topologique obtenue (Fig.5). Cette figure représente l'impact des modalités de la variable 50 sur les individus atteins du NPC. La première modalité est distribuée sur tous les neurones de la carte, ce qui est normal à cause du codage additif de la variable. La quatrième modalité n'est représentée sur aucun neurone. Il s'agit d'une modalité additionnelle pour représenter les valeurs manquantes de la variable. La deuxième et la troisième modalités sont fortement présentes dans la classe étudiée (zone bleue).
Au final, ce type d'analyse nous renseigne sur l'existence de profils statistiques distincts dans la population étudiée. L'analyse des profils dans chacune des classe nous a permis d'identifier des facteurs corrélés avec le cancer (ou l'absence du cancer). Par ce type d'analyse nous avons pu dresser des profils statistiques sémantiques des différentes catégories d'individus atteints ou non du cancer et ainsi extraire l'ensemble de variables explicatives de chaque profil.
Conclusion
Dans ce papier, nous avons présenté un système connexionniste pour l'extraction de profils cas témoins du cancer du nasopharynx (NPC) à partir des données issues d'une étude épi-démiologique. Ce système utilise un codage spécifique aux données qualitatives représentant des valeurs manquantes. Basé sur une carte topologique binaire, le système ainsi développé a permis d'une part, de trouver des groupes homogènes à partir d'une population globale regroupant des cas cancer et non-cancer et d'autre part, d'extraire les variables explicatives de chaque profil extrait. Nous avons pu grâce à ce système fournir aux épidémiologistes un outil d'aide à la décision qui leurs fournit un véritable outil de visualisation à partir des données multidimensionnelles. Cet outil permet également, d'éclater le profil général de la population des gens atteints du NPC en un ensemble de profils "type". Chaque profil étant caractérisé par un ensemble de variables explicatives des cas cancer ou non-cancer. Le système développé sera prochainement comparé avec d'autres méthodes d'extraction de connaissances.

Introduction
Le problème de réconciliation de références est un problème majeur pour l'intégration ou la fusion de données provenant de plusieurs sources. Il consiste à décider si deux descriptions provenant de sources distinctes réfèrent ou non à la même entité du monde réel (e.g., la même personne, le même article, le même gène, le même hôtel).
Il est très difficile d'attaquer ce problème dans toute sa généralité car les causes d'hétéro-généité dans la description de données provenant de différentes sources sont variées et peuvent être de nature très différente. L'hétérogénéité des schémas est une des cause premières de la disparité de description des données entre sources. De nombreux travaux, dont on peut trouver une synthèse dans Rahm et Bernstein (2001); Shvaiko et Euzenat (2005); Noy (2004) ;Euzenat et Valtchev (2004), ont proposé des solutions pour réconcilier des schémas ou des ontologies par des mappings. Ces mappings peuvent ensuite être utilisés pour traduire des requêtes de l'interface de requêtes d'une source vers l'interface de requête d'une autre source.
L'homogénéité ou la réconciliation de schémas n'empêchent cependant pas les variations entre les descriptions des instances elles-mêmes. Par exemple, deux descriptions de personnes avec les mêmes attributs Nom, Prénom, Adresse peuvent différer sur certaines valeurs de ces attributs tout en référant à la même personne, par exemple, si dans l'un des tuples le prénom est en entier alors que dans l'autre tuple il n'est donné qu'en abrégé.
Les travaux en nettoyage de données qui visent la détection de doublons dans des bases de données sont confrontés exactement à ce problème. La plupart des travaux existants (e.g., Galhardas et al. (2001); Bilenko et Mooney. (2003); Ananthakrishna et al. (2002)) se fondent sur des comparaisons entre chaînes de caractères pour calculer la similarité entre valeurs d'un même attribut, puis calculent la similarité entre deux tuples en combinant les similarités trouvées entre les valeurs de chaque attribut de ces deux tuples. Dans l'approche proposé par Benjelloun et al. (2006) la comparaison de références est générique mais reste une comparaison locale deux à deux. Quelques travaux très récents (Bhattacharya et Getoor. (2004); Kalashnikov et al. (2005); Dong et al. (2005); Singa et Domingos. (2005)) ont une approche globale exploitant les dépendances qui peuvent exister entre réconciliations de références. Souvent, ces dépendances découlent de la sémantique du domaine. Par exemple, la réconciliation entre deux références à des cours décrits par leur intitulé et le nom de l'enseignant responsable peut entraîner la réconciliation entre deux références à des personnes. Cela nécessite l'explicitation de connaissances supplémentaires sur le domaine d'application, comme le fait qu'un enseignant est une personne, et qu'un cours est identifié par son intitulé et n'a qu'un enseignant responsable. Dans Dong et al. (2005), des connaissances du domaine de ce type sont prises en compte mais doivent être codées dans le poids des arcs du graphe de dépendances dont les noeuds correspondent aux paires de références potentiellement réconciliables.
Dans cet article, nous étudions le problème de réconciliation de références dans le cas où les données à réconcilier sont décrites relativement à une même ontologie, vue comme un schéma sémantiquement riche, décrit en RDFS(http ://www.w3.org/TR/rdf-schema/ ) étendu par certaines primitives de OWL-DL (http ://www.w3.org/2004/OWL). OWL-DL sert à poser des axiomes qui enrichissent la sémantique des classes et des propriétés déclarées en RDFS. On peut ainsi par exemple exprimer que deux classes sont disjointes ou que telle ou telle propriété (ou son inverse) est fonctionnelle. Nous montrons l'intérêt d'une approche logique basée sur des règles de réconciliation qui peuvent être générées automatiquement à partir des axiomes du schéma. Ces règles traduisent de façon déclarative les dépendances entre réconciliations qui découlent de la sémantique du schéma. Elles permettent d'inférer de façon sûre des réconcilia-tions ainsi que des non réconciliations. Nous obtenons ainsi une méthode ayant une précision de 100%, et nous montrons que le rappel est augmenté de façon significative si on enrichit la sémantique du schéma en ajoutant des axiomes. Cette méthode permet d'obtenir comme produit dérivé direct un dictionnaire de synonymie entre chaînes de caractères. Ce dictionnaire peut être exploité dans une phase ultérieure de réconciliation des paires de références non traitées par la méthode logique et pour lesquelles nous envisageons une méthode numérique dont nous décrivons brièvement le principe à la fin de l'article.
L'article est organisé de la façon suivante. La section 2 définit le modèle de données (RDFS+) et le problème de réconciliation de références que nous considérons. La section 3 décrit la méthode logique que nous proposons qui repose sur des règles d'inférences traduisant les contraintes du schéma en des dépendances logiques entre réconciliations de référence. La section 4 fournit le résultat d'expérimentations que nous avons effectué sur deux jeux de données : celui de CORA qui sert de benchmark dans plusieurs travaux de nettoyage de données ; et un jeu de données fourni par France Telecom R&D dans le cadre du projet PICSEL3. La section 5 conclue l'article en situant notre approche par rapport à l'existant et en indiquant quelques perspectives.
Définition du problème
Nous décrivons d'abord le modèle des données que nous considérons, que nous appelons RDFS+ car il étend RDFS avec des primitives de OWL-DL. D'un point de vue "bases de données", RDFS+ peut être vu comme un fragment du modèle relationnel (restreint à des relations unaires et binaires) enrichi par la possibilité d'exprimer des contraintes de typage, d'inclusion ou d'exclusion entre relations, et de dépendances fonctionnelles.
Le modèle de données RDFS+
Le schéma : Nous considérons que nous disposons d'un schéma RDFS consistant en un ensemble de classes (relations unaires) structurées en une taxonomie et d'un ensemble de propriétés (relations binaires) qui peuvent être elles-mêmes structurées en une taxonomie de propriétés. Les propriétés sont typées. Dans la terminologie RDFS, on distingue les propriétés qui sont des relations, dont les domaines et co-domaines sont des classes, de celles dont le co-domaine est un ensemble de valeurs de base (numériques ou alpha-numériques), et qu'on appelle des attributs. On notera :
-R(C, D) pour indiquer que le domaine de la relation R est la classe C et que son codomaine est la classe D, et -A(C, Litteral) pour indiquer que l'attribut A a comme domaine C et comme codomaine un ensemble de valeurs (numériques ou alpha-numériques).
Les axiomes : Nous donnons la possibilité de déclarer des axiomes OWL-DL pour enrichir la sémantique d'un schéma RDFS. Les axiomes que nous considérons sont de plusieurs types. Nous ne donnons pas leur notation standard en XML, très verbeuse, mais nous précisons leur sémantique logique.
-Axiomes de disjonction entre classes. Nous notons DISJOIN T (C, D) l'axiome dé-clarant que les classes C et D sont disjointes, dont la sémantique logique est : ?X C(X) ? ¬D(X). -Axiomes de fonctionnalité d'une propriété. Nous notons P F (P ) l'axiome déclarant que la propriété P (relation ou attribut) est fonctionnelle, dont la sémantique logique est :
l'axiome déclarant que l'inverse de la propriété P (relation ou attribut) est fonctionnelle, dont la sémantique logique est :
Les données : Une donnée a un identifiant (appelé référence) et une description qui est l'ensemble des faits RDF (http ://www.w3.org/RDF/ ) qui mentionnent cet identifiant. Un fait RDF est : -soit un fait-classe de la forme C(i) où i est un identifiant, -soit un fait-relation de la forme R(i 1 , i 2 ) où R est une relation et i 1 et i 2 sont des identifiants, -soit un fait-attribut de la forme A(i, v) où A est un attribut, i un identifiant et v une valeur (numérique ou alpha-numérique).
Nous supposons que les données peuvent provenir de plusieurs sources et nous préfixons l'identifiant d'une donnée par l'identifiant de la source dont elle provient. Sauf mention explicite, nous posons par défaut l'hypothèse du nom unique sur chaque source. Cette hypothèse (notée UNA) a la sémantique suivante : deux données d'une même source ayant des identifiants distincts font référence à des entités distinctes du monde réel.
Exemple
Afin d'illustrer le modèle de données RDFS+, nous montrons ici un exemple de schéma RDFS, un ensemble d'axioms OWL-DL et enfin un exemple de données conformes à ce schéma qui porte sur le domaine des lieux culturels. Nous utiliserons pour l'exemple la notation graphique de RDFS. Ces axiomes expriment, par exemple, qu'une peinture ne peut être contenue que dans un seul musée et qu'un nom de peinture n'est associé qu'à une seule peinture.
FIG. 1 -Exemple de schéma RDFS
Soient S1 et S2 deux sources de données RDF conformes au schéma RDFS de la figure 1 et vérifiant les axiomes ci-dessus. Nous présentons le contenu de ces deux sources sous forme d'un ensemble de faits RDF.
La source S1 :
CulturalPlace(S1_m1) ; Painting(S1_p1) ; Artist(S1_a1) ; Museum(S1_m2) ; PaintingName(S1_p1,"La Joconde") ; PaintedBy(S1_p1, S1_a1) ; Contains(S1_m1, S1_p1), MuseumName(S1_m1,"musée du LOUVRE") ; ArtistName(S1_a1, "Leonard De Vinci") ; YearofBirth(S1_a1,"1452") ; Painting(S1_p2) ; PaintingName(S1_p2,"La Cene") ; PaintedBy(S1_p2, S1_a1), Painting(S1_p3) ; PaintingName(S1_p3,"Sainte Anne") ; PaintedBy(S1_p3, S1_a1) ; MuseumName(S1_m2, "musée des arts premiers") ; Located(S1_m2, S1_c1) ; CityName(S1_c1,"Paris") ; MuseumAddress(S1_m2, "quai branly") La source S2 : Museum(S2_m1) ; Museum(S2_m2) ; Artist(S2_a1) ; MuseumName(S2_m1,"Le LOUVRE") ; Located(S2_m1,S2_c1) ; CityName(S2_c1, "Ville de paris") ; Contains(S2_m1, S2_p1) ; PaintingName(S2_p1, "la Joconde") ; Contains(S2_m1,S2_p2) ; PaintedBy(S2_p2,S2_a1) ; PaintingName(S2_p2, "Vierge aux rochers") ; ArtistName(S2_a1, "De Vinci") ; YearofBirth(S2_a1,"1452") ; Contains(S2_m1,S2_p3) ; Located(S2_m2,S2_c1) ; MuseumName(S2_m2,"Musée du quai Branly") ; PaintingName(S2_p3, "Sainte Anne, la vierge et l'enfant jésus "), MuseumAddress(S2_m2, "37 quai branly, portail Debilly"),
FIG. 2 -Exemple de données RDF
Le problème de réconciliation
Soient S 1 et S 2 deux sources de données ayant le même schéma RDFS+. Soient I 1 et I 2 les deux ensembles d'identifiants de leurs données respectives.
Le problème de réconciliation entre S 1 et S 2 consiste à partitioner l'ensemble I 1 × I 2 des paires de références en 2 sous-ensembles Reconcile et N onReconcile regroupant respectivement les paires de références représentant une même entité, et les paires de références représentant deux entités différentes.
Dans la suite de l'article, on utilisera la notation relationnelle plutôt que la notation ensembliste : Reconcile(i1, i2) (respectivement N onReconcile(i1, i2)) pour (i1, i2) ? Reconcile (respectivement pour (i1, i2) ? N onReconcile ).
Une méthode de réconciliation est totale si elle produit un résultat (Reconcile(i1, i2) ou N onReconcile(i1, i2)) pour tout couple (i 1 , i 2 ) ? I 1 × I 2 .
La précision d'une méthode de réconciliation est la proportion, parmi les couples pour lesquels la méthode a produit un résultat (de réconciliation ou de non réconciliation), de ceux pour lesquels le résultat est correct.
Le rappel d'une méthode de réconciliation est la proportion, parmi tous les couples possibles de I 1 × I 2 , de ceux pour lesquels la méthode a produit un résultat correct.
La méthode de réconciliation que nous décrivons dans la section suivante est une méthode de réconciliation partielle qui a la caractéristique d'être globale et fondée sur la logique : elle traduit les axiomes du schéma par des règles logiques de dépendances entre réconciliations. L'intérêt d'une approche logique est qu'elle garantit une précision de 100%. Notre expérimen-tation se focalise donc sur l'estimation du rappel.
Méthode logique de réconciliation à base de règles
Notre approche consiste à traduire les axiomes associés au schéma, incluant l'UNA (quand elle s'applique), par des règles logiques (Section 3.1), et à appliquer un algorithme de raisonnement pour inférer des réconciliations et des non réconciliations (Section 3.2), ainsi que des synonymies entre valeurs de base qui seront conservées dans un dictionnaire (Section 3.3).
Génération des règles de réconciliation et de non réconciliation
Traduction de l'hypothèse du nom unique par des règles de non réconciliation : On introduit les prédicats unaires src1 et src2 pour typer chaque référence en fonction de sa source d'origine (srci(X) signifie que la référence X provient de la source Si). La contrainte de l'UNA au niveau des sources S1 et S2 se traduit par les quatre règles suivantes :
Les deux premières règles traduisent la non réconciliation de deux références provenant d'une même source. Les deux dernières traduisent le fait qu'une référence provenant d'une source S1 (resp. S2) peut être réconciliée avec au maximum une référence de la source S2 (resp. S1). -Pour toute relation R déclarée comme fonctionnelle par un axiome PF(R), la règle R6.1(R) est générée, qui traduit le fait que pour une instance de la classe du domaine de R il existe au plus une instance du co-domaine.
R6.1(R) : Reconcile (X, Y) ? R(X, Z) ? R(Y, W)
? Reconcile(Z,W) -Pour tout attribut A déclaré comme fonctionnel par un axiome PF(A), la règle R6.2(A) est générée, qui exprime que pour une instance de la classe du domaine de A il existe au plus une valeur de base appartenant au co-domaine. Le prédicat binaire EquiVals permet d'exprimer que deux valeurs de base sont synonymes. Il est l'équivalent sur les valeurs de base du prédicat Reconcile.
R6.2(A) : Reconcile (X, Y) ? A(X, Z) ? A(Y, W) ? EquiVals(Z,W)
-Pour toute relation R déclarée comme fonctionnelle inverse par un axiome PFI(R), la règle règle R7.1(R) est générée, qui exprime que pour une instance de la classe du co-domaine de R il existe au plus une instance du domaine.
R7.1(R) : Reconcile (X, Y) ? R(Z, X) ? R(W, Y) ? Reconcile(Z,W)
-Pour tout attribut A déclaré comme fonctionnel inverse par un axiome PFI(A), la règle R7.2(A) est générée, qui traduit le fait que pour deux valeurs de base synonymes appartenant au co-domaine il existe au plus une instance de la classe du domaine.
R7.2(A) : EquiVals (X, Y) ? A(Z, X) ? A(W, Y) ? Reconcile(Z,W)
Remarque. On peut traduire par des règles analogues aux règles précédentes des dépen-dances fonctionnelles impliquant plusieurs relations ou attributs.
Inférence de réconciliations et de non réconciliations
L'algorithme du chaînage avant est appliqué sur la base de connaissances composée de la base des règles présentées ci-dessus et de la base de faits contenant : -l'ensemble de faits-classe, faits-relation et de faits-attribut représentant les descriptions de l'ensemble de références des deux sources étendue par les propriétés obtenues par héritage ; -un fait de type src1(X) ou src2(X) pour toute référence X permettant de représenter sa provenance ; -un ensemble de faits instance du prédicat EquiVals(X,Y) qui expriment l'égalité à une normalisation près (élimination des ponctuations, des mots vides) des valeurs de base. Ainsi, le fait EquiVals("La Joconde", "la joconde") est posé car les deux chaines de caractères ne diffèrent que par deux majuscules.
En nous appuyant sur les données de la figure 2 et sur le schéma de la figure 1, nous allons montrer comment les décisions de réconciliations et de non réconciliation se propagent grâce à l'enchainement des règles.
Les règles R1 et R2 (traduisant l'UNA) puis les règles R5(CulturalPlace, Painting) et R5(Artist, Painting), traduisant des axiomes de disjonction entre classes, permettent d'infé-rer, entre autres, les non réconciliations suivantes :
NonReconcile(S1_m1, S1_m2), NonReconcile (S1_p1, S1_p2), NonReconcile(S2_m1, S2_p1), NonReconcile (S2_p1, S2_p2), NonReconcile(S1_m1, S2_p1), NonReconcile (S1_a1, S2_p1).
La règle R7.2(PaintingName) traduisant la propriété de fonctionnalité inverse de l'attribut PaintingName, et les faits PaintingName(S1_p1, "La joconde"), PaintingName(S2_p1, "La Joconde") et EquiVals("La Joconde", "la joconde") permettent d'inférer Reconcile(S2_p1, S1_p1), c'est-à-dire que S2_p1 et S1_p1 réfèrent à la même peinture.
Les musées S1_m1 et S2_m2 contenant ces peintures sont eux mêmes réconciliés par dé-clenchement de la règle R7.1(Contains), qui permet d'inférer Reconcile(S1_m1,S2_m2).
La propagation de ces nouvelles réconciliations permettra grâce à la règle R6.2(MuseumName) d'inférer Equivals("Le LOUVRE", "musée du LOUVRE"), éta-blissant la synonymie entre les valeurs de base "Le LOUVRE" et "musée du LOUVRE", et grâce à la règle R6.1(Located) d'inférer la réconciliation des deux références sur les villes S1_c1 et S2_c1. Le fait inféré Reconcile(S1_c1 , S2_c1) permet ensuite d'inférer, par le déclenchement de la règle R6.2(CityName), le nouveau fait EquiVals("ville de Paris","Paris") qui établit la synonymie entre les deux valeurs de base "ville de Paris" et "Paris".
Les règles R3 et R4 de traduction de l'UNA permettent d'éliminer toute autre possibilité de réconciliation. La règle R4 permet ainsi d'inférer NonReconcile(S2_m2, S1_m1) à partir des faits Reconcile(S1_m1, S2_m1) src1(S1_m1), src2(S2_m1) et scr2(S2_m2) : par UNA, on sait que les deux musées S2_m1 et S2_m2 sont différents, et comme on a réconcilié les deux références S1_m1 et S2_m2 au musée du Louvre, on est sûr que la référence S1_m1 de la source S1 au musée du Louvre n'est pas réconciliable avec la référence S2_m2 de la source S2 (qui est une référence au musée du quai Branly).
Génération et exploitation du dictionnaire
L'ensemble des synonymies qui sont inférées entre valeurs de base (prédicat Equivals) sont conservées dans un dictionnaire qui est alimenté au fur et à mesure des réconciliations de références provenant de différentes sources.
Le dictionnaire contient différents types de synonymies : -Des codifications telles que 1 pour oui, 75 pour Paris et (*) pour étoile -Des abréviations telles que apt pour appartement, ou acronymes tels que ACM pour Association for Computing Machinery -Des vrais synonymes tels que bon pour confortable -Des traductions telles que Milano pour Milan Nous avons vu que ces équivalences sont exploitées durant la phase de réconciliation ellemême. Le fait de conserver ces valeurs dans un dictionnaire permet également d'utiliser ces connaissances lorsque l'approche logique est appliquée à d'autres sources. Le dictionnaire peut permettre d'alimenter la base de faits intiale par tous les faits EquiVals qu'il contient. Ce dictionnaire peut aussi être exploité dans une méthode ou une étape de réconciliation numérique fondée sur le calcul de similarité entre chaînes de caractères.
Expérimentation
Nous présentons dans cette section les premiers résultats de l'expérimentation de notre approche logique de réconciliation de références. Cette méthode a été testée sur des données de deux domaines différents : des données du domaine du tourisme et des données d'un portail de publications scientifiques en informatique. Dans le premier jeu de données, en présence de l'UNA au niveau de chaque source, la réconciliation de références a pour objectif l'intégration de données entre différentes sources. Dans le second jeu de données, l'objectif de la récon-ciliation est de nettoyer une source (ie. éliminer les doublons) pour laquelle l'UNA n'est pas posée. 
Présentation des données de test (FT_HOTELS et CORA)
Résultats et validation
Comme l'ensemble des réconciliations et des non réconciliations est obtenu par un algorithme d'inférence à base de règles logiques, nous avons donc une précision à 100%. Il nous reste donc à évaluer le rappel. Pour calculer le rappel sur les données de CORA, nous avons comparé le nombre de réconciliations et de non réconciliations par rapport au nombre de celles qu'il fallait effectivement trouver. L'information concernant des réconciliations et des non ré-conciliations effectives est représentée dans les données de CORA sous forme d'annotations sur les références. En revanche, nous ne disposons pas de cette information pour le jeu de données FT_HOTELS. Nous avons donc effectué la validation à la main sur un échantillon de 1796 références représentant les références de deux sources de données de taille (404 x 1392). Pour nous faciliter la tâche de recherche des réconciliations oubliées, nous avons examiné l'ensemble des réconciliations possibles, après filtrage, par des recherches mots-clés.
Nous présentons dans le tableau 3 le rappel global concernant les décisions de réconci-liations (paires réconciliées ou non réconciliées). De plus, nous donnons le détail du rappel concernant l'ensemble des paires de références réconciliées et l'ensemble des paires de réfé-rences non réconciliées.
Dans le but de montrer l'intérêt de la richesse du schéma sur le rappel obtenu par notre algorithme, nous présentons les résultats sur le jeu de données FT_HOTELS dans deux cas : (1) pas d'axiomes de disjonctions entre classes d'hotels et (2) ajout au schéma d'un ensemble d'axiomes de disjonctions traduisant le fait que deux hotels situés dans deux pays différents sont forcément différents. Comme le montre la figure 3, sur le jeu de données FT_HOTELS, nous avons obtenu un rappel global de 8.3 % avec un rappel correspondant à celui du sous-ensemble des NonReconcile 8.2 % qui représente uniquement les inférences réalisées à partir des règles de l'UNA. Le rappel correspondant au sous-ensemble des Reconcile est de 54 % malgré les irrégularités dans les valeurs. Il s'agit essentiellement d'absences d'information (eg. adresse non renseignée) ou de variabilités dans les valeurs, en particulier dans les adresses : "parc des fées" vs. "parc des fées, (près de Royan)", ou encore "11, place d'arme" vs. "place d'arme". De plus, certaines données de FT_HOTELS sont décrites en plusieurs langues : "Chatatoa" vs. en basque "Chahatoenia". Ces résultats montrent également l'apport de l'enrichissement du schéma par une connaissance telle que la disjonction entre hotels situés dans des pays différents. Cet ajout est peu coûteux et pourtant il permet d'augmenter le rappel du sous-ensemble NonReconcile de 8.2 % à 75.9 %.
FT_HOTELS sans
En ce qui concerne le jeu de données CORA, nous avons obtenu un bon rappel sur le sous-ensemble Reconcile 79 %. Dong et al. (2005) obtiennent un meilleur rappel (97%) mais n'ont pas une précision de 100%. Le rappel sur le sous-ensemble NonReconcile est seulement de 33 %. En effet, ce dernier résultat est obtenu en exploitant uniquement deux axiomes de disjonction du schéma et sans hypothèse d'UNA.
Nous avons également inféré un ensemble de synonymes que nous avons stockées dans un dictionnaire. Par exemple, pour le jeu de données FT_HOTELS, pour lequel nous avons obtenu 1063 réconciliations (au total), le dictionnaire généré contient 3671 synonymies. Ces dernières représentent essentiellement des codifications d'information (eg.EquiVals("1","Y")), des traductions telles que EquiVals("Florence","FIRENZE") et des descriptions syntaxiquement proches telles que EquiVals("2100","DK-2100") qui sont des codes postaux au Danemark mais aussi EquiVals("Avignon -Le Pontet","LE PONTET").
Conclusion
L'approche logique que nous venons de présenter pour la réconciliation de références pré-sente l'intérêt de ne produire que des réconciliations et des non réconciliations sûres, ce qui la distingue des autres travaux existants. La sûreté de cet ensemble de réconciliations est un atout dans un domaine où il est difficile d'estimer à l'avance le taux d'erreurs d'une approche non supervisée Winkler (2006). Cette approche permet également de découvrir des synonymies entre valeurs de base, conservées dans un dictionnaire qui s'enrichit au fur et à mesure que de nouvelles réconciliations sont inférées, et dont l'enrichissement entraîne de nouvelles réconci-liations. Tout en garantissant une précision de 100%, les premières expérimentations montrent que notre méthode obtient un taux de rappel très satisfaisant, qui augmente significativement quand on rajoute des connaissances sur le schéma des données.
D'autres travaux -appelés blocking method -utilisent des connaissances du domaine pour réduire le nombre de paires de références à considérer Baxter R. (2003). Ces travaux considèrent seulement les paires qui possèdent une caractéristique donnée commune (exemple : le nom de famille pour des personnes). Ce type de connaissance peut être déclaré comme disjonction dans le schema afin d'être exploité comme le sont les autres connaissances du domaine. C'est ce que nous avons fait pour tirer parti des valeurs de pays des hotels. Nous avons mentionné dans l'introduction que des connaissances du domaine pouvait être également traduites par des poids dans approche numérique Dong et al. (2005).
La méthode que nous avons proposé est partielle car elle ne produit pas de résultat pour toutes les paires de références possibles. Nous envisageons d'augmenter le rappel des non réconciliations en exploitant les contraposées de règles. Cela permettra également d'enrichir le dictionnaire par de nouvelles connaissances sur les valeurs de base : l'inférence de non Equivals(v1,v2) nous permettra de sélectionner les valeurs dont la similarité syntaxique est bonne et qui, pourtant, ne signifient pas la même chose (exemples : musée du prado vs musée du lido, chatillon sur marne vs chatillon sur seine).
Pour obtenir une méthode totale de réconciliation de références, nous prévoyons d'appliquer une méthode numérique de réconciliation à l'ensemble des paires de référence pour lesquelles l'étape logique n'a pas produit de résultat. Pour cette étape numérique, nous avons plusieurs pistes pour exploiter les résultats de réconciliation et non réconciliation produit par

Introduction
Les services Web constituent la nouvelle génération des technologies du Web pour l'inté-gration d'applications. Ce sont des composants logiciels mis à disposition par des fournisseurs, invocables sur Internet par des clients (des utilisateurs ou d'autres services), et communiquant de façon asynchrone, par le biais de messages. Ils permettent de réaliser une intégration à faible couplage et à moindre coût, du fait qu'ils utilisent des standards généralistes fortement répan-dus (XML, HTTP). Toutefois, cette souplesse d'intégration n'est possible que si les utilisateurs d'un service savent comment interagir avec celui-ci. A un service doivent donc être associées des descriptions assez riches pour permettre de comprendre sa sémantique d'exécution.
Le langage WSDL, par exemple, spécifie l'interface d'un service : les opérations, les types de messages, le format des entrées-sorties. Cependant, Benatallah et al. (2004) ont montré que ceci était insuffisant dans l'optique d'une utilisation automatique des services Web, et ont dé-fini le protocole de conversation, qui permet de spécifier quelles sont les séquences ordonnées de messages (appelées conversations) qu'un service peut émettre ou recevoir. Benatallah et al. (2005a,b) ont ensuite ajouté des contraintes temporelles à leur modèle, rebaptisé protocole de conversation temporisé. Son utilisation offre de nombreuses applications, pour la vérifica-tion automatique de bon fonctionnement, de compatibilité, etc. Néanmoins, en pratique, de nombreux services ne possèdent pas une telle spécification. Il est donc légitime de chercher à obtenir le protocole de conversation d'un service s'il n'a pas été défini lors de la conception.
Fournir le protocole d'un service à ses partenaires et clients est bien sûr l'application la plus directe de ce problème de découverte ; mais il possède un intérêt bien plus grand pour l'ingé-nierie des services Web. Par exemple, un concepteur pourrait connaître le protocole « effectif » (réellement utilisé) d'un service, et savoir s'il correspond bien aux contraintes de conception ; il serait possible de faire évoluer un service plus facilement : l'utilisation d'un modèle visuel de son comportement (plutôt que la simple analyse de code) permettrait de faciliter l'ajout de nouvelles fonctionnalités, de nouvelles contraintes ou règles, etc.
L'extraction du protocole de conversation d'un service englobe de nombreux défis techniques. Le premier réside dans la modélisation du protocole découvert : il est important de prendre en compte l'incertitude du résultat, et de proposer des indices de confiance et des critères de qualité, permettant d'évaluer sa pertinence. Cette incertitude provient principalement du fait que les logs d'un service peuvent contenir des erreurs d'enregistrement (du « bruit »). Des outils sont donc nécessaires, pour analyser et nettoyer les données avant de les traiter. Il est également important de proposer à l'utilisateur des outils lui permettant de modifier et corriger le protocole découvert. Un autre point délicat est la corrélation des messages, i.e. l'identification et la séparation, dans les logs, des différentes conversations (qui peuvent se chevaucher).
Ce problème de découverte constitue en fait un cas particulier d'une problématique beaucoup plus large : la découverte d'un modèle à partir d'instances de celui-ci. De nombreux travaux sont liés à cette thématique ; on peut citer par exemple l'inférence grammaticale (Parekh et Honavar, 2000), la fouille de workflow (Cook et Wolf, 1998;van der Aalst et al., 2004), ou la fouille d'interactions de services Web (Dustdar et al., 2004). En inférence grammaticale, le but consiste à trouver, à partir d'un ensemble de mots appartenant ou pas à un langage donné, une grammaire permettant de générer ce langage. Pour la fouille de workflow (ou découverte de processus), le problème réside dans la construction, à partir des logs d'exécution d'un processus, d'un modèle formel permettant de représenter le fonctionnement de ce processus. En fouille des interactions de services Web, l'objectif est de découvrir, à partir des logs d'un ensemble de services, un workflow modélisant les interactions possibles entre ces services.
Dans le contexte applicatif des services Web, Motahari et al. (2006) ont proposé une mé-thode d'extraction de protocoles à partir des archives de conversation entre services (fichiers « logs »). Ce travail porte sur plusieurs des enjeux mentionnés plus haut. Cependant, il ne considère pas les aspects temporels du protocole de conversation.
Contribution. Notre travail se place également dans le contexte de cette problématique. Nous traitons un sous-problème important qui est la découverte des transitions temporisées du protocole de conversation, i.e. des changements d'état liés non pas à l'émission d'un message mais à l'existence d'une contrainte de temps. Bien qu'une telle transition ne figure pas de façon explicite dans les logs du service, il est possible d'identifier les conséquences de son existence. Ce sont donc ces « traces » que nous formalisons et que nous extrayons des données. Pour cela, nous introduisons la notion d'expiration propre, qui représente dans les logs ce que la notion de transition temporisée représente dans le protocole de conversation. Certains changements d'état du service ne sont pas liés à l'envoi de messages explicites, mais à des contraintes temporelles (durée de validité, échéance, etc.). Le modèle de base a donc été enrichi de transitions temporisées, et rebaptisé de ce fait protocole de conversation temporisé. Une transition temporisée se produit de façon automatique, après qu'un certain laps de temps se soit écoulé à partir du moment où elle a été permise (i.e. où l'état source de la transition est devenu l'état courant), ou bien après qu'une certaine date soit atteinte ; elle est étiquetée par la contrainte de temps associée. Notons que, puisque le modèle est déterministe, un état ne peut admettre plusieurs transitions temporisées comme transitions sortantes.
FIG. 1 -Exemple de protocole de conversation temporisé (Benatallah et al., 2005b).
Exemple 1 La figure 1 représente un protocole de conversation temporisé décrivant le comportement externe d'un service de commande de marchandises. Les transitions explicites sont représentées en trait plein, et les transitions temporisées en pointillés. Ce protocole spécifie que le client du service doit d'abord se connecter (opération login), puis chercher des produits (searchGoods). Il peut alors ajouter ou enlever des produits de son caddie (addToCart, removeFromCart), chercher d'autres marchandises (searchGoods), ou demander un devis (quoteRequest) qui sera valide seulement pendant 3 jours (i.e. 4320 minutes). Il peut alors commander les marchandises (order). S'il ne le fait pas, au bout des 3 jours la conversation se termine (par le biais de la transition temporisée sortant de l'état Quoted), et la commande est annulée.
RNTI -E -
Logs de conversation
Les différentes façons de collecter les logs d'interaction d'un service ont été décrites par Dustdar et al. (2004). En fonction de la façon dont les services sont implémentés et du type d'outils utilisés pour gérer leur exécution, différents types d'informations peuvent être présents dans les logs. Dans un scénario réaliste, les informations collectées sont en général, en plus du contenu du message, l'émetteur, le receveur et la date.
Ces informations peuvent ne pas suffire pour identifier une conversation de façon unique, dans le cas où l'on ne dispose pas d'un identifiant de chaque conversation enregistrée dans les logs. Il est mis en avant par Motahari et al. (2006) que le fait de fournir automatiquement un tel identifiant (s'il n'est pas présent par défaut) est un véritable problème en soi. Aussi, ont-ils supposé, pour mener à bien leur tâche de découverte du protocole de conversation, que cette information était présente dans les logs. Nous en ferons de même.
Les logs que nous traitons sont les enregistrements des messages émis ou envoyés par un service. Ces enregistrements sont effectués par le serveur hébergeant le service. Nous ne considérons pas les logs « internes » du service, qui peuvent être ajoutés au code par le concepteur. Les informations dont nous disposons sont les intitulés des messages, et leurs dates d'émis-sion ; la connaissance de l'émetteur ou du receveur ne nous est d'aucune utilité. Précisons également que nous ne tenons pas compte de la polarité des messages.
Exemple 2 Pour un service vérifiant le protocole de conversation représenté par la figure 1, on peut par exemple obtenir les conversations suivantes : (login, 9:18) (searchGoods, 9:20) (addToCart, 9:21) (quoteRequest, 9:22) (cancel, 9:51) ; (login, 11:03) (searchGoods, 11:04) (addToCart, 11:08) (quoteRequest, 11:12).
Spécification du problème
L'ensemble des intitulés des messages appartenant au protocole de conversation temporisé sera noté M sg. On notera L les logs de conversation dont on dispose. Formellement, L sera un multi-ensemble de conversations. Une conversation sera notée :
.. < t n C ; elle représentera une séquence d'occurrences de messages. On notera de plus :
Tâche de découverte
Considérons le cas où l'on ne connaît pas le protocole de conversation qui a permis de géné-rer les logs L. Le problème consiste à exhiber le fait que ces données traduisent la présence de transitions temporisées dans le protocole. Notre méthode vise à examiner les couples de messages dans les logs, afin de déterminer si, entre deux transitions explicites, a été déclenchée une transition implicite ou pas. Pour ce faire, nous allons calculer, pour chaque conversation, le laps de temps écoulé entre l'émission de deux messages consécutifs.
RNTI -E -D. Devaurs et al. Exemple 3 Les logs L 1 , associés au protocole P 1 représenté dans la figure 2, constituerons l'exemple que nous développerons dans la suite de cet article. Ici, nous devons par exemple mettre en évidence le fait, qu'après l'émission du message a, les messages c, d et e peuvent être émis uniquement avant un certain laps de temps, et que les messages g et h peuvent être émis uniquement après ce laps de temps. Il est important de noter que les dates d'enregistrement des messages sont les dates relatives au début de chaque conversation.
Hypothèses de travail
Nous supposerons dans la suite que les protocoles associés aux logs que nous traiterons, comme dans l'exemple précédent, ne possèdent pas de transition temporisée menant dans un état final, bien que cela puisse se produire dans les cas réels. Le fait de se ramener à ce cas particulier -qui est déjà relativement complexe en soi -nous aidera par la suite à mieux appré-hender le cas général. Nous supposerons également que les transitions du protocole de conversation sont étiquetées de façon unique, même si certaines correspondent à un même message. Si nous ne sommes pas dans un tel cas de figure, nous pourrons envisager de nous y ramener en effectuant un pré-traitement sur les données.
En ce qui concerne les logs, nous allons supposer qu'ils ne sont pas bruités, c'est-à-dire que les messages sont correctement enregistrés, et dans une séquence correcte. Ceci nous permettra, dans un premier temps, de proposer une méthode « complète ». Une approche probabiliste pourra ensuite être envisagée pour pallier le fait que les logs puissent être bruités dans les cas réels. Nous supposerons également que les logs sont suffisamment « complets » pour retrouver les transitions temporisées, c'est-à-dire que tous les « chemins » du protocole de conversation ont été parcourus.
Définition 1 (Episode) Un épisode constitue une séquence de deux intitulés de messages :
Si une telle séquence existe, on dira que l'épisode ? se produit dans la conversation C.
On notera Occ(?) l'ensemble des occurrences de l'épisode ? dans L. On dira que l'épi-sode ? se produit dans les logs L si ? se produit dans au moins une conversation C de L, i.e. si Occ(?) = ?. On notera Ep l'ensemble des épisodes se produisant dans les logs L.
Cette proposition exprime le fait que l'on peut construire une partition de l'ensemble des épisodes, où chaque partie est constituée de l'ensemble des épisodes dont le premier élément est un message m donné. Nous ne donnons pas de preuve pour ce résultat, qui est relativement trivial. Cette propriété va nous permettre de décomposer notre tâche de découverte. Au lieu d'examiner les épisodes dans leur ensemble, nous allons traiter chaque élément de cette partition séparément. Pour ce faire, nous allons définir la notion de durée d'occurrence.
Intuitivement, la durée d'une occurrence d'un épisode est la différence des dates des messages de l'épisode dans l'occurrence. A partir de ceci, on définit la durée d'occurrence minimale (respect. maximale) d'un épisode comme étant la plus petite (respect. la plus grande) durée de toutes les occurrences de cet épisode. L'intervalle de durée d'occurrence d'un épisode est l'intervalle qui englobe l'ensemble des durées d'occurrence de cet épisode. Du fait qu'elle soit la conséquence de la présence d'une transition temporisée, la relation de précédence présentée dans cet exemple nous sera utile dans la suite. En effet, si l'on inverse le raisonnement, trouver qu'une telle relation est vérifiée par les données pourrait nous conduire à la découverte d'une transition temporisée. Nous formalisons donc cette relation.  Cette proposition constitue une condition nécessaire à l'existence d'une transition temporisée. Notre problème de découverte serait résolu si cette condition était également suffisante (nous disposerions d'un objet équivalent à une transition temporisée), mais ce n'est pas le cas.
Relation d'ordre sur les ensembles d'épisodes
Remarque : La réciproque de la proposition 2 est fausse.
Contre-exemple On a {{a, c ? {{a, e (car D max ({{a, c = 3 < 4 = D min ({{a, e dans les logs L 1 , alors que les transitions étiquetées par c et e sortent du même état (cf. fig. 2).
La proposition 2 et l'hypothèse de complétude des logs nous assurent que l'ensemble des expressions de la forme A ? B vérifiées par les logs englobe l'ensemble des transitions temporisées. Toutefois, ces expressions peuvent aussi nous donner, en plus, de fausses informations, sur des transitions inexistantes. Ceci vient du fait que la relation ? ne prend pas en compte l'ensemble des informations induites par la présence d'une transition temporisée. C'est pourquoi nous définissons dans la suite une relation plus riche sur les ensembles d'épisodes.
Expiration
Définition 6 (Expiration) Soient m ? M sg, et A, B ? P m (A, B = ?). On dira que les logs L satisfont l'expiration E(m, A, B), ce que l'on notera L E(m, A, B), si : Exemple 6 Les logs L 1 satisfont les expirations, E(a, {c, d, e}, {g}), E(a, {c, d}, {g}), Remarque : La réciproque de la proposition 3 est fausse.
Contre-exemple L'expiration E(b, {f }, {g, h}) est satisfaite par les logs L 1 , bien qu'il n'y ait pas de transition temporisée entre les états s 1 et s 3 du protocole P 1 (cf. figure 2). Par contre, il existe une chaîne formée de deux transitions temporisées, reliant ces deux états.
RNTI -E -La proposition 3 et l'hypothèse de complétude des logs nous assurent que chaque transition temporisée du protocole de conversation peut être retrouvée par l'intermédiaire d'une certaine expiration satisfaite par les logs. Cependant, une expiration est satisfaite entre deux ensembles d'épisodes, aussi bien en présence d'une transition temporisée entre les états correspondant à ces ensembles d'épisodes, que d'une chaîne de transitions temporisées. Nous allons donc défi-nir une classe d'expirations plus restreinte, afin d'éviter cette ambiguïté. Un autre problème lié aux expirations est qu'elles sont beaucoup plus nombreuses que les transitions temporisées.
Exemple 7 Dans le cas du protocole P 1 (cf. figure 2), la transition temporisée présente entre les états s 1 et s 2 entraîne la satisfaction, par les logs L 1 , de l'expiration E(b, {f }, {c, d, e}), mais aussi de E(b, {f }, {c, d, e, g, h}) ; celle qui relie les états s 2 et s 3 entraîne la satisfaction de l'expiration E(a, {c, d, e}, {g}), mais aussi de E(a, {c, d}, {g}).
Cet exemple illustre le fait que plusieurs formes de « redondance » apparaissent. Or, nous voulons apporter le minimum d'informations nécessaires à l'utilisateur pour retrouver les transitions temporisées. C'est donc dans ce sens que nous définissons les expirations propres. 
Expiration propre Définition 7 (Expiration propre)
RNTI -E -
Exemple 8 Les expirations propres satisfaites par les logs L 1 (cf. figure 2) sont reportées dans le tableau 1 (rappelons que les ensembles P a et P b sont traités indépendamment l'un de l'autre). On vérifie également que : {c, d, e, g, h}), car {{b, c d e ? {{b, g h ; -L 1 EP (a, {c, d}, {g}), car {{a, e {{a, c d g 
Remarque : La réciproque de la proposition 4 est fausse.
Contre-exemple On a L 1 EP (a, {h}, {g}), alors que les transitions étiquetées par h et g sortent du même état (cf. figure 2). Ceci s'explique par le fait que, dans les logs L 1 , après que le message a ait été émis, le message g est toujours plus long à émettre que le message h.
D'après la proposition 4 et l'hypothèse de complétude des logs, puisque chaque transition temporisée engendre la satisfaction d'une expiration propre dans les logs, il est possible de toutes les retrouver. Toutefois, on peut découvrir plus d'expirations propres qu'il n'y a de transitions temporisées, dans le cas où certains messages sont toujours plus long à envoyer (ou à recevoir) que tous les messages associés aux autres transitions du même état. Le théorème suivant exprime le fait que ceci constitue le seul cas d'erreur possible.
, alors il existe dans le protocole de conversation : -ou bien deux états s 1 et s 2 tels que s 2 soit relié à s 1 par une transition temporisée, A corresponde à un sous-ensemble des transitions sortant de s 1 , et B corresponde à un sousensemble des transitions sortant de s 2 , -ou bien un état s tel que A ? B corresponde à un sous-ensemble des transitions sortant de s, et les messages de B soient toujours plus longs à émettre que les messages de A.
Bien que l'on ne puisse établir une correspondance totale entre ces objets, les expirations propres représentent, en pratique, le meilleur équivalent possible des transitions temporisées. En effet, le théorème 1 nous assure que, si l'on découvre une expiration propre dans les logs, alors il existe une transition temporisée dans le protocole de conversation, ou alors on est en présence de messages plus longs à émettre que d'autres, sachant que les logs seuls ne permettent pas de déceler si l'on se trouve dans un tel cas de figure. Ce résultat justifie la pertinence de la mise en place d'une méthode de découverte des transitions temporisées basée sur la recherche des expirations propres satisfaites par les logs.
La méthode de découverte « naïve » consiste à générer toutes les expirations propres possibles, et à tester pour chacune d'entre elles si les conditions de la définition 7 sont vérifiées. Cette méthode est cependant doublement exponentielle car, d'une part le nombre d'expirations propres possibles est exponentiel, et d'autre part pour chaque couple (A, B) de sousensembles de P m comparables grâce à la relation ?, il est nécessaire de vérifier que tous les RNTI -E -sous-ensembles de A et de B sont incomparables. Aussi, travaillons-nous actuellement à la définition d'une caractérisation des expirations propres conduisant à un algorithme de décou-verte polynomial. Cette caractérisation est basée sur la construction d'une partition de chaque sous-ensemble P m d'épisodes. Elle est actuellement en cours de démonstration.
Rappelons que les éléments de la partition {P m | m ? M sg} de Ep sont traités séparé-ment. A chaque partie P m va donc correspondre un ensemble d'expirations propres qui lui sont associées. Ce procédé peut sembler redondant, dans le sens où, si deux transitions étiquetées respectivement par les messages a et b arrivent sur un même état, d'où sort une transition temporisée, nous allons trouver que deux expirations propres différentes sont satisfaites dans les logs (une pour P a et une autre pour P b ), et les interpréter comme étant deux transitions temporisées différentes. Il est possible de résoudre ce problème en faisant des recoupements entre les différents ensembles d'expirations propres. Ceci permettra également de rejeter certaines expirations propres qui ne peuvent correspondre à des transitions temporisées.
Exemple 9 Considérons les expirations propres satisfaites par les logs L 1 (cf. tableau 1). L'expiration propre EP (a, {h}, {g}) (associée à P a ) ne peut correspondre à une transition temporisée, car h et g interviennent ensemble dans l'expiration propre EP (b, {c, d, e}, {g, h}) (associée à P b ). D'après le théorème 1, on sait que h et g correspondent à des transitions sortant du même état. Finalement, on trouve deux transitions temporisées : l'une correspondant à EP (a, {c, d, e}, {h}) et EP (b, {c, d, e}, {g, h}), et l'autre à EP (b, {f }, {c, d, e}).
Conclusions et perspectives
Notre travail se situe dans le contexte de l'extraction du protocole de conversation temporisé d'un service Web à partir de ses logs d'exécution. Il traite de la découverte des transitions temporisées, et constitue, à notre connaissance, la première contribution apportée à la résolu-tion de ce problème. Notre apport consiste en un cadre formel aboutissant à la définition de la notion d'expiration. Nous avons montré que l'ensemble des expirations propres satisfaites par les logs constitue une caractérisation de l'ensemble des transitions temporisées présentes dans le protocole de conversation d'un service.
Du fait qu'il concerne les aspects temporels du protocole de conversation, notre résultat s'inscrit en complément des travaux existants. Nous envisageons d'intégrer l'algorithme de découverte des transitions temporisées sur lequel nous travaillons à la méthode d'extraction du protocole de conversation (non temporisé) proposée par Motahari et al. (2006), au sein d'une plateforme commune de gestion de services Web. Ceci nous permettra de pouvoir effectuer des tests à grande échelle de notre méthode.
Signalons également que nous avons comme objectif d'élargir le cadre formel présenté ici. Nous envisageons pour cela d'essayer de relâcher les contraintes fixées au départ (par exemple le fait que les transitions du protocole de conversation soient étiquetées de façon unique, ou qu'il n'existe pas de transition temporisée menant dans un état final). La solution permettant de pallier ces limites pourrait être d'effectuer un pré-traitement sur les données, afin de se ramener au cas particulier défini par nos hypothèses de travail. Il serait également intéressant de prendre en compte le « bruit » présent dans les données.
RNTI -E -

Introduction
La gestion des connaissances dans l'organisation est abordée dans cet article comme la finalité des traitements cognitifs sur les informations générées, transformées ou acquises par l'organisation. Ainsi, nous pouvons distinguer deux types complémentaires de connaissances qui permettent à l'organisation d'être notamment réactive : la connaissance de l'environnement dans lequel l'organisation s'insère et avec lequel elle interagit, ainsi que la connaissance propre de l'organisation. Notre démarche s'adresse essentiellement à toutes les connaissances détenues par l'organisation. En effet, une bonne « connaissance » organisationnelle, avec tout ce qu'elle comprend de formel (organigramme, relations et hiérarchies explicites) et de plus informel (relations personnelles, affinités,…) est à la base de la performance. Cette connaissance constitue un élément essentiel de continuité pour le pilotage de l'entreprise : elle englobe le savoir relatif au marché (stratégies, fournisseurs, clients, concurrents), mais aussi et surtout les caractéristiques internes de l'entreprise (alliances, jeux de pouvoir, relations interpersonnelles). Cette connaissance est principalement détenue par les acteurs de l'organisation et il est nécessaire d'en assurer la conservation ou du moins d'en éviter l'évaporation. Cependant, comment cette connaissance de l'organisation est-elle appréciée par les acteurs de l'organisation ? Comment voient-ils et perçoivent-ils l'organisation ? Comment les connaissances sont-elles réparties dans l'organisation ? Toutes ces questions restent en suspens du fait notamment de la difficulté à identifier, localiser, et évaluer à proprement parler ces connaissances. Dans ce contexte, l'information détenue par l'organisation est vue comme le support de connaissances. Nous proposons une approche basée sur ces informations afin d'obtenir une vue globale des connaissances de l'organisation. Une représentation macroscopique des connaissances peut s'appuyer sur des systèmes de capitalisation (mémoire d'entreprise par exemple), mais aussi, et surtout, sur les connaissances détenues par les individus et non forcément capitalisées dans un système global de gestion de connaissances. Outre cette vue macroscopique, nous proposons une déclinaison microscopique de cette visualisation au niveau des différents acteurs. Cette visualisation met en évidence les connaissances détenues par chaque acteur mais également les connaissances « dynamiques » (connaissances en action) véhiculées au travers des échanges qu'ils entretiennent avec les autres acteurs de l'organisation. Ainsi, outre la vision statique des connaissances, nous proposons une vision de leur dynamique (circulation, échange, transformation) au sein de l'organisation. Le but de ces niveaux de visualisations est de permettre aux acteurs de mieux apprécier les connaissances de l'organisation pour, par exemple, rapidement et simplement identifier ou localiser une personne experte d'un domaine ou pour aider un nouvel arrivant à s'insérer dans l'organisation (problème de « Turn-Over »). Cette proposition repose sur des modèles d'individu associés à des cartes auto-organisatrices permettant de proposer une représentation graphique et un support de navigation dans l'espace des connaissances de l'organisation.
Le présent papier est structuré comme suit. La section 2 vise à faire une synthèse sur la gestion des connaissances associée aux outils de cartographie afin de souligner la complémentarité de notre démarche avec les propositions de ces domaines. Dans la section 3, nous proposons les visualisations macroscopiques et microscopiques des connaissances basées sur les modèles d'individu et sur les connaissances en action, avant de conclure.
Gestion des connaissances
« Dans une économie où la seule certitude est l'incertitude, l'unique source d'avantage concurrentiel durable est le savoir » (Nonaka, 2000). De ce constat est née la nécessité de capitaliser les connaissances d'une organisation et dans le même temps de cartographier ces connaissances afin d'en assurer une bonne visibilité pour les membres organisationnels. Le but de cette cartographie est de permettre aux acteurs de localiser les connaissances dans l'organisation pour les réutiliser afin d'en créer de nouvelles ou de prendre des décisions évitant ainsi des coûts conséquents pour l'organisation. Les coûts visés ici sont ceux pouvant être liés à la complexité de la recherche de la « bonne connaissance », de non-utilisation de la connaissance ou de reconstruction de connaissances existantes et, enfin, d'évaporation de connaissances.
Capitalisation des connaissances
Les connaissances, d'après (Skyrme, 1999), « ne sont ni des données, ni des informations, mais se définissent bien plus comme une capacité humaine acquise avec le temps et consistant à relier les informations en leur donnant du sens ». Ainsi, du point de vue des sciences de Gestion, l'information peut être considérée comme étant la matière première des connaissances. Cette idée est confortée par (Grundstein, 2002), qui présente le processus de création des connaissances par un individu à partir des informations qu'il reçoit et la création d'informations à partir de ses propres connaissances.
Du point de vue stratégique, la gestion des connaissances a deux finalités : ? une finalité patrimoniale : les organisations tentent de consolider leur patrimoine « intellectuel » afin d'accroître leur compétitivité. Comment préserver les connaissances, les réutiliser et les actualiser ?, ? une finalité d'innovation durable : au travers de l'apprentissage collectif (ou organisationnel), l'organisation tente d'accroître les connaissances de chaque individu tout en améliorant la connaissance collective (partagée entre les individus). En ce qui concerne la finalité patrimoniale, la communauté des chercheurs a beaucoup travaillé sur les mémoires d'entreprise. Une mémoire d'entreprise permet de structurer la connaissance sous forme d'une collection documentaire organisée et primordiale pour l'organisation. Elle permet donc de mémoriser et organiser le capital intellectuel détenu par cette dernière. La structuration de la mémoire d'entreprise offre un support à la manipulation (interrogation, visualisation de l'évolution) de la connaissance capitalisée. Les travaux relatifs à la mémoire d'entreprise s'intéressent donc aux différentes problématiques de la gestion des connaissances : repérer, actualiser, valoriser, préserver, « manager » la connaissance (Grundstein, 2004). Le lecteur pourra se reporter à (Balmisse, 2002) pour un panorama de ces technologies et leur utilisation en gestion des connaissances. La capitalisation des connaissances permet donc d'aider un individu à (re)construire les connaissances nécessaires à ses activités à partir de connaissances préexistantes. Un critère important de ces mémoires d'entreprise est la façon dont celles-ci restituent les connaissances de l'organisation à ses membres. Il s'agit là d'une problématique de visualisation plus que de construction des connaissances : ce problème bien connu est central, du fait de la difficulté à apprécier une masse importante d'informations/connaissances ; les nombreux appels d'offres autour de la problématique des masses de données en témoignent, tout comme la littérature sur la visualisation de telles grandes masses de données.
Cartographie des connaissances
Même si de nombreuses représentations de la cartographie de l'information existent (Kartoo, 2005), la cartographie des connaissances sous la forme d'une représentation visuelle est essentiellement produite sous forme de graphes. Par exemple, (Debourges et al, 2001) ou (Trébucq, 2005) se basent sur des lexicogrammes. Nous pouvons également citer (Abdenour, 2004) qui présente l'outil VICOTEXT proposant une approche originale de visualisation automatique et dynamique des connaissances textuelles. Ces travaux proposent une représentation visuelle des connaissances sous forme de graphes de termes. Cette visualisation se justifie car ces travaux se basent sur les techniques de Traitement Automatique du Langage Naturel (TALN) et des graphes conceptuels.
Dans un contexte plus général nous pouvons signaler l'outil de visualisation Umap 1 qui repose sur les « arbres de la connaissance » (figure 1). Cette représentation propose une visualisation du résultat sous forme d'îlots correspondant à des ensembles thématiques d'informations.
FIG. 1 -Visualisation proposée par Umap
La principale limite de ces approches est qu'elles reposent essentiellement sur la base documentaire détenue par l'organisation, pour effectuer cette représentation graphique : c'est ce que nous appellerons la visualisation macroscopique des connaissances.
Certaines approches proposent également de visualiser l'activité de communication dans les organisations comme le propose (Wittaker et al., 2002). Cependant, nous n'avons pas identifié dans la littérature d'approche qui complète cette visualisation par une visualisation microscopique c'est-à-dire au niveau des acteurs même de l'organisation autre qu'au travers de leurs communications. En effet, celles-ci se limitent à la gestion des contacts et des communications. Or, les connaissances de l'organisation sont exploitées, diffusées, recherchées : qu'elles soient partagées ou tout simplement transmises, ces connaissances vivent. Il est donc important d'évaluer dans quelle mesure chaque acteur intervient dans cette vie de la connaissance au sein de l'organisation, pour identifier par exemple les acteurs clés de la connaissance. Ainsi, l'organisation pourra veiller à préserver le capital ainsi acquis si ces acteurs venaient à quitter l'entreprise. C'est pour cela que la visualisation microscopique que nous proposons intègre à la fois l'aspect connaissance et l'aspect communication pour évaluer l'implication de chaque acteur dans la diffusion des connaissances.
En réponse à ces problématiques, nous proposons une approche de visualisation et de navigation dans les connaissances de l'organisation. La visualisation macroscopique donne une image globale des connaissances dans l'organisation alors que la visualisation microscopique donne une image locale de ces connaissances au niveau de chaque acteur. Cette dernière visualisation basée sur les modèles d'individu permet en outre d'identifier les connaissances en action au travers des échanges que chaque acteur entretient avec les autres membres organisationnels. La navigation s'effectue du niveau macroscopique au niveau microscopique et vice et versa.
Cartographie macroscopique et microscopique des connaissances dans l'organisation
Les cartographies proposées reposent donc sur deux niveaux de représentation : le niveau macroscopique et le niveau microscopique. Ces niveaux sont détaillés dans les sections 3.3 et 3.4. Ces deux types de cartographies reposent sur une visualisation des connaissances basée sur les cartes auto-organisatrices et sur un modèle caractérisant les individus (sections 3.1 et 3.2).
Les cartes auto-organisatrices
Les cartes auto-organisatrices (Kohonen, 1982) permettent, par le biais d'une classification non supervisée des informations, d'obtenir une représentation graphique et topologique sous forme de cartes en 2D ou 3D. Ces cartes ont été utilisées par exemple en recherche d'information pour présenter de façon globale les résultats de recherche d'information de manière intuitive. La carte est une grille où chaque cellule correspond à une classe de documents similaires. Les classes sont positionnées les unes par rapport aux autres suivant leur similarité respective (figure 2). (Lesteven, 1996) propose une utilisation de ces cartes dans le domaine de l'astronomie tandis que Websom propose une application des cartes auto-organisatrices sur le web (Lagus, 1996). (Poinçot, 1999) Un grand avantage de ce type de visualisation est que l'on peut naviguer à l'intérieur c'est-à-dire aller vers le détail des connaissances contenues (« forer »). Par ailleurs, le niveau de détails est visible directement au travers des étiquettes car elles permettent de localiser des « zones » de connaissances, liées au même domaine, par exemple.
FIG. 2 -Carte auto-organisatrice réalisée à partir d'un corpus documentaire en astronomie
Le choix de cette visualisation a été fait pour sa simplicité d'interprétation et la navigation qu'elle permet contrairement à d'autres visualisations comme Umap par exemple. 
FIG. 3 -Diagramme des classes du Modèle de l'individu
Modèle d'individu sous-jacent à la cartographie
Afin de caractériser les connaissances individuelles ainsi que collectives, notre approche repose sur un modèle d'individu regroupant différents aspects descriptifs des membres organisationnels. Ce modèle d'individu (cf. figure 3) repose sur celui proposé dans (Canut et al., 2005). L'individu est caractérisé au travers de 4 dimensions complémentaires : ? Les caractéristiques cognitives qui correspondent à une formalisation des connaissances que possède l'individu. Ces dernières reposent sur tous les documents possédés par lui, que ce soient les documents situés physiquement sur son espace personnel (disque dur, répertoire réseau…) ou les documents internet pour lesquels il a sauvegardé un signet dans son navigateur. L'individu s'approprie les documents au travers de son espace d'information personnel qu'il organise sous forme de thème et sous-thèmes (Topics). La gestion de cette organisation des documents peut être assistée par des outils semiautomatiques. ? Les caractéristiques sociales ou conatives (en gras dans le diagramme UML). Ces caractéristiques permettent d'évaluer le « faire-savoir » (Canut et al., 2005)  Ces différentes caractéristiques sont exploitées à différents niveaux dans la cartographie des connaissances dans l'organisation. Nous détaillons dans les sections qui suivent les deux niveaux de cartographie proposés.
Cartographie macroscopique
Cette cartographie a pour but de proposer une vue globale des connaissances détenues dans l'organisation.
Elle peut être effectuée pour un sous-ensemble des membres de l'organisation, qu'il soit formel (groupe de travail, équipe…) ou informel (détecté au travers des échanges d'emails). Ceci permet par exemple d'identifier les connaissances mises en jeu dans ce groupe.
Pour ce faire, un premier pas consiste à rassembler les différents documents détenus par les individus concernés par l'étude. A ce stade, un lien peut être fait avec un système de connaissances existant pour intégrer d'autres documents par exemple (mémoire d'entreprise ou autre SI). Ensuite, une extraction des mots-clés caractérisant au mieux le contenu des documents est nécessaire : il s'agit de la phase d'indexation.
Pour cela nous utilisons les méthodes classiques bien connues en recherche d'information : ? Elimination des mots vides (qui n'apportent pas de sens au contenu du document) (Zipf, 1949), ? Radicalisation des termes (Porter, 1980) permettant de réduire l'espace des mots-clés en rapprochant les termes ayant des racines proches voire similaires, ? Pondération des termes en fonction de leur importance dans les documents (Salton, 1983), (Singhal, 1997).
Cette indexation sert de base à la construction d'une carte auto-organisatrice dans laquelle il est possible de naviguer ou d'obtenir des détails (« forer »). De cette carte, il est à tout moment possible de basculer vers une visualisation microscopique c'est-à-dire de détailler la connaissance au niveau des individus. Ainsi, chacun, en parcourant la carte, découvrira les membres organisationnels qui sont liés aux connaissances qu'il est en train de découvrir. Cette démarche est expliquée dans la section 3.5.
Cartographie microscopique
La cartographie microscopique met en évidence les quatre dimensions d'un membre organisationnel présentes dans le modèle de l'individu ( fig. 3).
Les caractéristiques cognitives sont présentées sous la forme d'une carte autoorganisatrice. Celle-ci est construite à partir de tous les documents possédés par l'individu. La construction de la carte est régie par le même procédé d'indexation que celui présenté dans la cartographie macroscopique.
Les caractéristiques socio-cognitives sont également présentées sous la forme d'une carte auto-organisatrice. Celle-ci est construite à partir du contenu de tous les champs « sujet » des emails qu'un membre a reçus. Nous posons l'hypothèse que l'organisation possède un outil anti-spam permettant ainsi de limiter la cartographie des connaissances aux seules informations réellement liées à l'activité de l'individu.
Nous avons décidé de séparer arbitrairement les caractéristiques cognitives et sociocognitives car elles correspondent à une sémantique différente. En effet, la carte cognitive correspond à des connaissances capitalisées par l'acteur sur le long terme alors que la carte socio-cognitive souligne l'implication de certaines de ses connaissances dans les échanges avec les autres membres organisationnels. Il est à souligner que ces deux cartes peuvent présenter des différences.
Les caractéristiques formelles sont simplement affichées dans la cartographie et permettent de localiser un membre dans l'organisation, offrant par exemple la possibilité de le contacter.
Les caractéristiques conatives sont présentées sous la forme d'un indicateur de communication qui est calculé en fonction des autres indicateurs (Canut et al., 2005). Cette valeur numérique permettra aux autres membres organisationnels d'apprécier le comportement d'un individu vis-à-vis des autres : est-il utile de contacter un individu qui ne répond pas aux emails ? Ne doit-on pas préférer quelqu'un de plus réactif ? La réponse à ces questions peut être trouvée en recoupant les caractéristiques conatives avec les caractéristiques formelles (participations à des équipes de projet, fonctions..). De plus, une représentation cartographique du réseau des relations d'un individu peut être utilisée pour visualiser les personnes en relation avec lui (Canut et al., 2005). Ces caractéristiques ne sont que secondaires et permettent d'affiner le choix de la personne à contacter par exemple.
Du point de vue fonctionnel, ces deux cartes sont distinctes mais elles permettent toutes deux de faire le lien entre la cartographie macroscopique et la cartographie microscopique. Ainsi, à partir de ces cartes, il est possible de revenir à la cartographie macroscopique aux différents points sur la carte où l'on peut retrouver le membre organisationnel.
Passage macroscopique microscopique
Le passage du niveau macroscopique au niveau microscopique est effectué grâce aux cartes elles-mêmes. Chaque cellule correspondant à un ensemble de documents ou de sujets d'emails, nous pouvons réutiliser ces ensembles pour trouver des « ponts » entre les niveaux macroscopiques et microscopiques.
Le procédé peut être réalisé par le biais d'une mesure de similarité entre les différentes cellules des cartes. Pour cela, nous utilisons un descripteur du contenu de chaque cellule que nous allons comparer pour obtenir cette similarité. Ce descripteur peut par exemple être calculé par une méthode de construction de classifieur (Sebastiani, 2002) comme la technique dite de Rocchio (Rocchio, 1971). Cette technique permet, à partir d'une représentation des documents sous la forme de liste pondérée de termes (issue par exemple d'une phase d'indexation) de construire un vecteur représentant de l'ensemble des documents. Ce vecteur représentant maximise le poids des termes des documents faisant effectivement partie de la cellule et minimise le poids des termes se trouvant dans les autres cellules. Nous pouvons alors utiliser la mesure cosinus par exemple pour calculer la similarité entre deux vecteurs représentants caractérisant deux cellules.
Si la similarité est suffisante (i.e. supérieure à un seuil fixé), un lien entre ces cellules est établi et une possibilité de navigation de l'une à l'autre des cartes est alors possible. Ce procédé est illustré dans la figure 4. Ce seuil de similarité peut être fixé ou mis à jour automatiquement en utilisant un calcul de seuil optimal (Wu, 2001). Ainsi nous proposons de calculer et de mettre à jour ce seuil au même rythme que la génération des cartes ellesmêmes. Pour ce faire, nous utilisons la répartition des documents sur les cartes autoorganisatrices générées. Pour chaque cellule des cartes, un seuil optimal est calculé. Ce seuil permet de maximiser le nombre de documents pertinents et limiter le nombre de documents non pertinents dans la cellule. Afin d'obtenir un seuil de similarité suffisamment discriminatoire c'est-à-dire optimal pour chaque cellule, différentes approches ont été proposées (Hoashi, 1999)  (Wu, 2001). Dans notre cas, nous utiliserons une approche qui maximise la formule F 1 (1) comme dans (Ruiz, 2001). Cela signifie que le seuil correspond à la similarité entre, d'une part, le vecteur représentant la cellule, et, d'autre part, le document qui maximise cette fonction F1 dans cette cellule. Cette mesure de performance facile à calculer est définie comme suit :
où P représente la valeur de Précision (3) et R la valeur de Rappel (2) pour chaque cellule de la carte, calculées selon les formules (Salton, 1983)  
FIG. 4 -Evaluation des liaisons entre niveau macroscopique et microscopique
Ces différentes techniques supposent que toutes les cartes soient générées au moment de la navigation. Or, la limite d'utilisabilité des cartes auto-organisatrices est le temps de calcul. Même si des optimisations existent (Prudhomme, 2005), nous proposons que ces cartes soient établies de manière régulière puis stockées. En effet, même si l'organisation est un environnement en perpétuel mouvement dans lequel les acteurs évoluent, nous pouvons considérer que les modifications ne sont que « légères » et donc que l'échelle de temps entre deux cartographies peut être assez importante (semaine, mois). Une échelle de temps trop petite n'occasionnerait qu'une surcharge du système d'information de l'organisation, sans amélioration significative des visualisations proposées aux acteurs.
Conclusion et perspectives
Les travaux actuels sur les mémoires d'entreprise apportent des solutions significatives au problème de la gestion des connaissances dans les organisations, en particulier dans les phases d'acquisition, mémorisation, capitalisation des connaissances. Parallèlement et en complémentarité de ces travaux, l'accès et la réutilisation de ces connaissances reste une problématique très actuelle. C'est dans ce contexte que nous proposons un modèle de représentation associé à des possibilités de visualisation des connaissances à deux niveaux.
Le premier niveau de visualisation, dit niveau macroscopique, permet de visualiser les connaissances de l'organisation dans leur ensemble en s'appuyant sur le système d'information de l'organisation et utilise des techniques déjà bien identifiées.
Le deuxième niveau de visualisation, dit niveau microscopique, permet de visualiser les connaissances qui sont détenues mais également qui transitent au niveau de chaque membre de l'organisation. Pour ce faire, nous nous appuyons sur un modèle de l'individu décrit selon quatre dimensions : formelle, cognitive, conative, socio-cognitive.
Ces deux niveaux de visualisation sont mis en oeuvre au travers de cartes autoorganisatrices.
Le passage d'un niveau à l'autre permet de passer d'une connaissance détenue par l'organisation à l'individu qui la possède, la diffuse, la réutilise et vice versa.

Introduction
Les arbres de décision utilisés depuis longtemps en statistique (Morgan et Sonquist, 1963;Kass, 1980) sont devenus, suite aux ouvrages de Breiman et al. (1984) et Quinlan (1993) des outils très populaires pour générer des règles de classification et plus généralement des règles de prédictions. On parle ainsi d'arbre de classification lorsque la variable à prédire est catégo-rielle et que ses valeurs représentent donc des classes. Cependant, et contrairement à ce que l'expression d'arbre de classification peut laisser entendre, la classification n'est pas le seul intérêt des arbres de décisions. Par exemple, en sciences sociales où il s'agit plus de comprendre comment des prédicteurs peuvent affecter les valeurs prises par la variable à prédire que de classer des individus, ils peuvent avoir un intérêt descriptif, ou encore, comme en marketing notamment, on peut les utiliser dans une optique de ciblage. Dans ce dernier cas, plutôt que de prédire la valeur de la réponse, il s'agit de repérer les profils typiques des individus appartenant à chacune des classes de la variable à prédire. On inverse en quelque sorte le problème en cherchant à caractériser les profils propres à la classe, plutôt que la classe à partir du profil. L'évaluation de la qualité de l'arbre se fonde le plus souvent sur le taux d'erreur de classification. Ce taux de cas mal classés par les règles, qu'il soit calculé sur les données d'apprentissage, des données test ou encore en validation croisée est évidemment pertinent comme mesure de qualité des règles quand l'objectif est la classification proprement dite. Il ne l'est cependant plus lorsque l'on utilise l'arbre à d'autres fins, et il s'agit alors d'exploiter d'autres mesures mieux adaptées. Dans (Ritschard et Zighed, 2004;Ritschard, 2006), nous avons par exemple proposés des mesures de type déviance qui permettent de juger de la qualité descriptive de l'arbre en évaluant son aptitude à prédire la distribution de la variable réponse pour un profil donné. Ici, nous nous intéressons au cas du ciblage. Quelle information nous donne la règle sur la typicité du profil -la prémisse de la règle -pour sa conclusion ? Nous proposons de mesurer cette typicité à l'aide des concepts de l'analyse statistique implicative.
L'analyse statistique implicative introduite par Régis Gras (Gras, 1979;Gras et Larher, 1992;Gras et al., 1996) comme outil d'analyse de données, a connu ces dernières années un essor remarquable dans le cadre de la fouille de règles d'association du type « si l'on observe A alors on devrait aussi observer B » (Suzuki et Kodratoff, 1998;Gras et al., 2001Gras et al., , 2004. Son principe fondamental consiste à juger de la pertinence d'une relation de dépendance en fonction de la fréquence de ses contre-exemples. Une règle avec peu de contre-exemples est considérée comme plus implicative qu'une règle pour laquelle les contre-exemples sont fré-quents. Curieusement, et bien que nous ayons montré (Ritschard, 2005) qu'elle s'appliquait sans difficulté aux règles issues d'arbres, cette idée de force d'implication n'a guère été exploitée dans le contexte de l'apprentissage supervisé. Or, la force d'implication d'une règle évaluée par l'écart entre le nombre observé de contre-exemples et le nombre moyen que générerait le seul hasard correspond précisément à la notion de typicalité du profil pour la conclusion qui nous intéresse ici.
L'article est organisé comme suit. En section 2, nous rappelons les concepts d'indice et d'intensité d'implication et leur utilisation associée à un arbre de classification. Nous discutons ensuite (toujours en section 2) de l'analogie entre indice d'implication et résidus issus de la modélisation de tables de contingence et de l'intérêt de ces résidus comme mesures alternatives de la force d'implication. En section 3 et 4, nous illustrons sur un exemple réel deux utilisations de l'intensité d'implication : évaluer a posteriori les règles issues d'un arbre, et effectuer le choix de la conclusion d'une règle. Enfin, nous présentons des remarques conclusives et des perspectives de développement à la section 5.
Arbres et indice d'implication
Les arbres de classification sont des outils de classification supervisés. Ils déterminent des règles de classification en deux temps. Dans une première étape, une partition de l'espace des prédicteurs (x) est déterminée telle que la distribution de la variable (discrète) à prédire (y) diffère le plus possible d'une classe à l'autre de la partition et soit, dans chaque classe, la plus pure possible. La partition se fait successivement selon les valeurs des prédicteurs. On commence par partitionner les données selon les modalités de l'attribut le plus discriminant, puis on répète l'opération localement sur chaque noeud ainsi obtenu jusqu'à la réalisation d'un critère d'arrêt. Dans un second temps, après que l'arbre ait été généré, on dérive les règles de classification en choisissant la valeur de la variable à prédire la plus pertinente, en général simplement la plus fréquente, dans chaque feuille (noeud terminal) de l'arbre.
Pratiquement, on relève dans chaque feuille j, j = 1, . . . , le nombre n ij de cas qui sont dans l'état y i . Ainsi, on peut récapituler les distributions au sein des feuilles sous forme d'une table de contingence croisant les états de la variable y avec les feuilles (Tableau 1). On peut noter que la marge de droite de ce tableau qui donne le total des lignes correspond en fait à la distribution des cas dans le noeud initial de l'arbre.  Gras et al., 2004, p 19) d'une règle se définit à partir des contre-exemples. Dans notre cas il s'agit dans chaque feuille (colonne du tableau 1) du nombre de cas qui ne sont pas dans la catégorie majoritaire. Ces cas vérifient en effet la prémisse de la règle, mais pas sa conclusion. En notant b la conclusion (ligne du tableau) 1 de la règle j et n bj le maximum de la jème colonne, le nombre de contre-exemples est n¯ bj = n ·j ? n bj . L'indice d'implication est une forme standardisée de l'écart entre ce nombre et le nombre espéré de contre-exemples qui seraient générés en cas de répartition entre valeurs de la réponse indépendante de la condition de la règle.
Formellement, l'hypothèse de répartition indépendante de la condition, que nous notons H 0 , postule que le nombre N¯ bj de contre-exemples de la règle j résulte du tirage aléatoire et indépendant d'un groupe de n ·j cas vérifiant la prémisse de la règle j et d'un autre de n¯ b· = n ? n b· cas qui ne vérifient pas la conclusion de la règle. Sous H 0 et conditionnellement à n b· et n ·j , le nombre aléatoire N¯ bj de contre-exemples est réputé (Lerman et al., 1981) suivre une loi de Poisson de paramètre n e ¯ bj = n¯ b· n ·j . Ce paramètre n e ¯ bj est donc à la fois l'espérance mathématique et la variance du nombre de contre-exemples sous H 0 . Il correspond au nombre de cas de la feuille j qui seraient des contre-exemples si l'on répartissait les n ·j cas de j selon la distribution marginale, celle du noeud initial de l'arbre (ou marge de droite du tableau 1).
L'indice d'implication de Gras est l'écart n¯ bj ? n e ¯ bj entre les nombres de contre-exemples observés et attendus sous l'hypothèse H 0 , standardisé par l'écart type, soit
En termes de cas vérifiant la condition, cet indice s'écrit encore Pour expliciter le calcul de l'indice, on considère la variable « classe prédite » qui prend la valeur 1 pour chaque cas (exemple) appartenant à la classe majoritaire de sa feuille d'appartenance, et 0 pour les autres (contre-exemples). On note cette variable cpred. En croisant cette variable avec les conditions des règles, on obtient le tableau 2 où la première ligne donne pour chaque règle j son nombre n¯ bj de contre-exemples et la seconde ligne le nombre n bj de cas vérifiant la règle. De même, le tableau 3 donne les nombres espérés n e bj d'exemples et n e ¯ bj de contre-exemples dans le cas d'une répartition des cas couverts par chaque règle j selon la distribution marginale. Il est important de noter que ces effectifs attendus ne se déduisent pas des marges du tableau 2. Ils s'obtiennent en répartissant tout d'abord les cas selon la distribution marginale du tableau 1 et en procédant ensuite aux regroupements selon la classe majoritaire observée dans chaque colonne du tableau 1.
Indice d'implication et résidus
Dans sa formulation (1), l'indice d'implication a l'apparence d'un résidu standardisé du type (racine signée de) contribution au khi-deux de Pearson (voir par exemple Agresti, 1990, p.224). Il s'agit en fait de la contribution au khi-deux mesurant la « distance » entre les tableaux 2 et 3. En effet, il suffit de remarquer que le khi-deux ainsi défini peut s'écrire :
On reconnaît alors sous le premier signe de sommation dans l'expression (3) le carré de l'indice d'implication de Gras. Cette interprétation de l'indice d'implication en termes de ré-sidu (résidu de l'ajustement du nombre de contre-exemples par le modèle d'indépendance H 0 ), suggère que d'autres formes de résidus utilisés dans le contexte de la modélisation de tables de contingence puissent également s'avérer intéressantes pour mesurer la force d'implication d'une règle. En particulier on peut citer : 
)|, qui est la racine signée de la contribution (en valeur absolue) au khi-deux du rapport de vraisemblance (Bishop et al., 1975, pp.136-137).  (Bishop et al., 1975, p.137).
Le résidu standardisé, qui correspond à l'indice d'implication de Gras, est connu pour avoir une variance inférieure à 1. Le problème est que dans la pratique les nombres n b· et n ·j dé-pendent de l'échantillon considéré et sont donc eux-mêmes aléatoires. Ainsi n e ¯ bj n'est qu'une estimation du paramètre de la loi de Poisson. On doit alors tenir compte du fait que dans la formule (1), le dénominateur n'est qu'une estimation de l'écart type. Les résidus déviance, de Freeman-Tukey et ajusté sont mieux adaptés à cette situation et sont réputés avoir dans la pratique une distribution plus proche de la normale N (0, 1) que le simple résidu standardisé. Ce dernier, et par conséquent l'indice d'implication de Gras, tend à sous-estimer la force d'implication.
La figure 1 montre les valeurs des résidus (en ordonnée) en fonction de la proportion n bj /n ·j (en abscisse) de cas qui dans la feuille j vérifient la conclusion b de la règle. Les courbes sont représentées pour n = 100, une règle j associée à une feuille couvrant 20% des cas, et une proportion marginale n b· /n de cas vérifiant la conclusion b de 50%. A gauche de ce seuil, les indices prennent tous une valeur positive indiquant que la règle fait moins bien que le hasard. On peut relever le comportement curieux du résidu déviance dont la valeur tend vers 0 lorsque le nombre de contre-exemples tend vers 0. Cela suggère que la règle devient non implicative quand le nombre de contre-exemples devient nul, ce qui n'est évidemment pas sastisfaisant. L'indice de Gras et le résidu ajusté évoluent de manière linéaire avec la proportion n bj /n ·j , le résidu ajusté prennant ses valeurs sur une étendue plus importante. Quant au résidu de Freeman-Tukey, on relève que sa variation s'accélère lorsque le taux de biens classés de la règle approche de 1.
Intensité d'implication et p-valeur
Il est naturel de s'intéresser à la p-valeur, ou degré de signification, des indices d'implication observés. Cette p-valeur correspond à la probabilité p(N¯ bj ? n¯ bj |H 0 ). Quand n e ¯ bj est petit, le calcul peut se faire, conditionnellement à n b· et n ·j , avec la loi de Poisson de paramètre n e ¯ bj . Pour n e ¯ bj grand (? 5), la loi normale donne une bonne approximation, à condition toutefois de procéder à la correction pour la continuité, la différence pouvant atteindre encore 2.6 points de pourcentage pour n = 100. La figure 2 montre les fonctions de répartition de la loi de Poisson et de la loi normale avec et sans correction de continuité pour n e ¯ bj = 5. On peut relever que l'approximation par la loi normale, en particulier avec la correction pour la continuité, reste bonne même pour n e ¯ bj relativement petit. Ainsi, en notant ?(·) la fonction de distribution d'une normale standardisée, on a
On appelle intensité d'implication (Gras et al., 1996) le complémentaire à 1 de cette pvaleur. Gras et al. (2004) la définissent en termes de l'approximation normale (4), mais sans la correction pour la continuité. Pour notre part, nous la calculerons pour une règle j comme
Dans tous les cas, cette intensité s'interprète comme la probabilité d'obtenir, sous l'hypothèse H 0 , un nombre de contre-exemples supérieur à celui observé pour la règle j. majoritaire et leurs prémisses définies par les chemins, mutuellement exclusifs, qui mènent aux feuilles. Les sept règles sont explicitées au tableau 5. Remarquons en premier lieu que toutes les règles sauf une concluent à « buveur occasionnel », modalité majoritaire au noeud initial de l'arbre. Ceci est typique de situations de déséquilibre (répartition des modalités de y éloignée de la situation d'équiprobabilité), où les algorithmes d'apprentissage ont parfois du mal à discriminer les différentes classes. On relève également, qu'aucune règle ne conclut à « jamais ». La faible représentativité au noeud initial de l'arbre de cet état induit une difficulté à trouver des règles isolant ces individus. Le tableau 6 présente la classique matrice de confusion associée à cet arbre. Les défauts cités jusqu'ici y apparaissent de façon plus flagrantes, en particulier l'erreur associée à la modalité « jamais ». Les valeurs des résidus définis en section 2.1 sont présentées pour chacune des règles dans le tableau 7. Les valeurs négatives indiquent que le nombre de contre-exemples observé est inférieur à celui attendu sous la condition d'indépendance entre la prémisse et la conclusion de la règle. Dès lors, les valeurs négatives sont synonymes de « qualité ». La règle R6 ((syst2 ? 111) et (chlst < 278.5) et (syst1 < 157.5) ? buveur occasionnel) pour laquelle les résidus sont positifs, est une règle qui fait moins bien que l'indépendance au sens que le nombre de contre-exemples observé est supérieur au nombre moyen que générerait le hasard. La règle peut donc être considérée comme non pertinente.
Il est intéressant ici de faire une comparaison de la qualité implicative avec le taux d'erreur communément utilisé pour l'évaluation de règles de classification. Le nombre de contreexemples considérés est précisément le nombre d'erreurs produites par la règle sur l'échan-tillon d'apprentissage. Le taux d'erreur correspond ainsi au pourcentage de contre-exemples parmi les cas couverts par la règle, soit n¯ bj /n ·j pour la règle j, ce qui est encore le complé-mentaire à 1 de la confiance. Le taux d'erreur souffre donc des mêmes inconvénients que la confiance. En particulier, il ne nous dit rien sur ce que la règle apporte de plus qu'une clas- TAB. 7 -Valeurs des résidus pour chacune des règles de l'arbre.
sification indépendante de toute condition. Pour notre règle R6 par exemple, la confiance est de 55% contre 56% pour le classifieur naïf consistant à classer tout le monde comme « buveur occasionnel », classe la plus fréquente au noeud initial. La question est évidemment de savoir quoi faire d'une règle non pertinente d'un point vue implicatif. On peut soit décider de la conserver si le but est la qualité globale de classification. Dans le cas où, au contraire, l'on veut privilégier la force implicative de chaque règle, deux solutions sont envisageables :
-fusionner la règle avec une de ses règles soeurs ; -changer la conclusion de la règle. En fusionnant les règles R6 et R7, ce qui revient à élaguer la branche non pertinente de l'arbre, on obtient une nouvelle règle ((syst2 >= 111) et (chlst < 278.5) ? buveur occasionnel). Les valeurs des résidus pour cette nouvelle règle sont : res std = 1.11, res dev = 4.5, res F T = 1.11, res adj = 2.73. Ils sont positifs et indiquent clairement une détérioration par rapport à la situation précédente. En fait, on peut observer sur l'arbre de la figure 3 qu'en remontant la branche à partir de la feuille correspondant à la règle R6, on ne rencontre que des noeuds où la classe majoritaire « occasionnellement » a une fréquence inférieure à celle relevée au noeud initial. La fusion ne peut donc pas être une solution dans ce cas particulier tant que l'on garde le principe de la classe majoritaire pour le choix de la conclusion. Ceci nous amène donc à discuter l'autre solution consistant à changer la conclusion de la règle en choisissant la modalité qui maximise l'intensité d'implication.
Choix de la conclusion des règles
Si l'objectif est de maximiser l'intensité d'implication des règles, dans le but en particulier de déterminer les profils les plus caractéristiques de chaque état de la variable à pré-dire, il semble naturel de choisir la conclusion de la règle qui maximise cette intensité plutôt que la classe majoritaire. L'idée de choisir ainsi la classe maximisant l'intensité d'implication (i.e. minimisant le résidu) a notamment déjà été exploitée par Zighed et Rakotomalala (2000, pp.282-287). A titre d'exemple, nous donnons dans le tableau 8 la conclusion sélectionnée par cette procédure pour chacune des règles et selon le résidu utilisé comme critère de choix. On observe que si les conclusions restent celles de la classe majoritaire pour les règle R1, R4 et R5, le principe de la maximisation de l'implication donne des conclusions différentes Imp(j) Déviance Freeman- Tukey  Ajusté  Majorité  R1  2  2  2  2  2  R2  1  1  1  1  2  R3  2  3  2  3  2  R4  2  2  2  2  2  R5  2  2  2  2  2  R6  1  1  1  1  2  R7  3  3  3  3  3 1 = jamais, 2 = occasionnellement, 3 = régulièrement TAB. 8 -Conclusion selon le résidu utilisé comme critère.
pour les quatre autres règles. Pour la règle R3, la conclusion varie entre « occasionnellement » et « fréquemment » selon le critère implicatif retenu. Pour les règles R2, R6 et R7 les quatre indices d'implication conduisent au même résultat. Il est intéressant de relever également, qu'avec ce critère implicatif, chacune des trois modalités de la variable à prédire est retenue comme conclusion pour au moins une règle. De plus, on peut souligner que, dans tous les cas, les indices d'implication -dont les valeurs ne sont pas montrées ici -restent négatifs. Une expérience intéressante consiste à recalculer la matrice de confusion nouvellement obtenue. Le tableau 9 montre cette dernière pour le cas où l'on utilise le résidu standardisé, soit l'indice de Gras. Le taux d'erreur global est évidemment plus élevé qu'au tableau 6 ce qui n'est pas surprenant puisqu'on ne vise plus ici à minimiser l'erreur de classification. Le tableau fournit cependant des enseignements utiles sur deux plans. Premièrement, on peut observer que la maximisation de l'intensité d'implication améliore considérablement la valeur des mesures de rappel intra-classe pour les modalités faiblement représentées. On a également confirmation qu'il n'y pas ici, et contrairement au tableau 6, d'état de y qui ne puisse être prédit par au moins une règle.
Ensuite, et c'est ici l'intérêt principal du choix de la conclusion selon le principe de la maximisation de l'intensité, la matrice fait ressortir que les règles sont ici plus discriminantes par rapport à la répartition au noeud initial de l'arbre. Ainsi, l'arbre généré peut être vu comme la représentation d'une typologie des modalités de la variable à prédire y. L'interprétation des règles, qui ne sont plus alors des règles de classification, doit elle être revue en terme de typicité de la condition pour la conclusion choisie. Ainsi, les personnes ayant une pression artérielle systolique 2 inférieure à 111, une pression artérielle systolique 1 inférieure à 113.5 et un indice de masse corporelle inférieur à 26.04 sont caractéristiques des « jamais buveurs ». Au contraire, prédiction état réel  jamais  occasionnellement  régulièrement  jamais  107  12  12  occasionnellement  516  194  38  régulièrement  307  105  50 TAB. 9 -Matrice de confusion, maximisation intensité implicative, taux d'erreur = 72%.
les buveurs réguliers sont caractérisés par une pression artérielle systolique 2 élevée (? 111) et un taux de cholestérol également élevé. Enfin, les buveurs occasionnels ne sont pas clairement caractérisés, bien qu'il existe des circonstances typiques comme un taux de cholestérol élevé allié à un BMI également élevé, sans toutefois connaître de problème au niveau de la pression artérielle. La difficulté à discriminer les buveurs occasionnels des autres peut également venir du fait qu'il y a différents types de buveurs, la définition « occasionnellement » étant elle-même subjective.
5 Conclusion

Introduction
L'une des conséquences de la prolifération de l'information en ligne de nos jours est la diversité des données. XML se distingue comme le format par excellence pour la représentation, le stockage et l'échange de données sur Internet.
Les systèmes de recherche d'information dans les documents XML (RI-XML) utilisent soit le paradigme de l'appariement exact soit celui de l'appariement approximatif (ou appariement par classement). Dans le premier cas, la requête doit vérifier les contraintes sur le contenu et la structure spécifiées dans la requête, ainsi chaque item (document, fragment de document ou élément XML) sur lesquels la recherche est effectuée et jugé pertinent ou non. Dans le second cas, les items sont classés selon leur pertinence à la requête. Dans le contexte du Web, l'appariement approximatif est plus approprié. En effet, l'appariement exact nécessite un langage d'interrogation structuré et une connaissance a priori de la structure des documents recherchés. Cependant, dans un environnement ouvert comme le Web, les utilisateurs ne sont pas nécessai-rement aptes à exprimer leur besoin d'information avec un langage d'interrogation complexe. En outre, la structure des documents XML recherchés n'est pas toujours disponible. L'appariement exact naturellement nécessite le classement des résultats dans le but de présenter les items les plus pertinents en premiers.
La plupart des méthodes de classement existantes proposent d'étendre les modèles traditionnels de classement utilisés dans la RI classique. Ces méthodes définissent une fonction globale qui calcule un score (RSV : Relevance Status Value) pour chaque item. Les documents sont alors triés d'une façon globale selon leurs scores et présentés à l'utilisateur. Le score d'un document est calculé en fonction d'un ensemble de critères ayant un impact sur la pertinence du document, comme par exemple la fréquence d'apparition des termes de la requête. La moyenne pondérée des valeurs des critères est par exemple souvent utilisée pour le calcul du score. Notre approche diffère dans ce sens que nous ne trions pas les résultats d'une façon globale mais partielle. Les items sont comparés deux-à-deux pour déterminer un classement partiel les résultats de ce classement sont combinés pour obtenir un classement global. Dans ce but nous avons choisi d'utiliser une méthode d'aide à la décision et notre choix s'est porté sur la méthode PROMETHEE.
La suite de cet article est organisée comme suit : Dans la section 2 nous clarifions certaines différences primordiales entre la recherche d'information classique et la recherche d'informations dans les documents XML. Ceci est important pour la suite de cet article. Nous présentons dans la section 3 l'architecture générale de notre système de recherche d'information XML. La section 4 est un état de l'art des approches de classement dans XML. Dans la section 5 nous détaillons notre approche de classement ainsi que la méthode d'aide à la décision utilisée. Et nous terminons enfin par une conclusion
L'indexation
Deuxièmement, l'indexation d'un document XML, en plus du contenu, doit prendre en compte aussi la structure du document. Il est fréquent de considérer un document XML comme un arbre où les noeuds sont les éléments ou les noms d'attributs, où les arêtes représentent l'appartenance du noeud fils au noeud père et où les feuilles sont les contenus des éléments où les valeurs des attributs. Ainsi, l'indexation de la structure revient à garder une trace des relations père-fils et frère de entre les noeuds. Pour indexer le contenu des documents XML, les technique de la RI sont reprises dans la RI-XML, mais souvent avec des adaptation liées à l'existence de la structure dans les documents. En effet, l'unité d'indexation en RI-XML pouvant être l'élément au lieu du document entier, des statistiques comme par exemple la fréquence d'apparition des termes représentera dans la RI-XML la fréquence d'apparition d'un terme dans un élément non pas dans le document entier. Idem pour la fréquence documentaire (DF : Document Frequency) qui représentera en RI-XML le nombre d'éléments et non pas de documents contenant un terme donné.
Les résultats
La problématique de la RI-XML est similaire à la "recherche de segments de documents" (passage retrieval). En effet, les documents structurés peuvent contenir un large spectre d'information hétérogènes, il est donc préférable de retrouver une partie (ou des parties) de document qui répond à la requête de l'utilisateur plutôt que le document entier Wilkinson (1994). Survient alors une difficulté supplémentaire qui est de déterminer le niveau de granularité de l'élément retourné. Lorsque l'utilisateur exprime sa requête dans un langage structuré, il a la possibilité de définir la garnularité des éléments qu'il désire. Par exemple, avec la requête XPATH suivante : //poisson[milieu="eaux douces"]/nom l'utilisateur spécifie qu'il désire seulement le contenu du noeud nom. Lorsque en revanche nous sommes en situation où l'utilisateur s'exprime uniquement à l'aide de mots clés, le système devra calculer la granularité adéquate.
Une vue globale de notre système de RI-XML
Lors de la conception de notre système, notre but principal était son accessibilité à l'utilisateur. Ainsi, l'interrogation du système devait être adaptable au niveau de l'utilisateur. Les requêtes peuvent varier en complexité en partant d'un simple ensemble de mots clés à de complexes expressions booléennes. Le système analyse les requêtes, et détermine les éléments les plus susceptibles de correspondre au besoin de l'utilisateur et les retourne à l'utilisateur triés par ordre de pertinence. Les résultats sont présentés de façon à ce que l'utilisateur puisse naviguer dans la structure du document d'où l'élément est extrait.
Notre système est constitué de deux parties principales (figure 1. Dans la première partie, un parseur de documents XML analyse et indexe la structure des documents et leurs contenus. Le résultats de cette analyse est sauvegardé dans une base de données. La deuxième partie est constituée de trois composants l'analyseur de requête, le module d'appariement et le module de classement des résultats. L'analyseur de requête décompose la requête lorsqu'elle contient des opérateurs booléens et crée une structure qui permet son appariement avec les documents. Le module d'appariement retrouve dans la base de données les éléments les plus appropriés à la requête. Enfin le module de classement des résultats se charge de trier selon l'ordre de pertinence les éléments issus du module d'appariement.
Dans Abbaci et al. (2006) nous avons détaillé l'indexation des documents XML dans notre système, l'analyse d'une requête ainsi que le processus d'appariement d'une requête aux documents de la base de données. Dans cet article nous présentons le fonctionnement du module de classement des éléments.
Les approches de classement dans la RI-XML
Dans la RI un certain nombre de critères ont été décelés importants dans le jugement de pertinence d'un document à une requête donnée. Quelque uns de ces critères sont devenus classiques tels la fré-quence d'apparition des termes (TF) ainsi que leur pouvoir de discrimination (IDF Inverse Document Frequency). Il existe d'autres critères comme par exemple la proximité des termes de la requête dans le Certains critères sont utilisés tels qu'ils sont connus dans la RI dans ce cas le document XML est considéré comme un simple document texte, comme par exemple la proximité entre les termes de la requête. Ainsi, dans Sauvagnat et al. (2003) les auteurs considèrent la proximité entre deux termes comme étant le nombre de mots séparant ces termes dans une fenêtre de x termes et Kotsakis (2002)  et Sauvagnat et al. (2003) calculent TF et IDF de la même manière qu'en RI. Les auteurs de Theobald et Weikum (2002) quant à eux, intègrent la proximité sémantique (calculée sur la base d'une ontologie) entre les termes pour augmenter la performance de la fonction de calcul du score. D'autres critères liés à la structure des documents XML sont utilisés. Nous citons la distance entre les noeuds, dans Guo et al. (2003)   Cohen et al. (2003) des poids sont attribués aux noms d'élément afin de favoriser les scores des élément qu'on juge plus intéressant à retourner (par exemple préférer retourner le résumé d'un l ivre plutôt que son titre). Un autre critère lié à la structure des documents XML et qui rentre en compte dans le calcul du score des éléments est la spécificité des éléments à classer Guo et al. (2003)  Sigurbjörnsson et al. (2004), plus un élément est profond plus il est spécifique.
Notre approche de classement
La majorité des méthodes de classement dans la RI adoptent une approche global de classement. En d'autres termes, les critères de tri sont rassemblés dans une fonction unique qui doit être maximisée. L'inconvénient majeur de cette approche globale réside dans le fait que les critères peuvent mutuellement se compenser. Ainsi une solution dont l'un des critères présente une valeur faible peut ne pas être pénalisée si un de ses critères restants présente une valeur élevée.
Par conséquent nous avons opté pour une approche de classement partiel qui classe les items en les comparant deux-à-deux. Dans ce qui suit, nous décrivons la méthode d'aide à la décision que nous avons choisie. Le principe de cette méthode est qu'elle compare les solutions paire-par-paire. Ainsi, une solution a surclasse une autre solution b si au vue de la plupart des critères, a est meilleure que b. Les résultats de la comparaison par paire sont alors combinés afin d'établir un classement total des solutions.
PROMETHEE une méthode d'aide à la décision
Une méthode d'aide à la décision permet d'effectuer un classement d'un ensemble de solutions possibles à un problème donné en commençant par la solution la plus adéquate au vue d'un ensemble de critères et de l'importance relative accordée à chacun de ces derniers.
Nous décrivons dans ce qui suit la méthode PROMETHEE Vincke (1989)  
. . .
. . . a n f 1 (a n ) f 2 (a n ) · · · f j (a n ) · · · f k (a n )
TAB. 1 -Table d'évaluation.
Une fonction de préférence Pj(a, b) est définie afin d'attribuer un degré de préférence d'une solution a à une solution b au vue d'un critère fj. En général, Pj(a, b) modélise les différences des valeurs des solutions pour un critère donné d = fj (a) ? fj (b). La fonction Pj(a, b) est normalisée comme suit :
Deux paramètres q et p de seuil d'indifférence et de préférence respectivement sont définis. Lorsque la différence entre les évaluations de a et b est inférieure à q alors elle n'est pas significative. La fonction de préférence est donc égale à 0. Lorsque cette différence est supérieure à p elle est considérée comme trés significative et la fonction de préférence est dans ce cas égale à 1. Un classement des deux solutions a et b est construits en prenant en compte tous les critères et ce selon l'expression suivante (1) :
où wj > 0 sont les poids associés aux critères. Ces poids son des nombres naturels positifs qui ne dépendent pas des échelles des critères. ?(a, b) exprime le degré de préférence de la solution a à b au vue de tous les critères. Les valeurs de ?(a, b) et ?(b, a) sont calculées pour chaque paire de solutions a, b ? A. De cette façon, une relation de surclassement est définie dans A.
Deux flux de surclassement sont définies : -Flux de surclassement positif, qui représente la puissance d'une solution par rapport à toutes les autres. Plus ? + (a) est grand plus la solution a est mieux que les autres :
b?A,b =a -Flux de surclassement négatif, qui représente la faiblesse d'une solution par rapport à toutes les autres. Plus ? ? (a) est petit plus les autres solutions sont mieux que a :
P pour "préférable à" et I pour "indifférent à".  
Un exemple
TAB. 2 -Exemple d'un problème de décision multicritère.
Le seuil de préférence p est fixé à 0.2 pour chaque critère i.e. si deux solutions ont une différence de plus de 20% pour un critère donné, l'une des deux solutions est préférable à l'autre pour le critère en question. En outre, le seuil d'indifférence q est fixé à 0.05 pour chaque critère i.e. si deux solutions ont une différence de moins de 0.5% pour un critère donné, les deux solutions doivent être considérées comme égale pour le critère en question. Calculons le classement de ces voitures par la méthode PROMETHEE lorsque les poids de tous les critères sont fixés à 1 (Table 3).
Les résultats montrent que la solution idéale est la voiture numéro 3 et que la voiture la moins inté-ressante est la numéro 4. La méthode ne peut classer les deux voitures numéros 1 et 2.
Calculons à présent le classement des ces voitures par la méthode PROMETHEE lorsque le poids du critère Consommation est fixé à 4 (Table 4). 
TAB. 4 -Le classement par la méthode PROMETHEE lorsque le critère Consommation a un poids égal à 4.
L'effet de ce changement est clair, la voiture numéro 4 devient la solution idéale.
PROMETHEE pour classer les fragments de documents XML
Dans cette section, nous présentons les différentes étapes de l'adaptation de PROMETHEE à notre problématique de classement des fragments de documents XML, ces derniers étant le résultat d'une recherche d'information dans une collection de documents XML. D'abord nous définissons l'ensemble des critères que nous souhaitons prendre en compte dans le processus du classement. Ensuite, nous associons à chaque critère son ensemble des valeurs possibles ainsi que la valeur optimale souhaitée. Lorsqu'une requête est soumise à notre système, le tableau des critères est rempli avec les fragments des documents de la collection en guise de solutions.
Les critères de classement des fragments XML
-La pertinence du contexte (Doc.) : Nous partons du principe que la pertinence d'un élément est liée à la pertinence du document qui le contient. Ainsi le score global du document "père" constitue un critère pour la décision concernant le classement des éléments qu'il contient. Pour l'instant nous proposons que Doc. représente le score classique T F * IDF en ignorant la structure du document XML. Ainsi, pour une requête Q et un élément e du document D : (q1, q2, e), la distance entre les noeuds qui contiennent respectivement les deux mots clés q1 et q2 dans l'élément e est représentée par la longueur du chemin le plus court entre les deux noeuds. Cette distance est nulle lorsque les deux mots clés appartiennent au même élément.
Cpc(q1, q2, e) représente le chemin le plus court entre les deux noeuds où apparaissent respectivement q1 et q2.
-La fréquence d'apparition des mots clés (F req.) : Comme dans la RI classique, nous supposons que les termes fréquents dans un élément contribuent fortement à la description de ce dernier et par conséquent les éléments qui contiennent plusieurs occurrences des mots clés ont plus de chance d'être pertinents à la requête. | e | est le nombre de noeuds de l'élément e.
-Les liens structurels entre les mots clés (Rel.) : Nous faisons référence ici à la relation de parenté dans la structure d'un élément XML. Nous supposons que la relation "ancêtre-descendant" entre deux noeuds contenant respectivement deux mots clés de la requête est un bon indicateur de pertinence. De ce fait, nous calculons Rel. comme suit : DisV (qi, qj , e) représente la distance verticale (en profondeur dans l'arbre XML) dans l'élément e entre les deux noeuds qui contiennent qi et qj respectivement. ID(q) étant l'identifiant du noeuds où se trouve le terme q. Ansestors(ID(q), e) est l'ensemble des noeuds ancêtres du terme q dans l'élément e. 
Les valeurs optimales des critères de classement
TAB. 6 -Les échelles des valeurs des critères de classement.
nous envisageons d'effectuer plusieurs tests afin de les déterminer.
Conclusion
Nous avons présenté dans cette article notre approche pour le classement des éléments de documents XML pertinents à une requête donnée. Nous avons montré l'originalité de notre approche qui se distingue par la façon de combiner les différents critères qui rentrent en jeux dans le jugement de pertinence dans

Introduction
En analyse de données symbolique (voir Bock et Diday (2000)) une variable peut, entre autre être décrite par une distribution de probabilité continue. La classification en K groupes de ces données fonctionnelles peut être obtenue en utilisant une décomposition de mélange. Mais cette technique nécessite de pouvoir calculer la densité d'une distribution de fonction. Or l'espace des fonctions n'est pas un espace de dimension finie, tels que ceux où sont défi-nies les distributions classiques. Projeter les fonctions dans un espace multidimensionnel par échantillonnage (voir Diday (2002)) permet de contourner ce problème, pour autant que l'on choisisse des distributions conjointes adéquates. Dans la section 2 de cet article nous rappel-lerons brièvement la décomposition de mélange ainsi que l'algorithme des nuées dynamiques. Ensuite nous préciserons le cadre des distributions de fonctions et la construction de lois de ce type via les distributions multivariées. Nous terminerons ensuite, avant les conclusions, par une utilisation des nouveaux objets mathématiques définis pour la classification de données symboliques synthétiques.
Décomposition de mélange
Mélange de distributions
La décomposition de mélange est un outil important en classification. Elle consiste en l'estimation de la densité de probabilité qui est supposée avoir gouverné la génération d'un échantillon de données consitué de plusieurs groupes :
où les p i représentent les proportions de chacun des groupes (leur somme étant égale à 1), et les fonctions f (., ?) les densités de ces groupes. Chaque composante du mélange correspondant en fait à un groupe. Pour trouver la partition P = (P 1 , ..., P K ) la mieux adaptée aux données deux grands algorithmes ont été proposés : EM (Estimation,Maximisation) par Dempster et al. (1977) et l'algorithme des nuées dynamiques par Diday et al. (1974). Nous avons choisi d'utiliser ce dernier car il avait déjà été utilisé dans le cadre de l'Analyse Symbolique par Diday (2002).
Algorithme des nuées dynamiques
L'algorithme utilisé est en fait une extension de la méthode des nuées dynamiques (Diday et al., 1974) dans le cas d'un mélange. L'idée principale est, alternativement, d'estimer au mieux la distribution de chaque classe, et ensuite de vérifier que chaque objet symbolique appartient à la classe de densité maximale. L'étape d'estimation est réalisée en maximisant un critère de qualité, ici la log-vraisemblance :
i u?Pi La classification commence avec une partition initiale aléatoire, et les deux étapes suivantes sont donc répétées jusqu'à stabilisation de la partition : -Etape 1 : Estimation des paramètres Déterminer le vecteur (? 1 , ..., ? K ) qui maximise le critère de qualité. -Etape 2 : Distribution des objets symboliques dans les classes Les classes (P i ) i=1,...,K , dont les paramètres ont été calculés à l'étape 1, sont construites comme suit
Cet algorithme nécessite donc de pouvoir calculer la distribution, ou plus précisément la densité de probabilité, des objets à classer. Nous avons donc besoin de préciser la notion de distribution de fonctions. 
Distribution de fonctions
Définitions
Si la notion de distribution de fonction est facile à définir, il paraît, par contre, plus malaisé de donner immédiatement un moyen de la calculer. Considérons l'exemple de la figure 1. Supposons que les lignes continues forment un échantillon fonctionnel homogène. Si v est une de ces fonctions, calculer F X,D (v) peut se faire empiriquement :
#A Mais qu'en est-il pour les fonctions w et u ? Pour w on peut supposer intuitivement que la valeur de F X,D (w) est proche de 90%. Et pour u, est-ce 50%, car u est toujours supérieure à 10 des 20 fonctions de l'échantillon ? Et ce malgré le fait que u soit supérieure à 12 des 20 fonctions sur plus de la moitié du domaine ? Pour solutionner ce problème de calcul, nous allons projeter dans un espace multidimensionnel les fonctions, par nature définies dans un espace de dimension infinie. 
Si nous définissons ensuite les deux ensembles suivants :
alors nous pouvons utiliser l'approximation suivante :
où H est une distribution multivariée de dimension q. Nous pouvons donc utiliser une distribution conjointe pour approximer notre distribution fonctionnelle. Le choix de la distribution, ou de la famille de distributions, à utiliser est évidemment important. Avant de préciser ce choix, remarquons que pour une valeur choisie x ? D, il est très facile d'estimer la distribution des valeurs de X(x). 
Il est assez facile de calculer G et g à l'aide des techniques univariées. Ainsi, si X est un processus Gaussien, alors ces deux fonctions peuvent être calculées pour une valeur donnée de x par la fonction de répartition et la densité de la loi N (µ(x), ?(x)). Dans les cas où l'on ignore la loi suivie par X(x) on utilisera l'estimation empirique pour G et l'estimation à noyaux pour g :
La Fig. 2 montre ces deux surfaces avec l'exemple de la Fig. 1, dans le cas Gaussien. Etant donné qu'il est très facile de calculer les marges de la distribution H par :
l'idée de reconstruire cette distribution H à partir de ses marges a été proposée par Diday (2002) en utilisant les copules archimédiennes. 
Copules archimédiennes
Les copules sont des outils précieux dans la modélisation des structures de dépendance grâce au théorème de Sklar (voir Nelsen (1999)).
TAB. 1 -Générateurs archimédiens
De plus, si F 1 , ..., F n sont toutes continues, alors C est unique ; sinon C est unique seulement sur domF 1 × ... × domF n .
Définition 3.7 Les copules archimédiennes sont définies par
représente la dérivée d'odre k de ?.
Le tableau 1 montre trois familles de générateurs Archimédiens. Si nous utilisons conjointement les surfaces de distributions et les copules archimédiennes, alors notre approximation (8) peut directement se récrire :
La densité conjointe étant donnée par l'expression suivante : 
Cela signifie que la limite de l'expression (13), lorsque q ? ? est presque toujours nulle ! Pour éviter ce problème, nous proposons d'utiliser un nouveau type de distributions basée sur les moyennes quasi-arithmétiques.
Moyennes quasi-arithmétiques discrètes
où ? est une fonction continue strictement monotone, et ? = ? ?1 .
Le concept de moyenne quasi-arithmétique a été introduit par Kolmogorov (1930) et Nagumo (1930, et a été étudié dans le cadre des équations fonctionnelles par Aczel (1966).
est une distribution conjointe de marges
Nous appelons cette distribution Moyenne Quasi-Arithmétique de Marges (en anglais :QuasiArithmetic Mean of Margins (QAMM)).
Démonstration Il suffit de remarquer que si F i est une distribution univariée, alors F * i aussi, et d'ensuite utiliser ces nouvelles distributions et la copule générée par ? pour construire la distribution multivariée.
Distribution et densité définies dans un espace de dimension infinie
Moyennes quasi-arithmétiques continues
En utilisant l'expression (16) et en notant x n i+1 ? x n i = ? x (cf. (7)) ?i on peut écrire :  (14) comme dans le cas fini. Nous proposons donc d'utiliser une densité "directionnelle".
Densité de Gâteaux
Rappelons ici un concept provenant de l'analyse fonctionnelle : la dérivée de Gâteaux, qui est une dérivée directionnelle (cf. Atkinson et Han (2001)). Définition 3.10 Soient V et W deux espaces vectoriels normés, et F un opérateur de V vers W . La differentielle de Gâteaux DF (u; s) de F en u dans la direction s ? V est donnée par :
L'utilisation de ce type de différentiation nécessite donc de préciser dans quelle direction elle se fait. Nous proposons d'utiliser comme fonction de direction toute fonction permettant de mesurer la dispersion des données pour toute valeur de x, avec comme exemple le plus immé-diat l'écart-type ?. 
Définition 3.11 Soient
La dérivée de Gâteaux d'une transformée intégrale étant un résultat classique d'analyse fonctionnelle (cf. Lusternik et Sobolev (1974)), nous avons le résultat suivant.
est une mesure fonctionnelle de la dispersion des valeurs de X(x), alors la densité de Gâteaux de F X,D calculée en u dans la direction de s est donnée par :
Soulignons ici l'intérêt de diviser la différentielle de Gâteaux dans l'expression (21) par la norme de s. En effet, comme on peut le constater dans (22), sans cela, la densité de Gâteaux d'une Moyenne Quasi-Arithmétique Continue de Marges calculée avec deux paramétrages différents s 1 ? D s 2 pourrait donner la même valeur, pour autant que g 1 [t, u(t)] s 1 (t) = g 2 [t, u(t)] s 2 (t) pour toute valeur de t ? D. La division par la norme de la mesure de dispersion permet de réintroduire cette distinction.
Domaines des modèles
Remarquons maintenant que le calcul de l'expression (22) nécessite de pouvoir calculer g (x, y) sur l'ensemble des valeurs de D et que, ceci n'est en général possible que si la mesure de dispersion s est non nulle. Nous dirons que le domaine du modèle est l'ensemble des réels pour lesquels s(x) > 0. Nous appellerons donc domaine du modèle tout intervalle D ? {x ? R : s(x) > 0}. Ainsi, dans le cas des Moyennes Quasi-Arithmétiques de Marges utilisées conjointement avec la densité de Gâteaux, nous répondons aux deux questions évoquées plus avant concernant le nombre et les choix des points x n 1 , . . . , x n q : 1. quand à la valeur de q : on le choisit très grand (QAMM), voire on le fait tendre vers l'infini (QAMML) pour minimiser l'erreur due à l'approximation, 2. quand au choix des x n 1 , . . . , x n q : ils doivent se situer dans le domaine du modèle, c'est-à-dire pour les valeurs de x où il y a dispersion non nulle des valeurs de X(x).
Il faut noter que ces deux règles peuvent aussi s'appliquer dans le cas d'utilisation des copules et de la densité multivariée. En effet le même problème de calculabilité se pose avec l'expression (14) si la dispersion des valeurs X(x) est nulle pour au moins une des dimensions. Mais il est évidemment conceptuellement plus difficile de définir la notion de domaine de modèle dans le cas multivarié, car cela équivaut à ne pas toujours utiliser le même nombre de dimensions (et pas nécessairement les mêmes) pour calculer une même distribution ou sa densité en plusieurs endroits. Sauf, si l'on se souvient que nous ne sommes pas confrontés à de vraies données multivariées, mais à la projection en dimension q de données définies dans un espace de dimension infinie.
Application
Nous avons donc utilisé les Moyennes Quasi-Arithmétiques Continues de Marges, conjointement avec la densité de Gâteaux, dans le cadre de l'algorithme des nuées dynamiques sur des données de type symbolique : des densités de probabilités. Pour notre test nous avons utilisé un ensemble de 140 données synthétiques mixant des exponentielles, des normales et des bétas (Fig. 3). Pour constituer cet ensemble de données, pour chaque distribution nous avons généré 500 nombres aléatoires suivant la loi choisie et ensuite nous avons réalisé une estimation à noyaux à partir de ces nombres. Les résultat fonctionnel étant stocké à l'aide des fonctions splines. Remarquons que les distributions de probabilités sont des données fonctionnelles qui sont définies sur R, même si la valeur de la fonction peut être nulle sur une partie du domaine (exemple :la loi exponentielle). Or, en classification, seule une partie du domaine de la fonction est intéressante : celle où l'on peut distinguer cette fonction des autres, c'est-à-dire là où la fonction est non nulle (ou supérieure à fixé). Nous restreignons donc, pour des raison classificatoires, le domaine sur lequel nous calculons la Moyenne Quasi-Arithmétique Continue de Marges (ou sa densité de Gâteaux) aux valeurs distinguables de la fonction considérée. Pour cela nous utilisons une fonction de "confiance" :
Avec cette fonction l'expression (18) devient : 
Conclusions
Dans cet article nous proposons d'utiliser deux outils mathématiques nouveaux, les Moyennes Quasi-Arithmétiques de Marges et la densité de Gâteaux, dans le cadre de la décomposi-tion de mélange classifiante. Ces outils nous permettent d'apporter une réponse aux questions laissées sans réponse par les travaux précédents : à savoir le nombre et le choix des points d'approximation. L'utilisation de ces nouveaux objets mathématiques dans le cadre de la classification non supervisée sur des données symboliques synthétiques donne des résultats encourageants. D'autres outils et méthodes de classification de données fonctionnelles existent, mais l'utilisation de distributions adéquates permet d'obtenir une modélisation probabiliste des données. D'autre part un certain nombre de développements sont encore envisageables pour affiner cet outil mathématique : l'utilisation dans (18) d'une distribution autre que la distribution uniforme sur D, l'utilisation conjointe de la distribution des dérivées successives d'une fonction u ou encore l'utilisation de directions autres que s, plus discriminantes (cf. Ramsay et Siverman (2005)).
Summary
Individual data can be caracterized by continuous distributions and not by a single value. Those functional data can be used to classify individuals. In a elementary solution, we can reduce distribution to mean and variance. Another richer solution is proposed by Diday (2002) and implemented by Vrac et al. (2001) and Cuvelier et Noirhomme-Fraiture (2005). It uses cut points in the distributions and models those joint values by a multidimensional distribution built with copulas. We have shown in a previous work that even if this approach gives good results, classification quality depends on the number and the place of cutpoints. The questions of number and place of cuts remains open questions. We propose a solution to these questions, when the number of cuts tends to infinity. We suggest a new distribution adapted to the space of infinite dimension. We suggest also a density which uses the Gâteaux directional derivative. The chosen direction is dispersion of functions to be classified. Results are encouraging and offer multiples perspectives in all the domains where functional data distribution is necessary.

Introduction
Le volume de données stocké double actuellement tous les 9 mois (Lyman et al, 2003) et donc le besoin d'extraction de connaissances dans les grandes bases de données est de plus en plus important (Fayyad et al, 2004). La fouille de données (Fayyad et al, 1996) est confrontée au challenge de traiter de grands ensembles de données pour identifier des connaissances nouvelles, valides, potentiellement utilisables et compréhensibles. Elle utilise différents algorithmes pour la classification, la régression, le clustering ou les associations.
Nous nous intéressons plus particulièrement ici aux algorithmes de Séparateurs à Vaste Marge (SVM ou Support Vector Machine) proposé par (Vapnik, 1995) car ils se montrent particulièrement efficaces pour la classification, la régression ou la détection de nouveauté. On peut trouver de nombreuses applications des SVM comme la reconnaissance de visages, la catégorisation de textes ou la bioinformatique (Guyon, 1999). L'approche est systématique et motivée par la théorie de l'apprentissage statistique. Les SVM sont les plus connus parmi une classe d'algorithmes utilisant les méthodes de noyau (Cristianini et al, 2000). Les SVM et les méthodes de noyaux permettent de construire des modèles précis et deviennent des outils de fouille de données de plus en plus populaires. Mais malgré ces qualités, les SVM ne peuvent pas traiter facilement des données volumineuses. Les solutions des SVM sont obtenues par résolution d'un programme quadratique, le coût de calcul d'une approche de SVM est au moins d'une complexité égale au carré du nombre d'individus de l'ensemble d'apprentissage et la quantité de mémoire nécessaire les rend impossible à utiliser sur de grands ensembles de données. Il y a donc besoin de permettre le passage à l'échelle de ces algorithmes pour traiter de grands ensembles de données sur des machines standard. Une heuristique possible pour améliorer l'apprentissage à l'aide de SVM est de décomposer le programme quadratique original en une série de plus petits problèmes (Boser et al, 1992), (Chang et al, 2003), (Osuna et al, 1997), (Platt, 1999). Les méthodes d'apprentissage incrémental (Cauwenberghs et al, 2001), (Do et Poulet, 2006), (Do et Poulet, 2003), (Fung et Mangasarian, 2002), , (Syed et al, 1999) permettent de traiter de grands ensembles de données par mise à jour des solutions partielles en augmentant l'ensemble d'apprentissage sans avoir à charger l'ensemble de données total en mémoire. Les algorithmes parallèles et distribués (Do et Poulet, 2006), ) utilisent des machines connectées par internet pour améliorer le temps d'exécution de l'apprentissage de grands ensembles de données. Les algorithmes d'apprentissage actif (Do et Poulet, 2005), (Tong et Koller, 2000) permettent de choisir un sous-ensemble d'individus (ensemble actif) pour la construction du modèle. Nous présentons un nouvel algorithme de boosting de LS-SVM pour la classification de grands ensembles de données sur des machines standard. L'algorithme de LS-SVM proposé par (Suykens et Vandewalle, 1999) effectue un changement de la contrainte d'inégalité en égalité dans la résolution du problème d'optimisation permettant d'obtenir la solution par résolution d'un système d'équations linéaires au lieu du programme quadratique. Ce nouvel algorithme est donc beaucoup plus rapide en temps d'exécution. Nous avons étendu cet algorithme pour construire un nouvel algorithme de SVM incrémental, parallèle et distribué permettant de traiter des ensembles de données ayant de très grands nombres d'individus. Puis nous avons ajouté un terme de régularisation de Tikhonov (Tikhonov, 1943) et utilisé la formule de Sherman-MorrisonWoodbury (Golub et Van Loan, 1996) pour permettre au LS-SVM de traiter des ensembles de données ayant un très grand nombre de dimensions. Enfin nous avons appliqué la technique du boosting au LS-SVM pour obtenir un algorithme permettant la classification d'ensembles de données ayant simultanément un grand nombre d'individus et de dimensions. Les performances de l'algorithme sont évaluées sur des ensembles de données de l'UCI (Blake et Merz, 1998), Twonorm, Ringnorm (Delve, 1996), Reuters-21578 ( Lewis, 1997) et NDC (Musicant, 1998). Les résultats sont comparés avec ceux obtenus avec LibSVM (Chang et Lin, 2003).
Le paragraphe 2 présente brièvement l'algorithme de LS-SVM, le paragraphe 3 décrit l'algorithme incrémental de LS-SVM. Dans le paragraphe 4 nous présentons l'algorithme de boosting de LS-SVM puis les résultats des tests numériques dans le paragraphe 5 avant la conclusion et les travaux futurs.
Quelques notations sont utilisées dans cet article. Tous les vecteurs sont représentés par des matrices colonne. Le produit scalaire de deux vecteurs x et y est noté x.y. La norme d'un vecteur v est ||v||. La matrice A (de taille mxn) contient l'ensemble des m individus en dimension n. La classe (+1 ou -1) est stockée dans la matrice diagonale D (de taille mxm). e est un vecteur colonne de 1. w et b sont les coefficients et le scalaire de l'hyperplan, z est la variable de ressort et C est une constante positive. I représente la matrice identité. 
L'algorithme de LS-SVM
Considérons une tâche de classification binaire linéaire comme représentée sur la figure 1 avec m points x i (i=1..m) dans l'espace de R n , representés par la matrice A avec les étiquettes de classe (+1 ou -1) stockées dans la matrice diagonale D. L'algorithme de SVM cherche le meilleur hyperplan de séparation des données (meilleur au sens du plus éloigné possible des deux classes). Cela revient à maximiser la marge qui est la distance entre les plans supports des deux classes. Le plan support de la classe +1 [resp. -1] sépare tous les individus de la classe +1 [resp. -1] des autres. Ceci peut s'écrire sous la forme suivante (1) :
( 1 ) La marge entre les plans support est 2/||w|| (où ||w|| représente la norme du vecteur w). Dans le cas non linéairement séparable, les contraintes doivent être relaxées pour permettre à un point d'être du mauvais côté du plan support de sa classe, une variable de ressort est alors ajoutée dans la partie gauche de l'équation 1. Ensuite tout point du mauvais côté de son plan support est considéré comme une erreur et à une valeur de z positive (z i > 0).
Ensuite l'algorithme de SVM doit simultanément maximiser la marge et minimiser les erreurs. La formulation standard de l'algorithme de SVM avec un noyau linéaire est alors le programme quadratique (2):
( 2 ) où z représente la variable de ressort et c est une constante positive pour régler les erreurs et la taille de la marge.
L'hyperplan (w,b) obtenu est la solution du programme quadratique (2). Ensuite la classification d'un nouvel individu x se base sur sa position par rapport à l'hyperplan obtenu classe(x) = signe (w.x-b). Les algorithmes de SVM peuvent utiliser d'autres types de fonctions pour la classification comme par exemple une fonction polynomiale de degré d, une fonction RBF (Radial Basis Function) ou une sigmoïde. Le passage de cas linéaire au cas non-linéaire se fait par l'utilisation d'une fonction de noyau à la place du produit scalaire dans RNTI -X -l'équation (2). Plus de détails sur les SVM et les méthodes de noyaux peuvent être trouvés dans (Cristianini et Shawe-Taylor, 2000). La solution des SVM est obtenue par résolution d'un programme quadratique donc le coût en temps d'exécution est au moins proportionnel au carré du nombre d'individus et la place mémoire nécessaire les rend incapables de traiter des ensembles de données très volumineux. L'algorithme de LS-SVM proposé par (Suykens et Vandewalle, 1999) utilise une égalité au lieu de l'inégalité dans le problème d'optimisation (2) avec la fonction ? suivante :
En substituant z dans la fonction objectif ? du programme quadratique (2), nous obtenons alors (3) : 
où E=[A -e] (juxtaposition de la matrice A avec une colonne de -1), I° est la matrice diagonale identité dont le dernier élément est 0.
La formulation du LS-SVM (6) nécessite la résolution d'un système linéaire à n+1 inconnues au lieu du programme quadratique (2), donc si le nombre de dimensions de l'ensemble de données est inférieur à 10 5 , l'algorithme de LS-SVM (tableau 1) est capable de traiter un très grand nombre d'individus en un temps restreint sur une machine standard. Les tests numériques ont montré des résultats satisfaisants en comparaison à des algorithmes comme libSVM mais en se montrant beaucoup plus rapide. Par exemple, la classification d'un million de points en dimension 20 est effectuée en 1,3 seconde sur un PC (Pentium IV, 3GHz, 512Mo RAM). Pour traiter le cas de la classification non-linéaire, il faut remplacer la matrice A en entrée de l'algorithme par la matrice de noyau non linéaire K, par exemple :
2 ) L'algorithme de LS-SVM utilisant une matrice de noyau nécessitera aussi un temps de calcul et une place mémoire importante. 
LS-SVM incrémental
Bien que l'algorithme de LS-SVM soit efficace et rapide pour la classification de grands ensembles de données, il nécessite de charger l'ensemble des données en mémoire. Avec de très grands ensembles de données, par exemple un milliard de points en dimension 20, l'espace mémoire nécessaire est de 80Go. La plupart des algorithmes de classification actuels sont confrontés à ce problème. Nous allons nous intéresser à ce cas de figure, le traitement de très grands ensembles de données. Les algorithmes de classification incrémentaux (Do et Poulet, 2003, 2006, ) sont une méthode très efficace pour traiter de très grands ensembles de données car ils ne nécessitent pas le chargement de la totalité des données en mémoire : seul un petit bloc de données est considéré à un instant donné et le modèle est construit par modifications successives.
LS-SVM incrémental en ligne
Supposons que nous avons à traiter un ensemble de données ayant un très grand nombre de points et un nombre plus restreint de dimensions, nous pouvons décomposer cet ensemble de données en blocs de lignes A i , D i . La version incrémentale en ligne de LS-SVM va calculer la solution de l'équation (6) de manière incrémentale. Considérons un exemple simple avec un ensemble de données décomposé en deux blocs de lignes 
A partir des équations (7), (9) et (10), on peut en déduire l'équation (11) de l'algorithme de LS-SVM incrémental en ligne, avec un ensemble de données décomposé en k blocs de
L'algorithme incrémental en ligne de LS-SVM du tableau 2 peut donc classifier des RNTI -X -données très volumineuses sur une machine standard. La précision de l'algorithme est exactement la même que celle de l'algorithme original. Si le nombre de dimensions de l'ensemble de données est inférieur à 10,000 alors l'algorithme est tout à fait capable de classifier des ensembles de données de plusieurs milliards d'individus sur une machine standard. Entre deux étapes successives de l'algorithme il n'est nécessaire de conserver en mémoire qu'une matrice de taille (n+1) -initialiser : 
LS-SVM incrémental en colonne
Certaines applications comme la bioinformatique ou la fouille de textes nécessitent de traiter des données ayant un nombre très important de dimensions et un nombre d'individus plus réduit. Dans ce cas la matrice de taille (n+1)x(n+1) est trop importante et la résolution du système à (n+1) inconnues nécessite un temps de calcul élevé. Pour adapter l'algorithme à ce type de données nous avons appliqué la formule de Sherman-Morrison-Woodbury au système d'équations (6). Mais ce faisant nous avions une matrice singulière à inverser (I°). Nous avons donc ajouté un terme de régularisation de Tikhonov, ce qui est la méthode la plus couramment utilisée pour résoudre ce genre de problème. Avec le terme de Tikhonov (?>0) ajouté à (6) nous obtenons alors le système d'équations (12) [ ]
Ce système peut être réécrit sous la forme suivante (13) :
où H représente la matrice (n+1)x(n+1) diagonale dont le (n+1)ème terme est ? et les autres termes valent (1/c)+?. Ensuite nous appliquons la formule de Sherman-MorrisonWoodbury (14) dans la partie droite du système (13) : 
Boosting de LS-SVM
Pour pouvoir traiter des ensembles de données ayant simultanément un grand nombre d'individus et de colonnes il y a au moins deux problèmes à résoudre : le temps d'apprentissage devient rapidement déraisonnable et la quantité de mémoire nécessaire dépasse les capacités de mémoire des machines courantes. Bien que les algorithmes incrémentaux de LS-SVM puissent efficacement charger en mémoire des petits blocs de données successifs, ils nécessitent l'inversion de matrice de taille (mxm) ou (n+1)x(n+1). La quantité de mémoire nécessaire et le coût de calcul deviennent trop importants. Pour pouvoir traiter des ensembles de données très volumineux, nous avons donc appliqué l'approche du boosting au LS-SVM de manière analogue à . Cette solution présente deux avantages : résoudre le problème de passage à l'échelle et conserver la précision de l'algorithme original. Plus de détails sur le boosting peuvent être trouvés dans (Freund et Schapire, 1999) ou sur le site www.boosting.org. Nous décrivons brièvement ici le mécanisme de boosting de LS-SVM. Dans les années 1990, Freund et ses collègues ont introduit le boosting pour améliorer la précision des algorithmes d'apprentissage. La méthode de boosting consiste à utiliser k fois un algorithme d'apprentissage basique en se concentrant à chaque étape sur les erreurs commises à l'étape précédente. Pour ce faire, il est nécessaire de tenir à jour une distribution de poids sur l'ensemble des individus de l'apprentissage. Initialement, tous les poids sont identiques et à chaque étape du boosting le poids des individus mal classifiés est augmenté pour obliger l'algorithme à les prendre en compte de manière plus significative. Nous considérons l'algorithme de LS-SVM comme l'algorithme d'apprentissage basique et à chaque étape du boosting nous échantillonnons un sousensemble d'individus en tenant compte de la distribution des poids. Il faut remarquer que le LS-SVM n'effectue l'apprentissage que sur ce sous-ensemble d'individus (de taille moindre que l'ensemble original). La taille de l'échantillon est inversement proportionnelle aux nombres d'étapes du boosting. Les algorithmes de LS-SVM incrémentaux en ligne ou colonne peuvent ainsi être adaptés au traitement de très grands ensembles de données (à la RNTI -X -fois en nombre d'individus et de dimensions), avec de bons résultats en précision et besoin en mémoire.
Quelques résultats
Nous avons développé le programme en C/C++ sous Linux en utilisant la librairie Lapack++ (Dongarra et al, 1993) pour bénéficier de bonnes performances en calcul matriciel. Le programme peut donc classifier de très grands ensembles de données efficacement. Nous allons en présenter une évaluation prenant en compte les critères suivants : la précision, le temps d'apprentissage et la place mémoire requise. Nous avons sélectionné 3 ensembles de données artificiels générés par Twonorm, Ringnorm et NDC et 8 ensembles de données de l'UCI. Les caractéristiques de ces ensembles sont décrites dans le tableau 3 (les attributs catégoriques des ensembles Adult et Mushroom ont été convertis en binaire).
Nous avons utilisé le nouvel algorithme de boosting de LS-SVM (Boost-LS-SVM) et LibSVM (l'un des algorithmes de SVM les plus efficaces) pour effectuer la classification sur un PC (Pentium IV, 3GHz et 512Mo RAM). Les 8 premiers petits ensembles de données sont utilisés pour comparer la précision et le temps d'apprentissage (tableau 4). Boost-LS-SVM obtient de meilleures précisions dans tous les cas sauf un et un meilleur temps d'apprentissage dans la moitié des cas. On peut remarquer que le temps d'apprentissage de libSVM croit de manière très importante lorsque la taille des fichiers augmente. Par exemple sur l'ensemble de données Adult, Boost-LS-SVM est 190 fois plus rapide que libSVM.
Reuters-21578 est un ensemble de données réputé pour la catégorisation de textes. Nous avons utilisé Bow (McCallum, 1998) en prétraitement de ces données. Chaque document est vu comme un vecteur de mots, nous avons obtenu 29406 mots (dimensions) sans sélection de dimensions. Nous avons effectué la classification des 10 classes les plus nombreuses. Cet ensemble de données ayant plus de deux classes nous avons utilisé l'approche one-againstall. Les résultats sont présentés dans le tableau 5 avec la moyenne de la précision et du rappel (breakeven point) pour les 10 catégories. Boost-LS-SVM a obtenu une meilleure précision pour 9 des 10 catégories mais le temps d'exécution du Boost-LS-SVM est deux fois plus long que celui de LibSVM car le nombre d'itérations est important pour arriver à la même précision.
Temps (secs)
Précision ( Deux grands ensembles de données sont utilisés pour évaluer le temps d'exécution et la quantité de mémoire nécessaire aux algorithmes. LibSVM nécessite de charger la totalité de l'ensemble de données en mémoire, nous avons donc étendu la capacité de la RAM. Pour l'ensemble de données Forest Cover Type, nous avons effectué la classification des deux classes les plus nombreuses (Spruce-Fire : 211840 individus et Lorgepole-Pine : 283301 individus en dimension 24). Nous avons généré un ensemble de données de 55000 individus en dimension 20000 (2 classes) avec le programme NDC. Pour les deux ensembles de données, LibSVM n'a pu donner de résultat : pour Forest Cover Type, le programme a tourné pendant 21 jours et pour le second, il n'a pas pu être chargé en mémoire (4Go).
Boost-LS-SVM n'a utilisé que 512Mo de mémoire mais il a du relire les données à chaque étape de boosting (96% du temps est ainsi passé à charger les données en mémoire vive). Les résultats présentés dans le tableau 6 montrent que Boost-LS-SVM est capable d'effectuer la classification d'ensembles de données ayant simultanément un grand nombre d'individus et de dimensions sur une machine standard dans un temps raisonnable.
RAM (MB)
Précision ( 
Conclusion et perspectives
Nous avons présenté un nouvel algorithme de boosting de LS-SVM capable d'effectuer la classification de grands ensembles de données sur des machines standard. L'idée principale est d'étendre l'algorithme récent de Suykens et Wandewalle pour en construire une version incrémentale et un boosting de LS-SVM. La précision des nouveaux algorithmes est exactement la même que celle de l'algorithme original. La complexité de la version incrémentale en lignes est linéaire en nombre d'individus. Si le nombre de dimensions est suffisamment restreint (inférieur à 10000), il permet de classifier plusieurs milliards de données sur un simple PC. Quelques applications comme la bioinformatique ou la fouille de texte utilisent des données dont le nombre de dimensions est très important et le nombre d'individus plus faible, nous avons ajouté un terme de régularisation de Tikhonov et utilisé la formule de Sherman-Morrison-Woodbury pour construire la version incrémentale en colonne de l'algorithme de LS-SVM et traiter les ensembles de données ayant un grand nombre de dimensions. Puis nous avons étendu ces algorithmes en utilisant la technique du boosting pour la classification d'ensembles de données ayant simultanément un grand nombre de dimensions et d'individus. Les résultats des tests numériques montrent que le nouvel algorithme de boosting de LS-SVM est rapide et de bonne précision. Il permet le passage à l'échelle et obtient de bons taux de précision en comparaison à libSVM (l'un des algorithmes de SVM les plus efficaces). Pour des petits ensembles de données il présente un bon taux de précision et une bonne rapidité d'exécution. Pour des ensembles de données de très grandes tailles (à la fois en nombre de dimensions et d'individus), il a montré ses possibilités avec un bon taux de précision.
Une première extension de ces travaux va consister à étendre cet algorithme pour en faire une version parallèle et distribuée sur un ensemble de machines. Cette extension permettra d'améliorer le temps de la tâche d'apprentissage. Une seconde sera de proposer une nouvelle approche pour la classification non linéaire.
Remerciements. Nous tenons à remercier vivement Jason Rennie du MIT pour son aide sur la préparation de l'ensemble de données Reuters-21578.

Résumé. Cet article présente un modèle pour aborder les problèmes de classement difficiles, en particulier dans le domaine médical. Ces problèmes ont souvent la particularité d'avoir des taux d'erreurs en généralisations très élevés et ce quelles que soient les méthodes utilisées. Pour ce genre de problèmes, nous proposons d'utiliser un modèle de classement combinant le modèle de partitionnement des cartes topologiques mixtes et les machines à vecteurs de support (SVM). Le modèle non supervisé est dédié à la visualisation et au partitionnement des données composées de variables quantitatives et/ou qualitatives. Le deuxième modèle supervisé, est dédié au classement. La combinaison de ces deux modèles permet non seulement d'améliorer la visualisation des données mais aussi en les performances en généralisation. Ce modèle (CT-SVM) consiste à entraîner des cartes auto-organisatrices pour construire une partition organisée des données, constituée de plusieurs sous-ensembles qui vont servir à reformuler le problème de classement initial en sous-problème de classement. Pour chaque sous-ensemble, on entraîne un classeur SVM spécifique. Pour la validation expérimentale de notre modèle (CT-SVM), nous avons utilisé quatre jeux de données. La première base est un extrait d'une grande base médicale sur l'étude de l'obésité réalisée à l'Hôpital Hôtel-Dieu de Paris, et les trois dernières bases sont issues de la littérature.
Introduction
En apprentissage artificiel, on distingue deux grands thèmes, l'apprentissage supervisé et l'apprentissage non supervisé ; la plupart des problèmes d'apprentissage sont traités par l'une des deux approches. Selon les problèmes de classement, on a recours à de nombreuses mé-thodes telles que les machines à vecteurs de support (SVM) qui sont évaluées sur leur capacité à prédire correctement la classe des observations. Pour l'apprentissage non supervisé, on utilise souvent le modèle des cartes topologiques où les critères de qualité sont plus difficiles à dé-finir ; ils s'articulent autour de l'interprétation des regroupements ou des partitions obtenues. Parmi les problèmes d'apprentissage, il existe une catégorie de problèmes qui sont appelés dans la littérature : difficiles, complexes, et plus particulièrement le problème traité dans ce papier concernant des données mixtes avec des variables quantitatives et qualitatives.
Une catégorie de modèles d'apprentissage spécifiques, combinant l'apprentissage non supervisé et supervisé, aussi bien dans le domaine de la classification hiérarchique et les arbres de décision que pour la recherche de partitions ont été développées, pour ce type de problème ; mais la majorité de ces modèles ne traitent que des données numériques, (Liu et al (2001); Rybnik et al (2003); Lebrun et al (2004); Sungmoon et al (2004); Shaoning et al (2005); Benabdeslem (2006)). Dans Wu et al (2004), les auteurs proposent d'utiliser les cartes topologiques de Kohonen (Kohonen (1995)) pour filtrer les données. Les observations non étiquetées héritent de l'étiquette de la classe du vote majoritaire de son sous-ensemble. A la fin de cette phase, un seul SVM est appris sur l'ensemble d'apprentissage initial réétiqueté, sans tenir en compte la partition des données. D'autre méthodes sont aussi inspirées des méthodes de partitionnement et de classement comme la définition de cartes topologiques dans l'espace de redescription (Sungmoon et al (2004)) ou l'utilisation des vecteurs supports pour définir une partition (Ben-Hur et al (2001)).
Notre approche est dédiée aux données numériques et/ou données mixtes, elle consiste à diviser le problème global de classement en sous-problème de classement guidé par la structure et l'organisation des données de la base en utilisant les cartes topologiques mixtes, (Lebbah et al (2005)). Le partitionnement des données avec les cartes, en une partition constituée de plusieurs sous-ensembles organisés vont servir à définir un classifieur pour chacun en utilisant les SVMs, Vapnik (1995). Les cartes topologiques mixtes sont utilisées dans notre modèle parce qu'elles sont de plus en plus utilisées comme outil de visualisation et de partitionnement non supervisé de différents types de données quantitatives et qualitatives codées en binaires. Elles permettent de projeter les données sur des espaces discrets qui sont généralement de dimensions deux et d'avoir des prototypes (représentants) du même type que les données initiales (quantitatives et qualitatives). Le modèle de base, proposé par Kohonen (Kohonen (1995)), est uniquement dédié aux données numériques. Les machines à vecteurs de support ont été développées dans les années 90 par Vapnik (1995). Ces méthodes ont été utilisées dans notre modèle parce qu'elles s'avèrent particulièrement efficaces. Elles peuvent traiter des problèmes mettant en jeu un grand nombre de variables tout en assurant une solution unique (pas de problèmes de minimum local comme pour les réseaux de neurones). L'algorithme sous sa forme initiale revient à chercher une frontière de décision linéaire entre deux classes, mais ce modèle peut considérablement être enrichi en se projetant dans un autre espace permettant ainsi d'augmenter la séparabilité des données.
Pour la compréhension de notre modèle, nous présentons dans la section 2, les différentes notations utilisées. Pour simplifier la présentation du papier, le modèle SVM et le modèle des cartes topologiques ne seront pas présentés. Dans la section 2, nous présentons la combinaison des deux modèles que nous proposons d'utiliser pour le classement. Dans la section 3.1, une validation du modèle sur des données issues de la littérature ainsi que des données médicales réelles. Cette validation pemet de démontrer que notre modèle peut être utilisé pour augmenter les performances en classement du SVM sur ce type de bases de données.
Méthode hybride Cartes topologiques et SVM : CT-SVM
On suppose que l'on dispose d'une base d'apprentissage A = {(z i , y i ); i = 1..N, z i ? D} où l'observation est z i , y i l'étiquette de sa classe utilisé pour l'apprentissage du modèle SVM, et D représente l'espace des observations de dimension d. Les observations z i sont composées de deux parties : la partie numérique z
Comme tout modèle de cartes topologiques, nous supposons que l'on dispose d'une carte discrète C ayant N cell . Cette structure de graphe permet de définir une partition de D en N cell sous-ensembles qui sera notée P = {P 1 , ..., P N cell }. A chaque sous-ensemble P c , on associe un vecteur référent w c ? D qui sera le représentant ou le "résumé" de l'ensemble des observations de P c . Par la suite nous notons W = {w c = (w m suivant le même codage binaire que les données initiales, ce qui simplifie l'interprétation des référents. La partition P de D peut être définie d'une manière équivalente avec la fonction d'affectation de la carte ? qui est une application de D dans l'ensemble fini des indices I = {1, 2, ..., N cell }. Dans le cas où il y a eu regroupement des sous-ensembles, nous avons défini une application surjective ? de I dans l'ensemble des indices J = {1, 2, ..., S} où 1 ? S ? N cell . Si on utilise ces définitions, le sous-ensemble P c est alors représenté par P c = {z ? D/?(z) = c, ?(c) ? J }, (si ?(c) = 1 alors P = P c = A). On notera par la suite, l'ensemble des indices I p des sous-ensembles purs tel que I p = {c/?z ? P c , ?(?(z)) = c, vote(P c ) = y c }. y c est l'étiquette du vote majoritaire à 100% du sous-ensemble P c en utilisant la fonction vote. Par la suite, nous présentons un modèle de classement qui permet d'augmenter les performances en classement du SVM en utilisant le partitionnement des observations par les cartes topologiques.
Dans (Kuncheva , 2004, chapitre 6), l'auteur fournit une démonstration théorique pour ce type de modèles combinant partitionnement et classement. Si l'on considère que l'on dispose de S classeurs notés Cla associés à différents sous-ensembles P i et si on note par p(Cla i /P i ) la probabilité du classement correct avec le classifieur Cla i dans le sous-ensemble P i , alors la densité de probabilité du classement correct de notre système de partitionnement et de classement s'écrit :
Notre approche consiste à entrainer des SVMs (Cla i = SV M ) différents avec des sousensembles d'une partition P de la la base A. Ceci permet de redéfinir des espaces de redescription différents (ou les mêmes) pour chaque sous-ensemble P c ? P, L'objectif de notre méthode CT-SVM est d'améliorer la discrimination en entraînant un SVM pour chaque sous-ensemble P c ? P qui a plus d'une classe (les sous-ensembles non purs). Pour les sous-ensembles qui sont composés d'observation de la même classe, aucun un SVM ne sera entrainé. Afin de réduire la partition et par conséquent le nombre de SVMs entrainés, nous avons utilisé la classification hiérarchique (CAH), sur l'ensemble des référents W de la carte pour réduire la partition ainsi le nombre de sous-ensembles, Yacoub et al (2001). Cette phase de réduction de la partition, qui consite à fusionner certains sous-ensembles, est optionnelle et elle peut être déterminée en interaction avec les experts et après visualisation des cartes topologiques ou avec un autre indice de regroupement,  .
L'algorithme de note modèle CT-SVM est le suivant : Pour un nombre de sous-ensembles S fixé faire : -Phase 1 : Construction d'une partition P = {P 1 , ..., P N cell } en utilisant les cartes topologiques mixtes constituées de N cell cellules. -Phase 2 (optionnelle) : Si S < N cell appliquer l'algorithme de regroupement pour construire la nouvelle partition P = {P 1 , ..., P S /1 ? S ? N cell } -Phase 3 : Détecter l'ensemble des indices I p des sous-ensembles pures tel que I p = {c/?z ? P c , ?(?(z)) = c, vote(P c ) = y c }. y c est l'étiquette du vote majoritaire à 100% du sous-ensemble P c (toutes les observation de P c portent la même étiquette y c ). -Phase 4 : Apprentissage d'une SVM pour chaque sous-ensemble P i tel que i / ? I p .
Remarque :
Pour l'apprentissage des cartes topologiques mixtes, nous avons utilisé notre programme déve-loppé en C/C++. Pour le regroupement des sous ensembles nous avons utiliser la classification hiérarchique. Nous avons aussi utilisé les programmes et l'heuristique développée par l'équipe de Kohonen, , pour estimer la dimension de la carte. Pour l'apprentissage du modèle SVM, nous avons utilisé la bibliothèque des programmes DAG-SVM (Directed Acyclic Graph SVM) dévellopé par Platt et al (2000); Cawley (2000). Avec ce modèle CT-SVM, la topologie des observations est préservée grâce aux cartes topologiques. Lorsqu'on présente une nouvelle observation qui n'a pas participé à la phase d'apprentissage, elle sera projetée d'abord sur la carte topologique avec la fonction d'affectation associée ?. Puis, on utilisera la fonction d'affectation ? (voir §2), pour selectionner le sous-ensemble qui va déterminer le classifieur SVM associé. Cette methode d'affectation de notre classement permet de comprendre le comportement d'une observation à travers son réfé-rent w c et/ou redéfinir une nouvelle partition en interaction avec l'expert. Si on note par svm r la fonction de classement du modèle SVM du sous-ensemble P r alors la fonction d'affectation globale de notre système s'écrit comme suite :
où I p est l'ensemble des indice des sous-ensembles purs. ?(c) = c si P = {P 1 , ..., P c , ..., P N cell } et ?(c) = 1 si P = A .  (2006)). La base de données comporte des variables cliniques et biologiques, recueillies avant l'intervention chirurgicale. Les patients sont classés en deux groupes (oui/non) suivant la médiane de perte de poids observée 3 mois après la chirurgie (gastroplastie par anneau ajustable ou bypass gastrique). Si la perte de poids est supérieure à la médiane, le patient est étiqueté "oui". Sinon il est étiqueté par "non". Chaque patient est caractérisé par 37 variables réelles (par exemple, le poids, le BMI, ALAT, ASAT, HDL, CRP...) et 13 variables qualitatives (exemple : diabète oui/non), caractérisant l'obésité et ses aspects cliniques et métaboliques, ainsi que ses complications multiples.
Pour étudier le comportement de notre modèle en classement, nous avons procédé à une validation croisée en variant le nombre de sous-ensembles de la partition et par conséquent, le nombre d'observations associées à chaque apprentissage d'un SVM. Ainsi, nous avons dé-coupé la base complète en trois sous bases de même taille, B 1 , B 2 , B 3 . On apprend sur deux bases parmi les trois et on teste les performances en classement sur la troisième en utilisant l'étiquette de perte de poids (oui/non) à trois mois. Ainsi, en utilisant le modèle CT-SVM ( §2), trois cartes topologiques sont construites de dimension 3 × 4, ce qui fournit une partition de 12 sous-ensembles (N cell = 12). Pour montrer l'importance de la taille de la partition, nous avons calculé les performances en classement en, variant le nombre de sous ensembles de 1 à 12. Dans le premier cas, l'application de notre modèle CT-SVM sur une partition avec un seul sous-ensemble est équivalente à entraîner un SVM binaire classique sur toute la base.
La figure 1 montre les trois variations du taux de bon classement des trois bases de test en fonction du nombre de sous-ensembles de la même partition. Dans le cas où la partition contiendrait un seul sous-ensemble, un seul SVM est entraîné sur toute la base. Ainsi, dans ce cas particulier, la fonction d'affectation des cartes topologiques ? n'influe pas sur la fonction d'affectation globale de notre modèle CT-SVM (formule 1). On observe aussi dans la figure 1, que l'augmentation du nombre de sous-ensembles de la partition permet d'augmenter les performances en classement sur les trois tests. En revanche, on constate que lorsque la taille de la partition est très grande, les performances diminuent. La partition contenant plusieurs sous-ensembles permet d'apprendre autant de SVMs que de sous-ensembles. Ainsi, la fonction d'affectation globale de notre modèle (formule 1) utilise d'abord la fonction d'affectation des cartes topologiques ? pour choisir le sous-ensemble, ainsi le SVM associé avec sa fonction d'affectation svm. Avec le premier test, on obtient au maximum, un taux de bon  classement de 60.6% avec trois sous-ensembles ; avec le deuxième test, on obtient un taux de bon classement 70.6% avec trois sous-ensembles. Finalement, avec le troisième test, on obtient 55.9 avec quatre sous-ensembles. Dans l'entraînement des SVMs avec notre modèle CT-SVM, nous avons utilisé la même fonction noyau linéaire. La validation croisée avec une variation du nombre de sous-ensembles montre l'intérêt et la difficulté de choisir la bonne partition pour une bonne discrimination. Cette partition est déterminée dans notre cas par expérimentation et visualisation des cartes topologiques. Cette validation croisée montre aussi l'intérêt de subdiviser le problème de classement global en sous-problèmes de classement pour améliorer les performances en classement.
FIG. 1 -Taux de bon classement avec CT-SVM en fonction du
Discussion
Puisque notre modèle utilise les cartes topologiques, on dispose d'un pouvoir de visualisation de la partition. L'application d'abord des cartes topologiques mixtes, va nous permettre d'analyser la répartition des observations et par conséquent les sous-ensembles qui ont servi au classement avec le SVM. Cette discussion va nous permettre de montrer l'intérêt de projeter les patients sur la carte pour comprendre le comportement et le profil de perte de poids du patient après chaque classement. L'apprentissage d'une carte de dimension 3 × 4 cellules effectué sur la base entière des patients, fournit pour chaque cellule un référent w c composé de deux parties : la partie quantitative w r c et la partie qualitative w b c codée avec le codage disjonctif binaire. La figure 2.a présente la répartition des observations. On observe que la partition obtenue a permis de bien distribuer les observations sur 12 cellules de l'ensemble de la partition P = {P 1 , ..., P 12 }, mais pour cet exemple, aucun des ces sous ensembles n'est pur. La figure 2.b présente la même répartition en distinguant ceux qui ont perdu ou non du poids à 3 mois par rapport à la médiane de l'ensemble des patients. On constate que les sousensembles sont mélangés. A l'aide de cette carte topologique 3 × 4, il est possible d'effectuer un certain nombre d'analyses de la base étudiée. Notre premier objectif est celui de partition-ner les données, en tenant compte de leurs spécificités (données mixtes) pour augmenter les performances en classement. Pour visualiser la carte topologique, nous nous sommes limités à analyser les effets dûs à quelques variables pour lesquels l'exactitude des propriétés médi-cales retrouvées peuvent êtres vérifiées. En observant à la fois les deux figures 2.a et 2.b le médecin a détecté globalement trois grands groupes. Pour s'approcher de la partition du mé-decin, nous avons appliqué la CAH avec les référents de la carte pour avoir 4 sous-ensembles, P = {P 1 , P 2 , P 3 , P 4 }. La figure 7 présente la partition avec 4 sous-ensembles numéroté de 1 à 4. Cette répartition des données en quatre sous-ensembles et la répartition du médecin en trois sous-ensembles correspondent à la taille de la partition utilisée dans la phase de la validation croisée décrite ci-dessous.
En visualisant à la fois les figures 2,3, 4, 5, 6 et la figure 7, il est possible de demander au médecin de définir les profils des patients. Ces profils vont servir à décrire les paramètres (variables) liés à la perte de poids et fournir des hypothèses de travail sur la résistance à la perte de poids fournies par le classeur SVM. Trois grands profils de patients sont définis selon la perte de poids à 3 mois. Le profil 1 est plutôt un bon profil par rapport aux pertes de poids à trois mois (figure 2.b) et correspond aux deux sous-ensembles P 1 et P 2 de la CAH. Le profil 2 est caractérisé par une perte de poids moyenne à 3 mois et correspond approximativement au sous ensemble P 4 de la CAH. Enfin le profil 3 est caractérisé par une perte de poids médiocre à 3 mois, ce qui aboutit à dénommer ce profil comme un "mauvais" profil en terme de perte de poids. Ce profil correspond au sous-ensemble P 3 de la CAH. Nous détaillons par la suite les deux profils 1 et 3 par rapport aux différentes variables clinico-biologiques.
Le profil 1 est caractérisé par un poids, un BMI (Body Mass Index) et une Dépense Energé-tique de Repos mesurée par calorimétrie (DERm) élevés. Les patients appartenants à ce profil ont une glycémie à jeûn et insulinémie élevées (figure 3) sans être diabétiques (figure 4). Il s'agit donc de patients insulinorésistants avant le stade de diabète. Le reste du profil méta-bolique est caractérisé par des HDL plutôt bas, des triglycérides (TG) et enzymes hépatiques (ASAT, ALAT et GGT) élevées,(figure 3). Dans les classes qualitatives "HTA" (hypertension) ou "SAS" (Syndrome d'apnées du sommeil) ces patients sont classés "oui" (figures 5 et 6). D'un point de vue inflammatoire, la CRP, la férritinémie (FERR), la SAA et l'orosomucoide (ORO), toutes des protéines de la phase aigue de l'inflammation, sont modérément élevées. Sur le plan nutritionnel, la TSH est basse, le profil protéique (albumine, préalbumine, RBP) et vitaminique est favorable, sans déficit. En conclusion pour ce profil, il s'agit de patients avec un poids très élevé mais dont le profil métabolique (figure 3) n'est pas trop évolué (sans diabète), sans inflammation importante et un bon profil nutritionnel.
Le profil 2 correspond à des patients ayant un BMI élevé et une leptine élevée (LEP, figure  3). Ils sont insulinorésistants mais pas diabétiques. Ils ont majoritairement une HTA et un SAS (figures 5 et 6). Les paramètres hépatiques et métaboliques sont normaux. L'adiponectinémie (ADIPO) est plutôt basse. En revanche, les paramètres inflammatoires (SAA et CRP) sont très élevés. Sur le plan nutritionnel, la TSH est normale, haute et les marqueurs nutritionnels sont bas (bilan protéique avec albumine, préalbumine et RBP, fer, vitamines A, E, B1, B12). Le profil 3 est un profil intermédiaire en terme de paramètres clinicobiologiques.
En conclusion, les deux profils de patients 1 et 2 sont caractérisés par des paramètres clinico-biologiques différents, notamment en terme de marqueurs d'inflammation et nutritionnels et sont aussi différents en termes de profil de perte de poids à 3 mois. Nous pouvons donc formuler l'hypothèse que le statut nutritionnel et l'état d'inflammation des patients avant chirurgie pourraient être des éléments liés à la résistance à la perte de poids. 
Bases issues de la littératures
Dans cet exemple, trois bases d'apprentissage comportant un nombre variable d'observations ont été utilisées, (table 1)  Puisque toutes les variables sont quantitatives, l'utilisation des cartes topologiques mixtes se réduit pour ces bases à l'application de cet algorithme avec l'hyper-paramètre F = 0 qui correspond à la version batch des cartes topologiques classiques de Kohonen. Afin de mesurer la robustesse de notre système, l'apprentissage de notre modèle CT-SVM est réalisé sur les bases d'apprentissage présentées dans la table 1. L'affectation des observations de la base de test est réalisée à l'aide de la fonction d'affectation de notre modèle CT-SVM, présentée par la formule (1). La 
Conclusion
Dans cet article, nous avons présenté un modèle de classement hybride, associant une mé-thode de partitionnement et une méthode de classement qui sont respectivement, les cartes topologiques et les SVMs. Ce modèle utilise l'organisation des données fournis par les cartes topologiques mixtes pour subdiviser l'espace des données afin d'apprendre un SVM spécifique pour chaque sous-espace des données. Notre modèle CT-SVM utilise la partition résultat des cartes topologiques, pour associer un SVM à chaque sous-ensemble de la partition avec des hyper-paramètres différents si cela est nécessaire. Les expériences effectuées montrent la robustesse de celui-ci à traiter des bases classiques avec uniquement des données réelles ou des données mixtes. D'autres parts, dans le cadre d'une application médicale réelle, nous avons vu que la quantité d'information fournie par ce modèle CT-SVM à travers les cartes topologiques mixtes est très importante et que le pouvoir de classement avec les SVMs est très performant. Nous avons aussi constaté, qu'il est important de choisir la taille de la partition. Ceci nous amène à réfléchir sur des indices qui permettent d'estimer la partition idéale et de faire une comparaison. Une comparaison avec d'autres méthodes classiques de classement est envisagée dans nos futurs travaux.
Summary
This paper introduces a classification model combining mixed topological map and support vector machines. The non supervised model is dedicated for clustering and visualizing mixed data. The supervised model is dedicated to classification task. In the present paper, we propose a combination of two models performing a data visualization and classification. The task of our model is to train topological map in order to cluster data set on organized subset. For each subset, we propose to train a SVM model. The global classification problem is devided into classification sub problem corresponding to the number of subset. The model is validated related to the obesity problem, which is provied by Nutrition team located in hospital Hôtel-Dieu in Paris. 
FIG. 7 -
Carte topologiques 3×4 après le partitionnement de la CAH. P = {P 1 , P 2 , P 3 , P 4 }.

Introduction
Nous nous intéressons dans cette contribution aux applications à forte composante d'activité socio-sémantique -notion que nous définissons exemples à l'appui. Nous avons analysé ce type d'applications dans de précédents articles comme relevant du « Web sociosémantique » matérialisé en particulier par des cartes de thèmes co-construites au sein de groupes en s'appuyant sur le modèle Hypertopic (Cahier et al., 2004).
L'approche proposée dans cet article vise à lever certaines difficultés qui subsistent dans la mise en oeuvre effective de ces cartes de thèmes co-construites au sein de communautés réelles. Le souci de mieux modéliser l'activité socio-sémantique accompagne une série importante d'expérimentations et de travaux menés au laboratoire Tech-CICO, pour mettre en oeuvre le modèle Hypertopic dans le cadre du Web socio-sémantique (applications utilisant les outils Agorae, Porphyry ou Cassandre) ou le comparer aux modèles sous-jacents à d'autres applications (telles que l'Open Directory Project, Del.icio.us ou Flickr, en partie basées sur les folksonomies et illustrant la tendance du Web2.0). Ces applications permettent à une communauté non seulement de partager des ressources, mais aussi de s'organiser pour mettre en commun et rendre manipulable la description de ces ressources, et faciliter la recherche ou la navigation selon de multiples points de vue (Lejeune, 2002).
Nous proposons une approche basée sur des modèles génériques, s'adressant non seulement aux professionnels de la modélisation (analystes, informaticiens, etc.) en termes de méthode de conception externe mais aussi -à terme -aux utilisateurs finaux en termes de conception participative. Ces modèles génériques visent la représentation des connaissances, mais aussi la représentation de l'activité socio-sémantique qui la rend possible.
Pour cela nous présentons un cadre recourant conjointement au modèle Hypertopic pour la représentation des connaissances de domaine, et au modèle SeeMe (Hermann et al., 1999) pour la représentation des rôles et de l'activité. Nous montrons comment ces deux modèles se complètent, pour mieux ancrer, sur le plan formel et méthodologique, les approches de cartographie collective des connaissances et d'ontologie sémiotique (Zacklad, 2005).
La notation SeeMe offre des avantages pour représenter certains aspects de l'activité collective. Elle autorise notamment des méta-relations et l'expression de caractéristiques d'incomplétude et de modularité, permettant de décomposer le modèle en plusieurs modules tout en assurant la cohérence formelle de l'ensemble. Elle nous permet de comparer deux modèles d'activités (parmi de nombreux autres) que nous avons expérimentés pour la coconstruction de cartes de thèmes multi-points de vue Hypertopic : l'un de ces modèles d'activités est construit avec le concours d'un médiateur (modèle KBM ou Knowledge Based MarketPlace, (Cahier & Zacklad, 2002) ; l'autre est construit sans le concours d'un médiateur, dans une forme de construction dite « controversée » (Zaher et al., 2006), où plusieurs acteurs construisent concurremment chacun « leur » cartographie.
Constatant qu'il existe une complémentarité et de bonnes perspectives d'intégration entre les modèles SeeMe et Hypertopic, nous argumentons pour construire en les associant des applications du Web socio-sémantique plus spécifiques en termes de rôles.
Dans la partie 2, nous définissons l'activité socio-sémantique et nous posons le problème de la variété des modèles d'activité susceptibles d'être impliqués dans cette activité sociosémantique. Après un rappel de l'état de l'art sur la conception participative, en particulier pour ce qui concerne la conception participative des modes d'organisation, nous justifions la nécessité de faire appel à de telles approches participatives pour l'activité socio-sémantique. Nous introduisons alors les bases conceptuelles du modèle SeeMe de Thomas Herrmann (1999) qui est un modèle élaboré à des fins plus générales pour la conception participative de systèmes socio-techniques. Dans la partie 3, nous rappelons les principes du modèle Hypertopic, qui permet de représenter des cartographies de connaissances notamment pour classer des collections selon plusieurs points de vue portés par des acteurs de ces communautés. La partie 4 exprime et compare, selon la représentation SeeMe, deux modèles sociaux que nous avons expérimentés pour la co-construction de cartes de thèmes Hypertopic. La partie 5 trace quelques perspectives pour un programme à venir, dans le sens d'une meilleure intégration au niveau des modèles, des outils et des méthodes, contribuant à l'activité socio-sémantique sur le Web.
2 Activité socio-sémantique et conception participative des modèles de cette activité
Web et activité socio-sémantiques
Au sein du courant du Web sémantique, nous avons été amenés à mettre l'accent sur les applications relevant d'un courant que nous avons caractérisé comme « Web sociosémantique » (Cahier et al., 2004). Cette notion fait l'objet de discussions dans la communauté d'ingénierie des connaissances (Gandon, 2006). Simon Buckingham propose une notion, selon nous très proche, de « pragmatic Web » (Buckingham, 2006). Le Web socio-sémantique s'adresse à des communautés d'utilisateurs poursuivant des objectifs similaires. Social, il participe à la construction d'une représentation structurée du domaine et du collectif. Il implique une structuration progressive des réseaux sémantiques gérés par le collectif, cette structuration représentant un enjeu pour le réseau social lui-même. Le Web socio-sémantique est adapté à la description collective des connaissances et à la recherche RNTI -X -ouverte d'information, par des humains, dans des ressources complexes et évolutives (Zaher et al., 2006b). Il se veut ainsi complémentaire au Web sémantique « logique ».
Lors de la construction du Web socio-sémantique, les groupes mobilisent des méthodes et des outils qui relèvent d'approches collaboratives pour la gestion des connaissances (Dieng et al., 2000). Les communautés considérées co-construisent de nombreux types de « structure sémantique au sens large » telles que des index, des cartes de thèmes (Park & Hunting, 2002), ou des ressources terminologiques et ontologiques (Aussenac et al., 2004 
Une nécessaire conception participative
Notre approche constructiviste de l'activité et du social nous incite à tenir compte de la réalité singulière de chaque communauté. La modélisation nécessite donc d'impliquer fortement des membres de la communauté. Cette description doit reposer sur des modes d'expression souples afin qu'à travers l'autodescription de son organisation, la communauté se voie elle-même comme à la fois productrice et bénéficiaire de son activité sociosémantique.
Ainsi, l'organisation sociale permettant à un groupe de co-construire une cartographie de thèmes va devoir prend des formes ad hoc dans chaque cas : il peut exister une division du travail entre plusieurs rôles fixes, comme dans le cas du modèle KBM (Knowledge-Based Marketplace) qui articule des rôles liés aux activités de contribution et de structuration sémantique des cycles de validations (Cahier et Zacklad, 2002). Nous avons cependant observé -notamment dans le cas d'un « annuaire de compétences co-construit » pour l'ingénierie d'Airbus (Cahier et al., 2004) -que l'activité socio-sémantique, si elle peut souvent s'appuyer sur ce cadre général de rôles, a besoin de le raffiner.
Il est donc nécessaire que l'organisation soit comprise et « lisible » par les membres de la communauté dans leur activité quotidienne. Cette lisibilité peut, selon nous, être améliorée par une confrontation des utilisateurs à des diagrammes exprimant la représentation des rôles et de l'activité. L'apprentissage de ces diagrammes, la participation à leur critique et à leur amélioration, procurent aux membres du groupe une certaine conscience de l'organisation en place. La maîtrise accrue de son organisation par le groupe est facilitée par l'expression et l'édition possible de ces diagrammes, au niveau même de l'interface utilisateurs des «portails » qui supportent l'implantation informatique de tout ou partie des règles de gestion prescrites dans le modèle. Etant donné les particularités de l'activité socio-sémantique, il nous semble difficile de confier la conception à des spécialistes extérieurs au domaine comme des praticiens ontologistes, des organisateurs ou des modélisateurs du système d'informations par des méthodes telles que UML. Selon notre approche, les membres de la communauté sont les plus aptes à décrire leur activité avec leur propre sémantique.
RNTI -X -
Le cadre des systèmes socio-techniques
Nous expliquons dans cette section l'approche du système socio-technique qui fonde les bases conceptuelles du modèle SeeMe de Thomas Herrmann (1999); ce modèle, qui est utilisé depuis cinq ans en Allemagne et qui a fait l'objet de plusieurs expérimentations de terrain (cf. Herrmann, 2005) et de discussions dans la communauté CSCW, a été élaboré pour la conception participative de l'organisation, à des fins plus générales que celles que nous proposons d'utiliser ici pour la modélisation de l'activité socio-sémantique.
Une manière simple de définir les systèmes socio-techniques est de les considérer comme des systèmes ayant un sous-système social et un sous-système technique (Herrmann et al., 2000). En CSCW, le terme renvoie : (1) à la prise en considération des aspects relatifs à l'un et l'autre des deux sous-systèmes quand une organisation introduit une nouvelle technologie ou un nouveau artéfact ; (2) à la relation complexe entre ces deux sous-systèmes.
Trouvant ses origines dans l'étude de l'organisation du travail et des impacts de l'introduction de dispositifs technologiques sur les aspects sociaux de l'organisation et in fine sur la productivité du travail, l'approche socio-technique a ensuite été développée ou critiquée par certains auteurs (Ehn, 2002). Cette approche a fait appel aux concepts d'autonomie (le comportement du système dépend exclusivement de sa propre structure), d'autopoièse (le système représente une unité qui est sans interruption reconstruite par ellemême), de contingence (rapport entre les stimuli de l'environnement et les réactions du système), de sélectivité (sélection des informations à communiquer, de la manière utilisée pour la communication et des informations en cours de réception), d'autoréférence (le système inclut sa propre description comme une partie à part entière de lui-même), l'incomplétude des descriptions et l'anticipation de l'évolution du système (Herrmann 2005).
Un système socio-technique est une unité : ses deux sous-systèmes social et technique doivent être étroitement intégrés. L'interaction entre les deux sous-systèmes apparaît à travers les inscriptions qu'elle laisse dans les structures de contrôles des dispositifs techniques et les processus de communication du sous-système social. Ces inscriptions peuvent être implicites et peuvent être partiellement explicitées au cours de réflexions.
Les auteurs du modèle SeeMe (Herrmann et al., 2000) s'inscrivent dans cette approche du système socio-technique. Ils souscrivent également à la problématique d'un modèle spécifique du domaine face aux modèles universels. Ils insistent enfin sur l'importance de modèles explicites permettant de soutenir l'externalisation de la structure des activités qui facilitent leur compréhension et leur changement. Leur modèle graphique et formel basé sur des diagrammes effectue un certain nombre de choix, comme ceux de la description textuelle, de représentations diagrammatiques riches (Moody, 1996) qui nous semblent nécessaires à l'expression de l'activité socio-sémantique. 
Le modèle SeeMe
TAB. 1 -Relations standard du modèle SeeMe (mais le modèle prévoit aussi des relations personnalisées, des méta-relations, etc.).
Les relations sont représentées par des emboîtements ou des flèches. Si aucune autre indication n'est fournie, les flèches ont une signification standard qui dépend des éléments qu'elles relient. Une relation commence avec un point d'ancrage à son « point de départ » ; elle aboutit à l'autre extrémité à son « élément de fin ». Il y a neuf relations standard simples (Tab. 1). Dans l'exemple d'un système de gestion de base de données (Fig. 1b), introduire de nouvelles données ne modifie pas la structure. Dans la notation SeeMe, la relation alors indiquée par une flèche simple signifie « change » (« entrer des données change la base de données »). Au contraire, l'activité « modifier la structure des données » (par exemple introduction d'une nouvelle table par l'informaticien) change la structure de la base. C'est un changement profond justifiant de recourir à une méta-relation (flèche en ligne brisée).
RNTI -X -
Modélisation du Web socio-sémantique avec SeeMe
Expression du modèle Hypertopic
Avec SeeMe comme avec Hypertopic, la structure est essentiellement déterminée par l'emboîtement de sous-éléments et les relations entre eux. Dans le cas de Hypertopic (Fig.2b) des thèmes (topics) sont articulés hiérarchiquement au sein de points de vue multiples considérant une collection d'entités 2 .
FIG. 2 -a) L'activité socio-sémantique relie une communauté avec les structures de sa sémantique et de son organisation ; b) Le Modèle Hypertopic ; (notation SeeMe)
Ce modèle est congruent avec la co-construction de cartographies de collections selon de multiples points de vue, débouchant sur ce que l'on pourrait appeler des « représentations domaine » ou des « connaissances artefactuelles du domaine » (Fig. 2a). Pour représenter l'organisation sociale, le modèle d'activité, de rôles, d'autorisations, de droits et de devoirs de chacun dans la (sous-)communauté concernée, nous proposons la modélisation SeeMe en 2 L'appellation « d'entité » que nous employons depuis l'origine du modèle Hypertopic (Cahier et al . , 2004) se heurte à l'emploi très chargé du terme « entité » tant en informatique des bases de données (modèle « entité-relation » de modélisation de structures de données) qu'en philosophie où le terme connote un fort degré de réification. De plus le modèle SeeMe utilise aussi le terme « entité » en un sens bien précis. Pour ne pas provoquer trop de malentendus, nous avons cherché une meilleure appellation. De son côté l'appellation « d'objet » ne convient pas non plus, car elle est malheureusement très connotée chez les informaticiens à cause de la « conception orientée objet ». Dans un récent document de travail (Zacklad et al., 2007) réunissant la réflexion des chercheurs impliqués dans le modèle Hypertopic, nous proposons au lieu « d'entité » l'appellation « Stuff », bien rendue en français par « trucs » : soit une notion respécifiée pour chaque cas dans le langage de la communauté considérée, pas encore (forcément) substantifiée et restant en débat, mais que le groupe a déjà besoin de caractériser, d'évaluer, d'organiser, de documenter et de ranger au sein d'une collection.
RNTI -X -complément de Hypertopic. Comme l'organisation sociale et le modèle d'activité varient à chaque application du Web socio-sémantique, il est nécessaire de fournir aux acteurs des outils de conception participative et en particulier un langage de représentation à ce niveau.
La « méta-relation » proposée par le modèle SeeMe joue un rôle clé pour symboliser l'émergence de la structure sémantique. Elle caractérise le rapport entre la structure sémantique (les points de vue, la carte de thèmes) et la collection considérée.
L'activité socio-sémantique avec Hypertopic
Les expérimentations de terrain du modèle Hypertopic ont mis en évidence la nécessité d'une certaine diversité des méthodes de co-construction, pour s'adapter à une variété d'objectifs d'activité, de contextes humains, organisationnels, etc. Hypertopic structure la représentation de la connaissance mais ne répond pas à la question du modèle d'activité à mettre en oeuvre pour cette co-construction. Hypertopic peut être considéré comme relativement neutre quant aux formes d'organisation des rôles et des actions pour coconstruire une cartographie. Bien qu'Hypertopic influence la forme de cette activité, il ne conduit pas à un modèle d'activité unique ou optimal dans tous les cas. Le choix reste possible parmi plusieurs méthodes : ainsi, les deux méthodes que nous proposons dans la suite, mais aussi d'autres qui restent à imaginer, en fonction des buts, des phases de la construction, ou des maturités des communautés. Nous soulignons la facilité de la création ou l'adaptation de la méthode à chaque cas, grâce à un langage basé sur des diagrammes facilement compris par les acteurs eux-mêmes.
Comparaison de deux modèles d'activité socio-sémantique
Comment les acteurs doivent-ils procéder concrètement pour déterminer les points de vue, et construire une carte Hypertopic ? Nous donnons ici deux exemples : (1) une méthode «mono-concepteur consensuelle» (Fig. 3), et (2) d'une méthode de «conception controversée pluri-acteurs» (Fig.4). La première est une méthode de conception initiale par un unique médiateur enquêtant auprès du groupe et posant un jeu unique et cohérent de « dimensions d'analyse » structurant la carte et reflétant le consensus où la sémantique majoritaire dans le groupe. La seconde permet la construction simultanée de plusieurs points de vue individuels et leur juxtaposition dans un même artéfact, permettant alors la comparaison et favorisant d'éventuelles synthèses ultérieures. La première méthode a été appliquée à la phase initiale de conception, déjà évoquée, d'un annuaire métier en ingénierie selon le modèle KBM (Cahier et al., 2002(Cahier et al., et 2004 ; la seconde est actuellement mise en oeuvre dans l'application SeqXAM dans le cadre d'un projet DKN soutenu par l'UNESCO (Zaher, 2006a).
Les contraintes d'un modèle de co-construction
A partir du moment où l'on s'est accordé sur la collection qu'il s'agit de considérer, la méthode de co-construction doit répondre aux besoins de deux grandes étapes. (1) L'amorçage et la conception initiale du système : un ensemble de points de vue (ceux qu'expriment des membres de la communauté, ou des dimensions d'analyses assez consensuelles, etc.) ayant sens pour la communauté est construit sur la collection d'entités (2) La construction sémantique par un cercle élargi d'acteurs : une fois le système initialisé, RNTI -X -le dispositif doit permettre des formes d'utilisation et de co-construction sémantique du système en « rythme de croisière », autour des différents types de rôles nécessaires à la communauté. Par exemple la méthode choisie pourra recommander que les éditeurs sémantiques ne modifient plus directement le schéma des points de vue. Les points de vue, jouant un rôle très structurant, ne pourront être modifiés qu'après un niveau élevé de concertation et de consensus (mené sur un forum de discussions) et une décision collective. Ce sera alors l'une des différences entre la phase d'initialisation et la phase de croisière.
La méthode d'amorçage « mono-concepteur »
La méthode «mono-concepteur» (Fig. 3) est une méthode rapide utilisable par un collectif où règne un certain consensus sur les dimensions d'analyse et les catégories conceptuelles de la collection considérée. Dans le cas d'une carte concernant la collection des projets de logiciels libres (Yeposs), l'enquête a révélé un quasi-accord sur les dimensions d'analyse pertinentes. Celles-ci correspondaient aux divers rôles et métiers principaux confrontés à cette entité « projet » (points de vue juridique, business model, fonctionnel, etc.). Le médiateur a pu poser un jeu consensuel de « points de vue » faisant sens.
FIG. 3 -Amorçage de la construction avec médiateur
La carte est conçue par un seul analyste mandaté par le groupe. Dans la pratique, il mène l'enquête auprès d'un sous-groupe représentatif de membres de la communauté (de futurs contributeurs et éditeurs de la carte). Il centre l'analyse sur certains éléments représentatifs.
Le concepteur de la carte travaille seul, en tentant de résoudre les différents points de vue qu'il rencontre dans son enquête en un jeu de dimensions d'analyse pertinentes par rapport à l'échantillon et consensuelles par rapport au groupe. Il réunit une analyse ascendante et inductive consistant à déterminer le jeu des dimensions d'analyse à partir de la collection, et des éléments d'analyse « descendante » qu'il base sur sa propre expérience (sa vision du RNTI -X -domaine et de l'activité enrichie par l'enquête auprès des membres du groupe). Il ne s'agit pas à proprement parler d'une méthode de co-construction mais plutôt d'intermédiation.
La méthode de « co-construction conflictuelle»
FIG. 4 -« Co-construction controversée » sans médiateur
Cette méthode (Fig. 4) ne suppose pas un rôle spécialisé d'enquêteur médiateur. Elle vise à permettre aux membres de la communauté de poser explicitement des opinions concurrentes, et à faciliter leur dialogue en se servant d'autant de points de vue qu'il existe d'opinions entrant en controverse. Elle est davantage une forme co-constructive s'appuyant dès le commencement sur l'expression explicite de plusieurs points de vue portés par des acteurs du groupe. Chaque membre exprime son point de vue selon les concepts du modèle Hypertopic. Cette méthode aide à visualiser les différences de conceptions entre membres du groupe. La carte considérée est une carte de conceptions. Elle est utilisée pour construire, visualiser et comparer les propositions des acteurs. Les co-auteurs de la carte peuvent alors se contenter de juxtaposer ces conceptions différentes (en portant éventuellement les divergences à la connaissance de la communauté), car il n'est pas toujours possible ni souhaitable de résoudre les différences. S'ils entrevoient des consensus partiels ou globaux réalisables et intéressants à établir, ils peuvent élaborer une carte de synthèse conciliant progressivement certaines divergences, et imaginer le cas échéant les procédures de résolution nécessaires (attribution de pondérations, vote…).
RNTI -X -
Perspectives et conclusion
Nous pensons qu'il existe une complémentarité et de bonnes perspectives d'intégration entre les modèles SeeMe et Hypertopic, pour des applications socio-sémantiques plus finement adaptées en termes de rôles spécifiques et personnalisées ; i.e. cela donne une marge de liberté pour élargir le jeu de rôles du modèle KBM présenté au § 3.1 via des modèles d'activité ad hoc adaptés selon des formes participatives à chaque application Agorae.
Du point de vue technique, cela ouvre la possibilité d'une transposition plus facile de nouveaux services, en prise directe sur les modèles dans une application spécifique (gestion plus simple de l'affichage et du contrôle d'autorisation pour accéder à certaines actions, par exemple). On peut imaginer une évolution de l'outil où un membre connaît les rôles qui lui sont accessibles grâce au diagramme et accède dès lors à ces actions plus directement (par exemple par clic sur la partie correspondante, activable, du diagramme SeeMe).
Nous travaillons par exemple actuellement à l'adjonction : (1) d'un rôle modérateur pour la fonction de discussion des thèmes (chaque thème peut faire l'objet d'annotations éventuellement chaînables, etc. permettant des discussions ciblées exprimant les controverses dans la construction collective de la sémantique, que nous expérimentons dans l'application Yeposs). Le processus modélisé en SeeMe a aidé à concevoir les détails de ce nouveau rôle, et permet à tous les membres de comprendre les règles de gestion partagée pour les remarques et controverses. (2) d'un rôle facilitateur (user advocate), dans l'application DKN, autorisé à rajouter par exemple « des bulles d'aides spécifiques » pour aider à la compréhension des utilisateurs en rapport avec les objectifs métiers de la carte.
Il est cependant prudent de considérer que l'approche de conception participative et de modélisation engagée que nous mettons ici en avant n'est pas (toujours) suffisante en ellemême, et qu'on aura encore besoin de spécialistes (sous certains aspects qui resteraient à définir, et dans des processus et des rôles qui sont alors à considérer en profonde mutation) : organisateurs, sémanticiens, ingénieurs de la connaissance, spécialistes de ressources terminologiques et ontologiques (RTO, cf Aussenac et al., 2004). Même dans les domaines métiers qui évoluent rapidement, l'activité socio-sémantique est un type d'activité particulière, qui a aussi besoin dans une certaine mesure d'une part d'institutionnalisation. Les acteurs inventent et vont de plus en plus inventer, avec les évolutions à venir du Web à partir de sa version 2.0, des organisations pour l'activité socio-sémantique, d'où l'enjeu de poursuivre la réflexion et les expériences sur les modèles et les notations soutenant la description de cette activité. 
Références
RNTI -X -

Introduction
Une ontologie est une structure formelle dans laquelle les concepts d'un domaine et les relations entre ces concepts sont définis (Gruber (1993)). Notre ontologie porte sur l'astronomie : dans leurs articles scientifiques, les astronomes identifient manuellement les caracté-ristiques des objets célestes, afin de les associer ensuite à une catégorie (galaxie, étoile, ...). Les catégories sont pré-définies et l'astronome détermine la classe correspondant le mieux à l'objet étudié. Cette classification a permis de catégoriser 3.751.128 objets célestes. Pourtant, il reste encore des milliards d'objets à classifier et à caractériser de la manière la plus exhaustive possible. L'utilisation des articles scientifiques, très facilement accessibles sous format électronique, permettent de répondre à ces attentes.
Nous proposons une méthode semi-automatique de construction d'une ontologie sur le domaine de l'astronomie. Les concepts de l'ontologie sont des classes dont les instances sont les objets célestes. Les propriétés de chaque classe sont partagées par toutes ses instances. Ces propriétés sont extraites automatiquement des textes par un analyseur syntaxique partiel et robuste "Enju" de Miyao et Tsujii (2005). Objets et propriétés sont classés dans un treillis de Galois selon l'analyse formelle des concepts : FCA présentée dans Ganter (1999). Le résultat de cette méthode est fourni aux astronomes afin d'étiqueter chaque classe d'après les propriétés partagées par les instances de la classe.
Notre méthode présente plusieurs avantages : -elle peut être appliquée quelque soit le corpus de textes et le domaine spécifique sur lequel elle est utilisée, -elle est formalisée par la FCA, -elle est rapide comparée à une ontologie construite manuellement, -et elle permet d'enrichir l'ontologie résultante par la mise à jour du corpus de textes.
Notre méthode de construction d'ontologie s'établit à partir d'un corpus de textes. Nous choisissons de caractériser les objets célestes présents dans les textes par les verbes avec lesquels ils apparaissent en tant que sujet ou en tant que complément. Les verbes en effet, nous permettent de définir la nature des objets. Par exemple, tous les objets ne peuvent pas être sujet du verbe "´ emettre" : un objet émetteur peut être une étoile mais pas une planète.
Tout d'abord, nous extrayons les paires (sujet,verbe) et (complément,verbe) qui repré-sentent l'entrée de la FCA. Ensuite un treillis de Galois est construit avec le context formel K= (G, M, I) tel que : G l'ensemble des objets célestes, M l'ensemble des verbes (propriétés), et il existe une relation I (g,m) ssi : l'objet g est sujet ou complément du verbe m. De là, le treillis est transformé en une ontologie de concepts : les concepts sont représentés par les propriétés (intensions) du treillis et les instances par les objets célestes (extensions) du treillis. Le treillis définit ainsi l'ordre partiel des concepts, d'après l'ensemble des propriétés qu'ils partagent. Enfin, chaque classe d'objets est étiquetée par les experts du domaine.
Cette méthode non supervisée propose aux astronomes une classification des objets cé-lestes pour éviter un "goulot d'étranglement" dans l'acquisition des connaissances. Elle ouvre deux perspectives. D'une part, une analyse plus fine des résultats de l'analyseur syntaxique permettrait d'utiliser des patrons syntaxiques plus précis -au lieu du marqueur général "complément" préciser si c'est un complément d'objet direct, un complément de lieu, etc -ainsi que d'autres types de marqueurs -non seulement les sujets et les compléments sont pris en compte mais aussi les adjectifs, les adverbes, etc. D'autre part, afin de tenir compte des propriétés multivaluées pour obtenir de meilleures classes résultantes, une relation plus riche que la relation binaire dans la FCA devrait être envisagée.
Summary
This paper presents a semi-automatic method of building ontology from a textual corpus on a specific domain. This method is based, on the first hand, on a robust and partial syntactic parser and, on the other hand, the use of formal concept analysis for the construction of object class with a Galois lattice. The construction of the ontology from concepts and instances hierarchization is effected in a formal transformation of the lattice structure. The application domain of this method is astronomy.

Introduction
Les profils d'accès à un site Web peuvent être influencés par certains paramètres de nature temporelle, comme par exemple : l'heure et le jour de la semaine, des événements saisonniers, des événements externes dans le monde (guerres, crises économiques), etc. Dans ce contexte, la plupart des méthodes consacrées à la fouille de données d'usage du Web (Web Usage Mining) (Cooley et al., 1999) prennent en compte dans leur analyse toute la période qui enregistre les traces d'usage : les résultats obtenus sont donc naturellement ceux qui prédominent sur la totalité de la période. Ainsi, certains types de comportements, qui ont lieu pendant de courtes sous-périodes ne sont pas pris en compte, et restent donc ignorés par les méthodes classiques. Il est pourtant important d'étudier ces comportements et donc de réaliser une analyse portant sur des sous-périodes significatives. Le volume des données considérées étant très élevé, il est en outre important de recourir à des résumés pour représenter les profils considérés. L'analyse de l'usage a commencé relativement récemment à tenir compte de la dépendance temporelle des profils de comportement. Dans (Roddick et Spiliopoulou, 2002), les auteurs examinent les travaux antérieurs. Ils résument les solutions proposées et les problèmes en suspens dans l'exploitation de données temporelles, au travers d'une discussion sur les règles temporelles et leur sémantique, mais aussi par l'investigation de la convergence entre la fouille de données et la sémantique temporelle. Tout récemment, dans (Laxman et Sastry, 2006) les auteurs discutent en quelques lignes des méthodes pour découvrir les modèles séquentiels, les motifs fréquents et les modèles périodiques partiels dans les flux de données.
Le présent article propose de suivre le changement de comportement à l'aide des résu-més obtenus par une approche évolutive de la classification appliquée sur des sous-périodes de temps. L'article est organisé comme suit : la section suivante présente l'approche d'analyse de l'usage basée en sous-périodes temporelles. Nous présentons aussi dans cette section les expé-riences réalisées, en analysant les résultats et en les comparant à ceux des méthodes classiques. La dernière section présente les conclusions et les travaux futurs envisagés.
Approche de classification par sous-périodes de temps
La caractérisation de groupes d'utilisateurs consiste à identifier des traits d'usage partagés par un nombre suffisant d'utilisateurs d'un site Web et ainsi fournir des indices permettant d'inférer le profil de chaque groupe (Da Silva et al., 2006a,b). L'approche proposée dans cet article consiste dans un premier temps à diviser la période analysée en sous périodes plus significatives (mois de l'année). Ensuite, une classification est réalisée sur les données de chaque sous-période, aussi bien que sur la période complète. Les résultats fournis sont donc comparés les uns avec les autres.
Dans ce contexte, nous avons réalisé quatre classifications de la manière suivante : -Classification globale : cette classification est obtenue sur la totalité des individus ; -Classification locale indépendante : pour chaque zone temporelle a priori, on réalise une classification de l'ensemble des navigations concernées. Comme chaque zone est distincte, chaque classification est donc indépendante des autres ; -Classification locale "précédente" : ici, on utilise la structure classificatoire de la pé-riode temporelle précédente pour obtenir une partition de la période courante ; -Classification locale dépendante : ici, on initialise l'algorithme pour une période temporelle avec les résultats de cet algorithme appliqué sur la période précédente.
Algorithme et critères d'évaluation
Pour la classification des navigations, nous utilisons un algorithme de type nuées dynamiques (cf Celeux et al. (1989)) applicable sur un tableau de données (voir tableau 1). L'algorithme doit en particulier : (1) pouvoir affecter de nouvelles observations à une classification existante, et (2) pouvoir initialiser l'algorithme avec les résultats d'une autre réalisation de lui-même. Pour toutes les procédures de classification, nous avons demandé 10 classes avec un nombre d'initialisations aléatoires égal à 100, sauf dans le cas de la classification locale dépendante.
Pour analyser les résultats, nous utilisons deux critères. Pour une analyse classe par classe, nous considérons la F-mesure de van Rijsbergen (1979). Pour une analyse plus globale, nous utilisons l'indice de Rand corrigé (cf Hubert et Arabie (1985) 
Application et résultats
Les Nous avons réalisé un suivi des prototypes des classes (mois par mois) pour les classifications locales indépendante et dépendante, puis nous avons projeté ces prototypes dans le plan factoriel (voir figure 1). Sur cette représentation, chaque cercle représente un prototype. Dans la classification dépendante, les dix classes sont représentées par des couleurs différentes. On note une certaine stabilité malgré la diversité de mois analysés. Dans le cas de la classification indépendante, la trajectoire temporelle est simplement matérialisée par les lignes qui joignent un prototype à son plus proche voisin dans la période temporelle précédente. Cela ne donne pas des trajectoires parfaitement identifiées car certains prototypes partagent à un moment donné le même prédécesseur. On note en fait que seules quatre classes sont parfaitement identifiées et stables, les autres subissant des fusions et séparations au cours du temps. Par l'analyse de la variance intra-classe, nous pouvons constater que les classes obtenues par la classification locale indépendante présentent plus de cohésion au sens de ce critère (voir figure 2).
A partir des valeurs de l'indice de Rand corrigé (cf figure 3), dans le cas de confrontation des classifications indépendante versus globale il y a presque systématiquement des valeurs faibles, c'est-à-dire que certaines classes de la classification indépendante ne sont pas retrouvées dans la classification globale. On voit aussi que la classification "précédente" ne donne pas des résultats très différents de ceux obtenus par la classification dépendante, ce qui confirme l'intuition acquise par l'observation des prototypes dans le plan factoriel : ces derniers bougent "peu" au cours du temps.
Ces différences sont confirmées par la F-mesure (cf figure 4). Ce qui apparaît nettement, c'est que les classes sont très stables dans le temps si on utilise la méthode de classification 
FIG. 1 -Classifications locales : indépendante (gauche) et dépendante (droite).
FIG. 2 -Variance intra-classe des classifications : indépendante (trait noir), dépendante (trait rouge) et globale (trait bleu).
dépendante. En fait, aucun indice ne descend au dessous de 0.877, ce qui représente une très bonne valeur. Par contre, dans le cas de la classification indépendante, on obtient au contraire des classes très différentes de celles obtenues globalement (avec des valeurs inférieures à 0.5).
Conclusions et perspectives futures
Dans cet article, nous avons abordé la problématique du traitement des données dynamiques dans le contexte de l'analyse de l'usage du Web. A travers nos expérimentations, nous pouvons dire que la méthode de classification locale dépendante montre que les classifications obtenues ne changent pas ou peu au cours du temps, alors que la méthode de classification locale indépendante est plus sensible aux changements qui peuvent se passer d'une sous-période à l'autre. Dans un plan secondaire, l'approche de classification locale indépendante permet van Rijsbergen, C. J. (1979). Information Retrieval (second ed.). London : Butterworths.
Summary
The way in which a Web site is visited can indeed evolve due to modifications of the structure and the contents of the site, or because of changes in the behaviour of certain user groups. Thus, the models associated with these behaviours in the Web Usage Mining domain must be updated continuously in order to reflect the current behaviour of the users. A solution to this problem, proposed in this article, is to update these models using the summaries obtained by an evolutionary approach of the classification methods.

Introduction
Dans cet article, nous nous intéressons au problème suivant : étant donné un ensemble de n données d 1 , ..., d n et une matrice de similarité M (d i , d j ) entre ces données, comment permettre à un expert d'explorer cet ensemble de données de manière visuelle et avec une approche guidée par le contenu. Nous considérons que l'expert souhaite avoir une vue globale des données mais également exploiter localement les données Shneiderman (1996), et en particulier passer de l'une à l'autre par une relation de voisinage tenant compte de la similarité. Notre problème se décompose en deux parties : établir un graphe de voisinage entre les données à partir de la similarité, et visualiser ce graphe afin de permettre à l'utilisateur de l'explorer.
Nous allons donc nous concentrer sur les méthodes de construction de graphes de voisinage (voir un état de l'art dans Hacid et Zighed (2005)). Ce type de structure est également appelée graphe de proximité. L'utilisation de ces graphes se retrouve aussi bien en fouille de données (classiques, spatiales) que dans l'apprentissage ou la classification de données (Ester et al., 1997). Cependant, les algorithmes de construction de ces graphes sont d'une grande complexité (par exemple O(n 3 ) pour l'algorithme des Voisins Relatifs), ce qui les rend inefficaces face à de grands volumes de données.
Nous nous sommes intéressés dans cet article à une méthode biomimétique pour la construction incrémentale de ce type de graphe. Il s'agit d'une généralisation de l'algorithme AntTree proposé par Azzag (2005) dans sa thèse. L'auteur a introduit un algorithme de classification non supervisée hiérarchique capable de traiter n'importe quel type de données. Il se base sur le principe d'auto-assemblage observé chez une population de fourmis réelles présenté dans Lioni et al. (2001). Nous proposons ici une généralisation d'AntTree en utilisant un graphe comme modèle de structure. Nous allons donc chercher à construire un graphe de voisinage qui soit représentatif de la similarité existante entre ces données avec une complexité inférieure à celle observée dans les méthodes classiques (Voisins Relatifs, graphes de Gabriel, triangulation de Delaunay).
La suite de notre article est organisée comme suit : dans la section 2, nous présentons les principes des graphes de voisinage et quelques modèles de référence dont celui des Voisins Relatifs auquel nous comparons notre algorithme. Dans la section 3, nous détaillons l'algorithme de construction ainsi que l'ensemble des règles locales de comportement des fourmis artificielles. Nous précisons également, dans cette section, la méthode de visualisation utilisée pour réaliser l'affichage de graphes de voisinage. La section 4, quant à elle, est consacrée aux résultats et à l'étude comparative sur des bases de données numériques. La dernière section rassemble les conclusions faites au cours de l'article et présente des perspectives.
Graphes de voisinage et visualisation
Principes des graphes de voisinage
Considérons un graphe G(?, V) où ? est l'ensemble des noeuds du graphe et V l'ensemble des arêtes contenu dans le graphe. G est appelé graphe de voisinage si la propriété ci-contre est respectée : il existe une relation binaire entre deux points (a, b) ? ? 2 si et seulement le couple de points (a, b) ? V. En d'autres termes, Pour un point p donné de ?, son voisinage ?(p) est l'ensemble des points (sous-graphe) contenant p et tous les autres sommets directement connectés. Dans notre cas, chaque noeud (ou sommet) du graphe est une donnée. Les liens qui vont connecter les noeuds entre eux doivent représenter une information sur le voisinage des données. Cette information peut être une notion de distance entre les données.
Il existe plusieurs méthodes pour établir ce type de graphe. La triangulation de Delaunay Preparata et Shamos (1985) va relier entre elles les données qui vérifient la propriété suivante : le cercle passant par les trois sommets de chaque triangle ne contient aucune autre donnée. Le graphe de Gabriel se construit selon Gabriel et Sokal (1969)   Toussaint (1991). Récemment, une extension a été proposée afin de construire de manière incrémentale un graphe de voisinage dans Hacid et Zighed (2005). Cette approche a pour but de compléter un graphe existant lors d'une phase de mise à jour.
Visualisation de graphes par forces et ressorts
Il existe de nombreux algorithmes de visualisation de graphes Di Battista et al. (1998). Les travaux de recherche sur les algorithmes de ressorts commencent avec Tutte (1963) et se poursuivent avec Eades (1984). Ce dernier utilise l'analogie suivante pour expliquer la visualisation dynamique de graphes : il compare les arêtes dans un graphe à des ressorts. Le système, ainsi considéré, engendre des forces entre les sommets. Ce qui provoque naturellement des dé-placements de sommets. Les sommets s'attirent et se repoussent. La notion d'attraction entre sommets se réalise grâce aux arêtes qui cherchent à atteindre une distance cible associée. Eades (1984) ajoute la notion de forces de répulsion aux sommets. La condition d'arrêt initialement proposée pour un tel système est un nombre maximum d'itérations (évolution du graphe dans le temps).
Plusieurs recherches ont ensuite été consacrées au domaine. Nous pouvons citer entre autres Kamada et Kawai (1989), Frick et al. (1994) et Fruchterman et Reingold (1991. Ces différentes propositions ont amené à l'établissement de plusieurs modèles de visualisation dynamique de graphes. Nous nous baserons sur Fruchterman et Reingold (1991) qui offre une méthode générique de visualisation.
Etant donné un graphe de voisinage, on définit dans notre outil la longueur à atteindre entre chaque couple de noeuds voisins par la notion de similarité existante entre les deux données correspondantes. Ensuite, les noeuds sont placés initialement de manière aléatoire sur un plan 2D, et les forces et ressorts agissent jusqu'à stabilisation du graphe qui offre alors une visualisation homogène et agréable à l'utilisateur.
AntGraph
Notre modèle est une extension de l'algorithme AntTree présenté dans Azzag et al. (2003). AntTree effectue une classification non supervisée hiérarchique pour regrouper des données de n'importe quel type (numérique, symbolique, textuel, ...) sous forme d'un arbre. Nous gé-néralisons ainsi ces principes dans le but de construire un graphe de fourmis avec comme connaissance de départ la mesure de similarité entre les données. Nous détaillons ci-dessous les règles de construction du graphe par des fourmis artificielles.
Nous considérons ici un ensemble de données triées de manière aléatoire. Chaque fourmi va représenter une donnée à regrouper. Nous choisissons ensuite aléatoirement une fourmi f 0 FIG. 1 -Etapes de construction du graphe. En 1., f 0 est le premier sommet et constitue le support fixe. En 2., la fourmi suivante f 1 se déplace sur f 0 et se fixe à cette seule fourmi qui lui est la plus similaire. En 3., la fourmi f 2 se déplace sur f 0 , se compare à elle et se fixe. f 1 est une fourmi fille de f 0 . f 2 doit vérifier si f 1 lui est similaire ou pas. En 4., nous avons le principe généralisé de construction de AntGraph.
que nous considérons comme le point d'entrée dans l'étape de construction du graphe, et donc comme le premier noeud de ce graphe. Puis nous simulons les actions de chaque fourmi f i , qui entre dans le graphe par le noeud f 0 , se déplace de noeud en noeud, et ce jusqu'à ce qu'elle se connecte dans le graphe. On peut alors passer à la fourmi suivante. Lorsque f i est en dépla-cement, on note f pos la fourmi sur laquelle elle se trouve. Ensuite, le voisinage perçu par f i correspond à f pos ainsi qu'aux fourmis connectées à f pos . Intuitivement, lorsque f i arrive dans le graphe, elle va suivre le chemin de similarité maximum indiqué dans le voisinage qu'elle perçoit, puis elle va se connecter en établissant un ou plusieurs liens (voir 1). L'algorithme est le suivant :
Le choix de la fourmi la plus similaire pour f i implique plusieurs cas de décision par rapport à sa position : -f pos est la fourmi la plus similaire à f i et ne possède pas de fourmis voisines. f i se connecte alors directement à f pos . -f pos n'est pas la fourmi la plus similaire à f i et possède des fourmis voisines. Dans ce cas, f i se déplace sur la fourmi voisine la plus similaire. -f pos est la fourmi la plus similaire et possède des fourmis voisines. Ce dernier cas se présente lorsque f pos , la fourmi la plus similaire, possède des fourmis voisines qui pourraient être également similaires à f i . Dans ce cas de figure, f i se connecte sur f pos et sur toutes les fourmis voisines de f pos suffisamment similaires à elle par rapport à un seuil de tolérance S t . L'opération est répétée récursivement pour toutes les fourmis connectées aux voisines de f pos .
Algorithme 1 Vue d'ensemble de l'algorithme de construction incrémentale ENTRÉES: f 0 est le 1 er noeud du graphe. SORTIES: le graphe de voisinage G.
pour une fourmi f i non connectée faire f i se place en f 0 (i.e. f pos ? f 0 ) si f pos n'est pas la fourmi la plus similaire à f i alors f i se déplace sur la fourmi voisine de f pos qui lui est la plus similaire sinon si f pos est la fourmi la plus similaire à f i alors f i se connecte à f pos si f pos possède des fourmis voisines alors calcul du seuil de tolérance S t f i interroge les fourmis voisines de f pos f i se connecte aux voisines de f pos les plus similaires (? S t ) et récursivement aux voisines de ces fourmis avec la même condition (similarité ? S t ) finsi finsi fin pour Nous précisons que la valeur du seuil de tolérance S t est calculé de la manière suivante :
Cet algorithme est bien incrémental : les fourmis/données sont ajoutées une à une dans le graphe.
Etude comparative
Nous réalisons notre étude sur des bases numériques artificielles et réelles (voir la partie gauche de la table 1). Notre méthode peut traiter tout type de données, du moment que la similarité existe. Les bases de données artificielles {Art1, ..., Art6} nous sont fournies par Azzag (2005). Les bases réelles que nous utilisons proviennent du CE.R.I.E.S., Guinot et al. (2001), pour la base de même nom et du UCI Repository of Machine Learning, Blake et Merz (1998), pour les autres. Il est à noter que pour chaque test, la fourmi support f 0 est choisie aléatoirement. Il en est de même pour les fourmis ajoutées une à une dans le graphe. Nous pouvons remarquer sur la figure 2, que les visualisations obtenues pour AntGraph ont une ressemblance très forte avec les visualisations obtenues pour Voisins Relatifs. Nous distinguons bien les regroupements de données. Nous retrouvons visuellement les classes réelles : 3 classes pour Iris, 4 pour Art6 et 2 pour Art2.
Visualisations
La figure 3 nous présente le rôle du seuil de tolérance S t dans la qualité de construction du graphe. Les visualisations correspondantes nous permettent de constater que la valeur de la constante ? a une influence sur la formation des graphes. Nous avons fait évoluer la constante de 0,1 à 0,99. Nous présentons trois visualisations pour les valeurs 0,8, 0,9 et 0,97. En dessous de 0,9, nous obtenons des graphes où les noeuds sont pratiquement tous connectés les uns aux autres : les fourmis ne se connectent pas aux fourmis les plus similaires mais à beaucoup d'autres qui ne le sont pas du tout. Pour une valeur ? de 0,9 ou légèrement supérieure, nous visualisons de manière esthétique le regroupement des données en classes : les fourmis se connectent à leurs voisines les plus similaires. Enfin, plus la valeur de ? est proche de 1, plus le graphe perd de l'information : les fourmis ne se connectent pratiquement plus qu'à une seule voire deux fourmis similaires (le graphe se transforme en arbre). Les visualisations les plus probantes que nous obtenons s'obtiennent pour des valeurs ? supérieures à 0,9.
Temps d'exécution
Nous avons représenté dans la partie droite de la table 1 les temps d'exécution obtenus par les deux méthodes et uniquement pour le calcul du graphe. En comparaison avec l'algorithme des Voisins Relatifs, notre algorithme obtient les meilleurs temps sur la totalité des bases testées. Par exemple pour les bases Art2 et Art5, AntGraph est respectivement 500 à 800 fois plus  1)) ou en désaccord ((0,1) et (1,0)) entre les graphes construits par chacune des méthodes.
rapide. Nous pouvons remarquer que plus le nombre de données est important, plus notre algorithme est rapide par rapport à l'algorithme des Voisins Relatifs. La complexité de ce dernier étant en O(n 3 ), le temps d'exécution devient prohibitif pour les grands ensembles de données. A ce niveau, notre algorithme conserve des temps d'exécution très compétitifs, ce qui présage de nouvelles applications dans le cadre des grandes bases de données, et représente un avantage certain.
Cette baisse de la complexité est du au fait que les nouvelles fourmis qui arrivent ne se comparent pas à l'ensemble des données déjà présentes dans le graphe mais suivent les chemins de plus grande similarité et ne rencontrent ainsi qu'un nombre très limité de données. A titre indicatif, dans un arbre équilibré, en suivant une branche de la racine vers les feuilles on ne rencontre que O(ln(n)) noeuds.
Néanmoins, ces performances en terme de temps de calcul ne renseignent pas directement sur la qualité du graphe construit. Nous avons donc poursuivi nos expérimentations pour montrer que le graphe construit par AntGraph est représentatif de la similarité entre les données, au même titre que Voisins Relatifs.
Données
Voisins 
Qualité des graphes construits
Nous rappelons qu'un graphe peut se représenter sous la forme d'une matrice binaire où 1 correspond à l'existence d'un lien entre deux sommets. Nous pouvons donc mesurer, avec une matrice de confusion, l'accord en terme de liens entre deux graphes (voir table 2).
Nous pouvons remarquer dans un premier temps que pour l'ensemble de nos matrices de confusion, le rapport du nombre total de non-liens (un 0 entre deux sommets) pour Voisins Relatifs d'une part et pour AntGraph d'autre part est relativement proche de 1. C'est le cas par exemple pour la base Art4 où le graphe de Voisins Relatifs totalise 19658 liens à 0 pour un équivalent dans le graphe de AntGraph à 19454. Les deux méthodes sont donc en accord sur ce point. On remarque de même qu'il y a un accord entre les liens qui sont créés dans les deux graphes : la moitié des liens de Voisins Relatifs se retrouvent dans AntGraph. Enfin, nous constatons que notre algorithme produit entre 2 à 4 fois plus de liens que Voisins Relatifs. C'est le cas par exemple pour la base Iris (Voisins Relatifs contient 203 liens pour 495 liens dans AntGraph) avec un rapport de 2 et la base Art3 (Voisins Relatifs contient 1273 liens pour 5915 liens dans AntGraph) avec un rapport de 4.
On constate donc qu'AntGraph construit plus de liens que Voisins Relatifs. Dans le cas de Voisins Relatifs, il est difficile de rencontrer la situation où un noeud est voisin d'un grand nombre d'autres noeuds. Concernant AntGraph, nous devons alors nous interroger sur la pertinence de ces liens par rapport à Voisins Relatifs. Pour cela, nous avons mesuré, dans un premier temps, pour un graphe donné, la similarité moyenne entre les voisins. Dans un second temps, nous avons effectué une mesure entre les données en considérant les k plus proches voisins (k = 1, 2, ou 3). Et dans un troisième et dernier temps, la mesure a été faite sur l'ensemble des données.
La table 3 donne les résultats obtenus. On peut constater alors qu'AntGraph crée des liens supplémentaires mais pas de manière aberrante par rapport à la similarité. En effet, la similarité moyenne des liens de AntGraph est du même ordre de grandeur que celle des 2 plus proches voisins, et reste de plus très inférieure à la similarité moyenne du graphe complet.
Conclusion
Nous avons proposé dans cet article une méthode pour la construction de graphes de voisinage. Elle s'inspire d'un modèle de construction de structures chez les fourmis artificielles. Le graphe est construit de manière progressive et incrémentale à partir d'un noeud initial. Chaque fourmi se déplace en suivant le chemin de plus grande similarité afin de trouver un noeud sur laquelle elle se connecte. Nous avons testé notre approche sur un ensemble de bases et nous avons montré que les temps d'exécution sont compétitifs par rapport à l'algorithme des Voisins Relatifs. Par ailleurs, nos tests sur la qualité des graphes ont montré également que notre approche propose des graphes représentatifs de la similarité entre les données.
Dans les perspectives en cours d'étude, nous pouvons principalement indiquer les pistes suivantes. D'une part le traitement de grands volumes de données paraît possible gràce à des temps très courts. Ainsi nous souhaitons faire des tests sur des flux de données afin de visualiser leur évolution au cours du temps. Ensuite, un défaut de notre approche (et des Voisins Relatifs) vient du fait que la construction du graphe est découplée de l'algorithme d'affichage. Autrement dit, pour un graphe très important, il est facile de le calculer grâce à la complexité faible d'AntGraph, mais il devient difficile de l'afficher car le temps de convergence des algorithmes à base de forces et de ressorts devient important. Donc nous allons combiner la construction incrémentale du graphe avec une version incrémentale de l'algorithme de visualisation. Enfin, nous avons commencé à tester des opérations interactives permettant de parcourir le graphe, de le modifier en décrochant des noeuds qui vont ensuite être réinjectés dans le graphe. De cette manière nous allons rendre plus interactif cet outil d'exploration de données. 

Introduction
Le problème de la découverte des modèles temporels caractérisant le comportement des systèmes dynamiques est un enjeu majeur pour les tâches de contrôle et de surveillance. La raison de base réside dans la difficulté des experts humains d'apprendre et de formuler leurs connaissances sur la dynamique de ces processus. La surveillance est effectuée à partir d'un ensemble d'observations (séquences d'occurrences d'événements discret) produites par le système de pilotage. Les séquences d'observations remontées par le système de supervision sont porteuses de connaissances temporelles sur les relations causales entre les différentes variables du processus.
Notre approche est centrée sur la découverte des séquences particulières d'événements signe d'un comportement particulier. Nous proposons de représenter le comportement du systèmes sous la forme de chroniques (un formalisme graphique pour la représentation des motifs temporels où les noeuds sont les classes d'événements et les arcs représentent les contraintes temporelles liant les classes d'événements) (Dousson et Duong (1999), Ghallab (1996)). Ce choix de représentation s'est révélé particulièrement adapté à la représentation des évolutions de systèmes dynamiques, tout en maintenant une complexité raisonnable pour le traitement en temps réel des occurrences d'événements pour la supervision. Notre méthode de découverte des chroniques se déroule en deux phases : la première phase consiste à modéliser la séquence d'événements discrets en intégrant l'approche stochastique proposée par Le Goc et Bouché (2005). Cette approche est basée sur la représentation d'une séquence d'événements discrets sous les formes duales d'une chaîne de Markov homogène et une superposition de processus de Poisson. Dans la deuxième phase, nous proposons un algorithme, appelé BJT4R (Backward Jump with Timed constraints For Roads), destiné à la découverte des chroniques à partir du modèle obtenu durant la première phase. L'algorithme BJT4R est une extension de l'algorithme Viterbi (Viterbi (1967)) aux chaînes de Markov, basé sur l'application de la relation Chapmann-Kolmogorov comme fonctionnelle de coût.
La section suivante présente les principales approches de découverte des motifs séquentiels à partir de données datées. La section 3 introduit l'approche de modélisation adoptée, dans la section 4 nous présentons l'algorithme BJT4R. Les résultats préliminaires pour la découverte des processus de fabrication des wafers sont présentés dans la section 5. La conclusion de ce papier évoque les prochaines étapes de travail.
Contexte
La problématique générale est la suivante : étant donné un ensemble de comportements particuliers ou ordinaires, observés dans une série d'expériences, quelles sont les modèles temporels qui caractérisent au mieux ces comportements ? Des questions similaires ont été traitées dans le domaine de Fouille de Données Temporel.
Introduits pour la première fois dans Agrawal et Srikant (1995), les motifs séquentiels peuvent être vus comme une extension de la notion des règles d'associations (Agrawal et al. (1993)), intégrant la notion de temps. Dans Agrawal et Srikant (1995), les auteurs proposent une approche permettant la découverte des motifs séquentiels à partir de bases de données contenant des séquences de transaction d'achat effectuées par des clients. Une séquence est constituée de plusieurs transactions, réalisées par un client. Une transaction est caractérisée par un identifiant, une date de transaction et l'ensemble des produit achetés, appelés ItemSet. Le problème de la découverte de motifs séquentiels consiste à rechercher l'ensemble des sé-quences ayant des supports supérieurs à un certain seuil minimal. Un ensemble d'algorithmes ont été tirés de cette approche fréquentielle : AppioriAll, ApprioriSome et DynamicSome. Ces algorithmes de recherche des motifs séquentiels présentent quelques limites concernant la prise en compte des contraintes temporelles. Ce problème a conduit à la recherche des séquences généralisées définies dans (Srikant et Agrawal (1996)). Cette technique de recherche permet d'obtenir des motifs séquentiels respectant certaines contraintes temporelles définies par l'utilisateur (par exemple, regroupement des achats lorsque leurs dates sont assez proches, considération des Itemsets (achats) comme trop rapprochés pour apparaître dans le même motif fréquent).
Dans Mannila et al. (1997) . Lorsque les épisodes ont été découverts, des règles sont déduites afin de décrire ou de prédire toute ou partie d'une séquence. Cette méthode proposée traite le temps d'une manière implicite : la seule contrainte temporelle qu'elle autorise est une borne maximale (la largeur de la fenêtre temporelle) sur la durée des épisodes, celle-ci devant être fixée par l'utilisateur. Par contre, les contraintes temporelles liant les éléments des épisodes ainsi découverts sont ignorées. Ghallab (1996) propose une méthode permettant la découverte des modèles de chroniques à partir d'un ensemble de séquences d'alarmes, divisées en deux sous ensembles : un ensemble de séquences positives (exemples) et un ensemble de séquences néga-tives (contre exemples). L'idée générale de la méthode est de considérer dans un premier temps les séquences d'événements comme des suites ordonnées, sans tenir compte l'aspect temporel. Ceci revient à conserver l'ordonnancement des événements, et à supprimer toute notion de temps. Une fois les séquences dépourvues des écarts temporels, on détermine les chroniques les plus longues qui sont communes à tous les exemples, et qui ne sont pas reconnues par les contres exemples. Pour les motifs séquentiels découverts, les contraintes temporelles entre les éléments de modèles peuvent être déterminées par les experts ou par le calcul des écarts minimaux et maximaux entre chaque couple de deux alarmes afin d'englober toutes les occurrences de ces deux types d'alarmes. Une autre approche proposée par Dousson et Duong (1999) permettant la découverte des modèles de chroniques à partir d'une séquence d'alarmes, appelée journal d'alarmes. Cette approche repose sur une analyse fréquentielle de la séquence d'alarmes, visant à identifier les formes temporelles récurrentes. Cette méthode est une extension de l'approche proposée par Mannila et al. (1997) (Algorithme Minepi) par l'introduction des contraintes temporelles entre les alarmes.
Dans un article plus récent, Mannila (2002) dresse un bilan des principales limites des approches présentées et identifie le principal défaut : les relations entre les éléments des modèles que ces algorithmes permettent de découvrir sont trop locales pour constituer une véritable représentation de la séquence étudiée. Il invite donc à rechercher des algorithmes adoptant un point de vue plus global. Le Goc et Bouché (2005) proposent ainsi une approche stochastique globale permettant la modélisation des séquences d'événements discrets générées par un système à base de connaissances, de surveillance et de diagnostic de processus dynamiques. Cette approche est basée sur la représentation d'une séquence d'événements discrets sous les formes duales d'une chaîne de Markov homogène et d'une superposition de processus de Poisson.
3 Modélisation de séquences -Approche Stochastique-
Le couple (o k , o k+1 ) de deux occurrences successives liées à une même variable x décrit l'évolution temporelle de la fonction
. Nous allons utiliser la notation "e i :: C j " pour noter que l'événement discret e i appartient à la classe d'événement C j . Par extension, nous notons "o i :: C j " une occurrence d'un événe-ments discret appartenant à la classe C j . La relation binaire
décrit la relation orientée entre deux classes d'événements discrets contraintes temporellement. " [? ? , ? + ] " est un intervalle de temps pour observer une occurrence de la classe de sortie C o après l'occurrence de la classe d'entrée C i .
Dans ce contexte, un modèle de chroniques est un ensemble de relations binaires temporellement contraintes entre des classes d'événements discrets. Le modèle de chronique
) > définit deux relations binaires entre trois classes d'événements discrets vérifiant la relation suivante :
12 , ?
La figure 1 montre une représentation graphique de modèle de chronique M 123 représenté dans le "Langage ELP" ( Frydman et al. (2001) ; Le Goc et al. (2006) 23 , ? ?
Un modèle de chroniques peut être utilisé dans une tâche de diagnostic pour la prédiction d'une occurrence d'une classe particulière dans une séquence. Comme la classe d'événement C 3 dans le modèle M 123 . 
Lorsque la chaîne de Markov est homogène, la probabilité de transition d'un état i à la date t k?1 vers l'état j à la date t k dépend uniquement des états i et j :
Le processus de comptage des transitions d'états dans une chaîne de Markov homogène est un processus de Poisson (N i j (t);t ? 0) où N i j (t) compte le nombre des transitions X(t k?1 ) = i ? X(t k ) = j de l'état i vers l'état j. C'est le nombre de sous séquences ? = (o k?1 :: C i , o k :: C j ) dans la séquence ?. Le processus de Poisson N i j (t) est entièrement défini par l'unique paramètre ? i j , appelé le taux de Poisson, qui correspond au nombre de transitions (X(t k?1 ) = i) ? (X(t k ) = j) par unité de temps. Dans ce cas, la probabilité de transition dans la chaîne de Markov homogène est donnée par :
Les contraintes temporelles sont évaluées à partir des délais d(o k :: 
Le délai moyen D i j entre deux occurrences de classe C i et C j dans ? est donné par : L'algorithme BJT4R, basé sur l'approche stochastique, est une extension de l'algorithme Viterbi pour la recherche des chroniques dans un espace d'états Markovien. La relation de Chapmann-Kolmogorov est utilisée pour définir la fonction de coût des chroniques, afin de sélectionner les chroniques les plus probables. Cet algorithme opère en deux étapes :
1. Identification des chroniques sans contraintes temporelles, l'idée est d'identifier un ensemble des motifs séquentiels les plus probables liant une classe d'événement d'entrée à une classe d'événement de sortie. Cette étape est basée sur l'utilisation de la matrice de probabilité de transition construite à partir de la séquence ? et l'utilisation de la relation de Chapmann-Kolmogorov comme fonction de coût dans l'algorithme Viterbi.
2. Établissement des contraintes temporelles en utilisant la superposition des processus de Poisson, l'idée est d'utiliser directement les contraintes temporelles estimées à partir des processus de Poisson composés.
Découverte des chroniques sans contraintes temporelles
Soit X = (X(t k )), k ? 0, une chaîne de Markov homogène correspondant à la séquence ?. Soit S M l'ensemble des états de X correspond à l'ensemble des classes d'événements discrets ayant une occurrence dans ?,
M , sa matrice de transition des occurrences binaires de sous séquences
La longueur d'une chronique |? | = k est le nombre de relations binaires contenues dans ? (i 0 , i 1 , . . . , i k ). Une chronique réduit à un seul état a une longueur de 0. Selon la propriété d'absence de mémoire de chaîne de Markov, la probabilité
n=1 Comme la chaîne de Markov est homogène, la probabilité
. . , i k ) est égale à la probabilité de transition d'un état i 0 vers une autre état i k en passant par (k ? 1) états intermédiaires. Elle est donnée par le produit des probabilités des transitions d'un état à l'autre le long du chemin :
La probabilité p k i j d'aller d'un état i vers l'état j en k transitions est égale à la somme de toutes les probabilités des chroniques de longueur k liant l'état i à l'état j, donnée par la relation de Chapmann-Komologrov :
Si la chaîne de Markov est homogène, on a :
La relation 15 donne la probabilité totale de tous les chroniques possibles de longueur k menant de l'état i vers l'état j, y compris les chroniques de faible probabilité, qui correspondent à des comportements peu fréquents dans le processus modélisé par la chaîne de Markov. L'objectif de l'algorithme est d'identifier les m chroniques les plus probables liant deux états i et j dans une chaîne de Markov X = (X(t k ), k ? 0) construite à partir d'une séquence ?. -Un état est ajouté s'il n'apparaît pas dans les chroniques sous la construction. Cela signifie que l'algorithme interdit l'apparition de plus q'une classe d'événement dans une chronique (ligne 10). -Un état est ajouté dans la chronique s'il conduit à un nouveau chronique de probabilité supérieure à un seuil minimal (ligne 13-14).
Établissement des contraintes temporelles
La seconde phase de l'algorithme consiste à calculer les contraintes temporelles entre les classes d'événements des chroniques identifiées. L'idée est d'utiliser directement les contraintes temporelles estimées à partir des processus de Poisson composés déduits de la superposition des processus de Poisson (paragraphe 3, équations 9 et 10).
La contrainte temporelle entre chaque couple de classes (C i ,C j ) est un intervalle de la forme 0, 2 ? i j où ? i j est le taux de Poisson, qui correspond au nombre de transitions (o k?1 ::
L'algorithme BJT4R est un des outils développés au sein du " Laboratoire ELP ", un environnement Java dédié à l'analyse des séquences d'événements discrets ( Frydman et al. (2001)). La section suivante présente l'application industrielle de l'algorithme BJT4R pour la découverte des routes de fabrication des wafer dans la société STMicroelectronics.
Application
L'application proposée dans cette section concerne les routes de fabrication des wafers dans le site de production de STMicroelectronics. Un wafer est une galette de silicium sur laquelle sont gravées des puces électroniques pour la télécommunication. Le système de supervision de processus de fabrication génère une large quantité d'informations (? 10.000 alarmes par jour). Ces informations décrivent les différentes étapes de processus de fabrication sous la forme d'occurrences d'événements discrets correspondant aux débuts et fins de chacun des traitements appliqués sur la plaque. Cette suite de traitements (appelée route) transforme les plaques de silicium en wafers contenant des puces électroniques. -Recette. A ce niveau de granularité le plus élevé, une occurrence est un couple (r,t) où t est la date de début de la recette r. Dans ce cas, une route est une suite d'occurrences de recettes parmi les 5189 types (classes) de recettes. L'application présentée dans cet article concerne une première approche du problème, nous nous sommes intéressées au niveau de granularité plus bas : le niveau Équipement. L'objectif est de montrer la faisabilité de l'approche aux processus de fabrication exploités sur le site Rousset de la société STMicroelectronics. Dans cette application, un modèle de chroniques est un ensemble de relations binaires les plus probables liant les équipements deux à deux en satisfaisant les contraintes temporelles. Une contrainte temporelle est le temps moyen entre deux traitements successifs effectués sur les équipements de la relation binaire.
Pour appliquer l'approche stochastique, deux conditions doivent être satisfaites :
1. Les occurrences des événements doivent être indépendantes. 2. Le processus de génération des occurrences doit se comporter comme une superposition de processus de Poisson.
Dans notre application, ces conditions sont vérifiées. La première est assurée par la dé-finition du système de supervision (les alarmes sont générées indépendamment les unes des autres). Selon les experts de STMicroelectronics, le taux d'occurrences des événements discrets par jour est globalement stable durant toute la durée de fabrication, sauf incidents exceptionnels. La figure 3 montre six processus de Poisson correspondant à six équipements sélectionnés aléatoirement, cela garantie la deuxième condition.
FIG. 3 -Superposition des processus de Poisson
La chaîne de Markov associée comprend 309 états(i.e 95481 transitions). La figure 4 pré-sente une partie de la matrice P de probabilité de transitions entre les classes d'événements discrets. La séquence d'événements analysée correspond à une superposition de 309×(309?1) = 95172 processus de Poisson composés. Pour chaque processus de Poisson, le temps interoccurrences est constant, et correspond à la probabilité maximale de la loi exponentielle. L'application de l'algorithme BJT4R à la séquence ? produit un graphe (Fig 5) liant la classe d'événement d'entrée à la classe d'événement de sortie. La figure 5 montre partiellement l'ensemble des chroniques les plus probables menant de l'équipement 1130 à l'équipement 1206, la classe d'événement spéciale 0 dénote le début de la route.
L'algorithme BJT4R est paramétré afin de produire les cinq chroniques les plus probables de longueur 15. Par souci de lisibilité, seulement les débuts et les fins des chemins sont montrés dans la figure 5.

Introduction
De nos jours, bien que les moyens de stockage soient de plus en plus performants et de moins en moins chers, les entrepôts de données arrivent vite à saturation et la question des données à conserver sous forme d'historique va se poser rapidement. Il faut donc choisir quelles données doivent être archivées, et quelles données doivent être conservées actives dans les entrepôts de données. La solution qui est appliquée en général est d'assurer un archivage périodique des données les plus anciennes. Cette solution n'est pas satisfaisante car l'archivage et la remise en ligne des données sont des opérations coûteuses au point que l'on peut considérer que des données archivées sont des données perdues (en pratique inutilisables dans le futur) du point de vue de leur utilisation dans le cadre d'une analyse des données.
Dans cette communication, nous proposons une solution pour éviter la saturation des entrepôts de données. Un langage de spécifications de fonctions d'oubli des données anciennes est défini pour déterminer les données qui doivent être présentes dans l'entrepôt de données à chaque instant. Ces spécifications de fonctions d'oubli conduisent à supprimer de façon mécanique les données à 'oublier', tout en conservant un résumé de celles-ci par agrégation et par échantillonnage. L'agrégation et l'échantillonnage constituent deux techniques standard et complémentaires pour résumer des données. Considérons un entrepôt de données d'analyse des click-stream sur les sites web. Avec le temps, les données détaillées anciennes deviennent de moins en moins 'utiles' et peuvent donc être agrégées par jour ou par mois par exemple. En plus d'agréger des données, on peut conserver certaines données jugées intéres-santes ou choisies de façon aléatoire dans le but de pouvoir effectuer des analyses sur les données de l'entrepôt.
Le langage de spécifications est défini dans le cadre du modèle relationnel : sur chaque table, est défini au moyen de spécifications un ensemble de n-uplets à archiver. Pour des raisons applicatives, parmi les n-uplets à archiver, des échantillons peuvent être conservés dans le cadre de l'utilisation de l'entrepôt. De plus, des algorithmes pour mettre à jour le contenu de l'entrepôt conformément aux spécifications de fonctions d'oubli sont présentés. A noter que ces algorithmes gérés par l'administrateur de bases de données ont été étudiés et programmés dans un prototype sous Oracle. Cette communication a pour but d'apporter une réponse au problème de saturation des entrepôts mais l'approche décrite ici s'applique de façon générale aux bases de données relationnelles.
La suite de ce papier est structurée comme suit. La section 2 expose l'état de l'art des travaux en rapport avec les fonctions d'oubli. Dans la section 3, sont présentées les spécifica-tions de fonctions d'oubli, après avoir défini formellement l'âge d'une donnée et présenté un exemple de motivation. La section 4 propose des structures de données adaptées au stockage des données agrégées et présente les algorithmes pour mettre à jour le contenu de l'entrepôt après application des fonctions d'oubli. La section 5 traite de la conservation des échantil-lons. Enfin, dans la section 6, une conclusion et des perspectives associées à ce travail sont présentées.
Etat de l'art
Ce travail est à relier aux travaux sur le « vacuuming », une approche permettant de supprimer physiquement dans une base de données temporelle, les données plus anciennes qu'une certaine date seuil. Ce concept a été développé par Jensen (Jensen, 1995): lorsqu'une date particulière est spécifiée, les données antérieures à cette date sont considérées comme étant inaccessibles et doivent donc être supprimées physiquement de la base de données.
Dans (Skyt et al., 2001), une technique de réduction des données est décrite dans le cadre des bases de données multidimensionnelles. Un langage de spécifications est proposé pour réduire la granularité des données les plus anciennes en agrégeant les données anciennes à des niveaux supérieurs (plus grossiers). Ces travaux sont similaires aux nôtres. Cependant, les approches que nous proposons dans cette communication s'appliquent aux entrepôts de données et plus largement aux bases de données relationnelles, où des contraintes référentiel-les peuvent exister entre les tables, à la différence de ces travaux, qui ne s'appliquent que sur des données multidimensionnelles stockées dans un cube de données. De plus, les auteurs ne proposent pas de conserver le détail de certaines données dans une perspective d'analyse des données. Plus récemment, beaucoup de travaux sont effectués sur les flux de données, où il existe un fort besoin de résumer les données par rapport au temps. Par exemple, dans (Chen et al., 2002), le problème du calcul des agrégats temporels pour les flux de données est étu-dié, et il est suggéré de maintenir les agrégats à différents niveaux de granularité par rapport au temps : les données les plus anciennes sont agrégées à des niveaux grossiers, alors que les données récentes sont agrégées avec un niveau plus fin. Dans cette communication, nous reprenons des concepts de ces travaux mais la façon dont sont agrégées les données est spéci-fiée par l'administrateur de l'entrepôt au lieu que cela soit contrôlé par le flux.
Ce travail est à relier aussi à l'expiration des données (Garcia-Molina et al., 1998) dans le cadre des vues matérialisées (voir (Gupta et Mumick, 2005)) : l'approche consiste à détecter les données dont la suppression n'affecte pas la maintenance des vues matérialisées de l'entrepôt.
On peut citer également les travaux de (Toman, 2001) sur l'expiration des données : le problème est d'étudier l'expiration des données dans le contexte des bases de données historisées qui stockent l'historique des différents états de la base de données. Cependant, dans nos travaux, nous ne nous intéressons pas à l'historique des différents états de la base de données mais à l'historique des données stockées explicitement dans la base de données ou dans l'entrepôt de données.
On peut évoquer le principe du ramasse-miettes (Boehm, 2002) dans les programmes objet. Le ramasse-miettes constitue une forme de gestion automatique de la mémoire. Le principe est de déterminer les objets qui ne sont plus référencés et de récupérer le stockage utilisé par ces objets. Le ramasse-miettes doit analyser les liens entre les objets avant de les supprimer. Dans nos travaux aussi, lorsque les fonctions sont appliquées, des n-uplets peuvent être à archiver alors qu'ils sont référencés par d'autres n-uplets qui ne sont pas encore archivés. La section 4.3 traite un problème similaire, lorsque des contraintes référentielles (voir (Gardarin, 1999)) existent entre les tables.
Notre contribution dans cette communication est l'extension et la réalisation d'un prototype de mise en oeuvre du langage de spécifications de fonctions d'oubli présenté dans (Boly et al., 2004). Dans ce travail, les spécifications -définies par l'administrateur de la base de données -permettent d'appliquer de façon mécanique les fonctions d'oubli, qui gèrent à la fois la suppression de données à archiver (avec une prise en compte des contraintes référen-tielles) et la mise à jour des résumés conservés qui sont des agrégats et des échantillons de données détaillées.
Dans (Chaudhuri et al., 2001), les auteurs montrent qu'on peut répondre à des requêtes en utilisant seulement des échantillons et des données agrégées au lieu de la base totale. D'une part, lorsque l'on ne dispose que de données agrégées sans les données détaillées correspondantes, il est toujours possible de répondre à des requêtes d'agrégation en fournissant des réponses approximatives. D'autre part, dans la théorie de l'échantillonnage (voir (Ardilly, 1994)), il est montré qu'on peut inférer des propriétés sur un ensemble de données à partir des échantillons. De plus, dans la fouille de données comme les arbres de décisions par exemple, on peut seulement considérer des agrégats sur des données détaillées. Cela justifie notre approche de conservation de résumés de données à archiver.
Spécification de fonctions d'oubli
Notion d'âge d'une donnée
Dans le cadre de ce travail, nous considérons que chaque donnée (n-uplet d'une table dans notre étude) est associée à une date notée t s qui correspondra soit à la valeur d'un attribut de type Date explicitement associé à la donnée ou soit au timestamp du système représentant la date de dernière mise à jour de la donnée. L'âge d'une donnée calculé à la date courante notée t c est défini comme étant la différence entre les dates t c et t s . Les dates t c et t s peuvent être exprimées dans les unités de temps suivantes : la seconde (SECOND), la minute (MINUTE), l'heure (HOUR), le jour (DAY), le mois (MONTH), le trimestre ( Enfin, une relation d'ordre peut être définie entre les âges : considérant deux âges age1 et age2, on dit que age1 < age2 si et seulement si : en_secondes(age1) < en_secondes(age2), où en_secondes est une fonction qui transforme en secondes une durée (un âge) qui est exprimée dans une autre unité de temps, en utilisant les conventions présentées ci-dessus. Par exemple: 30 DAY < 3 MONTH < 1 YEAR. Les spécifications LESS THAN définissent quand les données détaillées (les n-uplets de la table COMMANDE) doivent être archivées et le résumé à conserver après application de la fonction d'oubli. Pour les données de moins de 30 jours (c'est-à-dire les données qui ont un âge inférieur à 1 mois), on garde le détail dans la table COMMANDE (c'est-à-dire les n- Enfin, la spécification « KEEP SAMPLE (1000) WHERE montant>4000 » signifie la conservation d'un échantillon aléatoire simple composé de 1000 n-uplets parmi les n-uplets archivés et qui ont un montant supérieur à 4000.
Exemple de motivation
Langage de spécifications de fonctions d'oubli
Un langage a été défini pour spécifier la fonction d'oubli associée à chaque table de la base de données. La grammaire du langage est présentée en annexe. Il comprend des caracté-ristiques supplémentaires telles que la discrétisation d'attributs numériques dans le but de les utiliser comme attributs sur lesquels peut s'effectuer une agrégation 1 . Lorsque les spécifications de fonctions d'oubli sont définies, l'application des fonctions d'oubli est automatique : des algorithmes et des structures de stockage sont nécessaires pour assurer la gestion des fonctions d'oubli. 
Gestion des fonctions d'oubli par agrégation
Structure de stockage pour les données agrégées
Mise à jour des données agrégées
Les algorithmes que nous proposons pour mettre à jour les données agrégées sont basés sur trois propriétés : (1) les mesures d'agrégation sont additives, (2) les différents cubes sont disjoints, (3) la mise à jour des données agrégées et celle des données détaillées commutent. En conséquence, tous les cubes d'une fonction d'oubli sont disjoints : aucune cellule d'un cube est incluse (fonctionnellement) dans une cellule d'un cube plus agrégé. Les différents cubes ont des contenus exclusifs ; ils stockent des données de périodes différentes par rapport au temps.
Additivité des mesures d'agrégation
Commutativité des mises à jour : les mises à jour à effectuer sur les données agrégées du fait des fonctions d'oubli sont de deux types : (1) un n-uplet de détail à archiver d'une table doit être stocké dans un cube (un seul), (2) des données déjà agrégées d'un cube peuvent être transférées dans un autre cube plus agrégé.
Puisque dans la répercussion des n-uplets de détail vers les cubes, chaque n-uplet de dé-tail n'est pris en compte que dans un seul cube, celui où il satisfait le critère 5 de la spécifica-tion correspondante, il est alors équivalent de commencer par répercuter le détail vers les cubes puis d'effectuer le transfert des données agrégées entre les cubes, ou de commencer par le transfert entre les cubes pour ensuite répercuter les n-uplets de détail. A noter que les mises à jour à effectuer du fait des fonctions d'oubli peuvent être appliquées à tout instant, soit de façon régulière (par exemple chaque jour), ou soit de façon irrégulière ou soit sporadiquement. A chaque application des fonctions d'oubli, de nouveaux n-uplets de la table peuvent satisfaire les critères des spécifications d'agrégation et doivent donc être agrégés et stockés dans les cubes correspondants. Egalement, lorsque des données agrégées d'un cube C ne vérifient plus le critère de la spécification correspondante mais vérifient le critère correspondant à un cube plus agrégé C', ces données doivent alors être transférées dans ce cube C'. A noter que l'on fait l'hypothèse que l'espace de stockage nécessaire pour contenir les données des différents cubes est limité. Cela est rendu possible par le fait que les différents cubes sont disjoints (propriété présentée ci-dessus), et par un système d'activation/désactivation pour les cellules des cubes. Lorsqu'une donnée doit passer d'un cube à un autre, la cellule qu'elle occupait ne va pas être supprimée mais désactivée. Et donc, pour répercuter une donnée vers un cube, on commencera par vérifier si elle peut occuper la place d'une cellule désactivée auquel cas, cette cellule est activée.
En conséquence de la propriété de commutativité des mises à jour, nous distinguons deux procédures qui peuvent être exécutées dans un ordre quelconque : (1) la procédure qui transfère les données agrégées entre les cubes, (2) la procédure qui répercute les n-uplets de détail archivés vers les cubes. Dans les algorithmes présentés ci-dessous, chaque cellule d'un cube est caractérisée par deux champs : la position et la mesure. La position d'une cellule est l'ensemble des valeurs des niveaux de dimensions qui déterminent cette cellule. Par exemple, la position de la cellule ('Paris', 'F', '15/05/06', 2000) est ('Paris', 'F', '15/05/06') et la mesure correspondante est égale à 2000.
Transfert des données agrégées entre les cubes
Pour chaque cube C j (j = n, n -1, n -2, …, 2) Pour chaque cube C i (i = j -1, j -2,…, 1) Pour chaque cellule c = (x 1 ,…, x n , t, m) dans C i avec age(S j ) ? age(t) > age(S i ) /* S j et S i sont les spécifications associées respectivement aux cubes C j et C i */ /* le parcours est accéléré par un index sur la dimension Temps */ soit c la cellule dans le cube C j couvrant c si ( c trouvé)
sinon /*vérifier s'il existe une cellule désactivée c trouvé dans C j qui peut remplacer Il est à noter que dans la mise à jour des cubes, il est possible que des cellules d'un cube C i ne puissent pas satisfaire le critère de la spécification correspondant au cube suivant C i+1 mais qu'elles vérifient le critère d'une spécification correspondant à un cube C p tel que p>i+1. Cette situation survient en général lorsque les fonctions d'oubli n'ont pas été appliquées pendant un certain temps : par exemple, considérant notre exemple présenté au paragraphe 3.2, lorsque les fonctions d'oubli n'ont pas été appliquées pendant cinq ans, les données du cube correspondant à la deuxième spécification (LESS THAN 3 MONTH : SUM (montant) BY Ville, Sexe, DAY) doivent être directement transférées au cube le plus agrégé. Ceci explique la présence dans l'algorithme des deux premières « boucles Pour » imbriquées.
Répercussion des n-uplets de détail vers les cubes
On note que l'ordre de traitement des cubes n'est pas important car chaque n-uplet de dé-tail ne va être stocké que dans un seul cube.
Pour chaque cube C j (j = 1, 2,…, n)  er de chaque mois), il peut arriver que des n-uplets de la table COMMANDE soient spécifiés à archiver alors que les n-uplets de FACTURE correspondants ne sont pas encore archivés (puisque pour les factures, on les garde pendant 4 mois et que pour les commandes au delà d'1 mois, elles peuvent être archivées). Nous proposons la solution suivante : suspendre l'archivage de t 1 jusqu'à ce que t 2 soit archivé et le marquer à archiver. A chaque application des fonctions d'oubli, il faut alors vérifier si les nuplets qui le référencent sont archivés, auquel cas le n-uplet devra être archivé. Et donc, dans notre exemple, on va marquer les n-uplets de la table COMMANDE spécifiés à archiver. Ils ne pourront être archivés que lorsque les n-uplets de la table FACTURE correspondants (qui les référencent) auront été archivés. A noter qu'aucune mise à jour n'est plus autorisée sur les n-uplets marqués à archiver.
Gestion de la conservation d'échantillons
Comme présenté au paragraphe 3.2, le langage de spécifications permet de conserver des échantillons de données archivées. Il s'agit de la spécification KEEP SAMPLE qui indique que l'on doit assurer la maintenance d'un échantillon aléatoire simple de taille fixe à chaque fois que des données détaillées sont archivées. Les n-uplets échantillonnés sont stockés dans une table séparée ayant la même structure que la table sur laquelle est définie la fonction d'oubli.
A chaque application des fonctions d'oubli, de nouveaux n-uplets peuvent être à archiver et doivent être pris en compte dans la mise à jour de l'échantillon. On suppose par exemple qu'un échantillon aléatoire simple de 1000 individus est conservé parmi les n-uplets à archiver, et que 100 nouveaux n-uplets sont à archiver depuis la dernière mise à jour d'oubli.
La maintenance de l'échantillon est assurée de manière incrémentale : on utilise l'algorithme réservoir de Vitter (Vitter, 1985). La propriété fondamentale de ce type d'algorithme est qu'après le traitement de chaque individu, le réservoir 7 constitue un échan-tillon aléatoire des individus visités. A noter que la taille des échantillons est fixée dans la définition de la spécification KEEP SAMPLE.
Comme indiqué à la fin de la section 2, des échantillons aléatoires sur une population peuvent être utilisés pour inférer de l'information sur cette population totale à partir des données observées dans les échantillons (voir (Ardilly, 1994)). Reprenant notre exemple, on peut estimer la somme des montants de commandes à partir de l'échantillon tiré sur les nuplets à archiver de la table COMMANDE. Pour effectuer de telles estimations, l'effectif de la population de n-uplets à échantillonner est conservé.
Conclusion et perspectives

Contexte
Notre travail s'inscrit dans le contexte d'un projet de fouille de données mis en oeuvre à Maroc Telecom et visant à mieux connaître la clientèle de la téléphonie mobile. Le niveau de consommation d'un abonné est souvent calculé à partir de la durée facturée qui s'avère insuffisante pour la plupart des cas. En effet, deux abonnés peuvent avoir la même durée d'appel pour des services différents mais sans avoir le même degré de consommation. D'où la nécessité d'introduire d'autres critères dans la détermination du niveau de consommation.
Problématique et approche de résolution préconisée
La problématique à laquelle on s'intéresse consiste à établir une échelle de mesure permettant de quantifier les niveaux de consommation afin discriminer entre les abonnés (Viertl, R. (2005)). L'approche de résolution proposée comporte trois étapes principales. Son originalité réside dans l'utilisation de la théorie des ensembles flous à travers la définition expérimentale d'une fonction d'appartenance (Mitaim, S. et B, Kosko. (2001)).
Dans une première étape, on attribue un score aux abonnés par rapport aux critères de type catégoriels (trafic, produits, services, plage horaire) caractérisant le niveau de consommation. La binarisation de chaque modalité de ces critères induit la création de plus de 60 variables indicatrices dans notre exemple qui traite 2 millions d'enregistrements. Afin de réduire la taille de ces indicatrices, l'Analyse des Correspondances Multiples (ACM) a été utilisée fournissant ainsi 10 facteurs expliquant 80,62% d'inertie totale. L'objectif de l'étape 2 est la segmentation des abonnés par produits et services afin de discriminer entre les abonnés en se basant sur le comportement d'utilisation des produits et services. Les facteurs obtenus par l'ACM ont été utilisés comme variables d'entrée des différents algorithmes non supervisés (K-means, Two Step, Réseau de Kohonen) qui ont été comparés. Le réseau de Kohonen a été plus concluant en terme d'homogénéité entre les classes. Les inerties intra classes de chaque facteur ont ensuite été utilisées dans l'étape 3 comme indicateur de la variation du niveau de consommation au sein de chaque classe. Un tel indicateur a permis d'établir une mesure du niveau de consommation tenant compte de la durée facturée en appliquant la théorie des ensembles flous (Masson, M. H. (2003)).
La fonction d'appartenance que nous proposons, dans l'étape 3, pour quantifier l'ensemble flou A = « niveau de consommation en téléphonie mobile », représente une mise à l'échelle exponentielle de la durée facturée. Cette fonction d'appartenance donnée par l'équation (1), présente non seulement l'avantage de ne pas masquer les optima locaux de la distribution de la durée facturée mais aussi l'avantage d'être paramétrée par des critères liés aux produits et services.
où d : la durée facturée, K = {C 1 , C 2 , C 3 , C 4 } : l'ensemble des classes issues du réseau de Kohonen, F j : l'ensemble des facteurs contribuant à la construction de la classe j, , x K j ? ij : l'inertie du facteur i appartenant à F j . I j : la fonction indicatrice de la classe j, p : une constante à ajuster (dans notre cas p : 0.01 suite aux différents tests numériques réalisés).

Introduction
Les méthodes conceptuelles de classification imposent une description monothétique des classes, cette contrainte forte devant a priori engendrer une perte de la qualité des partitions au sens des critères optimisés par les méthodes de partitionnement. Pour étudier ce phéno-mène nous avons comparé une méthode de classification conceptuelle appelée DIVCLUS-T qui est une méthode de classification descendante hiérarchique monothétique avec la méthode ascendante hiérarchique WARD et la méthode de partitionnement des k-means. Cette méthode conceptuelle s'applique à des données quantitatives et des données qualitatives. Cette comparaison a pu être effectuée car cette méthode conceptuelle optimise le même critère que les méthodes de WARD et des k-means. Ainsi comme WARD et les k-means, elle est basée sur la minimisation de l'inertie des classes mais à la différence de WARD et des k-means elle fournit par construction une interprétation simple et naturelle des classes. La question à laquelle nous allons chercher à répondre est la suivante : quel est le prix payé, en terme d'inertie, pour cette interprétation sous forme de règles des classes ?
Avant de présenter un peu plus en détail la méthode DIVCLUS-T et de comparer à partir de 6 bases de l'UCI les performances en terme d'inertie de DIVCLUS-T avec WARD et les k-means, nous donnons un petit exemple introductif. Il s'agit des données "protéines" de Hand et al. (1994)  
FIG. 1 -Dendrogramme obtenu avec WARD pour les données proteines
On obtient bien grâce à ce dendrogramme des classes de pays proches en terme de consommation en protéines mais si l'on souhaite avoir une interprétation facile de ces classes, une étape supplémentaire est alors nécessaire.
La méthode DIVCLUS-T a été appliquée au même jeu de données et le dendrogramme de la hiérarchie est représenté à la Figure 2 On note dans le Tableau 1 que les partitions de DIVCLUS-T sont meilleures que celles de WARD de 2 à 4 classes, puis WARD prend le dessus jusqu'à 7 classes et enfin les pourcentages sont identiques à partir de 9 classes (car les partitions sont identiques). La perte d'inertie due à la contrainte sur la description des classes est probablement compensée par le fait que DIVCLUS-T est un algorithme descendant qui trouve les partitions en peu de classes dans ses premières itérations tandis que WARD est ascendant est trouve donc ces mêmes partitions dans ses dernières itérations.
La méthode DIVCLUS-T
La méthode de classification monothetique DIVCLUS-T a d'abord été proposée dans le cadre plus général de l'Analyse des Données Symbolique (Chavent (1997)) et avait alors été implémentée sous le nom DIV dans le logiciel SODAS (Bock et Diday (2000)). Elle avait également été présentée de manière succincte dans Chavent (1998) pour des données quantitatives et dans Chavent et al. (1999) pour des données qualitatives. Dans Chavent et al. (1999) la méthode, appelée DIVOP à l'époque, était présentée dans le cadre d'une application en dermatologie conjointement avec une autre méthode divisive monothétique appelée DIVAF, basée sur l'analyse des correspondances multiples. Une méthode divisive de type monothétique utilisant le processus de Poisson a également été proposée par Pircon (2004). Récemment, une méthode de classification divisive proche de DIVCLUS-T a été implémentée dans la dernière version du logiciel SPAD sous le nom ICT pour Interactive Clustering Tree (Rakotomalala et LeNouvel (2006)).
Ici, le nom DIVCLUS-T a été choisi comme acronyme de DIVisive CLUstering Tree. Cette méthode procède comme toute méthode descendante hiérarchique par divisions successives et s'articule autour des 3 points suivants : -Les divisions s'arrêtent après étapes. On obtient donc le "haut" du dendrogramme c'est à dire les partitions de 2 à · ½ classes.
-A chaque étape cette méthode choisit de diviser la classe telle que la nouvelle partition ainsi obtenue soit d'inertie intra-classe minimum. Pour des données qualitatives, l'inertie est calculée avec la distance du ¾ sur le tableau disjonctif complet. Le critère d'inertie intra-classe étant additif cela revient à choisir la classe telle que la variation de l'inertie obtenue en la divisant soit maximum. Dans WARD on agrège à chaque étape les deux classes minimisant ce même critère de variation de l'inertie. Dans WARD et dans DIVCLUS-T on utilise donc le même critère pour indicer la hiérarchie et donc évaluer la hauteur des paliers dans le dendrogramme. Dans DIVCLUS-T le choix de la classe à diviser est nécessaire puisque l'on ne continue pas nécessairement les divisions jusqu'à l'obtention des singletons. -L'algorithme de bi-partitionnement d'une classe à Ò éléments en deux sous-classes n'éva-lue pas l'inertie intra-classe des ¾ Ò ½ ½ bi-partitions possibles pour en retenir la meilleure, mais évalue ce critère sur l'ensemble de toutes les bi-partitions induites par l'ensemble de toutes les questions binaires. On utilise donc ici l'approche monothétique des arbres de décisions et de régression (Morgan et Sonquist (1963), Breiman et al. (1984)) mais dans un cadre non supervisé. Les différences sont nombreuses. En particulier il n'y a pas de variable à expliquer et pas d'élagage. -construire une hiérarchie des modalités (en représentant chaque modalité par la descrption moyenne des objets qui la possèdent et en pondérant cet objet moyen par l'effectif de la modalité). On retient alors une partition en Ñ classes, étant suffisamment petit pour que l'on puisse parcourir toutes les partitions en deux classes de ces groupes de modalités -définir Õ ordres sur les Ñ modalités en utilisant l'ordre des modalités sur les Õ composantes principales issues de l'Analyse Factorielle des Correspondances Multiples.
Evaluation sur six bases de l'UCI
Nous avons voulu répondre à la question suivante : l'aspect rigide et simpliste du processus monothétique de DIVCLUS-T implique-t-il des partitions beaucoup moins bonnes en terme d'inertie intra-classe ? Pour donner un premier élément de réponse à cette question, nous avons comparé empiriquement le pourcentage d'inertie expliquée des partitions de 2 à 15 classes obtenues avec DIVCLUS-T, WARD et les k-means, sur 6 jeux de données de l'UCI Machine Learning repository (Hettich et al. (1998)). Ces six bases, trois quantitatives et trois qualitatives, sont décrites dans le tableau 2.
Le tableau 3 donne les résultats pour les trois bases quantitatives et les trois méthodes (colonne DIV pour DIVCLUS-T, colonne WARD pour WARD, colonne W+km pour les centres mobiles sur la partition de WARD et la colonne km pour les centres mobiles en conservant le meilleure solution de 100 initialisations au hasard). Pour les données GLASS, on note que DIVCLUS-T est parfois meilleur que WARD (pour 4 classes), parfois moins bon (pour 2, 3 classes et de 12 à 15 classes) ou encore parfois équivalent (de 5 à 11 classes). Pour les don- 
TAB. 3 -Données quantitatives
Pour les trois bases qualitatives (tableau 4) on obtient le même type de résultats. Pour les données Solar Flare et CMC, DIVCLUS-T est meilleur que WARD jusqu'à respectivement 10 et 8 classes. Pour les données Zoo, DIVCLUS-T reste toujours en dessous de WARD. C'est peut-être du aux fait que les variables sont binaires et que pour des données qualitatives, le nombre de bi-partitions évaluées à chaque étape augmente avec le nombre de modalités.
Echantillonages
Afin de mieux évaluer ces résultats nous avons créé pour chacune des trois bases quantitatives d'UCI (Glass, Pima et Abalone) 100 échantillons de taille 150 pour Glass, 500 pour 
TAB. 4 -Données qualitatives
Pima et pour Abalone. Sur chacun de ces échantillons nous avons appliqué les quatre mé-thodes de classification utilisées sur la base complète. Les premières colonnes des tableaux 5 et 6 donnent la moyenne des écarts entre la meilleure solution et la solution obtenue par la méthode. Par exemple dans la colonne DIV nous avons la moyenne des écarts entre DIV et la meilleure solution. Cette moyenne est souvent égale à zéro pour la méthode des centres mobiles (km) ce qui montre que cette méthode est presque toujours la meilleure. Ceci est aussi vrai pour la stratégie WARD+km sur la base Abalone mais sur la base Pima l'écart est un peu différent de zéro. Avec la base Glass la stratégie km est la meilleure quand le nombre de classes est inférieur à 7, la stratégie WARD+km devient la meilleure stratégie quand le nombre de classes est supérieur à 7. Les dernières colonnes donnent, en pourcentage, le nombre de fois où la solution obtenue par DIV est meilleure que la solution obtenue par WARD. Pour les bases Abalone et Glass la méthode DIV est meilleure que la méthode WARD lorsque le nombre de classes est petit, on observe cela aussi pour la base Pima mais uniquement lorsque le nombre de classes est égal à 2. Cet indicateur montre assez clairement que pour un nombre de classes assez petit la méthode DIV est plus efficace que la méthode WARD. 
Conclusion
DIVCLUS-T est une méthode monothétique qui a l'avantage par rapport aux méthodes polythétiques telles que WARD et les k-means, de donner une interprétation très simple et immédiate des classes et un arbre hiérarchique facile a lire et à comprendre par l'utilisateurs. Il est en outre normal que cette contrainte sur l'interprétation des classes, imposée dans le processus de classification, implique une perte de qualité au niveau du critère d'inertie. Il n'est donc pas surprenant que DIVCLUS-T soit généralement moins performant que WARD ou les k-means. En revanche, nous avons noté sur les exemples des 6 bases de l'UCI que le comportement de DIVCLUS-T reste tout à fait raisonnable en terme d'inertie, surtout pour les partitions en peu de classes.
Lorsqu'un utilisateur veut obtenir une partition en un nombre de classes relativement important, par exemple pour réduire le nombre d'objets, WARD et les centres mobiles sont certainement plus performants que DIVCLUS-T. Mais lorsque l'utilisateur s'intéresse aux partitions en peu de classes et à leur interprétation, alors DIVCLUS-T semble être une alternative inté-ressante aux méthodes classiques.

Préparation de données
Avec l'émergence des systèmes d'information au tournant des années 90, la récolte des données brutes a été rendue complètement indépendante de toute finalité statistique. L'analyse de ces données est un objectif qui intervient dans un second temps. La phase de préparation, dont le but est de construire à partir des données brutes une table de données pour modélisation, est donc devenue une partie critique et souvent coûteuse en temps du processus de fouille de données (Chapman et al., 2000).
L'analyste se trouve dans la situation suivante. D'une part, il dispose d'un entrepôt de données mis en place et alimenté dans un autre but que celui d'une quelconque analyse statistique. D'autre part, le propriétaire de l'entrepôt envisage d'exploiter ses données afin de compléter ses connaissances et pose une question à l'analyste. Celui-ci doit alors tourner la question en un problème d'analyse statistique, extraire de l'entrepôt les données susceptibles d'être pertinentes vis-à-vis de la question posée, les mettre sous forme d'une table, procéder à la modélisation et interpréter les résultats afin de répondre à la question initiale.
Entre autres, la préparation passe par la définition et la sélection des individus et variables constituant la table qui va servir à la modélisation. Les possibilités qui s'offrent à l'analyste lors de cette étape sont, virtuellement, limitées uniquement par son imagination. En pratique, l'extraction et la mise en forme sont soumises à deux contraintes : celle sur les ressources et celle sur le passage à l'échelle des méthodes de modélisation.
D'une part, le temps alloué à une étude est nécessairement limité, souvent très contraint. Les données brutes ne sont pas toujours accessibles facilement et rapidement. D'autre part, les algorithmes de modélisation sont rarement linéaires en le nombre de lignes et colonnes de la table. De manière plus insidieuse, l'analyste doit éviter de tomber dans le piège de la dimension, qui conduit à considérer un nombre de variables trop élevé pour le nombre d'individus à disposition. L'information portée par les individus est noyée dans un espace de représenta-tion de taille inadaptée et de nombreuses techniques de modélisation ont les pires difficultés à produire un modèle pertinent.
Nous laissons de côté la question de la définition des individus et nous intéressons à la définition des colonnes de la table. L'analyste doit capturer à l'aide d'un ensemble de variables l'information pertinente pour la question posée, sans autre aide que son intuition sur les variables "a priori susceptibles" d'expliquer le phénomène étudié et sous contrainte de ressource et de passage à l'échelle des procédés de modélisation. Dans cet article, nous nous proposons d'aider l'analyste dans cette tâche.
L'article est organisé comme suit. La section 2 présente le cadre de la sélection de variable, pour mieux circonscrire le lieu où se situe notre contribution. Notamment, nous montrons l'intérêt de disposer d'une méthode d'évaluation automatique et fiable d'une métrique. La section 3 déduit des travaux de Ferrandiz et Boullé (2006b) une telle méthode, dans le cadre de la classification supervisée. Pour la convenance du lecteur, la technique de modélisation introduite dans Ferrandiz et Boullé (2006b) est brièvement décrite dans la section 4. Enfin, la section 5 illustre notre propos par des expérimentations sur un problème de préparation de profils de consommation en téléphonie fixe dans un contexte supervisé. données brutes, une variable X : I ? X est construite en choisissant un espace de représen-tation X et en définissant un procédé de mesure X projetant chaque individu dans X. Une fois la variable X : I ? X obtenue, se pose la question de son intérêt relativement à la question étudiée. On entre alors dans le cadre de la sélection de variables.
De nombreux articles permettent de cerner les pratiques de la sélection, notamment Blum et Langley (1997), Kohavi et John (1997) et Guyon et Elisseeff (2003). Dans l'article Kohavi et John (1997), une distinction est opérée entre approche enveloppe (de l'anglais wrapper) et approche filtre (de l'anglais filter).
L'approche enveloppe consiste à évaluer l'impact d'une modification de l'ensemble de variables sur la performance d'un modèle. Une technique de modélisation étant spécifiée, elle est appliquée à différents ensembles de variables et l'ensemble de variables conduisant au modèle le plus performant est conservé. Chaque évaluation nécessite l'ajustement d'un modèle, ce qui se révèle coûteux en temps. Cette approche, en faisant intervenir le modèle dans l'évaluation, est plus adaptée à la phase modélisation qu'à la phase de préparation d'une analyse. L'approche filtre univariée de la sélection de variables repose sur l'emploi d'une méthode d'évaluation de l'intérêt d'une variable quelconque X : I ? X. Non seulement en pratique mais aussi sur le plan formel, l'espace de représentation X est souvent muni d'une métrique (ou : distance) ? : X × X ? R + . C'est notamment le cas lorsque X est multidimensionnelle numérique, mais aussi lorsque X est séquentielle.
L'information contenue dans une variable peut donc être observée à travers le prisme d'une matrice de distance sur les individus. Une telle matrice est obtenue en calculant les distances au sens de ? entre tout couple d'individus. Dans ce cas, l'évaluation de la métrique induit une évaluation de la pertinence de la variable X.
Evaluation probabiliste d'une métrique
Nous proposons ici un protocole d'évaluation de la pertinence d'une métrique exploitant des travaux antérieurs. Le contexte est celui de la classification supervisée : une variable cible catégorielle unidimensionnelle est à expliquer.
Dans le cas d'une variable statique numérique unidimensionnelle, Boullé (2006) aborde la question de l'évaluation de la pertinence vis-à-vis d'une variable cible catégorielle comme un problème de maximisation de la probabilité a posteriori. Les modèles considérés sont les partitions de la variable numérique en intervalles. Une approche bayésienne permet de définir
FIG. 1 -Exemples de partitions de Voronoi pour la métrique euclidienne.
un critère s'interprétant comme la probabilité que le modèle explique la variable cible catégo-rielle. La sélection du modèle le plus probable conduit à une méthode de discrétisation d'une variable statique numérique. La probabilité du modèle le plus probable s'utilise alors comme un indicateur de pertinence de la variable descriptive relativement à la variable cible.
Dans Ferrandiz et Boullé (2006b), l'approche est adaptée afin de traiter le cas où l'espace de représentation est muni d'une métrique. A l'aide de la métrique, une partition de Voronoi est associée à tout sous-ensemble d'individu (c.f. fig.1 pour des exemples). Le partitionnement d'une variable en intervalles est ainsi généralisé en un partitionnement de l'espace en cellules. L'ensemble des partitions obtenues constitue l'ensemble des modèles. La probabilité qu'un modèle explique la variable cible catégorielle est explicitée et la sélection du modèle le plus probable conduit à une méthode de sélection d'instances. Là encore, la probabilité associée au modèle sélectionné constitue un indicateur supervisé de pertinence de la métrique. La méthode est évaluée dans Ferrandiz et Boullé (2006a) en tant que méthode de sélection d'instances pour la classification par le plus proche voisin.
Soyons plus formels. L'ensemble des individus est noté I. Pour une métrique ?, tout sousensemble H de I définit une partition de I, dite de Voronoi : chaque individu est associé au plus proche élément de H au sens de ?. Notons H ? (I) l'ensemble des partitions de Voronoi.
Si on note Y la variable cible catégorielle, nous disposons d'un critère c ?,Y : H ? (I) ? R qui associe à chaque partition la probabilité que cette partition explique la cible Y . Nous proposons alors d'évaluer la métrique ? par la probabilité du modèle le plus probable :
La fonction c * fournit une évaluation supervisée de la qualité de la métrique ? et permet ainsi de comparer différentes métriques et, à travers elles, différents espaces de représentation et différentes variables. Pour une métrique ? donnée, on applique un algorithme d'optimisation combinatoire et on attribue la valeur rencontrée optimale du critère c ?,Y . Le critère c ?,Y et l'heuristique d'optimisation proposés dans Ferrandiz et Boullé (2006a) et Ferrandiz et Boullé (2006b sont décrits plus en détail dans la prochaine section.
Le critère c ?,Y est non paramétrique et régularisé. Il quantifie le compromis entre le nombre de groupes de la partition et la discrimination de la cible, ce qui correspond à un compromis entre complexité du modèle et ajustement du modèle aux données de l'échantillon. La régu-larisation est un moyen d'endiguer le phénomène de sur-apprentissage et d'assurer ainsi la fiabilité de la décision. Etant non paramétrique, l'évaluation se passe de validation ou de validation croisée. On dispose ainsi de plus d'individus pour ajuster le modèle, ce qui augmente sa qualité.
Afin de travailler avec un indicateur normalisé, nous considérons la transformation suivante de c * :
où c 0 (?) est la valeur du critère c ?,Y pour le modèle constitué par un seul groupe. D'après les travaux de Shannon (1948), l'opposé du logarithme d'une probabilité s'interprète comme une longueur de codage. L'indicateur g * (?) s'interprète alors comme un gain de compression. Il est supérieur à 0 et inférieur à 1. Si g * (?) = 0, la métrique ? n'apporte aucune information sur la variable cible. Plus la valeur de g * (?) est proche de 1, plus les classes cibles sont séparées, et plus la métrique ? est pertinente.
Sélection d'instances : critère et algorithme
L'évaluation de la qualité d'une métrique introduite ci-dessus repose sur la recherche de la meilleure partition de Voronoi induite par un sous-ensemble de l'échantillon. Pour la convenance du lecteur, nous décrivons dans cette section le critère et l'heuristique d'optimisation utilisés, déjà proposés et étudiés dans Ferrandiz et Boullé (2006a)  et Ferrandiz et Boullé (2006b).
Evaluation bayésienne d'une partition
Posons les notations. Soit I un ensemble de N individus. Soit Y : I ? L une variable cible catégorielle, L étant un alphabet de taille J. Soit X : I ? X une variable descriptive, l'espace de représentation X étant muni d'une métrique ?.
Soit H un ensemble de K individus. La partition de Voronoi V (H) = (V (k)) k?H associée à H est définie par :
Pour k ? H, la cellule de Voronoi V (k) contient les individus i dont k est l'élément de H le plus proche, relativement à ?. L'élément k est appelé prototype de la cellule V (k). La fig.1 donne des exemples de telles partitions. Si les éléments de H sont indexés de 1 à K, N k (1 ? k ? K) est le nombre d'individus dans la cellule du k eme élément de H et N kj désigne le nombre de tels individus de la j
L'approche adoptée dans Ferrandiz et Boullé (2006b) conduit à évaluer H par :
Le premier terme quantifie la probabilité d'apparition du nombre K de cellules de V (H), le second terme quantifie la probabilité d'apparition des K prototypes de H, et les derniers termes quantifient cellule par cellule la probabilité d'apparition de la variable cible. Ils résultent de l'adoption de l'approche bayésienne de l'évaluation : la somme des deux premiers termes correspond à l'a priori sur les modèles et la somme des deux derniers termes à la vraisemblance de la cible. La dernière somme, d'après la formule de Stirling log x! ? x log x ? x + O(log x), se comporte asymptotiquement comme N fois l'entropie conditionnelle de la variable cible Y en connaissance de la partition :
Intuitivement, le critère quantifie la discrimination des distributions par un terme entropique et la pondère par un coût structurel mesurant la complexité de la partition. Pour cela, il prend en compte diverses caractéristiques, comme le nombre de groupes, la répartition des instances dans les groupes (i.e les coefficients N k ), la répartition des instances dans les classes (i.e. les coefficients N kj ).
Heuristique d'optimisation
Nous disposons d'un critère d'évaluation des ensembles de prototypes H. Il reste à proposer un algorithme d'optimisation. Nous reprenons celui introduit dans Ferrandiz et Boullé (2006a), qui consiste à encapsuler une optimisation gloutonne d'un ensemble de prototypes dans une méta-heuristique. La complexité algorithmique de l'optimisation gloutonne est ré-duite en exploitant les propriétés du critère et des partitions de Voronoi. La méta-heuristique permet de remettre en question le résultat de la recherche gloutonne afin d'optimiser encore un peu plus le critère.
L'heuristique gloutonne Glouton(H) s'applique à tout ensemble H de K prototypes. Elle comporte K étapes. A chaque étape, tout sous-ensemble obtenu par élimination d'un prototype est évalué. Parmi ceux-ci, celui minimisant la valeur du critère est déclaré vainqueur. Cette étape gloutonne est itérée et appliquée à chaque vainqueur successif, jusqu'à évaluation d'un singleton. Le meilleur sous-ensemble rencontré est retourné.
Cette méthode considère O(K 2 ) sous-ensembles et chaque évaluation nécessite la recherche du plus proche prototype pour chaque instance. Une implantation directe de Glouton(H) possède une complexité en O(N K 3 ). L'exploitation des propriétés du critère et des partitions de Voronoi conduit à une implantation de complexité un O(N K log K) nécessitant un espace mémoire en O(N K).
L'heuristique gloutonne effectue rapidement un grand nombre d'évaluations. Il est naturel d'envisager une application répétée mais limitée de cet algorithme. Pour cela, la méta-heuristique de recherche à voisinage variable est adoptée (Hansen et Mladenovic (2001)). Elle consiste à appliquer l'heuristique de base (i.e. l'algorithme Glouton) à un modèle proche de la solution considérée. Si la nouvelle solution n'est pas meilleure, on considère un voisinage plus grand. Sinon, la méta-heuristique repart de la nouvelle meilleure solution avec une taille de voisinage minimale. Ce procédé est contrôlé par une taille maximale du voisinage à explorer.
Nous illustrons les apports de notre méthode par des expérimentations sur des données de consommation en téléphonie fixe. C'est un problème de classification de profils de consommation suivant 4 classes cibles A, B, C et D. La distribution des classes cibles sur l'échantillon est uniforme. On dispose de 168 variables descriptives numériques, chacune mesurant la consommation téléphonique sur une tranche horaire de la semaine. Nous répartissons uniformément les 3516 individus de l'échantillon entre un ensemble d'apprentissage (75% des individus) et un ensemble de test (25% des individus), de manière stratifiée (i.e. en respectant la distribution a priori des classes cibles).
Evaluation d'une variable séquentielle
La méthode d'évaluation d'une métrique proposée dans cet article, en plus de quantifier la pertinence d'une métrique, fournit un support explicatif : la partition la plus probable. Ainsi, on dispose d'une distribution des classes cibles et d'un prototype pour chaque groupe, ce qui autorise une explication du résultat purement numérique.
Nous illustrons cet aspect en appliquant notre méthode à la variable séquentielle constituée par les 168 variables descriptives du problème de classification de profils de consommation, l'espace de représentation étant muni de la métrique L 1 . Autrement dit, chaque individu se voit associer une suite de 168 mesures de consommation et la distance entre deux profils est mesurée à l'aide de la métrique L 1 . L'évaluation de la métrique fournit un gain de compression de 0.051, ce qui est très faible et caractérise un fort mélange des classes cibles. Mais il n'est pas nul et la méthode partitionne les individus en 7 groupes. Les distributions relatives à chacun des groupes sont représentées par des histogrammes groupés sur cette la fig.2. En calculant la valeur moyenne de chacune des 168 variables dans chaque groupe, on obtient 7 profils de consommation caractéristiques. Trois de ces profils sont reportés sur la fig.2, ainsi que le profil moyen de consommation (i.e. celui calculé sur tout l'ensemble d'apprentissage).
Le résultat étant visualisable, il est facilement interprétable. Par exemple, on voit que les individus du groupe 7 sont en grand nombre (35% de l'échantillon d'apprentissage), qu'ils ont une consommation moyenne plus élevée que la moyenne globale, et que ce comportement est majoritairement caractéristique de la classe A (la répartition dans les classes cibles A, B, C, D est (41%, 26%, 15%, 17%)). Le groupe 1 est quant à lui plus discriminant (la répartition dans les classes cibles est (16%, 20%, 57%, 6%)) avec un profil de consommation atypique (pics de consommation élevés), mais est de taille réduite (4% des individus). Le groupe 4 discrimine lui aussi la classe C, moins fortement tout de même que le groupe 1, et se différentie par une consommation moyenne très faible.
Afin de vérifier de visu la fiabilité du découpage effectué, nous calculons les distributions en test (reportées sur la fig.2). Bien que l'ensemble de test soit trois fois plus petit que l'ensemble d'apprentissage, on constate que la distribution des individus dans les groupes est stable ((4%, 5%, 6%, 11%, 19%, 21%, 35%) en apprentissage et (3%, 4%, 6%, 12%, 17%, 24%, 34%) en test), que les classes majoritaires dans chaque groupe en test sont les mêmes que celles observées en apprentissage, etc.
Notre méthode n'est pas la première à fournir de telles informations. Ainsi, l'analyse discriminante, linéaire ou quadratique (Hastie et al., 2001) 
FIG. 3 -Consommation moyenne sur l'ensemble d'apprentissage et consommations moyennes dans chacun des groupes 1, 4 et 7. Les individus du groupe 1 correspondent à de très fortes consommations, avec des pics très marqués. Ceux du groupe 7 correspondent aux faibles consommations.
des prototypes, comme les méthodes de quantification (Kohonen, 2001), conduisent à de telles visualisations. Mais c'est la seule à évaluer strictement les informations qui sont visualisées : le calcul du gain de compression prend en compte la répartition des individus dans les groupes, les répartitions des individus dans les classes cibles groupe par groupe. La visualisation n'en est que plus adaptée.
L'analyse discriminante suppose les individus d'une classe toutes générées par une même gaussienne. Les modèles considérés sont tous de même capacité, ce qui se traduit en pratique par un nombre de groupes égal au nombre J de classes. L'exemple étudié ici montre que contraindre la capacité revient à limiter la richesse de l'information extraite. De toute façon, les paramètres des J gaussiennes sont ajustés en maximisant la vraisemblance complète et la vraisemblance ne tient pas compte des différences de capacité : on ne peut utiliser la mesure de vraisemblance pour comparer un modèle d'analyse discriminante linéaire avec un modèle d'analyse discriminante quadratique, et encore moins avec un modèle d'analyse de mélange (qui autorise un nombre quelconque de gaussiennes par classe, c.f. Hastie et al. (2001)). On est ramené à appliquer un second critère. En pratique, c'est le risque empirique qu'on utilise, avec ses limites.
Les techniques de quantification, hautement paramétriques, nécessitent entre autre de fixer le nombre de prototypes. Le choix d'un "bon" nombre de prototypes repose donc sur un critère alternatif. En pratique, là encore, on estime le risque empirique. L'idée sous-jacente aux techniques de quantification étant de repousser les prototypes en cas de mauvais étiquetage et de les rapprocher dans le cas contraire, la présence de prototypes "morts" à la fin de l'optimisation constitue de plus un effet secondaire peu désirable. En effet, de nombreux prototypes sont dé-placés au point de ne plus être sollicités par la suite. En terme de partition de Voronoi associée, cela signifie que plusieurs cellules finales ne contiennent aucun élément de l'ensemble d'apprentissage. Le résultat perd de sa pertinence et la visualisation associée est rendue caduque.
S'il est usuel de mettre de côté un ensemble d'individus, dit de validation, pour ajuster certains paramètres ou contrôler la fiabilité de l'estimation, c'est inutile lorsqu'on utilise notre méthode. Elle est en effet non paramétrique et la fiabilité est intrinsèquement assurée par l'usage d'un critère régularisé. Tous les individus servent à la prise décision, ce qui profite nécessairement à la qualité de celle-ci.
Comparaison et sélection de variables séquentielles
La définition d'une variable séquentielle nécessite la définition d'un espace de représenta-tion. Celui-ci est souvent muni d'une métrique. Nous avons proposé dans ce qui précède une méthode d'évaluation supervisée de la pertinence d'une métrique. A travers le prisme d'une métrique, nous sommes donc en mesure d'évaluer la pertinence d'une variable séquentielle par le gain de compression que mesure notre méthode.
Nous illustrons son utilité par un problème de sélection de variables séquentielles. Pour cela, nous considérons les 24 variables séquentielles définies par les tranches horaires du problème de classification de profils de consommation. Chaque variable est composée de 7 mesures, correspondant à la consommation sur chaque jour de la semaine pour une tranche horaire fixée. Pour chacune, nous munissons l'espace de représentation de la métrique L 1 et appliquons notre méthode d'évaluation sur l'ensemble d'apprentissage. Le gain de compression et le taux de bonne prédiction en test sont reportés pour comparaison sur la fig.4. Utilisé pour de la sélection, le gain de compression conduit à choisir la tranche horaire 14, ou toute variable dont le gain de compression mesuré dépasse un certain seuil fixé a priori par l'analyste. A l'opposé, le gain de compression est nul pour les tranches horaires de fin de nuit. Ceci signifie qu'un seul groupe est constitué et que les classes cibles sont mélangées. Considé-rées isolément, ces variables ne sont d'aucun intérêt. C'est la présence d'une régularisation, qui consiste à contrôler la discrimination opérée par la capacité de la partition, couplée avec le fait que les partitions considérées fournissent des capacités allant d'un minimum (un seul groupe) à un maximum (autant de groupes que d'instances), qui rend possible une telle conclusion.
FIG. 4 -
Sélection d'une métrique
Dans l'expérience précédente, nous avons utilisé la métrique L 1 pour mesurer la distance séparant deux profils. Nous reproduisons cette expérience et considérons deux métriques supplémentaires : la métrique euclidienne et un noyau gaussien (définissant une métrique euclidienne dans un espace implicite). Les courbes de gain de compression sont reportées sur la fig.5.
Certains comportements des courbes sont analogues. Par exemple, quelle que soit la mé-trique ici considérée, les tranches horaires de fin de nuit sont déclarées non pertinentes relativement à la cible. Mais c'est l'utilisation de la métrique L 1 qui conduit aux meilleurs gains de compression, quasiment pour toutes les tranches horaires. Pour ces variables séquentielles et cette cible, l'analyste est conduit automatiquement et de manière fiable à choisir cette métrique au détriment des deux autres. S'il dispose de temps, il peut même s'aider de la visualisation proposée précédemment pour expliquer les différences de comportement sur chaque tranche horaire. 
FIG. 5 -
Conclusion
En fouille de données, dès lors que la récolte des données n'est pas orientée dans le sens de l'analyse, un travail de préparation est à mener. Une table doit d'abord être construite pour ensuite procéder à une modélisation statistique qui réponde à la question posée par le propriétaire des données. A priori, de nombreuses variables sont susceptibles d'expliquer le phénomène étudié et il s'agit d'inclure dans la table les plus pertinentes d'entre elles.
L'approche adoptée en préparation est l'approche filtre univariée, indépendante d'un modèle particulier et plus à même de faire face à un nombre élevé de variables. Dans ce cadre, la qualité de la méthode d'évaluation utilisée pour juger de l'intérêt d'une variable est cruciale. En exploitant des travaux antérieurs, nous avons ici proposé une méthode automatique et fiable d'évaluation d'une métrique, dans le cas de la classification supervisée. Nous avons illustré son apport sur un problème réel de classification de profils de consommation téléphonique.
Cet exemple d'application montre l'apport de notre méthode en préparation de données. L'analyste dispose grâce à elle d'un outil pour mener à bien la sélection filtre univariée des variables qu'il supposent a priori pertinentes. Cet outil permet d'évaluer la pertinence a posteriori (après observation des données) de variables dont l'espace de représentation est muni d'une métrique. Cette évaluation est automatique, fiable et se passe d'un ensemble de validation. Plus généralement, l'outil permet de sélectionner la métrique la plus adaptée.
Références Blum, A. et P. Langley (1997). Selection of relevant features and examples in machine learning.
Artificial intelligence 97(1-2), 245-271.

Introduction
Dans le cadre de la modélisation des étapes du raisonnement à partir de cas pour la réalisation d'un outil logiciel qui fera office d'un tuteur d'aide pour l'évitement des circonstances de pollution domestique exprimées dans des plaintes (Z. Bellia, 2004), nous souhaitons améliorer la méthode de tri basée sur la contiguïté des termes de la requête dans le texte d'un document source. À l'évidence, il est dans l'intérêt de l'usager du système de retrouver les cas les plus pertinents parmi les plaintes déjà traitées. Généralement, lorsqu'un utilisateur formule une requête au système, il compte retrouver les documents dont la signification du contenu se rapproche le plus de sa demande. Par exemple, pour la résolution d'une nouvelle plainte comportant le terme «couverture », il sera judicieux de retrouver les anciens cas de la mémoire archive relatifs non seulement au terme « couverture » lui-même, mais aussi aux «couettes », aux « duvets », aux « édredons », etc. Les documents contenant ces termes sont sans doute pertinents pour la plainte courante, néanmoins, ils ne seront pas sélectionnés par un modèle de recherche basé uniquement sur les occurrences directes des termes. Une solution incontournable est l'utilisation d'un réseau sémantique pour gérer le vocabulaire très variés qui peut être employé dans les plaintes. Dans l'étape de l'« élaboration » des cas en RàPC nous avons opté pour un modèle semi-structuré pour la constitution de la base. L'interface usager de notre système propose une série d'indexes sous forme de questions, dont les réponses apportent de l'information pour la description du problème. Nous avons proposé de traduire ces indexes sous forme de modèles de balise dans un document XML, et la partie renseignée par l'utilisateur représente pour nous le contenu des balises.
Après avoir présenté les outils à l'origine de notre approche, mesure de similarité conceptuelle et modèle de proximité, nous introduisons notre approche prenant en compte les deux aspects. Le développement d'un exemple montre l'intérêt de notre méthode.
Les outils
Dans ce chapitre, nous rappelons brièvement la notion de mesure conceptuelle pour la gestion de la sémantique ainsi que la notion de cooccurrence floue entre les termes. Pour formaliser les relations entre les termes nous les rattachons aux concepts de WordNet (C. Fellbaum, 1998).
Aspect sémantique
Zarga et Salotti (H. Zargayouna et S. Salotti, 2004) définissent une métrique conceptuelle inspirée des travaux de Wu et Palmer (Z. Wu et M. Palmer, 1994). Elles privilégient toujours les liens père-fils par rapport aux autres liens de voisinage en adaptant la mesure de Wu et Palmer qui pénalise dans certains cas les fils d'un concept par rapport à ses frères. Elles introduisent la fonction Spec pénalisant ainsi les concepts qui ne sont pas de la même lignée. Nous illustrons cela dans l'exemple développé dans la figure 1.
Tel que prof(C 1 ) est le nombre d'arcs entre la racine de la hiérarchie et le concept C 1 en passant par le plus petit généralisant (PPG) du couple C 1 , C 2 . La valeur de prof b (PPG) correspond au nombre maximum d'arcs qui séparent le PPG du concept Bottom.
Mesure de proximité
Cette mesure entre termes doit être mise en contexte lorsque l'on traite de documents. Le modèle vectoriel introduit par Salton (G. Salton et C. Buckley, 1998) exclut toute notion de position et de distance entre les mots. De surcroit, le modèle de Salton est mieux adapté à la codification des textes longs qu'à la codification des textes courts (A. Singhal, 1996). Compte tenu de la nature hétérogène des textes en notre possession, il est primordial d'élargir notre réflexion aux modèles de représentation adaptés à la nature de notre ressource (hétérogène). Nous avons étudié à cet effet le modèle de recherche basé sur la proximité des termes et inspiré du modèle booléen classique. L'approche de Mercier (A. Mercier et M. Beigbeder, 2005) repose sur l'hypothèse que plus les occurrences des termes d'une requête se trouvent proches dans un document de la base plus ce document est pertinent par rapport à cette requête.
RNTI -X -Z. Heddadji et al. 
Ainsi, la similarité est obtenue en normalisant l'ensemble des scores par la longueur du document.
Pertinence sémantique locale
Nous apportons une extension au modèle existant en le combinant avec la mesure de similarité conceptuelle de Zarga et Salotti de la manière suivante: RNTI -X -Par Syno(t) nous indiquons l'ensemble des termes proches sémantiquement de t. Un seuil de similarité est nécessaire pour caractériser l'ensemble de ses éléments. Nous fixons un seuil de similarité pour la valeur de Sim(t i ,t) qui correspond au degré de similarité entre t et le concept auquel est rattachée la balise où il apparaît. Dans l'exemple suivant, nous comparons les deux approches. Les scores de pertinence sont calculés aux différentes positions pouvant être prises dans un intervalle d'occurrence précis de taille fixe k=10 (par exemple). Prenons en tant que exemple, la forme filtrée lemmatisée de la balise <state> (balise décrivant l'état du logement) d'une requête combinant les mots-clés suivants: r={humidity, rampart, salon}. Nous pouvons imaginer un passage de la plainte initiale exprimée de la manière suivante: «There is humidity on the rampart in the salon». Supposons qu'il existe un dossier stocké en mémoire dont la partie problème contient cet extrait:«Many moistures gleamed all on the wall of my bed-room». d={moisture, wall, bed-room}. Les termes de la requête courante, a priori, n'appartiennent pas au texte du document source, néanmoins le sens de ces deux passages est résolument le même. Le Tableau  Pour l'application de cet exemple, nous nous sommes assurés que les similarités autorisées pour l'augmentation de la pertinence locale soient supérieures au degré de similarité entre le terme appartenant au document source et le terme associé à la balise où apparaît l'extrait du document source. Dans notre cas il s'agit d'un extrait de la balise <State>. Sim ZS (moisture,state)=0.62<0.70, Sim ZS (wall,state)=0<0.78 et Sim ZS (bedroom,state)=0<0.52. La pertinence sémantique de la requête par rapport au document source dans l'exemple correspond à Sim(r,d)=0.46, alors que ce score est nul si on applique la méthode directe. Ces résultats montrent que l'extension que nous proposons augmente de manière significative la qualité des résultats. Ceci tend à prouver que l'usage des ressources sémantiques est très utile dans la phase de recherche que nous souhaitons fine.

Introduction
La reconnaissance et l'extraction d'entités nommées cherche à localiser et à classer les éléments atomiques d'un texte en catégories prédéfinies telles que noms de personnes, organisations, localisation, dates, quantités, valeurs monétaires, pourcentages etc. Ce domaine de recherche est très actif, bien que des outils commerciaux existent déjà. Citons, par exemple, REX 1 (Rosette R Entity Extractor), Inxight SmartDiscovery 2 , Convera-RetrievalWare Entity Extraction 3 and Xerox-Research Entity Extraction systemDu coté recherche, certains systèmes de reconnaissance d'entités nommées utilisent des techniques à base de grammaires linguistiques, d'autres des modèles statistiques. Les systèmes à base de grammaires construits à la main obtiennent souvent de meilleurs résultats au prix d'un travail très important par des linguistes chevronnés. Par ailleurs, les systèmes à base de modèles statistiques demandent beaucoup de données d'apprentissage annotées, mais sont plus faciles à porter vers d'autres langages, domaines ou genres de textes.
Nous proposons une approche dans laquelle, à partir d'une liste connue d'entités, le système génère automatiquement des schémas de phrases pouvant contenir ces entités. Une étape d'apprentissages, à partir d'un très petit nombre de documents permet de ne garder que les schémas les plus pertinents. Cette approche s'inspire de celle utilisée pour l'extraction de données dans des documents semi-structurés tels que des pages Web (wrappers), basée sur la génération de programmes d'extraction à partir d'un petit nombre d'exemples (Kushmerick (2000); Adelberg (1998); Irmak et Suel (2006); Lerman et al. (2003); Liu et al. (2003)). Au lieu de s'appuyer sur les balises HTML des documents, nos règles s'appuient sur les syntagmes du langage (balises linguistiques). Cette approche ne nécessite pas de ressources linguistiques particulières (McNamee et Mayfield (2002); Cucerzan et Yarowsky (1999)) ni de larges collections de tests.
Nous avons testé cette approche sur le rapport d'activité de l'Inria. Il s'agit d'identifier, dans le rapport d'activité annuel, et plus particulièrement dans les sections décrivant les contrats de recherche et les relations internationales, les organismes cités avec lesquels les équipes de recherche coopèrent. Dans ce contexte, identifier le plus possible de ces organismes ("rappel") est plus important qu'une précision élevée puisque que la liste des entités extraite peut être revue manuellement, même si cette tâche de vérification s'avère très lourde pratiquement. D'autre part, ce genre de rapport étant répétitif d'une année sur l'autre, et les partenaires évoluant lentement, il est intéressant que le processus d'extraction puisse s'affiner avec le temps.
Domaine applicatif
Le rapport d'activité scientifique annuel de l'Inria est composé d'environ 180 rapports en anglais décrivant différents aspects de l'activité scientifique des équipe de recherche. Depuis quelques années l'Inria est intéressé à exploiter cette source riche d'information, disponible en XML. Nous nous intéressons ici à identifier les nombreux partenaires des équipes, en exploitant les sections spécifiques décrivant les collaborations et les contrats.
Ce travail se heurte à plusieurs difficultés inhérentes à la collection. Le style de ces sections est très peu homogène, parfois télégraphique ou peu rédigé, avec une représentation des noms de partenaires souvent approximative, voir avec des orthographes erronées. Ces noms eux-mêmes peuvent être très divers : sigles plus ou moins développés (FT R&D), localisations intégrées au nom (Inria Rocquencourt), noms d'organismes (EDF, MIT), de laboratoires (LRI, LSR), de réseaux ou de noms de projets souvent confondus avec des noms communs (Oasis, PARIS, Ondes). Le travail manuel d'annotation des documents, utilisés pour la phase d'apprentissage et pour l'évaluation, est une activité très coûteuses en temps et intrinsèquement difficile. Il serait exclu d'extraire les noms de ces organismes à la main dans toute la collection (et chaque année).
Dans un premier temps, nous avons essayé d'utiliser un outil existant de bonne réputation, à savoir ANNIE, un des composants du système GATE (Cunningham et al. (2002)) développé par l'université de Sheffield (UK). Ce premier essai a été très décevant. Le taux de rappel était seulement de 0,23 si nous cherchions la liste des organismes, et même de 0,17 si nous cherchions toutes les occurrences des noms. Une des raisons est sans doute le style elliptique de cette partie du rapport, très différent du type de collections standards sur lesquels ANNIE est généralement validé (journaux, etc.). Nous avons donc décidé de développer une approche différente, partant des données réelles plutôt que de collections standards pour l'apprentissage.
Méthode utilisée
Les documents sont au préalable annotés à l'aide de ANNIE qui détecte la fonction grammaticale (nom, verbe etc.) des mots utilisés. Ce sont ces fonction grammaticales (syntagmes) qui seront utilisés pour la construction de schémas tel que nous en parlerons plus loin.
De façon standard, nous travaillons sur un ensemble réduit de rapports (collection test), utilisés dans la phase d'apprentissage et pour l'évaluation des résultats. Dans ces rapports, les noms des organismes sont identifiés et annotés à la main. Cet ensemble est divisé en trois sous-ensembles : le premier sous-ensemble, noté L, sert à construire une   Ensuite, en partant de l'ensemble A annoté avec les seules entités de la liste L, nous appliquons un à un les schémas classés précédemment et nous extrayons de nouveaux organismes qui sont ajoutés à L à l'itération suivante. À chaque étape, la précision et le rappel sont évalués et l'algorithme s'arrête lorsque le rappel atteint un certain seuil, ou que la précision devient inférieure à un autre seuil. En principe, le rappel va augmenter à chaque étape puisque les schémas les plus performants sont ajoutés en premier. La précision, initialement égale à 1, puisque calculée à partir des seuls organismes de L présents dans A, ne peut que se dégrader.
À cette étape du processus, nous avons donc sélectionné des schémas d'extraction dont nous avons pu contrôler la performance sur l'ensemble A.
Dans la phase d'évaluation, nous appliquons les schémas précédemment sélectionnés pour extraire les noms d'organisation de l'ensemble de test B . Comme B a été lui aussi été annoté au préalable, nous pouvons calculer la précision et le rappel pour valider notre approche.
Expériences et résultats
Comme il est extrêmement fastidieux et difficile d'identifier les organismes cités dans les documents, nous ne voulions pas avoir à le faire pour plus de 20 documents. Afin de tester différents paramètres de l'algorithme, nous avons effectué des permutations aléatoires des documents dans les ensembles L, A et B afin de créer 10 jeux de test différents. Le tableau 2 présente les résultats pour 3 de ces jeux de test, pour des seuils d'apprentissage de 0,6 pour la précision et le rappel. Nous nous intéressons à la fois à l'identification des noms d'organismes (comptage simple), et à l'identification des occurrences de ces noms (comptage multiple).
On peut tout d'abord remarquer que le rappel de départ pour l'ensemble A est faible (0,17 et 0,21), ce qui indique une grande diversité de partenaires selon les différentes équipes. Bien que la précision de départ devrait être égale à 1 nous voyons que ce n'est pas tout à fait le cas. En effet par absence de normalisation si L contient "FT" et un document de A ou B contient "FT R&D" identifié comme nom d'organisme, FT sera identifié mais considéré comme non valide pour la calcul de la précision et du rappel.
Le rappel à la fin de la période d'apprentissage a été multiplié par plus de 2 en moyenne pour le comptage simple, mais reste malgré tout assez faible. Il faut rappeler que l'algorithme d'apprentissage s'arrête lorsque le rappel est plus grand que 0,6 ou la précision inférieure à 0,6 (pour le comptage multiple). On ne peut donc pas espérer des valeurs très élevées à la fin de l'apprentissage, en utilisant des schémas génériques et calculés automatiquement. Nous avons évalués les résultats sur 5 jeux de test identiques pour 3 couples de seuils différents. Le tableau 3 montre les résultats moyens sur les 5 jeux de test.
On peut voir que les seuils ont une influence non seulement sur la précision finale, mais aussi sur le nombre de schémas validés par apprentissage. Ce nombre est plus petit si la pré-cision demandée est élevée, ce qui avantage le temps d'extraction. En contrepartie le nombre d'entités extraites est inférieur ce qui est inconvénient pour notre application.
Il se trouve que la partie "contrats" contient plus facilement des partenaires industriels et la partie "collaboration" plus souvent des partenaires académiques. Nous avons donc faits des expériences en effectuant un apprentissage séparément sur chacun de ces groupes. Les résultats sont meilleurs pour la partie "collaborations", les noms d'universités étant plus facile à identifier, mais contrairement à notre attente, les résultats sont moins bons quand on traite les parties "contrats" et "collaborations" de façon séparée plutôt qu'ensemble.
Finalement, nous avons appliqué les schémas sélectionnés à l'ensemble des 180 rapports. Selon les expérimentations (non reportées ici, faute de place), 1500 à 3000 noms ont été extraits. Un essai de validation d'une liste de 1500 noms a montré la difficulté d'une telle tâche.
Conclusion
Nous avons présenté une méthode pour extraire les noms d'organismes dans des parties de documents assez peu rédigées. Notre approche s'inspire des méthodes inductives des extracteurs pour des documents semi-structurés, et ne requière pas d'importantes ressources linguistiques ni de mise au point manuelle. Les résultats, bien qu'un peu décevants, montrent qu'il est possible de découvrir un grand nombre d'organismes non connus à l'avance.
D'une année sur l'autre, il y a une certaine continuité dans les partenaires avec lesquels les équipes Inria travaillent. Il est donc raisonnable d'utiliser la liste des organisations d'une année pour initialiser l'extraction d'entités pour l'année N+1. Même si les listes produites demandent à être validées manuellement, c'est certainement plus rapide que d'extraire manuellement le nom des organismes à partir des 180 rapports d'activité. 

Introduction
Raisonner à partir de cas consiste à résoudre un problème à l'aide d'une base de cas, dans laquelle un cas représente un problème déjà résolu accompagné de sa solution (Riesbeck et Schank (1989)). Un système de raisonnement à partir de cas (RÀPC) sélectionne un cas dans la base de cas, puis adapte la solution associée. L'adaptation nécessite des connaissances spéci-fiques au domaine d'application. L'acquisition de connaissances d'adaptation a pour but d'extraire ces connaissances, ce qui peut être réalisé soit directement auprès d'un expert du domaine (d'Aquin et al. (2006)), ou encore par analyse de la base de cas (voir par exemple Hanney et Keane (1996), McSherry (1998), Craw et al. (2006)).
Un cas est généralement représenté par un couple (pb, Sol(pb)) dans lequel pb repré-sente un énoncé de problème et Sol(pb) une solution de pb. L'ensemble des cas sources (srce, Sol(srce)) d'un système de RÀPC constitue la base de cas BC. Lors d'une session particulière de RÀPC, le problème à résoudre est appelé problème cible, dénoté par cible. Une inférence à partir de cas associe à cible une solution Sol(cible), compte tenu de la base de cas BC et de bases de connaissances additionnelles, en particulier O, l'ontologie du domaine, qui introduit les concepts et les termes utilisés pour représenter les cas.
Le processus de RÀPC est principalement composé d'une étape de remémoration et d'une étape d'adaptation. La remémoration sélectionne (srce, Sol(srce)) ? BC tel que srce est jugé similaire à cible. Le but de l'étape d'adaptation est ensuite de résoudre cible en modifiant Sol(srce) de façon adéquate. Un problème d'adaptation est donné par un triplet (srce, Sol(srce), cible), et une solution d'un problème d'adaptation est une solution Sol(cible) du problème cible. Une étape de mémorisation d'un cas peut venir compléter le processus.
Le modèle d'adaptation adopté est une forme d'analogie transformationnelle (Carbonell (1983) 
L'étape d'adaptation est dépendante du domaine d'application car elle nécessite des connaissances spécifiques au domaine. Ces connaissances doivent être acquises 1 . C'est l'objet de l'acquisition de connaissances d'adaptation (ACA).
Dans la section suivante nous rappelons les différentes étapes du processus d'ECBD et détaillons la façon dont celles-ci sont effectuées dans notre système CABAMAKA. Puis, dans la section 3, nous nous intéressons à la définition d'indices de qualité pour classer les règles d'adaptation obtenues. Enfin, dans la section 4, nous montrons comment le système peut être amélioré pour extraire des dépendances qualitatives entre variables.
CABAMAKA
CABAMAKA (acronyme de case base mining for adaptation knowledge acquisition) reprend les idées principales présentées dans Hanney et Keane (1996). Dans ces travaux, les variations entre cas sources sont exploités pour apprendre des règles d'adaptation. Tous les couples (cas-source i , cas-source j ) de cas sources similaires dans la base de cas sont formés. Puis, pour chacun de ces couples, les variations entre problèmes srce i et srce j et solutions Sol(srce i ) et Sol(srce j ) sont représentés (?pb et ?sol). Des heuristiques sont ensuite mises en oeuvre pour regrouper ces règles d'adaptation et sélectionner la règle à appliquer lors d'une session de RÀPC. Il a été montré expérimentalement que l'utilisation de telles connaissances d'adaptation augmente la performance du système.
CABAMAKA se distingue néanmoins des ces travaux sur plusieurs points : -Les connaissances d'adaptation obtenues doivent être validées par un expert et des explications doivent y être associées pour qu'elles soient compréhensibles par l'utilisateur. En ce sens, CABAMAKA peut être considéré comme un système d'apprentissage semiautomatique. -Tous les couples de cas sources distincts de la base de cas sont pris en compte, pas seulement les couples de cas similaires. En conséquence, si n est la taille de la base de cas (n = |BC|) le volume de cas examinés s'élève à n(n ? 1). Dans notre application, n ? 650, ce qui amène à examiner un assez grand nombre de couples (n(n?1) ? 5·10 5 ).
C'est pourquoi des techniques efficaces d'extraction de connaissances (Dunham (2003)) ont été choisies pour ce système.
Principes
Principes de l'ECBD
Le but de l'ECBD est d'obtenir des connaissances à partir de données. Le processus d'ECBD se fait sous la supervision d'un analyste, qui est un expert du domaine. Une fois l'acquisition des données réalisée, il se déroule en trois étapes : la préparation des données, la fouille de données et la validation des connaissances extraites.
La préparation des données est une étape de mise en forme et de sélection des données. L'opération de mise en forme met les données dans un format acceptable pour l'algorithme de fouille choisi. La sélection des données permet de concentrer la fouille sur un sous-ensemble pertinent d'objets et/ou d'attributs, et d'éliminer les données bruitées.
La fouille de données extrait des éléments d'information à partir des données. Par exemple, CHARM (Zaki et Hsiao (2002)) est un algorithme de fouille de données qui réalise efficacement l'extraction de motifs fermés fréquents (MFF). CHARM prend en entrée un ensemble d'objets, chaque objet x étant un ensemble de propriétés booléennes. Un motif m est un ensemble de propriétés et son extension est l'ensemble des objets qui le contiennent. Le support de m, Supp (m), est la proportion d'objets x contenant m (m ? x). Autrement dit, pour une variable aléatoire X parcourant l'ensemble des objets avec une distribution uniforme de probabilités, on a Supp
La validation des connaissances extraites se fait avec l'aide de l'analyste, qui interprète les résultats. Cette étape d'interprétation produit des unités de connaissances.
Préparation des données
L'étape de préparation des données génère un ensemble d'objets à partir de la base de cas BC, en appliquant successivement deux transformations.
La première transformation ? formate chaque cas source (srce, Sol(srce)) en deux ensembles de propriétés booléennes : ?(srce) et ?(Sol(srce)). L'implantation de cette transformation dépend beaucoup du formalisme utilisé pour représenter les cas. Cette transformation entraîne en général une perte d'information, qui doit être minimisée. Le vocabulaire utilisé pour décrire les cas étant celui de l'ontologie du domaine O, si ?(srce) = {p 1 , . . . , p n }, on ajoute à ?(srce) toute propriété q qui peut se déduire de l'ensemble {p 1 , . . . , p n } en fonction de l'ontologie O.
La deuxième transformation produit un objet à partir de chaque couple de cas sources (?(cas-source 1 ), ?(cas-source 2 )). Suivant le modèle d'adaptation présenté en introduction, x doit encoder les propriétés de ?pb et de ?sol. ?pb encode les similarités et dissimila-
-Les propriétés communes à srce 1 et srce 2 (marquées par "="), -Les propriétés de srce 1 que srce 2 ne partage pas ("-") et -Les propriétés de srce 2 que srce 1 ne partage pas ("+").
Toutes ces propriétés sont reliées à des problèmes et sont marquées par pb. ?sol est calculé de façon similaire et x = ?pb ? ?sol. Par exemple, 
Interprétation
L'étape d'interprétation est supervisée par un analyste. Le système CABAMAKA fournit à l'analyste les MFF extraits et lui permet de naviguer parmi eux. L'analyste peut sélectionner un MFF, l'interpréter en règle d'adaptation, puis valider, corriger, voire généraliser la règle.
Chaque motif obtenu par CABAMAKA à la suite de l'étape de fouille peut se lire comme une règle d'adaptation qui exprime une relation entre :
-La présence ou non de certaines propriétés booléennes dans ?(srce), ?(cible) et ?(Sol(srce)), -La présence ou non de certaines propriétés booléennes dans ?(Sol(cible)). Par exemple, le motif m ex correspond à une règle d'adaptation qui peut se lire de la manière suivante :
si a est une propriété de srce mais n'est pas une propriété de cible, c est une propriété à la fois de srce et cible, d n'est pas une propriété de srce mais est une propriété de cible, A et B sont des propriétés de Sol(srce) et C n'est pas une propriété de Sol(srce) alors les propriétés de Sol(cible) sont ?(Sol(cible)) = (?(Sol(srce)) \ {A}) ? {C}.
Application
Le domaine d'application pour lequel cette étude a été réalisée est celui du traitement du cancer du sein. Dans cette application, un problème décrit une classe de patients par un ensemble d'attributs (comme l'âge ou la taille de la tumeur) et de contraintes sur les valeurs prises par ces attributs. Une solution est un ensemble de traitements (radiothérapie, chimiothé-rapie, etc.) recommandés pour ces patients. Ce motif peut être interprété ainsi : si srce et cible représentent tous deux des classes de patients de moins de 70 ans, si la différence entre srce et cible réside dans la taille de la tumeur -moins de 4 cm pour srce et plus de 4 cm pour cible -et si une mastectomie partielle avec curage axillaire est proposée pour srce, alors Sol(cible) est obtenue en remplaçant dans Sol(srce) la mastectomie partielle par une mastectomie totale.
Résultats
Cette règle traduit le fait que le type d'intervention chirurgicale proposé dépend de la taille de la tumeur du patient : plus la taille de la tumeur est grande, plus on augmente le geste chirurgical.
L'obtention de telles règles d'adaptation nécessite pour l'instant l'intervention d'un ingé-nieur de la connaissance qui sélectionne les motifs intéressants parmi les résultats, les interprète comme des règles d'adaptation puis les présente à l'analyste pour validation. Deux obstacles subsistent à un pilotage par l'analyste du processus complet d'extraction de connaissances :
-Les règles obtenues ne sont pas formulées dans un format intelligible par l'analyste : l'analyste n'est pas capable d'interpréter un motif. Pour devenir compréhensibles les règles obtenues doivent être exprimées en langue naturelle. -Les règles obtenues sont trop nombreuses pour être toutes présentées à l'analyste pour validation. La figure 1 présente les résultats expérimentaux -temps d'exécution de CHARM, implanté dans la plateforme CORON (Szathmary et Napoli (2005) 
FIG. 1 -Résultats de l'étape de fouille de données pour une base de cas test de 59 cas.
A cause du grand nombre de motifs que l'ingénieur de la connaissance doit examiner avant de pouvoir en proposer à l'analyste pour validation, la mise en oeuvre expérimentale pour l'évaluation du système prend également du temps. Il est donc nécessaire de doter l'analyste de moyens de naviguer dans l'ensemble des règles obtenues et de sélectionner les règles inté-ressantes.
Vers la définition d'indices de qualité pour les règles d'adaptation
Un moyen de sélectionner les règles intéressantes parmi l'ensemble des résultats est de les classer selon un indice de qualité. Cet indice pourra être adapté des indices utilisés pour les règles d'association.
On rappelle qu'une règle d'association est la donnée de deux motifs A et B disjoints et dénotée par A ? B. A est appelé l'antécédent de la règle et B le conséquent. Elle traduit, dans un ensemble d'objets, le fait que si le motif A est présent dans un objet, alors il est probable que le motif B soit également présent. Cette probabilité est appelée confiance de A ? B et est la probabilité conditionnelle d'avoir le motif B, sachant qu'on a le motif A :
(X est une variable aléatoire de distribution uniforme sur l'ensemble des objets). À l'issue de l'étape de fouille de CABAMAKA, un motif m peut être décomposé en deux sous-motifs ?pb et ?sol et lu comme la règle d'association ?pb ? ?sol : ?pb est l'ensemble des propriétés de m indicées par pb et ?sol est m\?pb. Dans ce cas, la confiance de la règle ?pb ? ?sol vaut :
Néanmoins, la règle produite par une telle décomposition du motif m ne correspond pas à une règle d'adaptation. En effet, la règle ?pb ? ?sol exprime ce que doit être le couple (Sol(srce), Sol(cible)) étant donné un certain couple (srce, cible). Une règle d'adaptation exprime quant à elle ce que doit être Sol(cible) étant donné un problème d'adaptation (srce, Sol(srce), cible). Le même motif doit donc se lire :
Or une telle règle ne correspond pas à une décomposition d'un motif en deux sous-motifs, donc les indices définis pour les règles d'association ne s'appliquent pas directement.
Pour définir une mesure de confiance qui soit plus adaptée aux règles d'adaptation, il convient de considérer non seulement l'ensemble des objets constituant l'extension d'un motif, mais également les couples de cas sources que ces objets représentent. Prenons par exemple un objet faisant partie de l'extension d'un motif m et le couple (cas-source 1 , cas-source 2 ) de cas sources représenté par cet objet. Si le motif contient une propriété p = pb , alors le couple de cas sources est tel que les deux problèmes sources partagent la propriété p, soit p ? ?(srce 1 ) ? ?(srce 2 ). Les propriétés constitutives du motif et leur marquage expriment ainsi un ensemble de conditions portant sur la présence ou non de certaines propriétés dans les ensembles ?(srce), ?(Sol(srce)), ?(cible) et ?(Sol(cible)) d'un couple de cas. La
sol ?(Sol(srce 1 )) ? ?(Sol(srce 2 )) ?(Sol(srce 1 ))\?(Sol(srce 2 )) ?(Sol(srce 2 ))\?(Sol(srce 1 ))
FIG. 2 -Appartenance des propriétés booléennes aux cas sources selon leur marquage dans un motif.
figure 2 résume à quel ensemble de propriétés d'un couple de cas sources doit appartenir une propriété suivant la façon dont elle est marquée dans un motif. Une règle d'adaptation décrit alors les propriétés que doit ou non contenir ?(Sol(cible)) compte tenu de la présence ou non de certaines propriétés dans ?(srce), ?(Sol(srce)) et ?(cible). La règle d'adaptation correspondant à un motif m peut être lue de la façon suivante :
? m}. On peut dès lors définir une mesure de confiance pour les règles d'adaptation, en considérant une variable aléatoire non plus sur l'univers des objets, mais sur l'univers des couples (?(cas-source 1 ), ?(cas-source 2 )) d'images par ? de cas sources distincts (avec une distribution uniforme sur cet ensemble). La confiance d'une règle d'adaptation mesure alors la probabilité conditionnelle que le conséquent d'une règle d'adaptation soit vérifié, sachant que l'antécédent l'est :
Le classement des règles selon cet indice n'a pour l'instant pas été mis en place. Des travaux sont en cours pour tenter de ramener le calcul de ce nouvel indice à des calculs de supports, ce qui faciliterait son opérationnalisation.
En dehors  
Exhiber des dépendances qualitatives entre variables
Lors de l'étape de préparation des données, les cas sources sont décrits par des ensembles de propriétés booléennes et un objet encode les variations de présence et d'absence de ces propriétés lorsqu'on passe d'un cas source à un autre. Lorsque ces propriétés booléennes représentent différentes modalités d'une même variable, comme par exemple l'âge, la taille ou le sexe, il peut être intéressant d'encoder également dans les objets les variations de modalité subies par ces variables. Les motifs obtenus expriment alors des dépendances qualitatives entre variables.
Prenons par exemple les deux propriétés booléennes âge = 30 et âge = 45, qui correspondent à deux modalités de la variable âge, l'une caractérisant les problèmes pour lesquels la valeur de l'âge est 30 ans et l'autre les problèmes pour lesquels la valeur de l'âge est 45 ans. Soient deux cas sources cas-source 1 et cas-source 2 , tels qu'à l'issue de la première transformation ?(srce 1 ) contient la propriété âge = 30 et ?(srce 2 ) contient la propriété âge = 45. L'objet produit lors de la deuxième transformation pour ces deux cas sources contient donc le motif {(âge = 30)
pb }. Ce motif correspond à une augmentation de la valeur de la variable âge lorsqu'on passe du cas source cas-source 1 au cas source cas-source 2 . Pour encoder également dans cet objet la variation qualitative que subit la variable âge, on peut enrichir l'objet d'une nouvelle propriété âge:varie, qui représente une variation de la variable âge, et d'une propriété âge:augmente, qui représente son sens de variation.
Si le même objet contient la propriété chimiothérapie:varie, qui représente une variation de dose prescrite pour la chimiothérapie, et la propriété chimiothérapie:diminue, qui représente une diminution de cette dose, un motif obtenu à l'issu de l'étape de fouille peut être le motif {âge:varie, chimiothérapie:varie}. Ce motif exprime une dépendance fonctionnelle entre les variables âge et chimiothérapie. De la même façon, le motif plus spé-cifique {âge:augmente, chimiothérapie:diminue} peut être obtenu. Ce motif exprime la dépendance qualitative âge ? ? ?? chimiothérapie, selon laquelle la dose de chimiothérapie diminue avec l'âge du patient 2 .
De telles règles abstraites ont l'avantage d'être plus facilement interprétables par l'analyste que les règles d'adaptation. De plus, chacune d'elle étant partagée par plusieurs motifs, elles constituent un moyen efficace de les regrouper hiérarchiquement.
Conclusion
Dans cet article, nous avons présenté CABAMAKA, un système qui met en oeuvre une technique d'ECBD, l'extraction de motifs fermés fréquents, pour extraire des connaissances d'adaptation à partir des variations qui existent au sein d'une base de cas. Ce système est assez unique en son genre car il extrait des connaissances à partir de connaissances.
Nous avons montré en quoi les indices de qualité existant pour les règles d'association sont inadaptés pour mesurer la qualité d'une règle d'adaptation obtenue par ce système et comment l'on peut s'en inspirer pour créer des indices plus appropriés. Des travaux sont actuellement en cours sur la définition de tels indices.
Nous avons également proposé une amélioration du système de façon à découvrir des règles plus abstraites qui expriment des dépendances qualitatives entre les variables entrant en jeu dans la description des cas. La validation de telles dépendances doit être plus facile car ces règles sont moins nombreuses et plus intelligibles par l'analyste. Cette méthode est actuellement en cours d'implantation.
Par ailleurs, les connaissances d'adaptation obtenues par CABAMAKA ont vocation à venir alimenter un portail sémantique (d'Aquin (2005)) développé dans le cadre du projet KASIMIR (Lieber et al. (2002)), dont l'objet est la gestion de connaissances et l'aide à la décision en cancérologie. En particulier, les travaux actuels portent sur l'intégration de ces connaissances au moteur de raisonnement à partir de cas de KASIMIR.

Classification des pages
L'objectif de cette phase est d'identifier les principaux types de pages composant le site analysé. Un type de pages est un ensemble de pages relativement similaires tant sur le plan syntaxique (code HTML) que sémantique (concept représenté par la page).
Pour atteindre cet objectif, un taux de similarité est calculé entre les pages du site sur la base d'un ensemble de critères tels que ceux décrits dans Ricca et Tonella (2003).
Analyse sémantique des pages
Lors de cette étape, l'utilisateur définit les composants qu'il souhaite extraire à partir d'un échantillon représentatif de pages d'un même type. Un composant est un concept présent au sein des pages d'un même type. Il peut être absent de certaines pages et/ou y apparaître plusieurs fois. De plus, on lui associe une indication de format (i.e. texte simple ou balisé) et de localisation. Dans Retroweb, cette dernière propriété est exprimée sous la forme d'un chemin (XPath) dans l'arborescence formée par les balises HTML.
La figure 1 illustre le scénario de construction d'une règle d'extraction.
(1) L'utilisateur sélectionne une instance du composant à définir et lui assigne un nom représentatif tandis que l'outil calcule son chemin d'accès XPath. (2) La règle est appliquée à chacune des pages de l'échantillon afin d'en vérifier la validité. (3) Si la valeur attendue pour chacune des pages n'a pu être extraite, la règle doit être raffinée. Pour ce faire plusieurs solutions sont proposées :
(1) Construction de la règle 
Extraction des données et de leur schéma
Le module d'extraction des données applique un ensemble de règles d'extraction à un ensemble de pages afin d'en extraire les instances de composants ainsi que leur structure (XML).
Conclusion
Retroweb est un outil d'extraction de données ciblées à partir de sources Internet. Ses avantages principaux sont sa facilité d'utilisation et la possibilité de se concentrer uniquement sur les types de données utiles pour un usage spécifique.
Références
Estiévenart, F., J.-R. Meurisse, J.-L. Hainaut, et P. Thiran (2006 
Summary
The Retroweb tool is dedicated to the extraction of web data. The proposed approach is user-oriented and semi-automated, since it requires minimal user input in order to focus only on those pieces of information that are of particular interest to them.

Introduction
Les motifs séquentiels sont étudiés depuis plus de dix ans (Agrawal et Srikant (1995)), ils permettent de mettre en exergue des corrélations entre événements suivant leur chronologie d'apparition. Les motifs séquentiels ont été récemment étendus dans un contexte multidimensionnel par Pinto et al. (2001), Plantevit et al. (2005) et Yu et Chen (2005). Ils permettent ainsi de découvrir des motifs définis sur plusieurs dimensions et ordonnés par une relation d'ordre (e.g. temporelle). Par exemple, dans Plantevit et al. (2005), des motifs de la forme "La plupart des consommateurs achètent une planche de surf et un sac à N.Y., puis ensuite une combinaison à SF" sont découverts. Les motifs séquentiels multidimensionnels sont bien adaptés aux contextes de stockage et de gestion des données actuels (entrepôts de données). En effet, les motifs ou règles obtenus permettent une autre appréhension des données sources. Cependant leur découverte nécessite certains paramètres dont en particulier le support minimal. Celui-ci correspond à la fréquence minimale d'apparition des motifs au sein de la base considérée. Si le support minimal choisi est trop élevé, le nombre de règles découvertes est faible mais si le support est trop bas, le nombre de règles obtenues est très important et rend difficile l'analyse de celles-ci. Un autre problème est la longueur des motifs extraits. Comment ajuster au mieux le support afin d'obtenir des séquences suffisamment longues pour être réellement utilisables ? L'utilisateur est alors confronté au problème suivant : comment baisser le support minimal sans générer la découverte de règles non pertinentes ? Ou comment augmenter le support minimal sans perdre les règles utiles ? Est-il alors nécessaire de faire un compromis entre qualité des connaissances extraites et support ?
L'utilisation des hiérarchies dans l'extraction de connaissances représente un excellent moyen de résoudre ce dilemme. Elle permet de découvrir des règles au sein de plusieurs niveaux de hiérarchies. Ainsi, même si un support élevé est utilisé, les connaissances importantes dont le support est faible dans les données sources peuvent être "subsumées" par des connaissances plus générales qui, elles, seront comptabilisées comme fréquentes.
La prise en compte des hiérarchies dans l'extraction de motifs séquentiels multidimensionnels a été proposée par Plantevit et al. (2006b)  Dans la suite de cet article, nous décrivons les différentes propositions prenant en compte les hiérarchies dans un contexte multidimensionnel. Après avoir rappelé les concepts associés aux motifs séquentiels multidimensionnels, notre contribution est détaillée en définissant les concepts de séquences convergentes et divergentes. Nous décrivons ensuite les algorithmes et les fonctions permettant l'extraction de telles séquences. Des expérimentations, sur des jeux de données synthétiques et réelles, montrent l'intérêt de notre approche M 2S_CD aussi bien en terme de robustesse des algorithmes que de pertinence des motifs extraits.
Travaux antérieurs : motifs multidimensionnels et hiérar-chies
Combiner plusieurs dimensions d'analyse permet d'extraire des connaissances qui dé-crivent mieux les données. Dans Pinto et al. (2001), les auteurs sont les premiers à rechercher des motifs séquentiels multidimensionnels. Ainsi les achats ne sont plus simplement décrits en fonction des seuls date et identifiant du client comme dans contexte classique, mais en fonction d'un ensemble de dimensions telles que Type de consommateur, Ville, Age. Cette approche ne permet que l'extraction de séquences définies sur une seule dimension (e.g. produit) caractéri-sées par une motif multidimensionnel. Ainsi, il est impossible d'extraire des combinaisons de motifs multidimensionnels suivant le temps.
L'approche proposée par Yu et Chen (2005) est très singulière puisqu'il existe un très fort lien hiérarchique entre les dimensions d'analyse. Les pages web sont visitées durant une ses-sion au cours d'une journée. Cette approche multidimensionnelle permet donc une gestion plus fine du temps mais ne permet pas de se situer réellement dans un contexte multidimensionnel.
Dans Plantevit et al. (2005), les règles extraites ne combinent pas seulement plusieurs dimensions d'analyse. Ces dimensions sont également combinées au cours du temps. Par exemple, dans la règle "Les ventes de pepsi augmentent à NY puis les ventes de coca augmentent à LA", N Y apparaît avant LA et pepsi avant coca.
Il existe très peu de travaux conciliant hiérarchies et multidimensionnalité lors de l'extraction de motifs séquentiels. Les travaux de Yu et Chen (2005) permettent une représentation plus fine du temps, mais ne répondent pas à notre problématique générale d'un nombre quelconque de dimensions. Seule l'approche HYPE, (Plantevit et al. (2006a), Plantevit et al. (2006b) Mais cette proposition ne permet pas d'extraire des séquences où des items de même dimension mais de granularité différente cohabitent tels que (N ice, Coca) et (F rance, Soda). En effet, pour assurer un passage à l'échelle dans un contexte d'explosion du nombre de motifs possibles, le choix de ne conserver que les items maximalement spécifiques a été fait.
A notre connaissance, il n'existe donc aucune approche proposant de prendre en compte les hiérarchies dans un contexte multidimensionnel tel qu'il existe des items comparables dans les séquences extraites. Nous proposons donc les nouveaux concepts de séquences multidimensionnelles convergentes et divergentes afin de permettre une extraction de connaissances plus complète et adaptée aux spécificités des contextes multidimensionnels. Nous dirigeons ainsi la génération des motifs soit du général au particulier soit du particulier au général afin de limiter le nombre de motifs candidats mais nous ouvrons ainsi la voie à des motifs composés de séquences plus longues.
M2S_CD : motifs séquentiels multidimensionnels convergents ou divergents
Dans cette section, nous introduisons un concept original. En effet, l'esprit humain raisonne souvent de deux façons différentes et symétriques. La réflexion s'exécute de l'exemple vers la théorie ou de la théorie vers l'exemple. Nous essayons donc de reproduire ce type de raisonnement dans les connaissances que nous souhaitons extraire. Nous introduisons donc le concept de séquence multidimensionnelle convergente ou divergente. Nous présentons les différentes définitions préliminaires associées aux motifs séquentiels multidimensionnels avec prise en compte des hiérarchies pour ensuite détailler les concepts de motifs convergents et divergents ainsi que les algorithmes associés.
Base Exemple
Pour illustrer les différents concepts et définitions, nous proposons la base exemple du tableau Tab. 1 qui décrit les ventes réalisées dans différentes villes du monde par différentes  
Définitions préliminaires
Soit une base de données DB où les données sont définies suivant n dimensions, nous considérons une tri-partition de l'ensemble des dimensions :
-L'ensemble des dimensions sur lesquelles sont extraites les règles 
Ainsi, la séquence in, N ice)}{(P errier, N ice)} est une sous-séquence de la séquence in, * ), ( * , M oscou)}{( * , N ice)(P errier, N ˆ imes)} Nous considérons que chaque bloc défini sur D R contient une séquence de données multidimensionnelles qui est identifiée par ce bloc. Un bloc supporte une séquence ? si ? est une sous-séquence de la séquence de données identifiée par ce bloc. Le support d'une séquence multidimensionnelle correspond donc au nombre de blocs définis sur D R qui contiennent cette séquence.
Dans le contexte dans lequel nous nous situons, nous considérons qu'il existe des relations hiérarchiques sur chaque dimension d'analyse matérialisées sous la forme d'arbres. Une hiérarchie est donc représentée par un arbre orienté dans lequel les arcs sont de type isa. La relation de généralisation/spécialisation s'effectue ainsi de la racine vers les feuilles. Chaque dimension d'analyse possède donc une hiérarchie qui permet de représenter les relations entre les éléments de son domaine. Soit T DA = {T 1 , . . . , T m } l'ensemble des hiérar-chies associées aux dimensions d'analyse où : (i) T i est la hiérarchie représentant les relations entre les éléments de la dimension d'analyse
On notê x un ancêtre de x dans la hiérarchie et?xet? et?x un de ses descendants. Par exemple, boisson = soda signifie que boisson est un ancêtre de soda dans la relation Généralisation/Spécialisation. Plus précisément, boisson est une instance plus générale que soda. Seuls les éléments qui sont feuilles dans l'arbre adéquat sont présents dans la base de données. Plantevit et al. (2006b) proposent une définition des concepts d'item, itemset et séquence multidimensionnels h-généralisés. Ainsi un item multidimensionnel h-généralisé e = (d 1 , . . .-, d m ) est un m-uplet défini sur les dimensions d'analyse D A tel que d i ? {label(T i )} (d i existe dans la hiérarchie adéquate). Contrairement aux données de DB, un item multidimensionnel h-généralisé peut être défini avec n'importe quelle valeur d i dont le noeud associé dans l'arbre hiérarchique n'est pas nécessairement une feuille.
Puisque les items multidimensionnels h-généralisés peuvent être définis sur différents niveaux de hiérarchies, il est nécessaire de définir une relation hiérarchique entre ces items. 
Définition 2 (Inclusion hiérarchique) Soient deux items multidimensionnels h-généralisés
En d'autres mots, pour tout item de la séquence, il n'existe pas un item plus général déjà présent à une date antérieure. La séquence M ontpellier)}, {(Coca, LR)(P epsi, P ACA)}, {(Soda, F rance), (Soda, Allemagne)} est une séquence divergente.
Définition 4 (Séquence convergente) Une séquence ? = 11 , . . . , e ij , . . . , e nk est conver-
En d'autres mots, pour chaque item de la séquence, il n'existe pas d'item plus spécifique déjà présent dans la séquence à une date antérieure. La séquence Eurasie)}, {(Soda, Europe)(B.A, Asie)}, {(Coca, F rance)(P epsi, Russie)} est une séquence convergente.
Mise en oeuvre
Ordre dans les séquences
Ordonner les séquences est une étape fondamentale afin d'améliorer l'implémentation et éviter les cas déjà examinés. Les méthodes existantes, basées sur les différentes philosophies (pattern growth (Pei et al. (2004)), générer/élaguer (Agrawal et Srikant (1995)  (2002))), ne sont pas directement applicables dans un contexte multidimensionnel. En effet, les items h-généralisés ne sont pas explicités dans la base de données. De tels items sont extraits par inférence puisqu'ils ne sont pas directement associés à un n-uplet dans la base de données.
Munich)(Coca, N ice)} N ice)(P epsi, Munich)}
TAB. 2 -Contre-exemple
Le tableau Tab. 2 montre un exemple de séquences de données qui ne peut pas être traité avec les approches existantes dans un contexte classique puisque les items h-généralisés ne sont pas "explicitement" présents dans la base. En effet, aucun ordre lexical total, prenant en compte les items h-généralisés, ne peut être directement utilisé. Ainsi, ces méthodes ne peuvent pas extraire la séquence N ice)(Soda, M unich)} (où Soda est un ancêtre de coca et pepsi). En effet, les méthodes basées sur le paradigme pattern growth trouvent l'item (Coca, N ice) avec un support de 2. Ensuite, elles construisent la base projetée pré-fixée par la séquence N ice)} Cette base projetée contient les séquences et epsi, M unich)} L'item h-généralisé (Soda, M unich) n'apparaît pas dans cette base projetée alors qu'il est fréquent dans la base initiale. Dans les approches de type générer-élaguer, le problème est similaire. Par exemple, dans Masseglia et al. (1998), la projection de la base de données dans l'arbre préfixé des séquences candidates est biaisée.
Il est impossible d'étendre l'ensemble de la base avec tous les item h-généralisés possibles avant le processus d'extraction. Par exemple, considérons une base de données contenant m dimensions d'analyse et n i items (feuilles) dans un itemset i, la profondeur moyenne des hié-rarchies est d. La transformation d'un itemset va produire d m ×n i items au lieu des n i initiaux, multipliant donc la taille de la base initiale par d m . Il est donc nécessaire de prendre en compte les items h-généralisés durant le processus d'extraction et non après un pré-traitement. Nous allons donc introduire un ordre lexical et matérialiser localement les items h-généralisés.
Définitions
Il est primordial de disposer d'un ordre lexicographique lors de l'extraction de motifs fré-quents puisque c'est la clef de la non-duplication des items durant le processus.
On dit qu'un itemset est étendu s'il est égal à sa fermeture transitive par rapport à la relation de spécialisation (< h ). La notion d'itemset étendu permet de prendre en compte tous les items h-généralisés qui peuvent être inférés à partir d'une séquence de données. Afin d'optimiser le traitement des données, nous introduisons un ordre lexicographico-spécifique (lgs), qui est un ordre alpha-numérique selon le degré de précision d'un item. Ainsi, les items les plus spécifiques sont prioritaires. Nous devons définir une fonction LGS-Closure qui transforme un itemset (transaction) en un itemset étendu contenant tous les items h-généralisés. L'extraction des items fréquents peut donc être effectuée sur chaque itemset étendu. Dans les approches pattern growth, les séquences sont extraites en ajoutant un item fréquent à une sé-quence fréquente de manière gloutonne. Il est nécessaire de définir un moyen efficace d'étendre les séquences à partir du dernier itemset de la séquence. Dans ce but, nous définissons une restriction de la fonction LGS-Closure de la façon suivante :
Définition 5 (Fonction LGS-Closure) La fonction LGS-Closure est une application d'un itemset i vers la fermeture de i avec l'ordre LGS (< lgs
Algorithmes
Les séquences divergentes sont extraites en utilisant l'algorithme M 2S_CD (Algorithme 1) suivant une exploration gloutonne en profondeur (paradigme pattern growth). Au lieu de parcourir l'intégralité de la base de données, niveau par niveau, comme le font les méthodes de type générer-élaguer, la base de données est projetée en fonction de la séquence actuellement explorée. La projection est différente de celle proposée par Pei et al. (2004). En effet, comme nous devons gérer les items h-généralisés, la projection doit prendre en compte la transaction (itemset) où l'item a été trouvé, et pas seulement l'item lui même comme dans Pei et al. (2004). Pour prendre en compte cette transaction, nous utilisons la fonction LGS-Closure en filtrant les items déjà trouvés.
L'utilisation des bases projetées permet d'éviter des passes inutiles sur des données déjà parcourues. En effet, considérons une séquence fréquente ? et la séquence actuellement explorée ? tel que ? ? ? or ? ? ?, si ces deux séquences partagent la même base projetée alors il est inutile de continuer l'exploration de la séquence ?. Nous avons seulement besoin de copier le sous-arbre (déjà extrait) de la séquence ? à la séquence ?.
L'algorithme 3 permet l'extraction des items localement fréquents sur la base projetée. Il est basé sur la fonction LGS-Closure. La base projetée est parcourue une seule fois pour extraire tous les items fréquents. Deux types d'items peuvent être extraits :
1. Les items qui ne peuvent pas être inclus dans le dernier itemset de la séquence courante ?. Ces items sont donc inclus dans un nouvel itemset de ?. Pour extraire ces items et prendre en compte les items h-généralisés, nous devons étendre les transactions de la base projetée (pas à pas) avec la fonction LGS-CLosure.
2. Les items qui peuvent être inclus dans le dernier itemset de la séquence courante ?. Dans ce cas, nous utilisons la fonction LGS-Closure X où X représente le dernier itemset de ?.
Algorithme 1: M 2S_CD Data : Base de données DB, support minimum minsup Result : Ensemble des séquences divergentes L begin /*-Initialisation -*/ Set L ? {}; Sequence ? ? /*-Extraction des séquences en profondeur-*/ SequenceGrowing(?, DB, L, minsup); return L; end Ces différents algorithmes permettent l'extraction de séquences divergentes. Pour extraire des séquences convergentes, il est nécessaire d'utiliser les mêmes algorithmes mais sur une base de données inversée. En effet, il suffit d'inverser la relation d'ordre (commencer par la fin) au sein de la séquence de données pour permettre un résultat du général au particulier.
Expérimentations
Dans cette section, nous reportons les expérimentations effectuées sur des jeux de données synthétiques et réels.
Algorithme 2: SequenceGrowing : Algorithme d'extraction
Data : Séquence ?,base projetée DB|?, ensemble des fréquents L , support minimum minsup Result : L'ensemble des séquences divergentes fréquentes et préfixées par ? begin insérer(?, L); /*-Vérifier si la séquence a déjà été parcourue-*/ if ?? | (? ? ? or ? ? ?) ? ? et ? partagent la même base projetée then Copie des descendants de ? dans ?; return Set F l ? getF requentItems(DB|?, minsup); foreach itemset is in other do /*-Recherche des items qui peuvent être insérés dans un nouvel itemset de ?-*/ SearchOtherTransFrequentItem e in LGS-Closure(is); /*-Recherche des items qui peuvent être insérés dans le dernier itemset de ?-*/ if is supports lastItemset(?) then SearchSameTransFrequentItem _e in LGS-Closure lastItemset(?) (is);
return (F l = {e|support(e) ? suppmin ? e est maximalement spécif ique}) end
Données synthétiques
Les expérimentations ont été effectuées sur une base de données synthétiques composée de 10, 000 n-uplets définies sur 5 dimensions d'analyse. Des hiérarchies sont définies sur les dimensions d'analyse. Les expérimentations reportent le nombre de fréquents obtenus et le temps d'exécution en fonction du support, du nombre de dimensions d'analyse, des spécificités des hiérarchies (degré et profondeur). Les figures 2(c) et 2(d) montrent le nombre de fréquents extraits et le temps d'exécution en fonction de la profondeur des hiérarchies pour un seuil de support fixé. Etendre la hiérarchie d'un niveau engendre une spécialisation supplémentaire des données (Soda devient pepsi ou coca). Il y a ainsi plus de valeurs différentes dans la base de données. M 2S_CD apporte une certaine robustesse face à ce phénomène de spécialisation. En effet, même si les données deviennent très détaillées (5 niveaux dans la hiérarchie), notre approche permet d'extraire des séquences définies sur plusieurs niveaux de hiérarchies. On remarque cependant que le temps de traitement est plus long quand le nombre de niveaux augmente. Ceci est dû au nombre d'items h-généralisés potentiellement fréquents qui augmente.
Les figures 2(e) et 2(f) montrent le nombre de séquences extraites et le temps d'exécution en fonction du degré des hiérarchies. Augmenter le degré d'une hiérarchie équivaut à spécia-liser les données (ajout de fils à une instance). Notre approche permet de continuer à extraire des connaissances lorsque la hiérarchie se spécialise. Le temps de traitement devient cependant plus coûteux.
Les figures 2(g) et 2(h) montrent le nombre de séquences extraites et le temps d'exécution en fonction du nombre de dimension d'analyse. Augmenter le nombre de dimensions d'analyse engendre une augmentation du nombre de fréquents et du coût de leur extraction.
Ces expérimentations menées sur des données synthétiques montrent la robustesse de M2S_-CD pour l'extraction des connaissances face à la diversité des données (nombre de dimensions, degré et profondeur des hiérarchies, etc.). Diversifier les données sources engendre un coût de traitement plus important qui reste cependant acceptable.
Données réelles
Nous avons étudié plusieurs parties du jeu Eleusis. Eleusis est un jeu de cartes dont le but est de trouver une règle secrète. Les règles secrètes sont des séquences de cartes contenant une partie droite et une partie gauche. Chaque partie peut contenir plusieurs cartes. Ce jeu permet de simuler la découverte scientifique qui est formée de tests, publications et réfutations. Nous avons donc analysé différentes parties du jeu développé par Dartnell et Sallantin (2005 
Conclusion
Dans cet article, nous proposons une méthode originale pour extraire des connaissances multidimensionnelles définies sur plusieurs niveaux de hiérarchies mais selon un certain point de vue : du général au particulier ou vice et versa. Nous définissons ainsi le concept de sé-quences multidimensionnelles convergentes ou divergentes ainsi que les algorithmes associés basés sur le paradigme "pattern growth". Des expérimentations, sur des jeux de données synthétiques et réelles, montrent l'intérêt de notre approche M 2S_CD.
Ce travail offre de nombreuses perspectives. L'efficacité de l'extraction peut être amélio-rée en s'appuyant sur des représentations condensées des connaissances extraites (clos, libres).

Introduction
L'extraction de motifs contraints est un champ significatif de l'Extraction de Connaissances dans les Bases de Données, notamment pour dériver des règles d'association. L'intérêt des motifs extraits est garanti par le point de vue de l'analyste exprimé à travers la sémantique de la contrainte. Par ailleurs, la complétude de l'extraction assure qu'aucun motif jugé pertinent par l'utilisateur ne sera manqué. La contrainte la plus populaire est certainement celle de fréquence minimale (Agrawal et al., 1993) qui permet de rechercher des régularités au sein d'une base de données. Malheureusement, le nombre de motifs fréquents est souvent prohibitif. Les motifs les plus pertinents sont alors noyés au milieu d'informations triviales ou redondantes que même d'autres contraintes d'agrégats (Ng et al., 1998) n'arrivent pas davantage à isoler.
Dans ces conditions, plusieurs approches proposent de comparer les motifs entre eux pour ne sélectionner que les meilleurs (Fu et al., 2000) ou une couverture (Mannila et Toivonen, 1997;Pasquier et al., 1999). De tels motifs révèlent alors une structure globale au sein des données. Le critère d'appartenance ou non à cette structure s'apparente à une contrainte globale. L'extraction de motifs satisfaisant une contrainte globale présente donc une finalité importante pour les utilisateurs. Cependant, leur extraction s'avère souvent ardue car leur localisation dans l'espace de recherche est loin d'être triviale. En particulier, trouver les k motifs maximisant une mesure d'agrégat (e.g., la fréquence (Fu et al., 2000)) devient problématique dès que la mesure ne satisfait aucune propriété particulière comme l'anti-monotonie.
Dans cet article, nous proposons d'extraire des motifs satisfaisant une contrainte globale. Notre première contribution est une méthode générale d'extraction appelée Approximer-etPousser. L'idée fondamentale est de déduire une contrainte locale qui est affinée au cours de l'extraction. Cette contrainte locale évolutive, exploitée par un algorithme indépendant, réduit alors l'espace de recherche. Ensuite, nous appliquons cette méthode pour rechercher les top-k motifs selon une mesure d'intérêt spécifiée par l'utilisateur. Nous expliquons comment constituer une approximation des top-k motifs à extraire. Puis, nous montrons que cette approximation peut être poussée pour réduire considérablement l'espace de recherche en se fondant sur une méthode de relaxation exposée dans (Soulet et Crémilleux, 2005). L'une des originalités de notre approche est d'autoriser des mesures sans bonne propriété de monotonie et ainsi, de ne pas se cantonner aux seuls top-k motifs fréquents.
Cet article est organisé de la manière suivante. Dans le contexte des contraintes globales, la section 2 définit la notion de top-k motifs selon une mesure et en présente la problématique de l'extraction. La section 3 décrit l'approche Approximer-et-Pousser dédiée à l'extraction des contraintes globales. La section 4 applique cette méthode pour la recherche des top-k motifs selon une mesure en détaillant les deux étapes. Enfin, cette approche est évaluée à la section 5. 
Contexte et travaux relatifs
Contexte et définitions
D
Trans.
Items
L'extraction de motifs cherche la collection de tous les motifs de L I satisfaisant un pré-dicat q, appelé contrainte, et présents dans le contexte transactionnel D. Un motif X est pré-sent dans D s'il apparaît dans au moins une de ses transactions. Introduite dans (Agrawal et Srikant, 1994), l'une des contraintes les plus utilisées est celle de fréquence minimale. La fré-quence d'un motif X, dénotée par freq(X), donne le nombre de transactions contenant X. La contrainte de fréquence minimale (i.e., freq(X) ? ?) sélectionne les motifs dont la fréquence excède un seuil ? fixé par l'utilisateur. De nombreuses contraintes remplacent la fréquence par une autre mesure d'intérêt pour juger au mieux la pertinence d'un motif (Ng et al., 1998). Parmi ces mesures, l'aire d'un motif X, notée area(X), correspond au produit de sa fréquence par sa longueur (i.e., freq(X) × count(X) où count(X) dénote la cardinalité de X). La vé-rification de certaines mesures d'intérêts m(X) ? ? où m : L I ? nécessite de complèter D avec des informations supplémentaires (e.g., une table associant des valeurs à chaque item pour sum(X.val)).
Ces mesures d'intérêt ne suffisent pas toujours à focaliser directement sur les motifs les plus significatifs. Il s'avère alors nécessaire de comparer les motifs entre eux pour n'en conserver que les meilleurs ou une couverture. De tels motifs révèlent alors une structure globale au sein des données. L'appartenance ou non à cette structure se formalise avec la notion de contrainte globale que nous définissons maintenant :
Définition 1 (Contrainte globale) Une contrainte est globale si sa vérification nécessite de comparer plusieurs motifs entre eux.
Typiquement, les contraintes "être-maximal" (Mannila et Toivonen, 1997), "être-fermé" (Pasquier et al., 1999) ou "être-libre" (Boulicaut et al., 2003) qui dégagent une représentation des motifs, sont des contraintes globales. Par exemple, la contrainte "être-fermé" extrait une couverture de la base de données en sélectionnant uniquement les motifs X dont toutes les spécialisations Y ? X ont une fréquence strictement inférieure à celle de X. La vérification de "être-fermé" nécessite donc la comparaison de X avec d'autres motifs. De même, tester si X est un motif fermé en le comparant avec sa fermeture est une comparaison entre deux motifs. Les contraintes globales mettent intrinsèquement en relation plusieurs motifs contrairement aux contraintes usuelles, dites locales, qui peuvent se vérifier isolément sur chacun des motifs.
Dans cet article, nous nous intéressons plus particulièrement à la contrainte globale correspondant aux top-k motifs selon une mesure d'intérêt. Le choix du seuil pour la contrainte de fréquence ou d'aire minimale (et plus généralement de m(X) ? ?) se révèle souvent difficile pour l'utilisateur. En effet, si ce seuil est trop élevé, trop peu de motifs sont extraits (au risque de n'obtenir que des informations triviales). A l'inverse, si ? est trop bas, le nombre de motifs explose et les motifs les plus intéressants sont noyés dans la masse. Comme plusieurs tentatives d'extraction sont nécessaires pour estimer ?, l'utilisateur préfère souvent fixer ce dernier relativement bas rendant parfois les extractions infaisables. Puis, parmi tous les motifs obtenus, il focalise son intérêt sur les premiers motifs maximisant sa mesure d'intérêt. 
La contrainte top k,m est clairement globale puisque X et Y sont présents conjointement dans la définition. Cette contrainte compare les motifs entre eux pour conserver ceux dont la mesure fait partie des k meilleures. Par exemple, les 3 motifs de plus grande aire correspondent exactement aux motifs satisfaisants top 3,area 1 : AB (3 × 2 = 6), AC (3 × 2 = 6) et ABC (2 × 3 = 6). Les motifs associés à la contrainte top k,m sont nommés les top-k motifs selon la mesure m. En fait, leur nombre est parfois supérieur à k (tous les motifs au-delà du k ème ont alors la même mesure). Typiquement les top-3 motifs fréquents sont 4 à savoir A (5), C (4), B (3) et E (3), car la fréquence ne permet pas de distinguer les motifs B et E. Notons que les k motifs minimisant une mesure m satisfont la contrainte top k,?m . Naïvement l'extraction des top-k motifs peut s'effectuer avec un post-traitement. Après l'extraction de tous les motifs dont la mesure m excède un seuil ?, il suffit de sélectionner les k motifs maximisant m. Outre l'inefficacité algorithmique, la difficulté du choix du seuil minimal persiste. Si celui-ci est fixé trop haut, moins de k motifs peuvent être extraits. En revanche, si ce seuil est trop bas, des motifs inutiles sont extraits et ce processus ne profitant pas du paramètre k devient très lent voire infaisable. Pour résoudre ce problème, il est préférable de pousser la contrainte top k,m au sein de l'extraction de motifs. Cette tâche est peu aisée car se pose une double problématique à travers la localisation des motifs dont la mesure est potentiellement élevée et la comparaison de ces mesures pour garantir la maximalité des mesures correspondant aux motifs finalement retenus. En fait, cette contrainte recouvre des problèmes inhérents à la vérification des contraintes globales.
Travaux relatifs
À notre connaissance, aucune méthode générale d'extraction des contraintes globales n'a été proposée dans la littérature auparavant. Individuellement certaines contraintes globales comme "être-maximal" (Mannila et Toivonen, 1997), "être-fermé" (Pasquier et al., 1999) ou "être-libre" (Boulicaut et al., 2003) ont des algorithmes spécifiques reposant principalement sur des élagages anti-monotones. Par exemple, la contrainte de liberté est anti-monotone : si un motif n'est pas libre aucune de ses spécialisations ne sera libre et on peut alors élaguer cette partie de l'espace de recherche. L'extraction des top-k motifs fréquents a été introduite dans (Fu et al., 2000). Les auteurs adaptent APRIORI (Agrawal et Srikant, 1994) pour ajuster le seuil de fréquence minimale au fur et à mesure de l'extraction en bénéficiant à nouveau de l'anti-monotonie de la fréquence. Dans (Hirate et al., 2004), la structure FP-tree permet d'optimiser l'extraction des top-k motifs fréquents. Plus récemment, la structure COFI-tree a aussi été utilisée (Ngan et al., 2005). Dans (Tzvetkov et al., 2003), les auteurs remplacent le langage L I par celui des motifs séquentiels pour extraire les top-k séquences fréquentes. Plusieurs travaux extraient aussi les top-k motifs fréquents satisfaisant un critère additionnel. Par exemple, les k motifs fermés les plus fréquents et de longueur minimale sont recherchés dans  en utilisant la structure FP-tree. D'autres recherchent les motifs les plus fréquents, fermés ou non, et de longueur minimale (Cong, 2001). Tous ces travaux sont restreints à la mesure de fréquence comme mesure d'intérêt car la contrainte de fréquence minimale est anti-monotone (i.e., les motifs satisfaisant top k,freq sont les plus généraux). Remarquons aussi qu'ils se focalisent principalement sur les motifs d'items.
Notre démarche se distingue donc en proposant une méthode adaptée à n'importe quelle mesure d'intérêt basée sur les primitives. Même si cet article est dédié aux motifs d'items, l'approche Approximer-et-Pousser est adaptable à d'autres langages (séquences, arbres, etc).
Par ailleurs, l'approche Approximer-et-Pousser constitue une première proposition générique pour l'extraction des contraintes globales.
3 Approximer-et-Pousser : une approche générique d'extraction pour les contraintes globales L'approche Approximer-et-Pousser est une méthode générique d'extraction des motifs satisfaisant une contrainte globale. Brièvement, l'idée est de restreindre l'espace de recherche lors du parcours en affinant la localisation des motifs susceptibles de vérifier la contrainte globale. Pour cela, cette approche s'appuie sur la répétition de deux étapes majeures (et qui forment son nom) : (1) approximer la collection finale à extraire, (2) pousser des informations issues de cette approximation pour diminuer l'espace de recherche. Plutôt que d'algorithme Approximer-et-Pousser, nous préférons parler d'approche Approximer-et-Pousser car par la suite, cette approche délègue l'élagage de l'espace de recherche à un algorithme indépendant. La condition d'élagage lui est donnée sous forme d'une contrainte locale d'extraction qui est dynamiquement affinée à chaque itération. Une telle approche Approximer-et-Pousser peut être alors vue comme une relaxation évolutive de la contrainte globale en une contrainte locale. Plusieurs illustrations de cette approche sont proposées dans (Soulet, 2006) pour extraire les contraintes "être-maximal" et "être-libre". La section suivante instancie cette aproche pour l'extraction des top-k motifs selon une mesure d'intérêt. Auparavant, nous développons, de manière générale, les deux étapes majeures de l'approche Approximer-et-Pousser :
Approximer La mise à jour de la collection de motifs candidats se décline en trois opé-rations : l'initialisation, l'ajout et la suppression. L'initialisation de la collection des motifs candidats doit être choisie avec attention afin de ne pas manquer de motifs. Lorsque l'espace de recherche est parcouru dans son ensemble, la collection est initialisée à vide ce qui assure la complétude. Ensuite, l'ajout et la suppression des motifs interviennent à chaque nouvelle étape d'approximation i.e., un nouveau motif postule pour entrer dans la collection. Ce dernier est ajouté à celle-ci si et seulement si au vu des motifs candidats déjà présents dans la collection, il peut éventuellement satisfaire la contrainte globale. Enfin, un motif est supprimé de la collection s'il est exclu par un motif postulant. Un motif peut être supprimé soit positivement (i.e., il est conservé car il satisfait la contrainte globale), soit négativement (sinon). Lorsqu'un motif est exclu par le motif postulant, cela n'implique pas toujours l'entrée de ce dernier.
Pousser Par l'intermédiaire de la collection de motifs candidats, cette étape doit permettre de pousser la contrainte globale au coeur de l'extraction et ainsi, réduire l'espace de recherche. Dans un premier temps, cette étape déduit certaines informations de l'approximation (e.g., un calcul effectué sur l'ensemble des motifs candidats). Ces informations évoluent au gré de l'ajout et de la suppression des motifs. Ensuite, celles-ci sont converties en une condition d'éla-gage afin d'éliminer des motifs de l'espace de recherche. Cette condition d'élagage peut par exemple être une contrainte locale adaptée à un algorithme d'extraction.
Extraction des top-k motifs selon une mesure
Aperçu de l'approche
Cette section donne un aperçu général de notre approche d'extraction des top-k motifs selon une mesure m en exploitant la méthode Approximer-et-Pousser.
L'extraction des top-k motifs selon une mesure m est épineuse car en général, on ne sait pas où se situeront dans l'espace de recherche les motifs vérifiant la contrainte. Par ailleurs, la définition 2 ne permet pas directement d'obtenir une contrainte locale qui pourrait être exploitée par un algorithme usuel. Afin de pallier en partie ce dernier point, nous introduisons une définition alternative des top-k motifs avec la propriété 1 : 2. Pousser : cette étape poussera la contrainte m(X) ? ? pour réduire l'espace de recherche.
Chacune de ces deux étapes est difficile. La première doit permettre de fixer le seuil temporaire ? de façon à ne pas éliminer de motifs satisfaisant top k,m (section 4.2.1). La contrainte à pousser m(X) ? ? n'est pas forcément anti-monotone. Nous utiliserons alors le principe de la relaxation anti-monotone (section 4.2.2). Ainsi, avec un algorithme d'extraction de contrainte anti-monotone (comme l'algorithme par niveaux (Mannila et Toivonen, 1997)), notre approche permet de traiter un large ensemble de mesures.
Description des deux étapes 4.2.1 Approximer les top-k motifs
L'étape d'approximation conserve les k motifs maximisant la mesure m parmi les motifs déjà extraits. De cette façon, lorsque l'algorithme d'extraction aura parcouru l'intégralité de l'espace de recherche, les k motifs candidats retenus seront exactement les top-k motifs selon la mesure m.
À l'initialisation de l'extraction, la collection des motifs candidats Cand ne contient aucun motif. La maintenance de cette collection commence alors par une phase de remplissage. Tous les motifs extraits sont ajoutés sans condition jusqu'à obtenir une collection de k motifs candidats. Durant cette phase, aucun motif de Cand n'est supprimé. Ensuite, l'évolution de Cand entre dans une phase sélective guidée par la propriété suivante : Dans notre approche, la collection C de cette propriété correspond aux motifs candidats Cand (ou à un de ses sous-ensembles). Dès que Cand a atteint k éléments, la propriété peut être appliquée sur un motif postulant pour savoir s'il est bien nécessaire de l'ajouter à la collection des motifs candidats. Plus précisément, un motif postulant X est ajouté à la collection si la mesure de X est supérieure à celle d'au moins un des motifs candidats. Dans le cas contraire, la propriété 2 nous garantit que le motif postulant ne pourra pas faire partie des top-k motifs selon m. En outre, un motif est supprimé de la collection dès que k autres motifs de Cand ont une mesure supérieure à la sienne. En effet, la propriété 2 assure à nouveau que ce motif ne sera jamais parmi les k motifs de plus forte mesure m et donc, ne satisfera pas la contrainte top k,m . L'introduction du seuil d'ajout permet d'unifier ces deux phases distinctes de l'étape approximer :
Définition 3 (Seuil d'ajout) Le seuil d'ajout, noté ?, est défini de la manière suivante :
sinon L'intérêt de cette approche est que ce seuil évolue au fur et à mesure des modifications de la collection des motifs candidats Cand. Basiquement, un motif postulant est ajouté à la collection si et seulement si sa mesure m est supérieure à celle du seuil d'ajout ?. Ainsi, durant la phase de remplissage, la collection accepte tous les motifs car leur mesure est toujours supérieure au seuil d'ajout alors égal à ??. Ensuite, les valeurs de la mesure de chacun des motifs de Cand, synthétisées par le seuil d'ajout, conditionne l'introduction ou non du motif postulant au sein de la collection.
Le tableau 2 décrit l'évolution des motifs candidats Cand au cours du processus d'extraction de la contrainte top 3,area (cf. section 2.1) avec l'algorithme APRIORI pour le contexte donné au tableau 1. L'algorithme par niveaux génère trois vagues successives de motifs postulants. La section 4.2.2 explique quels sont les motifs extraits par APRIORI. Pour chaque niveau, les motifs dont l'aire est supérieure à ? (motifs en gras) entrent dans la collection des motifs candidats. La valeur de l'aire est donnée par le chiffre entre parenthèses dans la colonne de
TAB. 2 -Les top-3 motifs selon l'aire avec APRIORI.
gauche et les motifs candidats sont rassemblés dans la colonne centrale. Le seuil ? (colonne de droite) est ajusté au fur et à mesure. Tant que le nombre de motifs de Cand est inférieur à k, le seuil ? a pour valeur ??. Ensuite, ? correspond à l'aire minimale satisfaite par un des motifs de Cand. Le motif E n'est pas exclu par l'entrée de B car son aire est égale ?. En revanche, B et E sont supprimés à l'arrivée du motif AB. À la fin du dernier niveau, Cand correspond aux 3 motifs de plus forte mesure d'aire.
Pousser l'approximation
Cette étape bénéficie de la collection obtenue des motifs candidats afin de réduire l'espace de recherche. Nous montrons maintenant comment il est possible de déduire de cette collection une contrainte anti-monotone afin de réutiliser des algorithmes efficaces bénéficiant de l'antimonotonie.
Les seuls motifs pouvant satisfaire la contrainte top k,m sont ceux qui peuvent être ajoutés à la collection des motifs candidats (car les autres sont immédiatement rejetés, cf. la propriété 2). Ces motifs doivent donc avoir une mesure supérieure au seuil d'ajout i.e., ils satisfont la contrainte locale m(X) ? ?. De façon générale, cette contrainte n'est pas antimonotone. Typiquement, la contrainte area(X) ? ? n'est pas anti-monotone. Par exemple, dans le contexte D, le motif ABC satisfait la contrainte area(X) ? 6, mais pas sa généra-lisation BC dont l'aire est seulement de 4. Afin d'obtenir dans le cas général une contrainte anti-monotone, nous proposons d'approximer la contrainte m(X) ? ? par une relaxation m (X) ? ? vérifiant les deux conditions suivantes :
, cette dernière assurant la complétude du processus. L'obtention de la contrainte relaxée m (X) ? ? n'est pas une tâche triviale. Une méthode automatique et générale pour toute contrainte fondée sur des primitives est donnée dans (Soulet et Crémilleux, 2005). À partir de la contrainte d'aire, nous allons montrer comment procéder.
Tout d'abord, on peut remarquer que la mesure freq(X) × l est décroissante lorsque X est croissant et celle-ci satisfait la condition C1. D'autre part, il est possible de fixer l pour être certain que freq(X) × l sera plus grande que freq(X) × count(X) pour tous les motifs X du jeu de données D (satisfaction de C2). Pour cela, il suffit que le seuil l soit supérieur à la longueur de chacun des motifs présents dans D. Or, la taille du plus grand motif correspond exactement à la taille de la plus grande transaction. De cette manière, l est fixé à 4 avec le jeu de données D. Ainsi, la relaxation anti-monotone freq(X) × 4 ? ? pourra être exploitée comme contrainte locale pour extraire les top-k motifs selon la mesure d'aire dans le jeu de données D.
Reprenons le déroulement de l'extraction des top-3 motifs selon l'aire présenté au tableau 2. La relaxation anti-monotone utilisée est freq(X) ? ?/4. À la fin du premier niveau, ? = 3 et la relaxation freq(X) ? 3/4 n'élimine aucun motif. Tous les motifs de longueur 2 et présents dans le contexte D sont donc générés. En revanche, à la fin du niveau 2, ? = 5 et la relaxation devient freq(X) ? 5/4. Pour le niveau 3, seuls les motifs de fréquence supé-rieure ou égale à 2 sont donc générés (il y a uniquement ABC dans cet exemple). Le processus d'extraction s'arrête alors car plus aucun motif ne peut être généré. Au final, l'approche Approximer-et-Pousser a économisé la génération de 8 motifs de longueur 3 et de 2 motifs de longueur 4. L'efficacité de cette approche Approximer-et-Pousser réside dans l'ajustement dynamique de la contrainte au cours de l'extraction. Plus précisément, la relaxation anti-monotone m (X) ? ? devient de plus en plus sélective car le seuil d'ajout ? croît pour tendre vers ? k,m . Cette approche Approximer-et-Pousser diminue donc significativement l'espace de recherche pour donner un processus d'extraction rapide comme le montre la section expérimentale suivante.
Expérimentations
L'objectif de ces expérimentations est de montrer l'efficacité de l'approche Approximeret-Pousser pour différentes mesures et différents jeux de données. Au-delà de la rapidité, nous souhaitons montrer la faisabilité de notre approche générique. Aussi, nous ne nous comparons pas aux algorithmes de la littérature limités à la seule mesure de fréquence, mais nous confrontons trois stratégies différentes d'extraction des top-k motifs basées sur l'algorithme APRIORI (Agrawal et Srikant, 1994) :
-Approximer-et-Pousser : cette stratégie extrait les top-k motifs en s'appuyant sur l'approche Approximer-et-Pousser. -Optimale à 50% : cette stratégie exploite la relaxation anti-monotone de m(X) ? ? en fixant le seuil ? à 50% du seuil idéal ? k,m . Ce seuil idéal est le seuil permettant d'obtenir exactement et directement les top-k motifs. Bien sûr, dans la réalité, ce seuil n'est pas connu et l'utilisateur procède plutôt par tâtonnement à partir de son intuition. -Post-traitement : les motifs sont extraits avec un seuil de fréquence minimale de 10%.
Puis, les k motifs maximisant la mesure sont conservés. Le seuil de 10% est un compromis entre faisabilité et exhaustivité (i.e., ne manquer aucun des top-k motifs). Pour toutes ces expériences, nous utilisons la même implémentation d'APRIORI. Les temps d'extractions sont donc comparables. Toutes les expériences sont effectuées sur un ordinateur doté d'un processeur Xeon 2.2 GHz et de 3GB de mémoire RAM avec le système d'exploitation Linux.
La figure 1 reporte les temps des extractions en fonction du nombre de motifs désirés k pour les jeux de données mushroom et letter (D.J. Newman et Merz, 1998) (www. ics.uci.edu/~mlearn/MLRepository.html). Sur chaque base, deux mesures ont alors été utilisées, à savoir la fréquence et l'aire. En plus, des trois stratégies exposées cidessus, nous ajoutons le temps d'extraction optimal comme courbe de référence. Cette valeur de référence consiste à donner directement la relaxation anti-monotone de m(X) ? ? k,m pour obtenir exactement les k meilleurs motifs. 
FIG. 1 -Temps d'extraction des top-k motifs.
La stratégie Post-traitement se distingue des deux autres car, quelque soit la valeur de k, le temps d'extraction est le même. Le plus souvent cette stratégie est la moins bonne (surtout lorsque k est peu élevé). Dans de rares situations où k est de valeur moyenne, cette stratégie dépasse les deux autres. En revanche, pour des valeurs de k trop grandes, il arrive que cette approche manque des top-k motifs. Cela se traduit par un arrêt des courbes sur les graphiques de la figure 1 car le processus n'effectue plus la tâche demandée. Par exemple, avec le jeu de données letter, quelque soit la valeur de k, cette stratégie manque des motifs. Elle ne fournit donc pas toujours le résultat souhaité et échoue parfois en temps.
Il est intéressant de remarquer que, globalement, les deux stratégies Approximer-et-Pousser et Optimale-50% ont le même comportement. Plus le nombre de k motifs à extraire est grand, plus le temps d'extraction augmente. Par ailleurs, lorsqu'une mesure est plus difficile à traiter qu'une autre, elle l'est pour les deux stratégies. Comme attendu, dans tous les cas, la courbe de référence est en deçà des deux stratégies. Un résultat important est que pour toutes les expériences, la stratégie Optimale-50% (pourtant optimiste) a de plus mauvais résultats que

Introduction
L'Internet représente un extraordinaire outil d'accès à un ensemble quasi infini de ressources et un puissant outil de communication. Elle prend une place grandissante dans la vie quotidienne et dans le monde professionnel. Le public qui y a accès est de plus en plus large, mais aussi de plus en plus jeune. Les enfants trouvent chaque jour un accès plus facile à la toile. Cet accès de plus en plus large ne va pas sans inconvénients, les sites à caractère adulte, violent, raciste exposent les enfants à des contenus qui peuvent heurter leur sensibilité, voire les choquer. En effet, ces sites sont souvent en accès libre, ce qui pose un problème évident vis à vis des enfants. Ces utilisations litigieuses de l'Internet, par des individus mal intentionnés, n'ont pas occulté les énormes possibilités de progrès personnel et social, d'enrichissement culturel et éducatif offertes par ce réseau. Ainsi, un ensemble de produits commerciaux sur le marché proposent des solutions de filtrage de sites Web. La majorité de ces produits traitent principalement le caractère adulte, alors que les autres caractères, comme le caractère néonazie, raciste et violent, ont été marginalisé. C'est ce dernier caractère qui sera traité dans cet article. La section suivante présente une revue de littérature sur les travaux qui ont porté sur le filtrage de sites web. Nous décrivons dans la section 3 notre approche de classification des sites Web à caractère violent par une analyse du contenu textuel et structurel des pages Web. Les résultats de l'expérimentation de l'approche proposée seront détaillés dans la section 4. La section 5 décrit l'architecture et le principe de fonctionnement de notre solution « WebAngels Filter » ainsi que la comparaison des résultats de ce dernier avec les logiciels les plus connus sur le marché. Enfin une conclusion et quelques perspectives feront l'objet de la dernière section.
Filtrage Web
Plusieurs techniques de filtrage Web ont été proposées pour bloquer les pages Web à caractère litigieux. Parmi ces techniques, on peut citer :
1. La technologie de l'étiquetage PICS 1 (Platform for Internet Content Selection) : c'est un standard de programmation permettant de véhiculer des informations concernant le genre de contenus qui sont représentés sur les sites. En se basant sur ce codage, le navigateur prendra la décision d'afficher ou non une page. Il est à noter que l'efficacité du PICS est relative, en effet, elle dépend fortement de l'engagement des concepteurs des sites à étiqueter leurs pages, en absence d'organisme qui les oblige à le faire.
2. La liste noire : représente un ensemble de sites, motifs génériques, ou domaines à exclure de la navigation. On garde donc la possibilité de naviguer librement d'un site à un autre, ce qui permet de conserver la spécificité de l'Internet, tout en restreignant les risques d'accéder à un site inapproprié. Cependant il est difficile de regrouper tous les sites inappropriés puisque de nouveaux sites apparaissent chaque jour. De ce fait, une liste noire ne peut jamais être exhaustive.
3. La liste blanche : contient l'ensemble de sites sur lesquels la navigation peut avoir lieu. C'est donc un ensemble de sites autorisés. Toute tentative d'accès à n'importe quel site ne figurant pas sur cette liste blanche sera automatiquement refusée. Les éditeurs de logiciels constituent rarement de telles listes blanches, dont l'élaboration est le plus souvent laissée aux parents. Cette solution, qui restreint strictement la navigation à un « jardin d'enfants », peut servir à sécuriser la navigation de très jeunes enfants. Ces listes demandent une vérification régulière par un administrateur, en effet, il arrive souvent que certains sites à contenu tout a fait licite disparaissent en laissant leurs adresses récupérées par des sites inappropriés. Les travaux proposés pour la classification et le filtrage des pages Web à caractère litigieux sont assez nombreux et variés, Cependant la majorité de ces travaux ne traitent que le caractère adulte. Nous proposons dans la section suivante notre approche pour la classification des sites à caractère violent.
Approche proposée
On se place dans le cadre des systèmes automatiques de classification et de catégorisation de sites Web pour proposer une approche de classification des sites Web à caractère violent. En effet, pour classifier les sites à caractère violent et les sites normaux, nous nous sommes basés sur le processus d'extraction des connaissances à partir des données. Le principe général de l'approche de classification est le suivant : Soit S une population de sites concernés par le problème d'apprentissage. A cette population est associé un attribut particulier appelé « attribut classe » noté C. Cet attribut peut avoir deux valeurs, la valeur 0 si le site est violent et 1 si le site est normal. A chaque site s peut être associée sa classe C(s)
Dans notre étude nous cherchons un moyen pour prédire la classe C. La détermination de ce modèle de prédiction est liée à un vecteur de caractéristiques X = (X i ) 1?i?p que nous avons établi a priori. Ce modèle de prédiction permet, pour un site s issu de S, pour lequel nous ne connaissons pas la classe C(s) mais nous connaissons son vecteur de caractéristiques, de prédire sa classe. La figure 1 illustre le schéma général de l'approche de classification proposée. Deux parties peuvent être distinguées : la première est celle de l'apprentissage consacrée à la FIG. 1 -Schéma général de l'approche proposée préparation du modèle de prédiction, la deuxième est celle de la classification des sites Web. Nous signalons que les modèles construits sont sensibles à la qualité des données qui leur sont fournies et nous avons été obligé de faire plusieurs itérations qui ont conduit à affiner la recherche et à élaborer de nouvelles variables ce qui nous a permis d'améliorer les résultats obtenus au fur et à mesure des différentes étapes.
Dans ce qui suit, nous détaillerons les différentes étapes de l'élaboration du modèle de prédiction des sites Web
Préparation des données
La préparation des données pour la phase d'apprentissage consiste à identifier les informations exploitables et vérifier leur qualité et leur efficacité afin de construire une table bidimensionnelle, à partir de notre corpus d'apprentissage. La recherche des attributs les plus informatifs est le point central de cette phase puisque c'est elle qui va conditionner la qualité des modèles établis lors de l'apprentissage, par conséquent cette étape a une influence directe sur la performance du classifieur.
Construction de la base d'apprentissage
Pour que l'apprentissage soit efficace, il faut que notre base d'apprentissage soit repré-sentative de la population et que le nombre des éléments de la base sur lequel il est fait soit important. Cette phase de collecte et de sélection des sites Web constitue une charge de travail considérable vu la diversité et le nombre énorme de sites Web sur Internet. Dans la collecte des sites violents, nous avons essayé d'avoir une base diversifiée en terme de :
-Contenu des sites : la recherche de ces sites s'est focalisée sur la guerre, l'attentat, la torture de prisonniers, l'assassinat, la violence explicite, la création de bombe, les films d'horreur, et les articles qui parlent des effets de la violence. Elle a été effectuée avec le moteur de recherche « Google » en se basant sur un ensemble de mots clés violents. Dans un premier temps on s'est intéressé sur le contenu textuel et ensuite sur les images qui représentent un contenu violent. -Langues traitées : nous avons traité deux langues à savoir la langue anglaise et la langue française. -Structure : certains sites collectés ne contiennent que du texte, d'autres ne contiennent que des images, et la majorité des sites contient les deux. Pour la sélection des sites non violents, nous avons inclus ceux qui peuvent prêter à confusion, en particulier des sites qui luttent contre la violence et des sites de loi, etc. Le reste des sites a été choisi au hasard, on trouve alors des sites de téléchargement de jeu, des sites de codes sources, des sites de bandes dessinées, des sites éducatifs, des sites d'enfance, etc.
Notre base d'apprentissage se compose de 700 sites dont 350 sont violents, et 350 sites non violents.
Analyse du contenu textuel et structurel
L'analyse du contenu textuel et structurel d'une page vise à extraire des variables structurelles et textuelles permettant de mieux discriminer les pages Web violentes de celles inoffensives. Dans cette phase nous nous sommes basé sur les connaissances acquises suite à notre étude des travaux de recherche existants et sur la sélection manuelle des pages Web lors de la construction de notre base d'apprentissage.
La fréquence des mots interdits dans une page Web nous semble la variable la plus discriminante. C'est pourquoi nous proposons d'utiliser deux variables textuelles qui sont n_v_mots, et pourcentage v_mots, qui présentent respectivement le nombre de mots violents qui figurent dans la page et leur pourcentage.
La structure d'une page Web est fondée sur un système de balises (chaînes de caractères délimitées par les symboles < et >) qui décrit leur type (liens hypertexte, images, mots clés, etc.). Glover et al. (2002) ont prouvé que l'analyse de cette structure combinée à une analyse textuelle ne peut qu'améliorer la classification et la description de la page Web. L'analyse de différentes balises nous a permis également d'extraire et de calculer d'autres variables dites structurelles comme n_v_url qui représente le nombre de mots violents dans l'URL, et le n_v_meta qui décrit le nombre de mots violents dans les balises meta. Pour récapituler ce qui précède, le vecteur de caractéristiques que nous avons utilisé pour classifier les pages Web est représenté par le tableau 1.
L'extraction des différentes caractéristiques précédentes nécessite l'analyse du code HTML d'une page Web. Il nous a donc fallu nous doter de : (1) un client HTTP, qui prend en paramètre une URL et renvoie une page de code HTML ; (2) un analyseur syntaxique (parser HTML), qui lit le code de la page, calcule les valeurs associées aux différents critères et stocke ces valeurs dans un fichier, qui sera utilisé par la suite dans une phase d'apprentissage. Il est à noter que le calcul de la majorité des variables a été effectué en se basant sur un vocabulaire de mots violents rassemblés dans un dictionnaire. Ce dernier a été construit manuellement et il comprend des mots clés français et anglais.
La phase suivante de notre démarche est celle de la sélection de variable. Dans cette phase, nous avons déterminé les variables qui ont une influence sur notre problème. La sélection des variables contribue à réduire la taille du problème en isolant les variables exogènes les plus pertinentes. L'élimination des variables inutiles et redondantes permet d'accélérer le processus d'apprentissage et d'augmenter la fiabilité du classifieur obtenu. Afin de sélectionner les variables les plus pertinentes nous avons utilisé une approche de type filtre et plus précisé-ment l'algorithme Relief (Kira et l. Rendel, 1992) vue qu'il est capable de travailler avec des variables bruitées et corrélées et de traiter des données nominales et continues.
Apprentissage supervisé
Il s'agit de trouver une fonction de classement efficace pour prédire les valeurs d'une variable catégorielle, dite à prédire, en fonction des valeurs d'une série de variables continues et/ou catégorielles, dites prédictives. Dans la littérature, il existe plusieurs techniques d'apprentissage supervisé comme les réseaux de neurones (Herault et Jutten, 1994), les graphes d'induction (Zighed et Rakotomalala, 2000), les réseaux bayesiens (Naïm et al., 2004), les machines à vecteurs supports (Schölkopf et al., 1998  (Quinlan, 1986), C4.5 (Quinlan, 1993), IMPROVED C4.5 (Rakotomalala et Lallich, 1998), SIPINA (avec ?=1 et ?=0.2) (Zighed, 1996).
Validation
Après la phase d'apprentissage, nous avons évalué la qualité et la stabilité des modèles obtenus à partir des quatre algorithmes de data mining par le biais de la méthode des taux d'erreur. En effet, il est délicat de formuler des indicateurs généraux pour valider les modèles, et dans la plupart des cas, les chercheurs travaillent sur le taux d'erreur parce qu'il est l'un des meilleurs indicateurs qui soit véritablement comparable d'un algorithme à un autre.
Les 
Expérimentations
Cette section présente les différentes expérimentations réalisées afin de trouver le modèle de prédiction le plus pertinent pour notre application. L'étape de recherche du modèle consiste à extraire la connaissance utile de l'ensemble de données que nous avons collecté dans les phases décrites auparavant. Avant de présenter les séries d'expérimentations et afin de clarifier leurs conditions, nous allons décrire brièvement les conditions d'expérimentation.
Conditions d'expérimentations et techniques de validation
Dans nos expérimentations, nous présentons deux séries de tests : La première est le résultat de notre système de classification sur les 700 sites qui constituent notre base d'apprentissage. Après une première phase d'apprentissage, nous avons évalué la qualité et la stabilité des modèles obtenus à partir des quatre algorithmes de data mining par le biais de la méthode des taux d'erreur. La deuxième série de test est le résultat de notre système de classification sur une base de test composée de 300 sites : 150 à caractère violent et 150 à caractère non violent. Cette série d'expérimentation a été réalisée afin d'éviter le phénomène de « surapprentissage » (overfitting). En effet, il est fréquent que certains classifieurs « apprennent » les données plutôt que le modèle.
Résultats
La figure 2 illustre les différents taux d'erreur correspondants à l'utilisation des quatre algorithmes d'apprentissage. Le meilleur algorithme étant SIPINA que ce soit avec ?=1 ou ?=0.2. Ces résultats peuvent s'expliquer par le fait que cet algorithme tente de réduire les inconvénients des méthodes arborescentes d'une part par l'introduction de l'opération de fusion et d'autre part par l'utilisation d'une mesure sensible aux effectifs. Nous pouvons signaler que, pour la majorité des algorithmes, le taux d'erreur a priori et a posteriori violent sont faibles par rapport à ceux non violent. Cela signifie que ces algorithmes fournissent une décision plus fiable en ce qui concerne la classification des sites violents.
FIG. 2 -Classification des sites Web à caractère violent
Encouragés par les résultats précédents, nous avons alors testé les différents modèles de prédiction, obtenus lors de la phase d'apprentissage, sur notre base de test. Les résultats des différentes expérimentations sont décrits par la figure 3. Un filtre efficace détermine les sites à filtrer et les sites à ne pas filtrer. En d'autres termes, le logiciel identifie tous les sites à caractère violent, disponibles sur le web. Ceci constitue le rappel. Ce qui différencie les bons filtres des moins bons filtres est leur capacité à correctement distinguer les sites trouvés. Les sites contenant le mot « violent » ne doivent pas tous être filtrés. L'accès aux sites violents doit être bloqué, mais les sites qui luttent contre la violence doivent rester accessibles. Cette capacité à distinguer les différents sites constitue la précision. Le rappel et la précision sont inversement proportionnels. Ainsi, si un filtre est capable d'identifier tous les sites à contenus inappropriés, l'accès à certains sites inoffensifs sera également bloqué. Mais, si le logiciel est très spécialisé et capable de trouver uniquement des contenus préjudiciables sur un sujet spécifique, de nombreux contenus inadéquats pourront toujours être consultés. La stratégie que nous avons choisie consiste à utiliser le modèle qui assure un meilleur compromis entre le rappel et la précision. Compte tenu des résultats obtenus dans les phases d'apprentissage et de test, nous avons opté pour l'utilisation du modèle de prédiction produit par l'algorithme SPINA (?=1). -récupérer le code source HTML de la page demandée ; -vérifier si l'URL appartient à une liste noire, et sinon analyser le code ; -déclarer cette page autorisée ou interdite ; -mettre à jour la liste noire ; -mettre à jour l'historique de navigation ; -afficher ou non la page. La figure 4 résume le fonctionnement de notre logiciel.
Comparaison avec quelques produits commerciaux
Afin de mieux évaluer notre modèle de prédiction produit par l'algorithme SIPINA (?=1), nous avons mené une étude comparative de notre solution avec quatre produits commerciaux, à Ces logiciels sont censés être capable de filtrer des sites à caractère litigieux, et ils ont été paramétrés de façon à filtrer que le caractère violent. Notre objectif, ici, est d'évaluer si nos résultats théoriques avaient un sens en les comparant aux résultats réels du logiciel. Cette étude a été effectuée sur notre base de test, totalement indépendante de celle utilisée dans la phase d'apprentissage. La figure 5 montre la performance de « WebAngels Filter » par rapport aux logiciels existants sur le marché avec un taux de classification égale à 81%. Ceci peut être expliqué par le fait que la majorité de ces logiciels traitent principalement le caractère pornographique des sites Web, alors que le caractère violent à été omis , et donc échappe à leurs filtres.
Conclusion et perspectives
Ce papier présente une solution de classification et de filtrage des sites Web à caractère violent par un apprentissage qui s'appuie sur plusieurs algorithmes de data mining avec non seulement une analyse du contenu textuel mais aussi du contenu structurel. L'étude compa-FIG. 5 -Etude comparative de l'approche proposée avec quelques produits du marché.
rative de notre solution avec les logiciels du marché les plus connus, sur notre base de test, montre la performance de notre système.
Ces résultats encourageants nous incitent à approfondir nos travaux de classification et filtrage de sites Web par une analyse conjointe de plusieurs modalités et à les appliquer à d'autres problèmes comme par exemple le filtrage de sites adultes, racistes, etc. D'autres pistes d'amé-lioration concernent l'élaboration du dictionnaire des mots clés qui a joué un rôle central dans les performances de « WebAngels Filter ». Or, l'élaboration de ce dictionnaire a été très laborieuse car manuellement améliorée étape après étape, et elle n'a vraisemblablement possible que grâce à la compréhensibilité des modèles obtenus par les techniques d'extraction de connaissance. Il serait donc intéressant d'automatiser par l'apprentissage à partir d'un corpus la construction d'un tel dictionnaire. Enfin, penser à une manière d'intégrer le traitement de l'aspect visuel dans les modèles de prédictions afin de remédier aux difficultés de classifier les sites Web violentes qui ne comprennent que des images.

Introduction
A l'heure actuelle, l'Internet est devenu une des sources d'information les plus importantes dans des nombreux domaines, comme celui de la santé. Afin de faciliter l'accès aux informations médicales disponibles en ligne, l'élaboration de nouveaux instruments et méthodes de recherche s'avère nécessaire. Le projet CISMeF 1 (Catalogue et Index des Sites Médicaux Francophones) Darmoni et al. (2000) est le catalogue de santé lancé par le CHU de Rouen en 1995. L'objectif du catalogue est de décrire et de classer les principales ressources (documents sur le Web) de santé en français pour aider les utilisateurs dans leur recherche d'information médicale de qualité disponibles en ligne.
Des efforts considérables ont été engagés par l'équipe CISMeF afin de développer des architectures d'indexation automatique, et des avancements significatifs ont été présentées Né-véol et al. (2006). Cependant, l'indexation automatique a des limites et un des principaux problèmes reste la difficulté d'indexation des médias non textuels, comme les images.
Les travaux précédents
Il existe deux approches principales pour rechercher des images : en utilisant le contenu ou en utilisant le contexte de l'image (les régions textuelles associées aux images). Initialement, les images étaient indexées à l'aide des index reposant sur des mots clés Frankewitsch et Prokosch (2001). Au cours des années, des méthodes basées sur le contenu visuel des images ont été proposées pour l'annotation, l'indexation et la recherche des images médicales non annotées Lehmann et al. (2003)  Müller et al. (2003). Dernièrement, nous avons remarqué un intérêt croissant pour les architectures qui proposent des descriptions d'images combinant les deux approches Deselaers et al. (2005)  Besancon et Millet (2006).
Dans cet article, nous proposons une architecture combinant le contenu et le contexte pour l'annotation des images médicales, le but étant d'extraire au mieux des informations bien défi-nies. Etant placés dans le contexte réel d'un catalogue en ligne, nous avons créé une application capable de traiter tous les aspects techniques de l'extraction d'information à partir de différents formats (cryptées, non structurées). En même temps, nous avons cherché à minimiser la sensibilité de notre système aux variations de contenu et de qualité spécifiques à l'Internet.
L'architecture du module MedIC
FIG. 1 -Le module MedIC
Dans le contexte du projet d'indexation automatique des documents de santé développé par l'équipe CISMeF, le module MedIC (Medical Image Categorization) a comme tâche l'annotation des images médicales. Le but du module est de localiser, d'extraire et d'annoter les images médicales à partir d'un document donné. Le MedIC a été conçu pour rechercher plusieurs types d'informations médicales : la modalité médicale, la région anatomique, l'angle de vue d'acquisition et la pathologie. Ces informations permettront aux utilisateurs du catalogue CISMeF, de formuler des requêtes orientées vers l'image, telles que <Trouve-moi les documents contenant des images ANGIOGRAPHIQUES (modalité) présentant un EMBOLISME (pathologie) PULMONAIRE (région anatomique)>. L'architecture de module MedIC est présentée dans la figure 1. Pour extraire l'information médicale, nous traitons : le contenu visuel des images Florea et al. (2006), les annotations marquées directement sur l'image Florea et al. (2005), et les régions textuelles associées aux images.
Même si la source a été prouvée comme étant précise, elle est rarement utilisable en ligne à cause du manque d'annotations marquées sur la majorité des images publiées sur l'Internet. Dans cet article, nous évaluons la pertinence de l'information extraite à partir des sources et et le gain de performance obtenu en combinant les deux approches.
Les images et les régions textuelles associées
Les ressources médicales (i.e. documents) contiennent des quantités considérables d'informations relatives aux images. Habituellement, il y a deux régions textuelles associées aux images : la légende -courte et placée près de l'image et le paragraphe plus long et plus détaillé.
Pour les expériences d'extraction/annotation des images que nous présentons dans cet article, nous avons créé une base de 657 enregistrements, extraits automatiquement à partir des documents classés par CISMeF. Les images représentant les six principales modalités médi-cales : l'angiographie, l'échographie, l'imagerie à résonance magnétique, la radiographie standard, la tomographie par l'ordinateur (scanner) et la scintigraphie. Chaque modalité est liée à une hiérarchie de régions anatomiques et sous anatomiques. Pour nous permettre l'évaluation automatique des performances d'annotation, chaque enregistrement est manuellement annoté avec la modalité médicale et la région anatomique capturée par l'image.
L'annotation basée sur le contenu visuel (V)
La catégorisation des images médicales basée sur le contenu d'image, peut être un outil d'annotation très performant, dans le contexte de la recherche d'images dans des bases non annotées. Notre approche de catégorisation est basée sur la classification supervisée des représentations numériques des images, reposant sur des attributs de texture (e.g. matrices de co-occurrence, dimension fractale, les réponses aux filtres de Gabor, et autres) et statistiques. Une analyse en composantes principales (PCA) est employée pour réduire la dimensionnalité de l'espace des attributs. Finalement, un classifieur SVM (Support Vector Machines) est employé par MedIC, pour la projection des données de test dans les catégories correspondantes. Plus de détails sur l'approche « visuelle », sont disponibles dans Florea et al. (2006).
Les performances d'annotation de la modalité sur les 657 enregistrements de notre base sont présentées dans le tableau 1. Pour chaque information extraite (modalité et région anatomique), nous présentons les résultats dans la forme des précision (p), rappel (r) et f-mesure (f m ), des mesures souvent utilisées en « recherche d'informations ». 
TAB. 1 -Décision visuelle (V)
Les résultats d'annotation que nous avons obtenus en utilisant le contenu des images sont plus faibles que prévu. En utilisant la même architecture, nous avons obtenu des résultats bien meilleurs au cours de la campagne d'évaluation CLEF Florea et al. (2006. Les résultats inferieurs sont dus principalement au fait que, pour les expérimentations présentées dans cet article, nous avons fait l'apprentissage SVM sur une base d'images de qualité sensiblement différente (meilleure) que notre base de 657 images de test. Généralement, avec des images extraites à partir des documents en ligne nous obtenons des résultats inferieurs, à cause de la faible résolution et compression élevée de ces images.
L'annotation basée sur des informations contextuelles (T)
Cette deuxième approche vise à traiter l'information portée par les régions textuelles associées aux images (légendes et paragraphes), pour extraire les mêmes annotations que pour l'approche visuelle : les modalités médicales et les régions anatomiques.
La première étape est de modéliser l'information à extraire sous forme de dictionnaires. Pour chaque information, des dictionnaires DELA ont été créés, basés sur la terminologie MeSH 2 et des termes et synonymes CISMeF. Les dictionnaires devraient nous permettre de localiser les termes MeSH, sous les diverses formes qu'ils peuvent prendre en langage naturel. Les dictionnaires ont été manipulés en utilisant l'environnement linguistique INTEX/NOOJ 3 . Les termes obtenus suite à l'application des dictionnaires, sont utilisés pour l'annotation des images, en employant une décision par vote majoritaire. Nous avons implémenté plusieurs stratégies de traitement des deux zones textuelles que nous disposons :
Légendes et paragraphes traitées ensemble T(L+P) -les légendes et les paragraphes sont considérés comme ayant la même importance.
Priorité aux légendes T(L en priorité) -nous traitons la légende en priorité, car souvent, la légende contient des informations plus précises et succinctes que le paragraphe.
Approche voisinage (pour les régions anatomiques) T(L en priorité+voisins) -Les confusions les plus courantes concernent les régions anatomiques voisines (e.g. bras/coude, avant-bras/main). Par conséquent, nous avons ajouté des conditions supplémentaires, en défi-nissant une table de voisinage. Même si cette stratégie de voisinage semble adaptée, les résul-tats concrets sont moins satisfaisants. 
TAB. 2 -Résultats de décisions modalité et régions anatomiques (T)
Les résultats pour l'annotation des modalités et des régions anatomiques sont présents dans le tableau 2. Une analyse détaillée nous a montré que les principaux responsables des erreurs sont : les légendes communes aux plusieurs images, les extractions erronées des couples imagetexte ou les erreurs grammaticales dans le texte original.
Il est important de remarquer que même si cette approche est incapable de proposer une décision pour toutes les images (faible rappel), elle est très précise (surtout pour les modalités).
Fusion des décisions (F)
Dans cette section, nous allons évaluer le gain de performance obtenu après la combinaison des sources et Les résultats obtenus sont présentés dans le tableau 3.
Approche prioritaire texte F(T en priorité) -la décision textuelle est traité en priorité (avec aussi des légendes prioritaires), pour exploiter la bonne précision de cette approche. Pour pondérer les informations, nous avons utilisé les rangs cumulés de chaque déci-sion pour les deux sources.
Approche équilibré F(T+V) -nous utilisons les mêmes critères de rangs cumulés. Par rapport à l'approche précédente, nous notons des faibles améliorations.
Approche équilibrée, avec la modalité décidée F(T+V)mod. Motivées par les bons ré-sultats de la décision sur la modalité, nous avons proposé une troisième approche où nous extrayons les annotations sur les régions anatomiques en utilisant la décision sur la modalité. Cependant, dans la pratique, cette approche est moins efficace.
Les approches de fusion (F) que nous avons essayées présentent toutes des améliorations de performance comparées aux méthodes reposant sur le contenu visuel (V) et sur le texte associé aux images (T), considérées séparément. Même avec les résultats plus faibles de la décision visuelle, la f-mesure globale, après la fusion, affiche une amélioration significative. Combinant les deux décisions, nous perdons généralement un petit pourcentage sur la préci-sion moyenne (comparée à la décision textuelle), mais nous obtenons des taux de rappel et f-measure bien plus élevés. 
Conclusion
Le but du module MedIC est de permettre l'extraction et l'annotation automatique des images médicales extraites à partir des documents de santé complexes. L'objectif est de fournir des annotations médicales précises pour l'indexation des documents et de leurs images attachées.
La catégorisation des représentations visuelles peut fournir des annotations précises pour des images médicales. En même temps, elle est dépendante de l'existence des images d'apprentissage déjà annotées et très sensible aux variations de qualité des images. L'approche textuelle que nous présentons dans cet article peut fournir des annotations médicales précises pour les images, mais, à son tour, elle est dépendante des dictionnaires linguistiques.
En perspective, nous allons développer cette architecture pour traiter des informations autres que les modalités et les régions anatomiques. L'architecture que nous avons présen-tée dans cet article est extensible en définissant des dictionnaires additionnels ou des classes d'apprentissage visuelles supplémentaires.
L'architecture présentée a été conçue pour être intégrée dans le module d'indexation automatique des documents du catalogue CISMeF, pour offrir aux utilisateurs un meilleur (et plus complet) outil de recherche d'information médicale sur Internet.

Introduction
Les entrepôts de données centralisent des données provenant de différentes sources pour répondre aux besoins d'analyse des utilisateurs. Le schéma de l'entrepôt est défini avec l'objectif d'analyser des mesures qui caractérisent des faits, en fonction de dimensions qui peuvent être organisées sous forme de hiérarchies, composées de différents niveaux de granularité, déterminant la manière selon laquelle sont agrégées les données.
Pour concevoir le schéma d'un entrepôt, nous distinguons dans la littérature différents types d'approches : celles guidées par les sources de données (Golfarelli et al., 1998), celles guidées par les besoins d'analyse (Kimball, 1996) et les approches mixtes qui combinent les deux approches précédentes, mettant en adéquation des schémas candidats générés à partir des sources de données avec les besoins d'analyse exprimés par les utilisateurs (Nabli et al., 2005).
Cependant, en pratique, les sources de données, tout comme les besoins d'analyse sont amenés à évoluer. Dans la littérature, il existe deux alternatives qui permettent l'évolution de schéma nécessaire suite à ces modifications. D'une part la mise à jour de schéma qui est réalisée grâce à des opérateurs qui font évoluer un schéma donné (Hurtado et al., 1999). D'autre part, la modélisation temporelle qui consiste à garder la trace de ces évolutions en utilisant des labels de validité temporelle. Ces labels sont apposés soit au niveau des instances (Bliujute et al., 1998), soit au niveau des liens d'agrégation (Mendelzon et Vaisman, 2000), ou encore au niveau des versions du schéma (Morzy et Wrembel, 2004). L'inconvénient de ce type de solutions est la nécessité d'une réimplémentation des outils d'analyse, de chargement, ... afin de gérer les particularités de ces modèles.
Les deux alternatives sont intéressantes pour répondre au problème de l'évolution de schéma suite à une modification dans les sources de données, puisque ce sont des solutions techniques devant être mises en oeuvre par l'administrateur. Cependant, elles n'impliquent pas directement les utilisateurs dans le processus d'évolution. De ce fait, elles n'apportent pas de solution au problème posé par l'émergence de nouveaux besoins d'analyse exprimés par les utilisateurs. Or, au cours de l'utilisation de l'entrepôt, de nouveaux besoins apparaissent étant donné que (1) il est difficile de déterminer de façon exhaustive les besoins d'analyse pour l'ensemble des utilisateurs lors de la conception de l'entrepôt, (2) ces besoins dépendent également des propres connaissances des utilisateurs, (3) il est impossible de déterminer les besoins futurs.
Dans cet article, nous proposons alors une approche originale, qui s'inscrit dans l'approche de mise à jour de schéma, impliquant les utilisateurs dans le processus d'évolution de l'entrepôt, dans le but de leur fournir des analyses personnalisées en fonction de leurs propres connaissances du domaine et de leurs besoins. Les connaissances utilisateurs concernent plus précisément la définition de nouvelles données agrégées et sont représentées sous la forme de règles de type «si-alors». Ces règles, dites d'agrégation, sont ensuite utilisées pour générer de nouveaux axes d'analyse en créant dans les hiérarchies de dimension de nouveaux niveaux de granularité. Notre approche est fondée sur un modèle d'entrepôt de données évolutif à base de règles nommé R-DW (Rule-based Data Warehouse), dont nous proposons ici une formalisation. Pour valider notre approche, nous avons développé une plateforme baptisée WEDriK 1 (data Warehouse Evolution Driven by Knowledge) et avons appliqué notre approche aux données bancaires de LCL 2 . Néanmoins, cette partie n'est pas développée ici en raison du manque de place et peut être consultée dans Favre et al. (2006). Dans la suite de cet article, nous introduisons tout d'abord dans la Section 2 un exemple simplifié motivant notre approche orientée utilisateurs. Puis, nous présentons dans la Section 3 notre approche ainsi que le principe du modèle R-DW sur lequel elle se base. Nous proposons ensuite la formalisation de ce modèle dans la Section 4. Enfin, nous concluons et indiquons les perspectives de ce travail dans la Section 5.
Exemple introductif
Pour illustrer notre approche de modélisation d'entrepôt de données évolutif à base de règles, nous utilisons le cas réel de la banque LCL. Le PNB annuel (Produit Net Bancaire) correspond à ce que rapporte un client à l'établissement bancaire. Cette mesure est analysée selon les dimensions CLIENT, AGENCE et ANNEE ( Supposons qu'un utilisateur veuille analyser les données selon le type d'agence ; il sait qu'il en existe trois : type «étudiant» pour les agences ne comportant que des étudiants, type «non résident» lorsque les clients ne résident pas en France, et le type «classique» pour les agences ne présentant pas de particularité. Ces informations n'étant pas présentes dans l'entrepôt, il est impossible pour lui d'obtenir une telle analyse. Notre objectif est donc de proposer à l'utilisateur d'intégrer dans le schéma de l'entrepôt sa connaissance sur les types d'agence pour créer le niveau de granularité TYPE_AGENCE (Figure 2b).  
Définition 1. Univers de l'entrepôt
est l'ensemble des   . Remarque : pour des raisons de simplification, nous supposons ici que chaque niveau de granularité est caractérisé par un seul attribut généré. Exemple 4. Dans l'exemple de la section 2, 
Définition 3. Hiérarchie de dimension et niveau de granularité
'non résident' , tel que : Une règle d'agrégation est une règle de type «si-alors». La conclusion de la règle (clause «alors») définit la valeur de l'attribut généré. La prémisse de la règle (clause «si») est basée sur une composition de (conjonctions V disjonctions) des termes de règles :  
Conclusion
Dans cet article, nous avons proposé une approche originale qui exploite les connaissances utilisateurs pour faire évoluer le schéma de l'entrepôt, afin d'obtenir des analyses personna-

Introduction
roduction de con ion de connaissances est en partie une retombée des échanges au sein de la communauté.
Dans une économie de plus en plus fondée sur la connaissance, au point que l'importance du facteur de production « connaissance » augmente par rapport aux facteurs de production traditionnels, on a assisté à l'émergence de nombreux concepts afin de mieux gérer ces connaissances (en tant que ressource centrale) existant au niveau de l'entreprise.
A l'heure actuelle, on s'intéresse fortement au concept de communautés de pratique car, par définition, elles constituent un lieu d'échange et de partage de connaissances de plus en plus utiles voire indispensables pour les entreprises. Wenger (1998)  Brown et Duguid (1991 voient dans ces communautés un lieu privilégié pour la création, la maintenance et la rep naissances. Cependant, notre revue préliminaire de la littérature a montré qu'il existe très peu d'articles qui se consacrent explicitement au processus de création de connaissances au sein des communautés de pratique. Et pourtant, la capacité de créer de nouvelles connaissances et de les transférer au sein d'une organisation est considérée comme étant la base d'un avantage concurrentiel (Inkpen, 1996). Dès lors, dans notre travail de recherche, les communautés de pratique seront analysées sous l'angle d'une théorie qui nous a semblé constructive pour traiter ce processus : la théorie de l'émergence, dans la mesure où le processus de créat Dans un premier temps, une revue de la littérature sur les communautés de pratique sera faite et nous proposerons une définition de travail pour les communautés de pratique. Dans un deuxième temps, nous considérerons les fondements principaux de la théorie de l'émergence. Ensuite, nous analyserons l'application aux communautés de pratique. La dernière partie décrit une première proposition de modélisation pour un support de l'émergence de connaissances. Nous conclurons en faisant le bilan de notre travail et indiquant les limites de notre approche ainsi que les perspectives de recherche.
considère que les communautés de pratique sont les « ressources en connaissances les plus versatiles et dynamiques des entreprises et qu'elles forment la base de la capacité cognitive et d'apprentissage des organisations ».
Les communautés de pratique
Premières définitions
La notion de communauté de pratique s'est développée à partir des travaux sur l'apprentissage en situation (Situated Learning) de John Seely Brown, Paul Duguid, Jean Lave, Lucy Suchman et leurs collègues du Palo Alto Institute for Research on Learning dans les années 1980. Selon la théorie de l'action située, la connaissance est étroitement dépen-dante du contexte dans lequel l'action se déroule, l'environnement de l'action constituant donc une ressource déterminante pour les processus cognitifs. Pour tout novice, l'apprentissage d'un métier passe par la participation aux pratiques socioculturelles d'une communauté de praticiens. C'est de ces pratiques concrètes que les capacités de résolution de problème des individus émergent. Dès lors, la connaissance reste pour une large part tacite et contextualisée (Zacklad, 2003).
Plusieurs définitions ont été proposées pour rendre compte du phénomène mais toutes ne correspondent pas à notre idée du concept. La première définition a été avancée par Lave et Wenger dans leur ouvrage fondateur en 1991 (Lave et Wenger, 1991  (Wenger et al, 2002) ou même plus court comme « groups of people informally bound together by shared expertise and passion for a joint enterprise » (Wenger et Snyder, 2000). Deux points complètent la première définition : le fait que les personnes sont reliées de façon informelle entre elles et le partage de pratiques communes.
Alors que les premiers travaux concernant les communautés de pratique se focalisaient sur des relations face-à-face, par la suite les études tiennent souvent aussi compte des relaier (2004). tions virtuelles, c'est-à-dire des échanges à distance en mode synchrone ou asynchrone, supportés par les technologies de l'information et de la communication.
McDermott définit : « A community of practice is a group that shares knowledge, learns together and creates common practices. » (McDermott, 1999a(McDermott, , 1999b 
D'autres types de communautés en sciences de gestion
Après son apparition dans la littérature au début des années 1990, la notion de communautés de pratique a vite rencontré un vif intérêt tant du côté académique que managérial. Par la suite, beaucoup de travaux liés à ce concept ou des concepts similaires ont été ou sont encore publiés. Certains auteurs, appliquent le concept « communauté de pratique », à des groupes dont les caractéristiques sont éloignées de celles des communautés de pratique dans le sens de Lave et Wenger, ou proposent même des extensions du concept dans des directions opposées à celles-ci (Vaast, 2002, Guérin, 2004.
En plus de la notion de communauté de pratique, on trouve actuellement une multitude d'autres termes sans que ceux-ci constituent pour autant des concepts scientifiques indépen-dants.
2 Bien que ces appellations sous-entendent des distinctions a priori claires entre les notions, leurs applications empiriques montrent des proximités et distances entre elles encore peu exploitées par les auteurs ; ceci les rend presque substituables entre elles et ne favorise pas la compréhension de leurs contributions propres (Vaast, 2002). Parmi toutes ces notions, les communautés de pratique représentent le concept de loin le plus développé, concept qui est maintenant généralement reconnu dans la littérature académique ainsi que managériale.
Dimensions retenues
Les définitions ont fait ressortir les dimensions qui composent une communauté de pratique mbres aux activités de r la base d'une particip . Ces dimensions nous serviront à proposer une définition de travail. La première dimension retenue est la participation volontaire des me la communauté. Contrairement aux entités d'organisation formelle telle que l'équipe de travail où c'est la tâche ou le projet qui construit le groupe, les membres d'une communauté de pratique sont tenus par un intérêt commun dans un champ de savoir.
La deuxième dimension mise en avant est le concept de partage. Su ation volontaire, les membres d'une communauté de pratique échangent et partagent des connaissances non seulement explicites, mais aussi et plus particulièrement implicites : des 2 sition… Ces autres « notions voisines » sont par exemple : communauté de métier, communauté épistémique, communauté virtuelle, communauté en ligne, communauté distribuée, communauté d'intérêt, communauté d'apprentissage, communauté d'action, communauté de po expériences, des pratiques des outils, des modèles, etc. A travers cet échange, les membres développent un langage commun et une compréhension partagée de leur environnement professionnel.
La troisième dimension évoquée fait référence à la place des communautés de pratique par ération est le mode de communicat différents points théoriques abordés nous amènent à formuler une proposition de dé-fini de personnes liées entre elles par le par rapport à l'organisation formelle. En effet, les membres d'une communauté de pratique peuvent être issus du même service d'une organisation, de différents services au sein de la même organisation, mais aussi de différentes organisations.
Enfin, la dernière dimension que nous prenons en consid ion. Les membres d'une communauté de pratique communiquent entre eux face-à-face ou virtuellement, c'est-à-dire à distance en mode synchrone ou asynchrone, en utilisant les technologies de l'information et de la communication. A l'heure de la mondialisation et des communautés distribuées, il importe de tenir compte des modes de communication à distance.
Les tion de travail pour les communautés de pratique :
« Les communautés de pratique sont des groupes tage de pratiques communes. Sur la base d'un échange volontaire et motivées par un intérêt commun dans un champ de savoir, les personnes appartenant à une communauté partagent des connaissances avant tout implicites, développant peu à peu un langage commun et une identité communautaire. Dans ce but, les membres de la communauté de pratique issus de la même organisation ou bien d'organisations différentes utilisent des modes de communication face-à-face ainsi qu'à distance. »
réductibilité et l'imprédictibilité des propriétés émergentes.
3 Selon cette définition, un phé-nomène est émergent si : -Il y a un système d'entités (ensemble d'agents) en interaction dont la description des états et de la dynamique se fait dans un vocabulaire ou une théorie D et n'est pas exprimée dans les termes du phénomène émergent à produire ; -La dynamique des agents en interaction produit un phénomène global qui peut être un processus, une structure stable, une trace d'exécution ou n'importe quel invariant statique ou dynamique ; -Ce phénomène global peut être observé et décrit soit par un observateur extérieur soit par les agents eux-mêmes dans un vocabulaire ou une théorie distincte, c'est-à-dire en des termes distincts de la dynamique sous-jacente.
L'application de la théorie de l'émergence aux communautés de pratique
Dans le domaine de la gestion de connaissances, on distingue souvent les connaissances individuelles, les connaissances organisationnelles et la relation entre les deux. Lors de la résolution d'un problème, les capacités de tous les individus sont utilisées pour agir ensemble et on constate que cette interaction fait que la connaissance organisationnelle n'est pas que le rassemblement des connaissances individuelles (Probst et al, 2003). La totalité des connaissances individuelles et organisationnelles, y compris les données et les informations sur lesquelles ces connaissances sont fondées, constituent la mémoire organisationnelle (Corporate Memory). L'organisation peut recourir au contenu de cette mémoire lors de la résolution d'une tâche.
Dans le cas des communautés de pratique, analogue à la situation décrite dans l'organisation, les connaissances existant au niveau de la communauté en tant que collectif font bien plus que la somme (dans le sens d'une simple juxtaposition) des connaissances de tous les membres (Brown et Duguid, 1991). Les communautés de pratique possèdent une méthode particulière de résolution de problème. En plus de l'échange de connaissances explicites, les membres de la communauté confrontés à un problème commencent à faire des récits plus ou moins circonstanciés de leurs expériences du passé relatives au problème actuel (« narration » ou « storytelling »). En transmettant des connaissances sur la base de récits, la communauté essaie d'arriver à une solution en accumulant les expériences de ses membres. Il s'agit d'un mode de communication tout à fait humain. Comme le soulignent Boland et Tenkasi (1995), « human cognition operates almost continuously in narrative, storytelling mode ». Par la confrontation de plusieurs points de vue, de différents angles d'attaque, tout en partageant des pratiques communes, les membres de la communauté mettent des connaissances existantes dans de nouveaux contextes et créent ainsi de nouvelles connaissances. Il y a donc ici émergence de connaissance grâce à la communication et l'interaction entre les membres de la communauté de pratique.
En quelque sorte, l'entreprise commune 4 et le répertoire partagé 5 constituent des résultats émergents des communautés de pratique. L'entreprise commune émerge d'un processus collectif permanent de négociation qui reflète la complexité de la dynamique de l'engagement mutuel des membres de la communauté. Elle désigne l'identité commune : « What the community is about ».
Le répertoire partagé émerge également grâce à la communication et l'interaction entre les membres de la communauté de pratique. Il s'agit de l'ensemble de ressources accumulées propres à une communauté de pratique particulière. Par la suite, ces ressources regroupant des supports physiques tels que des prototypes, des routines, des mots, des gestes, des symboles, des outils et des concepts etc. sont mises en commun pour favoriser la poursuite des buts à atteindre. De cette façon, les individus peuvent se servir du répertoire partagé déjà acquis pour ensuite créer de nouvelles connaissances.
Ces phénomènes émergent donc au niveau macro (la communauté comme ensemble) à partir d'une interaction entres les membres de la communauté de pratique (niveau micro).
D'ailleurs, il y a ici un parallèle entre le phénomène de l'émergence et les communautés de pratique : les deux ne se pilotent pas, mais on peut tenter de créer des conditions qui sont favorables au développement des interactions et par là favorables à l'émergence d'éléments nouveaux. Cette notion de faire-émerger nous amène à l'énaction (Varela 1989) qui par le poids qu'elle donne à l'action pourra nous offrir un éclairage complémentaire que nous pensons étudier dans une prochaine étape.
Proposition de modélisation
Avant de procéder à une proposition de modélisation pour un support de la création et de l'émergence de connaissances, il convient de considérer les caractéristiques des communautés de pratique pour en déduire les différents éléments à prendre en compte.
Premièrement, la communication et les interactions personnelles entre les membres d'une communauté sont essentielles pour le partage de connaissances. Il importe de bien connaître les autres membres avec lesquels on interagit, d'avoir des informations sur eux : l'échange de connaissances dans une communauté de pratique n'est pas seulement le ramassage et la mise à disposition anonyme de connaissances ; au contraire, l'échange se situe entre un producteur et un consommateur de connaissances, entre un auteur et un lecteur, en d'autres termes, comme Zacklad (2004) le formule, entre une situation source et une situation cible. Ainsi, le lien avec l'auteur est surtout important pour l'interprétation des énoncés souvent subjectifs. Un support des communautés de pratique devrait donc permettre de gérer les profils des membres ce qui permet de déceler « qui sait quoi » dans une communauté de pratique.
Il est à noter qu'en instituant ces profils, notre modélisation part de l'hypothèse que les connaissances ne sont pas distribuées uniformément parmi les membres 6 et que tous les acteurs d'une communauté de pratique ne sont donc pas égaux. En effet, il existe selon  Dans ce cadre, l'entreprise commune peut dépasser les frontières d'une société et être entendue comme un projet partagé par les membres de la communauté.
5
Ces notions sont issues des travaux de Wenger (1998). Selon lui, les communautés de pratiques sont caractérisées par trois dimensions : un engagement mutuel, une entreprise commune et un répertoire partagé.
« A community's knowledge is not held equally by all, but shared differentially across the community as a whole, though it is made available to all. », cf. Brown et Duguid (2001).
ger plusieurs niveaux de participation au sein d'une communauté. L'idée centrale du concept de « Legitimate Peripheral Participation » est que les personnes acquièrent les connaissances nécessaires pour avoir une performance compétente en devenant « insiders » ou membres légitimes de la communauté en question (Lave et Wenger, 1991, Lorenz, 2001). Au début, les nouveaux entrants se trouvent à la périphérie de la communauté, observant le comportement des membres anciens. En participant aux discussions, ils acquièrent peu à peu le langage et la conception du monde de la communauté. Avec le temps, ils deviennent des membres expérimentés capables de transférer des connaissances aux débutants. Le but est donc que beaucoup de membres profitent des contributions des experts de la communauté.
Une deuxième caractéristique concerne les relations entre différentes communautés de pratique. Il peut y avoir des problèmes si une communauté se concentre trop sur elle-même et s'isole ainsi. Dans ce cas, il faut encourager l'échange inter-communautaire dans lequel les communautés de pratique présentent leurs idées dans un contexte social plus vaste. Il est possible de surmonter les barrières entre des communautés à travers des « Knowledge Brokers », c'est-à-dire des individus qui appartiennent à plusieurs communautés, ou bien à travers des « objets frontière » (Boundary Objects), des objets qui sont intéressants pour chaque communauté impliquée, mais qui sont utilisés différemment. Dans une modélisation, il faut dès lors tenir compte du fait qu'un individu ou bien un objet puisse appartenir à plusieurs communautés de pratiques.
Enfin, le modèle doit permettre l'échange des connaissances implicites ainsi qu'explicites. Dès lors, des composants sont à prévoir pour sauvegarder la partie formalisable du répertoire partagé et pour faciliter la communication des connaissances implicites. Vu sous un autre angle d'attaque, il faut supporter la communication directe entre les membres, mais aussi la communication indirecte, c'est-à-dire proposer des possibilités pour la publication et la recherche d'informations.
Basé sur ces réflexions préliminaires, nous proposons ci-dessous un modèle générique de système pour un soutien des communautés de pratique. Dans la partie suivante seront décrits les différents composants et les interactions entre eux.
Interface A travers l'interface, le membre utilise les différents services et fonctionnalités proposés par les autres composants du système. Il a non seulement accès d'une manière passive au système, mais il peut aussi contribuer activement, par exemple en mettant à jour des informations, en ajoutant de nouvelles informations ou en communiquant avec d'autres membres.
Profils des membres
Ce composant contient une liste de tous les membres de la communauté de pratique qui sont enregistrés comme utilisateurs. Surtout dans des communautés de pratique virtuelles, en l'absence de contacts face-à-face, la mise à disposition des profils est une mesure pour établir de la confiance entre les membres.
Il y a trois possibilités pour l'établissement et la modification (la mise à jour) de ces profils. Premièrement, c'est l'utilisateur lui-même qui entre des informations dans le système. Cela concerne surtout des informations personnelles (nom, prénom, adresse etc.). De surcroît, l'utilisateur pourra donner des informations concernant sa qualification et ses intérêts. En ce qui concerne les intérêts du membre, on pourra cependant envisager de les compléter automatiquement.
Deuxièmement, le profil peut être modifié et complété automatiquement, c'est-à-dire de façon assistée par le système. Dans ce cas, le système surveille le comportement et les activités du membre au sein de la communauté : quels autres membres il contacte le plus souvent, RNTI -X -quels items de la base de connaissances il consulte ou même ajoute et comment il les évalue (soit explicitement en donnant une « note », soit implicitement en le regardant plus ou moins souvent ou plus ou moins longtemps), s'il participe vivement aux discussions du forum etc.
Enfin, le profil d'un individu peut -de façon limitée -être modifié par les autres membres si ceux-ci évaluent les contributions de l'individu aux activités de la communauté, par exemple relatives aux articles qu'un membre particulier a rédigés.
En principe, le composant gérant les profils des membres possède trois fonctions.  
FIG. 1 -
Modèle du système
e système proposé permet une « personnal à-dire donne à chaque individu le sys L isation », c'estla possibilité de modifier des paramètres traduisant ses intérêts ; par la suite, ce membre recevra une offre adaptée à ses besoins au niveau de messages et du contenu de la base de connaissances. Il s'agit d'une personnalisation explicite, initiée par le membre lui-même.
En plus, on pourrait envisager une personnalisation automatique et implicite où c'est tème qui fouille et exploite le profil et les activités d'un membre pour en déduire des informations qui complètent le profil. Ce principe est connu du domaine des systèmes de filtrage adaptatif (ou système de recommandation), dont l'objectif principal est d'envoyer des informations pertinentes aux utilisateurs tout en s'adaptant en permanence à leur besoin d'information.
La fonction « Matching » consiste à proposer des partenaires potentiellement intéressants pour un membre concerné, et ce, soit sur demande du membre, soit automatiquement par le système. Pour faire cela, le système compare les profils des membres et cherche en fonction de la situation particulière des similitudes ou bien des profils « complémentaires » qui enrichissent et comblent en quelque sorte les lacunes d'autres profils.
Dans les profils des membres, le système gére également les droits de chaque individu quant à l'accès aux informations et aux modifications. Alors que la plupart des membres ne sont autorisés qu'à modifier ou effacer les items qu'ils ont générés eux-mêmes et uniquement ceux-ci, un ou plusieurs membres prennent le rôle d'administrateur, doté de droits plus étendus, s'il est probable que ces membres soient experts, tous les experts ne seront pas administrateurs.
items.
Base de connaissances
Ce composant sert à sauvegarder la partie formalisable du répertoire partagé. Il s'agit de l'ensemble de ressources accumulées propres à une communauté de pratique particulière. Ce sont des items tels que des documents, des fichiers audio ou vidéo, des outils et des concepts… dont les individus peuvent se servir pour ensuite créer de nouvelles connaissances. Il importe que le lien entre l'auteur qui publie et l'item enregistré ou publié soit maintenu et facile à suivre puisqu'il s'agit souvent d'éléments subjectifs au niveau de la communauté qui ont un fort rapport avec une certaine personne, une situation ou un contexte bien particulier. Les fonctionnalités à prévoir concernent la création, la requête, la mise à jour (la modification) et l'effacement des Par ailleurs, deux modes d'accès semblent souhaitables : un mode « Pull » où l'utilisateur prend l'initiative en cherchant des informations ; un mode « Push » (couplé avec le composant messagerie) où le système met pro activement des informations ciblées ou des contacts potentiellement intéressants à la disposition de l'utilisateur sans que celui-ci les ait sollicités explicitement.
Messagerie Ce composant réalise la communication entre les membres et entre le système et les membres à travers différents canaux : d'abord, il y a la communication directe asynchrone entre des membres qui envoient des messages directement aux destinataires qu'il souhaitent contacter. Outre le support du processus de l'envoi du message, le système ne constitue aucune aide. En revanche, le système peut offrir une assistance lors du choix des destinataires : en fonction du contenu, le système propose à l'utilisateur de rajouter automatiquement des destinataires. Enfin, le système peut aussi créer des messages automatiquement en fonction de ce qui se passe au niveau de la communauté de pratique (nouveaux items dans la base de connaissances, modification dans les profils). Dans ce cas, il s'agit du mode « Push » cité cidessus : le système propose des informations à l'utilisateur qui pourraient être intéressantes pour lui selon son profil et selon les attributs du message.
Forum d'échange
Il s'agit d'un ou plusieurs forums plus ou moins formels dans lesquels les membres de la communauté de pratique peuvent discuter de différents sujets. C'est surtout dans ce cadre que les personnes échangent des expériences à travers des récits (« storytelling »). Pour instrumentaliser ce phénomène particulier, le concept des systèmes de raisonnement à base de cas nous semble particulièrement adapté aux communautés de pratique. En effet, ce travail fondé sur les échanges de récits présente des analogies avec le raisonnement à base de cas qui peuvent se révéler très constructives. La base de cas d'un tel système (comme instrumentalisation du composant base de connaissances) pourrait constituer le lieu de conservation et d'archivage des résolutions de problèmes déjà traités qui sont emmagasinées au sein d'une mémoire accessible à tous les membres de la communauté.
Composant d'échange
Ce composant permet des échanges entre les systèmes de différentes communautés de pratique. Il s'agit d'items de la base de connaissances ou bien de profils de membres qui appartiennent à plusieurs communautés.
Enfin, il est à noter qu'il existe également une communication entre les membres de la communauté de pratique hors du système, c'est-à-dire un échange de connaissances qui a lieu sans passer par les composants du système informatique. Il s'agit par exemple de rencontres face à face, de conversations téléphoniques ou -même si probablement moins souvent de nos jours -de correspondance. Le problème du point de vue informatique, c'est que les connaissances échangées par cette voie ne laissent aucune trace dans le système informatique et qu'on ne peut pas les exploiter.
Conclusion
Ce travail vise à considérer le processus de l'émergence de connaissances dans les communautés de pratique. La considération des caractéristiques de ces communautés nous a conduit à une définition de travail qui tient compte du caractère de plus en plus virtuel des communautés de pratique dont les membres peuvent être géographiquement distants. Ce fait influence aussi la manière d'utiliser les technologies de l'information et de la communication. En effet, surtout pour les échanges à distance, le support par des TIC semble indispensable. Ensuite, la création de nouvelles connaissances dans les communautés de pratique a été analysée sous l'angle de la théorie de l'émergence.
Sur cette base, nous avons proposé une modélisation pour un support de l'émergence de connaissances. Cette modélisation part de l'hypothèse que tous les acteurs d'une communauté de pratique ne sont pas égaux car les connaissances ne sont pas distribuées uniformément parmi les membres : un composant qui sauvegarde les profils des membres permet de localiser des experts pour le sujet abordé dans le but de profiter des connaissances de ces membres expérimentés.
Néanmoins, notre approche possède certaines limites. Tout d'abord, en proposant une modélisation pour un support de l'émergence et de la création de connaissances dans les communautés de pratique, notre approche suppose qu'il est possible d'influencer les activités au sein d'une communauté. Cette démarche s'inscrit donc dans une logique d'encourager et favoriser le développement de processus d'échange entre des acteurs volontaires. Toutefois, dans le cadre du présent travail, nous ne considérons pas encore des aspects liés à l'acceptation des outils des technologies de l'information et de la communication par les utilisateurs.
Deuxièmement, la modélisation considère la base de connaissances comme un lieu central de sauvegarde de connaissance. Cette approche se heurte à des limites dès que la base de connaissances devient trop volumineuse. A l'heure des communautés de pratique virtuelles, il est tout à fait possible que les communautés arrivent à des centaines de membres, voire plus. Dans ce cas, il faudrait envisager une sauvegarde décentralisée/distribuée des connaissances. Les profils des membres pourraient eux aussi être sauvegardés de façon distribuée. Cette configuration semble suggérer l'utilisation des systèmes multi-agents. Par exemple, un agent pourrait détenir le profil de l'individu auquel il est attaché. Lorsqu'un membre de la communauté de pratique fait une recherche d'expertise, c'est son agent qui questionne les autres agents de la communauté pour ensuite échanger des informations sur les profils de différents utilisateurs.
Les perspectives de recherche consistent donc à approfondir notre modélisation et à mettre en oeuvre un prototype de gestion de connaissances au sein des communautés de pratique qui permettra une validation de nos propositions.

Aperçu du processus d'enrichissement
Le processus d'enrichissement que nous avons proposé (Faïz et Mahmoudi, 2005, Mahmoudi et Faïz, 2006 ) émane d'un besoin informationnel réclamé par les utilisateurs des SIG. Pour extraire les connaissances incarnées dans les documents dans des temps raisonnables, nous procédons d'une manière distribuée en adoptant le paradigme multi-agents (Ferber, 1997).
L'approche que nous proposons est modulaire, elle peut être décomposée en trois grandes phases. Il s'agit de la segmentation et de l'identification des thèmes abordés dans les documents initiaux. Suite à cette phase, un nouveau document est généré pour chaque thème regroupant les segments de textes distribués entre les différents agents et traitant le même thème. La seconde phase consiste à affecter pour chaque thème un délégué responsable de l'extraction de l'essentiel d'information de son document généré. Enfin, un filtrage textuel s'opère, il consiste à éliminer toute portion de texte qui s'avère inutile à la compréhension du thème (Mahmoudi et Faïz, 2006 a ).
SDET : Un outil pour l'enrichissement des données
Notre approche a été mise en oeuvre pour permettre un support informationnel pour les utilisateurs de SIG. L'implémentation de notre approche a été réalisée en utilisant le langage Java. Ce choix est fondé sur le fait que Java supporte le multi-threading qui favorise le traitement parallèle entre les différents agents du système. Les fonctionnalités que nous proposons ont été intégrées à un SIG OpenSource : Open Jump 1 (Java Unified Mapping Platform). Ce SIG supporte les principaux standards industriels comme GML ainsi que le modèle objet spatial de l'OpenGIS Consortium. Open Jump offre une grande modularité et de nombreuses extensions.
SDET est une suite de fonctionnalités facilitant l'extraction de nouvelles données qui viennent enrichir celles déjà existantes dans la Base de Données Géographiques (BDG). Nous procédons comme suit : L'utilisateur du SIG à la quête d'informations, va solliciter en premier lieu la BDG du système. En cas d'insatisfaction (information non stockée dans la BDG, détails insuffisants…), l'utilisateur peut faire appel à notre outil d'enrichissement. Ceci est rendu possible en ajoutant une nouvelle option au SIG libre : Open Jump. L'enrichissement est initié par une collecte de corpus de documents. A partir de ces documents bruts, nous extrayons la liste des thèmes traités et leurs documents générés associés.
L'outil propose trois manières pour condenser les documents générés : Le résumé d'un segment sélectionné par l'utilisateur, le résumé de l'intégralité du document généré ou le résumé de l'intégralité du document généré, mais, avec un filtrage appliqué aux segments afin d'éliminer toute redondance d'information. Les résumés résultants peuvent être affichés et stockés pour une consultation ultérieure.
Références Faïz, S. et K. Mahmoudi (2005) 
Summary
Database enrichment is a mean to provide complementary data to end users. In a geograhic context, this is essential to make accurate decisions. SDET is our tool built to perform the data enrichment that we integrated to an open GIS.

Evaluer les catégorisations
La validation manuelle n'est pas forcément toujours faisable ou souhaitable. C'est pourquoi il convient de prendre en considération des méthodes automatiques quantitatives afin de donner une idée de la qualité des catégorisations. Nous nous basons sur la distinction entre critères "externes" et "internes" faite par Halkidi et al. (2002). Alors que les premiers reposent sur l'hypothèse d'une partition idéale des données (étiquettes données par l'utilisateur, par exemple), les seconds n'utilisent aucune information a priori pour juger de la qualité des catégorisations. C'est cette seconde approche que nous avons choisi d'adopter dans notre logiciel.
Contrairement à l'approche externe, aucun étiquetage préalable des données ne permet ici de comparer le résultat du clustering à un quelconque modèle idéal. De nombreux indices de validité ont été proposés et des travaux récents attestent de la vitalité de cette perspective de recherche. Ils se basent sur la recherche, thème classique en apprentissage non supervisé, d'un compromis entre les principes de similarité intra-classe et de dissimilarité inter-classes. Des indices caractéristiques de cette approche interne sont les indices de Dünn, Davies-Bouldin et Hubert modifié, qui ont été implémentés dans notre logiciel.
Logiciel et expérimentations
L'objectif du logiciel que nous proposons est d'aider l'utilisateur à comparer différentes partitions d'un même jeu de données sur la base de critères internes. Ces partitions peuvent être les résultats obtenus à l'aide d'un ou de plusieurs algorithmes de classification automatique, tels les k-means ou EM. Les données d'entrée sont, d'une part, la définition du langage de description et des exemples d'apprentissage décrits à l'aide de ce langage, et, d'autre part, les partitions qui feront l'objet de la comparaison. L'évaluation repose sur trois composantes : l'indice utilisé, la mesure de distance (ou de similarité) choisie, ainsi que la normalisation effectuée sur les attributs numériques. Le logiciel permet de lancer plusieurs évaluations en même temps et propose, en sortie, une visualisation des résultats obtenus. La visualisation est différente suivant que l'on traite un ou plusieurs critères. De plus, le caractère évolutif de notre logiciel donne l'opportunité d'ajouter très facilement de nouveaux indices ou de nouvelles distances.
La figure ci-dessus présente les résultats obtenus avec quatre algorithmes (k-means, EM, Farthest-first et PRESS) sur la célèbre base "vote" du répertoire UCI. Elle permet de constater la supériorité de l'un des algorithmes dans le cas mono-critère (indice de Davies-Bouldin), ici celui qui a obtenu la plus petite des aires. Le cas multi-critères, par contre, semble indiquer deux types de résultats distincts. L'utilisation de notre logiciel peut ainsi suggérer à l'utilisateur d'étudier plus attentivement les raisons de cette différence.
Conclusion et perspectives
Nous présentons un logiciel pour aider l'utilisateur à comparer les résultats obtenus par des algorithmes de classification. La caractéristique principale de ce travail est son caractère évolutif : ajout de nouveaux indices, de nouvelles distances, etc. Dans les perspectives à court terme, nous souhaitons étendre le logiciel aux indices externes, tels la F-mesure ou les fonctions entropiques. A plus long terme, cet outil devrait nous permettre de comparer, non plus les partitions ou les algorithmes, mais directement les critères de pertinence. Ces derniers pourraient alors être regroupées et mis en relation avec la nature des données traitées (données clairsemées, bruitées, à grande dimension, etc.). Ceci devrait mener à une contribution concernant l'évaluation des techniques d'apprentissage non supervisé, évaluation qui présente encore de réelles difficultés au jour d'aujourd'hui.
Summary
This paper details a software that can assist the user for clustering comparison. It gives a clear visualization of different criteria (Dunn, Silhouette, etc.) calculated on one or more partitions of the data. The main feature is its modularity in three components: a quality criterion, a comparison measure and a normalization on numerical attributes. Furthermore, it allows the user to add its own items into those components.

Introduction
Dans les méthodes qui génèrent des règles de décision du type Si condition Alors Conclusion comme les arbres de décision (Breiman et al., 1984;Quinlan, 1993), les graphes d'induction (Zighed et Rakotomalala, 2000),... les mesures d'entropie sont fréquemment utilisées. Or celles-ci reposent sur de nombreuses hypothèses implicites qui ne sont pas toujours justifiées.
Les mesures d'entropie ont été définies mathématiquement par un ensemble d'axiomes en dehors du contexte de l'apprentissage machine. On peut trouver des travaux détaillés dans Rényi (1960), et Aczél et Daróczy (1975). Leur transfert vers l'apprentissage s'est fait de manière peut-être hâtive et mérite d'être revu en détail.
Le présent travail examine et discute des propriétés des entropies dans le cadre des arbres d'induction.
Dans la section suivante, nous fixons quelques notations et rappelons le contexte d'utilisation des mesures d'entropie. Dans la section 3, nous présentons les mesures d'entropie et discutons leurs propriétés et leurs conséquences dans les processus d'induction. Dans la section 4, nous proposons une axiomatique conduisant à une nouvelle mesure d'entropie.
Notations, définitions et concepts de base
Nous nous plaçons dans le cadre des arbres de décision qui font explicitement appel aux entropies pour mesurer la qualité de la partition induite en apprentissage.
Soit ? la population concernée par le problème d'apprentissage. Le profil de tout individu ? de ? est décrit par p variables, X 1 , . . . , X p , dites variables exogènes ou variables explicatives. Ces variables peuvent être qualitatives ou quantitatives.
Nous considérons également la variable à prédire C, parfois appelée variable endogène ou variable classe ou encore variable réponse. L'ensemble des valeurs prises par cette variable sur la population est un ensemble discret et fini noté C. On note par m j le nombre de valeurs différentes prises par X j et par n le nombre de modalités de C. Ainsi, C = {c 1 , . . . , c n }. Et s'il n'y a pas d'ambiguïté, on notera la classe c i simplement par i.
L'objectif d'un algorithme d'induction d'arbre est de générer un modèle ?(X 1 , . . . , X p ) de prédiction de C que l'on représente par un arbre de décision. Chaque branche de l'arbre repré-sente une règle. L'ensemble des règles forme le modèle de prédiction qui permet de calculer, pour un nouvel individu dont on ne connaît que les variables exogènes, l'état de la variable endogène. Le développement de l'arbre s'effectue selon un schéma simple : l'ensemble d'apprentissage ? a est segmenté itérativement, à chaque fois selon une des variables exogènes X j ; j = 1, ...p de sorte à engendrer la partition de plus faible entropie sur la distribution de C. Les sommets obtenus à chaque itération définissent une partition sur ? a . Plus l'arbre grandit, plus la partition devient fine. Le sommet à la racine de l'arbre représente la partition grossière.
Chaque sommet s d'une partition S est caractérisé par une distribution de probabilités des modalités de la variable endogène C : p(i/s); i = 1, . . . , n.
Dans les arbres d'induction, l'entropie H sur la partition S à minimiser est généralement une entropie moyenne calculée comme suit :
. . , p(n/s)) est par exemple l'entropie de Shannon dont l'expression est donnée plus loin, et p(s) la proportion de cas dans le sommet s.  Khinchin (1957), Forte (1973) et Aczél (1973 ont fondé une axiomatique.
Entropie de Shannon
Soit une expérience E avec les événements possibles e 1 , e 2 , . . . , e n de probabilités respectives p 1 , p 2 , . . . , p n . On suppose que
Shannon de la distribution de probabilité est donnée par la formule :
Par continuité on pose 0 log 2 0 = 0. D'autres mesures d'entropie existent (Zighed et Rakotomalala, 2000).
Propriétés théoriques des mesures d'entropie
On considère que (p 1 , p 2 , . . . , p n ) pour n ? 2 est pris dans un ensemble fini de distributions de probabilités et on considère le simplexe d'ordre n n ? n = {(p 1 , p 2 , . . . , p n ) :
Une mesure d'entropie est définie comme suit :
avec les propriétés suivantes :
où ? est une permutation quelconque sur (p 1 , p 2 , . . . , p n ).
Stricte concavité La fonction h(p 1 , p 2 , . . . , p n ) est strictement concave.
Ainsi, l'évaluation de l'entropie h d'une partition S nécessite la connaissance de p(i/s); i = 1, . . . , n; ?s ? S.
Mesure d'entropie pour l'apprentissage inductif
Les propriétés des mesures d'entropie que l'on vient de lister ne nous paraissent pas adaptées à l'apprentissage inductif. En effet, d'une part l'incertitude maximale ne correspond pas nécessairement à la distribution uniforme, ainsi dans le cadre de la détection de transactions frauduleuses peu fréquentes il peut par exemple être opportun de conclure à une fraude dès que la probabilité de celle-ci dépasse un seuil de disons 10%, voire moins. D'autre part, dans la pratique, le calcul de l'entropie repose sur des probabilités estimées et devrait donc tenir compte de leur précision et donc de la taille de l'échantillon. C'est pourquoi nous proposons une nouvelle axiomatique que nous justifions très brièvement et dont l'objectif est d'aboutir à une entropie, que l'on pourrait qualifier d'empirique, qui tient mieux compte de ces considé-rations pratiques.
Propriétés requises
Soit la nouvelle fonction d'entropie que nous voulons bâtir. Nous voulons qu'elle soit empirique, c'est-à-dire fonction des fréquences f (i/.), sensible à la taille N de l'échantillon sur lequel elles sont calculées et qu'elle soit également paramétrée par une distribution W = (w 1 , . . . , w j , . . . , w p ) où elle sera maximale.
:
On notera, pour une distribution W fixée, W (N, f 1 , . . . , f i , . . . , f n ). Nous souhaitons que possède les propriétés suivantes :
P1 : Non négativité La fonction doit être à valeur non négative
P2 : Maximalité Soit W = (w 1 , w 2 , . . . , w n ) une distribution fixée par l'utilisateur comme étant la moins souhaitée et donc d'entropie maximale. Ainsi, pour N fixé,
pour toute distribution (f 1 , . . . , f n ) de taille n. P3 : Asymétrie La nouvelle propriété de maximalité remet en cause l'axiome de symétrie requis par les entropies classiques. Par conséquent, certaines permutations ? pourraient affecter la valeur de l'entropie : 1 , . . . , f n ) = ?1 , . . . , f ?n ). On peut facilement identifier les conditions dans lesquelles la symétrie serait conservée comme par exemple les cas où certains w i seraient identiques et a fortiori dans le cas de la distribution uniforme. P4 : Minimalité Dans le contexte classique, l'entropie est nulle dans le cas où la distribution est concentrée en une seule classe, les autres étant vides, c'est-à-dire lorsqu'il existe j tel que p j = 1 et que p i = 0 pour tout i = j. Cette propriété doit en effet demeurer valide sur le plan théorique. Seulement, en apprentissage ces probabilités sont inconnues. Il serait quand même gênant de dire que l'entropie est nulle dès lors que la distribution est concentrée en une classe. Il faut prendre en considération la taille de l'échantillon qui sert à estimer les p j . On exige simplement que l'entropie d'une distribution empirique pour laquelle il existe j tel que f j = 1, tende vers 0 quand N devient grand, soit
P5 : Consistance Pour un W donné et à distribution fixée, l'entropie devrait être plus faible sur un effectif plus grand. 
est une mesure d'entropie pour processus d'apprentissage inductif qui vérifie les propriétés P1 à P5.
Le graphique 1 visualise la forme de cette entropie pour deux classes avec un vecteur de maximalité W = (0.3; 0.7) et différentes valeurs de N = 5, 10, 50, ...
Conclusion
Dans ce travail nous avons défini une nouvelle fonction d'entropie qui possède, sur un plan théorique et algorithmique de bonnes propriétés. Cette fonction repose sur le choix du paramètre W . L'idée la plus simple pour fixer W est de prendre la distribution a priori observée sur l'échantillon d'apprentissage. En effet, si l'utilisateur met en oeuvre des techniques d'apprentissage c'est pour s'éloigner le plus possible de la distribution a priori. Il est donc naturel qu'elle soit associée à la plus forte entropie. Faute de place, nous n'avons pas pu reporter toutes les critiques que nous pouvons formuler sur l'utilisation des entropies classiques en apprentissage. L'intérêt de notre approche est qu'elle permet de répondre de façon assez simple et sans trop de

Introduction
L'évaluation des performances d'un modèle constitue l'étape finale de tout processus d'apprentissage supervisé. Elle est le retour nécessaire à l'utilisateur pour le guider dans la poursuite de sa fouille de données. Ces mesures, comme celles utilisées pour bâtir des arbres de décisions, sont généralement symétriques. De façon pratique, on entend par symétrique le fait que les erreurs sur chaque modalité de la variable endogène se voient attribuer une importance similaire. Or de nombreux exemples industriels nous montrent que cela n'est pas toujours le cas, en particulier lorsqu'on se trouve en présence de jeux de données fortement déséquilibrés : aide au diagnostic (Grzymala-Busse, 2000), identification de phénomènes inhabituels comme les fraudes lors des transactions par cartes bancaires (Chan, 2001) ou les pannes d'équipements de télécommunications (Weiss, 1998), et bien d'autres encore. Dans ce type de cas l'objectif principal est d'identifier les instances représentatifs de la classe minoritaire. Il est pour cela nécessaire d'utiliser des méthodologies d'apprentissage adaptées (Weiss, 2004) (Japkowicz, 2000), comme notamment les méthodologies sensibles au coût (Domingos, 1999) ou celles basées sur des techniques d'échantillonnage (Chawla, 2002), mais l'évaluation des performances des modèles résultant doit également prendre en considération cet aspect non symétrique de l'importance des modalités, sans se limiter à un simple taux de correction global. Une évaluation locale, c'est-à-dire par modalité, doit alors être conduite. Le taux de rappel et le taux de précision sont les deux indicateurs de base des performances d'un modèle vis-à-vis d'une modalité. Il est également possible de fusionner ces deux critères en utilisant par exemple la f-measure (Van Rijsbergen, 1979) ou de créer d'autres critères utilisant le dénom-brement de chaque type d'erreurs (insertion ou omission) (Makhoul, 1999). Nous proposons dans cet article un critère appelé PRAGMA (Precision and RecAll rates Guided Model Assessment), pouvant servir à la fois pour l'évaluation de modèles d'apprentissage supervisé ou encore de critère utilisé pour bâtir des arbres de décision, prenant en compte l'ensemble de ces aspects. Nous proposons ensuite une évolution des modèles de type forêts aléatoires utilisant ce critère pour les jeux de données fortement déséquilibrés.
PRAGMA : Precision and RecAll rates Guided Model Assessment
PRAGMA utilise deux principes : la notion d'importance d'une classe et la notion de pré-férence entre taux de rappel et taux de précision pour chaque classe. Tout d'abord l'importance d'une classe est représentée par un coefficient ? i fixé par l'utilisateur et utilisé en fin d'évaluation. Ensuite pour chaque classe, nous évaluons le modèle en fonction de son taux de rappel (r i ) et de son taux de précision (p i ). Cette fonction f (r i , p i ), que nous cherchons à minimaliser par analogie avec le nombre d'erreurs d'un modèle, doit avoir les propriétés suivantes :
(1) f (0, 0) = 1 , on fixe la valeur de la pire situation (r i = 0 et p i = 0).
(2) f (1, 1) = 0 , on fixe la valeur de la meilleure situation (r i = 1 et p i = 1).
(3)
df (r,p) dr < 0, r ? [0; 1] , à taux de précision égal, la mesure doit diminuer lorsque le taux de rappel augmente. (4) df (r,p) dp < 0, p ? [0; 1] , à taux de rappel égal, la mesure doit diminuer lorsque le taux de précision augmente.
Une telle fonction peut avoir la forme suivante :
Pour prendre en compte les souhaits de l'utilisateur en terme de préférence entre le taux de rappel et le taux de précision, nous décidons de pondérérer à la fois r i et p i :
Le ratio ?/? détermine la préference entre le rappel et la précision (plus celui-ci est grand (supérieur à 1), plus le rappel est préféré ; plus celui-ci est petit (inférieur à 1), plus la précision est préférée ; s'il est égal à 1, cela signifie qu'aucune distinction n'est faite entre le rappel et la précision). Pour déterminer, de manière instinctive et compréhensible, ces deux paramètres, l'utilisateur doit définir deux situations extrêmes qu'il juge de qualité équivalente. En pratique, ces deux situations sont : (a) celle où le taux de rappel est parfait (r i = 1) et (b) celle où le taux de précision est parfait (p i = 1). Il implique donc à l'utilisateur de définir deux valeurs x et y tel que f (1, x) = f (y, 1). Choisir ces deux valeurs peut être considéré comme répondre aux deux questions suivantes : Quel compromis êtes-vous prêt à faire vis-à-vis de la précision pour avoir un taux de rappel de 1 ? (répondre à cette question permet de définir x, avec 0 ? x < 1)(a) Quel compromis êtes-vous prêt à faire vis-à-vis du rappel pour avoir un taux de précision de 1 ? (répondre à cette question permet de définir y, avec 0 ? y < 1)(b) Avec cette dernière contrainte (5) f (1, x) = f (y, 1), nous pouvons déterminer les paramètres ? et ? :
La fonction f utilisée pour évaluer localement un modèle selon son rappel et sa précision sur une modalité est la suivante :
Cette fonction correspond à l'équation d'un plan où l'axe défini par les points (0,0,1) et (1,1,0) est fixe, et où le ratio ?/? détermine l'orientation du plan autour de cet axe (la préférence entre le rappel et la précision)( fig. 1). Ces évaluations locales (propres à chaque modalité) sont ensuite combinées lors de l'évalua-tion finale à l'aide d'une moyenne pondérée où les coefficients d'importance de chaque classe constituent les pondérations :
Evolution des modèles de type forêts aléatoires
Une forêt aléatoire (ou Random Forest RF) (Breiman, 2001) est un ensemble d'arbres de classification de profondeur maximale, chacun construit à partir d'un échantillon bootstrap du jeu d'apprentissage. De plus, pour l'obtention de chaque noeud on limite la recherche de la meilleure discrimination à k variables tirées au sort. La prédiction pour un objet est obtenue en comptabilisant les prédictions de chaque arbre pour l'objet (chaque arbre vote pour une modalité) puis en choississant la modalité ayant reçu le plus de voix parmi tous les arbres de la forêt (vote à la majorité). Les performances d'une forêt sont sensiblement supérieures à celles d'un arbre seul tel que C4.5 (Leon, 2004). Elle est également plus robuste au bruit et présente de meilleures facultés de généralisation (Breiman, 2001). Cependant, celle-ci n'est pas spécifiquement adaptée aux jeux de données déséquilibrés, et ses deux paramètres (le nombre d'arbres et le nombre k de variables à tirer au sort) ne permettent pas à l'utilisateur de spécifier ses préférences en termes de taux de rappel et de précision selon chaque modalité. L'évolution que nous proposons ici consiste à remplacer l'étape du vote classique à la majorité par une nouvelle stratégie de vote pondéré où la recherche automatique des poids optimaux se fait à l'aide de PRAGMA.
Stratégie de vote. Notre stratégie de vote consiste à donner plus ou moins d'importance aux voix attribuées par les arbres ( fig. 2). Une pondération par classe est déterminée (soit par l'utilisateur, soit automatiquement), laquelle multiplie le nombre de voix reçues par l'individu pour cette classe. Ainsi la modalité assignée à un objet n'est pas toujours celle dont il a reçu le plus de voix, mais celle dont le nombre de voix multiplié par son poids est le plus grand. Ceci permet d'augmenter les taux de rappel des classes minoritaires en leur affectant des pondérations fortes, ou plus généralement de jouer sur les taux de rappel et de précision de chaque classe en modifiant leur pondération.
Recherche automatique. Il peut être assez difficile de trouver manuellement les pondéra-tions ajustant au mieux les résultats du modèle aux besoins de l'utilisateur. Si pour un problème à deux modalités tout peut se ramener à un déplacement de la frontière, en terme de nombre de votes, entre les deux classes, dès qu'il y a plus de trois modalités le nombre de possibilités de paramétrage, c'est-à-dire de ratios entre chaque couple de pondérations, devient bien plus conséquent, et il apparaît nécessaire de rendre automatique la recherche des pondérations. L'algorithme utilisé pour automatiser la recherche des pondérations est construit autour d'un recuit simulé (Kirkpatrick, 1983) cherchant à optimiser la mesure PRAGMA paramétrée selon les souhaits de l'utilisateur. Ce procédé est parfaitement adapté et efficace pour ce type d'optimisation. Le surcoût calculatoire (comparé à une forêt aléatoire classique) est extrêmement (Hettich, 1999) faible. En effet, la forêt n'est construite qu'une fois, seul le résultat du vote après pondéra-tion est mis à jour pour en permettre l'évaluation par PRAGMA. De plus, en conservant pour chaque individu le nombre de votes non pondérés qu'il a reçu pour chaque modalité, il suffit de mettre à jour uniquement la matrice de confusion après pondération du vote pour évaluer le modèle et passer à l'itération suivante.
FIG. 2 -Exemples de distribution de votes pour : (A) le jeu de données Letters
Expérimentations
Nous présentons dans cette section les résultats obtenus par pondération automatique des votes d'une forêt aléatoire. Deux types de situations ont été envisagés : (1) Utilisation de l'optimisation sur des jeux de données équilibrés. Notre but ici, en tant qu'utilisateur, est de favoriser un maximum le taux de rappel de certaines classes jugées 'prioritaires'. Les tests sont réalisés sur les jeux de données de référence Autos et Letters (Hettich, 1999) dont les variables endogènes possèdent respectivement 6 et 26 modalités.
(2) Utilisation de l'optimisation des jeux de données déséquilibrés à 2 modalités. Notre but dans cette situation est de favoriser un maximum le taux de rappel de la classe minoritaire. Les tests sont réalisés sur les jeux Hypothyroïd et Satimage (Hettich, 1999) réduits à 2 classes (minoritaire ; fusion des autres classes), ainsi que sur le jeu Mammo, issu de la mise au point d'un système d'aide au diagnostic du cancer du sein.
Jeux de données équilibrés
Nous supposerons ici que l'utilisateur cherche à maximiser les taux de rappel des classes '_3' et '_2' pour le jeu Autos, et les taux de rappel des voyelles pour le jeu Letters. Ceci se traduit par le paramétrage de la fonction PRAGMA suivant : coefficient d'importance 10 et couple (x; y) = (10; 90) pour les classes prioritaires, coefficient d'importance 1 et couple (x; y) = (80; 80) pour les autres classes. Nous utilisons des forêts aléatoires de 20 arbres, avec respectivement 5 et 4 variables pour la randomisation. Les résultats présentés dans les tables 1 et 2 sont issus d'une 10-CrossValidation. La figure 3 montre les résultats détaillés sur le jeu Letters. Notez que la classe '__2' du jeu Autos ne contient que 3 objets, les résultats propres à cette modalité sont peu significatifs. Les différentes moyennes réalisées sont toujours pondé-rées par les effectifs des différentes classes. Ces différents résultats montrent la capacité de l'optimisation à retranscrire les volontés de l'utilisateur. Pour les deux jeux de données, les taux de rappel des classes ciblées ont augmenté. Il en résulte également (de manière logique) : (1) une baisse du taux de précision pour ces mêmes classes ; (2) une baisse du taux de rappel et une augmentation du taux de précision (en moyenne) pour les classes où aucune préférence n'avait été spécifiée. Notons également que ces changements n'entraînent pas forcément une diminution du taux de correction global. Celui-ci peut augmenter ou diminuer selon les jeux de données et le paramétrage de la mesure PRAGMA (ici augmentation du taux correction global pour Autos et diminution sur Letters).
FIG. 3 -Résultats détaillés pour Letters : (A) RF Classique ; (B) RF Optimisée, le rappel et la précision "s'organisent" selon les préférences de l'utilisateur.
Jeux de données déséquilibrés
Ce type de jeux de données assez courant dans le milieu industriel (détection de phéno-mènes anormaux, fraudes, pannes, aide au diagnostic...) constitue un réel challenge pour l'apprentissage automatique. L'objectif principal est de détecter un maximum d'objets de la classe minoritaire (taux de rappel élevé) sans présenter trop de faux positifs (taux de précision correct) ce qui aurait pour effet de rendre de tels systèmes inutilisables. Nos tests sont réalisés sur 3 jeux de données (table 3) : Hypothyroïd et Satimage (Hettich, 1999) réduits à deux classes en fusionnant les classes non minoritaires et Mammo issu de la mise au point d'un système d'aide au diagnostic du cancer du sein. Notons que ce dernier a été réduit en terme de variables et d'objets pour le rendre plus difficile et ne pas dévoiler des résultats industriels confidentiels. La volonté de maximiser le taux de rappel de la classe minoritaire se traduit par le paramétrage de PRAGMA suivant : coefficient d'importance 10 et couple (x; y) = (10; 90) pour la classe minoritaire, coefficient d'importance 1 et couple (x; y) = (80; 80) pour la classe majoritaire. Nous présentons les résultats obtenus en 10-CrossValidation avec C4.5 (témoin de référence des difficultés pouvant présenter les jeux de données), une forêt aléatoire classique, et une forêt aléatoire optimisée par pondération des votes à l'aide de la mesure PRAGMA (table 4). Les forêts sont composées de 20 arbres, avec respectivement 5, 6 et 15 variables utilisées lors de la randomisation pour les 3 jeux de données.
Les taux du rappel des classes minoritaires les plus élevés sont systématiquement obtenus par la forêt aléatoire optimisée, ceci sans provoquer de fortes baisses des taux de précision. La mesure PRAGMA guide en cela parfaitement le modèle vers les performances souhaitées par l'utilisateur. On remarque également que selon les cas l'optimisation permet également parfois d'améliorer le taux de précision de la classe minoritaire ou le taux de correction globale.
Des résultats détaillés obtenus en 10-CrossValidation sur le jeu Mammo sont présentés en figure 4. Ils permettent une meilleure description de l'effet de la pondération des votes et de l'utilisation de la mesure PRAGMA sur les performances du modèle. Quatre indices (taux de rappel, taux de précision, nombre d'erreurs, mesure PRAGMA) sont évalués pour différentes valeurs du ratio R : pondération de la classe 'Cancer' / pondération de la classe 'Non Cancer'. On remarque que les plus fortes variations pour les taux de rappel et de précision se produisent pour la classe 'Cancer' de par son effectif faible. Le graphe du nombre d'erreurs présente deux caractéristiques notables : (1) celui-ci est asymétrique, car une baisse légère du taux de rappel sur la classe majoritaire due à une forte pondération de la classe minoritaire crée logiquement plus d'erreurs qu'une faible variation du taux de rappel de la classe minoritaire ; (2) pour les ratios 2 ? R ? 5 le nombre d'erreurs total varie très peu alors que la nature des erreurs change (voir les graphes des taux de rappel et de précision). Une sorte de transfert d'erreurs se produit : R ? 2 : les objets mal classés appartiennent majoritairement à la classe 'Cancer' 3 ? R ? 4 : les proportions d'objets mal classés pour chacune des 2 classes sont similaires 5 ? R : les objets mal classés appartiennent majoritairement à la classe 'Non Cancer' La mesure PRAGMA permet de faire différentes observations : (1) l'asymétrie est inversée, montrant ainsi que les variations du taux de rappel de la classe 'Cancer' constituent la principale influence de la mesure (ceci s'expliquant par le fort coefficient d'importance et le paramétrage orienté vers le taux de rappel pour la classe 'Cancer') ; (2) pour les ratio 2 ? R ? 5 
Conclusion
Nous proposons dans cet article une nouvelle mesure de qualité des performances des modèles d'apprentissage supervisé appelée PRAGMA (Precision and RecAll rates Guided Model Assessment). Ce critère permet à l'utilisateur d'évaluer ses modèles vis-à-vis de ses attentes en termes de taux de rappel, de taux de précision et d'importance de chaque classe sous la forme d'une mesure unique. Nous montrons ensuite comment il est possible d'utiliser cette mesure comme critère à optimiser pour orienter les performances d'un modèle. L'exemple présenté ici est l'optimisation des forêts aléatoires par pondération (selon les différentes modalités) des votes. Les résultats montrent que cette adaptation permet à l'utilisateur d'orienter de manière effective les performances des forêts aléatoires selon ses souhaits. Dans nos travaux futurs nous projetons de tester d'autres types de fonctions que celle d'un plan pour l'évaluation locale d'une modalité vis-à-vis de son taux de rappel et de son taux de précision. Nous travaillons d'ores et déjà sur l'utilisation de la mesure PRAGMA comme critère de construction des arbres de décision en remplacement des différentes mesures d'entropie classiquement utilisées, mais nos différents tests ne sont pas encore finalisés.
Remerciements Nous tenons à remercier les membres de la société Fenics Sas (France), en particulier Simon Marcellin, Jérémy Clech, et Anne-Sophie Darnand, ainsi qu'Elie Prudhomme de l'université Lumière Lyon2 pour leur aide et leurs nombreux conseils qui ont permis à cet article de voir le jour. Ce travail a été réalisé dans le cadre d'une thèse cofinancée par le Ministère de la Recherche et de l'Industrie.

Résumé. Dans le domaine thermique, la plupart des études reposent sur des modèles à éléments finis. Cependant, le coût en calcul et donc en temps de ces mé-thodes ont renforcé le besoin de modèles plus compacts. Le réseau RC équivalent est la solution la plus souvent utilisée. Toutefois, ses paramètres doivent souvent être ajustés à l'aide de mesures ou de simulation. Dans ce contexte d'identification de système, les méthodes statistiques seront comparées aux méthodes classiquement utilisées pour la prédiction thermique.
Le contrôle de la température de jonction des composants est l'un des enjeux majeurs de l'évolution actuelle de l'électronique du fait qu'elle influe sur leur fiabilité et leurs caracté-ristiques. L'analyse par éléments finis apporte une solution numérique à ce problème mais ne peut pas être utilisée concrètement du fait d'un nombre de calculs trop important. C'est dans ce contexte que les CTM (Compact Thermal Model) ont été developpés (Lasance (2003)). Toutefois, en se rapprochant de l'identification de système, ces modèles ont ouvert la voie aux méthodes statistiques, et notamment à celles pouvant être utilisées dans des cas non-linéaires.
Le problème de la prédiction thermique en trois dimensions peut se résumer à trouver la fonction u(x, y, z, t), représentant la température du système à un instant donné. En discré-tisant le système, via un maillage, l'équation de diffusion thermique peut être ré-écrite sous forme matricielle (Bergheau et Fortunier (2004)) :
où u(t) est un vecteur représentant la température aux différents points du maillage, C la matrice élémentaire de masse et K la matrice élémentaire de rigidité. F (t) représente toujours la puissance dissipée mais discrétisée. Le système est alors représenté sous la forme de plusieurs blocs de matériaux homogènes mis bout à bout pour obtenir une structure réaliste. Si le flux de chaleur est supposé être unidirectionnel, alors un bloc peut être remplacé par un circuit électrique équivalent de type RC. Le modèle se trouve donc mis sous la forme d'un réseau RC correspondant aux différents "étages" du système. Toutefois, les conditions de cette simplification étant rarement respectées, les paramètres doivent souvent être ajustés à l'aide de simulations ou de mesures. 
dont les paramètres sont plus simples à estimer.
La précision obtenue par les modèles linéaires est très bonne. Toutefois, elle peut être améliorée car la linéarité des problèmes thermiques n'est pas toujours garantie. En effet, les effets de la convection mais aussi la conductivité thermique sont légèrement dépendants de la température. Comme pour le cas linéaire, l'identification de système peut être utilisée pour construire un modèle, notamment à partir de réseaux de neurones. Pour conserver le même type d'architecture que précédemment, un réseau de type NNOE (Norgard et al. (2000)) a été choisi.
Si les méthodes fondées sur les éléments finis sont inutilisables en temps réel, l'analyse classique à base de réseaux RC montre aussi ses limites. Les méthodes statistiques peuvent ainsi aider à simplifier les calculs et à améliorer la précision. Les meilleurs résultats pour les modèles linéaire et non-linéaire sont comparés dans la table 1. Ce travail a donc permis de montrer l'intérêt que pouvait encore avoir une méthode éprouvée telle que les réseaux de neurones, dans un domaine où l'apprentissage statistique est resté très peu utilisé. Les résultats obtenus pourront servir de référence pour l'utilisation de méthodes plus originales comme les réseaux bayésiens dynamiques ou les SVM. 
Summary
In the thermal field, most of the studies are based on finite elements model. However, the calculation -and thus time -cost of these methods have inlighted the need of more compact models. The equivalent RC network is so the most used solution. But, the parameters should often be tuned thanks to measurements or simulations. In this context of system identification, statistical methods will be compared with the classical ones for thermal predictions.

Introduction
Dans le cadre de l'ACI Masse de Données, le projet FoDoMuSt 
FIG. 1 -Extrait de l'ontologie d'objets géographiques.
bas niveau. On tente alors d'identifier et valider ces polygones en tant qu'objets géographiques par une classification. Lorsque cette dernière échoue, les concepts de l'ontologie d'objets géo-graphiques sont étudiés pour affecter une sémantique aux polygones. Cette étape nécessite un processus de navigation efficace dans l'ontologie, ainsi que l'élaboration d'une métrique adaptée à la comparaison des polygones aux concepts. L'objectif de cet article est donc de proposer un mécanisme de navigation, de comparaison et d'appariement de polygones avec des concepts d'une ontologie issue du domaine de la géographie et dédiée à l'analyse d'une banque de données images (aériennes et/ou satellites).
Dans la première section, nous abordons les approches existantes au niveau de la comparaison sémantique et des mesures de similarité. La deuxième section expose la méthode de navigation et de comparaison mise en place pour répondre à notre objectif. Ensuite, nous pré-sentons une utilisation de notre approche avec la plate-forme FoDoMuSt, avant de conclure et évoquer des perspectives à notre travail.
État de l'art
L'identification d'un polygone en utilisant l'ontologie implique une comparaison des propriétés du polygone avec celles des concepts de l'ontologie. Les propriétés (attributs et contraintes sur les attributs) d'un concept sont des conditions individuellement nécessaires et collectivement suffisantes pour établir la relation d'appartenance. Autrement dit, un objet doit toutes les satisfaire pour être membre de la catégorie (dans notre cas un concept) et tout objet les satisfaisant en est un membre. En terme fonctionnel, une catégorie peut être définie par un prédicat d'appartenance sous la forme d'une fonction qui retourne une valeur booléenne selon l'appartenance ou non de l'objet à la catégorie (Mariño Drews, 1993). Ce raisonnement s'appuie néanmoins sur deux hypothèses fortes. La première concerne les propriétés utilisées qui doivent être suffisamment discriminantes. La deuxième est l'hypothèse du monde clos qui présuppose que tout objet peut être parfaitement représenté dans un mode donné de représen-tation et qu'il est possible de déterminer s'il appartient ou non à une catégorie donnée. Les différentes approches présentes dans la littérature articulent la comparaison sémantique entre une stratégie et une fonction de comparaison (ou mesure de similarité).
D'une manière générale, les stratégies de comparaison admettent, explicitement ou non, une distinction entre structure locale (interne, intrinsèque) et globale (externe, extrinsèque) des concepts. La première correspond aux propriétés définies au niveau du concept lui-même, la deuxième correspond à la place du concept dans la structure globale et aux relations auxquelles il participe (héritage, relation de composition, etc.). Pour plus d'informations sur les approches existantes, nous reportons le lecteur à (Schvaiko et Euzenat, 2005).
En ce qui concerne l'évaluation de la similarité, le passage à une représentation "attributvaleur" permet le calcul de distances euclidiennes dans un espace multidimensionnel. Schwering et Raubal (2005)  Ces approches abordent donc simultanément les structures internes et externes des concepts. Le souci de définir des mesures asymétriques où la relation de généralisation/spécialisation influe sur la mesure, traduit l'idée d'appréhender les concepts dans toute leur complexité. Si de nombreuses mesures évaluent la similarité selon la profondeur des concepts, le plus court chemin entre eux, etc., certaines utilisent ces caractéristiques (explicitement ou non) comme facteur d'asymétrie ou de spécialisation.
Méthode d'appariement proposée
Un objet géographique potentiel est dans un premier temps un polygone sur lequel un certain nombre de caractéristiques (dimensions, indices divers tels que l'indice de Miller, etc.) sont calculées dans une phase de vectorisation. Un polygone est alors donné sous la forme d'attributs-valeurs. Notre approche d'appariement d'un polygone aux concepts de l'ontologie, est orientée attributs ("feature-based"). Elle consiste à vérifier la validité des valeurs selon les propriétés et les contraintes définies au niveau des concepts. Cependant, un polygone n'ayant pas de structure sémantique, nous ne pouvons pas nous baser directement sur les formules évo-quées en section 2, comme par exemple MDSM. Un polygone peut être a priori apparié à n'importe lequel des concepts. Les attributs d'un polygone permettant l'appariement à un concept ne sont donc pas identiques selon le concept étudié. Par exemple, le concept "Bâtiment" est défini par de nombreux indices (élongation, Miller, . . .) et des informations radiométriques, alors que le concept "Ombre" est défini uniquement à partir d'attributs radiométriques. Sans connaissance a priori, il est alors nécessaire pour chaque polygone, de calculer tous les attributs possibles, même si la majorité s'avéreront inutiles pour le traitement d'un concept.
Afin de tenir compte de toutes ces spécificités nous avons élaboré notre propre méthode d'appariement. Nous inspirant de la littérature pour affecter une sémantique aux polygones, nous avons plaçé au coeur du mécanisme d'appariement une mesure différenciant les composantes locales et globales des concepts. Nous avons défini une nouvelle mesure de similarité (locale) et un score d'appariement (global) pour évaluer la pertinence dans une hiérarchie de concepts.
Score d'appariement
La mesure de similarité (locale) compare les attributs d'un polygone P avec les attributs spécifiques à un concept C et est définie par la formule suivante :
où v i sont les valeurs dans P pour les attributs A i , et ? i les pondérations rattachées aux A i du concept C (traduisant des rôles plus ou moins discriminants). Notons que la mesure locale ne compare que les attributs communs entre ceux de P et ceux qui sont spécifiques à C (i.e. attributs surchargés ou définis dans ce concept).
Le score d'appariement (global) évalue la pertinence de l'appariement dans la hiérarchie de concepts. Il est défini par l'équation ci-dessous, où les C j sont les concepts constituant le chemin de la racine à C m .
S(P, C m ) = m j=1 ? j Sim(P, C j ) m j=1 ? j Ce score d'appariement est une combinaison linéaire des similarités locales obtenues avec les C j . Les similarités locales sont "propagées" par héritage aux concepts plus spécifiques. Nous intégrons dans ce calcul, un facteur de spécialisation ? basé sur la profondeur des concepts. Nous privilégions ainsi a priori la spécialisation, en considérant que toute nouvelle information apporte une nouvelle sémantique.
Notons qu'un polygone n'est pas comparé aux différents concepts avec les mêmes proprié-tés (car les concepts sont décris différemment). On ne peut donc pas assurer que les valeurs respectent les caractéristiques des distances (symétrie, séparation et inégalité triangulaire). Il semble alors logique que des mesures spécifiques aux différents concepts puissent être inté-grées par la suite au module d'appariement. On pourrait ainsi traiter spécifiquement les relations entre concepts (autre que l'héritage) dans une approche plus spécialement adaptée aux relations de composition par exemple.
Navigation dans l'ontologie
Le score entre un polygone et un concept étant défini, il faut à présent parcourir l'ontologie pour déterminer le ou les concepts les plus proches. Pour cela nous opérons un parcours en largeur en utilisant deux heuristiques pour diminuer l'espace de recherche et ainsi accélérer les traitements. Un seuil minimum pour le score d'appariement permet d'élaguer les branches dont le concept de départ ne donnerait pas un score suffisant. Cette stratégie est basée sur le fait qu'un concept ayant peu de propriétés vérifiées par le polygone ne sera pas pertinent au niveau des concepts spécialisés. Remarquons que dans le cas où la similarité locale ne serait pas calculable (aucun attribut à comparer), alors le seuil n'est pas pris en compte et le parcours se poursuit. La deuxième possibilité offerte à l'utilisateur consiste à fixer une profondeur d'exploration. Cela permet ainsi de choisir le degré de détail que l'on souhaite utiliser. Par exemple, sur la figure 1, si on fixe la profondeur à 3, seules les catégories usuelles seront visitées (végétation, eau, etc.). La navigation est bien sûr parametrable de façon à utiliser ou non ces stratégies.
Implantation
L'ontologie de FoDoMuSt a été créée avec Protégé2000 2 . Nous avons développé le module d'appariement sous JAVA en utilisant l'API de développement Protégé2000 qui permet d'aborder la base de connaissances sous forme de frames. Ces dernières sont liées au paradigme objet et sont particulièrement adaptées au traitement informatique et à la manipulation des concepts et de leurs relations. Nous travaillons avec notre propre mécanisme de raisonnement, sans passer par un raisonneur qui nécessiterait un formatage de la base de connaissance.
L'accent a été mis sur une implantation la plus générique possible, aboutissant à un environnement de développement pour l'appariement. Le module a été ensuite intégré à la plate-forme FoDoMuSt.
Utilisation dans la plate-forme FoDoMuSt
La plate-forme FoDoMuSt permet de manipuler et d'effectuer des traitements sur des images aériennes ou satellitaires. Il est entre autre possible d'effectuer une classification et/ou une segmentation de l'image afin de construire des polygones en fonction des informations radiométriques des pixels. A l'issue de cette étape, l'utilisateur a la possibilité de lancer l'identification d'un polygone avec notre méthode. Il est aussi possible de lancer le processus sur tous les polygones de l'image. Tout ceci se réalise par l'intermédiaire d'une interface graphique (affichage de l'image segmentée, sélection du polygone, choix des paramètres, restitution des résultats, etc.). Rappelons que les différents paramètres possibles sont principalement le seuil minimum du score d'appariement et le niveau de profondeur souhaité. Dans le contexte d'expérimentation, il faut aussi évoqué les valeurs de pondération des attributs de chaque concept. Cela a été réalisé en concertation avec les géographes. Le ou les concepts ayant obtenus le meilleur score sont proposés à l'utilisateur pour attribuer un label au polygone étudié. Dans le cas d'un traitement sur un seul polygone, une arborescence représentant un sous-ensemble d'intérêt de l'ontologie est aussi affichée avec pour chaque concept les informations concernant les calculs de l'appariement.
Des exprimentations ont été réalisées sur une image satellite (Quickbird MS) montrant une partie de la ville de Strasbourg. Dans un premier temps, le seuil minimum du score n'a pas était utilisé, et le niveau de profondeur a été fixé à 3. Nous avons alors pu identifier de manière très significative la végétation, l'eau, l'ombre et le minéral. De mauvaises identifications ont concerné principalement l'ombre. Dans une seconde expérience, nous avons laissé la navigation aller jusqu'aux concepts feuille de l'ontologie. L'évaluation est ici difficile étant donné le nombre de concepts. Nous allons prochainement travailler sur ce point. Cependant, les premières observations sont très encourageantes.
Conclusion
Nous avons présenté une méthode d'identification d'objets géographiques, utilisant des connaissances sous forme d'une ontologie. Notre approche est basée sur le parcours et l'appariement d'objets avec des concepts de cette ontologie. Nous avons défini pour cela une mesure

Introduction
La classification automatique (ou clustering) est un domaine d'étude situé à l'intersection de deux thématiques de recherches majeures que sont l'analyse de données et l'apprentissage automatique. Ce domaine est en perpétuelle évolution du fait de l'apparition constante de nouveaux besoins portant à la fois sur la quantité ou la nature des données à traiter (numériques, symboliques, spatiales, histogrammes, etc.) que sur le type de classification attendue (partition, hiérarchie, schéma flou, etc.).
Nombreuses sont les approches proposées afin d'organiser, de résumer ou de simplifier un ensemble de données à l'aide d'une structure de laquelle il est possible de faire émerger des classes d'objets similaires au sens d'un critère de proximité défini ou plus généralement au regard des propriétés que ces objets partagent. Il est de coutume de structurer ces approches en différentes catégories mutuellement non-exclusives (voir Jain et al. (1999)) comme par exemple, pour ne citer que les principales, les approches hiérarchiques, par partitionnement ou encore les modèles de mélanges.
Les approches par partitionnement, dont l'algorithme des k-moyennes (MacQueen, 1967) en est l'un des plus célèbre représentant, consiste le plus souvent à construire une collection de classes disjointes formant une partition des données par optimisation d'un critère objectif.
Ce critère étant généralement choisi de façon à minimiser la variance intra-classe (les objets à l'intérieur d'une classe doivent tous être assez similaires) et/ou à maximiser la variance interclasses (les classes doivent être séparées les unes des autres).
Les approches hiérarchiques aboutissent en revanche à une collection de classes emboîtées, que l'on peut représenter par un arbre ou plus généralement un graphe dont les arêtes modé-lisent une relation d'inclusion. Généralement agglomératifs (parfois divisifs) les algorithmes proposés procèdent par fusions successives de classes similaires et les plus utilisés restent sans nul doutes les méthodes agglomératives hiérarchiques (liens simple, complet, moyen ou critère de Ward) présentées dans Sneath et Sokal (1973).
Enfin, pour les approches de classification par mélange de lois, le problème est posé de façon à maximiser la vraisemblance d'un modèle faisant l'hypothèse que les données sont des observations d'un mélange de densités. Le modèle est caractérisé par le nombre de lois supposées et leurs paramètres. La méthode EM par exemple, proposée par Dempster et al. (1977) constitue une solution algorithmique incontournable à ce genre de problème d'optimisation.
Si la plupart des domaines d'application trouvent dans cette pluralité d'approches des ré-ponses satisfaisantes aux besoins exprimés, des domaines récents nécessitent d'adapter voire de reposer la problématique de la classification et d'y adjoindre une solution algorithmique efficace. Nous nous intéressons ici au problème de la classification de données en classes nondisjointes (également dites empiétantes ou recouvrantes)
1
. Ce type d'approche vise à structurer les données en une collection de classes telle que chaque objet puisse appartenir à plusieurs classes, correspondant alors à une organisation naturelle pour des données par exemple multimédia (texte, image et/ou vidéo) ou encore biologiques (gènes) (Banerjee et al., 2005).
Nous présenterons tout d'abord en Section 2 un ensemble de pistes proposées (approches pyramidales, classification floue, etc.) pour répondre plus ou moins directement aux besoins exprimés par les domaines d'application mentionnés ci-dessus. Nous montrerons alors qu'aucune de ces approches ne constitue une solution globale pour le problème posé. Nous tenterons en Section 3 d'en proposer une nouvelle formalisation et d'y adjoindre une première solution algorithmique en nous inspirant de l'algorithme simple et efficace des k-moyennes. L'approche ainsi présentée sera ensuite observée dans son fonctionnement puis évaluée sur deux jeux de données réelles dont la collection de textes Reuters (Section 4). Enfin nous proposerons en guise de conclusion, un ensemble de perspectives à cette étude, visant à positionner le problème de classification avec recouvrements comme sous-domaine à part entière des recherches menées en classification automatique.
Problématique de la classification avec recouvrements
Des solutions partielles
Une première voie de recherches conduisant à une structuration des données en classes empiétantes réside dans les techniques de classification pyramidales initiées par Diday (1984). Cependant, l'ensemble des recouvrements envisageables par une telle structure se limite aux collections de classes telles que chaque classe s'intersecte avec au plus deux autres classes.
Plusieurs autres structures hiérarchiques ont été proposées par la suite afin d'étendre les schémas atteignables de façon à approcher l'ensemble de tous les recouvrements possibles ; en particulier les hiérarchies faibles puis les k-hiérarchies faibles (Bertrand et Janowitz, 2003). Cependant deux points restent à déplorer : d'une part il n'existe pas aujourd'hui de méthode algorithmique permettant de construire de telles structures hiérarchiques, et d'autre part l'ensemble des recouvrements atteints -bien que très largement étendu -reste limité aux collections de classes vérifiant la propriété suivante : "l'intersection de (k + 1) classes arbitraires peut être réduite à l'intersection de k de ces classes". Un second axe de recherches a été assez fortement étudié ces dernières années et consiste soit à adapter des algorithmes existants (k-moyennes et sa variante floue ou encore EM) ou à développer de nouvelles méthodologies spécifiées pour la recherche d'un "bon" recouvrement des données en classes d'objets similaires. Dans cette dernière classe de méthodes on peut citer les algorithmes des k-moyennes axiales (Lelu, 1994) et CBC (Clustering By Committee) développé par Pantel (2003) tous deux motivés par l'application aux données textuelles (mots ou documents) ou encore l'algorithme plus général POBOC (Pole-Based Overlapping Clustering) proposé par Cleuziou et al. (2004).
De façon globale, qu'il s'agisse d'algorithmes nouveaux ou simplement adaptés, toutes ces méthodes consistent, en une ou plusieurs itérations, à rechercher des centres auxquels sont affectés les objets. Ces centres peuvent être des points de l'espace (k-moyennes mais aussi EM 2 ), des axes (k-moyennes axiales) ou encore des petits ensembles d'objets appelés committee dans CBC et Pole dans POBOC. Quelque soit la forme prise par ces centres, l'algorithme permettant de les obtenir ne prend pas en considération le fait que les classes finales formeront un recouvrement et pourront ainsi contenir des objets communs. Par exemple les méthodes d'agrégation autour des centres mobiles (k-moyennes et k-moyennes axiales) déterminent un centre après affectation de chaque objet à un seul de ces centres ; à l'inverse les variantes floues de ce type de méthodes considèrent systématiquement que tous les objets doivent participer à la définition de chaque centre ; l'une des hypothèses utilisées dans l'algorithme EM vise à considérer que chaque objet est une observation de l'une (et une seule) des lois du mélange ; enfin les algorithmes CBC et POBOC définissent les centres indépendamment de tout critère objectif de qualité du recouvrement induit par ces centres.
Ainsi définis, les centres sont déterminants pour la dernière étape d'affectation qui conduira au schéma final de classification. L'affectation est le plus souvent réalisée au moyen d'un seuil 3 , difficile à déterminer, quitte à violer les fondements théoriques sur lesquels l'algorithme repose, par exemple la minimisation d'un critère objectif. Finalement, l'hypothèse sous-jacente formulée par ces approches vise à considérer qu'"un recouvrement de qualité correspond né-cessairement à l'extension d'une "bonne" partition".
Recouvrements et partitions étendues
L'hypothèse précédemment formulée ne semble pas incohérente de prime abord. En théo-rie tout recouvrement R = {R 1 , . . . , R k } d'un ensemble d'objets X peut être obtenu par l'extension d'au moins une partition P = {P 1 , . . . , P k } telle que ?i ? 1, . . . , k , P i ? R i . En revanche, partant d'une partition P, l'ensemble des recouvrements possibles par extension correspond à une classe de recouvrements (notée C P ), qui n'est qu'un sous ensemble de tous les recouvrements possibles. Ainsi, considérer qu'un "bon" recouvrement (selon un critère W (.)) correspond nécessairement à l'extension d'une "bonne" partition (selon un critère V (.)) supposerait que : si P optimise le critère V (.) et R optimise le critère W (.) alors R ? C P Cette dernière propriété dépend bien sûr des critères V (.) et W (.) choisis. Nous montrons alors sur un exemple que l'on peut choisir des critères cohérents pour lesquels cette propriété n'est pas vérifiée.
Soit X = {x 1 , . . . , x 6 } un ensemble d'objets définis dans R 2 , présentés en figure 1 et que l'on souhaite organiser en deux classes (k=2). On pose V (.) et W (.) les critères objectifs pour l'évaluation respectivement d'une partition et d'un recouvrement et on les défini de la manière suivante :
FIG. 1 -Partition (à gauche) et recouvrement (à droite) optimaux selon les critères
On peut calculer que la partition P = {{x 1 , x 2 , x 3 }, {x 4 , x 5 , x 6 }} minimise le critère V (.) (V (P) = 12.0) et que R = {{x 1 , x 2 , x 4 , x 5 }, {x 2 , x 3 , x 5 , x 6 }} minimise le critère W (.) (W (R) = 12.0 également). Pourtant R n'est pas une extension de la partition P et n'appartient donc pas à la classe des recouvrements C P .
Nous venons donc de montrer par un exemple simple que la classification avec recouvrements ne se résume pas à étendre une partition par des affectations supplémentaires. Faire cette hypothèse consisterait à ne considérer qu'un sous-espace de l'espace de recherche d'une solution, ce dernier étant défini par l'ensemble des recouvrements possibles.
À la recherche d'un "bon" recouvrement
Définition du problème
Rechercher une partition P d'un ensemble X = {x 1 , . . . , x n } en k classes P 1 , . . . , P k selon un critère V (.) défini sur l'ensemble des partitions possibles est un problème NP-difficile dans la mesure où l'espace de recherche est de taille exponentielle (k n partitions possibles). L'algorithme bien connu des k-moyennes (MacQueen, 1967) propose une solution partielle au problème d'optimisation du critère des moindres carrés (aussi appelé critère de variance intra-classe) :
. Ce critère favorise les partitions dont les classes présentent une faible variance, autrement dit telles que les objets à l'intérieur d'une même classe sont faiblement dispersés. Cet algorithme procède par itérations de deux étapes (calcul des centres de classes puis affectation de chaque objet à son centre le plus proche) assurant la décroissance du critère et par la même, la convergence de la méthode vers une partition stable. La solution ainsi obtenue correspond à un minimum seulement local du critère et dépend de l'initialisation (tirage aléatoires de k centres) de l'algorithme.
Le problème de recherche d'un recouvrement minimisant un critère W (.) ne peut pas être considéré comme plus facile que le précédent puisque l'espace de recherche est de taille beaucoup plus importante (2 k.n recouvrements possibles). Par ailleurs le critère V (.) permettant d'évaluer la qualité d'une partition n'est plus adapté dans le cas des recouvrements car si R est un recouvrement de X en k classes on peut montrer que ?P, R ? C P ? V (P) ? V (R) ; un bon recouvrement selon V (.) ne pouvant alors être qu'une partition.
Dans cette étude, notre proposition porte ainsi sur la définition d'un nouveau critère de qualité d'un recouvrement d'une part et d'une solution algorithmique permettant d'approcher un recouvrement optimal selon ce critère d'autre part. Pour y parvenir nous nous inspirons de l'algorithme simple et efficace des k-moyennes.
Critère objectif pour les recouvrements
Pour définir un critère de qualité d'un recouvrement il est indispensable de se reporter aux motivations premières qui nous conduisent à rechercher ce type d'organisation. Dans le cas d'un document par exemple, choisir une classe thématique et une seule pour ce document peut réduire considérablement la représentation que l'on conservera de ce document dans la classification. En revanche, autoriser ce document à s'afficher selon plusieurs thèmes rendra une image certainement plus juste de son contenu. La qualité d'un recouvrement pourra alors être mesurée relativement à l'écart entre le contenu réel des objets et l'"image" que la classification (ici le recouvrement) établie renvoie d'eux. Nous formalisons cette intuition dans le critère suivant :
L'image d'un objet dans un recouvrement R est notée x i dans ce critère et correspond à un compromis entre les différentes classes auxquelles cet objet appartient. Ainsi pour un recouvrement R en k classes {R 1 , . . . , R k } de centres respectifs {c 1 , . . . , c k }, x i est défini par le centre de gravité de l'ensemble {c j |x i ? R j }.
L'algorithme OKM
L'algorithme OKM (Overlapping k-means) que nous détaillons dans cette section présente un squelette (figure 2) similaire à l'algorithme des k-moyennes. L'initialisation qui consiste à tirer aléatoirement k centres puis à dériver un premier recouvrement est suivie par l'itération de deux étapes : (1) la mise à jour des centres de classes puis (2) l'affectation des objets à ces centres.
L'intérêt de l'algorithme OKM réside dans la méthode employée pour Mettre_à_jour les centres et pour Affecter chaque objet à un ou plusieurs centres. Ces deux opérations doivent d'une part assurer la cohérence des classes en regroupant ensemble des objets similaires et d'autre part permettre la convergence de la méthode par décroissance du critère W (.).
Étant donné un ensemble C = {c 1 , c 2 , . . . , c k } correspondant aux centres des k classes respectives R 1 , R 2 , . . . , R k d'un recouvrement R, la méthode d'affectation d'un objet x i , pré-sentée en figure 3 consiste à parcourir l'ensemble des centres de classes du plus proche au plus éloigné (suivant une métrique d) et à affecter x i tant que son image est améliorée (d(x i , x i ) diminue). La nouvelle affectation de l'objet x i ne sera finalement conservée que si l'image de x i s'en trouve améliorée par rapport à l'ancienne affectation. Cette dernière précaution permet d'assurer la décroissance du critère W (.) lors de l'étape d'affectation.
Affecter(x i ,C) :
Initialisation : Soit c * le centre de C le plus proche de
Soit c * le centre de C le plus proche de x i et x i A le centre de gravité des éléments de A,
alors affecter x i aux centres de A, Sinon conserver l'ancienne affectation A .
FIG. 3 -Méthode d'affectation utilisée dans l'algorithme OKM.
Enfin, la mise à jour du centre c j de la classe R j est définie dans l'algorithme OKM par :
(1)
Dans cette expression, c j,v désigne la v ième composante du vecteur c j , ? i correspond au nombre de classes de R auxquelles x i appartient etˆxetˆ etˆx i j v symbolise la v ième composante du centre c j "idéal" pour l'objet x i , c'est à dire le centre c j tel que d(x i , x i ) = 0. De façon plus précise on a ˆ
où A désigne l'ensemble des centres des classes auxquelles x i appartient. Il découle de ce qui précède une définition plus intuitive du nouveau centre c j qui correspond finalement au centre de gravité du nuage de points {(
. On montre que chaque mise à jour d'un centre dans OKM permet d'assurer la décroissance du critère W (.) mais également que le nouveau centre calculé est celui qui minimise ce critère.
Preuve Soient X un ensemble d'objets définis dans (R p , d) où d est la distance euclidienne, et R un recouvrement de X en k classes de centres c 1 , . . . , c k . L'étape de mise à jour dans OKM consistant à recalculer chaque centre un par un, il suffit alors de montrer que le recalcul d'un nouveau centre c j quelconque minimise le critère
Par réécriture du terme x i et décomposition de la somme sur les objets de X on obtient :
Pour les objets n'appartenant pas à la classe R j , leur image x i est indépendante de c j , le premier terme est donc constant relativement à c j . Le second terme constitue une fonction quadratique de c j qui sera alors minimisée pour une dérivée égale à 0.
. Notons pour conclure sur la présentation de l'algorithme, que la méthode des k-moyennes peut être considérée comme un cas particulier de OKM. En effet si on restreint dans OKM chaque objet à n'appartenir qu'à une seule classe (? i =1) on retrouve exactement le processus de classification utilisé dans l'algorithme k-moyennes. Il s'agit donc d'un algorithme nondéterministe puisque le résultat dépendra de l'initialisation ; de plus, chaque classe n'étant plus indépendante l'une de l'autre dans un recouvrement, l'algorithme OKM dépendra également de l'ordre de parcours des classes lors de l'étape de mise à jour des centres.
Évaluations et applications
L'évaluation des méthodes de classification non-supervisée reste un problème entier dans ce domaine de recherches. Une piste possible pour évaluer (au moins partiellement) une telle méthode est de mesurer sa capacité à retrouver un schéma de classification préétabli ; nous l'utiliserons pour évaluer l'algorithme OKM en insistant toutefois sur les précautions qu'il s'impose de prendre lors de l'interprétation des résultats quantitatifs, notamment du fait de l'hypothèse non vérifiée que l'ensemble de descripteurs est pertinent pour établir la classification attendue. Les courbes de la figure 4 mettent en évidence un phénomène prévisible : la convergence plus lente pour l'algorithme OKM vers un recouvrement stable des données (19 itérations), par rapport à k-moyennes qui obtient une partition stable en seulement 12 itérations. On retiendra cependant que dès la cinquième itération, les deux méthodes ont généré un résultat de qualité qui évoluera peu par la suite.
Observation du fonctionnement de l'algorithme
X X X X X X X X X X La figure 6 présente la matrice de confusion et nous révèle que les deux méthodes identifient correctement les trois catégories d'iris puisque chacune des classes contient majoritairement l'une de ces trois catégories. Il est reconnu que, sur cette base de données Iris, la catégorie des "Iris Setosa" est plutôt facile à identifier tandis que les deux autres catégories sont répu-tées difficilement séparables, ce que l'on observe sur la figure 5. Ces phénomènes se vérifient également sur notre expérimentation :
Étiquettes
-les 50 individus de la catégorie "Iris Setosa" se retrouvent exclusivement dans la classe n?3 avec les deux méthodes. Dans le recouvrement obtenu avec OKM, cette classe contient 9 individus de la catégorie "Iris Versicolour" en supplément, qu'elle partage avec les autres classes
5
(faible intersection) ; -la séparation difficile des deux autres classes se manifeste par des erreurs de classification si l'on cherche à partitionner les données (k-moyennes) et par une intersection importante entre les deux classes lorsque ces données sont organisées en classes recouvrantes (OKM). Sur cette première expérimentation, nous avons d'une part observé un comportement satisfaisant de l'algorithme OKM et d'autre part noté que la structuration en classes recouvrantes fournit un résultat informationnel plus riche qu'une simple partition, notamment en ce qui concerne l'organisation des classes entre elles.
Classification de documents multi-thématiques
Comme nous l'avons mentionné en introduction, les recherches menées autour de la classification avec recouvrements des classes sont motivées par des besoins apparaissant dans des domaines d'application où des données peuvent appartenir à plusieurs catégories prédéfinies. Dans cette seconde expérimentation, nous évaluons l'impact de notre contribution dans le domaine de la Recherche d'Information et plus précisément pour la classification de documents multi-thématiques.
L'expérimentation est conduite sur la collection de documents Reuters 6 initialement composée de 21578 articles journalistiques en langue anglaise. Chaque document peut être étiqueté par une ou plusieurs étiquettes parmi un ensemble de 114 catégories. Après filtrage, nous avons retenu 2739 documents, en ne choisissant que ceux pour lesquels au moins une catégorie est proposée, dont le corps de l'article n'est pas vide et appartenant au sous-ensemble "TEST" selon la répartition suggérée dans Apté et al. (1994).
FIG. 7 -Classification de documents issus de la collection Reuters.
La représentation des documents est l'aboutissement d'une chaîne de traitements usuelle en Recherche d'Information : chaque document est représenté par un vecteur de dimension m dans lequel chaque composante x i,v correspond à l'information mutuelle du mot 7 w v pour le document x i . Une étape préalable de filtrage des descripteurs consiste à ne sélectionner que les m mots d'information mutuelle supérieure à un seuil ? fixé. Enfin, la similarité entre deux documents est évaluée au moyen du cosinus de Salton (Salton et McGill, 1983) :
L'évaluation que nous proposons consiste en des séries de 10 exécutions des algorithmes OKM et k-moyennes dans des conditions initiales identiques, sur un sous-ensemble de 300 documents pour un nombre de classes variant de 5 à 30. On appelle association issue de R, une paire d'objets appartenant à une même classe de R ; on dira de plus que cette association est correcte si ces deux objets contiennent au moins une étiquette de catégorie en commun dans la classification préétablie. Chaque partition ou recouvrement R est alors évalué relativement au nombre d'associations de R (noté n a ), qui sont correctes (n b ) par rapport au nombre total d'associations correctes attendues (n c ). Nous recourons aux indicateurs traditionnels en Recherche d'Information : la précision, le rappel et l'indice de F score (avec ?=1).
Nous présentons en figure 7 les moyennes comparatives des mesures de F score obtenues sur les recouvrements générés par OKM d'une part et les partitions obtenues avec k-moyennes d'autre part. La diminution observée du F score s'explique dans les deux méthodes par la ré-duction logique du nombre d'associations (et donc du rappel) lorsque le nombre de classes augmente. Pourtant, si dans le cas des partitions, l'augmentation en précision ne permet pas de compenser la diminution importante du rappel, cette compensation est possible lorsqu'il s'agit de recouvrements du fait d'une perte de rappel atténuée sous l'effet des intersections.
Conclusion et Perspectives
Cette étude part du constat suivant : les méthodes de classification actuelles ne sont pas adaptées à la recherche d'une organisation des données en classes recouvrantes ; ce type de schéma de classification devient pourtant indispensable pour appréhender les domaines d'application actuels tels que les documents multimédia ou les données biologiques.
Nous avons alors proposé une première solution visant à rechercher dans l'ensemble des recouvrements possibles des données, un schéma correspondant au mieux à l'organisation de ces données. Cette proposition s'appuie d'une part sur la définition d'un critère objectif permettant d'évaluer les recouvrements, et d'autre part sur une méthode d'exploration de cet espace des possibilités (l'algorithme OKM).
Des expérimentations menées sur deux ensembles de données ont mis en évidence la cohé-rence globale de la méthode proposée (sur les données Iris) et justifié de l'intérêt d'organiser les données en classes recouvrantes afin d'en conserver une synthèse riche en informations (sur les données Reuters). Cependant cette première contribution suggère plusieurs améliorations et perspectives importantes à mener.
Tout d'abord on peut noter que, afin d'assurer la convergence du critère objectif, la méthode d'affectation proposée dans OKM favorise mais ne garantie pas que chaque objet soit affecté uniquement à ses centres les plus proches (cf. figure 3). Si cette situation est en pratique suffisamment rare pour ne pas remettre en cause la cohérence globale du schéma, il conviendra de proposer une solution théorique à ce problème.
Nous serons également amené à confirmer la justification de cette approche en montrant sur des études comparatives plus larges son intérêt par rapport à d'autres méthodes mentionnées dans ce papier, en particulier les algorithmes CBC, POBOC ou encore des algorithmes de classification floue complétés par une étape supplémentaire d'affectation.
Enfin, nous envisageons d'étudier l'intégration d'une pondération différente des descripteurs pour chaque classe en construction (Modha et Spangler, 2003). Cette perspective s'appuie sur l'hypothèse qu'un objet multi-classé doit l'être sur la base de critères différents.
Références
Apté, C., F. Damerau, et S. M. Weiss (1994) 

Introduction
Les réseaux sociaux sont des systèmes complexes dont certains ont des structures maintenant bien identifiées : graphes de petits mondes et graphes sans échelle typique. Un graphe sans échelle typique est un graphe dont la distribution des degrés n'est pas groupée autour d'une valeur moyenne ; c'est le cas lorsque celle-ci suit une loi de puissance. Les études menées sur le world wide web, des réseaux de courrier électronique ou des réseaux P2P , le réseau des collaborations scientifiques , le réseau des relations sexuelles en sont des exemples (Bornholdt et Schuster, 2003). Les graphes sans échelle typique ont peu de sommets de degrés très élevés et beaucoup de faible degré, ces graphes ont la propriété de présenter des fluctuations locales des degrés d'autant plus importantes que la distribution des degrés est proche d'une loi de puissance. Si les sommets de forts degrés sont connectés entre eux on parle alors de phénomène de "club huppé"
Alors que les études ont en général été effectuées sur des réseaux sociaux contemporains nous analysons ici un réseau relatif à la paysannerie médiévale. Nous travaillons sur une base de contrats agraires signés d'une part entre 1240 et 1350 et d'autre part entre 1450 et 1520 dans une petite région du Sud-Ouest de la France. Cette base pour l'instant réduite à environ 700 actes sera amenée à plus de 8000 actes lorsque le travail de saisie et de désambiguïsation sera terminé. Les sommets du graphes sont les paysans et ils sont liés s'ils apparaissent dans un même contrat, nous définissons ainsi deux graphes G av et G ap ; nous avons éclairci la base en enlevant les seigneurs de notre étude. Nous ne possédons pas de données entre 1350 et 1450, intervalle temporel correspondant à la guerre de Cent Ans.
La notion de communauté varie en fonction du réseau que l'on étudie (Palla et al., 2005;Newman, 2006). Nous supposerons dans notre étude que les communautés sont constitués d'individus qui ont à la fois les mêmes liens à l'intérieur de la communauté (clique) et à l'exté-rieur de la communauté. Nous verrons dans la section 3 que cette définition assez contraignante permet pourtant de révéler une structuration très particulière de notre réseau.
Cet article est constitué de deux parties : nous allons tout d'abord commencer par vérifier si notre graphe partage les propriétés rencontrées dans les grands réseaux d'interaction (l'effet petit monde, la distribution des degrés et le phénomène de club huppé). Ensuite nous nous attarderons sur la détection et l'organisation des communautés grâce à des méthodes spectrales. Enfin en conclusion, nous ébaucherons une comparaison des graphes avant et après la guerre de Cent Ans.
Les indices des deux réseaux d'interaction
L'effet petit monde
L'effet petit monde regroupe deux propriétés : la première énoncant que la distance entre deux sommets quelconques est faible (ceci est relatif à la connectivité globale) et la deuxième que la connectivité locale est forte. Pour quantifier ces notions nous utiliserons dans le premier cas soit la moyenne des plus courts chemins < l > soit la longueur caractéristique L (médiane des moyennes des plus courts chemins de chaque sommet (Watts, 2003)) et dans le deuxième cas la moyenne C 1 des densités du graphe des voisins de chaque sommet (Watts, 2003).
Le tableau TAB. 1 résume les résultats obtenus sur les graphes des liens de sociabilités paysans avant et après la guerre de Cent Ans et les met en perspective avec d'autres exemples de réseaux dont les densités d'arêtes sont voisines. Dans le cas de nos deux réseaux signalons qu'ils ont respectivement un diamètre de 5 et de 6 et que 90% des paires de sommets sont à une distance inférieure ou égale à 3.  Iamnitchi et al. (2004), (b)  Montoya et Solé (2002), (c)  Watts et Strogatz (1998).
Les coefficients de clustering de nos réseaux sont sensiblement plus forts que ceux rencontrés habituellement.
La distribution des degrés
L'objectif de cette partie est de modéliser la distribution des degrés de nos graphes. Afin de lisser les fluctuations nous étudions la distribution cumulative des degrés P c (k) = ? j=k P (j) où P (j) est la probabilité d'avoir un sommet de degré j. Ce choix permet aussi de repérer plus aisément un degré de coupure éventuel au delà duquel la distribution décroit plus vite (Pastor-Satorras et Vespignani, 2004). Si beaucoup de réseaux récemment étudiés montrent une distribution cumulative des degrés qui suit une loi de puissance, la présence d'un degré de coupure est le signe d'un écart à cette loi. L' ajustement de la distribution par une loi de puissance tronquée par une coupure exponentielle (TPL) peut alors permettre de mieux expliquer l'ensemble de la distribution ; citons par exemple (Amaral et al., 2000;Achard et al., 2006).
Nous testons trois lois pour ajuster la distribution : loi de puissance figure FIG. 1  
TAB. 2 -Coefficients et erreurs quadratiques moyennes (10 ?3 ) des différents modèles d'ajustement de la distribution cumulative des degrés.
Le meilleur ajustement de nos données est obtenu pour une TPL. Le graphe G av échappe à une distribution des degrés en loi de puissance. Cette distribution est assez bien ajustée par une loi exponentielle même si on améliore l'erreur quadratique avec une TPL. Pour G ap une loi de faible puissance donne de meilleurs résultats que pour G av mais une TPL reste la meilleure modélisation.
L'effet « club huppé »
Nos deux graphes G av et G ap possèdent un club-huppé (Zhou et Mondragón, 2004) c'est-à-dire que les sommets de forts degrés (« les riches ») forment ensemble un sous-graphe dense. Ces individus jouant un rôle important dans l'organisation du réseau, les indices de centralité de proximité et de centralité d'intermédiarité (Degenne et Forsé, 1994)  
Recherche des communautés
L'effet petit monde avec un coefficient de clustering élevé associé à une faible densité du graphe nous indique la présence de communautés ; afin de les déceler nous étudions le spectre du laplacien (non normalisé). Nous définissons nos communautés ainsi : 
La démonstration consiste à étudier la matrice binaire A + I qui est de rang 2 dans le cas (i) et de rang 3 dans le cas (ii). On procède par épuisement des cas.
En utilisant le théorème énoncé dans van den Heuvel et Pejic (2000) et le théorème 1 précédent, nous extrayons pour le graphe G av 28 communautés de taille supérieure ou égale à 3 dont la plus importante est de taille 15 et pour le graphe G ap 31 communautés dont la plus grande est de taille 7.
En supprimant la partie du graphe ne contenant aucune communautés (cette partie contient le club-huppé) nous obtenons un graphe à plusieurs composantes connexes. Les communautés trouvées via l'étude du spectre ne sont donc guère liées entre elles, elles sont préférentielle-ment liées à la partie que nous avons otée et notamment au club huppé. Nous avons donc une structure inter-communautaire proche de celle d'une étoile. Le centre de cette étoile contient le club-huppé dont on visualise bien à présent le rôle central qu'il joue dans l'organisation du ré-seau social. Ce partitionnement en club-huppé et communautés permet une bonne visualisation des graphes (FIG. 2) 
Conclusion
Malgré le fait d'avoir enlevé les seigneurs de notre étude, nous constatons dans chacun des deux graphes G av et G ap la présence d'un groupe d'individus (le club huppé) possédant un rôle central. Ces deux graphes apparaissent sous forme d'une étoile de communautés. Dans G av , un fort nombre de communautés de petite taille cohabitent avec un nombre significatif de communautés plus importantes, ce qui explique le bon ajustement des distributions des degrés avec une TPL. Concernant G ap , la structuration est moins claire : le résidu est sensiblement plus important et la taille des communautés moins variable.
Si certaines de ces communautés correspondent à des zones géographiques comme on peut le voir dans (Hautefeuille, 2001), d'autres n'ont pour l'instant pas trouvé d'explications.
On remarquera un renouvellement quasi-complet des noms du club-huppé entre G av et G ap : la famille Combelcau très influente avant la guerre disparaît complètement après la guerre laissant la place à la famille Limairac, nouvelle famille qui paraît très influente.
FIG. 2 -Graphe des communautés de G av (à gauche) et G ap (à droite). Les disques repré-sentent les k-communautés extraites et le rectangle le reste des sommets (dont le club-huppé).

Introduction
RAS
1 , Reference Annotation System est un outil semi-automatique d'annotation de documents basé sur le contexte de citation, l'expert du domaine reste décideur de la fiabilité de l'annotation. L'approche d'annotation permet d'annoter un document sans connaissance préa-lable de son contenu, en se basant sur les références. Cet outil a été réalisé dans le contexte d'un besoin réel, celui d'une communauté souhaitant partager l'information existante et ceci sous certaines contraintes, la plus importante étant celle de l'absence de contenu des documents à partager. Afin de tester les résultats de l'annotation, nous avons utilisé une base avec un nombre important de documents qui s'inter-référencent. L'outil utilise les technologies suivantes :
-Python 2 comme langage de script ; -la base documentaire Citeseer 3 ; -L'ontologie dmoz 4 (informatique) ; -l'algorithme de classification fuzzy C-means Dunn (1973  Summary RAS (Reference System Annotation) is a documents annotation tool. This tool is the result of the implementation of our approach of annotation based on the context of citation. The approach is independent of the content and uses a regrouping set of themes of the references builds starting from a not-supervised fuzzy classification. The tool presented in this article was tested and evaluated with the base of scientific documents Citeseer.

Introduction
Le Web 2.0 a pour objectif de faciliter l'accès à l'information en représentant les documents Web par une structure sémantiquement riche et non par un traditionnel « sac de mots ». Cette structure est généralement définie par la représentation des documents sous forme d'arbres : des éléments de contenu, identifiés par la séquence étiquetée des feuilles de l'arbre, sont organisés selon une structure prédéfinie par un ensemble de noeuds internes représentant les relations entre éléments. Cette structure traduit les relations sémantiques ou logiques entre éléments de contenu. Les comparateurs de prix, le Web Sémantique sont des exemples de services fournis par le Web 2.0.
La plupart des documents du web utilisent des formats semi structurés comme le HTML, le XML, le PDF ou encore le WikiText. Ces formats permettent d'enrichir le texte à l'aide de balises et une interprétation directe de celles-ci permet de décrire les documents par un arbre, l'arbre DOM. Nous appellerons structure syntaxique cette structure directement liée à la manière dont l'information est codée. Les applications du Web 2.0 ne peuvent toutefois pas tirer directement profit de cette structure : elles ont toutes besoin de connaître à priori la structure utilisée et ne sont donc capables de ne traiter que les documents respectant strictement un schéma qui leur est spécifique. Ce schéma définit les structures que peuvent avoir les documents. Il est peu probable que les documents Web respectent un schéma donné à priori : en général, ceux-ci proviennent de plusieurs sources hétérogènes, utilisant chacune des structures différentes. De plus, dans le cas du HTML leur format ne contient que des informations de mise en page peu informatif pour ces applications. L'utilisation directe de la structure syntaxique par les application du Web 2.0 est donc impossible. Pour les rendre utilisables, il faut transformer ces sources hétérogènes en un format médiateur spécifique de l'application. C'est l'objet du travail que nous présentons. Nous nous intéressons plus spécifiquement à la conversion automatique de documents HTML vers un format XML prédéfini. Cette problématique spécifique tire son intéret de la masse d'information présente sur le web sous un format HTML.
Bien que moins riche que celle de nombreux formats semi-structurés, l'information de mise en page présente dans le HTML fournit une information qui est exploitée quotidiennement par de nombreux utilisateurs, notamment pour faciliter leur navigation ou la recherche d'information. En effet, avec le développement des sites basés sur des systèmes de gestion de contenu (blogs, site de nouvelles, ...), de plus en plus de pages sont générées automatiquement à partir de bases de données. Par leurs régularités, la mise en page des documents reflète leur structure logique et permet d'identifier des éléments (un titre, un commentaire, ...) ainsi que des relations entre ceux-ci (on peut par exemple préciser l'auteur d'une sous-partie du document). Ce nouveau type d'information, directement lié à la présentation des documents, peut, par exemple, être utilisé pour organiser les commentaires des visiteurs d'un site en threads (Figure 1)  Pour exploiter cette information additionnelle nous proposons de transformer les documents Web vers un format médiateur. Cette structure sera spécifiée par un schéma cible dépen-dant de l'application considérée. L'écriture manuelle de convertisseurs spécifiques à chaque source de documents et à chaque application est un travail présentant un grand risque d'erreur et qui est peu adapté à la richesse et à la nature dynamique du Web. Plusieurs solutions (Doan et al., 2003;Chung et al., 2002)  
de documents exprimés à la fois dans leur structure d'origine et dans la structure cible. Une des approches les plus prometteuses, (Chidlovskii et Fuselier, 2005), a formulé cette tâche comme une généralisation de l'analyse syntaxique : le document d'entrée est représenté par une séquence d'observations correspondant aux feuilles du document HTML ; la structure du document de sortie est reconstruite à partir de cette représentation grâce à une grammaire hors-contexte décrivant le schéma de sortie. Dans cette approche, la structure du document d'entrée est cependant ignorée, même si, comme nous le montrerons, elle fournit une information indispensable pour déterminer la bonne structure de sortie.
Dans ce travail, nous proposons une approche plus générale qui permet de considérer des caractéristiques arbitraires décrivant à la fois la structure de sortie et la structure d'entrée des documents. Nous commencerons par décrire un cadre général de transformation de documents fondé sur des techniques d'apprentissage structuré (paragraphe 2), puis nous détaillerons notre modèle (paragraphe 3). Nous présentons enfin un ensemble d'expériences prospectives sur plusieurs corpus de documents réels (paragraphe 4). 
La problématique de restructuration
Dans l'Équation 1, l'argmax traduit le parcours de l'espace de toutes les restructurations potentielles D(d in ) pour rechercher la meilleure solution. Cet ensemble doit inclure l'ensemble des segmentations et des réorganisations des noeuds de contenu (notamment les permutations, fusions et séparations de noeuds) et tous les arbres compatibles avec cette nouvelle organisation. La taille de l'espace de recherche est donc exponentielle par rapport au nombre de noeuds de contenu : en se limitant aux permutations de n noeuds, l'espace de recherche contient déjà n! éléments.
Pour trouver la meilleure solution en un temps raisonnable, plusieurs sources d'informations doivent être considérées pour élaguer l'espace de recherche : le contenu du document source, sa structure et la définition du schéma cible. Ainsi, sur l'exemple de la Figure 2, on peut dire que la restructuration est guidée par la structure du document d'entrée -les deux premières feuilles décrivent le même personnage, puisqu'ils ont le même parent -et contrainte par la structure cible -chaque personnage est composé d'un acteur et d'un nom : l'information apportée par les structures d'entrée et de sortie sont essentielles pour la tâche de restructuration. Dans la suite, nous supposerons que F est linéaire par rapport à ? :
. Avec cette hypothèse, la tâche de restructuration peut être vue comme un problème d'apprentissage structuré (Tsochantaridis et al., 2004). L'apprentissage structuré est une généralisation de l'apprentissage multi-classe permettant de traiter des problèmes dans lesquels les entrées et les sorties peuvent être décomposées en un ensemble de sous-parties inter-dépendantes. Utiliser des techniques d'apprentissage structuré est intéressant puisque ce formalisme nous fournit plusieurs méthodes pour apprendre à partir de caractéristiques arbitraires décrivant les structures d'entrée et de sortie. Les algorithmes de l'apprentissage structuré souffrent en général d'une complexité élevée. Cela prohibe leur utilisation pour des tâches complexes comme la tâche de restructuration et pour traiter des corpus de grande taille. Nous allons maintenant présenter une méthode d'apprentissage structuré qui présente l'avantage d'avoir une complexité suffisamment faible pour pouvoir être appliquée au problème de la restructuration de corpus de grande taille.
Modèle d'apprentissage
La méthode que nous proposons repose sur l'observation suivante : l'apprentissage structuré, tel qu'il est décrit par l'Équation 1, nécessite de construire un ensemble très grand de structures combinatoires et de retrouver celle de plus grand score. Nous proposons de considérer ces deux étapes séquentiellement. Plus précisément, l'algorithme proposé enchaîne deux étapes :
une étape de génération qui va construire GEN (d in ) un ensemble de N solutions candidates. Ce processus repose sur des hypothèses d'indépendance fortes entre les éléments des documents et sur l'utilisation d'un ensemble restreint de caractéristiques pour permettre une construction efficace des solutions candidates à l'aide d'un algorithme basé sur la programmation dynamique. une étape d'ordonnancement qui va trouver la meilleure restructuration parmi les solutions candidates GEN (d in ) générées à l'étape précédente en considérant des caractéristiques arbitraires aussi bien du document d'entrée que de la solution candidate. En particulier, puisqu'on travaille l'univers restreint GEN (d in ), on pourra considérer des caractéris-tiques globales des arbres d'entrée et de sortie, ce que ne permet pas la programmation dynamique.
Traiter ces deux étapes de manière séquentielle permet de conserver les avantages de la programmation dynamique (construction efficace d'une sortie structurée, mais qui ne considére que des caractéristiques locales) tout en considérant des caractéristiques globales lors de la seconde étape pour sélectionner la meilleure restructuration. Cette approche ne fournit qu'une solution approchée de l'Équation 1, puisque seules les solutions les plus prometteuses de la première étape sont évaluées. Toutefois, elle a déjà montré son efficacité dans plusieurs tâches de langue naturelle (Collins et Koo, 2005). Nous allons maintenant détailler ces deux étapes.
Génération des solutions candidates
La première étape de notre modèle a pour objectif de construire la structure du document de sortie à partir de la séquence des noeuds de contenu c = (c 1 , ..., c #c ). Cette construction peut se faire facilement avec des algorithmes d'analyse syntaxique (Jurafsky et Martin, 2000). L'utilisation de ces algorithmes est d'autant plus intéressante que les structures arborescentes des documents semi structurés se modélisent naturellement par des grammaires horscontexte (Chidlovskii et Fuselier, 2005). En effet, celles-ci permettent de décrire simplement des structures récursives à la fois horizontale (une section comporte plusieurs sous-sections) et verticale (il est possible de faire des listes de listes). De plus, elles permettent aussi de formaliser l'observation suivante : on peut identifier un élément soit par son contenu (une taille est définie par un nombre et une unité), soit par les éléments qui le constituent (une date est composée d'un jour, d'un mois et d'une année, même si la représentation et l'ordre de ces éléments peuvent varier).
L'utilisation d'une version probabilisée de cette grammaire (PCFG) permet, en outre, de caractériser les régularités et la variabilité du schéma cible : il est possible, par exemple, de modéliser le fait qu'une section regroupe un ensemble de sous-sections, et que, le nombre de sous sections suit une distribution donnée.
Formellement, une PCFG décrivant un document semi structuré est définie par le quintuplet G = ?, R, S, P c où :
-? est un ensemble de non-terminaux décrivant les étiquettes des noeuds internes (qui correspondent aux relations) ; -? est un ensemble de terminaux définissant les étiquettes des feuilles (qui correspondent aux noeuds de contenu) ; -S, un élément de ?, est le symbole initial décrivant la racine de l'arbre ; -R est un ensemble de productions dont chaque élément est associé à une probabilité.
Chaque production est une relation de ?×(???) * qui décrit les règles de composition : la règle html ? head body indique qu'un élément html regroupe (dans cet ordre) un élément head et un élément body ; -P c est un modèle de contenu qui défini une distribution de probabilité sur ? pour chaque noeud de contenu. Intuitivement, cette distribution permet de rajouter un ensemble de productions du type ? ? c i pour tous les éléments ? de ? et tous les éléments de contenu c i . Ces productions permettent de déterminer l'étiquette des feuilles. La Figure 3 montre un exemple de schéma cible et de la PCFG qui lui est associée.
Une PCFG permet d'associer une probabilité à chaque structure de sortie t. Cette probabilité mesure la compatibilité entre la structure arborescente et la séquence d'observations. Elle est définie par :
où r est l'ensemble des productions utilisées pour construire l'arbre t, c est la séquence des noeuds de contenu du document. L'estimation des probabilités p(r) sera détaillée dans le prochain paragraphe. La Figure 4 donne un exemple du calcul de cette probabilité.
Un algorithme d'analyse syntaxique permet de reconstruire efficacement les N -meilleures restructuration associées à une séquence de noeuds de contenu. Nous avons utilisé une exten-   (Jimnez et Marzal, 2000) pour construire les N meilleures solutions avec une complexité O(n 3 + N · #R #? · n · log n 3 ). Généralement, dans nos expériences N < n, cette complexité est donc du même ordre de grandeur que celle de l'algorithme reconstruisant la meilleure solution, qui est en O(n 3 ).
<!ELEMENT NEWS (HEADER BODY) > <!ELEMENT HEADER ( a u t h o r t i t l e | a u t h o r t i t l e d a t e ) > <!ELEMENT BODY
Apprentissage des paramètres L'apprentissage se fait à partir d'un ensemble de documents exprimés dans le schéma cible. Les paramètres décrivant les éléments de R sont estimés par maximum de vraisemblance (Jurafsky et Martin, 2000). La probabilité d'une production A ? ? est donnée par :
p(A ? ?) = #{A ? ?} A???R #{A ? ?} où #{A ? ?} correspond au nombre d'apparition de cette production dans le corpus d'apprentissage, A est un élément de ? et ? de ? ? ? * . Les probabilités de contenu sont estimées dans notre modèle par un classifieur maximisant l'entropie. Ce type de classifieur nous permet de prendre en compte facilement toutes les caractéristiques que nous jugeons pertinentes, sans nécessiter d'hypothèses d'indépendance entre celles-ci. Ces caractéristiques peuvent dépendre à la fois du noeud de contenu c et de l'étiquette ?. Nous pouvons donc définir aussi bien des caractéristiques décrivant le contenu des noeuds (nombre de majuscules, présence de chiffre, ...), leur contexte (le nombre de frères, la profondeur dans l'arbre, ...) et le type de données spécifié par le schéma. La Table 1 détaille une partie des caractéristiques utilisées.
Plus précisément, on a : 
FIG. 4 -Exemple de deux restructurations potentielles (les arbres (b) et (c)) de l'arbre (a). Avec la PCFG de la Figure 3, le score du document (b) est
où f est le vecteur de caractéristiques décrit au paragraphe précédent,? une étiquette, Z ? (c) est un coefficient de normalisation, ? le vecteur des paramètres à estimer et
Le vecteur des paramètres ? a été estimé en utilisant le principe de maximisation de l'entropie : l'algorithme d'apprentissage détermine, parmi toutes les distributions compatibles avec les observations, celle qui fait le moins d'hypothèse sur les valeurs non observées.
caractéristiques de contenu caractéristiques du contexte caractéristiques sur le type de données contains-http is-only-child is-xs_string begins-with-capitals has-1-to-3-siblings is-xs_duration contains-number is-descendant-of-title is-xs_time contains-1-to-5-spaces ...
TAB. 1 -Exemples de caractéristiques utilisées pour décrire les noeuds de contenu. La figure représente trois types de caractéristiques : contenu (e.g. une séquence contient "'http"'), contexte (e.g. ce noeud n'a pas de frère), type de données (e.g. type string selon la définition de la norme XML Schema).
Étape d'ordonnancement
in ; w), qui permet d'ordonner l'ensemble des solutions candidates. Ce score va notamment nous permettre de prendre en compte des caractéristiques globales sur d, et d in . Le vecteur de paramètres w est estimé à partir d'un ensemble d'apprentissage constitué par : -un ensemble de n documents, chaque document étant exprimé à la fois dans sa structure d'origine et dans la structure cible. Nous noterons T = d in cet ensemble ;
-pour chaque élément de T les N solutions candidates :
j=1 qui sont construites par la première étape. Nous supposerons, sans perte de généralité, que d i 1 est la meilleure restructuration pour d in 1 . L'apprentissage est effectué par un perceptron à noyaux (Collins et Duffy, 2002;Collins et Koo, 2005). C'est une méthode simple mais efficace, qui permet, grâce à l'utilisation d'un noyau, de considérer des espaces de grandes dimensions. Le score calculé par le perceptron à noyau pour une solution candidate d associée au document d'entrée d in est :
avec k une fonction noyau qui sera explicitée dans le paragraphe suivant, ? ij les paramètres appris, d 
Caractéristiques utilisées
Deux types de caractéristiques sont envisageables pour discriminer la meilleure solution candidate. Un premier type de caractéristiques décrit les dépendances à longue distance entre les noeuds de l'arbre, afin de décrire chaque noeud par un contexte plus riche que celui utilisé dans l'étape de génération. Un deuxième type de caractéristiques permet de mesurer une similarité entre le document d'entrée et la solution candidate. En effet, dans le cas de documents Web majoritairement textuel, certains groupes d'éléments doivent être conservés lors de la transformation, pour garder un sens.
C'est pourquoi nous allons utiliser une combinaison de deux noyaux :
L'addition de deux noyaux traduit la concaténation des deux espaces de caractéristiques. Le premier noyau, k tree est le noyau d'arbre de (Collins et Duffy, 2001) qui capture les dépendances à long terme entre les noeuds d'un arbre. Contrairement à une PCFG qui ne considère que les dépendances entre un noeud et ses fils, le noyau d'arbre considère l'ensemble de l'arbre pour modéliser le contexte d'un noeud : un arbre sera représenté par l'ensemble de ses sous-arbres. Le nombre de sous-arbres d'un arbre est exponentiel par rapport à la taille d'un arbre, mais, une solution basée sur la programmation dynamique permet de réaliser le comptage sans avoir à énumérer tous les sous-arbres.
Le second noyau utilisé, k f eatures est un noyau RBF (Radial Basis Function). Il utilise des caractéristiques globales à la fois sur le document d'entrée et sur la solution candidate. Ces caractéristiques incluent :
-une comparaison entre le nombre de noeuds du document d'entrée et du document de sortie -les couvertures communes au document d'entrée et au document de sortie. La couverture est définie, pour chaque noeud d'un arbre par la paire constituée de la position dans la séquence des feuilles de la première et de la dernière feuille du sous-arbre ayant ce noeud comme racine. De manière intuitive, les couvertures permettent de résoudre des problème comme ceux présentés Figure 4 ; -des contraintes imposées par le schéma cible (par exemple un film ne peut avoir qu'un titre mais plusieurs réalisateurs). Ces contraintes sont déduites automatiquement du schéma cible. -le score de la première étape Les valeurs de ces caractéristiques sont combinées à l'aide d'un noyau puis incorporées dans le noyau global.
Expériences
Nous avons testé notre modèle sur deux corpus différents. Le premier corpus est un ensemble de nouvelles publiées sur un site traitant d'actualité informatique LinuxFr (http: //linuxfr.org). Les pages du site ont été téléchargées et converties, à la main, en XML pour un schéma prédéfini. Chaque page correspond à une nouvelle et comporte un en tête regroupant les méta-informations (auteur, titre, date, ...), le corps de la nouvelle et plusieurs threads de commentaires des visiteurs du site. Le corps de la nouvelle comme les commentaires peuvent utiliser la plupart des tags HTML. La partie décrivant les commentaires des utilisateurs est très fortement structurée et présente un défi à la transformation du document : la structure logique des commentaires (i.e. : de quel commentaire un commentaire donné est-il la réponse) doit être reconstruite à partir de la structure du document d'entrée. Le corpus comporte 200 nouvelles, chaque nouvelle ayant, en moyenne, 70 noeuds de contenu et 50 noeuds internes. Le plus grand document a 165 noeuds de contenu et 114 noeuds internes. Le schéma cible définit 13 étiquettes possibles pour les noeuds de contenu et 11 pour les noeuds internes.
Le second corpus est basé sur les données d'IMDb (http://imdb.com). 710 descriptions de films ont été téléchargées et converties manuellement en XML suivant un schéma donné. Chaque description comporte, en moyenne, 35 noeuds de contenu et 35 noeuds internes ; la plus grande en a, respectivement, 212 et 211. C'est un corpus de type « base de données » : comme dans les bases de données relationnelles, les descriptions de film ont une structure attribut-valeur et seuls quelques noeuds ont des données textuelles (généralement les commentaires des utilisateurs et les résumés). En conséquence, la structure des documents est plus régulières que dans le premier corpus.
Chaque corpus a été séparé aléatoirement en un ensemble d'apprentissage et un ensemble de test. Tous deux comportent les documents d'entrée en HTML et les documents cible correspondant en XML, toutefois, pour le test seuls les documents d'entrée sont utilisés. Ces deux ensembles ont la même taille. Notre modèle a alors été utilisé pour retrouver la structure cible XML des documents du corpus de test. Différentes valeurs de N (le nombre de solutions candidates générées par la première étape) ont été testées. Notre modèle de base (N = 1) correspond au cas où seule l'étape de génération entre en jeu : la reconstruction a alors lieu sans tenir compte de la structure du document d'entrée et est proche du modèle proposé par (Chidlovskii et Fuselier, 2005). Nous proposons d'évaluer la qualité de la restructuration en mesurant la similitude entre le document reconstruit et le document convertit la main. Comme mesure de similarité, nous avons utilisé le pourcentage de constituants correctement reconstruit. Un constituant correspond à une paire (´ etiquette, couverture) et permet de décrire à la fois l'étiquette d'un noeud et sa position dans l'arbre reconstruit. Nous avons mesuré sépa-rément les résultats de la reconstruction sur les feuilles de l'arbre et sur les noeuds internes de celui-ci afin de pouvoir évaluer la capacité de notre modèle à identifier des éléments et à identifier des relations entre ces éléments.
La Table 2 rassemble les résultats de nos expériences. Dans tous les cas, l'utilisation de l'information sur le document d'entrée lors de l'étape de ré-ordonnancement améliore les résultats du modèle de base, pour une complexité globale sensiblement identique. Cette amélioration est significative lorsque l'on ne considère qu'un petit nombre de solutions candidates. Cependant, le fait d'augmenter le nombre de solutions candidates au delà d'un certain seuil lors de l'étape de ré-ordonnancement diminue les performances. Cette baisse est certainement due aux limites de l'algorithme d'apprentissage notamment par rapport au petit nombre de données fournies en apprentissage. Une analyse plus détaillée des résultats montre que les performances de l'approche sont très bonnes sur les parties les plus régulières des documents, mais que la qualité de la reconstruction chute dans les parties récursives des documents. Dans tous les cas, les résultats de l'étape de ré-ordonnancement dépendent de la qualité des solutions candidates. 
État de l'art
Des problèmes semblables (schema matching, intégration de données, ...) sont traités depuis plusieurs années par la communauté base de données et sont apparus, plus récemment, pour la recherche documentaire, la conversion (Chidlovskii et Fuselier, 2005) et l'intégration de documents (Chung et al., 2002), et l'alignement d'ontologies. Plusieurs techniques d'apprentissage ont été employées : classifieur multi-classes, spectre de graphe, ... Plusieurs travaux récents abordent la problématique de la transformation de documents, notamment avec des grammaires formelles (Chidlovskii et Fuselier, 2005;Wisniewski et al., 2006).
Une comparaison des différentes approches proposées en base de données est faite dans (Doan et Halevy., 2005). (Doan et al., 2003) présente une des approches les plus abouties pour travailler sur différents types de données (SQL, XML, ontologies). La tâche y est présentée comme un problème de classification supervisée multi-étiquettes. Toutefois les corpus considérés sont très différents des corpus auxquels nous nous intéressons : l'évaluation des méthodes développées a, généralement, été faite sur des corpus de petite taille, ayant une structure très stricte présentant peu de récursion et ne contenant que très rarement des données textuelles.
Conclusion
Nous avons proposé un cadre général pour la transformation de document semi structuré vers un schéma arbitraire. Ce cadre nous permet de considérer des caractéristiques du document d'entrée et de la structure de sortie, améliorant ainsi plusieurs approches existantes.

Introduction
De nos jours, le stockage de grands volumes de données est devenu possible et abordable. Ainsi, à des problématiques aussi diverses que l'analyse statistique de la fréquentation d'un lieu, la sécurisation de l'accès à des bâtiments, la surveillance de malades épileptiques dans des hôpitaux, ou encore la facturation des véhicules aux péages des autoroutes, les industriels proposent de plus en plus de solutions techniques basées sur l'acquisition numérique de sé-quences vidéo. Ces données vidéo sont tridimensionnelles (deux dimensions spatiales, et une dimension temporelle). Il s'agit donc d'un volume 2D+T tel que représenté sur la Figure 1 (b) 1 . Quelle que soit l'application, la première tâche d'un système d'analyse de séquences vidéo est toujours la détection de mouvement, et si possible, la détection (segmentation) des objets mobiles. La difficulté de cette tâche est très variable selon les conditions d'acquisition, la pré-cision et la rapidité du traitement escomptées. Une liste relativement exhaustive des difficultés liées à l'acquisition et au contenu de la scène peut être trouvée dans Toyama et al. (1999). Dans cet article, nous ne nous intéresserons qu'au cas d'une acquisition par caméra fixe. 
FIG. 1 -Différentes manières de représenter une séquence vidéo.
La plupart des algorithmes de détection de mouvement présents dans la littérature consistent à bâtir un modèle de l'arrière-plan de la scène filmée (c'est-à-dire, ce que l'on verrait s'il n'y avait aucun objet mobile), puis à comparer ce modèle à l'image visible à un instant t. Autrement dit, les données vidéo ne sont pas traitées comme un volume 2D+T (Figure 1(b)), mais comme une succession de couples d'images à deux dimensions (Figure 1(a)). Cette méthode est appelée soustraction de l'arrière-plan (background subtraction).
Le modèle le plus simple consiste à considérer à chaque instant t que l'image au temps t ? 1 représente l'arrière-plan, et que les zones en mouvement sont celles qui ont changé d'apparence entre t ? 1 et t. En d'autres termes, la détection de mouvement est obtenue par dérivation temporelle de la séquence. La dérivée ainsi obtenue est très rapide à calculer, mais elle est également très instable du fait de sa sensibilité à tout type de bruit. Par ailleurs, seul le passé immédiat est pris en compte donc les mouvements lents ou saccadés sont mal détectés. Ainsi, les auteurs qui utilisent la dérivée temporelle sont obligés d'ajouter des post-traitements afin de corriger le résultat. Dans Tian et Hampapur (2005), la dérivée est lissée par un opérateur de moyenne mobile, puis les points dont la direction du flot optique a beaucoup varié dans un passé proche sont éliminés du résultat. Le flot optique est défini comme la projection dans le plan de l'image du vecteur mouvement (3D) réel. Un panorama des différentes méthodes pour calculer le flot optique est présenté dans Barron et al. (1994). Les méthodes utilisant le flot optique prennent davantage en considération la dimension temporelle des séquences vidéo, mais se restreignent à un intervalle de temps de taille 2.
Une alternative à l'utilisation de l'image au temps t ? 1 comme modèle de l'arrière-plan est l'utilisation d'une image de référence. Malheureusement, une telle image n'est pas toujours disponible, et même lorsqu'elle l'est, elle devient vite obsolète, particulièrement en environnement extérieur (changement de luminosité, intempéries, etc.) C'est pourquoi les auteurs utilisant cette méthode proposent toujours une fonction de mise à jour de l'image de référence (background maintenance), comme dans Yang et al. (2004), où l'image de référence est continuellement mise à jour aux points où la dérivée temporelle est négligeable.
Souvent, le modèle de l'arrière-plan est un modèle statistique permettant d'évaluer la probabilité d'apparition d'un niveau de gris ou d'une couleur en un certain point. Si celle-ci est élevée, on considère que le pixel appartient à l'arrière-plan, sinon c'est qu'il appartient à un objet. Parfois, il s'agit de l'ensemble des paramètres d'une loi dont la forme est supposée connue. Par exemple, dans McKenna et al. (2000) le modèle de l'arrière-plan comporte la moyenne et l'écart-type des valeurs observées sur chaque canal R, G et B, car on suppose que la distribution de chaque composante couleur est gaussienne. Dans d'autres cas, on ne fait pas d'hypothèse a priori sur la forme de la loi à estimer, et on pratique alors une estimation non paramétrique comme dans Elgammal et al. (2000). Le modèle de l'arrière-plan est alors un ensemble d'observations passées qui permettront d'estimer les densités ponctuellement à l'aide d'une fenêtre de Parzen ou d'une fonction noyau.
La soustraction de l'arrière-plan peut également être vue comme un problème de prédic-tion. Les méthodes les plus utilisées sont le filtrage de Wiener et celui de Kalman. Dans Toyama et al. (1999), le modèle de l'arrière-plan est l'ensemble des dernières images et des coefficients pondérateurs d'un filtre de Wiener associés à chacune de ces images. Les coefficients sont mis à jour de manière à minimiser l'erreur quadratique entre l'image observée au temps t et sa pré-diction qui est la somme des images précédentes pondérée par les coefficients du filtre. Dans Koller et al. (1993), c'est un filtre de Kalman qui sert à prédire le vecteur des paramètres, en l'occurrence l'image observée ainsi que ses dérivées spatiales et sa dérivée temporelle.
Ainsi, rares sont les méthodes qui ne cherchent pas à estimer l'arrière-plan de la scène pour détecter les objets en mouvement. On notera néanmoins les travaux de Ma et Zhang (2001) où l'on considère que les zones en mouvement sont celles où l'entropie spatio-temporelle de la séquence est maximale. Contrairement aux méthodes citées précédemment, la dimension temporelle est pleinement prise en considération par un algorithme d'analyse semi-locale dans le volume 2D+T que constitue la séquence vidéo. Dans Guo et al. (2004), cette approche est légè-rement modifiée pour calculer l'entropie de la dérivée temporelle plutôt que celle des images d'entrée afin d'éviter de détecter les contours spatiaux comme étant des zones en mouvement. Les résultats obtenus sont assez proches d'une dérivée temporelle lissée par un opérateur de moyenne mobile. Notre étude se situe dans ce cadre : nous cherchons à détecter les objets mobiles en analysant le volume 2D+T de manière globale puis locale. Dans un premier temps nous préciserons l'espace de représentation choisi pour l'étude des séquences, puis dans une seconde partie nous expliquerons les critères utilisés pour la sélection des objets en mouve-ment. Enfin nous présenterons les résultats et les évaluerons.
Espace de représentation des données
Les données vidéo sont initialement représentées par une fonction définie dans un espace à trois dimensions : deux dimensions spatiales (x, y) et une temporelle (t). A chaque point de cet espace est associé un niveau de gris (ou un vecteur de composantes couleur) en un point (x, y) à l'instant t. Les différentes entités sémantiques (arrière-plan, objets mobiles) sont donc des sous-ensembles de points de cet espace. Afin de les identifier, il convient de les agréger en classes de points présentant des caractéristiques communes. Il va sans dire que le nombre de points à considérer est très important, surtout si l'on veut prendre en compte plus de deux trames pour détecter les objets en mouvement. C'est pourquoi l'approche consistant à bâtir un modèle de l'arrière-plan est si usuelle : les seuls points à considérer sont ceux de la trame courante, tandis que le modèle de l'arrière-plan est censé résumer toutes les observations passées. Nous pensons qu'il est préférable de conserver une connaissance moins synthétique du passé car l'information pertinente à en extraire n'est pas toujours la même. Nous envisageons donc de choisir un espace de représentation adapté davantage à la séquence elle-même plus qu'à chacune des trames et qui permette de prendre en compte le mouvement sans modifier l'information initiale. Comme représenté sur la Figure 1(c), à chaque point de l'espace image (x, y) est associé un vecteur contenant les niveaux de gris en ce point le long de l'intervalle de temps considéré. De plus, dans la perspective de l'utilisation des techniques d'analyse des données, la séquence n'est plus considérée comme une fonction mais comme un ensemble d'individus : les pixels que nous observons quand nous regardons la séquence. Dans cette phase, les relations spatiales entre les pixels sont donc ignorées. Pour éviter de devoir faire une analyse fine, ce ne sont pas les objets que l'on suit mais c'est une position fixe que l'on considère sur la surface de l'image. De chaque pixel on va retenir plusieurs valeurs de niveau de gris au cours du temps. On peut retenir une dizaine de valeurs, soit p et chaque pixel devient un individu caractérisé par un ensemble de paramètres. Les individus sont repérés dans un espace de dimension p. Comme notre méthode traite p trames à la fois, nous pouvons nous permettre d'être p fois plus lents que si nous traitions chaque trame individuellement, et donc d'utiliser des techniques plus coû-teuses en temps de calcul. Néanmoins, pour rester dans des temps de traitement raisonnables, presque temps réel, nous devons faire une réduction de la masse des informations. Il existe de nombreuses méthodes de réduction de dimension, telles que l'analyse en composantes principales (ACP), l'analyse factorielle des correspondances (AFC), toute la famille des méthodes d'analyse en composantes indépendantes (ACI), ou encore les algorithmes à base de réseaux neuronaux tels que les cartes de Kohonen. Le lecteur intéressé pourra se référer à Lebart et al. (2006) pour obtenir un panorama détaillé. L'ACP étant connue pour être la meilleure technique linéaire de réduction de dimension au sens des moindres carrés, nous avons choisi cette mé-thode dans le but de ne préserver que les informations qui permettront au mieux de discriminer les points et de construire des classes.
L'ACP a été développée au début du XIXème siècle pour analyser des données issues des sciences humaines. C'est une technique statistique qui vise à simplifier un ensemble de données en l'exprimant dans un nouveau système de coordonnées de manière à ce que les plus grandes variances soient observées sur les premières coordonnées. Cela permet de réduire la dimensionnalité de l'espace de recherche en ne conservant que les premières dimensions de l'espace de projection obtenu. Une base de cet espace est composée des vecteurs propres de la matrice de covariance des données, ordonnés par valeurs propres associées décroissantes.
On peut penser que les pixels correspondant au fond ont des composantes à peu près toutes égales alors que les pixels correspondant au passage d'un objet mobile comportent un changement. C'est ce changement que l'on veut mettre en évidence. Pour cela il est intéressant de trouver l'axe, c'est-à-dire la bonne base dans l'espace de dimension p où la variance du facteur est la plus grande.
Dans le cas d'une séquence vidéo, la matrice des données X contient donc l'ensemble des caractéristiques des points à considérer. Par la suite, nous noterons n le nombre de lignes de X, c'est le nombre de pixels de l'image, et p son nombre de colonnes, c'est le nombre de caractéristiques retenues pour chaque pixel. Les deux premières coordonnées peuvent prendre un nombre fini de valeurs (domaine de définition D P des pixels). En revanche, le domaine de définition de la troisième coordonnée (le temps) est a priori infini. Il convient donc de choisir une plage de valeurs qui devra contenir toute l'information pertinente. Nous proposons d'utiliser le domaine D t = {t ? ?t, . . . , t} où t est le temps courant. Nous choisissons de remplir la matrice X en considérant qu'une donnée (une ligne) est un point (x, y), et qu'une variable (une colonne) est un ensemble de niveaux de gris observés à chaque instant de D t . La nouvelle base de l'espace de représentation est alors associée aux vecteurs propres de la matrice de covariance C des données :
où ¯ X est la matrice des données centrées, et D p = 1 p I p (I p étant la matrice identité d'ordre p.) Comme nous n'avons aucune information supplémentaire, on suppose que chaque variable devrait présenter une variance comparable, et nous utilisons une ACP dite « simple » (données centrées) plutôt qu'une ACP « standard » (données centrées-réduites).
La méthode doit être le plus possible insensible aux diverses conditions dans lesquelles se font les acquisitions : nous nous intéressons plus aux variations de niveau de gris qu'au niveau de gris lui-même du pixel. Nous pouvons, dès le départ, supprimer une dimension de l'espace de représentation des données en choisissant de remplir la matrice de données avec les dérivées temporelles en tout point, on est alors dans un espace de dimension p ? 1. (Nous appellerons Y la matrice de données représentée dans cet espace.) Considérons une séquence de 10 trames de 288 lignes par 720 colonnes dont la première est représentée sur la Figure 2(a). La matrice X a donc 288 × 720 lignes et 10 colonnes, tandis que la matrice Y sur laquelle nous allons appliquer l'ACP a 288 × 720 lignes et 9 colonnes. La Figure 2 montre les neuf projections de Y sur les axes principaux issus de l'ACP. Plus précisément nous considérons le domaine de l'image et nous construisons une image dont le niveau de gris correspond à la valeur de la composante du vecteur de caractéristiques sur l'un des facteurs. D'après la Figure 2, les zones en mouvement apparaissent clairement lorsqu'on projette la matrice Y sur les deux premiers axes principaux. La différence entre une zone statique et une zone en mouvement est accentuée sur ces axes. Cette observation est confirmée par l'histogramme de la variance expliquée par les facteurs (Figure 3). La variance expliquée par un axe est définie par le rapport entre la valeur propre associée à cet axe, et la somme des valeurs propres de la matrice de covariance.
Ainsi, si l'on choisit de ne retenir que les deux premiers axes principaux, 20% de l'information initialement contenue dans notre matrice de données suffisent à préserver 80% de
FIG. 2 -(a) Séquence de travail. (b)-(j) Projections de Y sur chacun des axes mis en évi-dence par l'ACP. Les sous-figures sont ordonnées selon le rang du facteur correspondant.
FIG. 3 -Variance expliquée par les axes principaux.
la variance observée. Cette première expérimentation confirme donc notre approche qui reste une approche très globale. Dans la section suivante, nous utiliserons donc cet espace de représentation des données. Cela revient à calculer une ACP pour tout ensemble de 10 trames consécutives. De manière à gagner en robustesse nous allons maintenant considérer une approche plus locale qui repose sur cette première étude globale.
Détection de zones de mouvement cohérent
La représentation des données telle que dans la Figure 2(b) permet de facilement détecter les mouvements au niveau local (au niveau des pixels). En effet, il suffit de sélectionner les pixels dont la valeur absolue est élevée (les plus sombres et les plus clairs) pour obtenir une segmentation objets mobiles/arrière-plan. La Figure 4(b) représente la segmentation automatique de la projection de Y sur le premier axe principal.
Nous nous retrouvons alors dans le cas de la plupart des méthodes présentes dans la littérature, une telle image binaire serait étiquetée en composantes connexes pour obtenir une détection par objet mobile. Comme dans la plupart des cas, dans l'exemple de la 
FIG. 4 -(a) Projection de Ysur le premier axe principal, (b) segmentation de l'arrière-plan obtenue à partir de (a), (c) segmentation améliorée par des opérations morphologiques.
prétraitement de l'image serait nécessaire pour supprimer les faux positifs et pour rétablir la connexité des objets (c). Une telle approche fournit une segmentation précise, mais le choix des opérations morphologiques à effectuer est souvent délicat. Une erreur dans le choix d'un élément structurant pourrait effacer un objet intéressant, connecter deux objets différents, valider un faux positif, etc. Une phase d'apprentissage est nécessaire pour adapter la méthode générale au cas particulier de la séquence étudiée. Nous préférons donc éviter d'avoir à effectuer une telle étape, mais nous avons tout de même besoin de définir des zones connexes associées à un unique objet mobile. Pour gagner en cohérence nous allons perdre en précision. A partir de la représentation globale considérée précédemment, dans la population des pixels de l'image, des sous-populations de tailles égales sont isolées et seront comparées. Pour cela, nous commençons par fractionner les données (Y) en plusieurs sous-ensembles. Chaque sousensemble correspond à un bloc de b × b pixels maintenant caractérisés par les neuf valeurs de la matrice de données Y qui constituent les valeurs des facteurs mis en évidence dans l'étude globale. Sur la séquence initiale ce sont donc des blocs tridimensionnels de taille b × b × 10 qui sont étudiés au travers de 9 nouvelles caractéristiques. Les blocs que nous avons choisis, de manière à obtenir des résultats plus continus et sans augmenter trop les temps de calcul, se recouvrent par moitié le long des dimensions spatiales. Les individus des sous-ensembles ainsi obtenus sont représentés dans un espace de dimension p ? 1. Nous allons étudier les positions relatives de ces ensembles de points. Pour simplifier les calculs, nous représentons chaque ensemble de points par son ellipsoïde d'inertie. De plus nous comparons les projections des ellipsoïdes dans le plan formé par les deux premiers facteurs de la représentation globale.
Comparaison des zones détectées
La Figure 5 montre un ensemble d'ellipsoïdes d'inertie projetés sur le premier plan factoriel de l'image globale. Ils correspondent chacun à un bloc spatio-temporel tel que décrits dans la section 3.
Les ellipses observées se différencient par leur position dans le plan, leur surface, et leur orientation. Dans le cadre de cette étude, nous ne nous intéresserons pas à l'orientation des ellipses. Les données, étant centrées, le repère de la Figure 5 a pour origine la moyenne de Y (ou plus exactement la projection de la moyenne). Par conséquent, une ellipse qui se trouve loin de l'origine représente un bloc dont beaucoup de points sont en mouvement. La surface des ellipses donne une indication sur la variabilité des points du bloc qu'elle représente. On peut ainsi distinguer plusieurs cas :
FIG. 5 -Chaque bloc tridimensionnel est modélisé par l'ellipsoïde d'inertie des points qui le composent, et chaque ellipsoïde est projeté dans le plan formé par les deux premiers facteurs issus de l'étude globale (ACP).
1. Une petite ellipse proche de l'origine représente un bloc dans lequel aucun mouvement n'est présent.
2. Une grande ellipse proche de l'origine représente un bloc dans lequel les différents points ont des mouvements dissemblables, mais où la moyenne des mouvements est quasinulle. Autrement dit, il s'agit de bruit.
3. Une petite ellipse éloignée de l'origine représente un bloc dans lequel le mouvement moyen est important, et dont les points ont quasiment tous le même mouvement. Ce sont les blocs intégralement inclus dans un objet en mouvement.
4. Une grande ellipse éloignée de l'origine représente un bloc dans lequel le mouvement moyen est important, et dont les points présentent des mouvements assez variés. Ce sont les blocs qui peuvent par exemple se trouver à la frontière d'un objet en mouvement.
Pour détecter les objets en mouvement, les blocs les plus intéressants sont donc ceux qui correspondent aux cas 3 et 4, autrement dit, les ellipses éloignées de l'origine. Il faut donc effectuer un seuillage par rapport à la distance à l'origine des centres des ellipses, c'est-à-dire la moyenne (ou la somme) des points appartenant aux blocs correspondants.
Expérimentation
La taille des blocs spatio-temporels introduits à la section 3 reste à définir. Des blocs trop larges nuiraient à la précision des contours des objets détectés, tandis que des blocs trop petits impliqueraient des temps de calcul plus élevés, et la connexité des régions pourrait en souffrir. La  Nous constatons que le temps de calcul dépend peu de la taille des blocs (et donc de leur nombre). On peut donc choisir celle-ci uniquement en fonction de la séquence à analyser, sans se soucier du temps de calcul nécessaire. Dans notre cas, les images ont pour dimension 720 × 288 pixels, et la taille de bloc produisant le moins d'erreurs est 32 × 32.
Pour évaluer notre algorithme, nous utilisons cinq séquences vidéo qui se différencient par la problématique demandée par l'application et/ou les difficultés intrinsèques de la séquence. La première séquence illustre un problème de comptage de personnes passant par le sas situé en bas de l'image. La difficulté est liée au fait que plusieurs personnes restent longtemps plus ou moins immobiles dans le champ de vision avant de franchir (ou non) le sas. Il faut donc que l'algorithme ne détecte pas les mouvements insignifiants. La seconde vidéo représente égale-ment un problème de comptage de personnes, mais là, les personnes ont tendance à se déplacer en groupes connexes. Il faut donc un algorithme suffisamment précis pour pouvoir discerner les différents membres de chaque groupe. La troisième séquence représente une application de détection de passage de véhicules dans le premier plan. La difficulté provient du fait que l'image est très bruitée par le soleil passant à travers les arbres sur la gauche de l'image, et par des véhicules circulant dans l'arrière-plan. Les deux dernières séquences sont des séquences de test classiques utilisées dans de nombreux articles 2 . Elles sont utilisées dans le but de faciliter la comparaison des résultats présentés dans cet article avec d'autres méthodes. Sur la Figure 7 sont représentées les segmentations entre objets mobiles et arrière-plan obtenues sur ces cinq séquences vidéo, avec cinq algorithmes différents. La ligne 1 montre les résultats obtenus avec l'algorithme présenté dans cet article ; la ligne 2, la dérivée temporelle lissée de la séquence ; la ligne 3, la soustraction de l'arrière-plan en modélisant celui-ci par une loi gaussienne (McKenna et al., 2000) ; la ligne 4, la soustraction de l'arrière-plan quand celui-ci est modélisé de manière non paramétrique (Elgammal et al., 2000) ; la ligne 5, l'entropie spatio-temporelle de la différence entre images consécutives (Guo et al., 2004). Dans la littérature, les méthodes concurrentes que nous avons testées sont toujours suivies d'une phase de post-traitement pour faciliter l'extraction des composantes connexes. Pour les lignes 2 à 5, nous avons donc appliqué aux résultats obtenus une fermeture morphologique par un disque de diamètre 5 suivie d'une ouverture morphologique par le même élément structurant.
On constate que les algorithmes utilisant une modélisation statistique de l'arrière-plan (lignes 3 et 4) font apparaître les contours des objets mobiles de manière plus précise. En revanche, ces méthodes sont très sensibles au bruit, donc à moins de choisir très précisément le post-traitement en fonction de la séquence traitée, les résultats obtenus ne constituent pas une bonne segmentation des objets mobiles.
Les contours des objets sont également assez précis avec la méthode de dérivation temporelle (ligne 2). Cela dit, cette méthode a tendance à ne révéler que les contours des objets et à en ignorer l'intérieur. Cet inconvénient peut être compensé en augmentant le coefficient de lissage, mais l'on risque alors de créer un effet « fantôme », et de perdre la précision obtenue.
Notre méthode ainsi que celle de l'entropie spatio-temporelle ont en commun le fait de sacrifier la précision des contours au profit d'une plus grande robustesse. Le nombre de composantes connexes est néanmoins plus exact avec la méthode ici présentée (ligne 1).

Résumé. La classification des images sonar est d'une grande importance par exemple pour la navigation sous-marine ou pour la cartographie des fonds marins. En effet, le sonar offre des capacités d'imagerie plus performantes que les capteurs optiques en milieu sous-marin. La classification de ce type de données rencontre plusieurs difficultés en raison des imprécisions et incertitudes liées au capteur et au milieu. De nombreuses approches ont été proposées sans donner de bons résultats, celles-ci ne tenant pas compte des imperfections des données. Pour modéliser ce type de données, il est judicieux d'utiliser les théories de l'incertain comme la théorie des sous-ensembles flous ou la théorie des fonctions de croyance. Les machines à vecteurs de supports sont de plus en plus utilisées pour la classification automatique aux vues leur simplicité et leurs capacités de généralisation. Il est ainsi possible de proposer une approche qui tient compte de ces imprécisions et de ces incertitudes au coeur même de l'algorithme de classification. L'approche de la régression par SVM que nous avons introduite permet cette modélisation des imperfections. Nous proposons ici une application de cette nouvelle approche sur des données réelles particulièrement complexes, dans le cadre de la classification des images sonar. la flore. Ces imperfections rendent la tache difficile pour la caractérisation des fonds marins à partir de ce type de données. Il est donc nécessaire de proposer des algorithmes, robustes aux imperfections, pour la classification automatiques des images sonar.
Plusieurs choix sont envisageable pour remédier aux problèmes d'imperfections : soit nous tentons de supprimer ces imperfections, ce qui nécessite une compréhension, souvent difficile, de la physique qui a conduit à ces imperfections ; soit nous cherchons à développer des processus de traitement robustes à ces imperfections ; soit nous cherchons à les modéliser.
Le cadre théorique des théories de l'incertain offre la possibilité de modéliser finement ces imperfections. Parmi elles, la théorie des ensembles flous et la théorie des fonctions de croyance permettent de tenir compte des incertitudes et imprécisions.
De nombreuses approches ont été proposées pour la classification des images sonar par exemple dans Laanaya et al. (2005b) et Leblond et al. (2005). Ces approches ne tiennent pas compte de l'incertitude de l'expert lors de la segmentation de ces images. Nous adopterons dans ce papier l'approche que nous avons proposée dans Laanaya et al. (2006) avec une ré-solution du problème d'optimisation adaptée à la classification automatique des images sonar. Cette approche a donné des résultats intéressants sur des données générées, nous montrerons ici son intérêt sur les données complexes que sont les images sonar.
Ainsi, nous présenterons une description rapide des fonctions d'appartenance et des fonctions de croyance utilisées par l'approche de la régression par SVM. Nous rappelons ensuite, l'approche de la régression par SVM après une brève introduction du principe des SVM. Cette approche est comparée au SVM classique et discutée dans une dernière partie à partir d'images sonar.
Théories de l'incertain
Nous avons vu dans Martin (2005) que les théories de l'incertain telles que la théorie des sous-ensembles flous introduite par Zadeh (1965), la théorie des possibilités de Dubois et Prade (1987) ou encore la théorie des fonctions de croyance de Dempster (1967) et Shafer (1976 permettent la modélisation de données incertaines et imprécises dans le cadre de la classification d'images sonar.
Ces théories sont fondées sur les fonctions d'appartenance pour les premières et sur les fonctions de croyance pour la dernière. Afin d'intégrer directement les contraintes liées à ces fonctions dans un algorithme de classification, nous rappelons ici les caractéristiques des fonctions d'appartenance de la théorie des sous-ensembles flous et des fonctions de croyance de Dempster et Shafer.
Les fonctions d'appartenance
Les fonctions d'appartenance permettent de décrire une appartenance floue à une classe. Ainsi l'appartenance d'une observation x à une classe C i parmi N c classes, est donnée par une fonction µ i (x) telle que :
Dans ce cas, nous considérons les classes floues. Dans le cas de classes nettes, il est possible de considérer les distributions de possibilité. Typiquement x peut représenter une partie du fond marin et C i le type de sédiment présent sur l'image x. Nous verrons au paragraphe 4.1.2 comment ces fonctions µ i peuvent être choisie dans notre application.
Les fonctions de croyance
La théorie des fonctions de croyance est fondée sur la manipulation des fonctions de masse. Les fonctions de masse sont définies sur l'ensemble de toutes les disjonctions du cadre de discernement ? = {C 1 , . . . , C Nc } et à valeurs dans [0, 1], où C i représente l'hypothèse "l'observation appartient à la classe i". La contrainte de normalité couramment employée est ici donnée par :
A?2 ? où m(.) représente la fonction de masse. La première difficulté est donc de définir ces fonctions de masse selon le problème. Nous verrons comment il est possible de le faire pour notre application dans la section 4.1.2. A partir de ces fonctions de masse, d'autres fonctions de croyance peuvent être définies, telles que les fonctions de crédibilité, représentant l'intensité que toutes les sources croient en un élément, et telles que les fonctions de plausibilité repré-sentant l'intensité avec laquelle on ne doute pas en un élément. Afin de conserver un maximum d'informations, il est préférable de rester à un niveau cré-dal (i.e. de manipuler des fonctions de croyance) pendant l'étape de manipulation des informations pour prendre la décision sur les fonctions de croyance à l'issue de la manipulation de ces fonctions. Si la décision prise par le maximum de crédibilité peut être trop pessimiste, la décision issue du maximum de plausibilité est bien souvent trop optimiste. Le maximum de la probabilité pignistique, introduite par Smets (1990), reste le compromis le plus employé. La probabilité pignistique est donnée pour tout X ? 2 ? , avec X = ? par :
Similitudes
Ainsi les fonctions d'appartenance et les fonctions de masse permettent une modélisation de l'incertitude et de l'imprécision à partir de points de vue différents.
Ces fonctions ont toutes deux la particularité d'être à valeurs dans [0,1] et d'avoir une contrainte de normalité équivalente. Nous allons voir dans la section suivante comment intégrer ces contraintes dans une régression linéaire multiple. montré des performances remarquables sur des données générées. Afin d'assoir les notations utiles pour la suite, nous rappelons le principe des SVM sur laquelle s'appuie la régression floue et crédibiliste présentée ensuite.
Principe du classifieur SVM
Les machines à vecteurs de support initiées par Vapnik (1998), sont avant tout une approche de classification linéaire à deux classes. Elles tentent de séparer des individus issus de deux classes (+1 et -1) en cherchant l'hyperplan optimal qui sépare les deux ensembles, en garantissant une grande marge entre les deux classes. Un nombre réduit d'exemples pour la recherche de l'hyperplan est suffisant pour la description de cet hyperplan.
Dans le cas où les exemples sont linéairement séparables, on cherche l'hyperplan y = w.x + b qui maximise la marge entre les deux ensembles où w.x est le produit scalaire de w et x. Ainsi w est la solution du problème d'optimisation convexe :
sous les contraintes :
où les x t ? IR d représentent les l données d'apprentissage , et y t ? {?1, +1} la classe. Ce problème d'optimisation se résout par la méthode du lagrangien.
Dans le cas où les données ne sont pas linéairement séparables, les contraintes (5) sont relachées par l'introduction de termes positifs ? t . Nous cherchons alors à minimiser :
t=1 sous les contraintes données pour tout t :
où C est une constante choisie par l'utilisateur. Le problème se résout alors de manière similaire au cas linéairement séparable. Afin de classer un nouvel élément x il suffit d'étudier la fonction de décision donnée par :
t?SV où SV = {t ; ? 0 t > 0} pour le cas séparable et SV = {t ; 0 < ? 0 t < C} pour le cas non séparable, est l'ensemble des vecteurs de support, et ? t ? 0 sont les multiplicateurs de Lagrange.
Dans les cas non linéaire, le principe des SVM est de projeter, par une fonction noyau, les données de départ dans un espace de grande dimension (éventuellement infinie). Ainsi la classification d'un nouvel élément x est donnée par la fonction de décision :
t?SV où K est la fonction noyau, dont les plus utilisées sont le noyau polynomial
2 , ? ? IR + . Le choix du noyau et l'optimisation des paramètres de celui-ci reste délicat selon l'application.
Régression floue et crédibiliste par SVM
Nous avons situé cette approche dans la littérature Laanaya et al. (2006). Ainsi elle est novatrice par la prise en compte des contraintes similaires de normalisation des fonctions de croyance et d'appartenance dans le problème de régression multiple. De plus nous proposons ici d'employer une résolution du problème d'optimisation pouvant gérer de grande quantité de données.
Soient les vecteurs d'apprentissage x t ? IR d et les fonctions associées y t ? IR N , où N = N c le nombre de classes dans le cas des fonctions d'appartenance et N = 2
Nc dans le cas des fonctions de masse. Ainsi par la régression multiple linéaire, nous cherchons une fonctionnelle f = (f 1 , . . . , f N ) où les f n sont linéaires, de forme f n (x) = w n .x + b n . Nous cherchons à déterminer cette fonctionnelle telle que pour les (x t , y t ) de la base d'apprentissage |y tn ? w n .x t + b n | ne dépasse pas un certain fixé pour tout n. Nous supposons ainsi que tous les points sont à l'intérieur du cylindre défini par Afin de généraliser, nous associons un facteur C pour les points qui sont à l'extérieur du cylindre défini par Le problème d'optimisation convexe revient donc à celui exposé dans la section 3.1, et le critère à minimiser est : sous les contraintes données pour tout t et tout n :
Le lagrangien est donc donné par :
où les ?, ?, ? et ? sont les multiplicateurs de Lagrange et sont positifs.
Au point selle du lagrangien L, on a pour tout t et tout n, ?L/?b n = 0, ?L/?w n = 0, ?L/?? tn = 0 et ?L/?? * tn = 0. Ainsi :
En intégrant ces équations (13) dans le lagrangien (équation (12)), le problème revient à maximiser :
sous les contraintes :
Enfin, pour prédire la n ème sortie˜ysortie˜ sortie˜y n , d'un nouvel élément x, on calcule :
où b n est déduite des conditions de Kuhn, Karush et Tucker :
Si pour un t 0 , ? t0n ?]0, C[ alors, ? t0n = 0, ainsi b n = y t0n ? w n .x t0 ? un raisonnement identique sur ? * donne b n = y t0n ? w n .x t0 + La résolution du système d'optimisation de la régression par SVM pour des problèmes de grande dimension nécessite des mémoires de stockage de grande taille. Ainsi, l'application des algorithmes d'optimisation classiques est difficile. Ces limites ont été constatées dans Laanaya et al. (2006). Une solution est d'utiliser des méthodes d'optimisation itératives, où on essaye de résoudre des sous-problèmes du problème principale. Nous avons adapté la résolution par SMO (Sequential Minimal Optimization) développée par Platt (1998) pour les machines à vecteurs de support, pour notre problème d'optimisation. Il résout des sous-problèmes de dimension deux d'une manière analytique. Nous pouvons ainsi résoudre des problèmes de grande taille avec une vitesse remarquable.
Si on suppose que la relation entre les x t et les sorties˜ysorties˜ sorties˜y t est non-linéaire, nous pouvons représenter les données de départ en utilisant un noyau. Ainsi le produit scalaire entre les données de la base d'apprentissage peut être donc substitué par un noyau : le produit scalaire x.
). Une régression linéaire peut alors s'appliquer dans l'espace de représentation. Pour une observation x, la sortie˜ysortie˜ sortie˜y se prédit en considérant les N valeurs :
A partir de cette approche de régression sur les fonctions d'appartenance ou les fonctions de croyance, nous obtenons un classifieur en prenant la décision via le maximum des fonctions d'appartenance ou le maximum de la probabilité pignistique.
Expérimentations
Nous présentons ici l'application de notre approche pour la classification des images sonar. En effet, l'environnement sous-marin lui même est très incertain et les systèmes de mesure sont complexes et imprécis. Il est particulièrement important de classifier les fond marins pour de nombreuses applications telles que la navigation et la cartographie sous-marine. Nous trouverons plusieurs études sur la classification de images sonar, citons par exemple Martin et al. (2004), Laanaya et al. (2005a), Laanaya et al. (2005b) et Leblond et al. (2005).
Les données à classifier sont ainsi entachées de nombreuses imperfections dues aux bruits de mesure, aux interférences des signaux utilisés pour l'acquisition, aux bruits de chatoiement et à la faune et la flore. importantes pour la navigation sous-marine et les sédimentologues. Ainsi la première classe regroupe roche et cailloutis, la deuxième classe les rides et la troisième le sable et les vases. L'unité de classification retenue est l'imagette de taille 32×32 pixels (soit environ 640×640 cm.
Base de données
Extraction de paramètres
Afin de réduire les problèmes de représentativité des imagettes qui comportent plus d'un sédiment et les problèmes liés à l'évaluation (cf. Martin et al. (2006)), nous ne considérons ici que les imagettes homogènes (imagettes avec un seul type de sédiment). Nous avons ainsi 31957 imagettes.
Nous avons calculé sur ces imagettes six paramètres extraits à partir des matrices de cooccurrence calculés sur les imagettes Martin et al. (2004). Les matrices de cooccurrence C d sont calculées en comptant les occurrences identiques de niveaux de gris entre deux pixels contigus dans une direction d donnée. Quatre directions sont considérées : 0, 45, 90 et 135 degrés. Dans ces quatre directions six paramètres d'Haralick sont calculés : l'homogénéité, le contraste, l'entropie, la corrélation et l'uniformité. L'homogénéité qui a une valeur élevée pour des images uniformes ou possédant une texture périodique dans la direction d est donnée par :
où N G est le niveau de gris des imagettes. L'estimation du contrast est donnée par :
L'entropie qui a de faibles valeurs s'il y a peu de probabilités de transition élevées dans C d , est définie par :
La corrélation entre les lignes et les colonnes de la matrice est donnée par :
où µ x , ? x , µ y , ? y représentent respectivement les moyennes et écart-types des distributions marginales des éléments de la matrice de cooccurrence. La directivité qui définie l'existence d'une direction privilégiée de la texture est calculée par :
L'uniformité qui caractérise la proportion d'un même niveau de gris est donnée par :
Nous avons moyenné ces paramètres selon les quatre directions d, ainsi chaque imagette est représentée uniquement par six parammètres.
Modélisation des fonctions floues et crédibilistes
Nous avons utilisé l'approche de Keller et al. (1985) pour calculer la fonction d'appartenance des vecteurs d'apprentissage que nous utiliserons pour l'apprentissage du SVM flou, et l'approche de Denoeux (1995) pour estimer les fonctions de masses que nous utiliserons pour l'apprentissage du SVM crédibiliste. L'approche de Keller et al. (1985) est celle d'un k-plus proches voisins flou. Les fonctions d'appartenance d'un vecteur d'apprentissage x t sont estimées dans un premier temps par :
où k f est le nombre de plus proches voisins choisi pour le voisinage flou
Dans un second temps, nous calculons la fonction d'appartenance pour un vecteur x à classifier :
La norme employée est ici la norme euclidienne. La classe d'appartenance de x est ensuite décidée de manière classique comme la classe donnant le maximum des fonctions d'appartenance prédites par notre régression. L'approche de Denoeux (1995) calcule une estimation des fonctions de masses à partir d'un modèle de distance :
où C i est la classe associée à x (t,k) , qui sont les k vecteurs d'apprentissage les plus proches de la valeur x et la distance employée est la distance euclidienne. ? i et ? i sont des coefficients d'affaiblissement, et de normalisation. Les k fonctions de masse ainsi calculées pour chaque x sont combinées par la règle orthogonale normalisée de Dempster-Shafer. Cette règle est donnée pour deux experts et pour tout A ? 2 ? , A = ? par :
La décision est ensuite prise par le maximum sur les fonctions de masse prédites par notre régression. Dans ce cas, il est équivalent au maximum de probabilité pignistique car les seuls éléments focaux sont les singletons et l'ignorance.
Résultats
Nous avons effectué un tirage aléatoire de 3000 imagettes homogènes sur toute la base de données (31957 imagettes), ainsi la base d'apprentissage contient des effectifs différents pour les trois classes : 15.53% des imagettes contiennent du roche et cailloutis, 11.85% sont des imagettes rides et les 72.62% restants sont du sable et de la vase. La base de test est constituée de 1000 imagettes choisies de façon aléatoire. Nous avons répété cette opération 10 fois afin d'obtenir des estimations plus fiables des taux de classification.
Nous avons comparé la classification fondée sur les machines à vecteurs de support donnée par le logiciel libSVM de Chang et Lin (2001) et une version modifiée de ce dernier qu'on a développée pour intégrer notre approche.
Les matrices de confusion normalisées obtenues par le SVM classique (avec les paramètres par défaut de libSVM : noyau gaussien avec ? = 1 et C=1), SVM crédibiliste et SVM flou avec un noyau gaussien, avec ? = 1, C = 1 et = 0. 
Conclusion
Nous avons proposé dans ce papier une nouvelle résolution de l'approche de régression floue et crédibiliste à partir de machines à vecteurs de support pour la classification précé-dement introduite. Les résultats obtenus sur les images sonar ont montré l'intérêt de cette approche. En particulier l'approche crédibiliste donne de très bons résultats sur des données faiblement apprises, alors que l'approche floue permet d'avoir une meilleure classification pour chaque classe considérée.
Nous n'avons donné ici que des résultats en utilisant des valeurs empiriques pour les paramètres (C, et ?). Le réglage de ces paramètres peut se faire en utilisant les algorithmes génétiques par exemple, il est possible aussi d'intégrer l'optimisation de ces constantes dans le problème d'optimisation générale des SVM pour la régression.

Introduction
Le volume toujours plus important de textes rend l'exploitation de ces derniers par des méthodes automatiques de plus en plus complexes. Face à ce problème, la segmentation thé-matique offre la possibilité d'isoler dans un texte, des segments cohérents du point de vue de leur contenu informationnel. Ainsi, d'autres tâches telles que le résumé automatique ou la recherche d'information par exemple s'en trouve simplifiées. Mais l'on peut imaginer des tâches plus spécifiques telles que la création automatique de table des matières ou de plans à partir d'un gros volume de données non structurées. Nous présentons ici une approche originale de la segmentation thématique en nous appuyant sur les données du défi DEFT'06, Azé et al. (2006). Pour son édition 2006, DEFT a fixé comme tâche de retrouver les différents segments théma-tiques d'un grand volume de textes. Trois catégories de textes nous ont été soumises : -un ensemble de discours politiques.
-un ensemble d'articles de loi.
-un extrait d'un livre à teneur scientifique. Chacune de ces catégories a été divisées en deux corpus distincts : -Un corpus d'apprentissage, fourni au début du défi avec les segments thématiques éti-quetés, afin d'entraîner nos méthodes. -Un corpus de test, fourni à la fin du défi, sur lequel nous avons été évalués. Un calcul de F score sur les phrases frontières rapportées par les méthodes a permis l'éva-luation des résultats. Les modalités du calcul du F score, et du couple rappel / précision qui lui est lié, dans le cadre de ce défi sont explicités par Azé et al. (2006). La tâche de segmentation thématique peut être assimilée à la détection de frontières. Retrouver les segments thématiques au sein d'un texte, revient à retrouver la première phrase (ou la dernière) de chacun de ces segments. Cette phrase jouerait ce rôle de frontière, si toutefois l'épaisseur de la frontière se limite à la phrase (hypothèse fondamentale de l'évaluation). Les trois catégories de texte sont grandement différentes, et posent donc des problèmes diffé-rents. Dans le cas du corpus de la catégorie scientifique (que nous appellerons corpus « scientifique » par la suite) la segmentation thématique consiste à retrouver les différents paragraphes / chapitres du livre. Il nous faut regrouper les articles appartenant au même texte de loi dans le cas du corpus de la catégorie juridique (que nous appellerons corpus « juridique » par la suite). Enfin le corpus de la catégorie discours politiques (que nous appellerons corpus « discours » par la suite) pose lui un double problème :
-Il faut séparer entre eux les différents discours du corpus.
-Au sein même des discours, il faut retrouver les frontières entre les thèmes abordés par l'orateur. Nous sommes donc devant une triple tâche (voire quadruple si l'on considère la double tâche imposée par le corpus « discours »). Dans cet article, nous avons toutefois cherché à aborder cet ensemble de tâches complexes sous un angle unique et original, celui de la cohésion sémantique au sein d'un même thème, cohésion que nous chercherons à caractériser. Après avoir brièvement décrit quelques-unes des méthodes non supervisées les plus courantes à l'heure actuelle dans le domaine de la segmentation thématique, nous présenterons les diffé-rentes étapes de notre démarche, depuis la génération des vecteurs sémantiques jusqu'à l'identification des phrases frontières. Nous finirons sur une analyse de nos résultats dans le cadre de la campagne DEFT'06.
Méthodes de segmentation thématique non supervisées
Les méthodes de segmentation thématique non supervisées qui ne nécessitent donc ni apprentissage, ni règles, se basent principalement sur la notion de cohésion lexicale observée au travers de la répétition de termes. Par terme, on entend l'unité lexical minimum porteuse de sens, à savoir un mot la plupart du temps, mais parfois un groupe de mots (une collocation), tel que « cul de sac » par exemple. On peut regrouper ces méthodes en trois grandes familles que nous allons présenter ici.
Segmentation à partir de mesure de similarité entre segments de texte
Les méthodes de segmentation à base de similarité considèrent les différentes portions de texte du document à traiter comme autant de vecteurs. Les composantes des vecteurs étant, dans la plupart des cas, les fréquences d'apparition des termes au sein de la portion de texte, après que l'on ait retiré les termes inutiles (termes jugés comme peu porteurs de sens) de celleci. Parfois, cette fréquence des termes est pondérée par un IDF (Inverse Document Frequency), pour renforcer l'importance des termes supposés thématiquement saillants. L'objectif de ces méthodes est donc de mesurer la proximité ou l'éloignement des portions de texte étudiées grâce à l'angle que forment leurs vecteurs représentatifs. Elles s'appuient donc en général sur le cosinus de cet angle, qu'elles considèrent comme un indice de similarité. La similarité est ensuite exploitée de diverses manières. Choi (2000), par exemple, utilise la similarité pour effectuer un classement local et cette approche a retenu notre attention. Ces méthodes, notamment l'algorithme c99 de Choi (2000), sont, à l'heure actuelle, parmi les plus performantes. Ces méthodes bien qu'efficaces deviennent rapidement inutilisables à mesure que le volume de données augmente. En effet, ces méthodes s'appuient sur des matrices de similarité entre phrases. Cette approche est donc difficile à mettre en ¡uvre dans le cas de masses de données volumineuses telles que celles issues du défi DEFT'06 (pour un volume de 400000 phrases on obtient : 400000 * 400000 = 1.6 * 10 11 entrées dans la matrice ; même en utilisant la symétrie de la matrice pour diviser par deux le nombre d'entrées, ce dernier reste trop élevé).
Segmentation à partir de représentation graphique de répétition de termes
En passant par une représentation graphique des termes, il est plus facile de visualiser leur répartition le long du document étudié. Ainsi, la méthode du nuage de points, présentée par Helfman (1994), emploie cette représentation pour la recherche d'informations. Le principe est de positionner sur un graphique chaque occurrence des termes du document (les termes vides de sens ayant bien entendu été retirés au préalable). Ainsi, un terme apparaissant à une position i et une position j du texte, sera représenté par les 4 couples
Les portions du document où les répétitions de termes sont nombreuses apparaîtront alors sur le graphique comme les zones de forte concentration de points. Cette approche visuelle de la représentation d'un texte a été reprise et adaptée à la segmentation thématique par Reynar (1998) dans son algorithme DotPlotting. L'idée est d'identifier les segments thématiquement cohérents sur le graphique en cherchant les limites des zones les plus denses. La densité d'une région du graphique est calculée en divisant le nombre de points présents dans la région par l'aire de cette dernière. L'objectif de DotPlotting est d'isoler les segments thématiques soit en maximisant leur densité, soit en minimisant la taille des zones « vides » entre les segments. On notera que, dans son principe, cette méthode est très proche de l'algorithme c99 de Choi (2000) et donne des résultats proches même si elle est un peu moins efficace. Cette approche a même inspiré des méthodes originales, comme celle proposée par Ji et Zha (2003), qui consiste à remplacer le problème de segmentation thématique par un problème de segmentation d'image. Cette méthode utilise une technique de diffusion anisotropique (détec-tion des contours par lissage d'une image) sur la représentation graphique de la matrice de distance afin de renforcer les contrastes entre les zones denses et les frontières. Comme les précédentes approches, ces méthodes montrent vite leur limite face à de gros volume de texte.
Segmentation à partir de chaînes lexicales
La segmentation à base de chaînes lexicales relie les occurrences multiples des termes dans un document et estime qu'une chaîne est rompue si la distance entre deux occurrences du même terme est trop importante. Cette distance est généralement exprimée en nombre de phrases. Ainsi, la méthode Segmenter présentée par Kan et al. (1998), procède selon ce principe pour effectuer une segmentation thématique du document étudié. On notera tout de même une subtilité. La distance à partir de laquelle l'algorithme considère qu'il y a rupture dépend de la catégorie syntaxique du terme impliqué dans le lien. Une autre approche fondée sur les chaînes lexicales est proposée par Hearst (1997) avec son algorithme T ext T illing. Un score de cohésion est attribué à chacun des blocs de texte en fonction du bloc qui le suit. Il est quant à lui calculé sur la base d'un premier score dit « lexical » attribué à chaque paire de phrases en fonction de la paire de phrases qui la suit. Ce score lexical est lui même calculé à partir des paramètres que sont le nombre de termes en commun, de termes nouveaux et de chaînes lexicales actives dans les phrases considérées. Le score de chaque segment de texte est alors le produit scalaire normalisé des scores de chacune des paires de phrases qu'il contient. Si un segment présente un score très différent des segments précédents et suivants, alors la rupture thématique se situe au sein de ce segment. Ces méthodes ne résolvent pas le problème de la taille variable des frontières et / ou de la localisation précise de ces dernières.
Outre les remarques exprimées sur les limites de ces méthodes par rapport à la tâche demandée, toutes ces approches ont ceci en commun qu'elles s'appuient sur la cohésion lexicaleSYGFRAN (Chauché 1984). Le principe est de projeter un terme ou un groupe de termes dans un espace de concepts de dimension finie. Les concepts de cet espace sont issus d'un thésau-rus « à la Roget ». Dans notre cas, il s'agit du thésaurus Larousse (1992) qui comprend 873 concepts organisés sur 4 niveaux. L'analyseur SYGFRAN construit, pour une phrase donnée, un arbre syntaxique. Les feuilles de cet arbre sont les différents termes de la phrase, les n¡uds de l'arbre sont le regroupement des termes de la phrase en groupe ou proposition. La racine de l'arbre représente donc la phrase elle-même. Chaque n¡ud de l'arbre se voit attribuer un vecteur sémantique qui est une combinaison linéaire des vecteurs sémantiques de ses fils. Ainsi prenons par exemple la phrase « Le calcul du sens, qui dépend de la structure syntaxique, utilise une forme vectorielle. » (figure 1). Dans cette phrase le terme « calcul » peut ramener à plusieurs concepts : les mathématiques, la médecine (calcul biliaire), le comportement (quelqu'un de calculateur), entre autres. Toutefois la présence du terme « vectorielle » dans le groupe verbal va confirmer que nous parlons bien de mathématiques ici. La structure syntaxique entre aussi en ligne de compte dans le calcul du vecteur sémantique, principalement comme élément pondérateur des combinaisons linéaires. Toujours dans la même phrase, le groupe nominal prépositionnel rattaché à « calcul », avec les termes « sens » et « syntaxique » véhicule une forte notion de linguistique. Mais comme ce n'est qu'un groupe nominal prépositionnel son importance est jugée moindre et donc le concept de linguistique sera au final présent dans le vecteur sémantique de la phrase, mais dans une moindre mesure par rapport à celui de mathématiques.
Le calcul du sens qui dépend de la structure syntaxique utilise une forme vectorelle. 
Postulat sur l'organisation thématique d'un texte
En langue française, comme dans toutes les langues, la rédaction d'un texte suit un certain nombre de règles, souvent explicites, mais parfois implicites. Nous sommes partis de la constatation selon laquelle lorsqu'une portion de texte quelconque (paragraphe, chapitre, etc.) traite d'un thème particulier, les premières phrases exposent le sujet abordé, lorsque l'on avance dans le texte, on fait de plus en plus face à des exemples ou des illustrations, pour finir par une ou plusieurs 2 phrases de transitions, qui introduisent le thème suivant. Cette structure, relativement classique, est enseignée dès les premières années d'enseignement secondaire et influence donc la rédaction d'une grande majorité de textes, tant elle est « intégrée » dans notre approche de l'écriture. On peut donc considérer qu'un texte écrit « selon les règles » aura une structure analogue à celle représenté en 2. Ce postulat rejoint les constatations de Chauché et al. (2003).
FIG. 2 -Structure thématique d'un texte
Centroïde d'un segment
En partant du postulat, précédent nous avons décidé de représenter un segment thématique non pas par l'ensemble des vecteurs sémantiques (et donc des phrases) qui le composent, mais par un centroïde dont le calcul accordera plus d'importance aux premières phrases qu'aux dernières. Le vecteur centroïde est un barycentre dont les composantes sont calculées selon la méthode de Leibniz. Les dimensions de l'espace étant connues (les vecteurs sémantiques comprennent 873 composantes), nous avons pour j = 1 à j = 873, n nombre de vecteurs composant le segment thématique, A l'ensemble de ces vecteurs (A i étant le ième vecteur du segment dans l'ordre d'apparition et x j,A i la jème composante du vecteur A i ) :
avec C le vecteur centroïde du segment thématique, x j,C la jème composante du vecteur C et a i = n + 1 ? i. Ainsi la pondération a i qui détermine l'importance que l'on accorde au vecteur courant dans le calcul du barycentre sera égale à n pour le premier vecteur et à 1 pour le dernier, ce qui va dans le sens du postulat que nous avons énoncé plus haut.
2 mais généralement un petit nombre
La distance thématique
Afin de pouvoir mesurer la différence thématique entre deux phrases, deux centroïdes ou encore entre une phrase et un centroïde il nous faut disposer d'une fonction similarité ou d'une distance. Nous avons choisi d'adopter la distance thématique présentée par Lafourcade et Prince (2001). Ainsi, si X et Y sont deux vecteurs, D A étant la distance thématique recherchée, on a :
La distance D A est donc la distance angulaire entre les deux vecteurs X et Y exprimée en radians. Classiquement, le cosinus fait office de mesure de similarité en TALN. Le choix d'appliquer l'arc cosinus pour retrouver la distance angulaire s'est fait pour deux raisons :
-Elle correspond à une distance et présente donc l'avantage d'être réflexive, symétrique et de respecter l'inégalité triangulaire. -L'arccosinus est une fonction décroissante par rapport à la similarité, mais surtout fortement non linéaire pour des valeurs d'angles faibles (inférieur à ? 4 ), alors qu'elle se comporte de manière quasi linéaire pour les valeurs d'angles élevées. Ainsi on obtient une plus grande finesse d'analyse lorsque deux phrases sont sémantiquement proches. En nous appuyant sur ces différents outils, nous pouvons maintenant nous attacher à détec-ter les zones de transitions au sein d'un texte.
Détection des zones de transition
Afin de détecter les zones de transition abordées plus haut, nous faisons glisser une fenêtre le long du texte et attribuons à la phrase centrale de la fenêtre une valeur qui correspond à la distance thématique entre le centroïde calculé à partir des phrases précédant la phrase centrale dans la fenêtre (la phrase centrale exclue), et le centroïde calculé à partir de toutes les phrases suivant la phrase centrale (cette dernière étant cette fois incluse dans le centroïde comme le montre la figure 3).
Nous nous sommes servi du corpus d'apprentissage fourni par DEFT'06 plus comme un corpus de calibrage que d'apprentissage. Ainsi la taille de la fenêtre est de deux fois la taille moyenne d'un segment calculée sur le corpus d'apprentissage, une taille est calculée par type de texte. On suppose donc que le corpus de test, sensé être jumeau du corpus d'apprentissage, présentera les même caractéristique. Une fois la distance thématique estimée, on la compare avec un seuil à partir duquel on considère qu'il y a de fortes chances pour que la phrase fasse partie d'une zone de transition. Ce seuil est calculé à partir de chacun des corpus d'apprentissage comme suit :
1 Sur chaque corpus nous avons calculé les distances qui séparent chaque segments successif. 2 Nous avons calculé la moyenne et l'écart type de ces distances. 3 Le seuil est égal à la moyenne moins une fois l'écart type. L'usage de la plus petite distance observée sur le corpus comme valeur seuil a été envisagé, mais sur un tel volume de données traité il y a forcement des accidents et des valeurs particulières. Utiliser la moyenne n'aurait pas forcément été judicieux (éliminant trop de solutions et
FIG. 3 -Attribution d'une valeur de distance thématique à chaque phrase
faisant ainsi chuter le rappel). En utilisant un seuil égal à la moyenne moins l'écart type, on se prémunit des valeurs aberrantes qui pourraient survenir tout en étant moins restrictif que si l'on utilisait la moyenne seulement. Bien entendu, cela implique que nous supposons que le phénomène suit une loi normale. On notera que sur chacun des corpus ce seuil est proche de ? 4 , malgré la différence de structure et de discours qu'il existe entre les corpus. Au final, on obtient deux tableaux, l'un contenant des distances thématiques, l'autre des valeurs booléennes indiquant pour chaque phrase si elle fait partie d'une zone de transition ou non. Toujours dans un souci d'éviter les valeurs singulières, on élimine d'office toutes les phrases marquées comme zone de transition potentielle qui seraient isolées. Il nous reste à déterminer au sein de cette zone de transition quelles sont les phrases qui constituent vraiment une amorce de segment thématique. Pour ce faire, nous procédons de manière différente selon les corpus.
Les corpus scientifique et discours et la notion de phrase charnière
Toujours en nous appuyant sur la conception « classique » de la rédaction en langue française, nous avons émis l'hypothèse que pour qu'un texte soit bien construit, il doit comporter des phrases de transition ou phrases charnières (à ne pas confondre avec la zone de transition qui englobe la phrase de transition et les phrases adjacentes) entre chaque portion de texte thématiquement cohérente. Elles ont la particularité d'être la plupart du temps peu porteuses de thème, servant avant tout de lien logique entre deux parties d'un texte. Se trouvant à la frontière entre deux segments thématiques sans avoir de véritable importance thématique, la distance thématique d'une phrase de ce type au centroïde du segment thématique qui la pré-cède doit être proche de la distance au centroïde du segment suivant. Nous avons donc attribué à chacune des phrases traitées un score de transition. Si on désigne par St i le score de transition de la phrase i alors on a :
Où D p est la distance de la phrase examinée au centroïde du segment thématique précédent et D s la distance au centroïde du segment thématique suivant. Cette valeur est comprise entre 0 et 1 et se rapproche de 1 à mesure que les distances entre la phrase examinée et les centroïdes des segments thématiques adjacents se rapprochent. Si les deux distances sont égales (et donc que la phrase centrale est équidistante des deux segments thématiques) elle vaut 1. Si, au contraire, une des distances vaut ? 2 et l'autre 0 (et donc que la phrase centrale est complè-tement intégrée thématiquement à l'un des segments et pas du tout à l'autre) alors cette valeur vaut 0. Du fait de ces propriétés, nous pouvons utiliser ce score pour pondérer les distances thématiques des phrases suivantes.
A l'étape précédente, nous avons isolé de petites portions de texte susceptibles de contenir la phrase d'amorce d'un segment thématique. Ici, nous allons déterminer quelle phrase au sein de cette portion est la plus susceptible d'être la première phrase d'un segment thématique. Pour ce faire, nous partons du principe qu'une phrase est la première phrase d'un nouveau segment thématique si :
-La distance thématique qui lui a été attribuée est la plus élevée.
-La phrase qui la précède a de fortes chances d'être une phrase de transition. Comme il est peu probable que ces 2 conditions soient réunies simultanément, nous associons à chaque phrase de la zone de transition, une nouvelle valeur qui est le produit de la distance thématique attribuée à la phrase avec le score de transition de la phrase qui la précède. Il ne nous reste plus qu'à sélectionner le maximum.
Le corpus juridique et son traitement simplifié
La structure même d'un texte juridique exclut les phrases de transitions entre les articles. Il n'aurait donc pas été pertinent de rechercher ces dernières dans le cadre du traitement du corpus juridique. Toutefois, le corpus « loi » se présente sous la forme d'une succession d'articles tous précédés de la mention « Article X » (le X remplaçant le numéro de l'article afin que ce dernier ne soit pas immédiatement identifiable comme le premier d'une loi). Nous avons choisi de continuer à chercher les zones de transition selon la méthode présen-tée plus haut, mais pour déterminer quelle était la phrase d'amorce au sein de ces groupes de phrases nous recherchions simplement la phrase « Article X ».
Résultats expérimentaux
Pour évaluer les résultats, l'équipe organisatrice s'est appuyée sur trois calculs de F score différents. Si tous donnent la même importance à la précision et au rappel, les trois F scores se différencient par un certain degré de tolérance à l'erreur. Si le F score strict ne se calcul qu'en considérant les phrases ramenées, les F scores souples de taille 1 et 2 considèrent également les phrases autour de la phrase ramenée (immédiatement adjacentes pour la taille 1 et éloignées d'une phrase pour la taille 2).
Les résultats obtenus sont malheureusement partiels du fait d'un problème de temps lié au pré-traitement du corpus (le corpus « loi » n'étant traité qu'au quart dans la deuxième exécu-tion). La première exécution se base sur une méthode proche de la méthode décrite dans cet Toutefois, même partiels, ces résultats nous permettent de faire un certain nombre de constatations : -Ce qui ressort avant tout de ces résultats, c'est la faible précision de la méthode par rapport à la moyenne des participants. Ce manque de précision peut aisément s'expliquer. En effet, l'objectif de cette méthode est de détecter les zones au sein du texte où le thème change, pas la phrase exacte qui marque ce changement. -Le recour à un calcul de F score souple pour l'évaluation de cette tâche se voit totalement justifié. En effet, si les résultats avec un calcul de F score strict sont décevants, dès que l'on prend un tant soit peu de marge, ces derniers accusent une hausse significative. Cette remarque renforce l'idée qu'une frontière thématique est plus une zone floue, qu'une unité bien définie. -Le très fort rappel obtenu sur le corpus discours laisse supposer que les textes politiques obéissent probablement à un schéma d'organisation thématique. Il doit être possible d'extraire ou d'approximer ce dernier de manière algorithmique. On notera, qu'à l'exception d'une autre équipe que la nôtre, tous les participants au défi se sont appuyés sur des méthodes numériques dérivées de celle présentées au début de cet articles. Les équipes ayant utilisé des approches sémantiques et structurelles (dont la notre) ce situe dans le milieu de tableau (nous sommes 4ème sur le défi, l'autre équipe étant 5ème). Ces approches, étant encore jeunes comparées à des approches numériques, doivent être perfectionnées et ont donc une grande marge de progression devant elles.
Conclusion
Dans cet article, nous avons présenté une méthode originale de segmentation thématique qui s'appuie sur une approche sémantique. Cette dernière a déjà été testée sur différents domaines. D'abord en catégorisation de texte, où elle a donné de bons résultats, puis lors de la précédente édition de DEFT, pour identifier des auteurs, où elle a été moins performante. Autour de cette représentation plus sémantique du texte, nous avons étudié une méthode intégrant des contraintes stylistiques pour segmenter thématiquement le texte. Nous regrettons de n'avoir pu être évalués sur un jeu de données complet, toutefois les résul-tats obtenus, même partiels, nous laissent entrevoir des possibilités que nous allons explorer en dehors du cadre parfois restrictif d'une situation d'évaluation. Notre classement en milieu de tableau lors de l'atelier DEFT'06 (4ème sur 7) prouve que si notre approche n'est pas aussi efficace que les approches à base de méthodes numériques, elle reste viable, et mérite d'être approfondie. Même si l'utilisation d'un F score souple permet d'avoir une meilleure vision de l'efficacité des méthodes, le découpage même des textes peut être sujet à contestation. La notion de thème telle qu'elle est abordée dans le défi DEFT'06, à savoir l'idée directrice d'un segment de texte, est très subjective. Peut-on affirmer que les différents paragraphes du corpus scientifique forment bien des segments thématiques distincts ? Le découpage des discours politiques est-il approprié ? Sans mettre en doute la compétence des experts qui ont préparé ce corpus, d'autres experts auraient-ils découpé les corpus de la même manière ? Il pourrait être instructif de procéder à l'évaluation autrement, en proposant par exemple à des

Introduction
Cet article présente un outil de recherche d'information associant sémantique et contexte conceptuel. Notre objectif est d'utiliser conjointement l'analyse conceptuelle et la sémantique afin de fournir des réponses contextuelles aux requêtes des utilisateurs sur le web.
Dans cet article, nous présentons notre méthodologie et nous l'illustrons par une recherche d'information effectuée sur un ensemble de pages web relatives au domaine du tourisme. Le processus de recherche d'information est divisé en deux étapes :
-traitement hors ligne de pages web ; -traitement contextuel en ligne de requêtes utilisateurs. Le prétraitement consiste à construire un treillis conceptuel à partir de pages web, par exemple dans le domaine du tourisme, de manière à obtenir un contexte conceptuel global ; cette notion est définie dans la section 3.2. Chaque concept du treillis correspond à un groupe de pages web ayant des propriétés communes. Un appariement sémantique est effectué entre les termes décrivant chaque page et un thésaurus du domaine du tourisme (thésaurus de l'Organisation Mondiale du Tourisme), permettant de labelliser chaque concept de façon standardisée.
Tandis que le traitement des pages web est effectué hors ligne, la recherche d'information se fait en temps réel : les utilisateurs formulent leurs requêtes à l'aide des termes du thesaurus. Cette classe de termes est alors comparée avec les labels des concepts et les concepts les plus pertinents sont délivrés à l'utilisateur. Celui-ci peut alors naviguer à travers le treillis de manière à généraliser ou, au contraire, à spécialiser sa requête.
Cette méthode présente plusieurs avantages : -les résultats sont fournis à la fois en fonction du contexte de la requête et du contexte des données disponibles. Par exemple, seuls les raffinements de requêtes correspondant à des pages touristiques existantes sont proposés ; -l'ajout de sémantique peut dépendre de l'utilisateur cible ; -une sémantique plus puissante, comme les ontologies, peut être ajoutée. Ceci permet d'améliorer la formulation des requêtes et la pertinence des résultats. Cet article est organisé de la manière suivante : la section 2 introduit la notion de contexte, dans le sens général et dans le domaine de l'informatique. La section 3 décrit brièvement l'analyse formelle de concepts et les treillis de Galois, puis définit notre notion de contexte conceptuel global et instantané. Enfin, nous concluons et donnons quelques perspectives concernant la poursuite et l'application de ces travaux.
Notion de contexte
Un contexte est une notion abstraite et ne peut pas être défini de manière précise puisqu'il est lié à une situation particulière. Nous avons tendance à associer un contexte de manière implicite à un ensemble d'actions, une attitude, etc. dans des situations courantes. Des définitions de la notion de contexte ont émergé en psychologie cognitive, philosophie, ainsi que dans des domaines de l'informatique comme le traitement du langage naturel.
Le concept de contexte formel a été introduit par McCarthy dans (McCarthy 1968(McCarthy , 1987. Selon Giunchiglia, qui a également effectué des travaux de recherche sur la formalisation de contexte, « un contexte est une théorie sur le monde qui englobe les perspectives subjectives des individus ». Cette théorie est partielle -incomplète -et approximative du fait que le monde n'est jamais décrit dans tous ses détails (Giunchglia, 1993).
La notion de contexte est importante pour beaucoup de communautés de recherche comme l'intelligence artificielle, la résolution de problèmes, etc. (Brezillon 1999ade problèmes, etc. (Brezillon , 1999b, (Theodorakis et Spyratos, 2002). En ce qui concerne l'intelligence artificielle, l'interaction entre contextes se fait au moyen de règles, qui permettent de naviguer d'un contexte à un autre (Guha et McCarthy, 2003). Les contextes peuvent être représentés par des graphes contextuels, des topic maps, les logiques de description avec notamment les extensions OWL, etc.
Comme dans le cas du web sémantique, le contexte est utilisé soit en tant que filtre dans un but de désambiguïsation pour la recherche d'information (Dolog et al, 2006), soit pour définir des services web contextuels (Mrissa et al, 2006), ou enfin comme un moyen d'intégrer ou de fusionner des ontologies, (Bouquet et al, 2004), (Doan et al, 2002). Un contexte peut être spécifié à différents niveaux de granularité (document, page web, etc.). Cette information additionnelle peut être liée à chaque ressource.
RNTI -X -
Contextes conceptuels -Relation avec les ontologies
Dans la section précédente, nous avons présenté diverses définitions de la notion de contexte. Dans cet article, nous définissons des contextes conceptuels, basés sur l'analyse formelle de concepts, en particulier les treillis de Galois. Beaucoup de travaux de recherche ont appliqué les treillis de concepts à la recherche d'information (Priss, 2000). Les concepts formels peuvent être vus comme des documents pertinents pour une requête donnée. L'introduction d'une ontologie de domaine, combinée avec les treillis de concepts pour améliorer la recherche d'information est plus récente. (Messai et al, 2005) proposent une approche basée sur l'analyse formelle de concepts pour classifier et rechercher des sources de données pertinentes pour une requête donnée. Ces travaux ont été appliqués à des données de bioinformatique. Un treillis de concepts est construit en fonction des métadonnées associées aux sources de données. Puis, un concept construit à partir d'une requête donnée est fusionné à ce treillis de concepts. Dans cette approche, le raffinement de requête s'effectue en utilisant une ontologie de domaine. Le processus de raffinement d'OntoRefiner, outil dédié aux portails web sémantiques (Safar et al, 2004), est basé sur l'utilisation d'une ontologie de domaine pour construire un treillis de Galois pour le processus de raffinement de requête. L'ontologie de domaine évite de construire complètement le treillis ; ce travail vise à améliorer la construction du treillis, ce qui n'est pas l'objectif de notre travail. Enfin, le système CREDO (Carpineto et Romano, 2004) permet à l'utilisateur d'interroger des documents web et de voir les résultats à travers la navigation dans un treillis de concepts (http://credo.fub.it). (Dolog et al, 2006) ont proposé une méthode pour relâcher automatiquement des requêtes trop contraintes en se basant sur la connaissance du domaine et les préférences utilisateur. Leur approche combine raffinement et relaxation de manière à permettre un accès personnalisé à des données RDF hétérogènes. Contrairement à cette approche, notre méthode est dédiée à des requêtes imprécises et centrées utilisateurs.
Dans notre proposition, les treillis de Galois sont construits pour représenter le contenu de pages web. L'utilisateur peut alors naviguer dans ces treillis de manière à raffiner ou généraliser sa requête. Comparativement aux approches décrites ci-dessus, notre méthode n'est pas seulement dédiée à la recherche d'information mais peut être utilisée pour d'autres objectifs comme le peuplement d'ontologies, la comparaison de sites web à travers leurs treillis respectifs, une aide au concepteur de site web pour vérifier que le contenu du site reflète bien le message qu'il a voulu faire passer, etc.
Cette section est organisée de la manière suivante : après une brève introduction aux treillis de Galois, nous proposons notre définition de contextes conceptuels global et instantané.
Introduction à l'Analyse Formelle de Concepts et aux Treillis de Galois
L'Analyse Formelle de Concepts est une approche mathématique de l'analyse de données qui permet de fournir une structure à l'information. Cette approche peut être utilisée pour le clustering conceptuel, comme montré dans (Carpineto et Romano, 1993) et dans (Wille, 1984).
RNTI -X -La notion de treillis de Galois établissant une relation entre deux ensembles est à la base d'un ensemble de méthodes de classification conceptuelles. Cette notion fut introduite par (Birkoff, 1940) et (Barbut et Monjardet, 1970). Les treillis de Galois consistent à regrouper des objets en classes qui vont matérialiser les concepts du domaine d'étude. Les objets individuels sont discriminés en fonction de leurs propriétés communes, ce qui permet d'effectuer une classification sémantique. L'algorithme que nous avons implémenté est basé sur celui proposé dans (Godin, 1998).
Nous introduisons tout d'abord les principaux concepts des treillis de Galois. Soient deux ensembles finis E et E' (E est un ensemble d'objets et E' est l'ensemble de leurs propriétés), et une relation binaire R ? E x E' entre ces deux ensembles. La figure 1 montre un exemple de relation binaire entre deux ensembles. Selon la terminologie de Wille (Wille, 1992), le triplet (E, E', R) est un contexte formel correspondant à un unique treillis de Galois. Il représente des regroupements naturels d'éléments de E et E'.
Soient P(E) une partition de E et P(E') une partition de E'. Chaque élément du treillis est un couple, appelé aussi concept, noté (X, X'). Un concept est composé de deux ensembles X ? P(E) et X' ? P(E') satisfaisant les deux propriétés suivantes :
Un ordre partiel sur les concepts est défini de la manière suivante :
Cet ordre partiel est utilisé pour dessiner un graphe appelé diagramme de Hasse, illustré par la figure 1. Il existe un arc entre deux concepts C 1 et C 2 si C 1 <C 2 et s'il n'y a pas d'autre élément C 3 dans le treillis tel que C 1 <C 3 <C 2 . Dans un diagramme de Hasse, la direction des arcs est par convention toujours dans le même sens, soit vers le haut. Le graphe peut être interprété comme une représentation de la relation de généralisation / spécialisation entre les couples, où C 1 <C 2 signifie que C 1 est plus général que C 2 (et C 1 est situé au-dessus de C 2 dans le diagramme).
Fig. 1. Relation binaire et treillis de Galois associé (Diagramme de Hasse).
RNTI -X -Le treillis de concepts montre les points communs entre les objets du domaine. La première partie d'un concept est l'ensemble des objets et est appelée l'extension. La seconde partie -l'intension -révèle les propriétés communes des objets de l'extension. La partie droite de la figure 1 montre un exemple de treillis de concepts. Par exemple, dans ce treillis, le concept {(1, 4), (c, f, h)} contient dans son extension les objets 1 et 4, qui ont les propriétés c, f et h en commun. 1. Du fait des aspects fortement exploratoires et itératifs du processus de recherche d'information, la principale propriété de ce contexte conceptuel instantané est qu'il évolue à chaque fois que l'utilisateur modifie, raffine ou généralise sa requête. 2. La variation de ce contexte conceptuel instantané est bornée par le contexte conceptuel global, comme illustré dans la figure 3. 3. Une page web donnée peut appartenir à différents contextes conceptuels instantanés du fait des relations de généralisation/spécialisation inhérentes aux treillis de Galois.
Définition d'un contexte conceptuel
Enfin, l'information fournie par le contexte conceptuel (global ou instantané) est complémentaire à l'information intrinsèque des pages web (les propriétés, dans notre cas, représentent les termes les plus significatifs de la page).  
RNTI -X -
Méthodologie pour une coordination sémantique de contextes conceptuels et d'ontologies
Notre méthodologie de recherche d'information sémantique et contextuelle peut être divisée en deux étapes : un pré traitement hors ligne et un traitement contextuel en ligne des requêtes des utilisateurs. Dans cette section, nous décrivons ces opérations et les illustrons avec un exemple simple dans le domaine du tourisme.
Etape 1: prétraitement hors ligne
Sélection de sites web de tourisme
La première étape consiste à construire un contexte conceptuel global dans un domaine donné et sur un ensemble de pages web pertinentes. Ces pages peuvent appartenir à plusieurs sites web. Pour des besoins d'illustration, nous avons sélectionné 5 pages web à partir d'un site touristique français (le site de la mairie de Metz) : http://tourisme.mairie-metz.fr/. Cet ensemble de pages web constitue l'information de base à partir de laquelle la recherche d'information sémantique et contextuelle est réalisée.
Analyse des pages Web : génération des données d'entrée pour le treillis de Galois (objets et propriétés)
Le contexte conceptuel global est représenté par le treillis de Galois construit à partir de l'ensemble des pages web sélectionnées ; nous devons générer des données en entrée appropriées pour le calcul du treillis conceptuel :
-Chaque page web correspond à un objet;
RNTI -X --Les propriétés d'une page web sont les noms significatifs les plus fréquents de la page (ils sont extraits à l'aide de notre outil basé sur Tree-Tagger comme l'illustre la figure 4). La liste des objets et de leurs propriétés est stockée dans une base de données Mysql, comme le montre la figure 4. Par exemple, l'objet 3 (correspondant à une page web spécifique) est décrit par les propriétés spectacle et réservation.
Fig. 4. Extraction d'objets/propriétés et génération de la base de données.
Construction du treillis de Galois
La base de données décrite ci-dessus -contenant des objets et propriétés -est utilisée comme entrée pour la construction d'un treillis de Galois. Nous utilisons un algorithme existant pour la construction incrémentale du treillis de Galois (Godin, 1998), car notre contribution concerne l'exploitation et l'interprétation de ces treillis et non l'optimisation de leur construction. La construction du treillis consistant à identifier et structurer tous les regroupements d'objets en fonction de leurs propriétés communes, le nombre de concepts du treillis devient rapidement élevé si le nombre d'objets et/ou de propriétés est important. Dans le cas de sites possédant de nombreuses pages, cela aura donc un impact sur le temps de calcul. Ceci n'est pas trop pénalisant dans notre approche puisque cette étape est effectuée lors d'une phase de prétraitement.
En sortie, nous obtenons un treillis de concepts où chaque concept consiste en un ensemble d'objets ayant des propriétés communes. La liste des objets d'un concept est appelée l'extension d'un concept et les propriétés partagées correspondantes constituent l'intension du concept. Le treillis généré à partir de la base de données de la figure 4 est illustré sur la figure 5.
RNTI -X -
Fig. 5. Le contexte conceptuel global : le treillis de Galois.
Le treillis de la figure 5 contient 12 noeuds (concepts). Chaque concept est caractérisé par son extension -la liste des objets qu'il contient -et son intension -la liste des propriétés communes des objets de l'extension. Le noeud en haut du treillis est le plus général et contient tous les objets, qui n'ont pas de propriétés communes. Plus nous descendons dans le treillis, plus les noeuds deviennent spécifiques. Lors de l'interprétation du treillis, il est intéressant d'étudier quels objets de l'extension sont similaires à d'autres.
Ce treillis nous montre que l'objet 3 apparaît seulement dans deux concepts car il possède seulement deux propriétés et ne partage que la propriété réservation avec les objets 1, 2 et 4. Nous pouvons également remarquer qu'aucun objet ne partage la propriété spectacle avec l'objet 3. Au contraire, l'objet 2 apparaît dans cinq concepts car il possède plus de propriétés (4) que l'objet 3 et aussi parce que ses propriétés sont partagées par plus d'objets (4) que l'objet 3.
L'interprétation que nous pouvons faire de ce treillis est que : -Le contexte conceptuel global du site touristique de la ville de Metz indique que celuici est moins ciblé vers le spectacle et plus dirigé vers les centres d'intérêts de la ville elle-même. -Les requêtes des utilisateurs sur les spectacles ne pourront pas être raffinées mais généralisées sur le concept réservation, et de ce fait, sont fortement bornées par le contexte conceptuel global. En d'autres termes, si un utilisateur est plus intéressé par un tourisme basé sur le spectacle, la ville de Metz n'est pas l'endroit le mieux adapté (au vu des informations disponibles sur le Web). L'analyse des requêtes des utilisateurs va permettre de déterminer si l'utilisateur est plus intéressé par les spectacles ou par une autre forme de tourisme. Dans ce RNTI -X -cas, il sera possible de lui proposer un autre contexte conceptuel global plus orienté "spectacles". Ce nouveau contexte peut être relié au contexte conceptuel global du tourisme de la ville de Metz en réalisant un appariement avec des ontologies comme nous le proposons dans notre conclusion.
Labellisation des concepts du treillis
La dernière étape du traitement hors ligne consiste à affecter un label aux concepts du treillis de manière normalisée, c'est à dire en utilisant les termes d'un thesaurus ; nous avons utilisé le thesaurus de l'Organisation Mondiale du Tourisme qui décrit le tourisme et les activités de loisirs. La même opération peut être réalisée avec une ontologie plutôt qu'un thesaurus.
L'analyse des pages web consiste à extraire les mots les plus fréquents d'une page (voir section 4.1.2). Notre analyse va plus loin en réalisant un appariement syntaxique entre ces mots fréquents et les termes du thesaurus de l'OMT. Par exemple, la propriété spectacle est liée à l'entrée loisirs du thesaurus. Le label normalisé d'un concept est constitué de l'ensemble des labels normalisés associés aux objets de son extension.
Le lien entre les données textuelles -les pages web de tourisme -et une structure sémantique -le thesaurus de l'OMT -nous permet d'obtenir un contexte conceptuel global plus riche. En effet, ce contexte reflète aussi bien l'information issue des données d'origine que les connaissances générales du domaine.
Etape 2: traitement en ligne des requêtes des utilisateurs
Une fois le contexte conceptuel global construit, le contexte conceptuel instantané est calculé pour chaque requête utilisateur.
Formulation d'une requête avec des mots-clés
Les utilisateurs formulent leurs requêtes avec des mots-clés (nous pouvons restreindre ces mots-clés aux entrées du thesaurus ou de l'ontologie). L'intérêt d'avoir normalisé les labels des objets et concepts est la possibilité d'utiliser un vocabulaire contrôlé.
Identification des concepts les plus pertinents du treillis
La réponse à une requête correspond au concept dans le treillis dont les propriétés de l'intension s'apparient le mieux avec les mots-clés de cette requête. S'il n'existe pas de concept correspondant parfaitement, les concepts les plus pertinents sont proposés à l'utilisateur à travers un raffinement ou une généralisation, qui s'effectue en fonction des données disponibles (dans les pages web de tourisme), c'est-à-dire correspondant au contexte des données.
La figure 6 montre une autre représentation visuelle du treillis de Galois. Cette interface est plus riche du fait qu'elle montre également les liens avec les données sémantiques -les entrées du thesaurus de l'OMT. Supposons que l'utilisateur entre une requête avec le mot-clé réservation. Le contexte conceptuel instantané correspondant à cette requête est illustré dans la fenêtre de gauche avec une focalisation sur le concept du treillis (concept numéro 5), dont l'extension est {page1, page2, page3, page4} et la propriété commune entre ces objets est RNTI -X -{réservation}. Nous pouvons également noter que ce concept est labellisé avec trois concepts du thesaurus: monument, loisirs, europe.
Fig. 6. Navigation à travers le contexte global enrichi avec de l'information sémantique
A partir de ce contexte instantané (noeud numéro 5), l'utilisateur est libre de naviguer soit dans d'autres noeuds du treillis -à travers des concepts plus généraux et plus spécifiques -soit dans le thesaurus.
La partie en haut à droite de la figure 6 donne un aperçu de l'entrée monument du thesaurus, et la partie gauche de la fenêtre montre la hiérarchie du thesaurus -monument est une sous-classe de tourisme et une superclasse de architecture, musée et patrimoine-de même que les autres concepts du treillis labellisés avec l'entrée monument-soient les concepts 1, 3,5,9,10,7,4 et 11. Encore une fois, la navigation peut se poursuivre soit à travers le thesaurus ou en revenant sur le treillis conceptuel. L'avantage de cette interface de navigation est que l'utilisateur peut explorer facilement les données via le thesaurus -ou une ontologie -et le contexte conceptuel global, ceci de manière totalement transparente. L'élargissement du contexte des données avec la connaissance du domaine véhiculée par un thesaurus ou une ontologie permet de fournir des réponses plus riches et plus pertinentes aux requêtes des utilisateurs. Cette interface n'est qu'un exemple de représentation et de navigation illustrant l'utilisation des contextes conceptuels. Dans l'avenir, nous nous pencherons plus en détail sur la question des interfaces afin qu'elles permettent d'optimiser l'utilisation de ces contextes globaux et instantanés.
Conclusion et perspectives
Dans ce papier, nous avons présenté une méthodologie combinant l'analyse conceptuelle avec la sémantique. Notre méthodologie est divisée en deux étapes ; la première consiste en RNTI -X -un prétraitement hors ligne d'un ensemble de pages web à partir desquelles un treillis conceptuel est construit. Chaque concept correspond à un cluster de pages web partageant un ensemble de propriétés (un ensemble de termes communs). Puis, les termes pertinents des pages web sont mis en correspondance avec un thesaurus du domaine du tourisme, permettant ainsi d'affecter des étiquettes (labels) à chaque concept du treillis de manière normalisée. La deuxième étape est une phase de traitement contextuel des requêtes des utilisateurs. Les termes constituant la requête sont alors comparés avec les labels des concepts. L'utilisateur peut ensuite naviguer comme il le souhaite dans le treillis pour raffiner ou bien généraliser sa requête ; il peut également naviguer à travers la structure sémantique -thesaurus ou ontologie -s'il a besoin de la connaissance du domaine pour affiner sa recherche.
Nous avons illustré cette méthodologie dans le domaine du tourisme. Un thesaurus a été utilisé, mais les travaux futurs prévoient d'étendre cette méthodologie aux ontologies. L'avantage des ontologies ou des thesaurus pour les contextes est qu'il est possible de lier divers contextes à travers ces structures sémantiques. Le contexte peut être élargi par l'ontologie. Inversement, l'avantage des contextes pour les ontologies est que le fait de les relier à différents contextes revient à les instancier avec ces différents contextes. Nous pouvons donc voir ces contextes comme un moyen de peupler les ontologies.
Dans ce papier, nous avons montré que l'utilisation conjointe de treillis conceptuels et de la sémantique pouvait permettre d'obtenir des résultats intéressants pour la recherche d'information sémantique et contextuelle. Mais cette combinaison de l'analyse formelle de concepts et de la sémantique peut également être utilisée dans le cadre d'objectifs différents, comme par exemple le peuplement d'ontologies. Cette méthodologie peut également permettre de comparer différents sites web à travers leurs treillis respectifs. Un autre aspect intéressant est que, les termes correspondants à des points d'entrée d'un ou plusieurs treillis, les utilisateurs vont pouvoir naviguer d'un treillis à l'autre pour reformuler leurs requêtes. Enfin, une autre application possible est d'aider les concepteurs d'un site web du fait que le treillis va refléter le contenu du site. Il sera alors aisé de comparer le site web/treillis résultant avec les objectifs initiaux du concepteur de site, et ainsi de vérifier si le site est bien conforme à ces objectifs.

Introduction
L'un des problèmes majeurs rencontrés dans la fouille des règles d'association valides au sens de la confiance est le nombre souvent très élevé de ces règles. Plusieurs solutions à ce problème ont été proposées ou considérées dans la littérature. Parmi ces solutions figurent les bases, c'est-à-dire, des familles génératrices minimales (Zaki et Ogihara, 1998;Pasquier et al., 1999). La plupart de ces bases se caractérisent en terme d'un opérateur de fermeture de Galois sur l'ensemble des motifs du contexte considéré. Or, cet opérateur de fermeture correspond à une famille de Moore m-faiblement hiérarchique, où m ? 2 est un entier (Diatta, 2004). Plus précisément, les fermés de cet opérateur de fermeture coïncident avec les classes faibles associées à une certaine mesure de dissimilarité m-voies et forment donc, de ce fait, la hiérarchie m-faible associée à cette mesure de dissimilarité.
Dans cet article, nous considérons la caractérisation de ces bases pour les règles d'association, en remplaçant l'opérateur de fermeture de Galois par un opérateur de fermeture correspondant à la hiérarchie k-faible associée à une mesure de dissimilarité k-voies donnée, où 2 ? k ? m. Pour chaque valeur de k, l'ensemble de règles ainsi caractérisé sera appelé sousbase k-faible. Ces sous-bases k-faibles offrent une approximation de l'ensemble des règles valides, relativement à des ensembles d'items (classes k-faibles) ayant un certain degré d'homogénéité exprimé par le biais d'un indice d'isolation. Par ailleurs, la possibilité d'associer une sous-base (k?) faible à une mesure de dissimilarité (k-voies) permet d'intégrer la sémantique de cette mesure de dissimilarité dans le choix des règles à générer.
Règles d'association
Définition générale
Étant donné un contexte binaire K = (E, V), où E désigne un ensemble fini d'entités et V un ensemble fini de variables booléennes (ou attributs) définies sur E. On appelle motifs les sous-ensembles de V et on dit qu'une entité possède un attribut "x" si x(e) = 1.
Une règle d'association de (E, V) est un couple (X, Y ) de motifs, notée X?Y , où Y est non vide. X et Y sont respectivement appelés "prémisse" et "conséquent".
Étant donné un motif X, X désignera l'ensemble des entités qui possèdent tous les attributs de X, i.e., X = {e ? E : ?x ? X, x(e) = 1}. Un contexte binaire (E, V) contient 2 |V| (2 |V| ? 1) règles d'association parmi lesquelles beaucoup ne sont pas pertinentes. On utilise des mesures de qualité pour sélectionner uniquement les règles qui vérifient des contraintes données.
Règles d'association confiance-valides
Une mesure de qualité pour les règles d'association d'un contexte K est une application à valeurs réelles, définie sur l'ensemble des règles d'association de K. Beaucoup de mesures de qualité on été proposées dans la littérature, les plus utilisées d'entre elles étant le support et la confiance (Agrawal et al., 1993).
Le support d'un motif X est la proportion des entités de E qui possèdent tous les attributs de X ; on le définit par Supp(X) = |X | |E| , où, pour un ensemble fini S, |S| désigne le cardinal de S. Si on note p la mesure de probabilité intuitive définie sur (E, P(E)) par p(E) = |E| |E| pour E ? E, alors le support de X peut s'écrire en termes de p par :
. La confiance d'une règle d'association X?Y est la proportion des entités qui possèdent tous les attributs de Y , parmi celles qui possèdent tous les attributs de X ; elle est définie par Conf(X?Y ) =
conditionnelle de Y sachant X . Une règle d'association est confiance-valide si sa confiance est au moins égale à un seuil minimum (de validité) fixé. Une règle est dite confiance-exacte si sa confiance est égale à 1 ; elle est dite confiance-approximative si sa confiance est strictement inférieure à 1.
Bases pour les règles confiance-valides
L'un des problèmes majeurs de l'extraction de règles d'association est le nombre très élevé de règles générées. En effet, pour une mesure de qualité donnée µ, l'ensemble des règles d'association µ-valides contient fréquemment de nombreuses règles redondantes, i.e qui peuvent être déduites d'autres règles µ-valides. Pour palier ce problème on cherche à calculer une base de l'ensemble des règles valides, c'est à dire un ensemble minimal (au sens de l'inclusion) de règles d'association valides, à partir duquel toute règle valide peut être dérivée. Dans ce papier, nous considérons des bases qui se caratérisent en termes d'opérateurs de fermeture de Galois.
Opérateurs de fermeture de Galois
Le contexte binaire K induit une correspondance de Galois entre les ensembles ordonnés Barbut et Monjardet, 1970). Par ailleurs, la correspondance de Galois (f, g) induit un opérateur de fermeture ? := f • g sur (P(V), ?) (Birkhoff, 1967). Cet opérateur de fermeture sera dit de Galois, et un motif X sera dit fermé de Galois du contexte K, ou ?-fermé, si ?(X) = X. Pour un motif X, ?(X) sera appelé sa ?-fermeture.
Bases de Guigues-Duquenne et de Luxenburger
L'ensemble des règles d'association confiance-exactes est un système complet d'implications, i.e. il satisfait les axiomes d'inférence de Armstrong (1974). Il en découle que la base de Guigues et Duquenne (1986) pour les systèmes complets d'implications est aussi une base pour les règles d'association confiance-exactes.
La base de Guigues-Duquenne pour les règles d'association confiance-exactes est l'ensemble GD = {X??(X) \ X : X est ?-critique}, où un motif X est ?-critique s'il n'est pas ?-fermé et contient strictement la ?-fermeture de tout motif ?-critique Y tel Y ? X. Pour tout motif X, ?(X) est applelé la ?-fermeture de X.
La base de Luxenburger (1991) pour les règles d'association confiance-approximatives est l'ensemble LB = {X?Y :
4 Sous-bases k-faibles pour les règles confiance-valides
Opérateurs de fermeture et familles de Moore
Soit E un ensemble. Une famille de Moore sur E est une partie F de l'ensemble P(E) des parties de E telle que E ? F et F ? F impliquent ?F ? F. Si F est finie, donc si E est fini, alors la seconde condition peut être remplacée par :
Par ailleurs, étant donnée un famille de Moore F sur E, l'application ? F définie sur P(E) par ? F (X) = ?{Y ? F : X ? Y } est un opérateur de fermeture sur P(E).
Réciproquement, étant donné un opérateur de fermeture ? sur E, la collection F ? de sousensembles de E, définie par F ? = {X ? E : ?(X) = X} est une famille de Moore sur E.
Fermés de Galois et classes faibles
Une mesure de dissimilarité (mutuelle) sur E est une fonction
. Dans ce papier, nous remplaçons les deux premières conditions par la condition moins forte d 2 (x, y) ? d 2 (x, x). Une justification de cet affaiblissement peut être trouvée dans (Diatta, 2006).
Un sous-ensemble X de E est une classe faible associée à une mesure de dissimilarité
x,y?X,z / ?X est strictement positif. En d'autres termes, pour tous x, y dans la classe et tout z extérieur à la classe, au moins l'une des dissimilarités d 2 (x, z) et d 2 (y, z) est strictement supérieure à la dissimilarité d 2 (x, y). Les mesures de dissimilarité (mutuelle) se généralisent naturellement en mesures de dissimilarité dites multivoies permettant d'évaluer le degré de dissemblance globale entre entités d'un ensemble de plus de deux éléments.
Etant donnés un ensemble S et un entier k ? 1, désignons par S * ?k l'ensemble des sousensembles non vides de S ayant au plus k éléments. Alors, en adoptant la définition ensembliste proposée dans (Diatta, 2006), une mesure de dissimilarité k-voies sur E est une fonction monotone croissante définie sur E * ?k à valeurs réelles positives ou nulles, i.e., une fonction
On notera que les mesures de dissimilarité (mutuelle) correspondent au cas où k = 2. Par ailleurs, on parlera de mesure de dissimilarité multivoies pour signifier une mesure de dissimilarité k-voies quelconque, pour k ? 2. De plus, telle qu'observée dans (Bandelt et Dress, 1994), la notion de classe faible s'adapte tout aussi naturellement aux mesures de dissimilarité multivoies.
Ainsi, un sous-ensemble X de E est une classe faible associée à une mesure de dissimilarité
est strictment positif. Par ailleurs, il a été montré dans (Diatta, 2004) qu'il existe un entier m ? 2 tel qu'un sous-ensemble X de V est ?-fermé si et seulement si X est une classe faible associée à une certaine mesure de dissimilarité m-voies d m sur V. Il découle de ce résultat que les motifs ?-fermés forment une famille de Moore m-faiblement hiérarchique appelée la hiérarchie mfaible associée à la mesure de dissimilarité d m . Ainsi, ? est tout simplement l'opérateur de fermeture correspondant à cette famille de Moore au sens de la section 4.1.
Sous-bases k-faibles
Nous avons vu dans la section 3 qu'aussi bien la base de Guigues-Duquenne que celle de Luxenburger pour les règles d'association confiance-valides se caractérisent en terme de l'opé-rateur de fermeture ?. Par ailleurs, nous venons de voir dans la section 4.2 que l'opérateur de fermeture ? correspond à une famille de Moore m-faiblement hiérarchique, pour un certain entier m ? 2. Ces deux observations nous conduisent à considérer les analogues respectifs des bases de Guigues-Duquenne et de Luxenburger, en remplaçant l'opérateur de fermeture ? par un opérateur de fermeture correspondant à une famille de Moore k-faiblement hiérarchique quelconque, où 2 ? k ? m. Pour k = m, on peut retrouver ces bases sous certaines conditions mais, dans le cas g´néralg´néral, les ensembles de règles obtenus ne sont pas des bases au sens algébrique.
Ainsi, nous appellerons sous-base k-faible de Guigues-Duquenne pour les règles d'association confiance-valides un ensemble de règles GD ? k défini par :
où ? k est un opérateur de fermeture correspondant à une famille de Moore k-faiblement hié-rarchique.
On notera que les règles de GD ? k ne sont pas nécessairement exactes. En effet, certaines de ces règles peuvent avoir des exceptions pour la simple raison que la ?-fermeture de la prémisse d'une telle règle peut être strictement contenue dans sa ? k -fermeture, i.e., X ? k -critique et ?(X) ? ? k (X).
On notera également qu'il a été montré dans (Diatta, 2005) que si F une famille de Moore k-faiblement hiérarchique sur E, contenant toutes les parties de E d'au plus k ? 1 éléments, alors X ? E est ? F -critique si et seulement si X / ? F et |X| = k. Ce résultat généralise en fait son analogue obtenu par Domenach et Leclerc (2004) dans le cas particulier des familles de Moore faiblement hiérarchiques (k = 2).
De même, nous appellerons sous-base k-faible de Luxenburger pour les règles d'association confiance-valides un ensemble de règles LB ? k défini par :
On notera que les règles de LB ? k ne sont pas nécessairement approximatives. En effet, certaines de ces règles peuvent être exactes si par exemple leur conséquent est contenu dans la ?-fermeture de leur prémisse, i.e.,
Conclusion et discussion
Nous avons introduit la notion de sous-base k-faible relative à l'opérateur de fermeture correspondant à une famille de Moore k-faiblement hiérarchique. Ces sous-bases k-faibles peuvent permettre de réduire le nombre de règles générées tout en assurant que les règles ainsi générées soient relatives à des ensembles d'items (classes k-faibles) ayant un certain degré d'homogénéité exprimé par le biais d'un indice d'isolation. Le degré d'homogénéité de ces classes peut, dans une certaine mesure, garantir la cohérence du lien entre la prémisse et le conséquent d'une règle. En effet, on sait que des règles générées dans l'approche classique peuvent lier des motifs qui sont en réalité très peu liés voire statistiquement indépendants. Par ailleurs, la possibilité d'associer une sous-base (k?) faible à une mesure de dissimilarité (kvoies) permet d'intégrer la sémantique de la mesure de dissimilarité dans le choix des règles à générer. On notera qu'une sous-base k-faible n'est pas nécessairement une famille génératrice, donc pas une base au sens algébrique du terme. Toutefois, cela ne nous semble pas être un handicap. En effet, l'intérêt d'un ensemble de règles générées nous parait être plus dans la pertinence des règles qu'il comporte que dans sa capacité à permettre de reconstruire toutes les règles valides. Cela étant, les idées présentées dans ce travail posent, entre autres, les deux questions suivantes : (a) comment générer efficacement une sous-base k-faible ? (b) quel est le degré d'approximation des bases usuelles par les sous-bases k-faibles ?
Références
Agrawal, R., T. Imielinski, et A. Swami (1993). Mining association rules between sets of items in large databases. In P. 

Introduction
Les données issues du monde réel sont souvent entâchées d'imperfections. En particulier, il est très courant de disposer de nombreuses données incomplètes (pannes, erreur de format, oubli humain, ...). Or la présence de valeurs manquantes induit de très sérieux problèmes, les données contenant des valeurs manquantes étant souvent éliminées lors du processus de fouille de données. C'est notamment le cas pour l'extraction de motifs séquentiels. Cette technique de fouille de données, présentée comme une extension des règles d'association prenant en compte l'information temporelle des bases de données historisées, ne permet en effet que l'analyse des données complètes, sans tenir compte des enregistrements incomplets, ce qui constitue une grande perte d'information. Par ailleurs, les solutions de remplacement des valeurs manquantes sont souvent soit trop simplistes pour produire des résultats intéressants, soit trop coûteuses pour être mises en oeuvre sur de gros volumes de données.
Or, s'il existe à ce jour des techniques robustes aux valeurs manquantes pour l'extraction de règles d'association, il n'existe aucune méthode générique pour l'extraction de motifs séquentiels. En effet, dans le contexte de la recherche de motifs séquentiels, les valeurs manquantes n'ont pas été considérées jusqu'ici, l'application principale, les bases de données de supermarchés, n'en comportant quasiment jamais. Désormais, les motifs séquentiels sont utilisés afin d'extraire des connaissances d'applications industrielles (analyse de processus, web access logs, ...) qui contiennent inévitablement des données incomplètes.
Nous proposons donc ici une extension des principes décrits dans (Agrawal et Srikant, 1995) afin d'extraire des séquences fréquentes en présence de valeurs manquantes réparties aléatoirement dans une base de données. Cette approche a été inspirée par une méthode d'extraction de règles d'association sur des bases de données incomplètes (Ragel et Cremilleux, 1998) et sur une technique couramment utilisée en apprentissage (Liu et al., 1997) : ignorer les valeurs manquantes sans ignorer tout l'enregistrement associé. Le principe consiste à utiliser seulement l'information disponible (i.e. les attributs renseignés) et à ignorer les informations manquantes. Ainsi, seules des bases partielles complètes servent à l'extraction de chaque schéma fréquent et l'ensemble de la base est utilisée pour trouver tous les schémas. Nous transposons ici ce principe afin d'extraire des motifs séquentiels, des séquences fréquentes maximales sur des bases de données historisées incomplètes. Pour cela, nous avons adapté les notions utilisées pour l'extraction de séquences fréquentes. Puis, nous avons implémenté l'algorithme SPoID (Sequential Patterns over Incomplete Database) sur la base d'un algorithme d'extraction de motifs séquentiels, PSP (Masseglia et al., 1998). Cet algorithme a été testé sur des jeux de données synthétiques afin de montrer la validité de notre approche.
Dans la suite de cet article, section 2, nous présentons les méthodes qui permettent d'extraire des règles d'association en présence de valeurs manquantes, ainsi que les concepts liés à la découverte de motifs séquentiels. Dans la section 3, nous développons notre approche pour l'extraction de ces motifs à partir de bases de données incomplètes, puis nous présentons notre algorithme dans la section 4. La section 5 est ensuite consacrée aux expérimentations qui montrent la validité de notre approche. Enfin, nous concluons dans la section 6 par les perspectives qu'ouvrent ce travail.
Des données incomplètes aux motifs séquentiels
Les motifs séquentiels sont souvent présentés comme une extension des règles d'association, initialement proposées dans (Agrawal et al., 1993). Ils mettent en évidence des corré-lations entre des enregistrements d'une base de données, ainsi que la relation temporelle qui existe entre eux. Toutefois ces algorithmes ne prennent pas en compte les enregistrements incomplets contenus dans une base de données. Afin de diminuer le prétraitement lié à la pré-sence de valeurs manquantes et d'améliorer la qualité des motifs séquentiels extraits, nous proposons une technique d'extraction de motifs séquentiels dans des bases de données incomplètes, inspirées des méthodes existantes pour les règles d'association. Nous présentons dans cette section les méthodes qui permettent d'extraire des règles d'association en présence de valeurs manquantes ainsi que les notions liées à la découverte de motifs séquentiels.
Règles d'association et valeurs manquantes
Des travaux ont été proposés pour la recherche de règles d'association dans des bases de données incomplètes. Notamment, (Nayak et Cook, 2001;Ng et Lee, 1998), mettent en oeuvre un système d'approximation probabiliste dans lequel une valeur manquante peut prendre plusieurs valeurs lors de la découverte des règles. Ces méthodes sont particulièrement adaptées aux bases de données relationnelles, mais ne sont pas facilement transposables au format spé-cifique des données dont on extrait les motifs séquentiels, détaillé section 3.
Nous avons donc choisi de nous inspirer de l'algorithme RAR (Robust Association Rules), proposé dans (Ragel et Cremilleux, 1998). Cette méthode, complètement compatible avec la méthode originelle (Agrawal et al., 1993), permet la prise en compte des données incomplètes lors de l'extraction de règles dans des bases de données relationnelles incomplètes, par omission partielle et temporaire de ces enregistrements. Le principe consiste à ne prendre en compte que les attributs renseignés pour les enregistrements incomplets. La base de données entière n'est pas utilisée pour chaque règle mais pour générer l'ensemble des règles. Cette technique repose sur la définition de bases de données valides, complètes pour un ensemble d'items données, le reste de la base étant momentanément ignoré. Afin de prendre en compte ce partitionnement de la base, les concepts de support (pourcentage des enregistrements de la base qui contiennent tous les items de la règle) et de confiance (la probabilité qu'un enregistrement qui contient la partie gauche de la règle contienne également la partie droite) ont été redéfinis. Par ailleurs, une nouvelle notion est introduite afin de tenir compte de la taille de l'échantillon complet considéré pour déterminer le support de la règle. Cette mesure de représentativité permet ainsi d'éliminer de la liste des règles celles trouvées sur une base peu significative par rapport à la base initiale.
La recherche de motifs séquentiels consiste à extraire des ensembles d'items couramment associés sur une période de temps bien spécifiée. Elle permet de mettre en évidence des associations inter-enregistrement, par rapport à celle de règles d'association qui extrait des combinaisons intra-enregistrement. L'identification des individus ou objets est alors indispensable afin de pouvoir suivre leur comportement au cours du temps.
Motifs séquentiels
Les motifs séquentiels ont initialement été proposés par (Agrawal et Srikant, 1995) et reposent sur la notion de séquence fréquente maximale.
Considérons une base de données DB d'achats pour un ensemble O d'objets o. Chaque enregistrement R correspond à un triplet (id-objet, id-date, itemset) qui caractérise l'objet auquel est rattaché l'enregistrement, ainsi que la date et les items correspondants. Soit I = {i 1 , i 2 , · · · , i m } l'ensemble des items de la base. Un itemset est un ensemble non vide et non ordonné d'items, noté (i 1 , i 2 , . . . , i k ), où i j est un item. Une séquence s se définit alors comme une liste ordonnée non vide d'itemsets qui sera notée < s 1 s 2 · · · s p >, où s j est un itemset. Une n-séquence est une séquence de taille n, c'est-à-dire composée de n items. 
Les enregistrements de la base sont regroupés par objet et ordonnés chronologiquement, définissant ainsi des séquences de données. Un objet o supporte une séquence S si elle est incluse dans la séquence de données o. Le support (ou fréquence) d'une séquence est alors défini comme le pourcentage d'objets de la base DB qui supporte S. Une séquence est dite fréquente si son support est au moins égal à une valeur minimale minSup spécifiée par l'utilisateur. Une séquence candidate est une séquence potentiellement fréquente.
La recherche de motifs séquentiels dans une base de séquences telle que DB consiste alors à trouver toutes les séquences maximales (non incluses dans d'autres) dont le support est supérieur à minSup. Chacune de ces séquences fréquentes maximales est un motif séquentiel.
Des extensions ont été proposées pour prendre en compte la recherche incrémentale de motifs séquentiels (Masseglia et al., 2000), la gestion de valeurs numériques associées aux items (Hong et al., 2001;Chen et al., 2001;Fiot et al., 2005) ou encore la généralisation des motifs séquentiels pour différents paramètres temporels (espacement des différents évènements d'une séquence, rapprochement d'évènements proches en une même date...) (Srikant et Agrawal, 1996;Masseglia et al., 1999;Fiot et al., 2006). Toutefois, il n'existe pas, à notre connaissance, de techniques permettant de gérer les valeurs manquantes lors de la découverte de motifs sé-quentiels. C'est pourquoi nous proposons ici une approche permettant d'extraire des séquences fréquentes maximales dans une base de séquences incomplètes.
3 SPoID : une nouvelle approche du traitement des données incomplètes
Motivations
Nous souhaitons extraire les motifs séquentiels contenus dans la base de données TAB. 1.
TAB. 1 -Base de données complète.
TAB. 2 -Base de données incomplète.
TAB. 3 -Après suppression des valeurs manquantes.
Avec un support de 50%, les motifs obtenus sont :
Considérons maintenant la même base, mais incomplète, TAB. 2. Pour certaines séquences de données, les informations n'ont pas été transmises et des valeurs sont manquantes. On considère que ces valeurs sont identifiées comme des attributs de valeur indéterminée, contrairement à des attributs inexistants où la valeur sera identifiée comme non-renseignée. Afin de pouvoir extraire les motifs, les méthodes classiques requièrent une élimination des valeurs manquantes. La base sur laquelle s'effectue la fouille de données est alors la base TAB. 3 et les motifs obtenus pour minSup = 50% sont :
On constate que seule une petite partie de la base est utilisée pour extraire l'information et on ne retrouve qu'une partie des schémas fréquents extraits de la base complète : des sousséquences des motifs que l'on devrait extraire. C'est pourquoi il paraît nécessaire d'utiliser l'intégralité de la base lors de la fouille et non pas d'en supprimer une partie.
Sequential Patterns over Incomplete Database (SPoID)
L'élimination des enregistrements incomplets conduisant à une perte d'information, nous avons donc envisagé d'adapter une méthode d'extraction de règles d'association robuste aux valeurs manquantes pour extraire des motifs séquentiels. Nous présentons ici la méthode SPoID (Sequential Patterns over Incomplete Database), inspirée de l'algorithme RAR présenté dans (Ragel et Cremilleux, 1998).
Le principe général de notre méthode, comme de la méthode RAR, repose sur la désacti-vation des éléments incomplets, dans notre cas, des séquences. Alors que pour les règles d'association, l'algorithme RAR ne considère que les enregistrements complets, nous proposons ici de ne prendre en compte que les séquences de données complètes pour la séquence candidate recherchée. Autrement dit, on ne considèrera que les dates et attributs renseignés pour les séquences incomplètes. Ainsi pour chaque séquence on déterminera si elle est fréquente sur une base partielle, mais la totalité de la base sera utilisée pour l'ensemble des séquences fréquentes.
Considérant une séquence candidate S, l'ensemble O des objets de la base peut être divisé en trois sous-ensembles disjoints (FIG. 1 
Nous avons montré, sous certaines conditions peu restrictives, que cette définition respecte la propriété d'antimonotonie du support énoncée dans (Agrawal et Srikant, 1995).
La nouvelle définition du support étant antimonotone, on peut utiliser les différentes propriétés énoncées dans (Agrawal et Srikant, 1995) afin de réaliser l'extraction de motifs séquen-tiels sur base de données incomplètes. Toutefois, la notion de fréquence dépend du calcul du support et de la taille de la base de données valide utilisée pour le calculer. On définit un critère de représentativité minimale : une base de données valide doit être un échantillon significatif de la base de départ pour qu'on considère la séquence comme fréquente si elle valide le support minimal minSup.
Définition 4. La représentativité Rep(S) d'une séquence S est définie comme le ratio du nombre de fois où cette séquence apparaît complète ou n'apparaît pas avec certitude dans la base, par rapport au nombre total de séquences de données dans la base. Elle est donnée par :
Rep ( Pour être considérée comme fréquente, une séquence doit donc être trouvée fréquente sur une base de données valide et représentative, c'est-à-dire si sa représentativité est supérieure au seuil minimal de représentativité minRep et si son support est supérieur au support minimum spécifié par l'utilisateur minSup.
Seuil de représentativité et marge d'erreur
Les statisticiens disposent d'outils d'échantillonnage permettant de considérer un sousensemble d'une population afin d'estimer une proportion, à un pourcentage d'erreur près, avec un niveau de confiance suffisant. Ces outils permettent de déterminer la taille optimale d'un échantillon en fonction de la distribution des données. Ainsi, dans le cas d'une distribution au hasard des données, (Toivonen, 1996) utilise les bornes de Chernoff pour déterminer la taille minimale d'un échantillon tiré au hasard pour l'extraction de règles d'association. Ce résultat est également démontré théoriquement et empiriquement dans (Zaki et al., 1996).
Nous proposons donc d'utiliser deux formes de représentativité selon le souhait de l'utilisateur : soit celle-ci sera définie comme une proportion de la base, soit elle sera absolue et calculée à partir de formules statistiques dépendant de la distribution des données, en respectant un pourcentage d'erreur et un niveau de confiance spécifiés par l'utilisateur. Les expérimenta-tions montrent toutefois que le seuil de représentativité optimale n'est pas absolu mais dépend en fait du taux de valeurs manquantes contenues dans la base de données.
Mise en oeuvre 4.1 Illustration
Reprenons la base incomplète présentée TAB. 2. On pose minSup=50%, puis on calcule le support et la représentativité de chacun des items de la base pour déterminer les items fré-quents. L'item a est supporté de manière sûre par les trois objets son support est supp(a) = 3/3 = 100% et sa représentativité vaut 1. Il en est de même pour les items b et c. Même si ces motifs ne sont pas exactement ceux extraits sur la base complète, on constate qu'ils sont plus proches des motifs qui devraient être extraits des motifs obtenus de la base après prétraitement. Les expérimentations présentées dans la section 5 montrent qu'il existe une valeur de la repré-sentativité à partir de laquelle l'algorithme SPoID extrait d'une base incomplète l'intégralité des motifs extraits sur la base complète.
Algorithme
Le principe de l'algorithme SPoID est similaire aux algorithmes d'extraction de motifs séquentiels de type générer-élaguer. Il consiste à générer toutes les séquences candidates de longueur k à partir des séquences fréquentes de longueur k-1, puis à scanner l'ensemble de la base pour compter le nombre de séquence de données qui supporte chacune des séquences candidates. La différence réside dans le dénombrement des séquences de données incomplètes pour la séquence candidate considérée. La phase de comptage est décrite par l'algorithme ALG. 1 : pour chaque séquence candidate, pour chaque objet, -si on trouve la séquence candidate, on incrémente les valeurs absolues du support, -si on ne trouve pas la séquence candidate ni de séquence dans laquelle des valeurs manquantes pourraient être remplacées par les items correspondant dans la séquence candidate, alors l'objet ne supporte pas la séquence candidate. On n'incrémente pas le support. -on trouve une séquence incomplète dans laquelle des valeurs manquantes pourraient être remplacées par les items correspondant dans la séquence candidate. Dans ce cas, on ajoute cet objet à l'ensemble des objets désactivés. Une fois l'ensemble de la base parcouru, on divise la valeur absolue du support par la différence entre le nombre total d'objets et le nombre d'objets désactivés, on calcule la représentativité. Puis on procède à la phase d'élagage en supprimant toutes les séquences candidates qui ne sont pas fréquentes puis celles qui ne sont pas représentatives.
SPoID -Input : |O|, une base de séquences de données, minSup, support minimum spécifié par l'utilisateur minRep, représentativité minimum, spécifiée ou calculée Ouput : SP List, liste des séquences fréquente La complexité temporelle de cet algorithme est, dans le pire des cas, la même que celle de l'algorithme TOTALLYFUZZY présenté dans (Fiot et al., 2005). On utilise le même type d'optimisations afin de réduire le nombre de passes sur la base. Par contre, la complexité en mémoire est nettement moindre puisqu'elle est du même ordre que celle de PSP.
Expérimentations
Ces expérimentations ont été réalisées sur un PC équipé d'un processeur 2,8GHz et de 512Mo de mémoire DDR, sous système Linux, noyau 2.6. Nous utilisons un jeu de données synthétiques générés aléatoirement par une loi normale dans lequel nous remplaçons certains items par des valeurs manquantes, réparties de maniére aléatoire. On extrait les motifs séquen-tiels sur la base complète ainsi que sur la base prétraitée (dont les itemsets incomplets ont été supprimés). Puis on compare ces motifs extraits par les méthodes existantes aux motifs extraits par notre algorithme SPoID. Les résultats présentés ici ont été obtenus à partir du traitement de plusieurs jeux de données synthétiques comportant environ 2000 séquences de 20 transactions en moyenne. Chacune de ces transactions comporte en moyenne 10 items choisis parmi 100.
Nos analyses sont basées sur le calcul du nombre de bons motifs séquentiels trouvés par SPoID et du nombre de motifs différents extraits par SPoID, ces derniers regroupant les motifs extraits, qui n'existe pas dans la base complète et les motifs non trouvés, mais contenus dans la base complète. Le tableau 4 récapitule l'ensemble de ces notations.
? Nombre de motifs extraits par SPoID, contenus dans la base complète ? Nombre de motifs différents ? Nombre de motifs extraits par SPoID sur la base incomplète ? Nombre de motifs extraits sur la base de données complète TAB. 4 -Notations pour les différentes catégories de motifs séquentiels extraits.
La FIG. 2(a)
, tout d'abord, montre l'évolution du rapport ?/?, en fonction de la représenta-tivité minimale. On constate que ce taux croît à mesure que minRep augmente, ce qui signifie que parmi les motifs extraits par SPoID, la proportion des motifs également trouvés dans la base complète augmente avec la représentativité minimale.
On peut compléter cette observation par l'analyse de la FIG. 2(b), qui présente l'évolution du rapport ?/? (nombre de motifs extraits par rapport aux motifs à extraire) en fonction de la représentativité minimale. On constate que ce ratio diminue à mesure que minRep augmente, ce qui signifie qu'il est nécessaire de choisir une représentativité suffisamment faible pour permettre l'extraction de l'intégralité des motifs présents dans la base complète.
Nous avons donc mis en évidence l'existence d'une valeur optimale du seuil de représen-tativité, pour laquelle les ratios ?/? (bons motifs/motifs extraits par SPoID) et ?/? sont les plus proches possibles de 1. Cette valeur correspond au seuil pour lequel le nombre de bons motifs extraits sur la base incomplète est le plus élevé possible par rapport au nombre de motifs diffé-rents. La FIG. 2(c) met en évidence l'existence de cette valeur optimale de minRep. Ce graphe montre l'évolution du rapport (?/?), rapport du nombre de bons motifs extraits par SPoID et du nombre de motifs qui diffèrent des motifs extraits sur la base complète (manquants + supplémentaires). On constate qu'il n'existe pas de valeur absolue de la repésentativité minimale, commune à toutes les bases et dépendant d'une marge d'erreur donnée. D'après ces résultats, la représentativité minimale dépend uniquement du taux d'incomplétude de la base.
Quelle que soit la proportion de valeurs manquantes dans la base incomplète, l'allure géné-rale de la courbe est la même : le ratio (?/?) augmente avant d'atteindre un maximum puis de décroître. Ce point maximal correspond à la représentativité optimale, pour laquelle le nombre de bons motifs extraits par SPoID est le plus élevé et le nombre de motifs différents le plus faible. Le tableau TAB. 5 donne la valeur de la représentativité optimale trouvée expérimenta-lement pour chaque proportion de données incomplètes dans notre jeu de test. La FIG. 2(c) met aussi en évidence l'évolution de comportement de l'algorithme SPoID selon le taux de valeurs manquantes dans la base. On constate une différence entre l'allure de la courbe et la valeur du ratio ?/? pour les bases incomplètes contenant 40% de valeurs man-  
Perspectives
La découverte de motifs séquentiels est une méthode de fouille de données intéressante lorsqu'il s'agit d'extraire des connaissances dans une base de données historisée, telle que des relevés de processus industriel ou de fonctionnement de machines. Or, dans ce type de bases de séquences, la présence de valeurs manquantes est inévitable. Pourtant il n'existe aucune technique permettant de découvrir des séquences fréquentes à partir de bases de données incomplètes. Nous avons donc proposé dans cet article une adaptation des définitions originales liées à l'extraction de motifs séquentiels afin de pouvoir traiter les informations incomplètes distribuées au hasard, directement pendant la fouille plutôt que de supprimer ces enregistrements, comme cela était le cas avec les algorithmes existants. Notre méthode, SPoID (Sequential Patterns over Incomplete Database) a été implémentée et testée sur des jeux de données synthétiques, ce qui nous a permis de montrer sa robustesse aux valeurs manquantes jusqu'à un taux d'incomplétude d'environ 40%, alors que les méthodes classiques, après prétraitement, donnent de mauvais résultats dès 10% de valeurs manquantes.
Nous envisageons maintenant d'étendre cette méthode afin de pouvoir prendre en compte d'autres types de valeurs manquantes (non distribuées au hasard, par exemple), après avoir détecté les différents types d'informations incomplètes contenues dans la base. Il apparaît éga-lement nécessaire de pouvoir distinguer, quand cela ne l'a pas été fait au préalable, les valeurs d'attribut inexistant, qui n'ont donc pas à être considérées comme manquantes. Enfin, le bruit est également une imperfection courante dans les bases de données du monde réel. Il pourra donc être intéressant de le prendre en compte dans une version ultérieure de notre algorithme.

La plateforme SyRQuS
Même si cela fait plusieurs années que RDF est devenu un standard recommandé par W3C, le développement des langages de requête RDF a été plus long. Après l'apparition de RDF, des langages permettant d'accéder aux triplets RDF ont émergé, comme TRIPLE (Sintek et al., 2002) ou encore Squish (SquishQL, 2002). De ces premiers sont inspirés d'autre langages comme RQL, RDQL -langage d'origine de la plateforme Jena (Jen) -ou encore SeRQL langage de base de Sesame (Kampman et Broekstra). Tous ces efforts convergent aujourd'hui vers un langage SQL-like qui est en train de devenir la future recommandation W3C : SPARQL (Seaborne et Prud'hommeaux, 2006 
Conclusion et perspectives
SyRQuS est une application destinée à la recherche d'informations dans les documents RDF. Son implémentation permet d'obtenir des réponses plus complète par la combinaison de plusieurs graphes RDF fournissant des réponses partielles. Développée dans un souci de compatibilité avec le langage de requête SPARQL, elle permet le traitement des requêtes de type SELECT, ce type de requête étant le seul à pouvoir profiter de notre algorithme de recherche par combinaison de graphes.
Actuellement, nous travaillons sur l'intégration des ontologies référencées dans les documents RDF, ce qui nous permettra de procéder à des aproximations de réponses au moment de l'intérrogation de la base de documents. 
Références
Summary
Nowadays, RDF is a W3C recommended standard for describing knowledge about resources over the Web. In this context, we explore the RDF query answering field and take a closer look at the possibility of combining several RDF graphs while searching for query answers. We propose a prototype tool that generates virtual complete answers by combining partially answering RDF graphs. The generation of such answers is performed under a noncontradiction condition calculated through a similarity measure between the RDF graphes to be combined.

Introduction
Il est envisagé de déployer en France dans les prochaines années des compteurs communicants chez un grand nombre de clients des compagnies d'électricité, à l'image de ce qui a été déjà réalisé en Italie par la société ENEL. Ces nouveaux compteurs, reliés aux Systèmes d'Information (SI) des fournisseurs d'électricité, permettront de relever à distance les consommations d'électricité afin d'effectuer des opérations tels que la facturation, l'agréga-tion, le contrôle,... L'approche standard pour le traitement de ce type de données dans les SI est d'utiliser des Systèmes de Gestion de Bases de Données (SGBD) qui assurent leur stockage et permettent de les consulter par des requêtes. Cette approche trouve ses limites lorsque les flux de données sont importants (en débit et/ou en nombre). C'est pour cela que de nouvelles approches ont été développées dans le domaine des bases de données pour traiter de façon performante des flux de données, sans mémoriser l'ensemble des informations : il s'agit des Systèmes de Gestion de Flux de Données -SGFD -ou encore Data Stream Management Systems -DSMS -en anglais. Plusieurs prototypes de SGFD ont été développés ces dernières années pour répondre à ces besoins. Le rôle de ces systèmes est de traiter en temps réel un ou plusieurs flux de données par des requêtes dites continues. Le travail présenté ici décrit l'expérimentation de deux prototypes de SGFD du domaine public (STREAM (Arasu et al., 2004) et TelegraphCQ (Chandrasekaran et al., 2003)) pour la réali-sation de fonctions de traitement de données de consommation électrique, disponibles sous forme de flux à partir de compteurs communicants. La communication est organisée comme suit : la section 2 présente en général les SGFD et en particulier les systèmes STREAM et TelegraphCQ. La section 3 présente notre expérimenta-tion, la synthèse de celle-ci est exposée à la section 4. Enfin, nous concluons à la section 5 et donnons nos perspectives de recherche.
Systèmes de Gestion de Flux de Données
Les Systèmes de Gestion de Bases de Données (SGBD) sont des logiciels permettant de définir des structures de données, de stocker des données dans ces structures, de modifier ces données, et de les consulter à l'aide de requêtes. Les SGBD les plus utilisés sont ceux dits relationnels, où les données sont structurées sous forme de tables (aussi appelées relations). Le langage standard pour spécifier les requêtes aux bases de données relationnelles est le langage SQL. Les Systèmes de Gestion de Flux de Données (SGFD) préfigurent des prochaines générations de SGBD. En effet, devant la volumétrie sans cesse plus importante des données produites par les systèmes de mesure et les systèmes informatiques, certaines applications né-cessitent de lever l'hypothèse que toutes les données produites sont stockées dans la base de données avant d'être consultées par des requêtes. Pour les SGFD, la structure de données est celle d'une collection de flux de données entrants, qui sont traités au fil de l'eau pour ré-pondre aux requêtes des utilisateurs, elles-mêmes définies pour produire des flux sortants. A titre d'exemple, si les flux entrants de données correspondent à des mesures de consommation d'électricité à différents points du réseau électrique, une requête utilisateur peut consister en la restitution des consommations moyennes et maximales pour chaque département dans les trois dernières heures. Les travaux de recherche menés aux Etats-Unis sur le thème des flux de données se divisent principalement en deux grandes directions (voir (Babcock et al., 2002) et (Golab et al., 2003)) : (1) la conception de systèmes de gestion de flux de données, (2) la conception d'algorithmes de fouille de données (data mining) pouvant fonctionner sur des flux de données, c'est-à-dire sans mémoriser l'ensemble des informations apparaissant dans les flux. Nous nous intéressons ici uniquement à la gestion des flux de données. Les SGFD permettent de formuler des requêtes dites « continues » qui s'évaluent au fur et à mesure que les différents flux sur lesquels elles portent sont alimentés. Afin que ces requêtes aient un sens et ne nécessitent pas le stockage de l'ensemble du flux, des techniques de fenê-trage sont utilisées : une fenêtre définit un intervalle temporel exprimé soit en terme de durée (par exemple les 5 dernières minutes), ou soit sous forme logique exprimé en nombre de tuples (par exemple les 20 derniers éléments). Ces fenêtres peuvent être délimitées par des bornes fixes ou glissantes dans le temps. La figure 1 présente une architecture abstraite de SGFD, extraite de (Golab et al., 2003). Un moniteur d'entrée sert à réguler le débit d'arrivée des données et peut être amené à supprimer quelques tuples en cas de surcharge du système. Les données des flux sont stockées temporairement dans des buffers (working storage) pour permettre le traitement des requêtes fenêtrées. Des résumés des flux sont éventuellement constitués (summary storage) lorsque la capacité de stockage ne permet pas de gérer tous les flux entrants : le résultat des requêtes est alors ap-proché. Un stockage statique (static storage) assure la mémorisation des meta-données (structure et provenance des flux) ainsi que des données présentes de façon permanente dans la base. Lorsque les requêtes des utilisateurs sont définies, celles-ci sont stockées dans le système (query repository) et traitées par le processeur de requêtes (query processor). Ce dernier a la charge de définir un plan optimisé d'exécution de l'ensemble de requêtes (en mutualisant les ressources entre plusieurs requêtes) et de produire les résultats des requêtes soit directement sous forme de flux, soit sous forme de données stockées temporairement (output buffer) et remises à jour périodiquement. Il faut noter que les plans d'exécution des requêtes ne doivent pas être figés car ils peuvent être remis en question soit par l'ajout de nouvelles requêtes, soit par des variations dans le contenu et le débit des flux.
FIG. 1 -Architecture globale d'un SGFD
Si les applications qui ont motivé le développement des SGFD portaient sur la supervision des réseaux informatiques, de nombreuses autres applications se sont développées : le traitement des logs de sites web, des tickets de communications fixes ou mobiles, des données de capteurs (trafic routier, météorologie par exemple), mais aussi de données boursières. Plusieurs systèmes prototypes ont été développés, certains avec une vocation généraliste (tels que STREAM (Arasu et al., 2004), TelegraphCQ (Chandrasekaran et al., 2003), Borealis (Abadi et al., 2005), Aurora/Medusa (Zdonik et al., 2003)), alors que d'autres sont destinés à un type particulier d'application (tels que Gigascope (Cranor et al., 2003), Hancock (Cortes et al., 2000), NiagaraCQ (Chen et al., 2000)).
STREAM
STREAM (pour STanford stREam datA Management) est un système prototype de gestion de flux de données généraliste développé à l'Université de Stanford (Arasu et al., 2004). Ce système permet d'appliquer un grand nombre de requêtes déclaratives et continues à des données statiques (tables) et dynamiques (flux de données). Un langage déclaratif CQL (Continuous Query Language) (Arasu et al., 2003), dérivé de SQL, a été implémenté pour pouvoir traiter des requêtes continues. STREAM considère deux structures de données : -Flux : Un flux S est un ensemble d'éléments (s,t), où s est un tuple appartenant au flux, et t ?T est l'estampille temporelle (timestamp) du tuple, t croissant de façon monotone. -Relation : Une relation R(t) est un ensemble de tuples qui dépend du temps. A chaque instant une relation est susceptible d'avoir un nouveau contenu. Le language CQL spécifie des opérations de sélection, d'agrégation, de jointure, de fenêtrage sur les flux, ainsi que trois opérateurs transformant des relations en flux :
-Istream(R(t)) : envoie sous forme de flux les nouveaux tuples apparus dans R à la période t ; -Dstream(R(t)) : envoie sous forme de flux les tuples supprimés de R à la période t ; -Rstream(R(t)) : envoie sous forme de flux tous les tuples appartenant à R à la période t. CQL supporte des fenêtres glissantes sur un flux S, qu'on peut décliner en 3 groupes : (1)  
TelegraphCQ
TelegraphCQ est un Système de Gestion de Flux de Données réalisé à l'Université de Berkeley (Chandrasekaran et al., 2003). Ce système est construit comme une extension du SGBD relationnel PostGreSQL pour supporter des requêtes continues sur des flux de données. Le format d'un flux de données est défini comme toute table PostGreSQL dans le langage de dé-finition de données de PostGreSQL, et est créé à l'aide de la clause CREATE STREAM, avant d'être utilisé dans des requêtes continues. A chaque source de flux est affecté un traducteur (wrapper) qui transforme les données en un format compatible avec les types de données PostGreSQL. TelegraphCQ est un mode d'exécution de PostGreSQL, l'utilisateur pouvant aussi lancer un serveur PostGreSQL en mode normal. Les requêtes continues peuvent inclure une clause de fenêtrage sur les flux. Une fenêtre est définie sur un intervalle de temps, le timestamp de chaque n-uplet du flux étant assigné par le wrapper si celui-ci ne possède pas un attribut déclaré comme timestamp. La sémantique de fenêtrage a évolué au cours des différentes versions de TelegraphCQ. La version la plus ré-cente définit la clause de fenêtrage par [RANGE BY 'interval' SLIDE BY 'interval' START AT 'date et heure de début'] :
-RANGE BY : définit la taille de la fenêtre en terme d'intervalle de temps ; -SLIDE BY : définit l'intervalle après lequel la fenêtre est recalculée, spécifié en intervalle de temps ; -START AT : définit l'instant (date et heure) auquel la première fenêtre commence, dans un format de date standard. Les flux interrogés par les requêtes peuvent être archivés dans la base de données PostgreSQL à la demande de l'utilisateur (clause ARCHIVED). Dans le cas contraire, ils sont supprimés du système lorsque les requêtes n'ont plus à les conserver dans des espaces temporaires pour leur exécution.
Expérimentation du traitement de flux de consommations électriques
Nous avons installé les deux systèmes STREAM et TelegraphCQ version 2.1 et les avons testés sur un jeu de données composé d'index de consommations relevés à pas de 2 secondes pendant 150 jours. Ces relevés correspondent à trois compteurs électriques se trouvant dans des villes différentes.
Spécification des traitements
L'analyse des consommations électriques peut être utile pour le fournisseur d'électricité comme pour le client. Ce dernier pourra s'en servir, par exemple, pour détecter les causes d'une consommation excessive ou anormale.
Un compteur « communicant » est capable de transmettre toutes les 2 secondes un enregistrement (tuple) de données contenant le numéro de compteur, l'heure du relevé, la date, l'index de consommation, et d'autres informations annexes. La différence entre deux index à deux instants différents représente la consommation d'électricité en energie entre ces deux instants. Les tuples provenant de plusieurs compteurs forment un flux de données de consommation.
Pour notre expérimentation, le jeu de données que nous avons utilisé est composé de tuples dont la structure (simplifiée) est la suivante : Flux_index est le nom identifiant le flux de données. La date d'un relevé est spécifiée à l'aide de trois attributs : 'mois', 'jour', 'année' ; l'heure à l'aide de trois autres attributs : 'h', 'm', 'sec' (heure, minute, seconde). L'attribut 'compteur' désigne l'identifiant du compteur et 'index' désigne le relevé de l'index du compteur (en kWh).
L'analyse des consommations s'appuie sur des requêtes continues sur le flux. Parmi les requêtes possibles nous avons choisi trois requêtes qui nous semblent assez représentatives de ce type d'application.
1. Consommation des 5 dernières minutes, minute par minute, par compteur, ou par ville ; 2. Consommation historique minute par minute, par compteur, à partir d'une date de dé-part ; 3. Alarme de dépassement de consommation heure par heure par rapport à une consommation dite « normale » dépendant de la température.
Nous avons essayé d'implanter ces requêtes sur les deux SGFD TelegraphCQ et STREAM. Les sections suivantes, 3.2 et 3.3 présentent le résultat de ce travail. Elles précisent pour chaque SGFD les requêtes qui ont pu être implantées, et expliquent comment cette implantation a pu se faire.
Implantation sur Stream
Une des spécificités de STREAM est qu'il ne supporte pas les expressions de requêtes imbriquées. Cependant, il offre la possibilité d'associer un nom à une expression de requête (comme une vue) et de s'en servir dans l'expression d'une requête plus complexe.
Q1 -Consommation des cinq dernières minutes, minute par minute, par compteur ou par ville. Le flux est une suite infinie de tuples qui associent des relevés d'index à des dates de prélèvements (année, mois, jour, heure, minute, seconde). Dans ce cas, le calcul de la consomation sur une durée donnée (dans notre exemple une minute) peut se faire en calculant la différence entre les valeurs minimales de l'index observées pendant deux périodes (minutes) successives. L'implantation de la requête Q1 peut donc se faire sur STREAM en suivant les étapes suivantes : Pour afficher les consommations par ville, nous transformons la dernière requête en flux que nous appelons Elec.fluxconso. Les flux Elec.minflux1 et Elec.minflux2 sont calculés sur des fenêtres de 1 minute. Ensuite, nous réalisons une jointure entre la table Tableville et Elec.fluxconso fenêtré à 5 minutes.
Q2 -Consommation historique minute par minute, par compteur, à partir d'un point de départ. La réalisation de cette requête est facilitée par la clause START AT permettant de filtrer les tuples à partir d'un point de départ fixé. Q3 -Alarme de dépassement de consommation heure par heure de minuit à l'heure courante. La consommation normale est fournie dans une table consoNormale, pour chaque heure et pour une température de 20°C. Nous disposons d'un flux Elec.temperature qui donne la température relevée chaque heure pendant 150 jours. Ce flux est affecté à un deuxième wrapper csvwrapper2. Pour s'approcher des conditions réelles, nous construisons un flux de consommation normale Elec.fluxconsonormale qui dépend de la température. L'implantation de la requête Q3 a été faite en suivant les étapes suivantes :
1. Nous calculons les consommations cumulées de 24 heures heure par heure, et nous sé-lectionnons celles au-delà de 12h. Le flux Elec.fluxconsoParHeure a été construit en suivant la même démarche que pour la consommation minute par minute en prenant une fenêtre de 1 heure. wtime(*) donne le timestamp du dernier tuple de la fenêtre en cours. 
Synthèse des expérimentations Adéquation des langages
Au vue des implantations des requêtes à la section 3, nous remarquons que la logique de résolution des requêtes par SGFD est assez éloignée de la logique SQL. Dans le cas des bases de données, les requêtes manipulent des données persistentes qui sont sélectionnées selon leurs valeurs dans le contenu d'une ou de plusieurs relations. Dans le cas des flux, les données sont volatiles et le contenu est non borné ce qui rend plus complexe la sélection des données et leur traitement. Il faut (1) spécifier par le mécanisme des fenêtres temporelles un réservoir de tuples à constituer et sur lequel s'appliquera la requête, et (2)  Performances L'équipe STREAM s'est intéressée à améliorer les performances de son système. Pour une bonne utilisation des ressources, STREAM propose des mécanismes permettant un partage des requêtes entre les flux et un partage des flux entre les requêtes. Les développeurs ont implanté une interface graphique pour visualiser les plans d'exécution des requêtes ainsi que le taux d'utilisation du système grâce à des jauges disponibles sur les différents composants du plan de la requête. L'utilisateur peut surveiller l'état du système et ré-optimiser l'exécution des requêtes dynamiquement en manipulant les tailles des buffers pour mieux s'adapter aux conditions du système ou des flux. Cependant, STREAM tel qu'il est offert au public demeure un prototype utile pour effectuer des démonstrations sur les possibilités des SGFD, mais n'est pas adapté à un besoin réel tel que le nôtre pour y répondre de manière complète et robuste. En effet, nous avons été confrontés à des arrêts intempestifs du serveur à plusieurs reprises lors de l'exécution des requêtes. TelegraphCQ est un système robuste, en terme de charge et de complexité des requêtes acceptées. Il est possible d'ajouter des requêtes même si d'autres requêtes sont en cours d'exé-cution. Nous avons implanté toutes les requêtes présentées dans cet article sans le moindre souci. Néanmoins, nous ne nous sommes intéressés dans le cadre de cette étude qu'à l'as-pect fonctionnel de ces deux SGFD. Nous envisageons dans nos prochaines expérimentations d'augmenter le nombre de compteurs électriques de notre jeu de données pour tester la charge de TelegraphCQ.
Divers
Les données d'un flux de consommation électrique arrivent constamment et une requête continue ne connait pas à priori l'extrémité d'un flux (contrairement à une table dans les SGBD). Les fenêtres nécessaires à des opérations sur les flux telle que l'agrégation sont implémentées dans les deux SGFD testés. Toutefois, il était impossible d'utiliser la fonction min avec un opérateur de fenêtrage lors de l'exécution de la requête Q1 dans STREAM, pour une raison inconnue. Ceci nous aurait permis d'améliorer les performances en ne gardant que les données dont nous avons besoin pendant une période donnée. TelegraphCQ possède une syntaxe claire, une simplicité d'utilisation et de formulation des requêtes bien meilleure que STREAM. Ceci est dû aux fonctionnalités fournies par PostgreSQL telle que la gestion des formats de date et les méthodes associées. De plus, TelegraphCQ offre la possibilité de réutiliser les résultats des requêtes sous forme de flux, ou de les stocker dans un fichier résultat en spécifiant le format de sortie. Quant à STREAM, il se distingue par son interface graphique permettant de se connecter au serveur, de lui envoyer des requêtes et de visualiser le plan des requêtes. Par exemple, il a été possible de voir dans STREAM que les opérations d'agrégation (sans fenêtrage) de la requête Q2 s'effectuent de façon incrémentale sans garder en mémoire tout le flux pour recalculer le résultat de l'agrégation. Une autre caractéristique de STREAM est de pouvoir spécifier des fenêtres logiques exprimées en nombre de tuples, ces fenêtres peuvent être utiles dans certaines applications.
Conclusion et perspectives
Nous avons montré que deux Systèmes de Gestion de Flux de Données (SGFD) permettent de traiter des données de consommation électrique arrivant en rythme continu et rapide. D'après nos expérimentations, il apparaît que TelegraphCQ est mieux adapté à nos besoins que STREAM. En effet, il offre une simplicité d'utilisation, une syntaxe claire et des fonctionnalités intéressantes pour analyser les résultats. De plus, l'équipe de Berkeley travaillant sur ce projet est toujours active dans ce domaine alors que l'équipe STREAM a suspendu ses travaux depuis janvier 2006. Cependant, la comparaison entre ces deux SGFD est principalement fonctionnelle et n'a pas pris en compte les performances en temps d'exécution et en consommation de ressources système. Ce sera un objectif de nos prochaines expérimentations. Une des perspectives des nombreuses équipes qui ont implémenté des SGFD est de proposer une architecture distribuée de leur système. Une première proposition est faite par l'équipe de Aurora qui propose le système Boréalis (Abadi et al., 2005). Boréalis distribue le traitement des flux de données sur plusieurs sites d'Aurora pour des raisons de répartition de charge et de tolérance aux pannes. Nous testerons dans nos prochains travaux ce système qui a le mérite d'être en libre téléchargement. Nous surveillerons aussi les éventuelles versions futures de TelegraphCQ.

Summary
The goal of this work consists in designing and producing a Software Tool, by using the concepts of the Web Usage Mining, to offer to the Webmasters complete knowledge in order to make appropriate decisions. It works as follows: it extracts information from log files and it makes decisions to discover the behaviours of the users. It adapts to the users behaviour by adapting the contents and general aspects of the Web pages.

Introduction
L'analyse syntaxique et terminologique de textes organisés en corpus est envisagée depuis une quinzaine d'années comme une issue possible au problème de la construction d'ontologies ou de terminologies. Non seulement la référence à l'utilisation de la langue est le moyen d'assurer une certaine pertinence aux connaissances ainsi identifiées, mais surtout, la disponibilité de textes sous format électronique autorise le recours à leur analyse par des logiciels de Traitement Automatique des Langues (TAL). Enfin, dans le cas particulier où l'ontologie à construire doit servir à indexer ou annoter des documents par des méta-données, le recours aux textes fournit une terminologie riche et au plus près de celle utilisée dans ces documents.
Plusieurs méthodes ont été définies pour guider ce processus et le rendre systématique, comme Archonte (Bachimont, 2004) ou Terminae . Toutefois, ces méthodes se heurtent à la lourdeur du dépouillement des résultats des logiciels d'analyse syntaxique, lexicale ou sémantique des textes auxquels elles ont recours. De plus, toutes s'accordent à souligner le rôle majeur de l'analyste (que nous appelerons aussi par la suite ontologue) dans l'interprétation de ces résultats. Ainsi, lorsqu'un outil d'aide au repérage de relations selon une approche par patron présente des phrases contenant des termes en relation, il n'est pas sûr que cette relation doive être intégrée dans l'ontologie, ou encore qu'elle mette en relation exactement les concepts auxquels renvoient ces termes (Séguéla, 2001).
Aussi, depuis quelques années, plusieurs tentatives sont effectuées pour réduire la charge de l'analyste. Elles profitent des avancées de l'apprentissage automatique ou de méthodes de clas-sification pour exploiter plus automatiquement les données linguistiques extraites des textes. On parle d'Ontology Learning (Maedche, 2002). À titre d'exemple, l'approche préconisée par Maedche et Staab (2000) utilise des heuristiques et un algorithme de fouille de données appliqués aux textes pour « découvrir » des règles d'association entre termes, et définir ainsi à la fois des concepts et des relations entre concepts.
Notre système, Dynamo 1 , vise ce même objectif : réduire la part manuelle dans le dé-pouillement des résultats d'analyse de textes, et suggérer une amorce de réseau conceptuel en vue de construire une ontologie plus efficacement. L'approche retenue est tout à fait originale, et fait appel à un système multi-agent adaptatif. Ces systèmes sont capable de répondre aux changements induits par l'utilisateur en modifiant leur structure. Ce choix est motivé par les qualités qu'offrent les systèmes multi-agents adaptatifs : ils peuvent faciliter la mise au point interactive d'un système (Georgé et al., 2003) (dans notre cas, un réseau conceptuel), permettent sa construction incrémentale automatique par la prise en compte progressive de nouvelles données (issues d'analyses de textes) et enfin, ils peuvent être aisément implémentés de manière répartie. Ainsi le comportement de Dynamo a trois étapes : le système propose une taxonomie, l'utilisateur fait des modifications locales et le système se réorganise pour faire une nouvelle proposition. Contrairement à ASIUM (Faure, 2000), l'utilisateur invervient sur le résultat entier et non sur le contrôle de chaque étape du processus de classification.
Dynamo prend en entrée les résultats d'une analyse syntaxique et terminologique de textes. Il s'appuie sur un algorithme de classification hiérarchique distribué, basé sur un calcul de similarité entre termes qui exploite leurs contextes linguistiques. En sortie, Dynamo présente à l'analyste une organisation hiérarchique de concepts qu'il peut valider, affiner ou corriger, jusqu'à parvenir à un état satisfaisant du réseau sémantique.
Cet article présente l'architecture et l'approche retenue pour le système Dynamo, et particulièrement l'algorithme de classification hiérarchique distribuée implémenté dans les agents (section 2.2). Après une étude quantitative et qualitative des performances de cet algorithme (section 3), nous soulignons l'apport d'une approche multi-agent pour la classification. Enfin, nous discutons de son utilisation dans la perspective de construire des ontologies (section 4).
Présentation du système Dynamo
Architecture proposée
Dans cette section, nous présentons l'architecture de Dynamo. Elle se situe dans la lignée d'une réflexion portée dans un contexte plus global de gestion et de maintenance d'ontologies dynamiques en lien avec des collections de documents (Ottens et al., 2004). Contrairement à la réflexion précédente qui cherchait, entre autre, à aborder les besoins du Traitement Automatique des Langues (TAL), ici, nous nous attachons uniquement à combler les besoins de l'Ingénierie des Connaissances (IC).
Le système Dynamo se compose de trois grandes parties (cf. figure 1  (Bourigault et Aussenac-Gilles, 2003). Nous l'avons sélectionné principalement pour sa robustesse et la grande quantité d'informations extraites. En particulier, le réseau « Tête-Expansion », créé par cet outil, a déjà prouvé être une structure intéressante pour un système de classification (Assadi, 1998). Dans un tel réseau, chaque terme est relié, d'une part à sa tête 2 et son expansion 3 , et d'autre part à tous les termes dont il est lui-même tête ou expansion. Par exemple, « algorithme centralisé de classification » a comme tête « algorithme centralisé » et comme expansion « classification ». De même, « algorithme centralisé » est composé de « algorithme » et « centralisé ».
Le réseau de termes récupéré en sortie de l'extracteur est stocké dans une base de données. Pour chaque paire de termes, on suppose qu'il est possible de calculer son indice de similitude (ou similarité), en vue d'effectuer une classification (Faure, 2000)  (Assadi, 1998). De par la nature des données, nous nous intéressons uniquement à des indices de similarité entre objets décrits par des variables binaires, c'est-à-dire qu'un individu est décrit par la présence ou l'absence d'un ensemble de caractéristiques (Saporta, 1990). Dans le cas de termes, il s'agit, en général, des contextes d'utilisation. Avec Syntex, ces contextes sont identifiés par des termes et des relations syntaxiques caractérisées.
Le système multi-agent implémente l'algorithme de classification distribué décrit en détail dans la section 2.2. Il est conçu pour être à la fois le système produisant la structure résultante et la structure elle-même. En ce sens, chaque agent est une entité informatique représentant un concept, dont le comportement autonome lui permet de trouver sa place dans l'organisation, à savoir l'ontologie. Ils disposent tous de capacités de communication et d'algorithmes leur permettant de s'approcher ou s'éloigner selon différents critères. La sortie du système est donc l'organisation obtenue par l'interaction des agents, tout en tenant compte des pressions exté-rieures exercées par l'ontologue lorsqu'il effectue des modifications de la taxonomie selon ses besoins. Dans cet article, nous nous intéressons principalement au processus de classification distribué dans les agents, bien que la création d'une ontologie ne se réduise pas à cet aspect.
Un algorithme de classification distribué
Cette section présente l'algorithme de classification distribué, utilisé dans le système. Afin d'en améliorer la compréhension, et en vue de son évaluation dans la section 3, nous rappelons ci-dessous l'algorithme de base utilisé pour une classification hiérarchique ascendante dans un espace non métrique, mais dont la mesure de similitude utilisée est symétrique (Saporta, 1990) (ce qui est le cas avec les mesures utilisées dans notre système).
Algorithme 1 : Algorithme centralisé de classification hiérarchique ascendante
Entrées : La liste L des individus à hiérarchiser
Dans l'algorithme 1, à chaque étape de la classification, la paire des éléments les plus similaires est déterminée. Ces deux éléments sont regroupés, et la classe résultante est insérée dans la liste des éléments restants. L'algorithme s'arrête quand cette liste ne contient plus qu'un élément.
La structure hiérarchique résultante de l'algorithme 1 est nécessairement un arbre binaire de par le regroupement deux à deux. Le regroupement des éléments les plus similaires revient aussi à les éloigner des éléments dont ils sont le plus dissimilaires. L'algorithme distribué multi-agent présenté est conçu autour de ces deux constats. Cet algorithme est exécuté de manière concurrente par chacun des agents du système.
Notons aussi que dans la suite de l'article nous utilisons lors de l'évaluation des deux algorithmes un stratégie d'agrégation en moyenne (Saporta, 1990) et l'indice de similarité utilisé sur des termes dans Assadi (1998) ayant la formule suivante pour deux termes i et j :
Où, a, b, c et d sont respectivement le nombre de contextes partagés par i et j, présents uniquement chez i, présents uniquement chez j, et présents ni chez i ni chez j. Les contextes sont déterminés en explorant le réseau « Tête-Expansion » (Assadi, 1998). Pour nos tests, nous avons fixé ? à 0, 75. Tous ces choix conditionnent l'arbre résultat, mais n'influencent ni le déroulement général de l'algorithme ni sa complexité.
Le système est initialisé de la manière suivante : -un agent TOP n'ayant aucun parent est créé, il sera la racine de la taxonomie résultante ; -un agent est créé pour chacun des concepts à positionner dans la taxonomie, ils ont tous TOP comme parent ; ces concepts initiaux ont tous un terme du réseau comme référent. Une fois cette structure de base en place, l'algorithme se déroule en parallèle au sein de chaque agent, jusqu'à obtenir une position d'équilibre global et donc une première version de la taxonomie résultante est présentée à l'utilisateur. Par la suite, les modifications effectuées par l'utilisateur dans la taxonomie auront pour effet de réactiver les agents concernés, l'agorithme sera donc relancé localement. La première étape du procédé (figure 2) se déroule en parallèle dans les agents (ici on s'intéressera uniquement à A k ) ayant plus d'un frère (puisque nous cherchons à obtenir un arbre binaire). L'agent A k envoie alors un message à son père P indiquant le frère dont il est le plus dissimilaire (ici A 1 ). P reçoit donc un message du même type de chacun de ses fils. Par la suite, ce type de message sera appelé un « vote ».
Ensuite, lorsque P a reçu les messages de tous ses fils, il exécute la deuxième étape (figure 3). Grâce aux messages reçus indiquant les préférences de ses fils, P peut constituer trois sous-groupes parmi ses fils :
-le fils qui a reçu le plus de « votes » par ses frères, c'est-à-dire le fils étant le plus dissimilaire du plus grand nombre de ses frères. En cas d'égalité, un des ex aequo est choisi au hasard (ici A 1 ) ;
-les fils ayant permis « l'élection » du premier groupe, c'est-à-dire les agents ayant choisi leur frère du premier groupe comme étant celui qui leur est le plus dissimilaire (ici A k à A n ) ; -les fils restants (ici A 2 à A k?1 ). P crée alors un nouvel agent P (de père P ) et demande aux agents du second groupe (ici les agents A k à A n ) d'en faire leur nouveau père. 
FIG. 4 -Classification distribuée : Etape 3
Enfin, l'étape 3 (figure 4) est assez évidente. Les fils repoussés par P (ici les agents A k à A n ) tiennent compte de son message et choisissent P comme nouveau père. La hiérarchie se constitue ainsi de proche en proche.
Remarquons que cet algorithme converge nécessairement, puisque le nombre de frères d'un agent va en diminuant. Lorsqu'un agent n'a plus qu'un seul frère, son activité est stoppée (hors traitement des messages de ses fils). Il s'agit donc bien d'un algorithme de classification hié-rarchique distribué, les décisions étant prises par des négociations au sein de groupes d'agents autonomes. Les traitements ainsi que les connaissances étant locales aux agents, et la communication se faisant par envoi de message, un tel algorithme peut tout à fait être réparti sur un réseau de machines.
Évaluation des performances
3.1 Approche quantitative À présent, nous allons évaluer les propriétés de notre algorithme distribué. Pour cela, il convient de commencer par une évaluation quantitative, basée sur sa complexité, tout en le comparant à l'algorithme 1 de la section précédente.
La complexité théorique sera calculée pour le pire cas, en considérant l'opération de calcul de la similitude entre deux éléments comme élémentaire. Pour l'algorithme distribué, le pire cas signifie qu'à chaque étape, on ne peut constituer qu'un seul groupe de deux individus. Dans ces conditions, pour un jeu de données initial de n éléments, les deux algorithmes donnent le nombre de calculs de similitude suivant 4 :
4 Les calculs menant à ces équations ne sont pas donnés par souci de concision Ici, t 1 (n) et t dist (n) sont le nombre de calculs de similitudes effectués respectivement par l'algorithme centralisé et par l'algorithme distribué sur l'ensemble du système. Les deux algorithmes ont donc une complexité en O(n 3 ). Dans le pire cas, l'algorithme distribué effectue deux fois plus d'opérations élémentaires que l'algorithme centralisé. Cette différence de facteur 2 provient de la localité des prises de décision au sein des agents. Ainsi, les calculs de similitudes sont faits deux fois pour chaque paire d'agents. On pourrait envisager qu'un agent, après avoir effectué un tel calcul, envoie son résultat à l'autre partie. Toutefois, cela reviendrait simplement à déplacer le problème en générant plus de communications au sein du système. 
FIG. 5 -Résultats expérimentaux
La complexité en moyenne de l'algorithme a été déterminée expérimentalement. Pour cela, le système multi-agent a été exécuté avec des jeux de données en entrée allant de dix à cent termes. La valeur retenue est la moyenne du nombre de comparaisons effectuées pour une centaine d'exécutions sans intervention de l'utilisateur. Ceci donne les courbes de la figure 5. L'algorithme distribué est donc en moyenne plus efficace que l'algorithme centralisé, et sa complexité en moyenne plus réduite que pour le pire cas. Cela s'explique simplement par la faible probabilité qu'un jeu de données pousse le système à ne constituer que des groupes minimaux (deux éléments) ou maximaux (n ? 1 éléments) à chaque étape du raisonnement. La courbe 2 représente le polynôme logarithmique minimisant l'erreur avec la courbe 1. Le terme de plus fort degré de ce polynôme est en n 2 log(n), donc notre algorithme distribué a en moyenne une complexité en O(n 2 log(n)). Enfin, on remarque l'écart réduit entre les performances en moyenne, au maximum et au minimum. Dans le pire cas, pour 100 termes, l'écart type est de 1960,75 pour une moyenne de 40550,70 (soit environ 5%) ce qui souligne la stabilité du système.
Constat qualitatif
Bien que les résultats quantitatifs constatés soient intéressants, le réel intérêt de l'approche relève de traits plus qualitatifs que nous présentons dans cette section. Tous sont des avantages obtenus grâce à l'utilisation d'une approche multi-agent.
Le principal avantage de l'utilisation d'un système multi-agent adaptatif pour une tâche de classification est le côté dynamique introduit par un tel système. L'utilisateur peut intervenir pour effectuer des corrections et la hiérarchie s'adapte en fonction de cette demande. C'est particulièrement intéressant dans un contexte d'IC. En effet, la hiérarchisation fournie par le système est amenée à être modifiée par l'utilisateur puisqu'elle est le résultat d'un traitement statistique. Lors de retours nécessaires au texte pour examiner les contextes d'usage des termes (Aussenac-Gilles et Condamines, 2004), l'ontologue pourra interpréter le contenu réel et ainsi réviser la proposition du système. Ceci est très difficile à réaliser avec une approche centralisée. Dans la plupart des cas, il faut trouver l'étape du raisonnement qui a engendré le résultat erroné et modifier la classe correspondante manuellement. Malheureusement, dans ce cas, toutes les étapes de raisonnement subséquentes à la création de la classe modifiée sont perdues et doivent être recalculées en tenant compte de la modification. C'est pourquoi un système comme ASIUM (Faure, 2000) présente à l'utilisateur les classes créées à chaque étape du raisonnement, pour essayer de gommer ce problème grâce à une collaboration système-utilisateur. De plus, la hierarchie complète n'est visible qu'à la fin du processus. Ainsi, l'utilisateur peut ne constater l'introduction d'une erreur que trop tard. Dans Dynamo, on laisse l'algorithme se dérouler, bien que l'utilisateur ait l'opportunité de l'interrompre et le relancer quand il le souhaite. Il dispose à tout instant d'une hiérarchie complète qu'il peut modifier. Ses interventions déclencheront l'algorithme localement jusqu'à obtenir un nouvel état stable. Ce couplage système-ontologue est indispensable pour une création d'ontologie. Pour le mettre en place, aucun ajustement particulier sur le principe de l'algorithme distribué n'a été nécessaire car chaque agent effectue un traitement local autonome.
De plus, cet algorithme peut être réparti sur un réseau de facto. Il est tout à fait envisageable d'exécuter les agents sur des machines différentes. La communication entre les agents se fait par envoi de messages et chacun d'entre eux conserve son autonomie de décision. Donc, une telle modification du système, pour lui permettre de tourner sur plusieurs machines, ne néces-siterait aucun ajustement de l'algorithme. En revanche, cela demanderait de revoir la couche de communication et la gestion de la création des agents dans notre implémentation actuelle.
tralisé. En effet, le travail de synchronisation s'effectue grâce à des envois de message, un tel système est donc plus coûteux en temps machine qu'une implémentation centralisée.
Le principal point faible actuel de notre algorithme est que le résultat est dépendant de l'ordre d'ajout des données. Lorsque le système travaille de lui-même sur un jeu de données fixe, fourni à l'initialisation, le résultat final est équivalent à ce qui peut être obtenu avec un algorithme centralisé. En revanche, l'ajout d'un nouvel élément après une première stabilitisation a un impact sur le résultat final. L'utilisateur intervient alors et ajoute un nouveau concept représenté par le terme « hé-patite » sous la racine. Le système réagit et se stabilise, on obtient en résultat l'organisation présentée en figure 9. « hépatite » est bien situé dans la bonne branche, mais on ne retrouve pas la configuration désirée de la figure 6 du précédent exemple. Pour cela, il faut compléter notre algorithme distribué pour permettre à un concept de se déplacer le long d'une branche. Nous travaillons actuellement sur ces règles, mais la comparaison avec des algorithmes centralisés deviendra alors plus difficile.
Vers la construction d'ontologies
Notre projet n'a pas pour finalité de proposer un algorithme distribué de classification hié-rarchique, auquel cas celui présenté, qui crée un arbre binaire, suffirait, mais d'obtenir une taxonomie dynamique. Cela signifie que le système présenté dans cet article doit encore évo-luer afin d'être utilisable pour cette tâche.
Tout d'abord, dans le cas général, faute d'un voisinage utilisable dans le réseau « Tête-Expansion », certains termes extraits par Syntex ne sont pas traitables en utilisant notre algorithme de classification hiérarchique. Pourtant ils seraient intéressants pour la taxonomie. Il est donc nécessaire d'ajouter des capacités à nos agents pour tenir compte d'autres critères pour se positionner. En particulier, les éléments de plus bas niveau de la taxonomie pourraient être positionnés en examinant aussi la relation « Tête » des termes qui les représentent. Il a été constaté qu'il s'agissait d'un travail effectué par les utilisateurs de Syntex pour la structuration des niveaux les plus bas de la taxonomie . Cela permettrait de rapprocher par exemple « infection » et « infection viral » qui, dans notre corpus de test, ne pourraient être rapprochés par le système actuel que si leurs contextes d'utilisation étaient identiques.
Une ontologie n'est que rarement un arbre binaire, il faudra donc ajouter des critères supplémentaires afin d'effectuer des regroupements et ainsi obtenir des noeuds n-aires. De la même manière, les interventions de l'utilisateur poussent le système à introduire des niveaux dans la hiérarchie qui ne sont pas forcément nécessaires. On obtient parfois des situations dé-gradées où un noeud n'a qu'un seul fils. Il s'agit donc des deux axes de simplification à explorer pour obtenir une structure plus raffinée.
Conclusion
Après avoir été présentée comme une solution prometteuse, assurant la qualité des modèles et leur richesse terminologique, la construction d'ontologies à partir de l'analyse de corpus s'avère longue et coûteuse. Elle requiert la supervision de l'analyste et la prise en compte de l'objectif d'utilisation de l'ontologie. L'utilisation de logiciels de TAL facilite l'étude des connaissances repérables dans les textes à travers l'utilisation du langage. Cependant, ces logiciels produisent de gros volumes d'informations lexicales ou grammaticales qu'il n'est pas simple d'exploiter pour définir des éléments conceptuels. Notre contribution se situe à cette étape du processus de modélisation à partir de textes, avant toute forme de normalisation ou différenciation.
Nous proposons une approche de classification hiérarchique distribuée, implémentée à l'aide d'un système multi-agent adaptatif, pour suggérer à l'analyste une première structure taxonomique organisant des concepts. Notre système exploite en entrée le réseau terminologique résultant d'une analyse syntaxique de corpus faite par Syntex. L'état courant du déve-loppement permet de produire des structures simples, de les soumettre à l'analyste et de les faire évoluer en fonction des corrections qu'il a apportées. Les performances de cet algorithme distribué sont comparables à celle de la version centralisée. Ses points forts sont essentiellement qualitatifs, car il autorise des interactions avec l'utilisateur et une adaptation progressive à l'ajout de nouveaux éléments linguistiques. La décentralisation permet également de répartir les calculs effectués.
Dans la perspective de construire une ontologie, ce travail n'est qu'une ébauche. Il doit être poursuivi, à la fois pour assurer une meilleure robustesse à la classification, et pour parvenir à des structures plus riches que de simples arbres. Parmi ces enrichissements, un point difficile sera certainement l'étiquetage des relations et la gestion différenciée des types de relation.

Introduction
Suite au succès rencontré par les techniques de puces à ADN pour mesurer l'expression des gènes à grande échelle, la reconstruction de réseaux de gènes à partir de ces données d'expression a suscité depuis quelques années un intérêt croissant. Dans des travaux antérieurs [1,2], nous avons proposé une approche ayant pour but de découvrir différents types de règles entre gènes. Pour faciliter l'interprétation des règles par les experts, nous proposons dans ce papier une visualisation conviviale des règles générées. Nous montrons comment les règles peuvent être visualisées sous forme de graphe orienté présentant les diverses relations découvertes dans les données. L'originalité de notre proposition est de superposer différents types de règles dans un même suppport visuel. Nous proposons également aux utilisateurs de spécifier plusieurs gènes dits centraux, à partir desquels seront présentées uniquement les règles impliquant ces gènes centraux et limitant ainsi le coût de la génération des règles.
Approche proposée
Nous souhaitons avant tout réaliser un outil convivial et proposer ainsi une méthode de visualisation intuitive pour les experts. D'autre part, nous proposons d'appliquer un filtre sur les règles générées en fonction de cinq indices de qualité (support, confiance, lift, leverage et conviction). Ne seront donc visualisées que les règles les plus intéressantes pour les experts, il est donc suffisant de pouvoir visualiser les indices pour la règle ou l'attribut sélectionnés par un simple clic. L'interprétation des règles est une étape particulièrement délicate et très difficile, puisqu'une règle entre deux gènes impliquent également divers produits associés (protéines, facteurs de transcription...). C'est pourquoi les biologistes sont rarement intéressés par les règles avec plus de 3 ou 4 attributs en partie gauche, l'interprétation devenant vite très difficile. Enfin, nous souhaitons offrir la possibilité aux utilisateurs de choisir avant la généra-tion des règles, quelques gènes d'intérêts que nous appelons gènes centraux, qu'ils souhaitent voir apparaître en partie gauches ou droites des règles. Les biologistes pourront ainsi visualiser l'ensemble des règles associées à ces quelques gènes qui les intéressent plus particulièrement. Nous avons donc opté pour une visualisation par graphe qui nous semble être la méthode la plus facile à interpréter et la plus intuitive pour les experts, notre objectif étant aussi de nous positionner dans le cadre de la reconstruction de réseaux de gènes. L'originalité de notre approche réside dans le fait que nous proposons des graphes avec plusieurs sémantiques, que nous appelons réseaux globaux. Chaque sémantique est caractérisée par une couleur particulière. De plus, nous avons choisi de représenter les différents gènes composant les parties droites et gauches des règles, sous forme d'un cercle pour une meilleure lisibilité. Enfin, les gènes centraux sont colorés en rouge, permettant ainsi de les repérer plus facilement.
Implémentation
L'approche proposée a été développée spécifiquement pour les données d'expression de gènes. Un nouveau module appelé RG (Rule Generation) a été intégré à un logiciel gratuit et open-source consacré à l'analyse de données d'expression de gènes, le logiciel MeV. Les points-clés de ce module sont tout d'abord une interface conviviale permettant de choisir parmi plusieurs sémantiques et de spécifier les gènes centraux, le calcul de 5 indices de qualité pour chaque règle générée, la visualisation de réseaux globaux incluant diverses sémantiques, enfin la possibilité de visualiser sous forme textuelle les règles impliquant un gène en particulier avec les différents indices de qualité associés. La version étendue du logiciel MeV avec le module RG, proposant la visualisation de différents types de règles entre gènes, est disponible sur le site : http ://www.isima.fr/agier/GeneRules. 
Références
Summary
Reverse engineering of gene regulatory networks is one of the major challenges of the functional genomics. From gene expression data, various techniques exist to infer these networks. We propose in this paper an approach for the visualization of between-genes interaction networks, from gene expression data. The originality of our approach is to superimpose rules with different semantics within the same visual support and to generate only the rules which imply central genes. Those are specified by the experts and make it possible to limit the generation of the rules to the only genes which interest the experts. An implementation was carried out in the free software MeV of the TIGR institute.

Introduction
L'extraction d'informations à partir de séries temporelles est un domaine de plus en plus important dans la fouille de données. En effet de nombreuses applications utilisant des signaux réels nécessitent l'utilisation des outils de ce champ d'étude. Ainsi, la fouille de données dans des ensembles de séries temporelles utilise divers outils pour extraire des informations intrinsèques ou d'interdépendance entre ces séries temporelles : recherche de similarités entre séries Marteau et al. (2006), corrélation entre séries Pelletier (2005), ou classification Nunez et al. (2002) par exemple. Les domaines d'application pour ces outils sont très variés : finance Takada et Bass (1998), données météorologiques Harms et Deogun (2004), données médicales Summa et al. (2006), biotechnologies, etc. Par ailleurs les biotechnologies et plus particulièrement les bioprocédés offrent aujourd'hui des défis importants concernant l'extraction et la gestion des connaissances. L'analyse des états physiologiques survenant durant ces bioprocédés est un point essentiel pour leur contrôle et leur optimisation. Ces bioprocédés ont longtemps utilisé des approches à base de modèles mathématiques. En effet les bioprocédés forment un système complexe de réactions biologiques qui peuvent être décrites par un système d'équations dynamiques non-linéaires. Mais bien que ces modèles aient montré leur utilité, ils ne peuvent gérer des situations inattendues, car ils sont basés sur des simulations et ne tiennent pas toujours compte de tous les variables réelles mesurées lors des bioprocédés. Les méthodes qui ne sont pas basées sur des modèles sont de plus en plus utilisées. Parmi ces méthodes, beaucoup s'appuient sur l'analyse de signaux biochimiques (qui sont des séries temporelles) mesurés durant le bioprocédé. Ces méthodes se basent notamment sur des techniques de classification Regis et al. (2003), d'apprentissage ou de règles d'expert de type "if-then" Steyer (1991), Steyer et al. (1991). Cependant, si ces approches fournissent de bons résultats dans le cas où les états du bioprocédé sont bien connus (comme par exemple pour des bioprocédés de type "batch" 1 , voir Régis (2004), Roels (1983)), elles sont moins performantes dans le cas des bioprocédés pour lesquels on ne connaît pas parfaitement tous les états qui surviennent (c'est le cas des bioprocédés de type "fed-batch" voir Régis (2004), Roels (1983)). L'approche que nous proposons cherche à répondre à la problématique suivante :
Peut-on caractériser les états d'un système en s'appuyant sur l'analyse dynamique et statistique de séries temporelles ? Dans le cadre de l'application sur les bioprocédés, est-il possible d'extraire de l'information concernant les états physiologiques d'un bioprocédé de type fed-batch, à partir des séries temporelles mesurées pendant l'expérience, et en utilisant peu ou pas de connaissances a priori des experts du domaine ?
Nous proposons d'utiliser une méthode de clustering qui combine l'analyse des propriétés dynamiques et des propriétés statistiques pour détecter et caractériser les états physiologiques d'un bioprocédé fed-batch. L'analyse dynamique consiste à détecter et sélectionner les singularités présentes dans les séries temporelles en utilisant la méthode du maximum du module de la transformée en ondelettes Mallat et Zhong (1992); Mallat et Hwang (1992) et l'évalua-tion du coefficient de Hölder (aussi appelé exposant de Hölder ou exposant de Lipschitz). Ces singularités correspondent aux frontières des différents états. L'analyse statistique consiste à calculer les corrélations des différentes séries temporelles entre le début et la fin d'un état afin de caractériser chaque état. On rappelle que ces séries temporelles représentent pour cette application des variables biochimiques mesurées durant l'expérience. Il faut noter que l'approche que nous proposons est suffisamment générique pour être utilisée sur n'importe quel ensemble de séries temporelles ou n'importe quel système de flux de données, quelque soit le domaine d'application concerné. Cependant, pour une utilisation pertinente de cette approche, des corrélations implicites ou explicites doivent exister entre les séries temporelles, faute de quoi les résultats obtenus auront peu de sens et ne seront pas interprétables. Le plan de cet article est le suivant.
Nous présentons, dans le paragraphe 2 l'application biotechnologique, et dans le paragraphe 3, les travaux existants liés à cette application qui ont motivé la mise en place de la méthode proposée. Dans la section 4, nous présentons en détails l'approche que nous proposons. Enfin nous présentons des résultats expérimentaux dans le paragraphe 5 avant de conclure dans le paragraphe 6.
Les bioprocédés
Les bioprocédés sont des procédés industriels ou expérimentaux utilisant des micro-organismes dans le but de fabriquer des produits biochimiques (produits pharmaceutiques, biocarburants) ou de produire de la biomasse. Durant ces bioprocédés, de nombreuses variables biochimiques sont mesurées. Certaines peuvent être contrôlées tandis que d'autres expriment la biologie du système. Ces variables représentent essentiellement des gaz (dioxygène, dioxyde de carbone, etc.), des éléments rajoutés ou produits dans le milieu (substrat, base, biomasse, etc.) ou des variables physiques régulées (par exemple la variable agitation : c'est la vitesse de rotation des pales d'un moteur assurant l'homogénéité du milieu). Pour des détails sur la fonction biochimique des variables on peut se référer à Régis (2004) ou dans une moindre mesure à Ré-gis et al. (2004a). Les mesures sont effectuées de manière régulière avec des capteurs en ligne ayant la même fréquence pour tous les variables. Ces variables biochimiques sont représentées sous forme de séries temporelles qui évoluent au cours du temps et expriment la dynamique du système pendant le procédé. Ces séries temporelles sont cruciales car elles permettent de comprendre les phénomènes biochimiques et physico-chimiques et par conséquent d'optimiser le procédé. Ainsi l'utilisation d'outils de classification et d'extraction d'informations permet de caractériser de façon automatique les états du système. La classification consiste à segmenter les séries temporelles de telle sorte qu'une classe corresponde à un état physiologique donné. Pour effectuer l'analyse de ces séries temporelles, il convient de faire quelques remarques sur leurs propriétés (voir Régis (2004)). Ainsi il faut noter que :
-ce ne sont pas des signaux périodiques. Ces signaux traduisent des phénomènes biologiques faisant intervenir des micro-organismes. Ces micro-organismes n'ont pas de comportement physiologique périodique. -ce ne sont pas des signaux pairs ou impairs. Il n'y pas de symétrie ou d'antisymétrie dans ces phénomènes biologiques. -ce sont des signaux d'énergie finie. Ils n'ont pas de pics qui tendent vers l'infini. Ce sont des séries temporelles basse fréquence, elles n'ont pas des singularités oscillantes indéfiniment. -ce ne sont pas -pour la plupart en tout cas-des signaux déterministes. Certains signaux physiques régulés peuvent néanmoins être décrits par des formules mathématiques mais la plupart dépend de la biologie et il est impossible à l'heure actuelle de trouver une formule mathématique décrivant parfaitement l'activité liée à ces variables biologiques. -ce ne sont pas des signaux stationnaires. -aucune hypothèse n'est faite concernant un quelconque bruit présent dans ces signaux.
En effet, on suppose que les signaux ne sont pas bruités ou le sont très peu, en raison du dispositif matériel utilisé pour les capteurs. Un exemple de ces variables biochimiques est donné sur la figure 1.  
Travaux existants et motivation
Plusieurs travaux sur la détection des états ont montré que les singularités des signaux mesurés durant un procédé (que ce procédé soit biochimique ou physico-chimique) correspondent au début et à la fin d'un état du système. Ainsi plusieurs auteurs utilisant des méthodes très différentes les unes des autres sont arrivés à la conclusion que ces singularités représentaient les limites des états : c'est le cas de Steyer et al. (1991) (en utilisant la logique floue et un système expert), Bakshi et Stephanopoulos (1994) (en utilisant les ondelettes et un système expert) et Doncescu et al. (2002) (en utilisant la logique inductive). De plus, Jiang et al. (2003) se basent aussi sur cette assertion pour détecter le début et la fin d'un état. Cette hypothèse n'est pas seulement valable pour les procédés chimiques ou biochimiques. En effet dans la plupart des applications réelles dans lesquelles on utilise des séries temporelles décrivant des phénomènes non stationnaires, les singularités représentent des informations significatives. Ainsi par exemple, Struzick montre que les singularités sont significatives dans des séries temporelles issues de la finance Struzik (2000) ou de données médicales Struzik (2003).
Par ailleurs, la détection des états peut être suivie d'une phase de caractérisation automatique de ces états. Une des particularités des bioprocédés de type fed-batch est qu'un état peut apparaître plusieurs fois et à des moments différents de l'expérience. Il est donc nécessaire de caractériser ces états pour savoir s'ils réapparaissent au cours du temps. Plusieurs méthodes statistiques ont été proposées pour caractériser ces états. Par exemple, des méthodes de clas-sification basées sur l'Analyse en Composantes Principales (ACP) Ruiz et al. (2004), l'ACP adaptative Lennox et Rosen (2002), ou la kernel ACP Lee et al. (2004) permettent de distinguer et de caractériser les différents états d'un procédé. Nous proposons d'étudier la corrélation des variables biochimiques entre les intervalles temporels définis par les singularités, afin de caractériser les états du système. On tient compte ainsi de l'évolution dynamique des corrélations à l'image de ce qui est proposé dans d'autres approches Nunez et al. (2002), Pelletier (2005) utilisées pour d'autres applications du data mining. La méthode du maximum du module de la transformée en ondelettes Mallat et Zhong (1992); Mallat et Hwang (1992) est utilisée dans cet article pour détecter les singularités des signaux (afin de définir les limites d'un état) et l'étude des coefficients de corrélation entre ces signaux permet de caractériser ces différents états.
Description de la méthode proposée
La méthode que nous présentons se compose de deux étapes présentées dans les sous paragraphes suivants :
1. la détection et la sélection des singularités des séries temporelles, par les ondelettes et le coefficient de Hölder. Ces singularités correspondent aux frontières des différents états. 2. la caractérisation et la classification des états par la corrélation. Les corrélations sont calculées sur chaque intervalle temporel défini par les singularités.
Détection et sélection des singularités par les ondelettes et l'exposant de Hölder
Les singularités de séries temporelles (ici des signaux biochimiques) correspondent aux limites d'un état. Plusieurs méthodes utilisent les ondelettes pour trouver ces singularités afin de détecter les états : par exemple, Bakshi et Stephanopoulos (1994)   Jaffard (1989Jaffard ( , 1997. Le coefficient de Hölder permet de caractériser le type de singularité comme cela est illustré sur les exemples de la figure 2 et dans le tableau 1.
Ainsi il est possible de sélectionner les singularités les plus significatives à partir de leur exposant de Hölder. Plusieurs méthodes de calcul du coefficient de Hölder existent : ces mé-thodes sont soit basées sur des régressions linéaires Mallat et Hwang (1992), soit basées sur  l'optimisation d'une fonction de coût Mallat et Zhong (1992). Une méthode utilisant les algorithmes génétiques a été proposée pour optimiser la fonction de coût, et semble fournir des résultats plus précis que les méthodes classiques Manyri et al. (2003). La sélection des singularités est réalisée en fonction des attentes des experts en microbiologie. Ainsi pour les bioprocédés de type fed-batch, les experts privilégient les singularités brusques (pic, saut, palier, etc.) qui correspondent à des singularités dont les coefficients de Hölder sont inférieurs à 1. La détection des singularités par les ondelettes et l'évaluation du coefficient de Hölder ont déjà été testées dans un bioprocédé de type fed-batch pour une application proche de celle présentée dans cet article (voir Régis et al. (2004b)).
FIG. 2 -Sur ce signal, les valeurs des coefficients de
Caractérisation et classification des états par corrélation
Les singularités détectées et sélectionnées définissent les bornes d'intervalles temporels : on obtient ainsi une segmentation temporelle du bio-procédé en plusieurs intervalles de temps (un intervalle de temps représentant a priori un état). Nous proposons de caractériser chaque intervalle par les signes des différents coefficients de corrélations linéaires calculés deux à deux entre tous les variables. En effet, les règles de la logique floue du type "if-then" décrivent les relations entre les va-riables biochimiques du point de vue de l'expert en microbiologie Steyer et al. (1991);Steyer (1991). Nous avons déjà signalé que ces règles s'appliquaient parfaitement aux bioprocédés de type batch mais qu'elles rencontraient certaines difficultés face aux bioprocédés de type fed-batch. Nous faisons l'hypothèse dans l'approche que nous proposons, que les règles de type "if-then" peuvent être implicitement remplacées par l'analyse des corrélations entre les variables biochimiques. En fait, les corrélations décrivent les relations entre les variables mais d'un point de vue statistique. De plus ces corrélations décrivent les relations entre variables en fonction du contexte et de manière plus exhaustive que des règles d'expert. Par ailleurs cette approche permet de tenir compte implicitement de l'évolution dynamique des corrélations en fonction du temps, car ces corrélations sont modifiées du fait des réactions biochimiques complexes réalisées et régulées par les micro-organismes au cours du temps. Cette hypothèse qui consiste à utiliser l'évolution des corrélations de séries temporelles multiples au cours du temps comme élément discriminant de classification n'est pas propre aux séries temporelles de cette application biotechnologique ; elle peut être étendue à d'autres applications utilisant des séries temporelles multiples. Ainsi par exemple dans Pelletier (2005) il est indiqué que l'évolution des corrélations entre séries temporelles financières caractérisent les différentes phases du système ; l'auteur de cet article s'appuie donc sur une hypothèse similaire à celle que nous proposons dans cette approche. Les états physiologiques sont donc caractérisés par l'analyse des corrélations entre les signaux biochimiques. Sur chaque intervalle temporel défini à partir des singularités, le coefficient de corrélation est calculé entre les signaux deux à deux. Ce coefficient de corrélation (aussi appelé coefficient de Bravais-Pearson voir Saporta (1990)) est donné par l'équation suivante :
où Ü représente les valeurs de la première variable biochimique (sur un intervalle temporel donné), Ý les valeurs de la deuxième variable (sur le même intervalle temporel), Ò le nombre d'éléments, Ü la valeur moyenne des éléments Ü , Ý la valeur moyenne des éléments Ý , et Ü et Ý les écarts type pour chacune des deux variables. Les valeurs mesurées Ü et Ý des deux variables sont les mesures comprises entre les singularités détectées par les ondelettes (on rappelle que ces singularités représentent les bornes de l'intervalle temporel analysé). Ce coefficient de corrélation est en fait équivalent au cosinus du produit scalaire de deux variables projetées sur le cercle de corrélation pour une Analyse en Composantes Principales (ACP) effectuée entre ces deux variables. Sur chaque intervalle on garde le signe des coefficients de corrélation entre deux signaux. Chaque intervalle est ainsi caractérisé par un ensemble de signes positifs et négatifs. Les intervalles ayant la même série de signes sont regroupés dans la même classe et représente donc le même état (voir figure 3). Ruiz et al. (2004) proposent également une méthode basée sur l'ACP pour une application voisine, concernant le traitement des eaux usées : la méthode consiste à classer les données projetées préalablement dans l'espace défini par les deux premières composantes principales. Cette méthode réduit la dimension de l'espace analysé mais l'ACP ne tient pas compte du temps : l'évolution des signaux n'est pas prise en compte. Pour pallier ce problème, Ruiz et al. proposent d'utiliser  ne tient pas réellement compte des changements survenant dans le procédé. Ainsi, la méthode basée sur la segmentation temporelle à partir de l'exposant de Hölder des singularités, semble mieux adaptée si l'on veut tenir compte de la dynamique du système. Bien qu'elle repose sur une idée relativement simple (corrélation sur des intervalles temporels), l'approche que nous proposons peut être utilisée dans d'autres applications pour caractériser les divers états, car il existe plusieurs domaines dans lesquels les corrélations entre séries temporelles évoluent dynamiquement. On citera le cas de l'économétrie pour laquelle il existe des modèles qui tiennent compte de cette évolution dynamique des corrélations Pelletier (2005).
Résultats expérimentaux
Les tests ont été effectués sur un bioprocédé fermentaire de type fed-batch. Ce bioprocédé utilisant des micro-organismes (levures) appelés Saccharomyces Cerevisiae a duré environs 34 heures. 11 signaux biochimiques ont été mesurés durant l'expérience et ont chacun 2448 points de mesure. Les 11 séries temporelles ont été utilisées pour la classification. Ces signaux ont été normalisés (voir Régis (2004)). Pour la transformée en ondelettes, l'échelle maximum utilisée qui était égale à ¾ ½¼ , a été choisi empiriquement après discussion avec un expert en microbiologie. La méthode a permis de détecter et de caractériser une action externe réalisée durant l'expérience. La classification est composée au total de ¾¾ classes mais c'est surtout la classe numéro qui nous intéressait (voir figure 4). En effet cette classe correspond à l'ajout d'un acide dans le milieu. Toutes les apparitions de la classe correspondent exactement à l'ajout de cet acide. Autant que nous le sachions, c'est la première fois qu'une méthode (qui n'est pas basée sur un modèle) permet de trouver automatiquement l'addition d'un acide dans un bioprocédé fed-batch. Les résultats sont donc encourageants et une analyse biologique plus approfondie doit être réalisée. De même cette approche pourrait mettre en lumière des phénomènes récurrents dans d'autres applications utilisant des ensembles de séries temporelles. L'approche est générique pour les séries temporelles mulitples de quelque origine que ce soit, car : -la méthode n'utilise pas de modèle, ni de connaissance spécifique à la microbiologie.
Elle s'appuie uniquement sur les propriétés analytiques et statistiques des séries temporelles. La classification dépend donc uniquement du contexte et non de connaissance exogène. -même sur des séries temporelles bruitées (comme celles issues des domaines écono-mique ou physique par exemple) il est possible de détecter les singularités les plus significatives (qui ont souvent une amplitude supérieure à celle du bruit) en choisissant le maximum de l'échelle pour la transformée en ondelettes de façon adéquate (en ce qui concerne la détection des singularités par les ondelettes) et en sélectionnant les singularités en fonction de leur coefficient de Hölder (en ce qui concerne la caractérisation de ces singularités) 
Conclusion
Nous avons présenté une méthode de classification non supervisée basée sur l'analyse statistique et dynamique des variables mesurées pendant l'expérience. Elle s'appuie sur la détec-tion de singularités par le maximum du module de la transformée en ondelettes et des valeurs de leurs différents coefficients de Hölder pour tenir compte de la dynamique du système. Elle utilise les corrélations entre variables pour analyser les propriétés statistiques du système et caractériser les états de ce système. La méthode a été testée sur une expérience réelle et les résultats sont prometteurs. Elle a permis de détecter des informations pertinentes sans utiliser les informations a priori des experts de la microbiologie. Les perspectives et les approfondissement sont nombreux. D'une part, les résultats obtenus ont été validés par l'expertise humaine, mais ils peuvent être évalués avec des critères d'évaluation numérique ou être comparés aux résultats d'autres mé-thodes de classification. D'autre part, des travaux supplémentaires consisteront à utiliser les classes trouvées dans une méthode supervisée à tester en temps réel. Une voie à explorer est d'utiliser des plages de valeurs définies avec l'expert au lieu des signes de corrélations afin d'obtenir une certaine souplesse dans la caractérisation des états. Une autre possibilité serait de regrouper les vecteurs de signe des corrélations légèrement différents les uns des autres mais suffisement semblables dans une même classe. Cette méthode pourrait aussi être utilisée dans d'autres applications faisant intervenir des ensembles de séries temporelles. On rappelle en effet que l'approche n'est pas spécifique aux bioprocédés mais elle est utilisable sur tout système de séries temporelles dans lequel il existe un lien explicite ou implicite entre les séries. Ainsi tout système de flux de données pourrait être analysé par cette approche.

Introduction
Dans cette introduction, nous décrivons tout d'abord une situation particulière de l'apprentissage supervisé où l'on s'intéresse à prédire le rang d'une cible plutôt que sa valeur. Nous exposons ensuite deux approches qui permettent de passer d'une prédiction ponctuelle en régression à une description plus fine de la loi prédictive. Nous présentons ensuite notre contribution qui vise à fournir une estimation de la densité conditionnelle complète du rang d'une cible par une approche Bayesienne non paramétrique.
Régression de valeur et régression de rang
En apprentissage supervisé on distingue généralement deux grands problèmes : la classification supervisée lorsque la variable à prédire est symbolique et la régression lorsqu'elle prend des valeurs numériques. Dans certains domaines tels que la recherche d'informations, l'intérêt réside cependant plus dans le rang d'un individu par rapport à une variable plutôt que dans la valeur de cette variable. Par exemple, la problématique initiale des moteurs de recherche est de classer les pages associées à une requête et la valeur intrinsèque du score n'est qu'un outil pour produire ce classement. Indépendamment de la nature du problème à traiter, utiliser les rangs plutôt que les valeurs est une pratique classique pour rendre les modèles plus robustes aux valeurs atypiques et à l'hétéroscédasticité. En régression linéaire par exemple, un estimateur utilisant les rangs centrés dans l'équation des moindres carrés à minimiser est proposé dans Hettmansperger et McKean (1998). L'apprentissage supervisé dédié aux variables ordinales est connu sous le terme de régression ordinale (cf Chou et Ghahramani (2005) pour un état de l'art). Dans la communauté statistique les approches utilisent généralement le modèle linéaire généralisé et notamment le modèle cumulatif (McCullagh, 1980) qui fait l'hypothèse d'une relation d'ordre stochastique sur l'espace des prédicteurs. En apprentissage automatique, plusieurs techniques employées en classification supervisée ou en régression métrique ont été appliquées à la régression ordinale : le principe de minimisation structurelle du risque dans Herbrich et al. (2000), un algorithme utilisant un perceptron appelé PRanking dans Crammer et Singer (2001) ou l'utilisation de machines à vecteurs de support dans Shashua et Levin (2002)  Chu et Keerthi (2005). Les problèmes considérés par ces auteurs comprennent cependant une échelle de rangs fixée au préalable et relativement restreinte (de l'ordre de 5 ou 10). Autrement dit, le problème se ramène à prédire dans quel partile se trouve la cible, en ayant défini les partiles avant le processus d'apprentissage. On se rapproche alors plus d'un problème de classification et les algorithmes sont évalués selon leur taux de bonne classification ou sur l'erreur de prédiction entre le vrai partile et le partile prédit.
Vers une description plus complète d'une loi prédictive
Qu'il s'agisse de classification ou de régression, le prédicteur recherché est généralement ponctuel. On retient alors uniquement la classe majoritaire en classification ou l'espérance conditionnelle en régression métrique. Ces indicateurs peuvent se révèler insuffisants, notamment pour prédire des intervalles de confiance mais également pour la prédiction de valeurs extrêmes. Dans ce contexte, la régression quantile ou l'estimation de densité permettent de décrire plus finement la loi prédictive.
La régression quantile vise à estimer plusieurs quantiles de la loi conditionnelle. Pour ? réél dans [0, 1], le quantile conditionnel q ? (x) est défini comme le réél le plus petit tel que la fonction de répartition conditionnelle soit supérieure à ?. Reformulé comme la minimisation d'une fonction de coût adéquate, l'estimation des quantiles peut par exemple être obtenue par l'utilisation de splines (Koenker, 2005) ou de fonctions à noyaux (Takeuchi et al., 2006). Les travaux proposés dans Chaudhuri et al. (1994); Chaudhuri et Loh (2002) combinent un partitionnement de l'espace des prédicteurs selon un arbre et une approche polynômiale locale. La technique récente des forêts aléatoires est étendue à l'estimation des quantiles conditionnels dans Meinshausen (2006). En régression quantile, les quantiles que l'on souhaite estimer sont fixés à l'avance et les performances sont évaluées pour chaque quantile.
Les techniques d'estimation de densité visent à fournir un estimateur de la densité conditionnelle p(y|x). L'approche paramétrique présuppose l'appartenance de la loi conditionnelle à une famille de densités fixée à l'avance et ramène l'estimation de la loi à l'estimation des paramètres de la densité choisie. Les approches non paramétriques, qui s'affranchissent de cette hypothèse, utilisent généralement deux principes : d'une part, l'estimateur de la densité est obtenu en chaque point en utilisant les données contenues dans un voisinage autour de ce point ; d'autre part, une hypothèse est émise sur la forme recherchée localement pour cet estimateur. Très répandues, les méthodes dites à noyau définissent le voisinage de chaque point en convoluant la loi empirique des données par une densité à noyau centrée en ce point. La forme du noyau et la largeur de la fenêtre sont des paramètres à régler. Une fois la notion de voisinage définie, les techniques diffèrent selon la famille d'estimateurs visée : l'approche polynômiale locale (Fan et al., 1996) regroupe les estimateurs constants, linéaires ou d'ordre supérieur. On peut également chercher à approximer la densité par une base de fonctions splines. Cette dé-marche d'estimation de la loi complète a déjà été adoptée en régression ordinale dans Chu et Keerthi (2005) en utilisant des processus Gaussiens dans un cadre Bayésien .
Notre contribution
Nous proposons ici une approche Bayésienne non paramétrique pour l'estimation de la loi conditionnelle du rang d'une cible numérique. Notre approche utilise la statistique d'ordre en amont du processus d'apprentissage. La manipulation exclusive des rangs au détriment des valeurs rend notre estimateur invariant par toute transformation monotone des données et peu sensible aux valeurs atypiques. Si le problème étudié ne s'intéresse pas à des variables ordinales mais à des variables numériques on peut bien entendu s'y ramener en calculant les rangs des exemples à partir de leurs valeurs. Contrairement aux problèmes habituellement traités en régression ordinale, on considère en amont de l'apprentissage l'échelle globale des rangs de 1 au nombre d'exemples dans la base. Notre méthode effectue un partitionnement 2D optimal qui utilise l'information d'un prédicteur pour mettre en évidence des plages de rangs de la cible dont le nombre n'est pas fixé à l'avance. Disposant d'un échantillon de données de taille finie N et ne souhaitant pas émettre d'hypothèse supplémentaire sur la forme de la densité prédictive, nous nous restreignons à des densités conditionnelles sur les rangs constantes sur chaque cellule. Notre estimateur se ramène donc à un vecteur d'estimateurs de quantiles de la loi conditionnelle sur les rangs. A la différence de la régression quantile, le choix des quantiles n'est pas décidé au préalable mais est guidé par les partitions obtenues.
Suite à ce positionnement, la seconde partie est consacrée à la description de l'approche MODL pour le partitionnement 1D en classification supervisée puis pour le partitionnement 2D en régression. Nous détaillons dans la troisième partie comment obtenir un estimateur univarié et un estimateur multivarié Bayesien naïf de la densité prédictive à partir des effectifs de ces partitionnements. Dans la quatrième partie, les estimateurs obtenus sont testés sur quatre jeux de données proposés lors d'un challenge récent et sont comparés avec les autres méthodes en compétition. La dernière partie est consacrée à la conclusion.
et Merz, 1998) est tracé en fonction de la largeur des sépales, à gauche de la Fig. 1. La discré-tisation consiste à trouver la partition de [2.0, 4.4] qui donne le maximum d'informations sur la répartition des trois classes connaissant l'intervalle de discrétisation.  [ [2.95, 3.35[ [3.35, 4.4 
FIG. 1 -Discrétisation MODL de la variable Largeur de sépale pour la classification du jeu de données Iris en trois classes.
L'approche MODL (Boullé, 2006) considère la discrétisation comme un problème de sé-lection de modèle. Ainsi, une discrétisation est considérée comme un modèle paramétré par le nombre d'intervalles, leurs bornes et les effectifs des classes cible sur chaque intervalle. La famille de modèles considérée est l'ensemble des discrétisations possibles. On dote cette famille d'une distribution a priori hiérarchique et uniforme à chaque niveau selon laquelle : -le nombre d'intervalles de la discrétisation est distribuée de manière équiprobable entre 1 et le nombre d'exemples ; -étant donné un nombre d'intervalles, les distributions des effectifs sur chaque intervalle sont équiprobables ; -étant donné un intervalle, les distributions des effectifs par classe cible sont équiprobables ; -les distributions des effectifs par classe cible pour chaque intervalle sont indépendantes les unes des autres. Adoptant une approche Bayesienne, on recherche alors le modèle le plus vraisemblable connaissant les données. En utilisant la formule de Bayes et le fait que la probabilité du jeu de données soit constante quel que soit le modèle, le modèle visé est celui qui maximise le produit p(modèle) × p(données|modèle). Formellement, on note N le nombre d'exemples, J le nombre de classes cible, I le nombre d'intervalles d'une discrétisation, N i. le nombre d'exemples dans l'intervalle i et N ij le nombre d'exemples dans l'intervalle i appartenant à la jème classe cible. En classification supervisée, les nombres d'exemples N et de classe cible J sont connus. On caractérise donc un modèle de discrétisation par les paramètres
. On remarque que, dans la mesure où une discrétisation est caractérisée par les effectifs des intervalles, un tel modèle est invariant par toute transformation monotone des données. En considérant l'ensemble des discrétisations possibles sur lequel on adopte la distribution a priori hiérarchique uniforme décrite précédemment, on obtient que le log négatif du produit p(modèle) × p(données|modèle) peut s'écrire sous la forme du critère d'évaluation suivant (1) :
Les trois premiers termes évaluent le log négatif de la probabilité a priori : le premier terme correspond au choix du nombre d'intervalles, le second au choix de leurs bornes et le troisième terme décrit la répartition des classes cibles sur chaque intervalle. Le dernier terme est le log négatif de la vraisemblance des données conditionellement au modèle. On trouvera tous les détails nécessaires à l'obtention de ce critère dans Boullé (2006  Pour illustrer le problème du partitionnement 2D lorsque le prédicteur et la cible sont des variables numériques, nous présentons en Fig. 2 le diagramme de dispersion des variables longueur de pétale et longueur de sépale du jeu de données Iris. La figure montre que les iris dont la longueur de pétale est inférieure à deux cm ont toujours des sépales de longueur inférieure à six cm. Si l'on sépare les valeurs de la variable cible longueur de sépale en deux intervalles (inférieur et supérieur à six cm), on peut décrire la loi de répartition de cette variable conditionnellement au prédicteur longueur de pétale à l'aide d'une discrétisation du prédicteur comme en classification supervisée. L'objectif d'une méthode de partitionnement 2D est de décrire la distribution des rangs d'une variable cible numérique étant donné le rang d'un prédicteur. La discrétisation du prédicteur et de la cible comme illustrée en Fig. 3  En utilisant le modèle de discrétisation 2D et la loi a priori définis précedemment, on peut écrire le logarithme négatif du produit p(M) × p(données|M) sous la forme du critère (2) pour un modèle de discrétisation M :
Par rapport au critère obtenu dans le cas de la classification supervisée, il y a un terme supplémentaire égal à log(N ) pour la prise en compte du nombre d'intervalles cible selon la loi a priori et un terme additif en log(N .j !) qui évalue la vraisemblance de la distribution des rangs des exemples dans chaque intervalle cible. Nous présentons succintement l'algorithme adopté pour minimiser ce critère. Nous débutons avec une partition initiale aléatoire de la cible puis, tant que le critère décroît, nous optimisons alternativement la partition 1D du prédicteur pour la partition fixée de la cible et la partition 1D de la cible pour la partition fixée du prédicteur. Nous répétons le processus pour plusieurs partitions initiale aléatoire de la cible et nous retournons la partition qui minimise le critère. En pratique, la converge s'effectue très rapidement, en deux ou trois itérations. Les discrétisations 1D sont effectuées selon l'algorithme d'optimisation utilisé pour la classification supervisée. La valeur du critère (2) pour un modèle de discrétisation donné est relié à la probabilité que ce modèle explique la cible. C'est donc un bon indicateur pour évaluer les prédicteurs dans un problème de régression. Les prédicteurs peuvent être classés par probabilité décroissante de 3 De la discrétisation 2D à l'estimation de densité conditionnelle sur les rangs
Cas univarié
Le passage du partitionnement 2D à l'estimation de densité conditionnelle univariée est illustrée sur un jeu de données synthétique proposé lors du récent Challenge Predictive Uncertainty in Environmental Modelling (Cawley et al., 2006) qui comporte N = 384 exemples et un seul prédicteur. Le diagramme de dispersion ainsi que la partition MODL obtenue sont représentés en Fig. 4. Les intervalles de rangs 1 sont notés P i pour i = 1, . . . , 7 pour le pré-dicteur et C j , j = 1, . . . , 5 pour la cible. Comme pour le partitionnement 1D, les bornes des intervalles notées x 1 , . . . , x 6 et y 1 , . . . , y 4 sont obtenues par moyennage des valeurs du dernier individu d'un intervalle et du premier individu de l'intervalle suivant. Soit x la valeur du prédicteur pour un nouvel individu et P x i la plage de rangs à laquelle appartient le rang de x. Les effectifs de la grille nous permettent de calculer directement la probabilité que le rang de la cible de ce nouvel individu soit dans un intervalle donné C j :
En supposant la densité conditionnelle sur les rangs constante sur chaque plage de rangs cible que délimite la grille, on obtient une expression pour les probabilités élémentaires
rg ( 
FIG. 5 -Estimations MODL de la fonction de répartition conditionnelle univariée sur les rangs pour les sept intervalles de discrétisation du prédicteur
. On obtient un estimateur de la fonction de répartition conditionnelle sur les rangs en cumulant ces probabilités élémentaires :
N .l . Les estimateurs MODL de la fonction de répartition conditionnelle au prédicteur sont tracés pour chacun des sept intervalles du prédicteur en Fig 5. 
Cas multivarié
Dans le cas de P (P > 1) prédicteurs, on peut en première approche construire un estimateur sous l'hypothèse Bayesienne naïve que les prédicteurs soient indépendants conditionnellement à la cible. Soient (x 1 , . . . , x P ) les coordonnées d'un nouvel individu dans l'espace des prédicteurs et P x i l'intervalle de discrétisation auquel appartient chaque composante x i . Sous l'hypothèse Bayesienne naïve, la probabilité élémentaire multivarié s'écrit alors :
Cette dernière expression peut être estimée grâce aux effectifs des partitionnements 2D. On peut en effet directement estimer le premier facteur P (k ? rg(y) < k + 1) par la probabilité empirique 1/N . En ce qui concerne le produit, le premier facteur du numérateur s'obtient selon le même principe que les probabilités univariées en (4) en considérant la plage de rangs à laquelle appartient y après fusion des partitions de la cible induites par chaque prédicteur. Soit J le nombre d'intervalles cible pour la partition cible résultant de cette fusion et N M .j l'effectif de chaque plage de rangs C M j pour j = 1, . . . , J . Par construction, le partitionnement de la cible associé à chaque prédicteur est inclus dans ce partitionnement "multivarié" et l'on note N lM ij l'effectif de la cellule associée au ième intervalle du lème prédicteur et au jème intervalle cible du partitionnement multivarié. Chaque fraction du produit précédent peut alors s'estimer
.j
Evaluation d'un estimateur de la densité conditionnelle sur les rangs
En apprentissage supervisé, les fonctions de score les plus couramment utilisés pour éva-luer un prédicteur sont le score logarithmique et le score quadratique qui prennent différentes formes selon la tâche visée (classification, régression métrique ou ordinale) et l'approche adoptée (déterministe ou probabiliste). En régression ordinale, les approches déterministes citées en introduction sont évaluées sur l'erreur quadratique moyenne entre le rang prédit et le rang vrai en considérant les rangs comme des entiers consécutifs. Pour une prédiction probabiliste, le score logarithmique du NLPD est également utilisé dans Chu et Keerthi (2005) 
l=1 pour le jeu de données D = (x l , y l ) l=1,...,L . Nous utilisons cette fonction de score pour notre estimateur de la densité conditionnelle sur les rangs. Pour un nouvel individu (x l , y l ), on calcule son rang d'insertion k dans l'échantillon d'apprentissage et on estimê p(rg(y)|rg(x)) par la probabilité élémentairê P M odl (k ? rg(y) < k + 1|rg(x)) décrite en (5) et en (6) .
Evaluation expérimentale
Comme exposé en introduction, notre approche se distingue des problèmes habituellement traités en régression ordinale du fait qu'on estime la distribution pour l'échelle globale des rangs et non pour une plage de rangs restreinte à 5 ou 10 rangs distincts. D'autre part, peu de méthodes fournissent un estimateur de la loi complète. Afin de positionner la méthode, nous avons donc choisi de la comparer en premier lieu à d'autres estimateurs de densité conditionnelle sur les valeurs. Connaissant les valeurs associées aux rangs, chaque estimateur sur les rangs nous fournit un estimateur de la fonction de répartition conditionnelle en les N valeurs cibles de l'échantillon. Si l'on note y (k) la valeur cible de l'individu de rang k, on a en effet :
Pour calculer la densité prédictive en tout point à partir de ces N quantiles conditionnels, on a adopté les hypothèses utilisées lors du Challenge à savoir l'hypothèse que la densité conditionnelle soit uniforme entre deux valeurs successives de la cible et que les queues de distribution soit exponentielles 3 . Les différents estimateurs obtenus sont comparées sur le critère du NLPD sur le jeu de données test. Nous avons tout d'abord constaté les mauvaises performances de l'estimateur Bayesien naïf utilisant l'ensemble des prédicteurs. Sur les trois jeux de données réels, le NLPD est en effet supérieur au NLPD pour la méthode dite de référence qui calcule à partir des données d'apprentissage l'estimateur empirique de la loi marginale p(y). Lorsque l'hypothèse d'indé-pendance est trop forte, il est connu qu'elle dégrade fortement l'estimation des probabilités a posteriori (Frank et al., 1998). Ces mauvaises performances sont donc certainement dues à des corrélations importantes entre les prédicteurs. Le tableau 2 indique le NLPD sur le jeu de données test pour les estimateurs MODL univarié et bivarié ainsi que pour la meilleure méthode du Challenge 4 et pour la méthode dite de réfé-rence. On observe tout d'abord que pour tous les jeux de données, les estimateurs MODL sont meilleurs que la méthode de référence, ce qui est loin d'être le cas pour toutes les méthodes soumises. On observe ensuite de bonnes performances des estimateurs MODL, notamment sur les jeux de données SO2 et Precip où les estimateurs univarié et bivarié se placent en tête. Les bonnes performances du prédicteur univarié montrent la qualité du partitionnement 2D obtenu malgré la manipulation exclusive des rangs et non des valeurs durant cette étape. D'autre part, l'estimateur bivarié est toujours meilleur que l'estimateur univarié. Cela indique la présence 
Conclusion
Nous avons proposée une approche Bayésienne non paramétrique pour l'estimation de la loi conditionnelle du rang d'une cible numérique. Notre méthode se base sur un partitionnement 2D optimal de chaque couple (cible, prédicteur). Les effectifs de chaque partitionnement nous permettent d'obtenir des estimateurs univariés et un estimateur multivarié sous l'hypothèse Bayesienne naïve d'indépendance des prédicteurs. Une mise en oeuvre de ces estimateurs sur des données proposés lors d'un récent challenge démontrent la qualité des partitionnements 2D. Les très bonnes performances du prédicteur univarié et du Bayesien naïf utilisant le meilleur couple de prédicteur nous encourage à travailler à l'amélioration du Bayesien naïf utilisant l'ensemble des prédicteurs.

Introduction
L'hypothèse que le KM peut être observé comme un système sociotechnique 1 (Coakes et al., 2002) traduit un chemin de pensée plus proche du constructivisme que du positivisme. Plus précisément afin d'intégrer dans une même démarche l'interrelation permanente de deux composants, on peut dire que l'un est social (il incarne les besoins des individus) et l'autre est technique (il représente l'impératif technologique que l'on veut) du système vu en termes entrées/transformation/sorties. Si l'on détaille plus, on peut dire que la composante sociale est formée par deux sous composants interrelationés : domaine (activité ou business) et acteurs (sujet et autres). De même, la composante technique est formée aussi par deux sous groupes appelés : tâches (processus et données) et technologie 2 (méthodes et outils). La richesse de l'approche réside alors dans sa complexité 3 . En effet l'ensemble (social et technique) coexiste de façon harmonieuse avec les autres, mais dans différents niveaux systémiques en générant ainsi plusieurs niveaux explicatifs d'une même réalité. Dans cet article, nous verrons tout d'abord l'aspect social du KM, puis nous présenterons brièvement l'aspect technique du KM. Nous conclurons en analysant les perspectives de l'ingénierie des systèmes de connaissances pour le KM dans un cadre sociotechnique.
Aspect social du KM
Approche organisationnelle du KM
Historiquement, la connaissance dans le travail a été considérée comme un facteur décisif pour les entreprises (Ballay, 1997), c'est ainsi qu'au cours du temps l'on trouve comme un levier d'avantage productif dans une économie de production (1930), concurrentiel ou compétitif dans une économie de service (1960), coopératif dans une économie globalisée (1990), et collective dans une économie du savoir (2000). D'où l'émergence de nouveaux systèmes d'organisation du travail et mode de management, autour de la connaissance, tels que : knowledge worker (1967), knowledge society (1969), learning organization (1990), systems thinking (1990), actionable knowledge (1996), knowledge-creating company (1995), information ecology (1997), information age (1997), knowledge-based economy (1997), corporate knowledge (2000), corporate longitude (2000), knowledge-based assets (2000), social responsibility in the information age (2005), etc.
L'approche organisationnelle du KM trouve ses racines dans le concept knowledgecreating company (connaissance créatrice organisationnelle) proposé par Nonaka et Takeuchi (1995) comme un facteur critique d'innovation et un levier d'avantage compétitif de l'entreprise. Ces auteurs indiquent que « l'entreprise ne "traite" pas seulement de la connaissance mais la "créée" aussi ». Dans ce même esprit non simonien Varela (1989) affirme « la méta-phore populaire désignant le cerveau comme une machine de traitement de l'information n'est pas seulement ambiguë, elle est totalement fausse ». Dans ce contexte, une entreprise ne peut pas créer de la connaissance sans êtres vivants, car ils sont le moteur de la connaissance créatrice organisationnelle 4 .
Paradigme du ballon de rugby
L'équipe est à la base de la connaissance créatrice organisationnelle de l'entreprise (Nonaka et Takeuchi, 1995). Dans l'entreprise la connaissance existe dans un domaine individuel et collectif au travers de la convergence de deux dualités. Premièrement, la dualité sujet/objet traduit le fait que le sujet conscient (individu, groupe, entreprise) créée de la connaissance en s'impliquant lui-même dans l'objet (espace de travail, environnement, le travail et ses outils de production,…). Ceci forme la dimension épistémologique de la connaissance. Cette dimension a été visualisée à travers des travaux de Polanyi, développée dans ses livres, Personal Knowledge (publié en 1958) et The Tacit Dimension (publié en 1966), sur le champ phi-losophique de la "connaissance tacite" (savoir-faire). Pour Polanyi « les êtres humains acquièrent la connaissance en créant et organisant activement leurs propres expériences » (Nonaka et Takeuchi, 1995). Deuxièmement, la dualité sujet/autres traduit le fait que le sujet conscient créée de la connaissance dans la relation avec les autres (individu, groupe, entreprise). Ceci forme la dimension ontologique de la connaissance. Les travaux de Barnard sur la "connaissance comportementale" (savoir-être) développé dans The Functions of the Executive (publié en 1938), ont contribué à la formation de cette dimension. Nonaka et Takeuchi (1995) distinguent la connaissance tacite et explicite. La connaissance tacite est attachée à la conscience du sujet (niveau individuel) et du groupe (niveau collectif) à travers des idées, métaphores, créances, concepts, hypothèses, modèles mentaux, analogies, etc. La connaissance explicite est attachée à un objet (rapport, document, email, modèle, maquette, plan, etc.). Ainsi, la connaissance individuelle (tacite ou explicite) est au niveau individuel, la connaissance collective (tacite ou explicite) est au niveau du groupe, et la connaissance organisationnelle (tacite ou explicite) est au niveau de l'entreprise.
Un "ballon de rugby" permet de symboliser la capacité de l'entreprise de créer de nouvelles connaissances et de la valoriser sous forme de nouveaux produits ou services. Nonaka et Takeuchi (1995) affirment « la capacité d'une entreprise considérée dans son ensemble, de créer de nouvelles connaissances, de les disséminer au sein de l'organisation et de leur faire prendre corps dans les différents produits, services du système ». La création de connaissances n'est pas un fait isolé sinon qu'elle est liée à l'innovation continue et à l'avantage compéti-tif durable pour l'entreprise.
Le ballon (partie gauche figure 1) prend forme grâce à un processus de conversion de connaissances. Il s'agit d'un processus de causalité circulaire entre la dimension épistémolo-gique (tacite, explicite) et ontologique (individu, groupe, entreprise) de la connaissance, à partir de quatre mécanismes génériques. De gauche à droite : socialisation : conversion de connaissance individuelle tacite à connaissance collective et organisationnelle tacite, la connaissance est créée à travers le partage de l'expérience par apprentissage organisationnel (observation, réflexion, construction de sens, implication personnelle, engagement individuel, etc.) ; extériorisation : conversion de connaissance individuelle tacite à connaissance collective et organisationnelle explicite, la connaissance est crée à travers l'explicitation (formalisation, énonciation) de la connaissance individuelle tacite. Recours au langage ou support écrit pour communiquer idées, concepts, analogies, métaphores, hypothèses, modèles mentaux, etc. ; combinaison : conversion de connaissance individuelle explicite à connaissance collective et organisationnelle explicite, la connaissance est créée à travers la mise en commun de la connaissance explicite à travers des réunions, de changements d'informations, de données, etc. ; internalisation : conversion de connaissance collective et organisationnelle explicite à connaissance collective et organisationnelle tacite, la connaissance dans l'entreprise est créée à travers la réflexion à partir des modèles mentaux partage, savoir-faire technique, etc. entre ses membres. Plus loin nous caractérisons ces mécanismes en termes d'apprentissage organisationnel.
Le ballon avance (partie droite figure 1) entre la dimension épistémologique et ontologique sous forme de spirale ascendante 5 . La connaissance tacite et explicite est disséminée entre les acteurs (individu, groupe, entreprise) à tous les niveaux de l'entreprise (stratégique, tactique, opérationnel).
FIG. 1 -Paradigme du ballon de rugby.
L'apprentissage organisationnel prend forme dans la dimension épistémologique par une sorte de savoir-faire qui transforme la connaissance tacite en connaissance explicite (et vice versa), et d'autre part, dans la dimension ontologique comme un savoir-être entre l'individu, le groupe, et l'entreprise. Le KM est vu comme une dynamique de l'entreprise apprenante. Ingham commente dans l'introduction de l'édition française du livre de Nonaka et Takeuchi (1995), que « les processus d'apprentissage concernent les "savoir quoi faire" et les "savoir pourquoi faire" il s'agira d'apprendre "comment faire" et le résultat prendra généralement la forme d'un "savoir-faire". Mais ils pourront aussi entraîner une modification d'un comportement et avoir trait alors à un "savoir être" ».
L'approche organisationnelle du KM relie alors la connaissance à l'innovation (produits ou services), à l'avantage (productive, concurrentiel ou compétitif, coopératif, collectif) et à l'apprentissage organisationnel (dynamique de l'entreprise apprenante).
Gérer les connaissances de l'entreprise, caractérise les mécanismes de création de connaissances nouvelles et d'apprentissage organisationnel par la mise en place d'un processus de causalité circulaire parmi quatre états, dans le but de créer (connaissance créatrice organisationnelle), disséminer (individu, groupe, entreprise) et valoriser (nouveaux produits ou services) les connaissances de l'entreprise, ces états sont : Socialisation : savoir ou savoir technique pour exprimer la connaissance (tacite ou explicite) créée par le raisonnement de l'acteur (individu, groupe, entreprise), liés aux phénomènes d'intelligence humaine (individu, groupe), d'intelligence économique (entreprise), veille, etc. ; Extériorisation : savoir-faire collectif pour exprimer la connaissance organisationnelle ou collective, organisée comme un tout dans un système de connaissances et matérialisée par l'innovation (produits ou services) capable de produire un avantage compétitif durable pour l'entreprise ; Combinaison : savoirfaire ou savoir-faire technique pour exprimer la connaissance (tacite ou explicite) créée dans l'action (l'apprentissage) pour l'acteur (individu, groupe, entreprise) ; Internalisation : savoircomportemental relatif d'une part à la compétence (transformation de la connaissance en action) des ressources humaines qui matérialisent les qualités professionnelles de l'individu dans son espace de travail (l'environnement : le travail et ses outils de production), et au savoir-être (les qualités personnelles de l'individu) exprimé au travers du phénomène de l'intelligence émotionnelle.
Approche managériale du KM
Le fondement théorique de l'approche managériale du KM est la conception de l'entreprise comme un système ouvert (entrées/transformation/sorties) vis-à-vis de son environnement sur la base du concept d'enaction de Weick (1979). L'enaction décrit la relation entre entreprise et environnement comme deux sous-systèmes distincts, mais en interaction forte (confrontation), afin de maintenir une certaine stabilité dans la relation du système. Weick (1979) distingue un processus d'interaction par inclusion (l'un des deux systèmes doit dicter sa loi à l'autre) et un processus d'interaction par parallélisme (les systèmes négocient). Or, le concept d'enaction de Weick est important car il permet l'existence et l'opérabilité d'un système ouvert (entrées, sorties) mais aussi d'un système fermé (ni entrées, ni sorties) vis-à-vis de son environnement, et comme nous verrons plus loin l'approche de l'enaction de Maturana et Varela (1987) permet l'existence et l'opérabilité d'un système clos (organisation, structure, intelligence).
Les travaux de Ermine (1996), Tounkara (2002), Tounkara et al. (2002), Ermine (2003), Boughzala et Ermine (2004) sur le "patrimoine de connaissances" de l'entreprise (la connaissance est associée au métier) comme un système ouvert et les travaux de Grundstein (1996), Pachulsky et al. (2000), Pachulsky (2001), Grundstein et Rosenthal (2003), Ermine et al. (2006) sur les "connaissances cruciales" de l'entreprise (la connaissance est associée à l'action managériale) ont contribué à la caractérisation de l'approche managériale du KM sur la base du concept d'enaction de Weick (1979).
Paradigme de la marguerite
Si l'on fait l'hypothèse que l'entreprise (système ouvert) peut être décrite à travers un patrimoine de connaissances (approche de Ermine) ou un ensemble de connaissances cruciales (approche de Grundstein). Une "marguerite" (figure 2) permet de symboliser l'enaction environnement-patrimoine de connaissances (ou connaissances cruciales) à travers des processus internes (interaction par inclusion) et processus externes (interaction par parallélisme) (Ermine, 1996). La marguerite est composée d'un coeur pour abriter le patrimoine de connaissances de l'entreprise, autour de celui-ci on a quatre pétales. Le coeur et les pétales sont définis dans une dynamique circulante permanente entre cinq processus (internes et externes) pour créer et gérer le patrimoine de connaissances. De gauche à droite, le processus externe de sélection par l'environnement (phase de projection) permet de repérer les connaissances métiers (cruciales) du business par sélection d'information (formulation des requêtes vers l'environnement externe) afin d'élaborer le corpus d'information. Le processus interne de capitalisation et de partage des connaissances (phase de renseignement ou d'intelligence) permet de préser-ver les connaissances métiers (cruciales) du business, et de le mettre à la disposition de tous les acteurs (individu, groupe, …) de l'entreprise. Le processus externe d'interaction avec l'environnement (phase de confrontation) permet de valoriser les connaissances métiers (cruciales) du business qu'on met en correspondance avec l'extérieur pour la détection du besoin. Le processus interne d'apprentissage et de création de connaissances (phase de création de sens) permet de faire évoluer le patrimoine de connaissances (connaissances cruciales). Le processus interne d'évaluation du patrimoine de connaissances (phase de développement) permet de mesurer la valeur du patrimoine de connaissances ou les connaissances cruciales RNTI -X -de l'entreprise, par un système de mesure traduisant la rentabilité de leurs investissements en matière d'actifs immatériels.
FIG. 2 -Paradigme de la marguerite.
Le maintient de la marguerite en vie se fait par une confrontation permanente avec son environnement au travers de la sélection d'information.
L'approche managériale du KM relie la connaissance au sujet, c'est-à-dire la connaissance est liée à l'action managériale (Grundstein parle de connaissances cruciales), ou bien la connaissance est reliée à l'objet, c'est-à-dire que la connaissance est liée au métier (Ermine parle de patrimoine de connaissances).
Gérer les connaissances de l'entreprise, selon l'enaction de Weick (1979), implique d'une part que l'entreprise et son environnement sont en confrontation permanente, et d'autre part, la mis en place des processus de sélection, de capitalisation et de partage, d'interaction, d'apprentissage, de création, et d'évaluation dans le but de repérer, préserver, valoriser, faire évoluer, et mesurer les connaissances métiers (cruciales) de l'entreprise, où la connaissance dans l'entreprise est décrite à travers un système ouvert (entrées, sorties).
Approche biologique du KM
Le fondement théorique de l'approche biologique du KM prend ses racines dans la conception de l'entreprise par rapport à une opération de distinction. Cette opération permet d'indiquer que les causes et les effets sont distinguables dans des espaces forts différents. L'un est le domaine conceptuel (l'organisation) du processus organisationnel (c'est un processus conceptuel de description abstrait de l'organisation). L'autre est le domaine physique (la structure) du processus structurel (c'est un processus physique de description matérielle de la structure, il s'agit bien ici de la description des propriétés des composants de la structure de l'organisation). L'opération de distinction, est formulée par rapport à l'approche de l'enaction de Maturana et Varela (1987). L'enaction décrit la relation entre organisation et structure à partir de la spontanéité de trois processus : détermination structurelle, couplage structurel, et clôture opérationnelle. La détermination structurelle veut dire que l'entreprise afin de définir l'identité de l'organisation doit maintenir l'unité de la structure (l'entreprise assure une transformation définie dans et par l'organisation). Le couplage structurel signifie que l'entreprise pour maintenir son organisation doit modifier sa structure. La clôture opérationnelle se réfère au fait que l'entreprise est un système clos 6 au niveau de l'organisation (le résultat de la transformation se situe à l'intérieur des frontières du système lui-même), mais ouvert au niveau de la structure (le résultat de la transformation se situe à l'extérieur des frontières du système lui-même). Autrement dit, une fois définie l'organisation, il faut trouver la structure qui maintient l'identité dans et par l'organisation de l'unité. Comme Maturana et Varela (1987) soulignent « la clôture opérationnelle engendre une unité, qui à son tour spécifie un domaine phénoménal ». Et donc, il y a une spontanéité dans leur relation.
Partant de l'hypothèse que l'entreprise peut être approchée comme un système clos, c'est-à-dire, d'une part comme un système vivant caractérisé par l'identité (organisation) et l'unité (structure), et d'autre part comme un système viable caractérisé par l'autonomie (émergence d'un comportement intelligent 7 ) nous formulons le paradigme de l'arbre.
Paradigme de l'arbre
Un "arbre" (figure 3) permet de symboliser l'enaction organisation-structure d'un organisme vivant et viable. L'arbre des connaissances 8 établi un rapport essentiel entre les processus de détermination structurelle, de couplage structurel, et de clôture opérationnelle qui ont lieu à l'intérieur de l'organisme de façon spontané afin de garantir l'auto-maintient de l'identité de l'organisation, l'auto-organisation de l'unité de la structure, et l'autogestion de l'autonomie au cours du temps, comme l'a dit Varela (1989) « tout système autonome est opération-nellement clos ».
La connaissance d'un point de vue biologique est « ce qui nous unit à tous les hommes de tous les temps et la manière par laquelle nous faisons apparaître en nous nos significations existentielles, la manière par laquelle celles-ci sont créées, stabilisées, transformées. C'est justement, dans ce processus d'apprentissage social qu'émerge en nous mêmes la signification du monde dans lequel nous vivons … la connaissance n'est pas armée comme un arbre avec un point de départ solide qui croît progressivement jusqu'à épuiser tout ce qu'il faut connaître, car la connaissance est un mécanisme "circulant" et "d'émergence de signification" … la connaissance est propre de l'être vivant, la connaissance est un grain que l'on sème dans le plus profond de nous mêmes » (Maturana et Varela, 1987). Autrement dit, la connaissance est une conduite (mécanismes et moyens d'agir) qui permet à l'organisme de faire seulement des choses qui n'affectent pas sa survie. La connaissance correspond au fait de faire émerger chez l'organisme un comportement intelligent. Et donc, l'enaction organisation-structure est un mécanisme "circulant" et "d'émergence de signification". Par conséquent, la connaissance ne doit pas obligatoirement impliquer des représentations vraies de la réalité objective.
L'approche biologique du KM relie alors la connaissance à la survie (processus d'organisation et de structuration du vivant) et à l'intelligence (processus d'émergence d'un comportement, une action). Le KM est un mécanisme de survie et d'intelligence plutôt que de construction de sens (problème de la signification, vraies représentations de la réalité).
Gérer les connaissances de l'entreprise, selon l'enaction de Maturana et Varela (1987), consiste à mettre en place des processus de détermination structurelle, de couplage structurel, et de clôture opérationnelle dans le but de créer, stabiliser et transformer les connaissances de l'entreprise, où la connaissance est décrite à travers un système clos (organisation, structure, intelligence) et non pas comme un système ouvert.
FIG. 3 -Paradigme de l'arbre.
Le KM symbolise la capacité de l'entreprise à capitaliser, partager, et créer de la connaissance afin de maintenir l'identité de l'organisation et l'unité de la structure comme un tout dans le temps.
Synthèses des approches
Le tableau 1 montre une synthèse des approches organisationnelle, managériale et biologique du KM selon l'aspect social.
Les approches organisationnelle, managériale et biologique du KM font apparaître de vrais domaines de recherche pour aboutir aux concepts, méthodes et outils autour de méca-nismes génériques et processus du KM que nous avons synthétisés dans le tableau 1 : -Comment créer des connaissances nouvelles et faire de l'apprentissage organisationnel dans l'entreprise ? En effet, selon l'approche organisationnelle du KM, le KM n'est pas un système de traitement de l'information mais bien un système de création des connaissances nouvelles et d'apprentissage organisationnel.
-Comment évaluer (mesurer) le savoir de l'entreprise ? En effet, selon l'approche managé-riale du KM, le KM permet d'évaluer les connaissances du business (patrimoine de connaissances ou connaissances cruciales) de l'entreprise par des méthodes et outils issus de l'expé-rience en terrain.
-Comment "faire-évoluer" (c'est-à-dire créer de nouvelles connaissances là, où il n'y a pas de savoir) et "faire-émerger" (c'est-à-dire créer des connaissances nouvelles à partir d'une représentation non symbolique de la réalité) les connaissances de l'entreprise ? En effet, le KM selon l'approche de l'enaction de Weick (mis en évidence par Grundstein (1996), Pachulsky (2001), Ermine (1996), Tounkara (2002) permet la mise un place d'un système de connaissance bâti sur l'idée que la connaissance peut être définie au travers de l'information (données, traitements) qui prend une certaine signification (concepts, tâches) dans un contexte (domaine, activité) donné. En revanche, le KM selon l'approche de l'enaction de Maturana et Varela (mis en évidence par Limone et Bastias (2006), Jiménez (2005 
Aspect technique du KM
En se plaçant maintenant dans l'aspect technique, le KM est vu à travers de tâches (processus et données) et de la technologie (méthodes et outils).
Approche ingénierie des connaissances et compétences du KM
La connaissance dans l'entreprise peut être considérée d'une part comme un objet (données) que l'on peut approcher à partir d'un projet d'ingénierie des connaissances (méthodes et outils du KM 9 ), par exemple Ermine propose MASK (Method for Analyzing and Structuring Knowledge) et MKSM (Method for Knowledge System Management), permettant une analyse et une structuration d'un patrimoine de connaissance liée à la connaissance métier (une tâche en particulier) . Ermine (1996) affirme « MKSM est l'équivalent de MERISE pour les systèmes d'information, à savoir une méthode d'analyse de systèmes de connaissances pour aboutir à la conception d'un système opérationnel de gestion des connaissances », et d'autre part comme une action managériale (processus), par exemple Grundstein propose GAMETH (Global Analysis Methodology), une méthode pour le KM qui permet le repérage des connaissances cruciales de l'entreprise (Pachulsky et al. (2000). Grundstein (1996) dit « le management des connaissances … couvre toutes les actions managériales visant à actionner le cycle de capitalisation des connaissances afin de repérer, préserver, valoriser, transférer et partager les connaissances cruciales de l'entreprise ». La connaissance peut aussi être décrite à partir de cartes routières de compétences des acteurs de l'entreprise, par exemple Authier et Lévy (1992) proposent une méthode de repérage des savoirs et des savoir-faire, afin d'établir un arbre de compétences collectives pour l'entreprise. La méthode se trouve implémentée dans un progiciel appelé GINGO et développé par www.trivium.fr. Ces méthodes et autres 10 du KM symbolisent un macroscope (en empruntant le terme de Joël de Rosnay) pour observer l'acteur (individu, groupe, entreprise) dans son poste de travail (connaissance métier) en tant que facteur clé pour améliorer la capacité de l'entreprise pour maintenir l'organisation comme un tout dans le temps et non pas pour ajouter une autre application au parc informatique de l'entreprise, sinon nous risquons dans le futur d'enfermer le KM dans une problématique des systèmes à base de connaissances ou des systèmes experts.
Conclusion
Nous avons présenté dans cet article, une approche sociotechnique du KM. Cette approche caractérise davantage l'aspect humain de la connaissance et son support technologique. C'est ainsi qu'ont été mises en évidence quatre perspectives, à savoir : l'approche organisationnelle de Nonaka et Takeuchi (fondée sur le concept de knowledge creating-company) ; l'approche biologique de Maturana et Varela (fondée sur le concept de l'enaction) ; l'approche managériale de Ermine (fondée sur le concept de la marguerite) ; et l'approche ingénierie des connaissances et compétences du KM.
Au carrefour de ces quatre approches, deux concepts sont le fondement de la problémati-que essentielle du KM. L'un est le concept de "faire-évoluer" les connaissances, c'est-à-dire de créer de nouvelles connaissances là, où il n'y a pas de savoir. L'autre est le concept de "faire-émerger" la connaissance, c'est-à-dire de créer des connaissances nouvelles à partir d'une représentation non symbolique de la réalité. En effet, tous les modèles de KM de nos jours sont gérés à partir du passé (les bonnes pratiques, le retour d'expérience, la communauté de pratiques, etc.) et non pas à partir de l'avenir (l'inconnu, le chaos, le désordre, etc.). Nous pensons que les concepts "faire-évoluer" et "faire-émerger" la connaissance sont fort intéressants pour réfléchir sur la question.
Remerciments
L'auteur tient à remercier Christine Deville pour son aide à la correction grammaticale du texte et tout particulièrement Germain Lacoste (Directeur de l'ENI de Tarbes) pour son amitié, y finalmente, agradezco a algo tan vivo como es el canto siempre alegre de un picaflorcito entre flores… Références Authier M. et P. Lévy (1992). Les Arbres de Connaissances. La Découverte.

Introduction
Un réseau de neurones est un ensemble de neurones interconnectés qui communiquent entre eux et avec l'extérieur. Un réseau de neurones se présente comme un graphe où les noeuds sont les différentes unités de réseau et les arcs représentent les connexions entre ces unités. Le nombre de couches, le nombre de neurones par couche et les interconnexions entre les différentes unités du réseau définissent l'architecture (encore appelée topologie) de celui-ci. Un neurone peut être appelé unité ou cellule. Comme tout système d'apprentissage supervisé, les systèmes d'apprentissage supervisé à base des réseaux de neurones fonctionnent en deux phases : la phase d'apprentissage qui consiste à construire à partir des observations (exemples présentés sous forme (x, y) où y représente l'observation de la fonction f en x) un système capable d'approximer la fonction f dont l'expression analytique n'est pas facile à trouver ; la phase de classement qui utilise le modèle construit en phase d'apprentissage pour produire des décisions (prédire un nouvel exemple qui ne faisait pas partie des observations de la base d'apprentissage). Définir la structure du réseau pour de tel système n'est pas une tâche évidente (J. Han et Hamber, 2001;A.Cornuéjols et Miclet, 2002). En effet, il n'existe aucune méthode permettant de définir et de justifier la structure d'un réseau de neurones (J. Han et Hamber, 2001).
La définition de l'architecture du réseau de neurones multicouches pour la résolution d'un problème donné reste un problème ouvert. Outre les méthodes génétiques (D.Curran et O'Riordan, 2002), ce problème est souvent résolu en utilisant deux approches : la première consiste à ajouter successivement des neurones et des connexions à une petite architecture, la deuxième quant à elle consiste à supprimer des neurones et des connexions d'une architecture initiale maximale. Ces deux approches ont souvent comme inconvénient le temps d'apprentissage élevé et imprévisible.
Les domaines d'application des réseaux de neurones sont multiples (Dreyfus et al., 2002) : la biologie moléculaire (analyse des séquences d'ADN (Shavlik et Towell., 1994)), prédiction, classification, traitement d'images, le génie logiciel (estimation des coûts de logiciel (S. Mbarki et al., 2004)), etc. Aucune explication ne justifie à notre connaissance la définition des architectures utilisées. Pour les problèmes de classification en particulier, plusieurs méthodes ont été développées et sont proposées dans la littérature (J. Yang et al., 1999;Yang et al., 1996;Parekh et al., 1997b). On peut classer ces méthodes en deux catégories : celles qui construisent l'architecture en utilisant un ensemble de connaissances de domaine (exemple de KBANN (Shavlik et Towell., 1994)) et les autres qui définissent cette architecture sans aucune connaissance (J. Yang et al., 1999;Parekh et al., 1997b;Yang et al., 1996;Parekh et al., 1995). Les algorithmes de construction des réseaux de neurones artificiels que nous avons rencontrés dans la littérature produisent des réseaux ayant les caractéristiques suivantes (Parekh et al., 1995(Parekh et al., , 1997a(Parekh et al., , 2000 : architecture minimale, habile à trouver le compromis entre les mesures de performances telles que le temps d'apprentissage, habilité à généraliser, . . .etc. Ces méthodes constructives de ré-seau de neurones diffèrent par les facteurs suivants (Parekh et al., 1997a(Parekh et al., , 2000 : restriction des entrées (type de données en entrée), circonstances d'ajout d'une nouvelle unité, initialisation des poids de connexion de cette unité et son apprentissage.
Dans ce travail, notre intérêt porte sur les méthodes de recherche d'architecture des ré-seaux de neurones multicouches feed-forward (les informations circulent des entrées vers les sorties, sans retour) pour la résolution des problèmes de classification. Les principaux paramètres de mesure de performance traités sont : la taille du réseau (nombre de neurones, nombre de couches...), la complexité en temps et la capacité de généralisation. Certaines méthodes de recherche d'architecture de réseaux de neurones ont été évaluées sur des données de taille relativement petite et la qualité des résultats varie d'un ensemble de données à l'autre (Parekh et al., 1997a). D'autre part, une comparaison théorique des ces algorithmes n'a pas à notre connaissance été faite. Notre étude portera essentiellement sur la comparaison de ces algorithmes d'après les mesures de performances citées ci-dessus et des résultats expérimentaux sur les données tirées de la base UCI (Newmann et al., 1998). Les opérations supplémentaires de prétraitement de données telles que projection, binarisation, la normalisation et autres ne seront pas abordées dans cette étude.
Le reste du papier est organisé comme suit : la section suivante présente les réseaux de neurones multicouches, et quelques notions (définitions et apprentissage) liées aux réseaux de neurones multicouches ; la troisième section recense les algorithmes de construction d'architecture neuronale. Les analyses expérimentales et théoriques feront l'objet de la quatrième section.  (Rumelhart et al., 1986a,b). Cet algorithme produit des bons résultats lorsque l'architecture est appropriée, il est également utilisé lorsque l'architecture du réseau reste statique (D.Curran et O'Riordan, 2002). Des outils tels que WEKA (Witten et Frank, 2005) et SNNS (Stuttgart Neural Network Simulator) 2 offrent aux utilisateurs la possibilité de définir la structure de leur réseau, ces outils apprennent ces réseaux par retropropagation. La difficulté majeure est de trouver cette architecture (A.Cornuéjols et Miclet, 2002).
Algorithme d'apprentissage des poids de connexion
L'apprentissage dans les systèmes neuronaux peut se faire par unité ou par couche. Le principal algorithme d'apprentissage des unités neuronales est le perceptron ; lorsque les données ne sont pas linéairement séparables, il est remplacé par l'une de ses variantes : pocket with racket, barycentric,...etc. L'apprentissage d'une couche peut être généralisé à toutes ses unités ou se faire suivant le principe du Winner Take All (WTA). Afin de montrer l'influence de l'algorithme d'apprentissage sur la complexité du système, nous présentons (voir tableau 1) sans entrer dans les détails les complexités en temps de ces algorithmes. Les détails sur ces algorithmes d'apprentissage sont présentés dans (Parekh et al., , 2000, (Gallant, 1990) et (Frean, 1992a). Théoriquement, ces algorithmes utilisent de manière similaire l'espace mé-moire. Le tableau 1 présente les complexités en temps de ces algorithmes d'apprentissage dans lequel les variables désignent : n le nombre d'objets, M le nombre de classes, m le nombre d'attributs et N it le nombre d'itérations dans l'algorithme d'apprentissage.
Algorithmes de construction des RNA
Cette section présente un résumé des algorithmes MTiling (Yang et al., 1996), MTower (Gallant, 1990;Parekh et al., 1995Parekh et al., , 1997a, MUpstart (Parekh et al., 1997b), Distal (J.Yang
Algorithmes
Complexité en temps Perceptron et al., 1999). La convergence de ces algorithmes est démontrée dans (Parekh et al., 1995) et dans (Parekh et al., 1997a).
TAB. 1 -Complexité en temps des algorithmes d'apprentissage
3.1 L'algorithme Mtiling (Yang et al., 1996) Mtiling est une adaptation de l'algorithme Tiling (Mezard et Nadal., 1989) à la classification multiclasses. Cette méthode construit un réseau de neurones multicouches dans lequel les unités d'un niveau (couche) reçoivent des unités du niveau inférieur (immédiat). En cas de mauvais classement du réseau courant, la procédure détermine le neurone ayant fait le maximum d'erreurs, augmente un certain nombre de neurones (auxillaires) à la couche de sortie courante, elle augmente également une nouvelle couche de M neurones au réseau et connecte les entrées de cette couche aux sorties des unités de la couche de sortie ; cette couche (ajoutée) devient la nouvelle couche de sortie. Les neurones ajoutés sont appris individuellement par les algorithmes pocket ou une variante. Le nombre maximal de couches cachées H est spécifié par l'utilisateur. La couche de sortie fonctionne suivant le principe de WTA afin d'assurer qu'une seule classe soit active pour un exemple donné.
3.2 L'algorithme Mtower (Parekh et al., 1997a) Cette méthode construit le réseau sous forme de tour comme la méthode originale Tower (Gallant, 1990). L'architecture finale du réseau est telle que : les neurones au sommet (sortie) sont connectés à tous les neurones d'entrée et un neurone de la couche k reçoit de l'information de tous les neurones de la couche k ? 1 (immédiatement connectés à la couche k). La tour est construite en ajoutant successivement les couches de M unités au réseau. Ces unités sont apprises par l'une des variantes du perceptron. La couche ajoutée est complètement connectée à la couche de sortie et à la couche d'entrée ; cette couche devient ainsi la nouvelle couche de sortie. La modification du réseau est répétée jusqu'à l'obtention de la précision (de classement) fournie par l'utilisateur, l'algorithme peut aussi d'arrêter le nombre maximal de couche est atteint. La couche de sortie fonctionne aussi suivant le principe du WTA. La méthode Mpyramid (Parekh et al., 1997a) est semblable à Mtower à la seule différence que la couche ajoutée reçoit l'information de toutes les couches précédentes.
3.3 L'algorithme MUpstart (Parekh et al., 1997b) Mupstart est une version de l'algorithme Upstart (Frean, 1992b) pour la classification multiclasses. Comme Upstart, cet algorithme est basé sur une correction d'erreur produite par le réseau courant. Ce réseau a une couche d'entrée de N + 1 unités et une couche de sortie de M unités. Le fils gauche, X est augmenté au noeud en cas de " wrongly on " et le fils droit Y en cas de " wrongly off ". On parle de "Wrongly on" (resp. "Wrongly off") lorsque la sortie obtenue est "1" (resp. "0") alors que l'on désirait obtenir plutôt "0" (resp. "1"). Le neurone ajouté pour corriger l'erreur produite à cette étape est appris par l'algorithme du perceptron ou une de ses variantes. Ces neurones ajoutés sont appris avec pour sorties attendues (T x et T y) définies dans le tableau 2 ; dans ce tableau, y (resp. o) représente la sortie désirée (resp. sortie obtenue), T x (resp. T y) est la sortie attendue pour l'apprentissage des unités x (resp. y) pour la correction de la différence entre y et o. C'est un algorithme qui est basé sur le calcul de distance entre exemple (ou entre attribut). La distance peut être euclidienne, ou autre. Cette procédure calcule, trie d'abord les distances entre exemples (ou attributs) et les stocke dans une matrice D. La procédure Distal construit à partir de la matrice des distances la couche cachée du réseau. Cette couche cachée est construite de la manière suivante : un neurone est ajouté pour apprendre k exemples tel que k soit l'indice de la classe ayant la plus grande plage d'exemples consécutifs dans D appartenant à la même classe. Les neurones de la couche cachée dans Distal sont à seuil sphérique (neurone actif si ? low ? W X ? ? high et inactif sinon, où W X est le résultat en sortie) et les poids de connexion entre l'unité ajoutée et les unités d'entrée sont initialisés par les attributs de l'exemple i à partir duquel k a été trouvé. A l'étape suivante, tous les poids du réseau entre la couche cachée et celle de sortie sont multipliés par deux. Les unités de la couche fonctionnent suivant le principe de WTA.
Complexités -Evaluations théoriques
Les algorithmes de recherche d'architectures de réseau de neurones peuvent être classés suivant l'approche de construction du réseau en deux grands groupes :
1. l'approche descendante, le réseau final est obtenu par élimination des neurones et de connexions. L'architecture initiale comporte un nombre suffisant de neurones et de connexions capables de bien classer tous les exemples.
2. L'approche ascendante, le réseau est obtenu par ajout de neurones et de connexions. Les ajouts se font lorsque le réseau considéré produit des erreurs. L'architecture initiale comporte un nombre assez réduit de neurones, généralement deux couches (la couche d'entrée et la couche de sortie).
Tous les algorithmes présentés précédemment construisent le réseau par l'approche ascendante. Seul Mtiling (Yang et al., 1996) permet de combiner les deux approches en recherchant par l'approche ascendante une architecture maximale (architecture ayant le maximum H) de couches et ensuite élaguant cette architecture. Asymptotiquement, les algorithmes que nous avons présentés ont un comportement semblable. La complexité de ces algorithmes est calculée et présentée (tableau 3) au pire des cas : pour Distal par exemple, en général le nombre d'exemples est toujours supérieur au nombre d'attributs, c'est ce qui justifie que le temps maximal est obtenu lorsque la distance entre exemples est considérée. Le tableau 3 présente les complexités en temps et en espace mémoire des algorithmes tandis que le tableau 4 présente d'autres aspects non négligeables de ces algorithmes ; la projection consiste à ajouter un attribut supplémentaire ayant pour valeur la somme des carrés des autres attributs de l'exemple ( i x 2 i ). Les particularités de ces algorithmes sont
TAB. 3 -Complexité en temps et en espace mémoire
Algorithmes Nombre de Nombre de neurones opération couches cachées dans la couche cachée suplémentaire
TAB. 4 -Recapitulatif de certains aspects.
les suivantes : La particularité de l'algorithme Mtiling, la correction d'erreur se fait par ajout d'une couche de M neurones ("maitres"), cette couche devient la nouvelle couche de sortie ; ajout de k neurones à la couche cachée immédiatement connectée à la couche de sortie. Les connexions sont complètes entre couches adjacentes.
Mtower ajoute tout simplement une nouvelle couche de M neurones. Cette couche devient également la nouvelle couche de sortie et est complètement connectée à la couche d'entrée.
Distal est plus spécifique que les autres méthodes ; elle construit un réseau ayant une seule couche cachée, chaque neurone de la couche cachée permet de séparer un sous ensemble d'exemples. le réseau obtenu garantit le bon classement des exemples sans être appris.
La méthode MUpstart quant à elle ajoute à chaque correction un seul neurone ; ce neurone est appris individuellement et connectée au neurone ayant produit le plus grand nombre d'erreurs.
Expérimentations
Ces algorithmes ont été testés sur les données de la base UCI (Newmann et al., 1998), les attributs de ces ensembles sont tous numériques et sans valeur manquante. Les expérimentations précédentes sont présentées dans (Parekh et al., 1997a;J.Yang et al., 1999) Nos expériences ont porté sur les algorithmes Distal, Mupstart, Mtiling, Perceptron-Cascade, Mpyramid, Mtower. Ces expériences étaient validées par validation croisée d'ordre 10 (10-Cross-Validation). Dans nos expériences, la distance choisie (sans aucune justification) entre exemples pour Distal est la distance euclidienne. Les unités ajoutées au cours de la construction du réseau avec les autres ont été appris par "Perceptron with racket modification". Les résultats obtenus après le classement que nous avons obtenus ne sont pas aussi meilleurs que ceux présentés dans (Newmann et al., 1998 
TAB. 5 -Description des données utilisées pour les expérimentations
Au cours de ces expériences, l'attention s'est surtout portée pour chaque algorithme sur les aspects suivants : le temps nécessaire pour la construction du réseau, le nombre total de neurones du réseau construit et la capacité de généralisation.
Nos expérimentations sont classées en deux groupes : Distal est la seule méthode qui construit son réseau sans faire d'apprentissage ; pour cette raison, nous l'avons évalué séparément des autres. Cette méthode présente des très bons résultats expérimentaux en présence des données de petite taille. Mais lorsque les données atteignent une certaine taille, Distal ne produit plus de résultat. Cela est due à un besoin excessif de l'espace mémoire. La figure 5 montre l'évaluation de Distal en fonction de la taille de données.
Les autres algorithmes construisent un réseau de neurones en modifiant simultanément la structure de celui-ci et les poids de connexion. Leurs évaluations expérimentales sont très coû-teuses en temps. L'apprentissage des unités ajoutées lors de la construction du réseau s'est fait avec l'algorithme "Perceptron with racket modification" avec 500 itérations, un maximum de 350 (choix arbitraire) neurones par couches et une précision d'apprentissage et de test souhaitée à 100%. Le temps affiché ici est en secondes, mais représente le temps obtenu pour le meilleur système en validation croisée. 
FIG. 1 -Evaluation expérimentale de l'algorithme Distal
Les tableaux 6 et 7 présentent les résultats expérimentaux de ces algorithmes sur la base "spambase" avec respectivement comme nombre maximum de couches cachées 2 et 24. Ces tableaux montrent l'influence du nombre de couches sur le taux de généralisation. En effet malgré un temps plus élevé, nous obtenons une bonne capacité de généralisation lorsque le nombre de couches est élevé.
Les tableaux 8 et 10 présentent les résultats obtenus de l'expérimentation sur les données présentées plus haut. Le nombre maximal de couches cachées choisies au cours de ces expériences est 2 (ce choix est arbitraire et nous a permis d'avoir les résultats avec un temps relativement raisonnable). Le symbole "-" signifie que soit l'expérience a échoué (générale-ment un débordement de la pile), soit le nombre maximal de couches a été atteint avec une précision inférieure à celle souhaitée. Notons que les données "pendigits" et "optdigits" ont pour particularité par rapport à "spambase" le fait que les exemples appartiennent à 10. Le comportement des algorithmes sur les données ci-dessus n'est pas très appréciable, mais ne saurait être généralisé à toute la classification multiclasses. Les tableaux 9 et 11 montrent une amélioration de la capacité de généralisation lorsque le nombre maximum de couches passe de 2 à 24 ; ce qui explique l'influence du nombre de couches sur la capacité de généralisation.
Discussion -Conclusion
Ces méthodes de construction des réseaux de neurones sont presque semblables ; ces algorithmes initialisent le réseau à une structure minimale et modifient ce réseau par ajout des unités au fur et à mesure que les erreurs surviennent. La question qui se pose est de savoir comment diriger le choix d'un concepteur face au problème d'architecture du réseau de neurones. Le choix de l'architecture de réseau de neurones devra être orienté vers la satisfaction de l'utilisateur final du système c'est-à-dire que ce choix devrait aboutir à la production d'un réseau de neurones ayant une bonne capacité de généralisation. D'après notre étude, nous pouvons séparer ces algorithmes en deux catégories : Les algorithmes qui construisent les réseaux ayant une seule couche cachée ; dans cette catégorie, peut être classé Distal. Cet algorithme a aussi pour avantage la faible intervention de l'utilisateur ; mais il y a un besoin énorme en espace mémoire. Les autres algorithmes construisent des réseaux pouvant avoir H couches cachées ; l'utilisateur doit fournir à l'algorithme en plus du nombre maximal (H) de couches cachées, la précision souhaitée, l'algorithme d'apprentissage. A partir des tables 6 et 7, on remarque une variation de taux généralisation en fonction du nombre maximal de couches. On peut également noter le mauvais comportement de ces algorithmes sur les données "pendigits" et "optdigits" mais nous ne pouvons pas généraliser sur toutes les grandes bases de données. Pour généraliser, nous devons tenir compte de l'influence de la taille du réseau, de la répartition des données... Quelque soit la méthode de construction des réseaux de neurones, le choix des paramètres (modèle du réseau, nombre de couches, nombre de neurones par couches, définition des connexions, taux d'apprentissage,...) reste toujours très problématique : pour la méthode 

Introduction
Il y a deux visions d'un document XML : une vision « centrée données » et une vision « centrée document ». Les documents XML « centrés données » sont constitués d'un ensemble d'éléments ayant une structure régulière : un ensemble de fiches bibliographiques, par exemple. Les documents XML « centrés document » décrivent des textes plus ou moins structurés : des livres scientifiques, par exemple. Pour interroger des documents XML « centrés données », le langage de requêtes XQuery (le SQL de XML), défini par le W3C (W3C, 2006b), est tout à fait bien adapté. Par contre, pour interroger des documents XML « centrés document » XQuery n'est pas suffisant lorsque l'interrogation est de nature sémantique, comme par exemple, la recherche des chapitres de livres qui concernent un certain sujet. De telles requêtes sont traitées traditionnellement par les systèmes de recherche d'information (Baeza-Yates et Ribeiro-Neto, 1999). Ce constat a conduit le W3C à proposer une extension de XQuery, XQuery Full-Text (W3C, 2006a), pourvue de fonctionnalités de recherche plein-texte. Le coeur de XQuery Full-Text est une fonction nommée ftcontains qui permet de tester si le contenu textuel d'un élément est conforme à une requête exprimée à l'aide d'opérateurs spécifiques : troncatures, connecteurs logiques, calcul de distance entre mots, etc. La fonction ftcontains retourne un degré de similarité entre l'élément et la requête qui peut être strict (appartient à {0, 1}) ou flou (appartient à [0,1]). Cependant, XQuery FullText n'impose pas la façon de calculer ce degré de similarité, qui dépend donc de l'implémentation.
Parallèlement, dans le cadre de l'initiative INEX pour l'évaluation de la recherche d'information XML 1 , un langage de recherche d'information spécifique nommé NEXI (Trotman et Sigurbjörnsson, 2005), a été proposé. Ce langage est une version simplifiée de XPath (W3C, 1999), qui intègre une fonction about permettant de calculer le degré de similarité d'un élément XML avec une requête textuelle restreinte à un sac de mots éventuellement marqués d'un indicateur de présence ou d'absence. Pour ce langage aussi, la façon de calculer ce degré de similarité n'est pas imposée.
Dans (Le Maitre, 2005), nous avons proposé d'étendre le langage XQuery en y intégrant le langage NEXI muni d'une sémantique floue basée sur le modèle de recherche d'information vectoriel (Baeza-Yates et Ribeiro-Neto, 1999). Cet article améliore cette proposition sur le plan formel et décrit le prototype que nous avons construit au-dessus d'un moteur XQuery classique : Galax (Fernandez et al., 2003) et Saxon (Saxon, 2006), à l'heure actuelle.
Cet article est organisé de la façon suivante. Dans la section 2, nous décrivons le langage NEXI. Dans la section 3, nous proposons une sémantique pour ce langage, basée sur le modèle vectoriel et la logique floue. Dans la section 4, nous montrons comment intégrer une requête NEXI dans XQuery. Dans la section 5, nous présentons le prototype que nous avons réalisé afin de valider notre proposition. Enfin, dans la section 6, nous concluons et dressons quelques perspectives.
NEXI
Le langage NEXI (Trotman et Sigurbjörnsson, 2005), a été défini dans le cadre de l'initiative INEX dans le but d'effectuer de la recherche d'information dans des documents XML. Il permet d'exprimer deux types de requêtes : les requêtes « Content Only (CO) » qui ne portent que sur le texte d'un document et les requêtes « Content and Structure (CAS) » qui portent à la fois sur le texte d'un document et sur sa structure.
Une requête CO est une liste de termes, éventuellement affectés d'un indicateur imposant une présence ou une absence de ce terme dans le contenu textuel du fragment de document interrogé. Voici un exemple de requête CO : +XML XQuery -SGML Une requête CO s'applique à un document XML et a pour réponse la conformité du contenu textuel de ce document avec cette requête. La forme de la réponse à une requête CO et la façon de calculer cette réponse ne font pas partie de la spécification du langage NEXI. Cette réponse pourrait être le degré de similarité du document interrogé avec la requête.
Une requête CAS est une expression XPath simplifiée qui a la forme suivante dans sa version la plus générale :
Cette requête s'applique à un arbre de document XML et retourne les noeuds de cet arbre de type nt 2 conformes au prédicat p 2 qui sont des descendants des noeuds de type nt 1 conformes au prédicat p 1 . Les prédicats p 1 et p 2 sont construits à partir de prédicats textuels ou numériques connectés par des opérateurs de conjonction ou de disjonction. Un prédicat textuel a la forme suivante : about(chemin de localisation, requête CO) et un prédicat numérique a la forme suivante :
chemin de localisation connecteur littéral numérique
Ici encore, la forme de la réponse à une requête CAS et la façon de la calculer ne font pas partie de la spécification du langage NEXI. La réponse pourra être une séquence d'éléments de type nt 2 affectés de leur degré de similitude avec la requête.
Considérons par exemple un document XML, enregistré dans le fichier « actes.xml », conforme à la DTD suivante : Dans le langage NEXI original, une requête NEXI est adressée à un document XML dans son entier. Mais dans le but d'intégrer NEXI à XQuery, nous considérons qu'une requête NEXI est adressée à une séquence de noeuds éléments de l'arbre d'un document XML dont chaque noeud texte est supposé être annoté par un sac de termes obtenu en extrayant les mots du texte contenu dans ce noeud, et en les normalisant par lemmatisation ou par racinisation. Notons que le fait de ne considérer qu'un seul document n'est pas restrictif, car lorsque l'on souhaite interroger plusieurs documents, il est toujours possible de les rassembler dans un document unique.
Soit d l'arbre du document interrogé et M le nombre de termes indexant les textes contenus dans les noeuds textes de d. Conformément au modèle vectoriel, on associe à chaque noeud texte ou élément n j de d un vecteur à M dimensions v j = (w 1j , …, w Mj ) où w ij est le poids du terme t i dans le noeud n j (w ij ? [0, 1]). Nous plaçant dans le cadre de la logique floue, nous interprétons le vecteur associé au n j comme étant la conjonction floue p 1j ? … ? p Mj où chaque p ij est un prédicat flou dont la valeur de vérité est le poids w ij du terme t i dans le noeud n j . Ce poids est calculé de la façon suivante :
-Si n j est un noeud texte et lt j est le sac de termes annotant ce noeud, w ij est calculé selon la formule classique du modèle vectoriel : des k noeuds enfants (texte ou élément) du noeud n j . Cette fusion est interprétée comme la disjonction floue des conjonctions de prédicats associées à chacun de ces noeuds. Par exemple, si l'on choisit la fonction max comme opérateur de s-norme, on aura w ij = max(w' i1 ,…, w' ik ), ce qui correspond à la disjonction des valeurs de vérité des noeuds enfants pour le terme t i . Un vecteur v q = (w 1q , …, w Mq ) est associé à chaque requête CO q où w iq est égal à ief i si le terme t i est présent dans la requête q, ou à 0, s'il ne l'est pas. La valeur d'une requête CO q adressée à un noeud n j auquel est associé un vecteur v j est égale au cosinus de l'angle entre les vecteurs v j et v q .
Dans le langage XQuery + NEXI, une requête CAS a la forme suivante :
où e est une expression XQuery qui doit avoir pour valeur une séquence de noeuds élément. La valeur d'une requête CAS est une séquence floue de noeuds de l'arbre du document interrogé. Nous appelons séquence floue de noeuds une séquence S de la forme : 
Intégration de NEXI dans XQuery
L'intégration de NEXI dans XQuery est faite au moyen de la métafonction nexi et d'une extension de la clause for de l'opérateur FLWOR, similaire à celle de XQuery FullText (W3C, 2006a). L'appel de la métafonction nexi a la forme suivante :
où q est une requête CAS ayant la forme définie au paragraphe 3 et t est une expression XQuery dont la valeur appartient à [0, 1] et qui spécifie un seuil de pertinence. Cet appel retourne la séquence floue de noeuds résultant de l'évaluation de q suivie du filtrage des noeuds dont le degré d'appartenance est supérieur ou égal à t. Pour accéder aux noeuds de cette séquence et à leur degré d'appartenance, il faut utiliser la clause for étendue suivante :
for $n score $s in nexi(q, t) qui lie successivement chaque noeud de la séquence floue de noeuds résultant de l'appel nexi(q, t) à la variable $n et le degré d'appartenance de ce noeud à la variable $s.
Par exemple, la requête: « Titres et année des articles concernant SGML avec un seuil de pertinence de 0,5, triés par pertinence décroissante », peut s'exprimer de la façon suivante :
for $a score $s in nexi(fn:doc("actes.xml")//article[about(.,SGML)], 0.5) order by $s descending return <article>{$a/titre, $a/année}</article> Montrons maintenant que l'on peut traduire en XQuery standard l'appel de la métafonction nexi et la clause for étendue. Le principe consiste 1. à traduire l'appel nexi(q, t) en une expression XQuery qui retourne la séquence  [e, p]. où nexi:about est la fonction qui calcule la valeur d'une requête CO appliquée à un noeud élément ou texte.
Une clause for $n score $s in nexi(q, t) peut être traduite en la suite de clauses for et let suivante :
let $fs := traduction de l'appel nexi(q, t) for $k in 1 to fn:count($fs) div 2 let $n := $fs[2 * $k -1] let $s := $fs[2 * $k] 5 Description du prototype Le prototype que nous avons réalisé est composé :
-d'un préprocesseur qui reçoit en entrée une requête XQuery + NEXI et la traduit en une requête XQuery pure, en appliquant le processus de traduction défini au paragraphe 4 ci-dessus, -d'un moteur XQuery, -d'une interface utilisateur.
Le préprocesseur
Le processus d'évaluation d'une requête XQuery + NEXI est schématisé sur la figure 1. Le préprocesseur a été programmé en Java. L'analyse lexicale et syntaxique de l'appel à la métafonction nexi et de l'extension de la clause for, est réalisée au moyen des parseurs JFlex et Cup.
FIG. 1 -Evaluateur de requête XQuery + NEXI
L'évaluation d'une requête fait appel à un module XQuery de préfixe nexi qui contient les définitions des fonctions assurant les opérations liées à la recherche d'information textuelle : construction du sac de noeuds annotant un noeud texte, construction des vecteurs associés aux noeuds texte, aux éléments et aux requêtes CO, fonction about, etc.
Moteurs XQuery
Les caractéristiques des deux moteurs XQuery utilisables dans notre prototype, Galax et Saxon, sont :
-Galax traite des requêtes XQuery conformes aux recommandations du W3C. Il a été développé en Objective Caml, et fournit des APIs en Objective Caml, C et Java, ainsi que des exécutables binaires pré-compilés pour les différents systèmes d'exploitation. Le prototype fait appel au fichier binaire « galax-run.exe » pour évaluer la requête. L'avantage de ce type d'utilisation est que le traitement de la requête s'effectue sur les ressources système de la machine. L'inconvénient est que la portabilité du logiciel est dépendante des fichiers binaires dont on dispose. Un autre inconvénient réside dans le fait que l'implémentation de NEXI en XQuery nécessite l'utilisation de certaines fonctions mathématiques, comme par exemple le logarithme, qui ne sont pas fournies par Galax. -Saxon est un processeur XQuery/XSLT conforme aux recommandations du W3C.
Il a été développé en Java. Saxon est, entre autres, utilisé dans l'éditeur XML Oxygen. Le prototype utilise les APIs Java fournies par Saxon pour l'exécution des requêtes XQuery. L'avantage majeur de ce processeur est qu'il permet de lier des modules Java aux modules XQuery. Ainsi, la déclaration : -Les chemins d'accès aux fichiers nécessaires à l'exécution de la requête.
-Le moteur XQuery sur lequel sera exécuté la requête : Galax ou Saxon, dans la version actuelle. -La pondération globale d'un terme dans le vecteur associé à un noeud texte. Si la réponse est Non : le facteur ief a la valeur 1 et donc le poids d'un terme dans un noeud texte est égal à la fréquence de ce terme dans le sac de termes annotant ce noeud. Si la réponse est Oui : le facteur ief est calculé selon la formule indiquée au paragraphe 2 ci-dessus. 

Introduction
Dans les applications de gestion de la relation clients, les scores permettent d'identifier les clients les plus susceptibles de réagir positivement à une campagne marketing. L'interprétation du score apporte alors une information supplémentaire pour améliorer l'efficacité des campagnes marketing. L'utilisation de la méthode présentée 1 ici doit se faire après une étape de sélection de variable qui aura supprimer les variables redondantes pour ne pas risquer de diluer l'interprétation. L'interprétation d'un score est constituée de l'association de l'importance à l'instance (I) d'une variable d'entrée et de l'influence à l'instance d'une variable d'entrée (I v ) présentées ci-dessous.
Notations -Soit V j : la variable explicative j, X : un vecteur de dimension J, K : le nombre d'instances, X n : le vecteur représentant l'instance n, X nj : la composante j du vecteur n, F : le modèle, p : la sortie p du modèle, F p (X) : la valeur de la sortie p du modèle pour le vecteur X et F p j (X n ; X k ) désigne la sortie p du modèle étant donné le remplacement de la composante j de l'instance X n par celle de l'instance X k .
Importance à l'instance d'une variable d'entrée
Etant donné 2 le modèle F , l'instance considérée X n , la variable explicative V j du modèle et la variable à expliquer p du modèle, on définit la sensibilité du modèle S(V j /F, X n , p) par : la moyenne des variations mesurées en sortie du modèle lorsqu'on perturbe l'instance considérée X n en fonction de la distribution de probabilité de la variable V j . La variation mesurée, pour l'instance X n est la différence entre la "vraie sortie" du modèle F j (X n ) et la "sortie perturbée" du modèle F j (X n , X k ).
La sensibilité du modèle pour l'exemple X n à la variable V j est alors la moyenne des
2 sur la distribution de probabilité (distribution empirique observée sur K exemples) de la variable V j . On a alors :
En réalisant cette mesure de sensibilité pour la sortie p mais quelque soit la variable d'entrée 3 j on possède une distribution des sensibilités. On définit alors l'importance de la variable V j à l'instance X n , I(V j |F, X n , p), comme étant le rang, o, de la sensibilité du modèle S(V j |F, X n , p) parmi l'ensemble des sensibilités S(V j |F, X i , p) ?i, j. Cette mesure fournit l'importance d'une variable d'entrée pour l'instance X n relativement à toutes les autres instances et toutes les autres variables. Cette mesure relative permet de se concentrer sur les seules informations pertinentes pour chaque instance. Cette mesure a été testée avec succès pour des problèmes de classification dans (Lemaire et Clérot, 2004) elle est notamment reliée aux travaux de (Breiman, 2001;Féraud et Clérot, 2002) 
Influence à l'instance d'une variable d'entrée
Une variable peut "tirer vers le haut" (valeur forte) ou "tirer vers le bas" (valeur faible) la sortie du modèle. Pour l'exemple X n la valeur "naturelle" de la sortie p ' du modèle est par définition F (X n ). La valeur "perturbée" de la sortie du modèle pour l'exemple et en perturbant la variable d'entrée V j est F j (X n , X k ). La distribution des F j (X n , X k ) représente ce qu'aurait pu être la valeur de la sortie du modèle pour l'instance X n si sa variable V j avait été différente. La position de sa sortie "naturelle" au sein de cette distribution renseigne sur la nature de la valeur de sa variable V j . On définit alors l'influence de la variable V j à l'instance X n , I v (V j |F, X n , p), comme étant le rang, r, de la sortie "naturelle" parmi l'ensemble de ses sorties potentielles. Cette mesure fournit l'influence d'une variable d'entrée pour une instance relativement à toutes les autres valeurs "potentielles" de la variable.
Exemple d'utilisation pour un problème de classification
Dans le cas d'un problème de classification à deux classes (?1 ;+1) un rang important de I v dénotera une influence positive par rapport à la classe +1 et négative par rapport à la classe ?1 (et réciproquement pour un très faible rang de I v ). On obtiendra alors une interprétation de la forme (l'interprétation sera réalisée variable explicative, j, par variable explicative en entrée du modèle) : "Pour l'instance X n la variable j qui est I importante indique qu'elle est I v fortement de la classe +1".

Introduction
Dans un projet de fouille de données, la phase de préparation des données vise à extraire une table de données pour la phase de modélisation (Pyle, 1999;Chapman et al, 2000). La préparation des données est non seulement coûteuse en temps d'étude, mais également critique pour la qualité des résultats escomptés. La préparation repose essentiellement sur la recherche d'une représentation pertinente pour le problème à modéliser, recherche qui se base sur une sélection de variables. L'objectif de la sélection de variable est triple: améliorer la performance prédictive des classifieurs, le temps d'apprentissage et de déploiement des modèles, et leur interprétabilité (Guyon et Elisseeff, 2003). Deux approches principales, filtre et enveloppe (Kohavi et John, 1997), ont été proposées dans la littérature. Les méthodes filtres évaluent la corrélation entre les variables exogènes et la variable endogène, indépendamment de la méthode de classification utilisée. Les méthodes enveloppes recherchent pour un modèle donné le meilleur sous-ensemble de variables. Les méthodes enveloppes, très coûteuses en temps de calcul, sont plutôt adaptées à la phase de modélisation. Parmi les méthodes filtres, les méthodes procédant par analyse univariée permettent d'ordonner les variables exogènes par importance prédictive décroissante. Elles sont classiquement utilisées en phase de préparation des données pour rapidement extraire un sous-ensemble de variables pertinent pour la modélisation à partir d'un ensemble de variables candidates potentiellement de grande taille. Dans cet article, nous nous focalisons sur l'approche filtre.
L'approche filtre la plus fréquemment utilisée repose sur la mise en oeuvre de tests statistiques (Saporta, 1990), comme par exemple le test du Khi2 pour les variables exogènes caté-gorielles, ou les tests de Student ou de Fisher-Snedecor pour les variables exogènes numéri-ques. Ces tests d'indépendance sont simples à mettre en oeuvre, mais présentent de nombreux inconvénients. Ils se limitent à une discrimination entre variables dépendantes et indépendan-tes, sans permettre un ordonnancement précis des variables exogènes, et sont contraints par des hypothèses d'applicabilité fortes (effectifs minimaux, hypothèse de distribution gaussienne dans le cas numérique…). De nombreux autres critères d'évaluation de la dépendance entre deux variables ont été étudiés dans le contexte des arbres de décision (Zighed et Rakotomalala, 2000). Ces critères sont basés sur une partition de la variable exogène, en intervalles dans le cas numérique et en groupe de valeurs dans le cas catégoriel. En recherchant de façon non paramétrique un modèle de dépendance entre variables exogènes et endogène, ils permettent une évaluation fine de l'importance prédictive des variables exogènes. Dans le cas où tous les modèles de partitionnement de la variable exogène sont envisagés, un compromis doit être trouvé entre finesse de la partition et fiabilité statistique. Ce compromis est réalisé dans l'approche MODL (Boullé, 2005(Boullé, , 2006a en formulant le problème comme un problème de sélection de modèle et en adoptant une approche Bayesienne.
Les méthodes filtres univariées restent néanmoins limitées, en étant aveugles aux interactions entre variables exogènes. Ainsi, les variables redondantes, apportant la même information, ne peuvent être détectées. De même, les variables exogènes qui seules sont non informatives et simultanément le sont (cas du XOR par exemple) ne sont pas détectables par les méthodes filtres. L'évaluation supervisée de l'importance d'une paire de variables exogènes, qui fait donc intervenir trois variables, a été peu étudiée dans la littérature. Les diagrammes de dispersion catégorisés par valeur endogène permettent une visualisation des paires de variables exogènes numériques, mais cela ne permet pas de quantifier l'information prédic-tive. Le regroupement simultané des lignes et des colonnes d'une table de contingence a été étudié dans un cadre général (Govaert et Nadif, 2006), ou dans le cadre des arbres de déci-sion (Zighed et al, 2005)  Nous proposons dans cet article une extension des méthodes de partitionnement univariées MODL au cas de l'analyse bivariée, pour tout type de paires de variables, numériques, catégorielles ou mixtes. Chaque variable est partitionnée, en intervalles ou groupe de valeurs selon son type, ce qui permet de distribuer les individus sur les cellules d'une grille bidimensionnelle. La corrélation entre la grille et la variable endogène est alors évaluée pour mesurer l'importance prédictive jointe de la paire de variables. Le compromis entre information et fiabilité, lié à la finesse de la grille, est établi au moyen d'une approche Bayesienne de la sélection de modèle, qui aboutit à un critère d'évaluation des partitionnements joints de variables exogènes. Ce critère d'évaluation est optimisé au moyen d'une heuristique réutili-sant les techniques d'optimisation univariée MODL, en optimisant alternativement le partitionnement de chaque variable de la paire, le partitionnement de l'autre variable étant fixé.
L'article est organisé de la façon suivante. La section 2 rappelle l'approche MODL dans le cas univarié. La section 3 décrit l'extension de cette approche à l'analyse bivariée. La section 4 présente l'évaluation de la méthode. Enfin, la section 5 conclut cet article.
Analyse univariée supervisée
Cette section résume l'approche MODL de la discrétisation supervisée (Boullé, 2006a) et du groupement de valeurs supervisé (Boullé, 2005). La discrétisation supervisée traite des variables exogènes numériques. Elle consiste à partitionner la variable exogène en intervalles, en conservant le maximum d'information relative à la variable endogène. A titre illustratif, la figure 1 présente le nombre d'individus par classe du jeu de données Iris (Blake et Merz, 1996), pour chaque valeur de la variable largeur de sépale. Un compromis doit être trouvé entre la finesse de l'information prédictive, qui permet une discrimination efficace des valeurs endogènes, et la fiabilité statistique, qui permet une généralisation du modèle de discrétisation.
Discrétisation MODL
Dans l'approche MODL, la discrétisation supervisée est formulée en un problème de sé-lection de modèle. Une approche Bayesienne est appliquée pour choisir le meilleur modèle de discrétisation, qui est recherché en maximisant la probabilité p(Model|Data) du modèle sachant les données. En utilisant la règle de Bayes, et puisque la quantité p(Data) ne dépend pas du jeu de données, il s'agit alors de maximiser p(Model)p(Data|Model), c'est-à-dire un terme d'a priori sur les modèles et un terme de vraisemblance des données connaissant le modèle.
Dans un premier temps, une famille de modèles de discrétisation est explicitement défi-nie. Les paramètres d'une discrétisation particulière sont le nombre d'intervalles, les bornes des intervalles et les effectifs des classes endogènes par intervalle. Dans un second temps, une distribution a priori est proposée pour cette famille de modèles. Cette distribution a priori exploite la hiérarchie des paramètres: le nombre d'intervalles est d'abord choisi, puis les bornes des intervalles et enfin les effectifs par classe endogène. Le choix est uniforme à chaque RNTI -X - , ,
}. En utilisant la définition de la famille de modèles de discrétisation et de sa distribution a priori, la formule de Bayes permet de calculer explicitement les probabilités a posteriori des modèles connaissant les données. En prenant le log négatif de ces probabilités, cela conduit au critère d'évaluation fourni dans la formule (1).
Les trois premiers termes représentent la probabilité a priori du modèle: choix du nombre d'intervalles, des bornes des intervalles, et de la distribution des valeurs endogènes dans chaque intervalle. Le dernier terme représente la vraisemblance, c'est à dire la probabilité d'observer les valeurs de la variable endogène connaissant le modèle de discrétisation.
La discrétisation optimale est recherchée en optimisant le critère d'évaluation, au moyen de l'heuristique gloutonne ascendante décrite dans (Boullé, 2006a). A l'issue de cet algorithme d'optimisation, des post-optimisations sont effectuées au voisinage de la meilleure solution, en évaluant des combinaisons de coupures et de fusions d'intervalles. L'algorithme exploite la décomposabilité du critère sur les intervalles pour permettre après optimisations de se ramener à une complexité algorithmique en O (JN log (N)). 
Groupement de valeurs MODL
FIG. 2 -Groupement de valeurs MODL de la variable couleur de chapeau pour la classification de la base Mushroom en deux classes.
Le cas des variables exogènes catégorielles est traité au moyen d'une approche similaire, en évaluant les modèles de groupement de valeurs. Dans le cas numérique, il s'agit de partitionner les valeurs exogènes, avec une contrainte d'adjacence entre valeurs (partitionnement en intervalles). Dans le cas catégoriel, il s'agit toujours de partitionner les valeurs exogènes, cette fois sans aucune contrainte (partitionnement en groupes de valeurs). La figure 2 illustre le groupement des valeurs de la variable couleur de chapeau pour la classification de la base Mushroom (Blake et Merz, 1996).
M. Boullé
Soient N le nombre d'individus, V le nombre de valeurs exogènes; J le nombre de classes endogènes, I le nombre de groupes de valeurs, N i. le nombre d'individus dans le groupe de valeur i et N ij le nombre d'individus de la classe j dans le groupe i. L'application de l'approche Bayesienne de la sélection de modèle conduit ici à un critère d'évaluation d'un groupement de valeurs, fourni dans la formule (2). Cette formule possède une structure similaire à celle de la formule (1), en remplaçant dans les deux premiers termes la probabilité a priori d'une partition en intervalles par celle d'une partition en groupes de valeurs.
( )
B(V,I) est le nombre de répartitions des V valeurs exogènes en I groupes (éventuellement vides). Pour I=V, B(V,I) correspond au nombre de Bell. Pour I >V, B(V,I) s'écrit comme une somme de nombres de Stirling de deuxième espèce.
Le critère d'évaluation des groupements de valeurs est optimisé au moyen d'une heuristique gloutonne ascendante décrite dans (Boullé, 2005). Des étapes de pré-optimisation et post-optimisation sont utilisées, de façon à garantir une complexité algorithmique en O(JN log(N)) sans sacrifier aux performances de la méthode.
Les méthodes de discrétisation et groupement de valeurs MODL produisent le partitionnement des valeurs exogènes le plus probable connaissant les données. Des évaluations comparatives intensives (Boullé, 2005(Boullé, , 2006a) ont mis en évidence les apports de la méthode, tant d'un point de vue informatif que prédictif.
Extension à l'analyse bivariée supervisée
Nous présentons dans cette section la nouvelle méthode d'analyse bivariée, qui étend l'approche MODL à l'analyse supervisée des paires de variables exogènes. Après avoir introduit le problème au moyen d'un exemple illustratif, nous présentons un critère d'évaluation et un algorithme d'optimisation de ce critère.
Intérêt du partitionnement joint de deux variables
La figure 3 présente le diagramme de dispersion des variables V1 et V7 du jeu de données Wine (Blake et Merz, 1996), catégorisé par valeur endogène. Chaque variable isolément est faiblement discriminante. La variable V1 ne peut séparer les classes 1 et 3 au delà de la valeur 13. De même, la variable V7 confond les classes 1 et 2 au-delà de la valeur 2. Les deux variables conjointement autorisent une meilleure discrimination des classes.
L'approche utilisée pour qualifier l'information prédictive contenue dans la paire de variables repose sur un partitionnement des individus en une grille de données. Chaque variable exogène est partitionnée en intervalles (ou groupes de valeurs selon son type). Le produit cartésien des deux partitions univariées répartit les individus sur une grille de données, dont les cellules sont définies par des paires d'intervalles. Le lien avec la variable endogène se fait au moyen de la distribution des valeurs endogènes dans chaque cellule. Par exemple dans la figure 3, la variable V1 est discrétisée en 2 intervalles (borne 12.78) et la variable V7 en 3 intervalles (bornes 1.235 et 2.18). Les individus se répartissent dans les 6 cellules de la grille bidimensionnelle ainsi définie. Dans chaque cellule, nous obtenons une distribution des valeurs endogènes. Par exemple, le tableau de la figure 3  
Critère d'évaluation
Nous utilisons ici la même approche que dans le cas univarié pour rechercher le meilleur compromis entre information et fiabilité, en introduisant une famille de modèles de partitionnement bivariés, puis en choisissant le meilleur modèle au moyen d'une approche Bayesienne. Nous nous intéressons d'abord au cas des variables exogènes numériques, avant de généraliser à tout type de paires de variables, numériques, catégorielles ou mixtes. Définition 1. Un modèle de partitionnement bivarié supervisé est défini par une partition en intervalles pour chaque variable exogène, et par la distribution des valeurs endogènes pour chaque cellule de la grille de données déduite du croisement des deux partitions univariées.
Notations.
-  De façon similaire au cas de la discrétisation univariée, les deux premiers termes d'a priori correspondent au choix de la partition (nombre d'intervalles et bornes) de la première variable exogène. De même, les deux termes suivants correspondent au choix de la partition de la deuxième variable exogène. Le dernier terme d'a priori, en fin de première ligne, repré-sente le choix de la distribution des valeurs endogènes dans chaque cellule. Le dernier terme de la formule 3, sur la deuxième ligne, représente la vraisemblance, c'est à dire la probabilité d'observer les valeurs de la variable endogène dans les cellules de la grille connaissant le modèle de partitionnement bivarié supervisé.
Le critère d'évaluation bivariée se généralise au cas des variables exogènes catégorielles, en remplaçant les termes de partition en intervalles (de type log(N) + log( 
Algorithme d'optimisation
Nous proposons un algorithme d'optimisation, qui partant d'une solution initiale de partitionnement bivarié aléatoire, procède en alternant les optimisations partielles par variable:
1. initialiser une grille bivariée aléatoire, basée sur O(N ½ ) parties par variable, 2. tant que amélioration du critère, répéter:
(a) optimiser la partition de X 1 , la partition de X 2 étant fixée, RNTI -X -(b) optimiser la partition de X 2 , la partition de X 1 étant fixée.
Dans le cas univarié, on remarque que les critères d'évaluation se décomposent de façon additive en un coût de partition et des coûts par partie. Le coût de partition C (E) ne dépend que des caractéristiques globales de l'ensemble d'apprentissage (nombre total d'individus, nombre total de valeurs des variables) et de la taille de la partition. Les coûts par partie C Prenons maintenant le cas de la discrétisation bivariée supervisée (formule 3) et montrons que le critère obtenu en fixant une des partitions se décompose de façon additive sur l'autre partition. En fixant par exemple la partition de la variable X 2 , nous obtenons
log log log log
Le nombre de parties I 2 étant fixé (au même titre que le nombre de valeurs endogènes J), le coût de partition C (E) ne dépend effectivement que des caractéristiques globales de l'ensemble d'apprentissage et de la taille de la partition I 1 . Les coûts par partie C (P) i ne dépendent que des caractéristiques locales de chaque partie. Nous pouvons alors réutiliser l'algorithme de discrétisation univariée pour optimiser la partition de la variable X 1 , la partition de X 2 étant fixée. La complexité algorithmique est en O(JI 2 Nlog(N)), le facteur I 2 J s'expliquant par le coût d'évaluation des termes pour chaque partie.
Les expérimentations montrent que l'algorithme procédant par optimisations univariées alternées converge extrêmement rapidement, rarement en plus de deux itérations, ce qui confère à l'algorithme une complexité en O(JN 3/2 log(N)). Il est à noter que le choix initial de O(N ½ ) partie par variable est un choix heuristique, visant à assurer un bon compromis entre finesse de la partition initiale et complexité algorithmique.
Expérimentation
Cette section présente des résultats d'expérimentation, permettant d'évaluer l'impact de la méthode de partitionnement bivarié supervisé sur les performances en classification.
Protocole
Les expérimentations sont menées en utilisant 30 jeux de données de l'UCI (Blake et Merz, 1996)  Afin d'évaluer la méthode d'analyse bivariée intrinsèquement, on introduit un nouveau type de classifieur appelé BestBivariate (B2). Ce classifieur recherche d'abord la meilleure paire de variable, celle qui maximise la probabilité que son modèle de partitionnement en grille explique la variable endogène. Pour classifier un individu en test, on recherche la cellule exogène associée aux valeurs de l'individu pour la paire de variables, et on prédit la valeur endogène majoritaire de la cellule (d'après les effectifs collectés en apprentissage). Si cette cellule était vide en apprentissage, on prédit la valeur endogène majoritaire de l'ensemble d'apprentissage. On évalue également le classifieur BestUnivariate (B1) qui procède de la même façon à partir de l'analyse univariée, et on rappelle pour référence le taux de bonne prédiction du classifieur majoritaire (M).
Afin d'évaluer l'impact de la méthode sur un classifieur multivarié, on évalue le classifieur Bayesien naïf (Langley et al, 1992), basé sur les prétraitements univariés (NB1) ou bivariés (NB2). On utilise également l'amélioration de ce classifieur (SNB1 et SNB2) décrite dans (Boullé, 2006b), qui incorpore d'une part une méthode régularisée de sélection de variables et d'autre part une agrégation de modèles, aboutissant à une pondération des variables 1 . L'exploitation de l'analyse bivariée est ici rudimentaire, en considérant chaque partitionnement bivarié comme une nouvelle variable calculée enrichissant l'espace de représentation.
Le taux de bonne prédiction en test est évalué au moyen d'une validation croisée stratifiée à 10 niveaux. Les différences significatives sont évaluées au seuil de 95% au moyen d'un test de Student.
Résultats
Les résultats d'évaluation sont résumés de façon synthétique dans le tableau 2, en reportant pour chaque méthode la moyenne arithmétique de son taux de bonne prédiction en test sur les 30 jeux de données de l'UCI. Le nombre de différences significatives vis-à-vis du classifieur SNB2 est également présenté, ainsi que le rang moyen de chaque méthode. On constate que le classifieur basé sur une seule variable (B1) est aussi performant que le meilleur classifieur multivarié (SNB2) dans environ un quart des cas (pas de différence significative dans 7 cas sur 30), et que celui basé sur deux variables uniquement (B2) atteint la meilleure performance obtenue dans un tiers des cas (10 cas sur 30). La figure 4 analyse plus finement les apports prédictifs du meilleur classifieur univarié (B1), du meilleur classifieur bivarié (B2) et du classifieur Bayesien naïf (NB1), avec comme référence le classifieur majoritaire (M). Le classifieur bivarié est systématiquement meilleur que le classifieur univarié, ce qui confirme la capacité de la méthode d'évaluation bivariée à correctement sélectionner une paire de variable performante. Il est toutefois dominé de façon significative par le classifieur Bayesien naïf, qui exploite l'ensemble de toutes les variables. 
FIG. 4 -Différence du taux de bonne prédiction par rapport au classifieur majoritaire (M) pour les classifieurs BestUnivariate (B1), BestBivariate (B2) et Bayesien naïf (NB1).
La figure 5 analyse l'apport prédictif de la prise en compte de toutes les paires de variable (NB2) et de la sélection de variables (SNB1 et SNB2), en prenant pour référence le classifieur multivarié le plus simple (NB1). L'utilisation de paires de variables agrandit l'espace de représentation, ce qui permet potentiellement de détecter de nouvelles informations prédicti-ves. En revanche, les informations redondantes déjà présentes en univarié sont ici multipliées, ce qui éloigne la représentation des données de l'hypothèse d'indépendance des variables sur laquelle repose le classifieur Bayesien naïf.
La figure 5 montre que ces deux phénomènes sont constatés sur les jeux de données de l'expérimentation quand on prend en compte les paires dans le classifieur NB2, avec de fortes dégradations de performances sur les jeux de données 1, 2, 6, 9, 22, 26, et de fortes améliora-tions sur les jeux de données 16, 18, 20, 27. La méthode de sélection de variables (Boullé, 2006b) utilisée dans SNB1 confirme son apport systématique, mais faible par rapport au classifieur Bayesien naïf NB1. Conjuguée à l'utilisation des paires de variables, le gain de performance devient à la fois important, avec une amélioration moyenne de 2.5% (15% sur Letter), et significatif, avec 14 victoires significatives pour 0 défaites. 
Conclusion
La méthode d'évaluation bivariée supervisée présentée dans cet article se base sur un modèle de partitionnement (en intervalles ou groupes de valeurs) de chaque variable exogène, ce qui induit une partition bivariée. Cette partition bivariée permet de qualifier l'information apportée conjointement par les deux variables exogènes sur la variable endogène. Cette information est quantifiée au moyen d'une approche Bayesienne. Le critère d'évaluation obtenu est optimisé au moyen d'une heuristique gloutonne en alternant les améliorations partielles par variables.
Des évaluations intensives sur 30 jeux de données de l'UCI démontrent que le critère permet de sélectionner des paires de variables fortement informatives. L'ajout des paires de variables dans un classifieur Bayesien naïf n'est pas concluant en moyenne, les interactions constructives entre variables étant compensées par la redondance accrue de la représentation des données. En revanche, couplée avec une méthode performante de sélection de variables, la prise en compte des paires de variables apporte une amélioration systématique des performances, significative dans la moitié des cas.
De travaux futurs sont envisagés pour limiter le nombre de paires à évaluer et pour adapter les méthodes de classification à une prise en compte efficace des prétraitements bivariés.

Introduction
Le problème de classification automatique (clustering) est considéré comme une des problématiques majeures en extraction des connaissances à partir de données. Parmi les techniques de classification, la classification floue (fuzzy) via Fuzzy C-Means (FCM) est très connue. FCM a été introduite par Jim Bezdek en 1981(Bezdek (1981) comme une amélioration des méthodes clustering précédentes, et a été beaucoup développée dans les années 90. Cette approche a été appliquée avec succès dans plusieurs problèmes (diagnostic médical (Whitwell (2005)) , classification de textes (Rodrigues and Sacks (2004))), et est de plus en plus utilisé dans le domaine du data mining.
Dans un travail récent (Le Thi et al. 3 (2006)) nous avons formulé le modèle de FCM pour la classification floue sous la forme d'un programme DC (Difference of Convexe functions) et développé un schéma de DCA (DC Algorithm) pour sa résolution numérique. La programmation DC et DCA ont été introduits par Pham Dinh Tao en 1985 et intensivement développés par Le Thi Hoai An et Pham Dinh Tao depuis 1994(voir (Le Thi Hoai An (1997) - (Le Thi et al. 2 (2006)), ), (Pham Dinh Tao and Le Thi Hoai An (1998)) et leurs références) pour devenir maintenant classiques et de plus en plus populaire. Ils ont été appliqués avec succès à nombreux problèmes d'optimisation non convexe différentiable ou non de grande dimension dans différents domaines des sciences appliquées, en particulier aux problèmes du data mining (voir par exemple (Le Thi et al. 1 (2006)), (Le Thi et al. 2 (2006)), (Liu et al (2003)), (Neumann et al. (2004)), (Weber et al. (2005))). Les résultats numériques présentés dans (Le Thi et al. 3 (2006)) montrent que, comme pour les autres problèmes déjà traités en data mining, DCA est efficace pour FCM. Ils prouvent également la superiorité de DCA par rapport à K-means. Cet algorithme est itératif et consiste en la résolution d'un programme convexe à chaque itération. Le temps de calculs de DCA est donc proportionnel à celui de la méthode utilisée pour résoudre les programmes convexes générés. Dans (Le Thi et al. 3 (2006)) l'algorithme du gradient pro-jeté a été utilisé du fait que la projection en question est explicite, même s'il est connu pour être lent. Sans doute qu'avec d'autres décompositions DC on peut améliorer DCA pour la résolution de FCM. L'objectif de ce travail est de développer un nouveau schéma de DCA dans lequel la résolution des problèmes convexes générés est moins coûteux. Nous proposons une autre décomposition DC qui donne naissance à un DCA très simple dont les calculs sont explicites à chaque itération : le sous problème convexe est en fait la projection d'un point sur une boule. Les expériences numériques comparatives entre FCM et l'algorithme DCA étudié dans (Le Thi et al. 3 (2006)) sur les données réelles montrent la robustesse, la performance de cette nouvelle version de DCA et sa supériorité par rapport à FCM.
Le papier est organisé de la façon suivante. Dans la deuxième section, nous présentons la formulation du problème FCM. La résolution de ce probème par la programmation DC et DCA est étudiée dans la troisième section. Finalement, les résultats numériques de nos algorithmes DCA et FCM sont rapportés dans la dernière section.
Une nouvelle formulation du modèle de FCM
Soit X := {x 1 , x 2 , ..., x n } l'ensemble de n points à classer. Chaque point x i est un vecteur dans l'espace IR p . Nous avons à classer ces n points dans c (2 ? c ? n) classes différentes. Considérons une matrice de pourcentage U de taille (c × n) dont chaque élément u i,k définit le pourcentage d'appartenance d'un point x k à la classe C i . Il est clair que
Si la matrice de pourcentage U est déterminée, on en déduit la classification selon la règle suivante : le point x k (pour k = 1, . . ., n) est classé dans la classe C i (pour i = 1, . . ., c) si et seulement si u i,k = max{u j,k : j ? {1, . . ., c}}.
Considérons la fonction J m définie par :
où . désigne, dans tout le papier, la norme Euclidienne de l'espace correspondant, V est une (c × p) -matrice dont chaque ligne v i correspond au centre de la classe C i , et m ? 1 un paramètre entier qui définit le degré de flou du modèle. Chercher une classification revient ainsi chercher la matrice de pourcentage U et les centres v i . Le modèle mathématique de FCM s'écrit ainsi :
où seule la variable U est a priori bornée. En fait on peut aussi restreindre la variable V à un domaine borné. En effet, la condition nécessaire d'optimalité du premier ordre en
Considérons les nouvelles variables
Ce dernier est un problème d'optimisation non convexe dont la résolution sera décrite dans la suite. 
La programmation DC et DCA pour la résolution de FCM
Pour faciliter la compréhension de notre approche, nous présentons, en premier lieu de cette section, une brève description de la programmation DC et DCA.
Introduction à la programmation DC et DCA
La programmation DC joue un rôle central en programmation non convexe (différen-tiable ou non) car la quasi totalité des problèmes d'optimisation de la vie courante est de nature DC. Elle connaît des développements spectaculaires au cours de cette dernière décen-nie. DCA est une méthode de descente (de type primal-dual sans recherche linéaire) pour la résolution d'un programme DC de la forme
où g, h sont les fonctions convexes semi-continues inférieurement et propres sur IR p . Une telle fonction f est appelée fonction DC, et les fonctions convexes g et h les composantes DC de f. Il est à noter que la minimisation d'une fonction DC sur un ensemble convexe fermé C de IR p se ramène à un problème de type (5) car la contrainte x ? C peut être incorporée dans la fonction objectif à l'aide de la fonction indicatrice ? C définie par ? C = 0 si x ? C, +? sinon. Lorsqu'une de ses composantes DC est polyédrale la fonction f est dite DC polyédrale et le programme DC correspondant DC polyédral. La programmation DC polyédrale joue un rôle crucial en programmation non convexe.
La congugaison d'une fonction convexe g, notée g * est définie par
La dualité DC est définie via la conjugaison des composantes DC et le programme dual de (5) est donné par (ici l'espace dual de IR p est identifié à lui-même) :
Puisque chaque fonction h ? ? 0 (IR p ) est caractérisée comme le supremum d'une famille finie des fonctions affines, c.à.d.
Il est clair que (P y ) est un programme convexe et
Par suite ? = inf{h
Finallement on obtient, avec la convention naturelle +? ? (+?) = +? :
On observe ainsi la symétrie parfaite entre les programmes DC primal et dual : le dual de (6) est exactement (5). Le transport des solutions optimales globales entre l'ensemble des solutions optimales P de (5) et celui de (6) noté D s'exprime de la manière suivante ((Le Thi Hoai An and Pham Dinh Tao (2005)), )) :
La relation (8)  
L'égalité des valeurs optimales des programmes primal et dual (5) et (6) peut être traduite de manière équivalente par 
(Un tel point x * vérifiant (10) est appelé point critique de g ? h). La condition nécessaire d'optimalité locale (9) est également suffisante dans plusieurs cas rencontrés en pratique -par exemple, quand la fonction objectif f := g ?h est DC polyé-drale avec h polyédrale, ou quand f est localement convexe en x * . Basé sur les conditions d'optimalité locale et la dualité DC, DCA consiste en la construction de deux suites {x k } et {y k }, candidats respectifs aux solutions des problèmes primal et dual que l'on améliore à chaque itération (les deux suites {g( 
La première interprétation de DCA est simple : à chaque itération on remplace dans le programme DC primal la deuxième composante DC h par sa minorante affine
dont l'ensemble des solutions optimales n'est autre que ?g * (y k ). De manière analogue, la deuxième composante DC g * du programme DC dual (6) est remplacée par sa minorante affine (g * ) k (y) := g * (y k ) + y ? y k , x k+1 au voisinage de y k pour donner naissance au programme convexe
dont ?h(x k+1 ) est l'ensemble des solutions optimales. DCA opère ainsi une double linéari-sation à l'aide des sous-gradients de h et g * . Il est à noter que DCA travaille avec les composantes DC g et h et non pas avec la fonction f elle-même. Chaque décomposition DC de f donne naissance à un DCA. Pour un programme DC donné, la question de décomposition DC optimale reste ouverte, en pratique on cherche des décompositions DC bien adaptées à la structure spécifiques du programme DC étudié pour lesquelles les suites {x k } et {y k } sont faciles à calculer, (si possible) explicites pour que les DCA correspondants soient moins coûteux en temps et par conséquent capables de supporter de très grandes dimensions.
La convergence de DCA : ((Le Thi Hoai An and Pham Dinh Tao (1997)), (Le Thi Hoai An and Pham Dinh Tao (2005)), (Pham Dinh Tao and Le Thi Hoai An (1997)), (Pham Dinh Tao and Le Thi Hoai An (1998)), (Le Thi Hoai An and Pham Dinh Tao (2003)))
Soient C (resp. D) l'ensemble convexe qui contient la suite {x k } (resp. {y
DCA est une méthodes de descente sans recherche linéaire, qui possède les propriétés suivantes : i) Les suites {g(
Dans ce cas DCA se termine à l'itération k (convergence finie de DCA).
• h
Dans ce cas DCA se termine à l'itération k (convergence finie de DCA).
(resp. {{y k+1 ? y k 2 } converge. iii) Si la valeur optimale ? du problème (5) est finie et deux suites {x k } et {y k } sont bornées alors tout valeur d'adhérence x (resp. y) de la suite {x k } (resp. {y k }) est le point critique de g ? h (resp. h * ? g * ). iv) DCA a la convergence linéaire pour les programmes DC généraux. v) DCA a la convergence finie pour les programmes DC polyédraux.
Pour une étude complète de la programmation DC et DCA, se référer aux (Le Thi Hoai An (1997)) -(Le Thi Hoai An and Pham Dinh Tao (2005)), ), (Pham Dinh Tao and Le Thi Hoai An (1998) 
Nouvelle formulation DC de FCM
Dans toute la suite nous utilisons la présentation matricielle qui nous semble plus comode, sachant que l'on peut identifier une matrice et un vecteur (par ligne ou par colonne). La fonction objectif de (4) peut s'écrire de la manière suivante :
Pour tout (T, V ) ? S × C on a
Dans le lemme suivant nous donnerons les conditions pour que la fonction H soit convexe.
Lemme : soit B := ? n k=1 B k , où B k est la boule de centre 0 et de rayon 1 dans IR c . La fonction H(U, V ) est convexe sur B × C pour toute valeur de ? telle que
H est convexe si toutes les fonctions
Considérons la fonction suivante :
Le Hessien de la fonction f est donné par :
Pour tout (x, y) : 0 ? x ? 1; y ? ?, on a :
alors | J(x, y) |? 0, pour tout (x, y) ? IR 2 tels que 0 ? x ? 1, | y |? ?. Ainsi avec ? défini par (17), la fonction f est convexe sur [0, 1] × [??, ?]. Par conséquent les fonctions
Il en est de même pour les fonction h i,k , car
Ainsi, avec les valeurs données ci-dessus de ? et ?, la fonction H(T, V ) est convexe sur B × C. Dans toute la suite nous travaillons avec ces valeurs de ? et ?.
Il est clair que, pour tout T ? B et un V ? C fixé, la fonction J 2m (T, V ) est concave en variable T (car H(T, V ) est convexe), par suite son minimum sur B est atteint sur la frontière S de B,i.e., min
Le problème (4) peut être alors reformulé comme
qui est un programme DC avec la décomposition DC suivante :
est bien évidemment une fonction convexe grâce à la convexité de B et C.
Résolution de (18) par DCA
Selon la description de DCA dans la section 2.1, la résolution de FCM via la formulation (18) par DCA consiste en la détermination de deux suites (
se ramène à la résolution du problème suivant (voir Section 2.1)
Il s'en suit que (Proj étant l'application de projection)
Plus précisément :
3.3.1 Schéma DCA Initialisation :
Construction des classes
Expériences numériques
Pour comparer la performance de notre algorithme, nous avons réalisé les tests numériques sur deux ensembles des données : le premier ensemble de données contient 4 exemples très connus et beaucoup utilisés dans le domaine de classification pour l'évalua-tion des algorithmes :
-PAPILLON : un jeu de données connu sous le nom "jeux de papillon". Le deuxième ensemble de données est composé de deux jeux de données de biopuces : "Yeast" et "Serum" téléchargeables sur http ://genomics.stanford.edu/. "Yeast" : 2945 points (gènes) dans l'espace de dimension 15 ; "Serum" : 517 points dans l'espace de dimension 12 (voir (Dembele et al. (2003)) pour la description de ces données).
Les tests ont été réalisés sur un ordinateur de 2.8MHz, 512Mb Ram. La valeur de est fixée à 10 ?7 . La valeur de m est égale à 2 pour le premier ensemble de données. Pour les données de biopuces nous considérons différentes valeurs de m dans l'intervalle (1, 2) (il a été prouvé dans (Dembele et al. (2003)) que le choix de m = 2 n'est pas convenable à ces données).
Dans le Tableau 1, nous comparons notre nouvelle méthode DCA (DCA2) avec l'algorithme DCA développé dans (Le Thi et al. 3 (2006) Tableau 3 : Résultats comparatifs de données de biopuces "Serum".
Conclusion.
Nous avons introduit une nouvelle formulation DC du modèle de FCM pour la classification floue et développé un schéma de DCA pour sa résolution numérique. Avec cette décomposition DC notre algorithme itératif est extrêmement simple, il consiste en la détermination de la projection d'un point sur une boule Euclidienne, qui est explicite et non coûteux. Les résultats numériques montrent que, comme pour les autres problèmes déjà traités en data mining, DCA est efficace pour FCM. Ils prouvent la superiorité de DCA non seulement par rapport à l'algorithme FCM standard mais aussi par rapport au schéma de DCA proposé dans (Le Thi et al. 3 (2006)). Dans l'étape suivante nous devront exploiter cet algorithme pour la classification des données biologiques, en particulier des donnéées de biopuces.

Introduction
Depuis les travaux de Agrawal et al., (1993) les règles d'association ont été un modèle très utilisé pour extraire des tendances implicatives dans des bases de données. Rappelons que lorsqu'on dispose d'un ensemble E d'individus décrits par p variables {a, b, ….}, qui peuvent être des conjonctions de variables atomiques et que l'on supposera ici binaires, une règle d'association a ? b signifie que si a est vérifiée alors généralement b l'est également.
Lorsque l'on extrait un ensemble de telles règles partielles d'association, il est pertinent de s'interroger sur les « relations » que ces règles entretiennent entre elles. Cette question a été abordée dans la littérature selon différents points de vue. Dans une optique de structuration de l'ensemble des règles, différentes méthodes de classification ont été proposées (e.g. Lent et al., 1997 ;Gras et Kuntz, 2005). Des représentations visuelles bien adaptées permettent également de mettre en évidence des dépendances entre les règles (e.g. Lehn, 2000ou Couturier et Gras, 2005. Remarquons que des travaux antérieurs (Suzuki et Kodratoff, 1999 ;Suzuki et Zytkow, 2005)  Or, il existe, comme nous le verrons en donnant des exemples, des situations naturelles où un caractère exceptionnel associe les trois variables. Pour le prendre en compte et en étudier un modèle, nous étendons ici le sens précédent en accentuant ainsi le caractère surprenant (d'exception) d'une règle dérivée de deux règles simples.
Pour illustrer ce type de règle, nous faisons référence tout d'abord au cas de l'incompatibilité de groupes sanguins en ce qui concerne le facteur Rhésus.
Certaines femmes, non primo-parturientes, dont les globules rouges sont porteurs de deux allèles Rh-et dont l'immunisation anti-Rh+ est active, possèdent alors le phénotype Rh-(caractère a). Quel que soit le père en général, l'enfant qu'elles portent ne présentera pas, à la naissance, de problème sur le plan sanguin (caractère c). Nous sommes en présence de la règle : a ? c.
Un homme, de génotype Rh+ et Rh+, possède le phénotype Rh+ (caractère b). Quelle que soit la mère en général, l'enfant qu'il engendrera n'aura pas de problème à sa naissance (caractère c). C'est la situation où la règle b ? c est valide.
En revanche, un couple où la femme est Rh-et remplit les conditions a et l'homme est Rh+ (caractère b) pourra donner naissance à un enfant qui présentera un risque important du fait de l'incompatibilité Rhésus (caractère non c). Dans des cas exceptionnels, en effet, la mère s'immunisant contre le facteur Rh du foetus, fabrique des anticorps, qui détruisent les globules rouges de l'enfant. Même si la conjugaison des caractères a et b est rare, on rencontre cependant la réalisation de la règle, que nous avons appelée « règle d'exception », (a et b) ? non c. On sait d'ailleurs que des précautions sont prises pour éviter ce problème dès que sont connus les phénotypes des parents à la faveur d'une prévention adaptée (par exemple l'exsanguino-transfusion).
On trouve une situation comparable d'apparition de règle d'exception dans l'étude des phénomènes d'interférences lumineuses, par exemple dans l'expérience classique des franges de Young (Bruhat G., 1959). La même source lumineuse franchissant deux fentes identiques n ? ] sera proche de 1 : autrement dit, en général, on observe plus de contreexemples dans des circonstances aléatoires que l'on en a observés dans la contingence. Dans ce cas, le seul hasard conduit donc, en moyenne, à plus de contre-exemples que ce qui est observé.
Les parties hachurées correspondent aux contreexemples observés ou  (Gras, 1979 ;Lerman, 1981a et Lerman et al. 1981. On centre et on réduit cette variable en la variable Q(a, b ) ; l'observation contingente, sa réalisation, est q(a, b ). Par exemple, dans le cas de Poisson, on obtient :  La deuxième approche est basée sur l'extension, que nous avons proposée, des règles en R-règles (règles de règles) de type R? R', où R et R' sont elles-mêmes des règles (Gras et Kuntz, 2005). Intuitivement, ces règles sont comparables à celles qui apparaissent en mathématiques où un théorème R a pour conséquence un autre théorème R' ou est suivi d'un RNTI -X -corollaire R'. Elles sont construites selon un algorithme récursif utilisant un indice appelé « cohésion ». Celui-ci rend compte de la qualité des liaisons implicatives des variables de la règle R avec les variables de la règle R'.
Rappelons, qu'en logique formelle, la règle de règle, ou R-règle, a ? (b ? non c), 
Exemple numérique
Nous avons construit un fichier fictif de 200 sujets (cf. un tableau partiel en Annexe où nous en montrons la construction), sujets sur lesquels nous observons les variables binaires : a, b , a ? b, c et non c. Les valeurs associées des différentes intensités sont données dans TAB 1. Elles sont obtenues par le logiciel CHIC (Couturier et Gras, 2005)   Une modélisation hypergéométrique est écartée car elle n'induit pas de différence entre une implication et sa réciproque (Gras et al., 1996b).
Etablissons pour chacun de ces deux modèles retenus les intensités d'implication de la conjonction a ? b sur les variables c et non c (encore notée c ). Nous utiliserons la relation simple :   
Modèle binomial
Conclusion
Lorsque deux variables impliquent une 3 ème , que leur conjonction implique plutôt la négation de cette 3 ème , nous considérons que cette règle est d'exception, en un sens voisin mais différent de celui de E. Suzuki et Y.Kodratoff (1999). Nous avons étudié et illustré par un exemple numérique et un exemple de génétique, l'expression de ce caractère exceptionnel. Puis nous avons précisé les relations entre les paramètres des variables dans les deux modélisations selon lesquelles est construite l'Analyse Statistique Implicative : un modèle de Poisson et un modèle binomial, l'un et l'autre convergeant vers le même modèle gaussien.
RNTI -X -

Introduction
La Catégorisation de Textes (C.T) consiste à assigner une ou plusieurs catégories parmi une liste prédéfinie à un document. En d'autres termes, elle permet de chercher une liaison fonctionnelle entre un ensemble de textes et un ensemble de catégories (Sebastiani (2002)). La grande importance accordée cette dernière décennie au traitement des données multilingues, a donné naissance à un nouveau domaine de recherche. C'est la catégorisation de textes multilingues.
Dans cet article, nous allons proposer une nouvelle approche qui consiste à étendre l'utilisation de WordNet en C.T pour catégoriser des documents provenant de différentes langues. L'approche proposée est basée sur la traduction des documents à catégoriser vers la langue de Shakespeare afin de pouvoir bénéficier de l'utilisation de WordNet par la suite. Cette hybridation entre l'utilisation des techniques de traduction et l'utilisation de WordNet offre les avantages suivants: -Sans l'utilisation des techniques de traduction, il devient nécessaire de construire une ontologie WordNet pour chaque langue. Cette construction est très coûteuse en terme de temps et personnels.
-L'utilisation d'une ontologie bien construite et riche tel que WordNet permet de corriger certains erreurs de traduction en utilisant des relations tel que l'hypéronymie et la synonymie (Cruse (1986)).
L'approche que nous proposons dans cet article se compose de deux phases. La première phase est la phase d'apprentissage, elle consiste à:
-Utiliser WordNet pour mapper les mots en synsets; -Enrichir l'espace de représentation par l'extraction des hypéronymes ; -Utiliser la méthode ? 2 multivariée (Clech et al. (2003)) pour sélectionner les synsets caractérisant chaque catégories par rapport aux autres catégories. Les synsets sélectionnés pour chaque catégorie forment son profil conceptuel. La deuxième phase est celle de la classification, elle consiste à assigner le texte à catégo-riser à une ou plusieurs catégories en se basant sur les profils conceptuels déjà trouvés dans la première phase. Cela nécessite les étapes suivantes:
-Traduire le texte à catégoriser vers la langue anglaise afin de pouvoir utiliser WordNet pour créer le vecteur conceptuel ; -Pondérer les profils conceptuels des catégories ainsi que le vecteur conceptuel du texte à catégoriser ; -Calculer la distance entre le vecteur conceptuel du texte à catégoriser avec les profils conceptuels des catégories ;
Expérimentations
Afin de pouvoir montrer l'utilité de l'utilisation de WordNet en catégorisation multilingue, nous avons testé l'approche proposée sur le corpus monolingue Reuters21578 1 , ainsi que sur le corpus bilingue ILO. Les résultats des expérimentations ont montré que les performances obtenues sur les deux corpus sont très proches. Ce rapprochement dans les résulats nous mènent à confirmer que l'utilisation de WordNet en catégorisation multilingue est une piste prometteuse.

Introduction
On veut montrer brièvement les divers degrés d'exigence (vis-à-vis des résultats) que l'on peut avoir lorsque l'on procède à une analyse en axes principaux. Ces degrés correspondent à des modalités d'usage du bootstrap (Diaconis et Efron, 1983;Efron et Tibshirani, 1993). On examinera successivement le bootstrap partiel (section 2), trois types de bootstrap dit total (section 3), d'autres formes plus spécifiques de bootstrap (section 4). On revient ensuite sur les subtilités du bootstrap total de type 3 (section 5). On illustrera ces propos par une étape de travail extraite d'une analyse en composante principales (ACP).
Bootstrap partiel
Les axes principaux calculés à partir des données originales, non perturbées, jouent un rôle privilégié (en ACP, par exemple, la matrice des corrélations initiale C est en effet l'espérance mathématique des matrices C k « perturbées » par la réplication k). Pourquoi calculer des sous-espaces de représentation prenant en compte des perturbations, et donc moins exacts que le sous-espace calculé sur les données initiales? La variabilité bootstrap s'observe mieux sur le repère fixe initial, non perturbé. C'est l'option qui sera prise dans la suite de cet article. La technique de bootstrap que l'on appellera bootstrap partiel (sans recalcul des valeurs propres) proposée notamment par Greenacre (1984) dans le cadre de l'analyse des correspondances, répond à plusieurs des préoccupations des utilisateurs dans le cas de l'analyse en composantes principales 1 . Une réplication consiste en un tirage avec remise des n individus (vecteurs-observations), suivi du positionnement des p nouvelles variables ainsi obtenues en "variables supplémentaires" sur les q premiers axes de l'analyse de base. Les procédures décrites ci-dessus peuvent être mises en oeuvre avec un programme classique de projection d'éléments supplémentaires. On calcule donc les réplications de ce coefficient, ce qui revient à repondérer les individus avec les "poids Bootstrap" 0, 1, 2, ... qui caractérisent un tirage sans remise. On obtient, comme sous-produit, des réplications de la variance sur l'axe, qui sont évidemment distinctes de ce que seraient des réplications des valeurs propres.
Les s réplications étant projetées sur un repère commun (celui de l'analyse initiale), on caractérisera graphiquement la dispersion des réplications d'une variable donnée soit par l'enveloppe convexe de l'ensemble de ses réplications, soit par un ellipsoïde d'ajustement du nuage des réplications, qui résultera en fait d'une petite analyse en composantes principales de ce dernier nuage. L'enveloppe convexe a l'avantage de l'exhaustivité (toutes les réplications sans exception sont enveloppées), l'ellipsoïde a l'avantage de prendre en compte la densité du nuage des réplications, et d'être moins sensible à d'éventuelles rares réplications aberrantes.
Bootstrap total
Le bootstrap total consiste à réaliser autant d'analyses en axes principaux qu'il y a de réplications. Mais le système d'axes n'est plus le même d'une analyse à une autre. Il peut y avoir des changements de signes (les axes factoriels sont définis aux signes près), des interversions d'axes, des rotations d'axes 2 . Il faut donc procéder à une série de transformations afin de retrouver des axes homologues au cours des diagonalisations successives des s matrices de corrélation répliquées C k (C k correspond à la k-ème réplication).
Bootstrap total de type 1
C'est une épreuve sévère, très pessimiste : simple changement (éventuel) de signes des axes homologues pour les réplications. Il s'agit seulement de remédier au fait que les axes sont définis au signe près. Un simple produit scalaire entre axes originaux et axes répliqués de mêmes rangs permet de rectifier le signe de ces derniers. Le bootstrap total de type 1 ignore les possibles interversions d'axes et rotations d'axes. Il permet de valider des structures stables et robustes. Chaque réplication doit produire les axes initiaux avec les mêmes rangs (ordre des valeurs propres).
Bootstrap total de type 2
C'est une épreuve assez sévère, plutôt pessimiste : il y a changements de signes et éventuellement correction des interversions d'axes. Les axes répliqués sont affectés (séquentiellement, sans remise en cause d'affectations antérieures) du rang des axes originaux avec lesquels ils sont les plus corrélés en valeur absolue. Puis on procède à un éventuel changement de signe des axes, comme en bootstrap de type 1. Le bootstrap total de type 2 est idéal si on veut valider des axes, c'est-à-dire des dimensions cachées, sans attacher une importance particulière aux rangs de celles-ci.
Bootstrap total de type 3
3 C'est une épreuve plutôt laxiste si on s'intéresse à la stabilité des axes, mais apte à décrire la stabilité des sous-espaces de dimension supérieure à 1: une rotation dite procrustéenne (cf. Gower et Dijksterhuis, 2004) permet de rapprocher de façon optimale les systèmes d'axes répliqués et les systèmes d'axes initiaux. Le bootstrap de type 3 permet de valider globalement un sous-espace engendré par les axes principaux correspondant aux premières valeurs propres. Comme le bootstrap partiel, le bootstrap total de type 3 peut être qualifié de laxiste par les utilisateurs qui s'intéressent à l'individualité des axes, et pas seulement aux sous-espaces engendrés par plusieurs axes consécutifs (cf. section 5).
Autres types de bootstrap
On va mentionner trois techniques plus spécifiques. Comme les techniques précitées, les méthodes évoquées dans cette section sont implémentées dans le logiciel DTM qui peut être librement téléchargé à partir du site www.lebart.org.
Le bootstrap sur variables
Cette procédure n'a de sens que si il existe un « univers des variables » pour lequel la notion de « tirage de variables » a un sens. Les variables sont par exemple des événements nombreux, des instants, des zones échantillonnées, où, comme dans le cas de l'exemple de la sémiométrie (Lebart et al., 2003), des mots. Pour tester la stabilité des structures vis-à-vis de l'ensemble des variables, nous proposons de répliquer l'ensemble des variables lui-même par la méthode du bootstrap total. Nous supposons ainsi implicitement que l'ensemble des variables actives constitue un échantillon de m variables extrait aléatoirement d'un ensemble de variables potentielles. Nous perturbons cet échantillon de variables selon les mêmes principes que le bootstrap sur individus. Pour chaque réplication, les variables non tirées participent à l'analyse de l'échantillon répliqué avec un poids infinitésimal (variables supplémentaires) ce qui permet d'obtenir des nuages de réplications pour chaque variable (et pour chaque observation) (exemples dans l'ouvrage cité).
Le bootstrap spécifique (ou hiérarchique)
Le bootstrap spécifique intervient notamment dans les analyses de données textuelles, dans le cas des questions ouvertes, par exemple, pour lesquelles il existe deux niveaux d'individus statistiques : les mots, qui sont les individus des tables de contingences lexicales, et les répondants, qui sont les individus statistiques classiques des enquêtes. Le tirage avec remise des occurrences de mots peut être remplacé par un tirage avec remise des répondants qui sont en fait des « grappes de mots ». Les données ainsi répliquées peuvent donner lieu aux bootstraps partiels ou totaux. Elles conduisent en général à des zones de confiances plus larges si les réponses sont de tailles très différentes, circonstance fréquente en pratique.
Le bootstrap partiel pour cartes auto-organisées
Le bootstrap partiel peut être appliqué à d'autres opérateurs projections que ceux des variables supplémentaires usuelles. Ainsi, l'analyse de contiguïté permet de trouver des sousespaces de projection les plus proches possibles d'une carte auto-organisée de Kohonen (respectant la topologie de la carte au sens de la variance locale), ce qui permet de représenter par des zones de confiance l'incertitude sur la position des points. La projection des points répliqués ne se fait plus sur un plan factoriel, mais un plan sur lequel se projette la carte auto-organisée de façon optimale (ce plan est optimal au sens de la variance locale, qui est une variance calculée seulement à l'intérieur des classes et entre classes contiguës sur la carte). On trouvera détails et exemples d'application dans Lebart (2006).
Précisions sur la validation procrustéenne
Revenons maintenant sur le bootstrap total de type 3 qui implique la forme la plus élaborée de traitement statistique des réplications. On doit dans ce cas fixer un paramètre t (t comme tolérance), qui est le nombre d'axes pour lesquels on s'autorise à effectuer une transformation procrustéenne. Si par exemple le sous-espace des dix premiers axes répliqués coïncide avec celui des dix premiers axes initiaux, on pourra trouver une rotation qui fera coïncider les axes (ce qui nous ramène dans cas au bootstrap partiel).
Notons X (p,q) le tableau des q coordonnées factorielles initiales pour chacune des p variables, et X k sa k-ième réplication. Si les lignes de X k , d'ordre (p,q), subissent toutes un une même rotation, X k est transformé en X k B où B (q, q) est une matrice orthogonale (rotation ou symétrie par rapport à l'origine). On cherchera à rendre minimale la somme des carrés S des écarts entre X et X k B, qui peut s'écrire : S = trace (X -X k B )' (X -X k B) L'analyse procrustéenne orthogonale implique alors, pour chaque réplication X k la décomposition aux valeurs singulières de X'X k et donc la diagonalisation de la matrice X' X k X k ' X.
Si l'on s'intéresse à la stabilité du sous-espace de dimension q' < q formé par les q' premières colonnes de X, on peut ne garder que les q' premières colonnes de X k , et chercher une matrice B(q',q') (test sévère). Mais on peut aussi tolérer que certains des q' premiers axes aillent s'égarer dans un sous espace plus grand, et donc garder t colonnes, avec q'< t <q.
Les figures 1 et 2 sont relatives aux données sémiométriques disponibles sur http://ses.enst.fr/lebart/ (rubrique : logiciel, exemple EXA08 de DTM). 
FIG. 1 -Ellipses de confiance de trois mots (données sémiométriques) dans le plan (1, 2), ajustant 30 réplications. Les plus grandes ellipses correspondent à des rotations procustéennes pour t = 3 axes, les ellipses internes pour t = 12 axes.
70 mots sont notés par 300 individus. La figure 1 montre ainsi la différence d'ampleur des zones de confiance pour trois mots (Désir, Livre, Perfection). Pour t = 12, les ellipses sont plus petites que pour t = 3. Autrement dit, on gagne en précision dans le plan (1, 2) si l'on se contente de la stabilité d'un espace à 12 dimensions (sur 70 au départ). Si l'on exige un espace à 3 dimensions, on doit se contenter d'une précision moindre. 
FIG. 2 -Evolution de la somme des variances des réplications (liée à la taille moyenne des ellipses de confiance) dans le plan (1, 2) en fonction de la dimension t du sous-espace dans lequel sont effectuées les rotations procrustéennes des réplications.
La figure 2 fait un bilan global de la variance totale interne des ellipses pour l'ensemble des 70 mots. L'augmentation de la précision avec t est confirmée, mais reste modérée.

Introduction
Dans cet article, nous proposons une technique de clustering dynamique de données évo-lutives. Cette problématique est née de l'objectif initial de nos travaux visant à permettre, au cours de l'exécution d'un système multi-agents, de détecter des groupes d'agents liés à des phénomènes d'auto-organisation. On se trouve donc face à un problème de clustering dynamique qui présente les deux particularités suivantes : le cardinal de l'ensemble de données à clusteriser n'est pas constant et des données déjà clusterisées peuvent être modifiées du fait de l'évolution des agents correspondants.
Cela peut entraîner des modifications ou des réorganisations de l'ensemble existant de clusters. Ainsi, une méthode de clustering dynamique est nécessaire afin d'adapter continuellement l'ensemble des clusters afin qu'ils reflètent le mieux possible l'état courant des données.
Travaux connexes
Il existe de nombreux travaux portant sur les techniques de clustering où l'ensemble des données à clusteriser n'est pas totalement connu dès le départ comme en clustering classique. On trouve en particulier dans cette catégorie les techniques de clustering de flux de données et de flux de données évolutifs. Malheureusement, ces algorithmes ne prennent pas en compte le fait que des données déjà clusterisées puissent elles aussi évoluer.
Les travaux les plus proches de notre problématique concernent un algorithme de clustering de données mobiles présenté dans [Li et al. (2004)] : un micro-clustering est effectué en enrichissant les données d'un vecteur vitesse. Cependant, dans un deuxième temps, l'algorithme k-means doit être utilisé pour regrouper les micro-clusters, ce qui oblige à donner un nombre de clusters attendu et à ce que ce nombre soit constant.
Notre approche
Les algorithmes fourmis de clustering semblent plus adaptés à la prise en compte de l'évo-lution des données. Ainsi, l'algorithme AntClass [Monmarché (2000)] associe successivement en quatre phases un algorithme de fourragement et l'algorithme k-means. Cette approche n'étant pas compatible avec l'aspect dynamique de notre problématique, nous avons décidé de ne garder que l'algorithme fourmi utilisé dans AntClass et d'associer une couche d'agents cluster aux fourmis, chaque tas créé par les fourmis étant encapsulé dans un agent cluster.
Un agent cluster évolue au fur et à mesure que les fourmis lui donnent ou lui prennent des données et que les données qu'il contient évoluent. Mais il peut également se développer en attaquant d'autres agents cluster avec lesquels il est en intersection. Les clusters sont alors mis à jour par fusion complète ou partielle lors de la fuite de l'agent agressé. Ceci permet de compenser la tendance des fourmis à construire trop de tas et de prendre en compte l'évolution des données.
Prototypes et expérimentations
Nous avons développé une plateforme de simulation qui, connectée à la plateforme de clustering dynamique, sert de support à nos expérimentations. Les deux plateformes ont été développées en JAVA en utilisant l'environnement de développement de SMA MADKIT. La plateforme de simulation permet de simuler l'évolution de populations d'agents. Une population est considérée comme étant un ensemble d'agents qui vont, a priori, évoluer de manière similaire. Une population doit donc être normalement perçue, au niveau du clustering, comme un cluster qui va évoluer au cours du temps.
L'analyse de nos résultats repose pour le moment sur trois types de courbes temporelles : nombre de données agrégées, nombre de clusters et pureté moyenne des clusters. De cette analyse, il ressort que les clusters détectés sont en général purs, mais un peu trop nombreux, et que quelques données restent non clusterisées. L'algorithme fonctionne également bien sur des données bruitées.
Conclusion et perspectives
Des résultats prometteurs nous incitent à poursuivre dans cette voie en améliorant un certain nombre de points, notamment pour mieux prendre en compte la dissociation de clusters. Nous comptons notamment prendre en compte la vitesse des données comme cela est fait dans [Li et al. (2004)]. Enfin, d'un point de vue implantation, nous travaillons actuellement sur une nouvelle version des plateformes permettant un meilleur contrôle sur l'activation des différents agents. À plus long terme, notre objectif est d'appliquer cette méthode à différents problèmes concernant notamment l'émergence dans les systèmes multi-agents. 
Summary
In this paper, a multi-agents algorithm for dynamic clustering is presented.

Introduction
B-Ontology est un projet de recherche appliquée dont l'objectif est de construire le prototype d'une application capable d'extraire et d'organiser de l'information biographique. Cette information sera exploitée dans le cadre du processus de rédaction d'une agence de presse. L'agence Belga diffuse quotidiennement plus de 250 dépêches en deux langues (français et néerlandais). Cette masse textuelle représente environ 70.000 mots par jour (25 millions de mots en un an) par langue. Dans ce projet, nous nous intéresserons aux informations qui concernent les personnes, les organisations et les événements dans lesquels elles interviennent. Le résultat est stocké dans un ensemble de données structurées facilement consultable. Des systèmes comparables existent déjà (NewsExplorer 1 , KIM 2 ) mais ne couvrent cependant pas toutes les fonctionnalités désirées ici et sont souvent uniquement adaptés aux textes en anglais.
La première partie exposera les méthodes d'extraction d'information. La deuxième s'attardera sur le choix de l'organisation des données. Une troisième partie, présentera une réalisation concrète, mais limitée, de la base de connaissances et quelques aspects de data mining.
Extraction d'information
Définitions des entités et du formalisme d'annotation
L'extraction d'information passe par l'annotation sémantique du texte. Cette tâche néces-site avant tout une bonne définition des types d'entités recherchées. On définit le concept d'entité de manière assez large, dépassant ainsi la conception habituelle de l'entité nommée présentée par Chinchor (1998). Dans le cas d'une personne ou d'une organisation, elle peut être constituée uniquement du nom (l'entité nommée au sens strict), mais peut également être accompagnée d'un ensemble d'informations complémentaires. Celles-ci, que l'on défi-nit comme des entités associées, sont souvent accolées à l'entité nommée proprement dite. En les regroupant, on obtient une entité complexe. Une nomencalture contenant les principaux types d'entités a été mise au point (voir tableau 1). Tous les codes d'entité (tableau 2) peuvent se retrouver imbriqués dans n'importe quel autre code. Certaines combinaisons ne seront cependant que peu ou jamais rencontrées en raison de la sémantique trop éloignée des codes en question. Le nombre de niveaux d'imbrication n'est pas limité. À un niveau donné, plusieurs éléments de code identique peuvent coexister.
Extraction par grammaires locales et transducteurs
L'approche privilégiée consiste en l'utilisation de méthodes linguistiques qui offrent des possibilités d'analyses très fines. Les principaux types de ressource d'extraction sont les transducteurs à états finis (ou graphes) et les dictionnaires électroniques. Les transducteurs constituent un formalisme simple permettant une description très précise des entités à extraire. Pour des raisons de modularité et de facilité dans la construction des graphes, l'extraction s'effectue en plusieurs phases successives (cascade). Le processus mis en oeuvre est décrit de manière plus approfondie dans Kevers (2006). L'objectif est de repérer et de catégoriser des entités de plus en plus complexes, telles que celle illustrée ci-dessous :  Enfin, nous désirons disposer d'une structure sur laquelle il est possible de réaliser des inférences. La masse de données pourra ainsi être enrichie grâce à la mise en commun des informations extraites et des connaissances de raisonnement incluses au système.
Ontologie et base de connaissances
Un choix : l'ontologie
L'ontologie, concept bien connu dans le domaine du web sémantique, correspond aux exigences exprimées ci-dessus. Elle est définie par Gruber (1993) comme an explicit and formal spécification of a conceptualization. La construction d'une ontologie revient donc à exprimer de manière formelle la perception que l'on a d'un domaine. Cette formalisation (ici de l'information biographique) sert ensuite de modèle pour le stockage de données réelles.
Dans une ontologie, les concepts s'organisent en classes et disposent de propriétés. Cellesci se rapportent à une autre classe ou à un type de données particulier. Les classes peuvent être organisées de manière hiérarchique et diverses restrictions peuvent être exprimées. Les données réelles qui actualisent les classes sont apellées des instances. L'ensemble des données qui remplissent l'ontologie constitue une base de connaissances. En ce qui concerne les capacités de raisonnement, les raisonneurs OWL (ou DL-reasoner) et les langages de règles constituent les principales possibilités. Il est important de souligner qu'en la matière, il n'existe pas réellement de solution standard. Beaucoup de langages et d'implémentations cohabitent, sans qu'aucun ne s'impose pour l'instant. Ce contexte induit un certain flou quant au choix de la technologie adéquate.
Choix technologiques pour l'implémentation
Les raisonneurs OWL disposent de capacités d'inférence qui sont essentiellement utilisées pour vérifier l'intégrité d'une ontologie ou d'une base de connaissances, ainsi que pour déter-miner si une classe est une sous-classe d'une autre classe (ce qui permet d'établir la hiérarchie des classes). Ils se basent sur la logique de description qui est un sous ensemble de la logique des prédicats. Citons entre autres Racer 5 , Pellet 6 , Hoolet 7 ou encore FaCT++ 8 . L'utilisation d'un langage de règles (aussi appelé règles de Horn) permet quant à elle d'effectuer des inférences plus complexes. SWRL (Horrocks et al., 2004) semble être la solution émergeante. Il s'agit d'un sous ensemble de la logique des prédicats, mais disjoint de la logique de description.
Enfin, l'implémentation de clients ou de modules aditionnels pour protégé peut se faire à l'aide du langage Java. Cela permet de décliner les clients sous diverses formes : en interface graphique «classique», en application web 9 ou encore en service web 10 .
Structure partielle de l'ontologie biographique
Un premier prototype partiel sert de base au système complet. La définition de l'ontologie commence par la spécification des classes ( figure 1, cadre A)  figure 1 (cadre B).
FIG. 1 -Liste des entités, définition de «Organization» et conditions pour «Employment».
Les conditions associées aux classes sont des conditions nécessaires (si une instance appartient à une classe, elle doit vérifier ces conditions) ou des conditions nécessaires et suffisantes (si une instance est membre d'une classe, elle doit remplir ces conditions ; si elle remplit les conditions elle est membre de la classe). Elles font appel à la restriction existentielle (? ; hasProperty some Class décrit les instances qui ont au moins une relation hasProperty avec une instance de Class) ou à la restriction universelle (? ; hasProperty only Class décrit les instances dont toutes les relations hasProperty s'appliquent uniquement à des instances de Class).
Pour Employment (figure 1, cadre C), il est spécifié que a) toute instance de Employment est aussi une instance de Event ; b) si une instance de Employment a une relation hasContext, elle porte nécessairement sur une instance des classes Organization ou Person ; c) si une instance de Employment a une relation hasFunction, elle porte nécessairement sur une instance de la classe Function ; d) une instance de Employment possède au moins une relation hasFunction ou une relation hasContext ; e) si une instance de Employment a une relation isEmploymentOf, elle porte nécessairement sur une instance de la classe Person ; f) pour qu'une instance soit membre de la classe Employment, il suffit qu'elle soit reliée à une (ou plusieurs) instance(s) de Person (et uniquement à cette classe) par la relation isEmploymentOf ET qu'elle possède soit au moins une relation hasFunction soit au moins une relation hasContext.  ( ?a, ?b) ? isSiblingOf( ?x, ?y). Cette règle crée une proriété isSiblingOf chez x (vers y) et y (vers x) si les conditions sont vérifiées.

Introduction
Motivée par de nombreux domaines d'applications (e.g. marketing web, analyses financières, détections d'anomalies dans les réseaux, traitements de données médicales), l'extraction de motifs séquentiels fréquents est un domaine de recherche très actif Mobasher et al. (2002); Ramirez et al. (2000); Lattner et al. (2005). Les travaux menés ces dernières années ont montré que toutes les approches qui visent à extraire l'ensemble des motifs séquentiels deviennent cependant inefficaces dès que le support minimal spécifié par l'utilisateur est trop bas ou lorsque les données sont fortement corrélées. En effet, dans ce cas, et plus encore que pour les itemsets, les recherches sont pénalisées par un espace de recherche trop important. Par exemple, avec i attributs (appelés aussi items), il y a potentiellement O(i k ) séquences fréquentes de taille k Zaki (2001). Pour essayer de gérer au mieux ces problèmes de complexités spatiale et temporelle, deux grandes tendances se distinguent à l'heure actuelle. Dans le premier cas, les propositions comme PrefixSPAN Pei et al. (2004) ou SPADE Zaki (2001) se basent sur de nouvelles structures de données et une génération de candidats efficace. Les approches de la seconde tendance considèrent l'extraction d'une représentation condensée Mannila et Toivonen (1996). Même si l'utilisation d'une représentation compacte a montré son intérêt dans le domaine de l'extraction d'itemsets, la complexité structurelle des motifs séquentiels fait qu'il existe cependant peu de travaux utilisant une représentation condensée dans ce contexte.
Ainsi, seuls Clospan Yan et al. (2003) et Bide Wang et Han (2004) ont abordé ce problème en cherchant à extraire des motifs clos. Le problème que nous cherchons à résoudre dans cet article est le suivant : Est-il possible de trouver une nouvelle représentation condensée pour répondre à la problématique de l'extraction de motifs séquentiels ? Notre objectif est d'établir les premières bases formelles pour calculer les bornes supérieures et inférieures de la valeur du support d'un motif en utilisant le principe de l'inclusion-exclusion Knuth (1973). Ce principe nous permet d'obtenir des règles de dérivations via lesquelles nous pouvons déduire le support d'une séquence sans avoir à compter son support dans la base de données. Nous montrons également que ces règles peuvent être utilisées pour construire une représentation condensée de certains types de motifs. Cet article est organisé de la manière suivante. La section 2 introduit les concepts liés aux motifs séquentiels ainsi que les notions formelles utilisées dans le reste de l'article. Nous discutons l'utilisation de règles de déductions dans la section 3. L'approche NDSP est introduite dans la section 4. La section 5 présente les premières expérimentations menées qui confirment l'inté-rêt de notre approche et en discute les limites. Dans la section 6 nous présentons les travaux connexes autour des représentations condensées et des motifs séquentiels. Enfin, la section 7 conclut et présente les principales perspectives associées à ce travail.
Concepts préliminaires
Dans cette section, nous définissons le problème d'extraction des motifs séquentiels initialement proposé par Srikant (1995); Srikant et Agrawal (1996) et nous introduisons la notion de S-Apparition (une notion similaire est proposée dans Calders et Goethals (2002)     Par manque de place, les preuves de ce lemme, des prochaines propositions et des théorèmes ne sont pas détaillés dans cet article. Le lecteur peut se référer à Raissi et Poncelet (2006). Le lemme 1 sera utilisé dans la prochaine section afin de générer les règles de déductions.
Pour toutes les autres séquences
Règles de déductions
Dans cette section, nous étendons la définition d'expression de support introduite dans Calders et Goethals (2002)    Calders et Goethals (2002)
Nous montrons maintenant comment générer l'intervalle minimal pour chaque séquence candidate en utilisant l'ensemble des expressions de support, c'est à dire utiliser toute l'information disponible dans l'ensemble des expressions de support. Pour cela, nous présentons les liens théoriques entre l'implication logique définie ci-dessus et un système d'équations linéaires. 
En utilisant ce type d'inégalités, nous pouvons définir un théorème permettant de déduire directement des règles sur les bornes de l'intervalle de support des séquences candidates S. Ceci est possible car les algorithmes construits niveaux par niveaux contiennent l'information Support(J) = s J pour chaque sous-séquence de S.
L'utilisation de la séquence théorique maximale SP n'est pas possible d'un point de vue pratique. Pour cela, nous limitons la portée de notre lemme à la séquence S uniquement en utilisant un principe de projection de séquence sur la base de données D. Contrairement au problème d'extraction d'itemsets, cette projection modifie le support des sous-séquences de S puisque certaines séquences peuvent être incomparables à S tout en ayant des sous-séquences en commun. -I(? S (C trans )) ? I(S) (élimination de tous les items de C trans qui ne sont pas dans S).
un ensemble de transactions d'un client est incomparable avec la séquence projetée, il est inchangé). La projection de la base de données
Lemme 3 Soit D une base de données, pour chaque sous-séquence
D'après le lemme 2, il existe une variable x S pour chaque sous-séquence S SP. Le lemme 3 permet de réduire considérablement le système d'équations Sys(S) associé à l'ensemble des expressions de support S. Donc, avec une projection sur la séquence S, nous pouvons restreindre les variables x X à uniquement celles dont X S. 
Pour résoudre Sys(S) nous séparons les coefficients en une matrice booléenne :
Si nous représentons chaque matrice par un symbole, Ax = S avec A une matrice de contraintes de taille n × n (n étant le cardinal de l'ensemble contenant S et toutes ses sousséquences), x un vecteur colonne avec n entrées et S un vecteur colonne avec n entrées, la solution générale en utilisant la méthode d'élimination de Gauss-Jordan (A ?1 .S) est :
X où ? J X est la valeur absolue de l'élément xj de la matrice inverse A ?1 . Soit s S un entier choisi arbitrairement, d'après le lemme 4 l'ensemble des supports d'expression S pour la séquence S est satisfiable si et seulement si il existe une solution entière au système d'équations Sys(S). Donc d'après (1), Sys(S) est satisfiable si et seulement si :
X (Notons que (2) est similaire à la formule d'inclusion-exclusion de Knuth (1973)). En isolant (?1) |S?X| ? S X .s S et s s de la somme nous avons :
Comme ?X J ? S, nous obtenons :
X Les règles déduites de (3) sont les bornes supérieures de la valeur du support de la séquence S et les règles déduites de (4) sont les bornes inférieures. Ces règles seront notées R X (S) comme dans Calders et Goethals (2002) et la borne, notée ? X (S) est définie telle que :
Nous pouvons alors, pour chaque séquence S, déduire des règles de chaque sous-séquence X S. Ces règles peuvent être utilisées pour définir un intervalle sur la valeur du support de S avec U S (respectivement L S ) la valeur minimale des bornes supérieures (respectivement la valeur maximale des bornes inférieures) et donc L s ? Support(S) ? U s .
Exemple 2 Considérons la base de données D suivante :
Remarquons pour cette séquence que nous pouvons directement inférer le support sans avoir à le compter. ( est égal à 2 (2 ? Support( ? 2).
Corollaire 1 Soit X S, la différence entre la borne ? 
L'extraction des motifs séquentiels est fortement limitée par son aspect combinatoire. Afin de résoudre ce problème, il est souvent nécessaire et plus efficace d'extraire un sous-ensemble de motifs contenant ou pouvant permettre l'extraction des mêmes informations que l'ensemble des motifs séquentiels. Les règles de déductions peuvent être utilisées afin de construire une nouvelle représentation condensée des motifs séquentiels. Si les règles permettent de déri-ver exactement la valeur du support (i.e. U S = L S ) d'une séquence S en utilisant ces sousséquences (cf. exemple 2 avec la séquence alors il n'est pas nécessaire de garder S. Dans ce cas, S est appelée une séquence dérivable, notée DS. De la même façon, les séquences non-dérivables, notées N DS, sont les séquences qui ne peuvent pas avoir leur support dérivé de manière exacte. Nous allons montrer que l'ensemble des N DS permet la construction de tout l'ensemble des motifs séquentiels.
Théorème 2 Soit S une séquence et soit ? un item, l'intervalle calculé par les règles de dé-rivations pour la valeur du support de la séquence
(respectivement 2? S?? X ) plus petit que l'intervalle calculé pour la valeur du support de la séquence S.
Corollaire 2 Monotonie
Soit X S une séquence. Si X est une DS, alors S est aussi une DS.
Preuve. Si X est une DS alors U X ? L X = 0. En utilisant le théorème 2 nous savons que :
L'algorithme NDSP
Dans cette section nous présentons notre approche afin de construire une représentation condensée des motifs séquentiels à partir des règles de déductions extraite du théorème 1 et du corollaire 2. L'avantage d'une représentation condensée est qu'elle est souvent plus petite que l'ensemble des motifs séquentiels, noté F , ce qui rend cette représentation adéquate dans le cadre d'extractions de motifs à partir de données fortement corrélées ou très denses. Les motifs séquentiels non-dérivables sont donc adéquats pour l'extraction de grands ensembles de motifs séquentiels qui ne pourraient pas être obtenus à l'aide d'algorithmes classiques. 
Preuve. Extension de Calders (2003). La preuve est construite par induction sur la taille de la séquence S (Raissi et Poncelet (2006)).
Dans notre approche, la valeur ? S X utilisée afin de calculer nos règles de déductions n'est pas extraite d'une matrice inversée, afin d'optimiser les calculs, mais calculée par la fonction :
Où ? est le nombre de sous-séquences de taille |S ? 1| tel que X S. Pour le cas spécial où J = S, Si X = {} alors ? S {} = 1.
Corollaire 3 Soit S une séquence régulière alors S est non-dérivable.
Preuve. Notons qu'une séquence régulière possède une unique sous-séquence X avec |S ? X| = 1. A partir de l'équation (5) 
Nous avons developpé un algorithme NDSP (Non-Derivable Sequential Patterns) qui permet de de construire la représentation condensée N DSF (D, ?). NDSP est un algorithme par niveau et est complétement indépendant de la structure de données utilisée pour la représen-tation des séquences. L'algorithme 1 est basé sur la stratégie classique du générer-élaguer et est divisé en 3 étapes distinctes : (i) la génération de candidats (ligne 1 et 15 avec la fonction CandidateGeneration()) ; (ii) Le comptage de support (ligne 5) ; (iii) Déterminer les séquences non-dérivables dans F level grâce à la fonction ComputeBounds() (ligne 8), les séquences dérivables sont élaguées ligne 11. Le processus s'arrête quand il n'y a plus de candidats générés.   
Un algorithme d'extraction de motifs séquentiels clos (représentation condensée), noté
ClosF : CloSpan Yan et al. (2003).
Les tests ont été faits sur plusieurs jeux de données synthétiques générés avec l'outil DatGen 1 qui est une extension de l'outil IBM QUEST. Les différentes caractéristiques des jeux de données sont représentées dans la figure 2. Les tests se concentrent principalement sur les performances au niveau de la représentation condensée. L'algorithme NDSP a été implementé en JAVA et utilise une structure de données arborescente pour le stockage des séquences et des supports. La figure 3 montre les résultats d'extraction et les performances pour les 2 premiers jeux de données. Pour 0.05 ? ? ? 0.3, NDSP dépasse CloSpan et largement PrefixSpan au niveau de la condensation des motifs séquentiels extraits. De plus, pour ? = 0.1, le nombre des motifs séquentiels non-dérivables décroît plus rapidement que les deux autres approches. L'extraction s'arrête pour l'algorithme NDSP au niveau de profondeur 6 alors que les deux autres algorithmes s'arrêtent à la profondeur 8. Pour CL1000TR10000IT500I40, NDSP teste beaucoup moins de séquences candidates que PrefixSpan ou CloSpan. De plus, le nombre de motifs séquentiels non-dérivables tend à décroître plus rapidement avec le jeu de données CL10000TR1000IT1000I10. En effet ceci est dû principalement à la taille des séquences puisque le jeu de données CL1000TR10000IT500I40 contient un ensemble de motifs séquen-tiels très long avec très peu d'items différents sachant que les séquences longues ont plus de chance d'être dérivables puisqu'elles contiennent plus d'informations. NDSP réalise donc un meilleur taux de compression avec des données denses contenant de longues séquences. Le reste des jeux de données est documenté dans Raissi et Poncelet (2006).
Etat de l'art
Ces dernières années, de nombreuses recherches se sont intéressées à des représentations condensées pour les itemsets. Les représentations concises les plus importantes sont les itemsets fréquents clos Boulicaut et Bykowski (2000) qui sont basés sur l'opérateur de fermeture sur le treillis. De nombreux algorithmes ont été développés comme CLOSET Pei et al. (2000) et CHARM Zaki et Hsiao (2002)  
Conclusion
Dans cet article, nous avons abordé la problématique des représentations condensées pour les motifs séquentiels. Nos contributions principales sont les suivantes : Premièrement, nous avons jetés les bases formelles pour une nouvelle représentation. Nous introduisons les concepts théoriques des motifs séquentiels non-dérivables et prouvons que les bornes sur la valeur du support d'une séquence peuvent être déduites à partir de règles. Celles-ci sont calculées grâce au principe d'inclusion-exclusion. A notre connaissance, ce travail est le premier travail à introduire une nouvelle représentation condensée autre que la représentation close des motifs sé-quentiels. Deuxièmement nous avons developpé un algorithme NDSP qui dépasse, en terme de taux de compressions, les algorithmes actuels d'extractions de motifs séquentiels et de motifs séquentiels clos. Ce travail offre de nombreuses perspectives. Tout d'abord, le lemme 4 doit être affiné afin de prouver la complétude et l'adéquation de notre méthode, permettant ainsi l'extraction de motifs non-dérivables sans avoir à tester les cohérences des règles et passer par l'actuelle étape de comptage. De plus, cette approche peut-être couplée à des algorithmes très efficaces comme SPADE ou PrefixSPAN. Ce couplage permettrait d'augmenter la vitesse

Introduction
L'Extraction de Connaissances dans les Données (ECD) a été proposée afin d'aider les utilisateurs à mieux comprendre et appréhender des quantités de données de plus en plus volumineuses. La recherche de règles d'association constitue une question centrale de l'ECD. La plupart des travaux se sont focalisés sur la tâche d'extraction de règles d'association alors que les aspects visualisation de ces règles et interaction avec l'utilisateur-expert sont très peu représentés. De manière générale, le nombre de règles générées croît de manière exponentielle avec la taille des données. En situation réelle, un expert n'a ni le temps, ni les capacités cognitives de traiter ces flots d'information. Pour l'aider à y faire face, différents travaux proposés dans la littérature tournent autour de deux axes complémentaires : la réduction du nombre de règles d'association extraites et le développement d'outils de visualisation interactive. Dans ce papier, nous focalisons notre intérêt sur les méthodes de visualisation.
Un état de l'art des différentes techniques de visualisation de règles d'association est dé-crit dans Couturier et Mephu-Nguifo (2007). La limitation commune qui en ressort est que lorsque le nombre de règles est élévé, l'interaction avec l'utilisateur devient difficile. Partant de ce constat, nous proposons ici une nouvelle approche effectuant un regroupement de règles d'association, et particulièrement de règles génériques (Bastide et al. (2000)), en exploitant une représentation en oeil de poisson couplée à une représentation textuelle, 2D ou 3D pour la visualisation de toutes les règles. La représentation en oeil de poisson, dont une première approche est développée dans Couturier et al. (2006), va permettre à l'utilisateur de se focaliser sur un sous-ensemble de règles. Notre approche intègre donc une phase d'extraction de bases génériques et une phase de visualisation en oeil de poisson de ces règles génériques. Ces différentes phases peuvent bien évidement être itérées, et permettent un couplage fort des phases de fouille et de post-traitement. Cette approche est implémenté via le prototype, CB-VAR (Clustered-based Visualizer of Association Rules) permettant de gérer simultanément l'extraction et la visualisation de règles dans le but de pouvoir traiter des quantités plus importantes.
Le reste du papier est organisé comme suit : dans la seconde section, nous présentons le prototype que nous avons développé pour la visualisation de clusters de règles. La troisième section est consacrée à la présentation des résultats de nos expérimentations. Enfin, dans la dernière section, nous donnons nos conclusions et perspectives.
Le prototype CBVAR (Clustered-based Visualizer of Association Rules)
Dans ce papier, nous attaquons le problème du nombre de règles à visualiser sur deux fronts différents. Tout d'abord, nous travaillons sur des bases génériques de règles d'association, constituant un ensemble générateur de toutes les règles d'association, tout en étant de taille très compacte. Grâce à ce choix, nous réduisons en amont du processus le nombre de règles à traiter. Ensuite, l'organisation de l'ensemble de règles à visualiser se fait grâce à un ensemble de clusters. Ceci permet de diminuer la charge cognitive et de mettre en place un dispositif de visualisation interactif et coopératif. En aval du processus, nous optimisons l'espace écran pour visualiser notre ensemble de règles. En nous basant sur ces deux points, nous proposons le prototype CBVAR implémenté en JAVA et qui fonctionne actuellement sous l'environnement UNIX 1 . Sa principale caractéristique est qu'il intègre un module d'extraction de règles génériques et un module de visualisation qui sont présentés ci-après.
Extraction de règles génériques
Afin d'obtenir nos clusters, nous utilisons différents outils existants qui sont regroupés au sein d'un script shell. L'extraction des règles d'association nécessite en entrée un fichier texte (dat, txt, etc.). Chaque ligne de ce fichier contient la liste des items composant un objet. Les itemsets fermés fréquents et leurs générateurs minimaux associés, ainsi que les règles génériques sont stockés dans un fichier (.txt) grâce à l'algorithme PRINCE (Hamrouni et al. (2005)). Ce dernier génère également un fichier XML (.xml) contenant les informations relatives au support minimum (minsup) et à la confiance minimum (minconf). Ce fichier respecte le standard PMML (Predictive Model Markup Language) dans le but de ne pas limiter la portée de notre prototype sur des applications utilisant des DTD spécifiques. En outre, un fichier (.gen) contenant le méta-contexte associé est généré par PRINCE. Cette étape est très coûteuse en temps de calcul. Pour cette raison, notre prototype peut réutiliser un méta-contexte existant spécifié par l'utilisateur. S'il n'en sélectionne aucun, il sera regénéré. Durant la dernière étape de l'extraction, un fichier (.clu) est généré contenant pour chaque règle, un cluster associé. Le nombre de cluster k est spécifié par l'utilisateur. Grâce à ce fichier et au fichier XML correspondant, k fichiers (.xml), correspondant aux k clusters sont générés avec en plus, un fichier (.cluxml) de méta-cluster listant ces k fichiers XML. Le paquetage contenant ces fichiers constitue le point d'entrée du module de visualisation de CBVAR. Un exemple est présenté dans la figure 1. Chacun des outils utilisés est un paramètre possible pour la génération des clusters. Il est tout à fait possible de les changer en modifiant le script shell associé. Ce script est exécuté par CBVAR en utilisant différents paramètres fixés par l'utilisateur : le support, la confiance, le méta-contexte (si nécessaire) et le nombre de clusters.
Visualisation de règles d'association : L'approche Fisheye View
Plusieurs représentations connues en Interface Homme Machine (IHM), ont été proposées pour représenter des informations abondantes (Couturier et al. (2006)). Parmi ces représenta-tions, la déformation en oeil de poisson (Fisheyes view (FEV)) présentée dans Furnas (1986) nous paraît la plus adaptée et une première approche appliquée à la visualisation de règles d'association a d'ailleurs été proposée dans Couturier et al. (2006). Dans ce travail, la repré-sentation exploite une matrice 2D et une FEV pour visualiser des règles d'association tel que chaque règle constitue un point d'intérêt. Ce principe est réutilisé dans notre approche à la différence que notre point d'intérêt sera un cluster (cf. Figure 2).
Nous couplons les approches par matrice 2D et 3D dans notre prototype. La représentation 2D va permettre d'obtenir une vue globale des règles, dans laquelle chaque case va représen-ter une règle de chaque cluster. Cette règle est sélectionnée en fonction des mesures d'intérêt comme le lift (par défaut), le support ou la confiance. Pour obtenir le détail d'un cluster, l'utilisateur va activer la FEV sur l'une des cases. Pour le représenter au niveau du centre d'intérêt de la FEV, une représentation 3D en projection cabinet, gérant le problème d'occlusions (Couturier et Mephu-Nguifo (2007)), est utilisée pour les contextes épars alors qu'une représentation 2D l'est pour les contextes denses. Un contexte dense est caractérisé par la présence d'au moins une règle exacte, puisque au moins un générateur minimal est différent de sa fermeture associée, ce qui n'est pas le cas pour les contextes épars.
Expérimentations
Afin de démontrer la valeur de notre approche, nous avons mis en oeuvre des expérimen-tations sur deux jeux de données Le nombre de clusters est défini en fonction du nombre x de règles d'association que peut contenir un cluster. Nos résultats sont présentés dans la table 1. Tous les fichiers XML sont initialement illisibles dans un même espace écran à cause du nombre important de règles. Notre approche permet de visualiser ces jeux de données dans un même espace écran, avec approximativement les même temps d'affichage pour un nombre de clusters égal à ( # regles generiques 100 ) pour des données denses ou éparses. Nos tests exploitant différentes valeurs de x (i.e., x = 25 and x = 50) requièrent en général plus de temps à l'affichage. Cependant, pour les clusters contenant moins de 100 règles, il est possible de réduire le temps relatif à l'affichage d'un cluster puisqu'il est le seul à être affiché en détail tout en contenant moins de règles. Afin d'interagir en temps réel, il est nécessaire de charger toutes les règles en mémoire. Cependant, nous pouvons observer qu'avec des valeurs de minsup (resp. minconf ) égales à 0,2 (resp. 0,2) pour les données MUSHROOM et avec des valeurs de minsup (resp. minconf ) égales à 0,004 (resp. 0,004) pour les données T10I4D100K, il n'est pas toujours possible de le faire avec les visualisations classiques sans clustering. Notre approche permet de visualiser le premier jeu de données en 17 953 ms avec # regles generiques 100
clusters. Cet ensemble représente 7 057 règles qui sont incluses dans le même espace écran (cf. Tableau 1). Le second jeu de données est affiché avec notre approche pour les différentes valeurs de x utilisées pour nos tests. Dans ce cas, l'ensemble de règles d'association contient jusqu'à 3 623 règles (cf. Tableau 1). Dans ce contexte, nous n'avons pas utilisé de valeur de minsup élevée puisque le nombre de règles n'aurait pas été assez important pour justifier l'intérêt de notre approche. Une approche classique aurait elle aussi permis de les traiter.
Conclusion et perspectives
Dans ce papier, nous avons présenté une méthode de visualisation de grands ensembles de règles d'association. Ainsi, nous avons proposé d'utiliser le clustering sur un ensemble de règles génériques pour réduire la charge cognitive de l'utilisateur. Nous avons implémenté le prototype CBVAR correspondant. Les expérimentations que nous avons menées ont mis en exergue la possibilité de visualiser rapidement dans un même espace écran quelques milliers de règles en quelques secondes et que notre prototype est performant sur des quantités importantes de données. Ainsi, grâce à l'hybridation d'une représentation 2D et d'une FEV, notre approche permet d'obtenir simultanément une vue globale et détaillée de nos règles.
Pour la suite, nous souhaitons principalement focaliser nos efforts sur l'étape de clustering qui est centrale dans notre approche. Ainsi, la piste des k-hiérarchies faibles sera considérée. En outre, comme la famille des regroupements produite par une hiérarchie faible constitue une famille de Moore, alors la structure du treillis pourrait devenir une structure de visualisation privilégiée. Du point de vue IHM, nous souhaitons réaliser une évaluation utilisateur.

Introduction
La plate-forme de visualisation de graphes Tulip 1 (Auber, 2003) développée au LaBRI est dédiée à l'exploration de grands graphes. Elle autorise un utilisateur expert à visualiser un graphe à partir d'algorithmes de dessin parmi les plus récents. Elle facilite le calcul et le rendu visuel de statistiques sur les graphes afin d'en rechercher les propriétés structurelles. Conçue pour la manipulation et le calcul du clustering de grands graphes, l'interface utilisateur permet de fouiller l'information ainsi découpée en inspectant les clusters et leurs éléments, tout particulièrement lorsque le clustering produit une hiérarchie de sous-graphes.
De fait, Tulip se prête à la fouille interactive de données munies de relations binaires et enrichies d'attributs, numériques ou textuels. Le calcul d'un graphe à partir d'indices de similarité est un exemple typique où l'exploration interactive vient en appui au travail de fouille de données et d'extraction de connaissances. On peut penser au cas où des documents sont liés deux à deux par un indice de similarité (de leur contenu). En seuillant cet indice, on peut induire sur l'ensemble des documents une structure de graphes qu'il est alors utile d'examiner.
Nous proposons dans cet article de décrire une étude de cas exhibant les qualités de la plateforme Tulip et démontrant l'apport de la visualisation à la fouille de données et à l'extraction de connaissances.
Exploration visuelle et fouille de données
Nous nous pencherons sur le cas où l'on souhaite étudier une collection de documents afin d'avoir une idée des thématiques abordées dans la collection de manière globale, pour savoir si un sujet rassemble une majorité de documents, ou si au contraire il ne concerne qu'une toute petite part d'entre eux, par exemple. Typiquement, on extraira des documents un ensemble de mots-clés qui capturent leur contenu à divers degré. Le plus souvent, la présence de motsclés donnent lieu au calcul d'indices de similarité entre documents : deux documents seront d'autant plus similaires qu'ils ont des mots-clés en commun et que ceux-ci y apparaissent souvent. (Nombre de variantes existent ; voir (Hammouda et Kamel, 2004) par exemple, ou (Dubois et Bothorel, 2006) pour une approche récente intégrant les usages.) A l'inverse, on peut associer les mots-clés deux à deux afin de voir émerger certains mots au rang de concept (Figure 1). Les réseaux de co-citation, mêlant auteurs et publications scientifiques, sont un autre exemple de graphes qui se prêtent à l'examen visuel afin de définir une stratégie d'analyse des données sous-jacentes.
En d'autres mots, partant d'une collection de N documents, on se ramène à l'étude d'un graphe dont les sommets représentent les documents. La mesure de similarité est interprétée comme la donnée des arêtes qui sont alors valuées (voire orientées). Lorsque le graphe ainsi obtenu est complet ou lorsque son nombre d'arêtes avoisine N 2 (ou N (N ?1) dans le cas d'un graphe orienté, ou N 2 si on admet les boucles ou auto-référence), on peut filtrer certaines des arêtes pour alléger le graphe, en espérant ne retenir que les relations les plus marquées, c'est-à-dire les plus à même de rendre la structure inhérente à la collection de documents. Cette idée souvent empruntée remonte à (Tenenbaum et al., 2000) et s'avère utile, malgré quelques limitations attribuables à la difficulté du problème de la réduction de la dimension d'un jeu de données (dimensionality reduction) -voir (Balasubramanian et Schwartz, 2002). La Figure 1 illustre ce procédé ; dans cet exemple, les mots-clés sont associés selon leurs indices de cooccurences dans la collection de documents.
Toutefois, l'utilisateur peut rapidement être dépassé par le nombre d'éléments du graphe (sommets et arêtes) à étudier, et par la complexité du réseau qu'ils forment. C'est ici que la visualisation interactive entre en jeu : l'utilisateur pourra interagir sur la carte et acquérir une compréhension du réseau à travers sa navigation, plutôt que d'en rester à une carte statique. L'analyste peut s'appuyer sur une visualisation de l'ensemble des documents et l'explorer interactivement afin de percevoir des motifs structuraux dans l'organisation des liens. Soulignons que la visualisation n'est, la plupart du temps, pas une fin en soi mais permet de définir une stratégie d'analyse du corpus. A l'inverse, la visualisation évoluera en proportion de la compréhension que gagne l'analyste sur cet ensemble de données complexes. Ce scénario a été testé en taille réelle sur un jeu de données diffusé sur le web (Delest et al., 2004) 2 ; il a aussi été repris plus récemment dans le contexte bio-informatique (Iragne et al., 2005) 3 et pour l'exploration de grands réseaux spatiaux en géographie (Amiel et al., 2005) 4 -voir Figure 2. Il s'agit là, d'une certaine manière, d'un scénario très souvent mis en oeuvre avec Tulip pour venir en appui à la fouille et à l'exploration interactive de données complexes 5 .
Visualisation de graphes
La plate-forme Tulip prend le parti d'offrir en priorité à l'utilisateur un grand choix d'algorithmes de dessin de graphes (Graph Drawing 6 ), c'est-à-dire de représentations des graphes dans le plan ou dans l'espace à l'aide de sommets (des points) et d'arêtes (traits rectilignes).
C'est en quelque sorte une spécialisation de Tulip à un type précis d'abstractions visuelles ou de transformations visuelles, pour emprunter les termes de la taxonomie de Chi (Chi, 2000). Ainsi, on peut facilement faire appel aux algorithmes de dessin d'arborescences [ (Reingold et Tilford, 1981)  (Eades, 1992)  (Grivet et al., 2004)], de graphes orientés sans circuit (DAG), de graphes planaires et de graphes plus généraux à l'aide des algorithmes basés sur les analogies physiques (souvent appelés masses-ressort) [(Fruchterman et Reingold, 1991)  (Fricke et al., 1994)] ou s'appuyant sur l'analyse spectrale [   ].
Cela dit, l'interface, le format de description et les fonctionnalités internes à Tulip permettent à l'utilisateur de représenter les sommets par différentes objets graphiques simples. En particulier, Tulip autorise l'utilisateur à calculer, outre les positions des sommets dans le plan, divers attributs numériques pour les sommets et/ou les arêtes. La visualisation de ces attributs est alors naturellement traduits sous forme d'indices visuels comme la couleur ou la taille des sommets. Cette idée très simple a fait ses preuves pour aider l'analyste dans l'exploration de grands graphes (voir (Herman et al., 2000) par exemple).
Comme on peut s'y attendre, l'utilisateur est à même d'affecter à chaque sommet une forme différente et paramétrable, une couleur qui varie en fonction d'un indice structurel ou d'un attribut contextuel. S'il est possible d'effectuer ce choix au moment de la visualisation, on peut tout aussi bien l'insérer au niveau du fichier de description, ou encore concevoir un plug-in spécifique qui s'intégrera facilement à la plate-forme (voir Section 5). La plate-forme offre déjà nombre d'algorithmes calculant des indices structuraux sur les graphes -indices de centralité (Freeman, 2000), indice de clustering (Watts et Strogatz, 1998), . . . -et les indices plus habituels comme le degré entrant ou sortant, etc. D'autres algorithmes permettent de tester certaines propriétés comme l'acyclicité ou la connexité, et effectueront la sélection de composantes connexes, d'arbres couvrants ou de graphes acycliques couvrants, par exemple.
Ces dernières manipulations s'avèrent des plus pratiques pour sélectionner un sous-ensemble d'un jeu de données et le faire passer au rang de sous-graphe. Cet objet affiché dans une fenêtre qui lui est propre permettra de travailler à plus petite échelle, tout en héritant des propriétés calculées sur le graphe dont il est issu (voir Figure 5).
Cas d'étude
La Figure  
FIG. 2 -Histogramme des indices de similarités (normalisés).
cartographie (voir (Bertin, 1998), (Denègre, 2005)). Dans la Figure 1, la taille d'un sommet correspond à son degré dans le graphe. Le degré donne en effet une première indication du rôle que joue un sommet dans le réseau, au moins dans son voisinage immédiat. La couleur du sommet est, elle, associée à la connexité de son voisinage (selon l'indice de clustering de Watts (Watts et Strogatz, 1998)). Le choix des couleurs peut lui aussi être varié. L'exploration interactive permet à l'utilisateur, après avoir repéré un phénomène saillant (voisinage dense, sommet de degré plus élevé, etc.) de circonscrire son exploration à une région particulière du graphe. La Figure 3 illustre une manipulation typique, aisément exécutée avec Tulip. L'utilisateur aura sans aucun doute perçu la complexité du voisinage du sommet (concept) administration, et pourra alors sélectionner le sommet. Par déplacement, il peut ensuite isoler ce sommet en l'écartant de son voisinage. L'ensemble des liens suit et on est à même de percevoir comment est tissé cette région. Les arêtes du sommet sont mises en exergue par un effet de coloration pour signaler leur sélection, rendant à la fois l'effet de la manipulation et cette partie de la carte plus lisible et permettant à l'utilisateur de constater les liens avec les concepts processus, surveillance, ou programme, par exemple.  Figure) est organisé de manière similaire et suit une topologie en étoile typique de la méthode utilisée ici. Cette image est elle-même une vue partielle de tout le réseau : nous avons en effet accédé au cluster illustré à la Figure 4 et à son organisation multi-niveaux en zoomant à l'intérieur de l'une des composantes du graphe de départ. Observez le cluster contenant le mot administration qui avait retenu notre attention à la Figure 3, qui se trouve sur la droite. Avec le mot subversion auquel il est lié de manière significative, il forme une paire indissociable dans le graphe quotient. Les autres voisins du mot administration sont passés au second ordre, et reste encapsulé dans le noyau.
Clustering de graphes
On peut d'une certaine manière penser aux composantes placées en périphérie du noyau comme aux mots-clés formant avec certains de leurs voisins des composantes indissociables, mais qui se détachent suffisamment du noyau central. Typiquement, tout algorithme de clustering est susceptible de repérer un voisinage formant une clique -un sous-graphe complet, comme celui dans la partie gauche de la Figure 5. Au niveau le plus bas, on trouvera des composantes irréductibles, dans le sens où l'algorithme de clustering ne saura trouver de critère pour en extraire un sous-graphe. C'est évidemment le cas pour les cliques, mais aussi pour les graphes comme celui situé à droite dans la Figure 5, qui sans être une arborescence est principalement constitué de longues chaînes de mots sans voisinage touffu (sauf peut-être pour le voisinage du mot java). Incidemment, on peut chercher à suivre dans ce sous-graphe les chemins allant d'un mot à l'autre pour y trouver une proximité de sens et ainsi interpréter le contenu de la collection de documents. 
Conclusion
Plus qu'une suite logicielle, Tulip est une plate-forme de développement de laquelle peuvent être déclinées des applications spécifiques [EVAT (Auber et al., 2003b) pour la visualisation et la comparaison de grandes arborescence -arborescences de fichiers, classification des espèces, etc. -ProViz (Iragne et al., 2005)   (Auber et al., 2003a) ne reconnaît aucune composante à extraire.
d'algorithmes de clustering ou de dessin de graphes est tout aussi facile. A l'inverse d'autres outils de visualisation comme IVTK (Fekete, 2004) ou prefuse (Heer et al., 2005), Tulip est développé en C++, permettant ainsi de manipuler des données massives. La déclinaison du coeur de Tulip en une application dédiée nécessite l'utilisation de la librairie QT 8 , pour ce qui concerne l'environnement de fenêtrage et les modalités de l'interface graphique, auquel vient se greffer le noyau Tulip pour la gestion de la structure de graphes (hié-rarchiques) et l'utilisation d'algorithmes spécifiques. Notons au passage que Tulip est inclus dans plusieurs distributions Linux (Suse, Debian, . . .). Tulip est distribué sous licence GPL et est hébergé sur le site SourceForge 9 . Les fonctionnalités de la plate-forme Tulip constituent sans aucun doute une boîte à outils des plus utiles pour découvrir, concevoir et tester les hypothèses des chercheurs oeuvrant dans le domaine de l'extraction et la gestion des connaissances. Idéalement, la visualisation sera couplée aux méthodes et techniques propre à ce domaine afin d'offrir aux utilisateurs une cartographie sémantique interactive pour fouiller les grands espaces d'information, structurés ou non, ou encore comme interface d'accès à l'information.

Introduction
Le volume de données stocké double actuellement tous les 9 mois (Lyman et al, 2003) et donc le besoin d'extraction de connaissances dans les grandes bases de données est de plus en plus important (Fayyad et al, 2004). La fouille de données (Fayyad et al, 1996) vise à traiter des ensembles de données pour identifier des connaissances nouvelles, valides, potentiellement utilisables et compréhensibles. Cette utilisabilité est fonction des buts de l'utilisateur donc seul l'utilisateur peut déterminer si les connaissances extraites répondent à ses attentes. Les outils de fouille de données doivent donc être interactifs et anthropocentrés. Notre approche consiste à impliquer plus fortement l'utilisateur dans le processus de fouille par des méthodes graphiques interactives dans un environnement de fouille.
De nombreuses méthodes de visualisation ont été développées dans différents domaines et utilisées pour l'analyse exploratoire et la fouille de données (Fayyad et al, 2001), (Keim, 2002). Les méthodes de visualisation peuvent être utilisées pour le pré-traitement de données (par exemple la sélection de données) ou en post-traitement (par exemple pour voir les résultats). Des méthodes récentes (Ankerst, 2001), (Do et Poulet, 2004a et b), (Munzner, 1997) essayent d'impliquer plus significativement l'utilisateur dans le processus de fouille de données par le biais de la visualisation. Parmi les avantages que ce type d'approche présente on peut citer : l'utilisateur peut être un spécialiste du domaine des données et utiliser son expertise tout au long du processus de fouille, la confiance et la compréhensibilité du modèle sont accrues car l'utilisateur a participé à sa construction et enfin on peut utiliser les performances de la perception visuelle humaine en reconnaissance des formes.
Nous présentons une méthode d'exploration interactive des résultats des algorithmes d'arbres de décision comme C4.5 (Quinlan, 1993). Les arbres de décision sont des méthodes efficaces et populaires (Kdnuggets, 2003(Kdnuggets, et 2004 pour la classification. Un arbre de décision est un classifieur représenté sous la forme d'un arbre dans lequel chaque noeud est soit une feuille contenant l'étiquette de la classe, soit un noeud interne contenant un test à effectuer sur l'un des attributs avec un fils pour chaque résultat possible du test. Une règle d'induction (de la forme si alors) est créée pour chaque chemin partant de la racine de l'arbre et parcourant les tests (en faisant des conjonctions) jusqu'à la feuille qui est l'étiquette de la classe. Les arbres de décision sont particulièrement appréciés car ils permettent une compréhension aisée, mais lors d'une tâche de classification relativement complexe l'utilisateur n'est plus capable d'explorer efficacement les résultats obtenus sous forme textuelle.
Quelques techniques de visualisation d'arbres de décision existent déjà. Le TreeVisualizer de Mineset (Brunk et al, 1997) permet une visualisation en 3D avec une vue partielle des résultats. Seuls les premiers niveaux de l'arbre sont affichés initialement et le reste apparaît au fur et à mesure que l'utilisateur descend dans l'arbre.
Le Cone Tree (Robertson et al, 1991) utilise aussi une représentation 3D de l'information hiérarchique, le noeud courant est le sommet d'un cône contenant tous ses fils répartis à la base de ce cône 3D.
La technique focus+context (Lampig et Rao, 1996) utilise une portion de l'espace plus importante pour la zone où l'utilisateur se positionne. Le principe de cette visualisation est l'utilisation d'un plan hyperbolique et d'une projection sur une sphère.
Les arbres hyperboliques (hyperbolic trees (Munzner, 1997)) visualisent les arbres sous la forme de graphes dans un espace 3D hyperbolique. Cette technique utilise une métrique hyperbolique et optimise le placement des fils d'un noeud sur une hemisphère autour de la base du cône. L'exploration animée (Yee et al, 2001) de graphes dynamiques avec visualisation radiale est une technique d'animation de la transition d'un noeud à l'autre. Pour permettre une transition facile à suivre, l'animation correspond à une interpolation linéaire entre les noeuds source et destination avec des contraintes sur l'ordre et l'orientation des objets.
En ce qui concerne les arbres de décision dans un contexte de fouille de données, les impératifs importants pour l'utilisateur sont : une compréhension facile, la possibilité d'extraire des règles pertinentes et la possibilité d'effectuer un élagage de l'arbre dans une étape de post-traitement. Nous présentons une nouvelle méthode de visualisation radiale des arbres de décision pour l'exploration interactive. Nous utilisons pour cela des méthodes de visualisation telles qu'une représentation à la manière d'un explorateur, le focus+context, le fisheye, la visualisation hiérarchique et les techniques interactives pour représenter des arbres de grandes tailles de manière graphique plus intuitive que les résultats habituels des algorithmes d'arbres de décision. L'utilisateur peut explorer interactivement l'arbre de décision, extraire des règles intéressantes et élaguer l'arbre obtenu. Les résultats numériques des tests sur des ensembles de données réels montrent que les méthodes proposées améliorent la compréhension des résultats des arbres de décision.
Représentation type explorateur
Une représentation type explorateur va projeter la racine de l'arbre dans le coin supérieur gauche de la vue (habituellement de coordonnées (0,0)). Un noeud est représenté par un carré dont la couleur représente la classe (ou classe majoritaire). La taille d'une feuille sera proportionnelle aux nombres d'individus contenus dans cette feuille. Chaque noeud peut être développé ou non par un clic de souris. La figure 1 est un exemple de visualisation de type explorateur avec l'ensemble de données Shuttle de l'UCI (Blake et Merz, 1998) contenant 58000 individus en dimensions 9 et 7 classes.
L'utilisateur peut ainsi explorer facilement les résultats de l'algorithme d'arbre de décision dans un mode graphique plus intuitif que les résultats habituels sous forme de texte. La représentation du nombre d'individus contenus dans les feuilles aide à déterminer l'importance de la règle de décision. Les feuilles ne contenant que peu d'individus sont généralement de moindre importance. Ces indications sont une aide précieuse lorsque l'utilisateur cherche à élaguer l'arbre de décision obtenu.
FIG. 1 -Représentation type explorateur de l'arbre de décision des données Shuttle
L'utilisateur peut extraire des règles d'induction, en cliquant sur une feuille de l'arbre, le chemin de la racine à la feuille est sélectionné et la règle correspondante est alors affichée. L'exemple de la figure 2 montre une règle extraite de l'arbre de décision obtenu sur l'ensemble de données Shuttle. Grâce aux techniques interactives telles que le développement ou non d'un noeud, le focus ou le zoom, l'utilisateur peut facilement naviguer dans l'arbre avec l'affichage des informations associées à chaque noeud telles que le nombre d'individus RNTI -XTree-Viz ou le nombre d'erreurs (individus mal classifiés). Avec ces éléments l'utilisateur a une aide précieuse pour effectuer lui-même l'élagage de l'arbre de décision. Cet élagage permettra d'améliorer le taux de bonne classification sur l'ensemble de test en minimisant le surapprentissage. 
FIG. 2 -Extraction de règle d'induction
Visualisation radiale, fisheye, focus+context et techniques hiérarchiques
Pour des arbres de grandes tailles nous avons essayé d'améliorer la représentation 2D en utilisant une visualisation radiale, le fisheye et le focus+context dans une visualisation hiérarchique. Avec l'algorithme de visualisation radiale, la racine de l'arbre de décision est projetée sur le centre de la vue, les noeuds fils sont disposés en cercles ou arcs de cercle autour du noeud parent. L'espacement est fonction du nombre de noeuds descendants de chaque noeud, comme le montre l'exemple de la figure 5.
RNTI -X -
Tree-Viz
Nous utilisons ensuite la technique du fisheye qui aide l'utilisateur à détailler l'environnement immédiat du noeud courant ce qui lui permet de se focaliser sur les parties qu'il juge intéressantes de l'arbre de décision.
FIG. 5 -Visualisation radiale
Nous proposons aussi une nouvelle technique dérivée du fisheye et permettant un zoom sur une zone d'intérêt au détriment des autres parties. Considérons la racine O projetée en O1 et un noeud P projeté en P1, comme sur l'exemple de la figure 7. La racine va être translatée de O1 vers O2 en préservant l'angle ?= (Ox, OP). Le noeud P doit donc lui aussi être translaté de P1 à P2. L'angle obtenu (O1x, O1P2) est supérieur à l'angle (O1x, O1P1). Voici comment obtenir la position de P2 : P 2 (x) = x 1 + r*cos(?) ( 1 ) P 2 (y) = y 1 + r*sin(?) ( 2 ) P 2 (x) = x 2 + t*cos(?) ( 3 ) P 2 (y) = y 2 + t*sin(?) ( 4 ) En substituant P2(x) [resp. P2(y)] dans (1) [resp.(2)], on obtient les équations (5) et (6). r*cos(?) = x 2 -x 1 + t*cos(?) ( 5 ) r*sin(?) = y 2 -y 1 + t*sin(?) ( 6 ) => r 2 = (?x + t*cos(?)) 2 + (?y + t*sin(?)) 2 (7) avec ?x = x 2 -x 1 et ?y = y 2 -y 1 t 2 + 2(?x cos(?) + ?y sin(?))t + ?  
FIG. 7 -Focus sur les noeuds d'intérêt
Les expérimentations des systèmes de visualisation d'arbres de décision (Barlow et Neuville, 2001) ont montré qu'il n'y a pas une technique meilleure que les autres pour l'exploration de grands arbres de décision avec à la fois la simplicité, la vitesse, le focus, une vue globale et une bonne compréhension de l'utilisateur. Nous souhaitons donc combiner plusieurs techniques pour tirer partie des avantages de chacune. Notre méthode de visualisation utilise une visualisation type explorateur, une visualisation radiale, le fisheye et le focus. Le principe est de diviser l'espace écran en différentes zones dans lesquelles une visualisation différente peut être affichée. Nous avons choisi par défaut de visualiser l'arbre RNTI -XTree-Viz complet avec la visualisation radiale, le fisheye et le focus. L'utilisateur peut sélectionner un sous-ensemble de l'arbre et le visualiser avec une autre méthode comme la représentation type explorateur sur l'exemple de la figure 9.
FIG. 8 -Focus sur une région d'intérêt
FIG. 9 -Visualisation hiérarchique et type explorateur de Segment
La visualisation hiérarchique permet de préserver une vision globale de l'arbre avec une représentation plus fine grâce au fisheye et au focus. Le sous-arbre sélectionné peut alors être exploré aisément dans la représentation type explorateur. On allie donc la simplicité, la rapidité pour effectuer la tâche, la facilité d'utilisation et la compréhension du modèle. La figure 9 représente la visualisation de l'arbre de décision obtenu sur l'ensemble de données Segment de l'UCI (2310 individus en dimension 19 et 7 classes). L'environnement développé permet la visualisation d'arbre de taille relativement importante en permettant simultanément une vision de la totalité de l'arbre et un zoom sur les zones d'intérêt. Il essaye d'associer à la fois la hiérarchie complète de l'arbre, la simplicité d'utilisation, la rapidité d'exécution de la tâche et la satisfaction et bonne compréhension de l'utilisateur.
Exploration des résultats de la classification de spam
Conclusion et perspectives
Nous avons présenté un nouvel environnement graphique pour l'exploration des résultats des algorithmes d'arbre de décision (comme C4.5). Notre but était de satisfaire les demandes des utilisateurs dans le contexte de la fouille de données : facilité d'interprétation, extraction de règles pertinentes et élagage efficace de l'arbre dans une phase de post-traitement. Nous avons développé une nouvelle méthode de visualisation radiale pour permettre l'exploration interactive des résultats des arbres de décision. Cette méthode a été utilisée en collaboration avec d'autres méthodes telles que la représentation type explorateur, le focus+context, le fisheye, la visualisation hiérarchique et les techniques interactives pour permettre la visualisation d'arbres de tailles importantes de manière plus intuitive que les habituelles sorties texte des algorithmes d'arbres de décision. Grâce à l'ensemble des techniques telles que le développement ou non des noeuds, le focus, le zoom ou la rotation l'utilisateur peut aisément explorer les résultats des arbres de décision. Il peut aussi prendre connaissance des informations associées aux noeuds de l'arbre comme le nombre d'individus ou d'erreurs. Cette connaissance peut lui permettre d'extraire des règles d'induction ou d'élaguer lui-même l'arbre dans un mode graphique interactif et intuitif basé sur la profondeur, la taille, la couleur et l'information liée aux noeuds de l'arbre. L'utilisateur a ainsi une meilleure compréhension de l'arbre obtenu. Les test sur l'ensemble de données Spambase montre que la méthode proposé permet de mieux appréhender les résultats des algorithmes d'arbres de décision. Une extension future est de trouver une abstraction permettant de visualiser des arbres de taille plus importante, une autre extension envisagée est d'étendre cette approche à d'autres algorithmes de fouilles de données pour permettre de d'expliquer les résultats obtenus par ces algorithmes.

Le temps nécessaire pour écouter un flux audio est un facteur réduisant l'accès efficace à de grandes archives de parole. Une première approche, la structuration automatique des données, permet d'utiliser un moteur de recherche pour cibler plus rapidement l'information. Les listes de résultats générées sont longues dans un souci d'exhaustivité. Alors que pour des documents textuels, un coup d'oeil discrimine un résultat interessant d'un résultat non pertinant, il faut écouter l'audio dans son intégralité pour en capturer le contenu. Nous proposons donc d'utiliser le résumé automatique afin de structurer les résultats des recherches et d'en réduire la redondance.
Structuration
Recherche Résumé parlé Audio utilisateur Les données radiophoniques exploitées pour cette approche sont issues de la campagne ESTER (Galliano et al., 2005), évaluatrice de la structuration automatique d'émissions et de bulletins à caractère informatif. Le processus de structuration de notre système est le suivant : segmentation en classes acoustiques , segmentation en locuteurs (Istrate et al., 2005), transcription de la parole , segmentation thématique (Sitbon et Bellot, 2004), et reconnaissance d'entités nommées (Favre et al., 2005). Grâce à cette structuration, un moteur de recherche basé sur le modèle vectoriel permet de présenter à l'utilisateur la liste des segments correspondant à son besoin en information.
Fondé sur l'observation que 70% des phrases d'un résumé écrit manuellement proviennent des textes d'origines, le résumé par extraction est l'approche la plus utilisée actuellement en domaine ouvert pour le texte. En prenant pour hypothèse que cette observation est similaire pour la parole (les titres des journaux radiodiffusés), nous l'appliquons à la fois pour extraire des étiquettes thématiques structurant hiérarchiquement les résultats et pour extraire les segments les plus représentatifs du contenu des résultats.
L'algorithme Maximal Marginal Relevance (MMR), proposé par (Goldstein et al., 2000) pour sélectionner les segments maximisant la couverture en information tout en minimisant sa redondance, peut être appliqué pour sélectionner des mots-clés comme étiquettes thématiques dont on obtient une hiérarchie en faisant varier la granularité. Le critère de sélection par gain en -273 -RNTI-E-6
Accès aux connaissances orales par le résumé automatique couverture de MMR est modifié en transposant le paradigme de représentation des documents par des vecteurs de mots, afin de représenter des mots par des vecteurs de documents.
Ici, t est le vecteur modélisant un mot-clé, c res le vecteur centroïde des résultats, c sel le vecteur centroïde de la sélection courante et sim() la similarité mesurée par le cosinus de l'angle entre les vecteurs. Dans le domaine de l'information radiodiffusée, les mots-clés utilisés sont des entités nommées car les noms de lieux, de personnes et d'organisation permettent de caractériser des événements. Ces étiquettes thématiques sont proposées à l'utilisateur qui, en les sélectionnant, implique la restriction des résultats par conjonction avec les termes de la requête. Parallèlement, le résumé des segments audio est généré selon MMR classique pour permettre à l'utilisateur d'écouter l'équivalent d'un court bulletin d'informations.
Bien que le système permette une forte réduction du temps d'écoute, le résumé audio est soumis aux mêmes problèmes majeurs que le résumé textuel, à savoir les références non réso-lues et la réduction de redondance à l'interieur même des segments. S'ajoutent les erreurs de la structuration automatique et les désagréments liés à la parole comme les difficultés d'élocution ou les recouvrements de locuteurs dont l'impact est présent à l'écoute. Nous projetons pour la suite de ces travaux, d'adresser ces problèmatiques et d'évaluer le système d'accès aux flux de données parlées.

Introduction
Notre idée-clé est de s'attaquer au problème de la réduction des files d'attente à partir de l'analyse des journées d'hospitalisation non-pertinentes. Les études effectuées jusqu'ici ont été trop spécialisées (Vardi A., 1996). L'objectif de cette communication est de proposer un outil efficace, de haute qualité, accessible à un non-spécialiste d'aide à la décision pour réduire les files d'attente des patients, basé sur la visualisation des composantes des journées d'hospitalisation nonpertinentes dans les services cliniques aigus.
477 patients ont été inclus à partir de 3 spécialités différentes soit 4834 journées en soins aigus évaluées dans 4 services cliniques répartis dans 3 hôpitaux. Après avoir mis en évidence des associations (à facteur constant) de variables liées à la non-pertinence (Huet B., 2005), nous avons fait des analyses en correspondances multiples (sous SAS V8.2 / PC).
Les données visualisées
L'analyse des « processus de gestion médicale » (PGMs) de tous les patients (477) de tous les services a montré que 84% de la variance des données peuvent être modélisés en deux axes (figure1). Le 1er axe (70%) dépend essentiellement du nombre de journées nonpertinentes (30%), des causes de non-pertinence (25%), du taux de non-pertinence (jnp/jtot) (24%), durée de séjour (21%) tandis que le second axe (14%) dépend essentiellement de la durée de séjour (35%), du taux de non-pertinence (jnp/jtot) (28%), du nombre de journées non-pertinentes (24%), des causes de non-pertinence (13%). Ces 2 axes classent les PGMs selon une hyperbole classique, par leur « poids composé de non-pertinence »: du plus « léger » au plus « lourd » : blessures cutanéo-muqueuses, ablation de matériel opératoire, désintoxication alcoolique, fracture simple, chirurgie de courte durée, gastro-entérologie, médecine interne (non gériatrique), fracture complexe, chirurgie longue durée, médecine interne (gériatrique), démence et médecine interne, démence et troubles neuro-psychiques. RNTI-E-6
Aide en gestion hospitalière
FIG. 1 -Projection d'analyse des Processus de Gestion médicale classés selon leur « poids composé de non-pertinence ».
Discussion et Conclusion
Cette visualisation permet une lecture immédiate d'une information hautement significative, l'utilisation opérationnelle de ces données est parfaitement valable elle permet des non-spécialistes d'avoir accès à ces données hautement spécialisées. 
Références
Summary :
We present the visualizing of components associated with inappropriate hospital days with their causes and their queues. It is a highly significant information whose interpretation can be made by a non-specialist (hospital manager,…).

Introduction
Nous nous intéressons à la recherche d'outliers (individus atypiques) dans les ensembles de données ayant un grand nombre de dimensions. Pour pouvoir traiter de tels ensembles de données (par exemple les ensembles de données de fouille de texte ou de bio-informatique), la plupart des algorithmes de fouille de données actuels nécessitent un prétraitement permettant de réduire le nombre de dimensions (avec plus ou moins de perte d'information). L'approche la plus intuitive pour appréhender le problème des grandes dimensions est d'énumérer tous les sous-ensembles de dimensions possibles et de rechercher le sousensemble qui satisfait la problématique traitée. Cependant, le fait d'énumérer (rechercher) toutes les combinaisons possibles est un problème NP-difficile (Narenda et Fukunaga, 1977). Parmi les solutions proposées pour ce problème, on retrouve la réduction de dimensions (combinaison de dimensions, généralement linéaire) et la sélection de dimensions (on n'utilise qu'un sous-ensemble des dimensions originales). L'avantage de cette dernière solution est que nous ne perdons pas l'information que pourrait apporter la dimension, car elle est considérée individuellement non en combinaison (linéaire) avec d'autres dimensions. Les techniques de sélection de dimensions consistent donc à réduire l'ensemble des dimensions considérées. L'objectif est de réduire la complexité, augmenter la précision de la prédiction et/ou réduire le temps de traitement des données, en sélectionnant le sousensemble de dimensions de taille minimale (Dash et al., 1997). L'étude du problème de sélection de dimensions se justifie facilement par le fait qu'une recherche exacte a un coût exponentiel en temps de calcul et en espace mémoire. En effet, la sélection d'un sousensemble de dimensions demanderait l'exploration de tout l'espace de recherche. Pour |D| dimensions, la recherche exhaustive consiste à explorer 2  
?
. Lorsque l'on s'attaque à des problèmes réels, il faut se résoudre à un 0 s = compromis entre la qualité des solutions obtenues et le temps de calcul utilisé. Au milieu des années 1970 sont apparues des méthodes qui supervisent l'évolution de solutions fournies par des heuristiques. Ces méthodes assurent un compromis entre diversification (quand il est possible de déterminer que la recherche se concentre sur de mauvaises zones de l'espace de recherche) et intensification (on recherche les meilleures solutions dans la région de l'espace de recherche en cours d'analyse). Ces algorithmes ont été appelés métaheuristiques et ont pour objectif de trouver des solutions dont la qualité est au-delà de ce qu'il aurait été possible de réaliser avec une simple heuristique (Jourdan, 2003). Dans cet article, nous proposons un algorithme génétique pour le choix du sous-ensemble de dimensions à retenir.
Par ailleurs nous souhaitons donner un rôle plus important à l'utilisateur dans le processus de recherche et de sélection de l'algorithme génétique, pour cela nous avons choisi d'utiliser un algorithme génétique interactif. Nous présentons donc une nouvelle méthode interactive, proposant elle-même des solutions potentielles à l'utilisateur. Les solutions de l'algorithme génétique se présentent sous forme de sous-ensembles de dimensions. Puisque le nombre de dimensions utilisé est faible, on peut ensuite visualiser les éléments de l'ensemble de données sur ces sous-espaces de dimensions (à l'aide de matrices de scatter-plot (Carr et al., 1987) ou de coordonnées parallèles (Inselberg, 1985)) pour permettre à l'expert d'interpréter les résultats obtenus. Nous présentons donc des visualisations d'un ensemble de données projeté sur quelques sous-ensembles de dimensions à l'utilisateur, ce dernier pourra lui même juger de la pertinence de la visualisation présentée selon ses objectifs (repérer les dimensions les plus pertinentes pour la détection d'outlier). Pour cela, nous avons choisi d'utiliser un algorithme génétique interactif (AGI). D'une manière générale, cet algorithme fonctionne de la façon suivante : une première évaluation automatique se fait à l'aide d'un critère d'évaluation des sous-espaces de dimensions pour la détection d'outlier basé sur les distances, les données sont ensuite visualisées et présentées graphiquement à l'utilisateur pour une seconde évaluation visuelle. Ce dernier choisit celles qui lui semblent les plus pertinentes. Les caractéristiques visuelles des sous-espaces de données sélectionnés sont prises en compte par l'algorithme pour la génération suivante de sous-espaces, qui sont à nouveau présentés à l'utilisateur et ainsi de suite jusqu'à ce que la recombinaison des caractéristiques permette de générer une projection visuelle de données complètement satisfaisante pour l'utilisateur. Un des avantages de cette méthode est de faire collaborer deux méthodes, automatique et visuelle. La première automatique, à l'aide des critères d'évaluation permet d'éliminer les solutions redondantes ou bruitées et la seconde visuelle et interactive permet à l'utilisateur de participer au processus de recherche et d'aborder un aspect d'évaluation des solutions présentées sous forme visuelle qui représente justement un nouveau domaine de recherche. Un autre avantage est que la méthode s'adresse plus particulièrement au spécialiste des données qui peut utiliser les connaissances du domaine pour l'interprétation visuelle des résultats tout au long du processus de fouille et ainsi apporter un aspect d'aide à la décision. La méthode lui permet d'étiqueter les éléments atypiques, par exemple est-ce que ce sont des erreurs ou simplement des individus différents de la masse. Nous détaillons dans une première partie notre algorithme puis commentons les résultats obtenus sur quelques ensembles de données, nous essayerons ensuite d'interpréter visuellement les résultats obtenus, puis nous terminons par la conclusion et les travaux futurs.
2 Algorithme : Viz-IGA Face au problème de la prise en compte des préférences de l'utilisateur, des auteurs ont montré comment ce dernier pouvait sélectionner lui même directement les solutions qui le satisfont le plus, sans passer par une fonction automatique parfois impossible à définir (Takagi, 2001), (Hayashida et Takagi, 2000). Une nouvelle catégorie d'algorithmes génétiques est née, connue sous le nom d'Algorithmes Génétiques Interactifs. Ces algorithmes génétiques interactifs (AGIs) permettent ainsi des applications nouvelles comme l'obtention de belles images de synthèse ou de sons polyphoniques. Dans ces applications l'utilisateur note selon ses critères les individus qui représentent des images (ou des sons) et l'AGI fait évoluer les individus selon les préférences de l'utilisateur. Les algorithmes génétiques interactifs (AGIs) sont une extension des AGs dans lesquels a lieu une interaction entre la méthode de recherche et l'utilisateur, ce dernier guidant la méthode vers les solutions ayant les caractéristiques qu'il préfère. L'algorithme génétique standard doit être modifié comme suit : les étapes d'évaluation et de sélection automatique des individus sont remplacées par une présentation des individus à l'utilisateur qui sélectionne en un certain nombre. Cela implique notamment de limiter la population à un petit nombre d'individus. Les principales conditions d'application des AGIs citées précédemment sont particulièrement intéressantes dans notre cas d'évaluation visuelle des sous-espaces de dimensions sélectionnés. En effet, l'interprétation visuelle des résultats obtenus est importante pour la détection d'outlier et valider des sous-espaces qui présentent mieux les solutions attendues est aussi une étape importante dans le processus de recherche de sous-ensembles de dimensions. L'utilisateur peut ainsi intervenir dans le processus de recherche des sousespaces de dimensions pertinents.
Initialisation
Nous considérons que l'utilisateur veut avoir des représentations graphiques de données dans un sous-espace de dimensions sur lesquels il peut voir des outliers facilement détectables. Le but recherché est d'aider l'utilisateur à comprendre et interpréter ses données à travers les résultats de l'algorithme. Pour cela, on va lui proposer des représentations graphiques en k-D des données avec une des méthodes de visualisation de données (coordonnées parallèles (Inselberg, 1985), matrices de scatter-plot (Carr et al., 1987) ou star plot (Card et al., 1999)).
Le choix de k, de l'ensemble de données à traiter et de la méthode de visualisation revient à l'utilisateur. Nous proposons ces méthodes de visualisation car elles permettent à l'utilisateur d'interagir avec l'ensemble des données sous la forme d'une série de projections, l'utilisateur peut voir la pertinence d'une dimension et le comportement des éléments de l'ensemble de données.
Représentation de l'individu et opérateurs génétiques
Un individu est donc un sous-ensemble de dimensions représenté par une combinaison d'axes (Axe 1 , Axe 2 , …, Axe k ), ce sous-ensemble étant sélectionné par un algorithme génétique (Boudjeloud et Poulet, 2005). Le codage choisi consiste à fixer un nombre s (taille du sous-ensemble de dimensions à sélectionner), ainsi un individu de l'AG (ou de l'AGI) représente une combinaison possible de s dimensions. Ce type de codage permet une meilleure interactivité avec les dimensions. La taille s est un paramètre d'entrée de l'algorithme génétique. Pour notre problème nous nous basons sur des petites tailles pour faciliter l'interprétation visuelle des résultats (typiquement inférieur à 10). L'opérateur de croisement échange des sous-groupes de dimensions des individus parents, en respectant la contrainte de non présence de clones (les individus qui ont le même sous-ensemble de dimensions, présentées dans un ordre différent ou pas, sont interdits dans notre algorithme de même que des dimensions identiques dans un même individu). L'opérateur de mutation échange aléatoirement un gène en respectant les mêmes conditions. Nous avons opté pour un point de coupe "optimisé" (Boudjeloud et Poulet, 2005), où l'on détermine dans ce cas le meilleur point avant d'opérer la coupe, ce qui implique une évaluation de chaque individu issu de chaque coupe possible.
Evaluation visuelle semi-interactive
Pour que l'utilisateur puisse évaluer visuellement les individus de la population et avoir des représentations visuelles de sous-espaces de données sur lesquelles il peut voir des outliers facilement détectables, nous présentons les individus à l'écran (figure 1-a) en faisant une première évaluation automatique à l'aide d'un critère d'évaluation à base de distance (Boudjeloud et Poulet, 2005). Neuf individus choisis aléatoirement dans la population de l'AG sont affichés simultanément pour ne pas surcharger l'interface et faciliter les comparaisons. Pour évaluer la qualité d'un individu, l'expert dispose de la représentation en coordonnées parallèles qu'il peut éventuellement changer en matrices de scatter-plot ou starplot, selon ses préférences ( figure 1-b). Il ne faut pas oublier que nous traitons des ensembles de données de grandes dimensions (de l'ordre de dix à cent milles dimensions, cf. figure 4), notre objectif principal est d'obtenir des visualisations de données pas trop surchargées, pour cela l'AG traite et présente des visualisations de données avec des sous-ensembles de petite taille (de 4 à 10 pour que les visualisations restent claires). L'utilisateur peut aussi agrandir ou faire un zoom sur la visualisation d'un individu en particulier, changer l'ordre des dimensions et changer la méthode de visualisation, (figure 1-c). Les individus peuvent être affichés individuellement avec les matrices de scatter-plots (figure 1-d), les coordonnées parallèles (figure 1-e) ou avec la méthode Star Plot (figure 1-f). Trois possibilités sont offertes à l'utilisateur pour sélectionner une solution potentielle en cliquant directement sur la visualisation de l'individu, en le sélectionnant sur la partie droite de l'interface (figures 1-c, d, e, f)) ou en faisant un clic droit sur une visualisation en particulier, le choix de la sélectionner est alors offert à l'utilisateur ( figure 1-b). Une fois les sélections effectuées les solutions apparaissent de couleurs différentes, comme sur l'exemple de la figure (1-a)  
Description des étapes
Notre objectif principal est d'obtenir des visualisations de données significatives et pas trop surchargées, il est préférable de fixer s à de petites tailles (<10 pour que les visualisations restent claires). Nous avons choisi de représenter les données à l'aide des coordonnées parallèles (Inselberg, 1985) et des matrices de scatter-plot (Carr et al., 1987), cependant, ceci n'est pas figé, on pourra remplacer ou introduire d'autres méthodes de visualisation de données, nous avons notamment la méthode Star Plot (Card et al., 1999) dans les exemples présentés. Nous utilisons un algorithme génétique pour la recherche de sous-ensembles de dimensions pertinentes. L'interaction avec l'utilisateur intervient sur certaines générations afin de ne pas faire converger l'AG trop rapidement. Nous faisons intervenir l'utilisateur dans le processus de recherche dans deux étapes : l'évaluation et la sélection.
Population initiale : les individus de l'algorithme génétique représentent des sous-espaces de dimensions constitués à partir des dimensions décrivant l'ensemble des données. Une fois la population de départ prête, nous l'évaluons une première fois à l'aide d'un critère d'évaluation à base de distance décrit dans (Boudjeloud et Poulet, 2005).
Evaluation automatique : vu le nombre important de combinaisons de dimensions possibles, il est nécessaire de faire une sélection de dimensions en utilisant ce critère de validité des sous-espaces avant de les présenter à l'utilisateur. Une fois cette présélection automatique faite, l'utilisateur peut intervenir interactivement selon que la visualisation générée le satisfait ou pas.
Evaluation interactive : une fois la population évaluée et triée selon les différents objectifs, nous présentons à l'utilisateur 9 visualisations. Ces visualisations représentent la projection des données dans des sous-ensembles de dimensions choisis aléatoirement dans la population de l'AG (un individu de l'AG représente un sous-ensemble de dimensions, 9 individus sont choisi aléatoirement et sont présentés visuellement). Notre choix s'est fixé à 9 représentations pour ne pas surcharger l'interface. Les solutions sont représentées par des projections en coordonnées parallèles ou d'autres méthodes de visualisation selon le choix de l'utilisateur ( figure 1-b). Nous opérons un croisement et une mutation, puis, toutes les 100 générations, nous proposons à l'utilisateur d'autres visualisations, il peut en sélectionner certaines s'il le souhaite en cliquant dessus, selon qu'elles sont assez significatives pour lui. Pendant ces 100 générations l'AG travaille tout seul sans intervention de l'utilisateur. L'algorithme prend en compte le choix de l'utilisateur pour les prochaines générations dans le processus de recherche de deux manières. Nous avons choisi 100 générations pour que l'AG puisse éliminer les solutions redondantes, les moins intéressantes automatiquement et éviter d'avoir toujours les mêmes solutions qui seront affichées (présentées à l'utilisateur).
Sélection interactive : les solutions sélectionnées par l'utilisateur seront stockées dans une mémoire E' que nous faisons intervenir dans deux étapes de l'algorithme, la première étant la reproduction, la seconde pour remédier à la stagnation de la recherche.
Reproduction : nous faisons intervenir les solutions de E' (sélectionnées par l'utilisateur) dans la reproduction de la façon suivante : chaque nouvel enfant généré aura une partie des gènes d'un parent issu de E' et une partie des gènes d'un parent issu d'une sélection par tournoi où l'on sélectionne aléatoirement et uniformément 2 individus en ne gardant que le meilleur.
Stagnation : dès que la solution stagne (ne s'améliore pas pendant un certain nombre de générations) nous générons de nouvelles solutions à partir des solutions de E' (sélectionnées par l'utilisateur) en les faisant intervenir dans le processus de mutation. Lorsqu'un gène doit être muté, il sera changé par un gène d'un individu de E' (l'emplacement du gène et l'individu de E' sont aléatoires).
L'espace de recherche étant grand, il est important d'avoir une grande capacité d'exploration. Ces mécanismes permettent de maintenir une diversité dans la population en introduisant à certains moments de nouveaux individus. Ils permettent aussi d'éviter une convergence prématurée ou une stagnation des solutions. Nous utilisons ces mécanismes lorsque le meilleur individu est le même durant un certain nombre de mutations ? mut et de croisements ? croi (ces deux paramètres sont exprimés en pourcentage de la taille de la population). Alors, tous les individus de la population qui ont une valeur d'évaluation en dessous de la moyenne de la population (sous la médiane) sont remplacés par de nouveaux individus générés en respectant les conditions de non-présence de clones et de dimensions identiques dans un même individu. La différence entre notre AGI et les autres AGIs existants est que nous faisons coopérer les deux méthodes visuelle et automatique et que nous faisons intervenir l'utilisateur dans deux processus de l'AG : l'évaluation et la sélection.
Résultats et interprétation
Le système a été implémenté sous Windows 2000 dans un environnement très intuitif pour l'utilisateur. Les différentes possibilités de Viz-IGA ont été testées sur des ensembles de données du Kent Ridge Biomedical Dataset Repository (Jinyan et Huiquing, 2002). Les différentes figures (3a, 3b, 3c, 3d) sont créées à partir d'un exemple de détection d'outlier sur l'ensemble de données Breast cancer, Lung cancer, Ovarian et MLL. Notre méthode permet de retrouver des outliers sur des sous-espaces de dimensions identiques à ceux trouvés sur l'ensemble total des données détectés par LOCI (Papadimitriou et al., 2003) un algorithme récent, qui détecte les éléments outliers de l'ensemble des données que nous avons testé (Boudjeloud et Poulet, 2005). De plus, Viz-IGA permet de mettre en évidence les dimensions prépondérantes et souligner la pertinence et l'intérêt de certaines d'entre elles pour la détection d'outlier. Il permet d'isoler et de voir correctement l'élément outlier. Le nombre de dimensions peut être réduit jusqu'à un facteur 1000 en moyenne sans perte significative d'information, puisque nous arrivons à retrouver le même outlier dans les sousespaces de dimensions que sur l'ensemble total des données. Il faut entre 2 et 10 minutes pour arriver à la visualisation la plus pertinente pour les problèmes mentionnés précédemment, soit environ de 4 à 20 interventions de l'utilisateur. Au-delà de 15 minutes, l'expérience montre que l'utilisateur se lasse et commence à se fatiguer. Une autre difficulté qui peut lasser l'utilisateur est la stagnation des solutions. Il peut en effet avoir plusieurs fois les mêmes solutions proposées. Il peut par exemple penser qu'il a obtenu la solution finale alors que c'est juste un optimum local. C'est le cas par exemple lors des tests effectués sur l'ensemble de données Colon où à chaque intervention de l'utilisateur on voit bien sur la figure 2 l'amélioration de la courbe et la convergence de l'algorithme en comparaison avec l'AG (Boudjeloud et Poulet, 2005) aux mêmes générations (Viz-IGA converge en moins de générations que l'AG). Cependant, on voit sur la courbe de VIz-IGA des moments de stagnation, par exemple entre les générations 800 et 1000, les générations 1200 et 1400 (il y a deux niveaux de stagnation) et les dernières générations à partir de la génération 1600. Dans ces cas là, il peut arriver que les mêmes solutions soient présentées à l'utilisateur plusieurs fois à la suite. 
Modélisation de l'expertise
La visualisation des résultats obtenus sur quelques ensembles de données (figure 3) montre bien que les points détectés sont éloignés et présentent un comportement atypique par rapport au reste des données, néanmoins nous ne pouvons fournir plus d'explication sur le type des points détectés par notre algorithme (par exemple erreur ou "outlier réel"). En effet, dans le cas de valeurs extrêmes on ne sait pas dire si cette valeur est une valeur possible ou non. Seul l'expert des données peut répondre à cette question. Dans le cas où le point détecté est une erreur on l'élimine de l'ensemble des données et dans le cas contraire on le garde dans les données car il peut représenter à lui seul des informations importantes. Un des moyens de combler cette lacune est de créer un modèle des données permettant de qualifier les éléments détectés comme outliers ou erreurs. Ainsi, étant donné un nouvel élément introduit dans l'ensemble des données, nous pourrons utiliser le modèle pour prédire son état : outlier, erreur ou donnée normale.
FIG. 3 -Visualisation des résultats sur les différents ensembles de données.
Nous proposons donc de construire un modèle de l'expertise de l'expert. Celui-ci doit tout d'abord étiqueter les éléments qui ont été détectés comme étant outliers (on peut supposer qu'il n'y a que 2 types d'éléments : les erreurs et les "vrais outliers"). A partir de cet ensemble de données étiquetées, on utilise un algorithme de classification supervisée (par exemple un algorithme d'induction d'arbre de décision) pour construire un modèle de l'expertise du spécialiste des données. Les nouveaux éléments outliers seront alors analysés avec le modèle construit et la présence de l'expert n'est plus indispensable pour qualifier ces outliers.
Construction du modèle
Concernant la partie expertise de la détection d'outlier, nous n'avons pas pu avoir accès à un ensemble de données avec un spécialiste pouvant étiqueter les éléments détectés. Nous avons donc décidé de le faire à partir de l'ensemble de données Colon Tumor (2000 dimensions, 62 éléments) de (Jinyan et al., 2002). Le nouvel ensemble de données est créé en rajoutant des éléments que nous avons étiqueté nous même d'erreur ou d'outlier. Par exemple des éléments qui présentent des valeurs extrêmes sont des erreurs et ceux qui présentent un comportement différent par rapport au reste des données sont des outliers. Nous obtenons donc l'ensemble Colon plus quelques nouveaux éléments. L'ensemble de données Colon a 62 éléments, 5 ont été détectés comme outlier par notre algorithme, nous les étiquetons comme tel, nous créons leurs clones (5), ces clones ont les mêmes valeurs que les originaux sur l'ensemble des dimensions mais sont présentés dans un ordre différant en permutant les valeurs de certaines dimensions. Nous rajoutons au nouvel ensemble de données 10 éléments avec plusieurs valeurs extrêmes qui vont être considérés comme erreurs. Nous obtenons un ensemble de données que nous avons appelé "Colon-Bis" de 2000 dimensions, 77 éléments et trois classes :
Classe 1 : données correctes (57 éléments). Classe 2 : outliers (10 éléments). Classe 3 : erreurs (10 éléments). L'ensemble de données crée n'a qu'un petit nombre d'éléments erreurs et d'éléments outliers, s'ils étaient nombreux, ils ne seraient plus considérés comme tels. Nous avons pris pour l'ensemble d'apprentissage 67 éléments choisis aléatoirement et les 10 éléments restants pour l'ensemble de test. Reste à choisir un algorithme d'apprentissage qui pourra prédire la classe des nouveaux individus. De nombreux algorithmes d'apprentissage automatique peuvent être utilisés, nous avons choisi comme algorithmes les k-PPV (Cover et Hart, 1967), C4.5 (Quinlan, 1993), CART (Breiman et al., 1984) et LibSVM (Fan et al., 2005). Nous avons effectué des tests dont nous présentons les résultats dans le tableau 1. Ces résultats sont très satisfaisants, nous arrivons à prédire les nouveaux éléments avec un taux de précision de 100% avec le modèle établi par LibSVM. Une fois le modèle établi, le besoin d'étiqueter par le spécialiste des données n'est plus nécessaire. 
Conclusion
Nous avons présenté un algorithme génétique semi-interactif pour la sélection de dimensions appliqué à la détection d'outlier. Nous avons introduit une nouvelle représentation de l'individu de l'algorithme génétique. Notre choix s'est fixé sur des petites tailles de sous-ensembles de dimensions pour faciliter l'interprétation visuelle des résultats et souligner la pertinence des dimensions pour chacune des applications, ajoutant ainsi un aspect d'aide à la décision. Cependant, l'utilisateur est libre de fixer la taille des sousensembles de dimensions. Il peut aussi intervenir sur le choix de la méthode visuelle utilisée et sur l'ordre des dimensions dans la visualisation proposée. Notre algorithme nous permet la détection d'outliers dans des ensembles de données ayant un grand nombre de dimensions en n'utilisant qu'un sous-ensemble de dimensions de l'ensemble initial. Puisque le nombre de dimensions utilisé est faible, on peut ensuite visualiser ces éléments (à l'aide de matrices de scatter-plot ou de coordonnées parallèles) pour permettre à l'utilisateur de choisir les solutions qui lui paraissent pertinentes. Elles sont alors utilisées pour générer et visualiser d'autres solutions et aussi pour expliquer et valider les résultats obtenus. Il ne faut pas oublier que l'on travaille sur des données de grandes dimensions. Cette étape n'est possible que parce que nous n'utilisons qu'un sous-ensemble restreint de dimensions de l'ensemble de données initial. Cette interprétation des résultats serait absolument impossible en considérant l'ensemble des dimensions comme le montre la figure 4.
FIG. 4 -Visualisation de quelques centaines d'attributs de l'ensemble de données Colon Tumor.
Nous pensons étendre nos applications à des données symboliques et aussi améliorer notre méthode pour optimiser l'ordre des dimensions dans les visualisations en utilisant des critères de validité et étendre la méthode au clustering. Viz-IGA a permis de montrer que l'on peut gagner beaucoup en augmentant l'interaction entre l'expert du domaine et son outil de fouille de données. Pour finir, l'étude des AGI est certainement prometteuse dans d'autres domaines. Ces algorithmes proposent une interaction très simple et efficace pour un utilisateur non informaticien, ce qui peut leur assurer un certain succès dans les applications nécessitant une interaction homme/machine. Nous avons fait coopérer les méthodes automatiques et les méthodes de visualisation de données sur deux aspects, l'interaction avec l'utilisateur dans le processus de recherche en le faisant participer dans la sélection et l'évaluation des solutions proposées par l'AG et dans l'interprétation et la qualification des éléments détectés comme outlier à travers le modèle d'expertise du spécialiste des données. Nous avons proposé une partie expertise, à l'aide des visualisations présentées l'expert des données peut qualifier les outliers détectés (par exemple en deux classes : erreur ou élément significativement différent de la masse). Il ne faut pas oublier que l'on travaille sur des fichiers de grandes tailles. Cette étape n'est possible que parce que nous n'utilisons qu'un sous ensemble restreint de dimensions de l'ensemble de données initial. Cette qualification des outliers serait absolument impossible en considérant l'ensemble des dimensions comme l'illustre très bien l'exemple de la figure 4 où l'on ne peut détecter aucune information à propos des éléments de l'ensemble de données ou des dimensions. Une fois la qualification effectuée, nous utilisons un algorithme d'apprentissage pour créer un modèle de l'expertise du spécialiste des données. Les nouveaux outliers peuvent alors être qualifiés par le modèle construit sans la présence de l'expert des données. Cette étape souligne l'importance de la visualisation de données pour l'interprétation des résultats et son apport pour l'aide à la décision. Les tests effectués pour l'expertise ont été effectués sur un ensemble de données artificiel crée par nos soins car nous n'avons pas pu avoir accès à un ensemble de données et un spécialiste pouvant qualifier les éléments détectés d'erreur ou outlier réel. Nous avons obtenu des résultats satisfaisants par ce premier travail qui nous a permis de faire participer l'utilisateur dans le processus de recherche de sous-ensembles de dimensions pertinents pour détecter, interpréter visuellement et qualifier des éléments outliers.

Introduction
Les ontologies ont été créées dans le but de conceptualiser et partager des connaissances de manière structurée (Gruber, 1993). Leur usage en gestion des connaissances s'amplifie avec l'essor du Web sémantique. En effet, les ontologies ont la vertu de se traduire sous des formes très variées depuis de simples taxonomies comme les systèmes catégories (Yahoo, OpenDirectory), en passant par des systèmes de métadonnées interopérables (Dublin Core Metadata initiative) et allant jusqu'aux ontologies lourdes décrivant de véritables théories logiques. Notamment, on trouve des ontologies différentes portant sur le même domaine. Il s'avère donc nécessaire de disposer de techniques pour relier ces ontologies. Dans cette optique, l'alignement vise à trouver des relations entre deux ontologies (entre les classes, les relations, les propriétés...).
Dans la littérature, de nombreux travaux traitent de méthodes d'alignement. Ces approches reposent sur des techniques très différentes (Kalfoglou et Schorlemmer, 2003) comme l'apprentissage bayésien des probabilités jointes entre concepts (Doan et al., 2004), la classification conceptuelle (Stumme et Maedche, 2001), la fusion de schéma de bases de données (Madhavan et al., 2001), les modèles logiques en graphe conceptuels (Fürst et Trichet, 2005), la recherche de morphismes entre graphes représentant les ontologies (Melnik et al., 2002). La distinction entre ces travaux peut être faite au niveau des méthodes d'alignement utilisées pour la comparaison. La classification proposée par (Euzenat et Valtchev, 2003), distingue quatre familles de méthodes : (1) les méthodes terminologiques basées sur des mesures de similarité entre chaînes de caractères ou faisant intervenir une ressource terminologique externe ; (2) les méthodes structurelles comparant, d'une part, deux concepts à partir de mesures de similarité entre les constituants (attributs, propriétés) des concepts ou à partir de leur position respective dans leur hiérarchie (Noy et Musen, 2000) ; (3) les méthodes extensionnelles comparant les concepts à partir de leur ensemble d'instances respectif (Doan et al., 2004) ; (4) les mé-thodes sémantiques basées sur un modèle sémantique théorique utilisé pour la comparaison des concepts (Giunchiglia et al., 2004), (Fürst et Trichet, 2005).
La plupart de ces travaux utilisent des relations symétriques de similarité. Pourtant, d'autres types de relations asymétriques peuvent être utilisées dans le but d'enrichir l'alignement produit. Par exemple, la recherche d'implications (généralisations) permet de trouver les concepts équivalents (exemple : si auto ? voiture et voiture ? auto alors auto ? voiture), mais elle permet aussi de découvrir si un concept est plus général (ou plus plus spécifique) qu'un autre. Parmi les méthodes prenant en compte la relation d'implication, nous pouvons citer S-MATCH (Giunchiglia et al., 2004). Cette dernière évalue entre autre des relations d'équiva-lence et d'implication en s'appuyant sur un thésaurus (Wordnet). L'objectif de notre papier est de proposer une méthode extensionnelle d'alignement basée sur la découverte des relations asymétriques d'implication entre deux ontologies. Nous nous restreignons à des ontologies constituées d'une hiérarchie de concepts dont les concepts sont associés à des documents textuels partageant un vocabulaire commun.
Notre approche est divisée en deux parties consécutives qui utilisent toutes deux le modèle probabiliste d'écart à l'indépendance appelé intensité d'implication (Gras, 1979;Gras et al., 1996) : -L'extraction des termes pertinents pour chacune des deux hiérarchies. Ce processus permet d'extraire, puis de sélectionner, à partir des documents associés aux hiérarchies, un ensemble de termes pertinents pour chaque concept. -L'extraction d'implications entre les concepts sous forme de règles d'association (Agrawal et al., 1993). Nous prenons en compte la relation de spécialisation de chaque hiérar-chie afin d'extraire les règles les plus générales et ainsi réduire la redondance.
Nous présenterons dans une première section, la méthodologie suivie ainsi que la formalisation des données. Ensuite, nous préciserons les détails de notre approche en commençant par la phase d'analyse et de sélection des termes pertinents, suivie de la découverte de règles entre les deux hiérarchies. Finalement, une évaluation expérimentale nous permettra d'analyser et d'évaluer les performances de notre système.
-152 -RNTI-E-6 
Méthodologie et formalisation
Nous avons en entrée du processus, deux hiérarchies conceptuelles (figure 1). Chaque hié-rarchie est composée d'un ensemble de concepts C structurés par une relation d'ordre partiel is ? a (notée par la suite ?). Des parties d'un ensemble de documents D sont associées aux concepts de la hiérarchie. Nous représentons une hiérarchie par un n-uplet O = (C, ?, D, ? 0 ), où ? 0 ? C × D est une relation qui associe les concepts aux documents. ? 0 (c) désigne l'ensemble des documents associés au concept c. Chaque document d est composé d'un ensemble de termes {t|t apparait dans d}. Nous appellerons T , l'union des ensembles de termes contenus dans les documents et ? ? T × D, la relation associant les termes aux documents. Nous noterons ?(t), l'ensemble des documents qui contiennent le terme t. A partir de la relation d'ordre partiel portant sur les concepts, nous définissons la relation :
Une première étape (figure 1) d'extraction, de sélection et d'association des termes aux concepts, nous permet de représenter une hiérarchie O par le quadruplé O = (C, ?, T , ? 0 ), où T ? T représente l'ensemble des termes sélectionnés, et ? 0 ? C × T est une relation qui associe à chaque concept, ses termes significatifs. Nous notons ? 0 (c), l'ensemble des termes significatifs du concept c. A partir de la relation d'ordre partiel ?, nous définissons la relation suivante :
La deuxième étape concerne la découverte d'implications significatives entre les concepts issus des hiérarchies O
. Pour cela, nous nous appuyons sur le modèle des règles d'association (Agrawal et al., 1993). L'extraction des règles est réalisée sur l'ensemble des termes communs aux deux hiérarchies. Ainsi, nous définissons  O'
FIG. 2 -prétraitements linguistiques et extraction des termes pertinents
. La règle A ? B signifie que les termes significatifs du concept A ont tendance à être significatifs du concept B.
Extraction et sélection des termes significatifs
Notre objectif consiste à extraire un ensemble de termes significatifs pour chacun des concepts. L'idée principale de ce processus est la suivante : Un terme t sera significatif d'un concept c si il existe relativement peu de documents contenant le terme t qui ne sont pas associés au concept c. Nous choisissons d'associer le terme t au concept c si la règle d'association t ? c est significative selon l'intensité d'implication.
Afin d'extraire les règles t ? c, nous traduisons les relations ? et ? par la table a, figure 2. Ainsi, chaque document est représenté avec d'une part les concepts auxquels il est associé, et d'autre part les termes qu'il contient. L'ensemble des termes T 0 est constitué de verbes et de termes binaires (termes composés de deux mots significatifs). L'extraction de ces termes binaires se justifie par le fait qu'ils sont plus porteurs d'information et donc moins ambigus que de simples mots. L'acquisition des termes binaires est réalisée par le logiciel ACABIT (Daille, 2003) à partir des textes préalablement étiquetés (grammaticalement) et lemmatisés par la suite logicielle MontyLingua (Liu, 2004).
La deuxième étape (étape 2, figure 2) consiste à évaluer toutes les règles d'association binaires t ? c, avec t ? T 0 et c ? C, afin de constituer pour chaque concept c, un ensemble de termes significatifs ? 0 (c) défini par :
où n t?c = card(I(t) ? ?(c)) représente le nombre observé de documents contenant le terme t qui ne sont pas associés au concept c et N t?c représente le nombre attendu (sous hypothèse d'indépendance des descriptions t et c) de documents contenant le terme t qui ne sont pas associés au c. Comme les phénomènes étudiés sont rares, nous modélisons la variable aléatoire N t?c par une loi de Poisson de paramètre ? = n t .n c /n où n t est le nombre de documents contenant le terme t, n c , le nombre de documents non associés au concept c et n le nombre total de documents.
Notons que les fréquences des termes dans les documents ne sont pas prises en compte, nous nous intéressons seulement à la présence ou absence des termes dans les documents.
Découverte d'implications significatives entre concepts
Critères de sélection d'une implication significative
A partir de deux hiérarchies O 1 et O 2 , cette deuxième partie a pour objectif de découvrir des implications sous forme de règles binaires entre les concepts des deux structures. Pour cela, nous nous inspirons de la méthode de découverte de règles d'association généralisées proposée par (Srikant et Agrawal, 1995). 
FIG. 3 -Implication entre concepts
La significativité d'une règle est exprimée par la valeur d'intensité d'implication et par le caractère spécifique de sa conclusion combiné à la généralité de sa prémisse. Ainsi pour A ? C 1 et B ? C 2 , une règle A ? B sera dite "significative" si :
Le deuxième critère traduit la capacité d'une règle à générer d'autres règles. Il permet ainsi de réduire la redondance dans l'ensemble des règles extraites. En effet, à partir de la règle A ? B, nous pouvons déduire toutes les règles de la forme :
Ainsi, nous dirons que la règle A ? B est génératrice de l'ensemble des règles de type X ? Y . Par exemple, sur la figure 3, la règle A2 ? B4 permet de générer les règles A2 ? B1, A4 ? B4, A5 ? B4, A4 ? B1 et A5 ? B1.
Contrairement à (Srikant et Agrawal, 1995) qui s'appuie sur les mesures de support et de confiance, nous préférons utiliser la mesure d'intensité d'implication ?. La confiance et le support ne sont pas adaptés pour découvrir des règles génératrices car ils favorisent les règles ayant des conclusions générales. En effet, la confiance d'une règle A ? B est plus grande ou égale à la confiance d'une règle de la forme A ? Z avec Z ? 2 B. Par contre, ? prend en compte la taille de la conclusion et tend vers 0 quand la conclusion devient trop générale : par exemple, sur la figure 3, ?A ? C 1 , ?(A ? B1) = 0 car ? 1?2 (B1) = T 1?2 .
Algorithmes
La prise en compte de la relation d'ordre partiel entre les concepts des hiérarchies permet d'optimiser la phase de sélection des implications significatives. En effet, une recherche descendante de règles (du haut de la hiérarchie vers le bas) nous permet d'éviter l'évaluation Le premier algorithme (figure 4) prend en entrée un concept A de la hiérarchie O 1 , et un ensemble de concepts B courant ? C 2 de O 2 . Pour chacun des concepts de B courant , la recherche et la sélection des conclusions valides sont effectuées par le second algorithme (figure 5). Ensuite, la procédure est relancée récursivement sur les fils de A et une copie de l'ensemble B courant mis à jour par le second algorithme. L'ensemble de concepts B courant recense les sous-parties de la hiérarchie O 2 qui ne contiennent aucun concept figurant dans les conclusions des règles sélectionnées lors des récursions précédentes. Cette liste permet d'éviter d'évaluer des règles ne satisfaisant pas le critère 2 explicité dans la section 4.1.
Le deuxième algorithme (figure 5) prend en entrée un concept A de la hiérarchie O 1 , et un concept B de O 2 . Il recherche parmi l'ensemble {B x |B x ? 2 B}, un sous-ensemble de conclusions valides pour A. Une conclusion B s sera sélectionnée, si elle respecte les deux critères de la section 4.1.
La recherche se faisant de manière descendante dans la hiérarchie des descendants de B, nous choisissons d'arrêter la recherche dans une des branches si ?(A ? B x ), est en dessous de la valeur seuil ? r et si aucune spécialisation de la conclusion ne permettra de repasser au dessus du seuil ? r . Pour cela, nous nous appuyons sur une propriété de l'intensité d'implication qui définit A ? B x comme étant la meilleure spécialisation de la conclusion de A ? B x .
Lors de la recherche de règles, nous ignorons les racines des hiérarchies. En effet, les concepts racines des hiérarchies sont associés à l'ensemble des termes étudiés. Ainsi, la valeur d'intensité d'implication, ne peut être évaluée ou est nulle si un concept racine est en prémisse ou en conclusion. 
Résultats et analyse
Afin d'analyser le comportement de notre méthode de manière quantitative et qualitative, nous avons étudié dans un premier temps, l'influence du choix des valeurs seuils d'intensité d'implication ? t et ? r sur le nombre de règles sélectionnées. Pour les deux jeux de test présen-tés ci-dessus, nous avons testé notre algorithme en faisant varier (de 0,8 à 1) les valeurs seuils pour la sélection des groupes de termes pertinents et pour l'appariement de concepts.
Sur les deux graphiques de la figure 6, on peut remarquer que le choix du seuil ? t influence plus la quantité d'implications extraites. Prenons l'exemple de l'alignement des catalogues Cornell et Washington : pour une augmentation du seuil ? t (resp. ? r ) de 0,1 unité, le nombre d'implications extraite baisse en moyenne de 2,15 (resp. 1,2).
Dans un deuxième temps nous avons réalisé un test qualitatif à partir du jeu de test "Course catalog" et des alignements manuels fournis sur le site de A. Doan. Nous avons confronté les résultats produits par notre approche aux alignements manuels. Comme ces derniers sont de nature symétriques, nous avons procédé aux recherches d'implications dans les deux sens (Cornell vers Washington, puis Washington vers Cornell). Ensuite, à partir des implications extraites, nous déduisons des relations d'équivalences. Sur la figure 7, nous notons, comme pour le nombre d'implications extraites (figure 6), une plus grande influence du seuil ? t par rapport au seuil ? r . Nous pouvons observer une évolution de la précision sur des bons scores (de 0,71 à 1). Le choix des deux seuils influencent quasiment de la même manière l'évolution de la précision. Cependant, le rappel présente une faible moyenne de 0,29 (meilleur score égale à 0,54).
Ces scores mitigés au niveau du rappel sont justifiés, premièrement, par l'existence de concepts pour lesquels il n'y a aucun terme associé. Ce problème est dû à un manque de spéci-ficité du vocabulaire utilisé dans les descriptions des concepts et à une sélection trop stricte des règles terme ? concept. Ces implications entre concepts sont intéressantes même si elles ne figurent pas dans les alignements manuels. Nous pouvons aussi noter que notre méthode n'est pas sensible aux noms des concepts (une approche ne faisant intervenir qu'une similarité entre chaînes de caractères ne pourrait pas trouver la règle "Cognitive Studies Program -> Psychology PSYCH"). Ainsi, notre méthode prend en compte la sémantique des concepts.
-160 -RNTI-E-6
Conclusion
Nous avons proposé dans ce papier une approche d'alignement d'ontologies basée sur la découverte de relations d'implication significatives entre concepts provenant de deux ontologies distinctes. Notre méthode, est décomposée en deux phases : (1) L'extraction à partir des documents puis l'association d'un ensemble de termes pertinents pour chaque concept. (2) La découverte de règles d'implication significatives entre concepts en nous appuyant sur les termes sous-jacents. Les intérêts de notre méthode sont d'une part la prise en compte de la sé-mantique en utilisant des termes binaires contenus dans les documents associés aux concepts, et d'autre part la recherche d'implications permettant d'enrichir l'alignement produit. Notre démarche a été testée sur deux jeux de données réels portant respectivement sur des profils d'entreprises et sur des catalogues de cours d'universités. La confrontation des résultats obtenus et des alignements manuels fournis avec le jeu de test montrent tout d'abord que notre approche distingue parmi les alignements manuels des relations d'équivalence et des relations d'implication. De plus notre méthode permet d'extraire des relations pertinentes qui sont ignorées par l'ensemble d'alignements manuels.
Actuellement, nous n'avons considéré que des hiérarchies conceptuelles associées à un corpus de documents. Nous projetons d'étendre notre approche afin d'exploiter la définition et la structure interne des concepts, puis de permettre la recherche d'implications entre les relations.

Introduction
Les indicateurs techniques sont des fonctions des données de marché, historiques et actuelles, qui produisent un signal d'achat ou de vente. Ce sont les « briques » qui permettent de construire des stratégies de « trading » en réaction aux indicateurs et en fonction de la composition du portefeuille de l'investisseur.
Certaines techniques de fouille de données permettent d'attribuer une signature aux configurations de marché précédant le déclenchement d'un indicateur technique. La comparaison des performances de l'indicateur seul et du même indicateur précédé d'une signature permet de choisir les signatures qui améliorent les performances de l'indicateur. La stratégie de « trading » peut donc exploiter un filtrage pour se restreindre aux transactions qui se déclenchent après une signature et qui ont produit des gains dans le passé. Les signatures sont testées sur une période d'apprentissage afin d'exclure celles pour lesquelles l'indicateur technique a sous performé. Les meilleures signatures sont gardées pour la période suivante, dite de validation ou de « trading » pendant laquelle l'indicateur technique est pris en compte uniquement s'il est précédé par une « bonne » signature. L'analyse des signatures montre une remarquable stabilité temporelle, et l'indicateur filtré par les « bonnes » signatures de la pé-riode d'apprentissage sur performe régulièrement l'indicateur seul pendant la période de validation.
Nous utilisons une technique de fouille de données classique (i.e., la recherche de motifs fréquents) pour identifier les signatures qui caractérisent les quelques jours précédant le déclenchement de l'indicateur technique. Nous obtenons ainsi des règles d'analyse technique spécifiques au sous-jacent, basées sur plusieurs jours et adaptées à la période considérée.
Une première contribution de ce travail consiste à caractériser les configurations de marché afin que l'extraction de motifs fréquents puisse s'appliquer sur ce type de données. A notre connaissance, l'idée d'améliorer un indicateur technique par la méthode des signatures est également originale. Cet article ne propose pas un nième indicateur technique mais plutôt une méthodologie générale pour améliorer des stratégies quantitatives existantes.
L'amélioration des indicateurs techniques est un objectif bien différent de la prédiction des cours boursiers et a reçu moins d'attention. F. Allen et R. Karjalainen ont travaillé sur la combinaison et le calibrage des seuils de déclenchement des indicateurs par la programmation génétique (Allen et Karjalainen 1999). Les travaux plus récents de Becker rapportent que les règles de « trading » générées par la programmation génétique peuvent battre la stratégie « acheter et garder » en tenant compte des frais de transaction (Becker 2003). Ces travaux concernent le paramétrage d'un ensemble de règles de « trading » déjà connues et ne cherchent pas à caractériser une configuration de marché en addition d'un indicateur technique. Notre approche est donc complémentaire à ce type de travaux.
La Section 2 présente la méthodologie appliquée et le système de « trading » réalisé est rapidement présenté en Section 3. La Section 4 présente les résultats expérimentaux obtenus sur des données réelles. La Section 5 est une brève conclusion.
Méthodologie
Pour mettre en place la stratégie de « trading » basée sur des signatures il faut d'abord dé-finir le contenu d'une configuration de marché (scénario) et le transformer dans le format attendu par l'algorithme d'extraction des motifs utilisé (discrétisation et agrégation)..
La description des scénarii
Un scénario regroupe l'ensemble des données de marché historiques sur « N » jours qui constitue le domaine de l'indicateur. Les indicateurs techniques sont donc des fonctions des scénarii f qui associent à un scénario une valeur dans l'ensemble {Acheter, Vendre, Rien}. Par exemple, la fonction « f » pourrait être de type Acheter, si la moyenne mobile sur 2 semaines est supérieure à la moyenne mobile sur 30 jours, sinon Rien. Il s'agit évidemment d'un exemple hypothétique.
Comme dans la plupart des travaux dans ce domaine, chaque journée composant un scé-nario sera caractérisée par la valeur de l'indicateur technique, le cours d'ouverture (O) et de clôture (C) ainsi que les maxima (H) et minima (L) journaliers. Le nombre de jours « N » composant un scénario est déterminé empiriquement, et s'étend généralement sur le domaine de l'indicateur technique.
De nombreuses techniques de fouille de données travaillent sur des données discrètes et supposent un pré-traitement. Ainsi, nous voulons transformer les données de marché composant les scénarii en une représentation non numérique. L'association de chaque valeur à sa qquantile1 apparaît comme un choix de discrétisation simple facilement mis en oeuvre. Les deux scénarii quantilisés associés aux journées J1 et J2 seront graphiquement représentés comme suit : La méthodologie exposée dans cet article associera une signature aux scenarii fréquents en passant par l'extraction de motifs fréquents. L'extraction de motifs fréquents est l'une des tâches de fouille de données qui a été la plus étudiée depuis une dizaine d'années et nous supposons que le lecteur est familiarisée avec cette technique. Une annexe rappelle cependant le principe algorithmique qui a été utilisé.
Un motif f-fréquent (noté simplement fréquent dans la suite) est un sous-ensemble commun à une collection d'ensembles (scénarii dans le cas présent, collection de transactions dans le cadre classique des données de ventes) qui apparaît en au moins f ensembles. L'extraction des motifs fréquents intervient sur des ensembles de valeurs et il faut donc représen-ter chaque scénario sous une forme transactionnelle. On préfère souvent une représentation suffisamment riche pour pouvoir reconstituer le scénario de départ à partir d'un ensemble non ordonné, i.e., une bijection. La bijection a l'avantage de pouvoir retrouver la configuration de marché à partir de sa signature, et permet à un expert humain de comprendre a posteriori les règles générées par la technique de fouille de données.
En introduisant un système de coordonnées (x, y) comme sur la Figure 2, il est possible de définir une bijection pour chaque valeur composant un scénario.
Un exemple de transformation est donné par la formule : Code_ens = 5*x*q+y*q+value(x,y) où " q " est le nombre de quantiles et 5 le nombre de données par jour (indicateur, ouverture, clôture, maxima, minima).
FIG. 2 -Transformation de la configuration de marché en une représentation ensembliste.
Pour les deux dernières colonnes des scénarii de la Figure 1, on obtient les ensembles suivants : (q=3). Le calcul de la seconde valeur de la colonne J2-1 a été explicité. J1={2 ; 6 ; 9 ; 11 ; 13 ; 16 ; 5*3*1+3*1+2=20 ; 24 ; 25 ; 29} J2={3 ; 6 ; 9 ; 10 ; 15 ; 17 ; 20 ; 24 ; 25 ; 29} Après l'étape de discrétisation et de codification, chaque jour est représenté par un ensemble de 5 x N symboles obtenus par la formule précédente. C'est bien un ensemble et non pas une séquence, l'ordre des symboles n'a plus d'importance.
Les signatures
On souhaite caractériser les scénarii fréquents qui peuvent être à la base d'une stratégie quantitative. Dans le contexte d'amélioration des indicateurs techniques il n'est pas intéres-sant de signer les événements rares : il est souhaitable que la fréquence du motif soit comparable à la fréquence de l'indicateur et les motifs fréquents apparaissent donc opportuns.
La signature d'un scénario est un sous-ensemble (sous-motif) fréquent (dans les ensembles des scénarii) des motifs qui le décrivent. Les scénarii peu fréquents n'ont pas forcément de signature et un scénario peut avoir plusieurs signatures.
Nous donnons un exemple de signature (i.e., un sous-motif commun) des deux scénarii de la Figure 1. Le sous-ensemble commun de J1 et de J2 est {6 ; 9 ; 20 ; 24 ; 25 ; 29} ce qui, grâce à la bijection, correspond au motif visuel donné dans la Figure 3.
Notons que les sous-ensembles des signatures sont également des signatures. Dans le contexte de l'amélioration des indicateurs, la période étudiée comprend quelques centaines d'ensembles et les seuils de fréquences utilisés sont de l'ordre de quelques dizaines. RNTI-E-6 I. Albert-Lorincz et al. 
Algorithme d'extraction
L'extraction des sous-ensembles d'une collection de scénarii est une tâche difficile car les approches naïves se heurtent à une explosion combinatorique. De très nombreux algorithmes ont été proposés pour réaliser des extractions complètes de tous les ensembles fréquents dans des contextes réalistes. Pour notre système, nous avons utilisé notre propre implémentation des « FP-Tree » (Han et al. 2004) (voir Annexe) étendue à la gestion des motifs fermés (Pasquier et al. 1999). La structure « FP-tree » a fait ses preuves lors des extractions de très grande échelle. Elle est adaptée aux tâches courantes en finance quantitative qui ne concernent généralement que quelques milliers d'ensembles. L'extraction se fait quasiment en temps réel, ce qui rend la méthode proposée éligible pour le « trading » haute fréquence.
Le système de « trading »
Le comportement du sous-jacent est assujetti à des phénomènes de mode et à des régimes de « trading » qui varient dans le temps. Aussi, est-il nécessaire de suivre la validité des signatures dans le temps, d'enrichir la base des signatures par les nouveaux motifs qui apparaissent dans les cours et d'écarter (invalider) les motifs qui commencent à sous performer. Nous décrivons maintenant ce mécanisme d'apprentissage progressif. Les dates T0 à Tn déterminent n-1 périodes. La première est utilisée pour l'initialisation du système (apprentissage des « bonnes » signatures) qui seront utilisées pour le « trading » pendant la période suivante. A partir de l'apprentissage initial, chaque période sert à traiter avec les bonnes signatures identifiées jusqu'au début de la période et alimente les signatures de la période suivante.
La première période fournit les motifs m1. Cet ensemble est testé contre la performance de l'indicateur seul pendant cette même période pour n'en garder que les motifs µ1 qui amé-liorent les performances de l'indicateur. Le contenu de µ1 générera des transactions de la deuxième période, et sera ajouté aux motifs m2 de cette période pour calculer l'ensemble µ2, contenant les signatures de bonne qualité identifiées jusqu'à T2 (la fin de la 2° période).
Le système de « trading » bâti autour de ce re-balancement est illustré sur la Figure 4. Suivons d'abord les flèches qui émergent de la période p-1 jusqu'au rectangle « filtrage ». Cette partie correspond à l'apprentissage du système. L'étape « extraction » fournit les motifs fréquents de la période p-1. La mesure de performance évalue les gains de chaque motif fréquent identifié. Pour ce faire, la stratégie de « trading » à optimiser est effectuée sur la période p-1 et l'on calcule la performance de l'indicateur seul. Ensuite, pour chaque motif fréquent extrait, la stratégie est à nouveau appliquée. Cette fois, l'indicateur n'est retourné vers la stratégie que si son scénario d'occurrence contient le motif en cours d'étude. Si la performance de la stratégie précédée par un motif est supérieure à la performance de l'indicateur seul, le motif est ajouté à l'ensemble µp-1, i.e., la collection des bonnes signatures, et utilisé par le « trading » de la période suivante. Les indicateurs de la période p sont pris en compte uniquement si leur scénario d'occurrence contient une « bonne » signature de la pé-riode précédente. Dans ce cas, le filtrage génère un signal qui est envoyé à la stratégie. Après le trading, la période p devient une période d'apprentissage pour la période p+1.
FIG. 4 -Système de trading.
Un système de « trading » sur trois périodes (apprentissage, sélection, trading) est difficile de mettre en ouvre sans une diminution drastique de la fréquence des transactions. En fait, les signatures sont attachées à un régime de marché et leur apport disparaît si elles ne sont pas appliquées immédiatement en « trading » (impossibilité d'avoir une période de sé-lection). Le raccourcissement de la période d'apprentissage est délicat, car la diminution de points pour l'étape de fouille de données conduit à un sur-apprentissage, i.e., un phénomène de sur-spécialisation qui produit des bonnes performances pendant la période d'apprentissage mais qui se comporte de manière médiocre ultérieurement.
Pour démontrer le caractère générique de la méthode, elle a été appliquée à des données financières de type « action » et « fixed income », qui ont des comportements (volatilité, retour à la moyenne, etc.) très différents. Les indicateurs utilisés sont simples et bien connus dans le monde financier : Williams %R (Achelis 2000), croisement de moyennes mobiles, et un indicateur confidentiel, appelé boite noire (BN). La période étudiée s'étend sur cinq ans. La performance a été mesurée comme le gain réalisé en « N » jours.
Le Williams%R permet de déterminer quand un actif est sur vendu ou sur acheté sur une période (d'habitude 2 semaines). On voit bien que l'indicateur signé ne surpasse pas l'indicateur seul sur toutes les pério-des. Elle a néanmoins un meilleur comportement en moyenne. La somme des retours journaliers (dS/S) pour le Williams %R signé termine à 12.59 en retour cumulé alors que l'indicateur seul ne produisait que 7.29 sur 10 ans, soit un gain de 5.3% avec la méthode des signatures.
L'analyse des actions composant l'indice CAC40 depuis le 1 er janvier 2000 a permis de valider l'apport de la technique des signatures sur un grand nombre de périodes et d'actions. La méthode proposée a apporté 6% de gains par rapport au seul W%R sur 5 ans. Compte tenu du nombre de périodes comparées, le test t de Student a permis de conclure que les performances de la technique signée sont supérieures à l'indicateur seul de manière significative. Pour une stratégie quantitative dans la finance, ce gain est énorme ! La moyenne des performances de l'indicateur W%R signé est 96.93, comparé au W%R qui fait 90.63. La P(T<=t) unilatéral est 10,6%. Les tests peuvent être obtenus auprès des auteurs.
Nous avons également testé l'apport des signatures sur un indicateur propre à l'entreprise CIC et qui est appelé boîte noire (BN). L'amélioration est encore plus significative (seuil de 1%). La moyenne de la BN signé est 104.22, alors que la BN seul n'a fait que 98.13%. Cette fois la P(T<=t) unilatéral valait 0.011, ce qui est un très beau résultat. Données de type taux d'intérêt. Compte tenu de l'efficacité accrue des marchés de capitaux, le W%R ne dégage pas de performance sur la période étudiée. Cependant, les signatures permettent de baisser les pertes. Comme les graphiques suivants le montreront, par pério-des le W%R signé produit des gains alors que le W%R s'inscrit dans une tendance baissière. La Figure 5 illustre le « wealth process » de la stratégie, i.e., la valeur du portefeuille produit par la stratégie tout au cours de sa vie. Le W%R signé se distingue pendant les périodes ou le W%R perd, voir il réalise des gains par période (de 92.93 à 96.13, soit 3.2% de mieux).
FIG. 5 -Wealth process du taux swap 5 ans.
Le Tableau 2 illustre les arguments développés pour la Figure 5 
Discussion
La stratégie de « trading » en réponse de l'indicateur a été paramétrée pour mesurer les performances à N jours après la transaction. (5 jours pour les statistiques précédentes). Les signatures caractérisent une configuration de marché ponctuelle et ont un pouvoir de prédic-tion d'environ une semaine. Il est normal que leur validité soit limitée dans le temps : l'ensemble qui décrit la configuration de marché n'inclut aucune information sur le comportement historique du cours. La "mémoire longue" et l'auto corrélation ne sont pas utilisées. Nous pensons qu'en enrichissant la description des scenarii par des éléments du passé (minimum et maximum sur un passé plus long, moyenne mobile), le pouvoir de prédiction peut être étendu de quelques jours, mais l'extraction des motifs fréquents devient plus difficile à cause d'un alphabet plus large. La durée de vie d'une signature est assez courte, elle dépasse rarement trois périodes (y compris celle d'apprentissage).
Dans le cas du W%R, les signatures sur performent la stratégie non signée en perdant un peu moins d'argent. On doit alors se demander si l'amélioration des performances n'est pas essentiellement liée au filtrage ? En effet, appliquer moins souvent une stratégie perdante améliore forcément les performances. Cependant, l'étude visuelle intuitive du comportement des signatures sur la Figure 5 nous laisse à penser que non ! En effet, dans le cas d'un filtrage aléatoire, l'écart entre les deux courbes devrait se constituer progressivement, alors que l'indicateur filtré se distingue aux moments ou l'indicateur seul commence à perdre. De plus, le filtrage produit des résultats significativement meilleurs pour l'indicateur BN, et présente des gains alors que la stratégie de base est légèrement déficitaire. Stratégies croisement des moyennes mobiles. Ce type d'indicateur est basé sur la mémoire longue non corrélée avec les signatures qui caractérisent la configuration présente. Aussi, ces indicateurs ne sont améliorés par la méthode des signatures. Frais de transactions. L'effet du filtrage par les signatures est de limiter le nombre des transactions en gardant seulement celles qui ont le plus de chances d'aboutir. En conséquence, il réduit également les frais de transaction. L'étude présente sous-estime donc l'apport des signatures d'une part en négligeant les économies sur les frais de transactions et d'autre part en ignorant la rémunération du compte de dépôt.
Conclusion
Les tests effectués promettent d'aboutir à un système de « trading » automatique. Le « back testing » des indicateurs avec re-balancement s'est montré capable d'identifier les signatures caractéristiques à chaque période. Elles sont suffisamment stables dans le temps pour pouvoir améliorer les performances de l'indicateur technique durant la période suivante. Les techniques décrites apportent un moyen pour améliorer les stratégies quantitatives existantes et ne demandent pas la mise en oeuvre de nouvelles stratégies. Dans les exemples traités, la technique des signatures a produit 3% et 6% de gains en plus par rapport à l'indicateur seul pour trois indicateurs différents. D'autres mesures sont en cours pour tester l'impact de la méthode sur davantage d'indicateurs techniques. L'application à des données « haute fré-quence » est également à l'étude.
Remerciements. Les auteurs souhaitent remercier Swann Chmil (BNP Paribas Asset
Management) pour sa contribution à cette recherche et les relecteurs anonymes pour leurs suggestions.
Pour simplifier la présentation, chaque scénario sera représenté par un extrait de l'ensemble qui le décrit. Les lettres tiennent pour des valeurs discrétisés précédemment. On cherchera à identifier tous les sous-ensembles 2-fréquents (présents dans au moins deux ensembles) des extraits du Tableau 1 avec l'algorithme FP-growth (Han et al. 2004).
A) La première étape consiste à parcourir tous les ensembles pour trouver la fréquence de chaque symbole. Les ensembles sont ensuite réorganisés selon un ordre donné par la fré-quence des symboles : (f:3) > (d:3) > (c:3) > (a:2) > (b:2), en laissant de coté les symboles qui ont une fréquence d'apparition inférieure au seuil requis (3° colonne du Tableau A1). Le symbole "e" disparaît. On note que l'ordre et le FP-tree résultant ne sont pas uniques. B) Les ensembles réordonnés sont ensuite insérés dans une structure arborescente. Chaque ensemble est représenté par un chemin dans cet arbre, de manière à ce que les sousensembles communs soient proches de la racine. Chaque noeud de l'arbre contient un symbole et un compteur des ensembles qui le contiennent sur cette branche de l'arbre. Concrète-ment, le premier scénario va créer la branche <f:1, d:1, a:1>. Le deuxième scénario partage un préfixe commun avec le premier. Il fait augmenter les compteurs sur la portion commune et créer un fils du noeud "d" pour "c". L'insertion des ensembles restant produit l'arbre à gauche de la Figure A1. Pour des raisons exposées plus loin, cet arbre est enrichi de pointeurs transversaux (Figure A1 à droite) qui réunissent toutes les occurrences d'un même symbole. Le vecteur des pointeurs respecte l'ordre établi au point (A). Toute l'information présente dans les scénarii du Tableau A1 est contenue dans la structure FP-tree. Les chemins de l'arbre regroupent les symboles qui paraissent souvent ensemble, mais il faut utiliser les pointeurs transversaux pour trouver tous les motifs fréquents. En effet, la collection des branches (entre la racine et le symbole) le long des pointeurs d'un symbole "X" quelconque contient tous les ensembles où X est fréquent. Un traitement récursif permet de les extraire. C) Si l'arbre contient plusieurs branches, l'extraction commence par chercher les motifs contenant le symbole le moins fréquent, "b" dans le cas présent. La sommation des compteurs le long de la liste chaînée fournira la fréquence de "b". En suivant cette liste, tous les chemins entre la racine de l'arbre et le noeud contenant le symbole courant "b" {fdc:1, dca:1} sont regroupés. La fréquence de chaque branche est égale à la fréquence du symbole en cours sur la branche. Après l'extraction des branches, la procédure recommence à l'étape A avec les ensembles correspondant aux branches pour trouver tous les motifs fréquents contenant le symbole "b". Comme ni "f ", ni "a" n'apparaissent suffisamment souvent avec "b", l'arbre contiendra un seul chemin (dc:2) qui donnera les motifs fréquents bcd:2, bc:2 et bd:2. Quand l'arbre contient une seule branche, e.g., <d:2, c:2>, les motifs fréquents sont donnés par tous les sous-ensembles des symboles le long de la branche, e.g., d:2, c:2, et dc:2.
Après le traitement de "b", l'extraction continue avec le symbole immédiatement plus fréquent que "b", "a" dans le cas présent. Cette étape livrera les motifs contenant "a" mais ne 
Summary
Frequent itemsets were used to improve the performances of the technical trading indicators. Our method assigns a signature to the frequent market configurations, and overrides the usual trading rules by the information deduced from the signatures. An iterative back testing selects the best signatures, and combines them with the technical indicator to improve its performances. Combining frequent itemsets with technical trading indicators is an original contribution. The method statistically improves the performances of the trading strategies, and the authors recommend it for any automatic trading system based on technical indicators. This study was based on daily interest rate and stock market data, and three different indicators: Williams % R, moment crossing, and a confidential black box strategy. Signatures are particularly well suited for short memory processes.

Introduction
De par le développement rapide des techniques de stockage et de diffusion, les vidéos, notamment digitalisées, sont de plus en plus nombreuses et accessibles. En particulier, les agences de presse, les diffuseurs TV, les agences de publicité travaillent sur des ressources vidéo grandissantes. Pour être à même de travailler sur de tels volumes, des technologies adaptées doivent être mises en oeuvre. La « fouille des usages de la vidéo », qui cherche à analyser les comportements des utilisateurs sur des ensembles de vidéo est l'une des techniques clé émergentes pour optimiser les accès aux vidéos.
Dans cet article, nous proposons d'analyser le comportements des utilisateurs d'un moteur de recherche vidéo pour améliorer la qualité de l'indexation textuelle. Notre objectif est de comprendre pourquoi et comment chacune des séquences vidéo est visionnée. Par exemple, les utilisateurs recherchant des vidéos concernant le mot-clé « montagne » visionnent successivement les vidéos (18,73,29) qui sont retournées dans cet ordre par le moteur de recherche. Si l'on note que dans la majeure partie des cas, la vidéo 29 est visionnée totalement alors que les vidéos 18 et 73 ne le sont que partiellement, on en déduit que, selon l'utilisateur, le concept de « montagne » est mieux exprimé par la vidéo 29 que par les vidéos 18 et 73. En conclusion, la vidéo 29 doit être proposée en premier aux utilisateurs lors des futures recherches sur le concept « montagne ». Son poids dans la vidéo 29 s'en trouve augmenté et celui des vidéos 18 et 73 réduit.
Dans ce papier nous présentons une approche qui combine usage intra-vidéo et usage intervidéo pour générer des profils de visite sur un moteur de recherche vidéo dans le contexte de
FIG. 1 -Description globale du processus de fouille de données vidéo.
Un comportement intra-vidéo est modélisé par un modèle de Markov du premier ordre non caché. Ce modèle est construit en utilisant les différentes actions proposées aux utilisateurs lors des visionnages (lecture, pause, avance rapide, retour rapide, saut, stop). Nous proposons une technique de regroupement de ces modèles (K-Models). Cette technique est une adaptation de la technique classique des K-Means [MacQueen (1966)] adaptée à l'utilisation de modèles en lieu et place des moyennes. Nous caractérisons ainsi plusieurs comportements type (lecture totale, lecture partielle, survol...). Grâce à ces comportements type, nous sommes à même de savoir quelle fut l'utilité ou l'importance d'une séquence vidéo lors d'une visite (si elle est la plus importante ou seulement le résultat d'une recherche infructueuse...)
Un comportement inter-vidéo est modélisé par une session. Cette session est une séquence ordonnée des visionnages des séquences vidéo. Chaque visionnage étant représenté par l'identifiant de la vidéo et le modèle de comportement qui lui correspond pour cette visite. Pour regrouper ces sessions, nous nous sommes basés sur une technique de regroupement hiérar-chique ascendante que nous avons adaptée à ces données particulières.
S. Mongy
Cet article est organisé comme suit. La section 2 présente l'état de l'art dans le domaine de la fouille des usages de la vidéo et en spécifie les particularités. La section 3 décrit le contexte applicatif de notre approche qui est le montage de films. Elle présente ensuite notre modéli-sation à deux niveaux des comportements des utilisateurs exploitant un moteur de recherche vidéo. Enfin, la section 4 présente les premiers résultats obtenus sur des données test. La section 5 regroupe les conclusions et introduit les futures lignes directrices de notre travail.
Fouille des usages de la vidéo
Quelques approches existent dans ce domaine précis. Les travaux les plus proches peuvent être classés en deux catégories.
La première concerne l'analyse des comportements des utilisateurs sans considérer le contenu de la vidéo. Ces travaux permettent de produire des statistiques sur le comportement des utilisateurs et l'analyse des fréquences des accès vidéo. Par exemple dans [Reuther et Meyer (2002)] on analyse l'usage d'un système d'enseignement multimédia par les étudiants. Cette analyse se base sur les types de personnalités des étudiants. En effet, les besoins et les attentes en apprentissage dépendent des caractéristiques du type de personnalité de l'étudiant. Pour conduire cette analyse, les auteurs ont développé un programme permettant d'extraire les actions effectuées par les étudiants sur le système multimédia et construit des profils utilisateurs permettant de tracer ce que fait chaque étudiant chaque fois qu'il utilise le système. Ces profils utilisateurs comportent les statistiques suivantes : le nombre de sessions de visionnage des vidéos, le nombre total de secondes passées à visionner les vidéos, le nombre de sessions ayant duré plus de 20 minutes, la durée moyenne d'une session, le nombre moyen de commandes par minute lors d'une session, (lecture, pause, saut...). En se basant sur les statistiques collectées sur les divers types de personnalité des étudiants, les auteurs analysent comment le système d'enseignement multimédia peut être amélioré.
Dans [Acharya et al. (2000)] une analyse des données tracées obtenues à partir des accès utilisateurs à des vidéos sur le Web est présentée. Les auteurs examinent des propriétés telles que : les variations journalières des requêtes des utilisateurs utilisant des accès vidéo. Ils proposent de tirer avantage de ces propriétés pour la conception des systèmes multimédia tels que : les systèmes de proxy, de cache et les serveurs de vidéo. A titre d'exemple leur analyse a montré que généralement les utilisateurs visionnent la première portion d'une vidéo pour savoir s'ils sont intéressés ou pas. S'ils sont intéressés, ils poursuivent le visionnage, sinon ils l'interrompent. Cette analyse suggère que mettre en cache les premières minutes d'une vidéo permettrait d'améliorer les performances d'accès au serveur vidéo.
La seconde catégorie concerne l'analyse du comportement de l'utilisateur sur une vidéo unique.
Un projet, réalisé par Microsoft [Yu et al. (2003)], développe un modèle centré « utilisateur », combinant l'analyse de contenu de la vidéo et la fouille de données log vidéo afin de générer des résumés vidéo. Le modèle utilise l'expérience des visionnages antérieurs pour guider les futurs visionnages. La contribution essentielle du travail réside dans la pondération des plans « ShotRank » de la vidéo, et l'utilisation de cette pondération dans l'extraction des résumés de la vidéo. Le « ShotRank » mesure la probabilité de visionner un plan donné durant l'exploration de la base de vidéo. La pondération des plans est calculée par un algorithme d'analyse des liens, et utilise les interactions de l'utilisateur (« Vote ») pour évaluer l'utilité et l'importance de chaque plan. Des expériences avec des vérités de terrain ont confirmé que la pondération du plan « ShotRank » estime l'utilité et l'importance de chaque plan de vidéo, et améliore l'exploration des futurs visionnages.
Un autre projet similaire, réalisé par IBM [Syeda-Mahmood et Ponceleon (2001)], utilise également les données de log pour déterminer les type de comportement des utilisateurs visionnant des données vidéo. En se basant sur les actions réalisées et sur l'implémentation de modèles de Markov cachés, les auteurs proposent de définir des types de comportement tels que : -curieux ; -recherchant quelque chose en particulier ; -a trouvé un passage intéressant...
Le point non traité dans les travaux précédents est la non-corrélation des comportements généraux des utilisateurs avec leur comportement sur chacune des séquences vidéo. Ils ne prennent pas en compte les actions réalisées durant les visionnages lors de l'analyse de la navigation entre les vidéos (la session). Les concepts de navigation et de recherche dans une grande base de données vidéo ne sont pas définis. De plus, il n'existe à notre connaissance aucun jeu de données ou banc d'essai relatifs aux données log vidéo, ce qui ne simplifie pas la tâche de validation des résultats.
Deux points importants discriminent notre approche des travaux actuels. Tout d'abord, il n'existe aucun outil analysant l'utilisation complète d'une base de données vidéo. Les seuls travaux que nous avons référencés dans le domaine de l'analyse vidéo ne considèrent qu'une vidéo à la fois.
Ensuite, nous avons développé une technique de regroupement qui correspond à la nature de nos données. En effet, nombre de techniques du web mining utilisent des algorithmes basés uniquement sur les distances et l'approche par voisinage produisant des résultats difficiles à analyser. Il n'est pas rare de retrouver deux éléments totalement différents dans une même classe pour peu qu'ils soient connectés par une chaîne de voisins très proches les uns des autres. Nous proposons ici d'introduire un modèle de classe qui capitalisera les informations données par tous les éléments d'une même classe, éléments qui devront correspondre à ce modèle.
3 Notre approche 3.1 Contexte L'un des besoins des professionnels de l'audiovisuel est de pouvoir retrouver facilement des séquences vidéo particulières dans d'importantes bases de films afin de les réutiliser pour le montage de nouveaux films. Notre approche met en oeuvre un moteur de recherche adapté à ce besoin. Les recherches s'appuient sur une indexation des séquences vidéo. Mais beaucoup d'informations peuvent également être extraites des usages et utilisées pour optimiser la pertinence des séquences retournées par le moteur de recherche.
Pour réaliser cette analyse, nous devons tout d'abord définir ce qu'est une utilisation du moteur de recherche vidéo. Un tel comportement peut être divisé en trois parties. -1. Ecriture d'une requête : l'utilisateur définit les attributs sur lesquels porte la recherche et entre une valeur pour chacun d'entre-eux. Ces attributs sont utilisés pour trouver les vidéos pertinentes dans la base de vidéos indexée -2. Exploitation des résultats : les séquences vidéo retrouvées sont présentées dans l'ordre de leur proximité aux attributs. -3. Visionnage des séquences sélectionnées : l'utilisateur visionne les séquences qui lui semblent les plus pertinentes. Ce -394 -RNTI-E-6 visionnage est réalisé dans un lecteur offrant les fonctionnalités usuelles d'un lecteur vidéo (lecture, pause, avance rapide, retour rapide, stop, saut).
Les groupes de séquences visionnées forment des sessions. Ces dernières correspondent à une visite d'un utilisateur. Elles sont composées de plusieurs groupes recherche -exploitation et visionnage des résultats.
Collecte des données
Toutes ces données sont tracées et écrites dans des fichiers de log. Pour les créer, nous avons défini un langage adapté basé sur XML. La grammaire du langage de trace d'une session est le suivant. Une première partie contient la requête exécutée et la liste des séquences vidéo retournées. Une seconde partie trace le visionnage des séquences.
Tout comme les logs web, notre outil trace les actions de tous les utilisateurs. Pour les regrouper sous forme de sessions, nous avons développé un convertisseur XSLT (eXtensible Stylesheet Language Transformation) [w3c]. Ce convertisseur extrait et regroupe les sessions depuis les fichiers de log dans un format XML. La suite de l'article présente sous quelle forme sont modélisées les sessions.
Modélisation du comportement : un modèle à deux niveaux
A partir des données collectées précédemment, nous générons deux modèles pour repré-senter le comportement des utilisateurs. Le premier montre comment un utilisateur a visionné une séquence vidéo (lecture, pause, avance rapide, saut, stop). A ce niveau, nous définissions le « visionnage d'une séquence vidéo » comme une unité de comportement. Le second trace les transitions entre chaque visionnage. Cette partie regroupe les requêtes, les résultats et les séquences visionnées successivement. A ce niveau plus général, nous proposons la « session » comme unité de comportement.
Une session est une liste de séquences vidéo visionnées. L'intérêt des logs vidéo est de permettre de définir l'importance de chaque séquence pour chaque session. Plus que de les caractériser par un simple poids, comparable à l'exploitation du temps passé sur chaque document [Wang et Zaiane (2002)], nous allons ici construire plusieurs comportements type (lecture totale, aperçu, ouverture-fermeture, lecture partielle...).
Modélisation et regroupement des comportements intra-vidéo
Un comportement intra-vidéo est modélisé par un modèle de Markov du premier ordre (figure 2). Ce modèle représente les probabilités d'exécuter une action à chaque seconde du visionnage d'une vidéo. Les sommets correspondent aux différentes actions accessibles aux utilisateurs durant le visionnage. Ces actions sont Lecture, Pause, Stop, Avance Rapide, Retour Rapide, Saut, Stop. Par exemple, la transition de l'état Lecture vers l'état Pause de la figure (figure 2) signifie que quand un utilisateur regarde une séquence, il y a une probabilité de 8% qu'il effectue une pause la seconde suivante.
Ce modèle est totalement déterminé par les paramètres suivants : ? i la probabilité de démarrer dans l'état i.
A ij la probabilité de passer d'un état V i à un état V j la seconde suivante. Cette discrétisation du temps (avec la seconde comme unité) permettant de prendre en compte le paramètre temps dans un modèle de Markov a été introduite par [Branch et al. (1999)]. Elle permet de considérer le temps sans ajouter de paramètre supplémentaire.
Sa complexité limitée (nombre d'états peu important, premier ordre, modèle non caché) permet de proposer une méthode de regroupement efficace de ces comportements. Nous allons ici introduire la technique des K-Models. Cette technique correspond à une adaptation de la méthode bien connue des K-Means pour utiliser des modèles en lieu et place des moyennes. Nous essayons de découvrir K classes dans un ensemble de visionnages (actions exécutées sur la séquence vidéo par un utilisateur) par partitionnement de l'espace. Chaque classe est représentée par l'un des modèles de Markov décrits précédemment. La différence réside dans l'utilisation des probabilités en lieu et place des distances pour associer les visionnages aux classes. Nous calculons la probabilité qu'un visionnage ait été généré par les modèles. Nous l'assignons ensuite à la classe ayant la plus grande probabilité de l'avoir généré.
Un tel algorithme se découpe en trois phases : initialisation, allocation, maximisation. Allocation Pour chaque visionnage e = (e 1 ..e l ) e i appartenant à N , de longueur l, pour chaque classe définie par un modèle k, nous calculons la probabilité que k ait généré e. e est associé à la classe avec la plus grande probabilité.
Maximisation Chaque modèle k représentant une classe c de cardinalité m est mis à jour en fonction des données appartenant à c. Cette mise à jour correspond à compter chaque S. Mongy transition dans chaque élément e i et d'attribuer ces comptes aux probabilités de transition du modèle. Pour chaque cluster c, les probabilités sont mises à jour de cette manière.
A partir de ces modèles, nous construisons un vecteur v e de comportement pour chaque visionnage. Ce vecteur correspond aux probabilités que le visionnage ait été généré par chacun des modèles (1).
Modélisation et regroupement des comportements inter-vidéo
A partir des données initiales et des vecteurs de comportement générés par le regroupement intra-vidéo, nous construisons une représentation séquentielle des sessions. Une session est une séquence ordonnée dans le temps des vidéos visionnées. Chaque visionnage est caractérisé par l'identifiant de la vidéo et par le vecteur de comportement qui lui est associé. nous avons développé un algorithme de regroupement qui répond aux besoins suivants : -chaque élément appartenant à une classe a au moins une partie commune avec les autres éléments de la classe ; -le niveau d'homogénéité des classes repose sur la définition de paramètres entrés par l'utilisateur. Ces paramètres sont présentés ci-après.
Ces besoins ont conduit à la représentation suivante des classes : une classe c est représen-tée par un ensemble de S sessions s c de longueur minimale l. Une session s est attribuée à une classe si elle correspond au moins à p des S sessions. Une session s correspond à s c si s c est une sous-séquence extraite de s (2).
Ainsi, nous assurons l'homogénéité des classes et le fait qu'il y ait un facteur commun entre tout élément d'une même classe. De fait nous évitons de créer des classes composées d'éléments totalement différents, connectés par une chaîne de proches voisins généralement produites par les techniques de regroupement basées sur les distances [Guha et al. (1998)]. La longueur minimale d'une séquence représentative et le nombre de séquences nécessaires pour modéliser une classe sont donnés par l'analyste, lui permettant d'obtenir des classes d'une généricité en rapport avec ses besoins. L'algorithme de regroupement lui-même est un algorithme hiérarchique ascendant. Il dé-bute en considérant des petits groupes de sessions comme classes et fusionne itérativement les classes les plus proches. Le regroupement prend fin lorsque le niveau d'homogénéité requis est atteint. Son originalité réside dans la représentation des classes. Les techniques de regroupement de séquences usuelles ne traitent qu'une unique séquence pour représenter une classe. Nous avons développé un outil capable de comparer et de fusionner des classes représentées non plus par une unique séquence mais par un ensemble de séquences.
Comparaison des classes
Analyse du Comportement des utilisateurs exploitant une base de vidéos Cette fonction de distance (3) est basée sur la comparaison des sessions, comparaison elle-même basée sur l'extraction de la plus longue sous-séquence commune. Etant donnée I = [(i 1 , i 2 )] la liste de longueur l des indices des éléments sélectionnés des deux sessions comparées s 1 et s 2 , la distance entre elles est définie par (4) où v xy est le vecteur de comportement du y me élément de la session x. (5) est la fonction de distance entre deux vecteurs de comportement.
Fusion des classes La fonction de fusion est également basée sur l'extraction de la plus longue sous-séquence commune. Elle extrait les plus longues sous-séquences en comparant les séquences des modèles 2 à 2. En fusionnant les séquences 2 à 2, nous assurons que la proportion p est conservée jusqu'à la fin de l'exécution. Soient
i et j sont choisis pour maximiser la longueur des séquences fusionnées. Pour fusionner deux séquences, l'algorithme extrait la plus longue sous-séquence commune sans tenir compte du vecteur de comportement. Une fois cette sous-séquence créée, il fusionne les comportements correspondants des deux séquences en calculant la moyenne sur chaque élément du vecteur. Soient
) deux comportements à fusionner, le résultat de la fusion est donné par (7).
4 Resultats expérimentaux Cette partie met en avant les deux avantages de notre approche comparée aux techniques usuelles. Tout d'abord, nous allons voir comment l'analyse du comportement intra-vidéo permet de différencier des groupes de sessions composées des même séquences vidéo mais visionnées de manière différente. Ensuite, nous allons montrer l'avantage de décrire les classes par un groupe de sessions comparé à une simple extraction de sous-séquence.
Création des jeux de données
En raison de la nouveauté des données exploitées ici, nous avons réalisé nos expérimenta-tions sur des jeux de données de test.
La création des jeux de données de test se présente en deux phases. Premièrement, nous avons créé les modèles de comportement intra-vidéo. Nous avons défini quatre comportements type (lecture totale, survol rapide, lecture d'un passage précis, fermeture rapide). De nouvelles expérimentations sur des données réelles nous permettront de valider totalement ces modèles. A partir de ces modèles, nous avons créé aléatoirement un vecteur de comportement pour chaque visionnage des sessions vidéo.
Ensuite, pour générer les sessions vidéo, nous avons défini pour chaque classe une source de génération avec un ensemble de séquences d'identifiants vidéo. Pour chaque classe, nous avons généré les sessions en fusionnant aléatoirement ces séquences. Nous avons enfin ajouté dans ces sessions entre 5% et 20% de bruit en introduisant des identifiants de vidéo n'ayant pas de lien avec le contenu de la classe dans les séquences. Nous avons finalement généré différents jeux de données composés de 2000 sessions. Chacune est composée de 5 à 20 visionnages correspondant à des comportements de 10 à 20 actions de base (lecture, pause...). Les fichiers de tests sont donc composés d'environ 100.000 à 800.000 actions de base. La figure 3 présente quelques sessions générées pour le deuxième scénario ci-après. Une session est une séquence de vidéos visionnées à la suite d'une navigation par un utilisateur dans un intervalle de temps donné.
FIG. 3 -Exemple des sessions analysées.
La suite présente deux exécutions correspondant à des scénari prédéfinis qui mettent en avant les avantages de notre technique. Considérons dans la suite une base de données de vidéo en rapport avec la nature contenant des vidéos de montagnes et de volcans.
Exploitation du comportement intra-vidéo
Ce scenario est basée sur l'affirmation suivante. La base vidéo n'est pas correctement indexée et de nombreuses vidéos traitant des volcans sont indexées comme des vidéos de montagne.
Nous avons généré deux classes. La première correspond à une recherche sur les volcans et retourne uniquement une seule vidéo complètement visionnée. Elle correspond à la séquence (1, 2, 3, 4, 5, 6, 10) dans laquelle seulement la vidéo 10 est visionnée (représentée en gras dans la table 1). La seconde est le résultat d'une recherche sur les montagnes et toutes les vidéos sont visionnées complètement (table 1). Elle correspond à la séquence (1, 2, 3, 4, 5, 6) dans laquelle toutes les vidéos sont visionnées totalement. Avec une approche classique, ne prenant pas en compte le comportement intra-vidéo, les deux classes ne sont pas découvertes et le regroupement retourne une seule classe {(1, 2, 3, 4, 5)}.
Avec notre approche à deux niveaux, le regroupement est à même de découvrir que les vidéos ont été exploitées différemment et les deux classes sont effectivement découvertes. Pour TAB. 2 -Regroupement du second jeu de données En positionnant le nombre de séquences représentatives à 2 ou 3, les sessions correspondant à l'une des trois premières classes sont fusionnées pour former une classes « sessions traitant des stations de ski ». Avec une valeur de 4, cette classe est divisée et chaque classe source est découverte. Pour ces valeurs, la classe correspondant au trekking est correctement analysée et n'est pas fusionnée avec les autres données. Mais si nous positionnons le nombre de séquences à 1, nous plaçant ainsi dans le cadre d'une simple extraction de sous-séquence, toutes ces données sont fusionnées dans une unique classe et la différence entre les stations de ski et le trekking n'est pas détectée lors du regroupement. On voit finalement l'intérêt d'une approche multi-séquences. Elle permet de retrouver des éléments correspondant à plusieurs recherches réalisées les unes à la suite des autres dans un ordre variable et de les reconstituer dans une même classe. Sur notre exemple, on peut ainsi reformer une classe générique correspondant au travail sur les stations de ski (minSize = 2 ou 3).
Exploitation des modèles générés
Une fois ces modèles générés, l'objectif est de s'appuyer sur les connaissances extraites pour optimiser la qualité de l'indexation. Plusieurs applications sont envisagées.
Pondération des termes de l'index Pour des recherches courantes, nous pourrons mettre à jour l'ordre des vidéos retournées en fonction de l'usage fait de ces vidéos par les utilisateurs précédents. Par exemple, dans la recherche concernant la montagne et retournant les vidéos (18,73,29,41), si l'on note que la vidéo 29 est systématiquement la plus regardée, elle devra apparaître en tête de liste lors des recherches contenant les vidéos 18, 73 et 41.
Détection des erreurs d'indexation
Si une vidéo n'est jamais visionnée lors d'une requête précise (comme dans notre premier exemple), c'est que son indexation a été mal réalisée. Dans ce cas, il faut découvrir avec quel donnée sémantique ne correspond pas au contenu et la supprimer. Dans la recherche précédente, si la vidéo 41 n'est jamais visionnée alors qu'elle est pourtant retournée, cela signifie qu'elle ne correspond pas au concept de montagne pour les utilisateurs et cette notion doit disparaitre de son indexation.
Correction des erreurs d'indexation Lorsqu'une erreur d'indexation est découverte, il est également possible de tenter de la corriger plutôt que de la supprimer. Pour ce faire, il faut trouver, en fouillant les modèles, avec quelles autres vidéos la vidéo mal indexée est exploitée pour en déduire les bonnes données d'indexation.
Conclusion et travaux futurs
Nous proposons ici un modèle de représentation des utilisateurs exploitant un moteur de recherche vidéo à deux niveaux. Le premier niveau conduit à modéliser les comportements relatifs au visionnage d'une séquence vidéo (intra-vidéo), le second à modéliser les comportements observés sur un ensemble de séquence vidéo (inter-vidéo). A partir de cette repré-sentation, nous avons développé une technique de regroupement qui permet de retourner les vidéos les plus souvent retournées et les modes de visionnage de ces vidéos. Ces derniers nous permettent de pondérer l'importance et de mettre à jour les termes de l'index.
Notre futur objectif est d'utiliser les profils découverts pour optimiser le moteur de recherche. En effet, ces résultats nous permettront de repérer les données mal indexées et de proposer aux utilisateurs les séquences vidéo en relation avec l'historique de leur session.

FIG. 1 -page web et ontologie présentées à l'expert
le concept le plus spécifique avec lequel un élément doit être annoté. L'annotation est donc totalement dépendante de l'ontologie fournie.
Dans une première section, nous présentons le processus de marquage de la page par un expert. La seconde section présente l'algorithme d'apprentissage exploitant la structure arborescente d'une page web. La section 3 présente l'annotation de documents dont la structure est similaire. Enfin, nous évaluons notre méthode par rapport aux systèmes d'annotation séman-tique existants.
Génération d'annotations primaires par marquage
La première étape du processus est un marquage permettant de former un corpus d'apprentissage ; il s'agit de fournir au système quelques exemples d'éléments pertinents à partir desquels le système apprend à reconnaître l'ensemble des éléments à annoter. Pour cela, un expert marque des éléments pertinents de la page web, c'est-à-dire correspondant à des concepts de l'ontologie. Il dispose à cet effet d'un outil de visualisation, à la manière d'un navigateur web, qui lui permet de sélectionner un élément dans la page et de choisir dans l'ontologie le concept qui lui correspond. Dans l'exemple figure 1, le marquage est effectué en fonction de l'ontologie SWRC 1 , qui modélise notamment les personnes, organismes et projets d'une équipe de recherche. Pour chaque concept, un nombre suffisant d'éléments pouvant y être associé doivent être marqués ; ce nombre dépend de la régularité de la page d'apprentissage et des pages à annoter ; pour des pages très régulières, 2 ou 3 exemples suffisent pour chaque concept.
De manière interne, la page est représentée par son arbre DOM (W3C) dans lequel les noeuds contiennent les éléments de structure HTML et les feuilles les éléments de texte. Un chemin unique est ainsi défini depuis la racine jusqu'à chaque feuille. Lorsque l'expert marque
FIG. 2 -Arbre DOM et annotations primaires issues du marquage
un élément la chaîne de caractères sélectionnée, le chemin de la feuille contenant la chaîne et le concept de l'ontologie associé sont enregistrés au format XML ; la figure 2 présente un exemple d'enregistrement du marquage de la page web de la figure 1. L'ensemble de ces éléments marqués sont des annotations primaires qui jouent ainsi le rôle de corpus pour l'algorithme d'apprentissage.
2 Apprentissage exploitant une structure arborescente Définition d'un chemin dans l'arbre issu du DOM L'algorithme d'apprentissage est dérivé des travaux de Kushmerick et al. (1997) sur l'induction de wrappers. Un wrapper est une procédure utilisant les régularités syntaxiques d'un document pour identifier des éléments. Là où les travaux initiaux s'appuyaient sur des structures à plat, en considérant le document comme une suite de chaînes de caractères, notre système exploite la structure arborescente fournie par la représentation DOM de la page web.
Le DOM permet de définir le chemin de chaque élément (noeud ou feuille) de l'arbre. Pour chaque élément, nous définissons ce chemin comme un ensemble d'étapes depuis la racine. Chaque étape est un couple (balise :position) défini à partir de l'étape précédente (on considère l'étape 0 comme étant la racine du document). La position est le numéro du fils du noeud défini à l'étape précédente tandis que la balise est la balise HTML que le noeud représente. Par exemple, une page web contient un élément racine <html> qui a deux fils, <head> et <body>. Le chemin de l'élément <body> est donc body : 1. Cette définition de chemin est celle employée pour les annotations primaires présentées figure 2.
A partir de cette définition du chemin d'un élément de l'arbre, on définit la notion de chemin similarisé. Un chemin similarisé est la factorisation des chemins de plusieurs élé-ments. Le chemin ainsi généré est ainsi un chemin de plusieurs éléments. Pour cela, les étapes sont comparées 2 à 2 et les différences marquées par une astérisque. Prenons l'exemple des deux premières annotations primaires présentées figure 2. Le chemin du premier élément est body : 1,table :0,tbody :0,tr :0,td :0,b  Relations entre la représentation arborescente de la page web et l'ontologie Cette défini-tion de chemin est fondamentale pour l'apprentissage. Notre stratégie d'annotation est fondée sur l'hypothèse d'une corrélation entre la représentation arborescente d'un document et les concepts et rôles définis par l'ontologie. Ces hypothèses sont les suivantes : -chaque instance de concept est exactement une feuille de l'arbre, -les instances de rôles sont contenues dans des sous-arbres De ces hypothèses, on déduit qu'identifier une instance de l'ontologie revient à déterminer le chemin depuis la racine vers une feuille de l'arbre pour une instance de concept et vers un noeud, racine du sous-arbre, pour une instance de rôle. L'apprentissage consiste donc à déterminer un chemin similarisé pour chaque concept et chaque rôle de l'ontologie.
Apprentissage de chemins similarisés Pour chaque concept dont des exemples ont été marqués par l'expert, le chemin similarisé du concept est généré à partir de l'ensemble des chemins enregistrés dans les annotations primaires pour ce concept. Dans l'exemple figure 1, 5 annotations primaires sont définies pour le concept Project. En factorisant les chemins deux à deux, le chemin similarisé obtenu est body : 2, table : * , tbody : 0, tr : * , td : 1, a : 0, f ont : 0. Il ressort ainsi que les éléments correspondant aux concept Project sont situés dans la deuxième colonne des tableaux du document.
Pour les instances de rôles, une première étape consiste à déterminer les racines des sousarbres de chaque rôle tel que :
-il existe un rôle R A,B dans l'ontologie reliant des concepts A et B, -au moins une instance de A et une instance de B ont été marquées. Alors pour chaque instance marquée de A :
-le plus petit parent commun (pppc) dans l'arbre de cette instance avec chaque instance de B est déterminé, -le noeud le plus profond dans l'arbre parmi ces pppc est alors un noeud racine pour le rôle R A,B . Le chemin similarisé des noeuds racines générés est alors inféré. La sortie de l'apprentissage est donc un chemin similarisé de chaque concept et de chaque rôle de l'ontologie ayant des instances marquées dans le document.
Annotation par génération d'instances de l'ontologie
Annotation par application des chemins similarisés Les chemins similarisés sont appliqués sur une page dont la structure DOM est similaire à la page d'apprentissage. Les noeuds -308 -RNTI-E-6 reconnus par le chemin similarisé appris pour chaque rôle R A,B sont les racines des sousarbres en dessous desquels chaque instance de a est liée à une instance de b par une instance de R A,B . Les feuilles reconnues par le chemin similarisé d'un concept sont des candidates pour être instanciées par ce concept. Deux cas sont possibles : si une feuille n'est reconnue que par un seul chemin similarisé, cette feuille est instanciée par le concept correspondant à ce chemin. Pour toutes les feuilles situées dans un sous-arbre, une relation est générée entre les instances de concepts définies par le rôle. Si plusieurs chemins similarisés conduisent à la même feuille, un mécanisme de raisonnement doit être appliqué pour déterminer à quel concept cette feuille appartient. On atteint les limites d'une méthode purement syntaxique.
Dans notre exemple, le chemin similarisé du concept Project décrit ainsi le fait qu'il est associé aux éléments contenus dans la colonne de droite des tableaux tandis que les concepts Lecturer et FacultyMember sont associés au contenu de la colonne de gauche.
Annotation par un concept plus général dans l'ontologie Lorsqu'un même élément peut être annoté par deux concepts différents, un raisonnement est effectué au niveau de l'ontologie pour déterminer le concept subsumant les deux concepts candidats. Dans notre exemple, le raisonneur Pellet (Sirin et Parsia (2004) évalue les performances de Pellet pour la classification et les requêtes) est utilisé pour classifier les concepts de l'ontologie et déterminer le concept subsumant Lecturer et FacultyMember dans SW RC. Il s'agit de AcademicStaff. Une instance de ce concept sera donc générée pour les éléments reconnus par les chemins similarisés appris à partir des concepts Lecturer ou FacultyMember.

Introduction
La découverte de connaissances temporelles est un enjeu majeur pour le diagnostic de systèmes dynamiques (Das et al., 1998), (Dousson et Vu Duong, 1999), (Keogh et Smyth, 1997), (Agrawal et al., 1995), (Faloutsos et al, 1994). Récemment, Bouché P. et  ont proposés une approche stochastique pour découvrir des modèles de chroniques à partir d'une séquence d'événements discrets. Nos travaux visent à compléter cette approche pour identifier les classes d'événements contribuant le plus significativement à la prédiction de l'occurrence d'une classe particulière.
Les arbres de décisions (Breiman, 1984), (Murthy, 1998), sont largement utilisés pour classer des séquences de données (Kadous, 1999), (Geurts, 2001), (Drucker et Hubner, 2002), (Rodriguez et Alonso, 2004). Récemment, l'algorithme ID3 (Quinlan, 1986) a été adapté pour construire des arbres temporels de décision (Console et al., 2003) à partir d'un ensemble de situations. Cette adaptation montre que l'entropie informationnelle permet d'identifier les variables contribuant le plus significativement à une prise de décision.
Nous proposons donc d'utiliser un critère entropique pour analyser des modèles de chroniques. Après un bref rappel sur les arbres temporels de décision, cet article présente une adaptation de l'algorithme proposée par Console pour la déduction de modèles de chroniques à partir d'un ensemble de séquences d'occurrences d'événements discrets et montre sur un exemple comment l'approche entropique peut être utilisée pour compléter l'approche stochastique.
Arbres temporels de décision
Un arbre de décision est une structure T= <r, N, E, L> où N=N I ?N L est l'union d'un ensemble N I ={x i } de noeuds internes désignant une variable x i et un ensemble N L ={a i } de noeuds feuilles désignant une décision ai, r?N est le noeud racine de l'arbre, E ? N I × N est un ensemble d'arcs, un arc attribuant une valeur v j à une variable x i et L est une fonction d'étiquetage définie sur N?E, qui retourne le nom de la variable x i associé au noeud de N I , la décision a i associée à une feuille de N L ou la valeur v J associée à un arc de E.
L'algorithme ID3 utilise l'entropie informationnelle dans un ensemble de cas ? pour construire un arbre de décision de profondeur minimal. Un cas e est une collection de valeurs v j prises par un ensemble de variables x i conduisant à une décision a i particulière. A chaque noeud, ID3 choisit la variable x qui minimise l'entropie ?(x, ?) dans l'ensemble ? des cas : Temporal ID3 est une extension d'ID3 à des données datées selon une horloge à temps discrète (Console et al, 2003). Un arbre temporel de décision est un arbre de décision où un noeud est un couple (x i , t k ), x i désignant une variable et t k la date d'observation de sa valeur, et un arc défini une valeur v j de x i à la date t k (i.e. x i (t k )=v j ). Il s'agit donc d'une structure T= <r, N, E, L, ?> dotée d'une fonction d'étiquetage du temps ?: N I ?? + qui donne la date associée à un noeud interne. L'ensemble d'apprentissage est une collection de situations S= {s e=0,..,m }. Une situation s e est l'ensemble des valeurs v j prises par un ensemble de variables X= {x i } à chaque instant d'observation t k conduisant à une décision a n particulière (Table 1). Une situation s e réfère à une horloge à temps discret où t k ?kT, k?? et T?? + , T est une période d'échantillonnage. 
TAB. 1 -Exemple de table temporelle de décision.
Dans la table 1, les variables x 1 , x 2 , x 3 prennent une valeur qualitative n, h, l, ou v aux instants d'observations t 0 , t 1 , t 2 , t 3 . Dans la première situation s 1 (resp. s 2 ), la décision a 1 (resp. a 2 ) doit être prise au plus tard à la date t 3 (resp. t 2 ). Cette date est dite « limite » car la connaissance de la valeur des variables au-delà de cette date est inutile à la prise de décision.
[ ]
Une partition S e est un sous ensemble de S contenant des situations identiques sur un intervalle de temps :
Ainsi, à chaque instant d'observation t, S est partitionné en un ensemble de partitions S ?,t = {S e=0,.., m } (équation 3). TemporalID3 construit un arbre en recherchant un intervalle de temps qui maximise un critère lié au nombre de partitions. Puis, de la même manière qu'ID3, TemporalID3 choisit la variable x i (t) qui minimise l'entropie sur cet intervalle (4) et créé le noeud correspondant. Toutes les valeurs des variables à tous les instants précédent t sont éliminées du tableau, y compris celle de x i (t), puis TemporalID3 recommence son traitement. 
Une séquence ?={o k } k=0…m-1 est une suite ordonnée de m occurrences o k ?(t k , x, i) d'événements discrets e k ?(x, i) où t k ???? + est la date de l'affectation de la valeur i à la variable x . Un couple (o k , o k+1 ) de deux occurrences successives liées à une même variable x décrit l'évolution temporelle de la fonction discrète x(t), définie sur ? (équation 5). L'ensemble des événements discrets E d ={e k ?(x, i)} est partitionné en un ensemble de classes C j ={e k }. La notation "o i ::C k " signifie que l'occurrence o i appartient à la classe C k . Une fonction d fournit la date d'une occurrence en sorte qu'une séquence ? n d'un ensemble ?={? n } définie son propre sous ensemble ? ?n = {t j } de date inclus dans l'ensemble des dates ? défini par ? (équation 6 où O désigne l'ensemble des occurrences de ?). Un modèle de chroniques est un ensemble de relations binaires temporellement contraintes entre des classes d'événements (équation 7).
Une séquence classée Ok, notée ? n ok , est un exemple qui respecte toutes les contraintes logiques et temporelles d'un modèle de chroniques. Une séquence classée Ko, notée ? n ko , est un contre exemple. Un contre exemple respecte toutes les contraintes d'un modèle de chroniques sauf la dernière relation binaire qui concerne la classe à prévoir. 
Extraction d'un modèle de chroniques
La table temporelle à temps continu est une matrice H définie sur ?×X×? où un élément h[? i , x, t] défini la classe liée à la valeur de la variable x à l'instant t dans la séquence
La date limite d'une séquence est la date de la dernière occurrence sans prendre en compte les occurrences ajoutées par complétion. La figure 3 montre le résultat de l'application de TemporalID3 sur ?. Le modèle de chroniques est construit en ne considérant que les branches menant à une décision Ok en parcourant l'arbre en profondeur d'abord depuis les feuilles terminales à la racine de l'arbre (les dates ont été inversées). Le modèle est initialisé par la classe C 1 correspondant à la décision Ok (i.e. « B »). Pour tout arc (n 1 ?n 2 )?? de l'arbre, la classe C 2 associée n 2 est créée dans le modèle de chroniques et une relation R(
) est la moyenne des durées écoulées entre les occurrences de la même classe C i dans les séquences ? n de ?. Le modèle de chroniques de la figure 3 est construit à partir de la branche ((v_a, 0.092626)?(v_c, 0.68979)?(Ok)). La relation R(A, C, [0,2.14]) du modèle de la figure 1 n'apparaît pas dans le modèle produit par l'approche entropique. Cela induit l'idée que cette relation n'apporte que peu d'information pour prédire une occurrence de la classe B.
Conclusion
Cet article propose une adaptation de l'algorithme TemporalID3 pour la découverte des modèles de chroniques à partir d'un ensemble de séquences d'occurrences d'événements discrets. L'intérêt de cette approche est d'utiliser un critère de minimisation entropique pour identifier les classes des modèles de chroniques les plus signifiantes afin de prédire l'occurrence d'une classe dans une tâche de diagnostic de systèmes dynamiques. Les premiers résultats obtenus invitent à envisager une combinaison des approches entropiques et stochastiques pour découvrir des connaissances temporelles avec un fort pouvoir anticipatif.
Références
Agrawal, R., K. I. Lin, H. S. Sawhney, et K. Shim (1995). Fast similarity search in the presence of noise, scaling, and translation in time-series databases. In Proc. of the 21st Int'l Conf. on Very Large Databases, pp 490-50.

Summary
In this Paper, we present a database of Arabic image writing for the use in Arabic OCR systems. The topics addressed by ARABASE concern different styles of documents: machine printed text, off line and on line handwriting. Data corresponds to a variety of context: city names, literal amounts, isolated characters, digits, free texts, words/sub-words, isolated characters. ARABASE contains also information describing the process of data acquisition. Therefore, we use the method oriented object UML for modelling the system. ARABASE provides multiple functionalities to their users (webmaster and clients).

Introduction
Certains hôpitaux sont des entités complexes faites de plusieurs bâtiments plus ou moins dispersés. Des informations importantes sont rattachées à ces bâtiments et des décisions doivent être prises.
Or habituellement toutes ces informations sont éparpillées dans des schémas et des tableaux de chiffres de telle sorte que les décideurs qui utilisent ces données ont une représen-tation fragmentée de la réalité sous-jacente.
C'est dans ce contexte que nous avons décidé d'utiliser la méthode Caseview généralisé pour créer un support permettant de convoyer de façon synthétique des informations à la fois topographiques et quantitatives concernant un hôpital.
Méthode
La méthode Archiview est une méthode issue de la méthode Caseview généralisé (Lévy, 2004). Cette dernière consiste à visualiser des données au moyen d'un référentiel bidimensionnel construit en identifiant un pixel avec une entité informationnelle. Les pseudo pixels sont alors ordonnés selon 3 critères : un critère binaire, un critère nominal et un critère ordinal. 
Archiview, un outil de visualisation topographique
Une fois le référentiel construit on l'utilise pour visualiser des paramètres variés : chaque valeur associée à chaque entité informationnelle est placée dans le pseudo pixel lui correspondant dans le référentiel. Puis la définition d'une échelle de couleurs permet de visualiser le paramètre étudié. Dans la méthode Archiview les entités informationnelles sont les étages des bâtiments d'un hôpital. Le critère nominal est un critère topographique : chaque colonne du référentiel contient les « pixels-étages », regroupés par bâtiment, appartenant à des bâti-ments proches. Le critère ordinal est l'ordre des étages : dans la zone correspondant à chaque bâtiment les « pixels-étages » sont ordonnés de bas en haut par rapport à la base. Il n'y a pas de critère binaire.
Résultats
L'exemple choisi est la visualisation du nombre de points d'archivage d'un hôpital (FIG.1). On voit que ces points sont dispersés dans tout l'hôpital, le bâtiment 1 en ayant le plus grand nombre.

Introduction
Ces dix dernières années un grand nombre de travaux ont été proposés pour rechercher des motifs fréquents dans de grandes bases de données. En fonction des domaines d'applications les motifs extraits sont soit des itemsets (Srikant, 1995;Zaki, 2001;Pei et al., 2001;Ayres et al., 2002) soit des séquences (Agrawal et al., 1993;Han et al., 2000). Récemment les travaux issus de la communauté des chercheurs en base de données et en fouille de données considèrent le cas des data streams où l'acquisition des données s'effectue de façon régulière, continue ou incrémentalement et cela sur une durée longue voire éventuellement illimitée.
Compte tenu de la grande quantité d'information mise en jeu dans le cas des data streams, le problème de l'extraction de motifs fréquents est toujours d'actualité ( (Li et al., 2004;Jin et al., 2003;Demaine et al., 2002;Manku et Motwani, 2002;Golab et Ozsu, 2003;Karp et al., 2003)). Dans ce contexte, un motif est dit ?-fréquent s'il est observé au moins une fraction ?, appelée support du motif, sur tout le stream. Le paramètre theta, tel que 0 < ? < 1, est fixé par l'utilisateur.
Dans le cas des data streams, sujets à des mises à jour régulières et fréquentes, les approches traditionnelles ne conviennent pas car les résultats obtenus pour l'ancienne base ne sont que partiellement valables pour la nouvelle et il n'est pas envisageable de relancer l'algorithme sur la base de données mise à jour. En effet, dans tous les travaux développant une approche par mise à jour incrémentale (Masseglia et al., 2003;Cheng et al., 2004), la problématique principale d'optimisation et de performance consiste à construire et à maintenir, au fur et à mesure des différentes mises à jour successives, un ensemble de motifs candidats. Celui-ci est utilisé pour mettre à jour les motifs fréquents et éviter de relancer l'algorithme depuis zéro.
Il convient également de souligner une autre caractéristique intrinsèque des data streams qui découle du fait que la connaissance du stream n'est que partielle quel que soit l'instant considéré. En conséquence, il est nécessaire de prendre en compte l'incertitude engendrée par la connaissance toujours incomplète du data stream. Précisément, cela se traduit dans le cas de la recherche de motifs fréquents en soulignant que les motifs fréquents obtenus ne sont en fait que des motifs fréquents observés. En fait, à cause de cette incertitude, deux sources d'erreurs doivent être considérées :
-Les motifs observés comme fréquents ne sont peut être plus du tout fréquents sur une longue période d'observation du stream. -Inversement des motifs classés comme non fréquents peuvent le devenir sur une plus longue période d'observation du stream. Pour éviter ces erreurs, il est nécessaire de développer une approche pour connaître si un motif est fréquent sur une partie déjà examinée du stream. Cette approche doit de plus être prédictive pour savoir avec quelle probabilité un motif est fréquent ou non sur l'ensemble du stream. De nombreuses applications, par exemple dans le domaine de la prévision météorologique ou encore dans l'analyse de tendance en finance nécessitent ce type d'approche. Même en disposant d'une très grande partie du stream, d'un point de vue statistique, il est impossible de s'affranchir de ces deux sources d'erreur (Vapnik, 1998). Notre objectif sera donc d'essayer d'approcher le mieux possible une solution optimale.
Dans cet article nous proposons une approche qui permet, tout en considérant l'incertitude inhérente à la connaissance des streams, de construire et de maintenir des ensembles de motifs candidats bien choisis. Ceci constitue un préalable fondamental et nécessaire à toute approche pertinente dans le cadre de la mise à jour incrémentale des data streams.
La suite de la présentation est organisée de la façon suivante. Dans le paragraphe 2, nous introduisons les concepts qui permettent de contrôler l'incertitude découlant des sources d'erreurs. Au paragraphe 3, nous montrons comment obtenir les ensembles de motifs pertinents pour effectuer la mise à jour incrémentale. Le paragraphe 4 présente une expérimentation de notre approche, suivie d'une analyse comparative avec des travaux connexes au paragraphe 5. Nous concluerons notre étude au paragraphe 6.
dépendemment à partir d'une distribution D sur laquelle nous ne formulons aucune hypothèse, excepté celle de dire qu'il n'y a pas de biais. Nous renvoyons le lecteur intéressé par des approches prenant en compte le biais dans les cas de fouilles de données supervisées aux travaux de (Fan et al., 2004;Wang et al., 2003). La partie du stream observée est représentée par S dans la figure 1. A partir d'une valeur fixée par l'utilisateur du paramètre ?, le support théorique, on voudrait connaître l'ensemble des motifs vrais ?-fréquents de X. Cet ensemble nommé X ? est représenté en grisé sur la figure 1. Hormis l'incertitude et les aspects liés à l'estimation statistique, l'approximation de l'ensemble X ? recouvre un aspect combinatoire qui provient de la très grande taille de X même si cet ensemble peut être fini. Nous nommons S l'ensemble des motifs observés extraits du stream avec |S| = m (|S| << |X|). Nous réduisons cette différence à l'aide d'un algorithme qui permet d'obtenir un super ensemble S * de S avec |S * | = m * > m. Typiquement, S * contient des motifs supplémentaires obtenus à partir d'une généralisation des éléments de S (Mannila et Toivonen, 1997). Ce n'est pas le propos de cet article de traiter l'aspect combinatoire, l'essentiel est que S * ne sera jamais suffisamment grand pour englober X ? , au regard de la façon dont il est construit (voir figure 1). Ainsi l'importance du problème d'estimation statistique demeure entier.
Soit l'ensemble X * ? (figures 1 et 2) qui représente l'ensemble des motifs vrais ?-fréquents de X pour l'ensemble S , nous rappelons que cet ensemble ne peut être connu totalement compte tenu non seulement du fait que |S| << |X|, mais aussi des deux sources d'erreurs précédemment indiquées qui en découlent. Nous recherchons ? pour approcher au mieux l'ensemble X * ? , en utilisant S * ? (figure 2) qui représente l'ensemble des motifs observés ?
Pour apprécier l'approximation effectuée, consécutivement aux deux sources d'erreurs , les sous-ensembles X RNTI-E-6
Bordures statistiques pour la fouille incrémentale dans les data streams A partir des ensembles définis ci-dessus, on peut indiquer les formules de la précision et du rappel qui permettent d'estimer l'approximation effectuée.
La précision permet de quantifier la proportion des motifs ?-fréquents estimés qui sont en fait non vrais ?-fréquents, en dehors de S * ? . Si on cherche à Maximiser P, cela revient à minimiser la première source d'erreurs. Symétriquement, le rappel permet de quantifier la proportion de motifs, vrais ?-fréquents manquant dans S * ? . Si on cherche cette fois à maximiser R, cela revient à minimiser la seconde source d'erreurs. Une approche naïve pour approcher au mieux l'ensemble X * ? consisterait à choisir ? = ? ; autrement dit, la question que l'on pourrait se poser est de savoir si ? est un support statistique pour lui-même. Malheureusement, la principale et seule propriété de l'ensemble S * ? dans ce cas, est qu'il tend à correspondre avec une probabilité de 1 à l'ensemble X * ? lorsque le cardinal de l'ensemble S * tend vers ? selon le lemme de Borel-Cantelli (Devroye et al., 1996). Cela reviendrait à connaître tout le stream, ce qui n'est pas possible en pratique. Le théorème de Glivenko-Cantelli permet de montrer que l'on peut borner l'erreur commise sur les motifs vrais ?-fréquents en fonction de divers paramètres parmi lesquels le cardinal de l'ensemble S * , mais on ne peut pas faire mieux. Bien souvent, en traitement de l'information ou encore en medecine, plutôt que de simplement borner l'erreur, il est beaucoup plus important d'estimer et de contrôler des parties de l'erreur. On peut par conséquent développer une approche qui consiste à maximiser soit la précision P soit le rappel R. Le choix des valeurs aux limites de ? pourraient convenir en ce sens où l'on maximise la précision ou le rappel mais ces valeurs sont inintéressantes pour les applications en fouille de données. Par exemple, si on choisit ? = 0, on obtient S * 0 = S * et ainsi R = 1. Mais, nous avons également dans ce cas P = |X * ? |/|S * |, qui correspond à une valeur trop faible pour bien des applications, et donc tous les éléments de S * sont considérés comme des motifs vrais ?-fréquents du stream. On pourrait aussi choisir ? = 1 pour être sûr de maximiser P cette fois, mais il en découle que R = 0 et il se pourrait qu'aucun élément de S * ne soit un motif vrai ?-fréquent. Ces exemples avec les valeurs aux limites de ? nous permettent de bien cadrer les principes de notre approche. L'idée générale est de choisir subtilement ? différent mais suffisamment proche de ?, respectivement plus grand ou plus petit que ?, de sorte qu'il soit possible de contrôler en maximisant soit la précision soit le rappel, pour garantir avec une très forte probabilité que P = 1 ou respectivement que R = 1 tout en limitant la dégradation du paramètre non contrôlé. On obtient ainsi un ensemble S * ? pas trop petit contenant des informations significatives. Il y a une barrière statistique autour de ? qui empêche que ? ne soit ni trop proche ni trop éloigné de ? pour conserver la contrainte que P = 1 ou alors que R = 1, avec une forte probabilité. Notre objectif pour maximiser la précision ou le rappel avec une forte probabilité est par conséquent de rechercher les valeurs de ? les plus proches possibles de cette barrière. Le théorème ci-dessous permet d'établir les valeurs des supports statistiques que nous proposons en calculant la valeur de ? :
Choix de ?
Si on fixe ? = ? + ?, alors P = 1 avec une probabilité au moins de 1 ? ?. Si on fixe ? = ? ? ?, alors R = 1 avec une probabilité au moins de 1 ? ?. ? est le risque statistique lié aux data streams. Les valeurs ? + ?, ? ? ? sont les supports statistiques au sens de la définition 1.
Les supports obtenus (? = ? ± ?) sont statistiquement presque optimaux. Faute de place, nous renvoyons à l'article (Nock et al., 2005) 1 pour la démonstration complète. Celle-ci repose sur l'utilisation d'inégalités de concentration de variables aléatoires, qui, dans ce cas préçis, permettent d'obtenir des résultats statistiquement presque optimaux. Par optimalité, nous entendons que toute technique d'estimation obtenant de meilleures bornes est condamnée à se tromper (le critère à maximiser n'est plus ègal à un) quelque soit son temps de calcul.
Bordures statistiques pour la mise à jour incrémentale
Dans cette section nous introduisons deux bordures statistiques (supérieure et inférieure) qui vont être pertinentes dans le choix des motifs fréquents à conserver lors de la mise à jour incrémentale. L'objectif avec la bordure statistique inférieure est de maximiser la précision P, tandis qu'avec la bordure statistique supérieure il s'agit de maximiser le rappel R. Nous adoptons la notation probabiliste définie par (McAllester, 1999).
2
A partir du théorème 1, nous définissons l'ensemble suivant : pour un risque statistique ? fixé par l'utilisteur, en choisissant ? = ? + ?, on construit l'ensemble S * ?+? tel que ? ? , S * ?+? ? X * ? avec ? ? , P = 1. Ainsi, il n'y a plus la première source d'erreurs avec une forte probabilité. Tous les motifs de S * ?+? sont des motifs, vrais ?-fréquents de X * ? , mais on ne les a pas tous (voir figure 3). S * ?+? est le plus grand ensemble possible qui contient uniquement des motifs vrais ?-fréquents de X * ? après un temps d'observation du stream. Nous définissons cet ensemble comme étant la bordure statistique inférieure de X * ? à partir de l'échantillon des motifs S * de X. De façon symétrique, à partir du théorème 1, nous définissons l'ensemble suivant : pour un risque statistique ? fixé par l'utilisteur, en choisissant ?
Ainsi, il n'y a plus la deuxième source d'erreurs avec une forte probabilité, c'est à dire que S * ??? contient tous les motifs, vrais ?-fréquents de X * ? , mais il en contient d'autres (figure 4). S * ??? est le plus petit ensemble possible qui contient tous les motifs vrais ?-fréquents de X * ? après un temps d'observation du stream. Nous définissons cet ensemble comme étant la bordure statistique supérieure de X * ? à partir de l'échantillon des motifs S * du stream X.
FIG. 5 -Bordures statistiques pour la mise à jour incrémentale.
La figure 5 illustre l'utilisation de bordures dans un processus de mise à jour incrémentale. Nous recherchons les motifs ?-fréquents à partir des bordures statistiques (notées (a) et (b) sur la figure 5). Ces bordures sont construites pour le support ? choisi à partir de la valeur de ?, calculée pour S (DB). Ces bordures, nous permettent d'approcher au mieux l'ensemble de tous les motifs vrais ?-fréquents pour S , qui représente la base DB mise à jour par l'ajout de db, S = S ? db. Dans la section suivante lors de nos expérimentations, nous comparerons les motifs ?-fréquents obtenus grâce aux bordures statistiques par rapport à l'ensemble X * ? ( (c) sur la figure 5). Cet ensemble représente les vrais motifs ?-fréquents après mise à jour incrémentale (DB ? db). . Ces données représentent la navigation d'utilisateurs sur ce site. La taille du fichier de log représente environ 2,54Go (132k transactions). La deuxième base de données, appelée "BuAG", est obtenue à partir des 3,48Go (54k transactions) de Web log du serveur web de la bibliothèque de l'Université 4 . Pour analyser la qualité de nos bordures stastiques, nous évaluons différentes situations en faisant varier un ensemble de paramètres (une exception est faite pour ?, qui est fixé à .05). Les variations de ces paramètres sont décrites dans la figure 6. Le premier paramètre définit les différentes valeurs de support sur lesquelles nous allons réaliser l'expérimentation ("?" ). Le second paramètre, définit la taille de DB par rapport à la taille du stream simulé ("taille DB"). Le dernier paramètre, quant à lui, définit la taille de db par rapport à celle de DB. Dans notre cas db représentera au plus 50% de DB ("taille db"), ce paramètre permet de contrôler la taille de l'incrément par rapport à la partie stockée initialement. Pour gérer et organiser ces expériences un générateur est chargé de coordonner tous les tests à effectuer.
Au lieu d'utiliser un vrai data stream, qui aurait pu limiter la qualité de l'évaluation de nos bordures statistiques, nous avons choisi d'en simuler un à l'aide de la connaissance de son domaine X. Plus précisément, pour simuler le stream nous échantillonnons chaque base de données en fragments DB (S). Par exemple, nous pouvons considérer que les données arrivent successivement depuis la base de données "Dragons" et que nous ne pouvons en stocker que 20%. Pour cela nous prenons 20% des transactions contenues dans cette base de données et nous les conservons dans DB. Il ne reste alors plus qu'à prendre un incrément, db par exemple de taille 10%, de cette base. Nous construisons alors les bordures statistiques (bordure inférieure et bordure supérieure) définies dans la section 3 pour DB.
Les figures 7 et 8 montrent les résultats d'expériences obtenues, avec ? = 0.05, respectivement sur les bases Dragons et BuAG. Pour évaluer la qualité de ces bordures pour la mise à jour, nous représentons leurs comportements pour P et R par rapport à S (S = DB ? db). Ainsi, les courbes P ?+? et P ??? , représentent la précision respectivement pour la bordure statistique inférieure et pour la bordure statistique supérieure. De même les courbes R ?±? représentent le rappel. Les courbes P ? et R ? correspondent au choix trivial ? = ?. Nous constatons que ces courbes ont un comportement assez similaire :
-la précision P vaut ou approche 1 pour la plupart des bases stockées quand ? = ? + ?,  
FIG. 7 -représente les courbes P (à gauche) et R (à droite) sur la base Dragons pour trois valeurs de
valeurs de ? et deux tailles de S (% par rapport à |X|).
-le rappel R vaut ou approche 1 pour la plupart des bases stockées quand ? = ? ? ?. Ces observations sont en accords avec les résultats théoriques de la section 2. Nous observons également un autre phénomène : le rappel R associé à ? = ? + ? n'est pas très éloigné de celui associé à ? = ?. Il en est de même pour la précision P associée à ? = ? ? ?. Ce qui montre que la maximisation de la précision P ou du rappel R est obtenue avec un coût réduit au niveau de la dégradation de l'autre paramètre. Nous notons aussi que les courbes de précision P ont de meilleures performances que celles du rappel R notamment sur la figure 8. Ceci n'est pas très surprenant (c.f. section 2), car la fourchette de valeur pour la précision P est beaucoup plus restreinte que pour le rappel R.
La variation de la taille de S (DB) possède également une importance et vient conforter un résultat théorique auquel on pouvait s'attendre. En effet si on s'intéresse au rappel R, on constate, que ce soit sur les figures 7 ou 8, que les valeurs observées pour cette quantitée augmentent avec la taille de S. Cela traduit le fait que plus nous possédons de données observées plus la qualité de la prédiction augmente.
Sur ces bases de données un autre phénomène semble apparaître. Premièrement, à cause des faibles valeurs de ?, certains tests n'ont pas pu être réalisés car la valeur de ? ? ? était inférieure à 0. De plus, les différences observées entre les courbes semblent être liées aux tailles des bases de données utilisées. La base de données BuAG est plus petite que celle de Dragons d'un facteur 2.4. Nous pensons que ceci explique la différence entre les courbes : il s'agit de phénomènes liés à des bases de données de petites tailles et qui ne devraient pas être présents dans des bases de données plus conséquentes ou dans de vrais data streams.
Sur ces courbes (figures 7 et 8), le choix de ? = ? donne de meilleurs résultats en ce qui concerne la moyenne de P et R que le choix des deux valeurs de ? , sachant que ni P, ni R ne sont très proches de 1 avec une forte probabilité dans ce cas. Avec notre approche, on peut efficacement optimiser le processus de mise à jour incrémentale. Dans le cas où on aurait un  
FIG. 8 -représente les courbes P (à gauche) et R (à droite) sur la base BuAG pour trois valeurs de
valeurs de ? et deux tailles de S (% par rapport à |X|).
espace suffisant de stockage, on choisirait ? = ? ? ?, la bordure supérieure, pour laquelle on possède des garanties importantes sur la présence des futurs fréquents en son sein (R = 1). Dans ce cas, le nombre de calculs supplémentaires sera faible lors de la mise à jour. Par contre dans le cas où on devrait se limiter pour des raisons de taille à conserver une bordure disons moins volumineuse, ? = ? + ?, la bordure inférieure, sera alors utilisée car elle contient les informations les plus pertinentes (P = 1). On peut ainsi voir les bordures statistiques comme des outils de mise à jour incrémentale indiquant les limites au delà desquelles : soit il est inutile de stocker plus d'informations dans le cas de ? = ? ? ? (valeur limite inférieure pour ? ) ; soit la perte d'information serait trop importante ? = ? + ? (valeur limite supérieure pour ? ).
Travaux connexes
Depuis 1996, de nombreux travaux de recherche se sont focalisés sur la maintenance des ensembles de motifs fréquents obtenus dans les bases de données statiques. Dans ce paragraphe nous regardons leur adéquation par rapport à notre problématique. Partition et FUP (Fast UPdate) (Cheung et al., 1996) sont deux algorithmes où un partitionnement de la base de données est effectué pour rechercher dans un premier temps les itemsets fréquents locaux relatifs à chaque partition puis dans un second temps en déduire les itemsets fréquents globaux par validation croisée. Cela repose sur l'hypothèse que les itemsets fréquents de la base doivent l'être dans au moins l'une des partitions. (Parthasarathy et al., 1999) ont développé un algorithme de mise à jour incrémentale ISM (Incremental Sequence Mining) en maintenant un treillis, de motifs de la base, construit à partir de tous les motifs fréquents et de tous les motifs de la bordure négative. (Zheng et al., 2002) ont développé un algorithme IUS( Incrementally Updating Sequence) en utilisant une valeur support pour limiter la taille de l'espace des candi-dats de la bordure négative. Ces différentes approches se heurtent aux inconvénients inhérents à l'utilisation de la bordure négative : -l'espace des motifs candidats à maintenir est très important ; -il est nécessaire de considérer les relations structurelles qui existent entre les motifs notamment dans le cas des motifs qui auraient une faible valeur de support. Les différents algorithmes utilisant la bordure négative sont très coûteux en temps et sont consommateurs d'espace mémoire. (Masseglia et al., 2003) ont développé un algorithme de mise à jour incrémentale ISE utilisant une approche par génération et test de candidats. Les inconvénients sont que :
-l'espace des candidats peut être très grand, ce qui rend la phase de test très lente ; -l'algorithme requiert plusieurs passages sur toute la base. Cela est très coûteux en temps particulièrement pour les motifs séquentiels à séquences longues.
Vrais fréquents L'algorithme IncSpan (Incremental Mining of Sequential Patterns in large database) développé par (Cheng et al., 2004) repose sur une approche statistique où l'on construit un ensemble de motifs semi-fréquents (SFS) en diminuant la valeur du support à partir d'un ratio. L'idée est de dire qu'à partir d'un ensemble de motifs "presque fréquents" (SFS), plusieurs des motifs fréquents de la base mise à jour proviendraient de SFS ou alors seraient déjà observés fré-quents dans la base connue précédant la mise à jour. Autrement dit, SFS constituerait une zone frontière entre les motifs fréquents et non fréquents. Sur la figure 9, nous représentons un exemple d'ensemble SFS. Cette approche présente des insuffisances majeures d'un point de vue statistique tant pour l'estimation de l'incertitude intrinsèque liée aux data streams que pour la construction de l'ensemble des motifs candidats permettant la mise à jour incrémen-tale. En effet, pour diminuer la valeur du support, le choix du ratio est simplement heuristique sans justification théorique. Ainsi, cette approche n'offre aucune certitude ni indication sur l'erreur commise lors de la construction de l'ensemble des motifs semi-fréquents quant aux motifs vrais ?-fréquents du stream (zone identifiée avec les symboles -sur la figure 9). De plus, nous n'avons aucune garantie sur la minimalité de cet ensemble (zone identifiée avec les symboles + sur la figure 9), ce qui est fortement pénalisant pour sa réutilisation dans l'objectif d'optimisation de la méthode de mise à jour incrémentale.
Conclusion
Dans cet article, nous abordons la problématique de la mise à jour incrémentale pour les motifs fréquents dans le cas des grandes bases de données. Dans le contexte des data streams, il est plus pertinent, de construire et de maintenir des ensembles de motifs vrais ?-fréquents et non simplement observés comme tels. Plusieurs travaux, (Kearns et Mansour, 1998;Nock et Nielsen, 2004;Vapnik, 1998), ont montré l'intérêt de l'approche statistique notamment dans la détermination de règles de prévision pour l'optimisation d'algorithmes. Notre contribution majeure porte précisément sur ce point par l'introduction de supports statistiques en complé-ment des supports classiques permettant d'obtenir des bordures statistiques qui constituent les ensembles statistiquement presque optimaux (à une constante près) à considérer dans le cadre de la mise à jour incrémentale. Les expérimentations présentées montrent la robustesse de l'approche, dans le cas des motifs séquentiels, au regard de la taille des bases stockées et des différentes valeurs supports testées. Ces résultats encourageants sont des points positifs quant à l'applicabilité et le passage à l'échelle de la méthode. Plusieurs extensions de ces travaux sont possibles dans bien des domaines de recherche en fouille de données. On citera notamment les travaux qui portent sur les structures de données où l'on cherche à maintenir des ensembles d'items qui sont observés fréquents avec un rappel maximum, (Jin et al., 2003). Nos préoc-cupations actuelles concernent la question suivante : est-il possible de construire un ensemble intermédiaire qui conserverait au mieux les propriétés de chacune de ces bordures ? Celui-ci représenterait un compromis entre une bordure statistique trop imposante à stocker et une autre pour laquelle le nombre d'accès à la base de données est trop important.

Introduction
La visualisation des corrélations et similarités principales dans un échantillon de données est l'objectif des méthodes factorielles (Lebart et al., 1984). Ces méthodes cherchent souvent des directions informatives orthogonales dans un nuage de données. Ces directions concentrent l'essentiel de la variance projetée car l'inertie est porteuse de sens. Une décomposition pertinente de l'inertie sur des plans de projection révèle quels individus sont similaires et quelles variables sont dépendantes. Bien que ces méthodes soient très pertinentes, les grands échan-tillons de données demandent de nouvelles méthodes efficaces pour leur analyse. Dans ce contexte, les cartes de Kohonen (1997) sont connues dans le domaine de l'analyse exploratoire des données pour généraliser les méthodes factorielles telles que la méthode d'Analyse en Composantes Principales ou ACP (Lebart et al., 1984) pour les données continues. Plus généralement, les cartes auto-organisatrices ou SOM (Kohonen, 1997) sont des méthodes de classification avec une contrainte de voisinage sur les classes conférant un sens topologique à la partition finale. Le GTM ou Generative Topographic Mapping (Bishop et al., 1998) est une carte auto-organisatrice probabiliste avec des contraintes sur les moyennes d'un mélange gaussien pour données continues, mais ce modèle est inopérant pour des données catégorielles ou binaires. Des modèles récents (Girolami, 2001;Kabán et Girolami, 2001;Tipping, 1999) ont été proposés pour étendre le GTM aux modèles de mélanges classiques pour données discrètes. Hofmann et Puzicha (1998) ont par contre proposé l'approche du modèle symétrique à aspects
Le modèle BATM
Le modèle proposé repose sur l'hypothèse d'indépendance des I × J cellules x ij ? {0, 1} d'un tableau binaire en modélisant chaque probabilité unidimensionnelle d'observer x ij comme un mélange de K lois de Bernoulli : P r(x ij = 1) = E(x ij ) = k ? ki a jk avec ? ki les proportions des K composants telles que k ? ki = 1. Ce modèle génératif correspond à sélectionner pour chaque ligne x i = (x i1 , x i2 , · · · , x iJ ) fixée, une distribution discrète ? i de composantes ? ki , puis pour chaque j-ième composante x ij , sélectionner un état k avec la probabilité ? ki afin de lui attribuer une valeur binaire selon la loi de Bernoulli de paramètre a jk . Un modèle comparable pour la classification sans contrainte a été récemment proposé. La log-vraisemblance
Afin d'induire une auto-organisation topologique des probabilités, nous considérons les K coordonnées {s k } k=K k=1 d'une grille bidimensionnelle régulière qui modélise un plan discrétisé sur lequel l'ensemble des données est disposé par le BATM. La grille est projetée non linéaire-ment dans un espace de plus grande dimension L, par une transformation non linéaire constituée de L bases fonctionnelles ? , et telle que
T . Les a jk forment alors les noeuds d'une surface non linéaire discrète : les probabilités de Bernoulli sont paramétrées par des fonctions a jk = ?(w
Ce modèle s'interprète comme une version binaire cartographique du LSA probabiliste ou pLSA de Hofmann et Puzicha (1998). Les paramètres inconnus sont estimés dans la section suivante.
-446 -RNTI-E-6
Estimation par GEM
L'inférence de notre modèle se réalise en maximisant la log-vraisemblance par une mé-thode itérative ; une solution analytique exacte n'existe pas en raison des non linéarités du mélange et des fonctions sigmoïdes. Donc nous étudions l'approche de l'algorithme de montée de gradient par EM (Dempster et al., 1977) généralisé (GEM) de McLachlan et Peel (2000). Cette approche suppose la vraisemblance complétée par la connaissance de la parti-
avec z i les variables latentes ayant pour support {1, 2 · · · , K}. L'algorithme EM ou ExpectationMaximisation repose sur la maximisation de l'espérance conditionnelle sachant les données et les paramètres de l'itération précédente. Ayant P (t) (Z|D) du pas t précédent, nous maximisons au pas t + 1 :
k|i,j,xij /J. Pour résoudre w (t+1) = argmax w Q(?|? (t) ), nous effectuons des dérivations élémentaires du critère qui aboutissent au gradient Q (t) (t) j et au bloc de la hessienne H j . Le pas de NewtonRaphson suivant augmente alors localement la log-vraisemblance :
A la convergence du GEM, nous obtenons un estimateur au maximum de vraisemblance notéˆ?notéˆ notéˆ?. Pour éviter un surapprentissage et une instabilité numérique, nous ajoutons à la fonction Q un paramètre de régularisation bayésien (MacKay, 1992) : ?? j w T j w j . Cette correction ajoute ??w j au gradient Q j et ?? à la diagonale de la hessienne H j . La valeur de l'hyperparamètre ? est choisie manuellement comme la plupart du temps dans la littérature, ici nous avons pris ? = 0.01.
Formulation IRLS
Nous écrivons l'algorithme de Newton sous une forme matricielle qui est proche d'un pas d'Iteratively Reweighted Least Squares ou IRLS (McCullagh et Nelder, 1983) pour la régression logistique. Pour j de 1 à J :
-447 -RNTI-E-6
Carte auto-organisatrice probabiliste sur données binaires Nous avons R (t) j la matrice de taille K ×I qui compte pour cellules les probabilités a posteriori P (t) (t) (t) k|i,j,xij , la matrice diagonale G j a pour éléments non nuls i P k|i,j,xij , A j est le vecteur de i-ième composante a ij , a (t) (t) j est un vecteur colonne avec a jk pour k-ième composante, F (t) j est la matrice diagonale avec a
Pour accélérer numériquement l'algorithme, l'approche de Bohning (1993) remplace la matrice exacte, relativement lourde à calculer en pratique, par une matrice alternative fixe. Par exemple, la matrice B = ?
j ? B est non négative, symétrique, ce qui permet de maximiser la vraisemblance. Comme la convergence est lente, nous proposons un algorithme de type variationnel alternatif.
Estimation variationnelle
En suivant la borne 1 (Saul et al., 1996) sur log(1 + exp(? T k w j )), nous obtenons le nouveau critère à optimiser :
variationnel à estimer en maximisant˜Qmaximisant˜ maximisant˜Q. En dérivant ce nouveau critère, nous obtenons le pas de maximisation :
où A j est le vecteur colonne ayant x ij ? 0.5 pour i-ème composante. Finalement, trois algorithmes, et un quatrième décrit ci-après, sont présentés pour estimer les paramètres du modèle. Ayant éliminé la solution du gradient simple mais inefficace, on constate que l'algorithme IRLS donne la meilleure vraisemblance dans notre cas comme le montre les expériences dans la section suivante.
Simulations
Dans cette section, nous abordons tout d'abord deux éléments complémentaires à la mé-thode proposée, l'initialisation des paramètres du modèle et l'auto-organisation des probabilités sur les lignes afin d'obtenir la meilleure carte projective finale possible. Nous décrivons alors les résultats numériques de nos simulations sur des données binaires réelles.
Initialisation du modèle
Des tirages aléatoires répétés des paramètres initiaux sont une solution aux minima locaux que rencontrent les algorithmes basés sur le gradient. Pour obtenir la meilleure convergence possible on procède à une "bonne initialisation". Puisque les cartes de Kohonen sont des gé-néralisations de l'ACP, le premier plan de cette méthode fournit une intéressante première position (Elemento, 1999)   (Jolliffe, 2002), AFC (Benzécri, 1992), LSA (Deerwester et al., 1990) ou même celles obtenues suite à une projection non linéaire telle qu'un MDS (Sammon, 1969). Alors une grille régulière est dessinée sur cette première projection et chaque cellule de la grille correspond à un facteur du modèle BATM : x i est affectée à la z ki ? h(k, z i ) pour une fonction de lissage telle que celle de voisinage des cartes de Kohonen, i.e. h(k, z 
Lissage des paramètres sur les lignes
Il peut être intéressant d'ajouter une contrainte topologique sur les paramètres partitionnant les lignes afin d'accélérer la convergence de l'algorithme et améliorer la carte finale. Comme une solution par un soft-max (Bishop, 1995) nous apparaît relativement lourde, nous proposons une solution alternative en ajoutant un simple lissage par un terme de pénalisation issu de l'approche du TNEM (Priam, 2003). Brièvement, il s'agit de classer les vecteurs de données avec un lissage spatial sur les composantes ? ki du modèle BATM, à la manière d'un champ de Markov caché (Zhang, 1992;Celeux et al., 2003;Ambroise et Govaert, 1998). On pose :
où ? i est le vecteur de composantes ? ki , et V est -soit la matrice des fonctions de voisinage de la carte auto-organisatrice, i.e. V k = h(k, -soit la matrice binaire d'adjacence du treillis correspondant à notre carte probabiliste, i.e. V k = 1 ssi le k-ième noeud est voisin du Le pas complémentaire associé s'écrit :
et se résout en itérant l'égalité et en réinjectant dans le membre de droite les anciennes valeurs courantes des ? (t+1) ki jusqu'à ce que la stabilisation de leurs valeurs soit atteinte. Nous retrouvons évidemment le pas d'estimation non contrainte en annulant ?. Nous avons éliminé le terme additif du TNEM qui porte sur les entropies des ? i , donc nous obtenons un nouvel algorithme appelé TNEM2, plus général que le TNEM original puisqu'il s'applique à des probabilités non forcément a posteriori. Une alternative au TNEM est une estimation des ? ki paramétrés comme le GTM. La fonction à optimiser s'écrit alors Q I (?|?
où les w i sont les inconnues à déterminer. Leur estimation s'effectue comme précédemment, par une montée de gradient en réalisant une boucle sur l'indice i des lignes, de 1 à I, et en calculant les gradients Q ki . Nous proposons finalement le quatrième algorithme d'estimation noté IRLS+TNEM2 qui associe une maximisation sur les paramètres des colonnes par l'IRLS à un lissage des probabilités des lignes par le TNEM2. Nous expliquons dans la suite comment ce lissage se comporte en pratique.
Post-processing de la carte finale
La carte finale montre une grille de centres de classes bien organisées ; à chacune on affecte les données dont elle est le plus proche. Pour les cartes auto-organisatrices classiques on utilise la distance euclidienne entre le vecteur centre et le vecteur donnée. Ici le modèle permet une alternative probabiliste puisque nous avons la probabilité de génération d'une donnée par un composant k du mélange. Donc chacun des vecteurs x i est affecté à un centre par un maximum a posteriori (MAP), i.e. ˆ z i = argmax k ˆ ? ki . De la même manière, la j-ième variable est affectée au centre de labeî z j = argmax k ˆ a jk . Le MAP aboutit aux positions bidimensionnelles p i = s ˆ zi et p j = s ˆ zj pour les lignes et colonnes de la matrice projetée. Une seconde manière de projeter chaque donnée est par sa position moyenne (Bishop et al., 1998)  
Expériences
Nous expérimentons notre modèle sur plusieurs échantillons de données pour valider notre approche, par exemple, sur l'échantillon Zoo 2 , qui compte 101 animaux avec sept classes et 21 caractéristiques binaires. Notre méthode BATM converge vers une carte bien organisée où l'on reconnaît les sept classes que l'algorithme a projetées. La segmentation de la grille sur la figure 1 s'effectue à l'aide d'une procédure automatique consistant (Vesanto et Alhoniemi, 2000) en une classification ascendante hiérarchique avec agrégation par le diamètre (completelinkage) associée à une distance du ? 2 sur la matrice desˆ?desˆ desˆ? ki , qui donne les meilleurs résultats en pratique. En remarque, la classe contenant les reptiles est peu homogène d'après nos expé-riences, car ces animaux se regroupent mal. L'évolution de la log-vraisemblance par les quatre algorithmes est présentée à la figure 2 pour le tableau du Zoo, démontrant la supériorité de l'algorithme IRLS comparativement à des algorithmes plus récents alternatifs. Cependant, un surapprentissage peut amener à une solution non suffisamment lissée, et nous préférons l'approche IRLS+TNEM2 à cause de son efficacité malgré sa vraisemblance moins élevée. Cette valeur plus faible s'explique par le terme de pénalisation qui aide à une plus rapide et meilleure auto-organisation des lignes comme vérifiée ici puisque cet algorithme s'arrête bien plus tôt que les trois autres, pour un critère d'arrêt identique (log-vraisemblance relative inférieure au seuil 10e-5). Notre initialisation originale par une régression adéquate se base sur le premier plan principal d'une Analyse des Correspondances (AFC). Celle-ci s'illustre sur la figure 3, démontrant l'intérêt d'une carte auto-organisatrice : alors que la méthode factorielle linéaire n'est pas capable de montrer les sept classes sur ce premier plan, notre carte par BATM extrait ces sept classes et trouve leur lien statistique grâce à la propriété de voisinage. Un ensemble de données textuelles est également projeté. Cette base est un échantillon du fichier Classic3 (Dhillon et al., 2003) qui compte trois classes (MEDLINE, CISI, CRANFIELD). Nous avons tiré aléatoirement (tirage équiprobable sans remise) 450 documents de ce fichier en prenant 150 documents dans chaque classe. Nous avons sélectionné les termes dont la fréquence totale est supérieure à 30 sur le corpus entier et pour l'ensemble du vocabulaire de 4303 termes. Nous aboutissons, en éliminant les textes vides, à une matrice de taille aléatoire : 450 par 170 environ. Nous montrons les positions moyennes des labels des documents correspondants et projetés sur la figure 4 pour l'une de ces matrices. Nous sommes en mesure de visualiser assez distinctement les trois classes séparées par notre projection non linéaire.
Conclusion et discussion
Nous avons présenté une nouvelle méthode de carte auto-organisatrice -récapitulée sur la figure 5-pour données binaires, comme on peut en trouver dans le domaine du traitement de l'image et du texte. De nouveaux résultats pour l'initialisation d'une méthode de projection probabiliste de données qualitatives a également été introduit.
Une perspective de nos travaux est la construction de biplots non linéaires par carte topologique. Nous travaillons actuellement à la projection de matrices de taille plus importante ainsi que sur des ensembles d'images binaires qui donnent des résultats encourageants. Ensuite des variantes au modèle BATM se posent en remplaçant E(x ij ) par une hypothèse alternative, i.e. un mélange de lois différent tel que par exemple E(
. Le modèle BATM s'étend également à d'autres types de données comme il est proposé dans le paragraphe suivant. L'estimation peut encore être améliorée en déterminant notamment le meilleur hyperparamètre ?. En conclusion, le récent modèle du Block Clustering (Govaert et Nadif, 2003, 2005   Initialisation de ? (0) = ({a
Post-processing final de la carte de paramètresˆ?paramètresˆ paramètresˆ? ? Tableau segmenté par CAH, projections moyennes, biplot.
FIG. 5 -Schéma récapitulatif de la méthode BATM.
de mélange classique à un modèle de mélange croisé. Celui-ci est un modèle génératif flexible qui s'avère une alternative efficace et prometteuse au modèle à aspects. Il serait intéressant de l'étendre en lui ajoutant une propriété d'auto-organisation.
Annexe : paramétrisation probabiliste alternative au soft-max
Lorsque la matrice de données est un tableau de contingence, la loi de Bernoulli n'est plus valable, et une hypothèse de loi multinomiale est généralement supposée. Un paramétrage soft-max est alors introduit pour le cas de probabilités contraintes en régression et classification notamment. On écrit dans notre cas p j|k = e w T T j ?k / j e w j ?k avec j p j|k = 1. Donc cette paramétrisation implique l'inversion d'une matrice hessienne pleine pour procéder à l'optimisation. Nous proposons un moyen alternatif plus efficace. L'idée principale est d'aboutir à de nouveaux paramètres -sans la contrainte de somme à l'unité classique pour une multinomialeen écrivant p j|k comme une loi jointe de variables de loi de Bernoulli de paramètres inconnus. Il s'agit d'écrire la loi jointe de la j-ième colonne associée en mettant à l'unité la composante qui nous intéresse, et à zéro les autres, puis en supposant des lois de Bernoulli sur l'ensemble des composantes prises indépendantes pour le vecteur binaire résultant. L'expression p j|k = p jk j =j (1 ? p j k ) p jk avec p jk ? [0, 1] donne une solution valide au maximum de vraisemblance d'une loi multinomiale, pour des probabilités assez petites (éventuellement par l'ajout de composantes artificielles supplémentaires pour diminuer les valeurs), i.e. p  Nous retrouvons l'expression classique de l'estimation des paramètres de la loi multinomiale, 

Introduction
La classification, la segmentation et l'étiquetage de données séquentielles sont des problématiques au coeur de nombreux domaines comme la bioinformatique, la reconnaissance de l'écriture, l'extraction d'information. Une des problématiques principales dans ce type de domaine consiste en effet à transformer une séquence observée (un signal écrit par exemple) en une séquence d'étiquettes (on utilise également le terme de labels). Cette tâche peut être réalisée à différents niveaux. On cherche à segmenter le signal écrit d'une phrase en une séquence de mots, de même que le signal écrit de chaque mot doit être segmenté en une séquence de caractères, etc.
Les modèles Markoviens cachés (MMC) constituent l'approche la plus utilisée pour résoudre ce type de tâches bien qu'ils reposent sur des hypothèses d'indépendance fortes sur les données et qu'ils soient appris de façon non discriminante. Ce dernier point vient du fait que ce sont des modèles génératifs et qu'ils définissent une loi de probabilité conjointe sur la séquence d'observations X et la séquence d'étiquettes associée Y. Diverses ) , ( Y X P 1 Ce travail est en partie financé par le programme IST de la communauté européenne, à travers le réseau d'Excellence PASCAL IST-2002-506778. méthodes ont été proposées pour introduire de l'information discriminante dans des systèmes de type Markovien ou plus généralement basés sur des modèles génératifs (Jaakkola et al., 1999, Bahlmann et al., 2002, Moreno et al., 2003. Ces travaux reposent en grande partie sur des méthodes à noyau et des machines à vecteur support.
Des modèles sont apparus récemment qui visent à palier à l'ensemble des défauts des MMC. Ce sont des modèles conditionnels qui s'attachent à modéliser la loi de probabilité conditionnelle . On peut citer les modèles de Markov à entropie maximale (McCallum et al., 2000) et les champs de Markov conditionnels (Lafferty et al., 2001). Ces modèles ont été essentiellement utilisés jusqu'à présent dans le traitement de documents textuels, pour la reconnaissance d'entités nommées, l'extraction d' ) / ( X Y p information ou l'étiquetage morphosyntaxique. Les caractéristiques employées et les algorithmes d'apprentissage sont par conséquent adaptés à ces contextes applicatifs. Cette contribution explore l'emploi de modèles conditionnels pour la reconnaissance de données de type « signal », telles que la parole ou l'écriture en ligne. Plusieurs adaptations sont nécessaires. Tout d'abord, les observations sont de nature différente, ce sont généralement des séquences de vecteurs réels dans . Ensuite, les classes sont souvent multimodales, il y a par exemple plusieurs façons d'écrire un « a ». Enfin, l'étiquetage des données dans la phase d'apprentissage est le plus souvent partiel --on connaît la classe d'une séquence d'observations (e.g. un « a ») mais on ne connaît pas la séquence d'états correspondante --alors que les algorithmes proposés dans la littérature requièrent une base de données totalement étiquetée.
p R
Dans la suite, nous commençons par introduire les modèles conditionnels. Puis nous présentons des modèles conditionnels adaptés à des classes multimodales et dérivons les algorithmes pour apprendre ces modèles avec des données partiellement étiquetées. Enfin, nous fournissons des résultats expérimentaux pour deux tâches de classification de séquences, la reconnaissance de caractères manuscrits en ligne et la reconnaissance de comportements de l'utilisateur en se basant sur les mouvements de son oeil 2 , en comparant modèles conditionnels et modèles Markoviens standards.  Markov cachés (MMC) puis les modèles de Markov à entropie maximale (MMEM) (McCallum et al., 2000). Nous présentons ensuite les champs de Markov conditionnels (CMC) (Lafferty et al., 2001) ainsi qu'une extension, les CMC semi-Markoviens (Sarawagi et Cohen, 2005).  
Modèles conditionnels pour données séquentielles
Modèles de Markov cachés (MMC)
En comparant à l'équation (2) on voit que l'on a remplacé les probabilité de transition par des probabilités conditionnelles de la forme et les probabilités d'émission n'apparaissent pas dans cette formule. L'apprentissage est ainsi focalisé sur ce qui différencie une transition d'une autre sans avoir à modéliser complètement le processus de génération des données, i.e. on ne modélise pas la marginale P(X). Les MMEM sont des modèles discriminants. Un autre avantage vient du fait que cette modélisation ne réclame pas d'hypothèses particulières sur X et que l'on peut paramétrer les lois de probabilité de
transition par des caractéristiques exploitant des dépendances complexes entre les observations de la séquence X. (Lafferty et al., 2001) ont toutefois mis en évidence un comportement indésirable des MMEM qu'ils ont appelé « label bias » que nous ne décrivons que succinctement. Ce problème vient du fait que les lois de probabilités de transitions à partir d'un état sont normalisées, c'est à dire que 1 )
. Cela induit que si la ' s structure du modèle est telle qu'un état n'a qu'un état successeur, l'observation X n'a aucune influence sur le décodage.
Champs de Markov conditionnels (CMC)
Les champs de Markov conditionnels sont une instance particulière des champs aléatoires. Ces derniers permettent d'estimer des lois de probabilité jointes sur un graphe de noeuds représentant des variables aléatoires. Les champs aléatoires conditionnels font de même mais en conditionnant les valeurs des noeuds représentant les variables à estimer (les labels) par les valeurs de noeuds représentant les variables d'entrée. La figure 1 illustre les différences fondamentales entre les modèles MMC, MMEM et CMC en les représentant sous forme de modèles graphiques. La figure 1 (c) est une représentation graphique d'un champ aléatoire conditionnel (avec une structure de chaîne). Cette représentation est à comparer avec celle d'un MMC ( Fig. 1 (a)) et celle d'un MMEM ( Fig. 1 (b)). Notons que ces deux derniers modèles sont représentés par des graphes orientés qui permettent d'exprimer les lois de probabilité, jointe ou conditionnelle, suivant les formules (2) ou (3). Notons également que les MMEM et les CMC, étant des modèles conditionnels, ne requièrent pas d'hypothèses particulières sur X, ce qui explique les noeuds non décomposés X dans les figures 2 (b) et (c).
FIG. 1 -Représentation de MMC (a), MMEM (b) et CMC (c) sous forme de modèles graphiques, les noeuds grisés correspondent aux variables observées.
Nous considérons ici des champs aléatoires --définis par un graphe de noeuds et de liens--tels que, conditionnellement à X, la séquence d'étiquettes Y obéit à la propriété Markovienne exprimée par le graphe des noeuds.
signifie que t et t' sont des voisins dans le graphe. En exploitant la théorie des champs aléatoires (Lafferty 2001), on peut montrer que la probabilité conditionnelle d'une séquence d'étiquettes Y connaissant l'observation X peut se mettre sous la forme :  Fig. 1 (c)), les caractéristiques sont définies soit sur un état, soit sur deux états successifs. Soit l'ensemble des fonctions caractéristiques. Alors : Des caractéristiques typiquement utilisées dans des problèmes d'étiquetage morpho syntaxiques sont par exemple « le mot commence par une majuscule ». Dans le cas de données comme la parole ou l'écriture en ligne on ne peut pas définir des caractéristiques de haut niveau de ce type. Dans ces domaines, un signal d'entrée est transformé après prétraitement en une séquence de vecteurs de p caractéristiques réelles. On utilisera alors ces p caractéristiques dans les CMC.
L'inférence est réalisée à l'aide d'un algorithme de programmation dynamique. Si la topologie du graphe est une chaîne on utilise l'algorithme de Viterbi, si c'est un arbre on peut utiliser l'algorithme Belief Propagation (Weiss, 2001) et pour un graphe quelconque on utilise Loopy Belief Propagation (Pearl, 1988, Murphy et al., 1999. En apprentissage, on dispose d'une base d'apprentissage de K exemples complètement étiquetés,
k est la séquence des étiquettes (i.e. noeuds) correspondant à X k . Durant l'apprentissage, on cherche les paramètres W qui maximisent la log-vraisemblance conditionnelle sur l'ensemble d'apprentissage. Ce critère est convexe et peut être maximisé par une méthode de gradient. Il s'écrit :
Il faut noter que le facteur de normalisation implique une somme sur un nombre exponentiel de séquences d'états possibles, mais cette somme peut être calculée efficacement via un algorithme de programmation dynamique.
Champs de Markov conditionnels semi-Markoviens (SCMC)
Les CMC présentés précédemment permettent de déterminer la séquence de labels de probabilité maximale pour une séquence d'observations donnée . Cela correspond bien à des problématiques comme l'étiquetage morpho syntaxique, où l'on cherche une étiquette pour chaque mot. Cependant, on peut souhaiter travailler sur des données séquentielles telles que les observations prises isolément ont moins de sens que des segments de plusieurs observations successives. Afin d'intégrer des informations segmentales, (Sarawagi et Cohen, 2004) ont proposé une extension des CMC appelée CMC semi-Markoviens ou semi-CMC (SCMC). Etant donnée une observation , on cherche la meilleure segmentation de X en étiquetant les 
CMC pour la classification de séquences
SCMC pour la classification de données monomodales
Les CMC sont, comme nous l'avons déjà mentionné, traditionnellement utilisés pour l'étiquetage de données séquentielles. Pour concevoir un système de classification de séquences basé sur un CMC on peut utiliser une architecture basée sur une structure de chaîne pour chaque classe. La structure de chaîne est en effet naturelle pour modéliser des séquences d'une même classe dans lesquelles on veut distinguer les différentes parties comme le début, le milieu, la fin. Dans le cas de la reconnaissance de l'écriture manuscrite en ligne par exemple, on utilise un modèle de type chaîne pour chaque lettre. Dans la chaîne correspondant au caractère « a », le premier noeud correspond au début du tracé du « a », le second noeud à une partie intermédiaire, etc. On utilise alors une architecture du type mélange de structures en chaîne, telle que celle illustrée figure 2. Il y a une « branche » par classe. Les branches peuvent avoir des nombres d'états différents, il s'agit d'un choix a priori sur la structure du modèle. Bien entendu, la représentation « dynamique » de ce modèle est similaire à la Figure 1 (c), la différence vient du fait que toutes les transitions entre noeuds ne sont pas autorisées. Lorsque l'on apprend un modèle de ce type avec des données des N classes, on apprend des paramètres W qui permettent de discriminer au mieux entre les séquences d'observations des différentes classes.
FIG. 2 -Un champ aléatoire pour la classification de séquences dans le cas de données monomodales. Chaque « branche » correspond à une classe.
L'étiquetage disponible dans des bases d'apprentissage pour la classification de signaux est souvent minime et réduit à la classe. Cela signifie que l'on sait que la segmentation correspondant à une séquence d'observations correspond à une branche particulière (e.g. celle du « a ») mais on ne connaît pas la segmentation plus précisément. Or les algorithmes d'apprentissage des CMC requièrent cette information. Nous montrons ici comment apprendre un CMC sans disposer de cette information.
On dispose donc d'une base d'apprentissage de K exemples,  (Quattoni et al., 2004). Pour limiter les problèmes numériques et simplifier l'implémentation on peut choisir d'approximer la quantité précédente par :
En apprentissage, on estime les paramètres maximisant la log-vraisemblance conditionnelle, la segmentation (non disponible) des séquences d'apprentissage est donc apprise au fur et à mesure pendant l'apprentissage (Do, 2005). Il faut noter qu'à cause de ces variables cachées, le critère a plusieurs maximums locaux, l'optimisation n'assure donc pas de trouver l'optimum global.
SCMC pour données multimodales
Bien souvent les classes sont multimodales et la modélisation précédente peut s'avérer insuffisante. Nous proposons dans ce cas des modèles que l'on peut voir comme des mélanges de champs aléatoires conditionnels. Plutôt que d'avoir une structure en chaîne pour chaque classe, on en considère plusieurs, chacune pouvant se spécialiser, pendant l'apprentissage, sur une modalité de la classe.
La difficulté de l'apprentissage consiste ici encore dans le manque d'étiquetage des données en apprentissage. Ici, on dispose de la même information que précédemment pour une séquence d'apprentissage, sa classe. On sait par exemple qu'une séquence d'observations est un « a » mais on ne connaît pas l'allographe (i.e. la branche) correspondant ni la segmentation dans cette branche. On introduit alors un deuxième type de variable cachée, l'indicateur de la branche. On obtient donc :
Où B désigne une branche, désigne l'ensemble des branches correspondant à la classe , et S , et ont la même signification que précédemment.
Pour simplifier les calculs et l'implémentation on peut choisir d'approximer la quantité précédente en approximant la somme au numérateur par un maximum, comme dans l'équation (8). En apprentissage, on estime les paramètres maximisant la log-vraisemblance conditionnelle, la segmentation (non disponible) des séquences d'apprentissage est donc apprise au fur et à mesure pendant l'apprentissage. Il faut noter qu'à cause des variables cachées, l'optimisation n'assure pas de trouver l'optimum global.
FIG. 3 -Modèle SCMC avec plusieurs modèles par classe pour prendre en compte les allographes.
Mises en oeuvre expérimentales
Les implémentations des CMC ont été réalisées en exploitant une bibliothèque implémentant un algorithme de gradient avec la méthode de quasi-Newton avec mémoire limitée LBFGS (Nocedal, 1989).
Déterminer l'intérêt d'un utilisateur par des mouvements de l'oeil
L'étude réalisée dans cette partie a été réalisée dans le cadre d'un challenge organisé par des membres du réseau d'excellence PASCAL. Ce challenge consiste à explorer la possibilité d'inférer l'intérêt d'un utilisateur dans les différentes lignes de texte qui lui sont présentées à l'écran (typiquement retournées par un moteur de recherches) en fonction des mouvements de son oeil (Salojärvi et al., 2005). Les données ont été collectées de la façon suivante. Pour une requête donnée, on présente à l'utilisateur 10 titres (un titre est une réponse tenant sur une ligne) correspondant à 10 réponses possibles. Parmi ces 10 titres, un seul est correct, 4 sont pertinents, et 5 sont non pertinents. On observe les mouvements de l'oeil de l'utilisateur qui cherche parmi les 10 titres celui qui le satisfait. Une telle expérience (requête + 10 titres de réponses) est appelée dans la suite un assignement.
La trajectoire de l'oeil est segmentée en fixations et saccades, puis on détermine à quels mots de la page ces fixations correspondent. Cette séquence ordonnée temporellement de fixations est transformée en une séquence de vecteurs de caractéristiques, un par fixation. On dispose d'une vingtaine de caractéristiques, fournies par les organisateurs du challenge (Salojärvi et al., 2005). On dispose également pour chaque fixation, du titre auquel appartient le mot fixé, de la position du mot dans le titre et de la longueur (en mots) du titre.
Puisque la segmentation en titres est connue, en apprentissage ou en reconnaissance, nous avons calculé des caractéristiques segmentales par segment de fixations correspondant à un même titre. Les vecteurs de caractéristiques sont additionnés sur tout le segment. Dans la suite, on considère qu'un assignement est représenté par une séquence de vecteurs de 22 caractéristiques , où représente la somme des vecteurs caractéristiques Lorsqu'un utilisateur visite une seconde, ou une troisième fois un même titre, on peut supposer qu'il ne le visite pas de la même façon et que cela est informatif. Nous avons donc également utilisé des modèles dans lesquels on distingue la première visite d'un titre de la classe N (ou P ou C) d'une deuxième visite d'un tel titre. Pour cela, il suffit de multiplier les noeuds (Figure 4 (b)). Le dernier titre visité est souvent très important, car c'est souvent le titre correct. Pour le prendre en compte automatiquement, nous avons rajouté trois noeuds correspondant à la dernière visite. Le décodage consiste à tester parmi toutes les possibilités (étiquetage des 10 titres parmi N, P ou C) celle de probabilité maximale.
Nous avons comparé ces systèmes conditionnels à des systèmes Markovien plus standard. Pour cela, nous avons appris des MMC similaires à 3, 6 ou 9 états dans lesquels les probabilités d'émission sont modélisées par des lois gaussiennes sur les vecteurs de caractéristiques. Il y a 336 assignements dans la base d'apprentissage et 149 assignements dans la base de test. Les performances sont calculées suivant la procédure proposée dans (Salojärvi et al., 2005), c'est à dire que l'on n'évalue le système que sur les réponses qu'il fournit pour les titres qui ont été effectivement visités. Le tableau 1 montre les performances des systèmes génératifs (MMC) et des modèles conditionnels (CMC). Les meilleurs résultats ont été obtenus avec des MMC à 6 états alors que les SCMC ont montré des performances supérieures quel que soit le nombre d'états, et allant jusqu'à 73% lorsque l'on distingue la première visite, la dernière visite et les visites intermédiaires à un même titre. 
Reconnaissance de l'écriture manuscrite en ligne
Un signal d'écriture manuscrite en ligne est un signal temporel constitué des coordonnées successives d'un stylo, il est capturé sur une tablette digitale ou via un stylo électronique. La base UNIPEN (Guyon et al., 1994) est une base internationale de référence dans le domaine de la reconnaissance d'écriture. Nous avons travaillé sur une partie de cette base regroupant des signaux de 200 scripteurs et correspondant aux 26 caractères. Notre base contient environ 60000 exemples, nous en utilisons 33% pour l'apprentissage et 66% pour le test.
Le signal d'écriture étant très variable (il existe de nombreux allographes pour tracer un même caractère) l'usage de modèles de mélanges ou de systèmes basés sur des prototypes de tracés typiques est extrêmement répandu. Les systèmes les plus performants sont souvent à base de MMC. L'apprentissage de ce type de modèles n'est pas aisé car le nombre d'allographes ainsi que la topologie des modèles Markoviens les modélisant doivent être fixés à la main. Divers travaux ont été menés pour apprendre complètement les modèles de caractères à partir des données (Lee et al., 2001, Artières et Gallinari 2002, ils sont basés sur la construction d'un MMC à partir d'un seul tracé et utilisent une représentation des tracés sous forme de séquence de codes directionnels. Le système de référence est un système de ce type, dont les détails peuvent être trouvés dans (Marukatat, 2004). Un des intérêts du système de référence Markovien réside dans sa capacité à apprendre la topologie (nombre de branches, nombre d'états) des modèles à partir des données, ce que nous ne savons pas faire aujourd'hui dans le cas de CMC.
Nous avons donc mis en oeuvre des CMC en fixant leur topologie d'après celle apprise par le système Markovien. L'idée consiste à construire un modèle CMC multi branches en reprenant les topologies des modèles MMC appris. De plus, on utilisera dans ce CMC les mêmes caractéristiques que celles utilisées dans les MMC. Le tableau 2 résume les performances du système Markovien et de systèmes SCMC pour la classification des 26 caractères minuscules. Les performances s'améliorent avec la taille des modèles. Comme on le voit, les SCMC surpassent le système Markovien dans toutes les expériences. On voit ici encore que les systèmes à base de CMC surpassent dans tous les cas le système Markovien de référence, ce qui est très intéressant. Car ce dernier système a fait l'objet de nombreux travaux depuis quelques années dans notre équipe et est au niveau de l'état de l'art dans le domaine. Il reste néanmoins du travail à réaliser pour mettre en oeuvre des CMC. Les algorithmes d'apprentissage sont encore relativement coûteux. Et surtout, la structure des CMC est déterminée d'après la structure apprise par le système Markovien.
Conclusion
Nous avons exploré dans cette contribution l'emploi de champs aléatoires Markoviens conditionnels pour la classification et l'étiquetage de signaux. Tout d'abord, nous nous sommes focalisés sur des variantes des CMC exploitant la notion de segments et des caractéristiques segmentales. Nous avons développé des algorithmes pour réaliser l'apprentissage de nos modèles en présence d'une information de segmentation minimale, de type classe. Nous avons fourni des résultats expérimentaux montrant que ces modèles surpassent des modèles Markoviens plus traditionnels dans deux tâches très différentes de classification de données séquentielles.

Introduction
Cet article présente une étude expérimentale consistant à évaluer le taux d'élagage le plus adapté pour l'extraction de la terminologie. Nous allons décrire ci-dessous notre méthode globale d'extraction de la terminologie et rigoureusement définir l'élagage.
La première phase de notre travail d'extraction de la terminologie à partir de corpus spé-cialisés consiste à normaliser les textes en utilisant des règles de nettoyage décrites par Roche (2004). Les corpus que nous utilisons sont décrits dans la section 3 de cet article. L'étape suivante consiste à apposer des étiquettes grammaticales à chacun des mots du corpus en utilisant l'étiqueteur ETIQ développé par Amrani et al. (2004). ETIQ est un système interactif s'appuyant sur l'étiqueteur de Brill (1994) qui améliore la qualité de l'étiquetage de corpus spé-cialisés. Nous pouvons alors extraire l'ensemble des collocations Nom-Nom, Adjectif-Nom, Nom-Adjectif 1 , Nom-Préposition-Nom d'un corpus spécialisé. L'étape suivante consiste à sé-lectionner les collocations les plus pertinentes selon des mesures statistiques décrites par Roche et al. (2004c); Roche (2004). Les collocations sont des groupes de mots définis par Halliday (1976); Smadja (1993). Nous appelons termes, les collocations pertinentes.
Les termes binaires (ou ternaires pour les termes prépositionnels) extraits à chaque itération sont réintroduits dans le corpus avec des traits d'union afin qu'ils soient reconnus comme des mots à part entière. Nous pouvons ainsi effectuer une nouvelle recherche terminologique à partir du corpus avec prise en compte de la terminologie du domaine acquise aux étapes précédentes. Notre méthode itérative, proche des travaux de Evans et Zhai (1996), est décrite 1 Corpus en français uniquement 2 État de l'art des méthodes d'extraction de la terminologie De multiples approches de recherche terminologique ont été développées afin d'extraire les termes pertinents à partir d'un corpus. Nous ne traiterons pas ici les approches d'aide à la structuration et au regroupement conceptuel des termes qui sont détaillés dans les travaux de Aussenac-Gilles et Bourigault (2003).
Les méthodes d'extraction de la terminologie sont fondées sur des méthodes statistiques ou syntaxiques. Le système TERMINO de David et Plante (1990) est un outil précurseur qui s'appuie sur une analyse syntaxique afin d'extraire les termes nominaux. Cet outil effectue une analyse morphologique à base de règles, suivie de l'analyse des collocations nominales à l'aide d'une grammaire. Les travaux de Smadja (1993) (XTRACT) s'appuient sur une méthode statistique. XTRACT extrait, dans un premier temps, les collocations binaires situées dans une fenêtre de dix mots. Les collocations binaires sélectionnées sont celles qui dépassent d'une manière statistiquement significative la fréquence due au hasard. L'étape suivante consiste à extraire les collocations plus générales (collocations de plus de deux mots) contenant les collocations binaires trouvées à la précédente étape. ACABIT de Daille (1994) effectue une analyse linguistique afin de transformer les collocations nominales en termes binaires. Ces derniers sont ensuite triés selon des mesures statistiques. Contrairement à ACABIT qui est fondé sur une méthode statistique, LEXTER de Bourigault (1993) et SYNTEX de Bourigault et Fabre (2000 s'appuient essentiellement sur une analyse syntaxique afin d'extraire la terminologie du domaine. La méthode consiste à extraire les syntagmes nominaux maximaux. Ces syntagmes sont alors décomposés en termes de "têtes" et d'"expansions" à l'aide de règles grammaticales. Les termes sont alors proposés sous forme de réseau organisé en fonction de critères syntaxiques.
Pour discuter le choix du taux d'élagage selon le nombre d'occurrences, nous allons classer les collocations en utilisant la mesure Occ RV décrite dans les travaux de Roche (2004). Cette mesure qui a le meilleur comportement comme précisé par Roche et al. (2004a,c) classe les collocations selon leur nombre d'occurrences et les collocations ayant le même nombre d'occurrences sont classées en utilisant le Rapport de Vraisemblance de Dunning (1993). Cette mesure est parfaitement bien adaptée à cette étude car le classement effectué selon le nombre d'occurrences permet de discuter le choix du taux d'élagage. Le principe de l'élagage des collocations consiste à exploiter seulement les collocations présentes un nombre de fois minimum dans le corpus. L'élagage permet d'exclure les collocations trop rares qui peuvent se révéler comme non significatives pour le domaine. Sur chacun des corpus expérimentés, le tableau 1 présente différents élagages appliqués.
Description des corpus
La première observation à relever dans le tableau 1 tient dans le nombre d'occurrences des collocations qui diffère selon les langues. Par exemple, les collocations de type Nom-Nom sont beaucoup moins fréquentes sur les corpus en français par rapport aux corpus en anglais.
Suivant les domaines de spécialité écrits dans une même langue, les résultats peuvent éga-lement différer de manière importante. Par exemple, sur le corpus de CVs, le nombre de collocations de type Nom-Nom est beaucoup plus important que celui du corpus des Ressources Humaines également écrit en français. Le corpus des Ressources Humaines a pourtant une taille plus grande que le corpus de CVs. Ceci est dû au fait que les CVs sont écrits de manière condensée en employant un vocabulaire très spécifique : "emploi solidarité", "action communication", "fichier client", "service achat", etc.
Le tableau 1 montre qu'en éliminant simplement les collocations présentes une seule fois dans le corpus, plus de la moitié des collocations sont supprimées dans tous les cas, excepté la relation Adjectif-Nom du corpus des Ressources Humaines. Ce corpus a un comportement -3 ème catégorie : La collocation est pertinente mais très générale et pas nécessairement adaptée au domaine (exemple du corpus de CVs : "situation actuelle") -4 ème catégorie : La collocation est non pertinente (exemple du corpus de CVs : "jour quotidienne") -5 ème catégorie : L'expert ne peut pas juger si la collocation est pertinente (exemple du corpus de CVs : "master franchisé").
Par exemple, les collocations qui sont des instances de concepts peuvent être utilisées pour découvrir des règles d'association entre concepts présents dans les textes. Les concepts peuvent également être utilisés pour construire des patrons d'extraction utiles pour la recherche d'informations.
Par exemple, pour découvrir des règles d'association, les concepts utilisés doivent être précis afin de déterminer des associations éventuelles. Ce travail a des similarités avec les approches de Srikant et Agrawal (1997) qui consistent à utiliser une taxonomie pour généraliser des règles d'association extraites. Dans nos travaux précédents ; Kodratoff et al. (2003)) et dans la thèse de Azé (2003), les règles d'associations découvertes sont de la forme concept 1 ...concept n?1 ? concept n où n est le nombre de concepts impliqués dans les règles d'association extraites. Le détail de l'algorithme est présenté dans les travaux de Azé (2003).
À titre d'exemple, nous donnons deux règles d'association extraites à partir du corpus des Ressources Humaines :
Cette règle signifie que le stress s'exerce par l'intermédiaire de l'environnement. Cette règle d'association, bien que correcte, n'a pas été jugée comme particulièrement intéressante (n'apportant pas d'informations nouvelles) lors de la phase de validation.
Un autre exemple de règle qui a été extraite est donné ci-dessous.
"Implication dans l'entreprise" ? "Environnement".
Une telle règle exprimant des informations liées à l'implication dans l'entreprise a été jugée comme intéressante mais qui demande une expertise plus approfondie.
L'extraction des règles d'association s'effectue avec des concepts très précis qui intéressent l'expert. Ainsi, en reprenant les différentes catégories évoquées au début de cette section, les collocations pertinentes afin de découvrir des règles d'association entre concepts sont les collocations de la catégorie 1. Les collocations issues des catégories 2, 3 et 4 sont jugées comme non pertinentes. Enfin, les collocations de la catégorie 5 ne sont pas prises en considération car l'expert n'a pas été apte à les valider. Cette dernière catégorie correspond en fait à des collocations qui sont ambiguës ou qui n'ont pas pu être évaluées par méconnaissance partielle du domaine.
Évaluation de la terminologie et taux d'élagage
Nous allons évaluer les résultats sur le seul corpus de CVs. En effet, pour ce seul corpus, toutes les collocations Nom-Adjectif quel que soit le taux d'élagage établi ont été évaluées par un expert.
Expertise de la terminologie
Le tableau 3 donne le nombre de collocations de type Nom-Adjectif associées à chaque catégorie d'expertise. Nous rappelons que chacune des catégories a été décrite dans la section 5 de cet article.
Le tableau 3 présente les résultats des expertises effectuées selon différents taux d'élagage. Plus les collocations fréquentes sont privilégiées en appliquant un taux d'élagage important et plus la proportion de collocations de la catégorie 1 correspondant aux collocations pertinentes est élevée. À titre d'exemple, si toutes les collocations sont conservées (élagage à un), la proportion de collocations pertinentes est de 56.3% contre plus de 80% en faisant un élagage de quatre, cinq ou six. 
Les mesures de précision, de rappel et de F score
La précision est un critère d'évaluation parfaitement adapté à un cadre de travail non supervisé. La précision permet de calculer la proportion de collocations pertinentes extraites parmi les collocations extraites. En utilisant les notations du tableau 4, la précision est donnée par la formule V P V P +F P . Une précision de 100% signifie que toutes les collocations extraites par le système sont pertinentes.
-210 -RNTI-E-6
Une autre mesure typique du domaine de l'apprentissage est le rappel qui calcule la proportion de collocations pertinentes extraites parmi les collocations pertinentes. Le rappel est donné par la formule En règle générale, il est important de déterminer un compromis entre le rappel et la préci-sion. Pour cela, nous pouvons utiliser une mesure prenant en compte ces deux critères d'éva-luation en calculant le F score (Van-Risbergen (1979)) :
Le paramètre ? de la formule (1) permet de régler les influences respectives de la précision et du rappel. Il est très souvent fixé à 1 pour accorder le même poids à ces deux mesures d'évaluation. Le tableau 5 montre que la précision la plus élevée est établie après un élagage important (un élagage supérieur à quatre fournit une précision supérieure à 85%). Cependant, avec une telle précision assez élevée le rappel est souvent très faible, c'est-à-dire que relativement peu de collocations pertinentes sont extraites. Avec ? = 1, nous pouvons relever dans le tableau 5 que le F score est le plus élevé sans appliquer d'élagage. Ceci est dû au fait que le rappel est peu important dès qu'un élagage même faible est effectué. En effet, comme précisé dans le tableau 1, un seul élagage de deux permet d'empêcher l'extraction de 75% des collocations Nom-Adjectif du corpus de CVs.
Le tableau 6 montre que le fait de faire varier ? afin de donner un poids plus important à la précision donne un F score logiquement plus élevé dans le cas d'élagages importants. Ceci montre les limites d'un tel critère d'évaluation car les résultats du F score peuvent singulière-ment différer selon la valeur de ?. Ainsi, la section suivante présente un autre critère d'évalua-tion global fondé sur les courbes ROC. 
Les courbes ROC
Dans cette section et dans les travaux de Ferri et al. (2002), les courbes ROC (Receiver Operating Characteristics) sont présentées. La notion de courbe ROC est initialement issue du traitement du signal. Les courbes ROC sont couramment utilisées dans le domaine de la méde-cine pour évaluer la validité des tests diagnostiques. Les courbes ROC présentent en abscisse le taux de faux positifs (dans notre cas, taux de collocations non pertinentes) et en ordonnée le taux de vrais positifs (taux de collocations pertinentes). Les courbes ROC sont adaptées aux approches supervisées. Par ailleurs, l'aire sous la courbe ROC (AUC -Area Under the Curve), peut être vue comme la mesure globale de l'efficacité d'une mesure d'intérêt. Précisons que le critère relatif à l'aire sous la courbe est équivalent au test statistique de Wilcoxon-MannWhitney (voir les travaux de Yan et al. (2003)).
Dans le cas correspondant au classement des collocations en utilisant des mesures statistiques, une courbe ROC idéale correspond au fait d'obtenir toutes les collocations pertinentes en début de liste et toutes les collocations non pertinentes en fin de liste. Cette situation correspond à une AUC de 1. La diagonale correspond aux performances d'un système aléatoire, progrès du taux de vrais positifs s'accompagnant d'une dégradation équivalente du taux de faux positifs. Une telle situation correspond à AUC = 0.5. Enfin, si les collocations triées par intérêt décroissant sont telles que toutes les collocations pertinentes sont précédées par les non pertinentes, alors AUC = 0. Une mesure d'intérêt efficace pour ordonner les collocations consiste donc à obtenir une aire sous la courbe ROC la plus importante possible ce qui est strictement équivalent à minimiser la somme des rangs des exemples positifs.
L'avantage des courbes ROC provient de l'absence de sensibilité dans certains cas où un déséquilibre entre le nombre d'exemples positifs et d'exemples négatifs est rencontré. Illustrons ce fait avec l'exemple suivant. Supposons que nous ayons 100 exemples (collocations). Dans le premier cas, nous avons un déséquilibre entre les exemples positifs et négatifs avec 1 seul exemple positif et 99 exemples négatifs. Dans le second cas, nous avons 50 exemples positifs et 50 exemples négatifs. Supposons que pour ces deux cas, les exemples positifs soient tous placés en tête de listes établies par des mesures statistiques, c'est-à-dire que les collocations pertinentes sont toutes situées en début de liste.
Dans les deux cas, les courbes ROC sont strictement identiques avec une AUC égale à 1 (voir figure 1, (a) et (b)). Ainsi, le fait d'avoir les collocations pertinentes en début de listes est mis en valeur par les courbes ROC et les AUC. L'intérêt principal des courbes ROC est le fait de ne pas tenir compte d'un éventuel déséquilibre entre le nombre de collocations pertinentes et non pertinentes. Dans le cas du calcul du F score (avec ? = 1) qui prend en compte les mesures  Le tableau 7 montre que l'élagage le plus adapté en terme d'AUC correspond à un élagage à trois pour les collocations Nom-Adjectif du corpus de CVs. La figure 2 montre la courbe ROC relative à un élagage à trois. Ce critère objectif fondé sur l'AUC correspond au choix souvent empirique d'élagage à trois appliqué dans les travaux de Jacquemin (1997); Thanopoulos et al. (2002). 
Conclusion et perspectives
L'étude expérimentale menée dans cet article permet de discuter le choix du taux d'élagage pour la terminologie. Différents critères d'évaluation existent tels que la précision, le rappel et bien entendu le F score qui permet de prendre en compte ces deux critères. Le défaut de ce dernier critère réside dans le choix pas toujours évident du paramètre le plus adapté pour privilégier la précision ou le rappel dans le calcul du F score . Ainsi, dans cet article, nous proposons d'utiliser les courbes ROC et l'aire sous celles-ci afin d'évaluer au mieux le choix de l'élagage à effectuer. Ce critère permettant d'estimer rigoureusement l'élagage le plus adapté n'est pas sensible au déséquilibre entre les classes. Ceci est important car selon l'élagage effectué la proportion de collocations pertinentes et non pertinentes peut être très différente.
Nos expérimentations ont alors montré sur un corpus de CVs, qu'un élagage de trois semble bien adapté. Dans nos prochains travaux, nous proposons de comparer ce résultat à partir des autres corpus étudiés. Pour cela, il sera nécessaire d'effectuer une expertise complète des collocations d'autres domaines, ce qui demandera un travail conséquent aux experts.

Introduction
La classification automatique, comme la plupart des méthodes d'analyse de données peut être considérée comme une méthode de réduction et de simplification des données. Dans le cas où les données mettent en jeu deux ensembles I et J, ce qui est le cas le plus fréquent, la classification automatique en ne faisant porter la structure recherchée que sur un seul des deux ensembles, agit de façon dissymétrique et privilégie un des deux ensembles, contrairement par exemple à l'analyse factorielle des correspondances qui obtient simultanément des résultats sur les deux ensembles ; il est alors intéressant de rechercher simultanément une partition des deux ensembles. Ce type d'approche a suscité récemment beaucoup d'intérêt dans divers domaines tels que celui des biopuces où l'objectif est de caractériser des groupes de gènes par des groupes de conditions expérimentales ou encore celui de l'analyse textuelle où l'objectif est de caractériser des classes de documents par des classes de mots. Notons que dans ce domaine, les données se présentent généralement sous forme d'un tableau de contingence où chaque cellule correspond au nombre d'occurrences d'un mot dans un document.
Par ailleurs, les modèles de mélange de lois de probabilité (McLachlan et Peel, 2000) qui supposent que l'échantillon est formé de sous-populations caractérisées chacune par une distribution de probabilité, sont des modèles très intéressants en classification permettant d'une part de donner un sens probabiliste à divers critères classiques et d'autre part de proposer de nouveaux algorithmes généralisant par exemple l'algorithme classique des k-means. Dans le cadre de la classification croisée, on a pu ainsi montrer que l'algorithme Crobin (Govaert, 1983) adapté aux données binaires peut être vu comme une version classifiante de l'algorithme block EM (Govaert et Nadif, 2005) dans un cas particulièrement simple de mélange de lois de Bernoulli.
Dans ce papier, nous proposons d'étendre ce travail à la classification croisée d'un tableau de contingence. Dans la section 2, nous définirons le modèle de mélange croisé adapté à ces données. La section 3 sera consacrée à la présentation de l'algorithme Cemcroki2 dont l'objectif est la maximisation de la vraisemblance classifiante associée au modèle précédent. Nous montrerons dans la section 4 les liens de cet algorithme avec les critères du Chi2 et de l'information mutuelle. Dans la section 5, des résultats sur des données simulées et des données réelles confirmeront l'efficacité de cet algorithme et l'intérêt de notre approche qui peut être considérée comme une approche complémentaire de l'analyse des correspondances qui s'appuie sur la même représentation des données.
Notations Dans tout ce texte, on notera x = (x ij ) le tableau de contingence construit sur les deux ensembles I et J ayant respectivement r et s éléments, n = i,j x ij la somme des éléments du tableau et
. Une partition en g classes de l'ensemble I sera notée z = (z 11 , . . . , z ik , . . . , z ng ) où z ik = 1 si i est dans la classe k et z ik = 0 sinon. Nous adoptons les mêmes notations pour la partition w en m classes de l'ensemble J. Par ailleurs, pour simplifier la présentation, les sommes et les produits portant sur I, J, z ou w seront indicés respectivement par les lettres i, j et k et sans indiquer les bornes de variation qui seront donc implicites. Ainsi, la somme i,j,k, portera sur toutes les lignes i allant de 1 à r, les colonnes j allant de 1 à s, les classes en ligne k allant de 1 à g et les classes en colonne de 1 à m.
Modèle de mélange croisé
Pour aborder le problème de la classification croisée sous l'aspect modèle de mélange, nous avons proposé (Govaert et Nadif, 2003) un modèle dont la densité s'écrit sous la forme -458 -RNTI-E-6
Pour adapter ce modèle aux tables de contingence, on suppose que chaque valeur observée x ij dans un bloc kk de la table est la réalisation d'une variable aléatoire suivant une loi de Poisson de paramètre ? i ? j ? kk où les deux premiers termes expriment les effets en ligne et en colonne et le dernier correspond à l'effet du bloc kk.
La recherche d'une partition s'appuyant sur ce modèle consiste à maximiser la vraisemblance classifiante associée à notre modèle. Pour assurer l'identifiabilité du modèle, nous avons ajouté les conditions
Le problème de classification alors posé est de trouver les partitions z et w et le paramètre du modèle maximisant le critère
Algorithme de classification croisée
Pour maximiser L c (z, w, ?), nous proposons de maximiser alternativement cette fonction en fixant w et ? puis z et ?. /w)+g(x, w, ?) où le premier correspond à une log-vraisemblance conditionnelle associée à un mélange de distributions multinomiales appliquées sur les échantillons u 1 , . . . , u r et le second terme ne dépend pas de z. On peut alors utiliser l'algorithme CEM classique (Celeux et Govaert, 1992) pour obtenir la partition z. En faisant un travail analogue pour la recherche de la partition w, on obtient finalement l'algorithme Cemcroki2 suivant :
Les expressions des estimations des paramètres du modèle associés à chaque bloc kk sont données par 
Liens avec le Chi2 et l'information mutuelle
Après l'étape de maximisation, le critère s'écrit
. est l'information mutuelle associée au couple de partitions z et w. De plus, en utilisant l'approximation 2x log x ? x 2 ? 1, on obtient aussi
On peut ainsi observer que, lorsque les proportions sont fixées, la maximisation de L c (z, w, ?) est équivalente à la maximisation de l'information mutuelle I(z, w) et approximativement équivalente à la maximisation du critère ? 2 (z, w) : la maximisation du critère du ? 2 (z, w) utilisé par exemple dans l'algorithme Croki2 (Govaert, 1983)  
Données réelles
Pour illustrer l'algorithme Cemcroki2 sur des donnés réelles, nous avons choisi les données SMART (ftp.cs.cornell.edu/pub/smart). Ces données sont définies à partir de 1033 résumés issus de la base Medline, de 1460 résumés issus de la base CISI et de 1400 résumés issus de la base CRANFIELD. En sélectionnant alors 2000 mots intéressants, Dhillon (2001) définit ainsi les données Classic3. Nous avons alors comparé les résultats obtenus par Dhillon (2001) et Dhillon et al. (2003)  
Conclusion
En utilisant un modèle de mélange croisé de distributions de Poisson, nous avons proposé l'algorithme Cemcroki2 et montré qu'il pouvait être vu comme une extension de Croki2. Ceci permet d'interpréter cet algorithme Croki2 et d'en déduire par exemple que l'utilisation du ? 2 ou de l'information mutuelle supposent implicitement l'égalité des proportions des classes. Cette approche permet alors de prendre en considération de nouvelles situations comme celles où les proportions des classes sont très différentes. Les premières expériences sur des données simulées et réelles montrent que ce nouvel algorithme apparaît clairement meilleur que Croki2 dans cette situation.

Introduction
XML est devenu un standard pour la représentation et l'échange de données. Le nombre de documents XML échangés augmente de plus en plus, et la quantité d'information accessible aujourd'hui est telle que les outils, même sophistiqués, utilisés pour rechercher l'information dans les documents ne suffisent plus. D'autres outils permettant de synthétiser ou classer de larges collections de documents sont devenus indispensables.
Dans ce contexte, de nombreux travaux proposent des méthodes de classification, supervisées ou non, pour organiser ou analyser de larges collections de documents XML. (Denoyer et al. (2003)) combinent plusieurs fonctions d'affectation (classifiers) pour classer des documents XML multimédia, (Despeyroux et al. (2005)) identifient, pour une collection homogène donnée, les types d'éléments XML les plus pertinents pour un objectif de classification. La similarité entre documents peut être définie en étendant le modèle vectoriel pour tenir compte de la structure (Doucet et Ahonen-Myka (2002), Yi et Sundaresan (2000)), ou seulement à partir de la structure d'arbre des documents, selon l'objectif visé ou l'hétérogénéité de la collection. Ainsi, la similarité structurelle peut être basée sur la distance entre arbres (Francesca et al. (2003), Nierman et Jagadish (2002), Dalamagas et al. (2004)), ou sur la détection de § ¤ <?xml version="1.0" ?> <body> <sec sno="01"> <st>Title of<it>Section 1</it>: </st> <p>This is a paragraph</p> <au>Name of the first author</au> </sec> <sec sno="02"> <st>Title 2</st> <p1>Another type of paragraph</p1> <au> Name of the second author</au> <bib>References</bib> </sec> <sec sno="03"> <st>Title 3</st> <p>One paragraph</p> <bib>References</bib> </sec> </body> ¦ ¥
FIG. 1 -Le document Test.xml et sa représentation sous forme d'arbre
sous-arbres fréquents (Termier et al. (2002)), Costa et al. (2004)). Comme ces algorithmes ont une complexité assez élevée, les premiers travaillent sur des résumés d'arbres, tandis que les seconds sont basés sur les relations directes ou indirectes entre noeuds de l'arbre. Un inconvé-nient majeur est que la fréquence d'occurrence de certaines structures, comme les éléments de listes si fréquents dans les documents XML, est perdue pour certaines analyses. Nous proposons ici une nouvelle représentation de documents XML en vue de la classification, qui permet de prendre en compte soit la structure seule, soit la structure et le contenu de ces documents. L'idée est de représenter un document par une linéarisation de son arbre XML en un ensemble de chemins générés selon certains paramètres. Les expressions de chemins vues comme de simples mots, et leur fréquence associée, permettent de se ramener au modèle vectoriel et d'appliquer une méthode simple et standard de classification. L'avantage est que l'on peut travailler sur de larges collections, tout en conservant certaines propriétés structurelles et textuelles des documents.
Représentation des documents XML
Les documents XML sont souvent représentés par des arbres étiquetés dont les étiquettes correspondent aux balises, et éventuellement au nom des attributs. Un exemple de document XML et de l'arbre correspondant est donné en figure 1. Intuitivement, un chemin est le plus court trajet entre la racine de l'arbre et un noeud et un sous-chemin est un segment de chemin. Nous proposons de représenter les documents XML par différents ensembles de sous-chemins. Cette représentation permet d'aplatir les documents tout en conservant une partie de l'information sur leur structure et donc de diminuer la complexité des traitements.
Notre motivation de prendre en compte les sous-chemins, et pas seulement les chemins, est que la ressemblance entre documents peut apparaître à différents niveaux. Pour des collections de documents très hétérogènes, les différences et ressemblances apparaitront dès les premiers nivaux (proche de la racine). Pour les collections plus homogènes, il peut être intéressant de ne conserver que les sous-chemins proches des feuilles, comme contexte au contenu textuel.
La notion de chemin que nous définissons maintenant couvre à la fois celles de chemin et de sous-chemin.
-Le chemin d'un noeud est la suite des noeuds visités à partir de la racine pour atteindre ce noeud, en parcourant l'arbre de fils en fils. Un chemin terminal est un chemin qui se termine par une feuille de l'arbre. -L'ensemble des sous-chemins d'un noeud est l'ensemble des sous-séquences du chemin de ce noeud. Un sous-chemin est dit terminal s'il se termine par une feuille, initial s'il commence par le noeud racine. -Un chemin ou un sous-chemin textuel est un chemin ou un sous-chemin prolongé par un mot du contenu textuel associé à un noeud. Lorsque le chemin est terminal, on considère le contenu textuel de la feuille. Si le chemin n'est pas terminal, on considère l'ensemble du contenu des feuilles qui sont des descendants du noeud considéré. Les chemins sont individuellement considérés comme des mots. Ce choix pose le problème d'une éventuelle dépendance entre eux. Par exemple "body.sec.st.it" et "sec.st.it" ne sont évi-dement pas totalement indépendants. Ceci est important lorsqu'on utilise des algorithmes de classification comme le K-means qui supposent que les mots sont indépendants, même si cela n'est pas complétement vérifié en pratique. Pour remédier à ce problème, nous avons séparé ces chemins en utilisant une variable par longueur (pour plus de détail, voir section 3).
Si l'option "text" est spécifiée, on récupère le contenu textuel du dernier noeud du chemin ainsi que celui de tous ses descendants, et chaque mot est attaché au chemin courant, créant autant de chemins textuels que de mots.
Les tables 1 et 2 présentent quelques ensembles de chemins, avec leur fréquence, pour l'exemple de la figure 1.
Algorithme de classification automatique
SClust est un algorithme, dit de nuées dynamiques (Celeux et al. (1989)), permettant de partitionner un ensemble de données en un nombre k (prédéfini) de classes homogènes (Verde et al. (2001)). Cette méthode est assez proche de celle de K-means, où la distance entre deux classes est basée sur la fréquence des mots du vocabulaire choisi (modalités). La principale différence réside dans la façon de recalculer les centres de gravité : une fois par itération pour -435 -RNTI-E-6 les nuées dynamiques, à chaque affectation d'objet dans la classe pour le K-means. Si l'on utilise la distance euclidienne, la distance entre deux objets x et y est calculées par la formule :
, où m est le nombre de modalités. De plus, dans SClust, les objets peuvent être décrits par plusieurs variables, et chaque variable contient un ensemble de modalités caractérisant l'objet.
Par exemple, l'objet voiture possède les modalités poids, longueur, largeur, hauteur et couleur. Il existe clairement une dépendance entre le poids et les dimensions de la voiture, ce qui nous amène à créer deux variables, la première, dimensions, contenant les modalités longueur, largeur et hauteur, la seconde, caractéristiques, contenant poids et couleur.
Dans notre cas (partition des documents), les individus sont les documents XML et les modalités sont représentées par l'ensemble des chemins regroupés par longueur où chaque longueur représente une variable. La distance entre deux documents x et y est alors remplacée par la formule : d(x,y) = p k=1 mk j=1 (x k j ? y k j ) 2 , où p est le nombre de variables et m k est le nombre de modalités pour la variable p.
La complexité d'un algorithme de classification automatique est difficile à calculer, car elle dépend de la phase d'initialisation qui est faite de manière aléatoire. Généralement, on peut dire qu'elle est linéairement dépendante de la taille des données, du nombre d'exécutions (dû à l'initialisation aléatoire) et du nombre d'itérations (fixés par l'utilisateur).
Évaluation et mesures
Pour évaluer de notre algorithme, quatre métriques sont employées. Elles permettent de comparer le résultat de la classification avec une classification existante, lorsque celle-ci existe :
-Entropie : mesure la façon dont les classes prédéfinies sont distribuées ou réparties dans chaque classe calculée. Une valeur faible de l'entropie correspond à un meilleur paritionnement (Zhao et Karypis (2001)). -Pureté : mesure la proportion de documents de la classe calculée appartenant à la classe à priori la plus fréquente, c'est à dire le pourcentage de documents de la classe calculée appartenant à la classe majoritaire (Zhao et Karypis (2001)). La pureté d'une classe calculée est égale à 1 si tous les documents sont issus de la même classe à priori. RNTI-E-6
A.-M. Vercoustre et al.
FIG. 2 -Chaîne de Traitement
-F-measure : proposée par Larsen et Aone (1999), combine les mesures de précision et de rappel pour essayer d'assigner chaque classe calculée à une classe prédéfinie de telle sorte que deux classes calculées ne peuvent pas être assignés à la même classe prédéfine. La F-measure mesure un équilibre entre la précision et le rappel. C'est la moyenne harmonique des deux valeurs précédentes. -Rand corrigé : (ou Corrected Rand) proposé par Hubert et Arabie (1985) pour comparer deux partitions. On peut utiliser cette métrique pour comparer le résultat de la classification automatique avec une classification existante, ou bien de comparer deux partitions obtenues à partir de l'algorithme de classification automatique.
Traitement des collections
Le traitement pour partitionner une collection de documents XML se compose de trois principales étapes qui sont montrées dans la Figure 2.
Le prétraitement consiste principalement à réduire le nombre de chemins générés, en essayant de diminuer successivement le nombre de balises, de mots et de chemins différents. Le filtrage sur les balises utilise une connaissance sémantique de ces balises. On peut, optionnellement :
-Remplacer les balises synonymes par un seul représentant. Par exemple, remplacer ss1, ss2, ss3 par sec pour les sections ; numeric-list, bulletlist par list pour les listes ; h1, h2, h3, h4, h1a, h2a par h pour les headers. -Ignorer la descendance de certaines balises. Par exemple math et f ormula pour les formules mathématiques dont les descendants ont une granularité trop fine pour la classification, et table pour les tables. -Supprimer les balises de mise en forme du texte (bold, italic etc.). Pour le texte, nous utilisons les méthodes classiques de réduction du vocabulaire : -Utiliser une Stopword List qui est un ensemble de mots dits vides qui n'ont aucun rôle à jouer dans la classification automatique et donc ne doivent pas être inclus dans les chemins. -Supprimer les mots de moins de 4 caractères. -Stemming : pour le processus de normalisation des termes, nous avons utilisé l'algorithme de Porter (Porter's Stemmer) (Porter (1997)). Cet algorithme supprime les suffixes des mots en anglais. La suppression des suffixes permet de réduire la taille et la complexité des données pour améliorer les performances. Le principale avantage de cet algorithme est sa rapidité de traitement de grand volume de données. Enfin nous réduisons le nombre final de chemins en utilisant les fréquences relatives TfIdf de ces chemins : les chemins trop fréquents ou trop rares sont éliminés. Dans Denoyer (2004), la collection a été utilisée pour évaluer des méthodes de classification en essayant de classer automatiquement les documents dans un des 18 journaux de la collection. Devant la mauvaise qualité des résultats, l'auteur a étiqueté la collection en 6 thèmes (Ordinateur, Graphisme, Hardware, Intelligence Artificielle, Internet et Architecture Parallèle) et 2 classes structurelles (les transactions et les autres articles). Nous avons repris ces classifications pour notre évaluation.
Expériences et résultats
Différentes expériences sont lancées sur ce corpus, en utilisant soit la structure seule, soit la structure et le contenu. Avec la structure seulement, nous cherchons à séparer les 2 classes structurelles (transactions vs articles). En utilisant la structure et le contenu, nous cherchons à retrouver soit les 6 thèmes prédéfinis, soit les 18 journaux initiaux.
Utilisation de la structure seule : transactions vs articles
Dans cette expérience, nous avons représenté les documents par leurs chemins de longueur comprise entre 3 et 5 (groupés dans trois variables), commençant à la racine. La première variable essaie donc de partitionner à partir des structures de haut niveau, tandis que la deux autres variables pourront éventuellement capter des similarités dans des parties plus détaillées du document.
La figure 3 montre, pour différents nombres de classes, les valeurs des différentes mesure de similarité avec les 2 classes existantes.
Nous remarquons que les métriques sont cohérentes entre elles et atteignent leurs valeurs optimales lorsque le nombre de classes calculées est égal à 4. Dans ce cas, la répartition des deux classes (transaction et article) à l'intérieur de chaque classe calculée est montrée dans la figure 4.
Nous constatons que les articles se répartissent sur les classes 1, 2 et 3. Les transactions sont bien regroupées dans la classe 4. Dans la classe 1, nous constatons la présence de quelques transactions dont la structure est probablement assez proche de celle des articles.
Nous avons constaté que l'utilisation des attributs dans les chemins n'améliore pas les performances du système.
FIG. 3 -Mesures d'évaluation (classification en articles et transactions) en fonction du nombre de classes (structure seule).
FIG. 4 -Répartition des classes à priori Articles et Transactions sur quatre classes calculées.
FIG. 5 -Mesures d'évaluation (classification thématique) en fonction du nb de classes (structure + contenu).
FIG. 6 -Mesures d'évaluation (classification en journaux) en fonction du nombre de classes (structure + contenu).
Utilisation de la structure et du contenu
Dans ces expériences, nous n'avons utilisé que 5% de documents de la collection d'INEX (tirés au hasard), pour limiter la taille des données et donc les temps de calculs. Nous présen-tons ici les résultats en utilisant des chemins textuels de longueur 3, commençant à la racine, faute de place pour présenter plus de résultats. Nous considérons les deux classifications : les 6 thèmes et les 18 journaux.
La figure 5 montre la valeur des différentes mesures pour la comparaison avec les 6 thèmes, en faisant varier le nombre de classes calculées.
On peut noter que les quatre métriques sont cohérentes entre elles et ceci lorsque le nombre de classes calculées varie entre 8 et 15. Le meilleur regroupement est celui en 9 classes, qui permet d'avoir des valeurs optimales pour l'ensemble des métriques.
Une deuxième façon de prendre en compte le contenu des documents XML consiste à attacher le texte aux chemins terminaux. Nous avons constaté que les résultats différent peu, selon que l'on utilise des chemins textuels commençants à partir de la racine ou des chemins textuels terminaux. Cela vient sans doute de la faible profondeur des arbres.
FIG. 7 -Mesure d'évaluation (classification en Thèmes du RA) en fonction du nombre de classes (structure seule).
FIG. 8 -Mesure d'évaluation (classification en Thèmes du RA) en fonction du nombre de classes (structure + contenu).
La figure 6 montre la valeur des différentes mesures pour la comparaison avec les 18 journeaux, en faisant varier le nombre de classes.
Les résultats obtenus sont moins bons que ceux de la classification thématique. Ceci est due à l'hétérogénéité de la structure et du contenu des documents dans chaque revue, et confirme les résultats obtenus dans Denoyer (2004).
Rapports d'activités INRIA (RA)
L'INRIA publie un rapport d'activité annuel dont l'annexe scientifique est accessible sur le web. C'est un ensemble de rapports en anglais produits par chaque équipe de recherche. Pour plus de détails sur la structure de ces rapports, se reporter à (Despeyroux et al. (2005)).
Les équipes de recherches sont regroupées en 5 thèmes : Systèmes Biologiques, Systèmes Cognitifs, Systèmes de Communication, Systèmes Numériques et Systèmes Symboliques. On peut supposer que les rapports d'activité des équipes refléteront plus ou moins ces thèmes dans la présentation de leur activité de recherche et donc que l'on puisse retrouver ces thèmes en classant les documents à partir de leur contenu. Par contre, l'hypothèse est qu'en utilisant seulement la structure des rapports, on a aucune chance d'identifier ces thèmes. En effet la structure des rapports est basée sur un modèle commun (DTD), avec peu de parties optionnelles.
Utilisation de la structure seule
La figure 7 montre que l'hypothèse précédente est vérifiée, puisqu'en utilisant la structure seule (chemins non textuels de longueur comprise entre 3 et 5), la valeur des différentes mesures est uniformément mauvaise, quelque soit le nombre de classes (en particulier le Rand Corrigé est proche de zero). Ceci est tout à fait logique car, comme indiqué en section 4, le Rand Corrigé permet de comparer deux partitions. Or, les deux partitions sont construites de manière totalement différente (l'une est basée sur la structure et l'autre sur le contenu).
Par contre, nous pouvons essayer d'analyser quantitativement le résultat d'une partition en n classes.
Le tableau 3 présente les chemins les plus fréquents pour 7 classes (par ordre d'importance). Nous nous sommes limités à ne présenter que les chemins de longueur 2 pour plus de clarté. 
TAB. 3 -Les chemins importants dans les classes
On peut interpréter ces résultats en disant, par exemple, que les équipes ayant plus de logiciels et de contrats de collaboration se trouvent dans la classe 3, et que les équipes de la classe 4 se caractérisent par la diversité des activités internationales, le nombre important des publications et des contrats de collaboration. Il faut comprendre qu'un chemin comme raweb.logiciels synthétise (dans le tableau de résultats) en fait beaucoup de chemins de longueur 3 (raweb.logiciels,subsection) correspondant à la descriptions des différents logiciels. On voit ici l'importance pour certaines analyses de conserver tous les éléments de liste et pas seulement leur existence ou non existence.
Utilisation de la structure et du contenu
En utilisant des chemins textuels, nous constatons (figure 8) une amélioration des diffé-rentes mesures de comparaisons avec les Themes. Globalement, Le meilleur regroupement est celui en 11 classes, où l'on constate une décroissance de l'entropie, ce qui signifie que les documents ne sont pas trop éparpillés sur les classes, et une augmentation du Rand corrigé, signifiant que ce regroupement se rapproche un peu de la classification en thèmes.
État de l'art
Un aspect important et caractérisant des travaux sur la classification de documents XML est la manière de représenter les documents. Doucet et Ahonen-Myka (2002) représentent un document XML en utilisant le modèle vectoriel dans lequel les modalités sont soit les balises (balises XML), soit les mots du texte (contenu), soit une combinaison des deux (balises et texte). La technique TfIdf est utilisée pour normaliser les éléments du vecteur et l'algorithme de classification utilisé est le K-means. Ceci correspond à notre approche, dans le cas où l'on fixe respectivement la longueur des chemins non textuels à 1, textuels à 1, ou textuels et non textuels à 1. Yi et Sundaresan (2000), proposent un modèle vectoriel structuré afin de tenir compte de la structure des documents XML. Les éléments du vecteur sont des termes (mots) ou un autre vecteur structuré (récursivité). Ceci revient a représenté chaque élément par son chemin à partir de la racine (noeud et mots du texte). Ceci est donc équivalent à notre représentation à partir de chemins commencant à la racine, avec les chemins textuels allant jusqu'aux feuilles. La méthode de classification probabiliste basée sur le modèle de génération de documents de Bernoulli est ensuite appliquée sur ce vecteur.
Une autre amélioration (Jianwu et Xiaoou (2002)) consiste à modéliser des éventuels liens entre les documents dans le SVM (Structured Link Vector Model), où les éléments du vecteur sont les termes, la structure et le voisinage des documents. La méthode K-means est utilisée comme algorithme de classification. Notre approche ne tient pas compte des liens entre les documents.
Dans Yoon et al. (2001), deux représentations de documents sont proposées. La première consiste à utiliser une matrice binaire (bitmap) à 2 dimensions (2D) : les documents et les ePath (le chemin d'un élément feuille à partir de la racine). Cette représentation a été étendue à 3D : les documents, les ePath, et le contenu. Les ePaths correspondent au cas où nous utilisons les chemins de longueur quelconque allant de la racines au feuilles. Toutefois la methode de classification automatique est très différente. Liu et al. (2004) traite des documents XML homogènes (même DTD). Cette approche travaille sur des arbres XML ordonnés pour extraire des informations selon trois méthodes : chemins, paires des noeuds (père-fils, frères, etc.) ou bien une combinaison des deux (hybride). Ces informations sont, ensuite, transformées en un vecteur de la forme (information, nombre d'occurrence). Pour réduire la taille du vecteur, la technique d'analyse en composante principale (ACP) est utilisée. La première représentation des documents est celle qui est la plus proche de celle que nous utilisons (chemins de longueur inférieure à une certaine valeur, commençant à un niveau donné dans l'arbre). L'idée d'utiliser des chemins commençant à un certain niveau mériterait d'être explorée, car elle peut faciliter l'interprétation des résultats.
D'autres travaux porte sur des arbres étiquetés, en utilisant plusieurs techniques, comme la détection de sous-arbres fréquents (Termier et al. (2002)), le tree-matching (Costa et al. (2004)) et les résumés d'arbres (Dalamagas et al. (2004)). Ces travaux utilisent une représentation des documents à partir des relations binaires entre noeuds qui s'éloignent de notre représentation. Les méthodes de classification utilisées sont également très différentes et plus appropriées au traitement de collections hétérogènes.
Conclusion
Nous avons proposé dans ce travail un nouveau modèle de représentation des documents XML qui prend en considération la structure et le contenu de ces documents. Ce modèle est basé sur la notion de linéarisation des chemins qui nous a permis d'aplatir les documents. Avec cette représentation, nous avons pu ensuite appliquer un algorithme de classification simple et performant, en l'occurrence SClust, et montré nos résultats obtenus sur les deux collections, INEX et les rapports d'activité.
Bien que les deux collections utilisées ne soient pas très adaptées aux problèmes de classification automatique thématique à partir de la structure, notre modèle nous a permis tout de même de séparer les documents de structure différente (le cas 'articles vs transactions' dans INEX), et d'exhiber des propriétés inconnus des documents en analysant les parties des DTD les plus utilisées. Ce modèle nous offre aussi la possibilité de reconstruire les DTDs, et ceci en analysant l'ensemble des chemins représentant chaque classe.
Comme travaux futurs, nous envisageons les points suivants : -Prendre en compte l'ordre des noeuds dans l'arbre XML (modèle d'arbres ordonné).
-Appliquer notre approche sur d'autres collections de documents XML, comme MovieDB et WIPO, et surtout sur des collections dédiées au XML Mining comme les collections du track INEX 'XML Mining', collections synthétiques éventuellement plus hétérogènes .

Introduction
Les comptes-rendus de mammographies écrits en texte libre sont difficiles à interpréter et à analyser par un programme machine. La difficulté est liée à la nature informelle de ces comptes-rendus. Trouver un processus qui permet de structurer les comptes-rendus et donner une représentation formelle de leur contenu est une tâche difficile vue la complexité du langage naturel et des connaissances médicales (Zweigenbaum, 1994).
L'objectif principal de cet article est de montrer une utilisation possible dans le domaine médical des ontologies formelles en OWL, le langage standard d'ontologie du Web (OWL, 2004). Ce travail vise à fournir un outil d'aide à l'interprétation des comptes-rendus médi-caux mammographiques et à leur classification. Il a consisté d'abord à concevoir et réaliser une ontologie regroupant tous les concepts du domaine : concepts radiologiques, concepts pathologiques, et différentes classes ACR. Les classes ont été définies à partir de la classification dite ACR (ACR, 2000) et ont été représentées dans le langage OWL DL en utilisant l'éditeur Protégé et son plugin OWL (Holger, 2004). Notre système a pour tâche, d'extraire les faits correspondant au contenu des comptes-rendus de mammographies, puis, d'inférer la classe pathologique correspondante selon la classification ACR en utilisant le raisonnement par subsumption, et d'en déduire la conduite à tenir. L'interprétation des comptes-rendus utilisée dans le cadre de ce travail est basée sur l'exploitation de la logique de description comme langage d'ontologie, comme (Golbreich, 2003) qui reposait sur C-Classic pour l'indexation et la recherche d'images, et non les graphes conceptuels comme dans le système Menelas (Zweigenbaum, 1994) ou le traitement du langage naturel comme dans le système MedLee (Nilesh et al., 1995). L'avantage de notre méthode est double. Elle permet d'une part l'utilisation du standard du Web Sémantique OWL pour la description des connaissances, donc le partage et l'interopérabilité, et d'autre part des services puissants de raisonnement basés sur la logique de description.
L'idée principale est de suivre dans chaque phrase du compte-rendu radiologique donné en texte libre la trace des concepts, instances, et propriétés de l'ontologie. Si l'extracteur localise dans une phrase bien identifiée des concepts, instances ou propriétés, il va essayer de déterminer les relations entre eux, à partir des modèles de classes fournis par l'ontologie. Ensuite, un raisonneur pour la logique de description OWL DL identifie pour l'individu associé au compte-rendu la (les) classes possibles et l'attitude à tenir qui en résulte.
Le reste de l'article est organisé comme suit : dans la deuxième section, nous décrivons la Classification ACR introduite par les systèmes BIRADS et nous motivons notre besoin de construire une ontologie radiologique dans le format OWL ( §2). Ensuite nous décrivons l'ontologie ACR en OWL ( §3). La dernière section présente un aperçu de l'architecture globale du système et des fonctionnalités attendues ( §4).
Les systèmes BIRADS et la classification ACR
La mammographie permet d'établir un diagnostic et de donner des indications pronostiques concernant des lésions observées sur des images mammographiques. Son rôle principal est la détection précoce du cancer du sein chez des patientes asymptomatiques (ACR, 2000).
Les règles qui permettent d'établir une conclusion diagnostique et/ou pronostique à partir de caractéristiques morphologiques observées dans les images mammographies sont publiées dans le cadre de systèmes de classification. Les systèmes BIRADS de l'American College of Radiology (ACR, 2000) standardisent une classification des images mammographiques en six catégories en fonction du degré de suspicion de leur caractère pathologique. Ils ont pour objectif de standardiser le compte-rendu grâce à une structure claire et un lexique bien défini, afin d'éviter les erreurs d'interprétation. De ce fait, construire une ontologie partageable entre pathologistes et radiologues dispersés géographiquement est crucial. L'ontologie doit regrouper des concepts utilisés par les radiologues dans leur description physique des signes radiologiques issus de deux types d'examens complémentaires : l'échographie et la mammographie ainsi que leur morphologie (forme, taille, localisation, nombre, contour, densité, etc.), des concepts décrivant les anomalies pathologiques mammaires (Kystes, carcinomes, etc.) et les différentes classes données par la classification ACR décrivant la catégorie et l'attitude à tenir.
Dans une première étape, une construction manuelle ou supervisée, élaborée suite à une analyse approfondie de la classification normalisée ACR, une consultation d'une base de comptes-rendus mammographiques et des documents biomédicaux, combinée à des interviews d'experts, a permis d'identifier les principales classes du domaine impliquées dans la classification ACR et de définir une taxonomie des concepts du domaine présentée dans la figure 1. La deuxième étape a été de définir la représentation logique en OWL DL des diffé-rentes classes ACR et autres concepts. (a) Les classes primitives : 6 Classes 'primitives' ont été définies pour les concepts racines, au premier niveau de la figure 1: Anomalie, Attitude, Caractéristique Physique, Lesion, Signe, Anatomie. Notre premier travail a été d'identifier ces concepts et de trouver une taxonomie (Noy, 2001) assez puissante des signes de diagnostic en se basant sur les caracté-ristiques physiques e.g., morphologie, puis de donner les formules logiques combinant les signes radiologiques et les lésions correspondantes pour exprimer les conditions nécessaires et suffisantes ou nécessaires (Golbreich et al 2004) pour chaque Classe ACR.
(b) Les proprietés : puis nous avons défini les propriétés. Un concept peut être relié à d'autres concepts de l'ontologie par des propriétés OWL comme la forme, le bord, le signe, représentées par des objectProperty en OWL e.g., hasForm, hasBord, hasSign, etc. 
FIG. 2 -Conditions Nécessaires Et/Ou suffisantes De Anomalie1
Les autres anomalies sont représentées de la même manière, et la classe ACR2 est définie comme l'union des sous-classes d'anomalies de 1 à 8 :
L'axiome suivant exprime que les anomalies de la classe ACR2 impliquent ni surveillance ni examen complémentaire : ACR2 ? ¬(Survellance ? ExamenComplementaire) La vérification de la consistance et la classification de l'ontologie sont réalisées automatiquement à l'aide de Racer (Haarslev et al., 2001) dans l'environnement Protégé OWL.
L'application
L'idée principale de notre application réside dans l'extraction d'instances et de leurs propriétés à partir des comptes-rendus en utilisant des techniques de traitement du langage naturel comme dans (Ricky et al., 2001). Contrairement à cette approche nous exploitons une représentation formelle extraite des comptes-rendus et le raisonnement ontologique par Racer pour identifier les classes ACR des comptes-rendus. L'architecture globale et ses diffé-rents composants sont décrits figure3 : L'analyse structurelle : l'analyser structurelle a pour rôle de reconnaître la structure du compte-rendu : entête, dates, informations patients, contenu, etc. 
Conclusion
Dans le présent travail, nous avons conçu un système permettant la classification des comptes-rendus mammographiques en utilisant une ontologie formelle pour classer les images radiologiques-mammaires à partir des signes radiologiques observés et d'une Classification normalisée dite ACR. L'ontologie est représentée en OWL DL à l'aide de Protégé OWL. Un raisonneur comme Racer sera utilisé pour identifier les classes possibles d'appartenance pour les images présentant des signes d'anomalies. L'application est déve-loppée en Java en utilisant plusieurs API. Les analyseurs lexical, syntaxique et sémantique

Introduction
La classification, ou clustering (Jain et al., 1999), consiste à associer une classe à chaque élément d'un ensemble, les éléments similaires devant être regroupés dans une classe en n'utilisant que la similarité (ou distance) entre deux éléments ou groupes d'éléments. Le formalisme attributs-valeurs ne permettant pas de représenter les domaines complexes, l'apprentissage en logique du premier ordre, ou Programmation Logique Inductive (PLI), a attiré une attention croissante. Le language utilisé en PLI, DATALOG, est un formalisme relationnel ne permettant pas les fonctions, et dont le test de couverture, la ?-subsomption, est une restriction décidable mais NP-difficile de l'implication logique. Cet article présente une méthode permettant l'utilisation d'algorithmes de clustering sur des données relationnelles, en recherchant préliminaire-ment tous les motifs relationnels existant et en les utilisant pour définir une distance entre des clauses en DATALOG.
Présentation de l'algorithme
L'algorithme proposé consiste en trois étapes : la recherche de tous les motifs relationnels de la base, l'élimination des motifs inintéressants et le clustering des clauses DATALOG, en utilisant les motifs pour calculer la distance entre les exemples. La recherche des motifs relationnels est effectuée par JIMI (Maloberti et Suzuki (2003)) qui est une version relationnelle d'un algorithme de recherche en largeur d'itemset fréquents. Chaque exemple est tranformé en un vecteur booléen dont les valeurs correspondent au test de ?-subsomption 1 des motifs contre cet exemple, ces vecteurs permettant d'utiliser les distances existantes. Différents paramètres peuvent être utilisés : différents poids sur les motifs durant le calcul de la distance, tels que la taille des motifs ou l'inverse de la fréquence, utilisation des n premiers niveaux trouvés par JIMI plutôt que tous les niveaux, utilisation d'une partie des motifs (tous les motifs maximaux, i.e. fermés, ou les motifs minimaux).
Notre méthode a été testée sur 2 ensembles de données réelles avec un algorithme de clustering hiérarchique ascendant et une distance euclidienne. Le premier test concerne la détection d'accès hostiles sur le site web "www.slab.dnj.ynu.ac.jp". Les données, dont des résultats ont déjà été publiés dans Narahashi et Suzuki (2003)   Hirose et Suzuki (2005) 0.719 avec 2 clusters. Ce problème n'étant pas relationnel, les 2 premiers niveaux ont les meilleurs résultats, l'utilisation de plus de niveaux n'a conduit qu'à la création de plus de clusters. Le second ensemble de données, décrit dans King et al. (1995), concerne la détection de capacité à provoquer des mutations et représente 230 molécules, dont 138 positives et 92 négatives. Les résultats ont été médiocres, une précision de 0.51, car seule la description des atomes et de leurs relations a été utilisée, ce qui est insuffisant pour obtenir des motifs discriminants.
Conclusion et perspectives
Nous avons proposé une nouvelle méthode permettant le clustering de données relationnelles et nous avons utilisé ce système sur deux ensembles de données. Les résultats prélimi-naires montrent que ce système peut égaler les autres algorithmes sur des données non relationnelles, l'expérimentation sur des données relationnelles n'ayant pas permis de conclure. Parmi les perspectives, l'utilisation d'un algorithme de clustering pouvant gérer de grandes dimensions, tel que le subspace clustering, serait intéressante car le grand nombre de motifs rend les distances très instables mathématiquement.
Summary
This paper presents an algorithm for clustering of relational data in DATALOG formalism which searches all relational patterns in the base, then transforms each example in a boolean vector corresponding to the results of its covering tests against the patterns.

Introduction
L'analyse exploratoire d'un tableau de données, que ce soit un tableau classique croisant unités statistiques et caractères quantitatifs, ou un tableau de contingence croisant les modalités de deux caractères qualitatifs, est généralement réalisée par les quatre étapes de la procé-dure suivante :
1. Analyse factorielle exploratoire : selon le type de tableau, il s'agit d'une Analyse en Composantes Principales (ACP) ou une Analyse des Correspondances (AFC) ; 2. classification des lignes, à savoir des individus ou des modalités en ligne ; 3. interprétation des classes obtenues à l'aide du comportement des caractères originaux dans chaque classe ; 4. Étude des liaisons entre classes et axes factoriels. L'originalité de l'approche proposée dans cet article est d'unifier, dans une même mé-thode, l'analyse factorielle du tableau et les classifications des lignes et des colonnes. En effet, les plans factoriels obtenus sont directement associés aux noeuds des hiérarchies construites. Ce qui permet d'obtenir une interprétation conjointe des noeuds et des axes factoriels facilitant la synthèse des résultats. Les approches classiques résumées par les quatre étapes décrites ci-dessus ne permettent pas, dans bien des cas, d'obtenir rapidement cette vue synthétique de l'ensemble des résultats En effet, les relations existant entre noeuds et axes factoriels sont souvent difficiles à expliquer dans l'approche classique, surtout pour un utilisateur peu exercé.
Le problème de la classification des variables n'a pas été beaucoup été traité en littéra-ture, le seul exemple couramment utilisé par les non-spécialistes étant la procédure VAR-CLUS intégrée dans SAS (1999). Pourtant, Lerman (1981) a proposé un véritable système de méthodes de classification des caractères, intégré dans une méthodologie générale de classification.
Les méthodes qu'on propose ici, développées par Denimal (1997Denimal ( , 2001), se basent, comme d'ailleurs celles proposées par Qannari et al. (1999), sur des ACP ou, dans le cas original d'un tableau de contingence, sur des AFC. Nous nous distinguons, cependant, par le fait que l'on propose des méthodes hiérarchiques ascendantes, ce qui permet de retarder le choix du nombre de classes de la partition à retenir après la construction de la hiérarchie et l'interprétation des résultats. Ainsi, le nombre de classes à retenir est déterminé sur la base des résultats et non pas à priori. En outre, notre approche est en accord avec celle de Lerman pour laquelle chaque classe est également identifiée avec un facteur.
Les méthodes
Dans les deux cas on suppose les tableaux déjà normalisés, selon deux principes diffé-rents suivant le type de tableau. Au départ de l'algorithme, chaque colonne est vue comme un groupe singleton dont elle est la variable représentative. Ensuite la procédure itérative pour construire la hiérarchie est la suivante : à chaque pas 1. On effectue une analyse factorielle exploratoire sur tout couple de colonnes ; 2. On choisit comme noeud de la hiérarchie à construire le couple de classes dont la deuxième valeur-propre de l'analyse factorielle correspondante est minimum ; 3. Ce noeud établi, on choisit comme variable représentative du groupe ainsi formé le premier vecteur-propre de l'analyse factorielle correspondante. On vient de remarquer que la différence entre les deux analyses consiste en la différente analyse factorielle utilisée : pour le tableau quantitatif on utilise une ACP non normée, à savoir basée sur la matrice de variance-covariance des deux caractères ; pour le tableau de contingence on utilise une AFC, mais dans ce cas on n'obtiendrait qu'un seul facteur. Donc il est nécessaire d'introduire une astuce, consistant à associer à toute colonne j une colonne j* complément de j par rapport à la colonne marginale, à savoir telle que, pour tout i, n ij * = n i . -n ij . Cependant, pour tout couple de colonnes j, k, on effectue l'AFC du tableau de contingence croisant toutes les lignes avec les quatre colonnes {j, k, j*, k*}. Comme j + k = j* + k*, l'AFC ne fournit que deux facteurs non nuls. Une fois effectuée l'AFC, on ajoute les nouvelles colonnes j n1 et j n1 * telles que
où F 1 (i) est la coordonnée de i sur le premier facteur, f i est la fréquence la ligne i, et k (.,j n1 ) et k (.,j* n1 ) sont quantités définies par des égalités supplémentaires dépendant du signe des coordonnées de j, k, j*, et k* sur le premier facteur (Denimal., 2000).
On a les propriétés suivantes : -Le critère d'agrégation est de type Ward, les deuxièmes valeurs-propres, utilisés comme indices de la hiérarchie forment une séquence non décroissante ; -l'inertie totale du tableau est la somme des deuxièmes valeurs-propres des noeuds et de la première du noeud le plus haut ; -à tout noeud un plan factoriel est associé, où sont représentées les colonnes des groupes fusionnés ainsi que l'ensemble des lignes ; -dans ces plans, le premier vecteur-propre peut s'interpréter comme un compromis entre les 2 classes regroupées et le deuxième vecteur propre comme traduisant leurs différences ; -chaque groupe s'interprète comme un dipôle, mettant en opposition des caractères ou des modalités : ceci permet une interprétation des groupes plus imagée.
Les exemples d'application
Données quantitatives
Dans cet exemple on a appliqué la classification hiérarchique factorielle sur un tableau de données saisies dans une communauté végétale typique des pâturages de Campos dans le Brésil du Sud (Pillar et al., 1992).
Il s'agit de 60 relevés carrés de 0.5 x 0.5 m. alignés le long de quatre gradients, allant du haut en bas, dont on a la composition des 60 espèces présentes et 21 variables décrivant la structure du sol. On s'occupe ici des variables quantitatives décrivant la structure du sol. Dans le Tableau 1 on voit le dendrogramme de la hiérarchie formée par les variables et dans la Figure 1 le plan factoriel associé à l'avant-dernier noeud de la hiérarchie. 
Fréquences
Dans cet exemple, on étude le contenu iconographique des images de sceaux Mésopota-miens de la période Uruk-Jamdat Masr. Il s'agit de petits cylindres de pierre dont les images gravées viennent imprimées, lorsqu'on les fait rouler, sur la craie ou sur autre support.
Pour le codage des images, on les a décrites à l'aide d'un texte formalisé qui a été ensuite traité par des analyses textuelles (telle que l'Analyse des Correspondances Textuelles, Lebart et Salem, 1994;Camiz et Rova, 2001). Ici le tableau de contingence, croisant 834 images avec 169 formes textuelles, a été soumis à la classification hiérarchique factorielle basée sur l'AFC. Dans le Tableau 2 on voit la succession des deuxièmes valeurs-propres de la hiérar-chie, ainsi que l'inertie synthétisée par les variables représentatives des groupes formés. .37306 *********************************** 336 334 335 0.42892 4.07197 90.44504 ******************************************* 337 336 303 0.45188 4.28989 94.73494 ********************************************* Total var. representative 0.55460 5.26505 100.00000
TAB. 2 -La progression des indices de la hiérarchie des formes lexicales des sceaux.
Dans la Figure 2 on voit le plan factoriel associé à l'avant-dernier noeud, avec les formes plus intéressantes des deux groupes, ainsi que les images des sceaux les plus significatifs de ce type.
FIG. 2 -Les formes les plus significatives associées aux variables représentatives des groupes 334 et 335 se fusionnant dans le noeud 336 et les images des sceaux correspondants les plus typiques.
Conclusion
L'objectif des méthodes proposées dans cette nouvelle approche est d'obtenir une information synthétique de manière plus aisée et plus rapide pour l'utilisateur. En particulier on a remarqué que le plan factoriel associé au dernier noeud montre très souvent une structure très proche de celle issue d'une ACP où d'une AFC. En plus, si l'objectif de l'analyse porte sur l'identification des caractères les plus intéressants pour des études ultérieures, leur sélection, par exemple à l'aide de leur corrélation avec les variables représentatives des groupes, peut s'avérer fort intéressante. De même, si on cherche des règles d'extraction de données, on peut associer aux analyses une segmentation des unités pas à pas (Camiz et Denimal, 2003).
Il reste à comparer ces méthodes avec d'autres méthodes de classification, basées sur d'autres critères, en particulier avec ces de Lerman (1981) où l'idée des facteurs découle aussi des classifications.
Des généralisations semblent également possibles, soit relativement à d'autres types de données (plusieurs caractères qualitatifs, tableaux multiples), soit dans d'autres domaines, tels que l'analyse discriminante, les arbres de décision, etc.

Introduction et objectifs
Pour rendre compte avec exactitude des évolutions temporelles, cruciales dans beaucoup de domaines d'application (ex. : veille d'information), il est nécessaire à notre avis : 1) de partir d'une base stable, c'est-à-dire d'une classification :
-indépendante de l'ordre de présentation des données (exigence n°1), -indépendante des conditions initiales, que ce soit d'un choix de « graines de classes » arbitraires ou dépendantes des données (exigence n°2), -impliquant un minimum de paramètres, un seul si possible, pour réduire l'espace des choix et tendre vers un maximum de vérifiabilité et de reproductibilité (exigence n°3). 2) d'ajouter aux contraintes d'une bonne classification celle de l'incrémentalité (exigence N°4), afin de saisir les évolutions au fil de l'eau : rectifications de frontières entre classes, apparition de nouvelles classes, voire de « signaux faibles »... Pour nous, il y a incrémentalité véritable si le résultat de la classification est indépendant de l'ordre des données présentées antérieurement (exigence N°5), tout en découlant des données antérieures, par un historique pouvant faire l'objet d'interprétations.
Notre démarche a été de concevoir une méthode où la contrainte d'incrémentalité participer d'un tout cohérent, en vue d'aboutir à tout instant à une classification qui ait du sens, et dont la différence de représentation par rapport à l'instant précédent ne provient que des effets du temps, et non du mélange de ceux-ci avec la variabilité propre de l'algorithme, à la différence des principales méthodes de classification non supervisée.
Etat de l'art
Les méthodes hiérarchiques, divisives ou agglomératives, souvent conviviales et efficaces, satisfont à nos exigences 1 à 3 d'unicité des résultats. Mais au regard de la qualité des partitions obtenues à un niveau donné de l'arbre, un consensus existe pour leur préférer les méthodes à centres mobiles : un modèle hiérarchique de partitions emboîtées impose des déformations à une réalité ayant toutes chances de se rapprocher d'une organisation en treillis de partitions (par ex. treillis de Galois, pour des descripteurs binaires).
Les méthodes procédant par agrégation autour de centres mobiles, comme les K-means et leurs nombreuses variantes, font partie d'une famille basée sur l'optimisation d'un indicateur numérique global de qualité de la partition, dans laquelle prennent place les méthodes utilisant la procédure EM (Expectation Maximization) -cf. Buntine (2002). Ce problème d'optimisation étant NP-difficile, on ne sait que les faire converger vers un optimum local qui dépend de leur initialisation (par ex. positions initiales des centres choisies arbitrairement, ou en fonction des données), voire de l'ordre des données. Ce qui les disqualifie vis-à-vis de notre exigence N°2 en théorie comme en pratique. Un bon nombre de variantes incré-mentales de ces méthodes ont été proposées, dont on trouvera une revue partielle dans Gaber et al. (2005). Beaucoup sont issues de l'action DARPA « Topic Detection and Tracking » Mais toutes se concentrent sur l'efficacité informatique pour traiter des flux de dizaines ou centaines de milliers de dépêches d'agences, ou autres documents. C'est aussi dans ce cadre d'efficacité que Simovici et al. (2005) proposent un algorithme glouton pour optimiser un critère de qualité de partition original propre aux descriptions par variables nominales.
A notre connaissance, seules les méthodes basées sur la densité satisfont à notre exigence d'unicité des résultats, hors de portée des méthodes ci-dessus. Elles s'appuient sur la notion de densité d'un nuage de points, locale par définition : étant donné 1) un nuage de points multidimensionnel, 2) une définition de la densité en chaque point de cet espace, 3) la valeur du paramètre de localité de cette fonction densité (son « rayon »), le paysage de densité qui en découle est unique et parfaitement défini. L'énumération de l'ensemble de ses pics -ou éventuels plateaux -représente un optimum absolu ; ces pics balisent des noyaux homogènes de points ; dans les zones intermédiaires, on peut définir de diverses façons des zones d'influence des noyaux. Trémolières (1994) a proposé un algorithme général, dit de percolation, indépendant de la définition de la densité et du type de données, pour délimiter rigoureusement les noyaux, les points-frontière ambivalents et les points atypiques. Il procède par baisse progressive du niveau de densité depuis le point le plus dense, et diffusion autour des noyaux qui apparaissent successivement. D'autres travaux retrouvent le même principe de repérage des noyaux denses, le plus souvent avec une définition spécifique de la densité, et d'extensions de diverses sortes à partir des noyaux : Moody (2001), Guénoche (2004), Batagelj (2002)... A noter que ces méthodes peuvent se traduire en termes de partitionnement de graphe : définir une densité implique d'avoir fixé des relations de voisinage, donc un graphe. DBSCAN d 'Ester et al. (1996) utilise une définition de la densité au moyen de deux paramè-tres, dont l'un fixe le seuil à partir duquel les noyaux sont constitués et étendus. Ertöz et al. (2003) utilisent la notion de K plus proches voisins, qui permet de définir un « rayon adaptatif » ; plusieurs possibilités existent alors pour définir une densité au sein de ce voisinage.
Ici aussi des méthodes incrémentales ont été proposées : ainsi une version incrémentale de DBSCAN d 'Ester et al. (1998), ou la méthode de Gao et al. (2005), pour des descripteurs quantitatifs (âge, revenus, …). Mais c'est dans le domaine des protocoles auto-organisateurs de réseaux de communication radio dits « ad hoc » que le thème de l'incrémentalité pour le partitionnement dynamique de graphes évolutifs a été principalement abordé -ex. : Mitton, Fleury (2003) -avec des objectifs d'application assez différents des nôtres : les voisinages se modifient à chaque pas de temps, sans nécessairement comporter d'entrées ou sorties du réseau, l'optimalité est moins recherchée qu'une stabilité relative de la composition des classes (strictes) et de l'identité des chefs de classe (clusterheads). Le principe-clé est celui d'un algorithme intégralement distribué, pour lequel la connaissance à l'instant t par chaque unité à classer de ses voisins et des voisins de ses voisins (2-voisinage) est suffisante.
Algorithme incrémental GERMEN : modifications locales des voisinages, des densités et des classes
Nous avons publié et expérimenté sur des données documentaires [cf. Lelu, François (2003)] un algorithme de percolation modifié, aboutissant à extraire 1) les points isolés, 2) les noyaux stricts exclusifs d'une classe, 3) les points ambivalents appartenant à plusieurs classes, tous ces points se projetant à des hauteurs diverses sur chaque axe de classe. Nous y renvoyons en ce qui concerne la transformation préalable des données.
A la différence de cet algorithme, celui que nous présentons ici est incrémental. Dans cette optique, nous nous donnons l'état d'un graphe de similarité des K plus proches voisins à l'instant t, valué et orienté, dont les noeuds ont été caractérisés par leur densité, ainsi que par leur « couleur », c'est-à-dire par leur rattachement à un éventuel noeud « chef de classe », dont le numéro constitue l'étiquette. L'arrivée d'un nouveau noeud va perturber localement cet état : un certain nombre de noeuds dans le voisinage direct ou indirect du nouvel arrivant vont voir leur densité changer -ce changement du paysage de densité induisant à son tour un réajustement des zones d'influence des chefs de classe, voire des changements de chefs de classe, ou l'apparition de nouvelles classes (cf. plus bas le pseudocode détaillé).
Plusieurs règles d'héritage du numéro du (ou des) chef(s) de classe sont possibles. Mais dans tous les cas nous avons affaire à la mise à jour des « couleurs » d'un paysage de densité sous l'effet 1) de l'arrivée d'un nouveau point (changement structurel), 2) d'un changement localisé des densités (changement quantitatif). Si la mise à jour de la couleur d'un point ne dépend que de la couleur de ses voisins « surplombants », de densité supérieure, alors à paysage de densité donné et à graphe de voisinage donné on obtiendra un coloriage unique : l'attribution des classes sera elle aussi indépendante de l'ordre des données On construit progressivement et en la mettant constamment à jour une structure de données comportant pour chaque noeud la liste de ses &-et 2-voisins, sa densité, et son (ou ses) numéros de chef(s) de classe. A chaque arrivée d'un vecteur (noeud) nouveau, on calcule les changements de densité induits dans son 2-voisinage (il ne peut pas y en avoir ailleurs du fait de notre définition de la densité comme somme des liens présents dans l'ensemble de chaque 1-voisinage), puis les changements de chef(s) de classe induits. Pour ce faire, on met à jour et on parcourt itérativement la liste des noeuds susceptibles de changer de classe, compte tenu de la règle choisie pour l'extension des classes. Au pire cette liste peut comporter tous les documents antérieurs, mais elle ne peut que se vider (en pratique elle se stabilise autour de Valeurs ex-aequo de similarité et densité : leur prise en compte est indispensable pour assurer l'indépendance par rapport à l'ordre des données. Les K plus proches voisins d'un noeud peuvent donc être en nombre supérieur à K… En cas de voisins de même densité et dominant leurs voisinages, le numéro du noeud le plus ancien est attribué.
Illustration du contenu des classes par leurs descripteurs saillants.
Nous définissons comme suit la contribution relative C i (k) du descripteur i à la classe k, (dont on nomme liens(k) l'ensemble des liens internes) : si x it est le nombre d'occurrences du descripteur i dans le document N°t, de somme x .t pour l'ensemble de ces descripteurs, si c i (t,t') est la contribution pondérée du descripteur i au lien entre les documents t et t', de densités respectives d(t) et d(t'), alors :
Complexité informatique
Dans son implantation actuelle, l'introduction incrémentale du n ème document coûte de l'ordre de O(n) en temps de calcul, donc l'algorithme complet est en O(n²). Des optimisations sont possibles dans le calcul des similarités et des K plus proches voisins (vecteurs-
A. Lelu données très creux) ; mais c'est surtout en l'exécutant en mode distribué, ce à quoi il est adapté par construction, qu'il pourrait être rendu peu dépendant de la taille des données.
Exemple d'application, évaluation.
Ci-dessous un exemple de thème obtenu sur une base textuelle de test (193 résumés de la collection Gallimard-Jeunesse, vocabulaire de 888 mots retenus, K=3 plus proches voisins), parmi 28 classes obtenues : à t=193 ; environ 45% des documents appartiennent à des noyaux, 5% sont des isolés, et 50% des ambivalents. Pour évaluer la qualité des résultats, les corpus de test des campagnes TDT citées plus haut sont inadéquats pour deux raisons : 1) le nombre de documents va de 16 000 à plusieurs centaines de milliers, ce qui est pour l'instant un ou deux ordres de grandeur au dessus des possibilités de la version actuelle, 2) ce qui est évalué n'est pas « pur », c'est une chaîne indexation + algorithmes utilisant cette indexation. C'est pourquoi nous avons entrepris -cf. Lelu et al. (2005)  
FIG. 1 -Exemple de thème extrait par GERMEN
Conclusion et perspectives
Nous avons présenté un algorithme de classification non supervisée répondant aux exigences d'un suivi dynamique rigoureux d'un flux de documents : il est à la fois optimal, au sens de la description exhaustive d'un paysage de densité adaptative, et incrémental. Son efficacité actuelle est suffisante pour traiter un flux de plusieurs milliers de documents. Son passage à l'échelle est possible en mode distribué, par construction.. Mais il reste surtout à explorer en grandeur réelle et évaluer les diverses applications et usages nouveaux possibles, avec les problèmes qui vont avec : efficacité informatique, ergonomie de représentation, et

Introduction
Dans le cadre d'un processus complet de fouille de textes (Kodratoff et al., 2003, Amrani et al., 2004a, nous nous sommes intéressés à l'étiquetage morphosyntaxique des corpus de spécialité. L'étiquetage morphosyntaxique consiste à affecter à chaque mot dans la phrase son étiquette morphosyntaxique, en prenant en considération le contexte et la morphologie de ce mot. L'étiquette morphosyntaxique est composée de la catégorie syntaxique du mot (nom commun, nom propre, adjectif, etc.) et souvent comporte des informations morphologiques (genre, nombre, personne, etc.). Les outils informatiques nécessaires à l'opération d'étiquetage sont appelés « étiqueteurs ».
Un problème se pose lorsque les étiquettes des mots sont ambiguës. Par exemple, le mot functions peut être un nom au pluriel ('biological functions are…') ou bien un verbe au singulier ('this gene functions as…'). Le problème à résoudre est celui de trouver l'étiquette correcte selon le contexte. La correction de ces ambiguïtés est une étape importante pour obtenir un corpus de spécialité « parfaitement » étiqueté. Pour lever ces ambiguïtés et donc diminuer le nombre de fautes d'étiquetage, nous proposons une approche interactive et itéra-tive appelée Induction Progressive. Cette approche est une combinaison d'apprentissage automatique, de règles rédigées par l'expert et de corrections manuelles. L'induction pro-gressive nous a permis d'obtenir un corpus de biologie moléculaire « correctement » étique-té. Nous avons alors utilisé le corpus obtenu pour entraîner quatre étiqueteurs morphosyntaxiques supervisés, puis nous avons effectué une étude comparative.
Étiquetage morphosyntaxique 2.1 Les approches d'étiquetage morphosyntaxique
Il y a deux approches principales pour l'étiquetage morphosyntaxique : l'approche inductive et l'approche linguistique.
L'approche inductive nécessite la disponibilité d'un grand corpus annoté. L'annotation de corpus est tout apport d'information aux textes bruts. L'information requise ici est l'étiquette morphosyntaxique correcte de chaque mot. Parmi les étiqueteurs inductifs, nous pouvons citer : l'étiqueteur à base de transformation (Brill, 1995), l'étiqueteur à base de « Séparateurs à vaste marge (SVM)» (Giménez et Màrquez 2003) et les étiqueteurs probabilistes (Ratnaparkhi, 1996, Toutanova et al., 2003. Des étiqueteurs plus élaborés ont été développés comme les étiqueteurs basés sur la combinaison de plusieurs étiqueteurs individuels, permettant ainsi de pallier les déficiences de chacun des systèmes pris séparément (Brill et Wu 1998, Halteren et al., 2001. Les résultats publiés de ses étiqueteurs appliqués au corpus classique 'WSJ' sont de l'ordre de 96-97%.
L'approche linguistique, quant à elle, consiste à coder manuellement les connaissances linguistiques sous forme de règles. Les règles acquises sont ensuite utilisées pour l'étiquetage de nouveaux textes. L'un des travaux les plus important de cette approche est le développe-ment d'une grammaire de contraintes (Karlsson et al., 1995) et son application à l'étiquetage morphosyntaxique (Voutilainen,95). Cet étiqueteur peut être considéré comme le meilleur étiqueteur existant. En effet, il atteint une précision supérieure à 99% d'étiquettes correctes.
Afin de bénéficier des avantages des deux approches, plusieurs chercheurs ont combiné les étiqueteurs inductifs et les règles linguistiques (Tapanainen et Voutilainen 1994, Samuelson et Voutilainen 1997. Il existe d'autres systèmes d'étiquetage qui utilisent de petits corpus annotés pour accélérer l'annotation d'un corpus plus grand ; ces systèmes combinent l'utilisation d'un étiqueteur appris sur un petit corpus et l'intervention d'un humain via une interface interactive. Nous pouvons citer par exemple, les systèmes ANNOTATE (Plaehn et al., 2000) et KCAT (Won-Ho et al., 2000) qui sont basés sur des étiqueteurs statistiques.
Problématique de l'étiquetage morphosyntaxique
Quelque soit le système sur lequel ils sont basés, les étiqueteurs actuels atteignent des performances très satisfaisantes mais il est difficile de dépasser la précision de 96-97%. Plusieurs chercheurs justifient cette difficulté par les incohérences dans le corpus d'apprentissage (Ratnaparkhi, 1996, Toutanova et al., 2003. Les corpus sont annotés manuellement, ils peuvent donc contenir des erreurs. Par conséquent, l'amélioration de la qualité des corpus et la correction des erreurs ont une importance capitale.
De plus, les bons résultats des étiqueteurs supervisés s'expliquent par le fait que les travaux en question se situent dans le domaine de l'apprentissage supervisé où le corpus de test est de nature similaire au corpus d'apprentissage. Un véritable problème apparaît lorsque nous voulons traiter des corpus de spécialité pour lesquels nous n'avons pas de corpus anno-tés. L'acquisition d'un tel corpus est coûteuse et elle constitue le goulet d'étranglement pour construire un étiqueteur pour une nouvelle application ou un nouveau domaine.
La plupart des étiqueteurs utilisent des informations de nature essentiellement locale (une séquence de deux ou trois mots consécutifs). Par conséquent, ces étiqueteurs butent sur les ambiguïtés qui demandent la prise en considération d'un contexte large. Par exemple : l'ambiguïté relatif/conjonction pour que. Bien que l'approche linguistique engendre des modèles de très bonne qualité et traite efficacement les ambiguïtés difficiles, elle est coûteuse et laborieuse. Par exemple, le développement de l'étiqueteur ENGCG (Voutilainen, 95) a nécessité plusieurs années. Cependant, comme pour les étiqueteurs supervisés, les performances des étiqueteurs linguistiques se détériorent lorsqu'ils sont appliqués à de nouveaux corpus.
Induction progressive
La correction des ambiguïtés difficiles est une étape importante pour obtenir un corpus de spécialité parfaitement étiqueté. Pour corriger ces ambiguïtés et diminuer le nombre de fautes d'étiquetage, nous utilisons une approche itérative appelée Induction Progressive. L'induction progressive est une combinaison d'apprentissage automatique, de règles rédigées par l'expert et de corrections manuelles qui se combinent itérativement afin d'obtenir une amélioration de l'étiquetage tout en restreignant les actions de l'expert à la résolution de problèmes de plus en plus délicats. Le principe de l'induction progressive est le suivant : en utilisant le langage CorTag (détaillé dans la section suivante), l'expert écrit une règle (ou plusieurs règles) pour corriger une ambiguïté spécifique. Les règles de l'expert sont ensuite appliquées au corpus Corp 0 et engendrent un corpus CorpRegExp 0 . Un algorithme d'apprentissage de règles est ensuite utilisé pour apprendre la modification engendrée par les règles de l'expert. Les règles apprises sont aussi appliquées au corpus Corp 0 et engendrent un corpus CorpRegInd 0 . En utilisant une nouvelle version du logiciel interactif ETIQ (Amrani et al., 2004b, Amrani et al., 2005a, les différences entre les deux corpus (CorpRegExp 0 et CorpRegInd 0 ) sont alors visualisées pour faciliter leur analyse. Les points de désaccords sont souvent des cas particulièrement difficiles à étiqueter. Leur visualisation permet de :
-détecter les erreurs produites par les règles de l'expert.
-mettre à jour les règles de l'expert si elles se trompent. Pour ce faire, les règles induites ayant trouvé l'étiquette correcte sont une indication précieuse.
-confirmer ou corriger les étiquettes obtenues par les règles de l'expert, ainsi nous obtenons une base d'étiquettes sûres. Cette base servira pour améliorer progressivement la qualité des règles induites. Le langage offre la possibilité d'écrire des règles complexes en permettant à l'expert de manipuler des éléments dont la position peut être inconnue lors de l'écriture de la règle mais qui seront instanciés lors de l'application de celle-ci. Par exemple, le langage permet d'écrire la règle relationnelle suivante : Si le premier verbe avant le mot that appartient à une liste donnée alors le mot that est étiqueté IN (conjonction de subordination) sauf s'il y a un non commun (singulier ou pluriel) entre le mot that et ce verbe.
Le langage d'étiquetage : CorTag
Le langage dispose aussi d'une bibliothèque de fonctions intégrées qui permettent à l'expert d'exprimer des contraintes sur les mots, les étiquettes, les positions et la phrase.
Induction Progressive
L'étape 0 consiste à obtenir un corpus (Corp 0 ) de spécialité étiqueté par un étiqueteur morphosyntaxique généraliste. Pour ce faire, nous utilisons l'étiqueteur de Brill (Brill, 1995). Puis, avec l'aide de ETIQ (Amrani et al., 2004b, Amrani et al., 2005b, l'expert adapte les règles morphologiques et contextuelles au domaine étudié. Nous avons constaté que les erreurs dues aux mots inconnus de la spécialité sont facilement et efficacement résolues par les règles morphologiques d'ETIQ. Cependant, les confusions contextuelles qui nécessitent des règles relationnelles sont difficilement résolues. Pour les résoudre, nous utilisons l'induction progressive.
L'expert identifie alors des erreurs qui lui paraissent importantes, et rédige des règles de correction pour chaque ambiguïté. Ces règles peuvent être rédigées au sein de ETIQ, ou bien, s'il s'agit de règles fortement contextuelles, en utilisant un langage de programmation dédié à la rédaction de ces règles (le langage CorTag). Chaque règle s'applique à un contexte pré-cis et sert à corriger une erreur spécifique. L'expert produit ainsi un certain nombre de règles, qu'il applique au corpus Corp 0 et il obtient ainsi le corpus CorpRegExp 0 . Un problème quasi insurmontable se présente lorsque l'expert travaille sur de gros corpus : le nombre d'application des règles peut atteindre plusieurs milliers, et l'expert ne peut alors vérifier la validité de ses règles que sur un sous-corpus du corpus initial.
Une fois que l'expert estime qu'il a franchi une étape et qu'il pense avoir résolu un problème à peu près correctement, il fait alors appel à un algorithme d'apprentissage automatique de règles. Cet algorithme sert à apprendre les modifications produites par les règles de l'expert pour résoudre une erreur spécifique. Bien entendu, l'apprentissage correspondant peut se faire sans problème sur le corpus complet. Le logiciel ETIQ permet d'apprendre ces modifications en comparant deux versions du même corpus :
-Le corpus Corp 0 avant l'application des règles de l'expert.  Exemple. Dans la phrase "…that rad59 Delta exhibits synergistic effects...", le mot exhibits est étiqueté incorrectement comme NNS par les règles induites et il est étiqueté correctement comme VBZ par les règles de l'expert (le 1 er exemple du tableau 1). Cependant, dans la phrase "…gene responsible for adrenal hypoplasia congenita and blocks steroid biosynthesis by…", le mot blocks est étiqueté incorrectement comme NNS par les règles de l'expert et étiqueté correctement comme VBZ par les règles induites (le 5 ème exemple du tableau 1).
Ainsi nous disposons de trois versions successives du même corpus : CorpRegExp 0 , CorpRegInd 0 et CorpSûr 0 .
-CorpRegExp 0 est le corpus de départ sur lequel nous appliquons les règles de l'expert.
-CorpRegInd 0 est le corpus de départ sur lequel nous appliquerons les règles induites.
-CorpSûr 0 est le corpus dans lequel nous gardons les étiquettes corrigées manuellement (si l'induction 'gagne' comme pour les exemples 5, 6, 7 et 8 du tableau 1) ou confirmées par l'expert (si c'est lui qui a 'gagné' comme pour les exemples 1, 2, 3 et 4 du tableau 1) durant le processus. Encore une fois, si le corpus est suffisamment petit, l'expert peut corriger toutes les erreurs et il n'est pas vraiment nécessaire d'itérer ce processus. Comme nous partons du principe que le corpus est volumineux, l'expert ne peut pas examiner les milliers de cas où on peut voir une différence entre ses règles et les règles induites. Par contre, il peut noter certains des cas où il a 'perdu' par rapport à l'induction et afficher dans ETIQ la règle induite qui a 'gagné' sur lui (voir exemple ci-dessous). Il analyse ces règles et reçoit ainsi une indication sur la façon d'améliorer ses propres règles pour ne plus faire les erreurs qu'il a corrigées à la main. Il applique alors ces nouvelles règles à CorpSûr 0 qui devient le corpus de dé-part de l'itération suivante, pour engendrer CorpRegExp 1 en lui appliquant les nouvelles règles déduites au cours de l'itération « 1 ».
Exemple. Nous avons expliqué dans le tableau 1 que l'expert peut modifier 'à la main' ses erreurs. Il peut aussi apprendre de nouvelles règles qui lui sont suggérées par le programme d'induction de règles. Illustrons ce procédé sur le 7 ème exemple du tableau 1, relatif à la confusion NNS-VBZ. En examinant Corp 0 (le corpus étiqueté par un étiqueteur généraliste), l'expert avait écrit, en utilisant le langage CorTag (voir Section 3.1), quelques règles pour améliorer l'étiquetage. En appliquant ces règles au corpus Corp 0 , nous obtenons le corpus CorpRegExp 0 . En utilisant ETIQ, nous collectons les différences relatives à la confusion NNS-VBZ entre Corp 0 et CorpRegExp 0 . Puis, en nous basant sur ces différences, nous apprenons des règles en utilisant l'algorithme d'apprentissage de règles RIPPER (Cohen 1995). Pour cette confusion, l'algorithme a engendré 50 règles. L'attention de l'expert est attirée par son erreur sur l'exemple 7. Il peut alors cliquer sur le mot incorrectement étiqueté (displays), et faire ainsi apparaître la règle de RIPPER qui a trouvé, elle, la bonne étiquette. Dans ce cas, la règle qui s'affiche est : « SI le mot courant est étiqueté VBZ ou NNS ET SI il est suivi par un déterminant (DT) ALORS attribuer l'étiquette VBZ au mot courant » (c'est-à-dire qu'un verbe est souvent suivi d'un déterminant). Sur le corpus de départ Corp 0 (c'est-à-dire avant l'application des règles induites), cette règle s'applique correctement 630 fois et engendre 20 erreurs. Après avoir affiché les 630 phrases où un VBZ est suivi d'un DT, nous avons constaté que ces 630 étiquettes sont correctes. Cette règle doit donc contenir une certaine vérité linguistique. Nous avons ensuite affiché les 20 phrases où un mot étiqueté NNS est suivi par un déterminant (DT). Nous avons observé quatre cas : -cas 1 : Le déterminant (DT) est both ou each. Par conséquent, la règle était un peu trop générale et devrait tenir compte de ces deux exceptions (both et each). En fait, mis face à ses erreurs, l'expert a plutôt tendance à les corriger et à obtenir des règles plus efficaces. Le danger serait alors que le système induise des règles faisant les mêmes erreurs qu'à l'itération précédente, et que l'expert revoie ces mêmes erreurs à chaque itéra-tion. Nous n'avons jamais constaté ce comportement de la part de notre système inductif qui apprend des règles très différentes quand les ensembles d'apprentissage sont différents. On notera au passage l'importance des corrections manuelles de l'expert, même si elles sont relativement peu nombreuses, afin de partir d'étiquetage vraiment différent.
En conséquence, et en pratique, l'expert constate à chaque itération que le nombre de fois où il est mis en défaut par le système inductif diminue, au point qu'après quelques itérations il peut examiner toutes les différences et, éventuellement corriger toutes ses erreurs manuellement, sans recourir à des règles. Le corpus final, CorpSûr p est alors parfaitement étiqueté du point de vue de l'erreur d'étiquetage considérée au départ. Il est évidemment nécessaire de répéter le processus entier pour chacune des confusions que l'on désire corriger.
Les types de confusions traités par l'induction progressive
Les fautes les plus importantes sont celles qui faussent la compréhension de la phrase en détruisant la structure syntaxique. Les confusions les plus importantes que nous ayons résolu avec l'induction progressive sont les suivantes :
-Les erreurs d'étiquetage du mot « that » qui peut être étiqueté comme un déterminant « DT », une conjonction de subordination « IN », un pronom relatif « WDT », un pronom « PRP » (e.g. I do not like that.) ou un prédéterminant « PDT ».
-Les confusions entre les noms et les verbes, notamment la confusion entre un nom commun au pluriel « NNS » et un verbe au présent, à la troisième personne du singulier « VBZ ». Par exemple : le mot « functions ».
-La confusion qui concerne un mot qui se termine par « ed » qui peut être un verbe au participe passé « VBN », un verbe au passé « VBD» et adjectif « JJ ».
-La confusion qui concerne un mot se terminant par « ing » qui peut être principalement un nom au singulier « NN », un verbe au gérondif « VBG » ou un adjectif « JJ ». Ainsi, le processus doit être répété 3 ou 4 fois entièrement, ce qui n'est pas exagéré au vu des avantages qu'il y a à posséder un corpus spécialisé « correctement » étiqueté.
Nous avons appliqué notre approche à corpus de biologie moléculaire composé de 600 résumés d'intérêt parmi un corpus initial (de 6119 résumés) obtenu par requête sur Medline (http://www.ncbi.nlm.nih.gov) avec les mots-clés DNA-binding, proteins, yeast. Ce qui nous a permis d'obtenir un corpus « correctement » étiqueté.
De plus, à la fin du processus nous obtenons un ensemble de règles relationnelles intelligibles. Ces règles pourraient être utilisées pour résoudre les ambiguïtés qui ne sont pas bien traités par les étiqueteurs supervisés.
Comparaison des étiqueteurs supervisés
Dans la mesure où il existe déjà d'excellents étiqueteurs avec comme défaut, que nous l'avons déjà évoqué, de ne pas être capable de s'adapter à de nouveaux corpus. La tâche fondamentale d'un chercheur abordant un nouveau domaine consiste à créer un corpus correctement étiqueté. C'est cette tâche que nous venons d'illustrer. Maintenant, il est tout de même intéressant de comparer les performances des étiqueteurs existants. En utilisant le corpus « correctement » étiqueté obtenu, nous avons comparé les étiqueteurs suivants :
1-L'étiqueteur à base de transformation (Brill 1994). 2-L'étiqueteur à base de SVM (Giménez et Màrquez 2003). Pour entraîner cet étiqueteur nous avons utilisé la boite à outils SVMTool (Giménez et Màrquez 2004). 3-L'étiqueteur probabiliste de Stanford (Toutanova et al. 2003). Cet étiqueteur atteint une précision de 97,22% d'étiquettes correctes sur le corpus WSJ. 4-L'étiqueteur à base d'Entropie Maximale (Ratnaparkhi 1996 
Les critères d'évaluation
Pour comparer les étiqueteurs nous avons utilisé les mesures de précisions et de rappel pour chaque étiquette. La précision est définie par la formule suivante : Précision = nombre d'exemples positifs couvets / nombre d'exemples couvets. Le rappel est défini par la formule suivante : Rappel = nombre d'exemples positifs couverts / nombre d'exemples positifs. Il est important de déterminer un compromis entre le rappel et la précision. Pour ce faire, nous utilisons la mesure du F score (avec ?=1) : 
Résultat de la comparaison
Pour comparer les quatre étiqueteurs, nous les avons entraînés sur le même sous-corpus. Le sous-corpus d'apprentissage a été constitué à partir de deux tiers de notre corpus (87965 mots) de biologie moléculaire. Pour l'évaluation, nous avons utilisé le tiers restant du corpus de biologie moléculaire. Le corpus entier comporte 131 346 mots. Le tableau 3 présente les F score obtenus pour chaque étiquette par les différents étiqueteurs.
-254 -RNTI-E-6 A partir de cette expérience, nous remarquons que les performances réalisées par les éti-queteurs entraînés sur un corpus de la même spécialité sont significativement meilleures que celles obtenues avec des étiqueteurs généralistes. De plus, nous pouvons constater qu'un petit corpus d'apprentissage de la même spécialité est plus bénéfique qu'un grand corpus généraliste de plus grande taille. En effet, la taille du corpus d'apprentissage de biologie moléculaire (87965 mots) est inférieure au dixième du corpus du WSJ (1 million de mots).
Comme pour le corpus du WSJ, nous constatons que pour le corpus de biologie molécu-laire les deux meilleurs étiqueteurs sont l'étiqueteur de Stanford et l'étiqueteur à base de SVM, avec un léger avantage pour l'étiqueteur de Stanford.
Bien que les étiqueteurs que nous avons utilisés sont parmi les meilleurs étiqueteurs existants et leur performance sont globalement très satisfaisantes, nous constatons que les F score de certaines étiquettes sont encore assez faibles. Par exemple : les meilleurs F score obtenus pour les étiquettes VBN, VBG, VBD et VB sont respectivement de 92.75, 91.87, 92.35 et 92.63. En effet, même les étiqueteurs les plus performants sont incapables de résoudre ces ambiguïtés difficiles. Par contre, les règles relationnelles écrites par l'expert sont très efficaces pour corriger ces ambiguïtés.
Voici des exemples de règles relationnelles (CorTag) relatives à la correction des erreurs des VBD et des VBN:
-si (?1,been,) (*1,,RB) (0,@BEENedVBP,) alors (0,,VBD) « si le mot been est suivi par une suite d'adverbes et cette dernière est suivie par un mot (position 0) appartenant au groupe BEENedVBP alors ce dernier mot est étiqueté VBD » -si (?1,are,) (*1,,RB) (0,@AREedVBP,) alors (0,,VBN) « s'il existe un 'are' suivi par un certain nombre (éventuellement 0 mots) d'adverbes (RB) et un mot appartenant à la classe AREedVBP suit les adverbes alors ce dernier mot est étiqueté comme VBN » D'après ces exemples, nous constatons que la correction des ambiguïtés difficiles néces-site souvent des règles relationnelles. Les prémisses de ces règles sont fondées sur des contraintes pourtant sur les mots contextuels. Pour que ces règles soient efficaces, il est donc nécessaire que les mots du contexte soient bien étiquetés.
D'autre part, les précisions des étiqueteurs entraînés et évalués sur le corpus de biologie moléculaire sont meilleures que celles des étiqueteurs entraînés et évalués sur le corpus géné-ral du WSJ. Les étiqueteurs se comportent donc mieux lorsqu'il s'agit d'un corpus très spéci-fique. En effet, les corpus spécifiques sont plus homogènes et contiennent plus de régularités que les corpus généralistes (le WSJ corpus ou le Brown corpus).
Le tableau 4 présente les précisions obtenues par les différents étiqueteurs supervisés sur les corpus du WSJ et de biologie moléculaire : 

1
*Institut National des Télécommunications, 9 rue Charles Fourier, 91011 Evry Cedex **SNCF, Dir. de l'Innovation et de la Recherche, 45 rue de Londres, 75379 Paris Cedex 08 {anne.remillieux, christian.blatter}@sncf.fr
La SNCF souhaite mettre à la disposition de ses personnels un outil qui leur permette de partager et de développer leurs connaissances et expériences en matière de conduite du changement, c'est-à-dire de prise en compte des facteurs humains pour la réussite d'un projet.
. Ces connaissances sont, pour la plupart des acteurs, empiriques, particulièrement ancrées dans leur action et donc tacites. Comment recueillir puis formaliser ce type de connaissances en vue de leur partage ?
Nous utilisons deux types de techniques de recueil des connaissances tacites : celle de l'observation de situations de travail, que nous avons mise en oeuvre en assistant aux échan-ges entre acteurs d'une équipe projet dans l'entreprise ; et celle de l'entretien d'explicitation, diffusée en France par Vermersch (1994), dont nous expérimenterons prochainement l'apport pour une problématique de gestion des connaissances. Deux catégories de résultats sont issues de la première observation :
B a s e d e c o n n a is s a n c e s C o n n a is s a n c e s d i a c h r o n i q u e s C o n n a i s s a n c e s s y n c h r o n i q u e s R e s s o u r c e s S u r l' a c t io n S u r l e m o n d e S u r l ' a c t i o n S u r l e m o n d e E s t u ti lis é p a r A p o u r c o n te x t e S o u lè v e o u r é s o u t P r o b l è m e ( s ) C h a n g e m e n t ( s ) A c te u r ( s ) P a r a m è t r e s
FIG.1 -Orientations générales pour la spécification structurelle
-La première identifie les différents types de connaissances utilisées par les acteurs observés et fournit ainsi de premières orientations concernant la structure du futur outil. Parmi les catégories apparues, nous noterons celle des connaissances sur l'action (fournir aux destinataires une illustration du futur changement le plus tôt possible), opposée à celle des connaissances sur le monde, qui portent sur l'environnement au sein duquel les sujets agissent (Les destinataires du changement redoutent surtout la première mise en main du changement). Par ailleurs, les connaissances « diachroniques » descriptibles sous la forme d'une succession d'étapes dans le temps (la description des étapes à suivre pour concevoir un document de communication «concret») se sont distinguées de celles, « synchroniques », qui La conduite du changement à la SNCF s'énoncent indépendamment d'un facteur temporel (la communication auprès des agents doit être concrète).
-La seconde catégorie de résultats décrit le « processus de co-construction des choix » de conduite du changement, c'est-à-dire les 7 phases génériques que mènent collectivement les acteurs observés pour aboutir à la mise en place de solutions de conduite du changement. La description de ces phases, mais aussi des acteurs, des connaissances requises et des ressources (non décrites ici) auxquels elles sont associées, permet d'orienter la spécification fonctionnelle de l'outil.
FIG.2 -Le processus de co-construction des choix de conduite du changement à la SNCF
Pour conclure, précisons que les résultats présentés, destinés à fournir un cadre générique pour notre travail de formalisation, ne rendent pas compte de la seule spécificité de la conduite du changement à la SNCF. En ce sens, ils pourraient vraisemblablement convenir à d'autres types d'activité (comme la conduite de projet par exemple). La particularité des connaissances qui nous intéressent apparaîtra au moment d'insérer des contenus dans le cadre défini.
Références Nonaka, I. (1994), A dynamic theory of organizational knowledge creation, Organization Science Vol. 5, n°1, 14-37. 
Summary
Our research deals with elicitation, formalization and sharing of tacit knowledge about change management at the SNCF. The observation of the work of a project team in the company enabled us to make first assumptions about this knowledge.

Introduction
La conception, la réalisation et la maintenance d'un site web volumineux sont des tâches difficiles, en particulier quand le site est écrit par plusieurs rédacteurs. Pour améliorer le site, il est alors important d'analyser les comportements de ses utilisateurs, afin de découvrir notamment les incohérences entre sa structure a priori et les schémas d'utilisation dominants. Les utilisateurs contournent en effet souvent les limitations du site en navigant (parfois laborieusement) entre les parties qui les intéressent, alors que celles-ci ne sont pas directement liées aux yeux des concepteurs. A l'opposée, certains liens sont très peu utilisés et ne font qu'encombrer la structure hyper textuelle du site.
Une méthode d'analyse dirigée par l'usage consiste à réaliser une classification du contenu du site à partir des navigations enregistrées dans les logs du serveur. Les classes ainsi obtenues sont constituées de pages qui ont tendance à être visitées ensembles. Elles traduisent donc les préférences des utilisateurs. La principale difficulté de cette approche réside dans la nature des observations (les navigations). Comme celles-ci sont de taille variable, on peut en déduire de nombreuses mesures de dissimilarité entre les pages visitées, selon qu'on tient compte de la durée de la visite, du nombre de fois que la page est vue, etc. Dans le contexte de la classification, il est alors difficile de choisir a priori quelle mesure de dissimilarité est la plus adaptée à l'analyse du site.
Dans cet article, nous étudions un site web peu volumineux (91 pages), très bien structuré, et au contenu sémantique bien défini. Grâce à cet exemple de référence, nous comparons différentes dissimilarités afin de mesurer leur aptitude à révéler ce contenu sémantique.
Données d'usage
Préparation des données
Les données d'usage d'un site Web proviennent essentiellement des fichiers log des serveurs concernés. Chaque ligne du fichier log décrit une requête reçue par le serveur associé : elle indique ainsi le document demandé, la provenance de la requête, la date de la demande, etc. Diverses techniques de pré-traitement permettent d'extraire des logs des navigations, comme par exemple celles de Tanasa et Trousse (2004), utilisées dans le présent article. Une navigation est une suite de requêtes provenant d'un même utilisateur et séparées au plus de 30 minutes. Elle constitue donc la trajectoire d'un utilisateur sur le site. Nous nous contenterons dans cet article de considérer chaque navigation comme la liste ordonnée des pages demandées par un utilisateur, sans chercher à tenir compte du temps passé sur chaque page.
Analyse du contenu du site
L'un des buts de l'analyse de l'usage est d'améliorer le site web étudié. Pour ce faire, il est important de déterminer si les a priori sémantiques des concepteurs sont validés par les usages. On cherche en particulier à savoir si les catégorisations choisies a priori sont perçues comme telles par les utilisateurs. Pour ce faire, on construit à partir des données d'usage une mesure de (dis)similarités entre les pages du site. Les pages qui sont souvent ensembles dans les navigations seront ainsi considérées comme proches. En classant le contenu du site selon une dissimilarité induite par l'usage, on détermine des groupes de pages liées qu'on peut confronter aux hypothèses des concepteurs du site.
Toute la difficulté réside dans la nature des données. On peut en effet décrire chaque page par l'intermédiaire des navigations, sous forme de données complexes, en considérant les pages comme des individus et les navigations comme des variables. Une page p i est ainsi décrite par les variables n 1 , . . . , n N (pour N navigations). La valeur de la variable n k est l'ensemble des positions de la page p i dans la navigation n k . Si un site contient quatre pages, A, B, C et D, et est visité par la navigation n 1 = (A, B, A, C, D), la variable n 1 vaut {1, 3} pour la page A, {2} pour la page B, {3} pour la page C, et {4} pour la page D.
Ces données sont difficiles à analyser car les variables n'ont pas des valeurs numériques mais plutôt ensemblistes. De plus, le nombre de colonnes du tableau peut être très élevé. En outre, si le nombre de lignes est élevé (site volumineux), le tableau est alors en général très creux (i.e., contient beaucoup d'ensembles vides).
Dissimilarités
De nombreuses solutions sont envisageables pour traiter ce type de tableau de données. La plus simple consiste à binariser les valeurs des variables, en remplaçant un ensemble vide par la valeur 0 et un ensemble non vide par un 1. De nombreux indices de (dis)similarités ont été définis pour de telles données binaires (cf e.g. Gower et Legendre (1986)). Nous retenons la dissimilarité basée sur l'indice de Jaccard, définie comme suit
-410 -RNTI-E-6 F. Rossi et al.
où n ik vaut 1 si et seulement si la page p i est visitée par la navigation k et où |U | désigne le cardinal de l'ensemble U . Pour cette dissimilarité, deux pages sont proches dès que la plupart des navigations qui passent par l'une passent par l'autre.
Le défaut principal de la dissimilarité de Jaccard est qu'elle ne tient pas compte du nombre de passages par une page. Pour palier ce problème, on remplace le tableau binaire n ik par un tableau d'entiers positifs m ik : la valeur de m ik est le nombre de passages de la navigation k par la page i. On utilise ensuite une des nombreuses dissimilarités adaptées à ce type de données, comme par exemple la dissimilarité "cosinus" définie par
où N désigne le nombre total de navigations. Une autre dissimilarité intéressante est donnée par le modèle tf×idf, dans lequel on pondère une navigation en tenant compte à la fois du nombre de passages dans une page donnée, mais aussi de la longueur de la navigation : une navigation longue passe par beaucoup de pages et la similarité sémantique entre les pages n'est donc pas garantie, contrairement au cas des navigations courtes. La dissimilarité est donnée par
k=1 N l=1 m 2 il log P Pl où P désigne le nombre de pages et P k le nombre pages distinctes par lesquelles la navigation k est passée (cf par exemple Chen (1998)).
Site de référence
Présentation
Pour comparer les similarités retenues, nous utilisons le site du CIn, le laboratoire de deux d'entre nous. Ce site est réalisé par l'intermédiaire d'un ensemble de servlets programmées en Java. Les URL des pages sont assez complexes (car elles correspondent à l'appel des servlets) et il est extrêmement peu probable qu'un utilisateur accède directement à une page en saisissant l'URL (une URL complète pour le site comprend une centaine de caractères). Nous supposons donc que les utilisateurs accèdent au site en entrant par la page principale, puis en suivant les liens proposés dans un menu contextuel situé à gauche de la page.
Le site est petit (91 pages) et très bien organisé, sous la forme d'un arbre de hauteur 5. L'information est essentiellement située dans les feuilles de l'arbre (75 pages) alors que les noeuds internes jouent le rôle de pivots. Le site est très dense en liens, car le menu contextuel comporte toujours la page principale, les 10 pages de premier niveau, ainsi que les parents et les soeurs de la page courante. On dénombre ainsi parfois plus de 20 liens dans le menu lui-même, ce qui ne facilite pas toujours la navigation.
Nous -411 -
RNTI-E-6
Sémantique de référence
Nous avons étudié le contenu du site et construit manuellement une partition des pages en 13 classes, allant des pages recensant les publications du CIn à celles destinées à l'inscription des étudiants en mastère. Pour comparer les dissimilarités, nous utilisons un algorithme de classification et nous comparons les classes obtenues aux classes a priori. 
Classifications
Algorithmes et critères
Pour comparer les dissimilarités présentées dans la section 2.3, nous produisons des classes homogènes de pages, puis nous comparons ces classes à celles issues de l'analyse experte du site de référence. Pour la classification, nous utilisons un algorithme de type nuées dynamiques applicable à un tableau de dissimilarités (cf Celeux et al. (1989)) et une classification hiérar-chique basée sur le lien moyen. D'autres algorithmes sont bien entendu envisageables.
Pour analyser les résultats, nous utilisons deux critères. Pour une analyse classe par classe, nous étudions la F mesure de van Rijsbergen (1979) associée à chaque classe a priori : il s'agit de retrouver au mieux une classe experte dans l'ensemble de classes produites par un algorithme. Pour une analyse globale, nous utilisons l'indice de Rand corrigé (cf Hubert et Arabie (1985)) qui permet de comparer deux partitions. Pour les deux indices, une valeur de 0 correspond à une absence totale de correspondance entre la structure a priori et la structure obtenue, alors qu'une valeur de 1 indique une correspondance parfaite.
Nuées dynamiques
L'algorithme des nuées dynamiques demande de choisir a priori un nombre de classes. Pour limiter les effets de ce choix, nous étudions les partitions produites pour un nombre de classes allant de 2 à 20. Nous obtenons les résultats suivants : Pour l'analyse globale (indice de Rand corrigé), nous indiquons la taille de la partition maximisant le critère. On constate que tf×idf et Jaccard donnent des résultats assez proches (léger avantage pour la première) alors que cosinus obtient des résultats sensiblement moins bons. Pour l'analyse fine, nous cherchons pour chaque classe a priori une classe correspondante (au sens de la F mesure) dans l'ensemble des classes produites en faisant varier la taille de la partition, toujours de 2 à 20. Nous indiquons le nombre de classes parfaitement retrouvées et la plus mauvaise F mesure pour les classes non retrouvées. La mesure tf×idf apparaît comme la plus performante. Les classes retrouvées parfaitement par les autres dissimilarités le sont aussi par tf×idf (qui retrouve les classes 3, 4, 5, 7, 8, 9 et 12). On constate cependant que les classes parfaitement retrouvées le sont dans des partitions différentes, ce qui explique les indices de Rand relativement mauvais, comparativement aux résultats classe par classe.
Classification hiérarchique
Nous reprenons l'analyse conduite pour les nuées dynamiques dans le cas de la classification hiérarchique. Nous faisons varier ici le nombre de classes a posteriori, en étudiant tous les niveaux de coupure possible dans le dendrogramme. Nous obtenons les résultats suivants : Au niveau global, on constate une nette domination de Jaccard et une amélioration des résultats pour celle-ci. Le critère du lien moyen utilisé ici, ainsi que la structure hiérarchique, semble permettre une meilleure exploitation de la dissimilarité de Jaccard, alors que les résultats sont nettement dégradés pour les autres mesures. Les résultats classes par classes sont plus difficiles à analyser et semblent ne pas dépendre de la mesure. Cependant, les "bonnes" performances de tf×idf et de cosinus correspondent à une bonne approximation des classes pour des niveaux de coupure très différents dans le dendrogramme : il n'est donc pas possible d'obtenir avec ces mesures, une bonne récupération de l'ensemble des classes, alors que Jaccard se comporte globalement mieux.
Discussion et conclusion
La dissimilarité de Jaccard apparaît globalement comme la plus performante pour retrouver la sémantique a priori du site de référence, à condition d'être utilisée avec une classification hiérarchique. Tf×idf donne des résultats satisfaisants alors que cosinus semble incapable de retrouver les classes. Il est cependant important de confronter ces conclusions au contenu du site.
Il s'avère en effet que le site du CIn est un peu particulier à cause de sa très grande densité de liens, mais aussi en raison de la présence de pages de navigation, proches de "tables de matières". La classe 6 par exemple, contient 8 pages de description du laboratoire, alors que la classe 5 contient 6 pages présentant ses objectifs. Une des pages de la classe 5 est une description générale des objectifs, qui se contente pour l'essentiel d'orienter le lecteur vers les autres pages. En ce sens, elle pourrait faire partie de la classe 6. Cette page joue donc le rôle de pont entre la classe 6 et la classe 5. De plus la structure du site fait que le passage par cette page est obligatoire pour atteindre les autres pages de la classe 6. Ce problème est en fait assez généralisé dans le site, en raison de sa structure d'arbre. Il a des conséquences sur les dissimilarités car il force une certaine proximité entre des pages qui n'ont pas été placées dans une même classe a priori. La dissimilarité de Jaccard est particulièrement sensible à ce problème : elle a tendance, en particulier dans la classification hiérarchique, à regrouper par exemple les classes 5 et 6, excepté la page de présentation générale du CIn (qui est un pont entre la pagre principale du site et celles de la classe 6) et une page indique comme se rendre au CIn.

Introduction et etat de l'art
Les mammographies sont le moyen le plus répandu pour la détection du cancer du sein. Des études ont démontré qu'une lecture double des mammographies augmente la sensitivité du diagnostic jusqu'à 15% (Bird et al., 1992) et de plus, (Destounis et al., 2004), que les outils d'aide au diagnostic automatique du cancer du sein (ADACS) peuvent améliorer même les ré-sultats d'une double lecture des mammographies. Pourtant, les techniques existantes d'ADACS ont une série d'inconvénients.
Les méthodes existantes d'ADACS peuvent être classées dans deux classes : celles qui essayent d'identifier des signes de cancer (Pluim et al., 2003) et celles qui essayent une classification des mammographies (Zaiane et al., 2002). Dans le premier cas, les inconvénients principals sont le coût, dû au traitement d'images et à la classification des signes trouvés et le fait que les signes des phases de début de cancer sont plus subtiles que ceux recherchés par ces méthodes. Dans le deuxième cas, l'inconvénient principal est le taux de réussite plus faible (qui baisse parfois jusqu'à 56,25%) et sa forte variation selon le jeu de données considéré pour la validation.
Contribution
Par rapport aux autres techniques existantes dans le domaine d'ADACS, notre approche est basée sur les techniques des médecins et se propose d'utiliser moins le traitement d'images et plus des techniques d'apprentissage automatique afin d'obtenir une classification des clichés dans deux classes : symétriques et non symétriques.
Dans une première étape, nous comparons des zones des clichés pour obtenir une mesure quantitative de la similitude. Pour obtenir les zones, nous avons proposé trois méthodes issues de la pratique des médecins, plus précisément la méthode maillage (qui propose une segmentation tenant compte de la structure de symétrie du sein), la méthode fenêtre (qui représente un balayage vertical ou horizontal de l'image avec une zone de taille fixe) et la méthode rideau (qui propose un traitement progressif du cliché, soit en direction verticale soit horizontale).
Comparaison des mammographies
Le graphique des différences entre les zones des deux clichés, met en évidence, par des piques, les éventuelles asymétries. La hauteur des piques est une mesure de la taille des différences, tandis que la largeur des piques est une mesure de la localisation. Tenant compte du fait que les dissemblances naturelles sont normalement répandues sur une zone plus large, en temps que les asymétries dues au cancer sont plus localisées, nous avons choisi de prendre en compte la hauteur et la largeur des plus grands piques pour l'étape suivante de classification des clichés dans les deux classes : symétriques et non symétriques. Pour la classification nous avons utilisé les arbres de décision (Breiman et al., 1984), plus précisément des arbres C4.5 (Quinlan, 1993).
Nous avons testé les trois méthodes sur un jeu de données de 202 couples de clichés, en utilisant 73% de données pour l'apprentissage et le reste de 27% pour la validation. La méthode rideau a eu un taux de réussite de 62% et la méthode maillage un taux de réussite de 68%. Les meilleurs résultats ont été obtenus par la méthode fenêtre, avec un taux de réussite de 70%.
Conclusions et perspectives Les résultats que nous avons obtenu, (un taux de réussite de 70%), sont des résultats préliminaires. Tenant compte du fait que l'approche proposée est nouvelle dans le domaine et aussi du fait que nous avons eu une base de données spécialement annotées seulement sur les aspects de cancer, nous considérons les résultats encourageants.
Nous envisageons de continuer les tests sur une base de données mieux annotée, spécia-lement sur les aspects d'asymétrie. Nous envisageons aussi d'effectuer plusieurs tests afin de trouver les meilleurs paramètres des trois méthodes, la meilleure représentation des clichés et les méthodes de classification les plus adéquates.
Références

Introduction
Dans la dernière décennie, la conception de mesures d'intérêt adaptées à l'évaluation de la qualité des règles d'association est devenue un défi important dans le contexte d'ECD. Bien que le modèle des règles d'association (Agrawal et al., 1993) permette une extraction non supervisée de tendances implicatives dans les données, il produit malheureusement de grandes quantités de règles, ce qui les rend inexploitables sans la mise en oeuvre d'une étape lourde de post-traitement. Le post-traitement doit aider l'utilisateur (un décideur ou un analyste) à choisir les meilleures règles en fonction de ses préférences. Une manière de faciliter la tâche de choix de l'utilisateur consiste à lui offrir des indicateurs numériques sur la qualité des règles d'association : des mesures d'intérêt adaptées à ses buts et aux données étudiées.
Dans les travaux précurseurs sur les règles d'association (Agrawal et al., 1993;Agrawal et Srikant, 1994) , deux premières mesures statistiques sont introduites : le support et la confiance.
Celles-ci sont bien adaptées aux contraintes algorithmiques (cf apriori), mais ne sont pas suffisantes pour capturer l'intérêt des règles pour l'utilisateur. Afin de contourner cette limite, de nombreuses mesures d'intérêt complémentaires ont été proposées dans la littérature. (Freitas, 1999) distingue deux types de mesures d'intérêts : les mesures subjectives, et les mesures objectives. Les mesures subjectives dépendent des buts, connaissances, croyances de l'utilisateur et sont combinées à des algorithmes supervisés spécifiques afin de comparer les règles extraites avec ce que l'utilisateur connaît ou souhaite (Padmanabhan et Tuzhilin, 1998;Liu et al., 1999). Ainsi, les mesures subjectives proposent de capturer la nouveauté (novelty) ou l'inattendu (unexpectedness) d'une règle par rapport aux connaissances/croyances de l'utilisateur. Les mesures objectives, quant à elles, sont des indices statistiques qui évaluent la contingence d'une règle dans les données. De nombreux travaux de synthèse en récapitulent les définitions et propriétés (Bayardo Jr. et Agrawal, 1999;Hilderman et Hamilton, 2001;Tan et al., 2004  Gavrilov et al. (1999) ont étudiés la similitude des mesures afin de les classer. Gras et al. (2004) proposent un ensemble de dix critères : croissant avec un ou plusieurs des indicateurs prédéterminés, la dé-croissance doit respecter certaines attentes sémantiques, constraintes à respecter, décroissance avec la trivialité des observations, suffisamment souple et analytiquement générale, résistance discriminative à la croissance du volume de données ou une fonction discriminante, couplée avec sa contraposée b ? a, la comptabilité des propriétés analystiques, la formule et les algorithmes, l'indépendance entre les deux variables a et b.
Certaines de  Il existe aussi deux outils d'expérimentation sont HERBS (Vaillant et al., 2003) et AR-QAT (Huynh et al., 2005a). ARQAT (Association Rule Quality Analysis Tool) est un outil graphique développé à l'école polytechnique de l'université de Nantes, écrit en Java, avec une interface web. Les caractéristiques principales de cet outil sont : (1) l'analyse des ensembles de règles, (2) l'analyse de corrélation et de cluster, (3) l'analyse des meilleure règles, (4) l'analyse de la sensibilité, (5) l'analyse comparative. L'outil supporte différents formats pour l'exportation/importation des règles d'association et les mesures calculées : PMML, CSV et ARFF (employés par WEKA). Tous les résultats illustrés dans cet article issus de cet outil.
FIG. 1 -Les cardinalités d'une règle.
La cardinalité n ab correspond au nombre effectif de contre-exemples. Soient U et V deux ensembles aléatoires de transactions respectant les contraintes |U | = |A| et |V | = |B|. Nous définissons le nombre de contre-exemples prédits comme la variable aléatoire N ab = |U ? V |. La mesure II est alors définie par la probabilité :
La variable aléatoire N ab peut être modélisée selon plusieurs lois (hyper-géométrique, binomiale, ou poisson). En choisissant la loi hyper-géométrique, nous obtenons proba(
. La formule globale de II peut être efficacement calculée avec une formule récursive.
Une extension de l'II, appelée l'intensité d'implication entropique (EII) a été proposée par (Blanchard et al., 2003)   
CC(m
Graphe corrélé versus graphe non-corrélé
Malheureusement, le graphe de corrélation issu de la matrice de corrélation est complet, et n'est donc pas directement exploitable par l'utilisateur. Nous devons définir deux transformations afin d'extraire des sous-graphes plus limités et plus lisibles. D'abord, en employant la définition 2, nous pouvons extraire le sous-graphe corrélé partiel (CG+) : la partie du graphe où nous ne retenons que des arêtes liées à une forte corrélation ( ? -corrélation). En second lieu, la définition 3 nous permet de construire le sous-graphe non-corrélé partiel (CG0) où nous ne retenons que les arêtes liées aux valeurs de corrélation proches de 0 (?-noncorrélation).
Ces deux sous-graphes partiels peuvent ensuite être utilisés comme support de visualisation afin d'observer les liaisons corrélatives entre mesures.
On peut également y observer des clusters d'indices correspondant aux parties connexes des graphes.
Extension à graphe de stabilité
Afin de comparer les corrélations de mesures entre plusieurs jeux de données, nous introduisons une extension des graphes de corrélation aux graphes de stabilité.
Définition 4. Le graphe ? -stable CG+ (resp. ?-stable CG0) d'un ensemble de k jeux de règles R = {R(D 1 ), ..., R(D k )} est défini comme le graphe intersection moyenne des k sousgraphes corrélés (resp. non-corrélés) partiels CG+ (resp.CG0) calculés sur R. Chaque arête retenue est alors valuée par la corrélation moyenne des k arrêtes. Ainsi le graphe ? -stable CG+ (resp. ?-stable CG0) permettra de visualiser les fortes corrélations (resp. non-corrélations) stables, comme étant communes aux k jeux de données étudiés. Leurs complémentaires donnerons les corrélation instables, différentes.
Définition 5. On appellera clusters ? -stable (resp. ?-stable) les parties connexes du graphe ? -stable CG+ (resp. ?-stable CG0).  (Newman et al., 1998), et le second D 2 est un ensemble de données synthétiques T5.I2.D10k (T5 : taille moyenne des transactions est 5, I2 : taille moyenne au minimum des grands itemsets potentiellement est 2, D10k : nombre des items est 100). Les ensembles de règles d'association R 1 (resp. R 2 ) ont été calculés sur D 1 (resp. D 2 ) en employant l'algorithme Apriori (Agrawal et Srikant, 1994).
De plus, pour une évaluation plus fine du comportement des mesures sur les "meilleures règles", nous avons extrait R 1 (resp. R 2 ) à partir de R 1 (resp. R 2 ) comme l'union des 1000 premières meilleures règles (? 1% des 100000 règles disponibles) selon chaque mesure (voir Tab. 1).
Ces deux jeux de données ont été volontairement choisis pour leur caractère caricatural. Les attributs de la base de données D 1 sont fortement corrélés et délivrent de très nombreuses règles sans contre-exemples (confiance à 1). A contrario, la base de données synthétique D 2 est constituée d'attributs faiblement corrélés et délivre peu de règles sans contre-exemple. Ainsi, nous attendons de ce choix qu'il nous amène à ne découvrir que très peu de stabilités corréla-tives entre les deux jeux de données. 
Résultats et discussion
Dans notre expérience nous avons utilisé les trente-six mesures d'intérêt (trente-quatre mesures sont définies dans Huynh et al. (2005b) en ajoutant deux measures
. Les mesures EII(? = 1) et EII(? = 2) sont deux versions entropiques de la mesure II.
Notre expérience vise à trouver des corrélations stables, a priori inattendues, entre les quatre ensembles de règles. A cette fin, nous analysons les résultats produits dans : (1) les quatre graphes CG0 et le graphe CG0 montrant les indices non corrélés stables, (2) les quatre graphes CG+ et le graphe CG+ montrant les indices corrélés stables. Ensuite, nous pouvons observer que les graphes CG+ obtenus sur l'ensemble total des règles (CG + (R 1 ) et CG + (R 2 )) et le sous-ensemble des meilleures règles (CG + (R 1 ) et CG + (R 2 )) sont très semblables. Ceci nous indique que sur les deux jeux de données les corrélations et les clusters formés demeurent stables lorsque l'on sélectionne les meilleures règles.
D'autre part, comme on l'attendait, on observe un écart important entre les deux jeux de données, ce qui indique une sensibilité des mesures à la nature des données.
En revanche, nous pouvons observer un nombre important de corrélations entre mesures sur le jeu de données R 2 -même s'il est deux fois plus faible que sur R 1 -alors que nous en attendions peu du fait de la faible corrélation des données dans D 2 .
La figure Fig. 3 permet de visualiser les mesures non-corrélées, dont le point de vue sur les données diffère.
On y observe un très faible nombre de non-corrélations, ce qui indique que très peu de mesures sont en désaccord fort sur l'évaluation des règles. Toutefois, le nombre de mesures non-corrélées augmente lorsque l'on passe de la totalité des règles aux meilleures. En revanche, contrairement à ce que l'on pouvait attendre, il y a moins de non-corrélations sur le jeu de données synthétique R 2 .
Enfin, aucun comportement stable n'apparaît entre les mesures sur les quatre graphes CG0, et donc le graphe CG0 est vide.
FIG. 2 -Les 4 graphes CG+ (les clusters sont grisés).
Le graphe CG+ : étude de la stabilité des corrélations
Le résultat le plus surprenant apparait dans le graphe CG+. En effet, contrairement à notre attente, nous découvrons cinq clusters de mesures ? -stables, c'est-à-dire dont les corrélations demeurent inchangées entre les jeux de données. Ceci dénoterait d'une invariance avec la nature des données ! En analysant plus précisement ces cinq clusters ? -stable, nous notons quelques éléments intéressants.
(C1), le plus grand cluster, (Confidence, Causal Confidence, Causal Confirmed-Confidence, Descriptive Confirmed-Confidence, Laplace) rassemble des mesures dérivées de la mesure de confiance (Confidence). De plus, ce lien est fort, puisque le graphe est complet et les valeurs de corrélation supérieures à 0.97. Ceci indique un très fort accord entre ces cinq mesures.
(C2), ce cluster moins fortement corrélé que le premier, est constitué des mesures Phi-
Coefficient, Lerman, Kappa, Cosine et Jaccard. Ce cluster rassemble des mesures partageant les quatre propriétés de : symmetric under variable permuatation, antisymmetric under row/column permutation, et null invariance (Tan et al., 2004). Les deux mesures Jaccard et Cosine ne partagent que la cinquième propriété (null invariance) proposée par Tan et al. (2004).
(C3), rassemble trois mesures concernées par la première propriété (symmetry/asymmetry under variable permutation) proposée par Tan et al. (2004). L'existence de ce cluster est né-cessaire pour distinguer la règle a ? b de b ? a.
(C4), est un cluster constitué par deux versions de l'intensité de l'implication EII et EII 2, ce qui n'est pas surprenant.
(C5), la stabilité de la corrélation Yule'Q et Yule'Y, est elle aussi sans surprise puisque les deux mesures présentent une dépendance fonctionnelle. Ce cluster est lié à la deuxième propriété (row/column scaling invariance) proposée par Tan et al. (2004).
Les résultats du graphe ? -stable, donnent une piste intéressante pour construire une base réduite de mesures dont le point de vue est le plus différent sur les données. Il suffit pour cela de proposer à l'utilisateur de choisir cinq mesures, une parmi chacun des clusters. Nous pourrions aussi calculer automatiquement le meilleur représentant de chaque cluster en fonction des valeurs de corrélation. Contre toute attente, une étude de la stabilité des corrélations entre mesures d'intérêt, a fait apparaître cinq groupes de mesures stables entre deux jeux de données choisis pour leur nature opposée.
Bien sûr ces résultats préliminaires restent à confirmer sur un ensemble de données plus important. Nous envisageons de prolonger notre travail dans deux directions. En premier lieu, nous souhaitons un indice de similarité entre mesures meilleur que l'indice de corrélation linéaire dont les limites sont soulignées dans la littérature. En second lieu, nous souhaitons amé-

Contexte
Nos travaux s'insèrent dans un projet du réseau ARTCADHi visant à offrir aux chercheurs en Sciences Humaines des assistants à la construction du sens dans des bibliothèques numériques spécialisées. Dans ce cadre, limiter la description des documents à une indexation unique, fixe et effectuée par un tiers, revient à nier leur expertise. Porphyry propose l'instrumentation du travail des chercheurs par l'enrichissement itératif du corpus par des structures hypermédias. Ces structures sont construites par les spécialistes en fonction de leurs problématiques et de leurs spécialisations. Elles sont exprimées sous forme de réseaux de description, une variante des réseaux sémantiques dans laquelle seule existe la relation de composition (Benel A., 2003).
Dans son état actuel, Porphyry offre un moyen de visualiser des points de vue lorsqu'ils sont appliqués aux même cas expérimentaux. Cependant, ce n'est que la première étape dans le processus de confrontation mené par le chercheur, et les réseaux de description ne sont qu'un formalisme parmi d'autres. Nous proposons donc de spécifier un atelier multiformalisme d'aide à la construction de sens par confrontation de points de vue.
Du fait que Porphyry est adressé à des chercheurs en Sciences Humaines, le désaccord entre deux experts est matière à réflexion et à enrichissement. La confrontation des points de vue va donc au-delà de l'intégration de travaux réalisés de manière transversale dans le but d'en faire un tout unique et cohérent. L'accent est mis sur le partage des idées, la confrontation devant permettre d'outiller l'étude des différents points de vue pour que de nouvelles idées puissent voir le jour plus facilement.
Proposition
Nous envisageons cette démarche dans un cadre très général, bien que la plate-forme Porphyry en soit un élément principal. Nous travaillons sur des points de vue exprimés par leur saisie dans un système informatique, mais nous ne limitons pas ce système à Porphyry seulement. Dans ce cadre, nous définissons un point de vue comme une théorie sur un sujet d'étude exprimée par un modèle dans un langage. Nous regroupons sous le terme « langage » aussi bien la langue ou le formalisme que le modèle du document, qui clarifie les règles diverses auxquelles l'écriture se plie. Nous considérerons cependant de manière plus approfondie le cas des langages formels.
-725 -RNTI-E-6 
Summary
Porphyry today allows experts to express their points of view in a formal context. The next stage, which is discussed here, is to make possible to match these points of view.

Summary
Basel 2 regulations brought new interest in supervised classification methodologies for predicting default probability for loans. An important feature of consumer credit is that predictors are generally categorical. Logistic regression and linear discriminant analysis are the most frequently used techniques but are often unduly opposed. Vapnik's statistical learning theory explains why a prior dimension reduction (eg by means of multiple correspondence analysis) improves the robustness of the score function. Ridge regression, linear SVM, PLS regression are also valuable competitors. Predictive capability is measured by AUC or Gini's index which are related to the well known non-parametric Wilcoxon-Mann-Whitney test. Among methodological problems, reject inference is an important one, since most samples are subject to a selection bias. There are many methods, none being satisfactory. Distinguish between good and bad customers is not enough, especially for long-term loans. The question is then not only "if", but "when" the customers default. Survival analysis provides new types of scores.

Introduction
Les valeurs-tests
Pour faire un test de l'hypothèse nulle H 0 , le statisticien calcule une « probabilité critique » (ou p-value). C'est la probabilité, calculée sous H 0 , d'un événement au moins aussi extrême que l'événement observé. De façon intuitive, on comprend que cette probabilité est d'autant plus faible qu'on est loin de l'hypothèse nulle. Si l'événement observé est très improbable sous l'hypothèse nulle, on jugera que les observations sont vraisemblablement régies par un mécanisme non nul. Il est donc tentant d'utiliser cette valeur numérique pour évaluer l'écart entre ce qu'on a observé et la situation « sans intérêt » correspondant à ce qu'on aurait observé sous H 0 . Dans ce contexte, plus l'évaluation de l'écart est forte (plus la probabilité critique est faible), plus ce qu'on a observé est intéressant (Gras et al., 2002 ;Lerman et Azé, 2003 ;Lallich et Teytaud, 2004). Dans la pratique, on se rend compte que la p-value est difficile à manipuler ; elle peut atteindre des valeurs très faibles, très peu lisibles ; pire, dans certains cas, elle est inutilisable car on se heurte aux limites de l'approximation
Un critère numérique de classement et de sélection
La notion de critère invoquée ici est particulière. On utilise en effet les outils mis en oeuvre dans les tests d'hypothèses mais il ne s'agit en aucun cas de faire des tests au sens de la théorie classique des tests statistiques. L'objectif n'est pas, par exemple, de tester l'indépendance entre A et C puisqu'on tiendra pour vrai que A et C sont liés. La mécanique des tests est en quelque sorte dévoyée pour servir comme outil d'évaluation et non comme outil de prise de décision dans l'incertain.
On se place donc délibérément hors du cadre de la théorie de la décision statistique, et par conséquent hors d'atteinte des critiques qui s'y rapporteraient. Cette remarque doit écarter à l'avance les objections que ne manquerait pas de faire à juste titre le statisticien qui lirait ces lignes sans ce préalable.
Les comparaisons multiples
Comme on le voit dans tout contexte de Data mining et en particulier dans le logiciel SPAD, les valeurs-tests sont calculées pour évaluer, sur le même jeu de données, des dizaines ou des centaines d'écarts entre moyennes ou entre pourcentages, ou pour évaluer autant de corrélations entre variables mesurées sur les mêmes individus.
Dans le cadre de l'application classique des tests statistiques (on accepte de rejeter à tort l'hypothèse nulle avec une probabilité fixée ?), une telle situation nécessiterait de corriger le seuil confiance de chaque test pour assurer un risque global fixé à ? (correction de Bonferroni ou toute autre correction proposée dans la littérature). S'il s'agit d'évaluer dans quelle mesure les observations supportent l'idée qu'on est éloi-gné d'une hypothèse nulle, il n'y a pas de seuil de confiance en jeu : on évalue simplement des écarts relatifs entre les valeurs-tests. Il y a effectivement des comparaisons multiples mais la question de la correction pour comparaisons multiples ne se pose pas dans ce contexte.
Les inconvénients de la valeur-test
Evaluer la force ou l'intérêt des liaisons et des écarts est un problème qu'on rencontre dans le contexte du Data mining où le nombre des observations est toujours « très grand », bien au-delà des tailles d'échantillons rencontrés dans une situation de test d'hypothèse. Les probabilités critiques seront par conséquent généralement très faibles, et d'ailleurs souvent difficiles à calculer pour cette raison. En d'autres termes, les valeurs tests seront très grandes et toujours, à phénomène égal, fonctions croissantes du nombre d'observations. Illustrons cette remarque sur un exemple où l'on compare deux pourcentages en utilisant par exemple la loi hypergéométrique (on place les effectifs dans un tableau 2x2 dont les marges sont supposées fixées).
Considérons un groupe particulier d'individus représentant 56% des observations totales. Considérons un caractère qui est présent dans 23% de l'ensemble de ces observations, mais dans 28% du groupe particulier (le groupe noté A). Le même écart entre 23% et 28% correspond à une valeur test qui dépend évidemment du nombre des observations sur lesquelles il est calculé. Choisissons quelques cas correspondant à ces mêmes pourcentages pour des tailles croissantes, de n=100 à n=4000. Les données sont présentées dans les tableaux 2x2 de la Figure 1. Ce phénomène est bien connu des statisticiens : si la taille de l'échantillon croît suffisamment, le moindre écart à toute hypothèse nulle finit par devenir « significatif ».
Dans ces conditions, non seulement les valeurs-tests ne mesurent pas la « significativité » d'un écart, mais de plus elles ne peuvent pas être comparées si elles sont calculées sur des nombres différents d'observations.
La Valeur-test normalisée VT100 pour les règles d'association
On ne rappellera pas ici les nombreux travaux déjà publiés qui s'attaquent au problème de détection des règles d'association « intéressantes » parmi la liste toujours longue des règles fournies par les algorithmes classiques, comme l'algorithme A PRIORI (Agrawal et Srikant, 1994) ou ses divers avatars. On pourra utilement consulter certains articles de synthèse (Lenca et al., 2003 ;Tan et al., 2002).
Les remarques faites ci-dessus nous conduisent à proposer une sorte de valeur-test normalisée, jouant un rôle analogue à celui de la valeur-test habituelle, mais rendue indépen-dante du nombre d'observations sur lesquelles elle est calculée. A ce titre, son rôle essentiel sera de ranger les règles d'association selon leur ordre d'intérêt et accessoirement de suggé-rer un seuil en deçà duquel les règles n'auront vraisemblablement plus d'intérêt (quelque soit la taille des données).
Le principe en est simple : on décide de se ramener artificiellement au cas du nombre d'observations n=100. Cette valeur -a priori arbitraire -étant à rapprocher de la taille raisonnable des échantillons utilisés quand la théorie des tests s'est historiquement développée. Pour ce faire, on imagine le mécanisme suivant.
Considérons une règle d'association particulière construite sur les n observations disponibles (n étant grand comparé à 100). Il lui correspond un tableau 2x2 où la somme des fré-quences vaut n. Comme on l'a introduit plus haut, on mesure l'intérêt de la règle en s'appuyant sur l'écart à l'indépendance mesuré à partir de la loi hypergéométrique (on verra plus loin que le raisonnement sera analogue pour tout autre critère).
Dans le contexte hypergéométrique, les marges du tableau 2x2 sont fixées. On imagine donc les données comme étant des 0 et des 1 répartis dans deux colonnes à n composantes, de façon aléatoire, le nombre de 1 étant fixé dans chaque colonne. Un échantillon de taille 100 extrait de ces données correspond à un tirage au hasard de 100 lignes dans ce tableau à 2 colonnes de longueur n.
On répète un grand nombre de fois le tirage d'échantillons de taille 100. Pour chaque tirage, on construit le tableau 2x2 correspondant et on calcule la probabilité critique hypergéométrique. Finalement on calcule la probabilité critique moyenne (la p-value moyenne) que l'on obtient à partir de ces échantillons. Appelons VT 100 la valeur-test associée à cette probabilité critique moyenne. Cette moyenne VT 100 sera le critère utilisé pour caractériser l'intérêt de la règle.
Ce mécanisme de calcul du critère VT 100 tend à placer l'utilisateur dans le contexte classique d'un échantillon aléatoire de taille n=100 porteur de l'association à évaluer. On serait précisément dans ce contexte classique si la population réelle était l'ensemble des observations et si on ne tirait qu'un seul échantillon. En tirant de nombreux échantillons, on stabilise la valeur de la p value tout en conservant la taille 100 du support de calcul. Dans ce contexte, on ne s'interdira pas de penser ainsi : « si je trouve une VT 100 très supérieure à 1,645 (car 1,645 
écart type est le seuil des valeurs tests d'un test unilatéral à 5%), je suis vraisemblablement en présence d'une règle très intéressante ».
Ainsi le critère VT 100 permettra de ranger par ordre d'intérêt les règles calculées sur une même base de données. Il présente aussi l'avantage de permettre la comparaison de règles calculées sur des bases de données différentes, par exemple sur des bases de données de tailles différentes extraites à des dates successives.
Calcul pratique des VT100
Le mécanisme de tirage répété des échantillons de taille 100 est utile pour présenter le critère. Pour réaliser les calculs, on utilisera une procédure d'approximation rapide et suffisamment efficace : rapide, car elle ne nécessite pas la répétition des tirages, et donc des accès à la base ; efficace, car elle fournit des valeurs proches de celles du tirage répété et permet de classer les règles dans le même ordre. On suivra facilement cette procédure de calcul sur un exemple présenté en Figure 3. Considérons une règle A =>C définie dans le tableau 2x2 cidessous sur 2000 observations. On calcule le tableau correspondant des effectifs (décimaux) ramenés à un total égal à 100. 
FIG. 3 --Les effectifs sont décimaux
Il s'agit d'approcher par interpolation ce que donnerait une loi hypergéométrique appliquée à ce tableau d'effectifs décimaux. Chacun des trois effectifs décimaux écrit dans le tableau est compris entre deux entiers proches (par exemple 11,30 est compris entre 11 et 12). Ceci conduit à imaginer d'approcher le résultat cherché comme barycentre des résultats hypergéométriques calculés sur les 8 tableaux obtenus en combinant les effectifs entiers les plus proches des 3 valeurs décimales, les coefficients barycentriques étant les écarts aux entiers.
L'approximation peut paraître naïve mais elle est certainement suffisante pour l'objectif à atteindre qui est de comparer l'intérêt des règles. Rien n'empêche d'ailleurs de lui substituer à volonté un calcul plus raffiné pourvu que le temps de calcul reste raisonnable (noter que, la taille 100 étant fixée, la loi hypergéométrique à calculer n'a plus que 2 paramètres entiers libres, ce qui suggère l'alternative d'une tabulation à double entrée, stockée en mémoire pour éviter tout calcul hypergéométrique dans les applications).
Un même principe pour différentes mesures d'intérêt
Lorsqu'on se base sur la loi hypergéométrique pour évaluer l'intérêt d'une règle A=>C (c'est-à-dire pour évaluer l'écart à une situation d'indépendance entre A et C), le critère utilisé est symétrique en A et C et donnera le même résultat pour la règle C=>A. L'intérêt de la règle est mesurée en fait par l'écart entre son support et ce que serait ce support en cas d'indépendance de A et de C (si les effectifs respectifs de A et C sont considérés comme des quantités fixées). A ce titre, le critère VT 100 basé sur la loi hypergéométrique pourrait être considéré comme un versant statistique du critère numérique usuel appelé Lift dans le cadre des règles d'association.
Il y a bien sûr de nombreux autres points de vue possibles pour apprécier l'intérêt d'une règle. A titre d'exemples, nous allons évoquer deux autres utilisations du critère VT 100 . Dans tous les cas, il est commode de se représenter les observations concernant une règle A=>C comme 2 colonnes de longueur n, nommées A et C, contenant des 0 et des 1. Le cas d'indépendance déjà évoqué associé à la loi hypergéométrique consiste à répartir de façon aléatoire un nombre n(A) fixé de 1 dans la colonne A et un nombre n(C) fixé de 1 dans la colonne C.
Critère VT 100 associé à la confiance d'une règle
Un autre contexte d'indépendance entre A et C est défini par le mécanisme suivant : dans la colonne A , les 1 sont introduits avec la probabilité constante p(A) et, de façon indépen-dante, les 1 sont introduits dans la colonne C avec la probabilité constante p(C). Dans ces conditions (schéma dit binomial), les marges du tableau 2x2 ne sont pas fixées.
Une règle A=>C est d'autant plus intéressante que sa confiance, fréquence du conséquent sachant que l'antécédent est réalisé, calculée par n(AetC)/n(A), s'écarte davantage de la fréquence globale du conséquent n(C)/n. Avec le schéma binomial, et si on estime la probabilité p(C) du conséquent par sa fréquence empirique n(C)/n, la loi du support n(AetC) est la loi binomiale de paramètres p(C) et n(A).
La probabilité critique d'observer, sous l'hypothèse d'indépendance (loi binomiale), une confiance au moins aussi extrême que celle qu'on a observée servira à mesurer l'intérêt de la règle en terme de confiance. Transformée en nombre d'écarts types d'une loi normale, cette probabilité critique devient la valeur-test. Ce critère évaluant l'intérêt d'une règle possède la particularité de ne pas être symétrique (il est différent pour la règle C=>A) mais, comme le précédent, il a l'inconvénient d'être sensible aux effectifs.
Selon le principe présenté plus haut, on le normalise en ramenant le paramètre n(A) à la valeur 100. Dans ces conditions, on se trouve en présence d'une loi binomiale dont les paramètres sont connus (taille 100 et probabilité n(C)/n) mais il faudrait calculer la probabilité de dépasser une valeur non entière n(AetC)/n(A). L'approximation de cette valeur peut se faire par interpolation comme précédemment : on calcule les probabilités critiques binomiales pour les deux entiers qui encadrent la valeur décimale et on approche la valeur cherchée par leur barycentre en prenant comme coefficients les écarts aux entiers. La transformation de cette probabilité en nombre d'écarts types de la loi normale sera le critère VT 100 associé à la confiance de la règle.
On signale une propriété satisfaisante de ce critère (partagée bien sûr par d'autres critè-res). Considérons deux règles ayant le même antécédent A et les conséquents C 1 et C 2 . Supposons que ces deux règles aient la même confiance, donc n(Aet C 1 ) est égal à n(Aet C 2 ). Si n(C 1 ) est inférieur à n(C 2 ), la règle A=> C 1 sera préférée car elle creuse l'écart entre la confiance (qui est la même) et la fréquence du conséquent. Il est clair que le critère VT 100 calculé à partir de la loi binomiale favorisera bien la règle A=> C 1 .
Critère VT 100 associé aux contre-exemples
Un contre-exemple de la règle A=>C consiste en la réalisation de A alors que C n'est pas réalisé. L'intérêt pour une règle est d'autant plus grand que le nombre de contre-exemples noté n(Aet~C) est faible. Dans le schéma binomial précédent (répartition des 0 et des 1 dans les colonnes A et C avec des probabilités constantes), l'hypothèse d'indépendance implique une loi simple pour le nombre X de contre-exemples. On peut en effet estimer la probabilité d'un contre-exemple par le produit des probabilités des deux événements indépendants p = {n(A)/n} {n(~C)/n}. Ainsi X suivra une loi binomiale de paramètres connus n et p.
L'intérêt d'une règle mesuré sous l'angle des contre-exemples reposera sur la probabilité, calculée sous l'hypothèse d'indépendance, d'un nombre de contre-exemples au moins aussi extrêmes (c'est-à-dire, plus petit) que le nombre observé. Cette probabilité sera évaluée dans le cadre normé d'un échantillon de taille 100 puis transformée en nombre d'écarts types d'une loi normale pour définir la VT 100 associée aux contre-exemples. Ramené à 100 observations, le nombre de contre-exemples devient une valeur décimale ; on approchera donc la probabilité critique cherchée par le barycentre des deux probabilités binomiales calculées pour les entiers qui l'encadrent.
Parmi les propriétés de ce critère, on notera qu'il est non symétrique en A et C et qu'il attribue la même valeur à la règle A=>C et à sa contraposée qui lui est logiquement équiva-lente ~C=>~A.
Remarque
Puisque les effectifs utilisés sont supposés ici très élevés, les distinctions entre certaines situations d'indépendance peuvent s'estomper et les distributions de probabilités (hypergéo-métriques, binomiales, khi-2 …) tendre vers les mêmes lois limites. Ceci peut faire apparaî-tre assez artificielles dans certains cas les distinctions faites ici entre les schémas d'indépendance.
Un exemple d'application
Données et résultats
Les données utilisées sont extraites du fichier « Adult » disponible sur le site « UCI Machine Learning Repository » (Newman et al., 1998). Les données manquantes ont été remplacées par la moyenne pour les attributs continus, par une nouvelle modalité « manquante » pour les attributs discrets. Le tableau analysé ici possède 14 743 lignes (individus) et 12 colonnes (variables qualitatives dont certaines sont les variables quantitatives d'origine recodées en classes). La dernière variable est une variable Oui/Non indiquant si l'individu a un gain moyen supérieur ou non à US$ 50000. On s'intéresse aux règles dont le conséquent est l'attribut Non de cette variable et dont les antécédents peuvent contenir jusqu'à 3 items. Pour la sélection des règles par un algorithme classique « A PRIORI », on s'est fixé comme seuils un support supérieur à 20%, une confiance supérieure à 60% et un lift supérieur à 1,10. L'implémentation correspond au composant « A PRIORI MR » dans le logiciel TANAGRA (Rakotomalala, 2005), le code source est disponible en ligne.
Pour la clarté de l'illustration numérique, on présente une application "supervisée" où les règles doivent conduire à un conséquent unique choisi arbitrairement. Le critère naturellement s'applique avec les mêmes propriétés au cas général de sélection parmi toutes les règles d'association.
Le tableau 1 liste les 12 premières règles rangées par valeurs décroissantes du critère VT100. Considérons par exemple la première règle : Le critère VT100 est consigné dans la colonne 8 du tableau. Il indique que, si on ramenait la taille de l'échantillon observée n = 14 743 à la taille arbitraire n = 100, la probabilité d'un événement au moins aussi extrême, transposé dans ce contexte, serait égale à la probabilité d'être au delà de 3,2 écarts types de la loi normale (soit une p-value inférieure à 0,0007 calculée sur un échantillon de taille 100). Ce serait donc bien un événement exceptionnel qui nous ferait douter de l'indépendance entre l'antécédent et le conséquent, marquant par-là l'intérêt que présente cette règle. Dans ce tableau 1, les 7 premières règles correspondent à des événements dont la probabilité calculée sur un échantillon de taille 100 serait inférieure à 0,001. La dernière des 12 règles du tableau aurait elle-même une probabilité légèrement inférieure à 0,05. 
N°
TAB. 1 --Les 12 premières règles calculées sur les 14 743 individus Le support du conséquent est n[C] = 11 221
Quand il faut limiter la liste des règles fournies par les algorithmes de recherche de rè-gles, on peut choisir de les ranger en fonction du critère VT100 et fixer un seuil d'arrêt. Par analogie avec les seuils conventionnels adoptés par les statisticiens, on pourra s'arrêter à la valeur 1,645 du critère VT100 (correspondant au seuil 0,05 pour une p-value) ou à la valeur 2,326 (seuil 0,01 pour une p-value) ou encore 3,09 (seuil 0,001) dans les cas où il y aurait profusion de règles intéressantes.
Dans la colonne 9 du tableau 1, on fait figurer la valeur du critère VT100 estimée par simulation sur des échantillons réels de taille 100 extraits du tableau des observations. La procédure de simulation adoptée ici est la suivante : nous réalisons un tirage aléatoire avec remise parmi les individus couverts par la règle puis nous calculons la valeur test VT correspondante ; cette procédure est répétée N fois (N = 50) et la valeur test calculée par simulation est la moyenne arithmétique des valeurs test individuelles.
On remarque que la VT100 calculée par interpolation à partir de la loi hypergéométrique a tendance à surestimer la valeur obtenue par simulation sur des échantillons de taille 100.
Validation avec un échantillon test
Pour compléter cet exemple, on a procédé à une approche de validation sur échantillon test. Les 14 743 individus ont été répartis au hasard par moitié dans un échantillon d'apprentissage (7 371 cas) et un échantillon test (7 372 cas). Les règles ont été recalculées sur l'échantillon d'apprentissage. Dans le tableau 2-A, on liste les règles obtenues en les numérotant en colonne 1 avec le numéro qu'on leur a attribué dans le tableau 1. On constate que 9 des 12 règles se retrouvent avec l'échantillon d'apprentissage. L'élimination au hasard de la moitié des cas a entraîné la disparition des règles n°4 et n°7 qui étaient présentes dans le tableau 1 et n'a pas fait apparaître des règles nouvelles en haut du classement par les VT100. On constate dans le bas du tableau deux inversions de classement par les VT100 dont les valeurs numériques restent cependant proches.  
TAB. 2-B --Echantillon test (7 342 individus), le support du conséquent est n[C] = 5 575
Il est intéressant finalement de comparer, pour ces 10 règles, le critère VT100 évalué sur l'échantillon total, sur l'échantillon d'apprentissage et sur l'échantillon test. Les résultats qui apparaissent sur le tableau 3 montrent bien que le critère est surévalué sur l'échantillon d'apprentissage (phénomène classique) et qu'il établit un compromis (sorte de moyenne) avec la valeur calculée sur la totalité des cas disponibles.  TAB. 3 --Comparaison du critère VT100 appliqué à l'ensemble des observations, à l'échantillon d'apprentissage et à l'échantillon test
Conclusion
Le critère VT100 est proposé pour ranger et sélectionner un nombre raisonnable de règles d'association dans les cas d'applications réelles où les données à analyser sont volumineuses.
Ce critère présente des propriétés intéressantes :
• Facile à comprendre puisqu'on s'appuie sur le mécanisme usuel des tests en raisonnant sur la population des échantillons de taille 100 extraits des observations. • Facile à calculer quelle que soit la taille des données (car on effectue quelques interpolations dans la table hypergéométrique limitée à n=100).
• Souple puisque ne dépendant que d'un seuil qui peut s'exprimer en terme de pvalue (?=0,05 ou ?=0,01 etc.) ou en terme de nombre d'écarts types d'une loi normale (valeur-test 1,645 ou 2,326 etc.).
• Indépendant de la taille des données tout en faisant sur les informations (support, confiance, lift, etc.) les compromis que savent faire les critères statistiques classiques (qui, appliqués directement, sont très sensibles à la taille de l'échantillon). Si l'on veut rendre plus robuste la sélection des règles (quand le nombre de cas disponibles est élevé) on propose la stratégie suivante : on divisera par moitié les données en apprentissage et test. On calculera les règles sur l'échantillon d'apprentissage et on évaluera le critère VT100 sur l'échantillon test. On rangera les règles et on les sélectionnera par seuil en fonction du critère VT100 évalué sur l'échantillon test.
Enfin, notons que le critère VT100 peut être mis en application dans de nombreux autres problèmes de sélection d'items caractéristiques rencontrés dans le Data mining : par exemple le critère de choix des variables de coupure dans un arbre de segmentation ou le critère d'élagage associé ; le classement des items caractérisant une classe dans une typologie, ou caractérisant un facteur dans une analyse factorielle, etc.

Introduction
En France, l'apprentissage de la chirurgie orthopédique se déroule selon différentes modalités d'enseignement comme le compagnonnage (apprentissage en situation réelle), les travaux pratiques en laboratoire d'anatomie et quelquefois sur des simulateurs. Un travail antérieur que nous avons mené sur l'enseignement du métier de chirurgien nous a permis de montrer l'écart qui existe entre les contenus de la formation théorique et les besoins de la pratique (Vadcard, 2003). La formation théorique n'est pas orientée vers la résolution de problèmes en situation, et la situation réelle, n'étant pas construite à des fins didactiques, ne permet pas à l'apprenant de prendre le temps qu'il lui faut pour comprendre la résolution du problème qui se déroule (Bisseret, 1995). Car les connaissances du chirurgien ne se limitent pas à une partie déclarative et une partie gestuelle. Nous avons pointé l'existence et la valeur opératoire de connaissances pragmatiques, souvent implicites, qui permettent l'activité en situation. Ces connaissances, dont nous avons montré l'absence de prise en charge dans le système d'enseignement, nous semblent être un élément important à prendre en compte pour réduire l'écart entre la formation théorique, qui transmet des connaissances de nature prédicative et la formation pratique, qui transmet des connaissances gestuelles opératoires.
Notre objectif est ainsi de concevoir un environnement informatique qui constitue une étape intermédiaire entre les enseignements formels et le compagnonnage, et permet une pragmatisation des concepts théoriques et prescriptifs de l'action avant leur mise en situation. L'environnement comprend un simulateur de vissage du bassin, un ensemble de pages web annotées grâce à une ontologie du domaine, et un ensemble de cas cliniques qui sera intégré plus tard. Cet environnement est centré sur un modèle de connaissances, lequel intègre des connaissances tacites, ou pragmatiques, explicitement représentées. Cette représentation permet en particulier d'associer tous les composants et de produire des rétroactions adaptées selon les éléments de connaissance diagnostiqués. Nous menons actuellement ce travail dans le domaine de la chirurgie osseuse, pour la résolution des problèmes de vissage percutané des fractures du bassin.
De la didactique professionnelle à la conception d'EIAH
Nos travaux sont de nature pluridisciplinaire : informatique, didactique, psychologie et médecine travaillent à la réalisation de ce système d'apprentissage. Dans cet article, nous montrons comment l'aspect didactique de nos recherches nous permet de modéliser les connaissances en interaction avec les problématiques informatiques de représentation de ce modèle.
C'est dans le cadre de la construction d'EIAH orientés vers une approche socioconstructiviste de l'apprentissage qu'une problématique didactique peut être intéressante. De ce point de vue, nous considérons la construction des connaissances comme étant le résultat d'une interaction entre le sujet apprenant et son environnement, le milieu pour l'apprentissage (Brousseau, 1998). Ainsi, pour nous le milieu doit être organisé de façon à favoriser l'apprentissage : produire des rétroactions pertinentes en fonction des actions de l'apprenant sur le problème posé. En ce sens, le dispositif informatique d'aide à l'apprentissage devra également pouvoir réagir vers l'apprenant en fonction de ses actions à l'interface. Nous considérons que pour que les rétroactions de l'EIAH soient pertinentes au regard de l'apprentissage il faut que celui-ci réagisse en fonction d'une validation de la résolution proposée par l'apprenant en fonction d'un modèle des connaissances du domaine et non pas uniquement en fonction d'une solution experte déterminée a priori (Luengo, 1999).
En tant qu'élément de ce milieu, l'EIAH devra posséder certaines caractéristiques précisées par l'analyse de la connaissance qui est enjeu de l'apprentissage. Nos recherches touchent également au domaine de la formation professionnelle ; nous nous inscrivons ainsi dans l'approche de la didactique professionnelle (Pastre, 2002).
L'extraction de la connaissance
La construction d'un ensemble organisé de règles et de problèmes passe par une analyse des processus d'enseignement et d'apprentissage, par une analyse des connaissances, et par leur représentation. La méthodologie adoptée est structurée autour des points suivants.
Nous analysons et décrivons à partir de notre corpus d'observations la situation prescrite et la situation réelle. Ces deux facettes de l'activité sont analysées parallèlement, et s'enrichissent mutuellement (Pastré, 2002).
? La situation prescrite est analysée et décrite à partir à partir de cours et d'articles décrivant cette technique;
-664 -RNTI-E-6 ? La situation réelle professionnelle est analysée et décrite à partir d'observations de l'action du point de vue des interactions entre l'apprenant et l'expert : films et entretiens de verbalisation. Nous nous attachons dans nos analyses à faire apparaître les critères de validation qui sont sous-jacents aux actions et aux prises de décisions. C'est à ce niveau que se joue la conceptualisation de l'action. En particulier, nous identifions des critères de validation de l'action en situation qui n'apparaissent pas dans la situation prescrite. Ce sont des connaissances forgées par l'expert au cours de sa confrontation à la diversité des possibles de la situation (dans notre cas le vissage sacro-iliaque). Elles permettent à l'expert de faire face à la diversité des situations tout en conservant l'invariance globale de la résolution de l'activité (Vergnaud, 1996). Ce type de connaissances nous intéresse tout particulièrement puisque nous les intégrons dans le modèle de connaissances de notre environnement afin qu'il permette une réelle valeur ajoutée par rapport au déroulement actuel de la formation (théorie puis pratique sur le terrain) et comblant au moins partiellement, l'écart qui existe entre ces deux modalités de formation.
A partir de ces analyses (Vadcard, 2005), nous décrivons une famille de problèmes comme un ensemble de variables didactiques (voir tableau 1). Un champ de problèmes peut être engendré à partir d'une situation par la modification des valeurs de certaines variables qui, à leur tour, font changer les caractéristiques des stratégies de solution (coût, validité, complexité…etc.) Seules les modifications qui affectent la hiérarchie des stratégies sont à considérer (variables pertinentes). En d'autres mots nous nous intéressons aux variables avec lesquelles en agissant sur elles, on pourra provoquer des adaptations et des régulations : des apprentissages (Brousseau, 1998). 
TAB. 2 -Extrait de l'ensemble de problèmes classés à partir des variables didactiques.
Ensuite pour chaque famille de problèmes nous décrivons un ensemble d'opérateurs qui s'appliquent. Dans le modèle utilisé (Balacheff, 1995), nous appelons un opérateur (R) ce qui permet la transformation des problèmes ; ces opérateurs sont attestés par des productions et des comportements ou d'actions à l'interface. Enfin pour chaque opérateur nous identifions l'ensemble des contrôles (?) qui y sont associés. Une structure de contrôle contient les outils de décision sur la légitimité de l'emploi d'un opérateur ou sur l'état (résolu ou non) d'un problème. Le modèle de connaissance présenté permet de « poser des hypothèses sur l'état de connaissance qui est sous-jacent à une action de l'utilisateur sur l'interface de simulation. 
TAB. 2 -Extrait de l'ensemble d'opérateurs identifiés et des contrôles qui interviennent en fonction de la famille des problèmes PA.
Nous pouvons observer que dans le tableau qui précède nous différencions les contrôles selon leur nature. Les premiers contrôles (1 et 2) sont traités en tant que contrôles liés à la connaissance déclarative, car ils sont explicités et partagés, le deuxième type de contrôle (13 et 14) correspond à la connaissance qui est en partie tacite et qui se forge dans l'action. Cette classification n'est pas statique, elle peut évoluer dans le temps, mais elle nous permet en particulier de calculer la prise de décision didactique ou la forme de la rétroaction comme nous le verrons dans la suite.
La modélisation informatique
Les conceptions du domaine sont formalisées à l'aide de l'ensemble des problèmes, contrôles et opérateurs décrit antérieurement. Nous avons défini les relations de dépendance et de causalité entre les plusieurs ensembles afin de les représenter sous forme d'un réseau bayésien. Nous identifions dans la figure (1) les relations de dépendances suivantes : un problème P est résolu si les opérateurs associés R sont appliqués d'une manière valide. Un opérateur R est appliqué d'une manière valide si les contrôles ? associés et utilisés lors de la résolution de problème P sont valides. Un contrôle ? est valide si les traces des actions de l'apprenant VS (variables des situations) lors de la résolution sont cohérentes par rapport au contexte du problème P. Le réseau permette de diagnostiquer la connaissance mobilisée lors de la résolution de problème avec un degré d'incertitude. Actuellement le diagnostic permet de déduire l'état de contrôles utilisés dans la résolution de problème (valide ou non) en fonction, d'une part, du contexte du problème et, d'autres part, des actions de l'utilisateur dans l'interface de l'environnement (le simulateur de vissage sacro-iliaque). Le résultat de ce diagnostic est sous forme de probabilité sur les contrôles utilisés (Fig. 2).
Suite au diagnostic nous cherchons à calculer la décision didactique. Pour nous, une décision didactique se pose en termes d'états de connaissance diagnostiqués et visés. Cette prise de décision possède certaines caractéristiques, entre autres :
? Elle est dépendante du problème dans lequel se situe l'action qui est diagnostiquée, et des contrôles identifiés. Selon le type de contrôle la décision n'est pas la même. Un contrôle diagnostiqué qui est lié à des connaissances déclaratives renverra sur une partie d'un cours en ligne alors qu'un contrôle lié à des connaissances pragmatiques renverra sur un problème à résoudre. ? Elle est incertaine. Ce degré d'incertitude dépendra des informations disponibles lors de la prise de décision. Pour l'automatiser nous avons donc fait le choix de formaliser la prise de décision didactique sous la forme des diagrammes d'influence dans les réseaux bayésiens (Fig 2). Cette décision est ainsi calculée à partir des contrôles identifiés, grâce au diagnostic, et de l'instanciation du réseau par rapport aux actions de l'utilisateur et du problème traité. 
Conclusion
Du point de vue de l'architecture nous séparons le diagnostic de la prise de décision pour pouvoir les étudier et les valider séparément (Mufti-Alchawafa, et al. 2004). La condition pour qu'ils fonctionnent est sous-jacente au modèle : le diagnostic doit pouvoir identifier les contrôles qui sont intervenus dans une résolution, la prise de décision doit se faire en fonction des contrôles identifiés.
Nous travaillons actuellement sur la validation de notre représentation, sous forme de réseau bayésien. Du point de vue informatique, nous utilisons la notion de complétude, et d'adéquation de la représentation vis-à-vis du modèle. Ensuite, si la représentation est validée nous travaillerons sur l'optimisation des algorithmes pour la prise de décision dans le versant informatique et sur l'analyse des formes de rétroaction pour le versant didactique.
Le composant qui représente la prise de décisions sera ainsi un outil de test pour la didactique et la psychologie cognitive qui permettra l'analyse des différentes formes de rétroactions épistémiques dans environnement informatique d'apprentissage humain. Le but final étant de produire des rétroactions épistémiques adaptées et centrées sur l'activité du sujet, à l'interface, en situation d'apprentissage.

Introduction
Pour la recherche, le partage et l'échange de ressources (données, programmes, services), le modèle pair-à-pair constitue une alternative au modèle client/serveur. Les pairs peuvent à la fois offrir (rôle serveur) et demander (rôle client) des ressources. Il existe de nombreuses architectures des systèmes pair-à-pair, se basant sur des techniques différentes de localisation des données, qui se traduisent par des méthodes différentes de routage des requêtes. Pour améliorer la localisation d'une ressource recherchée par un pair, on ajoute de l'information aux tables de routage des requêtes : il peut s'agir du contenu des pairs, de l'historique de leurs requêtes, ou des concepts qu'ils traitent... La difficulté rencontrée lors de l'intégration de la sémantique du contenu des pairs, est de déterminer un espace de représentation commun à tous les pairs du réseau. Quelques systèmes tels que SON (Semantic Overlay Network) (Crespo et al., 2002) utilisent des concepts définis à priori pour résoudre ce problème. Mais cette solution ne s'applique qu'à un domaine précis.
Pour pallier cet inconvénient, PlanetP (Cuenca-Acuna et al., 2002) utilise une signature séman-tique pour représenter le contenu de chaque pair. Cette signature est définie par une structure de données appelée filtre de Bloom (Bloom, 1970).
Notre travail s'inscrit dans le cadre du projet RARE mené au sein du GET (Groupement des Ecoles des Télécommunications). Dans ce projet, plusieurs approches sont étudiées comme l'utilisation des filtres de Bloom, la propagation efficace des index via des algorithmes de Gossiping ou encore l'apprentissage sur les requêtes passées par des mémoires associatives. Dans ce papier, nous nous intéressons à l'utilisation des filtres de Bloom dans PlanetP et proposons une amélioration permettant de réduire la taille des filtres de Bloom, et par conséquent de faciliter leur diffusion à travers le réseau pair-à-pair, tout en maintenant les performances de la recherche d'information. L'approche est validée par des expérimentations sur les collections de données suivantes : CACM, CISI, MED et CRAN de SMART (Buckley, 1985).
Les sections 2 et 3 de cet article décrivent l'approche PlanetP et le fonctionnement du filtre de Bloom. La section 4 définit notre approche et la section 5 présente les différentes expérimentations menées.
Filtres de Bloom
Définition
Un filtre de Bloom (Bloom, 1970)  Pour introduire un terme dans un filtre de Bloom, on calcule les valeurs des fonctions de hachage et on active (on met à 1) les bits du vecteur correspondants.
Pour tester l'appartenance d'un terme t à un ensemble Y de termes introduits dans le filtre de Bloom, on lui applique les fonctions de hachage. Si au moins un des bits est à 0 alors le terme t n'appartient pas à Y. Par contre, si tous les bits sont à 1 alors t appartient probablement à Y avec un taux de faux positif moyen donné par la relation suivante :
Où m est la taille du vecteur du filtre de Bloom, n le nombre de termes indexés et k le nombre de fonctions de hachage. Pour minimiser ce taux, on choisit un nombre de fonctions de hachage k respectant la relation suivante : 
Recherche dans PlanetP
La recherche d'information est basée sur le modèle vectoriel. PlanetP propose une approximation à la mesure TFxIDF, qui nécessiterait une connaissance de tous les mots du réseau. Cette mesure adaptée aux P2P est l'Inverse Peer Frequency IPF, pouvant être calculée à l'aide des informations locales à chaque pair. La mesure IPF pour un terme t est donnée par :
Où N est le nombre de pairs connus dans le réseau par le pair effectuant la recherche et N t le nombre de pairs parmi ceux-ci ayant les documents contenant t. Un noeud qui reçoit une requête cherche dans son index local. S'il ne peut pas honorer la requête, il calcule les rangs des pairs de son index global. Pour donner un rang aux pairs, on utilise l'expression suivante :
Où Q est la requête, BF i le filtre de Bloom du pair i et t un terme de la requête. La requête est alors propagée aux pairs de plus grand rang.
Filtre de Bloom dynamique
Il existe des pairs qui contiennent plus de documents que d'autres. Généralement, la ré-partition des documents dans les pairs suit une loi de Zipf (Goh et al., 2005 -sid i ? 81 : quatre fonctions de hachage et un filtre de Bloom de 10 000 bits ; Cette méthode nous permet de réduire d'environ 50% la taille des filtres de Bloom, ce qui facilitera leur diffusion à travers le réseau. Le fait de réduire la taille des filtres de Bloom n'affecte pas le taux de faux positif car le nombre de fonctions de hachage a été choisi selon l'équation ( 2).
Expérimentations et résultats
Pour nos expériences, nous avons utilisé les quatre collections de documents utilisées par PlanetP pour son évaluation (requêtes et jugements de pertinence associés). La table 1 présente le contenu de ces collections : elles sont composées de fragments de textes et résumés, et sont relativement petites en taille. Nous les avons préalablement traitées grâce à l'outil de recherche d'information SMART (Buckley, 1985), afin d'extraire les mots lemmatisés, leurs fréquences et d'éliminer les mots vides.
Pour tester les performances de notre approche, nous avons utilisé les métriques standard, rappel(R) et précision (P), définies comme suit pour une requête Q : R(Q)= nombre de documents pertinents retournés/nombre de documents pertinents dans la collection P(Q)= nombre de documents pertinents retournés/nombre de documents retournés 
Où f D,t est le nombre d'apparitions du terme t dans D et |D| le nombre de termes dans D. Pour mesurer le rappel et la précision dans les collections, nous distribuons aléatoirement les documents sur 20 pairs virtuels selon la loi de Zipf sans redondance (un document n'est présent que sur un seul pair). Nous construisons la signature de chaque pair en utilisant chacune des méthodes suivantes :
-Filtre de Bloom fixe construit avec deux fonctions de hachage (comme dans le système PlanetP) avec une taille de 10 000 bits ; -Filtre de Bloom variable. Nous utilisons les configurations de l'exemple de la section 4. La taille des filtres de Bloom a été choisie afin d'assurer un taux de faux positif inférieur à 5%. Le rang des pairs est calculé par l'équation (4) pour chacune des requêtes des collections puis ceux-ci sont triés par ordre décroissant de la mesure du rang. Ensuite p pairs jugés pertinents sont sélectionnés, p variant de 1 à 20 pairs. Les documents contenus dans ces p pairs sont triés par la mesure de similarité selon l'équation (5). Nous extrayons les 30 documents de similarités les plus élevées et mesurons le taux de rappel et de précision. Par manque de place, nous ne présentons dans la figure 1 que les courbes obtenues pour la collection MED, des résultats similaires étant obtenus sur les autres collections. La figure 1 montre le taux de rappel et de précision en fonction du nombre de pairs contactés. Nous observons à travers ces courbes que l'utilisation d'un filtre de Bloom dynamique n'altère pas les taux de rappel et de précision par rapport à un filtre de Bloom fixe.
Conclusion
Le système PlanetP permet de faire de la recherche textuelle de documents dans un environnement distribué, en proposant un même espace de représentation partagé par tous les pairs, sous la forme de filtres de Bloom de taille fixe. Nous avons proposé de rendre cette taille dynamique, en fonction du nombre de documents stockés par chaque pair. Les expérimentations montrent que la dynamicité ne détériore pas les performances obtenues dans le cas classique (filtre de Bloom fixe). La bande passante utilisée pour la propagation des filtres de Bloom peut ainsi être réduite, ou bien le taux de leur diffusion à travers le réseau peut être augmenté, afin d'enrichir les index globaux. L'évaluation chiffrée de ces gains reste à réaliser, par simulation des échanges entre pairs, en utilisant le simulateur également développé dans le projet RARE.

Introduction
Dans un certain nombre de domaines (détection de fraudes, de défaillances, analyse de comportements), la recherche de connaissances temporelles est non seulement utile mais né-cessaire. Certaines techniques d'apprentissage permettent de gérer et de raisonner sur de telles connaissances, (Allen, 1990) a notamment défini des opérations sur des règles associées à des intervalles de temps. Des techniques d'extraction de connaissances cherchent quant à elles à extraire des épisodes récurrents à partir d'une longue séquence (Mannila et al., 1997), (Raissi et al., 2005) ou de bases de séquences (Agrawal et Srikant, 1995), (Masseglia et al., 1998). La recherche de telles informations devient d'autant plus intéressante qu'elle permet de prendre en compte un certain nombre de contraintes entre les évènements comme par exemple la durée minimale ou maximale séparant deux évènements.
C'est dans ce cadre qu'a été introduite la recherche de motifs séquentiels généralisés dans (Srikant et Agrawal, 1996). Cette technique de fouille de données permet d'obtenir des sé-quences fréquentes respectant des contraintes spécifiées par l'utilisateur, à partir d'une base de données de séquences (par exemple les achats successifs de différents clients d'un supermarché). Différents algorithmes ont été proposés afin de gérer ces contraintes soit directement dans le processus d'extraction, (GSP, Srikant et Agrawal (1996)) soit à l'aide d'un pré-traitement sur les séquences proposé dans GTC (Graph for Time Constraint), (Masseglia et al. (1999)).
Toutefois, si ces méthodes sont efficaces et robustes, elles ont pour principal inconvénient d'être spécifiées par l'utilisateur et nécessitent donc une bonne connaissance a priori des données et des durées à spécifier sous peine d'obtenir des connaissances peu pertinentes. Des travaux ont été proposés afin de déterminer de manière automatique la fenêtre optimale d'observation pour la recherche d'épisodes dans une séquence (Meger et Rigotti, 2004), mais ils sont difficilement adaptables à l'extraction de motifs séquentiels et dans ce domaine, aucun travail à notre connaissance ne propose une détermination automatique des contraintes de temps optimales. Par ailleurs, pour certaines applications il pourrait également être intéressant d'assouplir les contraintes spécifiées par les experts du domaine afin de compléter leurs connaissances. Enfin, le nombre de motifs séquentiels extraits, selon les contraintes de temps utilisés, peut rapidement devenir trop important pour que leur analyse soit efficace. Une mesure permettant l'exploitation des motifs séquentiels généralisés serait donc d'une grande utilité.
C'est pourquoi nous proposons d'étendre les contraintes de temps proposées pour l'extraction de motifs séquentiels généralisés en utilisant certains principes de la théorie des sousensembles flous. Notre méthode permet, en effet, à partir de contraintes de temps initiales et d'un degré de respect de ces valeurs, d'extraire des motifs séquentiels respectant des contraintes étendues et de fournir pour chacun d'eux sa précision temporelle. Nous offrons ainsi à l'utilisateur une flexibilité dans la spécification de ses contraintes ainsi qu'un outil d'analyse des nombreuses séquences fréquentes extraites.
Après avoir présenté les concepts fondamentaux associés aux motifs séquentiels et aux motifs séquentiels généralisés dans la section 2, nous présentons dans la section 3 notre définition des contraintes de temps étendues. La section 4 présente notre proposition d'algorithme mettant en oeuvre la gestion des contraintes de temps étendues lors du prétraitement des données. La section 5 présente ensuite quelques expérimentations montrant la faisabilité et l'efficacité de notre approche. Enfin, la section 6 conclut sur les perspectives qu'ouvrent nos travaux.
Des motifs séquentiels aux motifs séquentiels généralisés
Motifs séquentiels
Les motifs séquentiels ont initialement été proposés par Agrawal et Srikant (1995) et reposent sur la notion de séquence fréquente maximale.
Prenons par exemple une base de données DB d'achats pour un ensemble C de clients c. Une transaction t est un triplet <id_client, id_date, itemset> qui caractérise le client qui a réalisé l'achat, la date d'achat et les items achetés. Soit I = {i 1 , i 2 , · · · , i m } l'ensemble des items de la base. Un itemset est un ensemble non vide et non ordonné d'items, noté (i 1 , i 2 , · · · , i k ). Une séquence se définit alors comme une liste ordonnée non vide d'itemsets s i qui sera notée < s 1 s 2 · · · s p >. Une n-séquence est une séquence de taille n, c'est-à-dire composée de n items. -604 -RNTI-E-6 C. Fiot et al.
Les transactions de la base sont regroupées par client et ordonnées chronologiquement, dé-finissant ainsi des séquences de données. Un client c supporte une séquence S si elle est incluse dans la séquence de données du client c. Le support d'une séquence est alors défini comme le pourcentage de clients de la base DB qui supporte S. Une séquence est dite fréquente si son support est au moins égal à une valeur minimale minSupp spécifiée par l'utilisateur.
La recherche de motifs séquentiels dans une base de séquences telle que DB consiste alors à trouver toutes les séquences maximales (non incluses dans d'autres) dont le support est supérieur à minSupp. Chacune de ces séquences fréquentes maximales est un motif séquentiel.
Motifs séquentiels généralisés
Telle qu'elle a été introduite ci-dessus, la notion de séquence présente une certaine rigidité pour de nombreuses applications. En effet, si l'intervalle de temps entre deux transactions successives est très court, on pourrait envisager de les considérer comme simultanées. A l'inverse, deux évènements trop éloignés peuvent ne pas avoir de lien entre eux. C'est pourquoi la notion de séquence généralisée a été proposée par Srikant et Agrawal (1996) afin de pallier ces restrictions en introduisant la prise en compte de contraintes temporelles.
Ces contraintes sont au nombre de trois : mingap est une durée minimale que l'on doit respecter entre deux itemsets successifs d'une même séquence ; maxgap est l'écart maximal dans lequel doivent se trouver deux itemsets successifs ; windowSize est la fenêtre dans laquelle les items de deux transactions différentes peuvent être regroupés dans un même itemset. On modifie alors la notion d'inclusion décrite précédemment pour tenir compte de ces contraintes.
Une séquence de données  
Vers des contraintes de temps étendues
Dans cette section, nous allons examiner la mise en oeuvre des contraintes de temps éten-dues par analogie avec la théorie des sous-ensembles flous pour ensuite proposer leur définition et celle de la précision temporelle des séquences soumises à de telles contraintes.
Mise en oeuvre
La théorie des sous-ensembles flous, introduite par Zadeh (1965) autorise l'appartenance partielle à une classe et donc la gradualité de passage d'une situation à une autre. Cette théorie constitue une généralisation de la théorie ensembliste classique, des situations intermédiaires entre le tout et le rien étant admises. Dans ce cadre, un objet peut donc appartenir partiellement à un ensemble et en même temps à son complément.
On considère par exemple l'univers des tailles possibles d'un individu. Un sous-ensemble flou A (Petit ou Grand par exemple) est défini par une fonction d'appartenance µ A qui décrit le degré avec lequel chaque élément de l'univers considéré appartient à A, ce degré étant compris entre 0 et 1. Ainsi, un individu de 1m63 pourra à la fois être grand et petit avec, par exemple, un degré de 0.7 pour le sous-ensemble flou Grand et 0.3 pour le sous-ensemble flou Petit.
Les opérateurs en logique floue sont une généralisation des opérateurs classiques. On considère notamment la négation, l'intersection et l'union. L'opérateur ou t-norme (norme triangulaire) est l'opérateur binaire d'intersection :
Nous noterons (resp. ?) l'opérateur (resp. ?) généralisé au cas n-aire.
Notre proposition d'extension des contraintes de temps pour les motifs séquentiels est fondée sur une analogie avec la théorie des sous-ensembles flous. Ainsi, on ne souhaite plus simplement qu'une séquence respecte des contraintes spécifiées mais permettre à l'utilisateur de relaxer ces contraintes jusqu'à un certain seuil. Ce seuil correspond à un degré minimal de satisfaction des contraintes temporelles. Il sera spécifié par l'utilisateur pour chacune des contraintes, de même que leur valeur initiale. Soit ws, g et G ces valeurs initiales pour les contraintes windowSize, minGap et maxGap. On considère ? ws , ? mg et ? M G les niveaux de précision associés à chacune d'elles, ces niveaux étant compris entre 0 et 1, 0 indique alors que l'on souhaite parcourir l'ensemble des valeurs possibles et 1 que le paramètre correspondant est fixe. L'utilisation de tels degrés implique de considérer plusieurs valeurs pour les paramètres windowSize, mingap et maxgap. Il est alors possible de considérer plusieurs "chemins" ou séquences dans les achats des clients pour reconstruire un motif. Cependant chacune de ces séquences respecte "plus ou moins" la contrainte initiale ce qui est évalué par les niveaux de précision ? ws , ? mg et ? M G dont le calcul est détaillé au paragraphe 3.3.
Extension des contraintes de temps
La définition des contraintes de temps étendues est basée sur les valeurs limites utiles que les différents paramètres peuvent prendre. Ces valeurs utiles correspondent aux valeurs limites au-delà desquelles on ne pourra générer de séquences candidates respectant les contraintes.
Prenons l'exemple de windowSize. Dans le cas classique, windowSize prend une valeur fixe ws et la condition (2.2) signifie que date(
Reprenons l'ensemble des clients C. Pour chaque client c, ses transactions ont des dates comprises entre 
FIG. 2 -Variation de la précision selon la valeur de windowSize
De même, on a pour maxgap :   
Précision temporelle d'une séquence
Pour chacune des séquences fréquentes trouvées à la fin du processus d'extraction, on calcule la précision avec laquelle chacune d'elles respecte les contraintes de temps. On définit la précision d'une séquence s pour un client c comme le degré de respect simultané des trois contraintes de temps (1), (2) et (3), calculé à l'aide d'une t-norme (). Pour chaque client, on cherche, parmi toutes les séquences d'achats ? c , l'occurrence de s qui respecte au mieux les contraintes de temps, en utilisant une t-conorme (?).
La précision temporelle d'une séquence s =< s 1 · · · s n > pour le client c est donnée par :
Pour l'ensemble de la base, le degré de respect des contraintes de temps est donné par la moyenne des degrés de chacun des clients, c'est-à-dire :
c?C
GETC -Graph for Extended Time Constraints
L'algorithme GTC proposé dans (Masseglia et al., 1999) permet de transformer une sé-quence d'un client en un graphe de séquences respectant les contraintes de temps. Les graphes de séquences des différents clients sont ensuite utilisés pour déterminer les séquences fré-quentes par un algorithme d'extraction de motifs séquentiels tel que PSP Masseglia et al. (1998). L'efficacité de cette approche ayant été démontrée dans (Masseglia et al., 1999), nous avons choisi de nous en inspirer pour développer la nôtre. Nous proposons donc un algorithme permettant de construire un graphe de séquences pour les contraintes de temps étendues et également de calculer la précision des motifs séquentiels généralisés extraits.
GETC -les algorithmes
A partir d'une séquence d'entrée d, l'algorithme GETC construit son graphe de séquences G d . Cet algorithme regroupe un certain nombre de sous-fonctions (addEdge, propagate, pruneM arked et convertEdges), non détaillées ici, qui sont présentées de manière plus approfondies dans (Fiot et al., 2005a).
GETC commence par créer les sommets correspondant aux itemsets de la séquence puis ajoute à l'ensemble des sommets l'ensemble des combinaisons d'itemsets permises selon les différentes valeurs de windowSize. Pour cela, l'algorithme addW indowSize (non présenté ici) parcourt chaque sommet x et détermine pour chacun d'entre eux quels autres sommets y peuvent être "fusionnés" avec x (si y.date() -x.date() ? ws). Chaque sommet correspond 
ALG. 1: GETC alors à un itemset x.itemset() qui possède une date de début x.begin() et une date de fin x.end(). Ces sommets sont regroupés en niveau par date de fin des itemsets. Cela permet de tester le respect des contraintes pour un niveau et non pour chaque sommet. On accède à l'ensemble des prédécesseurs d'un sommet x par x.prev() et à ces successeurs par x.succ().
Ensuite pour chacun des sommets du graphe de séquences, GETC ajoute les arcs respectant les contraintes minGap et maxGap. Ainsi, pour chaque sommet, on cherche le premier niveau accessible pour la contrainte minGap (ie. l.begin() -x.end() > g ? ) et pour chaque sommet z de ce niveau, on construit les arcs (x,z), pour chaque sommet z tel que z.end() -x.begin() ? maxGap. La fonction addEdge permet d'éviter les inclusions de chemins, grâce à la construction d'arcs temporaires dans des cas d'inclusions possibles.
L'algorithme addEdge construit les arcs entre des sommets qui respectent les contraintes de temps sur minGap et maxGap. Il crée un arc définitif si les sommets ne sont pas déjà liés par une séquence ou une inclusion de leurs successeurs ou prédécesseurs. Dans ce cas, l'arc construit est temporaire et ne devient définitif que si la séquence qu'il forme est maximale. C'est également lors de l'exécution de cet algorithme que les sommets inclus sont marqués, pour pouvoir ensuite être supprimés s'ils sont inutiles.
Dans le cas où pour un sommet x, on ne peut atteindre le niveau l à cause du non respect de la contrainte mingap, on utilise la fonction propagate pour "propager ce saut". Pour chacun des sommets y de ce niveau inaccessible, on ajoute si nécessaire et si on respecte les contraintes minGap et maxGap, un arc entre chacun des successeurs de x et ce sommet y. Comme dans addEdge, on construit des arcs temporaires ou définitifs selon que la séquence construite peut éventuellement être incluse ou au contraire n'a aucune chance de l'être.
Enfin, l'algorithme pruneM arked élimine les sommets de sous-séquences incluses. Puis, convertEdges supprime les arcs de sous-séquences incluses : pour tout arc temporaire de x vers y, si y est inclus dans un successeur z de x et si les successeurs de y sont également tous des successeurs de z, alors il existe une sous-séquence incluse, l'arc est supprimé. Sinon, l'arc est indispensable pour obtenir toutes les séquences maximales.
GETC étant utilisé comme prétraitement pour la prise en compte de contraintes temporelles en vue de l'extraction de motifs séquentiels, il doit générer absolument toutes les séquences issues d'une séquence de données. Par ailleurs, afin d'améliorer le temps d'extraction, il est nécessaire que GETC n'extraie que les séquences maximales, nous avons donc montré dans (Fiot et al., 2005a) que l'algorithme GETC construit exactement toutes les séquences de la plus grande taille possible pour les séquences respectant windowSize, minGap et maxGap.
Construction du graphe de séquences
Nous utilisons GETC comme prétraitement pour la prise en compte des contraintes de temps étendues et PSP (Masseglia et al., 1998) pour l'extraction des motifs séquentiels. Cette approche du type générer-élaguer utilise une structure d'arbre prefixé pour organiser les sé-quences candidates et permettre de trouver plus efficacement l'ensemble des candidats inclus dans une séquence de données. En utilisant ainsi le graphe de séquences obtenu par GETC, la vérification des contraintes de temps est rendue inutile pendant le parcours des candidats, seule l'inclusion devant être vérifiée. Cette méthode est similaire à celle proposée dans (Masseglia et al., 1999) qui permet d'optimiser l'extraction de motifs séquentiels généralisés grâce à un parcours pour les contraintes de temps indépendant et sans retour arrière de l'arbre des séquences candidates et ainsi la vérification du moins de combinaisons possibles.   (1) (2 3) (4) (4) (5) (6)
18
(1 2 3)
( 5 6) (1) (2 3) (3 4) * (4) (4) (5) (6) (7) * (8) * 
Calcul de la précision temporelle d'une séquence
Une fois le graphe de séquences construit, on connaît toutes les séquences autorisées par les contraintes de temps et celles qui sont interdites. Cependant, certaines séquences respectent les contraintes fortes de l'utilisateur alors que d'autres ont été construites en appliquant les contraintes étendues, elles ne sont donc pas équivalentes. On calcule alors la précision de chacun des chemins (séquences maximales) et on l'affecte aux sous-séquences qui le composent.
Afin de déterminer l'appartenance du chemin vis-à-vis des contraintes de temps, on value chaque arc (x,y) par (µ mg (y.begin()-x.end()),µ M G (y.end()-x.begin())). Chaque sommet est valué par windowSize. La précision d'une sous-séquence, et donc le degré de respect des contraintes de temps est alors donnée par la formule (5). Grâce à l'algorithme valueGraph (non présenté ici), le graphe est alors parcouru et à chaque sommet s, on attribue la valuation µ ws (s.end()-s.begin()) et à chaque arc entre s et t, la valuation (µ mg (t.begin()-s.end()),µ M G (t.end()-s.begin())).
Exemple 6. A partir de la base de données TAB. 2 et des contraintes de temps spécifiées dans l'exemple 5, on construit les trois graphes de séquences correspondants. En prenant minSupp=70%, on obtient six séquences maximales fréquentes : <(2 3 4)>, <(2 3)(4)(5 6)>, <(2)(4 5)>, <(3 4)(5)>, <(3 4)(6)> et < (3)(4 5  
Expérimentations
Ces expérimentations ont été réalisées sur un PC équipé d'un processeur 2,8GHz et de 2Go de mémoire DDR, sous systèmes Linux, noyau 2.6. Le but de ces mesures est de comparer le comportement de GETC par rapport à GTC. Pour certaines comparaisons, nous utilisons également une implémentation de PSP, intégrant ou non la gestion des contraintes de temps. Les résultats présentés ici ont été obtenus à partir du traitement de plusieurs jeux de données synthétiques comportant environ 1000 séquences de 20 transactions en moyenne. Chacune de ces transactions comportant en moyenne 15 items choisis parmi 1000.
La première étape a consisté à comparer les temps d'exécution en l'absence de contrainte de temps, c'est-à-dire en prenant windowSize = 0, minGap = 0 et maxGap = ? pour GTC ainsi que pour GETC, avec une précision de 1 pour les trois paramètres. Ainsi, nous avons pu comparer le temps d'exécution de notre algorithme avec ceux de PSP et GTC. La figure FIG. 6(a) montre que le comportement de GETC est similaire à celui de GTC et que les temps d'exécution sont quasiment identiques, pour des motifs extraits qui sont les mêmes.
Nous avons ensuite répété ces mesures en introduisant le traitement de contraintes de temps, toujours avec une précision de 1 afin de comparer le comportement de GETC et GTC pour la gestion de contrainte de temps non-étendues. La FIG. 6(b) montre l'évolution du temps d'extraction en fonction de la valeur de windowSize. GETC a un comportement linéaire proche de celui de GTC. La différence provient de la phase de traitement de la précision, dont le temps augmente légèrement avec windowSize, puisque si ce paramètre augmente, on augmente le nombre de sommets dans le graphe de séquences.
La deuxième partie de nos expérimentations a porté sur l'analyse des motifs séquentiels extraits par GETC en fonction de la précision des différentes contraintes. La figure FIG. 6(c) présente les temps d'extraction comparés de GTC et GETC en fonction du support minimum selon des valeurs choisies des différents paramètres. Ces valeurs ont été calculées afin que les contraintes de temps utilisées pour GTC et GETC avec une précision de 1 correspondent aux valeurs limites de GETC avec une précision différentes de 1. Les paramètres retenus sont :
-GETC avec windowSize=0, minGap=1 et maxGap=5 avec une précision de 0.75, qui nous donne ws ? = 4, mg ? = 0 et M G ? = 10, -GETC avec windowSize=4, minGap=0 et maxGap=10 avec une précision de 1, -GTC avec windowSize=4, minGap=0 et maxGap=10, On peut constater que l'utilisation de GETC avec des contraintes de temps étendues n'est pas plus coûteuse que celle de GTC avec les valeurs limites des contraintes, tout en permettant d'obtenir les mêmes motifs séquentiels, accompagnés de leur précision temporel. Il est inté-ressant, dans le cas où on ignore la valeur optimale d'une ou plusieurs contraintes de temps, d'utiliser GETC avec une précision différente de 1 pour certains paramètres, afin de balayer un ensemble de possibilités plus large. L'analyse des motifs obtenus et de leur précision pourra renseigner sur une valeur plus adéquate des contraintes de temps. Enfin, la FIG. 6(d) montre l'évolution du temps d'extraction en fonction de la précision pour un support minimum de 0.37. On constate que le temps d'extraction atteint une valeur limite qui correspond à la valeur maximale utile des trois paramètres de contraintes de temps. 
Conclusion et perspectives
Les motifs séquentiels généralisés présentés par Srikant et Agrawal (1996) permettent une définition plus large de l'inclusion en introduisant l'utilisation de contraintes de temps. Toutefois, cette définition reste encore trop rigide, notamment dans le cas où l'utilisateur n'a qu'une vague idée des contraintes temporelles qui lient ses données. Dans cet article nous proposons donc une extension des contraintes de temps pour les motifs séquentiels généralisés, qui permet plus de souplesse dans la spécification des paramètres de contraintes temporelles. Notre approche se base sur la construction de graphes de séquences pour intégrer les contraintes de temps dans le processus d'extraction de motifs séquentiels. La faisabilité et la robustesse de cette méthode ont été montrées pour GTC dans Masseglia et al. (1999). Le principe de GETC étant le même, nous avons pu montrer son efficacité pour résoudre le probléme de la recherche de séquences généralisées avec des contraintes de temps, étendues ou non, et la similitude de son comportement avec celui de GTC. Nous avons également pu mettre en évidence la flexibilité offerte par la mise en place des contraintes étendues, il nous reste encore à en valider la robustesse. Enfin, nous envisageons d'étendre les motifs séquentiels flous présentés dans Fiot et al. (2005b) à des motifs séquentiels généralisés, avec contraintes de temps étendues ou non.

Introduction
Pour améliorer l'efficacité des algorithmes de classification, il existe plusieurs algorithmes de préparation des données, dont la désuffixation. Cependant, le langage médical, et les comptes rendus hospitaliers sont rédigés dans un langage très technique, avec peu de formes flexionnelles. Nous nous sommes demandés si l'implémentation d'un algorithme de désuf-fixation dans ce contexte pouvait améliorer significativement les résultats obtenus. Nous avons mis en évidence qu'il était possible d'obtenir de meilleurs résultats que les algorithmes actuels d'une part en développant un algorithme spécifique basé sur un large corpus de documents, d'autre part en enrichissant ces derniers en fonction des racines lexicales des termes médicaux.
Plusieurs algorithmes de désuffixation ont été proposés, les plus célèbres d'entre eux étant Porter (1980), Lovins (1968) et Paice (1996. Malheureusement, il s'agit d'algorithmes de désuffixation pour la langue anglaise, dont les dérivés morphologiques se prêtent facilement à ce type d'adaptation.
Présentation de l'algorithme EDA et résultats
Afin d'améliorer les performances des algorithmes de classification de comptes rendus hospitaliers (projet Rhea), nous proposons une technique de désuffixation qui donne des résultats intéressants dans le contexte médical. Nous nous sommes constitué une base de 29 393 comptes rendus, tous utilisés dans cette étude. Par ailleurs, la terminologie médicale possède une structure sémantique forte. Jujols (1991).
L'algorithme EDA fonctionne en deux phases. La première phase consiste à préparer le mot en appliquant quelques modifications (transformation en minuscules, séparation des caractères ligaturés, suppression des signes diacritiques, etc.). La seconde phase consiste à enrichir le corpus de textes en fonction des structures sémantiques des termes (par exemple : foie=hépat, langue=glosso, rate=spléno, coeur=cardio,…).
Pour expérimenter nos résultats, nous avons choisi d'utiliser Naïve Bayes comme algorithme de classification, et la F-mesure pour l'évaluation. Ce qui donne les résultats suivants :
Désuffixation
Résultat (F-mesure) Aucune désuffixation 69.23% Désuffixation avec Carry 72.27% Désuffixation avec EDA 74.72%
TAB. 1 -Gains sur la F-mesure selon la méthode utilisée.
Conclusion et perspectives
Sur 25 275 termes différents présents dans 30 000 comptes rendus, 10 602 ont été regroupés, soit 42%. L'utilisation de cet algorithme de désuffixation nous a permis de mesurer une amélioration de 5.49 %. Les deux tiers du gain résultent de la désuffixation, le dernier tiers de l'enrichissement des documents par la recherche de racines lexicales des termes mé-dicaux.

Introduction
Notre travail s'inscrit dans le contexte du projet européen SEMIDE (Système euro mé-diterranéen d'information sur les savoir-faire dans le domaine de l'eau). Le SEMIDE vise à développer une ontologie spécifique aux connaissances dans le domaine de l'eau. Ce travail s'est basé dans un premier temps sur un thésaurus du domaine de l'eau, or les ressources d'informations ne cessent de s'accroître de sources hétérogènes dans les formats, mais aussi dans le vocabulaire employé (agences de l'eau, ministères,...) engendrant une ontologie insuffisante et peu structurée. Cette ontologie doit pouvoir s'enrichir au fur et à mesure que de nouveaux documents apparaissent, mais également rester cohérente.
Nous nous intéressons à deux grandes parties : lŠannotation des ressources et l'enrichissement de l'ontologie globale définie par la communauté du SEMIDE. Ces deux grandes parties ne sont pas indépendantes étant donné que l'enrichissement de l'ontologie est fonction des nouvelles ressources et des concepts obtenus lors de l'annotation. La suite de cet article traitera la deuxième partie.
Notre hypothèse est qu'il serait intéressant de rajouter des relations ontologiques (est-un, partie-de, etc.) à l'ontologie du SEMIDE. Celle-ci prendrait donc la forme d'un pseudo-réseau sémantique ou les noeuds seraient des acceptions. Cependant, nous ne concevons la mise en place d'un tel réseau sémantique que via une automatisation poussée. La validation de certaines occurrences de relations entre acceptions pouvant être éventuellement l'objet d'un travail manuel d'un expert. Cette automatisation peut être envisagée à partir de deux types de sources : des corpus monolingues d'un même domaine technique, et des collections de bi (ou tri)-textes (textes traductions l'un de l'autres). Ce faisant, les occurrences de relations doivent d'abord être identifiées dans les parties monolingues avant d'être migrées dans la partie interlingue.
Nous attaquons le problème de l'enrichissement ontologique selon deux biais. La premier, via l'exploitation de paires de textes traduits, est la mise en correspondance directe de terme identifiés contre traduction mutuelle. Une acception (un sens de mot) peut être artificiellement créée, mais le problème des doublons potentiels et de l'identification et élimination n'est pas directement résolu. La seconde approche, à partir de corpus monolingue, consiste pour des termes cibles, à extraire le plus grand nombre des relations qu'ils peuvent entretenir avec d'autres mots. Les termes cibles sont identifiés comme tels via des méthodes classique de -709 -RNTI-E-6 
Extraction de nouvelles relations -patrons d'extraction
Notre travail a consisté dans un premier temps à analyser des documents du Semide afin d'extraire des mots clés qui définiront nos règles d'extraction, cette analyse a donné une liste d'hypothèses d'extraction de relations entre les termes que nous définissons dans ce qui suit. Hypothèse 1 : Si l'expression A est un B où A appartient à l'ontologie du Semide alors B est une spécialisation de A dans l'ontologie. Si par ailleurs, B appartient à l'ontologie globale alors B est une généralisation de A. Hypothèse 2 : Si l'expression C qui a la forme suivante : A de B où A appartient à l'ontologie du Semide alors C est une spécialisation de A dans l'ontologie. Si, par ailleurs, C appartient à l'ontologie globale alors A est une généralisation de C. Hypothèse 3 : Si l'expression C qui a la forme suivante : A B où A appartient à l'ontologie du Semide alors C est une spécialisation de A dans l'ontologie. Si par ailleurs, C appartient à l'ontologie globale alors A est une généralisation de C. Hypothèse 4 : Si on a l'expression C avec la forme suivante A non B où A appartient à l'ontologie du Semide alors C est une spécialisation de A dans l'ontologie. Et si C appartient à l'ontologie globale alors A est une généralisation de C. Les quelques patrons d'extraction présentés ci-dessus ne sont qu'indicatifs de la méthode employée. D'autres patrons sont utilisés, en particulier pour extraire des relations d'autres natures. Par exemple, la relation de méronymie (partie de) est extraite des corpus afin de structurer l'ontologie, et de déterminer le plus finement possible les cas de doublons. Les doublons sont des termes identifiés comme des concepts synonymes et doivent être représentés comme tels dans l'ontologie.
Summary
The description of resources inside a community (or domain) must follow a controlled vocabulary. This is precisely a set of terms defined by a working group in order to tag contents and describe documents. Our problem at hand is slightly different from classical issues in controlled vocabulary as we focus ourselves on relations that may exist between concepts. Still, our resource description is based on ontology. The ontology is the backbone of a controlled and organized vocabulary and corresponds to the formalization of explicit relations created between terms of the vocabulary. Our work sticks to two main directions which are the resources annotations and the global ontology enhancement as defined by the SEMIDE community. The EMWIS (SEMIDE) is an organization viewed as a tool for exchanging information and knowledge on water between countries of the Euro-Mediterranean Partnership.

Introduction
Il est communément admis que le temps de préparation des données peut occuper jusqu'à 80% du temps lors d'un projet industriel de fouille de données (Pyle, 1999). L'hétérogénéité des sources, la présence de valeurs manquantes, les erreurs de saisie ou de calcul, les pannes de capteurs, une mauvaise fusion de données sont autant de causes qui peuvent introduire erreurs et incohérences dans une table de données. ESIEA Datalab est une plateforme évolu-tive programmée en Java qui met à disposition de nombreux outils pour aider à la détection d'incohérences, la correction d'erreurs, la transformation ou la contrainte de variables, etc.
Le concept du logiciel
Le nettoyage et la préparation de données peuvent être vus sous la forme d'un processus représenté par la figure 1.
FIG. 1 -Le nettoyage et la préparation de données vus comme un processus.
Le logiciel n'impose pas ce processus à l'utilisateur, mais fournit tous les outils nécessai-res à sa réalisation. En parallèle, le nettoyage et la préparation des données sont tracés dans
ESIEA Datalab, un logiciel de nettoyage et préparation de données la console afin de pouvoir retrouver toutes les transformations et modifications effectuées sur les données et des agents fonctionnent en tâche de fond pour faire des suggestions et orienter l'utilisateur.
Les outils
Outre un vaste ensemble d'outils classiques, dans lesquels les algorithmes utilisés ont été adaptés à un contexte où toute valeur peut être manquante ou bien en erreur, ESIEA Datalab possède quelques outils originaux puissants qui permettent de traiter facilement des cas difficiles de nettoyage ou d'offrir des moyens de visualisation intéressants.
Type structuré. Grâce à la notion de type structuré, le logiciel est capable de détecter des erreurs dans des données symboliques possédant une structure. Une fois la structure d'une colonne spécifiée ou inférée, on peut contraindre les éléments de la structure à l'aide de formules et mettre ainsi en erreur les valeurs ne respectant pas l'une des contraintes.
Outils de visualisation.
Parmi les outils de visualisation disponibles, ESIEA Datalab dispose de graphiques interactifs (matrice de nuages de points, coordonnées parallèles, etc.) qui permettent la sélection de valeurs et la réalisation d'actions sur celles-ci. On trouve aussi des outils originaux comme la carte « vue d'avion ». C'est un graphique qui représente dans une forme condensée toute une table, que l'on va utiliser avec des filtres qui vont colorer une sélection de valeurs. On a ainsi une vision totale de la table qui peut par exemple nous aider à estimer la densité des valeurs manquantes ou bien détecter des motifs.
Conclusion
ESIEA Datalab est un logiciel évolutif dont la simplicité d'utilisation des outils et les fonctionnalités adaptées permettent d'obtenir un gain de temps important sur le nettoyage et la préparation des données. Plusieurs améliorations sont en projet, notamment l'ajout d'une passerelle vers la librairie Java WEKA (Witten et Eibe, 2005 
Summary
ESIEA Datalab is an evolvable Java software program which goal is to clean and prepare data before an analysis. The software looks like a toolbox ready to use, including some interactive visualisation tools, suggestion agents and advanced functionalities implementing Data Mining algorithms.

Introduction
Le problème abordé dans le cadre de cet article est celui de l'accès à une base de connaissances annotée sémantiquement par une ontologie du domaine.
Les connaissances peuvent être de natures diverses : documents scientifiques et techniques, fiches de retour d'expérience, descriptions de compétences, documents multimédias, etc.. L'utilisation d'une ontologie 2 du domaine permet d'indexer et de classer les éléments de la base de connaissances. L'indexation repose sur l'analyse des contenus textuels (et péri textes ou méta données dans le cas des documents multimédias) au regard du vocabulaire associé à l'ontologie. La classification considère les concepts de l'ontologie comme autant de répertoires virtuels auxquels sont associés les éléments de la base de connaissances.
Par accès nous entendons la navigation -accès et recherche -guidée par la modélisation du domaine. Nous nous intéresserons plus particulièrement dans le cadre de ce travail à la navigation sous la forme de visualisations interactives. Nous parlerons alors de « cartes sémantiques interactives » dans la mesure où l'on souhaite aider l'utilisateur dans ses accès à la base de connaissances en exploitant les concepts du domaine et leurs relations.
Cette approche relève du « web sémantique » dans la mesure où à chaque information est associée un URI (Uniform Resource Identifier) et que la gestion des informations repose sur une conceptualisation distincte représentée à l'aide de formalismes d'échange du consortium W3C (RDF, RDF Schema, OWL).
Notre objectif a été dans un premier temps d'étudier et d'évaluer les différents paradigmes de visualisations interactifs pour l'accès et la recherche d'informations annotées sémantiquement. Pour cela, nous avons au préalable défini avec l'aide des utilisateurs un certain nombre de critères d'évaluation comme la capacité à pouvoir se focaliser sur une partie de l'ontologie ou bien encore, la facilité de parcours des liens hiérarchiques tout en gardant une vision globale de la structure de la base de connaissances. Nous avons été ainsi amenés à réaliser plusieurs cartes interactives. L'analyse des retours d'expérience nous a permis d'identifier et de rajouter de nouvelles fonctionnalités, telle que la capacité à parcourir un ensemble d'informations dans un espace non uniforme tout en gardant un point fixe, et nous a permis de spécifier un nouveau type de navigateur dédié à la gestion de bases de connaissances annotée sémantiquement : le « Eye Tree », navigateur de type « polar fisheye view ».
Domaine d'Application
Contexte
Le « Groupement pour la Recherche sur les Échangeurs Thermiques » (GRETh) a mis en place un site internet pour la diffusion des connaissances et des informations scientifiques et techniques au service de leurs adhérents, principalement des industriels. Ces informations, articles, thèses, rapports techniques et scientifiques, sont regroupées au sein d'une base de données. Tous ces documents se rapportent aux métiers du GRETh, basés sur la mécanique des fluides et la thermique des échangeurs.
Besoins
L'objectif du GRETh est de pouvoir accéder à sa base documentaire, non pas en fonction de mots-clés présents dans les documents, mais selon la modélisation du domaine définie par les experts en termes de concepts métier. L'accès à la base de connaissances doit également gérer le multilinguisme.
La modélisation du domaine, c'est-à-dire la représentation des concepts, de leurs relations et de leurs propriétés, a abouti à la construction d'une ontologie spécifique au GRETh. Les concepts étant ici communs et partagés par les différentes communautés, il a été possible d'indexer l'ensemble des documents, quelle que soit leur langue, sur la même ontologie. L'indexation des documents est effectuée de façon automatique par une analyse linguistique multilingue de leur contenu 3 . L'ontologie du GRETh a été réalisée en privilégiant la relation hiérarchique de « généralisation -spécialisation » entre concepts, en considérant qu'elle est simple et non multiple. Les concepts se structurent ainsi sous la forme d'une arborescence.
Les besoins des utilisateurs peuvent alors se résumer de la façon suivante : comment d'une part appréhender l'ensemble des informations à travers les concepts métier relatifs à la mécanique des fluides et à la thermique des échangeurs ; et comment d'autre part accéder à ces informations en parcourant l'ontologie selon la relation hiérarchique de « généralisation -spécialisation ».
Nous sommes donc ramenés à un problème de construction de « cartographies sémantiques interactives » d'une arborescence de concepts.
Méthode et critères d'évaluation
Notre approche repose principalement sur la prise en compte des retours d'expérience (Plaisant 2004)  De l'expression des besoins nous avons pu identifier, pour notre problématique, trois critères d'évaluation des différents paradigmes de visualisation à base d'arborescence de concepts.
1. Visualisation de l'organisation des concepts : Dans la mesure où la conceptualisation du domaine joue un rôle central dans l'accès aux connaissances, il est important de pouvoir visualiser la structure globale de l'ontologie, et ce quelle que soit sa taille. Étant donné que nous privilégions la relation hiérarchique de « généralisation -spécialisation », il est important que la disposition des concepts dans la carte respecte le mieux possible cette sémantique et ce dans un espace qui peut être réduit. La métaphore graphique à utiliser doit donc exprimer au mieux cette sémantique. 2. Association d'informations aux concepts : À chaque concept sont associées une liste de documents et une liste de termes. Il est donc nécessaire de pouvoir accéder et visualiser ces informations. La représentation des noeuds, en termes de variables graphiques comme la taille, la forme ou la couleur (Bertin et Barbut 1967), doit être porteuse de sens. Un utilisateur doit pouvoir accéder rapidement et intuitivement aux informations associées à un concept. 3. Interaction & navigation : L'utilisateur doit pouvoir naviguer au sein de son espace informationnel sans se perdre. À tout moment il doit pouvoir se localiser et identifier où il doit aller. Nous avons ensuite mis en oeuvre les principaux paradigmes connus (par exemple les arbres hyperboliques, les « Treemaps », …) pour les soumettre aux utilisateurs afin d'identifier les caractéristiques essentielles à prendre en compte.
Techniques de visualisation
Pour le domaine technique considéré, celui des échangeurs thermiques, il nous a été demandé de réaliser différents navigateurs graphiques d'accès aux documents techniques en s'appuyant sur la modélisation du domaine (c'est-à-dire de pouvoir parcourir l'ensemble de la base en suivant les liens hiérarchiques de « généralisation -spécialisation » entre concepts). Un concept peut ainsi être interprété comme un « répertoire » contenant les documents qui s'y réfèrent.
Étant donné la volonté de privilégier la relation de « généralisation -spécialisation », nous avons retenu les techniques graphiques de type « noeud -lien » appliquées aux données hiérarchiques. En effet, ces techniques ont l'avantage de représenter explicitement la structure de l'arbre et par conséquent elles expriment mieux la sémantique recherchée.
Cette contrainte nous a donc amené à écarter des techniques de type « space-filling » (Baker et Eick 1995)  Cette visualisation exploite : -une structure d'arbre dépliable pour représenter une hiérarchie de répertoires ; -des icônes de dossier pour représenter les répertoires ; -différentes icônes pour représenter les fichiers.
Généralement, pour la gestion de fichiers, la vue est découpée verticalement en deux avec à gauche la hiérarchie des répertoires et à droite, une zone pour afficher le contenu du répertoire.
Retour d'expérience. Cette technique est directement appropriable par l'utilisateur : les répertoires sont étiquetés par les noms des concepts et le déploiement d'un noeud en noeuds plus spécifiques correspond bien à une interprétation naturelle de la relation de spécialisation. De plus, elle permet d'associer aux noeuds un nombre important d'informations qui peuvent être visualisées dans une zone dédiée (par exemple liste de documents).
Enfin, les interactions et la navigation au sein de l'arbre sont faciles et efficaces et l'utilisateur maîtrise son parcours qui reste visible à tout moment. Ceci est principalement dû au fait que les utilisateurs sont habitués à ce type de représentation.
Cependant, dans le cadre d'applications concrètes où les ontologies peuvent être de taille importante, il devient difficile d'avoir une vue globale de la structure de l'arbre, a fortiori s'il est complètement déplié. L'utilisateur a alors du mal à naviguer au sein de la base de connaissances.
Prototype 2 : Les arbres de cônes
Principe. Afin de palier à la critique émise sur les simples « treeviews », nous avons réalisé un deuxième navigateur à base d' «arbres de cônes ». Les arbres de cônes (Robertson et al. 1991), tout comme les simples « treeviews » sont des arbres de type « noeud-lien ». Le principe consiste à dessiner l'ensemble de la hiérarchie en 3 dimensions (et non une vue partielle). Chaque noeud constitue le sommet d'un cône dont les fils se répartissent sur un cercle qui en constitue la base (cf . FIG. 1 -Arbres de cônes).
Retour d'expérience. Si une telle visualisation donne un aperçu global de la structure de l'arborescence en termes de répartition des concepts, et semble séduisante par son interactivité, l'utilisateur est confronté à un phénomène d'occlusion et l'accès aux noeuds cachés par la structure nécessite de nombreuses manipulations de l'arbre.
Le parcours de la relation de généralisation -spécialisation est complexe. L'utilisateur n'est pas habitué à évoluer dans un espace informationnel à trois dimensions et se perd rapidement à l'intérieur d'un tel espace. L'effort cognitif est important et la prise en main de l'outil nécessite un long apprentissage.
FIG. 1 -Arbres de cônes.
Prototype 6 : Arbres hyperboliques
Principe. L'idée ici n'est plus de vouloir visualiser de manière uniforme tous les noeuds, mais d'en visualiser certains de façon claire tout en permettant l'accès aux autres noeuds. Les arbres hyperboliques (Lamping et al. 1995) utilise une technique graphique de vue non uniforme de type « fisheye » (Furnas 1981;Sarkar et Brown 1992;Leung et Apperley 1994) qui permet de placer dans la vue un nombre important de noeuds.
Cette vue utilise une géométrie non euclidienne : la géométrie hyperbolique. La représentation de la hiérarchie des concepts est alors un arbre radial placé sur un plan hyperbolique. Grâce à la géométrie de ce plan, l'utilisateur à l'impression que la taille des noeuds et la distance entre chaque noeud sont inversement proportionnelles à leur distance au centre du disque. Ainsi, les noeuds sont toujours visibles sinon accessibles et il suffit à l'utilisateur de glisser au centre ceux qu'il souhaite voir plus en détails.
On obtient ainsi une vue de type « focus + context » (Card et al. 1999) où le focus est toujours au centre du disque.
FIG. 2 -Arbre Hyperbolique.
Il existe des variantes en trois dimensions (Munzner et Burchard 1995;Hughes et al. 2004) mais elles ont l'inconvénient d'apporter des effets d'occlusion éliminant ainsi l'apport de « vision globale » de la vue en deux dimensions.
Retour d'expérience. Si de prime abord la forte interactivité des arbres hyperboliques séduit, elle souffre de plusieurs défauts qui peuvent en limiter sa réelle utilisation. Dû aux effets de la déformation, les étiquettes associées aux noeuds ne sont pas alignées et parfois se superposent. Mais c'est principalement son utilisation qui pose problème. En effet, lors de la manipulation de la structure, les éléments à la frontière de l'espace de visualisation se retrouvent projetés de façon « imprévisible ». Ces effets ont tendance à perturber l'utilisateur qui cherche en permanence à rétablir la situation engendrant un effort cognitif plus important et une prise en main assez délicate.
Ces effets de projection sont dus à la géométrie utilisée. En effet, les éléments sont représentés dans un plan hyperbolique qui n'est pas commun à nos sens. C'est pourquoi, le résultat des transformations appliquées au plan n'est pas prévisible « naturellement ».
5 Notre proposition : le paradigme d' « Eye Tree » Les retours d'expérience de l'utilisation de ces différents types de navigation nous ont permis d'identifier, dans le cadre de notre application, quatre critères principaux pour la réalisation d'une carte sémantique interactive :
utiliser une technique de type « focus + context » pour permettre à l'utilisateur de se concentrer sur certains éléments tout en facilitant l'accès aux autres éléments ; -utiliser une géométrie euclidienne pour ne pas perturber la perception naturelle des manipulations du plan ; -proposer une vue globale de l'ontologie permettant à l'utilisateur de facilement appréhender l'ensemble des concepts du domaine ; -pouvoir parcourir la base de connaissances tout en gardant un point fixe de référence. Forts de ses résultats nous avons été amenés à définir un nouveau paradigme basé sur une technique de visualisation de type « fisheye » avec un plan qui possède une géométrie euclidienne : la technique « Polar Fisheye View » (Sarkar et Brown 1992) prenant en compte les critères précédents.
Cette technique fait partie des techniques de représentation avec déformation (Leung et Apperley 1994). Pour cela, les noeuds sont répartis radialement dans l'espace euclidien avant de subir une transformation via une fonction d'amplification continue appliquée aux coordonnées polaires des noeuds.
La figure suivante illustre les opérations opérées sur le plan avant de le visualiser :
Le résultat ressemble aux arbres hyperboliques, mais les interactions de l'utilisateur (par exemple translations) sont appliquées sur un plan euclidien. Elles sont donc « naturellement » prévisibles par l'utilisateur. La transformation étant linéaire, le résultat n'est pas perturbant pour les utilisateurs tout comme pour les « Perspective Wall » (Jock et al. 1991).
Voici le résultat que nous avons obtenu avec un noeud sélectionné et la liste des documents associés : -294 -RNTI-E-6
Le plan donne l'impression d'être projeté sur une sphère. Lorsque l'utilisateur manipule l'arbre, il a l'impression de déplacer le plan sur la sphère. Le tout donne l'illusion d'un oeil d'où le nom : « Eye Tree ».
L'Eye Tree permet aussi d'avoir une vue globale de la structure en faisant varier la force de la fonction d'amplification (plus la force est importante, plus les éléments sont ramenés vers le centre) et la distance entre les noeuds à l'aide de deux curseurs.
FIG. 5 -Vue globale de la structure.
Enfin, les utilisateurs, face aux problèmes que pose l'utilisation des arbres hyperboliques et en particulier face au fait que toute modification locale entraîne des perturbations globales, ont exprimé la possibilité de pouvoir parcourir un ensemble d'éléments au sein d'un espace avec déformation tout en conservant une référence par rapport à un point fixe, en particulier par rapport à la racine de l'ontologie. Nous avons pour cela introduit les rotations du plan euclidien avec pour centre la racine de la structure hiérarchique. Cette interaction a pour conséquence de faire défiler tous les éléments du même niveau par rapport à un point fixe. Ainsi pour la recherche, il est possible de parcourir avec une seule interaction l'ensemble des sous éléments d'un élément donné (en accord avec le modèle MVC (Krasner et Pope 1988), ces rotations sont associées aux événements de la souris correspondant aux actions de la molette).
Conclusion
Le choix d'un paradigme de visualisation nécessite de définir au préalable des critères d'évaluation en fonction du type d'application et des attentes des utilisateurs.
La problématique de l'exploration de bases documentaires techniques guidée par une ontologie de domaine impose un certain nombre de contraintes. Ces contraintes ont été identifiées suite aux retours d'expérience de l'utilisation de différents modes de visualisation. Ainsi, dans la mesure où l'on se focalise sur les concepts et la relation hiérarchique de « généralisation -spécialisation », les visualisations de type « noeud -lien » sont à privilégier par rapport à des techniques de type « space -filling ».
De même, l'utilisation d'une géométrie euclidienne, à l'inverse des géométries hyperboliques, permet de ne pas perturber la perception naturelle des manipulations du plan. Ce point est important dans la mesure où une ontologie est davantage qu'un simple réseau de noeuds : la distribution des concepts doit rester constante dans leur affichage.
Enfin, la visualisation d'ontologies importantes nécessite une approche de type « focus + context » qui permet de focaliser l'affichage sur certains noeuds tout en permettant l'accès aux autres noeuds. L'ensemble de ces considérations a permis de spécifier et de réaliser un nouveau navigateur dédié à la gestion de documents techniques annotés par une ontologie de domaine : le « Eye Tree ». Ce navigateur de type « polar fisheye view » (focus+ context avec déformations linéaires) permet des interactions dédiées à l'exploration d'ontologies (parcours de sous éléments par rapport à un point fixe).
Pour être en accord avec le mantra de Shneiderman ("Overview first, zoom and filter, then details on demand") (Shneiderman 1996), l'accès à ce type de base de connaissances nécessite de combiner une approche globale puis une approche locale. C'est pourquoi, nos prochains travaux porteront sur l'intégration à l'Eye Tree de paradigmes de type « treeview simple » pour permettre à l'utilisateur de se focaliser sur une partie de l'arborescence. De plus, pour valider ces travaux, nous élaborerons une expérimentation pour compléter nos retours d'expérience par des résultats quantifiables.

Introduction
Aujourd'hui, la lecture automatique des documents manuscrits se limite à quelques cas applicatifs particuliers : lecture automatique de chèques ou d'adresses postales, reconnaissance des champs d'un formulaire. Cette lecture est possible car le contenu de ces documents est très largement contraint : structure du document stable, position des informations connue, redondance de l'information, lexique limité, etc. Lors de la lecture, le système bénéficie ainsi d'informations a priori importantes permettant de limiter ou de vérifier les hypothèses de reconnaissance, autorisant une lecture fiable des documents.
Peu de travaux abordent des problèmes de reconnaissance moins contraints car il est alors plus difficile de bénéficier de moyens automatiques de vérification des hypothèses de reconnaissance. C'est le contexte de nos travaux portant sur la lecture automatique des courriers entrants manuscrits. Il s'agit de courriers manuscrits tels que des lettres de réclamation, de changement d'adresse, de modification de contrat, etc., reçus en très grand nombre quotidiennement par des grandes organisations. Contrairement aux applications précédemment citées, aucune information a priori n'est disponible : le contenu, la structure, l'expéditeur ou encore l'objet du document sont totalement inconnus du système de lecture, ce qui rend la lecture intégrale du document extrêmement délicate. Il est cependant possible de considérer des problèmes de lecture partielle du document, visant à en extraire l'information pertinente. C'est ce que nous envisageons dans cet article en proposant une méthode de localisation et de reconnaissance de champs numériques (numéros de téléphones, codes clients, etc.) dans des courriers entrants manuscrits (voir figure 1). La reconnaissance de ces champs permettra par -23 -RNTI-E-6 exemple d'identifier l'expéditeur par le biais du numéro de téléphone, ou de déterminer le type de contrat à l'aide du code client, ce qui autorise un acheminement du courrier vers le service concerné au sein de l'organisation.
FIG. 1 -Exemple de courriers manuscrits où les champs numériques à extraire sont encadrés.
La méthode présentée comporte trois grandes étapes : -Une première étape de localisation rapide sans reconnaissance chiffre ni segmentation permet d'extraire des séquences de composantes susceptibles de constituer des champs numériques. Cette étape basée sur l'exploitation de la syntaxe connue des champs a déjà été présentée dans Koch et al. (2004) puis ameliorée dans Chatelain et al. (2004). Nous la décrivons donc sommairement dans cet article et rappelons ses performances pour justifier les deux étapes de traitement suivantes. -La deuxième étape consiste à soumettre les hypothèses de localisation à un module de reconnaissance de champs fournissant leur valeur numérique. Cette étape repose sur l'utilisation d'un classifieur chiffre et d'un module de segmentation de chiffres liés. -La troisième étape consiste à traiter le problème des fausses alarmes générées par l'étape de localisation. Nous présentons un module de vérification qui accepte ou rejette les hypothèses de champ numérique en exploitant une combinaison d'informations provenant des différentes étapes de traitement. L'article est organisé de la manière suivante : la section 2 décrit sommairement la mé-thode d'extraction des champs dans les documents manuscrits libres. La section 3 présente la méthode de reconnaissance des champs basée sur un classifieur chiffre et une méthode de reconnaissance de chiffres liés. Nous présentons dans la partie 4 les performances de notre -24 -RNTI-E-6 système, ainsi qu'une étape de vérification des hypothèses de reconnaissance des champs afin de rejeter les fausses alarmes.
Localisation des champs
2.1 Une approche "dirigée par la syntaxe" L'approche proposée ici pour la localisation des champs est basée sur une modélisation markovienne d'une ligne de texte. Nous avons déjà eu l'occasion de présenter cette approche dans Koch et al. (2004); Chatelain et al. (2004). Rappelons seulement que ce modèle exploite la syntaxe spécifique des champs numériques que l'on souhaite extraire (nombre de chiffres, présence et position de séparateurs...) pour parvenir à localiser les séquences numériques, sans toutefois procéder à la segmentation des composantes connexes ni à la reconnaissance des chiffres. Nous interprétons globalement la séquence des composantes connexes de chaque ligne pour associer à chaque composante son étiquette : textuelle ou numérique. Toutefois, puisque l'approche ne procède pas à la segmentation des composantes connexes, une composante numérique peut correspondre à un ou plusieurs chiffres, ou même un séparateur (point, tiret...). De ce fait, on doit introduire dans le modèle de ligne des étiquettes correspondant à ces situations : D (Digit ou chiffre), DD (Double Digits ou chiffres liés), S (Séparateur). En ce qui concerne les composantes textuelles, le modèle ne comprend qu'une seule classe, appelée classe Rejet, pour décrire l'ensemble des situations possibles : caractère isolé, fragment de mot, mot, diacritique, signe de ponctuation. La figure 2 représente une ligne de texte avec les étiquettes associées à chacune des composantes qui la constitue.
FIG. 2 -Exemple d'étiquetage des composantes d'une ligne comprenant un code client.
Ces quatres classes constituent les états du modèle markovien. La construction des modèles se fait de la manière suivante : nous fixons le nombre d'état Digit, Double Digit et Séparateur pour chaque type de champ, ainsi qu'un état Rejet. La matrice des probabilités de transitions est déterminée par une estimation statistique sur une base annotée. La figure 3 montre les modèles de Markov ainsi construits, où les flèches entre les états représentent les probabilités de transition non nulles.
L'alignement des séquences de composantes reconnues sur ces modèles garantit de ne conserver que les séquences syntaxiquement correctes. L'extraction des champs numériques dans les lignes de textes consiste alors à rechercher le meilleur alignement dans le treillis des hypothèses de classification. Ceci est réalisé par l'algorithme de Viterbi Forney (1973).
Le processus d'extraction des champs repose donc sur les étapes suivantes : Segmentation en lignes : les lignes de texte sont extraites grâce à une approche de regroupement des composantes connexes inspirée de Likforman-Sulem et Faure (1995).
Classification des composantes connexes : il s'agit de classifier les composantes connexes de chaque ligne selon qu'elles appartiennent à un champ numérique (Digit, DoubleDigit, Sé-parateur) ou non (Rejet). La caractérisation des composantes est réalisée à l'aide de deux jeux -25 -RNTI-E-6 de caractéristiques, présentés à deux classifeurs de type MLP entrainés grâce à l'algorithme de rétropropagation du gradient. Nous combinons ensuite les résultats des deux MLP. Analyse syntaxique : cette dernière étape permet d'extraire les champs recherchés grâce à l'analyse syntaxique des lignes de texte. L'analyseur syntaxique corrige les éventuelles erreurs de classification de l'étape précédente en alignant les hypothèses de reconnaissance sur un modèle markovien d'une ligne de texte pouvant contenir un champ numérique.
Cette méthode d'extraction des champs est une alternative intéressante à l'utilisation d'une stratégie de segmentation-reconnaissance sur l'intégralité du document, puisque seuls les champs extraits seront soumis à un reconnaisseur.
Résultats
Les expérimentations ont été réalisées sur deux bases distinctes d'images de courriers entrants manuscrits : la première (292 images) a été utilisée comme base d'apprentissage pour la classification des composantes connexes ainsi que pour déterminer les probabilités de transition des modèles de Markov et pour paramétrer le système ; la seconde (293 documents) a servi à tester notre approche.
La détection des champs numériques est réalisée en effectuant l'analyse de chaque ligne d'un document. L'analyseur syntaxique se prononce pour la présence (détection) ou l'absence (rejet) d'un champ sur la ligne en cours d'analyse. Un champ est considéré comme convenablement détecté si et seulement si "aucune composante du champ étiqueté n'est rejetée et si toutes les composantes connexes dans le champ détecté appartiennent au champ étiqueté".
Le tableau 1 donne les taux de détection des champs en rang 1, 2 et 5. 
On constate que suivant le type de champ, 70 à 80 % des champs sont détectés en première proposition. Ces résultats augmentent significativement lorsque l'on considère les 2 ou 5 premières propositions de l'analyseur. Les résultats sont meilleurs pour les champs qui possèdent une syntaxe plus contraignante tels que le numéro de téléphone et le code client (nombre de chiffres plus important, présence de séparateurs) que sur les champs faiblement contraints (codes postaux).
La majorité des champs ont ainsi été localisés, sans reconnaissance chiffre. L'étape suivante consiste à soumettre les hypothèses de localisation des champs à un module de reconnaissance afin d'obtenir leur valeur numérique.
Reconnaissance des champs
Contrairement à la majorité des systèmes de reconnaissance de documents manuscrits où la localisation et la reconnaissance des informations sont intimement liées, l'exploitation de la connaissance a priori sur la syntaxe des champs nous a permis de localiser les champs numériques sans les reconnaître. La reconnaissance intervient donc en fin de traitement et permet la vérification des hypothèses de localisation.
L'étape de reconnaissance des champs numériques s'appuie sur l'exploitation des hypothèses de classification fournies lors de l'étape de détection. En effet, nous bénéficions pour chaque champ extrait de l'hypothèse de classification "Digit", "Séparateur" ou "Double digit" des composantes. Il s'agit donc de déterminer l'hypothèse de classification chiffre pour chacune de ces composantes (voir figure 4). Pour les composantes dont l'hypothèse de classification est "Digit", il suffit de soumettre l'imagette à un classifieur chiffre qui déterminera la meilleure hypothèse de classification "chiffre". La description du classifieur chiffre est présentée dans la section 3.1. Les composantes "Séparateur" sont ignorées lors de cette étape, puisqu'elles n'interviennent pas dans la valeur numérique du champ à reconnaître. La reconnaissance des composantes classifiées comme "Double digit" est effectuée de la manière suivante : comme nous savons que la composante contient deux chiffres liés, il nous faut trouver la meilleure segmentation des deux chiffres, et les reconnaître. Cette étape est présentée dans la section 3.2. Dans la section 3.3, nous présentons les résultats obtenus sur les champs numériques isolés.
FIG. 4 -Détermination des hypothèses de classification chiffre à partir des hypothèses de classification
Classifieur chiffre
La reconnaissance de chiffres isolés a bénéficié de très nombreux travaux ces dernières années, notamment dans le cadre de la reconnaissance de montants numériques de chèques, de champs numériques dans les formulaires, ou encore de reconnaissance de codes postaux dans les adresses postales Plamondon et Srihari (2000). Ces systèmes reposent sur l'extraction de nombreuses caractéristiques Trier et al. (1996) et l'utilisation de classifieurs performants Jain et al. (2000). Néanmoins, aucun extracteur ni classifieur n'a pu montrer de supériorité incontestable par rapport aux autres. Partant de ce constat, il est intéressant d'exploiter la complémentarité entre plusieurs classifieurs par une combinaison de type parallèle ou séquentielle. Nous avons ainsi choisi d'effectuer une combinaison parallèle de deux classifieurs de type perceptron multicouche (ou "MultiLayer Perceptron : MLP") auxquels sont soumis deux vecteurs de caractéristiques : -Le vecteur de caractéristiques du chaincode extrait du contour des composantes a montré son efficacité dans de nombreux problèmes de reconnaissance Kimura et al. (1994 
Reconnaissance des chiffres liés
Nous avons présenté le classifieur chiffre permettant de reconnaître les chiffres isolés, nous nous intéressons dans cette partie à la reconnaissance des composantes dont l'hypothèse de classification lors de la première étape est "chiffres liés" (DD). Une stratégie pour cette opération pourrait être la reconnaissance globale de la composante à l'aide d'un classifieur "chiffres liés" comportant autant de classes que de combinaison possibles, soit 100 (classifieur -28 -RNTI-E-6 100 classes [00..99]). Cette stratégie nécessite toutefois une base d'apprentissage conséquente comportant un nombre suffisamment élevé d'éléments dans chaque classe, afin de couvrir la variabilité inhérente à un problème réel : différents type d'écriture, nature des liaisons entre les chiffres (liaisons hautes, basses, multiples), etc. La conception d'un tel classifieur est donc a priori difficile à envisager. Nous avons donc orienté notre approche vers une segmentation de la composante pour identifier les deux chiffres qui la constituent. Dans la mesure où il est très difficile de déterminer sans reconnaissance le meilleur chemin de coupure pour séparer deux chiffres liés, nous avons mis en oeuvre une stratégie de segmentation-reconnaissance à l'échelle de la composante. Plusieurs hypothèses de segmentation sont générées et soumises au classifieur chiffre qui se prononce sur les deux chiffres. Le choix de la meilleure hypothèse est déterminé à partir des confiances fournies par le classifieur. Cette stratégie repose donc sur la mise en oeuvre d'un module de segmentation permettant la génération de plusieurs chemins de coupures, et sur un module de décision qui se prononce sur le choix du meilleur chemin de segmentation. Nous décrivons maitenant ces deux étapes.
Segmentation des composantes
Il existe de très nombreuses méthodes de segmentation explicite, généralement basées sur l'analyse des contours Casey et Lecolinet (1996). Nous avons utilisé une méthode de segmentation inspirée de l'algorithme "drop fall" Congedo et al. (1995), qui consiste à segmenter la composante selon le chemin emprunté par une goutte d'eau qui coulerait selon les contours de la composante. Lorsque la goutte est boquée au fond d'une vallée, celle-ci coupe la composante et continue sa chute. Cet algorithme permet de générer quatre chemins de coupures, suivant que la goutte descende ou qu'elle monte, et suivant la direction prioritaire (gauche ou droite) qu'on lui impose lorsqu'elle rencontre un extrema (mont ou vallée). Ces quatre variantes fournissent généralement des chemins différents contenant au moins une bonne segmentation (voir figure 5).
Sélection du meilleur chemin de segmentation
Nous pouvons donc générer quatre chemins de coupures selon les variantes du drop fall présentées précédemment. Il s'agit dans ce module de sélectionner le "meilleur" chemin parmi les hypothèses générées, décision pour laquelle il nous faut définir un critère fiable traduisant la qualité de la segmentation. Nous proposons de soumettre chaque paire de composantes segmentées à notre classifieur chiffre, et d'utiliser comme critère le produit des scores de confiance associés aux propositions du classifieur pour les deux chiffres. En effet, si les chiffres liés sont bien segmentés, les confiances associées aux deux premières propositions seront élevées ; dans le cas contraire, les hypothèses de classification chiffre devraient voir leur score chuter. La figure 5 présente la segmentation et la reconnaissance d'une composante "double digit" selon les quatre variantes du "drop fall" ; ici le drop fall ascendant gauche maximise le produit des confiances, cette hypothèse est donc conservée.
Le taux de reconnaissance des chiffres liés est de 90% en première proposition sur une base étiquetée d'environ 150 "Double Digit".
La reconnaissance de chiffres liés est évaluée sur une base étiquetée d'environ 150 "double digit" extraits de séquences numériques. Une composante est comptabilisée comme bien re--29 -
RNTI-E-6 connue si les deux chiffres qui la constituent sont bien classifiés. Le taux de reconnaissance obtenu sur cette base est de 90%.
Résultats de la reconnaissance des champs isolés
Pour évaluer la reconnaissance des champs numériques, nous avons constitué une base d'environ 500 champs isolés disposant de l'étiquetage "syntaxique" (Digit, Séparateur, Double Digit), et annoté au niveau chiffre. La base provient de courriers entrants manuscrits réels, et les trois types de champ recherchés sont représentés (codes postaux, numéros de téléphone et codes clients). Nous ne comptabilisons comme bien reconnus que les champs dont toutes les composantes ont été bien reconnues au niveau chiffre. Le taux de reconnaissance au niveau champ est de 80%.
Performances du système
Rappel-précision du système
L'évaluation d'un système d'extraction d'information se fait classiquement par la mesure du rappel et de la précision du système. Ces deux critères sont définis de la manière suivante : rappel = nombre de champs bien reconnus / nombre de champs à reconnaître Le rappel traduit donc la capacité du système à localiser et reconnaître correctement tous les champs numériques d'un document. précision = nombre de champs bien reconnus / nombre de champs proposés par le système La précision indique la pertinence des résultats, c'est-à-dire la capacité du système à ne proposer que des champs d'intérêt et à limiter les fausses alarmes. En effet, notre système a tendance à proposer à l'issue de la première étape (localisation sans reconnaissance) des séquences de composantes qui ne sont pas des champs. Ces "fausses alarmes" ont plusieurs origines : il peut s'agir de séquences textuelles (détection d'un champ dans une zone de texte en présence notamment de caractères bâtons) ; numériques et textuelles (défaut d'alignement) ; ou même strictement numérique (détection d'un champ dans un autre, défaut d'alignement, ou erreur lors de l'étape de reconnaissance chiffre sur un champ bien localisé).
Nous présentons sur la table 3 le compromis rappel-précision de notre système à l'issue de la reconnaissance des hypothèses de localisation, en fonction du rang considéré. On constate que le système est capable de reconnaître de 54 à 63% des codes postaux, codes clients et numéros de téléphone des documents. La précision du système est en revanche relativement faible. Nous proposons donc un module de vérification permettant d'accepter ou de rejeter les séquences de composantes reconnues.
Vérification des hypothèses de reconnaissance
Le but de cette étape de vérification est d'analyser les hypothèses de champs de manière à rejeter les fausses alarmes et à accepter les séquences numériques qui étaient effectivement à détecter. Ce module est basé sur l'interprétation d'un certain nombre d'informations obtenues tout au long de la chaine de traitement, permettant d'accepter ou de rejeter ces hypothèses. L'étape de localisation fournit des scores d'alignement des séquences de composantes sur les modèles markoviens traduisant la qualité de l'alignement, l'étape de reconnaissance fournit des scores de confiance permettant de déceler les éventuelles composantes non numériques. Ces scores, auxquels nous avons rajouté des informations sur la régularité des boites englobantes des composantes, constituent les caractéristiques d'un vecteur soumis à un classifieur de type MLP, entrainé sur une base de champs numériques et de fausses alarmes. L'unique sortie du classifieur se prononce sur l'acceptation (sortie du MLP > 0,5) ou le rejet (sortie < 0,5) de l'hypothèse de champ. Le MLP a été entrainé sur une base de 17000 séquences de composantes (16800 fausses alarmes et 200 véritables champs).
Nous décrivons maintenant le vecteur de 14 caractéristiques provenant des trois familles : caractéristiques issues de la localisation, de la reconnaissance, et des boites englobantes des composantes.
Caractéristiques provenant de la localisation
Lors de l'étape de localisation, l'analyseur syntaxique fournit pour chaque ligne un score d'alignement des composantes sur les modèles (voir figure 6). Ce score est une indication pré-cieuse sur la fiabilité de la localisation du champ et doit donc être retenu comme caractéristique dans notre vecteur. Lorsque le champ n'est pas proposé en première solution par l'analyseur syntaxique, nous remarquons que l'écart entre les scores est généralement faible avec les premiers alignements. Nous avons donc retenu comme caractéristiques les écarts entre le score de l'alignement du champ et les scores des autres alignements de la même ligne. L'expérience montre que la bonne proposition n'est jamais au delà de la cinquième proposition de l'analyseur. Nous avons ainsi retenu 6 caractéristiques issues de l'étape de localisation.
-31 -
RNTI-E-6 
Caractéristiques provenant de la reconnaissance
Une autre famille de caractéristiques pour la discrimination des fausses alarmes provient de l'étape de reconnaissance. Partant de l'hypothèse selon laquelle une séquence de composantes non numériques produit des confiances basses lors de l'étape de reconnaissance (voir figure 7) 
Caractéristiques morphologiques
L'observation d'un certain nombre de champs numériques et de fausses alarmes a montré que les boites englobantes des chiffres constituant un champ numérique présentent générale-ment des régularités que ne possèdent pas les fausses alarmes (voir figure 8). Nous avons donc ajouté dans le vecteur 5 caractéristiques traduisant la régularité dans la succession des boites englobantes :
FIG. 8 -
-L'écart type des ordonnées minimum et maximum des chiffres -32 -RNTI-E-6 C. Chatelain et al.
-L'écart type des hauteur et largeur des chiffres -L'écart type entre les abscisses des centres de gravité des chiffres Résultats à l'issue de la vérification La figure 9 montre l'évolution du rappel et de la précision du système avant et après l'étape de vérification des hypothèses de champs numériques.
FIG. 9 -Courbe rappel/précision du système avant et après vérification des hypothèses de reconnaissance.
Nous constatons que le rejet permet d'améliorer considérablement la précision du système, pour tous les rang considérés. Le rappel du sytème est peu affecté par ce rejet pour le rang1, mais diminue de quelques points pour les rangs plus élevés.
Conclusion et perspectives
Dans le cadre du traitement automatique de courriers manuscrits, nous avons présenté une méthode d'extraction des champs numériques dirigée par la syntaxe, et la méthode de reconnaissance associée. L'intérêt de la méthode réside dans le fait qu'elle utilise la syntaxe d'un champ numérique comme infomation a priori pour le localiser. La reconnaissance des champs est largement contrainte par la méthode de localisation utilisée et permet de reconnaître plus de 60% des champs.
Notons que l'intégration d'un tel système en milieu industriel pourra bénéficier d'un certain nombre de connaissances a priori spécifiques aux types de champs recherchés (connaissances que nous n'avons pas intégrées ici dans le processus de localisation) afin d'en améliorer les performances et en particulier la précision. Par exemple, un numéro de téléphone commence toujours par un '0' ; les codes postaux se trouvent généralement dans la partie supérieure du document ; etc. Un module de mise en concurence des champs pourra également être développé pour éviter les fausses alarmes dues à l'inclusion d'un champ dans un autre.
Afin de fiabiliser notre système, la mise en oeuvre de stratégies alternatives pour la localisation des champs pourra être effectuée. Il serait intéressant d'appliquer des modèles de lignes intégrant les valeurs numériques des chiffres, afin de pouvoir prendre en compte les contraintes mentionnées précédemment (numéro de téléphone comencant par "06", etc.) dès la phase de localisation. Cette méthode impose la localisation de tous les chiffres dans le document : chiffres isolés et chiffres liés. Contrairement à la méthode présentée dans cet article, une -33 -RNTI-E-6
Extraction automatique de champs numériques dans des documents manuscrits phase de segmentation des composantes est donc nécessaire. La clé du problème réside dans le contournement des stratégies classiques de sur-segmentation systématique des composantes qui entrainent une combinatoire très importante et donc des temps de traitement conséquents. Nous travaillons actuellement sur ce sujet. Les deux stratégies pourront ainsi être mise en concurrence afin de fiabiliser les hypothèses de localisation et de reconnaissance des champs.

Introduction
La recherche d'objets vidéo est une tâche difficile compte tenu de la richesse des informations multiples dans l'image. Pour trouver de manière automatique ces objets vidéo, il est important de tenir compte de trois étapes principales qui sont la segmentation, l'identification et le suivi d'objets en mouvement par flot optique.
Le but de la segmentation active est de détecter et d'extraire des informations pertinentes dans une image. Différents modèles de contours actifs ont été proposés dans la littérature, mais on peut distinguer deux principales approches: Des approches basées contours et d'autres basées régions. L'implémentation de n'importe quel modèle de contour actif exige la minimisation d'une fonctionnelle d'énergie. Cette énergie a deux composantes: énergie externe, qui est caractérisée par la régularité de la courbe et l'énergie interne qui a pour fonction d'attirer la courbe vers les gradients les plus forts (les forts contraste de l'image).
Les contours actifs classiques ont été proposés pour la première fois par Kass et al (Kass et al., 1987) pour la segmentation d'images médicales. L'idée de base consiste à faire évoluer la courbe vers la frontière de l'objet à détecter. Ce modèle a été confronté à plusieurs contraintes, liées à l'initialisation, au paramétrage et à l'impossibilité de changement de topologie du snake. Une autre méthode a été introduite par Osher & Sethian (Sethian, 1999) connue par la méthode des ensembles de niveaux. Son principe consiste à faire évoluer une courbe initiale jusqu à ce qu elle détecte la forme de l'objet à extraire. Ensuite, les contours actifs géodésiques (Caselles et al., 1997) ont été présentés comme une alternative géométri-que aux snakes, présentant l'avantage d'être indépendant du paramétrage. Les contours actifs basés régions adoptées par Barlaud et Jehan-Besson (Jehan-Besson et al., 2002). Ils utilisent des descripteurs statistiques des régions, de manière générale, on peut dire que cette méthode est efficace, quand l'ensemble des objets à segmenter est homogène.
Le flot optique se calcule entre deux images: c'est le champs de vecteurs mouvement, rapportés aux pixels, pour passer d'une image à l'autre. Pour faire ce calcul plusieurs approches ont été proposés (Horn et al., 1981). Il existe aussi une autre classe de méthodes pour estimer le mouvement, telles que celles utilisées en compression: le bloc-matching (Koga et al., 1981).
Dans la section suivante nous présentons notre approche de segmentation basée région qui est une implémentation rapide d'un modèle de contour actif qui permet de tenir compte d'informations de couleur et de texture. Ensuite, nous abordons le problème général du flot optique et présentons notre implémentation de l'estimation du mouvement par une approche basée sur la méthode d'Horn & Schunck. Dans la section 4, nous proposons une méthode de mise en évidence (à partir de la détection et de suivi d'objets visuels) en utilisant une approche mixte qui combine à la fois les contours actifs et le flot optique. La section 5 sera consacrée aux résultats obtenus à partir des différentes approches présentées, pour la segmentation des objets en mouvement sur une série d'images vidéo. Enfin, nous terminons notre article par une conclusion en indiquant quelques améliorations possibles.
Segmentation par contour actif
Nous avons amélioré la méthode de chan & vese  en utilisant une fonction générale de la gaussienne qui permet de mieux tenir compte des caractéristiques divers de texture et de couleur dans l'image. Pour les images en couleur, on a choisi de travailler dans un espace de couleur perceptuel (tels que Lab), ainsi est le vecteur couleur du pixel j.
le vecteur moyenne des composantes couleur de la région ? et ? la matrice de covariance. Ces deux derniers caractérisent le comportement des deux régions de couleur et la probabilité d'appartenance d'un pixel à une région donnée est présentée dans la fonction cidessous:
-42 -RNTI-E-6 C est la courbe (ou un ensemble de courbes) qui doit évoluer dans le temps en fonction des régions in/out. Un pixel change d'état en fonction de sa position (intérieur/extérieur) et de l'énergie calculée E CA (positive ou négative).
Estimation de mouvement par flot optique: Modèle de Horn & Schunck
Les méthodes de détermination du flot optique font partie des principales contributions qui ont été présentées pour extraire une information dense et précise du mouvement, sans nécessairement se fier à une connaissance à priori. Horn & Schunck (Horn et al., 1981)    
Détection et suivi d'objets visuels par une approche mixte «CAFO»
L'idée principale consiste à utiliser une segmentation active des régions d'intérêt avec un critère qui est fonction du mouvement. L'algorithme de base est divisé en deux parties: l'estimation du mouvement et la segmentation par contour actif. Pour simplifier la tâche et gagner en rapidité et en efficacité, nous allons minimiser une fonctionnelle d'énergie unique pour la segmentation et l'estimation:
Il s'agit d'une résolution simultanée du problème d'estimation de mouvement et de segmentation active d'une image vidéo, par minimisation de l'énergie F. La première étape consiste en l'initialisation du contour sur l'image courante. Elle peut être effectuée de manière automatique en utilisant le résultat d'une étape de séparation du fond et des objets. Une fois le contour initialisé, un processus de déformation intervient jusqu'à convergence en -43 -RNTI-E-6 utilisant les forces décrites précédemment. Il va nous permettre de déterminer la position d'un objet(t) à l'instant t en se basant sur sa position précédente objet(t-1). L'intérêt de notre méthode consiste à extraire des objets visuels en mouvement de mêmes contraste que le fond de l'image. L'approche CAFO combine les avantages des deux métho-des «Contour Actif» (CA) et «Flot optique» (FO). En effet, la segmentation active (CA) donne des résultats satisfaisants sur des images complexes (objet + décors). Alors que l'estimation du mouvement par flot optique donne d'assez bons résultats quand les régions d'intérêt sont texturées. L'approche CAFO améliore le processus de segmentation et résout les problèmes d'occlusion et d'ouverture connus quand on estime le mouvement des pixels entre deux images.
D'un autre côté, cette approche est particulièrement adaptée pour le suivi d'objets. La méthode peut être composé de deux étapes qui sont l'initialisation et la déformation successivement sur chaque image de la séquence vidéo. Tout d'abord le contour est initialisé en utilisant le résultat obtenu à l'image précédente. Il est ensuite déformé en utilisant à la fois des énergies issue du modèle de contours actifs et de la force issue du calcule du flot optique. Le contour final de l'image précédente sera utilisé comme contour initial sur l'image courante. Une fois le contour initialisé, un processus de déformation intervient jusqu'à convergence en utilisant les forces décrites précédemment. Cette méthode va nous permettre de déterminer la position d'un objet(t) à l'instant t en se basant sur sa position précédente objet(t-1). En général, les méthodes différentielles échouent avec ces images. L'estimation de mouvement (ou l'appariement de blocs) entre deux images successives, donne des informations supplémentaires mais insuffisantes pour l'extraction du contenu visuel des séquences. La figure 1.d montre le résultat de l'approche mixte qui intègre à la fonctionnelle d'énergie des régions les forces issues du flot optique. Cela a permis de mettre en évidence plus nettement des objets d'intérêt parmi des décors plus complexes, comme le montre notamment l'image du hall. L'avantage de combiner les informations du mouvement avec la segmentation est multiple. D'abord, cela peut contribuer à l'indépendance de l'approche de segmentation par contour actif, de la phase d'initialisation du contour initial (par exemple, en utilisant le FO comme initialisation de l'image suivante). Ensuite, elle permet de compenser les informations manquantes dans une image, ou non détectables (détails) à cause la moyenne. Enfin, elle permet de privilégier l'une des deux approches (grâce aux poids) en fonction des applications. Toutes les expériences ont été faites en utilisant l'approche de segmentation active, et le calcul du flot optique développés en langage C++ dans l'environnement Pandore.

Introduction
Le problème de l'extraction de motifs séquentiels dans un grand ensemble de données statiques a été largement étudié ces dernières années (Agrawal et Srikant (1995), Masseglia et al. (1998), Pei et al. (2001), Wang et Han (2004), Kum et al. (2003)). Les schémas extraits sont utiles dans de nombreuses applications comme le marketing, l'aide à la décision, l'analyse des usages, etc. Depuis peu, des applications émergentes comme (entre autres) l'analyse du trafic réseaux, la détection de fraude ou d'intrusion, la fouille de clickstream 1 ou encore l'analyse des données issues de capteurs ont introduits de nouveaux types de contraintes pour les méthodes de fouille. Ces applications ont donné lieu à une forme de données connues sous le nom de "data streams". Dans le contexte des data streams l'utilisation de la mémoire doit être réduite, les données sont générées de manière continue et très rapide, les opérations bloquantes ne sont pas envisageables et, enfin, les nouvelles données doivent être prises en compte aussi vite que possible. Ainsi, de nombreuses méthodes ont été proposées pour extraire des items ou des motifs dans les data streams (Datar et al. (2002), Chang et Lee (2003), Cormode et Muthukrishnan (2005)). Dans ce domaine, l'approximation a rapidement été reconnue comme un facteur clé pour fournir des motifs à la vitesse imposée par l'application Garofalakis et al. (2002). Ensuite, des méthodes récentes (Chen et al. (2002), Giannella et al. (2003), Teng et al. (2003)) ont introduit différents principes pour gérer l'historique des motifs extraits. L'idée principale est que l'on est généralement plus interessé par les changements récents que par les changements plus anciens. Giannella et al. (2003) a ainsi introduit la notion de logarithmic tilted time window pour stocker les fréquences des motifs avec une granularité fine pour les changements récents et une granularité plus large pour les changements plus anciens. Dans Teng et al. (2003), une technique de regression est utilisée pour représenter les fréquences et une technique permettant de régler la finesse de représentation est introduite. Enfin, dans Raissi et al. (2005), les auteurs proposent une nouvelle structure de données destinée à extraire les motifs séquentiels fréquents d'un data stream. Cependant, dans cet article, nous montrons que les phénomènes combinatoires liés à l'extraction des motifs séquentiels rendent toute méthode exhaustive potentiellement bloquante. En effet, si dans le cas des règles d'association le nombre de possibilité est fini, ce n'est pas le cas des motifs séquentiels, pour lesquels un item peut se répéter à l'infini. Dans cet article, nous proposons l'algorithme SMDS (Sequence Mining in Data Streams) qui est basé sur l'alignement de séquences (comme Kum et al. (2003), Hay et al. (2002)  
Adapter la problématique des motifs séquentiels
Pour les méthodes traditionnelles de Web Usage Mining, le principe gérénal est similaire à celui de Masseglia et al. (2000). Les données brutes sont collectées dans des fichiers logs par les serveurs. Chaque entrée dans le fichier log représente une requête faite par une machine cliente au serveur. L'objectif est alors de déterminer, grâce à une phase d'extraction, les séquences de ce jeu de données, qui peuvent être considérées comme fréquentes selon la définition 2. Les résultats obtenus sont du type < ( 10 ) ( 30 ) ( 20 ) (30 ) > (ici avec un support minimum de 66% et en appliquant les algorithmes de fouille de données sur le fichier représenté par la figure 1). Ce dernier résultat, une fois re-traduit en termes d'URLs, confirme la décou-verte d'un comportement commun à minSup utilisateurs et fournit l'enchaînement des pages qui constituent ce comportement fréquent. Enfin, l'exploitation par l'utilisateur des résultats obtenus est facilitée par un outil de requête et de visualisation.
SMDS : motivation et principe général
Notre méthode est basée sur un environnement de découpage du data stream en "batches" (inspiré de celui proposé par Giannella et al. (2003)) et par la structure d'arbre préfixé de PSP Masseglia et al. (1998) pour gérer les séquences extraites. Nous proposons d'abord une étude des limites d'une approche intégrant un algorithme exhaustif d'extraction de motifs sé-quentiels. Nous présentons ensuite notre méthode, basée sur le principe de l'alignement de séquences.
Limites de l'extraction de motifs séquentiels
FIG. 2 -Limites d'un environnement intégrant PSP
Dans SMDS, le data stream est traité sous forme de batches de taille fixe. Soient B 1 , B 2 , ... B n , les batches et B n , le batch courant. Le principe de SMDS est d'extraire les motifs sequentiels représentatifs de chaque batch b de [B 1 ..B n ] et de stocker les motifs extraits dans une structure d'arbre préfixé. Considérons que les motifs sont extraits par une méthode exhaustive (comme celles conçues pour les données statiques). Une telle méthode présentera au moins un type d'opération bloquante. Considérons par exemple le cas de l'algorithme PSP Masseglia et al. (1998). Nous avons testé cet algorithme sur des bases de données ne contenant que deux séquences (s 1 et s 2 ). Les deux séquences sont égales et contiennent des répétitions d'itemsets de taille 1. Plus précisément, la première base de test contenait onze répétitions des itemsets (1)(2) (i.e. s 1 =< (1)(2)(1)(2)...(1)(2) >, longueur(s 1 )=22 et s 2 = s 1 ). Le nombre de candidats générés à chaque passe est reporté dans la figure 2. La figure 2 reporte aussi le nombre de candidats générés pour les bases contenant des séquences de longueur 24, 26 et 28. On peut observer que, pour la base contenant des séquences de longueur 28, PSP est incapable de fournir les résultats (la mémoire est saturée par le nombre de candidats). Nous avons fait la même observation pour l'algorithme prefixSpan 2 (Pei et al. (2001)) pour lequel ce cas de figure conduirait à un blocage du data stream. Dans le contexte des flots de données issus des usages d'un site Web, il n'est pas rare de trouver de nombreuses répétitions d'un ou plusieurs items (fichiers pdf, php, etc.). 
Principe général
Dans les grandes lignes, SMDS fonctionne de la manière suivante : classification de l'ensemble des séquences de chaque batch de transactions suivi d'un alignement pour chaque cluster ainsi obtenu. Cela permet d'obtenir des clusters de comportements qui représentent les usages du site en temps réel. Pour chaque cluster dont la taille est supérieure à minSize (spé-cifié par l'utilisateur) SMDS ne stocke donc que le résumé du cluster. Ce résumé est donné par l'algorithme d'alignement de séquences appliqué sur chaque cluster.
Algorithme glouton de classification des séquences
Notre schéma classificatoire est basique. Il repose sur le fait que les navigations sur un site sont souvent : soit plutôt proches, soit très éloignées. De manière empirique, on peut constater que les utilisateurs qui demandent les pages concernant les offres d'emplois d'ITA ne vont probablement pas consulter (dans la même session) les pages relatives aux prochains séminaires organisés par l'unité de recherche de Sophia Antipolis. Dans le but d'obtenir une classification des navigations aussi rapide que possible, notre approche gloutonne fonctionne de la manière suivante : l'algorithme est initialisé avec une seule classe, qui contient la première navigation. Ensuite, pour chaque navigation n dans le batch, n est comparée avec chaque cluster c. Aussitôt que n est similaire à une séquence de c alors n est insérée dans c. Si n n'est insérée dans aucun cluster, alors un nouveau cluster est crée et n est insérée dans ce nouveau cluster. La similitude entre deux séquences (sim(s 1 , s 2 )) est donnée dans la définition 4. s est insérée dans c si la condition suivante est respectée : ?s c ? c/sim(s, s c ) ? minSim, avec minSim la similitude minimum, spécifiée par l'utilisateur.  : m i1 , ...x it : m it ), où m it est le nombre de séquences qui contiennent l'item x i à la p eme position dans la séquence alignée. Enfin, n p est le nombre d'occurrences de l'itemset I p dans l'alignement. L'exemple 2 décrit le processus d'alignement de quatre séquences. À partir de deux séquences, l'alignement commence par insérer des itemsets vides (au début, au milieu ou à la fin des séquences) jusqu'à ce que les deux séquences contiennent le même nombre d'itemsets. À la fin du processus d'alignement, la séquence alignée (SA 14 dans la figure 3) est un résumé du cluster correspondant. Le motif séquentiel correspondant peut être obtenu en spéci-fiant k : le nombre minimum d'occurrences d'un item pour que celui-ci soit considéré comme fréquent. Par exemple, avec la séquence SA 14 de la figure 3 et k = 2, la séquence alignée filtrée sera : <(a,b)(e)(h,i)(m,n)> (ce qui correspond aux items qui ont un nombre d'occurrences supérieur ou égal à k).
Exemple 2 considérons les séquences suivantes :
Stockage et gestion des séquences
Les séquences alignées obtenues à la fin de l'étape précédente sont stockée dans un arbre préfixé similaire à celui de Masseglia et al. (1998). Si une nouvelle séquences s est découverte, alors l'arbre est modifié pour stocker cette nouvelle séquence. Sinon, s est déjà dans l'arbre et son support est mis à jour. La figure 4 donne un exemple d'arbre préfixé. Chaque chemin de la racine à un noeud de l'arbre représente une séquence extraite. L'arbre de la figure 4 contient 6 séquences (<(a c)>
, <(a d)>, <(b)>, <(c d)>, <(c)(e)>, <(d)(a)>).
Tout chemin de la racine à une feuille représente une séquence et le noeud de profondeur l représente le l eme item de la séquence. Le changement d'itemset est représenté par des branches de différents types. Par exemple, le lien pointillé entre les noeuds c et e de la figure 4 illustre le fait que e n'est pas dans le même itemset que c. À chaque noeud est associé k, le filtre utilisé pour obtenir cette séquence alignée dans le cluster correspondant. L'exemple 3 donne une illustration de la gestion des séquences et de leur support. 
Expérimentations
La méthode SMDS a été implémentée en Java sur un Pentium (2,1 Ghz) exploité par un système Linux Fedora. Nous avons évalué notre proposition sur des données synthétiques et des données réelles 3 .
Temps de réponse et robustesse de SMDS
Dans le but de montrer l'efficacité de SMDS, nous reportons dans la figure 5 le temps né-cessaire pour extraire les motifs séquentiels les plus longs sur chaque batch correspondant à des données d'usage du Web (à gauche de la figure 5) et des données synthétiques (à droite de la figure 5). Pour le site Web de l'Inria, les données ont été collectées sur une période de 14 mois et représentent 14 Go. Le nombre total de navigations est de 3,5 millions pour 300000 navigations. Nous avons découpé le fichier log en batches de 4500 transactions (soit environ 1500 séquences en moyenne). Pour ces expérimentations, le filtre k est fixé à 30% (notons que ce filtre a un impact sur les temps d'exécution, dans la mesure où il modifie la taille des séquences à gérer dans l'arbre préfixé). De plus, nous avons "injecté" dans les données des séquences parasites. Le premier batch ne subit pas de modification. Dans le second, nous ajoutons dix séquences contenant deux répétitions de deux items (C.f. les séquences s 1 et s 2 décrites en section 3.1). Dans le troisième batch, nous ajoutons dix séquences de trois répétitions, et ainsi de suite jusqu'au trentième batch qui contient 10 séquences de trente répé-titions. L'objectif est de montrer que les méthodes d'extraction traditionelles (PSP, prefixSpan, ...) risquent de bloquer le data stram alors que SMDS continuera sa tâche d'extraction. Nous pouvons observer que le temps de réponse de SMDS varie de 1800 ms à 3100 ms. PSP propose des motifs avec de très bonnes performances pour les premiers batches et se trouve pénalisé par le bruit ajouté par les séquences parasites (voir le batch 19). Le test a également été fait avec prefixSpan et le comportement exponentiel est similaire. Pour PSP comme pour prefixSpan, le support minimum spécifié était le maximum possible tout en assurant que les séquences "parasites" (répétitions) seraient trouvées. Nous avons ajouté à la figure 5 le nombre de sé-quences de chaque batch pour expliquer les différences de temps d'exécution d'un batch à un autre. On peut observer, par exemple, que le batch 1 contient 1500 séquences et que SMDS demande 2700 ms pour en extraire les motifs séquentiels. Pour les données synthétiques, nous avons généré des batches de 10000 transactions (qui correspondent à environ 500 séquences en moyenne). La longueur moyenne des séquences était de 10 pour 200000 items. Le filtre k est fixé à environ 30%. Nous indiquons dans la figure 5 (partie droite) les temps de réponse et le nombre de séquences correspondant à chaque batch. Nous pouvons observer que SMDS traite 10000 transactions en moins de 4 secondes (par exemple pour le batch 2).
Motifs extraits sur les données réelles
La liste des comportements découverts par SMDS couvre plus de 100 objectifs de navigation (classes de séquences de navigations) sur le site Web de l'Inria Sophia Antipolis. La plupart des motifs découverts peut être considérée comme rare (support faible) et pertinente (haute confiance, car le filtre utilisé est élevé). Nous reportons ici quelques exemples de ces comportements. A) k = 30%, taille de la classe = 13, préfixe="http ://www-sop.inria.fr/omega/" : Pour les navigations sur le site de l'Inria Sophia Antipolis, nous avons également constaté que SMDS est capable de détecter les séquences parasites qui avaient été injectées dans les batches. Ces séquences sont simplement regroupées par SMDS dans un cluster qui ne contient qu'elles, sans impact sur les temps d'exécution.
Impact de la taille des batches
FIG. 6 -Taille des batches et pire cluster, pas à pas.
Puisque la complexité de SMDS dépend de la taille de batches, nous avons mené une étude concernant l'impact de cette taille sur les temps de réponse. Nous reportons dans la -635 -RNTI-E-6 figure 6 (partie gauche) les temps de réponses quand S (le nombre de séquences du batch) varie de 100 à 5000 séquences. La courbe temps représente les temps d'exécution, cluster représente les nombre de clusters extraits, cluster 1% représente le nombre de clusters tels que : |c| > 0.1 × S. En effet, nous pensons qu'il ne faut considérer que les clusters dont la taille est supérieure à une certaine proportion du nombre total de séquences (on ne garde que les clusters les plus grands). Avec 1% et un batch de 1000 séquences, par exemple, un cluster c tel que |c| < 10 ne sera pas considéré. time 1% représente le temps d'exécution quand on ne garde que les clusters de taille supérieure à 1% de S. On peut observer que le nombre de clusters augmente de façon linéaire. Le temps de réponse est de toute évidence lié à la taille des batches, mais il est raisonnable de dire que l'utilisateur final peut choisir la taille de ses batches en fonction du degré de rapidité souhaité.
Analyse de la qualité des clusters
FIG. 7 -Distance globale étape par étape et batch par batch
Afin de mesurer la qualité des classes produites par SMDS, notre principal outil sera la distance entre deux séqunces. Soit s 1 et s 2 , deux séquences, la distance dist(s 1 , s 2 ) entre s 1 et s 2 est basée sur sim(s 1 , s 2 ), la mesure de similitude donnée par la définition 4 et telle que
2 ) proche de 0 signifie que les séquences sont proches (similaire si cette valeur est nulle) alors que dist(s 1 , s 2 ) proche de 1 signifie que les séquences sont éloignées (ne partagent aucun item si cette valeur est 1). Nous avons utilisé deux mesures pour la qualité des classes. La première est le diamètre d'une classe C. Il s'agit de la plus grande distance entre deux séquences de C. Un diamètre de 0% montre que la classe est constitué uniquement de séquences égales alors qu'un diamètre de 100% montre que la classe contient au moins deux séquences qui ne partagent aucun item. Lors de nos expérimentations, le diamètre moyen a varié entre 2% et 3%. La seconde mesure est la "double moyenne". Elle est basée sur le centre de la classe. Soit C une classe, le centre de C est une séquence c telle que : ?s ? C, x?C dist(s, x) ? y?C dist (c, y). Nous sommes donc en mesure de donner, pour C, la distance moyenne (DM ) entre c et toutes
|C|
. Nous reportons dans la figure 6 (partie droite) quelque distances moyennes parmi les pires obtenues durant nos expérimentations. Pour chaque séquences ajoutée dans une classe, nous donnons la valeur DM de cette classe. Par exemple, après l'ajout de la dernière séquence dans le cluster 1, la valeur de DM pour ce cluster est 22%. On peut observer que DM varie de 0 (quand |C| = 1 l'unique séquence est le centre) à 50%. DM décroit alors rapidement jusqu'à des valeurs comprises entre 20% et 35%, ce qui est un bon résultat compte tenu du fait que la figure 6 (partie droite) ne comprend que les résultats des clusters les moins homogènes. Les autres clusters sont homogènes et offrent à l'algorithme d'alignement un cadre adéquat. Nous reportons dans la figure 7 (partie gauche) la double moyenne DBM après avoir traité chaque séquence d'un batch. DBM est calculée dist (x,ci) de la manière suivante : soit C l'ensemble des classes, DBM = i?C x?C i |C| avec c i le centre de C i (la i eme classe). On peut observer dans la figure 7 (partie gauche) que pour le second batch, DBM augmente rapidement jusqu'à 2% (séquence 220), puis augmente lentement jusqu'à 3,7%. La valeur finale de DBM à la fin du batch est donnée par la figure 7 (partie droite). On peut y observer que DBM est toujours comprise entre 2% et 9%. A la fin du processus, la valeur moyenne de DBM est de 4,3% (une qualité moyenne des classes de 95,7%).
Conclusion
Dans ce papier, nous avons proposé la méthode SMDS, conçue pour extraire rapidement les séquences d'un data stream et en proposer un résumé significatif. Notre algorithme repose sur une technique de classification combinée avec un alignement des séquences. Le processus d'alignement repose sur un algorithme de classification glouton qui considère que dans le contexte du Web les navigations ont certaines caractéristiques qu'il faut prendre en compte. De plus nous avons proposé une solution adaptée pour gérer efficacement les séquences et leur historique dans un arbre préfixé. Grâce à cette façon de traiter le data stream, SMDS est capable de détecter des comportement partagé par un nombre relativement faible d'utilisateurs (e.g. 13 utilisateurs ou encore 0, 5%) ce qui est proche du difficile problème de l'extraction de motifs séquentiels avec un support très faible. De plus, nos expérimentations ont montré que SMDS traite le data stream assez rapidement pour être intégré dans un contexte temps réel. Nous avons également montré que SMDS propose des classes de très bonne qualité, ce qui permet d'extraire les motifs les plus pertinents et de façon exhaustive.
Summary
In recent years, emerging applications introduced new constraints for data mining methods. These constraints are typical of a new kind of data: the "data streams". In a data stream processing, memory usage is restricted, new elements are generated continuously and have to be considered as fast as possible, no blocking operator can be performed and the data can be examined only once. We argue that the main issue is the combinatory phenomenon related to sequential pattern mining. In this paper, we propose an algorithm based on sequences alignment for mining approximate sequential patterns in data streams. To meet the constraint of one scan, a greedy clustering algorithm associated to an alignment method are proposed. We will show that our proposal is able to extract relevant sequences with very low thresholds.

Introduction
Le développement d'Internet comme source d'informations a conduit à l'élaboration de programmes nommés wrappers pour collecter de l'information sur les sites Web. Ces programmes sont difficiles à concevoir et à maintenir. Deux approches sont alors envisageables : la première consiste à assister l'utilisateur, c'est le cas du système Lixto (Baumgartner et al., 2001) dans lequel on spécifie le wrapper dans un langage logique avec l'aide d'un environnement visuel ; la seconde consiste à générer automatiquement le wrapper en limitant l'intervention de l'utilisateur à l'annotation des informations à extraire sur quelques documents. Cette approche est fondée sur le fait que la plupart des documents sur Internet sont générés par programme et présentent des régularités exploitables par les méthodes d'apprentissage automatique.
Les premiers systèmes d'induction de wrappers n'utilisaient que l'aspect textuel des documents (Hsu et Dung, 1998;Kushmerick, 1997). Avec l'apparition de XML, ces approches textuelles sont devenues insuffisantes. Les systèmes actuels utilisent la structure arborescente des documents du Web (Carme et al., 2005;Cohen et al., 2003;Kosala et al., 2002;Muslea et al., 2003;Thomas, 2003). Nous nous inscrivons dans cette démarche en proposant un système d'induction qui utilise à la fois les vues textuelle et arborescente. Beaucoup de systèmes d'induction de wrappers sont conçus pour des tâches unaires. Un wrapper unaire extrait un ensemble de valeurs, par exemple l'ensemble des noms de produits disponibles sur un site marchand. Un wrapper n-aire extrait les instances d'une relation n-aire, par exemple les couples (nom du produit, prix). Il existe deux approches pour induire un wrapper n-aire : soit combiner n wrappers unaires, soit apprendre directement le wrapper n-aire. La première approche nécessite l'obtention d'un modèle pour la combinaison, ou une intervention de la part de l'utilisateur (Jensen et Cohen, 2001;Muslea et al., 2003), ou encore l'utilisation d'heuristiques. La seconde approche est illustrée par les systèmes WIEN (Kushmerick, 1997) et SOFT MEALY (Hsu et Dung, 1998) utilisant des délimiteurs textuels pour repérer les composantes des tuples et le système LIPX (Thomas, 2003) basé sur la logique du premier ordre.
-415 -RNTI-E-6
Extraction de relations dans les documents Web Nous proposons un système d'induction de wrappers n-aire pour les documents du Web utilisant les vues textuelle et arborescente. Pour cela, nous étudions à la section 2 les différentes représentations arborescentes de tables dans les documents Web. Nous proposons ensuite un système basé sur les deux principes suivants : l'extraction est incrémentale, c'est-à-dire que le système extrait l'ensemble des premières composantes, puis l'ensemble des couples pour les deux premières composantes, jusqu'à l'ensemble des n-uples ; pour extraire l'ensemble des tuples de longueur i, des informations sur les tuples de longueur i ? 1 sont utilisées. Les algorithmes d'extraction et d'induction sont présentés dans la section 3. Le système est évalué sur des jeux de données réels dans la section 4. Les résultats montrent que notre système peut appréhender l'extraction de relations dans les documents Web pour des organisations fréquentes que les systèmes existants ne sont pas capables de traiter.
Représentations arborescentes d'une relation n-aire
Nous considérons que les données sont stockées dans des documents au format XML ou XHTML qui peuvent être considérés selon plusieurs vues. La vue DOM considère le document comme un arbre (arbre d'analyse) ; la vue séquentielle comme un flux de caractères ; la vue feuillage comme la séquence des feuilles textes de la vue DOM. Ces différentes vues sur les données sont illustrées par la 
FIG. 1 -Vue DOM (gauche), vue textuelle (en haut à droite), vue feuillage (en bas à droite).
Une étude des documents du Web nous a amené à distinguer les cas suivants : Cas 1. Les données sont dans une table dont la première ligne contient les noms des composantes et les lignes suivantes contiennent les données. Dans la vue DOM, il existe un plus petit sous arbre contenant chaque tuple et seulement lui (Figure 1).
Cas 2. Une autre représentation est celle d'une liste où les tuples sont stockés séquentiel-lement. Pour les vues textuelle et arborescente, il peut être difficile de retrouver les tuples sans information auxiliaire surtout dans le cas de valeurs manquantes.
Cas 3. Les données sont dans une table tournée dont la première colonne contient les noms des composantes et les colonnes suivantes contiennent les données. Les tuples sont entrelacés dans les vues textuelle et feuillage. Dans la vue DOM, les composantes d'un même tuple ont le même numéro de fils dans des arbres différents de racine tr. Ceci est illustré par la relation ternaire (Season, Club name, Score) de la figure 2 où les tuples à extraire sont (2002,PSG,17) et (2002.  Table croisée 3 Apprendre à extraire une relation n-aire
Les processus d'extraction et d'induction sont incrémentaux : on extrait les premières composantes, puis les tuples de longueur 2, jusqu'aux tuples de longueur n pour une relation n-aire cible. Pour la première composante appelée graine, la tâche d'extraction considérée consiste à extraire certaines feuilles textes d'un document arborescent (nous ne considérons pas les cas où les données à extraire sont situées dans une feuille texte ou sur plusieurs feuilles textes). Nous utilisons un codage attribut-valeur des feuilles : le codage de base d'un noeud p est fourni par les attributs suivants : label, position dans la séquence des fils du père de p, profondeur et hauteur, nombre de fils, taille du sous arbre enraciné en p, label du frère gauche et du frère droit de p, valeurs éventuelles des attributs XHTML class et id. La représentation d'une feuille l est donnée par l'application du codage de base à l, à ses 5 ancêtres dans la vue DOM, puis par le contenu brut des feuilles textes précédente et suivante dans la vue feuillage. Ainsi une feuille est représentée par 57 attributs. Une fois codées en vecteurs d'attributs, les feuilles sont fournies à un classifieur qui classe chacune d'entre elles. Une feuille est extraite si elle est classée comme à extraire.
Considérons maintenant une relation cible n-aire et sa restriction aux i premières composantes. Le codage rep i de la relation d'arité i va utiliser des informations sur les composantes 1 à i ? 1, il est définie par : la représentation d'un tuple d'arité i se fait par le codage de base de la i-ième composante notée l et de la description des dépendances entre l et la graine -417 -RNTI-E-6
Extraction de relations dans les documents Web
Algorithm 1 Extraction
Input: un document d ; n classifieurs ci à valeurs dans {?1, +1} travaillant sur les représentations repi. Notation: L est l'ensemble des feuilles de d, Si est l'ensemble des tuples candidats à l'étape i, Ti est l'ensemble des tuples extraits à l'étape i sélectionnés dans Si. 1: S1 = {(l) | l ? L} ; T1 = {t1 ? S1 | c1(rep 1 (t1)) = +1} 2: for i = 2 to n do 3:
?ti ? Si, si ci(rep i (ti)) = +1, ajouter ti à Ti 5:
si tous les ti basés sur le même ti?1 sont classés comme négatif par ci, ajouter (ti?1, null) à Ti 6: end for Output: Tn l'ensemble des tuples n-aire extraits ainsi qu'entre l et la composante i ? 1. Le codage de la dépendance entre deux feuilles p et m est constituée du codage de leur plus petit ancêtre commun a, des longueurs des plus courts chemins dans la vue DOM entre p et a, m et a et entre p et m, du nombre de feuilles textes se trouvant entre p et m dans la vue feuillage, et pour 1 ? k ? 5 de la différence entre la position (relative à leur père) du k-ième ancêtre de p et du k-ième ancêtre de m. Un tuple est représenté par 220 attributs quel que soit i différent de 1.
L'ordre des composantes étant déterminé par la relation cible fournie par l'utilisateur du système, l'extraction est réalisée par l'algorithme 1. On peut noter qu'il est possible d'extraire plusieurs tuples de longueur i pour un même tuple de longueur i ? 1 (ligne 4) et que les valeurs manquantes sont gérées en ligne 5.
Du point de vue de l'apprentissage, la tâche d'induction d'un programme d'extraction naire est définie par l'algorithme 2 et correspond à n problèmes de classification supervisée, chaque problème consistant à classer des tuples d'arité i comme étant à extraire ou pas. L'algorithme de classification supervisée utilisé est C5.0 (Quinlan, 2004) 
Expériences
Toutes les expériences sont réalisées avec deux documents annotés en apprentissage. La qualité de notre système est évaluée à travers la f-mesure (F ) des wrappers produits. Les critères d'évaluation sont stricts : un tuple extrait est correct si toutes les composantes sont égales exactement aux composantes du tuple à extraire.
Des expériences ont été réalisées sur les jeux de données classiques du domaine, en l'occurrence RISE 1 . Les organisations correspondent aux deux premiers cas de la section 2. Notre système réussit parfaitement sur tous les jeux de données à l'exception de S1 et IAF. Sur S1,
Algorithm 2 Apprentissage
Input: un échantillon S de documents où les tuples à extraire sont annotés.
1: S + = {t = (l1, . . . , ln)} l'ensemble des n-uplets de feuilles à extraire dans S. 2: C = ? 3: for i = 1 to n do 4:
+ } # projection des n-uplets à extraire sur les i premières composantes 5: 
soit ci le classifieur appris par C5.0 avec l'échantillon T
ajouter ci à la séquence C de classifieurs 9: end for Output: C   FIG. 4   (town, day, weather, high, low). La composante town est factorisée, elle apparaît dans l'entête de page. Les autres composantes sont présentées dans une table tournée à 5 colonnes comme le montre la figure 4. Nous obtenons F = 100, ce qui montre la capacité de notre système à gérer ce type d'organisation. Ensuite le site Bureau of Labor Statistics 4 et la relation (value, year, quantile). Il s'agit ici d'un exemple d'organisation en table croisée. Nous obtenons F = 98.52 à cause de la présence de valeurs manquantes dans certaines tables.
Conclusion
Nous avons présenté un système capable d'induire automatiquement des programmes d'extraction n-aire à partir de documents du Web. Ce système présente les avantages suivants :

Introduction
A cette date, de nombreuses méthodes d'étiquettage d'entités biologiques pour les corpus de spécialité ont été proposées ; quelles soient à base de règles (Fukuda et al. (1998)) ou encore réposant sur des techniques d'apprentissage (Collier et al. (2000)). Néanmoins, la simple détection de la présence d'une entité nommée dans un texte ne suffit pas pour l'identifier et l'associer à une instance d'entité biologique particulière. Le couplage des méthodes d'extraction des entités nommées avec l'utilisation de dictionnaires semble être une solution particulière-ment adaptée à ce type de problématique (Koike et al. (2003)). De plus, la majorité de ces techniques d'extraction d'entités nommées ont été développées dans le but de ne détecter que quelques types particuliers et spécifiques d'objets biologiques, notamment les gènes et les protéines, et ne peuvent être facilement adaptés à d'autres contextes. Il existe trois principales difficultés à prendre en compte lors d'une recherche à base de dictionnaire :
-la présence de termes synonymes et la résolution des différentes abréviations et acronymes, -la variabilité des mots tant au niveau de l'orthographe que de la morphologie et de la syntaxe mais aussi d'un point de vue lexico-sémantique, de la présence d'insertions/déletions et permutations, -la présence de noms ambigus que se soit entre des entités de même nature, entre des entités de natures différentes ou des collisions avec le dictionnaire anglais standard. Ces différents points restent particulièrement difficiles à traiter dans les textes de biologie et de médecine. Les problèmes d'ambiguïté dans les corpus biomédicaux sont résumés dans (Tuason et al. (2004)).  (Lindberg et al. (1993)).
Description des dictionnaires
Nous stockons dans nos dictionnaires, non pas les noms bruts issus des alias et des acronymes de chaque base de données, mais les formes variantes (orthographiques, morphologiques, lexico-sémantiques, etc) de chaque alias et symbole que nous normalisons. Nous utilisons aussi les informations issues des nomenclatures spécifiques de chaque base de données afin de générer de nouveaux alias d'une entité. Ces formes inédites peuvent être retrouvées dans les publications scientifiques alors qu'elles sont absentes des bases de données. Par exemple, un effort important a été fourni afin de produire l'ensemble des combinaisons de noms complets/formes acronymiques potentielles d'une même entité ("chemokin like receptor 1", "CMKLR1", "CMKL receptor 1", "CMK light receptor 1", "chemokin like R 1" et "chemokine LR 1") et les multiples insertions/déletions et permutations de groupes de mots descripteurs ("class III alcohol dehydrogenase", "alcohol dehydrogenase class III" et "adenosine monophosphate deaminase 1 isoform M", "adenosine monophosphate deaminase 1" ). La construction de tels dictionnaires ne sera pas décrite dans cette article. Actuellement, les dictionnaires contiennent un total de 205 736 variants dont 49 656 entités distinctes. Les différentes molécules, cellules, organes et sites de liaison sur l'ADN répertoriés proviennent de l'humain ou à défaut de mammifères.
Recherche des entités nommées dans les textes
Chaque document est tout d'abord découpé en phrases grâce à des heuristiques puis à chaque mot de chaque phrase est associé son part of speech grâce à GENIA POS Tagger (Tsuruoka et al. (2005)). Nous n'utilisons pas de shallow parser mais uniquement les informations fournies par les part of speech. Ceci à le mérite d'alléger la procédure à condition que l'éti-quettage soit correct (Amrani et al. (2005)). Tous les syntagmes plus ou moins complexes sont extraits de chaque phrase séquentiellement, découpés en plus petites unités, grammaticalement correctes en biologie, et normalisés jusqu'à correspondance exacte avec une entité du dictionnaire. En effet, les entités nommées présentes dans les publications biomédicales peuvent être relativement complexes et s'étendre sur plus d'un groupe nominal.
Extraction des entités nommées Les groupes de mots pouvant potentiellement représenter un ou plusieurs objets biologiques sont dégagés des textes de la façon suivante :
1. Les groupes nominaux simples correspondant aux blocs de noms propres ou communs avec les éventuels symboles, cardinaux et adjectifs associés sont extraits. Par exemple "Interleukine 2".
2. Sont rattachés aux groupes nominaux simples les verbes au gérondif ou au participe passé en suffixe, ou en préfixe si le mot précédent le verbe n'est pas un modal, un pronom ou un adverbe. Deux groupes nominaux simples sont concaténés si un de ces verbes permet d'en faire la jonction. Par exemple "Interferon regulating factor 8".
3. Deux de ces groupes nominaux étendus peuvent être ensuite réunis si certaines prépo-sitions ou conjonctions telles que "of", "in", "at", "on", "by", "for", "to" ou "with" les séparent. Par exemple "regulator of G-protein signalling 4" ou "cell adhesion molecule regulated by oncogenes".
4. De la même manière, deux des groupes nominaux étendus provenant de l'étape précé-dente sont reliés entre eux s'ils sont séparés par une conjonction de coordination "and, "but" ou "or". Par exemple "Signal transducer and activator of transcription 3 interacting protein 1".
A cette étape, nous pouvons donc avoir des syntagmes de complexité très différentes à analyser. A priori, chaque syntagme représente une seule et même entité. Nous cherchons donc son occurrence au sein de nos dictionnaires après normalisation (cf paragraphe suivant). Néanmoins si la mise en correspondance exacte n'a pu être réalisée, nous considérons que le syntagme contient alors plus d'une entité, chaque entité pouvant être représenté par une portion indépen-dante du texte. Nous devons donc redécouper le bloc de texte contenu dans le syntagme en sous-unités de complexité légèrement moindre. Chaque sous-unité est alors testée individuellement contre nos dictionnaires et si la mise en correspondance s'avère infructueuse, celle ci est décomposée en constituants plus simples, et ainsi de suite, jusqu'à détection positive de la présence d'une entité ou obtenir une portion de texte non résolue et atomique. Le découpage des syntagmes est réalisé grâce à la règle contextuelle décrite ci-dessous que nous appliquons en fonction des séparateurs suivants, séquentiellement :
1. les conjonctions de coordination, 2. les prépositions (sauf s'ils sont précédés d'un verbe), 3. les gérondifs et participes passés (et l'éventuelle préposition associée).
La précédence du séparateur numéro (2) sur le séparateur numéro (3) a été décidé empiriquement en analysant la composition de nos dictionnaires.
Les différentes combinaisons de blocs de texte de part et d'autre d'un séparateur sont gé-nérées. Par exemple, l'expression "suppressor of G2 allele of SKP1 pseudogene" donne les combinaisons "suppressor of G2 allele" et "G2 allele of SKP1 pseudogene". Des nouveaux syntagmes générés, ceux possédant le plus grand nombre de séparateurs ont précédence sur ceux en contenant moins et sont traîtés en priorité par la suite. De même, en cas de présence de prépositions, la position des entités nommées au sein des expressions est située préféren-tiellement à droite des séparateurs. Nous traitons donc en priorité les nouveaux syntagmes en fin de texte. Sur l'exemple précédent, l'ordre de priorité est désormais : "G2 allele of SKP1 pseudogene" puis "suppressor of G2 allele". Le syntagme original, non découpé à l'étape en cours, est utilisé par la règle suivante. En revanche, chaque nouveau bloc est de nouveau traité par la règle en cours.
Par exemple, le syntagme "modulator of G-protein signalling 4 down-regulated by oncogenes" contient une entité nommée : "G-protein signalling 4" que l'on souhaite découvrir. Les séparateurs détectés dans le texte sont "of" et "down-regulated by". Nous testons contre nos dictionnaires les blocs de texte suivants dans cet ordre : tout d'abord "modulator of G-protein signalling 4 down-regulated by oncogenes" puis -d'une part "G-protein signalling 4 down-regulated by oncogenes" puis -"G-protein signalling 4" et "oncogenes" indépendamment -et d'autre part "modulator". Une limite principale à la stratégie actuellement mise en place consiste en l'impossibilité de retrouver des concepts basés sur des groupes verbaux. Ceci n'influe pas sur la capacité du système à détecter des noms d'objets biologiques mais réduit son abilité à reconnaître des concepts de plus haut niveau.
Identification des entités nommées
Chaque bloc de texte que l'on souhaite mettre en correspondance avec les entités présentes dans nos dictionnaires sont tout d'abord normalisées (génération des variants morphologiques, suppression des déterminants, lemmatisation et passage sous la forme de compound nouns). Les portions de texte que nous obtenons à l'étape d'extraction sont à base de groupes nominaux plus ou moins complexes, or il est très fréquent de trouver associés aux entités nommées des noms satellites qui peuvent soit décrire une action dont l'objet est l'entité biologique (par exemple "assimilation", "transcription", "screening", etc) ou qualifiant l'entité (par exemple "gene", "protein", "experiment", etc). En anglais, de tels termes sont majoritairement ajoutés en suffixe au nom de l'entité. Aussi nous supprimons au fur et à mesure les noms à droite du texte avant de les soumettre à la recherche dans les dictionnaires. Cette méthode simple permet de répondre convenablement à ce type de problème. Il reste néanmoins des cas de figure non négligeables de construction de groupes nominaux où les noms d'action ou descriptifs sont retrouvés devant le nom de l'objet biologique (par exemple, "interleukine protein IL2") et qui sont non résolus automatiquement pour le moment. La présence en préfixe d'adjectifs (par exemple "ubiquitous"), cardinaux ou symboles est éga-lement prise en compte. La seule différence avec la technique précédente réside dans le sens de la réduction des termes : ici ce sont les adjectifs, cardinaux ou symboles les plus à gauche du texte qui sont enlevés.
Désambiguation Pour l'instant l'étape de désambiguation des noms est assez rudimentaire :
-Afin d'améliorer la qualité des groupes nominaux à tester, sont détectées les énuméra-tions simples du type "interleukine 1, 2 and 3 receptors" impliquant des numériques ou des symboles/identifieurs afin d'être explicitées sous la forme "interleukine 1 receptor and interleukine 2 receptor and interleukine 3 receptor".
-Beaucoup d'auteurs d'articles en biologie définissent entre parenthèses des abréviations qu'ils utilisent tout au long du document en lieu et place de l'objet biologique tel qu'il est décrit dans nos dictionnaires. Il est très important de pouvoir les détecter et les associer correctement aux entités qu'elles représentent. Les termes entre parenthèses qui précèdent une entité nommée identifiée mais dont la nature est inconnue seront automatiquement associés à cette entité reconnue lors d'une prochaine occurrence dans le texte.
Typage Dans nos dictionnaires, nous pouvons avoir une même entité associée à différents descripteurs biologiques. Seul le contexte dans lequel l'entité a été identifiée peut permettre de clarifier sa nature. Lorsque plusieurs entités de nature différente (par exemple facteur de transcription ou site de liaison à un facteur de transcription) correspondent à un même bloc de mots, les noms éliminés à l'étape d'Identification des entités nommées puis d'Extraction des entités nommées permettent de mesurer la vraisemblance respective de chaque descripteur "contextuel" associé à l'objet biologique grâce à un lexique de mots contrôlés. Ce lexique contient un ensemble de termes qui sont associés spécifiquement à un ou plusieurs types d'entités dans les textes (par exemple, "neuropeptide" qualifie exclusivement une protéine, "transcription" un gène et "assay" un protocole expérimental). Le type prédominant parmi les mots satellites résolus par le lexique doit ainsi permettre d'aider à clarifier le type contextuel de l'entité. Le recours à des techniques plus perfectionnées est alors nécessaire lorsque l'entité n'est associée à aucun terme du lexique (pour le moment, l'entité reste non-résolvable) ou lorsque des contradictions apparaissent (ici l'entité est a priori considérée comme étant un faux positif).
Résultats et conclusion
L'outil a été développé en Java et la base de données implémentée avec PostgreSQL. Nous avons mesuré les performances du système sur le corpus de référence GENIA (JinDong et al. (2003) Les approches par dictionnaire ont pour principale limitation de ne pouvoir détecter des entités encore inconnues mais restent efficaces pour permettre de caractériser les relations entre ces différents objets biologiques ou certaines de leurs propriétés en combinaison avec un système d'extraction d'information en aval. Le principal avantage des techniques mises en oeuvre dans cet article est leur relative généricité permettant de traiter des objets biologiques de natures très différentes sans avoir à utiliser différentes méthodes complexes en parallèle. Plusieurs difficultés restent néanmoins en suspens : la principale, et la plus difficile à résoudre, est l'inévitable cas des termes au sens variable selon le contexte (et notamment comment distinguer la véri-table nature d'une entité et savoir si l'on a affaire à une véritable entité biologique ou non. Ce qui se pose particulièrement pour les sites de liaison aux facteurs de transcription d'après nos résultats préliminaires). Une autre difficulté réside dans l'exhaustivité très relative des dictionnaires présentés ici et la nécessité d'avoir des sources contrôlées et vérifiées, ce qui rend les mises à jour encore assez ardues. Beaucoup d'entités présentes dans les dictionnaires sont inappropriées ou issues d'erreurs de saisie lorsque les bases de données respectent mal ou peu les nomenclatures en vigueur.

TIMC-IMAG Institut de l'IngØnierie et de l'Information de SantØ
FacultØ de MØdecine F-38706 LA TRONCHE cedex Delphine.Bernhard@imag.fr http://www-timc.imag.fr/Delphine.Bernhard Les mØthodes d'extraction automatique de termes utilisent couramment des patrons dØ-crivant la structure des termes (Ibekwe-Sanjuan et Sanjuan, 2004;Enguehard, 1992;Vergne, 2005). Dans les domaines scientiiques ou techniques comme la mØdecine (Namer, 2005), de nombreux termes appartiennent au vocabulaire savant et sont construits partir de formants classiques grecs ou latins situØs en dØbut (extra-, anti-) ou en n de mot (-graphe, -logie). La mØthode que nous proposons utilise la structure morphologique des termes en vue de leur extraction et de leur regroupement MOEme si cette expression rØguli?re est limitØe aux formants se terminant par a, i ou o, elle n'est pas uniquement valable pour le franais. On trouvera, par exemple, "chimio-hormonothØrapie" en frannais, "chemo-radiotherapy" en anglais ou "Chemo-radiotherapie" en allemand.
Une fois les formants identiiØs, les termes sont repØrØs l'aide d'un patron qui dØcrit leur structure morphologique : F+M oø F est un formant et M un mot du corpus de longueur supØrieure 3. Le caract?re + indique la succession possible de plusieurs formants en dØbut de terme. Lorsque ce patron s'applique un des mots du corpus, deux termes sont reconnus : le terme de structure F+M et le terme de structure M. Ainsi, partir du mot "radiothØrapie" qui contient le formant "radio", on extrait les termes "radiothØrapie" et "thØrapie".
AAn de faciliter l'analyse des termes extraits, des familles de termes sont formØes en regroupant les termes contenant le mOEme mot M. Le mot M est appelØ reprØsentant de la famille. De plus, deux familles sont rØunies si leurs reprØsentants ont une chaane initiale commune de longueur supØrieure ou Øgale 4 et si l'on retrouve le mOEme formant dans un terme de chaque famille. Le reprØsentant nal de chaque famille est le terme le plus frØquent.
Les rØsultats de l'extraction terminologique sont prØsentØs sous forme de liste pondØrØe au format HTML (voir gure 1). Ce type de liste se caractØrise par l'utilisation d'un code de couleur et d'une taille de police dØpendant de la frØquence d'occurrence d'un terme (VØronis, 2005). Seuls les termes reprØsentants de chaque famille sont afchØs et le poids d'une famille dans la reprØsentation nale est determinØ par la frØquence cumulØe de tous les termes de la famille.
FIG. 1 Visualisation des termes sous forme de liste pondérée (à gauche) et détail d'une famille de termes (à droite)
Le syst?me a ØtØ expØrimentØ sur 4 corpus de textes couvrant deux domaines scientiiques distincts, celui de la volcanologie et du cancer du sein, dans deux langues diffØrentes, le franais et l'anglais. Les premiers rØsultats obtenus montrent que l'utilisation de la structure morphologique permet de mettre jour des termes peu frØquents qu'une approche purement frØ-quentielle ne pourrait identiier. Ces deux approches sont donc complØmentaires. L'algorithme de regroupement permet quant lui de rassembler les variantes orthographiques, exionnelles et dØrivationnelles des termes dans une mOEme famille.

Classification croisée de données biologiques
Afin d'étudier les séquences d'acides aminés représentant les protéines, nous avons utilisé des techniques de text mining afin d'extraire des descripteurs. Ces descripteurs nous permettrons de construire un tableau de données Protéines × Descripteurs. L'une des techniques les plus utilisées est l'extraction des x-grammes (Miller et al. (1999), Mhamdi et al. (2004)), x étant la taille d'un descripteur.
Plusieurs méthodes de classification croisée ont été proposées (Govaert (1977), Ritschard et Nicoloyannis (2000)). Récemment, des méthodes de classification croisée ont été appliquées aux données biologiques (Cheng et Church (2000)). Cependant, plusieurs de ces méthodes restent très coûteuses en temps de calcul.
FaBR-CL : méthode de classification croisée
Afin d'effectuer une classification croisée, nous nous sommes basé sur une méthode de classification peu coûteuse en temps de calcul (Erray (2005)). La méthode proposée, FaBR-CL, utilise FaUR dans une approche "Combinaison itérative de regroupement des lignes et des colonnes" afin d'obtenir un regroupement complet des protéines et des 3-grammes. Ainsi, nous effectuons le regroupement des protéines, dans un premier temps, et le regroupement des 3-grammes dans un deuxième temps. La complexité de cette méthodes est en O(l log l + p log p), l étant le nombre de protéines et p le nombre de descripteurs.
Nous avons travaillé sur les trois familles de protéines PAD, TLR et AD afin de valider notre approche. Les trois études portant à chaque fois sur des protéines appartenant à deux familles, montrent que la méthode FaBR-CL donne un classement très proche de la réalité (PAD, TLR et AD). Aussi, nous obtenons des groupes de 3-grammes fortement pertinents par rapport à chaque classe de protéines. L'étude de toutes les protéines des trois familles, confirme ces résultats.
Conclusion
La méthode proposée permet, avec un coût calculatoire très faible, de classer les protéines. Aussi, cette méthode permet de mettre en évidence des groupes de 3-grammes, de faibles effectifs, et qui permettent d'identifier une classe de protéines par leurs présences ou par leurs absences.

Introduction
Dans le cadre de ce travail, nous nous intéressons au problème d'extraction de règles associatives, initialement introduit par Agrawal et al. Agrawal et al. (1993). Plusieurs travaux basés sur l'analyse formelle des concepts (AFC) Ganter et Wille (1999), proposent des approches de sélection de règles associatives qui véhiculent le maximum de connaissances utiles. Ces approches reposent généralement sur l'extraction d'un sous-ensemble générique de toutes les règles associatives, appelé base générique, tout en satisfaisant certaines caractéristiques jugeant de sa qualité, mais qui dans la plupart des cas ne sont pas satisfaites dans leurs totalités Kryszkiewicz (2002).
Dans cet article, nous introduisons une nouvelle approche de génération d'une base minimale et générique (MGB) de règles associatives. L'originalité de cette approche est qu'elle est autonome : elle commence directement à partir du contexte d'extraction pour dériver une base générique minimale de règles associatives FAST-MGB.
Fondements mathématiques
Dans cette section, nous rappelons brièvement les notions mathématiques relatives à l'analyse formelle des concepts Ganter et Wille (1999).
Notions de bases
Contexte de fouille. Un contexte de fouille est un triplet
L'opérateur de fermeture de la connexion de Galois Ganter et Wille (1999) est la composi-
. L'ensemble des concepts réduits fréquents forme un semi-treillis appelé treillis de l'Iceberg de Galois. Bastide et al. (2000).
Les algorithmes d'extraction de bases génériques
Généralement, le nombre de règles associatives dérivées par un processus de fouille de données peut devenir très important surtout quant les mesures de fréquences deviennent assez faibles ou encore dans le cas de bases de données denses telles que les données textuelles.
Une solution possible à ce problème serait de se restreindre à l'extraction des règles strictement liées aux besoins de l'utilisateur, en d'autres termes, se limiter à une base générique des règles associatives, répondant à certains critères et à partir de laquelle les règles redondantes pourront être dérivées. Différentes approches de dérivation de bases génériques ont été proposées dans la littérature présentant chacune certaines limites. Nous distinguons quatre bases à savoir : la Base représentative (RB P han ) Luong (2001), la Base des règles associatives non redondantes (MNR) Bastide et al. (2000), la Base générique pour les règles représentatives (RR) Kryszkiewicz (2002), et la Base Générique Informative (IGB) Gasmi et al. (2004). Une étude comparative des quatre bases est donnée dans Latiri et al. (2005).
Dans le cadre de notre travail, nous définissons une base générique comme suit :
Définition 1 Une base générique est un ensemble de taille réduite de règles associatives ne contenant aucune règle redondante. Il existe trois critères pour évaluer une base générique quantitativement (en nombre de règles) et qualitativement (par la sémantique), à savoir :
1. Informativité : c'est la possibilité de déterminer avec exactitude le support et la confiance des règles redondantes dérivées à partir de la base générique, ce qui nécessite de garder la trace des concepts réduits fermés fréquents ou de leurs générateurs. Nous passons dans ce qui suit à la présentation de la nouvelle base générique FAST-MGB.
Une nouvelle base Générique minimale : FAST-MGB
Notre contribution consiste à introduire une nouvelle base générique minimale des règles associatives non redondantes, notée par FAST-MGB et ne contenant que des règles implicatives (i.e. prémisse différente de ?). Cette approche assure la compacité et une informativité partielle avec un système axiomatique complet et valide. L'originalité de cette approche c'est qu'elle est autonome : elle commence directement à partir du contexte d'extraction pour dé-river l'ensemble des concepts réduits fréquents, le treillis de l'iceberg de Galois et la base minimale générique de règles associatives FAST-MGB.
Génération de la base FAST-MGB
Dans le cadre de notre travail, nous considérons les règles associatives qui minimisent le nombre de termes dans la prémisse et qui maximisent le nombre de termes dans la conclusion.
Définition formelle
La base générique FAST-MGB est définie comme suit : Définition 2 Soit AR k l'ensemble des règles associatives pouvant être extraites à partir d'un contexte d'extraction k. Une règle R : X ? Y ? AR k est redondante par rapport à R 1 : X 1 ? Y 1 si et seulement si les deux conditions suivantes sont vérifiées :
Dans la suite, nous définissons la base FAST-MGB comme suit :
1. L c : le treillis de l'iceberg de Galois, contenant tous les itemsets fermés fréquents pouvant être extraits à partir d'un contexte d'extraction k, et associés à leurs géné rateurs minimaux ainsi que leurs supports respectifs.
2. S : l'ensemble des successeurs immédiats d'un itemset fermé fréquent c i .
3. G ci : l'ensemble des générateurs minimaux d'un itemset fermé fréquent c i . 
TAB. 1 -Notations utilisées par l'algorithme GEN-FAST-MGB
Formellement la base générique FAST-MGB est définie comme suit :
Dans ce qui suit, nous présentons l'algorithme de construction de la base FAST-MGB directement à partir du contexte d'extraction (voir l'algorithme 1 et le tableau 1). Étape1 : Générer l'ensemble des concepts réduits fréquents du treillis de l'iceberg de Galois enrichi par les générateurs minimaux L'algorithme GEN-CRF (voir algorithme 2) est itératif. Dans chaque itération k, il construit un ensemble de concepts formels réduits candidats (CRC k ) qui sera élagué ensuite, en respectant la contrainte de minsupp.
Étape 2 : Générer le treillis de l'iceberg de Galois enrichi par les générateurs minimaux La dérivation du treillis de l'iceberg de Galois, illustrée par l'algorithme 3 passe principalement par deux étapes, à savoir, i) La génération de la liste de tous les successeurs d'un concept donné et ii) À partir de la liste ainsi obtenue, ne retenir que les successeurs directement placés audessus du concept en question. 

Introduction
La quantité de sources d'information disponible sur Internet fait des systèmes d'échanges pair-à-pair (P2P) un genre nouveau d'architecture qui offre à une large communauté des applications pour partager des fichiers, partager des calculs, dialoguer ou communiquer en temps réel, etc (Miller (2001), Ngan et al. (2003)). Les applications P2P fournissent également une bonne infrastructure pour les opérations sur de grandes masses de données ou avec de très nombreux calculs, comme la fouille de données. Dans ce cadre, nous considérons une nouvelle approche pour améliorer la localisation de ressources dans un environnement P2P non structuré selon deux aspects principaux pour extraire des comportements fréquents :
1. L'ordre des séquences entre les actions réalisées sur les noeuds (requête ou télécharge-ment) est pris en compte pour améliorer les résultats. 2. Les résultats des calculs distribués sont maintenus via un "Pair centralisé" pour réduire le nombre de communications entre les pairs connectés. Connaître l'ordre des séquences des actions réalisées sur les pairs offre une connaissance importante. Par exemple, en examinant les actions réalisées, nous pouvons savoir que pour 77% des noeuds pour lesquels il y a une requête concernant "Mandriva Linux", le fichier "Mandriva Linux 2005 CD1 i585-Limited-Edition-Mini.iso" est choisi et téléchargé. Cette requête est suivie par la demande des images iso (i.e "Mandriva Linux 2005 Limited Edition"), et dans la grande quantité de résultats retournés, l'image "Mandriva Linux 2005 CD2 i585-LimitedEdition-Mini.iso" est choisie et téléchargée. L'un des problèmes principaux des systèmes P2P non structurés comme Gnutella est que les requêtes sont envoyées à un trop grand nombre de noeuds (broadcast) entraînant ainsi une consommation excessive de la bande passante (Ng et al. (2003)). Proposer à l'avance à l'utilisateur les fichiers souvent associés à une requête ou à un téléchargement permet d'éviter une consommation excessive de la bande passante dans la mesure où l'on connaît à l'avance les ressources à extraire. Il suffit alors d'enrichir le résultat de la première requête avec des informations complémentaires sur les fichiers majoritairement téléchargés par les autres utilisateurs.
Rechercher des règles d'association ou des motifs séquentiels dans un système aussi distribué que les systèmes P2P n'est pas une tâche facile. En effet, par nature, ces systèmes sont très dynamiques, i.e. les noeuds agissent indépendamment les uns des autres, et les connaissances acquises ne sont alors plus forcément représentatives. Par exemple, quand un noeud disparaît, les séquences de ce noeud disparaissent également de la base distribuée et la connaissance extraite doit être reconsidérée. Les approches d'extraction de motifs séquentiels traditionnelles (Srikant (1995), Pei et al. (2001)) qui considèrent que la base est disponible dans son intégra-lité ne sont donc plus utilisable dans un contexte aussi dynamique. Notre proposition se situe dans ce cadre et prend en considération l'aspect dynamique des systèmes pairs à pairs non structurés.
Dans la suite de cet article, la section 2 présente la problématique de la recherche de motifs séquentiels dans une base de données distribuée. Dans la section 3, nous proposons une nouvelle approche basée sur une heuristique. Les expérimentations menées sont décrites dans la section 4. Puis la section 5 conclut cet article.
Problématique
Dans cette section, nous étendons la problématique intitiale de la recherche de motifs sé-quentiels Srikant (1995) à un environnement P2P non structuré. Soit I = {x 1 , . . . , x n } un ensemble de littéraux distincts appelés items. Nous considérons par la suite que pour chaque item nous connaissons l'action réalisée, i.e. requête ou téléchargement. Un item est vide jusqu'à ce qu'un noeud envoie sa séquence. L'architecture P2P non structuré que nous proposons utilise un pair spécial (appelé par la suite pair "Distributed SP ") qui est connecté à tous les nouveaux pairs qui arrivent sur le réseau (l'instruction send(v,@Distributed SP ) dans l'algorithme 2, permet à Distributed SP d'être au courant de l'arrivée du noeud "v"). Notre méthode utilise alors une distribution des séquences candidates comme illustrée par la figure 1. Le pair "Distributed SP " réalise les instructions de l'heuristique Distributed SP , de "GetValuation" à "Broadcast". Tout d'abord, l'ensemble des items fréquents est extrait des pairs connectés. Puis l'ensemble de tous les candidats de taille 2 est généré. Ces candidats sont évalués par les pairs connectés (u t ..v t ) pour connaître ceux qui ont un nombre d'occurrences suffisant sur toute la base, i.e. sur
Les résultats sont récupérés par le pair Distributed SP (i.e. fonction "GetValuation"). L'heuristique, basée sur des opérateurs génétiques est alors appliquée et le nouvel ensemble de candidats est envoyé aux pairs connectés pour évaluation. Ce processus est répété tant qu'il existe des noeuds connectés.
Distributed SP débute lorsqu'un noeud u t se connecte ((recvu t )). L'ensemble des motifs fréquent est alors initialisé avec la séquence de u t . Tant que des noeuds sont disponibles, nous considérons les motifs envoyés à Distributed SP par la fonction getV aluation afin de dé-terminer si une séquence est fréquente. SCORE correspond à une note moyenne donnée par tous les noeuds pour les candidats. Si SCORE est plus grand ou égal à la valeur de support, le candidat devient fréquent et est stocké dans F Dt . Nous conservons également les sous sé-quences candidates non fréquentes, appelées séquences approximatives et stockées dans˜Fdans˜ dans˜F Dt , -471 -RNTI-E-6
Analyse des usages dans les systèmes pair-à-pair dont la taille par rapport à la taille totale des séquences candidates vérifie une contrainte de distance spécifiée par l'utilisateur mindist. Ces séquences seront utilisées pour les phases de génération de candidats. Grâce aux séquences fréquentes approximatives et aux opérateurs de voisinage, de nouveaux candidats sont générés et envoyés par broadcast aux noeuds connectés. Par manque de place, nous ne décrivons pas les opérateurs de voisinage utilisés. Le lecteur intéressé peut se reporter à Masseglia et al. (2003) où nous utilisons, dans un autre contexte, des opérateurs génétiques similaires.
Deux opérations principales sont réalisées dans l'algorithme node. Premièrement, lorsqu'un nouveau pair v t essaye de se connecter à un noeud u t (recv(v,connect)), un message lui indiquant l'adresse de Distributed SP lui est retourné. Deuxièmement, lorsqu'un message de Distributed SP est reçu, un score représentant la distance entre un candidat et les opérations locales effectuées sur le noeud est calculé. Si un candidat est inclus, son score est positionné à 100 + size(candidate). Comme notre approche est heuristique, nous récompensons fortement les candidats complètement inclus dans une séquence. En outre comme nous recherchons les comportements les plus longs, nous récompensons les longues séquences. Ceci est réalisé par l'algorithme Longest-Common-Subsequence (LCS) Cormen et al. (2001).
Experimentations
Pour évaluer notre approche, différentes expériences ont été menées sur des jeux de données réelles : le fichier de données "Pumsb" Repository et un fichier d'access log (AccessLog). Les premières expériences ont été réalisées pour analyser la convergence des résultats ainsi que les coûts de communication. Dans le premier cas, un algorithme traditionnel de recherche de motifs a d'abord été appliqué sur la base globale. Chaque population de candidat proposée par notre approche est alors comparée au résultat réel de manière à déterminer sa qualité. Pour cela, nous mesurons pour chaque population de candidat, la plus longue sous séquence -472 -RNTI-E-6 F. Masseglia et al. commune (LCS) entre les séquences candidates et le résultat réel. Puis la base est partitionnée en différents noeuds et notre approche est appliquée. Les résultats de l'expérience sont décrits par la figure 2. Nous remarquons que pour Pumsb, à la première génération, la qualité de la population candidate est supérieure à 50%. A la seconde, nous avons 70%. Pour les deux jeux de données, à partir de la génération 6, la qualité du résultat est proche de 95% et nous devons attendre la génération 7 pour avoir 100%. Ces résultats montrent que notre approche peut rapidement obtenir des longues séquences fréquentes en ne réalisant que 7 opérations de broadcast.
FIG. 2 -Qualité des résultats pour les populations proposées
Pour évaluer le comportement de notre approche lorsque des noeuds apparaissent ou disparaissent, nous avons considéré les deux jeux de données. L'idée principale est la suivante : nous voulons analyser le comportement de notre approche lorsque x% séquence de la base d'origine Pumsb sont remplacés par x% séquences de la base destination AccessLog. Nos expériences consistent donc à estimer la qualité des résultats par rapport à un algorithme traditionnel une fois que toute la base a été modifiée. Pour simuler un comportement réaliste du système, nous avons procédé à des remplacements par intervalle de 1-3% à chaque génération. Les résultats ont montré que lorsque Pumsb était remplacé à un rythme de 1% par génération, la qualité des -473 -RNTI-E-6
Analyse des usages dans les systèmes pair-à-pair résultats à la fin du processus de remplacement est de 100%.
Conclusion
Dans cet article, nous proposons une nouvelle approche pour améliorer la localisation de ressources dans des systèmes P2P non structurés. Cette approche est inspirée des algorithmes génétiques pour retrouver efficacement les séquences fréquentes dans les noeuds du réseau. Les expériences réalisées ont montré que cette approche est efficace d'une part pour retrouver les comportements fréquents (100% des fréquents sont déterminés en 7 générations quelque soit leur longueur) mais également pour prendre en compte les évolutions dans le réseau (modification forte du comportement des noeuds).

Introduction
Poussés par la demande des étudiants branchés, un grand nombre d'universités et d'éta-blissements scolaires se sont lancés dans le design, le développement et l'utilisation des technologies de l'information et de la communication pour créer, partager et diffuser leur matériel pédagogique.
Le but de notre de recherche est de favoriser l'accès aux ressources pédagogiques afin de promouvoir la formation continue et la diffusion des derniers résultats de recherche. Plus pré-cisément, nous voulons développer un système de classification et d'organisation qui permettra de donner accès aux ressources pédagogiques créées par les professeurs suivant les besoins des utilisateurs. Cet accès pourra se faire :
-suivant la structure d'enseignement (plan de cours) ; ce sera le chemin d'accès privilégié des étudiants inscrits dans une université ; -suivant des ontologies de domaines ou par mots-clés ; ce sera le chemin d'accès privilé-gié du grand public qui recherche des documents sur un sujet ou un thème donné -suivant les compétences que permettent de développer la lecture des documents ; ce sera le chemin privilégié des personnes qui veulent parfaire leur formation ou acquérir de nouvelles compétences.
Modèle de compétences et ressources pédagogiques
L'utilisation de la notion de compétence par les gestionnaires et les spécialistes des ressources humaines, a permis aux organisations de comprendre l'importance de leurs ressources humaines et de reconnaître que les gens, les connaissances, les capacités et les habiletés réunis dans le milieu du travail constituent un levier fondamental pour leur réussite.
Suite à ce constat, de nombreuses recherches ont porté sur la compréhension et la définition de la notion de compétence. Les conclusions d'une étude transcanadienne montre que les élé-ments communs qui ressortent le plus souvent dans la définition du concept de compétences à travers les organismes canadiens sont : les connaissances, les habiletés, les capacités, les aptitudes, les qualités personnelles, le comportement et l'impact sur le rendement du travail. 
Conclusion et Travail futur
Ce travail est la première partie du développement d'un serveur de ressources pédago-giques basés sur les compétences. Ce serveur s'intégrera dans l'architecture de Zone Cours (zonecours.hec.ca), outil de gestion de ressources pédagogiques de HEC Montréal, et viendra compléter nos outils de diffusions de connaissances vers le grand public. Bloom B. (1956) 
Références
Summary
The aim of our of research is to give access to teaching resources according to users needs and according to competences they want to acquire. We present here a model of competences and resources on which our future system will be based.

Introduction
La recherche d'information dans les bases de données image est toujours un défi. Pour l'être humain, l'accès à la sémantique d'une image est naturel et non explicite. Par conséquent, la sémantique provient de l'image sans processus cognitif explicite. Dans la vision par ordinateur, il existe plusieurs niveaux d'interprétation. Le plus bas est celui des pixels et le plus haut est celui des scènes ; entre eux beaucoup de niveaux d'abstraction existent. Le défi est alors de remplir la gouffre entre le bas niveau et le haut niveau.
Il existe au moins deux issues intermédiaires auxquelles nous nous intéressons. La première est la représentation de l'image sous forme de vecteurs qui est appelée indexation. Elle consiste à extraire quelques caractéristiques (composantes d'un vecteur) à partir de la représen-tation de bas niveau(Pixel). Par exemple, l'histogramme des couleurs, les différents moments, les paramètres de forme, etc. La seconde issue est l'ensemble des étiquettes associées à une image. Ces étiquettes sont fournit par l'humain au moyen de mots, d'adjectifs, ou au moyen de tout autre attribut symbolique. Les étiquettes sont compréhensibles et mieux manipulées. La sémantique peut être considérée comme le résultat du traitement des attributs symboliques qui sont liés à l'image.
Donner à l'ordinateur la capacité d'imiter l'être humain dans l'analyse de scènes nécessite d'expliciter le processus par lequel il peut se déplacer de la représentation bas niveau à la haut niveau (niveau sémantique). Le bas niveau utilise les caractéristiques, qui peuvent être extraites, à partir des données multimédias comme la couleur, la texture, la forme, etc. Le haut niveau quant à lui, consiste généralement en une liste de mots-clés qui est associée à la donnée multimédia qui sert à décrire son contenu sémantique. L'utilisation d'annotations textuelles présente deux inconvénients principaux : le premier est le fait que cette tâche est lente et très coûteuse ; la seconde est lié à la subjectivité de l'annotation des données multimédias. En effet, par exemple, deux personnes différentes peuvent annoter la même image de deux manières différentes.
A cause de ces inconvénients, l'interrogation est généralement faite en utilisant les caractéristiques de bas niveau. Dans toutes les approches d'interrogation de données multimédias (l'approche bas niveau ou l'approche haut niveau), chaque donnée est localisée par ses coordonnées dans un espace multidimensionnel R p . Un vecteur de caractéristiques (caractéristiques de bas niveau ou annotations textuelles) est associé à chaque donnée. Rui et Huang (1999) estiment que l'interrogation par le contenu ne peut être effectuée de manière efficace uniquement en combinant les deux niveaux (bas niveau et haut niveau). Cependant, ceci peut soulever le problème de la subjectivité des annotations ce qui est un problème important qui peut détériorer considérablement les performances d'un système de recherche d'informations par le contenu.
Afin de capturer des aspects sémantiques à partir des caractéristiques de bas niveau, l'utilisation d'un index multimédia est nécessaire. Un index permet de regrouper des individus ayant des caractéristiques assez proches.
Plusieurs systèmes de recherche d'informations par le contenu se basent sur le principe des k plus proche voisins (Fix et Hudges, 1951) en utilisant une mesure de similarité (Veltkamp et Tanase, 2000). L'idée est de trier les individus de la base de données, en fonction de leur distance, par rapport à l'individu requête, et ensuite répondre à la requête en retournant un nombre k fixe d'individus les plus proches. Par exemple, le système QBIC, dans son implé-mentation pour le musée de l'Hermitage 1 (Faloutsos et al., 1994) renvoie les 12 plus proches images voisines de l'image requête. Les inconvénients d'une telle approche sont discutés dans Scuturici et al. (2004).
Le modèle de structuration des bases de données multimédias est (ou peut être vu) comme un graphe basé sur des relations de similitude entre les individus, par exemple K-NN (Mitchell, 1997) ou le graphe des voisins relatifs (Scuturici et al., 2004). L'objectif est d'explorer une base de données d'images par les similarités entre les images. Explorer les similarités peut être considéré comme la recherche des voisins des images requêtes.
Le modèle de structuration est très important car les performances d'un système de recherche d'informations par le contenu dépend fortement sur la structure de représentation (structure d'indexation) qui gère les données.
Plusieurs systèmes de recherche d'informations multimédias ont été proposés. les systèmes de recherche d'images sont plus répandus que ceux pour la vidéo. Nous pouvons citer par exemple QBIC (Flickner et al., 1995;Niblack et al., 1993), CANDID (Kelly et al., 1995), CHABOT (Ogle et Stonebraker, 1995), VIRAGE (Ogle et Stonebraker, 1995), PhotoBook (Pentland et al., 1994), BlobWorld (Carson et al., 1999), VisualSeek (Chang et al., 1996;Smith et Chang, 1997) et RETIN (Fournier et al., 2001) pour les images, et CVEPS (Chang et al., 1996), JAKOB (La-Cascia et Ardizzone, 1996), VISION (Li et al., 1996), et SWIN (Zhang et al., 1995) pour la vidéo.
A partir de maintenant, nous considérerons le contexte de l'interrogation des grandes bases de données images par le contenu pour illustrer les propositions de cet article.
Graphes de voisinage
Les graphes de voisinage sont utilisés dans divers systèmes. Leur popularité est due au fait que le voisinage est déterminé par des fonctions cohérentes qui reflètent, d'un certain point de vue, le mécanisme de l'intuition humaine. Cependant, plusieurs problèmes relatifs au graphes de voisinage sont toujours d'actualité et exigent des travaux détaillés afin de les résoudre. Ces problèmes sont principalement liés à leur coût de construction élevé et à leurs difficultés de mise à jour. Pour cette raison, les optimisations sont nécessaires pour leur construction et leur mise à jour.
Afin d'éviter quelques problèmes liés à l'utilisation des K-NN (problème de symétrie, subjectivité liée à la détermination du parcmètre k), l'utilisation d'un autre modèle de structuration basé sur les graphes de voisinage a été proposé dans ( (Scuturici et al., 2004)). Cette proposition a beaucoup d'avantages, c'est pourquoi nous adoptons la même approche (l'utilisation des graphes de voisinage) pour l'interrogation d'images par le contenu. Nous allons dans ce qui suit présenter les graphes de voisinage.
Les graphes de voisinage ou graphes de proximité sont des structures géométriques qui utilisent le concept de voisinage pour déterminer les sommets les plus proches d'un sommet donné. Pour cela, ils se basent sur les mesures de "distances" (Toussaint (1991)). Nous allons utiliser les notations suivantes dans cet article :
Soit ? un ensemble de points dans un espace multidimensionnel R d . Un graphe G(?,?) est composé de l'ensemble de points ? et de l'ensemble d'arêtes ?. A chaque graphe nous pouvons associer une relation binaire R sur ?, dans laquelle un couple de points (?, ?) ? ? 2 sont en relation binaire si et seulement si (?, ?) ? ?. En d'autres termes, (?, ?) sont en relation binaire si et seulement s'ils sont directement reliés dans le graphe G. A partir de là, le voisinage V (?) d'un point ? dans le graphe G, peut être considéré comme un sous-graphe qui contient le point ? ainsi que tous les points qui sont directement relié à ce point.
Plusieurs possibilités ont été proposées pour la construction des graphes de voisinage. Nous pouvons citer la triangulation de Delaunay (Preparata et Shamos, 1985), le graphe des voisins relatifs (Toussaint, 1980), le graphe de Gabriel (Gabriel et Sokal, 1969) et l'arbre de recouvrement minimum (Preparata et Shamos, 1985). Dans cet article, nous considérons seulement l'un d'entre eux, le graphe des voisins relatifs (RN G). La motivation principale pour ce choix est sa simplicité et sa large utilisation. Nous décrivons ci-après deux exemples de graphes de voisinage : le graphe des voisins relatifs (RN G) et le graphe de Gabriel (GG).
Graphe de voisins relatifs
Dans un graphe de voisins relatifs G rng (?, ?), deux points (?, ?) ? ? 2 sont des voisins s'ils vérifient la propriété de voisinage définie ci-après.
Soit H (?, ?) une hyper-sphère de rayon ? (?, ?) et de centre ?, et soit H (?, ?) une hypersphère de rayon ? (?, ?) et de centre ?. ? (?, ?) et ? (?, ?) sont des mesures de similirité entre les deux points ? et ?. ? (?, ?) = ? (?, ?). Alors, ? et ? sont des voisins si et seulement si la Toussaint (1980)). Formellement :
La figure 1 illustre le graphe des voisins relatifs.
Graphe de Gabriel
Ce graphe est proposé par Gabriel et Sokal (1969) dans un contexte de mesure de variations géographiques. Soit H (?, ?) l'hyper-sphère de diamètre ? (?, ?) (cf. figure 2). Alors, ? est le voisin de ? si et seulement si l'hyper-sphère H (?, ?) est vide.Formellement
Algorithmes de construction des graphes de voisinage
Nous pouvons considérer deux situations quand nous traitons le problème d'optimisation des graphes de voisinage. La première situation est quand nous avons à disposition un un graphe déjà construit. Dans cette situation, si nous utilisons une méthode d'approximation, nous risquons d'obtenir un autre graphe avec un voisinage de moindre qualité que l'existant, nous pouvons obtenir plus ou moins de voisins pour quelques individus. Dans ce cas, nous devons trouver une solution pour mettre à jour efficacement le graphe sans le reconstruire entièrement. La deuxième situation est celle où le graphe n'est pas encore construit. Dans cette situation nous pouvons appliquer une méthode d'approximation pour avoir un graphe qui est aussi similaire que possible à celui que nous pouvons obtenir en utilisant l'algorithme standard. Nous sommes intéressés dans cet article par le premier cas.
Plusieurs algorithmes pour la construction des graphes de voisinage ont été proposés. Les algorithmes que nous citons ci-après concernent la construction du graphe des voisins relatifs.
L'une des approches commune aux différents algorithmes est l'utilisation des techniques dédé n raffinement ? z. Dans ce type d'approches, le graphe est construit par étapes. Chaque graphe est construit à partir du graphe précèdent, contenant toutes les connexions, en éliminant un certain nombre dŠarrêtes qui ne vérifient pas la propriété de voisinage du graphe à construire. L'élagage (élimination des arrêtes) se fait généralement en tenant compte de la fonction de construction du graphe ou à travers des propriétés géométriques.
Le principe de construction des graphes de voisinage consiste à chercher pour chaque point si les autres points de lŠespace sont dans son voisinage. Le coût de cette opération est de complexité O(n 3 )(n étant le nombre de points dans lŠespace). Toussaint (Toussaint, 1991) a proposé un algorithme de complexité O(n 2 ). Il déduit le RN G à partir dŠune triangulation de Delaunay (Preparata et Shamos, 1985). En utilisant les voisins géographiques (Octant neighbors) Katajainen (1988)  En ce qui nous concerne, l'approche que nous proposons est une amélioration de celle déjà proposée dans Scuturici et al. (2004). En effet, avec l'ancienne méthode, le graphe n'est pas vraiment mis à jour. Les voisins d'un individu requête sont considérés comme étant les voisins de son plus proche voisin. Cette approche n'est pas correcte car dans un espace multidimensionnel et avec les contraintes géométriques à respecter, les voisins d'un individu ne peuvent être ceux de son voisin le plus proche. Ainsi, en utilisant cette méthode le graphe sera inévita-blement détérioré. Nous proposons dans ce qui suit une méthode de mise à jour locale efficace qui est stable et insensible aux effets de la dimension des données).
Recherche d'informations par le contenu : approche par graphes de voisinage
L'interrogation des bases de données images est généralement faite par la soumission d'une requête au système, cette requête est généralement sous forme d'image, le système pré-traite (segmente, égalise, etc.)la requête et produit un vecteur de descripteurs qui représente un point dans un espace multidimensionnel. Ce point est inséré dans la structure de représentation (structure d'indexation) et ses voisins sont alors retournés comme une réponse à la requête.
Dans notre cas, une approche naïve en utilisant les graphes de voisinage serait la reconstruction du graphe de voisinage qui contient les données déjà existantes dans la base de données tout en ajoutant l'individu requête. Cette approche n'est, malheureusement, pas appropriée car elle est très coûteuse particulièrement quand le nombre d'individus dans la base de données est important. Une autre approche est de mettre à jour localement le graphe de voisinage, c'est-à-dire, trouver une manière de telle sorte que seuls les individus potentiellement voisins soient affectés par la possible modification ou interrogation.
La tâche de mise à jour locale des graphes de voisinage passe par la localisation du point inséré aussi bien que les points qui peuvent être affectés par la mise à jour. Pour cela, nous procédons en deux étapes principales : nous recherchons d'abord une surface optimale de l'espace de représentation pouvant contenir un nombre maximum de points potentiellement voisins au point requête. La deuxième étape est réalisée dans le but de filtrer les individus trouvés préa-lablement afin de récupérer les vrais voisins en considérant une propriété de voisinage. Cette dernière étape cause la mise à jour effective des relations de voisinage entre les points concernés.
L'étape principale dans cette méthode est la détermination de la surface de recherche. Ceci peut être considéré comme un problème de détermination d'une hyper-sphère ayant pour centre -16 -RNTI-E-6 le point requête ? maximisant les chances de contenir les voisins du point requête tout en réduisant au minimum le nombre de points qu'elle contient.
Nous tirons profit de la structure générale des graphes de voisinage afin d'établir le rayon de l'hyper-sphère. Nous nous concentrons particulièrement sur le concept du voisin le plus proche et le concept du voisin le plus éloigné. Ainsi, deux observations en relation avec ces deux concepts nous semblent intéressantes :
-Les voisins du voisin le plus proche du point requête sont des candidats potentiels au voisinage du point requête.
A partir de là et par généralisation, nous pouvons déduire que : -Tous les voisins directs d'un point sont également des candidats au voisinage d'un point requête pour lequel il est voisin. Concernant la première étape, le rayon de l'hyper-sphère ayant les propriétés citées cidessus est celui comprenant tous les voisins du plus proche voisin de la requête. Ainsi, en considérant que l'hyper-sphère est centrée dans ?, son rayon est égale à la somme des distances entre le point requête ? et de son plus proche voisin et celle entre le voisin le plus proche et son voisin le plus éloigné.
Le contenu de l'hyper-sphère est traité pour vérifier s'il existe quelques voisins (ou tous les voisins). La deuxième étape constitue une étape de renforcement et vise à éliminer le risque de perdre des voisins ou d'en inclure des faux. Cette étape procède de telle sorte à tirer profit de la deuxième observation. Ainsi, nous prenons tous les vrais voisins du point requête, récupérés précédemment (ceux retournés dans la première étape), ainsi que que leurs voisins et mettons à jour les relations de voisinage entre ces points.
Considérons alors ? le point requête et ? son plus proche voisin avec une distance ? 1 . Considérons aussi ? le voisin le plus loin de ? avec une distance ? 2 . Le rayon SR de l'hypersphère peut être exprimé avec la formule suivante : SR = ? 1 + ? 2 + est un paramètre de relaxation, il peut être fixé selon l'état des données (leur dispersion par exemple) ou par la connaissance du domaine. Nous avons fixé expérimentalement ce paramètre à 1.
La complexité de cette méthode est très basse et se rejoint parfaitement notre objectif de départ (localisation des voisins d'un point dans un temps trés court). Elle est exprimée par :
avec -n :le nombre d'individus dans la base de données.
-n :le nombre d'individus dans l'hyper-sphère (<< n). Cette complexité inclue les deux étapes décrites précédemment, à savoir, la recherche du rayon de l'hyper-sphère et la mise à jour du voisinage dans l'hyper-sphère. Le deuxième terme correspond au temps nécessaire pour la mise à jour effective des relations de voisinage entre les vrais voisins. Le temps necessaire pour cette opérations est trés faible. Cette complexité constitue la complexité maximale et peut être optimisée de plusieurs manières. La plus simple est d'employer un algorithme plus robuste pour la recherche du plus proche voisin à la place du parcours séquentiel. L'exemple ci-après illustre et récapitule le principe de la méthode. En ce qui nous concerne, nous sommes intéressés dans la présente section par deux différents types d'évaluation : évaluation de la validité des résultats obtenus par l'utilisation de la méthode proposée et l'évaluation des temps de réponse. Nous utilisons pour cela une base de données d'images (Nene et al., 1996). Cette base de données contient 7200 images repré-sentant 100 objets différents pris avec sous diverses vues. Pour réaliser nos expérimentations, nous pré-traitons la base de données en appliquant quelques algorithmes de traitement d'image pour extraire certaines caractéristiques. Nous employons particulièrement la couleur, quelques caractéristiques de texture et de forme. Chaque image est représentée sous forme d'un vecteur de caractéristiques. Un graphe de voisinage est alors construit en utilisant les vecteurs. L'interrogation est également faite de la même manière en employant un vecteur de caractéristiques. Ainsi, chaque image est représentée comme un point dans un espace multidimensionnel.
Notre intérêt en effectuant ces expériences est de montrer que notre méthode donne les mêmes résultats que ceux obtenus en construisant un graphe de voisinage avec un algorithme standard (qualité des résultats).
Pour vérifier la validité des résultats obtenus, nous avons besoin d'un graphe de référence. Pour cela, nous prenons la base de données entière (vecteurs de descripteurs) et nous construisons un graphe de voisins relatifs. Après avoir établi le graphe de référence, nous pouvons commencer les tests de validité. Nous prenons arbitrairement un individu de la base de données et nous construisons un nouveau graphe de voisins relatifs avec les individus restant dans la base de données (les n ? 1 individus). Après cela, nous insérons l'individu préalablement retiré dans le graphe en employant la méthode proposée. Les voisins du point inséré sont alors comparés aux voisins de ce même individu dans le graphe de référence. Cette opération est répétée plusieurs fois en prenant différents individus à chaque itération.
Le Tableau 1 illustre un voisinage obtenu en insérant des individus aléatoirement pris de la collection d'image. Chaque individu dans l'ensemble de données est identifié par un identifiant unique. La première colonne de chaque tableau constitue l'individu requête. Nous pouvons clairement voir que les résultats obtenus après l'insertion de l'individu restant (Graphe 2), sont exactement les mêmes que ceux obtenus en employant l'algorithme standard (Graphe 1). Ainsi, nous ne perdons pas de voisins. Une évaluation globale (utilisant les mesures de rappel et de précision) est donnée dans (Hacid et Zighed (2005)  IMG74  IMG74  IMG87  IMG87  IMG85  IMG118  IMG85  IMG118  IMG131  IMG131  IMG3665  IMG3665  IMG484  IMG484  IMG623  IMG623  IMG7170  IMG7170  IMG4804  IMG4763  IMG4804  IMG4763  IMG4803  IMG4803  IMG4805  IMG4805  IMG6532  IMG6532  IMG4345  IMG4345  IMG6781  IMG6781  IMG6791  IMG6791  IMG6807  IMG6795  IMG6807  IMG6795  IMG6825  IMG6825  IMG6830  IMG6830  IMG74  IMG74  IMG87  IMG87  IMG7196  IMG7171  IMG7196  IMG7171  IMG7195  IMG7195  IMG7197  IMG7197 TAB. 1 -Comparaison du voisinage obtenu en utilisant un graphe de référence (Graphe 1) et en utilisant des insertions locales (Graphe 2)
La validité des résultats obtenus étant acquise, à présent nous sommes intéressés par les temps de réponse en utilisant la méthode proposée, c'est-à-dire, les temps que la méthode prend pour insérer un individu requête dans une structure existante. Nous employons le même protocole que celui décrit précédemment, mais au lieu de récupérer les individus voisins, nous prenons en considération uniquement les temps de réponse de chaque requête. Nous utilisons pour ces expériences une machine avec un processeur INTEL Pentium 4 (2.80 Gz) et 512 Mo de mémoire. Les temps de réponse de 10 insertions, sont montrés dans le graphique dans la figure 4.
Les temps de réponse (exprimés en millisecondes) sont intéressants en considérant le volume de données utilisé, ils sont variable d'un individu à un autre, ceci est due particulièrement 
Conclusion et travaux futurs
La recherche d'informations par le contenu dans les bases de données multimédias est une tache complexe en raison de, principalement, la nature des données multimédias et la subjectivité liée à leur interprétation. L'utilisation d'une structure d'index appropriée est primordiale.
Dans cet article nous nous sommes intéressés aux graphes de voisinage qui constituent notre structure d'indexation, nous avons proposé une méthode pour mettre à jour localement les graphes de voisinage dans un but d'apporter une amélioration à une approche existante. Notre méthode est basée sur la localisation des individus potentiels, qui peuvent être affectés par la mise à jour. Les expérimentations effectuées sur différents ensembles de données ont montré l'efficacité et l'utilité de la méthode proposée.
En tant que travaux futurs, nous projetons de fixer le problème du paramètre de relaxation en proposons une fonction de détermination automatique en tenant compte de quelques statistiques comme la dispersion. Par ailleurs, nous envisageons d'étendre cet algorithme afin de mettre en place une version incrémentale de construction des graphes de voisinage. L'intégra-tion de cette approche et son application dans plus de fonctions liées aux données multimédias comme l'annotation automatique et la classification de données multimédias constituent une suite logique de ce travail. Carson, C., M. Thomas, S. Belongie, J. M. Hellerstein, et M. Jitendra (1999). Blobworld : A system for region based image indexing and retrieval. Visual information and informa-

La gestion des connaissances en conception
La réutilisation des connaissances métier produites lors des projets antérieurs est une stratégie majeure pour améliorer les processus de conception. Actuellement, il est critique de mettre à la disposition des concepteurs les ressources documentaires et bases de données représentant ces connaissances. Les sources des connaissances métier auxquelles nous nous intéressons sont les Systèmes de Gestion des Données Techniques (SGDT). Ces outils sont considérés parfois comme des systèmes de gestion des connaissances quand il s'agit d'optimiser les liens et les relations entre ressources produites par les différents collaborateurs (Cattan, 2001). Pour rendre les ressources disponibles dans les SGDT au service des concepteurs, il faut prendre en compte non seulement les SGDT de l'entreprise mais aussi les SGDT des partenaires (sous-traitants, clients, fournisseurs…). Dans des travaux précédents (projets industriels), nous avons mis en place des solutions de gestion des connaissances autour des SGDT et nous avons rencontré les obstacles suivants : rigidité des structure des données, difficulté de migration et d'interopérabilité, pauvreté des fonctions de recherches. Pour dépasser ces limites, nous avons fait appel à l'approche du Web Socio-Sémantique.
Le Web Socio-Sémantique en support des SGDT
Dans le cadre de la conception de produits industriels, notre préoccupation principale est de doter les connaissances métier d'une représentation formelle pour rechercher et réutiliser plus pertinemment ces connaissances. Le contenu des ressources SGDT représentant les connaissances à réutiliser doit être ainsi interprétables par les outils informatiques pour qu'ils soient capables de répondre aux requêtes des utilisateurs. D'où notre recours au Web Séman-tique qui vise, selon Tim Berners-Lee, rapporté par (Dieng et al., 2004), à rendre le contenu sémantique des ressources du Web interprétables non seulement par l'homme mais aussi par des programmes, pour une meilleure coopération entre humains et machines.   
Summary
In this paper we discuss an approach based on the Socio-Semantic Web concept enabling knowledge reuse through an extensible infrastructure, in the product development process.

Introduction
XQuery devenant le standard pour interroger XML, de nouveaux besoins apparaissent pour la recherche d'information. Buston et Rys (2003) spécifient des prédicats et fonctionnalités de recherche d'information à intégrer à XQuery, comme la recherche d'élément contenants des mots-clefs, le classement de résultats selon leur pertinence, la recherche basé sur des suffixes ou préfixes de mots. Un premier ensemble des fonctionnalités requises pour XQuery Text est défini par Buxton et Rys (2003). TexQuery, Amer-Yahia (2004), en est le langage précurseur.
Certaines des fonctionnalités citées précédemment, comme la simple recherche de motsclefs, sont très communes et présentes dans la plupart des SGBD. Dans le cas de données distribuées, il faut d'abord recomposer les partitions avant de pouvoir effectuer une recherche sur le contenu ; d'importantes fonctionnalités souvent nécessaires aux applications ne sont pas faciles à implanter dans un système distribué. Le classement des résultats, les recherches conjonctives de mots-clefs, les recherches sur les racines de mots, leurs préfixes ou suffixes, sont difficilement réalisables car il faut auparavant recomposer les données dispersées. stockant dans un Patricia trie. Cette approche nécessite des extensions pour garder l'ordre des chemins et permettre la correspondance partielle des noms d'éléments.
Notre approche propose d'unifier les capacités des sources au travers de vues. Le médiateur définit une vue des données distribuées sur plusieurs sources, et permet son interrogation en XQuery Text. Le médiateur ne matérialise pas la vue pour éviter la réplication des données ; il indexe la position du texte dans les éléments structurant la vue. Nous proposons un système d'indexation efficace du contenu textuel de la vue qui repose sur un guide de la vue (applé ViewGuide), véritable résumé structurel de la vue, dérivé de la définition de la vue. Notre système est adapté pour la localisation de mots-clefs dans la vue, et la localisation des données sur les sources. XQuery/IR, Bremer et Gertz (2002), propose des techniques similaires de recherche d'information pour XQuery. Il est aussi basé sur une indexation adaptée à la structure arborescente de XML, et permet de résoudre des requêtes "tree pattern" (arbre de filtres applicables à des données XML) dans un système centralisé. L'originalité de notre approche est d'indexer des vues virtuelles et d'avoir une solution complète opérationnelle et efficace, comme le montre les premières mesures.
La suite de cet article est organisée comme suit. La section 2 présente le système d'indexation de vues proposé. La section 3 présente le traitement des requêtes sur les vues indexées et la méthode de classement des résultats suivant leur pertinence. Des résultats expérimentaux de notre système sont ensuite rapportés. La conclusion rappelle les contributions et introduit les travaux futurs.
Indexation textuelle de vues
La principale question est de savoir comment intégrer des méthodes de recherche de contenu sur des sources distribuées et hétérogènes. Les systèmes de médiation utilisent souvent les vues pour cibler la recherche sur les sources de données pertinentes. Pour combiner l'intérêt des vues avec la recherche d'information, nous avons décidé d'utiliser des vues semi-matérialisées : le contenu de la vue est indexé par le médiateur, mais n'est pas stocké.
Principes de base
Nous avons choisi d'indexer le contenu de la vue lors de sa création et de maintenir l'index lors des mises à jour ; la position des termes de la vue est mémorisée au niveau du médiateur, ce qui permet de répondre efficacement à des requêtes XQuery Text. L'index détermine indirectement l'adresse des éléments ; cela évite d'importants transferts de données entre les sources et le médiateur : seules les données pertinentes sont échangées. Cela évite également au médiateur de manipuler au travers d'opérations complexes de recherche d'information l'ensemble des données. Ainsi, gérer un index de vue compact et efficace est le but de notre approche pour éviter au médiateur ces opérations difficiles.
Les identifiants utilisés dans notre index référencent à l'aide de structures gérées par les adaptateurs des objets sur les sources. Ces structures permettent la localisation, l'extraction et la recomposition de données de la vue efficacement à partir des sources. Lorsqu'une source est mise à jour, celle-ci doit le reporter au médiateur afin de mettre à jour les identifiants dans l'index. Ceci est fait par un mécanisme de trigger ou bien par polling périodique de la source. Le mécanisme de report dépend de l'adaptateur de la source.
Position des termes dans la vue
Pour retrouver les éléments contenant un terme, le contenu textuel de la vue doit être indexé de façon précise. La position d'un terme dans la vue est identifiée par le document de la vue et le chemin (path) de l'élément le contenant. Nous proposons un système de d'identification d'élément pour coder cette position de façon compacte et unique.  Le ViewGuide est quelque peu similaire au DataGuide (Widom et al.), mais il diffère par les points suivants : (i) c'est uniquement un résumé structurel des documents de la vue ; (ii) il est dérivé de la définition de la vue (c'est-à-dire de la requête définissant la vue) ; (iii) ses liens sont annotés avec la cardinalité (multiple ou non) des éléments. Le ViewGuide permet d'attribuer un identifiant d'élément (IDE) à chaque élément d'un document de la vue. Chaque élément correspond à un identifiant numérique unique, déterminé par un parcours préfixe de l'arbre. La figure 1 présente un exemple de ViewGuide. Un élément critic d'un document de cette vue ne contient qu'un élément book (monovalué) mais peut englober un ou plusieurs éléments review (multivalué). La création du guide impose que la requête définissant la vue (figure 1) décrive complètement la structure des documents dans sa clause Return.
Système de numérotation
Un IDE est composé de la façon suivante : -Un préfix, l'identifiant correspondant au chemin dans le guide de vue.
-Un suffixe, regroupant les cardinalités de tous les éléments multivalués traversés de la racine à l'élément concerné. Pour identifier le chemin correspondant à critic/review/p, le ViewGuide traverse les éléments I, VII puis VIII. Seul le dernier identifiant (VIII) est utile car le chemin est unique dans le ViewGuide. Le document exemple de la figure 1 possède un élément review contenant deux éléments p, ce qui correspond au chemin critic/review[1]/p [2]. L'identifiant de cet élément sera donc codé par VIII(1,2). Le suffixe (1,2) associé à l'identifiant de chemin VIII permet d'identifier de manière unique le deuxième élément p du premier élément review. Le suffixe n'est utilisé que dans le cas d'éléments multivalués ; un élément monovalué n'aura pas de suffixe, par exemple VI code le chemin critic/book/title.
Pour indiquer la position d'un élément dans la vue, l'identifiant de document (IDG) est associé à l'identifiant de chemin (IDE). Dans la vue de la figure 1, chaque élément critic est un document. L'élément author de la seconde review du quatrième document de la vue est identifié par <4-X(2)>. Un tel couple identifie de manière unique un élément dans la vue.
Pour coder des chemins multiples, nous utilisons des patterns d'identifiant d'éléments permettant de spécifier un ou plusieurs éléments multivalués traversés. Le chemin critic/review/p[1] ne spécifie pas quel élément review choisir. Il se code avec le pattern VIII(*,1), correspondant au premier p de n'importe quel review. Les patterns d'identifiants sont utilisés pour le traitement de requête.
Définition: Pattern d'identifiants (IDEP)
. IDE ayant pour suffixe des *, signifiant que tout élément est valide.
Index des mots
Le médiateur gère une liste inversée des termes importants, donnant pour chacun sa position dans la vue virtuelle. 
Définition: Index des mots (Word index
Position dans les sources de données
Une structure appelée "Source Map" maintient un mapping entre un document (IDG) et les sources contenant les données composant ce document. Ces données sont considérées comme des documents locaux référencés par un identifiant. Cet identifiant est associé à une opération d'extraction.
Définition : Identifiant de document local (IDL).
Identifiant numérique alloué par un wrapper permettant de récupérer les données correspondantes sur une source.
Lors de la création de la vue, chaque wrapper de source contenant des données pour la vue reçoit une requête pour extraire des données. Les wrappers fournissent ces données au médiateur qui construit les résultats suivant la définition de la vue. Pour chacune de ces données renvoyées, un IDL est créé.
La correspondance IDL vers objet local dépend du wrapper. Pour un wrapper fichier, l'IDL peut être l'URI du fichier. Pour des bases XML, ce peut être un identifiant de document, pour des bases relationnelles, une référence à une requête SQL/XML ou XQuery permettant la transformation de tables relationnelles en XML. A partir d'un IDL, le wrapper est capable d'interroger la source pour renvoyer les données correspondantes. Le médiateur recompose alors le document suivant la définition de la vue en récupérant les données.
Definition: Source Map. Structure de mapping entre un IDG et un ou plusieurs IDL. Par exemple, si les données book, review et comment de la vue sont réparties sur trois sources, l'IDG du document exemple aura alors quatre IDL (correspondant successivement à un book, une review et deux comment).
Traitement de requête
L'algorithme de traitement de requêtes retrouve les entrées de l'index correspondant à une recherche textuelle (liste de mots-clef). Les parties correspondant aux documents sélectionnés sont alors extraites pour recomposer les documents. Nous détaillons dans cette partie comment s'effectue une recherche d'éléments contenant des mots-clefs et comment étendre cette recherche aux fonctionnalités XQuery Text. Nous introduisons aussi notre système de classement de résultats suivant leur pertinence. 
Trouver les éléments pertinents
FIG. 2 -Algorithme d'intersection
La recherche des éléments résultat s'effectue en 3 étapes : 1. Déterminer l'espace de recherche ; 2. Calculer l'ensemble des éléments contenant chacun des mots-clefs ; 3. Extraire les données des sources et recomposer les résultats. L'espace de recherche se définit en utilisant les patterns d'identifiants d'élément. Le NIP est dérivé de l'expression XPath correspondant au prédicat de la requête.
Le XPath définissant l'espace de recherche dans la requête exemple est critic/review. Il correspond au pattern VII(*). La recherche des mots-clefs s'effectue sur l'élément review et tous ses descendants ; les mots-clefs peuvent se trouver parmi les éléments comments, comment, p, author ou rating. Pour retrouver les identifiants correspondant au descendant d'un élément, une matrice booléenne indique la relation ancêtre/descendant entre deux identifiants. Un élément C ij = 1 si i est un ancêtre de j dans le guide de la vue. Cette matrice fournit une méthode rapide pour retrouver la relation entre deux éléments. L'espace de recherche critic/review est ainsi déterminé par les patterns allant de VII(*) à XII(*).
Le médiateur interroge l'index des mots pour calculer l'ensemble des entrées contenu dans l'espace de recherche pour chaque mot-clef. L'ensemble des éléments résultat est
Classement des résultats
Une méthode de classement associe un poids, basé sur la pertinence, à chaque résultat. Le médiateur doit classer les résultats provenant de plusieurs sources et les regrouper pour les renvoyer dans l'ordre du classement. L'architecture proposée permet de pré-calculer le poids de chaque résultat avant de recomposer les données ; le calcul est réalisé lors de l'interrogation de l'index des mots. La formule de classement doit être précise mais aussi calculable avec les informations contenues dans l'index.
Le poids d'un résultat est la somme des poids de chaque élément du résultat contenant un ou plusieurs mots-clefs. Notre approche est basée sur la spécificité de chaque résultat. Cette méthode donne plus d'influence aux éléments proche de la racine du résultat. En effet, les mots proches de la racine sont plus importants que ceux dans des éléments plus profonds de l'arbre résultat. Cette méthode reste encore assez simple puisqu'elle ne prend pas en compte la position des mots relativement entre eux.
Les éléments contenant plusieurs mots recherchés voient leur influence également augmenter. Le pourcentage de mots-clefs présent dans l'élément par rapport à l'ensemble des mots-clefs recherchés est un facteur polynomial ajustant le poids d'un élément.
Finalement, la formule suivante calcule le poids d'un élément:
W i est le poids du mot recherché k i , basé sur le tf.idf du mot, N est le nombre total de mots recherchés dans le prédicat. Ni est le nombre de mots recherchés présents dans l'élément et d est la distance entre l'élément et la racine du résultat (nombre de liens). La constante ? permet de faire varier l'influence de la distance à la racine. ? est un facteur polynomial qui permet d'augmenter l'influence des éléments contenant plus de mots recherchés. Le poids total d'un résultat est la somme des poids de chaque élément contenant des mots-clefs.
Cette formule modulable peut s'adapter au besoin de l'utilisateur. Elle peut être étendue ou remplacée. Ainsi le médiateur peut intégrer d'autres formules reposant sur les informations de l'index des mots. La formule est ajustable suivant l'application.
D'autres systèmes proposent des solutions concrètes pour classer les résultats d'une requête de recherche de mots-clefs. XRANK, Lin et al. (2003), propose une formule calculée suivant le nombre d'arc entrant et sortant (inter et intra document) d'un élément. Ce système utilise comme métrique de proximité des termes la fenêtre minimum contenant les termes, facteur qui reste trop global ne tenant pas compte de la structure du résultat. La distance est aussi prise en compte comme notre approche, ou les éléments les plus éloignés ont moins d'importance. D'autres systèmes comme XXL, Anja et Gerhard (2002), se base sur un opérateur d'imprécision donnant un degré de similarité entre la structure d'un résultat et la demande de la requête. XIRQL, Norbert et Kai (2001), découpe les documents en objets et recherche ces objets suivant leur pertinence de contenu.
Retour d'expériences
Nous avons testé les performances du système sur trois jeux de données. Les données présentées dans le tableau 3 sont stockées dans des SGBD XML. La taille des données est la taille de la vue composée suivant la structure de la vue critic. Nous avons mesuré le temps de recherche pour trois prédicats : Le second sous-tableau du tableau 3 donne les temps d'exécution de q01 pour 5 motsclefs. Pour chaque jeu de données, l'exécution est réalisée avec une vue indexée, et avec un opérateur de recherche (le médiateur s'occupe de l'opération de recherche dans chaque document). Le temps présenté pour la recherche avec l'index inclut le temps de recherche dans l'index (Word Index et Source Map), l'extraction et la recomposition des résultats par le médiateur. Pour l'opérateur de recherche, le temps inclut la recomposition des résultats suivant la définition de la vue, puis l'application du prédicat de recherche de mots-clefs par un opérateur de sélection. Comme prévu, l'exécution en utilisant l'index est plus efficace car seulement les résultats pertinents sont extraits des sources. Pour chaque jeu de donnée, un ratio de 3 est obtenu pour des requêtes sélectionnant 66% de l'ensemble des documents. Le temps de recherche dans l'index (algorithme d'intersection de listes) est présenté dans le dernier tableau du tableau 3, pour la requête q01. Comparé au temps d'exécution total de la requête (tableau précédent), la recherche dans l'index représente moins de 1%. Ces tests préliminaires montrent la validité de l'approche : moins de données sont transférées, et l'opération de recherche au niveau du médiateur est plus rapide avec un index.
Le premier graphe de la figure 3 illustre les temps mis par l'algorithme d'intersection pour retrouver les entrées pertinentes pour les requêtes q02 et q03. On remarque que q02 s'exécute plus rapidement que q03 ; ceci est dû aux sélections faites dans l'algorithme d'intersection, l'espace de recherche de q02 étant plus restreint que celui de q03.
Le second graphe de la figure 3 présente les temps de recherche dans l'index pour q01. Pour chaque jeu de donnée, le temps de recherche augmente linéairement lorsqu'un nouveau mot-clef est ajouté à la recherche. Finalement, pour des requêtes basiques (moins de 10 mots-clefs), le système est efficace et le temps de recherche dans l'index est négligeable.
Conclusion
Dans cet article, nous avons présenté l'intégration de XQuery Text dans un médiateur XML. La principale difficulté est d'intégrer des sources ne répondant pas à ces capacités. Pour cela nous proposons d'utiliser des vues indexées pour permettre d'intégrer ces fonctionnalités à ces sources. Le médiateur indexe les vues en utilisant un résumé structurel de la vue. Ce guide permet de coder la position des éléments (XPath) de la vue pour en indexer le contenu. L'opérateur de recherche de mots-clefs utilise des algorithmes basés sur ce système d'identifiant pour l'exécution de requête XQuery Text. Le système intègre une formule de classement adaptée à la structure arborescente des résultats XML.
Il reste d'autres aspects important à aborder dans la gestion des capacités des sources. Lorsqu'une source reconnaît une partie de XQuery Text, les vues construites devraient prendre en compte cette capacité et limiter l'indexation aux sources non capables en distribuant le traitement de la requête aux sources capables. Le classement de résultats semble simple pour une vue ou une source, mais le classement global doit être testé plus en détails, notamment en comparaison avec d'autres formules dans de vraies applications. Le dernier aspect à préciser est la gestion des mises à jours sur les sources et dans l'index, qui doit être mis à jour lors de l'insertion ou de la suppression d'objets dans les sources.

Introduction
Le problème auquel s'intéresse cet article est la découverte de nouvelles familles de ré-actions chimiques à partir de bases de données de réactions. Cet article montre en quoi ce problème peut se reformuler en un problème particulier de fouille de graphes. La découverte de nouvelles réactions présente un grand intérêt pour la synthèse en chimie organique, discipline dont le but est la conception de molécules complexes à partir de composants chimiques usuels et de réactions. En effet, plus un expert de la synthèse a de réactions à sa disposition, plus il peut créer de nouveaux produits à partir d'un ensemble donné de molécules et plus il peut optimiser le plan de synthèse d'une molécule cible donnée. Par ailleurs, la découverte de dizaines de millions de réactions a vite rendu leur recensement nécessaire à travers la constitution de très grandes bases de données de réactions. Ces bases de données réactionnelles sont plus particulièrement exploitées par les experts de la rétrosynthèse. Cette méthode consiste à inférer le plan de synthèse d'une molécule cible en recherchant les réactions qui permettent d'aboutir à la cible, puis à réitérer récursivement le processus en prenant pour cibles les réac-tifs des réactions ainsi trouvées et ce jusqu'à l'obtention de réactifs de départ jugés ordinaires. La rétrosynthèse peut donc tirer un excellent parti de tout modèle prédictif capable de propo-ser des réactions qui n'ont jamais été testées mais qui ont de forte chance d'être réalisables expérimentalement.
Pour établir un tel modèle prédictif qui soit suffisamment fiable, certaines méthodes d'apprentissage automatique ont été appliquées aux bases de données réactionnelles, notamment des méthodes de voisinage symbolique (Régin, 1995). Mais leurs résultats restent limités tant leurs calculs de généralisation (appliqués de surcroît à des graphes) s'avèrent longs , et tant leurs procédures d'induction se révèlent sensibles aux inexactitudes engendrées par la pauvreté des graphes moléculaires en tant que modèle de représentation des réactions. A ce titre l'emploi par Berasaluce et al. (2004) d'une méthode de recherche de motifs fréquents (Agrawal et Srikant, 1994) s'est révélé judicieux dans la mesure où de telles méthodes sont à la fois adaptées à de grands volumes de données et robustes aux incohérences partielles des données puisque basées sur des probabilités. La principale faiblesse d'une telle approche est de travailler sur des données booléennes et donc de ne pouvoir réellement prendre en compte la topologie des atomes dans une molécule, pourtant essentielle à la compréhension des réactions.
Les travaux présentés dans cet article se situent dans le prolongement de ceux de Berasaluce et al. (2004). La différence majeure réside dans la prise en compte de la topologie des molécules par le recours à des techniques de fouille de graphes, c'est-à-dire de généralisation des méthodes de fouille de données booléennes à des graphes. L'apport principal de cet article est de montrer comment la recherche de nouveaux schémas de réactions à partir de bases de réactions peut se reformuler en un problème particulier de fouille de graphes. Pour se faire, les notions de chimie organique utiles à la compréhension du problème sont introduites (section 2). Les réactions décrites dans les bases de données réactionnelles sont ensuite modélisées sous la forme de graphes de réaction, à partir desquels des schémas de réactions particuliers appelés graphes de réaction partiels sont dérivés (section 3). Après un bref état de l'art des algorithmes de fouille de graphes, le problème d'apprentissage des mécanismes réactionnels est reformulé comme un problème particulier de fouille de graphes de réaction partiels (section 4).
Les graphes moléculaires et les schémas de réactions
Une molécule est un assemblage géométrique d'atomes solidaires liés par des liaisons de covalence. La forme développée ou graphe moléculaire est une représentation de la topologie des liaisons d'une molécule sous forme d'un graphe étiqueté g(V, E, ?) où les sommets V (g) = V et les arêtes E(g) = E représentent respectivement les atomes et les liaisons de covalence de la molécule et où la fonction d'étiquetage ? : V ? L associe (au minimum) à un sommet l'élément chimique, tel le carbone (C) ou l'hydrogène (H), de l'atome représenté par ce sommet. Un graphe moléculaire est un graphe particulier en ce sens que les sommets  FIG. 1 -Equation de la réaction de déshydratation du propan-2-ol avec appariement partiel des atomes (numéros encerclés) représentant un même élément chimique ont des degrés (i.e. le nombre d'arêtes incidentes à un sommet) tous égaux à la valence de cet élément (4 pour C, 1 pour H). Une réaction chimique est quant à elle un processus physique qui, en chimie organique, brise certaines liaisons de covalence pour en créer de nouvelles, transformant ainsi un ensemble de molécules appelées réactifs en un ensemble de nouvelles molécules appelées produits. Elle se représente par une équation chimique, comme illustrée sur la figure 1, mettant en rapport les formes développées des réactifs et des produits.
Les schémas de molécules (resp. les schémas de réactions) sont des graphes moléculaires (resp. des équations chimiques) dont certains sommets représentent des variables, remplaçant par une opération dite de contraction, un radical, c'est-à-dire un groupe d'atomes connectés. De telles variables peuvent être typées auxquels cas leurs ensembles de définition se restreignent à des radicaux d'un type particulier. Certains types de schémas qualifiés dans cet article de partiels peuvent de plus, autoriser à ce que certains sommets ne soient pas saturés, c'est-à-dire que leur degré puisse être strictement inférieur à la valence de leur élément chimique. Un graphe moléculaire satisfait un schéma partiel s'il contient un sous-graphe qui par une série de contractions (compatibles avec les types des variables du schéma) est isomorphe au schéma. Ainsi le schéma de réactions représenté sur la figure 2 est satisfait par la réac-
2 -Un schéma de la déshydratation d'un alcool secondaire tion de la figure 1, le groupe méthyl CH 3 étant une valeur acceptable pour une variable R de type alkyle, représentant toute chaîne linéaire d'atomes de carbone saturée en hydrogène. Ce schéma est partiel puisque les atomes de carbone (de valence 4) numérotés 1 et 2 ne sont pas incidents à 4 liaisons et sont donc non saturés. Un schéma de molécules (resp. de réactions) permet de représenter une molécule (resp. une réaction) générique instanciée par une famille de molécules (resp. une famille de réactions) de la même manière qu'un concept d'un langage de représentation des connaissances est satisfait par ses instances. produits, pourtant indissociables, ne peuvent être mis en corrélation que par une information totalement étrangère (les appariements) à leur mode de description (les graphes moléculaires). L'introduction d'un graphe de réaction, illustré sur la figure 3 (a), permet de rattacher cause et effet de la réaction en un seul objet. Ce graphe de réaction résulte de la superposition des atomes appariés entre les graphes moléculaires des réactifs et des produits. Formellement ce graphe se construit à partir des graphes moléculaires des réactifs auxquels on ajoute les arêtes élémentaires nouvellement crées par la réaction (une liaison multiple étant dissociée en un nombre de liaisons élémentaires égal à sa multiplicité). De plus chaque arête est marquée d'une étiquette précisant s'il s'agit d'une arête inchangée, brisée ou créée. La figure 3 (a) représente le graphe de réaction associé à l'équation chimique de la figure 1. Les arêtes stables, brisées et créées y sont représentées respectivement en trait continu, en pointillés et en en trait épais. 
, peuvent être définis trois sous-graphes présentant un intérêt particulier.
-Le graphe du coeur C(R) = R · (E ? (R) ? E + (R)) est le sous-graphe de R réduit 1 par l'ensemble de ses arêtes brisées ou créées. Ce graphe représente le coeur de la réaction, c'est-à-dire l'ensemble des atomes dont les liaisons de covalence sont modifiées lors de la réaction. Le graphe de coeur du graphe de réaction de la figure 3 (a) est représenté sur la figure 3 (b). -Le graphe des réactifs R(R) = R · (E ? (R) ? E 0 (R)) est le sous-graphe de R réduit par l'ensemble de ses arêtes brisées ou inchangées. Ce graphe représente l'union des graphes moléculaires des réactifs. Le graphe des réactifs associé au graphe de réaction de la figure 3 est identique à la partie gauche de l'équation chimique de la figure 1. -Le graphe des produits P(R) = R·(E 0 (R)?E + (R)) est le sous-graphe de R réduit par l'ensemble de ses arêtes inchangées ou créées. Ce graphe représente l'union des graphes moléculaires des produits. Le graphe des produits associé au graphe de réaction de la figure 3 est identique à la partie droite de l'équation chimique de la figure 1. On démontre que le graphe de coeur est un ensemble connexe de cycles de longueurs paires disjoints par leurs arêtes. Chaque cycle est une suite alternée de liaisons brisées et de liaisons créées. La preuve repose sur une démonstration similaire à celle, demeurée célèbre, qu'Euler apporta au problème des ponts de Konigsberg (Pour plus de détails, on se référera à un manuel de théorie des graphes comme par exemple Gondran et Minoux (1995)). Vu que dans un graphe de coeur un cycle de longueur 0 n'a pas de sens et qu'un cycle de longueur 2 peut être vu comme une liaison stable, on peut conclure que les graphes de coeur de réaction sont des systèmes de cycles alternés de longueurs paires supérieures ou égales à 4.
D'un point de vue purement informationnel, le graphe de réaction est rigoureusement équi-valent à une équation chimique appariée puisqu'il est possible de passer indifféremment d'un formalisme de représentation à l'autre. Mais les avantages du graphe de réaction sont multiples : outre certains avantages en terme de complexité algorithmique qui sont ici hors sujet, le graphe de réaction permet de représenter naturellement le lien entre la cause et l'effet d'une réaction et ce en un seul objet, via un graphe connexe. Cette association est indispensable pour généraliser les réactions et approcher l'expression des mécanismes réactionnels sous-jacents qui rattache nécessairement les effets à leurs causes. Enfin le graphe de coeur peut servir à réaliser une partition et donc une indexation efficace des réactions d'une base de données ré-actionnelles.
A ce titre, on introduit ici la notion de réaction nulle qui désigne une absence de toute ré-action lors de la mise en présence d'un ensemble donné de réactifs dans des conditions expéri-mentales données. Le graphe de coeur d'une réaction nulle est évidemment le graphe vide (sans sommets). Les graphes de réaction des réactions nulles sont les seuls à ne pas être connexes et dans ce cas uniquement se confondent avec les graphes des réactifs (ou indifféremment les graphes des produits). Les réactions nulles servent d'exemples négatifs supplémentaires utiles pour interdire certaines généralisations irréalistes de réactions. Malheureusement les bases de données réactionnelles ne contiennent pas la description de réactions nulles, ce qui est pour le moins normal vu l'intérêt tout aussi nul qu'elles présentent en synthèse organique. On peut cependant construire des réactions nulles en émettant l'hypothèse que la plupart des réactions d'une base de données réactionnelles forment des produits stables. Le graphe des produits de cette réaction peut alors servir de graphe de réaction pour une nouvelle réaction nulle. C'est pourquoi on suppose désormais qu'une base de données réactionnelle est un ensemble de graphes de réaction indexés par leur graphe de coeur éventuellement nul.
Les graphes de réaction partiels
L'introduction des graphes de réaction permet de définir un type particulier de schémas de réactions (au sens de celui défini dans la section 2) tenant compte de l'appariement des atomes des réactifs et des produits. Ces schémas appelés graphes de réaction partiels, tiendront lieu de motifs dans nos algorithmes de fouille de graphes.
Un graphe de réaction partiel est défini comme un schéma de molécule contenant un et un seul graphe de coeur et dont les variables sont toutes mono-atomiques, c'est-à-dire qu'elles ne peuvent représenter qu'un seul atome. Les types des variables mono-atomiques se confondent alors avec un ensemble (L, d'étiquettes généralisées ordonné selon un ordre de subsomption : les étiquettes les plus spécifiques sont les éléments chimiques et l'étiquette la plus gé-nérale, notée remplace tout atome quel qu'il soit. Les étiquettes intermédiaires dans l'ordre induit par permettent de regrouper les éléments dont les propriétés chimiques sont similaires, comme par exemple la famille des halogènes dont font notamment partie le chlore et le brome. La figure 4 (a) présente un graphe de réaction partiel qui est satisfait par la réaction de la figure 1.
Ces schémas de réactions sont introduits ici car ils revêtent d'excellentes propriétés vis à vis des algorithmes de fouille de graphes. Ainsi chaque sommet ne peut être apparié qu'à un seul atome ce qui facilite la gestion des appariements entre les graphes partiels et les graphes Il est donc possible par une suite de spécialisations de générer tout graphe de réaction partiel à partir de son graphe du coeur. Par ailleurs et compte tenu de la connaissance des mécanismes réactionnels dont les chimistes disposent, la grande majorité des réactions sont le résultat d'une succession de réactions élémentaires dont le graphe de coeur se réduit à un seul cycle. Notre étude peut donc se restreindre à l'étude des réactions élémentaires dont les graphes partiels peuvent être générés à partir de l'ensemble des cycles alternés de longueur paire supérieure ou égale à 4 et dont tous les sommets sont étiquettés par Ce dernier ensemble est lui même trivial à générer. La figure 5 représente la suite (incomplète) de spécialisations passant du  Au coeur de chaque réaction élémentaire se trouve un mécanisme réactionnel, c'est-à-dire un processus temporel et déterministe de transformation qui brise certaines liaisons de covalence pour en créer d'autres. Les graphes de réaction partiels peuvent servir de modèles de représentation des mécanismes réactionnels. Ce modèle n'est pas exact et induit des erreurs de prédiction lorsqu'il est confronté à une base d'exemples de réactions. Ces erreurs se manifestent par des exemples qui satisfont la cause de la réaction générique sans en satisfaire l'effet. L'effet d'un mécanisme réactionnel est modélisé par le graphe de coeur du graphe partiel, traduisant la redistribution des liaisons de covalence, alors que la cause du mécanisme est modélisée par un graphe que l'on décide d'appeler réacton. Le réacton d'un schéma désigne l'ensemble des schémas des réactifs, c'est-à-dire le graphe des réactifs déduit du graphe de réaction partiel. Sur la figure 4 est illustré le réacton (b) associé au graphe partiel (a). Plus formellement étant donné un ensemble R des réactions (y compris des réactions nulles) dans des conditions réactionnelles fixées, on note g r (r) (resp. g re (r)) le graphe de ré-action (resp. le graphe des réactifs) d'une réaction r. Etant donné un graphe partiel de réaction s, la réaction r satisfait s si s est un sous-graphe de g r (r), c'est-à-dire si la cause (l'environnement topologique du coeur) et l'effet (la redistribution des liaisons de covalence) décrits par s se retrouve dans r. A l'inverse une réaction r infirmera s si le réacton g re (s) de s est un sous-graphe du graphe des réactifs g re (r) sans que r satisfasse s, c'est-à-dire si la cause se trouve dans r alors que l'effet ne s'y trouve pas. Un exemple de réaction qui satisfait (resp. qui infirme) un schéma s est dit positif (resp. négatif ) pour s. La figure 6 exhibe un exemple positif (a) et un exemple négatif (b) pour le graphe de réaction partiel de la figure 4. Il est important de noter que ces exemples ne sont valables que dans des conditions réactionnelles bien précises (ici une température supérieure à 110 °C). Si la température devient inférieure à ce seuil de 110 °C, l'expérience montre que la réaction de la figure 1 devient une réaction nulle et passe donc du statut d'exemple positif à celui d'exemple négatif. Etant donné un ensemble {r i } 1?i?n d'exemples de graphes de réaction éventuellement nulles, il est alors possible de définir la fréquence positive f + (s) (resp. la fréquence négative f ? (s)) d'un graphe partiel s comme le nombre d'exemples positifs (resp. négatifs) pour s. Plus la fréquence négative d'un schéma est faible, plus le schéma est fiable, plus la fréquence positive d'un schéma est grande, plus le schéma est général.
La fouille de graphes
La majorité des méthodes de fouille de données et d'apprentissage s'applique à des données où chaque objet est décrit par la liste des attributs booléens qu'il vérifie. La recherche des motifs fréquents et d'extraction des règles d'association (Agrawal et Srikant, 1994) est une de ces méthodes les plus employées. Etant donné une base d'objets décrits par la liste de leurs attributs, choisis dans un ensemble A, l'extraction de motifs fréquents consiste à énumérer dans un premier temps les conjonctions d'attributs, ou motifs, qualifiés de fréquents, c'est-à-dire dont le nombre d'occurrences (définies par les objets contenant simultanément tous les attributs du motif) dans la base de données, encore appelé fréquence ou support, est supérieur à un seuil fixé arbitrairement. L'extraction des règles d'association consiste dans un deuxième temps à déduire les règles d'association non triviales entre motifs fréquents dont la probabilité conditionnelle, ou confiance, est supérieure à un second seuil.
Certains travaux initiés notamment par Inokuchi et al. (2000)  
La fouille de graphes de réaction partiels
Les modèles retenus pour la représentation des mécanismes réactionnels sont les graphes de réaction partiels. Etant donné une base de données réactionnelles, chaque graphe partiel g r peut être associé à une fréquence positive f + (g r ) et une fréquence négative f ? (g r ). Ces deux fréquences sont des fonctions décroissantes par rapport à l'ordre partiel (G r , ? G ) des graphes de réaction partiels (i.e.
. Un graphe partiel est d'autant plus pertinent que sa fréquence positive est élevée et que sa fréquence négative est faible, comme l'illustre la figure 7 élaborée dans le prolongement des exemples précédents. Dans cet exemple, la base de réactions est censée contenir N 1 , N 2 et N 3 réactions mettant respectivement en jeu des alcools primaires, secondaires et tertiaires (i.e des molécules dont l'atome de carbone C portant le groupe alcool OH est relié respectivement à un, deux et trois autres atomes de carbone). Les réactions mettant en jeu des alcools primaires sont nulles alors que les alcools secondaires et tertiaires réagissent selon le coeur de la figure 4 (b). En outre la base de réaction est supposée représenter de manière homogène les différents types d'alcool. N 1 , N 2 et N 3 sont donc supposés tous égaux à N , de manière à pouvoir calculer simplement les fréquences des graphes de réaction partiels de la figure 7 (a), regroupées dans le tableau (b). Il apparaît ainsi que le graphe (c) est plus pertinent que le graphe (a) du fait d'une fréquence négative plus grande pour une même fréquence positive et que le graphe (e) du fait d'une fréquence positive plus faible pour une même fréquence négative. 
FIG. 7 -Graphes de réaction partiels ordonnés et leurs fréquences
Différentes familles de graphes partiels peuvent être étudiées, chacune accordant une importance différente aux fréquences positives et négatives. En raison de sa simplicité, on s'inté-resse ici à la famille G(s + ) des graphes partiels dont la fréquence négative est minimale tout en ayant une fréquence positive supérieure ou égale à un seuil s + fixé arbitrairement. Ainsi l'exemple de la figure 7 permet d'établir que G(2 · N ) = {c} et G(N ) = {c, e, f } en supposant par ailleurs que tous les graphes non représentés contenant les graphes (c), (e) et (f) ont une fréquence positive qui leur est strictement inférieure (et donc, pour être totalement exact, que les graphes partiels représentés regroupent l'ensemble des graphes de leur fermé).
Pour calculer facilement une approximation G(s + ) de cet ensemble, on choisit dans le cas tangent où un graphe g 1 est inclus dans un autre graphe g 2 et que ces deux graphes ont les mêmes fréquences négatives, de ne conserver dans G(s + ) que le graphe le plus spé-cifique g 2 . Ce choix certes arbitraire, parfois même contraire au choix du graphe le plus pertinent, permet lorsqu'il est combiné au caractère décroissant de la fréquence négative, d'approximer l'ensemble G(s + ) par la frontière positive de s + c'est-à-dire par l'ensemble des graphes partiels fréquents (positivement par rapport à s + ) maximaux, soit l'ensemble
G(N ) = {e, f } omet l'élément c pourtant le plus pertinent. Les inexactitudes introduites par cette approximation sont toutefois compensées par la possibilité de traiter le problème comme celui d'une recherche de graphes fréquents maximaux et de le résoudre en tant que tel grâce à certains algorithmes adaptés comme Spin de Huan et al. (2004). Deux modifications doivent malgré tout être apportées à un algorithme vertical de fouille de graphes pour qu'il soit adapté à notre formulation du problème : -D'une part la génération des motifs par les algorithmes de fouille doit être modifiée afin de construire les motifs, c'est-à-dire les graphes de réaction partiels, à partir d'un graphe de coeur, c'est-à-dire un cycle alterné de liaisons brisées et créées. Ceci est nécessaire afin de garantir la présence unique et entière d'un graphe de coeur dans chaque motif. Cette modification est facile à intégrer au sein des algorithmes de type verticaux dont les motifs croissent à partir d'un motif initial, en général égal au motif vide mais qui peut être initialisé à une autre valeur. -D'autre part la phase de détermination des fréquences par le balayage de la base de graphes doit être modifiée pour être capable de calculer deux fréquences. En particulier la fréquence négative doit se calculer en testant dans les graphes de la base l'inclusion non du motif mais celle d'un motif secondaire (le réacton) dérivé du motif initial. Il est donc possible d'adapter certains algorithmes existants de fouille de graphes pour qu'ils extraient l'ensemble des graphes de réaction partiels maximaux fréquents (positivement). Cet ensemble de résultats peut ensuite être soumis à un expert en chimie afin qu'il analyse l'exactitude et l'originalité des schémas de réactions découverts.
Discussion
Le choix arbitraire de l'ensemble de graphes partiels maximaux fréquents comme ensemble d'étude mérite une analyse critique. On propose ici une formalisation abstraite du problème gé-néral de l'apprentissage de mécanismes réactionnels afin de mieux situer la pertinence de la méthode proposée au paragraphe 4.3. Le constat initial pour une telle formalisation est qu'un graphe de réaction partiel représente d'autant mieux un mécanisme réactionnel que sa fré-quence positive est grande et que sa fréquence négative est faible. Ces deux fréquences étant toutes deux des fonctions décroissantes par rapport à ? G , la recherche des graphes partiels les plus représentatifs n'a de sens que si on se donne un critère d'optimalité capable de pondérer l'importance accordée à la fréquence positive relativement à la fréquence négative. Ce critère peut se définir formellement à partir d'un ensemble totalement ordonné (E, ?) et d'une fonction c : R 2 ? E qui associe aux couples des fréquences (f + (g), f ? (g)) d'un graphe partiel g un élément de E. La seule contrainte imposée est que c soit une fonction croissante (resp. décroissante) de f + (resp. de f ? ). Entre deux graphes comparables par ? G , le graphe dont le critère c est le plus petit peut ainsi être éliminé. Les graphes partiels résistant à cette élimina-tion, c'est à dire les maxima locaux de c, sont sans nul doute les plus pertinents au sens de c. Cet ensemble peut alors être mis sous la forme d'une liste L triée selon les valeurs décrois-santes de leur image par c, c'est-à-dire par ordre d'intérêt décroissant. Seuls les motifs dont le critère c est supérieur à un seuil arbitraire c 0 sont conservés dans L. La liste L constitue l'ensemble des graphes de réaction partiels les plus pertinents relativement à c et à la base de données réactionnelles considérée.
Le choix de c est arbitraire et ouvre de nombreuses possibilités pour qualifier différem-ment l'ensemble L des résultats. L'ensemble G(s + ) présenté au paragraphe 4.3 peut ainsi être formalisé par l'ensemble L associé à un ensemble E = R + × R ? muni de l'ordre lexicographique et par le critère c : (
. Le caractère alambiqué de l'expression de c paramétré de surcroît par un seuil arbitraire s + tend à prouver le manque de pertinence du choix de G(s + ). Cet ensemble a été présenté dans cet article uniquement parce que certains algorithmes existants de fouille de graphes fréquents permettent de l'estimer directement. Nous envisageons plutôt de travailler avec d'autres critères c, comme celui que nous appelons fiabilité, défini par c(f
f + +f ? compris entre 0 et 1. Dans la mesure où la base de réactions couvre de manière homogène l'ensemble des phénomènes physiques qu'elle est censée décrire, la fiabilité constitue une approximation de la confiance (c'est-à-dire de la probabilité conditionnelle) associée au schéma de la réaction, interprété dans ce cas comme une règle de transformation reliant une hypothèse à une conclusion. En ce sens la fiabilité est un critère plus pertinent qui par ailleurs croit bien avec f + et décroît avec f ? . Nous exigeons de plus que la fréquence f + reste supérieure à un seuil s + de manière à ne retenir que les sché-mas suffisamment généraux (en particulier en écartant comme schémas les graphes de réaction de la base dont la fiabilité est évidemment égale à son maximum 1 mais dont la fréquence positive valant aussi 1 est trop faible) et surtout de manière à limiter l'espace de recherche infini à un sous domaine fini (contrainte qui s'avère indispensable pour éviter toute récursion infinie de la part des algorithmes de fouille verticaux). Un autre critère envisageable est la différence c(f + , f ? ) = f + ? f ? . Le tableau (b) de la figure 7 évalue pour chaque graphe les deux critères de fiabilité et de différence. Les maxima locaux de fiabilité sont (c), (e) et (f). Il est à noter que la fiabilité ne peut prendre en compte la généralité d'un schéma et favoriser ainsi le graphe partiel (c) vis à vis des deux autres. Pour pallier ce défaut, il faudrait rajouter la fréquence positive comme critère secondaire à la manière du double critère du paragraphe 4.3. Le critère de différence permet de tenir compte très simplement de la généralité : il donne bien le graphe (c) comme unique maximum. A terme d'autres critères inspirés de la théorie de l'information permettront d'optimiser le compromis fiabilité -généralité.
La méthode d'optimisation de tels critères c, que nous pensons intéressante, est incompatible avec les algorithmes de fouille de graphes actuels. La recherche de maxima locaux de fonctions c non monotones dans (G r , ? G ) vont à l'encontre du principe de ces algorithmes qui parcourent l'ensemble des motifs selon un ordre prédéterminé en évitant toutes comparaisons avec les motifs voisins. Nous envisageons donc un nouveau type d'algorithme adapté à la formalisation du critère c qui ne peut être détaillé ici par manque de place.
Conclusions
Le problème de la recherche de réactions génériques fiables peut grâce à une modélisation adéquate se reformuler en un problème de fouille de graphes. Ce problème s'apparente à celui de la recherche de motifs qui sont à la fois fréquents dans une base et non fréquents dans une autre, avec cette originalité qu'un motif s'exprime différemment dans chacune des deux bases. Toutefois des particularités induites tant par la connaissance des mécanismes réactionnels que par notre propre modélisation du problème nous poussent à entrevoir une méthode générale de fouille de graphes pour laquelle les algorithmes existants sont inadaptés. Un nouvel algorithme inspiré de cette modélisation est en cours de développement et devrait à terme valider les idées introduites dans cet article. De nombreuses expériences sur des bases de données réactionnelles seront alors possibles et permettront notamment d'étudier l'influence du critère d'optimisation c. La méthode introduite n'étant pas spécifique à la chimie, il sera également possible de l'appliquer à d'autres types de données modélisables sous forme de graphes. Cette perspective motivante ne doit cependant pas faire oublier que les performances de calcul et plus encore la pertinence des résultats aux yeux des chimistes restent deux inconnues majeures du problème que seule l'expérimentation pourra lever.

Introduction
Ce papier traite du problème de forage de plusieurs bases de données gigantesques et géo-graphiquement distribuées dans le but de produire un ensemble de règles de classification qui expliquent les groupements de données observés. Le résultat de ce forage sera donc un méta-classificateur aussi bien prédictif que descriptif. En d'autres termes, nous visons à produire un modèle qui permet non seulement de prédire la classe de nouveaux objets, mais qui permet aussi d'expliquer les choix de ses prédictions. Nous croyons que ce genre de modèles, basés sur des règles de classification, devrait aussi être facile à comprendre par des humains, ce qui est également l'un de nos objectifs. Il faut dire toutefois que nous nous plaçons dans le contexte où il est impossible de rapatrier toutes ces bases dans un même site, et ce, soit à cause du temps de téléchargement, soit à cause de l'impossibilité de traiter la base ainsi agrégée.
Dans la littérature, les techniques de forage distribué de données à la fois prédictives et descriptives sont malheureusement peu nombreuses. La plupart d'entre elles tentent de produire -95 -RNTI-E-6 un méta-classificateur sous forme d'un ensemble de règles à couverture disjointe où un objet est couvert par une et une seule règle. Nous montrerons dans cet article que cette contrainte de couverture disjointe n'est pas nécessaire pour produire un méta-classificateur fiable. Ainsi, nous proposons une technique simple où un objet peut être couvert par plusieurs règles. La relaxation de cette contrainte de couverture disjointe nous permet de produire un classificateur final rapide, sans que le taux d'erreur de celui-ci n'en souffre. Cet article procède comme suit. Dans la section 2, une vue d'ensemble des techniques d'agrégation de modèles les plus connues est présentée. Puis, dans la section 3, nous présen-tons notre solution au forage distribué des données (FDD) employant l'agrégation de modèles (FDD-AM) basée sur un coefficient de confiance. Dans la section 4, nous présentons nos ré-sultats d'expérimentations qui démontrent la viabilité de notre méthode. La section 5 compare la complexité asymptotique de notre méthode à celles rencontrées dans la littérature. Nous présentons finalement une conclusion et nos travaux futurs.
Techniques existantes d'agrégation de modèles
Nous présentons dans ce papier uniquement les techniques qui ont été développées dans un but de forage distribué de données. Conséquemment, nous ignorons volontiers : le système « Ruler » (Fayyad et al., 1993)  (Fayyad et al., 1996) qui a été construit dans le but de regrouper plusieurs arbres de décision construits sur un même ensemble de données dans un système centralisé, le système d'apprentissage distribué (Sikora et Shaw, 1996) développé dans un cadre de gestion des systèmes d'information afin de bâtir un système apprenant distribué (Distributed Learning System, DLS) et l'approche de fragmentation (Wüthrich, 1995) qui utilise des règles probabilistes. En outre, nous ignorons les techniques purement prédictives telles que bagging (Breiman, 1996), boosting (Schapire, 1990), stacking (Tsoumakas et Vlahavas, 2002), arbiter et combiner (Chan, 1996), (Prodromidis et al., 2000).
L'algorithme MIL
L'algorithme MIL (Multiple Induction Learning) a été initialement proposé par Williams (1990) afin de résoudre le conflit entre les règles conflictuelles (voir définition ci-dessous) dans des systèmes experts. Hall et al. (1998a,b) ont repris la technique de Williams pour agré-ger des arbres de décision bâtis en parallèle et préalablement transformés en règles. En outre, ils ont étendu la technique pour prendre en considération d'autres types de conflits. Le processus d'agrégation proposé par ces auteurs n'est autre qu'un regroupement des règles muni d'un processus de résolution des éventuels conflits. Il est à noter que cette résolution des conflits ne traite qu'un couple de règles conflictuelles à la fois. Deux règles se voient en situation de conflit quand leurs prémisses sont consistantes tandis qu'elles produisent deux classes diffé-rentes (Williams, 1990) (appelé conflit de type I), ou lorsque les conditions des prémisses se chevauchent partiellement (Hall et al., 1998a) (appelé conflit de type II) ou quand les règles ont le même nombre de prédicats avec des valeurs différentes associées aux conditions et classent les objets vers la même classe (Hall et al., 1998b) (appelé conflit de type III). La résolution de conflits consiste soit à spécialiser une ou les deux règles en conflit (conflits type I et II), soit à ajuster la valeur de la condition, c.-à-d., la borne de test, pour les conflits de type II et III et éventuellement fusionner les deux règles en conflit (conflit de type III). Dans certains cas -96 -RNTI-E-6 (conflits de type I et II), de nouvelles règles sont ajoutées en se basant sur les ensembles d'entraînement de celles-ci pour récupérer la couverture perdue par l'opération de spécialisation.
Le système DRL (« Distributed Rule Learner »)
La technique DRL (« Distributed Rule Learner ») (Provost et Hennessy, 1996) a été conçue et implantée en tirant avantage de la propriété du cloisonnement-invariant (Provost et Hennessy, 1994). DRL commence par partitionner les données d'entraînement E en nd sousensembles disjoints, assigne chacun (E i ) à une machine, et fournit l'infrastructure pour la communication entre les différents apprenants (nommé RL et roulant chacun sur une machine différente). Quand une règle répond au critère d'évaluation pour un sous-ensemble des données (f ? (r, E i , nd) ? c ; f ? est une fonction d'évaluation d'une règle et c une constante), elle devient une candidate pour répondre au critère d'évaluation global ; la propriété de cloisonnementinvariant étendue garantit que chaque règle qui est satisfaisante sur l'ensemble des données sera acceptable au moins sur un sous-ensemble. Lorsqu'une copie locale du RL découvre une règle acceptable, elle envoie la règle aux autres machines pour mette à jour ses statistiques sur le reste des exemples. Si la règle répond au critère d'évaluation global (f (r, E) ? c ; f est la fonction d'évaluation principale et c une constante), elle est signalée comme règle satisfaisante. Dans le cas contraire, ses statistiques locales sont remplacées par les statistiques globales et la règle est rendue disponible pour la spécialiser encore plus. La propriété de cloisonnementinvariant garantit que chaque règle satisfaisante sur l'ensemble des données sera trouvée par l'un des RL.
Fusion d'ensembles de règles générés en parallèle
Le travail présenté par Hall et al. (1999) est un mélange des deux derniers travaux présentés ci-dessus, en d'autres termes les travaux de (Williams, 1990), (Hall et al., 1998b) et (Provost et Hennessy, 1996). Spécifiquement, à chaque règle créée lui est associée une mesure de sa « qualité » qui est basée sur la précision ainsi que le nombre et le type des exemples qu'elle couvre.
La technique proposée dans (Hall et al., 1999) est l'utilisation de ce que Provost et Hennessy (1996) proposent (voir §2.2), à une différence près où la suppression de la règle de l'espace des règles en considération ne se fait que lorsque la règle classe toutes les données des différentes bases et qu'il s'avère que sa mesure f (r, E) est inférieure au seuil. Il est à noter que chaque règle ne « voyage » pas d'un site à un autre toute seule, mais bel et bien accompagnée des valeurs nécessaires pour calculer la mesure associée à chaque règle.
Toutefois, les auteurs de (Hall et al., 1999) démontrent que, dans le cas extrême, la propriété de cloisonnement-invariant risque de ne pas être satisfaite. Ainsi, ils suggèrent que la précision des règles agrégées peut être très différente de la précision des règles bâties sur l'ensemble d'entraînement entier. En outre, les auteurs soulignent qu'en cas de conflits entre règles, ces derniers peuvent être résolus, comme décrit par (Hall et al., 1998b) et (Williams, 1990).
Par ailleurs, Hall et al. (1999) traite un nouveau type de conflit entre règles. Il s'agit d'une règle ayant un intervalle (c.-à-d., deux conditions) chevauchant un intervalle d'une deuxième règle. Dans ce cas, une règle plus générale est créée en combinant les deux règles conflictuelles et en ajustant les bornes des intervalles.
Discussion
La technique MIL souffre de plusieurs défauts. Tout d'abord, le processus de résolution de conflit ne fait que spécialiser encore plus les règles en se basant sur les ensembles d'entraî-nement des règles de classification. Les règles générées peuvent exhiber un faible pouvoir de classification si elles sont appliquées à de nouveaux objets, et ce, surtout dans le cas de bases d'entraînement très bruitées. En plus, si les règles sont déjà très spécifiques à un ensemble d'entraînement, cette méthode est incapable de les généraliser puisqu'elle ne fait que regrouper puis spécialiser encore plus les règles en conflit. En outre, l'adaptation de la technique de Williams afin de traiter des bases distribuées implique une augmentation du volume de données échangées entre les différents sites. En effet, d'une part, chaque règle voyage accompagnée de l'index des objets couverts et, d'autre part, en cas de conflit, tous les objets couverts par une des deux règles en conflit sont rapatriés du site d'entraînement vers le site qui résout les conflits.
Le plus important inconvénient du système DRL est le temps d'exécution. En effet, lorsqu'une règle est jugée acceptable par un site donné, elle doit passer par tous les autres sites afin de mettre à jour ses variables statistiques en fonction de leurs données. En d'autres termes, toute règle acceptable sur un site doit classer toutes les données de tous les autres sites. Ainsi, la règle doit, d'une part, « voyager » à travers tous les sites, et d'autre part, classer les données de chaque site. Si une règle n'est pas jugée satisfaisante sur l'ensemble des données, celle-ci est spécialisée et le processus recommence si la nouvelle règle est jugée localement acceptable. Il est clair que ce processus risque d'être très gourmand en temps d'exécution.
Quant au système de fusion de règles générées en parallèle, ce système est identique au précédent à une différence près ; toute règle générée dans un site donné traverse tous les autres sites afin de mettre à jour ses variables statistiques. Ainsi, le nombre de règles voyageant entre les différents sites est plus important que le nombre de règles en transite dans le système DRL. Par conséquent, il est clair que cette technique est encore plus lente que la précédente.
La technique d'agrégation de modèles proposée
Afin de construire notre méta-classificateur, nous proposons une architecture basée sur les agents logiciels. À cette fin, deux types d'agents sont mis en oeuvre : les agents mineurs qui minent chaque base de données répartie et un agent collecteur responsable de regrouper les informations produites par les agents mineurs.
Tâches d'un agent mineur
La tâche d'un agent mineur est décrite par la figure 1. Il est à noter que le coefficient de confiance c r d'une règle r est calculé en utilisant le théorème limite centrale. En effet, ce théorème stipule que la somme d'un grand nombre (? 30) de variables aléatoires indépendantes et identiquement distribuées suit une distribution qui peut être approximée par une loi Normale. Ainsi, comme nos classificateurs sont bâtis sur un large volume de données, le taux d'erreur E r (T ) d'une règle r calculé sur un ensemble de test T , disjoint de l'ensemble d'entraînement D, peut être approximé par la loi Normale au vrai taux d'erreur E r , qui est le taux d'erreur de r appliqué à toute la population, avec l'écart-type ? Er . À l'aide du taux d'erreur E r (T ) et de l'écart-type ? Er associés à une règle r, nous pouvons -98 -RNTI-E-6
Pour un agent mineur Ami travaillant sur la base de données DBi, faire : calculer l'intervalle de confiance dans lequel nous retrouvons le vrai taux d'erreur de r, E r , dans N % des cas, comme suit :
où la constante z n est choisie en fonction du degré de confiance N % désiré. Le coefficient de confiance de chaque règle est déduit de l'intervalle de confiance de l'erreur. Nous l'avons défini comme étant : 1 moins le pire taux d'erreur calculé dans N % des cas : (1 ? E r (T ) ? z n ? Er ) ; en d'autres termes, 1 moins le taux d'erreur de la règle et moins la moitié de la largeur de l'intervalle de confiance de l'erreur, ainsi nous visons à prendre en compte le pire cas.
Tâches de l'agent collecteur
L'agent collecteur, quant à lui, a pour tâche de regrouper les informations produites par tous les agents mineurs. Sa tâche est détaillée par l'algorithme de la figure 2. Nous pouvons voir dans cet algorithme que l'agent collecteur passe globalement par deux phases. La première phase est la phase principale qui consiste à regrouper toutes les règles dans une même base de règles R. Cette base de règles est notre méta-classificateur original. La deuxième phase, optionnelle, représente une phase de raffinement par filtrage des règles. Il s'agit de supprimer de la base des règles celles qui ont un faible coefficient de confiance. En d'autres termes, il faut supprimer les règles qui, d'après la mesure de confiance, calculée statistiquement rappelonsle, n'auront vraisemblablement pas un bon pouvoir prédictif lorsque confrontées à des données nouvelles. L'ensemble de règles résultant de cette étape est le méta-classificateur R t .
L'ensemble R comme méta-classificateur
L'ensemble R représente l'agrégation de tous les classificateurs de base. Cet ensemble de règles est utilisé comme modèle aussi bien prédictif que descriptif. D'un point de vue prédictif, la classe prédite d'un nouvel objet est la classe majoritaire prédite par les différentes règles qui le couvrent pondérée par leurs coefficients de confiance. Toutefois, en cas d'égalité des pondérations, nous proposons d'effectuer un vote à majorité simple. Ce qui revient, à peu près, à déterminer la classe votée par la majorité des classificateurs de base. Il est à noter que, contrairement à ce qui est identifié dans la littérature (voir §2), nous appelons règles en conflit seulement les règles qui couvrent un même objet différemment. Si plusieurs règles couvrent 2. Étape optionnelle, filtrage des règles : Éliminer de R les règles ayant un coefficient de confiance inférieur à un seuil t : Rt = {rik ? R | cr ik ? t} (t est à déterminer empiriquement) ;
FIG. 2 -Algorithme détaillant les tâches d'un agent collecteur.
un même objet et prédisent la même classe, nous ne les considérons pas comme conflictuelles. Dans de rares cas, même le vote à majorité simple risque d'aboutir à une égalité. Le cas échéant, nous choisissons la classe majoritaire dans l'ensemble des bases d'entraînement. Il est à signaler que tout objet peut être couvert par au plus nd règles -sachant que nd est le nombre de sites. Le nombre de règles n'est pas exactement égal à nd car la phase de détermination du coefficient de confiance risque dans certains cas d'échouer, et ce, à défaut d'une couverture, et par conséquent la règle en question est ignorée. Par ailleurs, en regroupant les ensembles R i , une même règle peut apparaître dans plus d'un classificateur de base. Dans ce cas, une seule occurrence de la règle est retenue en lui attribuant un coefficient de confiance égal à la moyenne des coefficients de confiance de ses différentes occurrences.
D'un point de vue descriptif, les règles qui couvrent un objet expliquent sa classe même s'il y a eu égalité du vote à majorité simple ou pondéré. Comme le système est développé dans un but de forage de données, en d'autres termes, comme support à la prise de décision, les règles couvrant un objet sont proposées à l'utilisateur qui doit juger, de par son expertise, de leur pertinence. Le fait de présenter à un décideur plus qu'une règle afin d'expliquer la classe d'un objet a ses avantages puisque celui-ci aura une vue plus large et plus complète des « limites » de chaque classe. Nous rappelons en outre, qu'en apprentissage automatique, la limite qui définit la séparation entre différentes classes n'est généralement pas unique et par conséquent, plusieurs règles produisant une même classe peuvent représentées les « hyperplans » séparant les différentes classes, fournissant diverses vues sur ces données.
Expérimentation
Afin d'effectuer nos tests, nous avons utilisé dix jeux de données tirés de la banque de données de l'UCI (Blake et Merz, 1998) et dont la taille varie de 351 objets à 45222 objets. Il s'agit des bases : adult, chess end-game (King+Rook versus King+Pawn), Crx, housevotes-84, ionosphere, mushroom, pima-indians-diabetes, tic-tac-toe, Wisconsin Breast Cancer (BCW) (Mangasarian et Wolberg, 1990) and Wisconsin Diagnostic Breast Cancer (WDBC). La subdivision de ces bases afin de simuler des bases distribuées est bien détaillée dans (Aounallah et al., 2005). Pour des fins de comparaison, nous utilisons l'algorithme C4.5 appliqué sur la totalité des données. Le résultat, l'ensemble de règles R ? , représente le cas idéal où toutes les données peuvent être regroupées dans un site central. Les résultats obtenus avec C4.5 sont seulement à titre de référence et nous supposons qu'en pratique il n'est pas possible de regrouper les données dans un même site. L'algorithme C4.5 est aussi utilisé pour construire les classificateurs de base.
Les taux d'erreur obtenus avec R et R t
Nous commençons par regrouper les ensembles de règles R i afin de créer le méta-classificateur original R. Le tableau 1 représente le taux d'erreur de R ? pour chaque ensemble de test avec son intervalle de confiance à 95% ainsi que ceux de R. L'avant dernière colonne représente une comparaison entre les taux d'erreur des ensembles R et R ? ; nous y trouvons : -« Empire » (resp. « Améliore ») qui signifie que R est statistiquement à 95% du temps pire (resp. mieux) que R ? du point de vue du taux d'erreur de classification. La dernière colonne indique la valeur absolue de cette différence. -« ? » indiquant que R est statistiquement comparable à R ? . Ce tableau montre bien que dans 8 cas sur 10, le taux d'erreur de R est comparable à celui de R ? et même dans les deux autres cas la différence n'est pas très importante. Toutefois, cet excellent résultat pourrait être la conséquence de bases distribuées très riches en informations. Afin de vérifier si tel est le cas, nous avons appauvri les bases de données BD i en y introduisant du bruit, et ce, en inversant l'attribut de classe 1 pour 10%, 20%, 25% et 30% des objets. 
TAB. 1 -Les taux d'erreur du méta-classificateur original R comparés à ceux de C4.5.
En utilisant les bases appauvries (50 jeux de données différents : 10 bases de départ et 4 bases appauvries de chacune), nous constatons que les taux d'erreur de R sont toujours aussi comparables à ceux de R ? . Par ailleurs, R arrive à produire de meilleurs taux d'erreur (statistiquement, avec un taux de confiance de 95%) que R ? , et ce, au fur et à mesure que le bruit augmente dans les bases.
Quant aux taux d'erreur de R t , avec t = 0.01, seuil optimal sur les 50 bases tel qu'évalué empiriquement, ils sont sensiblement les mêmes (ou comparables statistiquement avec un taux de confiance de 95%) que ceux de R pour les 50 jeux de données.
Le nombre de règles formant le méta-classificateur
Le tableau 2 représente le nombre de règles formant le classificateur obtenu : R ? , R, R t . Il est clair de ce tableau que nos méta-classificateurs R et R t ont un nombre de règles raisonnable qui est même dans certains cas inférieur au nombre de règles de notre classificateur de référence. Ce résultat est très encourageant puisque nos méta-classificateurs ne sont ni plus difficiles ni plus faciles à interpréter que R ? .
Adult 
Évaluation asymptotique
Dans cette section nous comparons la complexité asymptotique de nos méta-classificateurs R et R t à ceux présentés dans la section 2. Pour ce faire, nous notons par n la taille maximale de l'ensemble d'entraînement dans un site, m le nombre d'attributs dans la base de données, k : le nombre maximum de valeurs par attribut, l : le nombre maximum de prédicats (littéraux) dans une règle, p : le nombre maximum de règles produites des n objets d'entraînement, d : le nombre de sites et n ? : la taille maximale de l'ensemble de test dans un site quelconque.
Coût de la technique proposée
La technique proposée, rappelons-le, fonctionne sur deux phases : une phase distante accomplie par des agents mineurs et une phase centralisée achevée par l'agent collecteur.
Coût des tâches de l'agent mineur
Les tâches d'un agent mineur sont détaillées dans la figure 1. Globalement, il s'agit de bâtir le classificateur de base (tâche 1) et de calculer le coefficient de confiance de chaque règle (tâche 2).
Le coût de la tâche 1 est le coût de l'application de l'algorithme C4.5. Il est bien connu que ce coût est de l'ordre de O(m 2 n). Le coût de la tâche 2 se résume au calcul de la couverture de chaque règle. Ce coût est le suivant :
-Le nombre de tests à faire afin de savoir si une règle couvre un objet est l (le nombre de prédicats dans une règle). -Coût de déterminer la couverture de toutes les règles (au nombre de p) sur un site ayant n ? objets dans l'ensemble de test est : n ? × l × p. Donc, le coût total des tâches d'un agent mineur est donc O(m 2 n + n ? lp).
-102 -RNTI-E-6
Coût des tâches de l'agent collecteur
Les tâches de l'agent collecteur sont détaillées dans la figure 2. Globalement, il s'agit de regrouper toutes les règles issues des différents agents mineurs dans un même ensemble R (tâche 1) et d'en extraire celles qui ont un coefficient de confiance inférieur à un certain seuil afin d'avoir l'ensemble R t (tâche 2).
Le coût de la tâche 1 peut être considéré comme négligeable. Le coût de la tâche 2 est égal au nombre de règles dans R. En considérant que le nombre de règles issues d'un seul site est p et le nombre de sites est d, le coût de cette tâche est dp.
Ainsi, le coût total de notre méta-classificateur R t est O(m 2 n + n ? lp + dp). Puisque le nombre de sites d est constant, le terme dp peut être remplacé par p et celuici peut être négligé devant n ? lp et par conséquent le coût de R t est au pire de l'ordre de O(m 2 n + n ? lp) qui n'est autre que le coût de l'agent mineur. Ainsi, le temps de regroupement et de filtrage des règles ne présente asymptotiquement aucun surcoût par rapport au temps nécessaire pour produire (en parallèle) les classificateurs de base.
Coût des techniques existantes
L'algorithme MIL
Afin de résoudre les conflits (de type 1 (Williams, 1990)), l'algorithme MIL a besoin de rapatrier sur un même site tous les objets couverts par les règles en conflit. Dans le pire cas, toutes les règles issues d'un site B provoquent des conflits avec les règles du site A. Afin de résoudre ces conflits, il faut récupérer tous les objets couverts par les règles issues du site B. En d'autres termes, il faut récupérer l'ensemble d'entraînement du site B. Ceci risque d'être très lent, voire même irréalisable, vu les hypothèses que l'on s'est fixées au départ, à savoir que nous nous plaçons dans le contexte où il est impossible de transférer toute une base de données d'un site à un autre.
Ainsi, comme la quantité de données qui transite d'un site à un autre n'est pas bornée, cet algorithme est dans le pire cas non comparable aux autres algorithmes, car il viole l'une des hypothèses de l'apprentissage distribué. Il sera par conséquent ignoré durant notre comparaison.
Le système DRL
Pour simplifier la comparaison, nous supposons que la complexité de l'algorithme utilisé pour construire l'ensemble de règles (RL) est la même que celle de l'algorithme C4.5 que nous utilisons dans notre technique (voir ci-dessus), malgré que, d'après (Hall et al., 1999), C4.5 est plus rapide que RL.
Cette technique se base sur une fonction d'évaluation de chaque règle. Si cette fonction est évaluée sur un ensemble indépendant de l'ensemble d'entraînement, son coût est le même que celui du calcul de notre coefficient de confiance (voir ci-dessus).
Lorsqu'une règle satisfait au critère d'évaluation local, elle est envoyée à tous les autres sites afin de mettre à jour ses statistiques en fonction de leurs données. Ainsi, cette règle, ayant l prédicats, doit classer tous les objets de tous les autres sites, au nombre de dn. Le coût associé à cette opération pour une règle est O(ldn) = O(ln).
Dans le pire cas, toutes les règles peuvent satisfaire au critère d'évaluation local. Ainsi, le coût de mettre à jour les statistiques des règles d'un site donné, au nombre de p, est O(lnp). Dans le pire cas, chaque site devrait classer les règles des d autres sites. Ainsi, ce coût est aussi de l'ordre de O(dlnp) = O(lnp).
Si une règle ne satisfait pas le critère d'évaluation global, elle est renvoyée à son site de départ pour qu'elle soit spécialisée encore plus. Nous notons par ? le coût de cette opération. Puis, si la nouvelle règle satisfait toujours le critère d'évaluation local, le processus est réitéré. On aura par conséquent, un autre coût de l'ordre de O(ln) (c'est le coût d'une règle classant les données de tous les autres sites).
En conclusion, le coût global de cette technique est de l'ordre de O(m 2 n + n + lnp + ? + ln) = O(m 2 n + lnp + ?).
Fusion d'ensembles de règles générées en parallèle
L'étude de complexité au pire cas de cette technique est sensiblement la même que le système DRL puisqu'il s'agit exactement de la même technique augmentée par un processus de résolution de conflit selon l'algorithme MIL. Par conséquent, globalement, la complexité de cette technique est pire ou égale à la complexité de la technique précédente.
Comparaison
La complexité de notre technique lorsque la validation est réalisée en considérant les échan-tillons comme ensemble de test est de l'ordre de O(m 2 n + ln ? p). La complexité du système DRL ainsi que la technique de fusion de règles en parallèle est de l'ordre de O(m 2 n+lnp+?). Ainsi, les trois techniques ont sensiblement la même complexité asymptotique à un terme près qui est dans notre technique fonction de la taille de l'ensemble de test dans un site, et dans le système DRL, fonction de la taille de l'ensemble d'entraînement. Comme la taille de l'ensemble d'entraînement est généralement plus importante que la taille de l'ensemble de test, notre technique est dans ce cas asymptotiquement plus rapide que le système DRL.
Conclusion
L'objectif de ce papier est de faire une comparaison entre les techniques existantes d'agré-gation de modèles dans un but de forage distribué de données (FDD), d'une part, et une version simplifiée de notre technique de FDD   (Aounallah et al., , 2005 d'autre part. Pour ce faire, nous avons présenté un survol des techniques d'agrégation de modèles existantes les plus comparables à la nôtre ainsi qu'une description de la version simplifiée de notre technique de FDD.
Les expériences menées ont démontré que notre technique performe d'un point de vue prédiction aussi bien, ou même mieux, qu'un classificateur bâti sur la totalité des données, utilisé comme point de référence. Par ailleurs, nos méta-classificateurs sont toujours de tailles comparables au classificateur centralisé de référence.
Une étude asymptotique démontre, en outre, que nos techniques sont asymptotiquement comparables ou plus rapides que les techniques existantes de FDD.

Introduction
Il est communément admis que le temps de préparation des données peut occuper jusqu'à 80% du temps lors d'un projet industriel de fouille de données (Pyle, 1999). L'hétérogénéité des sources, la présence de valeurs manquantes, les erreurs de saisie ou de calcul, les pannes de capteurs, une mauvaise fusion de données sont autant de causes qui peuvent introduire erreurs et incohérences dans une table de données. ESIEA Datalab est une plateforme évolu-tive programmée en Java qui met à disposition de nombreux outils pour aider à la détection d'incohérences, la correction d'erreurs, la transformation ou la contrainte de variables, etc.
Le concept du logiciel
Le nettoyage et la préparation de données peuvent être vus sous la forme d'un processus représenté par la figure 1.
FIG. 1 -Le nettoyage et la préparation de données vus comme un processus.
Le logiciel n'impose pas ce processus à l'utilisateur, mais fournit tous les outils nécessai-res à sa réalisation. En parallèle, le nettoyage et la préparation des données sont tracés dans
ESIEA Datalab, un logiciel de nettoyage et préparation de données la console afin de pouvoir retrouver toutes les transformations et modifications effectuées sur les données et des agents fonctionnent en tâche de fond pour faire des suggestions et orienter l'utilisateur.
Les outils
Outre un vaste ensemble d'outils classiques, dans lesquels les algorithmes utilisés ont été adaptés à un contexte où toute valeur peut être manquante ou bien en erreur, ESIEA Datalab possède quelques outils originaux puissants qui permettent de traiter facilement des cas difficiles de nettoyage ou d'offrir des moyens de visualisation intéressants.
Type structuré. Grâce à la notion de type structuré, le logiciel est capable de détecter des erreurs dans des données symboliques possédant une structure. Une fois la structure d'une colonne spécifiée ou inférée, on peut contraindre les éléments de la structure à l'aide de formules et mettre ainsi en erreur les valeurs ne respectant pas l'une des contraintes.
Outils de visualisation.
Parmi les outils de visualisation disponibles, ESIEA Datalab dispose de graphiques interactifs (matrice de nuages de points, coordonnées parallèles, etc.) qui permettent la sélection de valeurs et la réalisation d'actions sur celles-ci. On trouve aussi des outils originaux comme la carte « vue d'avion ». C'est un graphique qui représente dans une forme condensée toute une table, que l'on va utiliser avec des filtres qui vont colorer une sélection de valeurs. On a ainsi une vision totale de la table qui peut par exemple nous aider à estimer la densité des valeurs manquantes ou bien détecter des motifs.
Conclusion
ESIEA Datalab est un logiciel évolutif dont la simplicité d'utilisation des outils et les fonctionnalités adaptées permettent d'obtenir un gain de temps important sur le nettoyage et la préparation des données. Plusieurs améliorations sont en projet, notamment l'ajout d'une passerelle vers la librairie Java WEKA (Witten et Eibe, 2005 
Summary
ESIEA Datalab is an evolvable Java software program which goal is to clean and prepare data before an analysis. The software looks like a toolbox ready to use, including some interactive visualisation tools, suggestion agents and advanced functionalities implementing Data Mining algorithms.

Contexte et problématique
Les systèmes d'aide à la décision visent à transformer les données opérationnelles en informations facilement interprétables par les décideurs afin que ces derniers puissent effectuer des analyses complexes et prendre les meilleures décisions en temps utiles pour assurer la compétitivité et la pérennité de l'organisation considérée. Dans un tel contexte, plus que le patrimoine matériel, le patrimoine immatériel est important pour capitaliser un maximum d'informations, de connaissances et d'expertises afin de prendre les décisions adaptées. Nos travaux visent à proposer aux organisations plus qu'un système d'aide à la décision, un véri-table outil de Mémoire d'Expertises Décisionnelles (MED).
Les données décisionnelles
Il est reconnu que les Bases de Données Multidimensionnelles (BDM) sont adaptées pour le stockage et la manipulation des données décisionnelles (Inmon, 1996). Les modèles conceptuels (Ravat et al., 2005) des BDM organisent les données en sujets et axes d'analyses au sein d'un schéma en étoile (Kimball, 1996). Tout sujet d'analyse est représenté par un fait composé de plusieurs mesures (indicateurs d'analyse). Les dimensions représentant les axes d'analyse sont formées de paramètres en fonction desquels les mesures sont étudiées. Les paramètres sont organisés en hiérarchies, de la granularité la plus fine (attribut racine servant d'identifiant à la dimension) à la plus générale (cet attribut est symbolisé par All).
développées et d'un prédicat de sélection (Agrawal et al., 1997 ;Gyssens et al., 1997 
Vers une mémoire d'expertises décisionnelles
Les TM ont pour objectif de faciliter les prises de décisions, mais elles s'avèrent parfois complexes et difficiles à interpréter. En effet, les prises de décisions reposent non seulement sur les données brutes mais également sur les réflexions, les commentaires des analystes voire la confrontation de différentes interprétations. À notre connaissance, il n'existe pas d'outil logiciel permettant aux décideurs d'analyser les données décisionnelles en intégrant les tâches qu'ils conduisent de manière manuelle sur des tableaux de bord : annoter, comparer… La mémorisation et la réutilisation de l'expertise des analystes permettent à l'organisation de préserver un patrimoine tout aussi important que les données elles-mêmes.
Notre objectif est de proposer un cadre informatique permettant d'exploiter, de partager les données multidimensionnelles tout en supportant des fonctionnalités d'annotation pouvant comprendre des fils de discussion (support de communication utilisé dans les forums). Ainsi, en permettant d'enrichir interactivement les composants des BDM et des TM, les décideurs deviendront des lecteurs actifs en insérant notamment des commentaires. Pour cela, nous proposons un outil informatique permettant :
-de visualiser les données décisionnelles sous la forme d'une TM ; -de manipuler une TM au travers d'opérations multidimensionnelles ; -d'annoter les schémas de BDM afin d'expliciter les composants décisionnels ; -d'annoter les TM tout en permettant le dialogue sous forme de fil de discussion ; -d'exploiter ces annotations au travers de fonctions d'exploration, de sélection, etc. De nombreux travaux ont apporté une réponse aux deux premiers objectifs : des algèbres pour la définition et la manipulation de BDM ont été proposées (Gray et al., 1996 ;Agrawal et al., 1997 ;Cabibbo et Torlone, 1998 ;Marcel 1999 ;Abelló et al., 2003).
Existant : l'activité d'annotation
À notre connaissance, les systèmes d'annotation couplés aux BDM n'ont pas fait l'objet d'étude. Une première proposition consisterait à transposer aux BDM le principe des commentaires associés aux schémas de BD transactionnelles. Cette proposition est insuffisante car elle reste difficilement exploitable et très limitée (table, vue ou attribut). La deuxième solution consiste à se baser sur les systèmes d'annotation existants pour la gestion électroni-que de documents. Dans ce cadre, les annotations sont qualifiées de commentaires, notes, explications ou autres types de remarques qui peuvent être associés à tout ou partie d'un document sans avoir à le modifier 1 . Ces annotations sont dites informelles, contrairement aux annotations formelles du Web Sémantique reposant sur l'utilisation d'un langage formel (ex. : ontologie) pour cataloguer et indexer les documents. Dans cet article, seules les annotations informelles seront exploitées car nous ne souhaitons pas contraindre les décideurs à employer un vocabulaire normé et restreint.
La majorité des systèmes d'annotation informatisés (SAI) opèrent sur des documents textuels. Dans ce cadre, les annotations sont matérialisées sous différentes formes : texte ou marques libres (astérisques, etc.). Ces marques mettent en valeur des passages en y associant éventuellement un commentaire. À ce jour, on dénombre plus de vingt SAI tels que Amaya du W3C (Koivunen et Swick, 2001) ou Office Web Discussions de Microsoft (Brush, 2002). L'intérêt d'un SAI se situe à deux niveaux : à un niveau personnel, il aide l'utilisateur dans sa tâche de lecture active tandis qu'au niveau collectif il permet de partager ces annotations entre les utilisateurs. Au regard de nos problématiques, nous proposons d'adapter les fondements des SAI à l'analyse décisionnelle. L'intérêt des annotations dans un tel contexte est qu'elles seront utilisées pour véhiculer l'expertise des analystes.
Annotations décisionnelles
Les TM servent de support à des experts pour la prise de décisions. Cependant, afin de spécifier une MED, nous souhaitons permettre aux analystes d'enrichir ces TM avec des annotations qui visent à conserver les décisions et les commentaires formulés lors des analyses de la TM. L'expertise que véhiculent ces annotations est utilisée à des fins personnelles ou collectives et peut par conséquent contribuer à améliorer les analyses futures. Pour cela, les annotations apportent les fonctionnalités et avantages suivants :
-au niveau du schéma, elles permettent d'améliorer la compréhension des composants d'un schéma de BDM et ainsi de tirer des conclusions analytiques cohérentes ; -au niveau des valeurs, elles peuvent contenir des informations expliquant un phé-nomène général, le contexte d'étude ou signaler la singularité d'une valeur spécifi-que. Les fils de discussion suscités par ces annotations permettent notamment aux experts de valider ou de compléter les commentaires de leurs collègues ; -les liens spécifiés vers des annotations ou des documents permettent de construire un « dossier d'analyse ». Ils permettent également d'illustrer la réflexion des analystes pour expliquer et justifier les éventuelles conclusions qu'ils ont tirées ; -les traitements automatisés tels que la classification permettent de faire émerger des liens implicites entre analyses permettant d'aiguiller les analystes dans leur tâche. 
Modèle conceptuel d'annotation décisionnelle
Informations subjectives
Une annotation décisionnelle comporte les informations subjectives suivantes :
-le contenu textuel saisi par l'annotateur ; -le type de l'annotation caractérisant son contenu textuel. Le type, défini par le créateur de l'annotation, peut être : commentaire, question (permet d'interroger les autres analystes directement en contexte au travers de la TM), conclusion (l'annotation présente les conclusions de l'analyse et les éventuelles décisions prises) et réfé-rence (références vers des documents ou d'autres TM). Ces types ne sont pas exclusifs : une annotation peut être un commentaire qui comprend des références ; -la portée de l'annotation qui est soit locale soit globale. Une annotation est locale si elle n'est présentée qu'au travers d'une TM précise. Au contraire, une annotation globale est indépendante des TM. Cependant, la cible d'une annotation détermine parfois sa portée. Ainsi, une annotation sur les valeurs des mesures de la TM sera locale car fortement dépendante de l'analyse courante.
Informations objectives
Pour chaque annotation décisionnelle ad g , le système définit également l'ensemble IO g contenant les informations objectives suivantes : -son identification : identifiant caractérisant sa position dans le fil de discussion ; -son créateur : son identité (nom, prénom), sa fonction et son adresse email ; -son point d'ancrage spécifiant la localisation précise de l'annotation dans la tma. La définition du point d'ancrage ne peut se faire de manière naïve e.g. repérage en ligne et en colonne car la TM peut regrouper, en ligne et en colonne, une hiérarchisation de paramè-tres d'une même dimension. De plus, l'analyste peut réorganiser les axes des abscisses et des ordonnées grâce à l'opérateur de permutation de paramètres par exemple (Ravat et al., 2005).
Pour ces deux raisons, le point d'ancrage est défini par un CheminTM : ? | <(op(m))+>, ? C 1 (resp. C 2 ) exprime le chemin de la première (resp. deuxième) dimension :
? | NomDimension (.NomHierarchie)* (/NomParamètre [=Valeur])* ? R précise éventuellement la condition de restriction : ? | ExpressionBooléenne, avec: ? représente l'absence de valeur, Valeur précise la valeur exploitée pour le paramè-tre considéré et / représente un opérateur de descendance entre paramètres dans la hiérarchie.
Seuls les éléments annotés figurent dans le chemin, ce qui explique que tous les champs sont facultatifs. Nous présentons ci-dessous quelques exemples de chemins de localisation représentés par la figure 2 (où R représente l'expression de restriction de cette TM). 3. Le service des ventes avertit les différents analystes : les ventes prises en compte sont celles des télévisions 16/9 e . Le chemin de la cellule où l'annotation sera rattachée est : (VENTES, ?, ?, ?, R).
Création, stockage et restitution d'une annotation décisionnelle
Pour créer une annotation, le décideur sélectionne directement dans la table la ou les cellules qu'il souhaite annoter. Pour compléter cette annotation il doit fournir toutes les informations qu'il souhaite inclure dans l'annotation. Nous stockons les annotations sur un serveur dédié i.e. indépendamment des TM pour faciliter le partage et la recherche des annotations.
La restitution des annotations est non intrusive et se fait en parallèle de la construction de la TM. Elles sont directement intégrées sous forme de pictogrammes au niveau de la TM.
Conclusion et perspectives
Nos travaux visent à proposer des solutions permettant la constitution d'une mémoire d'expertises décisionnelles pérenne. Cette mémoire permet de stocker et de restituer aussi bien les données nécessaires aux prises de décisions que les annotations qui véhiculent les commentaires des analystes. Les données décisionnelles sont représentées au travers de concepts multidimensionnels et restituées sous forme de tables (TM). Ces différents composants servent de support à la formulation, à l'utilisation et à la restitution d'annotations décision-nelles qui peuvent aussi bien servir à un usage personnel qu'à un usage collectif.
Nous proposons d'étendre ces travaux selon deux directions. La première consiste à éten-dre le modèle conceptuel des TMA en intégrant le concept de groupe d'utilisateurs avec les droits d'accès aux annotations associés. La seconde perspective consiste à coupler le principe de discussion asynchrone avec un outil de workflow. Cet outil pourrait valider et organiser le circuit des annotations ainsi que de définir les délais pour une prise de décision fiable.

Introduction
La qualité des règles d'association est généralement évaluée par des mesures d'intérêt (classiquement le support et la confiance) et de nombreuses autres mesures ont été proposées (Tan et al., 2002). Mais, on peut légitimement se demander quel est l'intérêt de telles règles, notées LHS AE RHS, si 30 % des données de LHS sont obsolètes, 20% des données de RHS sont imprécises, et 15% des données de LHS proviennent d'une source réputée peu fiable. La thèse défendue dans cet article est que les mesures d'intérêt pour la découverte de règles d'associations ne sont pas autosuffisantes pour représenter effectivement la qualité des rè-gles. Des mesures décrivant la qualité des données à partir desquelles sont calculées les rè-gles doivent être intégrées au processus de découverte, de même que le coût d'une décision de choisir (ou non) ces règles « supposées intéressantes » doit être également considéré. Ceci a motivé donc nos travaux que nous formalisons dans les sections suivantes.
(I i ? I). Pour chaque itemset I i , le vecteur composé des scores sur toutes les dimensions de qualité (normalisées sur [0,1]) est appelé vecteur qualité et noté q(I i ) dans l'espace qualité Q de tous les vecteurs qualité possibles. Nous définissons la qualité d'une règle d'association R avec une fonction de fusion notée "o j " dont la sémantique est spécifique à la dimension de qualité j considérée. Cette fonction permet de fusionner chaque composante des vecteurs qualité correspondants aux ensembles de données présents dans les partie gauche et droite de la règle. La qualité de la règle R est donc un vecteur k-dimensionnel tel que:
On peut alors définir la qualité moyenne de la règle R notée q(R) par une somme pondérée de chaque dimension des vecteurs qualité des jeux de données composant la règle : avec
Le Tableau 1 présente plusieurs exemples de définition ainsi que la sémantique que l'on peut donner à la fonction de fusion pour combiner les scores de qualité sur la dimension considérée pour deux ensembles de données x et y composant la règle.
L a f r a îc h e u r d e la r è g le x AE y e s t e s t im é e d e f a ç o n p e s s im is t e p a r le p ir e s c o r e d e f r a îc h e u r d e s 2 e n s e m b le s d e d o n n é e s c o m p o s a n t la r è g le .
2
P r é c i s i o n q 2 ( x ) . q 2 ( y ) L a p r é c is io n d e la r è g le x AE y e s t e s t im é e p a r le 
TAB. 1 -Différentes fonctions de fusion pour combiner les scores d'une dimension qualité
Nous considérons que choisir et utiliser (ou non) une règle d'association est une décision qui désigne la règle comme étant légitimement intéressante (D1), potentiellement intéressante (D2), ou inintéressante (D3) à la fois sur la base de « bonnes » mesures d'intérêt mais, également, en connaissant la qualité effective des données composant les parties gauche et droite de la règle. Cette décision a nécessairement un coût lié à l'incertitude et à la méconnaissance du jeu de données et du processus de recueil qui ne garantit généralement pas la bonne qualité des données. Pour formalisme, nous employons P CE (x) la probabilité que le jeu de données X soit "de mauvaise qualité" sur une ou plusieurs dimensions de qualité et P CC (x) la probabilité que X soit "de qualité correcte" dans une gamme de valeurs acceptables sur chaque dimension de qualité. P AE (x) représente la probabilité que X soit détecté correct alors qu'il est effectivement incorrect, P AC (x) représente la probabilité qu'il soit détecté incorrect alors qu'il est effectivement correct (voir Figure 1). Pour q ? Q, la qualité moyenne des données LHS ? RHS de la règle R, on note P(q?Q | CC) ou f CC (q) la probabilité conditionnelle que la qualité moyenne q corresponde à celle des jeux de données qui sont classifiés comme étant de qualité correct (CC). De la même façon, on note P(q?Q | CE) ou f CE (q) la probabilité conditionnelle que la qualité moyenne q corresponde à celle des jeux de données qui sont classifiés erronés ou comme étant de mauvaise qualité (CE). On note d la décision de choisir une règle légitimement intéressante (notée D 1 ), potentiellement intéres-sante (notée D 2 ), ou inintéressante (notée D 3 ) et s le statut de la qualité des jeux de données à partir desquelles a été calculée la règle. On note P(d=D i , s=j) et P(d=D i | s=j) respectivement les probabilités conjointe et conditionnelle que la décision D i soit prise lorsque le statut de la qualité des données à l'origine du calcul de la règle R est j (i.e., CC, CE, AE, AC). On note c ij le coût de la décision D i pour classifier la règle sur la base de la qualité des données j composant les parties gauche et droite de la règle. A partir des coûts présentés dans le Tableau 2, l'objectif est de minimiser le coût moyen c qui résulte d'une telle prise de décision tel que :
Coût
PAC(x) Actually correct
A partir du théorème de Bayes, tel que:
on suppose que q est la qualité moyenne des jeux de données composant la règle tirée aléatoirement dans l'espace de tous les vecteurs qualité possibles. La probabilité conditionnelle P(d=D i | s=j) est définie à partir de la fonction de densité de probabilité f j du vecteur qualité de telle sorte que la variable aléatoire q prenne des valeurs dans l'intervalle [q jMin , q jMax ] correspondant aux seuils de qualité selon j telle que : 
Minimiser ce coût moyen conduit à rechercher la décision optimale pour la classification des règles en trois catégories notées D  
TAB. 3 -Les 10 meilleures règles découvertes avec leur qualité, coût et décision associés
Le Tableau 3 montre les 10 meilleures règles d'associations découvertes avec leur confiance, leur support (en nombre d'enregistrements), le bénéfice prédit, les scores par dimension de qualité, la qualité moyenne, le coût de sélection pour chaque règle d'association et enfin, la région décisionnelle de chaque règle. Il est intéressant de noter que le bénéfice prédit par règle peut être considérablement affecté par le coût de sélectionner une règle calculée à partir de données de mauvaise qualité : par exemple, la deuxième meilleure règle R2 dont le bénéfice prévu est $61.73 a un coût de $109.5 si elle est sélectionnée alors qu'elle est classifiée comme "inintéressante" à cause de cette mauvaise qualité de données. Dans ces expériences, notre but était également de démontrer que des variations de la qualité de données pouvaient avoir un impact considérable sur la validité des résultats issus de la fouille de règles et dans le cas du challenge KDD-Cup-98, invalider totalement les prédictions de béné-fices. Ainsi, la Figure 2(a) montre le comportement du coût de la décision dans le choix des règles quand la qualité moyenne des données (InitQual) se dégrade de -10%, -30%, à -50% ou s'améliore de +10%, +30% à +50% avec 0 ? = 0.200 et en l'absence de problème de classification. Nous observons que la dégradation de la qualité des jeux de données composant les règles augmente le coût de la sélection de ces règles. L'amélioration de qualité de données se manifeste par une stabilisation du coût de la décision pour des règles légitimement intéres-santes. Un autre résultat intéressant est montré dans la Figure 2(b) : parmi les 10 meilleures règles découvertes pour une qualité de données initiale (InitQual), seulement 5 règles (R1, R5, R7, R9 et R10) sont potentiellement intéressantes. Lorsque la qualité augmente de 30%, 3 règles deviennent légitimement intéressantes (R5, R7 et R9). Cette observation offre deux perspectives intéressantes pour l'exploitation des règles d'associations et pour la gestion de la qualité de données : la première pour guider l'élagage du nombre de règles sur la base à la fois des indicateurs de qualité de données et des coûts de décision dans le choix des règles ; la seconde pour établir des stratégies et des priorités dans l'amélioration de la qualité de données (par un nettoyage ciblé par exemple). 
Conclusion
Dans cet article, nous proposons une méthode pour définir la qualité des règles d'associations en intégrant des mesures de la qualité de données à partir desquelles celles-ci sont dé-couvertes. Ensuite, nous proposons un modèle décisionnel probabiliste basé sur le coût que peut engendrer le choix de règles d'association certes intéressantes d'après leur support et confiance mais basées sur des données de mauvaise qualité. Le modèle définit les trois seuils pour déterminer si les règles découvertes sont légitimement, potentiellement intéressantes, ou inintéressantes. Pour valider notre approche, nos expériences sur l'ensemble de données de la KDD-Cup-98 ont consisté à : i) générer des indicateurs synthétiques de qualité des données, ii) calculer les dix meilleures règles d'association (en terme de support/confiance) et calculer leur qualité moyenne à partir de la qualité de leurs données, iii) calculer le coût qu'entraîne la décision de choisir des règles illégitimement intéressantes, iv) examiner le coût et la décision sur le choix de ces règles quand la qualité des données varie. Nos expériences confirment notre hypothèse initiale : les mesures d'intérêt des règles d'associations découvertes ne sont pas autosuffisantes et la qualité d'une règle d'association dépend de la qualité des données à partir desquelles elle est calculée. La qualité de données inclut de diverses dimensions devant être considérées pour assurer et valider la qualité des connaissances extraites.
Références
Tan P-N., Kumar V., and Srivastava J., (2002 
Summary
In this paper we propose to exploit the measures describing the quality of data for defining the quality of association rules resulting from the rule mining process. We propose a probabilistic cost-based decisional model for the selection of legitimately, potentially interesting or not interesting rules when the quality of data they are computed from is correct, medium or low. The experiments on the KDD-CUP-98 dataset show that the Top 10 rules selected with good support and confidence measures are not interesting unless the quality of their data is correct or improved.

Introduction
La compréhension des évolutions du bâti s'appuie sur l'analyse conjointe de connaissances spécifiques et de connaissances génériques ayant, dans le champ du patrimoine architectural, des caractéristiques très handicapantes vis à vis des technologies actuelles de gestion d'information localisées spatialement. Ces connaissances s'appuient en effet sur des informations hétérogènes, réparties, fortement pluridisciplinaires, mais également floues, incertaines, régulièrement remises en question, à ré-interroger comparativement sur un territoire donné ou entre territoires. Dès lors l'apport attendu de l'application des NTIC au domaine du patrimoine en matière de production et surtout d'échanges de connaissances reste pour l'essentiel prospectif, si ce n'est du strict point de vue de la vulgarisation.
Pourtant, de nombreux travaux menés traitent des aspects liés à l'acquisition de données 3D (De Luca et al., 2003), la gestion d'informations localisées spatialement (Sebillo, 2003), ou encore de la représentation de données spatio-temporelles (Renolen, 1997) (Spaccapietra, et al., 2004). En parallèle, l'acquis en matière de visualisation de données (y compris à caractère spatio-temporelles) dans le champ de la visualisation d'informations constitue une -347 -RNTI-E-6 base de références conséquente. Cependant, l'aptitude de la représentation architecturale « digitale » ( i.e. de la maquette numérique 2D/3D) à transmettre des éléments de connaissance sur un territoire et à souligner les vecteurs de différence/ressemblance entre objets architecturaux, reste encore très largement terra incognita. L'étude du patrimoine architectural peut se voir comme un processus d'acquisition de connaissances à l'appui duquel l'auteur de l'étude rassemble des éléments et indices correspondant à trois grandes familles de ressources : -des connaissances a priori (i.e. théorie : typologies, savoir-faires, courants stylistiques et artistiques, contraintes constructives, solutions canoniques, etc.); -des sources documentaires hétérogènes (manuscrits, représentations artistiques, étu-des provenant de diverses disciplines, cartographie, etc.; -des observations contemporaines (campagnes de fouilles, relevés, etc.).
Ces trois grandes familles de ressources donnent lieu à analyses croisées afin d'une part de mieux comprendre le bâti au sens large et d'autre part de proposer des représentations traduisant graphiquement ce que l'auteur de l'étude a compris de l'évolution de l'édifice. Or si, dans chacune de ces trois grandes familles, des travaux de recherche et de développement sont menés pour mieux prendre en compte les spécificités du patrimoine architectural et de son étude, peu de travaux ont à ce jour porté sur « comment mettre en vis-à-vis des éléments appartenant à ces différentes familles». En réponse, nous avons montré (cf. Blaise et Dudek, 2004b) que l'utilisation de la forme architecturale comme moyen d'organiser et de donner accès à ces jeux d'indications, permettait de visualiser ces derniers au travers de maquettes 2D/3D simulant les évolutions d'édifices ou de tissus urbains. Nous avons en conséquence introduit l'idée qu'un modèle théorique, intensionnel, de l'objet architectural peut jouer le rôle de filtre sur notre jeu d'indications. A l'issu de ce travail, nous avons cependant mis en évidence les limites d'une telle approche intensionnelle face aux réalités souvent confuses du patrimoine bâti (transformations, réemplois, incertitudes diverses) (cf. Fig. 2).
C'est dans cette continuité que s'inscrit notre contribution. Elle souligne un constat de manque : comment prendre en charge les étapes amont de l'étude d'un lieu, quand nous ne savons encore presque rien de lui ? Comment formaliser de façon opérationnelle un processus d'acquisition de connaissances adapté aux contraintes spécifiques du patrimoine bâti (étapes d'études non-ordonnées, données floues et qui le resteront, interdisciplinarité, etc.) ?
Nous introduisons dans cette contribution une démarche de modélisation réflexive objets physiques-connaissances associées qui s'attache à décrire ce que nous comprenons d'un bâti et pas un bâti idéalisé sans rapport avec les complexités du patrimoine réel. Elle se traduit par la mise en relation dans une maquette virtuelle de l'objet physique (ainsi placé au lieu et au temps HWWGHHFHHTXLLMXVWLILHHVDDSUésence en au temps évaluées par exemple en terme de vraisemblance). Cette démarche met en relation dynamique trois éléments : informations qualifiées par des descripteurs propres au domaine (incertitude par exemple), corpus d'objets architecturaux théoriques multi-échelles et maquettes virtuelles dans lesquelles les informations susmentionnées sont visualisées et délivrées par l'intermédiaire de codes graphiques affectés aux instances du corpus théorique d'objets architecturaux.
Cette démarche de modélisation, que nous nommons modélisation informationnelle, a pour objectif un gain de compréhension du lieu architectural et des informations qui lui sont associées. Notre contribution situe brièvement nos travaux antérieurs sur le sujet, puis introduit les filiations de cette démarche, et discute de son application au cas concret de la place centrale de Cracovie.
-348 -RNTI-E-6
Contexte et hypothèses
De Quatremère De Quincy jusqu'à Le Corbusier l'architecture a été considérée sous le double angle : d'une discipline fondée sur l'idée de mesure et d'un art (Fichet, 1979, et Corbusier, 1958. Il apparait donc légitime, au vu du premier qualificatif, de tenter de circonscrire les qualités observables de stabilité (Lemoigne, 1977) de l'objet architectural, autrement dit de l'inscrire a priori dans un univers de connaissance (Francis et al., 1999) dont un ou des modè-les structurent de façon abstraite les composantes. Nous considérons l'objet patrimonial comme la conjonction d'un bâti (observable ou détruit) et de connaissances hétérogènes. A partir de là, la proposition méthodologique que nous avons faite (Blaise et Dudek 2004a) consiste à établir un canevas de règles permettant d'isoler au sein du corpus architectural les concepts architecturaux à modéliser. L'élément ainsi isolé, identifie une famille d'objets, famille qui transcende les particularismes régionaux et facilite le travail de comparaison (cf. Fig. 1  Blaise et Dudek, 2004b), il s'appuie sur le formalisme objet, largement répandu aujourd'hui et qui impose rigueur dans la relecture des références utilisées pour établir la logique d'identification et de dérivation des concepts (cf. Ducournau et al, 1998). Sa double organisation spécialisation / agrégation se prête bien à la définition d'un modèle médiateur entre plusieurs vues sur l'édifice.
Mais le champ de l'architecture patrimoniale implique des contraintes particulières : -échelonnement des phases de construction et transformation ; -masquages successifs d'états cohérents ; -incomplétude d'un objet, objets "mal formés", incertitude sur les données, etc. ; -données évolutives / incomplètes.
En conséquence, son application se heurte à des limites opératoires réelles et appelait une prise de distance avec l'approche déterministe, que nous avions privilégiée au départ. En réponse, nous nous sommes focalisés sur l'idée de développer une méthode de travail permettant de ne décrire que ce que nous comprenons d'un bâti, et donc ne faisant appel à l'instanciation du modèle déterministe présenté ci-avant qu'in fine, si et seulement si le niveau de connaissance réel sur l'objet le permet. C'est cette méthode que notre contribution présente, d'abord par ses filiations scientifiques (cf. section 3), puis par son application concrète à l'instrumentation d'un processus d'acquisition de connaissances (cf. section 4). 
-Limites d'un modèle intensionnel : objets réemployés -( a.) appui et deux baies obstruées ; désemployés -( a.) arc de décharge ; déformé -( b.) oculus ; ( e.) linteau, incomplets -( b.) baie ; non reproduits -( d.) ; fragmentaires -( f.). La morphologie canonique de ces objets ne recouvre plus la réalité observée, mais ils conservent leur identité d'instance d'un concept général (arc, baie,…).
Deux filiations, une méthode de travail
Dans le champ de l'architecture patrimoniale, l'étude des évolutions de lieux s'appuie prioritairement sur l'analyse d'une documentation permettant de circonscrire petit à petit un état de connaissance sur ces lieux. Le terme documentation recouvre ici à la fois données brutes (sources bibliographiques, relevé architectural, compte-rendu de fouilles, ...) ou interprétées (restitutions, hypothèses, ...). Mais une telle documentation est presque toujours hété-rogène, incomplète, contradictoire et progressive. Notre état de connaissance s'en trouve lui aussi sujet à évolution. Ces difficultés nous ont conduit à formuler trois constats :
-la forme d'un lieu bâti 1 reste le plus souvent hypothétique; -la représentation fixe une étape dans l'évolution de lieu , mais aussi un moment dans notre étude de ce lieu 2 ; -une distinction opératoire claire doit être introduite entre l'acte de restituer une hypothèse et celui de visualiser ce que l'on sait vraiment.
A partir de là, trois hypothèses de travail ont été identifiées, sur la base desquelles de nouvelles expérimentations ont été conduites (Blaise et Dudek., 2005): -la forme architecturale constitue un médiateur naturel entre les informations à manipuler (parce que l'on peut y rapporter les différentes couches d'information sur l'évolution de l'objet) ; -la forme architecturale est un bon moyen de les trier, de les interfacer (parce qu'elle localise dans l'espace et dans le temps des jeux d'informations) ; -la forme à représenter doit être comprise comme un substitut de l'objet, elle n'est pas à l'image de l'objet,
elle est l'idée que nous nous faisons de l'objet en fonction des connaissances réelles dont nous disposons au temps t de notre étude.
Dès lors nous sommes en face de deux contraintes à première vue presque contradictoires : d'abord manipuler des formes architecturales et donc les représenter, et de l'autre manipuler des informations sur les formes architecturales et donc les visualiser. Comment sur notre terrain d'expérimentation concilier les exigences de la représentation d'objets physiques (par exemple l'exhaustivité géométrique) et les exigences de fidélité aux informations sous jacentes (par exemple « nous ne savons pas comment était bâti cet édifice ») ?
En réponse, la démarche de modélisation informationnelle que nous tentons de circonscrire fait un pont entre le domaine de la modélisation spatiale proprement dite (représentation géométrique, multi-représentations, multi-résolutions, etc.) et celui de la visualisation d'informations au sens d'E.R. Tufte (Tufte, 2001) Force est de constater que l'apparition des NTIC n'a pas contribué de façon notable à l'émergence de cette démarche. Aujourd'hui, dans le champ de l'architecture les scènes virtuelles sont vues avant tout comme un dispositif de vulgarisation. Ces scènes sont réguliè-rement présentées à un large public pour « montrer » comment un objet architectural a pu évoluer au cours du temps, quelle que soit l'échelle considérée (de l'ensemble urbain aux décors) (Bonfigli et al., 2000), voire pour exploiter un lieu à distance (Salonia et al., 2004). Mais cette utilisation du graphique est remise en cause dans notre champ en particulier sur deux points (cf. Kantner, 2000): -un manque de lisibilité des représentations dû au fait que les inférences faites pour la reconstruction géométrique des objets sont masquées dans la scène finale ; -un manque d'efficacité affligeant pour les chercheurs eux-mêmes qui investissent temps et moyens dans la production de reconstructions qui restent un effet de bord de leur étude, puisque ne donnant pas accès aux couches d'informations plus profondes comme la bibliographie par objet, l'inscription typologique, terminologique, etc.. Autrement dit, l'effort d'acquisition et d'analyse d'informations fait pour comprendre l'objet architectural apparaît comme totalement absent du résultat final, une reconstruction dite virtuelle. Une telle représentation n'est pas liée aux sources documentaires en justifiant le contenu, elle n'est pas mise à jour dynamiquement quand de nouveaux éléments d'informations sont rassemblés, elle ne mentionne même pas ce qui est en définitive le plus significatif pour l'analyste : l'incertitude des données initiales. Dans le champ de la visualisation d'informations au contraire, le graphique est non seulement utilisé pour interroger des jeux de données, mais également pour les trier, ou dit autrement pour réduire la multitude des données dans les mots de J.Bertin (Bertin, 1998). Le rôle du graphique, dans les mots d'ER Tufte (Tufte, 1997) « … Nous visualisons des informations pour raisonner sur des connaissances, pour documenter, communiquer et préserver ces connaissances … », nous y semble beaucoup plus compatible avec nos objectifs scientifiques réels.
Nous posons en hypothèse l'existence d'une démarche de modélisation qui s'appuie sur la double filiation établie ici. Dans la pratique, la démarche 2 de modélisation informationnelle se matérialise par un jeu de règles de production graphique qui agissent comme gardefous dans l'implémentation du processus d'acquisition de connaissances (cf. section 4). Nous en donnons ci-dessous un extrait : -s'appuyer sur un modèle théorique autorisant comparaisons et réutilisation ; -affecter les objets a une échelle donnée ; -produire les représentations au vol pour refléter un état de connaissance courant ; -adjoindre a la représentation d'objets une évaluation qualitatives des sources documentaires utilisées pour leur étude (incertitude par exemple) traduite graphiquement ; -développer un jeu de signes graphiques permettant de traduire ce que l'on sait et ce que l'on ne sait pas ; -produire des représentations interrogeables objet par objet pour consulter les sources justifiant les inférences faites au cours du processus d'analyse des formes ; -choisir un niveau d'abstraction adapte au contenu informationnel a délivrer et/ou au niveau de complétude dans l'étude de l'objet ; -enfin, règle des règles, si le graphique ne produit pas un gain d'intelligibilité des sources derrière les objets représentes, le considérer inutile.
Un processus d'acquisition de connaissances
L'étude de l'édifice à ses différentes échelles fait appel à une masse importante de documents historiques, que nous décrivons et attachons à des instances d'un modèle architectural théorique. Mais les interprétations qui sont faites des documents ne permettent pas nécessai-rement de faire ce travail d'attachement ou de filtrage de la documentation, soit parce que la quantité d'informations n'est pas suffisante, soit parce que le travail est en cours . Puisque nous voulons utiliser la forme architecturale pour filtrer notre documentation, nous pouvons être en face d'une situation où nous devons mettre en relation l'idée encore floue que nous avons d'un lieu architectural HWW OHH PRGèle théorique décrit en section 2, modèle déterministe, univoque. A partir de ce constat nous établissons d'abord qu'un premier indice est souvent un repère terminologique (nom du concept dont relève le lieu ), complété par un repère toponymique -nom(s) historique(s) de ce lieu 1RXVV DYRQVV HQ conséquence développé d'abord un modèle générique de toponyme architectural qui permet de localiser de façon univoque, dans une hiérarchie simulant la notion d'échelle et dans une chronologie, un jeu de données par rapport aux objets qu'il documente. Cette localisation toponymique définit un lieu particulier, éventuellement repéré dans l'espace par des coordonnées géographiques, ou simplement repéré comme sous-partie d'un toponyme plus géné-ral. La localisation toponymique s'accompagne d'une localisation terminologique, qui définit la notion générale correspondante. Ces deux notions sont implémentées sous la forme de classes (au sens de la POO) liées par agrégation et dont les états des instances sont figés dans des fichiers XML exploités soit pour la visualisation 2D/3D soit textuellement. Ces « bases » terminologiques et toponymiques sont modestes (800 termes, hiérarchie de 20 toponymes) : elles n'ont pas vocation à l'exhaustivité mais doivent se construire au fur et à mesure des expé-riences.
Hiérarchisation / localisation
Instanciation / Représentation
Informations spécifiques Documentation (analyse)  4 Indices présence d'un « objet potentiel » déclenchant l'étude, données brutes hétérogènes (de l'archive au relevé), hétérogénéité des systèmes d'information préexistants à l'étude (lorsqu'ils existent), éventail disciplinaire, rôle des comparaisons et des analogies, etc.. Il faut noter que ces éléments ne se situent pas nécessairement dans une continuité, ni en terme de succession (la conclusion d'une phase ne déclenche pas le démarrage d'une autre), ni en terme de planification (l'échelonnement des -   (visualisations, simulations, comparaisons et analogies, affiliations stylistiques phases dans le temps peut être considérable), ni enfin en terme de chaînage (chaque phase apporte son jeu de conclusions intermédiaires propres, une phase n'est généralement pas un pré-requis à une autre).
Terme du vocabulaire
Instance(s)
Toponyme
Indices initiaux
5 La maquette 2D/3D sert cet objectif (dispositif de visualisation) en phase aval de l'étude. Nous avons ici besoin d'un équivalent à la maquette, mais d'un équivalent plus abstrait. Il s'agit donc en fait d'une démarche de généralisation du modèle architectural tel que présenté dans (Blaise et Dudek, 2004).
-354 -RNTI-E-6 ou typologiques, etc.). Un dispositif de visualisation (diagramme SVG la forme d'une spirale, cf. Fig. 5, 6, 7) donne de chaque objet architectural étudié une vue synthétique montrant les éléments d'informations rassemblés autour de l'objet, module par module.
La présentation de cette spirale de niveau d'investigation permettra de cerner concrète-ment le cadre de ce développement. Elle est utilisée à la fois comme dispositif de visualisation et comme moyen de navigation dans les différents systèmes ou applications adaptés aux jeux de données concernés. Nous avons appliqué ce dispositif en priorité au cas de la place centrale de Cracovie (17 édifices, une cinquantaine d'évolutions) et sur le cas du château comtal de Carcassonne. La spirale est le résultat d'une méthode d'affichage de l'objet MIR (au sens de la POO) qui la produit après lecture des valeurs courantes de ses composants (eux-mêmes objets liés par agrégation). En conséquence, elle reflète un état toujours à jour des connaissances que nous avons rassemblé. Chaque objet partie-de MIR est accessible et manipulable indépendamment de ce concept « de synthèse » au travers d'applications Web construites sur un même modèle 6 . L'interface de navigation dans ces applications (cf. Fig. 6) s'ouvre sur un portail général permettant d'accéder par l'objet architectural (haut droite) aux différents modules rassemblés (terminologie, toponymie, documentation, etc.), mais aussi de manipuler indépendamment chacun de ces modules (bas droite, la terminologie). 
Summary
In order to understand and represent the evolutions of architectural artefacts, an issue renewed by the development of information technologies, an analyst bases its study on pieces of knowledge and information progressively gathered and filtered. In our domain of interest i.e. the architectural heritage, these elements are often space related (a link between the artefact's GHVFULSWLRQQDQGGLWVVORFDWLRQQ (ex. heterogeneity, ambiguity, contradicting character, etc.). We take advantage of the spacerelation property in order to gather and unify the various pieces of knowledge related to an artefact: generic knowledge, documentation, present-day observations. This approach, that we have named informative modelling, aims at gaining insight into an architectural artefact's evolution and into the information gathered about it. Our contribution introduces the scientific background of our approach, and the methodological framework we have derived from it. It also discusses its implementation -the Main Square in Cracow (Poland) -in order to evaluate the possible benefits in terms of knowledge management and visualisation.

Introduction
Le cadre général de l'apprentissage automatique part d'un fichier d'apprentissage comportant n lignes et p colonnes. Les lignes représentent les individus et les colonnes les attributs, quantitatifs ou qualitatifs observés pour chaque individu ligne. Dans ce contexte, on suppose également que l'échantillon d'apprentissage est relativement conséquent par rapport au nombre d'attributs. Généralement la taille de l'échantillon est de l'ordre de 10 fois le nombre de variables pour espérer obtenir une certaine stabilité, c'est-à-dire une erreur en généralisation qui n'est pas trop loin de l'erreur en apprentissage. De plus, l'attribut à prédire est supposé à valeur unique. C'est une variable à valeurs réelles dans le cas de la régression et c'est une variable à modalités discrètes, appelées classes d'appartenance, dans le cas du classement. Ces questions relatives aux rapports entre taille d'échantillon et taille de l'espace des variables sont étudiées de façon très approfondies dans les publications relatives à l'apprentissage statistique (Vapnik, 1995). Dans ce papier nous décrivons une situation d'apprentissage qui s'écarte significativement du cadre classique tel que décrit plus haut. En effet, le contexte expérimental ne nous permet pas de disposer immédiatement d'un ensemble d'apprentissage conséquent, chaque individu peut appartenir à plusieurs classes simultanément, et chaque individu, au lieu d'être décrit par un ensemble attributs-valeurs, l'est par un texte en langage naturel en anglais.
Avant de décrire l'approche que nous préconisons pour apprendre dans ce contexte, nous allons tout d'abord rappeler la problématique de l'application visée (section 2). En section trois, nous décrivons l'approche méthodologique retenue. Dans la section quatre, nous décrivons les étapes mises en oeuvre pour mettre en forme les données et notamment, la stratégie d'analyse linguistique mise en oeuvre pour extraire les principaux concepts qui vont jouer le rôle de variables. Nous décrivons ensuite, section 5, les modèles topologiques à base de graphes de proximité qui nous permettent de gérer le multi-classes. Dans un but comparatif, nous utilisons une méthode à base d'arbre de décision qui nous sert également à mieux identifier les concepts discriminants. En section 6, nous présentons les résultats issus de l'analyse linguistique et de l'apprentissage. Nous décrivons également le principe de l'apprentissage par boucle de pertinence (relevance feedback). Ce concept est central car il met l'usager dans la boucle visant à améliorer le modèle de prédiction. Nous détaillons les performances obtenues. En section 7, nous concluons et détaillons les perspectives de ce travail, notamment l'utilisation de méthodes de règles d'association.
Cadre expérimental
Problématique
Ce travail s'inscrit dans un projet en collaboration avec le Bureau International du Travail (BIT). Plusieurs pays ont signé des conventions avec le BIT qui les lient au droit du travail international. Plus concrètement, l'accord porte sur deux conventions élaborées par le BIT, 1 La Convention n°87 et la Convention n°98. Celles-ci contiennent une série d'articles de lois que le signataire s'engage à respecter. Ces derniers sont soumis, une fois par an, à une inspection ayant pour but de vérifier la bonne application de ces conventions. A la fin de chaque inspection, les experts du BIT délivrent un rapport au pays concerné. Le rapport fait état des règles non appliquées, des violations constatées à partir de faits concrets et souligne les efforts à mettre en place pour être en adéquation avec les conventions. Il est en texte libre sans codification rigide des violations. Tous les rapports d'experts sont stockés dans une banque de données accessible aux membres et aux experts qui effectuent régulièrement des analyses, définissent de nouvelles recommandations et étudient les évolutions du droit du travail dans les différents pays, etc. L'objectif de notre travail est définir et de mettre en place des méthodes et des outils de data mining permettant de traiter plus efficacement et plus rapidement ces corpus qui deviennent inexploitables manuellement. Les experts du BIT souhaiteraient avoir des outils permettant le repérage automatique des textes signalant la violation d'une ou plusieurs règles par pays. La finalité étant la catégorisation automatique des textes non étiquetés. Les experts pourront alors synthétiser plus vite les difficultés que rencontrent les différents pays dans l'application de ces conventions, et, le cas échéant, identifier les moyens de les aider.   Soit ? un algorithme d'apprentissage supervisé. Cela peut être un graphe d'induction (Zighed et Rakotomalala, 2000), un SVM (Vapnik, 1995), etc. Le résultat d'un apprentissage est un modèle noté M et un taux d'erreur ? en généralisation estimé sur échantillon test ou par cross validation.
Description du corpus
L'application du modèle M sur un échantillon de textes anonyme ? de taille relativement modeste, disons une vingtaine de cas, permet de prédire pour chaque individu anonyme ?? les règles qui seraient violées, ?(??) = {c , c } par exemple. Le   (Toussaint (1980)) qui font partie des méthodes d'apprentissage à base d'instance permettent cela. Toutes ces techniques supposent par ailleurs que les individus sont plongés dans un espace de représentation sur lequel on peut définir une métrique. Les textes doivent par conséquent être transformés en un ensemble de vecteur. Chaque texte pourra être alors considéré comme un point de R p . Les coordonnées d'un texte ? dans cet espace seront X(?) = (X 1 (?), X 2 (?),…, X p (?)) . Que représente alors ces variables, comment sont elles extraites ? C'est l'objet de la partie analyse linguistique. L'objectif étant de trouver les concepts et les plus adaptés.
Application sur les données du BIT
Nous effectuons l'extraction de la terminologie sur l'intégralité du corpus. La finalité de cette extraction est la construction de concepts relatifs aux deux Conventions. Les concepts ainsi extraits constituent l'espace de représentation des documents. Les textes étiquetés par les experts du BIT (violations connues), nous servent ensuite de base d'apprentissage. Nous utilisons deux classifieurs : C4.5 (Quinlan, 1993) et les graphes des voisins relatifs (GVR) (Toussaint, 1980) dans le but de prédire les violations contenues dans les textes non étiquetés. La méthodologie est décrite par (figure 1). RNTI-E-6 V. Pisetta et al.
FIG. 1 -Méthodologie d'analyse
Espace de représentation des textes
Extraction de la terminologie
Nous choisissons de construire notre espace de représentation par extraction de concepts. Un des avantages de cette technique par rapport à des méthodes telles que les N-grammes, ou les matrices de co-occurrences de mots, est la réduction importante de la dimensionnalité, permettant notamment l'usage de classifieurs utilisant des mesures de similarité. Diverses applications basées sur ce principe ont données des résultats intéressants (Kumps et al., 2004). Deux méthodes différentes existent pour la construction de concepts : par apprentissage et par extraction.
La première (statistique) recherche les mots les plus discriminants selon un attribut à prédire. Les mots sont ensuite regroupés en concepts sur la base de leur co-occurrences ou à partir de règles d'association (Kumps et al., 2004). La seconde méthode (linguistique) consiste à extraire la terminologie du corpus et à regrouper les termes extraits selon leur proximité sémantique.
Notre préférence se porte vers les techniques d'analyse linguistique. Ce choix se justifie par le fait que l'analyse linguistique permet de lutter contre la polysémie et de lever certaines ambiguïtés liées au contexte (Flurh, 2000). Elle fonctionne également sur de petites unités textuelles (Pouliquen et al., 2002). De plus, notre base d'apprentissage comportant peu d'exemples, il nous semble difficile d'utiliser les techniques d'apprentissage décrites plus haut. Notre travail est effectué en collaboration avec des experts du domaine juridique, ce qui est une raison supplémentaire pour utiliser les techniques linguistiques. Nous utilisons la chaîne de traitement décrite en (figure 2).
FIG. 2 -Chaîne de traitement linguistique
Après une phase de normalisation (traitement des noms propres, conservation ou non des majuscules, etc.), nous effectuons un étiquetage grammatical du corpus grâce au logiciel BRILL (Brill, 1995). Cette opération a pour but d'attribuer à chaque mot son étiquette grammaticale. Nous passons ensuite à l'extraction de la terminologie à l'aide d'EXIT (Roche et al., 2004). La méthodologie mise en place dans EXIT est avant tout basée sur une approche statistique, contrairement à d'autres approches (Bourigault et Jacquemin, 1999). L'extraction terminologique passe par la recherche de candidats-termes. Ces derniers sont des ensembles de deux ou plus unités adjacentes lexicales (mots), syntaxiques (mots étiquetés grammaticalement, ce qui est notre cas) ou sémantiques (mots étiquetés conceptuellement). Nous groupons ensuite les candidats-termes extraits selon leur proximité sémantique, de manière à repérer les concepts présents dans les textes.
Choix des candidats termes et création de concepts
Nous reprenons ici une méthodologie utilisée par (Baneyx et al., 2005). Nous distinguons deux étapes dans la sélection des candidats termes : -Dans un premier temps, nous parcourons l'ensemble des résultats donnés par EXIT, et nous étudions tout d'abord les termes dont la fréquence d'apparition est supérieur à un seuil l. Dans un premier temps, nous fixons un seuil élevé. Cette étape préliminaire permet de repérer les grands axes conceptuels ; -Nous regroupons ensuite les candidats termes sémantiquement proches à l'aide d'outils tels que WordNet (Miller et al., 2005). Le recours à l'expert est ici primordial. Certaines plates-formes proposent des outils plus sophistiqués comme Syntex-Upery (Bourigault et Jacquemin, 1999) qui permettent d'analyser la proximité distributionnelle entre les candidats-termes.
Les phases de construction 1 et 2 étant itératives, nous augmentons très rapidement la représentation en examinant les candidats-termes dont la fréquence d'apparition dans le corpus est inférieure au seuil l. L'augmentation progressive du nombre de candidats-termes possède deux issues : -certains « nouveaux » candidats-termes viennent renforcer des concepts existants ; -d'autres « nouveaux » candidats-termes créent de nouveaux concepts qui peuvent être des concepts fils de ceux existants.
Représentation vectorielle des documents
A ce niveau se pose le problème du choix du modèle de représentation. Nous avons choisi le modèle vectoriel (Salton, 1971) qui nous paraît plus adapté que le modèle booléen. La raison est qu'il semble simpliste d'appliquer une logique binaire à une recherche d'information. De plus, le modèle vectoriel permet de calculer des scores de similarités entre différents documents (Pouliquen et al., 2002).
Le modèle vectoriel propose de représenter un document sur les dimensions représentées par les mots. Nous l'avons adapté pour représenter un document par un vecteur de concepts. Et, plutôt que de le représenter en fonction de la fréquence du concept dans le document, nous utilisons la pondération TF x IDF (Salton et Buckley, 1988). Ce score permet de donner une importance au concept en fonction de sa fréquence dans le document (TF = Term Frequency) pondérée par la fréquence d'apparition du concept dans tout le corpus (IDF = Inverse Document Frequency). Ainsi un concept très spécifique au document aura un score correspondant à sa fréquence d'apparition, par contre, un concept apparaissant dans tous les documents du corpus aura une pondération maximale. Nous calculons donc, pour chaque concept dans un document, son score TF x IDF. 
Modélisation et outils de généralisation
Nous avons à présent défini un espace de représentation pour les documents. L'objectif est maintenant d'utiliser des techniques d'apprentissage dans le but de classer automatiquement les documents. Dans notre étude, nous sommes amenés à classer des textes « multi-étiquettes », autrement dit, susceptibles de comporter plusieurs violations. Deux possibilités sont alors envisageables :
-une approche globale ; -une approche binaire en une division en m sous-problèmes.
Nous effectuons les deux approches. Deux classifieurs différents sont utilisés. Nous utilisons les arbres de décision dans l'approche binaire et les graphes de voisinage dans l'approche globale.
Prédiction par le graphe des voisins relatifs
La représentation vectorielle de nos documents est d'une dimensionnalité très raisonnable et nous permet par conséquent d'avoir recourt à des classifieurs basés sur la notion de voisinage. Nous avons choisi les graphes de proximité provenant de la géométrie computationnelle (Preparata et Shamos, 1985) plutôt que les k-NN. Les graphes présentent plusieurs avantages par rapport aux k-NN et permettent de mieux définir la proximité entre des individus (Clech, 2004). Ils nécessitent une mesure de dissimilarité (Toussaint, 1980). Nous choisissons la distance euclidienne. Plusieurs modèles de graphes existent. Notre choix se porte sur le graphe des voisins relatifs qui est un bon compromis entre nombre de voisins  ( , ) est vide (Toussaint, 1980). De façon formelle: 
Prédiction par arbre de décision
Nous nous plaçons ici dans l'optique de prédire la présence ou l'absence de chaque violation. Nous construisons par conséquent autant d'arbres qu'il existe de règles. Plus formellement, nous considérons chaque règle comme étant un attribut booléen c i = {0,1} . S'il existe k règles pour la violation v ? , nous construisons k arbres. Chaque arbre est alors un modèle M i prévoyant la présence ou l'absence de chaque règle c i . Nous obtenons ainsi k modèles qui renvoient c i si la règle i est estimée violée, ? sinon. Notons que cette approche est valable dans la mesure où les violations sont a priori indépendantes. Il suffit ensuite d'agréger les modèles pour obtenir un « méta-modèle » donnant la liste des violations détectées pour le texte ? ' . La discrimination est effectuée par l'algorithme C4.5.
Résultats, méthodes et comparatif
A l'issue de l'analyse linguistique, nous obtenons 17 concepts pour la Convention n°87 et 11 concepts pour la convention n°98. Nous présentons les résultats observés sur la Convention n°98. Notre base de textes étiquetés est de taille modeste (65 textes). A ce jour, une étape du processus de relevance feedback a été réalisée. Elle concerne 20 textes qui ont été étiquetés par les experts du BIT et qui n'étaient pas présents initialement dans la base d'apprentissage. Nous présentons les résultats de la prédiction sur ces 20 textes issue de C4.5 et GVR. Dans un but comparatif, nous avons utilisé les SVM selon le même principe que pour C4.5. Les SVM sont des méthodes robustes résistant très bien à la forte dimensionnalité des données (Joachim, 1998). La différence essentielle réside dans le fait que les SVM sont utilisées sans pré-traitement des textes (excepté la normalisation). Les résultats sont présentés en 6.1.
Résultats obtenus
Nous présentons les résultats obtenus en terme de reclassement. Les résultats sont décrits dans (tableau 1).
On observe de bon taux de reclassement. Notons qu'il n'existe qu'une seule violation pour laquelle SVM fait mieux que C4.5 ou GVR. La non prise en compte de séquences de mots par SVM rend les prédictions parfois instables, ce qui se traduit par une mauvaise sensibilité ou spécificité. Nous observons des taux de sensibilité-spécificité parfois nuls pour GVR. Ceci est dû au fait que deux des dix violations (n°4 et n°10) sont peu fréquemment rencontrées dans le corpus d'apprentissage. Ainsi, il y a peu de chances que les quelques textes contenant ces violations soient en nombre suffisant pour être pris en compte dans le voisinage de l'individu à étiqueter. Ce problème peut éventuellement se résoudre par la technique de retour pertinent décrite précédemment. 
C4.5 GVR SVM
Conclusion et perspectives
La finalité de ce travail est de proposer un modèle de prédiction capable de déterminer les violations de plusieurs pays concernant deux convention de droit du travail. Une approche d'apprentissage automatique a été adoptée. Dans un premier temps (préparation des données), nous avons extrait, grâce aux techniques d'analyses linguistiques, un ensemble de candidats termes qui nous permettent ensuite de construire des concepts relatifs au corpus étudié. Cette opération a pour but de réduire la dimensionnalité de l'espace de représentation des textes du corpus. Nous avons été ainsi en mesure d'utiliser les graphes de voisinage, en plus d'une méthode plus classique (C4.5) pour la catégorisation automatique.
Les résultats semblent intéressants dans la mesure où les deux méthodes de prédiction que nous utilisons aboutissent à des taux de reclassement tout à fait acceptables en dépit d'une base d'apprentissage comportant peu d'exemples. Nous envisageons à présent d'augmenter la taille de celle-ci dans le but d'améliorer la prédiction et d'aboutir à des résultats plus robustes. La phase de test avec les experts du BIT est en cours. La liste des concepts extraits du corpus a été validée par ces derniers.
L'une des perspectives de ce travail est d'observer l'impact du relevance feedback sur la qualité de prédiction. En effet, cette dernière devrait augmenter au fur et à mesure du nombre d'interventions des experts. De plus, il serait intéressant de comparer de nouveau la qualité de prédiction de notre approche avec les SVM lorsque la base d'apprentissage sera plus conséquente. L'utilisation d'autres techniques de catégorisation textuelles, comme Winnow (Dagan et al., 1997) et éventuellement d'autres classifieurs peut aussi s'avérer intéressantes.

Problématique
Tout géographe s'accorde à dire que tout phénomène à un endroit est lié à l'influence du voisinage (première loi en géographie). Ceci revient à dire que les données spatiales ne sont pas indépendantes et que leurs analyses nécessitent, en plus des caractéristiques des objets à analyser, la prise en compte des caractéristiques des objets du voisinage et des relations spatiales qui les relient.
Approche proposée
Dans notre état de l'art, nous avons recensé des insuffisances dans les outils d'analyses spatiales ; et afin d'y remédier, nous avons proposé une méthodologie pragmatique fondée sur des bases théoriques en tenant compte : de l'inexistence des entrepôts de données dans la majorité des organismes ; de la nature complexe des données à référence spatiale ; des limites des fonctionnalités analytiques des outils existants entre autre Systèmes d'Informations Géographiques ( SIG) et datamining ; etc. Afin de résoudre cette problématique, nous proposons la combinaison d'un SIG avec un ensemble de techniques de datamining.
Dans un premier temps, nous avons proposé le cadre conceptuel permettant de définir la manière selon laquelle la combinaison devrait s'opérer. Ce cadre a été illustré par la présentation d'un enchaînement de phases devant constituer le processus décisionnel incluant un SIG et un ensemble de techniques d'extraction. Ce processus se présente comme suit : Préparation des données : Consiste à préparer la base de données géographiques .
Consultation des données :
Consiste à analyser les données préparées pour vérifier si les critères du problème posé ont été respectés.
Création d'un index de jointure : Cette étape est d éfinie comme la spécificité du datamining spatial par rapport au datamining classique. Elle permet de pré-calculer la relation spatiale exacte entre les objets spatiaux de deux collections puis de les stocker dans une table, pour y appliquer les techniques de datamining pour une meilleure exploitation. La méthode que nous avons développée pour la création des index de jointures est une méthode proposée par Zeitouni (2000). Cette méthode est une extension de l'index de jointure qui stocke la valeur de distance entre les objets. Contrairement aux anciennes méthodes proposées pour la jointure basée sur la distance, seuls les objets ayant une distance raisonnable (définie par le concepteur de l'index) sont stockés dans cet index. Ce qui optimise à la fois la construction et l'utilisation de l'index. Dans le cadre de cette étude, une attention particulière a été portée au paramétrage du critère de jointure.
Choix de l'algorithme de calcul : Notre étude porte sur une seule technique de datamining spatial qui est la classification supervisée par les arbres de décisions. Un arbre de décision a pour but de trouver les attributs explicatifs et les critères précis donnant le meilleur classement. L'arbre est construit par l'application successive de critères de subdivision sur une population d'apprentissage afin d'obtenir des sous populations plus homogènes. Dans le cadre de notre étude, nous proposons une extension de la méthode CART Zeitouni (2000) ainsi qu'une extension de la méthode ID3 Zeitouni (2000). Ces deux dernières reposent sur le calcul d'un gain informationnel pour apprécier la subdivision.
Exécution de l'algorithme de calcul : Une fois que l'index de jointure est créé, nous pouvons lancer l'exécution de l'algorithme de classification par arbre de décision.

Introduction
Pour créer un nouveau médicament, la pharmacologie opère en deux temps. Tout d'abord elle synthétise un grand nombre de molécules. Ces molécules sont ensuite appliquées sur un substrat simulant la pathologie que le médicament recherché doit combattre. Le débit de molécules synthétisées puis testées a grandement augmenté ces dernières décénnies avec l'introduction de la synthèse combinatoire et le criblage à haut débit (Hou et al., 2004). Ce processus peut néanmoins être encore amélioré. En effet, une propriété essentielle des médicaments est de pouvoir être solubles pour circuler à travers le système sanguin afin d'atteindre la partie malade de l'organisme, or cette propriété n'est pas vérifiée par toutes les molécules. Idéalement, les molécules non solubles ne devraient être ni testées ni même synthétisées afin d'accélérer le processus.
La solubilité d'une molécule est représentée par un attribut numérique nommé indice de solubilité. Les laboratoires pharmacologiques connaissent cette valeur pour un grand nombre de molécules. Ceci motive l'utilisation de méthodes issues de la fouille de données pour induire un modèle qui, à partir de la structure d'une molécule, prédit son indice de solubilité.
Dans le cadre de cette application, une base de données permet de décrire les molécules à partir de trois tables : - La table molécule contient les caractéristiques globales de la molécule, réduites,  Dans cet article, nous commençons par proposer une méthode de construction automatique d'attributs. Nous détaillerons ensuite notre algorithme de modèle d'arbre couplé à du bagging qui, à partir des attributs précédemment créés, permet d'induire un modèle de régression. Enfin, nous étudions les performances de notre approche, en démontrant l'apport de chacune des techniques utilisées et en comparant nos modèles induits avec les meilleurs de la littérature sur ce sujet.
Construction automatique d'attributs
L'enrichissement manuel des données demandant du temps, des compétences et des logiciels couteux, nous proposons donc une technique pour construire automatiquement des attributs à partir de la structure des molécules.
Jusqu'à récemment, la construction des nouveaux attributs à partir de données relationnelles se basait soit sur la sélection (Kramer et al., 2001) qui construit des attributs booléens (par exemple est-ce que la molécule a une liaison carbone-oxygène ?), soit sur l'agrégation (Perlich et Provost, 2003) qui contruit des données nominales ou numériques (par exemple le nombre d'atomes de la molécule). Seule l'union de l'agrégation et de la sélection permet d'exprimer des attributs comme : nombre de liaisons carbone-oxygene. Vens et al. (2003) sont les seuls, à notre connaissance, qui combinent l'aggrégation et la sélection. Ils utilisent des forêts aléatoires pour effectuer une sélection dans l'espace des attributs constructibles.
Nous nous proposons d'utiliser les graphes de sélections afin de définir des attributs constructibles. Les graphes de sélection introduits par (Knobbe et al., 1999) permettent d'exprimer graphiquement des motifs sur des données multi-relationnelles. Un motif est un ensemble de caractéristiques de structure qui peuvent être, ou non, vérifiées par une molécule. Un exemple est donné à la figure 1 représentant une liaison (de type quelconque) entre un atome de carbone et un atome d'oxygène.
FIG. 1 -Exemple de graphe de sélection à gauche avec son interprétation comme motif chimique à droite.
Avant de créer les nouveaux attributs, nous cherchons les motifs dans les données. Pour ce faire, nous construisons les graphes de sélection suivant une grammaire simple. Les graphes de sélection sont composés d'un noeud molécule, suivi d'une alternance de noeuds atome et liaison. Au niveau des motifs cela permet d'exprimer des séquences d'atomes. Pour des questions de calculabilité, nous nous limitons à des séquences d'au plus 2 atomes. En effet, nos tests ont montré que des séquences plus longues (jusqu'à 4 atomes) n'améliorent pas le résultat.
L'algorithme considère l'ensemble des graphes de sélection constructibles pour les données, et pour chacun construit un nouvel attribut numérique. La valeur des attributs construits sera, pour chaque exemple, le nombre d'occurrences du graphe de sélection.
L'algorithme construit également des attributs plus complexes. Pour ce faire, nous procé-dons comme précédemment, mais au lieu de se placer au niveau de la molécule, nous nous plaçons au niveau des atomes et des liaisons en ajoutant, par exemple, pour chaque atome, un attribut représentant le nombre de ses liaisons avec d'autres atomes. Il faut néanmoins noter que ces attributs ne sont pas des descriptions en soit de la molécule mais uniquement des composants de la molécule. Pour répercuter l'information au niveau de la molécule, nous utilisons les opérateurs classiques d'agrégation suivants : moyenne, somme, minimum, maximum. Ainsi, on peut ajouter à la description de la molécule le nombre moyen de liaisons de chacun de ses atomes par exemple.
Algorithme d'induction de modèles de régression
Le problème de prédiction de l'indice de solubilité est généralement résolu en utilisant soit un réseau de neurones multicouches (Tetko et al., 2001;Huuskonen, 2000) soit une régression linéaire multiple (Hou et al., 2004;Delaney, 2004).
Dans le domaine de la fouille de données, d'autres algorithmes ont été développés et donnent de bons résultats, nous nous intéressons particulièrement aux algorithmes à base d'arbres.
-671 -RNTI-E-6 Dans l'approche proposée, l'algorithme induit un arbre de modèles où les feuilles et les noeuds internes jouent des rôles complémentaires. Une feuille effectue la tâche de régression proprement dite. A partir d'un ensemble d'exemples, la feuille effectue une régression linéaire multiple avec les attributs autorisés par ses noeuds ascendants. Une feuille racine n'a accès à aucun attribut. Un noeud interne, appelé un raffinement, a pour but de simplifier la tâche de sa descendance. Pour cela, un noeud peut soit partitionner l'espace des exemples (cas typique des arbres de décision) selon un seuil sur un des attributs numériques construits, soit proposer un nouvel attribut qui sera accessible à sa descendance. Le nombre de fils d'un noeud dépend du type de raffinement effectué, un raffinement partitionnant l'espace des exemples selon une condition booléenne aura deux fils, tandis qu'un noeud introduisant un nouvel attribut n'aura qu'un fils. La construction de l'arbre se déroule de la façon suivante. L'algorithme débute en créant une feuille initiale. Comme elle n'a accès initialement à aucune variable numérique, la régression linéaire se réduit à une constante. Ensuite cette feuille est raffinée, l'algorithme teste tous les raffinements possibles, c'est-à-dire tous les ajouts d'attributs et tous les partitionnement possibles. Enfin le raffinement améliorant au mieux l'erreur du modèle est conservé.
L'arbre ainsi induit est bien souvent sujet au sur-apprentissage. Dans le cas des arbres, une façon de palier le sur-apprentissage est de procéder à un élagage de l'arbre obtenu. Nous utilisons l'élagage par réduction d'erreur définie par Quinlan (1987) avec un jeu de validation représentant un tiers du jeu d'apprentissage.
Enfin nous utilisons la technique du bagging (Breiman, 1996) en induisant 50 modèles avec chacun une partition jeu d'apprentissage et jeu de validation différent, ce qui donne une indépendance aux modèles induits. La prédiction finale est la moyenne des prédictions des 50 modèles.
Résultats
Les jeux de données utilisées dans nos tests correspondent à des ensembles de molécules de différents types dont la solubilité est connue. Ces données nous ont été fournies par le laboratoire d'Infochimie ULP/CNRS UMR 7551. Dans tous les cas les résultats sont obtenus à l'issue d'une validation croisée en 10 partitions.
Les premiers test ont été réalisés sur un jeu de 511 molécules. Les résultats obtenus sont présentés dans la table 1. On peut remarquer que l'utilisation conjointe du bagging et des arbres de modèles permet de réduire sensiblement l'erreur. Le bagging donne de meilleurs résultats sur les arbres de modèles que sur les régressions linéaires multiples.
Nous avons également comparé nos résultats à ceux obtenus dans le milieu de l'infochimie. Pour ce faire, nous avons utilisé un jeu d'apprentissage de 1635 molécules et le jeu de Yalokowsky (Yalkowsky et Banerjee, 1991)  
Conclusion
Dans cet article nous avons proposé une nouvelle méthode d'induction de modèles de pré-diction de la solubilité des molécules. Cette méthode se base sur l'utilisation de techniques nouvelles dans ce domaine d'application, comme les arbres de modèles et le bagging. Nous proposons également une méthode pour utiliser les données multi-relationnelles brutes, sans l'ajout d'attributs experts, par construction automatique d'attributs numériques en utilisant des opé-rateurs d'agrégation. Nos résultats sont proches de ceux obtenus par les meilleurs approches développées jusqu'à présent, approches utilisant des attributs experts.
Plusieurs voies d'évolution sont possibles. La première consisterait à augmenter la capacité de construction d'attributs en ne la limitant plus à des motifs simples. Cela peut passer par l'utilisation d'éléments de plus haut niveau dans les motifs, comme l'utilisation des cycles aromatiques (plusieurs atomes de carbone formant un cercle). Une seconde voie consiste en l'extension des arbres de modèles qui, plutôt que de se limiter à des régressions linéaires multiples, utiliseraient, quand c'est indiqué, des réseaux de neurones de topologie simple.
Références Appice, A., M. Ceci, et D. Malerba (2003). Mining model trees : A multi-relational approach.
In ILP, Volume 2835 of Lecture Notes in Computer Science, pp. 4-21. Breiman, L. (1996). Bagging predictors. Machine Learning 24(2), 123-140.

Introduction
Depuis plusieurs années, des travaux de recherche importants sont déployés pour permettre aux radars de réaliser des tâches liées à l'intelligence artificielle, telle que la reconnaissance des cibles. Pour l'homme, l'acquisition des images et l'identification de cibles s'effectuent par l'intermédiaire du système visuel. L'oeil humain peut être défini comme un capteur qui va transmettre ses données au cerveau de façon à traiter les informations et prendre une décision. Le système de perception visuel humain « oeil-cerveau » est régi par des mécanismes très complexes, qui ont toutefois des limitations. Par exemple, l'oeil n'est sensible qu'à certaines longueurs d'ondes, son spectre de visibilité est limité et sa sensibilité diminue avec l'obscurité. Pour résoudre ces problèmes, des systèmes d'aide à la décision ont été développés. Ils sont capables de percevoir l'environnement au-delà du système sensoriel et de réaliser des étapes de perception de plus en plus fines. Pour répondre à ce besoin, dans le domaine militaire, différentes technologies ont été mises au point par l'intermédiaire de capteurs spécifiques tel que le radar. En contrepartie les quantités d'informations à gérer sont devenues gigantesques et délicates voire impossible à traiter rapidement pour prendre une décision. Ainsi que la sensibilité des signaux radar aux conditions opérationnelles, sujettes aux perturbations environnementales et aux conditions de mesure, exige une prise en compte. Le problème traité dans ce papier s'insère dans le cadre général de l'identification noncoopérative d'une cible aérienne à partir de la rétrodiffusion d'un signal radar multifréquentiel (Toumi et al., 2005) ( Hoeltzener et al., 2003).
Cadre méthodologique et processus ECD
Les aspects académiques concernent davantage le problème de l'extraction de primitives les mieux adaptées aux données radar et la prise en compte des imperfections avant de passer à l'étape de fouille de données (Frawley et al., 1991). Se rajoute à cela, l'idée d'un fonctionnement en mode supervisé pour la validation des informations candidates. L'intervention de l'opérateur humain s'est montrée utile notamment, dans le cadre des expérimentations et simulations des données radar (cf. section suivante)
Base de données en expérimentation radar
L'utilisation conjointe des données synthétiques et réelles est une pratique courante pour la validation des différentes méthodes de reconnaissance. C'est dans ce contexte que nous avons eu recours à des données expérimentales acquises en laboratoire dans une chambre anéchoïde1 en utilisant des maquettes à l'échelle 1/48 ème modélisant l'interaction radar cible. En outre, le volume de la base de données augmente très rapidement avec le nombre de cibles pour un état donné (configuration) du système d'acquisition, la première base de données renseignée a été réalisée à partir de simulations fines du système d'acquisition produisant l'ensemble de données 1D et 2D. La base des données 2D contient plus de 2430 images pour 15 maquettes et par polarisation (4 polarisations au total).
Dans ce papier, nous traitons que les données images (images ISAR) (Kok, 1998) reconstruites à partir des signature à haute résolution à une dimension (profils distance) par la transformée de Fourier.
Orientation vers le calcul de descripteurs de forme
Pour la phase de préparation des images ISAR, nous aurons recours aux techniques essentiellement issues de recherches effectuées en reconnaissance de forme. Plusieurs états de l'art on été présentés dans (Rui et al., 1999). Généralement les primitives visuelles sont regroupées en trois classes (Mezhoud et al., 2000) : les descripteurs liés à la couleur (histogramme), les descripteurs de texture (matrice de cooccurrence, indice de direction principale et de rugosité, filtre de Gabor et ondelettes) et les descripteurs de formes (descripteur de Fourier et des moments, points caractéristiques, etc.).
Dans ce contexte de travail, nous nous intéressons à trouver une présentation pertinente des images ISAR via des primitives visuelles significatives et fiables. Nous traiterons la modélisation et l'indexation logique des images ISAR avec la prise en compte de l'imprécision liée au système de mesure (section 4).
Dans notre approche, le premier traitement à effectuer est la segmentation des images ISAR en niveau de gris. Il est difficile de définir, de manière absolue, une bonne « segmentation ». Sa qualité est en partie fonction des résultats obtenus par les traitements situés en aval qui utilisent les primitives extraites.
Pour le cas des images ISAR à traiter, les techniques de détection de contour fondées sur des techniques dérivatives ne fournissent que des ensembles de contours non fermés révélant un certain nombre de disparités localisées au sein même de la signature de la cible. Leur traitement à ce titre entraînerait une complication pour la reconstruction d'une forme plus générale de la cible. Afin d'atteindre l'objectif, le choix s'est porté sur des opérateurs de morphologie mathématiques (Beucher et Meyer, 1993), la technique retenue est celles des lignes de partage des eaux (LPE).
Segmentation des images ISAR par LPE
La segmentation morphologique consiste à appliquer la LPE sur l'image gradient, dérivée de l'image originale. La LPE est une technique très importante parmi les techniques de segmentation. Elle utilise la terminologie de la géographie qui définit la LPE comme la crête qui forme la limite entre deux bassins versants dans une image considérée comme une surface 3D. Le choix de telle méthode est sollicité par le fait que les points brillants sont caractérisés par des pics assez importants des autres régions reflétant le signal émis par un radar.
L'application directe de la technique LPE sur le gradient des images ISAR, donne des images sursegmentées. Pour dénouer ce problème, nous avons procédé au renforcement les variations de niveaux de gris en passant par l'image simplifiée. Par conséquent, l'image initiale va être transformée en image mosaïque (Beucher et Meyer, 1993). L'image mosaïque peut s'interpréter comme un graphe sur lequel sont évalués des arcs. Le graphe valué de l'image mosaïque est appelée gradient mosaïque. Ce gradient est la fonction h définie sur tous les arcs de l'image et ses niveaux de gris correspondent à la différence de valeur existant entres deux composantes connexes. En éliminant les premiers niveaux, nous arrivons à garder l'information nécessaire révélant la forme générale de la cible (cf. figure 1).
Une fois l'image segmentée, on a besoins d'extraire les informations du contour pour procéder à la reconnaissance des formes. C'est pourquoi, elle est généralement représentée dans un formalisme par des descripteurs de forme (cf. Section 3.1.1). 
FIG. 1 -LPE du gradient mosaïque.
Formes et descripteurs
Le problème fondamental dans la reconnaissance de forme est de déterminer dans quelle mesure deux formes sont similaires, indépendamment de leur position dans l'image. Il en découle que les descripteurs de forme doivent être précis, compacts et invariants à un certain nombre de transformations géométriques (translation, rotation, changement d'échelle et du point origine de la forme). Nous trouvons une étude plus détaillée des descripteurs de forme utilisant des méthodes standards telles que Fourier (Mezhoud et al., 2000) ou Fourier Mellin (Teoch et al., 2004) qui sont largement utilisés dans les systèmes de recherche actuels. Nous proposons ici, de modéliser la forme d'une cible par les descripteurs de Fourier.
On considère f(x) comme étant une fonction périodique continue différentiable définie sur [0,2?] qui pourra être une des fonctions de contours de la forme à étudier (en forme complexe). Une telle fonction peut être approximée par une série de Fourier C(k) où les coefficients de Fourier dépendent généralement de la forme de la cible : les coefficients tendent à décrire les caractéristiques globales d'une image pour des k petits et décrivent beaucoup plus finement ces formes pour des k plus grands. Enfin, seule une vingtaine de descripteurs de Fourier (conservation de l'énergie par l'égalité de Parseval) est gardée pour représenter la forme d'une façon compacte et précise pour des mises en correspondance lors du processus de recherche/reconnaissance.
Cependant, un grand défi réside dans le fait que les connaissances dont nous disposons sont imparfaites. La prise en compte de ces imperfections dans l'étape d'acquisition et préparation des données (en référence au processus ECD) doit conduire à tenir compte de la variabilité/hétérogénéité des primitives extraites pour différentes expérimentations.
Prise en compte de l'imperfection
Vu que les données acquises sont fortement liées au scénario d'acquisition, les données sont souvent dépendantes des paramètres environnementaux (le taux d'humidité, la chaleur, la présence d'autres objets, etc). La manière la plus commune de caractériser l'imperfection attachée à une mesure consiste alors à répéter cette mesure dans les mêmes conditions expérimentales (même expérimentateur, même matériel, même protocole). Le résultat de la mesure varie généralement d'une expérience à une autre. Ce phénomène peut malheureusement engendrer des imperfections au niveau de l'étape de préparation de données, telle que, l'obtention pour différentes expérimentations, de différentes formes extraites des images pour une même cible. Par conséquent, nous avons mis en place une démarche pour la prise en compte de cette imperfection en introduisant la notion de qualité de données.
Méthodologie
La gestion d'imperfection s'appuie donc sur un modèle de référence qui est considéré le plus précis et le plus complet pour un paramétrage donné du système d'acquisition. Ce modèle est caractérisé par des connaissances jugées suffisantes et extraites d'un ensemble de données simulées, mais aussi par un ensemble de règles établies par l'expert du domaine radar. Ces règles sont exprimées sous la forme de règles floues (Dubois et Prade, 1992)  (1) Si écart est important alors la qualité est mauvaise (2) Si la bande est large alors la résolution est fine
Nous cherchons à partir de ces règles à attribuer une qualité aux données en vue de la sélection de tel ou tel traitement ultérieur, voire d'aider à leur paramétrage, et ceci avant la phase de fouille de données.
Pour ce qui est de la règle (1) et de l'estimation de l'écart, nous évaluons le signal réfléchi durant l'acquisition des données, par rapport à un ensemble de propriétés : Spectre, distribution de probabilité du signal, etc. Sachant que cette évaluation se fait au niveau pour chaque réponse impulsionelle de chaque angle de visée du radar, une agrégation est réalisée à partir des différentes réponses qui participent à la reconstruction d'une image (exemple : on sélectionne un domaine angulaire de 20° pour reconstruire une image ISAR). C'est ainsi qu'un écart noté Ecart est calculé entre une donnée de référence D réf et la données acquise D acq pour la propriété P i : 
Conclusion
Dans ce papier nous avons présenté les travaux concernant la phase de préparation de données du processus ECD appliqué en expérimentation radar (reconstruction des images ISAR et modélisation de la forme par les descripteurs de Fourier après extraction par LPE). Notre démarche, motivée par la présence des imperfections dans les données et qui sont liées à leur sensibilité aux conditions de mesure et aux perturbations environnementales, tire profit des fonctions d'évaluation floues. Ceci permet de prendre en compte l'imperfection du système de mesure dans la préparation des données.
Ces travaux conduisent aussi à catégoriser les paramètres de la segmentation et de l'extraction de la forme pour chacune des qualités attribuée à une image. Il est en ce cas possible de prévoir une automatisation du système.

Introduction
Nous nous intéressons au problème de prétraitement de grands ensembles de données. Notre but est de réduire les informations contenues dans les ensembles de données volumineux aux informations les plus significatives. Il existe des techniques expérimentalement validées pour ce faire. D'un point de vue applicatif, un problème majeur se pose quant au choix d'une de ses méthodes. Une solution qui constitue notre contribution dans ce travail serait d'utiliser une combinaison de techniques ou de stratégies. A cet effet, nous nous appuyons sur la théorie du consensus. L'utilisation de cette combinaison de stratégies ou d'expertises peut être justifiée par l'un des faits suivants : -il n'est pas possible de déterminer a priori quelle méthode de sélection de sous-ensemble d'attributs est meilleure que toutes les autres (en tenant compte des différences entre le temps d'exécution et la complexité), -un sous-ensemble optimal d'attributs n'est pas nécessairement unique, -la décision d'un comité d'experts est généralement meilleure que la décision d'un seul expert. Les résultats obtenus après des expérimentations permettent de conclure que l'approche proposée réduit de façon significative l'ensemble de données à traiter et permet de les traiter interactivement. Cette contribution commence par un état de l'art et la problématique du sujet abordé, puis, l'algorithme de sélection d'attributs est présenté. Enfin, nous procédons à des expérimentations avant la conclusion.
Etat de l'art et problématique
Nous essayons de résoudre le problème suivant : comment sélectionner des attributs d'un ensemble de données pourvu de plusieurs attributs et rejeter les autres sans nuire à la qualité de l'algorithme de fouille visuelle utilisé ensuite ? Ceci tout en sachant que : -la visualisation de plus de deux dizaines d'attributs rend souvent inutilisable la fouille visuelle de données, -un sous-ensemble optimal d'attributs n'est pas nécessairement unique, -il n'est pas possible de déterminer a priori quelle méthode de sélection de sous-ensemble d'attributs est meilleure que toute les autres, -la décision d'un comité d'experts est généralement meilleure que la décision d'un seul expert. Des techniques performantes (John et al., 1994), (Kira et Rendell, 1992), etc. de sélection de sous-ensembles d'attributs ont été développées mais, il n'existe pas une méthode qui soit meilleure que toutes les autres dans tous les cas.
Nous avons défini un nouvel algorithme de sélection d'attributs qui combine des déci-sions pondérées de plusieurs experts. Plus précisément, étant donné deux ou plusieurs mé-thodes de sélection de sous-ensembles pertinents d'attributs dans un ensemble de données, la question est de savoir comment l'on peut utiliser ces différentes méthodes pour fournir un résultat efficace. Afin de répondre à cette question, nous nous sommes appuyés sur la théorie du consensus qui peut être définie comme un procédé de prise de décision qui utilise entiè-rement les ressources d'un groupe. La théorie du consensus trouve l'une de ses justifications dans le fait qu'une décision prise par un groupe d'experts est meilleure en terme d'erreur quadratique moyenne que la décision d'un seul expert. Une telle démarche possède de nombreux avantages : statistiquement parlant, la consultation de plusieurs expertises lors de la résolution d'un problème est une façon subjective d'accroître la taille de l'échantillon dans une expérience, un ensemble d'experts permet d'obtenir plus d'information qu'un seul expert (Clemen et Winkler, 1999).
La section suivante présente l'algorithme de sélection d'attributs proposé.
autres. Chaque critère possède des attributs de qualité spécifiques. Il est nécessaire de prendre en considération tous les différents attributs de qualité.
Nous avons aussi un sous-ensemble d'attributs
Chaque attribut sélectionné par un sous expert e j a une fréquence d'apparition freq = 1/nb dans la décision finale, où nb est le nombre d'attributs sélectionnés par le sous expert. Nous définissons un critère de préférence d'un attribut (règle de consensus) comme étant le produit des fréquences d'apparition de l'attribut dans les sous-ensembles d'attributs des experts. Nous utilisons la formule ci-dessous pour le calcul de la préférence d'un attribut d. Les poids affectés aux experts doivent à cet effet être proportionnels à leurs décisions. La méthode d'affectation de poids que nous proposons a pour fondements théoriques un principe de la théorie de Gestalt (une vue d'ensemble est meilleure que la somme des parties) et des propriétés pré-attentives de la vision humaine. En ce qui concerne le principe de Gestalt, en visualisant l'ensemble d'éléments intervenant dans une décision, un processus cognitif se met en place. L'application du principe de Gestalt se résume en une représentation graphique multi vues à base de coordonnées parallèles (Inselberg, 1985) des attributs sélectionnés. Les coordonnées parallèles permettent de représenter en 2D des données multidimensionnelles sans perte d'information. Chaque vue représente le point de vue de chaque expert (une des données d'entrée de CTBFS), comme l'indique la figure 1.
FIG. 1 -Outil d'affectation visuelle de poids aux experts intervenant dans CTBFS.
Six experts de type filtre ont servi à la sélection des attributs visualisés dans la figure 1. L'expert 1 représente le critère de sélection consistance, l'expert 2 représente l'entropie de Shannon, l'expert 3 quant à lui utilise la distance comme fonction d'évaluation. La fonction d'évaluation pour l'expert 4 est le gain d'information, le coefficient de Gini pour l'expert 5 et le coefficient de Cramer pour l'expert 6.
Il est à noter que les outils usuels d'affectation de poids sont des « boîtes noires ». L'avantage principal de l'approche ainsi proposée tient au fait que l'utilisateur est impliqué et participe dans le processus de prise de décision. Il existe un ensemble de propriétés visuelles qui sont traitées de manière pré attentive très rapidement, avec précision et sans effort particulier, ce qui permet aux utilisateurs d'affecter des poids convenables aux différents experts.
De plus, les techniques de visualisation permettent d'améliorer la résolution de problè-mes. La visualisation permet de découvrir plus aisément des motifs dans les données, de réduire l'espace de recherche d'information par rapport aux méthodes automatiques, de procéder à des opérations perceptuelles d'inférence et d'augmenter la mémoire et les ressources de traitement de l'utilisateur (Card et al., 1999).
Expérimentations
Pour les besoins d'expérimentation de la technique proposée qui a été développée sous Windows avec Java et le langage R, nous utilisons un pentium IV, 1.7 GHz. Les ensembles de données que nous utilisons ont été référencées par (Blake et Merz, 1998) et (Jinyan et Huiqing, 2002). Pour les besoins de ces expérimentations, les poids affectés aux différents experts ont pour valeur 1.
Le domaine considéré dans le cadre de cette première expérimentation est constitué d'un ensemble M constitué de 3 experts de type filtre et de 3 experts de type enveloppe E = {consistence, entropie de Shannon, distance, (LDA, QDA, Kppv)  (Ripley, 1996)}, le nombre d'attributs susceptibles d'être traités convenablement est C cmd = 20.
Les résultats de l'algorithme proposé (CTBFS) sont comparés à ceux de Las Vegas Filter (Liu et Setiono, 1996), un algorithme de type filtre et StepClass du package KlaR (langage de programmation R), un algorithme de type enveloppe. A cet effet, nous évaluons les performances des ensembles de données pourvus des attributs sélectionnés par ces trois métho-des avec l'algorithme des k plus proches voisins kppv (implémentation de WEKA (Witten et Eibe, 2005)). Nous avons fixé le paramètre K de l'algorithme des kppv à 1.
Les ensembles de données à traiter dans le cadre de cette première expérimentation sont pourvus de nombreux attributs (colonne 2 du tableau 1) qu'il serait impossible de visualiser en une seule fois à l'écran quelque soit la méthode de représentation graphique.
Les résultats exposés dans le tableau 1 permettent d'observer que l'algorithme CTBFS permet de réduire considérablement le nombre d'attributs des ensembles de données comme le montre les résultats de la colonne 3. La colonne 5 de ce tableau quant à elle fait observer que la précision de l'algorithme de kppv est améliorée pour 4 ensembles de données sur 7. Pour les trois autres ensembles de données, on assiste certes à une perte de précision avec un écart maximal de 16.97% avec un minimum de précision de 68.87% mais l'ensemble de données final peut être visualisé et traité de manière interactive, ce qui n'est pas le cas des ensembles de données initiaux comme nous l'avons souligné.
-62 -RNTI-E-6 On observe à travers la colonne 3 du tableau 2 que la méthode LVF permet de sélection-ner un nombre très important d'attributs, qu'il serait impossible de visualiser (par exemple pour les ensembles de données Arrhythmia, Isolet, ColonTumor et CentralNervSyst). Par rapport à la méthode proposée, la précision obtenue pour ces ensembles de données est équi-valente voire supérieure par exemple pour l'ensemble de données Isolet, sachant que l'algorithme CTBFS renvoie au maximum 20 attributs. En ce qui concerne l'algorithme Stepclass, l'ensemble de données Promoter possède aussi un nombre important d'attributs.
En terme de précision, en dehors de l'ensemble de données Promoter pour lequel CTBFS a une précision inférieure à celle de Stepclass et de LVF, la précision obtenue pour les autres ensembles de données avec l'algorithme proposé est au moins égale suivant les cas à celle de LVF ou à celle de Stepclass mais avec un nombre d'attributs qui convient à la fouille visuelle de données.
Conclusion
Nous avons présenté un algorithme basé sur la théorie du consensus et l'affectation visuelle de poids pour la sélection d'attributs significatifs en FVD. En effet, lorsque le nombre d'attributs et/ou le nombre d'observations d'un ensemble de données est important, il s'avère impossible ou alors pénible de représenter graphiquement l'ensemble de données et d'observer des corrélations dans cet ensemble de données.
La technique présentée permet de définir un nombre maximum d'attributs à sélectionner dans l'ensemble de données à traiter, rendant possible la visualisation de ces données. La

1
François Jacquenet, Christine Largeron, Cédric Udréa Laboratoire EURISE -Université Jean Monnet 23 rue du Docteur Michelon -42023 Saint-Etienne Cedex 2 -France {Francois.Jacquenet,Christine.Largeron,Cedric.Udrea}@univ-st-etienne.fr Deux voies sont envisageables pour limiter le nombre de motifs extraits dans un processus de fouille de données. La première s'efforce, lors de la génération des motifs, de ne conserver que les seuls motifs semblant présenter un intérêt immédiat pour l'utilisateur (Boulicaut, 2005), tandis que la seconde voie consiste à stocker tous les motifs extraits par les algorithmes de fouille de données dans des structures de données efficaces et à développer des outils d'interrogation et de manipulation permettant de les traiter (Grossman et al., 1999;Tuzhilin et Liu, 2002;Zaki et al., 2005). C'est en suivant cette démarche que nous nous sommes intéressés à la recherche de règles d'association non redondantes alors que la plupart des travaux antérieurs consacrés à ce problème se sont plutôt attachés à l'extraction de règles non redondantes directement à partir des données (Zaki, 2000;Bastide et al., 2000;Li et Hamilton, 2004;Goethals et al., 2005).
Dans la suite, en nous inspirant d'une définition de (Bastide et al., 2000), nous considérons qu'une règle d'association B ? H est non redondante si et seulement si il n'existe pas de règle de la forme B ? H telle que B ? B et H ? H . Chaque partie de la règle d'association peut être représentée par un vecteur qui possède autant de bits qu'il existe d'items dans la base de transactions (Morzy et Zakrzewicz, 1998). Chaque bit est alors associé à un item particulier et la valeur du bit est de '1' si et seulement si l'item correspondant est présent dans la partie de la règle associée au vecteur de bits.
En utilisant ce codage, nous proposons de déterminer la redondance d'une règle R = B ? H vis-à-vis d'une autre règle R = B ? H , en exploitant la propriété suivante :
k }) le vecteur de bits correspondant à la partie gauche (respectivement droite) de la règle X où IB X i (respectivement IH X i ) est égal à 1 si l'item i est présent dans la partie gauche (respectivement droite) de la règle X, 0 sinon. Nous démontrons alors que la règle R est redondante par rapport à la règle R si et seulement si
dé-signe le nombre de '1' dans IB X , N h (X) le nombre de '1' dans IH X et (R AND R ) désigne la règle ayant en partie gauche l'intersection des parties gauches des règles R et R et en partie droite l'intersection des parties droites des règles R et R . Nous avons développé un algorithme, basé sur cette propriété, et réalisé plusieurs tests pour comparer les temps nécessaires pour extraire les règles non redondantes d'un ensemble 

Introduction
Étant donné l'explosion du volume de données disponibles sur Internet, il devient indispensable de proposer de nouvelles approches pour faciliter l'interrogation de ces grandes masses d'information afin de retrouver les informations souhaitées. L'une des conditions sine qua non pour permettre d'interroger des données hétérogènes est de disposer d'un (ou de plusieurs) "schéma général" que l'utilisateur pourra interroger et à partir duquel les données sources pourront être directement accédées. Malheureusement les utilisateurs ne disposent pas de moyen de connaître les modèles sous-jacents des données qu'ils souhaitent accéder et l'un des challenges dans ce contexte est donc de fournir des outils pour extraire, de manière automatique, ces sché-mas médiateurs. Un schéma médiateur est alors considéré comme une interface permettant à l'utilisateur l'interrogation des sources de données : l'utilisateur pose ses requêtes de manière transparente et n'a pas à tenir compte de l'hétérogénéité et de la répartition des données.
XML étant maintenant prépondérant sur Internet, la recherche de moyens d'intégration de tels schémas est un domaine de recherche actif. Si les recherches permettant l'accès aux données, quand un schéma d'interrogation est connu, sont maintenant bien avancées (Xylème, 2001), les recherches concernant la définition automatique d'un schéma médiateur restent incomplètes et non satisfaisantes (Tranier et al., 2004). Il est alors intéressant de considérer les travaux réalisés dans le contexte de la fouille de données afin d'obtenir un schéma fréquent ou un ensemble de sous-schémas fréquents. Ces derniers offrent alors des éléments pertinents pour la construction du schéma médiateur. Dans le but de proposer une approche permettant de ré-pondre à cette dernière problématique, nous nous focalisons sur la recherche de sous-structures fréquentes au sein d'une base de données de schémas XML. Une sous-structure fréquente est un sous-arbre se trouvant dans "la plupart" des schémas XML considérés. Cette proportion est examinée au sens d'un support qui correspond à un nombre minimal d'arbres de la base dans lesquels le sous-arbre doit se retrouver pour être considéré comme fréquent. Une telle recherche est complexe dans la mesure où il est nécessaire de traduire l'ensemble des sché-mas en une structure aisément manipulable. Cette transformation des données conduit parfois à doubler ou tripler la taille de la base initiale dès lors que l'on souhaite utiliser des propriétés spécifiques permettant d'améliorer le processus de fouille. Il n'existe pas de solution efficace à ce problème alliant une représentation compacte à des propriétés intéressantes. L'objet de cet article est la définition d'une approche de fouille de données de type XML répondant à cet objectif.
Cet article est structuré de la manière suivante : la section 2 introduit les définitions des différentes inclusions dans le contexte des structures hiérarchiques et propose un aperçu des principales approches existantes de fouille de données arborescente. Nous présentons égale-ment en détail la problématique étudiée. La section 3 présente notre proposition : une méthode de recherche de sous-schémas fréquents utilisant les propriétés d'une structure de données arborescentes compacte et originale. Les différentes expérimentations menées sur des bases de schémas XML sont décrites dans la section 4. Enfin, la section 5 conclut et présente les principales perspectives associées à nos travaux.
Définitions, problématique et travaux connexes
Définitions préliminaires
Un arbre est un graphe orienté, connexe sans cycle. Il est composé d'un ensemble de noeuds reliés par des arcs et il existe un noeud particulier nommé racine. Il s'agit d'un arbre ordonné s'il existe un ordre entre les fils d'un noeud et d'un arbre non ordonné sinon. 
Définition 1 Un arbre enraciné, étiqueté et ordonné
De plus, dans la suite de cet article, nous utilisons le mot arbre pour un un arbre enraciné, étiqueté et ordonné. 
3. ? préserve les relations :
Une inclusion est dite induite si les relations de parenté sont préservées. Par ailleurs, si les relations d'ancestralité sont respectées, il s'agit d'une inclusion incrustée. Par exemple, considérons les arbres S, T 1 , T 2 , et T 3 représentés dans la figure 1. Si les relations de parenté sont respectées, il s'agit d'une inclusion induite, donc S est inclus de manière induite dans l'arbre T 1 (S T 1 ). Si les relations d'ancestralité sont conservées, alors on trouve une inclusion incrustée avec S T 1 et S T 3 . S n'est pas inclu dans T 2 car ? ne préserve pas l'ordre entre les frères.
Dans la suite de cet article, nous considérons une inclusion de type induite car nous souhaitons traiter l'ordre existant entre les noeuds dans la hiérarchie de façon directe et l'ordre entre les noeuds de même niveau de façon indirecte. Nous pouvons donc définir le support d'un sous-arbre selon cette inclusion de la manière suivante :
Problématique
La problématique étudiée au sein de cet article est la recherche de sous-structures fré-quentes, i.e. de sous arbres qui apparaissent suffisamment fréquemment dans des documents XML. Nous considérons, par la suite, qu'une étape initiale de pré-traitement est réalisée sur les documents XML de manière à ne retenir que leur structure sous forme d'arbre. Nous considérons également qu'à l'issue de cette phase, l'étiquetage des noeuds est homogène, i.e deux noeuds de même étiquette dans deux arbres différents partagent non seulement la même syntaxe mais également la même sémantique.
L'objectif consiste alors à rechercher, à partir de la forêt d'arbres obtenue D et en fonction d'un support minimal spécifié par l'utilisateur, les sous arbres qui apparaissent suffisamment fréquemment, i.e. dont leur nombre d'occurrences dans D est supérieur ou égal au support minimal. Pour répondre à cette problématique, nous nous trouvons donc confrontés aux deux problèmes suivants :
1. Quelle structure de représentation efficace utiliser ? Idéalement, étant donné que nous considérons de grandes quantités d'arbres, nous souhaitons avoir une structure qui non seulement soit efficace en mémoire mais également adaptée aux traitements que nous souhaitons faire.
Comment tester efficacement l'inclusion d'un arbre dans un sous arbre ?
Rechercher l'ensemble des sous arbres fréquents nécessite de parcourir tous les arbres et d'effectuer de très nombreuses comparaisons pour réussir à extraire des sous parties communes. Il est donc indispensable de pouvoir trouver rapidement à partir de quel noeud la comparaison peut être effectuée si nous souhaitons améliorer l'efficacité de la recherche.
Les travaux existants
Dans cette partie, nous nous intéressons non seulement aux approches de recherche mais nous examinons également les méthodes de représentation des arbres. Les travaux dans le domaine de la fouille de données arborescentes peuvent être distingués selon qu'ils traitent les arbres ordonnés ou non. Nous situant dans le contexte de schémas XML, il s'avère nécessaire de traiter l'ordre des éléments si celui-ci est spécifié. Nous nous focaliserons donc sur des propositions prenant en charge les arbres ordonnés.
A notre connaissance, il existe très peu de travaux proposant des méthodes d'extraction pour les arbres ordonnés (Zaki, 2002;Asai et al., 2002). Ainsi Zaki (2002) propose l'algorithme TreeMiner pour extraire des sous-arbres fréquents selon une inclusion incrustée. Une représentation originale des arbres facilite la gestion des candidats et offre des performances intéressantes. (Asai et al., 2002) traitent également de la problématique des arbres ordonnés selon la définition de l'inclusion induite. L'approche proposée, FREQT, adopte une structure de représentation du type « first-child/next-sibling » comme illustrée figure 2. Lors du processus de fouille, pour chaque structure fréquente, FREQT conserve la liste des noeuds les plus à droite dans les arbres de la base de données supportant cette structure. Nous illustrons ceci figure 3 où pour le fréquent a, les 6 positions dans la base de données sont stockées, et pour le fréquent c ? a les 3 positions les plus à droite sont stockées. Cette information représente les positions où cette structure est supportée dans la base.
Si nous examinons plus attentivement la représentation verticale adoptée dans TreeMiner, elle aboutit en fait à stocker trois fois la taille d'un arbre, i.e. 3|T |. De la même manière la structure utilisée dans FREQT offre des performances attractives, mais cette représentation conduit également à tripler la taille de la base afin de stocker les informations nécessaires.
Même si elles n'abordent pas la même problématique, des approches de représentation efficaces des arbres en 2|T | ont été récemment proposées (Wang et al., 2004;Chi et al., 2004Chi et al., , 2003. Cependant, outre le fait qu'elles ne considèrent pas la notion d'ordre, elles n'utilisent pas des propriétés aussi intéressantes que les travaux précédents afin d'améliorer le processus d'extraction.
Notre objectif est donc de permettre une recherche de sous-arbres ordonnés mais, contrairement aux approches existantes dans ce contexte, d'utiliser une représentation peu coûteuse en mémoire, i.e. en 2|T |. Cette structure doit en outre posséder des propriétés intéressantes  pour améliorer le processus d'extraction. C'est dans ce contexte que se situe notre proposition RSF décrite à la section 3.
Proposition
Dans cette section, nous proposons de nouveaux algorithmes permettant l'extraction efficace de sous-arbres fréquents ordonnés au sein d'une base de données arborescentes. Dans un premier temps, nous décrivons la structure adoptée et nous en soulignons ses intérêts. Dans un second temps, nous proposons un survol de notre approche d'extraction et nous montrons comment les propriétés de la structure sont utilisées pour améliorer le processus de fouille. Finalement, nous décrivons plus formellement les algorithmes proposés.
Pour illustrer nos propos, nous utiliserons la base d'arbres de la figure 4.
Représentation des arbres
Pour représenter les différents arbres manipulés au sein de notre approche, nous adoptons la représentation proposée dans (Del Razo et al., 2005). Un arbre est ainsi décrit à l'aide de deux vecteurs comme proposé dans Weiss (1998) Cette représentation permet de retrouver en temps constant le père d'un noeud. De plus, elle permet la localisation directe de la feuille la plus à droite par rapport à l'index k. En parcourant l'arbre, il est ainsi possible d'obtenir toutes les relations directes père-fils entre noeuds. Le deuxième vecteur, nommé lb, est utilisé pour enregistrer les étiquettes de l'arbre avec lb[i], i = 0, 2, ..., k ? 1 représentant l'étiquette de chaque noeud n i ? T .
La structure adoptée permet une représentation des arbres peu coûteuse puisqu'elle se ré-duit à 2|T |. De plus elle possède des propriétés intéressantes, évoquées au paragraphe suivant, pouvant être utilisées lors de la recherche de sous-structures fréquentes.
Aperçu général
Notre proposition est basée sur une approche classique de type « générer-élaguer », i.e. à chaque étape, nous générons différents candidats et nous testons si ceux-ci sont inclus dans la bases d'arbres. L'inclusion dans notre cas est bien entendu définie comme étant de type « induit ».
La méthode de représentation des arbres que nous proposons permet de générer de manière efficace les sous-arbres candidats puis d'élaguer les sous-arbres non fréquents (après calcul du support). Les candidats de taille 2 sont générés en combinant deux à deux tous les fréquents de taille 1. La génération des candidats de taille k ? 3 s'effectue de la même manière que dans les approches classiques de type Apriori (Agrawal et Srikant, 1994), par combinaison des fréquents de taille k ? 1. Nous adoptons la stratégie de génération de candidats selon la branche la plus à droite comme proposée dans (Asai et al., 2002;Zaki, 2002) et illustrée figure 7. Nous pouvons ainsi constater l'intérêt de notre structure de représentation puisque, naturellement, il suffit d'ajouter un nouvel élément dans la représentation de l'arbre en spécifiant le père du nouveau noeud.
Le calcul du support de chaque candidat consiste à compter le nombre d'arbres de la base qui contiennent ce sous-arbre candidat. Ainsi pour chaque arbre de la base, nous recherchons les points d'ancrage sur lesquels la racine du sous-arbre à tester peut s'instancier. Ces points correspondent en fait aux noeuds dans l'arbre qui correspondent à la racine de l'arbre à tester. Pour chaque point d'ancrage trouvé, on cherche alors à instancier l'ensemble des noeuds de l'arbre candidat au sein de l'arbre courant testé, i.e. les fils du noeud à tester. Notons que dans le cas d'une inclusion induite, nous recherchons une instanciation exacte du candidat au sein des arbres de la base. Si tous les noeuds du candidat ont été trouvés, l'arbre supporte le candidat et le support de la structure candidate est alors incrémenté.
Les algorithmes
L'algorithme RFS (Algorithme 1) fonctionne de la manière suivante : un premier parcours sur la base est réalisé pour extraire les items dont le nombre d'occurrences est supérieur au support minimal. Ces items constituent des arbres résumés à une seule racine, l'item considéré. Nous obtenons ainsi l'ensemble F 1 des arbres fréquents de taille 1. Ces derniers sont combinés entre eux pour former des candidats de taille 2 et un parcours sur la base permet d'obtenir l'ensemble F 2 constitué des arbres de taille 2. L'algorithme se poursuit en générant des candidats de taille k+1 et en effectuant un parcours sur la base pour compter le nombre d'occurrences de chaque candidats. Lorsque plus aucun candidat ne peut être généré l'algorithme se termine.
L'algorithme GenCandidats(F k?1 ) (Algorithme 2) décrit la génération des candidats qui utilise la branche la plus à droite des sous-arbres fréquents de taille k ? 1 afin de proposer des candidats de taille k. Pour chaque arbre fréquent de taille k ? 1, il génère un nouveau candidat en étendant l'arbre par la branche la plus à droite. Cette génération est obtenue par l'intermédiaire de la fonction Bpd. Ainsi, pour chaque noeud, nous lui ajoutons les seules extensions possibles, i.e. celles qui s'avèrent fréquentes dans F 2 .
Entrée : D = {T 1 ,T 2 ,..,T n } base de données d'arbres ; ? le support minimal. Sortie : F sous-arbres fréquents.
Entrée : F k?1 des (k ?1)-sous-arbres fréquents. Sortie : C k des (k)-sous-arbres candidats.
F 1 ? arbres fréquents de taille 1; F 2 ? arbres fréquents de taille 2;
Algorithme 2: GenCandidats(F k?1 ).
Entrée : C candidat , T un arbre, i index de la racine de l'ancrage. Sortie : vrai si T supporte C.
si trv alors + + cnt;
Algorithme 3: Support(C).
Algorithme 4: Ancre(C, T, i).
Le calcul du support de chaque candidat consiste à compter le nombre d'arbres de la base qui contiennent ce sous-arbre candidat.
Pour chaque arbre de la base, une recherche est effectuée pour voir s'il existe des points d'ancrage sur lesquels la racine du sous-arbre à tester peut s'instancier (appel à l'algorithme Ancre). Si un sous-arbre existe son nombre d'occurrences est alors incrémenté et son support est retourné.
Considérons l'algorithme de gestion des points d'ancrage (Algorithme 4). Pour chaque point d'ancrage trouvé, i.e. pour chaque noeud du sous arbre candidat c qui possède le même label dans l'arbre T , on cherche à instancier l'ensemble des noeuds de l'arbre candidat au sein de l'arbre couramment testé T . En d'autres termes, nous souhaitons projeter le sous-arbre candidat c dans l'arbre T . Ceci est réalisé par l'intermédiaire des algorithmes Ancre et Poursuit (cf. algorithmes 4 et 5).
L'algorithme Poursuit est utilisé pour chercher une instanciation exacte du candidat au sein des arbres de la base. Si tous les noeuds du candidat ont été trouvés, l'algorithme retourne alors la valeur V RAI (l'arbre supporte le candidat). Il retourne la valeur F AU X si tous les noeuds de l'arbre ont été parcourus sans trouver l'ensemble des noeuds du candidat.
Algorithme : Poursuit(N niv_act , T, nbnoeuds)) Entrée : N niv_act ensemble de noeuds à trouver ; T l'arbre ; nbnoeuds le nombre de noeuds vérifiés. Sortie : vrai si tous les noeuds de N niv_act ont été trouvés.
si (nbnoeuds= |N niv_act |) alors retourner vrai; sinon retourner faux;
Algorithme 5: Poursuite de la recherche.
Expérimentations
Nos expérimentations ont été réalisées avec un PC Pentium ayant 512 Mo RAM sous le système Linux 2.4. Les programmes ont été développés en C++ et compilés avec gcc 3.2.2.
Nous avons utilisé 6 bases de données construites en employant le programme de généra-tion d'arbres XML proposé par (Termier et al., 2002) Nous avons souhaité évaluer notre proposition selon deux aspects : temps de réponse et occupation mémoire. En effet, nous argumentons notre proposition comme étant plus efficace pour un réel passage à l'échelle mais ceci n'est pas toujours synonyme d'efficacité en temps de réponse. En fait, les expérimentations réalisées prouvent que notre proposition répond aux deux critères.
Pour évaluer les performances sur les temps d'exécution, nous nous sommes comparés à l'algorithme FREQT-nodd sans détection des duplicats de (Asai et al., 2002) permettant de rechercher des inclusions induites puis à une optimisation de celui-ci FREQT-dd limitant le parcours dans les arbres lors de la vérification des candidats.
La figure 8-(a) représente l'occupation mémoire utilisée pour la représentation de la base de schémas XML. Comme nous nous y attendions RSF occupe moins d'espace mémoire puisqu'il adopte une structure de représentation plus réduite que FREQT-nodd et FREQT-dd. Ces deux derniers adoptent la même structure.
Les Nous souhaitons à présent mettre en oeuvre une optimisation de parcours de la structure proposée afin d'améliorer les performances en terme de temps d'exécution. Une telle optimisation est tout à fait réalisable et constitue l'une de nos perspectives principales. Nous devrions alors obtenir des performances supérieures à celles obtenues pour FREQT-dd tout en conservant une structure en 2|T |. 
Conclusion et perspectives
Dans cet article, nous proposons une approche efficace d'extraction de sous-arbres fré-quents. RSF est la première proposition de recherche de sous-arbres fréquents selon une inclusion induite à l'aide d'une représentation de la base de schémas en 2|T |. Les premières expérimentations réalisées sur des données synthétiques soulignent l'intérêt de notre proposition par rapport aux approches de référence. Les perspectives immédiates concernant RSF suivent deux axes :
-Tout d'abord, il est possible d'améliorer l'algorithme en optimisant les parcours réalisés lors de la vérification des candidats comme proposé dans l'optimisation de FREQT (Asai et al., 2002). Toutefois, nous souhaitons mettre en place un procédé moins coûteux en terme d'espace mémoire. -Ensuite, nous souhaitons utiliser la même structure de représentation des arbres pour réaliser une recherche de sous-arbres fréquents en se basant sur une inclusion incrustée. Ces travaux ont pour objectif d'être utilisés dans le cadre de la médiation de données, les sous-arbres fréquents extraits servant de support à la construction automatique d'un schéma médiateur. Une telle solution peut également être adoptée dans le cadre de la fouille de données en ligne (data streams) pour le traitement à la volée de données XML. Cette perspective permettra de traiter les gros volumes de données transitant sur Internet de manière efficace et rapide.

Introduction
Les progrès techniques récents ont eu pour conséquence l'augmentation du nombre de flux d'information et la croissance rapide de leurs débits. L'architecture traditionnelle de l'analyse de données -où les données, préalablement stockées, sont analysées puis rafraîchies -étant inadaptée au traitement de ces flux, une nouvelle famille de techniques, dites de stream mining, se propose d'inverser radicalement cette architecture et de mettre en oeuvre des systèmes reposant sur des capacités de stockage minimales qui sont mises à jour à la vitesse du flux. L'objectif de cet article est d'expliquer comment nous avons utilisé des techniques de stream mining afin d'identifier en temps réel, dans un réseau IP, les préfixes dont la contribution au trafic dépasse une certaine proportion de ce trafic pendant un intervalle de temps donné.
considéré. Cependant, les concepts et les algorithmes que nous allons présenter ici peuvent naturellement s'appliquer à tout flux de données de la forme précédente 1 . Classiquement, sur ce type de flux de données, on définit le compte a i (? ) d'un identifiant i ? U à l'instant ? par a i (? ) = ? t=0 c t ? i,it , où ? i,it vaut 1 si i = i t et 0 sinon. Dans l'exemple ci-dessus, le compte a i (? ) d'une adresse IP i représente le nombre total d'octets à l'instant ? qui ont été envoyés vers l'adresse i et qui ont transité par le point P considéré. On peut alors s'intéresser aux objets massifs (heavy hitters) du flux à l'instant ? , c'est-à-dire, aux identifiants i dont le compte a i (? ) est supérieur ou égal à une fraction ?N (? ) du compte total N (? ) = ? t=0 c t du flux. Dans le cas particulier où l'ensemble U des identifiants étudié peut être organisé de façon hiérarchique, il peut également être intéressant d'effectuer une recherche d'objets massifs au sein de cette hiérarchie 2 . Ainsi, dans l'exemple ci-dessus, on peut regrouper les adresses IP par préfixe 3 , puis associer à un préfixe donné la somme des comptes des adresses IP commençant par ce préfixe et finalement rechercher à un instant donné tous les préfixes dont le compte est supérieur ou égal à une fraction donnée du compte total du flux. L'inconvénient de cette approche est que si un préfixe donné est un objet massif, alors tous les préfixes contenus dans ce préfixe (autrement dit tous les préfixes qui sont des ancêtres du préfixe considéré) seront aussi des objets massifs, alors qu'il est parfois souhaitable de ne plus tenir compte de la contribution de cet objet massif lorsque l'on recherche des objets massifs parmi les préfixes plus courts. C'est pourquoi la notion d'objet massif hiérarchique (hierarchical heavy hitters) a été introduite (Cormode et al., 2003). Les objets massifs hiérarchiques d'un flux dont les identifiants appartiennent à une hiérarchie sont définis de façon récursive : les objets massifs hiérarchiques de niveau 0 (le niveau le plus bas de la hiérarchie) sont les objets massifs du flux ; les objets massifs hiérarchiques de niveau l > 0 sont les sommets de la hiérarchie de niveau l dont la somme des comptes des identifiants qui sont leurs descendants et qui n'appartiennent pas à des objets massifs hiérarchiques de niveau inférieur à l, est supérieure ou égale à ?N (? ). Pour une présentation plus détaillée de la notion d'objet massif hiérarchique, on pourra consulter Cormode et al. (2003).
L'algorithme de recherche d'objet massif hiérarchique de Cormode et al.
laquelle on peut obtenir, pour tout instant ? , une estimationâestimationˆestimationâ S (? ) du compte a S (? ) de chaque sommet S de niveau l de la hiérarchie 4 . Pour obtenir la liste (estimée) des objets massifs hié-rarchiques à un instant ? , on explore récursivement la hiérarchie en commençant par le haut de la hiérarchie (cf. Cormode et al. (2003, § 4) pour plus de détails). La liste estimée obtenue coïncide avec la liste exacte lorsque toutes les comptes estimésâestimésˆestimésâ S (? ) utilisés lors de l'exploration de la hiérarchie coïncident avec le compte exact a S (? ). Dans Cormode et al. (2003), cette estimation est effectuée à l'aide de l'algorithme Random Subset Sums (RSS) introduit dans Gilbert et al. (2002). L'algorithme RSS est un algorithme de Monte Carlo qui garantit que l'on a P {|â S (? ) ? a S (? )| ?N (? )} 1 ? ?, où ? et ? sont des réels positifs inférieurs à 1, en utilisant un espace mémoire en O( 
L'algorithme que nous avons mis en oeuvre
Récemment, Cormode et Muthukrishnan (2005) ont proposé un autre type de sketch, le Count-Min Sketch (CMS). Comme l'algorithme RSS, l'algorithme CMS permet d'obtenir une estimationâestimationˆestimationâ S (? ) du compte a S (? ) d'un sommet S à l'instant ? . Cependant, l'algorithme CMS a pour avantage de garantir que l'on a ˆ a S (? ) a S (? ) et P {â S (? ) a S (? ) + ?N (? )} 1 ? ? en utilisant un espace mémoire en seulement O( 1 ? ln(1/?)) mots pour le stockage d'un sketch et un nombre d'opérations en seulement O(ln(1/?)) pour la mise à jour d'un sketch, ainsi que pour le calcul de l'estimateur du compte d'un sommet. C'est pourquoi, comme suggéré par Cormode et Muthukrishnan (2005), nous avons utilisé l'algorithme CMS à la place de l'algorithme RSS pour estimer le compte de chaque sommet. Nous obtenons ainsi un algorithme de recherche des objets massifs hiérarchiques qui nécessite un espace mémoire en O( h ? ln(1/?)) mots pour le stockage de l'ensemble des sketches, un nombre d'opérations en O(h ln(1/?)) pour la mise à jour de l'ensemble des sketches et un nombre d'opérations en O( hq ? ln(1/?)) (où q est le nombre maximum d'enfants de chaque sommet de la hiérarchie) pour la recherche des objets massifs hiérarchiques. La réduction substantielle du nombre d'opérations nécessaire pour chaque mise à jour 5 est particulièrement appréciable. En effet, si l'on souhaite traiter le flux de données en temps réel, il est indispensable d'effectuer cette mise à jour à la cadence à laquelle on reçoit les éléments du flux ; par conséquent, en ayant nettement moins d'opérations à effectuer par mise à jour, on pourra, à puissance de calcul identique, traiter en temps réel des flux de données arrivant à une cadence bien plus élevée.
Application au cas d'un réseau IP
L'algorithme présenté au § 2.3 a été appliqué à des données réelles provenant d'un routeur Cisco installé sur un réseau IP de France Télécom. Pour des raisons pratiques, l'analyse a été effectuée sur les adresses IP des flots de données enregistrés par la sonde Netflow qui a été activée sur ce routeur (Sommer et Feldmann, 2002). La trace dont nous présentons ici l'analyse corrrespond à une vingtaine de minutes de trafic, 3,6 millions de paquets IP et 2 Go de volume. Notre analyse a été réalisée sur les adresses sources, toutes destinations confondues, pour un seuil ? = 10 ?2 que nous avons jugé représentatif des seuils choisis par les utilisateurs 6 . La hiérarchie utilisée pour l'analyse avait pour hauteur h = 32. Nous avons tracé le taux de faux négatifs 7 et le taux de faux positifs 8 en fonction de la précision ?, pour différentes valeurs de la probabilité d'échec ?. A titre d'exemple, nous avons représenté sur les figures 1 et 2 les courbes obtenues pour une probabilité d'échec ? = 10 ?2 9 . Nous avons également étudié la quantité de mémoire utilisée par l'algorithme décrit au § 2.3 en fonction des paramètres ? et ? 10 . Les résultats que nous avons obtenus montrent que, si l'on choisit judicieusement les paramètres ? et ?, l'algorithme présenté au § 2.3 permet d'obtenir, sur la trace étudiée, un taux de faux positifs et de faux négatifs négligeable tout en nécessitant une quantité de mémoire raisonnable en pratique : par exemple, en prenant ? = 10 ?2 et ? = 10 ?4 , on obtient un taux de faux positifs et de faux négatifs inférieur à 1% en utilisant environ 2 millions de mots.
Conclusion
Comme suggéré par Cormode et Muthukrishnan (2005), nous avons utilisé l'algorithme CMS à la place de l'algorithme RSS afin de rechercher les objets massifs hiérarchiques à l'aide de la méthode présentée par Cormode et al. (2003, § 4). Cependant, alors que Cormode et Muthukrishnan (2005) ne préconisent cette solution que dans le cas de flux de données dont la marque c t peut être négative 11 , nous avons délibérément appliqué cet algorithme au cas de flux de données dont la marque c t est obligatoirement positive ou nulle. En effet, les méthodes proposées par Cormode et al. (2003, § 3) pour rechercher les objets massifs hiérarchiques dans des flux de données dont la marque c t est obligatoirement positive ou nulle présentent pour nous les deux inconvénients suivants : d'une part ces méthodes nécessitent une quantité de mémoire qui (pour une précision donnée) augmente avec le compte total N (? ) 12 et d'autre part ces méthodes ont été conçues uniquement pour rechercher les objets massifs hiérarchiques dans des flux de données dont la marque c t est obligatoirement positive ou nulle ; leur adaptation au cas de flux de données dont la marque c t peut être négative nous paraît très difficile. La solution que nous avons mise en oeuvre ne présente pas ces inconvénients. Dans notre cas, le nombre de mots nécessaire en mémoire reste constant avec le temps et en particulier ne dépend pas du compte total N (? ) : il n'est pas nécessaire d'estimer finement à l'avance le volume de trafic qui sera observé ni la durée totale pendant laquelle l'analyse sera effectuée 13 . De plus, comme la mémoire nécessaire aux méthodes de Cormode et al. (2003, §  6 , qui est rapidement atteint en pratique dans l'application que nous avons étudiée. Par conséquent, la solution que nous avons mise en oeuvre est bien pertinente et nous permet de mieux exploiter la mémoire disponible que les méthodes proposées par Cormode et al. (2003, § 3). D'autre part, l'adaptation de la solution que nous avons mise en oeuvre au cas de flux de données dont la marque c t peut être négative est extrêmement simple : il suffit pour cela de modifier l'estimation du compte de chaque préfixe en remplaçant la minimisation qui intervient dans cette estimation par une recherche de médiane (Cormode et Muthukrishnan, 2005). La solution que nous avons mise en oeuvre possède donc une grande flexibilité qui nous permettra de réutiliser plus facilement les programmes déjà écrits lorsque nous étudierons des flux de données dont la marque c t peut être négative. sous-utilisé la mémoire disponible si le trafic est plus faible que prévu durant la période étudiée. 13 Il faut simplement s'assurer au préalable que la taille des mots utilisée reste suffisante pour toutes les situations envisageables.
-57 -RNTI-E-6
Recherche de préfixes IP massifs Les auteurs remercient Guillaume Picard pour ses commentaires très pertinents sur une version préliminaire de cet article.

Introduction
Les patients hospitalisés en unités de réanimation sont soumis à une surveillance étroite de la part du personnel soignant. Un grand nombre de variables physiologiques sont enregistrées en ligne à des fréquences élevées (une mesure par seconde) sur ces patients. Ces enregistrements produisent des flots de données temporelles importants, que le personnel soignant doit analyser à chaque visite au patient. Les services de réanimation sont en demande d'outils d'aide à l'interprétation de ce flot de données, afin de limiter la charge cognitive que leur interprétation représente (Calvelo et al.,99,Lowe et al.,01,Hunter and McIntosh,99).
Afin d'aider le médecin dans sa tâche d'analyse des données, nous avons développé une méthode d'extraction en ligne d'épisodes temporels permettant de transformer une série temporelle univariée en une succession d'intervalles décrivant l'évolution de la variable. L'information fournie par la méthode est de la forme suivante : « la variable est stable depuis l'instant t1 jusqu'à l'instant t2, à la valeur v1. Elle est croissante de l'instant t2 à l'instant t3 de la valeur v1 à la valeur v2 … ». L'information fournie sur la tendance du signal {stable, croissant, décroissant} correspond au vocabulaire utilisé par les médecins pour décrire l'évolution d'un e physiologique. La méthode d'extraction d'épisodes se règle à partir de 3 paramètres de réglages dont les valeurs dépendent des variables physiologiques traitées, mais sont indépendants du patient ou de l'enregistrement, l'hypothèse sous-jacente étant que le bruit s'ajoutant sur une variable biologique ne dépend pas du patient mais de la variable monitorée. Or, dans la pratique, cette hypothèse n'est pas toujours vérifiée. La variance des variables monitorés peut changer, suivant l'état physiologique du patient ou le contexte des soins. Par exemple, la variance des variables respiratoires (ex: la fréquence respiratoire) sera très différente suivant que le patient est en ventilation spontané ou en mode de ventilation contrôlé par le ventilateur. Les informations sur la tendance à extraire ne sont alors plus les mêmes. Une petite modification de la fréquence respiratoire pourra être porteuse d'information (modification de l'état du patient) si le patient est en mode de ventilation débit contrôlée, alors qu'une modification du même ordre ne sera pas significative pour un patient en ventilation spontanée. Actuellement, pour que la méthode d'extraction d'épisodes temporels fonctionne de façon optimale, il faut préciser le mode de ventilation du patient. Le réglage d'un des paramètres de la méthode sera alors automatiquement divisé par 2. L'objectif de ce papier est de présenter la version adaptative de la méthode d'extraction d'épisodes et son utilisation pour reconnaître de manière automatique des aspirations trachéales. Les résultats obtenus par cette méthode sur des variables enregistrées sur des patients dans des contextes de soins différents seront présentés. Dans un premier temps, nous rappellerons le principe de la méthode d'extraction d'épisodes temporels, puis nous présente-rons les modifications apportées pour rendre la méthode adaptative. Nous présenterons ensuite quelques résultats obtenus en utilisant cette nouvelle méthode pour reconnaître des évènements particuliers à partir des signaux : les aspirations trachéales.
Extraction d'épisodes temporels en ligne
La méthode d'extraction d'épisodes temporels se décompose en quatre phases successives (Charbonnier, 2005) : décomposition en ligne des données en segments de droites, classification des segments en 7 formes temporelles, transformation des formes en épisodes temporels semi-qualitatives, agrégation de l'épisode temporel courant avec les précédents
Décomposition en segments de droite
Un algorithme de segmentation consiste à décomposer en ligne les données en une succession de segments de droite (Charbonnier 2005). La technique utilisée pour déterminer l'instant où l'algorithme doit recalculer une nouvelle approximation linéaire est celle de la CUSUM, qui correspond à une intégration numérique des différences entre le signal et l'extrapolation par la dernière droite calculée. La valeur absolue de la cusum est comparée à chaque période d'échantillonnage à 2 seuils appelé th 1 et th 2 . Si la CUSUM est inférieure à th 1 , l'approximation linéaire est acceptable. Si la CUSUM est supérieure à th 1 , la valeur du signal et le temps correspondant seront stockés dans une liste appelée : Liste des valeurs anormales. Si la CUSUM est supérieure à th 2 , l'approximation linéaire n'est plus acceptable et une nouvelle approximation linéaire sera calculée par la méthode des moindres carrés, à partir des données stockées dans la liste des valeurs anormales, si la taille de la liste est supé-rieure à 3. Dès qu'une nouvelle approximation linéaire a été calculée, la cusum est remise à zéro. La décomposition en segments de droite est essentiellement réglée par le seuil th 2 . Si th 2 est petit, les segments seront recalculés fréquemment et des petites oscillations dans le signal seront segmentées, alors que si th 2 est grand, les segments seront plus longs et certains phénomènes transitoires seront filtrés.
Classification en formes temporelles
Chaque nouveau segment calculé par l'algorithme de segmentation, associé au segment précédent constitue une forme qui peut être classée dans l'une des 7 catégories retenues : stable, croissant, décroissant, échelon positif ou négatif, échelon positif, transitoire croissantdécroissant ou décroissant-croissant.
La classification est effectuée en calculant des caractéristiques sur les segments : diffé-rence entre la fin du segment précédent et la fin du segment courant, différence entre la fin du segment précédent et le début du segment courant, différence entre le début du segment courant et la fin du segment courant .
Un arbre de décision permet le classement en 7 formes, à partir d'un seuil de réglage th c . Ce seuil correspond à la valeur à partir de laquelle une variation sur la variable sera considé-rée significative. Si la valeur d'une des 3 caractéristiques dépasse le seuil fixé, th c , la forme temporelle est du type croissant ou décroissant. Sinon, la forme est stable. Si le seuil th c est grand, les petites variations sont considérées comme non significatives et n'apparaissent pas dans les épisodes temporels extraits. Seules les grandes variations sont exprimées sous forme d'épisodes.
3. Les formes temporelles sont ensuite découpées en 1 ou 2 épisodes temporels élémen-taires, définis par trois grandeurs {stable, croissant, décroissant} 4. Les épisodes temporels courants sont ensuite agrégés avec les épisodes précédents.
La méthode est donc réglé par 3 paramètres th 1 et th 2 qui déterminent la segmentation du signal et th c qui fixe le niveau de variation à partir duquel une forme est considérée comme croissante (ou décroissante) plutôt que stable. Dans la version non adaptative de l'algorithme, ces seuils gardent une valeur fixe qui dépend de la variable à traiter et, pour certaines variables respiratoires, du mode de ventilation.
Adaptation des seuils de réglages de l'algorithme d'extraction d'épisodes
La variance des variables physiologiques peut varier en fonction du contexte de soins dé-livrés au patient. Afin de prendre en compte ce changement de variance sur les variables, nous avons développé une version adaptative de la méthode d'extraction de tendance. Les seuils th 1 , th 2 et th s ne sont plus fixes et réglés en fonction du mode de ventilation, mais s'adaptent en ligne en fonction de la variance estimée du signal.
Estimation de la variance du signal
L'estimation de la variance du signal s'effectue à partir des résidus. On appelle résidus la différence, à chaque période d'échantillonnage, entre le signal et l'approximation linéaire calculé par l'algorithme de segmentation. Cette différence (résidu) correspond à la partie du signal qui n'est pas expliquée par l'approximation linéaire. La variance des résidus est alors estimée à chaque période d'échantillonnage sur une fenêtre glissante de 60 secondes et la médiane de la variance obtenues sur les x dernières secondes est calculée. Nous noterons cette valeur Mx. Nous avons choisi une fenêtre glissante de 60 secondes, car c'est une durée suffisamment courte pour que l'hypothèse de stationnarité du signal reste plausible et suffisamment longue pour faire une estimation peu biaisée de la variance. Le calcul final de la variance se fait par valeur médiane sur une fenêtre d'apprentissage de x secondes, ce qui permet de filtrer les augmentations de variance dues à des artéfacts sur le signal. La taille de la fenêtre, x, a été choisi égale à 300 secondes. C'est un compromis entre la durée de l'information passée à prendre en compte et la sensibilité de la mesure Mx à des artéfacts sur le signal, qui sont d'autant mieux filtrer que la période d'apprentissage est longue.
Algorithme d'adaptation des seuils
A chaque nouvelle segmentation, de nouvelles valeurs sont affectées aux seuils th 1 , th 2 et th s , en fonction de la valeur de Mx. La fonction d'adaptation que nous avons choisi est un cycle d'hystérésis. 2 jeux de seuils { th 1 , th 2 , th s } important et { th 1 , th 2 , th s } faible sont applicables à l'algorithme. Ils correspondent aux jeux optimaux de réglage quand la variance du signal est faible et quand elle est plus importante. { th 1 , th 2 , th s } important correspondent aux valeurs de seuils proposés dans (Charbonnier, 2005). { th 1 , th 2 , th s } faible correspondent aux valeurs de { th 1 , th 2 , th s } important divisé par 2. L'utilisation d'un cycle d'hystérésis permet de limiter des alternances répétées entre les 2 jeux de réglages, quand la mesure de Mx est proche du seuil de commutation.
Choix des seuils de commutation
Afin de ne pas augmenter trop fortement le nombre de seuils à régler sur l'algorithme, les seuils de commutation (Com1 et Com2) du cycle d'hystérésis ont été normalisés pour chaque variable physiologique, à partir de la fonction de répartition de la variance des différentes variables. La fonction de répartition de la variance des différentes variables physiologiques a été estimée à partir d'une ensemble d'enregistrements obtenus sur une vingtaine de patients différents. La variance du signal a été estimée à chaque période d'échantillonnage en utilisant une fenêtre glissante de 60 secondes sur l'ensemble de ces enregistrements. L'ensemble des mesures de variance calculées sur chaque enregistrement et sur une variable a été mis en commun et un tirage aléatoire a été effectué. Un sous-ensemble constitué du quart des valeurs de la base de données a été réalisé et a servi à estimer la fonction de réparti-tion de la variance pour chaque variable physiologique. La valeur correspondante au 95 ème percentile a été relevée pour chaque variable. Les seuils de commutation du cycle d'hystérésis ont été choisis arbitrairement à 15% et 25% de la valeur du 95 ème percentile pour chaque variable physiologique. Le jeu {th1, th2 et ths} faible est appliqué à l'initialisation de l'algorithme et est maintenu tant que la valeur de Mx ne dépasse pas 25% du 95 ème percentile. Quand la commutation vers le jeu {th1, th2 et ths} important a eu lieu, le jeu sera maintenu jusqu'à ce que la valeur de Mx devienne inférieure à 15% du 95 ème percentile.
Résultats
Nous avons appliquée la méthode adaptative sur l'ensemble de la base de données dont nous disposons. La version adaptative de l'algorithme a été appliquée aux variables physiologiques suivantes : Pression artérielle systolique (PAS), fréquence cardiaque (FC), pression maximale dans les voies aériennes (Pmax), débit maximal dans les voies aériennes (Dmax), volume expiré (VE), fréquence respiratoire (FR), ventilation minute (VM).
Afin de valider les résultats obtenus, nous avons utilisé les épisodes temporels extraits pour reconnaître un évènement particulier : les aspirations trachéales. Cette validation a été effectuée hors ligne. L'algorithme a été appliqué sur chaque enregistrement sur l'ensemble des variables physiologiques et les épisodes temporels extraits ont été stockés. Un algorithme de reconnaissance d'évènements a ensuite été appliqué, qui utilise ces épisodes.
Une forme temporelle multivariable correspondant à une aspiration trachéale a été modé-lisée de la manière suivante : {épisode Décroissant sur Pmax ou VE ou VM dont la valeur finale est inférieure à une valeur seuil} et, simultanément, {transitoire décroissant_croissant ou instabilité sur Pmax, VE, VM, FR, Dmax.}. Un transitoire décroissant_croissant est une forme temporelle monovariable composé de 3 épisodes successifs (stable, décroissant, croissant) ou (décroissant, croissant, stable). Une instabilité est un intervalle de temps où la variance des résidus d'une variable est supérieure ou égale au 95 ème percentile. Un indicateur de la certitude de la présence d'une aspiration trachéale est proposé, en ajoutant une valeur à chaque forme temporelle monovariable. Ainsi, un transitoire décrois-sant_ croissant et une instabilité ont une valeur de 1. Une chute a une valeur de 2 si la valeur finale est inférieure à un certain seuil et une valeur de 3 pour un seuil plus petit. La valeur attribuée à une variable correspond à la valeur maximale relevé sur cette variable. Une aspiration trachéale est reconnue si la valeur totale de l'événement (somme des valeurs de toutes les variables observées simultanément) est d'au moins 3, qu'au moins deux variables différentes interviennent dans le calcul de la valeur totale et qu'on a détecté au moins une chute sur une variable. Ainsi, pour reconnaître une aspiration trachéale, la condition minimale est que soient présents simultanément une chute d'une des variables sous une valeur seuil et une instabilité ou un transitoire sur une autre variable (valeur totale égale à 3).
Une analyse quantitative des résultats est en cours qui permettra de préciser le nombre d'aspirations trachéales correctement reconnues, non reconnues et faussement reconnues. Elle nécessite l'analyse de tous les signaux par un observateur extérieur, qui n'a pas encore été effectuée. Un exemple provenant d'un enregistrement sur un patient hospitalisé en réani-mation est présenté figure 1. La méthode a détecté 3 aspirations trachéales, au cours des 4 heures d'enregistrement. Les zones d'aspirations sont représentées par les étoiles en haut des variables.
Les résultats préliminaires obtenus sur les 25 enregistrements de la base de données sont les suivants : 51 aspirations trachéales ont été détectées par l'algorithme, 29 d'entre elles correspondent à des formes temporelles détectées sur au moins 3 variables, et 22 à des formes temporelles détectées sur 2 variables seulement.
La figure 2 présente l'histogramme des valeurs obtenues sur ces 51 aspirations. 28 aspirations trachéales ont été détectées avec une valeur d'au moins 5. Ces aspirations semblent concordantes avec l'analyse visuelle des signaux. Les 23 autres sont plus difficiles à classer et pourraient correspondrent à des toux ou des périodes d'instabilité du patient. La méthode est capable de détecter des aspirations trachéales aussi bien sur des patients ventilés en mode débit contrôlé que sur des patients en ventilation spontanée.
Conclusion
Dans ce papier, nous présentons des travaux en cours de développement : la version adaptative d'un algorithme d'extraction d'épisodes temporels. L'extraction des épisodes se fait en ligne et les seuils de réglages de la méthode sont recalculés en cours d'extraction. Des choix ont été effectués pour définir l'algorithme d'adaptation des seuils. Les résultats partiels semblent montrer qu'il est possible de reconnaître des évènements particuliers (les aspirations trachéales) à partir des signaux, sans connaître à priori le mode de ventilation du patient.

Introduction
L'important volume de documents disponibles en langue naturelle et leur évolution rapide font émerger la nécessité de définir des approches permettant de retrouver rapidement des informations pertinentes dans ces documents.
Ce papier présente une approche qui utilise une ontologie de domaine pour identifier automatiquement des concepts du domaine dans un corpus en langue naturelle. Cette identification de concepts peut servir dans différents contextes : annotation des documents, indexation d'une collection de documents, etc. L'approche proposée est complètement automatique et non-supervisée, mise à part l'utilisation d'une ontologie de domaine. Etant donnés une ontologie O et un corpus C, le but est de retrouver dans C des termes w qui sont l'expression linguistique des concepts de l'ontologie O. On peut ainsi étiqueter les termes retrouvés dans le corpus par des concepts de l'ontologie. Cet étiquetage est réalisé en trois étapes : (1) une première étape emploie des techniques de fouille de textes pour identifier des termes du domaine dans le corpus; (2) pour chaque terme w retrouvé, le voisinage sémantique V(w) est identifié ; (3) en supposant que les relations dans le voisinage du terme w soient déjà dans l'ontologie, le positionnement des relations dans l'ontologie et des mesures statistiques sont utilisés pour étiqueter le terme w.
Le papier présente l'approche adoptée en répondant à un certain nombre de questions : Comment extraire des termes à partir du corpus ? Comment identifier le voisinage sémanti-que des termes extraits ? Ces questions sont traitées dans le paragraphe 2. Etant donnés le terme et son voisinage sémantique, quelles sont les stratégies d'étiquetage ? Une réponse est apportée dans le paragraphe 3. Le paragraphe 4 présente les résultats d'une première expéri-mentation en accidentologie; les perspectives à donner à ce travail sont discutées dans le paragraphe 5.
Extraction des termes et du voisinage sémantique
La fouille de textes est employée pour retrouver des termes du domaine qui représentent l'expression linguistique des concepts (Ville-Ometz et al., 2004). La technique adoptée consiste à rechercher dans le corpus des associations de catégories lexicales susceptibles d'engendrer des regroupements de mots valides. Une telle association de catégories lexicales constitue un patron lexical, par exemple (Verbe, Préposition, Nom).
Un algorithme de reconnaissance (Ceausu et Desprès, 2005) identifie, dans le corpus annoté par l'analyseur syntaxique TreeTagger (Schmid, 1994)  (square, esplanade). L'algorithme suivant permet d'assigner les termes aux concepts de l'ontologie en utilisant cette heuristique : (1) identification des classes de verbes dans l'ensemble d'instances des patrons verbaux ; (2) identification des arguments -sujet et complément -des constructions verbales du type « verbe, prépo-sition » ; (3) utilisation de la représentations des relations dans l'ontologie pour étiqueter les termes. Un pré-traitement de regroupement des arguments des constructions verbales est utilisé pour réduire la variance linguistique entre les arguments dont le sens est voisin. Des mesures statistiques entre les chaînes de caractères, présentées infra, sont utilisées.
Mesures de similarité lexicale
Une mesure de similarité associe un nombre réel r à une paire de chaînes de caractères (S1,S2). Une valeur importante de r indique une similarité importante entre (S1,S2). Diffé-rentes approches permettent de calculer les similarités entre chaînes de caractères (Cohen et al., 2003). Les mesures de Jaccard, Jaro, Jaro-Winkler, Monge-Elkan ont été implémentées dans le cadre de ce travail.
La mesure de Jaccard estime la similarité entre deux chaînes S et T :
(1) Cette mesure est le rapport entre le nombre des sous chaînes communes à S et T et le nombre total de sous chaînes de T et de S. Si les sous chaînes sont des caractères, la mesure de Jaccard correspond au nombre de caractères communs aux deux chaînes. 
Les mesures de Jaro et
La mesure de Jaro-Winkler (1999) est une extension de la mesure de Jaro qui utilise la taille P du plus long préfixe commun aux deux chaînes. En posant 1 m a x ( , 4 ) P P ? , on écrit :
(1 ( , )) 1 0
P J a r o W in k le r s t J a r o s t J a r o s t
Il existe aussi des approches hybrides qui calculent les similarités entre deux chaînes de manière récursive, en analysant des sous chaînes des chaînes initiales. Ainsi, la mesure de Monge-Elkan estime la similarité entre 
Pré-traitement des classes de verbes : regroupement des arguments
Le rôle de cette étape de prétraitement est d'identifier des similarités entre les arguments des relations du type « verbe, proposition » pour les regrouper. Pour une relation donnée, les arguments présentent différents niveaux de granularité, par exemple : partie -partie gauche -partie droite ; rétroviseur -rétroviseur extérieur -rétroviseur intérieur. Un algorithme de regroupement des arguments, fondé sur l'utilisation de la plus grande spécificité des arguments composés de plusieurs mots sur les arguments mono mot, permet de construire des clusters de termes similaires. Un cluster est composé d'un terme central appelé centroïde c et ses k plus proches voisins.
L'algorithme construit une liste de centroïdes composée des arguments mono-mot et utilise la fonction Monge-Elkan pour ajouter des termes aux clusters. Cette fonction est utilisée car elle a la capacité d'agglomérer autour d'un mot les termes dérivés de ce mot.
Etiquetage des termes en utilisant l'ontologie
Dans ce paragraphe on présente l'étiquetage des termes extraits par des concepts de l'ontologie. On dispose d'une ontologie O, contenant un ensemble C de concepts liés par des relations appartenant à un ensemble R et des classes de verbes contenant des constructions grammaticales de type : (sujet), (verbe, préposions), (complément objet). Le prétraitement des classes a regroupé les arguments en clusters homogènes.
L'hypothèse de travail est que les relations verbales appartiennent à R . Pour chaque relation verbale du type (verbe, préposition), la relation r qui lui correspond dans l'ontologie est retrouvée. Les concepts de l'ontologie liés par r sont identifiés et utilisés pour étiqueter les termes en adoptant une des stratégies d'étiquetage décrites ci-dessous.
Stratégies d'étiquetage
Les stratégies d'étiquetage définissent la manière dont les termes seront assignés aux concepts de l'ontologie. Les termes extraits sont déjà organisés en clusters, chaque cluster ayant un centroïde et des termes qui lui sont similaires (cf. 2.2.3).
Une première stratégie traite un cluster comme un ensemble non hiérarchisé de termes. Pour chaque terme du cluster ses similarités avec les concepts de l'ontologie sont évaluées en utilisant les mesures (1) à (3). Le terme est étiqueté par le concept qui maximise la valeur de cette similarité, si cette valeur dépasse un seuil imposé. Si toutes les valeurs des similarités se situent au-dessous du seuil, le terme sera étiqueté comme « inconnu ».
Les stratégies suivantes prennent en compte la structure hiérarchique de chaque cluster. Ainsi, la stratégie top-down identifie d'abord les concepts de l'ontologie qui vont étiqueter les centroïdes. Si le centroïde d'un cluster est étiqueté comme inconnu, la même étiquette est attribuée à chaque terme du cluster. Si le centroïde d'un cluster est étiqueté par un concept c, les étiquettes pour les termes du cluster seront cherchées parmi les sous-concepts de c. Cette stratégie a l'avantage de réduire l'espace de recherche.
Une troisième stratégie adopte une approche bottom-up. Pour chaque cluster, on évalue d'abord les similarités entre ses termes et les concepts de l'ontologie. Une des mesures (1) à (3) est utilisée et les termes sont étiquetés selon le principe de la première stratégie. Ensuite, la similarité du centroïde avec un concept de l'ontologie est donnée par : Les termes extraits de ce corpus sont étiquetés et les résultats obtenus seront analysés selon deux points de vue : pour le même coefficient de similarité, comparer les résultats de chaque stratégie d'étiquetage ; pour une même stratégie d'étiquetage, comparer les résultats de chaque coefficient. Les arguments objets du verbe «circuler avec » sont étiquetés. La stratégie bottom-up permet d'éliminer le centroïde « feu », qui est étiqueté comme « inconnu ». Elle pénalise les centroïdes ayant engendrés des clusters de taille réduite, qui sont assignés aux concepts de l'ontologie avec un faible coefficient, ou sont étiquetés «inconnu». Les résultats des trois stratégies sont similaires pour les coefficients Jaro et Jaro-Winkler, Cette similarité est normale car Jaro-Winkler représente juste une variation de Jaro. Dans le cas du coefficient Jaccard, la stratégie bottom-up montre une défaillance, en assignant le terme « véhicule » au concept «véhicule de service». Quel que soit le coefficient choisi, l'étiquetage top down est plus rapide et donne de meilleurs résultats.
Conclusion et perspectives
Nous avons présenté une approche permettant d'assigner des termes extraits d'un corpus en langue naturelle aux concepts d'une ontologie. Des métriques pour calculer la similarité entre chaînes de caractères ont été implémentées et interviennent dans différentes étapes de l'approche. Une première expérimentation dans le domaine de l'accidentologie montre que les coefficients de Jaro et Jaro-Winkler donnent des estimations de similarités plus fines que Jaccard. Parmi les stratégies d'étiquetage, l'étiquetage top-down est plus rapide et engendre de meilleures assignations des termes aux concepts de l'ontologie.
En perspective, des ressources terminologiques telles que WordNet peuvent être prises en compte afin d'améliorer l'estimation des similarités entre les termes du corpus et les concepts de l'ontologie. Cela permettra d'enrichir le voisinage sémantique du terme par d'autres types de relations, comme la synonymie. Une autre perspective peut être l'ajout d'un feed-back dans le processus décrit, permettant à l'utilisateur non seulement d'étiqueter les termes du domaine, mais aussi d'intégrer dans l'ontologie certains des termes découverts.
Références Alfonseca, E. and S. Manandhar. ( 2001 
Summary
This paper presents an ontology supported approach to automatically recognize concepts of a specific field in a natural language corpora. This in a non-supervised solution that can be applied to any field for which an ontology was already created. A natural language corpora of the field is used in which specific concepts are recognized. In a first phase of the process, terms are extracted from the corpora using text mining techniques. Then, a domain ontology is used to label these terms. A label is assign to each term according to his semantic neighborhood and statistic measures. This paper gives a brief overview of employed text mining techniques and then it focus on the labeling process. A first experimentation of our approach in the field of accidentology was done and his results are also presented.

Calcul du gain
Afin de mesurer le gain d'information d'une règle, nous nous appuyons sur les variations possibles du support du motif M obtenu en réunissant les propriétés des parties gauches et droites sans que les supports des sous-motifs ne changent. L'intervalle de variations obtenu a un centre, et nous décidons que le gain d'information correspondant aux motifs de support central est nul. Plus le support du motif s'éloigne de ce centre, plus la valeur absolue du gain augmente. Cela donne la formule suivante pour le gain : g=2^(L-1)*(s-c), où s est le support du motif M, L la longueur de ce motif et c le centre de l'intervalle de variation.
Le gain de la règle fait partie des indices de qualité au même titre que le support, la confiance et la plupart de ceux dont on peut trouver la définition dans Guillet (2004). Toutefois, il ne mesure pas comme les autres indices la qualité intrinsèque d'une règle, mais la valeur additionnelle d'une règle avec prémisse composée par rapport à celles avec prémisses plus simples. Nous avons défini précédemment des RA floues sur des propriétés numériques (Cadot et Napoli, 2004). Le calcul du gain se prolonge sans problème à ces RA floues, les valeurs du support et du centre n'étant plus nécessairement entières.
Application
Le corpus traité est constitué de 3203 notices bibliographiques extraites de la base PASCAL sur le thème de la géotechnique et indexées manuellement. Nous avons calculé Règles d'association avec prémisse composée : Mesure du gain d'information. quatre classifications avec la méthode des K-means axiales (Lelu et François 1992)  C50 Pression Pores AE C20 Inélasticité C50 Champ pétrole AE C20 Inélasticité A première vue l'intitulé "Champ pétrole" peut paraître surprenant. L'analyse des données qui sont regroupées dans ces classes (titre des articles, résumés, indexation) permet de comprendre cette règle. En effet la classe "Champ pétrole" est essentiellement consacrée aux roches magasins et aux distributions des contraintes dans ces roches. La classe "Inélasticité" est dominée par des aspects liés à l'élastoplasticité et à l'analyse des champs de contraintes. Cette règle apporte ainsi un gain d'information par rapport aux règles simples puisqu'elle lie les notions de pression de pores (donc de roches poreuses plus ou moins saturées) et de distribution des contraintes dans des roches magasins (roches poreuses plus ou moins saturées) avec la notion de champ de contraintes dans le domaine élastoplastique.
Conclusion
Le gain que nous proposons combine les avantages des indices de qualité des RA, et de l'élagage du jeu de RA. Il garde les règles simples, construites sur deux propriétés qui ont été extraites à l'aide d'un indice de qualité choisi pour sa valeur sémantique, et sont donc aisément interprétables. Les autres règles, qui ne sont gardées que si leur gain est significatif, sont également simples d'interprétation car elles renforcent l'information tirée des premières. Au final, l'ensemble des règles obtenu est de taille réduite. Malgré tout, le filtrage par ce gain laisse quelques règles incohérentes. La construction d'un test permettant d'établir la significativité du gain est en cours afin de les éliminer.
Summary
In order to filter set of Association Rules with complex premises, we define a criteria which measures the improvement of information supported by the rule ABAEC compared to the simple rules AAEC or BAEC. Application to clustering results.

Introduction
La conception de personnages virtuels simulant un comportement humain réaliste, y compris d'un point de vue émotionnel (Aylett et Luck, 2000), connaît un engouement croissant. Dans ce contexte, il est alors nécessaire de doter des agents intelligents virtuels de caractéristiques psychologiques humaines. Pour ce faire, les informaticiens sont amenés à recueillir l'expertise de psychologues.
Notre travail s'inscrit dans ce processus et consiste à modéliser l'expertise psychologique de spécialistes dans le but d'appliquer leurs connaissances à l'élaboration d'agents intelligents. Nous montrerons donc comment les graphes orientés et RDF peuvent permettre d'accomplir cette tâche.
La modélisation des interactions
Différents concepts psychologiques sont exploités dans ce projet. Tout d'abord, le comportement définit par l'ensemble des réactions observables chez une personne. Il est propre à chaque individu. La société PerformanSe en a développé un modèle selon 10 dimensions bipolaires : couples de traits de personnalité antagonistes.
Ensuite, les émotions qui caractérisent un ressenti à plus court terme. Le modèle OCC (Orthony et al., 1988), conçu par des psychologues, offre une modélisation facilement implémentable.
Enfin, les interactions sociales qui sont une des notions clés dans les comportements collectifs d'individus. Elles représentent la faculté de ressentir, d'exprimer et d'interpréter les émotions.
C'est l'expertise de la société PerformanSe concernant la perception de l'état émotionnel d'autrui, qui a été transposée en langage naturel semi-structuré, que nous cherchons à modéliser. La recherche d'une solution permettant la représentation formelle et l'exploitation de ces connaissances fait l'objet de notre étude.
L'expertise psychologique des spécialistes a été exprimée sous la forme d'un ensemble de règles. Chacune d'entre elles se compose d'un ensemble d'actions dont l'exécution est conditionnée par une condition booléenne. Une règle peut s'exprimer sous la forme : condition ? {action1, action2, …, actioni} En initiant le concept de Web Sémantique, Tim Berners Lee (Berners, 1999) a jeté les bases des langages rendant possibles la représentation sémantique des contenus. Parmi ces langages fédérés par le W3C et organisés en couches, nous avons choisi d'utiliser RDF (Resource Description Framework), car il propose un niveau de complexité adapté à notre projet.
Un document RDF pouvant se représenter sous la forme d'un graphe orienté et étiqueté, nous avons été amenés à adapter les formalismes existants (graphes ET/OU et hypergraphes) afin de représenter une condition booléenne sous cette forme. Les actions associées aux règles d'interactions sociales consistent à faire évoluer un composant émotionnel de l'agent, ce qui se concrétise par l'évolution d'un attribut. Ceci ce représente facilement sous la forme de graphe, de plus RDF propose une classe nommée "sac" (bag), permettant de stocker l'ensemble des actions associées à une règle.
Conclusion
L'avantage majeur du codage en RDF des règles d'interactions sociales est sa simplicité de mise en oeuvre. En effet, grâce à la disponibilité de bibliothèques spécialisées dans ce domaine (comme JENA), il est relativement simple d'exploiter ces connaissances.
De plus, la solution proposée offre la possibilité d'exprimer les règles sous la forme d'expressions dont la syntaxe est facilement maîtrisable par un non-informaticien. Ce dernier point est primordial dans notre projet car la manipulation des connaissances psychologiques doit être accessible aux psychologues travaillant sur le projet. L'enrichissement de la base de connaissances nous permettra à terme de valider le modèle. 
Références
Summary
Designing an emotional intelligent agent implies to model the expertise of psychologists in term of emotions, cognition and social interactions. This poster presents our work for modeling this knowledge thanks to directed graphs expressed in the RDF language.

Introduction
Le projet RAMCESH est un projet dans lequel sont impliquées diverses organisations ayant trait à la géotechnique. Son objectif est de réaliser un système d'aide à la conception pour les projets géotechniques.
Une approche de la géotechnique
On définit la géotechnique comme l'étude l'interaction d'un sol et d'un construit, qu'il soit ouvrage d'art, bâtiment ou route. Le sol est un système qui défie l'étude en ce qu'il est majoritairement invisible et demande certaines approximations pour être appréhendé : on lui applique le résultats de sondages qu'on estime représentatifs. Le construit lui-même est, en phase de conception de projet, hypothétique, et ses interactions avec le sol sont donc d'autant plus difficiles à évaluer.
De surcroît, la géotechnique est un domaine hétérogène à deux titres : elle dépend de conditions régionales, et rassemble des spécialités différentes (chimie, mécanique, géologie, etc…). Cette hétérogénéité ajoute à l'inconnaissabilité une complexité méthodologique et terminologique discernable dans les documents du domaine.
Dans un tel contexte, le spécialiste géotechnicien adopte une attitude pragmatique se reposant sur un ensemble de savoirs et de savoir-faire très souvent tacites, mais essentiellement construits par analogie d'un contexte vis-à-vis d'un autre.
Un modèle pour la géotechnique
Qui veut modéliser le domaine géotechnique est donc confronté à un problème de grande ampleur ; cependant, la communauté géotechnique aurait l'utilité d'outils informatiques qui puisse l'assister dans la gestion quotidienne de cette complexité.
Il faut un modèle flexible qui puisse s'adapter aux diversités du domaine et rendre compte des disparités d'usage pour faciliter un accès pertinent aux connaissances du système. De plus, l'expérience du domaine des années 1980 et 1990 avec les systèmes Représentation des connaissances en géotechnique experts a enseigné qu'un système réellement utile aux spécialistes est un système qui serait maîtrisé et implémenté par les spécialistes eux-mêmes, sans passer par un intermédiaire.
Une solution est donc de choisir un modèle supportant une sémantique riche, mais peu formalisé afin de ne pas dérouter les spécialistes-utilisateurs.
Une première approche de la modélisation du domaine est donc envisagée à l'aide d'une ontologie informelle structurée qui tienne compte des variations terminologiques au moyens d'ensembles de synonymes et cas d'usage spécifiques fondés sur des emplois métaphoriques (métonymies, synecdoques, etc…) très courants dans la documentation du domaine.
Cette ontologie repose sur deux hiérarchies, subsomption et agrégation, et supporte un formalisme nommé granule de connaissances, dont le rôle est de représenter la connaissance contextuellement, en situation.
Le granule de connaissances
Le granule de connaissances rassemble des concepts définis dans l'ontologie en plus de relations spécifiques. Articulé autour de deux clauses (prémisses et conclusions) liées par une relation d'implication (qui peut servir à définir une hiérarchie de causalité au niveau des granules eux-mêmes), il subdivise les concepts ontologiques en fonction d'un rôle thématique et d'un rôle prédicatif. Le thème, obligatoire, est représenté par un concept unique et correspond à la définition sommaire « ce dont il s'agit ». Le prédicat peut être vide ou rassembler autant de concepts que nécessaire et correspond à la définition « ce qui est dit du thème ». Chaque ensemble thème-prédicat définit ce qu'on appelle une phrase, liée à d'autres phrases au sein de la même clause par un ensemble de booléens.
Les prémisses définissent un contexte spécifique, les conclusions décrivent leur implication. Cet ensemble est extrait des documents du domaine.
Un granule rassemble également les instances et valeurs associées au contexte décrit. L'approximation d'un contexte s'opère avec la variation des éléments conceptuels du granule selon les hiérarchies de l'ontologie (opération baptisée « glissement sémantique »). L'agrégation de granules de connaissances autorise également la représentation de projets géotechniques complets. La méthode utilisée pour agréger des granules est similaire à celle de l'agrégation des knowledge components.
Les travaux en cours concernent les premiers développements collaboratifs de granules de connaissance par les spécialistes du domaine et l'utilisation d'une ontologie descriptive d'environ 5000 mots.

Introduction
La classification supervisée constitue un problème d'apprentissage classique. On dispose dans ce cas, en plus des variables descriptives (ou endogènes), d'une variable cible (ou exogène). En phase d'exploration des données, c'est la dépendance de la variable cible vis-à-vis des variables descriptives qu'on vise à expliciter. En phase de modélisation, le but est de fournir la meilleure prédiction possible pour toute nouvelle instance à classifier. Quelle que soit la situation, la connaissance est à extraire d'un échantillon de N instances étiquetées.
Une méthode de classification usuelle est la règle de classification suivant le plus proche voisin introduite par Fix et Hodges (1951). Elle consiste à attribuer à une instance l'étiquette de l'instance la plus proche parmi celles constituant l'échantillon. La mise en oeuvre de cette modélisation soulève deux questions fondamentales : -Quelle mesure de similitude employer ? -Quelles instances de l'échantillon conserver ? La première question couvre plusieurs champs d'investigation : gestion de la présence jointe de variables continues et symboliques, normalisation des variables continues, prétrai-tement des variables symboliques, pondération de la contribution des variables, etc. Dans le cas continu, l'usage a consacré l'emploi de la distance euclidienne et des distances L p (p ? 1) de Minkowski. La distance de Mahalanobis (Duda et al., 2001), en effectuant une transformation globale des instances et au prix d'un coût de calcul plus élevé, permet d'intégrer dans le calcul de la similitude les corrélations entre couples de variables descriptives. Pour des mesures successives d'une même quantité, le Dynamic Time Warping est un procédé traitant la corré-lation temporelle (Berndt et Clifford, 1996). Dans le cas symbolique, la distance de Hamming est d'autant plus simplificatrice que le nombre de modalités des variables croît. C'est pourquoi des mesures de similitude basées sur les probabilités d'occurence sont souvent utilisées, par Stanfill et Waltz (1986) entre autres. Un procédé de gestion de la mixité (variables continues et symboliques) est proposé par Wilson et Martinez (1997a). On ne s'intéresse pas dans cet article à la question du choix d'une mesure de similitude et on se focalise sur la sélection d'instance.
La classification par le plus proche voisin impose, pour chaque nouvelle instance à classifier, de parcourir l'ensemble de l'échantillon. Ceci entraîne un coût de déploiement rédhibi-toire. Une étape de sélection des instances permet de diminuer le coût de recherche. Classiquement, les méthodes prédictives s'attellent à qualifier le degré d'utilité prédictive d'une instance et conservent les instances jugées utiles. Ces méthodes sont décrites à la section 2.
Les instances sélectionnées (ou : prototypes) induisent une partition de Voronoi de l'espace, à chaque groupe étant associé un prototype. L'ensemble des instances se répartit dans les groupes de cette partition. On considère alors la distribution des étiquettes dans chacun des groupes. Là où les méthodes prédictives ne prennent en compte que l'étiquette du prototype , on associe à chaque prototype cette distribution de probabilité. Cette dichotomie reflète celle observée dans tout processus de fouille de données entre la phase de préparation des données et celle de modélisation (Chapman et al., 2000). On présente à la section 3 une approche descriptive de l'évaluation de la qualité de telles fonctions, ainsi que le critère qui en découle. La question du sur-apprentissage étant prise en charge par le critère, on propose à la section 4 une heuristique d'optimisation poussée. Enfin, une comparaison expérimentale sur données réelles est menée à la section 5.
La sélection d'instances
La méthode CNN (pour Condensed Nearest Neighbor) décrite par Hart (1968) est la plus ancienne méthode de sélection d'instances. Toute instance mal classifiée par son plus proche voisin parmi les prototypes déjà sélectionnés est aussitôt conservée. Ce procédé incrémental est itéré tant qu'il existe des instances mal classifiées par l'ensemble de prototypes. La méthode est consistante, dans le sens où tout élément de l'échantillon est bien classifié par son plus proche prototype. La complexité au pire de cet algorithme est un O(N 3 ). Une amélioration est proposée par Gates (1972) : RNN (pour Reduced Nearest Neighbor). Une fois la règle CNN appliquée, toute suppression d'un prototype ne provoquant aucune mauvaise classification d'une instance est validée. L'intérêt de cette méthode réside dans sa capacité à produire un sous-ensemble de prototypes de taille minimale relativement à la condition de consistance, sous réserve qu'un tel sous-ensemble soit inclus dans la solution proposée par CNN.
Les décisions prises par ce type de techniques sont peu robustes (conservation du bruit, par exemple). Afin d'y remédier, une idée consiste à prendre en compte les K plus proches voisins (typiquement, K = 3). Le procédé est décrémental et une instance est éliminée si elle est mal classifiée par un vote à la majorité sur ses K plus proches voisins. C'est la règle ENN, pour Edited Nearest Neighbor, présentée par Wilson (1972 Dans ce but également, la notion d'association est utilisée par Wilson et Martinez (1997b). Pour K ? N est fixé, si x est l'un des K plus proches voisins de y, on dit que y est associé à x. Autrement dit, x est associé à y s'il participe à la classification de y. Dès lors, x est éliminé si le nombre de ses associés bien classifiés ne diminue pas après sa suppression. La règle ENN est préliminairement appliquée. De plus, les instances sont considérées par ordre décroissant de distance à la plus proche instance de classe différente. La méthode obtenue, DROP3, est de complexité un O(KN 2 ). A la croisée des chemins entre IB3 et DROP3, on trouve un test statistique évaluant l'hypothèse de non contribution d'une instance à la classification de ses associés (Sebban et al., 2002). Ce critère est paramétrique et son calcul nécessite l'approximation de la densité de la statistique associée. Une adaptation de l'algorithme AdaBoost permettant de traiter des classifieurs locaux (les prototypes) aboutit à une heuristique de recherche incrémentale. Dans sa version la plus rapide, l'algorithme est de complexité un O(KN 2 ). Cameron-Jones (1995) a proposé un critère d'évaluation de la qualité prédictive d'un ensemble de prototypes. L'approche adoptée est de type MML (pour Minimum Message Length) et le critère obtenu s'écrit :
avec K le nombre de prototypes, N le nombre d'instances, E le nombre d'instances mal classifées par leur plus proche prototype et J le nombre de classe cibles. La quantité F (U, V ) mesure la longueur du mot de code nécessaire à la spécification de U instances parmi V et est évaluée par la formule :
u où log * (x) désigne la somme des termes positifs log 2 (x), log 2 (log 2 (x)), etc. Les termes K log 2 (J) et E log 2 (J ?1) correspondent aux longueurs de code nécessaires à la spécification des étiquettes des K prototypes et des E exceptions respectivement.
Une heuristique est également proposée, qu'on nomme ici Explore. Une première phase itérative consiste à ajouter une instance si la valeur du critère diminue. Une fois toutes les instances considérées, tout prototype dont la suppression conduit à la diminution de la valeur du critère est effectivement éliminé. Enfin, 1000 mutations sont évaluées et acceptées si la valeur du critère décroît. Une mutation est soit un ajout d'une instance à l'ensemble des prototypes, soit une suppression d'un prototype, soit un échange entre une instance et un prototype. Au final, la méthode est de complexité un
Une approche descriptive
L'approche prédictive classique consiste à évaluer la qualité prédictive d'un classifieur, en mesurant le risque structurel empirique par exemple. C'est le parti pris par l'ensemble des mé-thodes de sélection. On s'intéresse pour notre part à la qualité de la distribution des étiquettes conditionnellement aux instances. Dès lors, une mesure de cette qualité doit être proposée et on adopte ici une approche descriptive.
Notations
Si P ? X, la partition de Voronoi V (P ) = (V (p)) p?P associée à P est définie par :
Pour p ? P , la cellule de Voronoi V (p) contient les points x pour lesquels p est l'élément de P le plus similaire, relativement à ?. L'élément p est appelé prototype de la cellule V (p). La figure 1 donnent des exemples de telles partitions.
FIG. 1 -Exemple de partitions de Voronoi.
On définit ici un modèle (descriptif) comme un couple (v, ?) où v est une partition de Voronoi formée de K cellules et ? une matrice de taille (K, J) dont le coefficient (k, j) donne la probabilité de l'étiquette j dans la cellule k. Autrement, à chaque cellule (i.e. à chaque prototype) est associée une distribution de probabilité sur L.
Si v est une partition de Voronoi composée de
-424 -RNTI-E-6
Formalisation
On considère le modèle comme étant aléatoire et on cherche à spécifier la probabilité jointe
. Cette probabilité est décomposée à l'aide de la formule des probabilités itérées. Du fait qu'on s'intéresse à cette probabilité, et non à la probabilité de mauvaise classification du classifieur associé, comme décrit par Vapnik (1996), on qualifie l'approche de descriptive.
Plus précisément, si K désigne le nombre de cellules de V , on commence par écrire :
ce qui permet de comparer des ensembles de prototypes de différentes tailles. On itère ensuite la dépendance en utilisant la formule de Bayes :
On suppose les comportements des distributions dans chaque cellule conditionnellement indépendants, ce qui donne :
leurs étiquettes. On applique une nouvelle fois la règle de Bayes et on obtient : 
Spécification
) a été décomposée à l'aide de la formule de Bayes. On spécifie maintenant chacune des probabilités, en précisant à chaque étape le support de la probabilité concernée et en appliquant un a priori uniforme.
En ce qui concerne le nombre de cellules, i.e. la probabilité P (K/D (x) ), les valeurs possibles de K sont comprises entre 1 et N . L'application de l'a priori uniforme donne :
La partition de Voronoi est caractérisée uniquement par ses prototypes. Les ensembles de prototypes considérés sont les parties de D (x) . Adopter un a priori uniforme nous conduirait à introduire le coefficient binomial N K , puisque l'on doit choisir K prototypes parmi les N instances. Mais ce coefficient est symétrique relativement à K. Comme on préfère les valeurs faibles de K, on utilise le coefficient N +K?1 K?1 , croissant avec K, proche de N K pour les faibles valeurs de K, nul pour K = 1, caractérisant ainsi plus finement notre préférence :
Dans la k eme cellule, on exploite la dépendance aux données et on restreint le support des distributions possibles aux probabilités rationelles avec N k pour dénominateur. Formellement, le support est
La partition et les fréquences des classes cibles dans chaque cellule sont à ce stade connues. Il reste à spécifier les étiquettes de chaque instance dans chaque cellule. Dans chaque cellule, le support est restreint relativement à la dépendance : pour la k eme cellule (1 ? k ? K), le problème revient à placer les éléments de la cellule dans J urnes, sous la contrainte d'effectif N kj dans la j eme urne (1 ? j ? J). Le coefficient multinomial donne le nombre exact de ces possibilités et on obtient :
, un modèle descriptif M est évalué par la formule suivante :
Le premier terme correspond à la description du nombre de groupes, le second à la description des prototypes, le troisième à la description des fréquences des étiquettes dans les cellules et le dernier à la description de l'attribution des étiquettes aux instances dans les cellules. Notons que, d'après l'approximation de Stirling (log x! ? x log x?x+O(log x)), le dernier terme de la formule se comporte asymptotiquement comme N fois l'entropie conditionnelle de la distribution des Y n en connaissance de la fonction d'assignement associée à la partition :
Heuristique d'optimisation
On dispose d'un critère évaluant tout sous-ensemble de l'ensemble des instances. L'espace de recherche a pour cardinal 2 N , rendant la recherche exhaustive peu réaliste. On propose une nouvelle heuristique, encapsulant une optimisation gloutonne descendante d'un ensemble de prototypes dans une méta-heuristique de recherche à voisinage variable.
Optimisation gloutonne d'un ensemble de prototypes
L'heuristique gloutonne GLOUTON(P ) s'applique à tout ensemble P de p prototypes. Chaque ensemble obtenu par suppression d'un élément de P est évalué. Parmi ces ensembles, celui minimisant le critère est déclaré vainqueur de l'étape. Ce procédé est itéré par application aux vainqueurs successifs jusqu'à l'évaluation finale d'un singleton. Le meilleur ensemble rencontré lors du parcours est renvoyé. Autrement dit, l'algorithme GLOUTON(P ) s'écrit :  (N p log p).
A chaque étape, toute suppression d'un prototype conduit à réattribuer uniquement les instances appartenant à la cellule de ce prototype. A l'étape K, les N instances ne sont donc à traiter qu'une fois, pour un coût qu'on peut rendre constant.
En effet, si l'on dispose pour chaque instance de la liste triée des éléments de P par distance croissante, l'acquisition du plus proche prototype suivant se fait à coût constant. L'algorithme GLOUTON(P ) se voit donc adjoindre une phase d'initialisation qui devient prépondérante avec une complexité temporelle un O(N p log p) (construction de N listes triées de taille p). Le stockage de ces N listes induit une complexité spatiale en O(N p).
La valeur du critère est également mise à jour à coût constant. Seuls les deux derniers termes dépendent de la répartition des instances dans les cellules. La suppression d'un prototype conduit tout d'abord à soustraire sa participation à la valeur du critère. Ensuite, la réattribution d'une instance à son plus proche prototype k suivant induit une simple incré-mentation unitaire des compteurs N k et N kj0 , où j 0 est l'indice de la classe à laquelle appartient l'instance. Le terme du critère porté par le prototype k est donc mis à jour en ajoutant log(N k + J) ? log(N kj0 + 1).
Notons que l'on peut introduire dans GLOUTON une contrainte de préservation de certains prototypes : si P ? P , GLOUTON(P ,P ) n'évalue la suppression d'un prototype que si celui-ci n'appartient pas à P . Dans l'optique d'une inclusion dans une méta-heuristique, cette modification permet de limiter la redondance de la recherche.
Recherche à voisinage variable
L'heuristique gloutonne est par nature susceptible de s'empêtrer dans un optimum local. Il est donc naturel d'envisager la remise en question de la solution proposée par GLOUTON. Pour cela, on applique la méta-heuristique de recherche à voisinage variable décrite par Hansen et Mladenovic (2001). Celle-ci consiste à modifier localement une solution et à réappliquer l'heuristique de base, ici l'heuristique gloutonne. Si on n'obtient pas ainsi de meilleure solution, on réitère en explorant un voisinage plus éloigné. Sinon, on réitère en considérant un voisinage de taille minimale de la nouvelle meilleure solution. Le nombre d'étape est usuellement contrôlé par une valeur maximale spécifiée par l'utilisateur.
Une notion de voisinage d'une solution doit être définie. Pour un ensemble P 0 de p prototypes, un voisin est tout ensemble de prototypes P = P 1 P 2 tel que P 1 est inclus dans P 0 et P 2 est un ensemble d'instances appartenant aux cellules de V (P 0 ) associées aux éléments de P 0 \ P 1 . Si t ? [0, 1], le voisinage V t (P 0 ) contient tous les voisins P = P 1 P 2 de P 0 tels qu'une proportion t de prototypes dans P 0 est remplacée par une proportion t d'instances dans l'union des cellules correspondantes (cf Figure 2).
FIG. 2 -Exemple de partitions voisines pour t = 0.35. (a) Répartition des instances dans les cellules de la partition. (b) 2 prototypes (soit 35% des prototypes) sont remis en cause et 3 instances (soit 35% des instances appartenant aux cellules associées) les remplacent. (c) Partition voisine obtenue.
Un unique paramètre Niveau quantifie le degré d'optimisation souhaité par l'utilisateur. Une incrémentation unitaire de ce paramètre revient à doubler le temps consacré à l'optimisation. L'algorithme RVVGLOUTON(N iveau), de complexité au pire un O(2 N iveau N 2 log N ), s'écrit alors :
Expérimentation
On évalue les méthodes de sélection d'instances selon trois axes : la performance prédictive (i.e. le taux de bonne classification en test), le taux de compression (i.e. le rapport du nombre de prototypes au nombre d'instances) et la robustesse (i.e. le rapport du taux de prédiction en test au taux de prédiction en apprentissage). Ces indicateurs sont estimés par validation croisée stratifiée à 10 niveaux. Notre méthode est comparée à IB3, DROP3, Explore et la règle de classification par le plus proche voisin NN. Le niveau de RVVGLOUTON est fixé à 5, ce qui donne un temps de calcul du même ordre que celui d'Explore. Les jeux de données sont issus de l'UCI (Blake et Merz, 1996). Les jeux de données utilisés (tableau 3) sont ceux pour lesquels la performance pré-dictive de la règle de classification par le plus proche voisin est significativement supérieure à celle du prédicteur majoritaire (qui attribue à toute nouvelle instance la classe majoritaire sur l'échantillon). Afin d'éviter toute interférence sur les résultats relative au choix de la distance, du type de prétraitement, etc, on ne considère que des jeux de données sans valeurs manquantes, que les variables continues. La distance de Minkowski L 1 fait office de mesure de similitude.
RVVG
Le taux de bonne prédiction est reporté dans le tableau 1 et le taux de compression moyen dans le tableau 2. Les méthodes classiques, représentées par IB3 et DROP3, réalisent une compression de l'ordre de 20 à 30 pour cent, avec une perte en terme de performance prédictive (69.2% et 71.5% respectivement contre 74.8% pour la classification par le plus proche voisin). Notre méthode sélectionne un minimum d'instances (1.7% contre 2.5% pour Explore en moyenne) sans que la performance prédictive n'en soit affectée.
Les méthodes se plaçant dans le cadre de l'apprentissage de modèles sont donc plus performantes que les méthodes classiques basées sur des définitions (nécessairement heuristiques) d'utilité individuelle. L'approche descriptive adoptée ici permet de gagner encore en compression par rapport à Explore. Ceci conduit à améliorer la robustesse (tableau 2).
La fiabilité du résultat est une propriété importante, que l'approche descriptive permet encore d'améliorer. Ceci est d'autant plus intéressant que les modèles considérés (des fonctions de probabilités conditionnelles) sont plus riches que les modèles usuels (des classifieurs) : à chaque prototype est associé une distribution de probabilité sur les étiquettes. Notre méthode extrait donc plus de connaissance, et de manière plus fiable. Ceci rend profitable son utilisation en phase de préparation des données, là où les méthodes prédictives sont inadaptées. 
Conclusion
La classification suivant le plus proche voisin repose sur la construction d'une partition de Voronoi. Dans cet article, nous avons proposé un critère d'évaluation des partitions induites par les ensembles de prototypes inclus dans l'ensemble des instances formant l'échantillon. L'approche descriptive adoptée ayant permis de faire porter la gestion du sur-apprentissage par le critère, nous avons également proposé une heuristique d'optimisation poussée de ce critère.
Les expériences sur jeux de données de l'UCI ont montré que notre méthode est compétitive en terme de performance prédictive, tout en sélectionnant un minimum d'instances. L'étude de la robustesse a également illustré le fait que la méthode ne sur-apprend pas : la décision prise est fiable et pertinente. Cela concourt à l'emploi et au déploiement de modèles de classification par le plus proche voisin. De par la richesse de la connaissance extraite, l'utilisation de la méthode n'est pas limitée à la prédiction.

Introduction
A l'heure actuelle, les données arrivent plus vite que la capacité de traitement des algorithmes de fouille de données ne permet de les traiter. L'amélioration des performances des algorithmes de fouille de données est indispensable pour traiter de grands ensembles de données. Nous nous intéressons au cas de la classification supervisée et plus particulièrement à une classe d'algorithmes : les SVM [Vapnik, 1995]. En règle générale, ils donnent de bons taux de précision mais, l'apprentissage des SVM se ramène à résoudre un programme quadratique et est donc coûteux en temps et mémoire. Pour remédier à ce problème, les méthodes de décomposition [Platt, 1999], [Chang et Lin, 2003] travaillent sur des sousensembles arbitraires de données, on utilise alors des heuristiques [Do et Poulet, 2005] permettant de choisir les sous-ensembles de données. D'autres travaux visent à construire des algorithmes incrémentaux [Fung et Mangasarian, 2002] dont le principe est de ne charger qu'un petit bloc de données en mémoire à la fois, de construire un modèle partiel et de le mettre à jour en chargeant consécutivement des blocs de données. Les SVMs parallèles et distribués ] utilisent un réseau de machines pour améliorer les performances. Nous présentons un nouvel algorithme de SVM linéaire et non-linéaire pour traiter de grands ensembles de données dans un temps restreint sur du matériel standard. A partir de l'algorithme de Newton-GSVM [Mangasarian, 2001]  (i = 1, 2, …, m) dans l'espace réel en dimension n. La matrice diagonale de ±1 D [mxm] représente les classes y i des m individus. I est la matrice identité, e est le vecteur colonne de 1, c est une constante positive, z est la variable de ressort (slack) et w, b sont les coefficients et le scalaire de l'hyperplan. Le paragraphe 2 présente le principe de l'algorithme de Newton-GSVM. Ensuite, nous décrivons une version incrémentale de l'algorithme de Newton-GSVM dans le paragraphe 3 puis la construction de l'algorithme parallèle et distribué de Newton-GSVM. Les résultats numériques sont présentés dans le paragraphe 5 avant de conclure sur nos travaux. 
Méthode de Newton pour GSVM
(1) où e est un vecteur colonne de 1 Les distances des erreurs sont notées par des variables de ressort (z i ? 0 ; i=1, 2, …, m). Si l'individu x k est du bon côté de son plan support, alors z k est égal à 0. La recherche de l'hyperplan optimal se ramène à simultanément maximiser la marge et minimiser les erreurs. La formulation primale du problème est exprimée par le programme quadratique (2) :
où une constante c > 0 est utilisée pour contrôler la marge et les erreurs. Le plan optimal (w, b) est obtenu par la résolution du programme quadratique (2) dont la mise en oeuvre est coûteuse en temps et mémoire vive. L'algorithme de generalized SVM (GSVM) [Mangasarian, 1998] modifie l'algorithme de SVM en maximisant la marge par (1/2) ||w, b|| 2 et minimisant les erreurs par (c/2) ||z|| 2 . On obtient alors la formule primale de GSVM : 
, on peut réécrire le problème (5) en : [Mangasarian, 2001] a proposé d'utiliser la méthode itérative de Newton pour résoudre efficacement le problème d'optimisation (6). Le principe de la méthode de Newton est de minimiser successivement les approximations au second ordre de la fonction objectif ? en se basant sur un développement de Taylor au second ordre au voisinage de X v .
On minimise la fonction quadratique ?(X), ce qui fournit (8) pour que la dérivée première ?'(X) soit égale à zéro.
A l'itération p, on construit ? p , approximation quadratique ? au voisinage de X p , que l'on minimise pour obtenir X p+1 , défini par (8). Pour minimiser la fonction ?(X) dans le problème d'optimisation (6), on calcule d'abord la dérivée première
. L'algorithme itératif de Newton pour GSVM est construit à partir des calculs de la dérivée première (9)  
L'algorithme incrémental en ligne de Newton-GSVM linéaire peut traiter de très grands ensembles de données sans difficulté. Entre deux étapes incrémentales ne sont conservées en mémoire que la dérivée première ?'(X), vecteur colonne de taille (n+1) et le Hessien ?''(X), matrice de taille (n+1)x(n+1). Si l'on se place dans le cas où les données sont telles que le nombre d'individus est beaucoup plus important que le nombre de dimensions, la matrice ?''(X) conserve une taille raisonnable car seulement fonction du nombre de dimensions ce qui explique les bonnes performances de l'algorithme de Newton-GSVM linéaire incrémental dans ce cadre d'utilisation.
Parallélisation et distribution de l'algorithme incrémental de Newton-GSVM
La quantité de données stockées ne cesse de croître, à l'heure actuelle elle dépasse parfois les possibilités de traitement. Pour pouvoir faire face à cet afflux, une solution est de paralléliser et distribuer le processus de fouille. Nous avons parallélisé l'algorithme incrémental de Newton-GSVM. Les deux caractéristiques incrémentale et parallèle de cet algorithme permettent à la fois d'optimiser au mieux l'utilisation de la mémoire (grâce à l'aspect incrémental) et le temps d'exécution (grâce à l'aspect exécution en parallèle).
Soit un grand ensemble de données découpé en k blocs lignes A 1 , D 1 , …, A k , D k et distribué sur plusieurs machines distantes (PC 1 , PC 2 , … PC k ). Sur chaque machine distante, le bloc de données peut être traité en une seule fois ou être encore re-décomposé en blocs lignes. Cet aspect est intéressant pour la mise en oeuvre du calcul sur un ensemble de machines disparates : on adapte les données à la capacité mémoire disponible. Le calcul du modèle (partiel) est effectué sur chaque machine distante et le résultat envoyé au serveur qui effectue la mise à jour de la solution pour chaque itération de l'algorithme. Le résultat final est absolument identique à ce que l'on aurait obtenu en utilisant l'algorithme séquentiel sur l'ensemble de données. Pour chaque itération on calcule parallèlement et indépendamment sur les machines distantes les
. Ces résultats sont envoyés au serveur qui met à jour la solution et renvoie ce résultat aux machines distantes pour le calcul de la prochaine itération si la condition d'arrêt (?'(X) ? 0) n'est pas vérifiée. Nous avons choisi une solution très simple basée sur le mécanisme XML-RPC pour pouvoir travailler directement sur des entrepôts de données hétérogènes distribués sur le WEB, et donc lancer des portions de calcul sur les sites distants quel que soit le système d'exploitation et l'architecture des machines.
Résultats
L'ensemble du programme est écrit en C/C++ sous Linux (PC) avec la librairie Lapack++. Nous nous intéressons ici à évaluer les performances en temps d'exécution et taux de bonne classification en fouille de grands ensembles de données. Les jeux de test ont donc été effectués avec les ensembles de très grandes tailles : nous avons utilisé Ringnorm [Delve, 1996] pour générer des ensembles de données de dix mille à 1 milliard d'individus en dimension 20 avec 2 classes où la classe 1 a une moyenne égale à 0 et une variance égale à 4 et la classe -1 a une moyenne égale à 2/sqrt(20) et une variance égale à 1. Une description des ensembles de données est fournit dans le tableau 1. Les tests ont été réalisés sur des PCs Pentium-4 (3 GHz, 512 Mo RAM).
Nous avons découpé l'ensemble de données en blocs (égaux pour chaque machine). Ensuite nous avons fait varier la taille des blocs (les données sur une machine peuvent être traitées en une fois ou par morceaux). Dans le pratique, si la taille des blocs est trop grande alors il y a saturation de la mémoire, le système d'exploitation passe tout son temps à « swapper ». La taille des blocs a donc une forte influence sur la vitesse d'exécution des algorithmes, de même que les caractéristiques matérielles de la machine utilisée. Dans la classification non linéaire des données Ringnorm, [Do et Poulet, 2005]  
Conclusion et perspectives
Nous avons présenté un nouvel algorithme incrémental, parallèle et distribué de Newton-GSVM pour traiter linéairement et non linéairement des grands ensembles de données dans un temps restreint sur du matériel standard : un milliard d'individus en 20 dimensions sont classifiés linéairement en deux classes en 43 minutes et non linéairement en 25 heures sur dix Pentium-4. L'apprentissage incrémental permet de traiter de très grandes quantités de données sans difficulté de mémoire sur une machine standard. Le traitement parallèle et distribué utilise un groupe de machines standard pour améliorer les performances en temps d'exécution. La complexité de l'algorithme varie linéairement avec le nombre d'individus de l'ensemble de données, le nombre de machines utilisées et le carré du nombre de dimensions. Avec une tâche de classification linéaire, un milliard d'individus (voire plus) sont classifiés sans difficulté sur des PCs. Dans le cas non linéaire, l'algorithme prend une matrice de noyau en entrée au lieu de la matrice de données. La complexité de l'algorithme et la qualité du modèle dépendent de l'ensemble des vecteurs support en entrée. Il faut rechercher de bonnes

Contexte
L'ensemble des mandats d'intervention en lien avec une mauvaise qualité de l'air dans les espaces fermés et leurs différentes solutions constitue notre base documentaire qui symbolise l'image représentative de la connaissance et du savoir faire des experts. Le raisonnement que nous mobilisons autour de cette base pour notre système d'aide à la décision est le raisonnement à partir de cas. L'interface interactive que nous avons développée nous permet de sauvegarder l'ensemble des cas dans un formalisme XML reproduisant la structure logique des plaintes. Les modèles de balises correspondent aux différentes rubriques citées dans les textes des plaintes (antécédents, symptômes, environnement, etc.). Le RàPC s'appuie d'abord sur le module de remémoration des cas de pollution similaires au contexte de la plainte courante. Ce module est fortement lié au contenu et à la structure des cas situés en mémoire. Le RàPC repose aussi sur le module d'adaptation qui ajuste le rapport du cas jugé le plus similaire au contexte du cas courant. Il est constitué également du module de consensus assurant la validation humaine multi-experte des cas, et du module d'apprentissage du nouveau cas pour capitaliser la nouvelle expérience en mémoire.
Phase de remémoration
Nous nous basons sur la mesure de similarité conceptuelle de (Zarga et Salotti, 2004) pour établir ensuite une distance sémantique entre le problème énoncé dans la plainte cible et les problèmes sources. Inspirées des travaux de (Wu et Palmer, 1994) Zarga et Salotti utilisent une mesure de spécificité qui favorise les liens père-fils par rapport aux autres liens :
où Depth btm (PPS(C 1 , C 2 )) est le nombre maximum d'arcs séparant le concept bottom ( le concept le plus bas du réseau conceptuel) du plus petit subsumant de C 1 et C 2 ; PPS(C 1 , C 2 ). Dis(C 1 , C 2 ) est la distance en nombre d'arcs entre C 1 de C 2 .
-727 -
RNTI-E-6
Système d'aide à la décision : surveillance des ambiances intérieures 2.1 Modèle de proximité flou (Mercier & Beigbeder, 2004) estiment que plus les termes de la requête apparaissent proches au niveau d'un élément de la base plus ce dernier est pertinent par rapport à la requête cible. Le modèle de proximité flou du terme A par rapport au terme B est formalisé par:
(t) désigne l'ensemble des positions prises par le terme t, et k une constante fixe choisie représentant la taille de la fenêtre glissante des cooccurrences des termes.
Notre nouveau modèle: le modèle de proximité flou sémantique
La mesure de (Mercier & Beigbeder, 2004) est très intéressante, néanmoins elle ne tient pas compte de la sémantique des termes (dans le cas où des termes sémantiquement proches des termes utilisés dans la requête apparaissent directement proches au niveau d'un élément de la base). En effet, ce modèle est limité par la relation de cooccurrence directe des termes qui ne permet pas de capturer la proximité sémantique entre les mots. L'équation présentée dans le modèle de Mercier et Beigbeder devient :
Simens(A) est l'ensemble des termes proches de A suivant la mesure d'appariement conceptuelle utilisée. Notre nouveau modèle apporte la connaissance de la sémantique au modèle existant. les résultats des degrés de proximité que nous avons obtenus à l'étape d'expérimentation sur des données réelles et sur des données simulées sont très améliorés à l'aide de l'augmentation sémantique. 
Références
Summary
We present a new information retrieval model based on the semantic proximity level of term occurrences by using a conceptual network to identify the closest cases of the used request. Using this framework, we aim to implement a case based reasoning approach to help decision making in situations where indoor air is suspected to be responsible of health effects.

Introduction
Aujourd'hui, les techniques d'analyse et d'intégration de données sont devenues des atouts majeurs pour les entreprises et les services gouvernementaux. En effet, ces techniques permettent un gain de temps pour regrouper et croiser l'information distribuée. Dans le domaine du développement durable, ces techniques sont notamment indispensables afin de rassembler et d'analyser les pratiques agricoles et ainsi garantir la traçabilité des pratiques. Plus précisément nos travaux se situent dans le cadre d'un projet Dans cet article, nous nous plaçons dans le cadre d'un système de médiation suivant une approche Local As View (LAV), où les vues sont décrites via des requêtes sur le schéma global. Cette approche est connue pour être flexible car l'ajout et la suppression de sources de 1 Ce projet est réalisé en collaboration avec le Cemagref, http ://www.cemagref.fr/ 2 http ://www.acta-informatique.fr/ -77 -RNTI-E-6
Fouille de données pour la réécriture de requêtes données n'affectent pas le schéma global. Nous nous intéressons plus particulièrement au problème de la réécriture de requêtes en termes de vues en présence de contraintes de valeurs. Les contraintes de valeurs correspondent à la notion de type énuméré en base de données. Elles permettent de spécifier les valeurs autorisées pour un attribut donné. Les contraintes de valeurs sont utiles dans beaucoup d'applications, comme par exemple pour la vérification des contraintes d'intégrité ou pour exprimer une forme d'information incomplète (Borgida et Patel-Schneider, 1994  ). Nous présentons un nouveau prédicat, nommé P 2 , issu de la formalisation du problème du calcul de ces nouveaux cas de réécriture dans le cadre de Mannila et Toivonen (1997). Ce prédicat P 2 , en conjonction avec le prédicat P 1 présenté dans (Jaudoin et al., 2004) et rappelé ici, permet de résoudre le problème de réécriture. -Enfin, nous décrivons succintement une implémentation de notre approche de réécriture qui exploite et adapte un algorithme de fouille de données existant, Apriori. Nous pré-sentons ensuite les résultats de nos expérimentations qui viennent conforter l'intérêt des techniques de fouilles de données dans notre cadre. Les premiers résultats montrent la capacité de notre prototype à passer à l'échelle en supportant le traitement d'un grand nombre de vues (jusqu'à 15000). Notons que dans la littérature, peu d'articles présentent les résultats expérimentaux de leurs approches de réécriture. Ils se concentrent généra-lement sur les résultats théoriques. A notre connaissance, (Pottinger et Halevy, 2001) est une des rares références qui décrit l'évaluation des performances d'un algorithme de réécriture. Aussi la réalisation d'un prototype et son expérimentation constituent à notre avis une contribution dans le domaine de la réécriture.
La suite de l'article est organisée comme suit. Dans la section 2, nous donnons les prérequis né-cessaires à la formalisation du problème de la réécriture de requêtes en présence de contraintes de valeur dans le cadre des logiques de description. La section 3 reformule ce problème dans un cadre de découverte de connaissances dans les bases de données. La section 4 présente l'implémentation et les expérimentations réalisées. En section 5, une conclusion et des perspectives sont données. Les démonstrations des lemmes et théorèmes de cet article sont donnés dans Jaudoin et al. (2005).
Réécriture de requêtes en présence de contraintes
Prérequis sur les logiques de description
Les logiques de description sont un formalisme de représentation des connaissances qui permet de représenter des structures complexes et de raisonner avec elles (Baader et al., 2003). Elles permettent de décrire un domaine d'application à l'aide de concepts (prédicats unaires) et de rôles (prédicats binaires). Une logique de description est définie par un ensemble de constructeurs. Dans cet article, nous nous intéressons à la logique ALN (O v ), dont les constructeurs sont listés dans la table 1, colonne 1, où C est un ensemble de noms de concepts, N un ensemble de noms de valeurs, R C un ensemble de noms de rôles dont l'image est un concept de C et R V un ensemble de noms de rôles qui prennent leurs valeurs dans N .
Constructeurs
Sémantique
TAB. 
Plus précisément, la sémantique des constructeurs de ALN (O v ) est donnée dans la colonne 2 de la table 1. Une interprétation est un modèle pour un concept C ssi C I = ?. Un concept est inconsistant ssi C I = ? pour toute interprétation I.
Etant donnée cette sémantique, il est possible de définir la notion
terminologie. Une terminologie est un ensemble fini d'axiomes terminologiques de la forme :
Dans cet article, nous supposons que les terminologies sont acycliques, i.e. aucun concept ne fait référence à lui-même directement ou indirectement dans sa définition ou dans sa spécification. La sémantique des terminologies est obtenue en étendant la notion d'interprétation aux axiomes terminologiques comme suit. Une interprétation I est un modèle pour une terminologie T ssi I est un modèle pour chaque axiome de T .
Pour traiter le problème de la réécriture dans la logique ALN (O v ), nous nous appuyons sur la forme normale donnée dans Jaudoin et al. (2005). La forme normale d'un concept permet d'exprimer un concept sous une forme canonique. Cette forme normale transforme tout concept C en concept ou en une conjonction d'atomes de la forme ?w.P avec w un mot défini sur l'ensemble des rôles de R C ? R V 5 et P est soit un concept atomique A ou une restriction de cardinalité (? nR ou ? nR) ou un ensemble de valeurs E. Par la suite, on note ?w.P ? C si ?w.P apparaît dans la description du concept C.
Réécriture dans ALN (O v )
Dans cette section, nous définissons le cadre de médiation et le problème de la réécriture dans le cadre des logiques de description. Puis nous donnons les caractéristiques des réécri-tures dans ce contexte. Le schéma global S est une terminologie formée de définitions de concepts dans ALN (O v ). Une requête Q est un concept dans ALN (O v ). Q est décrite en termes des concepts de S. De plus Q est supposée être dans sa forme normale. V est une terminologie formée de spécifications primitives dans ALN (O v ). Les spécifications primitives de V permettent de décrire les vues. Les vues sont décrites en termes de S et sont supposées être données dans leur forme normale.
Dans ce contexte, on cherche à répondre à une requête Q en ayant uniquement connaissance des vues de V. Une technique pour répondre à Q est de reformuler Q en une expression qui utilise uniquement les vues de V. L'expression obtenue est appelée réécriture.  (Levy et al., 1996). Informellement, cette approche fonctionne de la manière suivante. Etant donné une requête Q ? ?w 1 .P 1 . . . ?w n .P n , la principale idée est de considérer chaque atome ?w i .P i de Q isolément. A chaque atome ?w i .P i de la requête est associé un panier/ensemble qui contient toutes les réécritures conjonctives maximales de cet atome. Ensuite dans une deuxième étape, les réécritures candidates de Q sont calculées en effectuant le produit cartésien entre les paniers. Ceci permet d'obtenir un sur-ensemble de toutes les réécritures conjonctives maximales de Q. Pour obtenir effectivement les réécritures conjonctives maximales, les réécritures inconsistantes et non maximales doivent ensuite être supprimées de ce sur-ensemble.
Les travaux présentés dans Jaudoin et al. (2005)  
6 au sens de l'inclusion ensembliste 7 la profondeur d'un atome ?w.P est égal à la longueur du mot w, i.e. le nombre de rôles dans w.
-81 -
RNTI-E-6
Fouille de données pour la réécriture de requêtes . Aussi, dans la suite de l'article, nous nous concentrons sur les problèmes du calcul des réécritures engendrées par les contraintes de valeurs car ces problèmes posent de nouvelles difficultés en termes de réécriture de requêtes.
Soit une requête Q ? ?w.P où P est un ensemble de valeurs E ou une restriction de cardinalité ? n R v . Considérons maintenant le problème de la création du panier B(w, P ). Nous nous intéressons aux problèmes du calcul des réécritures des cas (b.1) et (b.2) du lemme 1. Pour définir plus précisément ces problèmes, nous introduisons l'ensemble V w = {V 1 , ..., V p } qui désigne le sous-ensemble de vues de V telles que ?i ? {1, ..., p},
Le problème E_conj_rewrite(E,w) correspond au calcul des réécritures de type (b.1). Il est défini comme suit :
Le problème E_conj_rewrite(E,w) consiste à calculer les plus petites conjonctions de vues de V w subsumées par ?w.E.
Le problème N_conj_rewrite(n,wR) correspond au calcul des réécritures de type (b.2). Il est defini comme suit :
Le problème N_conj_rewrite(n,wR) consiste à calculer les plus petites conjonctions de vues de V wR subsumées par ?w. ? n R.
L'exemple suivant illustre les solutions des problèmes présentés ci-dessus.
Exemple 3 Soit une requête Q telle que Q ? ?numDepartement.E ?aRecu. ? 3 typeP roduit, avec E = {63, 43, 03}. Supposons qu'il existe 4 vues V i , i ? {1, ..., 4} telles que V i ?numDepartement.E i où E 1 = {23, 15, 18, 80, 43, 03}, E 2 = {03, 63}, E 3 = {01, 07, 11, 43, 63}, E 4 = {26, 63}, et 3 vues V i , i ? {5, 6, 7} telles que V i ?aRecu.typeP roduit.E i pour i ? {5, 6, 7} où E 5 = {P 1 , P 10 , P 15 , P 20 , P 27 }, E 6 = {P 1 , P 10 , P 15 , P 20 , P 26 }, E 7 = {P 1 , P 10 , P 15 , P 26 , P 27 }. 
Ici on a
De la même manière à partir de V aRecu.typeP roduit = {V 5 , V 6 , V 7 }, on obtient la solution de N_conj_rewrite(3,aRecu.typeProduit) : V 5 V 6 V 7 . En effet, l'intersection des ensembles E 5 , E 6 , E 7 donne un ensemble dont la cardinalité est inférieure à 3. Ainsi V 5 V 6 V 7 appartient au panier B(aRecu, (? 3typeP roduit)).
Dans la section qui suit, nous montrons comment les deux problèmes E_conj_rewrite(E,w)
et N_conj_rewrite(n,wR) se rattachent à un cadre de découverte des connaissances dans les bases de données.
Vers la mise en place de techniques de fouille de données
Le cadre de Mannila et Toivonen (1997)
Pour rattacher les problèmes énoncés précédemment à un cadre de découverte de connaissances, nous nous appuyons sur le cadre théorique introduit dans Mannila et Toivonen (1997). Il formalise un problème basique de découverte de connaissances dans des bases de données, qui peut être énoncé de la manière suivante : Soit r une base de données, L un langage pour exprimer des propriétés ou définir des sous-groupes des données, et P un prédicat de sélection. Le prédicat P permet d'évaluer si une phrase X ? L est "intéressante" dans r. L'objectif est de trouver la théorie de r selon L et P , i.e. l'ensemble T h(r, L, P )={ ? ? L |P (r, ?) est vrai}, qui correspond à l'ensemble des phrases intéressantes de r.
Soit une relation de spécialisation/généralisation, i.e. un ordre partiel , sur les motifs de L. On dit que X généralise Y et que Y spécialise X quand X Y . Soit S un ensemble de phrases de L tel que si ? ? S et ? ? alors ? ? S. Alors S peut être représenté par sa bordure positive Bd + (S) ou sa bordure négative Bd ? (S).
La bordure positive correpond aux éléments les plus spécifiques de la théorie, tandis que la bordure négative correspond aux éléments les plus généraux de la théorie. Elles permettent chancune de retrouver toutes les phrases X ? L "intéressantes" dans r, i.e. celles pour lesquelles P (r, X) est vrai. Notons que si le prédicat P de T h(r, L, P ) est anti-monotone par rapport à (i.e. si ?X, Y ? L tels que X Y et P red(r, Y ) est vrai alors P red(X, r) est vrai), alors la théorie peut être représentée par ces bordures. Ce cadre peut être appliqué à de multiples problèmes (Mannila et Toivonen, 1997), comme par exemple le problème de la découverte des motifs fréquents (Agrawal et al., 1993). La section qui suit s'attache à montrer comment le problème de la réécriture peut se ramener à une formulation ensembliste, puis être transposé dans le cadre précédemment introduit.
Formulation des problèmes de réécriture dans un cadre de décou-verte de connaissances
Formulation ensembliste de la réécriture
Pour un mot w, un ensemble E, un entier n donnés, nous cherchons maintenant à donner une formulation ensembliste des problèmes E_conj_rewrite(E,w) et N_conj_rewrite (n,w). Pour reformuler plus précisément ces problèmes, nous introduisons les définitions suivantes :
S 1 (w, E) caractérise les plus petits sous-ensembles de F w dont l'intersection des éléments est contenue dans E, tandis que S 2 (w, n) caractérise les plus petits sous-ensembles de F w dont la cardinalité de l'intersection des éléments est inférieure à n. Le lemme suivant caractérise les solutions de E_conj_rewrite(E,w) et N_conj_rewrite(n,wR) avec les ensembles S 1 (w, E) et S 2 (wR, n).
Lemme 2 Soient n un entier, w et wR des mots et E un ensemble. Soient deux problèmes E_conj_rewrite(E,w) et N_conj_rewrite(n,wR). Soit
La formulation ensembliste du problème N_conj_rewrite(n,wR) est illustrée dans l'exemple qui suit.
Exemple 5 Reprenons l'énoncé de l'exemple 3. On a F aRecu.typeP roduit = {E 5 , E 6 , E 7 }. On a S 2 (aRecu.typeP roduit, 3) = {{E 5 , E 6 , E 7 }}. On retrouve alors la solution de N_conj_rewrite(3,aRecu.typeProduit
Dans la section suivante, nous montrons comment cette représentation ensembliste peut être transposée dans le cadre de Mannila et Toivonen (1997).
Calcul des réécritures dans un cadre de découverte de connaissances
Identification de S 1 (w, E) Dans ce contexte, le premier ensemble S 1 (w, E) peut se ramener au cadre de découverte de connaissances précédent de la manière suivante :
-la relation r est vide.
-le langage L w est l'ensemble des parties de F w , i.e. P(F w ).
-le prédicat, noté P 1 , est défini de la façon suivante : Soient X ? L w , X = {E 1 , ..., E k } et E un ensemble de valeurs.
La théorie T h(?, L w , P 1 ) est alors l'ensemble des éléments de F w qui vérifient le prédicat P 1 . De plus, le prédicat P 1 étant anti-monotone, les notions de bordure positive et négative s'appliquent ici. Le théorème suivant permet de caractériser S 1 (w, E) en fonction de la bordure négative.
Théorème 1 Soit le problème E_conj_rewrite(E,w). S
Nous pouvons de la même manière caractériser S 2 (w, n) dans le cadre théorique de Mannila et Toivonen (1997).
Identification de S 2 (w, n) Comme précédemment, la relation r est vide, L w consiste en l'ensemble des parties de F w = {E 1 , ..., E p }, et la relation d'ordre est l'inclusion. Nous introduisons un nouveau prédicat P 2 (n, X) défini comme suit :
Le prédicat P 2 (n, X) est anti-monotone par rapport à l'inclusion, ce qui garantit l'existence des bordures. Par conséquent, le théorème suivant donne une caractérisation de S 2 (w, n) en fonction de la bordure négative. N_conj_rewrite(n,w).
Théorème 2 Soit le problème
L'exemple qui suit illustre la formulation de S 2 (w, n) dans le cadre introduit ci-dessus.  (Agrawal et Srikant, 1994) pour trouver les solutions de E_conj_rewrite(w,E). Cet algorithme est l'algorithme classique de découverte des motifs fréquents. Il effectue un parcours par niveau de l'espace de recherche, et utilise une stratégie d'élagage à partir de motifs de la bordure négative pour limiter le nombre de motifs générés. Les avantages de cet algorithme pour résoudre notre problème sont multiples. Cet algorithme, tout en recherchant les motifs fréquents, découvre uniquement les motifs de la bordure néga-tive, ce qui n'est pas le cas d'une grande partie des autres approches. De plus, sa stratégie et son efficacité ne dépendent pas du prédicat étudié. A l'opposé, une grande partie des autres algorithmes fondent leur efficacité sur des techniques propres au prédicat "être fréquent". L'efficacité de ce type d'approche pour un autre prédicat est donc difficilement prévisible.
L'implémentation d'Apriori utilisée est une adaptation de l'implémentation C++ de Borgelt (2003). Cette implémentation est reconnue pour être l'implémentation la plus efficace d'Apriori actuellement (Goethals et Javeed Zaki, 2003;Bayardo et al., 2004). L'implémenta-tion initiale d'Apriori a été modifiée de façon à rendre l'algorithme indépendant du prédicat étudié. Plus concrètement, pour pouvoir appliquer Apriori à un nouveau prédicat, il suffit de définir les opérations propres à ce prédicat, et de le passer en paramètre de l'algorithme. Actuellement, en plus de différents prédicats liés aux motifs fréquents, le prédicat P 1 a été implémenté permettant ainsi de trouver les solutions de E_conj_rewrite(w,E) par Apriori. L'avantage de notre implémentation est donc de faciliter l'utilisation d'Apriori pour résoudre d'autres problèmes que ceux de fouille de données, en évitant d'avoir à réécrire à chaque fois l'algorithme.
Expérimentations Nous nous concentrons ici sur l'expérimentation de la phase de résolu-tion de E_conj_rewrite(w,E). Cette phase étant l'une des plus coûteuse, son étude va nous permettre d'estimer le nombre de vues pouvant être traitées. Notons que cette borne correspond au nombre de vues que l'algorithme de réécriture a identifié comme étant pertinentes à la réécriture d'un atome de la forme ?mot.valeurs, i.e. les vues de V mot . Ainsi cette borne conditionne uniquement la taille de l'entrée d'Apriori et ne fait pas figure de limite sur le nombre de vues que l'algorithme de réécriture peut traiter.
Les expérimentations ont été réalisées sur des jeux de données synthétiques. Les jeux d'essais ont été créés à l'aide du générateur aléatoire d'Oracle, de façon à ce que la cardinalité des contraintes de valeurs soit égale à un entier n ou comprise entre 1 et un entier n tel que n ? {10, 20, 30, 40}. On a mesuré les temps d'exécution d'Apriori sur ces jeux d'essais. Ces expérimentations ont été réalisées sur un pentium IV pro 2.6 Ghz avec 3 Go de mémoire.
Comme le montre la figure 1, lorsque la taille des contraintes de valeurs est petite, il est possible de prendre un grand nombre de vues en entrée (e.g. 15000 vues pour des contraintes de taille inférieure à 10). Dès que la taille des contraintes augmente, le nombre de vues que peut traiter l'algorithme, diminue (figure 1). En effet, plus la taille des contraintes est grande, plus les contraintes risquent de s'intersecter et que cette intersection ne soit pas incluse dans E. Par conséquent, le nombre de motifs intéressants est susceptible d'être important et le programmme dans ce cas, nécessite plus d'espace mémoire que disponible. Notons que lorsque les contraintes de valeurs sont de taille fixe (figure gauche de la figure 1), Apriori est mis en difficulté plus rapidement que lorsque les contraintes sont de taille variable. Ceci s'explique par le fait que par exemple pour des contraintes de cardinalité au plus 10, les contraintes sont composées en moyenne de 5 valeurs.
D'une manière générale, ces jeux ont montré qu'il est possible de prendre en entrée pour la réécriture des atomes de la forme ?mot.valeurs, jusqu'à 15000 vues. Néanmoins, il est difficile de comparer les performances de notre prototype avec d'autres applications de réécri-ture de requêtes dans la mesure où dans ce domaine les résultats théoriques ont toujours primé sur les résultats expérimentaux. A notre connaissance, (Pottinger et Halevy, 2001) est une des 
Conclusions et perspectives
Dans cet article, nous avons confirmé l'intérêt des techniques de fouilles de données pour traiter le problème de la réécriture en présence de contraintes de valeurs. En effet dans ALN (O v ), de nouveaux cas de réécritures engendrés par les contraintes peuvent bénéficier d'une formulation dans le cadre de découverte de connaissances de Mannila et Toivonen (1997). De plus, l'implémentation de notre approche basé sur une implémentation générique d'Apriori permet de traiter un grand nombre de vues et d'envisager le passage à l'échelle de notre algorithme de réécriture. Toutefois, l'exécution d'Apriori devient problématique quand une grande partie de l'espace de recherche doit être parcourue, i.e. quand il existe de grands motifs intéressants. Dans de telles configurations, pour le problème de la découverte des fré-quents, des algorithmes ont été proposés afin de trouver plus efficacement les motifs de grande taille (Han et al., 2000;Uno et al., 2003;Flouvat et al., 2004). L'adaptation de certaines de ces approches pourrait donc permettre de traiter un nombre plus important de vues.

Refléter le vocabulaire du métier
Teximus Expertise incorpore un outil interactif qui permet aux experts d'un domaine de travailler en utilisant les concepts clés de leur métier. Le logiciel reflète exactement ces concepts et, plus important encore, leur interrelation.
Dans le domaine de la formation, ce vocabulaire parlera de cours, de module, de matériel, d'exercices, d'évaluations, de suggestions, de demandes de changements, de clientèles cibles, de variantes, de versions, etc.
Dans une application Teximus, chacun de ces concepts est reflété directement dans la base de données.
Environnement interactif de capture
Les caractéristiques d'édition permettent la saisie sur-le-champ et de façon intuitive, tout en garantissant la cohérence de l'information. Pour chaque type de contenu du domaine d'expertise, l'outil définit des fiches de connaissance qui permettent d'entrer l'information. Dès qu'un concept est défini, il est possible de saisir l'information, instantanément.
Teximus Expertise utilise également les fiches de connaissance pour créer des liens hypertextes, de façon intuitive et sans entretien. Si un objet change de nom, tous les liens hypertextes qui s'y réfèrent changent aussi automatiquement, peu importe leur emplacement, qu'ils soient à l'intérieur d'un texte ou d'une image.
Présentation Web instantanée
Teximus Expertise inclut un assistant à la présentation pour la création de pages Web qui permettent de visualiser et d'éditer le contenu. L'assistant permet d'agencer et de cacher les
Teximus Expertise : un logiciel de gestion de connaissances sections, de sélectionner le style des liens hypertextes, et supporte la traduction en plusieurs langues.
Spécifications techniques
Client Web
Les auteurs et les lecteurs peuvent utiliser Internet Explorer ou les fureteurs de la série Mozilla, dont Firefox et Netscape 7. La modélisation de la connaissance et l'édition requiè-rent Internet Explorer v.5.5 ou plus récent. SVG sert à afficher les modèles (logiciel SVG gratuit sur www.adobe.com)
Serveur Web
Un serveur « libre » (open source) est inclus dans la configuration comme serveur Web par défaut. Il s'agit du serveur Resin 3 de Caucho Technologies. En production, Resin peut être utilisé seul ou de concert avec IIS ou Apache.
Bases de données
Teximus Expertise est testé avec les bases de données Oracle 9, DB2 version 7. 
Matériel
L'application est hébergée sur un serveur. Pour les prototypes : Pentium IV 2GHz ou mieux; mémoire : 1GB ou mieux; pour la production, 2GB est recommandé. L'espace disque nécessaire varie selon l'application.
Summary
Teximus Expertise software is an advanced tool for dynamic knowledge management. It is based on a semantic repository. This integrated suite makes easier knowledge and information sharing across organizations.

Introduction
Les données traitées par l'analyse statistique implicative (en abrégé : A.S.I.) se présentent sous forme de tableaux numériques croisant une population E de sujets, ou individus ou objets, associé chacun à une ligne, et un ensemble V de variables simples ou conjointes (attributs binaires, variables numériques, rang, intervalle) chacune associée à une colonne. A l'intersection de la ligne x et de la colonne j figure la valeur prise par le sujet x selon la variable j. La finalité première de l'A.S.I. vise à dégager de V ou de l'ensemble de toutes les conjonctions d'éléments de V 1 , des règles d'association non symétrique, contrairement à la similarité, sur une base statistique, du type : « si la variable ou une conjonction de variables a est observée sur E alors la variable b a tendance à être observée », règle notée a ? b. Une mesure de qualité, non symétrique, de telles règles 2 est définie par :
Q(a,b) est le nombre aléatoire de contre-exemples à l'implication (cf. l'algorithme de la vraisemblance du lien de I.C. Lerman (Lerman, 1981a). Ce critère d'admissibilité est comparable à celui du philosophe des sciences H. Atlan dans « A tort et à raison. Intercritique de la science et du mythe », Seuil, 1986. Il écrit : « … [en accord avec Jung] si la fréquence des coïncidences n'excède pas de façon significative la probabilité qu'on peut leur calculer en les attribuant au seul hasard à l'exclusion de relations causales cachées, nous n'avons certes aucune raison de supposer l'existence de telles relations ». . La distribution de la variable aléatoire Q(a,b) dépend des hypothèses de tirage : par exemple, une loi hypergéométrique ou une loi binomiale, ou une loi de Poisson (Lerman et al., 1981b) -360 -RNTI-E-6 R. Gras et al. 
FIG.1 Graphe implicatif à 7 variables FIG.2 Hiérarchie cohésitive à 7 variables
-ou bien plus ou moins typiques du comportement moyen de la population ; en d'autres termes, le comportement de ces sujets sera ainsi en harmonie avec le comportement statistique de la population à l'origine de la classe C, -ou bien contribuant le plus à la constitution de C ; en d'autres termes, plus ou moins responsables de l'agrégation conduisant à C.
Une approche comparable est faite pour étudier la typicalité et la contribution des sujets et des variables supplémentaires à la constitution d'un arc ou d'un chemin du graphe 4 .
Puissance implicative de classe et de chemin
Couples génériques
L'idée directrice suivie consiste à porter notre attention sur les « lignes de force », (ou, selon une autre métaphore : les « lignes de crête ») des associations, plutôt que de les retenir avec le risque afférent d'être submergé par leur nombre et contraint par les bruits qui les accompagnent. Plaçons-nous à un niveau k de la hiérarchie où viennent de se réunir, pour 
Mais, dans chaque sous-classe de C, existe également un couple générique. Précisément, si C est constituée de g (g?k) sous-classes (C comprise), il y a g couples 4 Le travail présenté ici diffère de celui de (Gras et al., 1996a) par la distinction de ces deux notions. Pour l'étude de la responsabilité du sujet dans la similarité, voir par ex. (Lerman, 1981a). 5 Nous convoquons l'intensité ? mais toute la suite est valable avec l'intensité dite classique ? . 6 C'est ce couple, généralement unique, qui intervient par le sup. dans le calcul de l'implication de A sur B (Gras et al, 1996b  Ainsi, à x, nous pouvons associer g nombres ? x,1, ? x,2,..., ? x,g correspondant aux g valeurs respectivement prises par x selon les g règles génériques de la classe ou du chemin C.
Définition 3 :
Le vecteur ( ? x,1, ? x,2,..., ? x,g) est appelé vecteur contingent générique de x ou puissance implicative de x sur C. Le sujet théorique xt qui admettrait ( ? 1, ? 2,...., ? g) comme vecteur contingent générique est appelé sujet typique optimal En effet, on peut interpréter ce vecteur comme étant celui d'un individu « typique » des règles génériques puisque les valeurs prises par ce sujet selon ces règles sont exactement celles de l'ensemble de la population. Ce sujet, image conforme de E, n'existe pas réellement en général. Dans ces conditions, on peut munir l'espace des puissances [0,1] g d'une métrique afin d'obtenir un contraste accentuant les effets de fortes intensités génériques ou, réciproquement, minorant les effets d'une faible intensité générique.
Définition 4 : On appelle distance de typicalité d'un sujet quelconque x à la classe ou
Dans le logiciel CHIC, le calcul des typicalités (et des contributions) se fait cependant en modulant ces valeurs, à l'aide d'une fonction ad hoc, afin de mieux prendre en compte la sémantique des valeurs attribuées par x à a et à b. Par exemple, pour a=0 et b=1, la fonction prend, dans CHIC, la valeur 0.682.
Ce nombre, qui vérifie formellement les 3 axiomes d'une distance, n'est autre également que la distance du type ? 2 entre les deux distributions {1-? i } i et {1-? x,i } i qui expriment les écarts entre les implications génériques contingentes et l'implication stricte. Elle exprime, aussi et en particulier, l'écart observé sur les règles génériques entre le sujet considéré x et le sujet théorique typique optimal, écart nuancé par ces intensités. C'est pour cette raison que nous avons choisi le mot typicalité pour quantifier le comportement de x selon les règles génériques. Nous allons le préciser plus loin. Lorsque ? i =1, une légère correction sur cette valeur permet d'éviter la division par zéro (par exemple, prendre ? i = 0.99999999) ce qui ne change pas fondamentalement la distance.
Remarque : Une classe C étant donnée, on peut définir une structure d'espace métrique sur E par la donnée de la distance indicée par C entre deux sujets quelconques de E, distance qui mesure la différence de comportement des sujets x et y à l'égard de C :
On voit alors que la distance de typicalité donnée plus haut n'est que la spécification de d C aux sujets respectivement x et x t . La distance d C permet de conférer à E une C-structure topologique discrète. Cette topologie est équivalente à celle qui serait définie sur l'ensemble des vecteurs contingents (? x,1 , ? x,2 ,...,? x,g) , sous-ensemble d'un espace vectoriel normé de dimension g et de norme :
. L'opérateur symétrique associé à la forme quadratique qui conduit à cette distance, a pour matrice la matrice diagonale
-1 pour i=1,…,g -Il est bien évident que la somme de deux tels vecteurs n'a qu'un sens théorique, c'est-à-dire hors du contexte dans lequel nous travaillons en A.S.I..
Une application intéressante peut consister à déterminer le ou les sujets appartenant à une boule de diamètre donné et de centre l'un des sujets pré-désignés, comme par exemple, l'individu optimal. En prolongement de cette approche métrique, le problème de complétion des données manquantes pourrait y puiser une solution originale. 
Typicalité
Nous définirons la mesure de typicalité à partir du rapport entre la distance de typicalité relative au sujet considéré et la distance à C la plus grande dans l'ensemble des sujets. Cette distance maximale est celle des sujets y dont les ? y,i sont tous nuls ou très faibles. Ces sujets sont donc les sujets les plus opposés aux règles génériques. La typicalité d'un sujet sera alors d'autant plus grande qu'il s'écartera de ces mêmes sujets, donc qu'il aura un comportement comparable à celui du sujet théorique optimal. La typicalité d'une catégorie de sujets ou d'une variable supplémentaire G 8 s'en déduira :
Définition 5 : La typicalité de x à C est :
Afin de donner au chercheur le moyen de savoir ou de vérifier rapidement si telle catégorie de sujets qui l'intéresse est statistiquement déterminante dans la constitution d'une classe implicative ou d'un chemin transitif, un algorithme a été élaboré en s'appuyant sur les deux notions que l'on définit ci-dessous : groupe optimal et catégorie déterminante.
Définition 6 : Soit E la population étudiée. Un groupe optimal d'une classe implicative ou d'un chemin C, groupe noté GO(C), est le sous-ensemble de E qui accorde à C une typicalité plus grande que le complémentaire de GO(C) et qui forme avec celui-ci une partition en deux groupes maximisant la variance inter-classe de la série statistique des typicalités individuelles des sujets les constituant. Une telle partition est dite significative. L'existence de ce groupe optimal est démontrée dans (Gras R. et al., 1996a et b). Les propriétés utilisées sont aussi celles qui le sont pour établir l'algorithme sur lequel se basent les modules des programmes informatiques qui construisent, automatiquement dans C.H.I.C., chaque sous-groupe optimal.
En effet, considérons une partition {G i } i de E. Cette partition peut être définie par une variable supplémentaire correspondant par exemple à un descripteur de E à deux ou plus modalités binaires, par exemple des catégories socio-professionnelles. Soit X i une partie aléatoire de E ayant le même cardinal que G i et Z i la variable aléatoire Card (X i ? GO(C)).
Selon un modèle équiprobable, Z i suit une loi binomiale de paramètres : card G i et card (GO(C)) / card E qui est la fréquence du groupe optimal de la classe ou du chemin C. Définition 7 : On appelle variable supplémentaire ou catégorie la plus typique de la classe implicative ou du chemin C, la catégorie qui minimise l'ensemble {p i } i des probabilités p i telles que:
Ainsi, établir que G j est la catégorie la plus typique revient à déceler, parmi les catégories, celle dont le nombre de sujets appartenant en même temps au groupe optimal est le plus étonnamment grand eu égard à son cardinal. Une catégorie G 0 est dite déterminante au risque ou au seuil ? si la probabilité associée p 0 est inférieure à ?. Autrement dit, le risque de se tromper en affirmant cette propriété est donc au plus égale à ?.
Par suite, la signification d'une classe ou d'un chemin ayant été donnée par l'expert, il lui associera la sous-population la plus porteuse de ce sens, celle correspondant au risque minimum. Cette approche est comparable à celle de (Lerman,1981a) pour l'analyse des similarités, mais au moyen d'une modélisation et de concepts différents. D'ailleurs, nous pouvons remarquer qu'il est possible d'associer au groupe optimal une variable binaire correspondant à la fonction indicatrice de ce sous-ensemble de E. De la même façon, nous pouvons également associer à la catégorie G i ou bien à la variable supplémentaire correspondante, une variable binaire dont l'indice de similarité s= n a? b ? n a n b n n a n b n , au sens de I.C. Lerman, vérifie : p i =Pr[S?s], S étant la valeur aléatoire dont s est la réalisation. Ainsi, minimiser l'ensemble des probabilités {p i } i revient à maximiser l'indice de similarité entre les variables binaires, indicatrices de sous-ensembles, associées respectivement l'une au groupe optimal GO(C) et les autres aux différentes catégories {G i } i .
Cette remarque permet d'étendre efficacement la notion de variable supplémentaire la plus typique à des variables numériques, prenant leurs valeurs sur [0,1]. Il suffit alors d'extraire la plus forte des valeurs de similarité entre la variable binaire indicatrice définie par le groupe optimal et les différentes variables numériques placées en supplémentaire, l'indice étant calculé selon le principe retenu en analyse implicative pour les variables numériques. Nous savons que sa restriction au cas binaire coïncide avec sa valeur s dans le cas où les 2 variables sont binaires. = 0,90 sont plus typiques que ceux qui lui attribuent la valeur 0,98. Ceux-ci sont à une distance plus grande que les premiers pour le comportement statistique de la population. La nuance entre cette notion et celle de contribution définie dans 2.3 prend tout son sens dans l'étude des variables modales ou numériques.
Spécificité
Si à chaque classe ou chemin C j on peut associer au moins un groupe typique, il est pertinent de mettre en évidence le couple (variable supplémentaire G i , classe ou chemin C j ) remarquable quant à l'optimalité de sa conjugaison. D'où la définition : Définition 8 : La variable supplémentaire G i étant donnée, le couple (G i , C j ) est dit mutuellement spécifique lorsque G i est la variable la plus spécifique de la règle associée à C j et lorsque la probabilité (le risque) p i k de G i par rapport aux autres classes de la hiérarchie ou aux chemins C k du graphe implicatif est supérieure à un seuil ? (à la discrétion de l'utilisateur).
Une analyse étant donnée, il peut exister 0 ou plusieurs couples mutuellement spécifiques. Ce ou ces couples offrent l'intérêt de faire porter l'attention de l'expert sur les plus fortes associations prenant origine dans une variable supplémentaire. Définition 9 : De la même façon, un individu x étant donné, le couple (x, C j ) est mutuellement spécifique lorsqu'il appartient au groupe optimal relatif à la règle associée à C j et que sa typicalité à C j est maximale par rapport à toutes ses autres typicalités aux classes de la hiérarchie cohésitive ou aux chemins du graphe implicatif.
Contribution
Cette notion se distingue de la précédente, ce que nous ne faisions pas en 1996, par l'examen de la responsabilité des individus, puis des variables supplémentaires -qui peuvent en être des descripteurs-à l'existence d'une règle ou d'une règle généralisée entre variables principales.
Supposons, en effet, que deux variables a et b (resp. plusieurs variables sur un chemin du graphe ou bien deux classes de la hiérarchie) soient réunies par un arc sur un graphe à un certain seuil ( resp. en un chemin transitif C du graphe ou bien en une classe C dans une hiérarchie à un certain niveau). Connaissant la valeur ? x,i attribuée par l'individu à la règle i : a ? b (resp. règle i du chemin C ou bien de la classe C constituée de g règles génériques) supposée admissible, on donne la Définition 10 : On appelle distance de contribution de x à (a,b) ou à C : Remarque : A l'instar de ce que nous avons fait pour la typicalité, nous pouvons définir sur E une topologie discrète d'espace normé dont la norme est associée à la distance entre deux 2 1 g i sujets quelconques suivante :
Définition 11 : On appelle contribution de x à C le nombre :
Cette définition est la restriction de celle de la typicalité au cas où, cette fois, on compare le sujet x aux « pires » sujets par rapport aux règles génériques : leur comportement s'oppose à l'implication de chaque règle (1 pour la prémisse et 0 pour la conclusion). Cette contribution a pour maximum 1 dans le cas où l'individu x a donné la valeur 1 à toutes les règles i. Ceci permet de concilier sémantique et définition formelle. En effet, plus la différence est importante, plus le sujet observé a un comportement voisin de celui du sujet théorique optimal et plus il s'éloigne de ceux qui réfutent les règles génériques : on peut donc dire qu'en contribuant à l'émergence de la classe, ils en sont responsables.
La suite des définitions et des algorithmes de calcul (contribution d'une catégorie ou d'une variable supplémentaire G, groupe optimal d'individus, catégorie ou variable supplémentaire la plus contributive , couple mutuellement spécifique) se transpose immédiatement à partir des principes de la typicalité et la spécificité. Mais dans les situations réelles, nous observons la nuance entre les deux concepts ce qui enrichit l'information exploitable par l'utilisateur. Cependant, le concept de contribution est plus volontiers retenu pour l'interprétation dans une perspective inductive.
Application
Dans le cadre d'une enquête de l'Association des Professeurs de Mathématiques de l'Enseignement Public (APMEP) auprès de professeurs de mathématiques de classes terminales (séries scientifiques S et ES, littéraires LI et technologiques TE sont les variables supplémentaires), nous avons recueilli et analysé (Bodin et al., 1999)   Considérons la classe C= [E ? (OP8 ? OP7)] ? OPX. Son sens, analysé plus en détail dans (Bodin et al., 1999), est fortement marqué par l'importance accordée à l'imagination et à la recherche personnelle, par les enseignants d'accord avec ces objectifs et ces opinions, La variable la plus typique pour cette classe est S (série Scientifique) avec un risque de : 0.00393.
En effet, 116 des enseignants de S parmi les 155 de cette série qui ont répondu au sondage, figurent dans le groupe optimal (GO) de cardinal 201 relatif à C. Soit X une partie aléatoire de même cardinal (155) que S et Z la variable aléatoire égale au cardinal de l'intersection de X et du groupe optimal GO. Selon un modèle équiprobable de distribution des enseignants, Z suit la loi binomiale de paramètres 155 et 201/311 soit 0.656. La probabilité pour que Z soit plus grande que 116 est le risque annoncé, soit 0.00393. Mais pour S, c'est le couple (S, (OP8, OP7)) qui est mutuellement spécifique au seuil ? = 2.10 -5 .
On retrouve une telle mutuelle spécificité pour TE avec le couple (TE, (B,K)) à un seuil 5.10 -7 nous confirmant, sans surprise, que les enseignants des sections techniques (TE) considèrent que les mathématiques doivent être utiles à la vie professionnelle (B) et, en conséquence, aux autres disciplines (K) et y sont les plus attachés.
FIG. 3 -Hiérarchie cohésitive significative
Les calculs de contribution à la classe C montrent que, cette fois, 111 enseignants sur les 311 sondés, participent au groupe optimal. Le nombre d'enseignants de S a diminué (il passe de 116 à 67) et, surtout, sa proportion est bien moindre que précédemment dans le GO. Ceci se ressent dans le seuil qui est 0.0251, soit un risque 6 fois plus élevé que pour la typicalité. Ce sont les enseignants sondés de S qui sont les plus typiques, c'est-à-dire « conformes » au comportement général de la population elle-même sondée. Mais ils sont moins contributeurs dans les relations strictes entre les 4 variables constituant C. Cette remarque nous montre les nuances apportées par les deux concepts : typicalité et contribution Certaines liaisons apparues et commentées ci-dessus se retrouvent dans le graphe de la FIG. 4. Les contributions calculées dans CHIC montrent encore que les enseignants de la série S contribuent le plus au chemin : E ? OP8 ? OP7 ? OPX avec un risque d'erreur de 0.00746, la transitivité le long de ce chemin étant assurée au niveau 0.75.
Conclusion
Les applications de la méthode A.S.I. ont d'ores et déjà donné des résultats très satisfaisants, non seulement dans la discipline où elle a pris naissance, la didactique des mathématiques, mais aussi dans d'autres domaines de l'éducation ou de recherche scientifique différente (biologie, économie,…) comme l'a montré la 3 ème Rencontre Interna nationale ASI 3 de Palerme en octobre dernier. Le plus souvent, les interprétations des experts s'appuient complémentairement sur l'analyse de similarités ou/et sur les méthodes factorielles, tout en obtenant des informations qui sont spécifiques de l'A.S.I. en raison de son caractère non symétrique. Mais ces méthodes visent un objectif commun : l'accès à la signification d'un tout non réduit à la somme des significations de la somme de ses parties. Les analyses bénéficient efficacement du logiciel C.H.I.C., qui permet, avec une certaine convivialité, tous les traitements algorithmiques et graphiques des questions évoquées dans cet article. Son développement suit régulièrement toutes les nouvelles avancées de la théorie de l'implication statistique. Ses fonctions respectives de révélateur et d'analyseur qui semblent opérer avec bonheur dans de multiples domaines nous promettent encore d'intéressantes perspectives théoriques et appliquées.

Problématique
Le raisonnement à base de règles générales pouvant comporter différentes exceptions et le raisonnement non-monotone sont des domaines qui ont été bien étudiés et formalisés en Intelligence Artificielle. Ainsi, le Système P (Kraus et al., 1990) fournit un ensemble de postulats de rationalité permettant de définir les conclusions plausibles pouvant être obtenues à partir d'un ensemble de règles pouvant contenir des exceptions. De plus, différentes méthodes de raisonnement, en accord avec le Système P , ont été proposées. Une question cependant subsistait : comment obtenir de telles règles à partir d'informations fréquentielles, en d'autres termes, comment apprendre de telles règles ?
De récents travaux ont montré comment se baser sur des distributions de probabilités particulières, les distributions de probabilités à grandes marches (Snow, 1999), afin d'obtenir des règles et leurs exceptions. Dans une distributions de probabilités à grandes marches, chaque élément à une probabilité supérieure à la somme des probabilités des évènements qui lui sont moins probables. Contrairement aux approches classiques basées sur les règles associatives, les règles ainsi extraites peuvent être utilisées dans le cadre du raisonnement non-monotone, en accord avec le Système P et avec la base initiale (Benferhat et al., 2003).
Cependant, ces distributions de probabilités à grandes marches ne peuvent être obtenues qu'en regroupant les différents individus de la base (simple) d'apprentissage, chaque regroupement pouvant aboutir à des ensembles de règles différents et incompatibles. À ce jour, aucun algorithme de regroupement réellement satisfaisant, tant d'un point de vue de temps de calcul que des règles générées, n'avait été proposé.
Les contributions de ce travail se déclinent en deux points principaux : -la proposition de différents algorithmes de regroupement d'où des règles peuvent être générées ; -l'implémentation de ces différents algorithmes afin de les valider (tant sur les temps de calcul que sur la qualité des règles extraites).
L'extraction des règles depuis la base d'observations se décompose en 3 phases bien distinctes : une phase de dénombrement de chaque observation ; une phase de regroupement de ces observations afin d'obtenir des distributions à grandes marches ; une phase d'extraction des règles à partir du regroupement précédent.
La phase de regroupement est une phase déterminante pour la génération de règles. En effet, l'unique moyen d'influer sur la qualité et la pertinence des règles apprises se situe donc dans la construction des différentes classes de la base initiale. Or, il est impossible de calculer tous les regroupements possibles et de choisir le meilleur, le nombre de ces regroupements étant exponentiel.
Le logiciel. Le logiciel en démonstration et développé par les auteurs, Area, comporte différents algorithmes reposant sur différents facteurs, le but de chacun de ces algorithmes étant de tendre vers un regroupement optimal permettant de générer le meilleur ensemble de règles possible. Le logiciel offre également la possibilité de modifier les regroupements générés par les algorithmes, au moyen d'outils de manipulation des regroupements permettant à un utilisateur de déplacer les observations. Ces fonctions permettent d'affiner les règles apprises par le système.
Il peut être noté que la robustesse d'une règle dépend du regroupement dont elle est issue : plus la règle provient d'un regroupement avec une population élevée (ie. plus elle est générique) moins elle sera sensible à l'ajout de nouveaux éléments dans la base. 
Summary
Area is a Java software which purpose is to extract default rules from simple databases. The main property of these rules is to be compatible with the System P of Kraus, Lehmann and Magidor. Hence, these rules can be used with non-monotonic reasoning systems.

Introduction
Avec l'expansion d'Internet et du Web, on assiste à une prolifération des ressources hété-rogènes (données structurées, documents textuels, composants logiciels, images), conduisant à des volumes considérables. Dans ce contexte les outils d'accès à l'information (moteurs Web, SGBD, etc.) délivrent, dans des temps de plus en plus longs, des résultats massifs en réponse aux requêtes des utilisateurs, générant ainsi une surcharge informationnelle dans laquelle il est souvent difficile de distinguer l'information pertinente d'une information secondaire, ou même du bruit.
Une solution à l'amélioration de cette pertinence est la personnalisation ou l'adaptation des réponses fournies aux utilisateurs selon leurs profils c'est-à-dire selon leurs besoins et leurs préférences 1 . Ainsi la formulation du besoin d'information est devenue un des éléments clés pour obtenir des résultats pertinents dans un processus d'accès à l'information. Pour aider à cette formulation, des travaux Bouzeghoub (2004), Zhu (2000) et Burgess (2002) proposent d'introduire la notion de qualité. Il est par exemple possible de poser une requête en spécifiant des préférences extrinsèques en termes de qualité comme une réponse rapide ou une information fraîche. Ainsi on peut définir un profil qualité comme un ensemble de préférences ou besoins en termes de qualité d'information caractérisant un utilisateur ou groupe d'utilisateurs.
Dans cet article nous proposons un modèle flexible de qualité de l'information décrivant les différents facteurs de qualité influant sur la personnalisation. Ce modèle va permettre de structurer les différents facteurs de qualité dans une hiérarchie afin d'assister l'utilisateur dans la construction de son propre profil selon ses besoins et exigences en terme de qualité.
Dans la section suivante, nous présentons un état de l'art sur les approches existantes sur la modélisation de la qualité des données. La section 3 sera consacrée à la présentation de notre modèle de qualité d'information. Enfin nous terminerons cet article par des conclusions et des perspectives.
Personnalisation et qualité d'information
La personnalisation de l'information s'exprime par un ensemble de critères et de préfé-rences spécifiques à chaque utilisateur ou une communauté d'utilisateurs. Les données décri-vant les préférences des utilisateurs sont souvent sauvegardées sous forme de profils. Parmi les données du profil on trouve une dimension relative à la qualité Bouzeghoub (2004). Afin de définir les facteurs de qualité relatifs à l'information influant sur la personnalisation, il est nécessaire d'analyser les différents travaux menés sur le thème de la modélisation de la qualité des données.
Modélisation de la qualité des données. La qualité des données est un domaine de recherche qui a suscité depuis longtemps un vif intérêt, mais qui émerge tout juste comme champ de recherche à part entière, tel que peuvent l'indiquer Wang (1997), Jarke (1997) et Berti (1999). Dans le cadre de la modélisation de la qualité des données, de nombreuses propositions ont été faites. La première difficulté réside dans l'absence de consensus sur la notion même de qualité. Tout le monde s'accorde en effet sur le fait que la qualité des données peut se décomposer en un certain nombre de dimensions, catégories, critères, facteurs, paramètres ou attributs, mais aucune définition ne fait aujourd'hui l'unanimité (TAB. 1). Dans Naumann (2000) les auteurs identifient trois approches d'analyse des critères de la qualité des données :
? approche orientée sémantique : elle est basée uniquement sur la signification des critères. Cette approche est la plus intuitive (il s'agit d'une approche où les critères sont examinés de façon générale, c'est-à-dire séparés de tout cadre d'information). ? approche orientée traitement : elle classe les critères de qualité de l'information selon leur déploiement dans les différentes phases du traitement de l'information. ? approche orientée objectif : elle est caractérisée par une définition des objectifs de la qualité à atteindre et un classement des critères selon les objectifs définis.
Limites des modèles existants. L'inconvénient des approches proposées pour caractériser la qualité des données semble être une certaine rigidité qui parait ne laisser que relativement peu de choix à l'utilisateur, sans pour autant l'aider à construire un ensemble cohérent et minimal de critères de qualité ou bien l'assister dans leur spécification. En effet elles repré-sentent la qualité comme une collection de critères. La plupart des approches proposées sont limitées dans leur applicabilité. Elles sont utiles seulement dans le domaine pour lequel elles ont été conçues ainsi la réutilisation de la définition de la qualité est limitée. La majorité des définitions proposées de la qualité des données ne distinguent pas le point de vue utilisateur et le point de vue système. Par exemple pour la fraîcheur des données on distingue la fraîcheur comme un point de vue utilisateur et la fréquence de mise à jour des données comme un point de vue système. Cette confusion rend difficile l'intégration de la qualité dans le processus d'exécution des requêtes.  (2000) »Approche orientée traitement » 3 Classes d'évaluation des critères » 11 Critères qualité de données Zhu et Gauch (2000) »Approche orientée sémantique » 5 Critères de qualité des pages web Marotta (2002) »Approche orientée traitement » 2 points de vue : système et utilisateur » 6 Catégories » 31 Critères TAB. 1 -Quelques approches de modélisation de la qualité des données.
Proposition d'un modèle de qualité de l'information
Objectifs du modèle
L'objectif de notre modèle est de fournir une définition des facteurs de qualité de l'information, afin de permettre à l'utilisateur de construire son propre profil de qualité et d'avoir ainsi une personnalisation au niveau de la définition et de l'évaluation de la qualité. Dans notre modèle la définition des facteurs de qualité influant sur la personnalisation de l'information repose principalement sur l'hypothèse suivante :
Hypothèse : la définition de la qualité de l'information est relative à l'utilisateur.
La définition de la qualité est propre à l'utilisateur c'est-à-dire elle est relative à la satisfaction de ses besoins en termes de choix et d'appréciation des facteurs de la qualité.
Approche multidimensionnelle pour la qualité
La définition des facteurs de qualité influant sur la personnalisation de l'information ne réside pas dans la définition des facteurs de qualité elle-même mais dans la structuration et la représentation de la qualité. En se basant sur notre hypothèse, notre hiérarchie de qualité se décompose en un ensemble de dimensions (FIG. 1). Dans la suite nous proposons les diffé-rentes dimensions de la hiérarchie de qualité de l'information.
Dimensions source
Ce type de dimension décrit la source ou la provenance de la qualité comme source d'information ou support d'information. Elle se décompose en une ou plusieurs dimensions utilisateur ou système. On part du constat que s'il est difficile de garantir la qualité intrinsè-que de l'information on peut déterminer a priori les sources de qualité :
? support de l'information : les facteurs de qualité liés aux documents. ? Source de l'information : les facteurs de qualité liés aux fournisseurs de l'information (Base de données, Site Web, Bibliothèque numérique...). ? usage de l'information: les facteurs de qualité liés à l'usage des informations comme par exemple les formes de popularité (citation).
Dimensions système
Les dimensions système décrivent l'ensemble des critères de qualité vis-à-vis du système. En se basant sur le modèle de Naumann et Rolker (1999) nous proposons l'ensemble de critères préliminaires de la qualité (FIG. 1).
Dimensions utilisateur
Les dimensions utilisateurs sont des dimensions d'agrégation personnalisables par l'utilisateur. Elles se décomposent en une ou plusieurs dimensions utilisateurs ou système. En s'inspirant de la catégorisation de la qualité de Marotta (2002) nous proposons les principales dimensions utilisateur suivantes :
-la qualité opérationnelle : l'ensemble des facteurs de qualité liés à l'accès à la source d'information ou support d'information. -la qualité du contenu : l'ensemble des facteurs de qualité liés à la source d'information ou support d'information elle -même. -la qualité opérationnelle de l'usage : les diverses formes de popularité liés à l'accès à l'information comme téléchargement ou liens. -la qualité du contenu de l'usage : les diverses formes de popularité liés à l'appréciation du contenu de l'information comme citation.
En raison du nombre de dimensions système disponibles dans notre modèle on a besoin d'une simple hiérarchie permettant à l'utilisateur de trouver facilement les dimensions système souhaitées d'où la proposition des sous-dimensions utilisateur suivantes :
- 
Conclusion et perspectives
Dans ce travail, nous avons présenté un modèle flexible de qualité de l'information. La multi-dimensionnalité de la hiérarchie de la qualité proposée permet à l'utilisateur d'obtenir différents points de vue selon différentes dimensions et selon différents niveaux de « curiosité » personnalisables vis-à-vis de la qualité d'information. En termes de perspectives à notre travail nous comptons : établir les métriques et les méthodes d'évaluation des différentes dimensions de qualité ; proposer un modèle formel de représentation et construction d'un profil qualité. 
Références
Summary
This work is included in the general problems of information retrieval and more particularly in personalization and quality of information. In this paper we propose a multidimensional model of information quality describing various quality factors influencing the information personalization. This model makes it possible to structure the various information quality factors in a hierarchy in order to assist the user in the construction of his own profile according to its requirements in term of quality. Résumé. Cet article présente un système automatique d'annotation sémantique de pages web. Les systèmes d'annotation automatique existants sont essentiellement syntaxiques, même lorsque les travaux visent à produire une annotation sémantique. La prise en compte d'informations sémantiques sur le domaine pour l'annotation d'un élément dans une page web à partir d'une ontologie suppose d'aborder conjointement deux problèmes : (1) l'identification de la structure syntaxique caractérisant cet élément dans la page web et (2) l'identification du concept le plus spécifique (en termes de subsumption) dans l'ontologie dont l'instance sera utilisée pour annoter cet élément. Notre démarche repose sur la mise en oeuvre d'une technique d'apprentissage issue initialement des wrappers que nous avons articulée avec des raisonnements exploitant la structure formelle de l'ontologie.
Annotation sémantique de pages web
Le système que nous présentons permet d'automatiser l'annotation sémantique de pages web. Notre objectif est de classifier des pages concernant des équipes de recherche, afin de pouvoir déterminer par exemple qui travaille où, sur quoi et avec qui. La classification s'appuie sur des mécanismes de raisonnement qui nécessitent une représentation formelle du contenu des pages ; nous exploitons ainsi une ontologie qui représente les concepts du domaine et les relations entre les concepts dans un langage de représentation des connaissances.
Notre système génère des annotations sémantiques qui sont des métadonnées sur les élé-ments d'un document liées à une ontologie. Pour cela nous devons résoudre deux grandes questions. La première est d'identifier automatiquement, dans une page web, les éléments qui sont pertinents. La seconde est de déterminer quels sont les concepts de l'ontologie les plus spécifiques possible, pour annoter chacun de ces éléments.
L'automatisation repose sur un apprentissage à partir d'un corpus constitué d'éléments marqués par un expert. Le marquage associe à chaque concept de l'ontologie des éléments de la page en rapport avec ce concept. L'apprentissage génère un wrapper capable d'annoter des éléments du document sous la forme d'instances de concepts et de rôles de l'ontologie fournie. Des mécanismes de raisonnement exploitant l'ontologie sont utilisés pour déterminer
FIG. 1 -page web et ontologie présentées à l'expert
le concept le plus spécifique avec lequel un élément doit être annoté. L'annotation est donc totalement dépendante de l'ontologie fournie.
Dans une première section, nous présentons le processus de marquage de la page par un expert. La seconde section présente l'algorithme d'apprentissage exploitant la structure arborescente d'une page web. La section 3 présente l'annotation de documents dont la structure est similaire. Enfin, nous évaluons notre méthode par rapport aux systèmes d'annotation séman-tique existants.
Génération d'annotations primaires par marquage
La première étape du processus est un marquage permettant de former un corpus d'apprentissage ; il s'agit de fournir au système quelques exemples d'éléments pertinents à partir desquels le système apprend à reconnaître l'ensemble des éléments à annoter. Pour cela, un expert marque des éléments pertinents de la page web, c'est-à-dire correspondant à des concepts de l'ontologie. Il dispose à cet effet d'un outil de visualisation, à la manière d'un navigateur web, qui lui permet de sélectionner un élément dans la page et de choisir dans l'ontologie le concept qui lui correspond. Dans l'exemple figure 1, le marquage est effectué en fonction de l'ontologie SWRC 1 , qui modélise notamment les personnes, organismes et projets d'une équipe de recherche. Pour chaque concept, un nombre suffisant d'éléments pouvant y être associé doivent être marqués ; ce nombre dépend de la régularité de la page d'apprentissage et des pages à annoter ; pour des pages très régulières, 2 ou 3 exemples suffisent pour chaque concept.
De manière interne, la page est représentée par son arbre DOM (W3C) dans lequel les noeuds contiennent les éléments de structure HTML et les feuilles les éléments de texte. Un chemin unique est ainsi défini depuis la racine jusqu'à chaque feuille. Lorsque l'expert marque
FIG. 2 -Arbre DOM et annotations primaires issues du marquage
un élément la chaîne de caractères sélectionnée, le chemin de la feuille contenant la chaîne et le concept de l'ontologie associé sont enregistrés au format XML ; la figure 2 présente un exemple d'enregistrement du marquage de la page web de la figure 1. L'ensemble de ces éléments marqués sont des annotations primaires qui jouent ainsi le rôle de corpus pour l'algorithme d'apprentissage.
2 Apprentissage exploitant une structure arborescente Définition d'un chemin dans l'arbre issu du DOM L'algorithme d'apprentissage est dérivé des travaux de Kushmerick et al. (1997) sur l'induction de wrappers. Un wrapper est une procédure utilisant les régularités syntaxiques d'un document pour identifier des éléments. Là où les travaux initiaux s'appuyaient sur des structures à plat, en considérant le document comme une suite de chaînes de caractères, notre système exploite la structure arborescente fournie par la représentation DOM de la page web.
Le DOM permet de définir le chemin de chaque élément (noeud ou feuille) de l'arbre. Pour chaque élément, nous définissons ce chemin comme un ensemble d'étapes depuis la racine. Chaque étape est un couple (balise :position) défini à partir de l'étape précédente (on considère l'étape 0 comme étant la racine du document). La position est le numéro du fils du noeud défini à l'étape précédente tandis que la balise est la balise HTML que le noeud représente. Par exemple, une page web contient un élément racine <html> qui a deux fils, <head> et <body>. Le chemin de l'élément <body> est donc body : 1. Cette définition de chemin est celle employée pour les annotations primaires présentées figure 2.
A partir de cette définition du chemin d'un élément de l'arbre, on définit la notion de chemin similarisé. Un chemin similarisé est la factorisation des chemins de plusieurs élé-ments. Le chemin ainsi généré est ainsi un chemin de plusieurs éléments. Pour cela, les étapes sont comparées 2 à 2 et les différences marquées par une astérisque. Prenons l'exemple des deux premières annotations primaires présentées figure 2. Le chemin du premier élément est body : 1,table :0,tbody :0,tr :0,td :0,b  -chaque instance de concept est exactement une feuille de l'arbre, -les instances de rôles sont contenues dans des sous-arbres De ces hypothèses, on déduit qu'identifier une instance de l'ontologie revient à déterminer le chemin depuis la racine vers une feuille de l'arbre pour une instance de concept et vers un noeud, racine du sous-arbre, pour une instance de rôle. L'apprentissage consiste donc à déterminer un chemin similarisé pour chaque concept et chaque rôle de l'ontologie.
Apprentissage de chemins similarisés Pour chaque concept dont des exemples ont été marqués par l'expert, le chemin similarisé du concept est généré à partir de l'ensemble des chemins enregistrés dans les annotations primaires pour ce concept. Dans l'exemple figure 1, 5 annotations primaires sont définies pour le concept Project. En factorisant les chemins deux à deux, le chemin similarisé obtenu est body : 2, table : * , tbody : 0, tr : * , td : 1, a : 0, f ont : 0. Il ressort ainsi que les éléments correspondant aux concept Project sont situés dans la deuxième colonne des tableaux du document.
Pour les instances de rôles, une première étape consiste à déterminer les racines des sousarbres de chaque rôle tel que :
-il existe un rôle R A,B dans l'ontologie reliant des concepts A et B, -au moins une instance de A et une instance de B ont été marquées. Alors pour chaque instance marquée de A :
-le plus petit parent commun (pppc) dans l'arbre de cette instance avec chaque instance de B est déterminé, -le noeud le plus profond dans l'arbre parmi ces pppc est alors un noeud racine pour le rôle R A,B . Le chemin similarisé des noeuds racines générés est alors inféré. La sortie de l'apprentissage est donc un chemin similarisé de chaque concept et de chaque rôle de l'ontologie ayant des instances marquées dans le document.
Annotation par génération d'instances de l'ontologie
Annotation par application des chemins similarisés Les chemins similarisés sont appliqués sur une page dont la structure DOM est similaire à la page d'apprentissage. Les noeuds reconnus par le chemin similarisé appris pour chaque rôle R A,B sont les racines des sousarbres en dessous desquels chaque instance de a est liée à une instance de b par une instance de R A,B . Les feuilles reconnues par le chemin similarisé d'un concept sont des candidates pour être instanciées par ce concept. Deux cas sont possibles : si une feuille n'est reconnue que par un seul chemin similarisé, cette feuille est instanciée par le concept correspondant à ce chemin. Pour toutes les feuilles situées dans un sous-arbre, une relation est générée entre les instances de concepts définies par le rôle. Si plusieurs chemins similarisés conduisent à la même feuille, un mécanisme de raisonnement doit être appliqué pour déterminer à quel concept cette feuille appartient. On atteint les limites d'une méthode purement syntaxique.
Dans notre exemple, le chemin similarisé du concept Project décrit ainsi le fait qu'il est associé aux éléments contenus dans la colonne de droite des tableaux tandis que les concepts Lecturer et FacultyMember sont associés au contenu de la colonne de gauche.
Annotation par un concept plus général dans l'ontologie Lorsqu'un même élément peut être annoté par deux concepts différents, un raisonnement est effectué au niveau de l'ontologie pour déterminer le concept subsumant les deux concepts candidats. Dans notre exemple, le raisonneur Pellet (Sirin et Parsia (2004) évalue les performances de Pellet pour la classification et les requêtes) est utilisé pour classifier les concepts de l'ontologie et déterminer le concept subsumant Lecturer et FacultyMember dans SW RC. Il s'agit de AcademicStaff. Une instance de ce concept sera donc générée pour les éléments reconnus par les chemins similarisés appris à partir des concepts Lecturer ou FacultyMember.

Présentation
Dans le cadre du projet RIAM 1 « Relaxmultimédia » mené conjointement avec deux agences de presse (AFP et Relaxnews) nous présentons une approche destinée à gérer deux aspects d'un modèle métier défini avec UML : son extensibilité et la possibilité de naviguer entre les classes et les instances définies à partir de ce modèle. Nous montrons que la transformation du modèle UML en un schéma RDF sur lequel est utilisable SeRQL, un langage d'interrogation, présente des caractéristiques intéressantes pour gérer de tels aspects.  Pour conclure, nous soulignerons que l'utilisation d'UML permet de définir précisément le modèle de base et les points d'extension grâce à l'utilisation de stéréotypes. Il n'offre cependant pas de possibilités sur l'interrogation du modèle. Un modèle fondé sur RDFS, associé au langage SeRQL, permet cette interrogation à la fois sur les classes et les instances ce qui est utile pour permettre une navigation conjointe dans le modèle et dans les données.
Extensibilité et navigation dans le modèle métier

Introduction
Dans le domaine des sports en équipe, de plus en plus d'entraîneurs font appel à des outils informatiques durant leur activité pédagogique, en particulier de logiciels de simulation afin d'enseigner aux joueurs à améliorer leur tactique. Jusqu'à présent, ces logiciels qui permettaient essentiellement à l'entraîneur de faire se déplacer sur un écran des agents joueurs, nécessitaient de sa part de spécifier quasiment trame par trame la position des agents. Par voie de fait, un entraîneur souhaitant montrer le déploiement d'un schéma tactique particulier doit effectuer un important travail avant que la simulation puisse être lancée.
Dès lors, rendre les agents plus autonomes, améliorer le réalisme de leur comportement et leur capacité de prendre des décision allégerait le travail de l'entraîneur, et lui permettrait de n'avoir qu'à spécifier des schémas tactiques relativement abstrait pour voir comment des agents joueurs déploieraient ce schéma « intelligemment » sur le terrain.
Notre objectif est donc d'utiliser diverses techniques d'intelligence artificielle pour amé-liorer l'autonomie des agents devant déployer un schéma spécifié par l'entraîneur. Cette tâche peut être considéré comme un sous-ensemble du problème de la simulation sportive (par exemple la RoboCup), du fait que les agents se voient indiqués la route à suivre (le schéma tactique), mais doivent pouvoir en dévier s'ils croisent un adversaire qui leur prend la balle.
Dans un premier temps, un système multi-agents a été construit, dans lequel les agents suivent un comportement décrit dans une base de règle. Nous avons montré sur quelques schémas que les comportements obtenus étaient parfois insuffisants, du fait que les agents ne s'autorisaient pas à dévier suffisamment des indications de l'entraîneur pour faire face à l'adversaire. Ensuite, nous avons implémenté un algorithme d'apprentissage par renforcement qui a permis aux agents de se comporter correctement dans les cas ou ils échouaient avec le système à base de règles. Enfin, nous avons créé une plateforme logicielle intégrant ces différents algorithmes et permettant à l'entraîneur de faire des simulations aisément.
Dans ce papier, après avoir introduit les schémas tactiques, nous présentons essentiellement les résultats des expérimentations de l'algorithme d'apprentissage par renforcement, que nous comparons sur différents schémas aux performances des agents basés sur des rè-gles. Nous montrons en particulier que l'apprentissage converge rapidement malgré la dimension importante du problème.
La simulation de schémas tactiques
En regardant l'état de l'art du sujet en question nous avons trouvé deux axes principaux : les outils commerciaux et les travaux scientifiques.
Les représentants du premier axe sont destinés aux professionnels du football et traitent effectivement les schémas tactiques, mais ils proposent une solution simple pour le déploie-ment d'un schéma tactique. En fait, ils ne font qu'une animation d'une séquence de positions prédéfinies en utilisant des techniques d'interpolation d'images où les objets ont une trajectoire rectiligne entre deux positions successives. Le résultat final est donc une animation dont la qualité dépend de l'intervalle entre chaque image. Plus l'intervalle augmente plus la fiabilité diminue. Cette approche laisse tout le travail fastidieux et répétitif à l'utilisateur du programme qui doit prévoir et décrire en détail le déplacement des objets à chaque instant. En particulier si un changement se présente ce travail est à refaire.
La RoboCup est le plus grand représentant du deuxième axe. Il s'agit d'un projet de coopération international destiné à encourager le développement de l'intelligence artificielle (IA), de la robotique et d'autres domaines connexes. Du point de vue des systèmes multiagents (SMA) le modèle footballistique crée par la RoboCup est un défi intéressant car il regroupe plusieurs caractéristiques (Noda et al., 1997), dont un environnement qui évolue dynamiquement, une nécessité pour les agents de communiquer et se coordonner pour atteindre leurs objectifs.
Actuellement, il n'existe pas encore d'équipe utilisant le déploiement de schémas tactiques tel quel le font les joueurs de football. Les équipes ont déjà suffisamment de problèmes à résoudre avec les contraintes et les définitions imposées par le modèle en question.
Nous proposons une solution qui s'inscrit dans le premier axe (pour les professionnels du sport) pour la simulation de schémas tactiques basé sur les SMA comme la RoboCup mais sans toutes ses contraintes et déterminations. Nous avons opté pour l'utilisation des méthodes d'apprentissage automatique pour la conception des agents, évitant ainsi la tâche complexe de programmer les comportements des joueurs. Dotés de la faculté d'apprendre, les agents gagneront en autonomie, et seront capables de s'adapter à leur adversaire ou à leur environnement.
En entraînant des agents à jouer avec certains schémas tactiques, l'entraîneur pourra dé-velopper chez ces agents des aptitudes particulières liées à ces schémas. Par exemple : il pourra créer un défenseur en mettant un agent joueur face à des attaquants avec le ballon, avec pour objectif apprendre à récupérer le ballon.
Algorithme d'apprentissage
Nous avons choisi l'apprentissage par renforcement parce que l'agent apprend par interaction avec l'environnement sans avoir besoin d'exemples. Dans les sections suivantes, nous définirons les récompenses et l'agent devra découvrir par un processus d'essais et d'erreurs, l'action optimale à effectuer pour chacune des situations afin de maximiser ses récompenses (Sutton, 1998). Pour modéliser un problème en utilisant de l'apprentissage par renforcement, on doit se poser les questions suivantes : quelles actions peuvent être effectuées par les agents ? Quelle représentation de l'environnement employer ? Quelles récompenses leur donner ?
Les espaces d'actions et d'état
En ce qui concerne l'espace d'état, nous avons choisi une représentation basée sur des distances, afin que les comportements des agents ne dépendent que des positions relatives des uns par rapport aux autres, et non de leur position absolue sur le terrain (voir le TAB. 1). 
TAB. 1 -Description des caractéristiques des états
En utilisant comme base les comportements de navigations primaires décrites par Reynolds (1999), nous avons créé les actions suivantes :
-Déplacement vers un point : elle combine les comportements seek et unaligned collision avoidance afin d'aller vers un point en évitant les collisions ; -Déplacement en groupe vers un point : Cette action permet au joueur de se dépla-cer tout en restant proche du groupe, grâce au comportement cohesion. -Positionnement : inspiré de l'action se positionner de Veloso et al. (1999)  Stone et McAllester (2001) pour trouver le temps nécessaire à l'interception du ballon. Avec cette information l'agent peut savoir vers où il doit aller pour attraper le ballon. Il y a d'autres actions également importantes, mais qui n'utilisent pas les comportements de navigation, on peut lister : faire une passe et prendre le contrôle du ballon.
Le renforcement
Pour définir les récompenses d'une manière plus facile nous avons déterminé un ordre de priorités. Tout d'abord il faut réaliser le schéma tactique (arriver aux objectifs avec les conditions satisfaites), ensuite ne pas perdre le ballon et enfin ne pas mettre le ballon hors du terrain. D'après les priorités définies, nous avons attribué les valeurs du TAB. 2 comme récom-pense. 
Fonction d'évaluation
Nous avons créé une fonction pour évaluer la situation actuelle d'une équipe par rapport à la réalisation d'un schéma tactique. Cette fonction est donc la somme des évaluations individuelles de tous les joueurs de l'équipe.
Pour évaluer individuellement un joueur j nous prenons en compte le nombre d'objectifs qui ont été déjà réussis (noté or i ), la distance normalisée (entre 0 et 1) au prochain objectif o (notée dn oj ), et nous calculons la valeur e( j) = (or j + dn oj ) no j , où no j est le nombre total d'objectifs du joueur j. Dès lors, pour évaluer une équipe t, nous définissons la fonction s(t) égale à la moyenne des évaluations des joueurs de l'équipe.
!
Expérimentation et résultats
Pour interagir avec l'entraîneur, réaliser nos expérimentations et voir les résultats, nous avons réalisé une plateforme comportant différents modules dont : un module d'interaction avec l'entraineur, un module d'apprentissage par renforcement. En plus des agents basés sur l'apprentissage par renforcement, nous avons implémenté un autre type d'agents dont le comportement dérive d'un système à base de connaissances (SBC), ce qui constitue une approche plus classique. Ainsi, en comparant les deux approches, nous pouvons mesurer le gain apporté par un algorithme d'apprentissage. Pendant nos expérimentations, nous avons utilisé un CMAC avec 32 tableaux de 9 cases (pour les paramètres continus et 1, 2 ou 3 cases pour les valeurs discrètes). Les configurations utilisées sont résumées par le TAB. 3.
-146 -RNTI-E-6 Le graphe qui montre l'évolution de l'apprentissage de nos agents selon la quantité d'épisodes (axe des abscisses) et notre fonction d'évaluation (axe des ordonnées) pour la configuration a est présentés par la Fig. 3. -147 -RNTI-E-6
Pour toutes les configurations, nous avons un résultat final similaire, montré par la Fig. 2, où l'agent qui n'a pas d'objectif apprend qu'il faut chercher le ballon (sinon l'équipe adverse va le prendre) pour ensuite faire une passe à son coéquipier pour pouvoir accomplir le sché-ma tactique donné.
Conclusion et travaux futurs
Le travail présenté ici montre comment une approche SMA adaptative pour la simulation de schémas tactiques peut être mise en oeuvre, quel type de résultats on peut en attendre et quels sont les apports vis-à-vis des autres solutions existantes.
Nous allons poursuivre la recherche en variant les composants de l'architecture du SMA et les composants de la méthode d'apprentissage (e.g. types d'agents, de mémoire, etc.). Nous nous intéressons notamment à l'emploi de techniques pour améliorer l'évolution et la coordination de l'apprentissage. De plus nous étudions l'utilisation de l'apprentissage par imitation pour apprendre à partir de séquences vidéo numérisées, en accélérant donc l'apprentissage.
A travers ce travail, nous espérons ouvrir une voie nouvelle dans les approches de la simulation numérique dans le milieu de tactiques sportives et contribuer à la conception de nouveaux outils d'aide aux entraîneurs et autres professionnels du sport.

Introduction
Le transit des flux d'information dans le réseau Internet à l'échelle mondiale est régi par des accords commerciaux entre systèmes autonomes. La négociation de ces accords commerciaux repose implicitement sur une hiérarchie des systèmes autonomes et la position relative de deux systèmes débouche sur un accord de type client/fournisseur (un des systè-mes, le client, est nettement mieux classé que l'autre, le fournisseur, et le client paye le fournisseur pour le transit des flux d'information) ou sur un accord de type "peering" (transit gratuit du trafic entre les deux systèmes).
Les politiques de routage déduites de ces accords commerciaux sont ensuite mises en oeuvre via le protocole de routage BGP (Border Gateway Protocol). Ainsi, l'établissement des routes à l'échelle mondiale obéit à des règles d'efficacité économique déduites d'une hiérar-chisation entre systèmes autonomes (une route ne peut pas, par exemple, "descendre" d'un fournisseur à son client pour "remonter" vers un autre fournisseur : quel client accepterait de payer pour porter les trafic de ses fournisseurs ?), règles bien différentes des règles d'ingénie-rie qui régissent le routage à l'intérieur des systèmes autonomes (Griffin et al (2002), Gao et Wang (2002)).
En dépit de l'importance de cette hiérarchisation des systèmes autonomes, que ce soit pour la compréhension des phénomènes de routage à grande échelle dans l'Internet ou pour les systèmes autonomes eux-mêmes à fins de négociation, il n'existe pas de hiérarchie publiquement disponible (les clauses commerciales des accords entre systèmes autonomes ne sont pas nécessairement publiques) ni même de consensus sur le moyen d'en établir une (les diffé-rents fournisseurs de services ont chacun leur propre façon d'établir ce classement).
Nous proposons ci-dessous une méthode permettant d'établir un tel classement qui satisfasse aux contraintes suivantes :
1. reposer sur des données publiquement disponibles; 2. permettre de simuler les conséquences de l'ajout ou du retrait d'une relation de connectivité entre systèmes autonomes; 3. permettre de pondérer l'importance accordée aux différents systèmes autonomes; 4. permettre d'étudier les contributions de son voisinage au classement d'un système particulier. La notion d'importance d'un système autonome (AS1) dans le cadre du routage dans l'Internet mondial repose sur sa capacité à permettre à d'autres systèmes d'établir une connexion alors qu'ils n'ont pas de connectivité directe (AS2 et AS3); cette capacité ne repose pas forcément sur une médiation "directe" AS2 -AS1 -AS3 entre les systèmes autonomes mais peut aussi reposer sur une médiation "indirecte" par le biais des connectivités nouvelles offertes à AS2 et AS3 par AS1 : AS2 -AS1 -AS4 -AS3 où il est entendu que AS2 n'a pas de connectivité avec AS4, ni AS3 avec AS1 (voir la Figure 1). Ainsi, dans l'exemple ci-dessus, l'importance de AS1 dépend de l'importance de AS4 : en se connectant à AS4, AS1 "hérite" d'une partie de l'importance de AS4 en ce qu'il peut maintenant proposer à AS2 une connectivité plus étendue (les systèmes comme AS3 que AS1 n'atteint pas directement mais auxquels AS4 a accès); la réciproque est vraie pour AS4 qui hérite d'une partie de l'importance de AS1.
Cette notion d' "héritage" à partir des voisins dans la constitution de la notion d'importance est au coeur de l'analyse de l'importance des positions dans les réseaux sociaux et trouve une de ses formalisations dans la notion de "centralité spectrale" détaillée ci-dessous. C'est à partir de cette notion d'importance que nous proposons d'établir un classement des systèmes autonomes de l'Internet à partir de leur seul graphe d'interconnexion.
Ce graphe d'interconnexion est facile à établir à partir des tables de routage BGP (Chen et al (2002)). Remarquons que, par construction, ce graphe ne comprend que les liens qui apparaissent dans au moins un chemin BGP.
Centralité spectrale dans les réseaux sociaux
Nous reprenons ici la généralisation de la notion de centralité spectrale pour les graphes asymétriques introduite par Bonacich et Lloyd (2001). Le vecteur X des importances des noeuds dans un graphe (donné par sa matrice d'adjacence asymétrique pondérée A) possède deux origines de nature différente, un terme intrinsèque E qui ne dépend que du noeud considéré isolément et un terme provenant de l'effet de réseau (héritage linéaire de l'importance des voisins), ce qui se traduit par une équation du type X=?AX+E où ? doit approcher (par valeurs inférieures) l'inverse de la valeur propre principale de A pour que le résultat obtenu par cette méthode soit cohérent avec celui obtenu par la méthode spectrale usuelle dans le cas de graphes non orientés (Bonacich et Lloyd (2001)).
Techniquement, la solution de l'équation ci-dessus est obtenue par itération jusqu'à convergence de X i+1 =?AX i +E, l'inversion directe X=(I-?A)
-1 E faisant apparaître une matrice non creuse en général bien trop volumineuse.
Cette formulation de la centralité permet de répondre directement à deux de nos objectifs : 1. la notion d'importance intrinsèque permet de pondérer l'importance accordée aux diffé-rents systèmes autonomes; dans la littérature, on rencontre surtout le choix E i =1, i ? mais rien n'impose ce choix dans l'absolu; la seule condition imposée à E est que les valeurs soient indépendantes des effets de réseau (par exemple, il ne serait pas cohérent de pondérer chaque noeud par son degré  (Decima et al. (2004) 72.7 % Percentage of nodes with degree ? 50. 0.9 %
TAB. 1 -Propriétés topologiques du graphe d'interconnexion des systèmes autonomes
La notion de degré dans le graphe d'interconnexion est une description de la connectivité physique d'un noeud; cette notion de "connectivité physique" ne capture pas la notion d' "atteignabilité" ("reachability") qui recouvre la capacité d'un système autonome à atteindre d'autres systèmes autonomes en exploitant les chemins à travers l'Internet. Cette dernière notion dépend des relations logiques entre les systèmes autonomes.  (Gao (2000)). La première correspond à un service de transit payant offert par un système autonome (le fournisseur) à un autre système autonome (le client); le deuxième type correspond à un accord de transit gratuit entre deux systèmes autonomes; le troisième type correspond à un cas particulier de transit mutuel entre deux systèmes autonomes et sera ignoré dans la suite. Ces types de relations logiques structurent la hiérarchie des systèmes autonomes de l'Internet car les politiques de routage entre systèmes autonomes répondent à des critères d'efficacité économique. Typiquement, un client doit se trouver en dessous de ses fournisseurs dans la hiérarchie. De même, plus un système autonome est important, plus il sera à même de négocier à son avantage les accords avec les autres systèmes autonomes.
Il est possible d'inférer ces relations logiques à partir des chemins déduits des tables de routage BGP. Le tableau 2 résume le résultat de cette inférence à partir de la méthode proposée par Gao (2000).
La connaissance des relations logiques permet de construire une hiérarchie des systèmes autonomes mais cette façon de procéder ne répond pas à un des objectifs que nous nous sommes fixés dans cette étude, à savoir pouvoir évaluer l'impact de l'ajout ou du retrait d'une connexion dans le graphe; en effet, l'inférence repose sur les chemins BGP qui sont la résul-tante de décisions politiques distribuées de l'ensemble des acteurs et il est impossible, par exemple, de prédire comment les chemins BGP se ré-arrangeraient si on supprimait une connexion. Nous travaillerons donc seulement à partir du graphe d'interconnexion.
La centralité fondée sur le degré
Cette notion de centralité est la plus simple qui inclue un effet de réseau; les experts estiment que ce classement est assez satisfaisant pour les systèmes les plus importants (en particulier, les 5 systèmes autonomes les plus importants de l'Internet ("Tier 1") qui sont tous reliés entre eux par des accords de peering et ne sont clients de personne sont correctement placés en tête de classement, voir le tableau 3) mais surestime l'importance des systèmes autonomes ayant de nombreux voisins "terminaux" (des noeuds qui n'ont qu'un seul voisin).
Pour mémoire, les cinq systèmes autonomes qui forment une clique de pairs au sommet de l'Internet sont Uunet (701), Sprint (1239), ATT (7018), Level 3 (3356), Qwest (209). Une définition moins restrictive du Tier 1 y ajoute les autres systèmes autonomes ayant au moins un lien de peering avec les précédents; s'ajoutent, entre autres, à la liste Cogent (174), NTT Verio (2914), Global Crossing (3549) ou Savvis (ex Cable Wireless) (3561). Ces systèmes ne font pas partie de la clique de pairs mais, ayant au moins une relation de peering avec cette clique, ils ont un accès gratuit à la totalité de l'Internet.
Bien que très simple et obtenant des résultats assez satisfaisants, cette centralité fondée sur le degré ne permet pas de simuler les conséquences de l'ajout ou du retrait d'une connexion au-delà de la seule variation triviale de connectivité des deux systèmes concernés. C'est une conséquence de la différence entre les notions de "connectivité" (mesurée par le degré) et d' "atteignabilité" qui nous intéresse ici.
La centralité spectrale à partir du graphe d'interconnexion symétri-que
Le graphe d'interconnexion entre systèmes autonomes tel qu'on peut le déduire des informations de routage publiquement disponibles est évidemment un graphe symétrique.
On peut lui appliquer le calcul classique de centralité spectrale pour les graphes symétri-ques (cette approche a également été proposée par Gkantsidis et al (2003)). Le classement obtenu (voir le tableau 3) est clairement insatisfaisant pour les experts du domaine; en particulier, les cinq systèmes autonomes formant le "Tier 1" ne sont pas en tête du classement, l'un d'entre eux ne figurant même pas parmi les 20 premiers. 
Une heuristique pour orienter le graphe d'interconnexion en fonction de la hiérarchie des systèmes autonomes
Le défaut de l'approche précédente est de laisser symétrique la relation entre deux noeuds au lieu de tenir compte de leur différence de centralité pour orienter le graphe au sens d'une relation client/fournisseur.
Nous proposons ci-dessous une heuristique itérative simple permettant d'introduire progressivement cette asymétrie dans le graphe à partir des différences de classement entre systèmes autonomes. Le résultat de cette heuristique est de permettre de transformer progressivement le graphe d'interconnexion "physique" (symétrique) en un graphe d'adjacence "logique" (asymétrique) en cohérence avec le classement des systèmes autonomes associé.
L'heuristique proposée est la suivante : Initialisation : 1. tous les noeuds recoivent une importance intrinsèque; 2. le graphe d'interconnexion est considéré comme un (di-)graphe orienté pondéré, chaque arête non orientée donnant naissance à deux arêtes orientées dans des sens opposés et de poids ½; Calcul de la centralité et du graphe d'adjacence "logique" : 1. on calcule le score de centralité associé au graphe pondéré obtenu à l'étape précédente et aux importances intrinsèques; 2. on modifie la pondération des arêtes en renforçant l'asymétrie de la relation entre deux noeuds en fonction de leur différence de centralité et on obtient une nouvelle matrice d'adjacence asymétrique. Ces deux dernières étapes sont déroulées jusqu'à convergence du score de centralité. Plusieurs modifications de la pondération de l'arête w n ab entre des noeuds a et b sont envisageables; ci-dessous, nous avons opté pour une modification en fonction des scores c En choisissant une valeur faible du paramètre ?, on a une adaptation très progressive de l'orientation des arcs à la structure de centralité, ce qui permet de ne pas "figer" brutalement la structure obtenue à l'initialisation.
Le paramètre p permet de faire varier l'importance accordée à un faible écart de centralité entre noeuds : choisir une valeur p>1 accorde une importance faible à de faibles écarts de centralité, ce qui permet d'explorer plus finement la notion de "peering" entre noeuds.
3.5 Résultats expérimentaux pour l'heuristique proposée 3.5.1 Classement de systèmes autonomes Le tableau 3 montre le classement obtenu à convergence pour p=1. Ce classement paraît satisfaisant aux experts du domaine; nous notons en particulier le classement correct des systèmes autonomes du Tier 1. Pour mémoire, le classement des dix premiers systèmes est le suivant : Uunet, Sprint, ATT WorldNet, Level 3, Qwest, Cogent, Abovenet, Globix,Colt, NTT Verio.
FIG. 2 -Rang obtenu par l'heuristique proposée en fonction du rang déduit du degré
La figure 2 montre la variation du classement obtenu en fonction du classement déduit du degré; on peut observer que pour les systèmes autonomes les mieux classés, la corrélation est forte mais que quelques différences significatives sont néanmoins observables. Noter que les grands écarts de rangs observés pour les systèmes mal classés ne sont guère significatifs, tous ces systèmes étant partiquement ex-aequo avec des scores de centralité très proches de 1. Comme on peut le constater, l'utilisation de p=3 permet de faire émerger une plage de systèmes autonomes qui sont dans une relation quasi-symétrique vis-à-vis de OPENTRANSIT (poids de l'arête proche de ½ ) et pourraient donc être considérés comme des "peers" potentiels de OPENTRANSIT au sens du classement obtenu. Ce classement dépendant de la valeur du paramètre p choisi, nous proposons ci-dessous une façon de rechercher la valeur du paramètre la plus adaptée. Nous soulignons ici que notre objectif n'est pas d'identifier précisément les peerings existants 3 mais plutôt d'identifier les peerings potentiels, c'est-à-dire les connexions concernant des systèmes autonomes d'importances comparables.
Pour cela nous nous restreignons à l'étude des connexions mettant en jeu au moins un système autonome dont la valeur du critère de centralité spectrale dépasse 2. Comme nous avons choisi uniformément une valeur d'importance intrinsèque de 1, tous les systèmes autonomes ont une valeur de centralité spectrale au moins égale à 1. Ce seuil à 2 permet de ne considérer que des connexions mettant en jeu au moins un système autonome pour lequel l'effet de réseau a une importance supérieure à cette importance intrinsèque uniforme; en effet, la notion de "peering" n'a de sens que pour des systèmes autonomes jouant le rôle de fournisseur pour une partie du réseau. . Les arêtes sont ensuite séparées aléatoirement en un ensemble d'apprentissage et un ensemble de test en proportions égales et pour chaque valeur du paramètre p, on recherche le seuil t(p) optimal pour la règle de classification :
• si |w ij -1/2|< t(p), l'arête (i,j) est de type "peering"
• si |w ij -1/2|> t(p), l'arête (i,j) est de type "non-peering" Pour chaque valeur de p, le seuil optimal t*(p) est déterminé à partir du seul ensemble d'apprentissage. Le critère de performance est défini comme la demi-somme du taux de bonne classification dans les classes "peering" et "non-peering". Nous avons choisi ce critère pour donner la même importance à la petite classe des "peerings" face à la classe largement majoritaire des "non-peerings" (voir le tableau 2). La variation du critère de performance (pour le seuil optimal) est donnée sur la figure 6, pour l'ensemble d'apprentissage et pour l'ensemble de test.
Les performances sont proches sur l'ensemble d'apprentissage et sur l'ensemble de test, ce qui montre que le classifieur défini par la règle simple au-dessus possède de bonnes capacités de généralisation; la valeur optimale de p se situe autour de p*=2.2 (associée à une valeur optimale de seuil t*(p*)=0.34). Le taux de bonne classification est de 0.96 dans la classe peering et 0.77 dans la classe non-peering. Le fait que beaucoup de connexions "client-fournisseur" (23%) soient classées "peering" par le modèle reflète simplement le fait que du point de vue du classement, de nombreuses connexions se font entre systèmes autonomes d'importance équivalentes qui pourraient se traduire par des accords de peerings en fonction des volontés politiques des acteurs concernés.
Conclusion
L'approche décrite dans cette communication emprunte la notion de centralité spectrale généralisée au domaine de l'analyse des réseaux sociaux et identifie l'importance d'un système autonome de l'Internet à cette centralité.
S'appuyant sur la différence de centralité entre systèmes autonomes pour orienter et pondérer progressivement la matrice d'adjacence du (di-)graphe d'interconnexion, l'heuristique proposée réalise le passage de la description du graphe en termes de connectivité "physique" à une description du graphe en termes de connectivité "logique". Le classement obtenu est en accord avec les attentes des experts du domaine.
La méthode ne repose que sur des données publiquement disponibles, est reproductible et permet : 1. de simuler les conséquences de l'ajout ou du retrait d'une relation de connectivité

Introduction
Notre étude a été motivée par le problème suivant : nous disposons de données concernant plusieurs dizaines de milliers d'individus décrits par quelques milliers d'attributs binaires assez rares et nous recherchons les éventuels liens entre certains attributs ou groupes d'attributs. La similitude de nos données avec des données de transactions nous a naturellement amenés à utiliser un algorithme de recherche de règles d'association. Cependant, le nombre élevé d'attributs conjugué à leur rareté conduit à un très grand nombre de règles dont les supports sont très faibles et les confiances très élevées. C'est pourquoi nous avons cherché à compléter l'approche support-confiance pour extraire les règles les plus pertinentes. De nombreux indices ont été proposés dans la littérature pour évaluer l'intérêt des règles d'association. Quelques uns font l'objet d'une analyse graphique à l'aide de courbes de niveaux. Nous exposons ensuite une application sur données industrielles.
Contexte
Ce travail est issu d'un projet industriel où l'objectif est d'exploiter une partie de l'informationnel d'un grand constructeur automobile afin d'extraire de nouvelles connaissances. Les données, issues du process de fabrication des véhicules, sont sous la forme d'une matrice où chaque véhicule est décrit par la présence ou l'absence d'attributs binaires. La connaissance d'éventuelles corrélations entre certains attributs ou groupes d'attributs représente un avantage non négligeable pour le constructeur automobile qui met un point d'honneur à améliorer son niveau de qualité de façon continuelle. Pour répondre à cette problématique, nous utilisons la méthode de recherche de règles d'association.
Soit la règle d'association A?C où l'ensemble A, la partie antécédent ou prémisse, implique l'ensemble C, la partie conséquent ou conclusion. A et C sont des ensembles disjoints d'attributs binaires. Dans le contexte particulier de notre application, il est nécessaire de préciser que le sens de l'implication n'a pas d'importance. Une règle d'association est entiè-rement caractérisée par son tableau de contingence (TAB.1).
Plusieurs algorithmes permettent de rechercher les règles d'association de façon détermi-niste à partir d'une base de données contenant n cas décrits par des variables binaires. Parmi eux, on peut citer Apriori (Agrawal et al., 1993(Agrawal et al., , 1994, l'algorithme fondateur de la recherche de règles d'association, ou l'algorithme Eclat (Zaki, 2000), qui est plus rapide. Tous les algorithmes procèdent en deux étapes. Tout d'abord, ils recherchent les sous-ensembles fréquents, c'est-à-dire les conjonctions d'attributs qui apparaissent avec un support (P(AC)) supérieur à un seuil fixé par l'utilisateur. Puis, la seconde étape consiste à construire les règles à partir des sous-ensembles fréquents trouvés lors de la première étape. Seules les règles dotées d'une confiance (P(C/A)) supérieure à un seuil minimum défini par l'utilisateur seront conservées.
Bien souvent, l'approche support-confiance précédemment décrite conduit à l'obtention de règles en trop grand nombre. Par conséquent, il est impossible de les faire valider par un expert. Dès lors, il est utile de les trier par ordre décroissant de leur intérêt au sens d'un indice de pertinence, tel que le lift (Brin et al., 1997), pour citer un des plus connus.
Choix de quelques indices de pertinence
Une typologie existante
Il existe tellement d'indices de pertinence des règles d'association qu'il est très compliqué pour l'utilisateur de savoir lequel choisir. Nous trouvant dans cette situation, pour aider à orienter notre choix, nous nous sommes appuyés sur une suite de travaux réalisés sur ce sujet. Afin d'évaluer les indices de pertinence, Lenca et al. (2004) définissent huit propriétés formelles telles que la décroissance en fonction du nombre d'occurrences du conséquent ou la facilité à fixer un seuil d'acceptation de l'indice. Ces propriétés permettent d'évaluer les indices de pertinence et de leur attribuer des notes. Les auteurs proposent ainsi un classement d'une vingtaine d'indices. Cette étude formelle a ensuite été complétée par une étude expéri-mentale  où les auteurs illustrent le fait que les indices ont un comportement différent en fonction des données traitées : une classification ascendante hiérarchique de 18 indices de pertinence est réalisée à partir d'une matrice de distance déduite de la matrice de décision issue de l'évaluation formelle ; elle aboutit à la partition suivante : 
Sélection des meilleurs indices
Afin de mieux compléter l'approche support-confiance, nous nous intéressons aux indices qui ont obtenu les notes les plus élevées selon Lenca et al. (2004) parmi ceux des classes 1 et 3 de la typologie précédente : la confiance centrée dans la classe 1, à laquelle nous rajoutons le lift en raison de son utilisation très répandue et de son interprétation facile ; le multiplicateur de cotes et le Loevinger dans la classe 3. Le tableau 2 rappelle les propriétés de ces indices dans les cas extrêmes :
TAB. 2 -Indices de pertinence retenus et leurs valeurs de référence.
Proposition de deux indices de pertinence supplémentaires
Dans notre cas, les quatre indices de pertinence détaillés ci-dessus prennent des valeurs extrêmement élevées car de nombreuses règles ont un conséquent très fréquent par rapport au support de la règle. Dans le cas des données de transaction, cela équivaut à une règle du type {dictionnaire?lait}. Le lait est un achat tellement commun que de nombreux caddies en contiennent en sortie de caisse. L'achat d'un dictionnaire est moins fréquent mais toutes les transactions contenant un dictionnaire risquent de contenir aussi du lait. La règle {diction-naire?lait} aura un faible support étant donnée la rareté de dictionnaire, mais sa confiance sera proche de 100%. A titre d'exemple, considérons 100 consommateurs : 8 ont acheté un dictionnaire, 40 ont acheté du lait et 7 ont acheté les deux en même temps. Cette règle qui n'a, en réalité, aucun intérêt va tout de même présenter des indices de pertinence élevés (TAB. 3 ET 4). Selon le lift, le nombre d'exemples de {dictionnaire?lait} est deux fois plus grand que sous l'indépendance de {dictionnaire} et {lait}. Cela nous amène à proposer un autre indice de pertinence qui pénalise les règles où le conséquent est fréquent par rapport à l'antécédent, l'indice d'accords désaccords (IAD), qui correspond à un indice proposé par Kulczynski (1927) :
accords positifs IAD P( AC ) P( AC ) désaccords = = + Plus cet indice est grand, plus l'antécédent et le conséquent sont présents simultanément. IAD peut également s'exprimer de la manière suivante :
L'indice d'accords désaccords est proche de l'indice de Jaccard :
La différence entre les deux indices se situe au niveau du dénominateur : pour l'indice IAD, c'est un "ou" exclusif (différence symétrique) alors que c'est un "ou" inclusif pour celui de Jaccard (union). Malgré cette différence, les deux indices sont parfaitement équivalents : ils conduisent à un classement identique des règles d'association car :
L'indice de Jaccard présente l'intérêt d'être borné entre 0 et 1 (TAB. 5).
TAB. 5 -Valeurs de référence pour les indices d'accords désaccords et de Jaccard.
De manière empirique, IAD et Jaccard permettent une meilleure sélection des règles issues de nos données. Sur un exemple typique, comme celui du tableau 3, ils se montrent plus sévères que les autres indices retenus tels que le lift, en effet : IAD=0,21 et Jaccard =0,17. Ce résultat est généralisable à l'ensemble de nos données mais en aucun cas à toutes les applications. 
Comparaison graphique des indices de pertinences
Afin de comparer graphiquement les indices, nous les avons exprimés en fonction des probabilités conditionnelles, notées A P(C / A)
TAB. 6 -Equations des différentes courbes de niveaux.
Ensuite, nous avons tracé des courbes de niveaux pour chaque indice (FIG. 1 ET 2). 
TAB. 7 -Comportement des indices de Jaccard et IAD.
Note : D'un point de vue marketing, la règle inverse {Caviar?Vodka} est aussi intéressante car le caviar est un produit de luxe rarement acheté. Aussi, il est intéressant de savoir que les consommateurs de caviar achètent systématiquement de la Vodka en même temps. Cette règle présente un conséquent beaucoup plus fréquent que son antécédent. Elle est donc de la même famille que {dictionnaire?lait}, qui est le type de règles que nous cherchons à sanctionner avec les indices de Jaccard et IAD, et qui ne sera donc pas retenu.
Une application à un ensemble de règles
Nous disposons d'un ensemble de plus de 80000 véhicules décrits par plus de 3000 attributs binaires rares. La recherche de règles d'association sur ce jeu de données, avec un support minimum de 100 véhicules et une confiance minimum de 75%, conduit à plus de 1,5 millions de règles. Une classification de variables préalable (Plasse et al., 2005) a permis de réduire considérablement le nombre de règles candidates. Après avoir obtenu une partition en 10 classes, nous avons recherché les règles d'association à l'intérieur de chaque groupe. Nous avons ensuite établi un classement des règles obtenues selon les indices de pertinence décrits ci-dessus.
Classements des règles
Les graphiques suivants montrent les différences de classement selon les indices de pertinence, des règles contenues dans deux des dix classes obtenues : les classes A et B dans lesquelles se trouvent respectivement 19 et 29 règles candidates. 
Analyse factorielle des classements
Une analyse en composantes principales des rangs attribués à chaque règle par les diffé-rents indices confirme ce qui précède. Les deux premiers facteurs expliquent 95% de l'inertie. Les cercles de corrélation des classes A et B sont identiques et aboutissent à une typologie des indices légèrement différente de celle de Vaillant et al. (2004) 

Introduction
L'exploitation de données textuelles issues de fonds scientifiques est un objectif de recherche ambitieux dans le domaine de la gestion et de l'acquisition des connaissances. Une des premières étapes pour la mise en place d'un système d'information est la construction d'une ontologie du domaine. Dans cette étude, nous abordons le problème de construction d'une ontologie spécialisée avec une approche mixte (ou semi-automatique). Pour cela, nous nous intéressons à l'étape d'extraction automatique de classes terminologiques susceptibles d'être ensuite validées comme concepts puis structurées par un expert du domaine, l'embryon d'ontologie résultant devant par la suite être enrichi automatiquement.
La tâche de regroupement de mots peut être envisagée de différentes manières (selon l'application visée, les connaissances disponibles sur le domaine ou les traitements possibles). Les études proposées dans ce domaine s'intéressent généralement à l'une des deux étapes suivantes : la définition d'une mesure de proximité entre mots et/ou la proposition d'une méthode de regroupement efficace.
Il existe de nombreuses mesures destinées à évaluer la proximité sémantique entre des mots. On peut classer ces mesures en trois grandes catégories : statistiques, syntaxiques ou utilisant une base de connaissances. Les mesures statistiques proposées se fondent le plus souvent sur l'étude des cooccurrences de mots dans les textes ou parties de textes en utilisant l'hypothèse de Harris et al. (1989) selon laquelle deux mots sémantiquement proches apparaissent souvent dans des contextes similaires. Ces contextes d'utilisation peuvent être plus précisément repérés en identifiant la syntaxe des phrases. Par exemple Bouaud et al. (1997) analysent les relations de type nom-adjectif extraites de syntagmes nominaux et évaluent la proximité entre deux noms en comparant les ensembles de modifieurs (adjectifs) associés. Enfin, la proximité sémantique entre deux mots peut être appréhendée relativement à une base de connaissances structurée, comme par exemple un thésaurus ou une ontologie pré-existante dans le domaine. Les travaux de Rada et Bicknell (1989); Wu et Palmer (1994); Lin et Kondadadi (2001) consistent ainsi à rechercher dans le thésaurus WordNet la position relative des mots dans la hiérarchie de concepts.
Les travaux dans le domaine du regroupement offrent également un assez large éventail de choix d'algorithmes pour organiser un ensemble de mots en classes via une mesure de proximité sur cet ensemble. Les méthodes génériques de regroupement (par exemple k-moyennes (MacQueen, 1967), classification ascendante hiérarchique (Sneath et Sokal, 1973)) restent les plus utilisées malgré quelques propositions récentes d'approches plus adaptées (Lelu, 1993;Turenne, 2000;Lin et Kondadadi, 2001;Pantel et Lin, 2002;Cleuziou et al., 2004).
L'orientation que nous proposons dans cette étude est fondée sur la définition d'une nouvelle mesure de proximité utilisant les informations syntaxiques contenues dans les documents d'un corpus spécialisé. Cette mesure est couplée avec une méthode de regroupement agglomé-ratif hiérarchique, adaptée aux besoins de l'étude.
L'article est organisé comme suit : la section 2 présente le projet BIOTIM ainsi que certaines notions fondamentales du domaine de recherche. Les deux sections suivantes sont destinées respectivement à l'étude des proximités entre mots (section 3) et à la proposition d'une méthode de regroupement adaptée (section 4). Cette dernière partie présente également les premiers résultats expérimentaux sur un corpus de botanique. Une synthèse des avancées proposées dans cet article ainsi qu'une discussion sur les nombreuses perspectives de ce travail sont présentées dans la dernière partie.
Contexte de l'étude
Le projet BIOTIM
L'étude menée s'inscrit dans le cadre du projet BIOTIM 1 dont l'objectif est de concevoir des méthodes génériques d'analyse automatique de masses de données regroupant textes et images dans le domaine de la biodiversité. Nous nous intéressons, pour notre part, à la construction semi-automatique d'une ontologie textuelle du domaine à partir de corpus botaniques.
La complémentarité des équipes associées au projet BIOTIM (Traitement du Langage Naturel, Apprentissage, experts du domaine, etc.), permet d'assurer un traitement adapté aux particularités des données. L'utilisation de termes spécialisés, la structure complexe des phrases rencontrées dans le corpus (longues descriptions, souvent sans verbe) et la masse importante de données à traiter sont autant de spécificités à prendre en compte.
Le choix a été fait de ne pas laisser à l'expert la difficile tâche d'identifier seul les concepts du domaine. Il nous a semblé préférable de l'assister pour cette étape stratégique en proposant des embryons de concepts potentiels, émergeant directement et automatiquement des corpus. Ainsi, le travail de l'expert consistera à juger si un groupe de mots peut traduire ou non un concept du domaine.
Dans la suite de l'étude nous utilisons le corpus de la "Flore du Cameroun", composé de 37 volumes et commercialisé par l'Herbier National Camerounais. Chaque volume a fait l'objet d'une procédure de numérisation, à l'origine de quelques erreurs d'OCR (Optical Character Recognition).
Ontologies et dépendances syntaxiques
La chaîne de traitements 2 effectuée pour extraire un ensemble de termes à partir du texte brut est détaillée dans Rousse et de la Clergerie (2005). Les sorties de ce traitement linguistique sont des termes de la forme Nom-Adjectif ou Nom- (Prép.(Dét.))-Nom. Au total, près de 35 000 termes construits sur une base de plus de 12 000 lemmes (noms et adjectifs) ont ainsi été extraits sur le corpus de la "Flore du Cameroun". On dénombre par exemple 102 termes différents contenant le lemme "foliole" ; parmi les plus fréquents dans le corpus on peut citer les termes suivants : "foliole terminal", "foliole oblong", "foliole à sommet", "paire de foliole" ou encore "feuille à foliole".
On Nous présentons ci-dessous le principe de rapprochement de ces lemmes à partir de l'analyse de leurs contextes d'apparition dans les textes.
Modélisation en graphes
Partant de l'hypothèse de Harris, nous utilisons les "dépendances syntaxiques" entre lemmes à l'intérieur des termes pour construire un graphe dont les sommets correspondent aux lemmes présents dans les termes. L'existence d'une arête entre deux sommets indique que les deux lemmes associés partagent des contextes identiques (Bouaud et al., 1997).
Considérons par exemple les termes "arbre à feuille" et "arbre à foliole" ; le contexte "arbre à ?" est partagé par les deux lemmes "feuille" et "foliole", favorisant ainsi leur liaison dans le graphe. Réciproquement on note que "? à feuille" et "? à foliole" correspondent à deux contextes d'apparition pour le lemme "arbre".
Nous présentons en figure 1, un exemple de sous-graphe obtenu sur le corpus botanique. Il est d'usage, pour cette modélisation en graphes, de recourir à un seuil afin de ne retenir que les dépendances dites non artificielles 
FIG. 1 -Exemple de sous-graphe représentant les dépendances syntaxiques entre lemmes dans le domaine de la botanique.
classes de sous-graphes (cliques, composantes connexes, etc.) on peut alors tenter de faire émerger des embryons de catégories sémantiques. Sur le corpus botanique que nous utilisons un traitement similaire à celui proposé par Bouaud et al. (1997) (un seuil minimum de 10 contextes partagés est requis pour placer une arête entre deux sommets) conduit aux mêmes observations, à savoir la présence dans le graphe d'une composante connexe de taille importante accompagnée de très petites composantes connexes assez pertinentes d'un point de vue sémantique.
Modélisation numérique
Parallèlement à cette modélisation en graphes, une approche numérique est possible. Le Moigno et al. (2002) 
introduit alors plusieurs coefficients dérivés des graphes précédents :
Le coefficient a correspond au nombre de contextes partagés par deux lemmes (par exemple a(feuille,fleur)=10 d'après le graphe de la figure 1).
La productivité d'un lemme, notée prod(m), correspond au nombre de contextes différents dans lesquels ce lemme apparaît. De manière analogue, la productivité d'un contexte, prod(c) correspond au nombre de lemmes différents apparaissant dans ce contexte. Par exemple, une analyse du lemme "foliole" sur le corpus botanique montre que 102 termes diffé-rents contiennent ce lemme ; "foliole" apparaît alors dans 102 contextes distincts (prod(foliole) = 102). Inversement, seuls les lemmes "sommet", "nervation", "marge" et "pé-tiole" apparaissent dans le contexte "foliole à ?" (prod(foliole à ?) = 4).
Le coefficient prox utilise cette notion de productivité. Ce coefficient formalise l'intuition que si un contexte est très productif sa contribution dans le rapprochement de deux mots est plus faible que celle d'un contexte peu productif. Soit C i (resp. C j ) l'ensemble des contextes d'apparition du lemme m i (resp. m j ), prox est défini par
Le coefficient J (non symétrique) tente enfin de formaliser le déséquilibre pouvant exister entre un mot très productif et un autre peu productif :
J(m i , m j ) sera d'autant plus élevé que m i partage beaucoup de ses contextes avec m j . La formalisation numérique entraîne nécessairement une perte d'information : par exemple on ne retient que le nombre de contextes partagés par deux lemmes et non la liste de ces contextes. Cependant nous montrerons qu'il est possible de tenir compte de ce dernier aspect dans le processus de regroupement. Dans la suite, nous nous attachons à définir une mesure globale de proximité entre deux lemmes, définie à partir des différentes notions précédentes.
3 Une mesure de proximité non symétrique Le Moigno et al. (2002) ont introduit, via le coefficient J, la notion de déséquilibre à propos de la proximité entre deux mots. L'idée est alors de considérer à la fois ce qui rapproche deux mots (leurs contextes partagés) et ce qui les différencie (leurs contextes propres).
Considérons par exemple les deux mots "pétale" et "fleur". On observe sur le corpus les caractéristiques suivantes : a(fleur,pétale)=54, prod(fleur)=284 et prod(pétale)=196. Nos connaissances générales dans le domaine nous permettent de dire qu'un "pétale" est une partie d'une "fleur". La notion de "pétale" est donc sémantiquement très dépendante de celle de "fleur" tandis que l'inverse n'est pas vrai. La seule donnée du coefficient a ne permet pas d'observer cette propriété tandis que l'information supplémentaire apportée par les productivités respectives des deux mots le permet : "pétale" partage plus de 27% de ses contextes avec "fleur" alors que "fleur" n'en partage que 19% avec "pétale".
Cette vision relative du nombre de contextes partagés permet de faire émerger des dissymétries dans les proximités et nous encourage alors à proposer une mesure qui tienne compte de ces deux informations (nombre de contextes partagés et non partagés) mais également du fait que la proximité entre deux mots n'est pas nécessairement une notion symétrique.
De même que prox est une extension du coefficient a, nous définissons le coefficient ? par extension du coefficient J, en introduisant la notion de productivité sur les contextes. Soient m i et m j deux lemmes, C i et C j les contextes d'apparition associés :
Le coefficient J concerne l'aspect quantitatif de la proportion de contextes partagés relativement à la productivité d'un mot tandis que le coefficient ? introduit une dimension qualitative en considérant, en plus, la qualité de ces contextes à travers leur productivité. Ce coefficient sera donc d'autant plus élevé que les coefficients partagés par les deux mots sont peu productifs.
Nous présentons ci-dessous les propriétés vérifiées par ? (pour tout couple de mots (m i , m j ) : Les résultats obtenus sont encourageants pour la suite du processus de construction de classes terminologiques. Nous nous attachons dans ce qui suit à présenter une méthode agglomérative adaptée au regroupement de mots d'après leurs contextes d'apparition dans les textes.
Le regroupement de mots
La problématique générale du regroupement (ou clustering) consiste à organiser un ensemble d'objets en groupes de façon à ce que deux objets similaires se retrouvent dans un même groupe et deux objets dissimilaires dans des groupes distincts. De nombreuses stratégies ont été proposées pour répondre à cette problématique (Jain et al., 1999), comme par exemple les approches par partitionnement (k-moyennes), les algorithmes hiérarchiques (agglomératifs ou divisifs), les méthodes utilisant des mélanges de densités de probabilité, des découpages en grilles, etc.
La plupart des travaux présentés dans le domaine du regroupement de données textuelles (chaînes graphiques, lemmes, termes, mots-clés, documents, etc.) se focalisent davantage sur le sens à donner à la notion de proximité que sur l'algorithme permettant de regrouper les unités textuelles considérées. Certaines études proposent cependant des approches originales afin de regrouper des objets textuels en tenant compte de leurs spécificités telles que la polysémie d'un mot ou l'aspect multi-thématique d'un document (Lelu, 1993;Pantel et Lin, 2002;Cleuziou et al., 2004). Malgré ces travaux récents et marginaux, l'étape de regroupement reste généralement réalisée par les méthodes classiques (algorithme des k-moyennes ou approche agglomérative) car simples et maîtrisées par les utilisateurs.
Une méthode de regroupement adaptée
Dans notre travail, nous proposons d'adapter le processus de regroupement aux données dont nous disposons (lemmes et contextes associés) ainsi qu'à la tâche visée (aide à l'élabora-tion d'une ontologie). L'approche présentée ici est une adaptation de l'algorithme agglomératif hiérarchique du lien moyen. Ce dernier procède par fusions successives des deux plus proches groupes 5 , en partant des feuilles (un objet par groupe) et aboutissant à une racine (tous les objets dans un même groupe). Ce type d'approche présente l'avantage de conserver une trace de l'élaboration des groupes à travers l'arbre hiérarchique (ou dendrogramme) construit. En revanche, un problème récurrent pour cette méthode est la recherche des groupes pertinents parmi l'ensemble des noeuds de l'arbre.
Pour cela nous choisissons d'interdire l'agglomération autour d'un groupe lorsque cette fusion conduit à un groupe conceptuellement non pertinent (cf. définition 4.1 ci-dessous). La structure ainsi obtenue est une hiérarchie partielle, soit un ensemble de (petits) dendrogrammes (cf. définition 4.2 ci-dessous).
Définition 4.1 Soient P un groupe constitué des lemmes {m 1 , . . . , m n } et C 1 , . . . , C n les ensembles de contextes d'apparition associés à chacun d'eux, P est conceptuellement non pertinent si il n'existe aucun contexte dans lequel apparaissent l'ensemble des lemmes de P :
Définition 4.2 Soit H un ensemble de parties non-vides sur un ensemble d'objets X, H est une hiérarchie partielle si les propriétés suivantes sont vérifiées :
L'ajout de la propriété "X ? H" permet de se ramener à la définition formelle classique d'une hiérarchie (complète).
L'algorithme agglomératif hiérarchique adapté au regroupement de mots relativement à leurs contextes d'apparition est le suivant : initialement chaque lemme constitue un groupe à lui seul (feuille) auquel est associé une caractérisation (ensemble des contextes d'apparition du lemme) ; à chaque itération, on recherche parmi les fusions possibles (respect de la contrainte de pertinence) celle qui permet d'agglomérer les deux groupes les plus proches selon la mesure de proximité spécifiée. De cette fusion résulte un nouveau groupe auquel est associée une nouvelle caractérisation "mère", intersection des deux caractérisations "filles". La matrice des proximités entre groupes est mise à jour. Lorsque la contrainte de pertinence interdit toute fusion, l'agglomération est terminée et l'algorithme retourne l'ensemble des groupes constitués, les arbres hiérarchiques et les caractérisations associées. On pourra choisir de ne considérer par la suite que les items associés à des groupes de taille supérieure à 1.
L'ajout d'une contrainte de pertinence est essentiel dans cet algorithme. L'utilisation du lien moyen pour évaluer la proximité entre deux groupes P i et P j permet de considérer tous les couples de lemmes dans P i £ P j (contrairement aux liens simple et complet). Cependant cette information seule n'apporte aucune garantie quant à l'existence d'une propriété commune à l'ensemble des objets des deux groupes. Cette "propriété commune" est pourtant indispensable pour définir un concept. La contrainte de pertinence apporte cette garantie ; les lemmes d'un groupe apparaissent tous dans un même contexte au minimum. De plus la caractérisation d'un groupe aidera l'expert à proposer une étiquette au concept associé.
Application aux données botaniques
Nous avons testé l'algorithme de regroupement sur les lemmes extraits du corpus botanique. Parmi les 12 000 lemmes repérés, nous avons sélectionné ceux partageant au moins trois contextes avec un autre lemme, restreignant ainsi à 2 024 la quantité de données à traiter.
La mesure de proximité utilisée est la mesure p, en choisissant comme proximité pour deux lemmes donnés m i et m j , la plus petite des deux valeurs possibles par p :
Quelques groupes obtenus sont présentés dans les figures 2 et 3. Les dendrogrammes proposés sont ceux mettant en jeux les 10 premières fusions (itérations de l'algorithme). Ces arbres hiérarchiques sont représentatifs de l'ensemble des résultats obtenus. On peut les organiser en trois catégories en fonction des termes mis en jeu : les termes spécifiques au domaine, les termes génériques et enfin ceux relevant des abréviations des noms propres ou des mots étrangers.
Les arbres mettant en jeu des termes spécifiques au domaine (figures 2 et 3, arbres c, d, e, f et j) sont difficiles à évaluer pour des lecteurs non-experts du domaine. On peut malgré tout appréhender la sémantique globale de certains groupes : par exemple le groupe f est la repré-sentation textuelle du concept "aspect du limbe"
6
. D'autres concepts spécifiques émergent de l'analyse de l'ensemble des résultats, par exemple la "forme du sépale" ou plus généralement du lobe (linéaire-lancéolé, ovale-lancéolé, ensiforme), la "forme d'une foliole ou d'un lobe" (deltoïde, linéaire, ovale, rhomboïde, falciforme, cunéiforme, polymorphe), l'"apparence" que peut prendre une espèce végétale (plante, herbe, liane, arbrisseau), etc.
Les arbres contenant des termes génériques sont cette fois plus faciles à évaluer (figure 3, arbres g, h et i). Leur analyse vient confirmer l'impression de qualité puisque les groupes observés peuvent effectivement être associés à des concepts du domaine :
-g est une représentation textuelle du concept de "couleur d'écorce", -h est une représentation textuelle du concept de "variantes de couleurs" (en particulier pour les teintes noir, marron et jaune), -i est une représentation textuelle du concept d' "unité de mesure" (en particulier pour indiquer la hauteur des végétaux). Ces concepts peuvent notamment être identifiés plus facilement grâce à la caractérisation proposée. Parmi les résultats non présentés ici, on retrouve d'autres concepts simples tels que : la "forme" associée à un élément d'une plante (bec, dôme, languette, ruban, gouttière), les "points cardinaux" (nord, sud, ouest, Ouest), les "mois" du calendrier (janvier, mai, août, novembre, décembre), etc.
Enfin, les arbres de la dernière catégorie (figure 2, arbres a et b), sont composés en grande partie de termes correspondant à des abréviations des noms propres ou des mots étrangers. Les termes de ce type pourront être supprimés en sélectionnant dans les documents, uniquement les parties descriptives de plantes (travail en cours de réalisation). Pour effectuer une synthèse de nos premières analyses, nous pouvons conclure à la pertinence globale des groupes obtenus et noter l'aide précieuse apportée par la caractérisation associée à chaque groupe. Ce résultat est imputable pour partie à la mesure de proximité proposée mais également à l'adaptation de l'algorithme de classification.
Conclusion et perspectives
Notre étude s'est focalisée sur la tâche de regroupement de mots dans un domaine de recherche plus vaste qu'est la construction semi-automatique d'ontologies spécialisées. Nous avons défini une nouvelle mesure de proximité entre mots d'une part, et proposé une méthode de regroupement adaptée d'autre part.
La mesure de proximité présentée se place dans la lignée des mesures utilisant les dépen-dances syntaxiques. Contrairement aux précédentes propositions, nous ne considérons pas la proximité comme une notion symétrique. L'algorithme de regroupement utilisé est une adaptation des méthodes de classifications ascendantes hiérarchiques. Plutôt que d'aboutir à un arbre hiérarchique complet, c'est un dendrogramme partiel qui est élaboré par l'ajout d'une contrainte de cohérence sur les fusions effectuées. Finalement, tous les objets initiaux ne sont pas nécessairement utilisés, les groupes obtenus sont de petite taille et complétés par des informations structurelles (dendrogramme) et conceptuelle (caractérisation).
Dans le cadre du projet BIOTIM, ce travail a été appliqué sur des textes du domaine de la botanique. Les résultats obtenus avec la mesure de proximité mettent en évidence des paires de lemmes pertinentes. Ce résultat est confirmé par l'analyse des groupes finalement obtenus en utilisant cette mesure couplée à la méthode de classification proposée. En effet la plupart de ces groupes semble correspondre à des concepts du domaine.
Nous envisageons actuellement d'améliorer la qualité des groupes obtenus en travaillant selon deux axes :
-réduire l'impureté par le renforcement des contraintes de cohérence appliquées aux fusions (extraction d'ensembles de contextes fréquents), -proposer des groupes exhaustifs en autorisant la réutilisation de lemmes déjà agglomérés pour définir de nouveaux concepts (approches pyramidales).
Parallèlement à ces perspectives, nous tâcherons de mettre en évidence les fortes analogies qui existent entre les mesures de proximité étudiées dans cet article et les indices proposés dans le cadre d'approches plutôt probabilistes utilisant les cooccurrences : information mutuelle, coefficient de Dice, etc. (Cleuziou et al., 2003).
Enfin nous développerons une interface de validation destinée aux experts du domaine. Cette interface permettra d'une part d'aider les experts à construire l'ontologie en intervenant sur le processus de classification (validation/structuration des concepts) et d'autre part d'éva-luer quantitativement la pertinence de la méthodologie proposée.

Introduction
Associées notamment au succès des nouveaux langages du Web sémantique, les ontologies suscitent un intérêt croissant au sein des communautés de l'ingénierie et de la gestion des connaissances (Gruber, 1993;Fürst, 2004). Cependant, malgré le développement d'outils d'aide à leur manipulation, le développement et l'exploitation des ontologies restent des phases complexes dans un processus global de gestion de connaissances. En amont, une des difficultés majeures concerne la structuration des ensembles de concepts dont la taille ne cesse de croître. Et en aval, le problème consiste à rechercher efficacement des sous-ensembles de concepts à la fois en temps de calcul et en pertinence sémantique des résultats.
Pour faciliter ces tâches, le recours à des mesures sémantiques semble judicieux ; il permet de constituer une « connaissance heuristique » directement exploitable. De façon générale, une mesure sémantique est une application de l'ensemble C×C des paires de concepts d'une ontologie dans IR + qui permet d'évaluer quantitativement la proximité ou l'éloignement sémantique de deux concepts. Quelque soit le domaine applicatif, la pertinence de la mesure utilisée est étroitement associée à l'efficacité des algorithmes qui l'intègrent. Cependant, son choix reste un problème délicat. Pour comparer les mesures existantes, plusieurs approches complémen-taires sont envisageables (Budanitsky, 1999). L'analyse formelle vise à étudier précisément leurs propriétés à la fois algorithmiques et statistiques. La comparaison avec le jugement humain analyse la corrélation entre les valeurs des mesures et les évaluations subjectives de sujets humains. L'évaluation applicative restreint l'expérimentation à un ou plusieurs cadres applicatifs bien identifiés. Dans cet article, nous nous centrons sur une analyse formelle. Nous nous restreignons ici aux relations d'hyperonymie et d'hyponymie associées au lien hiérarchique (is-a). Ce lien qui est commun à la majorité des ontologies est généralement celui autour duquel s'organise une partie de la structuration des concepts (Rada et al., 1989). Notons que la plupart des mesures sémantiques proposées dans la littérature se restreignent également à ce lien.
Dans une première partie nous rappelons dans un cadre formel unifié les définitions des principales mesures utilisées dans la littérature. Nous distinguons les mesures basées uniquement sur l'information issue de l'ontologie de celles utilisant en complément un corpus de textes. Aucune des mesures étudiées n'exploite complètement l'information qui caractérise la proximité sémantique entre concepts sans utiliser un corpus de textes en complément de l'ontologie. Pour palier ces limitations, nous proposons ici une nouvelle mesure de similarité : la PSS (Proportion of Shared Specificity). Celle-ci est indépendante de tout corpus et intègre l'ensemble des paramètres mis en évidence lors de notre étude.
Principales mesures existantes
Soit C l'ensemble des concepts de l'ontologie considérée, A ? C × C l'ensemble des arcs traduisant une relation soit d'hyperonymie soit d'hyponymie entre les concepts de C, e : A ?? {hyper, hypo} une fonction qui associe à chaque arc un type de relation. Une ontologie peut être représentée par un 1-graphe G(C) = (C, A, e) connexe orienté sans boucle (Berge, 1973)  D'une façon générale, on peut distinguer deux grandes familles de mesures : celles qui extraient de l'information uniquement à partir d'une ontologie et celles qui utilisent un corpus de textes en complément de l'ontologie. Le corpus de textes est utilisé comme échantillon statistique dont on extrait le nombre d'ocurrences de chaque concept de l'ontologie. On en déduit alors pour chaque concept, la fréquence d'occurrence de ce concept ou de l'un des concepts qu'il subsume directement ou indirectement. Cette fréquence est souvent interprétée -parfois abusivement -comme une probabilité dans la littérature ; nous la notons donc P (c i ) pour c i ? C. Parmi les mesures les plus fréquentes dans la littérature, considérons ici deux mesures qui se basent uniquement sur une ontologie et deux autres qui utilisent un corpus en complément : Rada et al. (1989). Cette distance sémantique est simplement fonction du chemin élémen-taire entre deux concepts c i et Wu et Palmer (1994). Cette similarité tient également compte de la longueur du chemin d'origine c i et d'extrémité c j mais aussi de la profondeur de leur subsumant commun le plus spécifique, autrement dit de la longueur du chemin d'
lenn(che(ci,cj ))+2 * lenn(che(mscs(ci,cj ),c0)) . Resnik (1995). Cette similarité repose sur l'hypothèse selon laquelle plus deux concepts partagent d'information en commun, plus ils sont similaires. Sur la base de la théorie de l'information, l'auteur propose de considérer le contenu informationnel des concepts : CI(c i ) = ? log(P (c i )). L'information partagée par deux concepts est alors égale au contenu informationnel de leur subsumant commun le plus spécifique : sim r (c i , c j ) = ? log P (mscs(c i , c j )).
Lin (1998). Cette similarité, qui fait partie des plus étudiées sur le plan théorique, tient compte de l'information partagée par les deux concepts comme Resnik, mais aussi de ce qui les distingue : sim l (c i , c j ) = Nous avons également étudié d'autres mesures intéressantes d'un même point de vue théo-rique (Sussna, 1993;Leacock et Chodorow, 1998;Hirst et St-Onge, 1998) La comparaison de mesures issues des deux différentes familles n'est pas évidente a priori. La clé de cette comparaison réside dans l'algorithme de calcul du contenu informationnel. Les occurrences de chaque concept sont comptabilisées par un balayage du corpus et l'occurrence d'un concept est prise en compte également pour tous les concepts qui le subsument. Cet algorithme de construction confère des caractéristiques à P (c i ) relatives à la structure de l'ontologie. En effet, si on considère c i appartenant à un chemin élémentaire allant de la racine à un concept quelconque, P (c i ) décroît exponentiellement en fonction de la profondeur de c i , et ce plus ou moins vite en fonction des densités locales (nombre de fils d'un concept) des concepts appartenant à ce chemin élémentaire.
Chacune des mesures repose sur une axiomatisation qui a guidé son élaboration. Notre étude (Blanchard et al., 2005) nous a permis de cerner toutes les propriétés de l'ontologie exploitées par ces mesures et d'en faire une synthèse sous la forme de quatre paramètres dans 0 , mscs(c i , c j )
G(C). Les longueurs des chaînes élémentaires che(c
)).
Les mesures basées sur le contenu informationnel sont sensibles à la densité locale au niveau des concepts appartenant à l'un des chemins élémentaires che(c i , c j ) et che(c 0 , mscs(c i , c j )). On dégage alors les deux nouveaux paramètres
Notons qu'une mesure sensible à p 3 (resp. p 4 ) est sensible à p 1 (resp. p 2 ) tandis que la réci-proque n'est pas vraie.
Il faut souligner que la mesure de Sussna est la seule qui prenne en compte la densité des concepts sur che(c i , c j ) sans utiliser un corpus. Finalement, aucune des mesures étudiées qui n'utilisent pas de corpus n'est sensible aux quatre paramètres.
3 Une nouvelle similarité sémantique : la proportion de spé-cificité partagée
Devant les limites des mesures existantes, nous avons cherché à proposer une nouvelle mesure qui n'utilise que l'ontologie et qui soit sensible à tous les paramètres évoqués. Seule la mesure de Lin est sensible à l'ensemble des paramètres, mais elle utilise un corpus. Nous avons donc adapté sa définition de manière à s'en passer. Sans corpus, on ne peut pas calculer la probabilité P (c i ), c'est pourquoi nous estimons cette probabilité par la seule considération de la structure de l'ontologie. La mesure de Lin permet de tenir compte à la fois de ce que les concepts ont en commun et de ce qu'ils ont de différent. Sur la base de ces propositions, Lin propose une définition générique de sa mesure qui n'est pas utilisable en l'état puisqu'elle doit être instanciée : sim(c i , c j ) =
IC(commun(ci,cj )
IC (description(ci,cj ) . Le calcul de la quantité d'information IC se base comme dans la définition de Resnik sur la théorie de l'information en utilisant la notion d'information propre qui correspond au logarithme négatif de la probabilité d'occurrence IC(c i ) = ? log(P (c i )). Cette notion traduit l'évolution de l'information portée par un concept qui croît avec sa rareté. La comparaison de deux concepts c i et c j de C, revient à comparer deux instances quelconques x i et x j de ces deux concepts. Les deux propositions précédentes peuvent être traduites par les événements suivants : 
Sous l'hypothèse d'une distribution uniforme du nombre d'instances associées à chaque concept, on peut montrer que :
Cette formule intègre à la fois la profondeur du concept c i par une définition récursive et la densité de liens pour les concepts qui subsument c i . Notons que dans le cadre plus large d'une hiérarchie non disjonctive (un concept peut alors avoir plusieurs hyperonymes) nous utilisons pour des contraintes de complexité algorithmique cette estimation comme approximation de P (c i ) en considérant le plus court chemin élémentaire.
Si l'ontologie comporte une racine virtuelle dont on peut considérer qu'il n'existe pas de subsumant, il faut considérer que deux concepts quelconques de la taxonomie n'ayant que la racine en commun ont une similarité nulle et pour cela P (c 0 ) est fixé à 1. Dans le cas contraire, le choix d'un entier k devra être fait pour fixer P (c 0 ) à 1/k : P (c 0 ) = 1 (ou 1/k)
Conclusion
Cet article met en avant les points clés de certaines mesures qui évaluent les liens séman-tiques entre deux concepts d'une ontologie. L'étude des paramètres propres à l'ontologie qui influençent ces mesures nous a conduit à l'élaboration d'une nouvelle mesure que présente cet article. Nous nous sommes basé principalement sur la mesure de Lin pour définir une mesure de similarité -la proportion de spécificité partagée -qui est sensible à l'ensemble des paramètres précédemment isolés.
Une comparaison des différentes mesures sur un échantillon d'un millier de concepts de WordNet 2.0 nous a permis de mettre en évidence d'une part les bonnes capacités de discrimination de la PSS, et d'autre part une corrélation positive avec la mesure de Lin, qui elle nécessite un corpus additionnel. Cette mesure pourra être utilisée dans des applications né-cessitant une certaine précision et où aucun corpus n'est disponible. Un autre intérêt de cette mesure est d'avoir une sémantique basée sur des propriétés formelles explicites qui peuvent être appréhendées plus facilement par un expert que les mesures recourant à un corpus.
Nous avons choisi comme cadre expérimental un des référentiels organisé sous forme d'une ontologie parmi les plus accessibles actuellement. Nos premières comparaisons numériques nous ont permis de confirmer la pertinence de notre indice par rapport aux indices précédem-ment proposés dans la littérature. Dans le cadre de nos recherches actuelles en gestion des connaissances (Berio et Harzallah, 2005), nous prévoyons de mener une analyse comparative sur une ontologie d'entreprise.

Introduction
La fouille de données définie comme étant l'extraction à partir de données brutes de connaissances potentiellement exploitables, n'en demeure pas moins un processus complexe dès lors qu'il s'agit d'interpréter les résultats fournis. Les techniques de fouille de données représentent une étape fondamentale du processus d'Extraction de Connaissances dans les Bases de Données connu sous le nom ECD ou KDD (Knowledge Discovery in Databases) (Han 2001).
Dans ce papier nous nous intéressons à l'une de ces techniques : la classification non supervisée. Celle-ci est définie comme un ensemble de processus aptes à être exécutés sur ordinateur pour constituer des hiérarchies de classes ou de simples partitions établies à partir de tableaux de données (Jambu 1978). Les règles d'interprétation des structures classificatoires obtenues (hiérarchies, partitions, etc.) à l'issue de ces classifications n'ont pas la simplicité des méthodes descriptives uni-dimensionnelles.
Notre objectif, dans ce travail, est de proposer une aide aux utilisateurs afin d'interpréter les résultats des méthodes de classification. En effet, les modules de classification proposent des techniques de visualisation des résultats très intéressantes et conviviales (Song 1998), (Sprenger et al. 2000), (Wills 1998) mais la plupart ont fait l'impasse sur la structuration des résultats.
En partant de ce constat, nous proposons une nouvelle approche qui consiste à utiliser les métadonnées comme moyen de représentation des connaissances capitalisées au cours du processus de classification.
Les métadonnées sont souvent définies comme étant des données sur les données (Grossmann 1999), (Kent et al. 1997). Elles sont aussi définies comme un ensemble d'informations pertinentes pour la collecte, le traitement, la diffusion, l'accès, la compréhension et l'utilisation des données (Zeila, 2004). En ce sens, elles peuvent aider à comprendre dans quelles circonstances les données originales ont été collectées et de quelle manière elles ont été agrégées puis classifiées.
Dans ce travail, nous proposons une architecture basée sur notre modèle de métadonnées (Baldé et Aufaure, 2005)  
Notre approche
L'aide à l'interprétation consiste en toute technique ou calcul qui permet d'éprouver le bien fondé des classes obtenues en rendant raison de la formation de celles-ci (Jambu 1978). Dans la section suivante nous allons présenter notre architecture basée sur le modèle de métadonnées.
Architecture
L'architecture que nous présentons exploite les métadonnées produites au cours du processus de classification automatique (figure 1).
Cette architecture est constituée d'un ensemble de couches définies ci-après : -un modèle de métadonnées; -un gestionnaire de métadonnées : qui va servir de tampon entre notre modèle de métadonnées et les manipulations qui y seront effectuées; -une couche gérant des requêtes d'utilisateurs en interrogeant le gestionnaire de métadonnées. C'est cette dernière couche qui traite les requêtes des utilisateurs exprimées en Xquery. Cette couche traite les requêtes exprimées en Xquery (Chamberlain 2004). Pour implémenter ces requêtes nous avons utilisé le processeur Saxon 2 . Contrairement à d'autres processeurs comme Berkeley DB XML 3 , Saxon est un ensemble d'outils dédiés aux traitements des documents XML et est performant en terme de rapidité et conforme aux spécifications du W3C.
Notre approche présente l'avantage qu'il ne soit plus nécessaire de procéder à des modifications de certains critères (relatifs à l'homogénéité par exemple) et de relancer le module de classification pour observer le gain ou la perte d'homogénéité. Le but est, à partir des résultats des requêtes, de pouvoir comparer plusieurs scénarios d'interprétation.
FIG. 1 -Architecture de production et de traitement des métadonnées
Notre objectif étant d'aider les utilisateurs dans l'interprétation de leurs résultats, nous avons mis à leur disposition un certain nombre de scénarios d'interprétation (exprimés en Xquery) définis par les experts du domaine. Ceux-ci ont permis d'interpréter des résultats de modules de classification tels que Sclust 4 (Chavent et al. 2003). Cependant l'utilisateur a la possibilité de modifier les critères utilisés pour réaliser son propre scénario. A travers cet outil nous lui donnons la possibilité d'interpréter les résultats dont il dispose et surtout de manipuler automatiquement ceux-ci.
Processus d'extraction
Les éléments de métadonnées extraits sont de deux types : les métadonnées correspondant aux informations fournies par l'utilisateur et celles qui sont associées aux données et aux résultats de la classification. Voici des exemples de métadonnées fournies par l'utilisateur : le nombre de classes, les informations sur l'auteur, la description de la méthode de classification, la distance utilisée, l'unité de mesure utilisée pour certaines valeurs de données, etc… Les métadonnées liées aux données et aux résultats sont par exemple : les paramètres de la méthode de classification, le nombre d'individus dans une classe, la source des données originales, la description des variables, l'usage des variables (active, prédictive,…), les valeurs des critères d'hétérogénéité et/ou d'isolation pour chaque classe, la contribution de chaque variable dans la construction de chaque classe, etc… Ces métadonnées sont extraites au cours de l'exécution de l'algorithme de classification. En sortie de l'algorithme, nous obtenons le fichier de métadonnées. Ce fichier va nous servir de base à la réalisation des requêtes d'interprétation. L'utilisateur pourra effectuer les calculs qu'il souhaite afin de mieux affiner son interprétation. Cette flexibilité permettra d'interpréter au mieux les résultats suivant le domaine visé par la classification. Par exemple un utilisateur privilégiera le critère f-mesure ou le critère d'inertie intraclasse ou encore la contribution marginale des variables, etc… Pour rendre ces fichiers de métadonnées plus lisibles et mieux compréhensibles par les utilisateurs nous avons utilisé le processeur XSLT (Kay 2001). Pour le traitement des requêtes posées par les utilisateurs, nous avons utilisé le processeur Saxon.
La description des critères utilisés dans ce travail est dans (Jambu 1978). Il faut savoir que la relation fondamentale dans une interprétation, basée sur l'inertie, est : T = B +W où T est l'inertie totale indépendante de la partition, B l'inertie de nuage des centres de gravité munis de poids (l'inertie interclasse) et W l'inertie d'une classe k par rapport à son propre centre de gravité (l'inertie intra classe).
Expérimentation
Pour cette étude de cas, le module de classification utilisé est Sclust. Ce module est utilisé pour partitionner un ensemble d'individus décrits par des données symboliques (Bock et Diday 2000) en un nombre k de classes homogènes. Pour interpréter ces classes obtenues, l'utilisateur dispose uniquement d'un fichier listing inexploitable algorithmiquement et mal structuré.
Les données qui sont traitées proviennent des navigations recensées sur les deux serveurs de l'INRIA (siège et Sophia) pour la période du 1 er au 15 janvier 2003. Ces navigations ont été prétraitées suivant un certain nombre de critères (El Golli et al.2005) avant de procéder à leur classification. Les informations sur l'outil de prétraitement sont décrites dans (Tanassa et Trousse 2004) ou sur le site web www-sop.inria.fr/axis/axislogminer. Le but de la classification est de voir si les projets de recherche qui ont des activités scientifiques ou tout au moins des centres d'intérêt communs se retrouvent en analysant uniquement le parcours des internautes sur ces deux sites. Le tableau de données est composé de 100 groupes de navigations décrits par 127 variables.
Supposons que l'utilisateur soit intéressé par la contribution de chacune des variables dans la formation des classes, alors en utilisant notre outil il obtiendra l'ensemble des variables ayant une contribution supérieure au seuil qu'il a fixé. Par exemple, l'utilisateur cherche des variables ayant une contribution supérieure à 1.5 fois la moyenne. Il obtient : Pour conclure, nous pouvons dire que les métadonnées peuvent assister les utilisateurs dans la recherche de l'information, dans l'interprétation de leur contenu et elles peuvent aider dans les post-traitements des classes construites. Nos travaux futurs s'articuleront autour de la création d'une ontologie du domaine de la classification. Pour ce faire nous nous appuierons sur les travaux réalisés au niveau des méthodes de fouille de données par (Cannataro 2003). Cette ontologie permettrait une interprétation automatique des classes et des partitions obtenues par des modules de classification.
Une autre perspective à ce travail serait d'utiliser des techniques de visualisation pour améliorer le processus d'interprétation des résultats. Les métadonnées pourront aussi servir à aider dans la détermination du bon nombre de classes.
Summary
A huge volume of data is produced by many applications. Data mining techniques are used to extract knowledge from this mass of information since it is no longer possible to manually examine this data. In the meantime, the interpretation of the results obtained by applying data mining techniques is not easy. In this paper, we focus on unsupervised learning, and we propose a tool in order to help the end-user to interpret the clusters obtained. Our objective is to facilitate the interpretation process and to point out that metadata can play a major role for this purpose. Metadata will help the user to understand how the original data has been collected, aggregated and then classified. One of the characteristics of this work is that users have the possibility of carrying out the calculations that they wish. These calculations were done by using Xquery queries. To validate our work, we present an example.

Introduction
Un des objectifs de l'extraction de connaissances à partir de données consiste à fournir des énoncés valides et utiles aux utilisateurs propriétaires de ces données. L'utilité de ces énoncés est d'autant plus grande qu'ils décrivent une réalité du domaine non encore explicitée jusqu'ici, autrement dit, une nouvelle connaissance. Nous nous intéressons à l'extraction de connaissances au moyen de règles descriptives comme les règles d'association (Agrawal et al., 1993). Les problèmes posés par l'extraction de telles règles ont été étudiés intensivement ces dix dernières années. Bien que l'extraction de toutes les règles fréquentes et valides soit difficile dans de grands jeux de données, des dizaines d'algorithmes efficaces ont été proposés (Goethals et Zaki, 2003, par exemple). Un second problème concerne le nombre considérable de règles qui peuvent être fréquentes et valides et donc extraites. Une première solution consiste à rechercher des couvertures des ensembles de règles, ou si l'on préfère, à éliminer des règles redondantes. Des travaux importants dans cette direction concernent l'exploitation de représentations condensées des ensembles fréquents comme les ensembles fermés (Pasquier et al., 1999;Boulicaut et al., 2000) ou bien les ensembles ?-libres (Boulicaut et al., 2003). (Jeudy, 2002) est une étude assez complète de ces propositions.
-569 -
RNTI-E-6
Par exemple, les règles dites ?-fortes, car construites à partir d'ensembles fréquents ?-libres, ont des propriétés intéressantes : membre gauche minimal, fréquence minimale mais aussi niveau de confiance controlé par le nombre d'exceptions toléré (le paramètre ?) pour la règle (voir Becquet et al., 2002, pour une application en biologie moléculaire). Cependant, l'élimination des redondances indépendamment du domaine d'application montre clairement ses limites. Pour éviter de présenter aux utilisateurs experts des milliers de règles fréquentes, valides et non redondantes, il faut alors travailler soit avec d'autres mesures d'intérêt objectives (i.e., au delà des seules mesures de fréquence et de confiance), soit assister la prise en compte de l'intérêt subjectif de l'analyste. La première direction de travail a donné lieu à de multiples propositions (voir par exemple Azé, 2003, pour une synthèse récente). Une seconde direction de travail consiste à assister le post-traitement des collections de règles extraites pour la prise en compte de la connaissance du domaine et ainsi éviter de présen-ter des informations triviales et/ou attendues. Notre hypothèse de travail est que les règles dites « intéressantes » sont celles qui non seulement satisfont certaines contraintes sur des mesures d'intérêt objectives (e.g., fréquence minimale, confiance suffisante) mais aussi sortent du cadre des connaissances existantes pour l'utilisateur. Ainsi, il est nécessaire de s'intéresser à la modélisation et l'exploitation des connaissances de l'expert dans un contexte d'extraction de règles d'association. Les travaux de Padmanabhan et Tuzhilin (1998) ont montré l'utilisation des connaissances de l'utilisateur par la définition de règles. Cette approche a ensuite été formalisée par un réseau de croyances (Padmanabhan et Tuzhilin, 2000)   (Agrawal et al., 1996). La différence entre le support estimé sur les données et le support inféré à partir du réseau bayésien est calculée pour chaque ensemble d'attributs. Les motifs les plus intéressants sont ceux pour lesquels la divergence entre les connaissances de l'utilisateur (i.e., l'évaluation au moyen du réseau) et ce qui est observé dans les données réelles est la plus forte. Ces ensembles d'attributs sont ensuite soumis à l'utilisateur pour une éventuelle mise à jour de la structure ou des paramètres du réseau bayésien. Dans cet article, nous proposons également une approche méthodologique pour exploiter des connaissances du domaine dans le cadre de la découverte de motifs locaux intéressants, typiquement des règles d'association. Nous validerons cette approche sur un cas d'application réel en aéronautique. La section 2 présente l'approche envisagée. La section 3 introduit les notations et détaille la solution proposée. La section 4 décrit le cas d'application utilisé et les expérimentations qui ont été menées. Enfin la dernière section est une brève conclusion.
Approche envisagée
Nous décrivons l'approche méthodologique envisagée pour faciliter le processus de décou-verte de connaissances à bases de motifs fréquents. On peut considérer quatre étapes importantes :
-explicitation et modélisation des connaissances a priori de l'expert, - 
Explicitation et modélisation des connaissances a priori
Cette phase de modélisation a pour principal objectif de représenter les connaissances dont l'expert dispose par rapport au domaine d'application. Il est possible, au départ, de modéliser uniquement les connaissances les plus évidentes. Puis, au fur et à mesure de l'exploration des différents résultats d'extractions, l'expert peut souhaiter améliorer son modèle pour faciliter la découverte de motifs toujours plus intéressants en éliminant ceux qui apparaissent triviaux au regard des connaissances déjà connues. Pour ce faire, il faut disposer d'un formalisme de représentation adapté. L'approche par réseau bayésien pour la modélisation des connaissances apparaît comme particulièrement adaptée. En effet, les réseaux bayésiens permettent de considérer dans un formalisme commun les modèles de causalité 1 et les probabilités. Ils ont aussi été utilisés de manière intensive pour des applications de modélisation de la connaissance (Naïm et al., 2004, chapitre 8) et il existe de nombreux outils pour les construire et les exploiter. Dans notre cadre d'application, il n'est pas souhaitable de réaliser un apprentissage automatique de la structure et des paramètres du réseau bayésien. En effet, on s'intéresse aux réseaux bayésiens pour leur capacité à représenter de manière compacte et intelligible la connaissance d'un expert plutôt qu'à une utilisation « directe » (prédiction, aide à la décision, etc.). Ainsi, la structure du réseau est définie, puis mise à jour par l'utilisateur expert du domaine. L'apprentissage automatique des paramètres du réseau est possible (Heckerman, 1997, notamment), mais cette solution n'a pour l'instant pas été implémentée.
Extraction d'une représentation condensée des ensembles fréquents
Cette phase concerne l'utilisation d'un algorithme d'extractions de motifs, en l'occurrence des règles d'association. Les algorithmes de type Apriori permettent d'extraire toutes les règles d'association au dessus d'un certain seuil de fréquence et de confiance (spécifiés par l'utilisateur). Un premier reproche classique vis à vis des algorithmes de ce type est qu'ils ne sont pas utilisables sur des volumes denses et/ou fortement corrélées, tout du moins pour des seuils de fréquences qui paraissent pertinents aux experts. Un second problème vient du fait que toutes les règles qui satisfont les contraintes de fréquence et de confiance sont extraites. La question de la redondance de ces collections, quel que soit le domaine d'application a été très étudiée (voir les nombreuses propositions de couvertures de collections de règles). Par contre, il y a encore peu de travaux pour l'élimination de motifs redondants au regard des connaissances déjà acquises par l'expert. Pour résoudre le premier problème, on utilisera un algorithme (Boulicaut et al., 2000) capable d'extraire une représentation condensées des ensembles fréquents, les ensembles dits ?-libres fréquents. Cet algorithme permet également, en calculant la ?-fermetures de tels ensembles, de produire une collection concise de règles d'association à forte confiance appelées règles ?-fortes. En effet, le paramètre ? détermine le nombre d'exceptions toléré pour les règles et sa valeur est supposée être petite au regard du seuil de fréquence utilisé. Pasquier (2000) a d'ailleurs étudié les propriétés de ces collections lorsque ? = 0. Pour résoudre le second problème, il faut intégrer la connaissance de l'expert au calcul de l'intérêt des règles d'association extraites. A partir de la proposition décrite dans Jaroszewicz et Simovici (2004), nous définirons une mesure d'intérêt des règles ?-fortes. Cependant, l'objectif de notre démarche ne consiste pas seulement à définir une nouvelle mesure de similarité, mais plutôt à mettre en place un cadre méthodologique permettant l'exploitation de la connaissance expert et s'appuyant sur des algorithmes d'extraction efficaces. Certains liens importants entre les réseaux bayésiens et les règles d'association seront aussi explicités.
Utilisation du modèle de connaissance pour faciliter la lecture des motifs extraits
En utilisant le formalisme des réseaux bayésiens, l'expert explicite les connaissances qui vont lui être utiles pour le processus de fouille de données. Ce modèle de connaissance a pour but de faciliter la découverte de motifs pertinents, c'est-à-dire ceux qui ne sont pas pris en compte par le modèle de connaissance, ou ceux qui le contredisent. Plus précisément, ce modèle permet à l'utilisateur de définir -a priori-des dépendances entre des attributs qu'il ne souhaite pas retrouver dans les résultats de l'extraction. Pour cela, on utilise les capacités d'inférence du réseau bayésien pour mesurer l'intérêt des règles extraites, en comparant les dépendances déduites du réseau bayésien (construit à partir des connaissances du domaine) et les règles d'association (extraites à partir des données réelles). Une divergence forte indique un motif potentiellement intéressant ; inversement, une convergence entre les données réelles et l'estimation effectuée à partir du réseau bayésien indique des motifs déjà connus.
-572 -RNTI-E-6
Analyse et interprétation des motifs extraits
Il est peu probable que les premières itérations du modèle parviennent à éliminer correctement les motifs non intéressants. Par contre, nous pensons que des mises à jour successives du modèle de connaissance vont pouvoir s'appuyer sur les extractions réalisées. Ainsi, à chaque itération du processus, deux possibilités se présentent : -Les résultats de l'extraction font apparaître des motifs connus ; ce cas de figure nécessite de la part de l'expert une reformulation des motifs découverts, de manière à pouvoir intégrer de nouvelles dépendances dans le modèle de connaissance. Le but est à la fois de pouvoir éviter par la suite la présentation de ce type de motifs, mais aussi de capitaliser une certaine connaissance du domaine sous la forme de dépendances quantitatives et qualitatives entres les variables du domaine. -Les résultats de l'extraction présentent des motifs potentiellement intéressants (du point de vue de l'expert) ; cela implique généralement un travail d'analyse et de recherche dans l'ensemble des données relatives au domaine d'application. L'expert peut ensuite déterminer si les motifs sont effectivement porteurs de nouvelles connaissances ou si ils révèlent une insuffisance (en terme de représentativité ou d'exhaustivité des attributs pris en compte) des données disponibles. L'expert peut, par exemple, décider d'enrichir la base de données en intégrant de nouvelles variables (attributs), avant de procéder à une nouvelle itération du processus. Ainsi, l'expert doit reformuler les connaissances induites par les motifs découverts afin de pouvoir les intégrer progressivement à son modèle. Nous allons ainsi pouvoir éliminer progressivement les règles triviales ou connues et faciliter l'émergence de règles plus intéressantes. Notons que, dans ce contexte, le réseau bayésien représente un modèle partiel (et dégradé) des connaissances de l'expert. Il n'est défini et utilisé que pour faciliter la lecture et l'interprétation de règles d'association extraites (post-traitement des règles).
Réseaux bayésiens et motifs fréquents
Définitions et notations
Soit BD une base de données booléenne, et H = {A 1 , A 2 , . . . , A n } l'ensemble de ses attributs booléens.
(i) dénote la probabilité pour que l'ensemble d'attributs I ? H prenne comme valeur le vecteur i. Un itemset est représenté par la paire (I, i) avec I ? H ensemble d'attributs fini non vide et i ensemble des valeurs des attributs de I. Lorsque cela n'est pas strictement nécessaire, l'itemset (I, i) sera désigné simplement par I. Un réseau bayésien RB est un graphe dirigé acyclique défini par un ensemble de noeuds correspondants aux attributs de H et par E ? H × H l'ensemble des arcs du graphe. A chaque noeud on associe une distribution de probabilité conditionnelle P Ai|?A i , où ? Ai = {A j |(V Aj , V Ai ) ? E} représente les parents du noeud A i . Pour une discussion détaillée sur les réseaux bayésiens consulter Pearl (1988). Une des propriétés du réseau bayésien est de définir de manière unique la distribution de probabilité jointe de H : 
et par estimation sur les données :
On travaille sur une représentation condensée des itemsets fréquents au moyen d'itemsets ?-libres et fréquents. En fait, l'algorithme utilisé produit une collection de couples (I, ? ? f ermeture(I)\I). Chaque élément I est un itemset ?-libre fréquent. Sa ?-fermeture est l'ensemble de tous les attributs qui sont vrais pour un enregistrement lorsque ceux de I le sont à ? exceptions près. Il s'agit donc d'une généralisation de la notion classique de cloture au sens de la connection de Galois puisque, lorsque ? = 0, I ? ? ? f ermeture(I) est un ensemble fermé fréquent. L'entier positif ? permet donc de borner le nombre d'exceptions 2 d'une règle dite ?-forte, i.e., une règle R de la forme I ? ? ? f ermeture(I)\I. Il faut comprendre que -par construction-les règles ainsi générées ont une partie gauche minimale et une partie droite maximale, ce qui implique une confiance maximale de 1 sur BD lorsque la règle ne comporte pas d'exceptions (par exemple, lorsque l'on exige ? = 0. En s'inspirant de Jaroszewicz et Simovici (2004), nous définissons maintenant une mesure de l'intérêt d'une règle d'association. Cette mesure est basée sur la différence entre la confiance de la règle estimée à partir des données et celle inférée par le réseau bayésien. Elle s'exprime de la manière suivante :
Exploitation du modèle de connaissance
Nous disposons d'un algorithme qui calcule une collection de règles d'association ?-fortes, d'un formalisme pour modéliser les connaissances a priori de l'expert, ainsi que d'une mesure prenant en compte ces connaissances pour évaluer l'intérêt des règles. Pour se faire une idée du comportement de la mesure d'intérêt et des liens existants entre les règles extraites et les implications au niveau du modèle de connaissance, nous allons décrire de manière empirique deux cas de découverte de règles. Une règle peut représenter (1) un motif connu de l'expert, mais qui n'est pas encore pris en compte par le réseau bayésien ou (2) un motif connu et pris en compte par le réseau.
Soit R = A ? B le motif extrait (conf BD (R) est proche de 1 puisque nous ne calculons que des règles ?-fortes). Dans le cas (1), R n'est pas prise en compte par le réseau bayésien. On se place alors sous l'hypothèse d'indépendance entre A et B pour calculer l'intérêt de R selon le réseau bayésien. Ainsi, à partir de l'équation 4, on obtient : Int(R) = |conf BD (R) ? P B |. Ici, l'intérêt dépend donc principalement de la distribution de probabilité P B définie dans le réseau bayésien. Par exemple, si l'expert a défini P B = 0, 95, la règle R aura un intérêt presque nul car B est un événement très fréquent. Dans ce cas, l'association n'apporte pas de connaissance supplémentaire. Inversement, si P B est faible, alors l'intérêt de la règle sera élevé, signifiant alors à l'utilisateur la possible existance d'une dépendance entre A et B. On se place maintenant dans le cas (2) où la règle R est prise en compte par le réseau bayésien. Cela signifie que l'on a défini explicitement un lien de dépendance A ? B ainsi que la probabilité P B|A . On a alors Int(R) = |conf BD (R) ? P B|A |, soit un intérêt dépendant de P B|A ; ce qui correspond bien au comportement souhaité. En effet, dans le cas où P B|A est proche de 1, l'association n'est pas jugée intéressante car elle est correspond au modèle défini par l'expert. Inversement, si l'on a défini P B|A faible alors que conf BD (R) est proche de 1, l'intérêt de la règle sera plus important, mettant ainsi en évidence une contradiction entre les données réelles et la modélisation de l'expert.
Application à la fouille de données d'interruptions opéra-tionnelles
Dans le domaine aéronautique, une interruption opérationnelle est un retard au départ (dé-collage) de plus de quinze minutes, une annulation ou une interruption de vol suite à un problème technique (panne ou dysfonctionnement). Un tel événement est aujourd'hui considéré comme important par les compagnies aériennes pour le coût et les mécontentements engendrés. De ce fait, lors du lancement de nouveaux projets avions, les ingénieurs doivent fournir dès la phase de conception une prédiction la plus réaliste possible de la fréquence des interruptions opérationnelles, qui sera mesurée lors de la future exploitation commerciale des avions. Ces prédictions initient, guident et valident les choix de conception. Pour effectuer cette activité, les ingénieurs utilisent un outil informatique implémentant un modèle mathématique stochastique intégrant les paramètres dont les impacts sur la fréquence des interruptions opérationnelles sont connus. Cet outil est calibré et paramétré par le retour d'expérience obtenu à partir d'avions, de systèmes ou d'équipements en service comparables. Les besoins de recherche portent sur l'amélioration des modèles de calcul utilisés par cet outil de prédiction. Ainsi, la fouille des données en service est intéressante car elle permet de découvrir de nouveaux facteurs qui pourraient être intégrés à ces modèles pour améliorer la prédiction de la fréquence des interruptions opérationnelles. On se propose d'encadrer ce processus de découverte par l'approche méthodologique présentée dans la section 2. L'analyse des données doit permettre de valider les hypothèses qui ont été prises et d'enrichir le modèle de prédiction. Plus précisément, il s'agit d'aider l'expert à détecter, ou à vérifier, la présence de contributeurs de la fiabilité opérationnelle par la fouille des données en service.
Expérimentations
La base de données relative aux interruptions opérationnelles regroupe les détails de tous les problèmes techniques. Pour notre étude nous avons pris, en accord avec l'expert, un sousensemble de la base de données initiale. Après pré-traitement, on dispose de 23 attributs discrétisés et de plus de 12000 enregistrements décrivant les interruptions opérationnelles. On se propose, dans un premier temps, de regarder les résultats issus de l'extraction des règles d'association ?-fortes. Ces règles sont présentées à l'expert en fonction de différentes mesures d'intérêt : confiance (Agrawal et al., 1993), J-mesure (Smyth et Goodman, 1992) et moindre contradiction (Azé, 2003). Cette première extraction comporte de nombreuses règles connues de l'expert, ce qui permet de se rendre compte de la nécessité d'expliciter certaines connaissances exactes (taxonomies) mais aussi des croyances fortes de l'expert sur son domaine. On peut donc identifier plusieurs catégories de connaissances que l'on peut expliciter dans le réseau bayésien :
-Taxonomie, la présence d'une telle structure dans les données est intéressante car elle permet de capturer différents niveaux de détails dans les règles d'association. Cependant, l'existence de taxonomies introduit des dépendances exactes qui vont être capturées par un grand nombre de règles, occultant ainsi la lecture de règles potentiellement intéressantes. -Valeur d'attribut prépondérante, lorsqu'un attribut de la base de données a une valeur très dominante (e.g. 95% des problèmes ont eu pour conséquence un retard) alors il est important de pouvoir intégrer cette information au modèle de connaissance afin d'éviter la production de règles triviales. -Croyance forte, elle peut être représentée comme une dépendance entre un ou plusieurs attributs du réseau bayésien et par la définition des tables de probabilités jointes correspondantes. L'outil le plus connu et le plus facile à mettre en oeuvre pour décrire les probabilités est l'échelle de probabilité (Druzdzel et van der Gaag, 2000). Après intégration de ces connaissances dans un réseau bayésien, nous pouvons procéder au calcul de l'intérêt des règles d'association. Les résultats mettent alors en avant d'autres connaissances, plus pertinentes, pour le domaine d'application.
Résultats obtenus
Considérons d'abord l'extraction sans exploitation d'un réseau bayésien. L'extraction a donné 17760 règles ?-fortes. Le tableau 1 montre des exemples -choisis-de telles règles au moyen de l'algorithme décrit dans (Boulicaut et al., 2000) (support min = 100, ? = 15). Sur une configuration PC de bureau, l'extraction a demandé 2 minutes et 55 secondes.
Afin d'éclaircir la lecture de ces résultats, il peut être utile de faire quelques précisions. Les mots-clés remove, ecam, mel, etc. indiquent que l'analyse du texte libre rédigé par un technicien a permis de déceler une action particulière : « pose/dépose » d'un équipement, apparition de messages d'alertes, application d'une procédure spécifique. Lorsqu'un mot-clé est préfixé de last= cela signifie qu'il s'agit du dernier mot-clé, correspondant à une action, détecté dans la description du problème. Les nombres de 2, 4 ou 6 chiffres désignent les équipements incriminés dans l'interruption opérationnelle. Ces nombres obéissent à une taxonomie bien précise : la norme ATA 100. Ainsi l'équipement 286322 est un sous-équipement de 2863, -576 -RNTI-E-6 Une analyse des résultats du tableau 1 permet de mettre en avant la présence d'associations correspondant : -à la taxonomie des équipements (règles 2, 4, 5, 7, 9 et 10), -à des relations triviales entre l'identification du dernier mot-clé et la présence de ce mot clé dans le texte (règles 1, 3, 8), -à la prépondérance dans les données de certaines valeurs d'attributs comme DY ou CS.
-ou encore à des connaissances plus spécifiques du domaine, telles que les liens entre les compagnies et leur aéroport principal (règle 8), ou encore entre un mois de l'année et l'apparition d'incidents sur un équipement spécifique (e.g., système de conditionnement d'air pendant les périodes d'été, règle 10), etc. Ainsi, beaucoup de règles contiennent des connaissances bien connues de l'expert. Nous montrons maintenant que l'approche proposée permet d'améliorer la découverte d'informations pertinentes grâce à l'exploitation d'un modèle des connaissances du domaine, de type réseau bayésien.
Taxonomie et associations évidentes L'intégration de ce type de connaissance au réseau bayésien est triviale. L'expert va ajouter un lien de causalité entre les attributs qui correspondent à cette information. Par exemple, on peut définir un lien entre ATA4d et ATA2d, puis un lien entre ATA2d et Category, etc. Les tables de probabilités sont ensuite définies de manière à exclure toute autre relation entre ces attributs, e.g., P (ATA2d =43|ATA4d = 4345) = 1.0 ; et ainsi de suite pour tous les attributs obéissant à ce type de relation.
Itemsets prépondérants Il est important de définir la probabilité d'apparition des événe-ments fréquents. En effet, la définition des distributions de probabilités des événements fré-quents permet de limiter le facteur d'intérêt lié à la présence de ces attributs dans la partie 
TAB. 2 -Utilisation du réseau bayésien pour mesurer l'intérêt des règles d'association
droite d'une règle. Par exemple, on pourra définir P (Effect = DY) = 0, 97 ou encore P (Remove = true) = 0, 95. Ainsi la présence de l'attribut DY dans la partie droite d'une règle d'association n'influera pas sur le calcul de l'intérêt de cette règle.
Connaissances plus spécifiques L'expert peut vouloir définir des connaissances fortes du domaine, par exemple, le lien entre une compagnie et sa base principale, ou encore entre un mois de l'année et l'apparition d'un problème sur un équipement particulier. L'expert doit reformuler sa connaissance du domaine pour l'adapter à la modélisation du réseau bayésien : il doit créer des liens de causalité entre un ou plusieurs attributs du réseau puis définir les distributions de probabilités correspondantes. Pour cela, nous pouvons employer la méthode de l'échelle des probabilités pour expliciter, par exemple, qu'il est probable que la compagnie OP1 soit associée à l'aéroport ST1, etc. La figure 2 reprend les différentes informations que l'expert a pu extraire à partir des premiers résultats obtenus et montre une modélisation partielle de ces connaissances, après analyse et reformulation des motifs déjà extraits. Les tables de probabilités jointes ont elles aussi été dé-finies par l'expert mais elles ne sont pas présentées pour des raisons de clarté. Ce réseau est ensuite utilisé pour calculer l'intérêt des règles d'association présentées dans le tableau 1. Le calcul montre (tableau 2) que les différentes règles ont un intérêt faible. Ce résultat est cohérent puisque le réseau bayésien a été défini de manière à pouvoir éliminer la plupart de ces motifs. Néanmoins les règles (6) et (7) ont un intérêt supérieur aux autres et nécessitent une analyse plus poussée. La règle (6) montre un lien entre un aéroport et l'absence d'action de maintenance (last = none) associés à une compagnie aérienne, ce qui pousse à s'intéresser sur le fonctionnement de cette compagnie. En effet, certaines compagnies préfèrent effectuer les opérations de maintenance « lourdes » dans leur base principale. La règle (7) met en avant une relation entre une tranche de retard assez importante (0.5_1.5) et un équipement particulier (434512). Ce motif paraît particulièrement intéressant, mais une analyse plus poussée reste nécessaire pour valider ou non la réalité de cette association.
Conclusion
A partir des travaux de Boulicaut et al. (2000) et de Jaroszewicz et Simovici (2004), on a mis en place une méthodologie permettant de faciliter la découverte de règles d'association potentiellement intéressantes. Notre approche met en avant la collaboration entre réseaux bayé-siens et collections de règles d'association (et donc ensemble fréquents). Cette méthodologie a été testée sur un cas d'application concret concernant la fouille des données d'interruptions opérationnelles dans l'aéronautique et elle montre des résultats encourageants.

Introduction
La fouille de données a pour objectif d'identifier des relations cachées entre les motifs de grandes bases de données. La recherche de règles d'association est une des tâches les plus importantes de la fouille de données. L'extraction de règles d'association est un domaine de l'extraction de connaissances dans les bases de données (ECBD), qui se définit comme un procédé pour trouver des motifs valides, utiles et compréhensibles dans les données (Fayyad et al., 1996). Une règle d'association est une proposition de la forme "80% des étudiants qui suivent le cours Introduction à Unix suivent également Programmation en C" (Han et Kamber, 2001).
Jusqu'à présent, la littérature s'est intéressée à la recherche des règles d'association valides fréquentes (c'est-à-dire les règles d'association avec un support et une confiance suffisamment élevés). Cela requiert d'abord l'extraction des motifs fréquents de l'ensemble des données. Le problème de l'extraction des motifs fréquents était au départ un sous-problème de la fouille de règles d'association (Agrawal et al., 1996), mais il s'est révélé plus tard utile dans différents domaines, tels que la fouille de motifs séquentiels (Agrawal et Srikant, 1995), de règles d'association spatiales (Koperski et Han, 1995), de règles d'association cycliques (Özden et al., 1998), de règles d'association négatives (Savasere et al., 1998), la recherche de motifs fermés fréquents (voir Section 1.1), de motifs fréquents maximaux (voir Section 1.1), etc.
Nous faisons l'hypothèse que certains phénomènes rares dans les bases de données peuvent également véhiculer une connaissance. C'est donc plus particulièrement l'extraction de motifs rares que nous étudions dans cet article.
Travaux en relation
L'extraction de motifs rares et la génération de règles d'association rares n'ont pas encore été étudiées en détail dans la littérature. Dans cet article, nous partons d'une vue d'ensemble de la recherche de motifs fréquents pour introduire notre méthode d'extraction des motifs rares.
Plusieurs approches ont été proposées pour trouver les motifs fréquents dans les bases de données. La première est basée sur l'algorithme Apriori, qui fut le premier algorithme par niveau à réaliser cette tâche (Agrawal et al., 1996). Cette méthode identifie les i-motifs à chaque i eme itération puis génère les (i+1)-motifs fréquents à partir des i-motifs 1 . A chaque itération il requiert un passage sur la base de données pour compter le support des motifs candidats et ensuite élague les candidats infréquents. Cet algorithme est très simple et efficace pour des données peu corrélées. Apriori a été suivi par de nombreuses variations dans le but d'en amé-liorer l'efficacité (Brin et al., 1997;Toivonen, 1996). La deuxième approche s'intéresse à la recherche de motifs fermés fréquents dans la base de données (Pasquier et al., 1999). Les motifs fermés fréquents permettent une représentation condensée et sans perte d'information des motifs fréquents, puisque l'ensemble des motifs fréquents (et leur support) peut être retrouvé à partir des motifs fermés fréquents. Cette idée fut implémentée dans Close (Pasquier et al., 1999), qui est aussi un algorithme par niveau. Depuis Close d'autres algorithmes ont été proposés pour la recherche de motifs fermés fré-quents Zaki et Hsiao, 2002;Wang et al., 2003).
Un autre sous-ensemble intéressant de motifs fréquents est l'ensemble des générateurs minimaux. Bastide et al. ont montré comment utiliser les générateurs minimaux pour trouver les règles d'association informatives 2 (Bastide et al., , 2000b. Parmi les règles partageant les mêmes individus comme support et ayant la même confiance, les règles construites à partir d'un motif fermé et ayant un motif générateur en partie gauche sont celles qui contiennent le plus d'information (Pasquier, 2000).
Le premier algorithme pour trouver les générateurs minimaux fut Pascal (Bastide et al., 2000a). Pascal peut réduire le nombre de passages sur la base de données et compter le support des candidats plus efficacement. Pascal trouve tous les motifs fréquents et tous les générateurs minimaux, mais ce n'est pas suffisant pour trouver les règles informatives. Pour la génération des règles d'association informatives, il faut identifier parmi les motifs fréquents, les motifs fermés et les associer aux générateurs minimaux. Pour résoudre cette insuffisance, un autre algorithme appelé Zart a été proposé récemment . Zart est un algorithme multifonctionnel d'extraction de motifs qui étend Pascal de manière à ce qu'il soit conforme à la génération de règles d'association informatives. Zart trouve les motifs fréquents, les motifs fermés fréquents et les générateurs minimaux. De plus, les générateurs minimaux sont associés à leur fermeture. En conséquence, la génération des règles informatives peut être réalisée très rapidement et aisément avec Zart.
Une quatrième approche est basée sur l'extraction des motifs fréquents maximaux. Un motif fréquent maximal a les propriétés suivantes : tous ses sur-motifs sont infréquents et tous ses sous-motifs sont fréquents. Des expériences ont montré que cette approche est très efficace pour trouver de grands motifs dans les bases de données (Bayardo, 1998;Agarwal et al., 2000;Lin et Kedem, 1998;Gouda et Zaki, 2001). Les algorithmes basés sur cette approche identifient, comme Apriori, l'ensemble des règles d'association.
Contributions et motivations
Nous présentons une nouvelle méthode pour trouver les motifs rares dans une base de données en deux étapes. La première étape identifie un ensemble générateur minimal appelé ensemble des motifs rares minimaux. Dans la seconde étape, ces motifs sont utilisés pour retrouver tous les motifs rares.
La découverte des motifs rares peut se révéler très intéressante, en particulier en méde-cine et en biologie. Prenons d'abord un exemple simulé d'une base de données médicale où nous nous intéressons à l'identification de la cause des maladies cardio-vasculaires (MCV). Une règle d'association fréquente telle que "{niveau élevé de cholestérol} ? {MCV}" peut valider l'hypothèse que les individus qui ont un fort taux de cholestérol ont un risque élevé de MCV. A l'opposé, si notre base de données contient un grand nombre de végétariens, une règle d'association rare "{végétarien} ? {MCV}" peut valider l'hypothèse que les végétariens ont un risque faible de contracter une MCV. Dans ce cas, les motifs{végétarien} et {MCV} sont tous deux fréquents, mais le motif {végétarien, MCV} est rare. Un autre exemple est en rapport avec la pharmacovigilance, qui est une partie de la pharmacologie dédiée à la détection et l'étude des effets indésirables des médicaments. L'utilisation de l'extraction des motifs rares dans une base de données des effets indésirables des médicaments pourrait contribuer à un suivi plus efficace des effets indésirables graves et ensuite à prévenir les accidents fatals qui aboutissent au retrait de certains médicaments (par exemple en août 2001, la cérivastatine, mé-dicament hypolipémiant). Finalement, un troisième exemple basé sur les données réelles de la cohorte STANISLAS (Siest et al., 1998;Maumus et al., 2005) montre l'intérêt de l'extraction des motifs rares pour la fouille de données dans des cohortes supposées saines. Cette cohorte est composée d'un millier de familles françaises présumées saines. Son principal objectif est de mettre en évidence l'influence des facteurs génétiques et environnementaux sur la variabilité des risques cardio-vasculaires. Une information intéressante à extraire de cette base de données pour l'expert dans ce domaine consiste en des profils qui associent des données génétiques à des valeurs extrêmes ou limites de paramètres biologiques. Cependant, ces types d'associations sont plutôt rares dans les cohortes saines. Dans ce contexte, l'extraction de motifs rares pourrait être très utile pour atteindre les objectifs de l'expert.
Organisation de l'article
Dans la section suivante, nous donnons une vue d'ensemble des concepts de base. La Section 3 détaille notre approche pour l'énumération des motifs rares basée sur les treillis, et contient également les définitions essentielles. Nous décrivons ensuite dans la Section 4 les deux étapes de notre méthode et nous en fournissons les algorithmes, ainsi que des exemples les appliquant. Enfin, les conclusions sont présentées dans la dernière section.
Concepts de base
Ci-dessous nous utilisons les définitions usuelles de la fouille de données. Nous considérons un ensemble d'objets O = {o 1 , o 2 , . . . , o m }, un ensemble d'attributs A = {a 1 , a 2 , . . . , a n } et une relation R ? O × A, où R(o, a) signifie que l'objet o possède l'attribut a. En analyse de concepts formels (Ganter et Wille, 1999), le triplet (O, A, R) est appelé contexte formel. Un ensemble d'attributs est appelé motif. Un motif de taille i est appelé i-motif. Nous disons qu'un objet o ? O contient le motif P ? A, si (o, p) ? R pour tout p ? P . Le support d'un motif P indique combien d'objets contiennent le motif. Un motif est dit fréquent si son support est supérieur ou égal à un support minimum donné (noté min_supp par la suite). Un motif est dit rare ou infréquent si son support est inférieur ou égal à un support maximum (noté max_supp par la suite). P 2 est un sur-motif de P 1 ssi P 1 ? P 2 . Dans cet article, nous nous sommes placés dans le cas particulier où max_supp = min_supp ? 1, c'est-à-dire qu'un motif est rare s'il n'est pas fréquent. Cela implique l'existence d'une seule frontière entre motifs rares et fréquents. Boulicaut et al. (2003) fait par ailleurs lui aussi mention de cette frontière. Un motif X est dit fermé s'il n'existe pas de sur-motif Y (X ? Y ) de même support. L'extraction de motifs fréquents consiste à générer tous les motifs (fermés) fréquents (avec leur support) dont le support est supérieur ou égal à min_supp. L'extraction de motifs rares consiste à générer tous les motifs (avec leur support) dont le support est inférieur ou égal à max_supp.
Une approche basée sur les treillis pour l'énumération des motifs rares
Avant d'exposer nos algorithmes pour trouver les motifs rares, nous présenterons notre méthode du point de vue des treillis (voir Ganter et Wille (1999) pour une description détaillée des treillis).
La Figure 1 montre le treillis de l'ensemble des parties P (D) de l'ensemble des attributs dans notre base de données exemple D 3 (voir Tableau 1). L'ensemble des motifs rares forme un semi-treillis "join" car il est fermé pour l'opération "join", c'est-à-dire que pour tous motifs rares X et Y , X ? Y est aussi rare. D'un autre côté, il ne forme pas un semi-treillis "meet", car la rareté de X et Y n'implique pas celle de X ? Y . Notons que les motifs fréquents forment un semi-treillis "meet", c'est-à-dire que pour tous motifs fréquents X et Y , X ? Y est aussi fréquent.
Prenons l'exemple de la base de données D (Tableau 1) et fixons min_supp = 3, ce qui signifie que max_supp = 2. Les motifs peuvent être séparés en deux ensembles formant une partition : les motifs rares et les motifs fréquents. Une frontière peut être dessinée entre ces deux ensembles. En bas du treillis nous trouvons le plus petit motif, l'ensemble vide. A chaque niveau se situent les motifs de même taille. Au sommet du treillis on trouve le motif le plus long qui contient tous les attributs. Le support de chaque motif est indiqué dans le coin en haut à droite (voir Figure 1).
Avant d'énoncer les définitions essentielles, nous empruntons à Apriori (Agrawal et al., 1996) ses deux principes fondamentaux que nous rappelons ici :
Propriété 1 (propriété de fermeture vers le bas). Tous les sous-ensembles d'un motif fré-quent sont fréquents.
Propriété 2 (propriété d'anti-monotonocité). Tous les sur-motifs d'un motif infréquent sont infréquents.
L'ensemble des motifs rares et l'ensemble des motifs fréquents ont tous deux un sousensemble minimal générateur. Dans le cas des motifs fréquents, ce sous-ensemble est appelé ensemble des motifs fréquents maximaux (MFM). Définition 1. Un motif est un MFM s'il est fréquent (et ainsi tous ses sous-motifs sont fréquents) et si tous ses sur-motifs ne sont pas fréquents.
Ces motifs sont dits maximaux, parce qu'ils n'ont pas de sur-motifs fréquents. Du point de vue du nombre de ces motifs ils sont minimaux, c'est-à-dire qu'ils forment un ensemble générateur minimal à partir duquel tous les motifs fréquents peuvent être retrouvés 4 . Nous pouvons définir les motifs rares minimaux (MRM) en tant que complémentaires des MFMs, de la manière suivante : Définition 2. Un motif est un MRM s'il est rare (et ainsi tous ses sur-motifs sont rares) et si tous ses sous-motifs ne sont pas rares.
Ces motifs forment un ensemble générateur minimal à partir duquel tous les motifs rares peuvent être retrouvés. Tous les motifs fréquents peuvent être retrouvés à partir des MFM. Dans un premier temps, nous devons prendre tous les sous-ensembles possibles des MFM. Dans un deuxième temps, le support des motifs fréquents peut être calculé grâce à un passage sur la base de données. Un processus similaire est mis en oeuvre pour retrouver les motifs rares. Nous devons d'abord générer tous les sur-motifs possibles des motifs rares minimaux, puis calculer le support des motifs rares grâce à un passage sur la base de données.
Parmi les motifs rares, nous distinguons deux sous-ensembles : a) les motifs rares de support 0, et b) les motifs rares de support supérieur à 0. Cette distinction est importante, car le nombre total de motifs rares peut être élevé, et ainsi nous avons privilégié les motifs dont le support est non nul.
Définition 3. Un motif est appelé motif à support nul si son support est égal à 0. Autrement, il est appelé motif à support non nul.
Pour tous les motifs rares nous avons déjà décrit l'ensemble des motifs rares minimaux. Pour les motifs à support nul, un sous-ensemble générateur minimal semblable peut être défini : Définition 4. Un motif est un générateur minimal à support nul (GMSN) si c'est un motif à support nul (ainsi tous ses sur-ensembles sont des motifs à support nul) et si tous ses sousmotifs sont des motifs à support non nul.
Sur la Figure 1 se trouvent deux GMSN : {BD} et {DE}. De plus, les GMSN forment une représentation condensée et sans perte d'information des motifs à support nul, c'est-à-dire qu'à partir des GMSN tous les motifs à support nul peuvent être retrouvés avec leur support (qui est toujours 0). Pour cela, nous avons seulement besoin de générer tous les sur-motifs possibles des GMSN en utilisant les attributs de la base de données.
Trouver les motifs rares
Dans cette section nous présentons les deux étapes de notre méthode pour trouver les motifs rares. La première étape trouve seulement les motifs rares minimaux, tandis que la seconde retrouve les motifs rares non nuls à partir de l'ensemble des motifs rares minimaux.
Nous ne générons pas les motifs à support nul à cause de leur grand nombre. Pour éviter les motifs à support nul, nous utiliserons les GMSN. La seconde étape de notre méthode (voir Section 4.2) permet de restaurer tous les motifs rares non nuls à partir des MRM à l'aide d'une approche par niveau. Si un candidat a un sous-motif GMSN, alors ce candidat est de manière sûre un motif à support nul et peut être ainsi élagué. Autrement dit, à l'aide des GMSN nous pouvons réduire l'espace de recherche pendant que nous retrouvons tous les motifs rares.
Trouver les motifs rares minimaux
De manière surprenante, les motifs rares minimaux peuvent être trouvés simplement à l'aide de l'algorithme bien connu Apriori. Apriori est basé sur deux principes (voir Propriétés 1 et 2). Il est conçu pour trouver les motifs fréquents, mais, puisque nous sommes dans le cas où non fréquent signifie rare, cela a pour "effet collatéral" d'explorer également les motifs rares minimaux. Quand Apriori trouve un motif rare, il ne générera plus tard aucun de ses surmotifs car ils sont de manière sûre rares. Puisque Apriori explore le treillis des motifs niveau par niveau du bas vers le haut, il comptera le support des motifs rares minimaux. Ces motifs seront élagués et plus tard l'algorithme peut remarquer qu'un candidat a un sous-motif rare. En fait Apriori vérifie si tous les (k ? 1)-sous-motifs d'un k-candidat sont fréquents. Si l'un d'entre eux n'est pas fréquent, alors le candidat est rare. Autrement dit, cela signifie que le candidat a un sous-motif rare minimal. Grâce à cette technique d'élagage, Apriori peut réduire significativement l'espace de recherche dans le treillis des motifs.
Une légère modification d'Apriori suffit pour conserver les MRM. Si le support d'un candidat est inférieur au support minimum, alors à la place de l'effacer nous l'enregistrons dans l'ensemble des motifs rares minimaux (voir Algorithme 1).
Algorithme 1 (Apriori-Rare) :
Description : modification d'Apriori pour trouver les motifs rares minimaux Entrée : base de données + min_supp Sortie : tous les motifs fréquents + motifs rares minimaux
SupportCount(C i ) ; // compte le support des motifs candidats 6) R i ? {r ? C i | support(r) < min_supp} ; // R -pour les motifs rares 7)
; // C -pour les candidats 9)
i ? i + 1 ; 10) } 11) I MR ? R i ; // motifs rares minimaux 12) I F ? F i ; // motifs fréquents Fonction Apriori-Gen : à l'aide des k-motifs fréquents, génère les potentiellement fréquent candidats de taille (k + 1). Potentiellement fréquent signifie ne pas avoir de sous-motif rare, c'est-à-dire pas de sous-motif rare minimal. Inclure un motif rare implique être rare (voir Propriété 2). Pour une description détaillée de cette fonction consulter Agrawal et al. (1996).
L'exécution de l'algorithme sur la base de données D (Tableau 1) avec un support minimum de 3 (équivalent à un support maximum de 2) est illustrée dans le Tableau 2.
En prenant l'union des R i , l'algorithme trouve les motifs rares minimaux ({D} avec support 1, {AB} et {AE} avec support 2).
Dans la prochaine sous-section, nous montrons comment restaurer les sur-motifs des MRM (c'est-à-dire comment reconstruire tous les motifs rares) en évitant les motifs à support nul.
Retrouver les motifs rares
Tous les motifs rares sont retrouvés à partir des motifs rares minimaux. Pour cela nous avons besoin de générer tous les sur-motifs possibles des MRM. Les générateurs minimaux à support nul sont utilisés pour filtrer les motifs à support nul pendant la génération des surmotifs. De cette manière l'espace de recherche peut être réduit de manière considérable. Dans cette section nous présentons un algorithme prototype pour cette tâche appelé Arima 5 (A Rare Itemset Miner Algorithm, voir Algorithme 2). L'exécution de l'algorithme sur la base de données D (Tableau 1) avec un support minimum de 3 (équivalent à un support maximum de 2) est illustrée dans le Tableau 3.
L'algorithme prend d'abord le plus court MRM, {D}, qui est rare et ainsi copié dans R 1 . Ses sur-motifs de taille 2 sont générés et stockés dans C 2 ({AD}, {BD}, {CD}, et {DE}). Avec un passage sur la base de données leur support peut être compté. Puisque {BD} et {DE} sont des motifs à support nul, ils sont copiés dans la liste des GMSN. A partir des MRM, les 
2-motifs sont ajoutés à C 2 et les motifs non nuls sont stockés dans R 2 . Pour chaque motif rare dans R 2 tous ses sur-motifs sont générés. Par exemple, à partir de {AD} nous pouvons générer les candidats suivants : {ABD}, {ACD} et {ADE}. Si un candidat possède un sousmotif GMSN, alors le candidat est de manière sûre un motif à support nul et peut être élagué ({ABD}, {ADE}). Les candidats potentiels non nuls sont stockés dans C 3 . Dans les C i les doublons ne sont pas permis. L'algorithme s'arrête quand R i est vide. L'union des R i donne tous les motifs rares à support non nul. A la fin nous avons aussi collecté tous les GMSN. Ainsi si on a besoin des motifs à support nul, cette liste peut être utilisée pour les retrouver. Le procédé est similaire : nous aurions besoin de générer tous les sur-motifs possibles des GMSN. Dans notre cas nous ne nous sommes intéressés qu'aux motifs non nuls, mais il est possible de travailler avec les motifs à support nul.
Conclusions et travaux futurs
Dans cet article, nous avons présenté une méthode pour extraire les motifs rares d'une base de données. Notre méthode est composée de deux étapes : 1) nous trouvons un sous-ensemble générateur minimal des motifs rares appelés MRM (algorithme Apriori-Rare) ; 2) à l'aide des MRM nous retrouvons les motifs rares dont le support est strictement supérieur à 0 (algorithme Arima).
Notre méthode fait partie des premières à s'intéresser spécifiquement aux motifs rares. Apriori fut le premier algorithme pour trouver les motifs fréquents et a été suivi par de nombreux algorithmes plus efficaces. De manière similaire, il ne fait aucun doute que nos algorithmes prototypes pourraient être améliorés de nombreuses manières. Dans le futur nous aimerions travailler sur ce sujet.
Parmi les motifs fréquents un certain nombre de sous-ensembles utiles ont été découverts, parmi lesquels les motifs fermés fréquents, les motifs fréquents maximaux, les générateurs (clés) minimaux, etc. Nous sommes curieux de découvrir si de tels sous-ensembles peuvent être définis pour les motifs rares. Nous connaissons déjà le complémentaire des motifs fréquents maximaux, qui est l'ensemble des motifs rares minimaux. Une autre question intéressante est la suivante. Les motifs fermés fréquents déterminent sans ambiguïté tous les motifs fréquents et leur support. Existe-t-il un sous-ensemble similaire qui déterminerait les autres motifs rares avec leur support ?
Un domaine important de l'utilisation des motifs rares est la génération des règles d'association rares. Par manque de place, nous n'avons pas pu développer ce sujet ici mais nous prévoyons d'étudier cette question en détail dans un autre article.
D'autre part, dans un futur proche, nous prévoyons de décrire des expériences réalisées sur des données réelles de la cohorte STANISLAS, dans le but de fournir un exemple concret de ce nouvel aspect prometteur de la découverte de connaissances dans les bases de données.
Références
Agarwal, R. C., C. C. Aggarwal, et V. V. V. Prasad (2000). Depth first generation of long patterns. In KDD '00 : Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 108-118. ACM Press.

Introduction
L'explosion des quantités de données stockées sur différents supports informatique conjointement à l'avènement des Technologies de l'Information et de la Communication a introduit des bouleversements importants dans le management des entreprises. En plus des connaissances explicites (courriers électroniques, procédures, notes de services, ...), il faut capitaliser l'ensemble des connaissances tacites, c'est à dire les connaissances qui ne sont pas formalisables aisément avec des mots (bonnes pratiques, savoir-faire, ...) (Alavi et Leidner, 2001;Earl, 2001). L'objectif est de rendre cette connaissance accessible aux utilisateurs concernés, de la conserver et de l'analyser pour la faire évoluer et par ce biais faire ainsi évoluer l'entreprise. La capitalisation, l'exploitation et l'enrichissement des connaissances se fait de plus en plus souvent par l'intermédiaire d'un système de gestion des connaissances (SGC) informatisé que l'on peut qualifier de mémoire d'entreprise (Dieng- Kuntz et al., 2001). Le processus de créa-tion d'une mémoire d'entreprise est un passage d'une mémoire de travail à une mémoire organisationnelle qui se définit comme un capital de connaissances accessible indépendamment des acteurs qui l'ont créée (Prax, 2003).
La construction de cette mémoire dépend des sources de connaissances disponibles et valides qui peuvent être utilisées tels que de la documentation technique, les experts humains ou des courriers électroniques. Selon les sources et les objectifs définis conjointement avec les utilisateurs, différentes approches ont été proposées : GED, mémoire documentaire, mémoire à base de connaissances, mémoire à base de cas, ... Chaque approche est associée à des techniques spécifiques de recueil de données comme des entretiens, des observations de l'expert en situation de travail ou de la transmission via un éditeur. Pour de nombreuses formes d'interaction, des outils de visualisation adaptés s'avèrent des médiateurs efficaces, facilitateurs de dialogue (Aubertin et al., 2003;Colloque ESIEE, 2002). Ils permettent d'apporter aux intervenants humains un substrat artificiel qui transcrive un grand nombre d'informations et qui soit un support à leurs connaissances et à leurs intuitions pour qu'ils puissent non seulement plus facilement exprimer leurs savoirs tacites et implicites mais aussi découvrir des nouvelles connaissances (e.g. relations). D'une façon plus générale, la visualisation de connaissances est un domaine naissant en plein essor (Eppler et Burkhard, 2005) et l'analyse de cet aspect fondamental dans un processus de gestion des connaissances (GC) n'en est qu'à ses débuts (Burkhard, 2004).
Dans cet article nous nous focalisons sur l'intégration de la visualisation dans la phase d'opérationnalisation d'une mémoire à base de connaissances. Le serveur de connaissances Atanor qui nous sert d'environnement de référence utilise une approche orientée vers le dé-ploiement de connaissances portant sur des systèmes complexes et provenant de sources multiples (Guillet et al., 2002). On peut citer comme exemples réels d'applications l'aide à la maintenance de machines de tri de courrier, le maintien en fonctionnement de sous-marins ou de navires de surface, le dépannage automobile.
Dès la conception d'Atanor, une formalisation graphique des connaissances basée sur une adaptation des arbres de décision et de défaillances a été proposée (Guillet et al., 2000). Cependant, l'instanciation de ce modèle visuel dans différents contextes applicatifs a mis en évidence des redondances qui peuvent entraver l'interprétation synthétique du fonctionnement d'un processus et masquer des points critiques. Bien que pour des raisons implicites de simplicité, les modèles d'arbres soient souvent privilégiés en GC, nous discutons ici de l'intérêt de l'introduction d'un modèle graphique basé sur des graphes qui sont des outils privilégiés bien connus pour modéliser un système de relations entre des entités. Ils permettent de caractériser préci-sément des propriétés d'un tel système via un arsenal combinatoire sophistiqué (Berge, 1973) tout en facilitant l'accès au profane à des structures complexes via notamment des représen-tations visuelles adaptées et nous présentons une comparaison entre ces deux approches pour une application réelle. Dans cet article nous proposons une extension du modèle d'arbre à un modèle de graphe orienté en niveaux.
Le reste de cet article est organisé de la façon suivante : le paragraphe 2 propose un état de l'art des différentes approches pour la visualisation des connaissances dans les systèmes de GC. Un descriptif général du serveur de connaissances Atanor est rappelé dans le paragraphe 3. Le paragraphe 4 décrit la représentation par logigrammes sous forme arborescente et présente le nouveau modèle de graphes. Une illustration dans le cadre de la maintenance de machines de tri de courrier de la Poste est décrite dans le paragraphe 5.
Visualisation graphique en Gestion des Connaissances
Les représentations visuelles proposées dans les systèmes de gestion des connaissances sont pour la plupart basées, au moins implicitement, d'un point de vue formel sur des modèles d'arbres ou plus généralement de graphes. D'un point de vue théorique, ces méthodes ont pour la plupart été initialement développées dans la communauté de la visualisation d'information (Herman et al., 2000). D'un point de vue applicatif en GC, la paire indissociable {modèle, représentation visuelle} dépend à la fois des connaissances dont on dispose, du mode de raisonnement sur ces connaissances et des différents points de vue "utilisateurs" considérés dans le SGC. Nous nous restreignons ici aux références associés à des applications en GC.
De façon générale, les modèles de visualisation sont souvent basés sur le modèle générique des réseaux sémantiques (Lehmann, 1992). Ils représentent avec les sommets d'un graphe des concepts et avec les arcs des relations sémantiques entre ces concepts. La majeure partie des techniques présentées ci-dessous pourrait être considérée, dans le cadre d'une représentation descriptive, comme des spécialisations des réseaux sémantiques.
Représentation par arbres
Les représentations sous forme d'arbres qui sont parmi les plus abouties, regroupent des techniques très différentes :
-Les cartes cognitives (Buzan et Buzan, 1996)  
Représentation par graphes
La plupart des représentations par graphes en GC se retrouvent associées à trois grandes classes de modèles dont les intersections peuvent être importantes : les graphes conceptuels, les ontologies et les réseaux bayésiens.
-Les graphes conceptuels ont été à l'origine proposés comme une représentation graphique de la logique de premier ordre. Ils permettent de simplifier la mise en relation entre la logique et les langues naturelles (Sowa, 1992)   (Corby et Dieng, 1998).
Descriptif général du serveur de connaissances Atanor
Le serveur de connaissances Atanor dans lequel s'intègre notre travail est construit autour de quatre modèles associés à des représentations graphiques :
1. un "modèle d'expertise" représentant les processus métiers qui permet de maintenir des connaissances procédurales exprimées sous formes de règles de raisonnement ; 2. un "modèle organique" qui permet de décrire la structure du système à travers sa décom-position organique ;
3. un "modèle des compétences" associées au système décrivant les compétences des acteurs sur le système (de l'équipe à l'entreprise) en les hiérarchisant du plus global au plus spécifique ;
4. un organigramme des personnes associé à un modèle de compétences pouvant intervenir sur le système (Vergnaud et al., 2004 Ces modèles interagissent entre eux : les connaissances portent sur des composants d'un système dont la manipulation nécessite des compétences elles-mêmes portées par des individus de l'organisation en charge de ce système (voir la figure 1). L'architecture technique d'Atanor est basée sur un serveur de connaissances réalisé majoritairement en Prolog. Le choix de ce langage pour l'implémentation du serveur facilite la gestion interne des connaissances recueillies, mais surtout permet d'activer ces connaissances. Le serveur est composé d'un ensemble de modules proposant différentes vues sur les modèles dont le module Expert qui permet aux experts de capitaliser la connaissance en décrivant les modèles et leurs associations.
Des logigrammes aux graphes en niveaux pour le module Expert
Nous nous focalisons dans la suite sur les représentations du modèle d'expertise qui est associé au module Expert car il représente ici la mémoire organisationnelle. Il permet de représenter des connaissances procédurales actionnables liées à un savoir-faire se décomposant en une suite d'étapes. Par exemple, dans le cas d'un diagnostic pour l'aide à la maintenance d'un système industriel, la stratégie mise en oeuvre par les experts consiste à tester successivement des hypothèses sur l'état des composants ou des fonctionnalités du système, et ceci en procédant généralement des hypothèses les plus simples aux plus complexes.
Représentation visuelle par des logigrammes
Le premier modèle proposé, appelé logigramme d'expertise, associe chaque étape du raisonnement de l'expert à un sommet d'un arbre. La représentation graphique des logigrammes a été présentée à l'origine comme une généralisation enrichie des arbres de décision et des arbres de défaillance (Figure 2). Deux types de sommets structurants sont mis en évidence :
(1) Les sommets tests associés à une variable, typiques des arbres de décision, dont les fils ne sont pas ordonnés, mais dont chaque arc est associé à une valeur de la variable. La variable est généralement associée à l'état dans lequel se trouve un élément du système sous-jacent ; (2) les sommets modules, absents des arbres de décision et de défaillance, dont les fils sont ordonnés de gauche à droite et généralement du plus simple au plus complexe, au sens de l'expert. Chacun de ces sommets permet de définir un "module de connaissances" permettant d'intégrer -315 -RNTI-E-6 des principes cognitifs caractéristiques des stratégies de décision expertes dont un principe de parcimonie/décidabilité (Barthélemy et Mullet, 1992) : les premiers sommets fils d'un module permettent d'arriver à une décision à moindre coût par des opérations simples (parcimonie) et les sommets fils suivant offrent la possibilité de réaliser des opérations de plus en plus complexes afin d'arriver à une prise de décision même si elle s'avère coûteuse (décidabilité). Il existe aussi deux types de sommets terminaux : les sommets associés à un diagnostic indiquant une résolution du problème ainsi que la réparation à effectuer et les sommets associés à un échec indiquant une non résolution du problème et provoquant la mise en oeuvre d'un mécanisme de retour au dernier sommet module traité et la transition au sommet suivant au sens de l'ordre induit par ce sommet module.
Une propriété importante de cette formalisation graphique réside dans la possibilité de transformer un logigramme en un ensemble de règles de production, en traduisant l'ensemble des chemins menant de la racine à chacune des feuilles. Ainsi le logigramme de la figure 2 se transforme en 4 règles : 
FIG. 3 -Exemple de graphe en niveaux pour le modèle d'expertise.
La représentation graphique des connaissances par logigramme a l'avantage d'être beaucoup plus intelligible et synthétique qu'un ensemble équivalent composé d'une liste de règles de production. Cependant, des sous-arbres correspondant à des sous-ensembles de règles de production utilisés dans différentes phases peuvent être dupliqués à l'issue de la phase d'expertise. Cette duplication lorsqu'elle est fréquente peut nuire à l'intelligibilité de la représentation visuelle.
Le modèle Graph'Atanor et sa représentation visuelle
Pour palier aux limites du logigramme d'expertise, nous avons développé un modèle de graphe en niveaux. Ce modèle a pour avantage de pouvoir exploiter directement le modèle Prolog qui associe directement un sommet avec l'ensemble de ses fils sans redondance. Dans le modèle de graphes, les sommets représentent comme dans les logigrammes les tests, les modules ou les diagnostics. La différence majeure est ici l'unicité ; un sommet ne peut pas être dupliqué. Ces sommets sont ordonnés dans des niveaux : en notant 1 le premier niveau, le niveau i contient les sommets dont la longueur du plus long chemin originaire du niveau 1 vaut i. Les arcs représentent pour les sommets tests les différentes valeurs possibles de la variable associée à ce sommet. Pour les sommets modules, comme dans les logigrammes, les arcs sont associés à un numéro d'ordre qui définit l'ordre de priorité de la transition (voir la figure 3).
L'objectif général de la représentation visuelle d'un tel graphe est de fournir un tracé intelligible sur un support standard. La qualité du dessin est décisive pour l'appropriation de la représentation par l'utilisateur (Purchase, 2000). Lorsqu'une convention de tracé est donnée (ici le tracé en niveaux), on retient en général deux concepts de base (Di-Battista et al., 1999) :
1. les contraintes du support et de l'oeil humain qui imposent notamment des écarts minimums à respecter entre les sommets et les arcs.
-317 -RNTI-E-6
Visualisation graphique en GC -le modèle Graph'Atanor 2. les critères "esthétiques" qui définissent les propriétés à satisfaire pour faciliter l'intelligibilité. Ces critères sont définis par des contraintes combinatoires : minimisation du nombre de croisement d'arêtes, minimisation de la somme des longueurs des arêtes, minimisation des coudes dans certains types de tracés, ... La plupart de ces critères ne peuvent cependant pas être satisfaits simultanément. Des travaux récents en psychologie cognitive ont montré que la réduction des croisements d'arêtes est le critère prépondérant pour la lisibilité et la mémorisation (Purchase, 1997).
Dans le cas d'un tracé en niveaux, la minimisation du nombre de croisements d'arcs peut sembler plus simple que le problème plus général de minimiser le nombre de croisement d'arêtes sur un graphe quelconque puisque le choix de coordonnées géométriques pour les sommets est ici remplacé par le choix d'un ordre des sommets sur chaque niveau. Le problème reste néanmoins NP-complet (Garey et Johnson, 1983). Un grand nombre d'heuristiques pour ce problème suivent le principe du balayage successif des différents niveaux : les sommets de chaque niveau sont réordonnés de façon à réduire le nombre de croisements d'arcs. Des stratégies très variées ont été proposées pour le réor-donnancement (e.g. Laguna et al. (1997) pour plus de détails). Les plus utilisées sont basées sur des méthodes de tris qui utilisent le nombre de croisements d'une façon proche des tris classiques et des heuristiques basées sur le principe selon lequel le nombre de croisements diminue si un sommet se trouve à peu près au milieu de ses voisins sur les niveaux adjacents (Sugiyama et al., 1981). Plus récemment différentes méthaheuristiques ont été développées pour ce problème : recherche tabou (Laguna et al., 1997), GRASP (Marti, 2001) et les algorithmes génétiques hybridés dont on a expérimentalement montré qu'ils sont très compétitifs et peuvent présenter des avantages certains dans le cadre d'un tracé interactif (Pinaud et al., 2004), cadre privilégié en GC. Ainsi, la représentation visuelle de Graph'Atanor a été effectuée par un algorithme génétique hybridé avec deux spécificités majeures (Kuntz et al., 2006) : deux opérateurs de croisements adaptés aux représentations en niveaux, et une recherche locale combinant différentes transformations locales.
Analyse comparative des modèles graphiques
Nous avons comparé les logigrammes et les graphes en niveaux dans le cadre applicatif de l'aide au diagnostic pour la maintenance de machines de tri automatique de courrier (La Poste). Après une période d'adaptation et de formation à l'éditeur de connaissances, les experts ont commencé à assurer la construction, la mise à jour et l'évolution des connaissances maintenues par l'outil. La phase de recueil des connaissances s'est étalée sur deux ans et s'est appuyée sur quatre experts géographiquement dispersés. Elle a permis de mettre en évidence une trentaine de pannes possibles, nécessitant la construction d'un logigramme par panne. Les experts ont ainsi fait apparaître plus de 400 sommets tests et environ 200 diagnostics différents ont été répertoriés.
La figure 4-a propose un extrait d'un logigramme pour la résolution d'une panne précise. La figure 4-b est un extrait du graphe en niveaux pour la même panne. Pour obtenir un dessin le plus lisible possible, la contrainte qui impose un ordre des fils dans les sommets modules dans le modèle d'arbres est relaxée. Pour les sommets tests, les réponses aux tests permettant de choisir le chemin à suivre ne sont pas affichées. Plusieurs avantages apparaissent clairement sur la représentation avec un graphe. Le principal est ici que les parties dupliquées dans le logigramme ne le sont plus dans le graphe. Cette non-duplication permet une meilleure exploitation du tracé par l'expert ou le manager :
-on voit clairement qu'un même diagnostic peut être effectué rapidement en un nombre minimum de tests ou plus lentement avec plus d'étapes, -un même diagnostic peut avoir des effets multiples (non représenté ici), -les sommets avec un degré important ont statistiquement plus de chances d'être utilisés dans les diagnostics et donc une attention particulière préventive pourra être portée sur les parties du système concernées (maintenance préventive). Une conséquence immédiate est de pouvoir améliorer la répartition des experts ou des techniciens pour être certain d'avoir toujours une personne compétente sur les pannes risquant de se produire souvent. De plus, le comptage des passages dans chaque sommet différent est plus simple dans le graphe que dans le logigramme. Cette statistique permet à l'expert ou au manager de recenser les composants du graphe peu utilisés (pannes peu fréquentes) de ceux qui le sont fréquemment (pannes fréquentes pouvant indiquer une faiblesse dans le système).
Conclusion
La visualisation graphique des données en GC est d'une importance majeure pour une bonne utilisation des SGC et leur appropriation par les utilisateurs. Nous avons montré dans le cadre d'un véritable exemple industriel que les représentations en arbres, qui sont parmis les plus abouties, sont limitées par rapport à un modèle de graphes. Ce modèle qui peut paraître à priori plus compliqué, permet d'améliorer les représentations visuelles pour s'assurer d'une meilleure exploitation des modèles de connaissances par les experts.
Bien qu'étant de taille limitée et utilisant peu de données, les représentations de la figure 4 sont déjà trop grandes pour tenir correctement sur un support standard (feuille de papier ou écran) tout en restant lisible. Par exemple, le graphe permettant de représenter l'ensemble des pannes de la machine de tri de courrier compte 553 sommets et 625 arcs. Ce graphe permet d'apporter des informations supplémentaires intéressantes pour les experts mais même après optimisation du tracé, il reste encore environ un millier de croisements d'arcs. Ce graphe ne peut donc pas être représenté entièrement sur un écran et d'autres méthodes complémentaires de visualisation adaptées pour ces grandes structures de données sont à envisager (Munzner, 2000). De plus, lors de la saisie des graphes par les experts, il est intéressant d'optimiser le tracé après l'ajout de sommets et d'arcs pour conserver un graphe lisible et compréhensible. Dans ce cas le tracé devient intéractif et les algorithmes de visualisation doivent prendre en compte le tracé obtenu à l'instant t avant de produire celui de l'instant t + 1 pour respecter au mieux la carte mentale de l'utilisateur (Eades et al., 1991) et ainsi éviter à ce dernier de dépenser une énergie cognitive importante inutilement pour redécouvrir le tracé.
Références
Aissaoui, G., D. Genest, et S. Loiseau (2003). Le modèle des cartes cognitives de graphes conceptuels : un modèle graphique d'aide à la prise de décision. In Actes 2° journées francophones Modèles Formels de l'Interaction (MFI), pp. 243-248. Cepaduès.

Introduction
Les méthodes de fouille visuelle de données ("Visual data mining") tentent de résoudre les problèmes d'interprétation et d'interaction dans les processus de découverte de connaissances en faisant appel à des visualisations dynamiques et à des requêtes graphiques sur les données et connaissances représentées (Cleveland, 1993), (Shneiderman, 1996), (Wong et Bergeron, 1997). A titre d'exemples classiques, nous pouvons citer les visages de Chernoff (Chernoff, 1973) qui représentent des données sous la forme d'icones en s'appuyant sur le fait que l'esprit humain analyse facilement les ressemblances et différences entre visages. Nous pouvons citer également les "scatter plots" (Becker et Cleveland, 1987) qui permettent d'obtenir des vues multiples sur les données et d'observer les données à l'aide de techniques graphiques comme le "brushing" qui donne la possibilité de sélectionner des données dans une vue tout en soulignant ces mêmes données dans les autres vues.
Ces méthodes apportent des nouveautés et poursuivent des objectifs qui sont prometteurs pour le domaine de la fouille de données : utilisation de la perception visuelle et souvent de la perception pré-attentive (Healey et al., 1993), interaction dynamique avec les données, simplicité d'utilisation, exploitation directe des résultats. Cependant, ces méthodes ont également des limites en ce qui concerne la fouille de données : les données visualisées sont le plus souvent numériques, les visualisations et leur manipulation nécessitent un apprentissage (comme c'est le cas par exemple pour interpréter des graphiques de types "parallel coordinates" (Inselberg, 1985)), l'interaction dynamique demande beaucoup de ressources de calcul (modifications en temps réel) et doit donc faire appel à des algorithmes les plus rapides possible (mais qui doivent par ailleurs fournir le plus d'information possible).
Dans ce travail, nous proposons une nouvelle méthode de fouille visuelle de données, adaptée elle-même des méthodes à base de points d'intérêt utilisées pour la visualisation de données textuelles. Nos objectifs, outre ceux poursuivis par la fouille visuelle de données, sont les suivants : pouvoir représenter tous les types de données en se basant uniquement sur l'existence d'une fonction de similarité (ou de distance) entre les données, avoir des affichages très rapides lors des interactions dynamiques et traiter si possible de grands volumes de données (algorithmes de complexités temporelle et spatiale linéaires en fonction du nombre de données), utiliser une visualisation nécessitant un temps d'apprentissage le plus court possible (donc compréhensible par la majorité des utilisateurs potentiels qui ne sont pas considérés comme des experts en fouille de données).
L'article est organisé comme suit : la section 2 décrit les techniques initiales utilisant les points d'intérêts dans le contexte de la visualisation de données textuelles. Dans la section 3 nous décrivons notre approche en commençant par spécifier l'utilisation des points d'intérêt pour la fouille de données puis en évaluant cette méthode sur des données aux caractéristiques connues. Dans la section 4 nous décrivons l'application de notre méthode sur des données classiques, puis le contexte d'application réelle de cette étude, à savoir extraire visuellement des connaissances à partir de données issues d'enquêtes de satisfaction. Nous concluons et dégageons des perspectives dans la section 5.
Survol des méthodes visuelles à base de points d'intérêts
Cette méthode est désignée par les termes "points d'intérêts" ou "points de références" (en anglais POIs pour "Points Of Interest"). Il s'agit dans cette méthode de positionner sur une surface, représentée par un disque, quelques icônes (POIs) relatifs aux attributs d'une donnée et ensuite d'afficher les icônes des données à des positions déterminées par la similarité entre les POIs et les données. Par exemple, cette visualisation a été utilisée comme méthode d'affichage de documents issus d'un moteur de recherche, facilitant la navigation dans l'ensemble des documents retournés par une requête. Les POIs sélectionnés sont généralement des motsclés utilisés dans la requête et les données sont les documents se positionnant par rapport à ces mots-clés. Le choix des mots-clés dépend de la fréquence de leur occurrence dans les documents. Pour visualiser ces données, on utilise en général pour cela des techniques à base de ressorts et de forces, la force s'exerçant entre un POI et une donnée étant proportionnelle à la similitude entre ce POI et cette donnée. Les systèmes VIBE (Korfhage, 1991), ou un système rigoureusement simplifié de VIBE (Morse et al., 2002), SQWID (McCrickard et Kehoe, 1997), Radviz (Hoffman et al., 1999) ou encore la visualisation radiale du système Information Na- (Au et al., 2000) (image gracieusement fournie par Stefan Rüger).
FIG. 1 -Exemple de visualisation radiale
vigator (Au et al., 2000) utilisent ces principes. Parfois il est difficile de voir exactement vers quel point d'intérêt est attiré une donnée. Dans ce cas ces systèmes permettent alors de supprimer et d'ajouter des points d'intérêt sur le disque pour permettre une meilleure représentation des données. Ce sont les principales opérations interactives proposées par ces méthodes.
Radial (Au et al., 2000) est une visualisation qui est très semblable au système VIBE (Korfhage, 1991), à Radviz (Hoffman et al., 1999) et à Lyberworld (Hemmje et al., 1994).
Initialement, après extraction du résultat de la requête, Radial identifie une série de termes clés relatifs à ce résultat. Ensuite les 12 premiers termes les plus recensés dans l'ensemble du résultat sont arrangés tout autour d'un cercle. Il est possible de modifier la liste des termes affichés, le choix se faisant sur deux listes placées à gauche de l'écran. Un nuage de points est alors affiché à l'intérieur du cercle, un point représente une donnée. Il n'est affiché que les données en rapport avec les mots-clés alignés autour du cercle. Un point est comme suspendu par des ressorts reliés aux mots-clés en rapport avec celui-ci. Il est donc impossible de déplacer un point en cliquant dessus, du fait des forces exercées par les ressorts. Par contre, en cliquant sur un point, les mots-clés en rapport avec ce point sont éclairés et une bulle affiche des informations sur cette donnée. Il est possible de déplacer les termes à l'extérieur du cercle et ainsi de déplacer tous les noeuds des données en rapport avec ces termes. Ceci permet de faire un classement manuel des résultats en catégories (voir la figure 1).
Tous ces systèmes ont montré que ce type de visualisation dynamique apporte un grand intérêt pour l'utilisateur qui peut extraire lui-même de l'information en toute simplicité. La rapidité d'affichage couplée à la possibilité d'interaction apportent un plus à ces méthodes. Par ailleurs, elles peuvent a priori visualiser des données de différentes natures comme des données symboliques ou numériques. A notre connaissance, ces méthodes à base de points d'intérêt n'ont pas encore été utilisées pour la fouille visuelle de données comme nous allons le présenter dans les sections qui suivent.
FIG. 2 -Principes de base de la visualisation avec illustration du positionnement d'une donnée D i en fonction des k points d'intérêt.
3 Utilisation des points d'intérêt pour la fouille de données
Principes de base de la visualisation
On considère n données D 1 , ..., D n et une matrice de similarité Sim entre ces données. Sim(i, j) est la similarité entre les données D i et D j , cette matrice étant symétrique et avec une diagonale à 1. On note également que si Sim(i, j) = 1 alors les données D i et D j sont identiques, et que si Sim(i, j) = 0 alors elles sont totalement différentes.
Dans un premier temps, nous allons considérer que les POIs sont un sous-ensemble de ces données notés D 1 , ..., D k . Nous affichons ces k données sur un cercle avec un arc de longueur constante entre chaque POI (voir la figure 2). On note par
On veut ensuite positionner les n ? k données restantes en fonction de leur similarité aux POIs D 1 , ..., D k . On utilise la formule suivante pour calculer les coordonnées d'affichage
le poids w j est calculé de la manière suivante :
Ajoute comme POI 
FIG. 3 -Principales interactions avec les données et les POIs.
Si D i est identiquement similaire à l'ensemble des POIs, elle sera affichée au milieu du cercle. Inversement, si elle est totalement similaire à un POI et totalement différente des autres, sa position sera confondue avec celle de ce POI. Si sa similarité est biaisée vers certains POIs, celle-ci aura tendance à se rapprocher de ces POIs.
Plus généralement, notre méthode est telle que deux données proches l'une de l'autre dans l'espace d'origine le seront donc également vis à vis des POIs, et elles se retrouveront donc proches dans l'espace 2D. L'espace visualisé devient donc un espace de distance entre des points choisis (les POIs) et les données. C'est de cette manière que cette méthode peut traiter tout type de données. Par contre, la réciproque de cette propriété n'est pas vraie : deux données proches dans l'espace 2D ne le sont pas forcément dans l'espace d'origine (tous les points à distance égale de deux POIs dans l'espace d'origine forment une droite médiatrice, et ne sont pas systématiquement proches les uns des autres). Il faudra utiliser d'autres méthodes pour lever ces ambiguïtés (voir la dernière section).
Enfin, l'affichage, comme nous le recherchons, nécessite très peu de calcul et ne demande que de calculer qu'une partie seulement des similarités (k × (n ? k)).
Plusieurs interrogations sont soulevées par cette méthode. Tout d'abord, le choix initial des points d'intérêt doit être effectué. Dans un premier temps, nous considérons que si les données sont supervisées (on dispose d'un label de classe), alors nous prenons le premier représen-tant de chaque classe comme POI initial. Il y aura donc autant de POIs que de classes dans la première visualisation proposée à l'utilisateur. Si les données sont non supervisées, nous choisissons les k premières données. D'autres choix automatiques sont possible (et certainement plus judicieux) comme nous le décrivons dans la dernière section, et nous rappelons qu'il s'agit ici de proposer des choix initiaux que l'utilisateur va pouvoir modifier interactivement et dynamiquement en fonction de ce qui est affiché (voir section suivante). Une deuxième interrogation vient de l'ordre des POIs : si un grand nombre de données sont attirées par deux POIs, alors il y a tout intérêt à ce que ces POIs soient proches les uns des autres sur le cercle. Une situation critique consisterait à placer ces POIs de manière diamétralement opposée, ce qui engendrerait une visualisation peu lisible (beaucoup de données au centre). Nous proposons une solution interactive à ce problème dans la section suivante, mais il est évident que des solutions automatiques peuvent être trouvées comme ordonner les POIs en fonction de leur similarité (voir dernière section). Il serait également possible de ne pas conserver un arc de longueur fixe entre les POIs, afin de représenter les similarités qui existent entre POIs.
Interaction avec la visualisation
Pour être réellement efficace, la visualisation d'information doit être interactive et permettre d'affiner dynamiquement l'affichage et de répondre aux requêtes graphiques de l'utilisateur. Dans la visualisation avec des POIs, l'utilisateur peut se poser au moins les questions suivantes : quelle est cette donnée (ou ce POIs), comment agrandir cette partie de la visualisation (zoom sans perte de contexte), comment changer de POIs (en enlever, en rajouter, changer leur ordre, et comme on va le voir dans cette section, définir des POIs qui ne soient pas nécessairement des données de l'ensemble de départ).
Lors du passage de la souris sur un point, nous indiquons donc quel est ce point. Ensuite, il est possible de "zoomer" sur une donnée par clic de la souris. Le zoom qui se déclenche alors effectue les opérations suivantes : il centre la donnée sur le centre du cercle, il agrandit la zone centrée sur cette donnée et repousse les autres données vers les bords de la visualisation. La déformation est calculée à l'aide d'une fonction hyperbolique. Ce zoom permet de grossir la vue tout en conservant l'ensemble des données. Actuellement cette fonction utilise des coordonnées cartésiennes en considérant le carré dans lequel est contenu le cercle de la repré-sentation, mais nous allons définir un zoom utilisant des coordonnées polaires, afin de laisser toutes les données à l'intérieur du cercle. La zone d'agrandissement sera plus petite mais les données resteront dans la zone habituelle.
En ce qui concerne les POIs, nous avons représenté sur la figure 3 les principales interactions possibles : tout d'abord, il est possible d'enlever un POI. Cela s'effectue très simplement en faisant glisser un POI à l'intérieur du cercle. Ce POI reprend alors sa place au sein des données. La vue est recalculée dynamiquement. Une transition dynamique et progressive est mise en place pour que l'utilisateur puisse suivre le changement de représentation. Ce dernier a la possibilité d'annuler son action, ce qui a pour effet de remettre le POI sur le cercle. Il est possible également de choisir une donnée et de la définir comme POI. Pour cela, on fait glisser la donnée sur le cercle. Si la donnée est placée sur un POI, elle remplace celui-ci, et si elle est placée entre deux POIs, elle s'insère en décalant les autres POIs de manière à maintenir constant les longueurs des arcs entre les POIs. Ces fonctionnalités sont très importantes puisqu'elles vont permettre à l'utilisateur de redéfinir à volonté la représentation.
Enfin, il est possible de généraliser les POIs de manière à ce qu'ils ne soient plus nécessai-rement des données, mais plus généralement tout point de l'espace de représentation et même tout objet pour lequel il est possible de calculer une similarité avec les données. Ainsi, on peut représenter des données "idéales", n'existant pas réellement, et par rapport auxquelles l'utilisateur voudrait positionner les données réelles. Nous présentons dans la section 4 une application -340 -RNTI-E-6 typique de cette fonctionnalité. Egalement, il serait possible de représenter par exemple une règle de décision comme un POI, et de placer les cas qui se rapprochent le plus de cette règle. Cette fonctionnalité offre de nombreuses perspectives en visualisant non plus seulement des données mais également des connaissances.
Autres propriétés
Pour illustrer le fonctionnement de notre méthode, nous avons représenté sur la figure 4 un exemple "jouet" de données dont on connaît parfaitement les caractéristiques. Dans cette figure, nous avons utilisé comme POIs les centres de chacune des classes. Après calcul de la matrice de similarité sur la base de la distance Euclidienne (les attributs sont préalablement normalisés), on constate que la représentation respecte l'organisation d'origine des classes et les relations de voisinage qui existent entre les données. A titre de comparaison, nous avons représenté les mêmes données avec les coordonnées parallèles, dont la représentation semble moins intuitive que notre approche. Nous montrons également dans cette figure qu'il peut être important de faire apparaître des rayons particuliers sur notre cercle. Ainsi, les rayons médians (en traits pleins) marquent la séparation entre les données attirées par l'un ou l'autre des deux POIs considérés. Ils permettent de dire qu'une donnée est plus proche de tel POI que de tel autre. Les rayons issus de chaque POI (traits en pointillés) marquent au contraire le fait qu'une donnée est à similarité égale entre les deux POIs opposés.
Sur la figure 5, nous avons utilisé une configuration différente des données "jouet". Une des classes est allongée, et l'on constate que l'on retrouve cette information dans notre visualisation. Cependant, d'autres dispositions des données (voir figure 6) donnent des informations locales plus difficiles à interpréter. Les relations de voisinage sont respectées.
Résultats
Bases artificielles et classiques
Nous avons évalué cette méthode sur un ensemble de bases composées de différentes données artificielles et classiques. La figure 7 représente la base de données artificielles Art1 composée de 400 données et de 4 classes. Nous illustrons en particulier les effets du zoom. Lorsque l'on dispose seulement de deux classes, les données des deux classes sont positionnées sur le segment de droite allant du POI1 au POI2 (cf. figure 8 où nous utilisons une base de 1000 données avec 2 classes). Pour aider l'utilisateur à mieux visualiser les données, on a permis l'ajout d'un POI supplémentaire tel que dans la figure 8(b) ou de plusieurs POIs ( figure 8(c)).
Enfin, nous avons testé notre approche sur des bases classiques (issues du "Machine Learning Repository", (Blake et Merz, 1998)). Nous avons ainsi représenté sur la figure 9 les données Iris (150 données, 3 classes), Wine (178 données, 3 classes) et Segment (2310 données, 7 classes). On retrouve les formes de classes usuelles (comme pour Iris et Wine par exemple).
Application réelle
Dans le cadre de ses activités, Agicom collecte des données issues d'enquêtes de satisfaction à l'aide de questionnaires. Ces données se présentent sous la forme d'un tableau
FIG. 4 -Exemple "jouet" de visualisation de données représentées par deux attributs numé-riques x1 et x2 (voir en a)), avec visualisation sous la forme de coordonnées parallèles (en b)), puis représentation avec des POIs (en (c) et (d)).
individus×variables où ces variables sont qualitatives, i.e. des variables dont les modalités sont symboliques et naturellement ordonnées ("ravi", "satisfait", "insatisfait", "déçu"' et "NSP" (Ne Sais Pas)). Pour qu'un consultant, ou qu'un expert, puisse exploiter ces données, il est important de pouvoir visualiser graphiquement la satisfaction des clients afin de détecter des correspondances possibles entre individus, de connaître l'évolution des clients d'un segment à l'autre, mais aussi de visualiser la relation existante entre une variable définie et les autres variables. Notre but est donc d'élaborer un outil de représentation graphique des résultats d'enquêtes de satisfaction contribuant au but de savoir comment améliorer la satisfaction des clients.
Nous avons évalué et testé notre méthode sur une première base Agicom1 composée de 31 données non supervisées. La figure 10(a) illustre cette première application dans laquelle les POIs ne sont pas des données mais des profils type de variables. Un profil correspond donc à une répartition de modalités (réponses) de cette variable. Ainsi, les POIs représentent différentes typologies connues de variables ou à priori (réponses très positives, combinaisons de modalités de variables).
Nous présentons un deuxième exemple sur la base Agicom2 (cf. la figure 10(b)). Dans cette (a) (b) (c)
FIG. 5 -Autres données "jouet" initiales (a), puis visualisation avec notre méthode, (b) et (c).
(a) (b) (c)
FIG. 6 -Autre disposition de données initiales (a) et visualisation, (b) et (c).
application, nous avons permis aux utilisateurs d'Agicom d'interagir sur les caractéristiques des POIs et sur le zoom (cf. la figure 10(b)). De plus, sur cette base nous avons permis la visualisation des différentes classes. Une validation avec des utilisateurs réels est en cours.
Conclusions et perspectives
Nous avons décrit dans cet article une nouvelle méthode de visualisation inspirée des travaux réalisés dans le contexte de la recherche documentaire sur les points d'intérêt. Elle consiste à transformer un espace d'origine représenté par une matrice de similarité en une représentation visuelle 2D de ces similarités. Cette méthode possède des atouts comme la rapidité d'affichage, une présentation intuitive des données et d'apprentissage plutôt rapide, des capacités interactives. Nous avons détaillé son comportement sur des données jouet, sur des données classiques et enfin dans le cadre d'une application réelle en cours de déploiement.
Plusieurs perspectives peuvent se dégager. Nous avons mentionné l'importance du choix des POIs ainsi que de leur disposition sur le cercle. Une première extension consiste à étudier l'utilisation d'un algorithme d'optimisation afin de trouver la disposition de POIs la plus efficace visuellement. Il s'agit donc de trouver, dans l'espace des permutations des k POIs choisis, l'ordre qui permettra de maximiser certaines propriétés comme la ressemblance entre deux POIs successifs sur le cercle. Une autre perspective importante consiste à étendre la visualisation de manière à enlever les ambiguïtés liées au chevauchement des points. Nous comptons utiliser une méthode d'affichage de graphe à base de forces et de ressorts afin d'éloigner les points qui se trouvent trop proches sur le graphe. Il s'agit aussi de distinguer les points qui sont placés au même endroit mais qui ont une similarité moyenne différente avec les POIs. Une approche 3D sera testée. Enfin, comme nous l'avons mentionné, nous allons ajouter une méthode classique de zoom hyperbolique travaillant sur des coordonnées polaires afin que les points ne sortent pas du cercle. 

Introduction
De plus en plus de connaissances scientifiques sont accessibles soit grâce à des documents publiés sur le web, soit dans des bases de données. Certaines de ces connaissances reposent sur des interprétations humaines de résultats d'expériences. Ces connaissances sont, entre autres, indispensables pour la vérification, la validation ou l'enrichissement du travail des chercheurs du domaine considéré. Mais la quantité énorme de données provenant de sources internes ou externes aux organisations rend très difficile la détection, le stockage et l'exploitation de ces connaissances. Ceci est le cas de la recherche dans le domaine de la biologie moléculaire et plus particulièrement dans le domaine des puces à ADN.
Les biologistes travaillant dans ce domaine manipulent de grandes quantités de données dans différentes conditions expérimentales et doivent se référer à des milliers de publications scientifiques liées à leurs expériences. Ces biologistes ont donc sollicité un support méthodo-logique et logiciel qui les aiderait dans la validation et/ou l'interprétation de leurs résultats et qui leur faciliterait la planification de nouvelle expérimentation.
C'est dans ce contexte que le projet MEAT a été proposé en fournissant des solutions permettant de remédier à ces problèmes.
Après la présentation du contexte général et de la problématique de ce travail, nous dé-taillons notre approche adoptée pour MEAT (Khelif et al, 2005) ainsi que les différentes composantes de notre architecture et nous concluons avec une comparaison avec des travaux similaires.
Contexte
La technologie des puces à ADN a été développée après le séquençage dans le but de dé-couvrir les fonctions des gènes dans différents contextes biologiques. Ces expériences permettent l'accès à des milliers de gènes simultanément et fournissent une masse énorme de données, ce qui engendre des difficultés pour les biologistes particulièrement dans la validation et l'interprétation des résultats obtenus.
Les besoins exprimés par les biologistes travaillant dans ce domaine sont: ? Une vue sur les expériences connexes : essayer d'identifier des relations entre les expé-riences (bds locales ou en ligne) et de découvrir des nouvelles pistes à explorer. ? Aide à la validation des résultats expérimentaux : en recherchant dans les articles traitant le phénomène étudié des informations qui argumentent, confirment ou infirment leurs hypothèses de départ, ce qui nécessite une richesse dans les annotations. ? Aide à l'interprétation des résultats : en identifiant de nouvelles relations entre gènes et/ou les interactions pouvant exister entre eux, avec des composants cellulaires ou des processus biologiques. Ces besoins nous ont conduits à réaliser le projet MEAT en collaboration avec les biologistes de la plate-forme puce à ADN de Sophia Antipolis (localisée à l'IPMC 1 ) qui distribue des puces pour les autres laboratoires français. Ce projet nous permet ainsi d'explorer l'intérêt d'un « web sémantique organisationnel » sur l'échelle d'une communauté.
Mémoire d'entreprise et web sémantique
Actuellement, les techniques du web sémantique peuvent jouer un rôle très important dans la gestion des connaissances et la construction des mémoires d'entreprise. En effet, les ontologies peuvent être utilisées dans la représentation des connaissances en fournissant un cadre formel pour décrire les différentes sources de connaissances et en guidant la création d'annotations sémantiques facilitant la description, le partage et l'accès à ces sources.
Dans (Dieng-kuntz, 2005), il est proposé de matérialiser une mémoire d'entreprise à travers un « web sémantique d'entreprise » en utilisant les ontologies pour formaliser le vocabulaire partagé dans une communauté, et les annotations sémantiques basées sur ces ontologies pour décrire les sources de connaissances hétérogènes (corpus textuels, base de données, experts…) et faciliter leurs accès via Internet/Intranet. riences ou les articles ; ce qui a mené au développement du module MeatEditor. 5. L'aide à la validation des résultats expérimentaux en proposant une recherche documentaire guidée par MeatOnto et utilisant les annotations sémantiques; nous avons donc dé-veloppé un module nommé MeatSearch. 6. L'aide à l'interprétation des résultats en faisant des inférences plus avancées sur les annotations sémantiques pour expliquer un comportement particulier; ce qui mène à d'autres fonctionnalités du module MeatSearch.
Base d'annotations
MeatOnto chargement
MeatSearch
MEDIANTE
Interfaces
FIG. 1 -Architecture de Meat
Les composantes de MEAT
L'ontologie : MeatOnto
Comme nous l'avons décrit précédemment notre but était de construire une ontologie qui décrit toutes les ressources du domaine des puces à ADN, nous avons donc opté pour une ontologie modulaire composée de trois sous-ontologies dédiées à différentes parties : ? UMLS : ce projet élaboré par la NLM (National Library of Medecine de Bethesda) propose depuis 1986 de mettre au point un langage médical unifié (Humphreys et al, 1993). Pour ce faire, ce langage repose sur : (1) un métathesaurus qui énumère tout le vocabulaire médical existant et qui comprend des millions de termes ; (2) un réseau sémantique constitué d'une hiérarchie de 134 types sémantiques et de 54 relations ; il représente une classification de tous les concepts représentés dans le métathesaurus ainsi que les relations pouvant exister entre eux. Par analogie, nous avons considéré, le réseau sémanti-que de UMLS comme une ontologie : la hiérarchie des types sémantiques est la hiérar-chie des concepts et les termes du métathesaurus sont des instances de ces concepts. ? MGED 4 : c'est une ontologie proposée pour décrire les expériences des puces à ADN afin de faciliter le partage des résultas (Stoeckert, 2003). Nous avons utilisé cette ontologie dans MEAT afin de décrire les expériences stockées dans MEDIANTE. ? DocOnto : Nous avons développé cette ontologie pour décrire des métadonnées sur les articles (auteurs, sources…) et sur les annotations (generated_by, validated_by…). Elle représente aussi la structure des articles (abstract, sentence, relation…) et fait le lien entre les articles et les concepts de UMLS (has_relation, speaks_about_genes…). Nous avons effectué le codage de l'ontologie UMLS automatiquement à l'aide d'un script permettant de traduire le réseau sémantique de son format textuel vers une ontologie représentée dans le format RDFS (McBride, 2004). L'ontologie DocOnto a été construite progressivement pour couvrir tous nos besoins concernant la description des connaissances contenues dans les articles.
MeatAnnot 3.2.1 Génération des annotations
Malgré ses avantages, la création d'une annotation sémantique est un processus difficile et coûteux pour les biologistes. Ceci nous a amené à développer le système MeatAnnot qui à partir d'un texte (articles fournis par les biologistes) permet la génération d'une annotation structurée, basée sur MeatOnto et qui décrit le contenu sémantique de ce texte.
MeatAnnot repose sur des outils de TALN (Traitement Automatique de la Langue Naturelle) : GATE (Cunningham et al, 2002), TreeTagger (Helmut, 1994) et RASP (Briscoe et al, 2002) et sur nos propres extensions dédiées à la détection des relations sémantiques et l'extraction des instances des concepts de UMLS.
Dans chaque phrase où il détecte une instance d'une relation sémantique de UMLS, MeatAnnot essaie d'extraire les instances des concepts liés par cette relation et génère une annotation décrivant cette interaction.
La méthode de génération est composée de trois phases : Phase 1 : Détection des relations Dans cette phase nous avons utilisé JAPE (Cunningham et al, 2002), un langage basé sur les expressions régulières et qui offre la possibilité de créer des grammaires permettant l'extraction d'informations d'un texte traité par GATE. Pour chaque relation de UMLS (interacts_with, plays_role…), nous avons créé manuellement une grammaire permettant d'extraire toutes ses occurrences dans le texte. Nous nous sommes basés sur l'analyse de telles occurrences dans un corpus initial d'articles scientifiques fournis par les biologistes.
L'exemple ci-dessous montre une grammaire permettant la détection des instances de la relation sémantique « Has an effect » dans toutes ces formes lexicales (has an effect, had effects, have a positive effect…).
{Token.lemme == "have"} | {SpaceToken} ({Token.string == "a"}| {Token.string == "an"})? ({SpaceToken})? ({Token.string == "additive"} | ({Token.string == "synergistic"}| {Token.string == "inhibitory"}| {Token.string == "greater"} | {Token.string == "functional"} | {Token.string == "protective"}| {Token.string == "monogenic"}| {Token.string == "positive"})? Après ces deux phases, vient l'extraction des termes candidats. MeatAnnot utilise une fenêtre de taille quatre (quatre mots successifs peuvent représenter un terme) et pour chaque terme candidat: s'il appartient à UMLS, il passe au mot suivant, sinon, la fenêtre d'extraction est diminuée jusqu'à ce qu'elle devienne nulle. L'interrogation de UMLS se fait à travers le serveur de connaissances UMLSKS qui offre l'accès à toutes les ressources de UMLS et qui permet de les interroger et d'y naviguer à distance. UMLSKS nous renvoie une réponse en XML (si le terme existe dans le métathesau-rus). Ce résultat est analysé par MeatAnnot qui en extrait toutes les informations nécessaires sur le terme (type sémantique, définition…). L'utilisation de ce serveur nous a offert une analyse linguistique plus fine car ce dernier traite quelques variations linguistiques simples (« development of lung » est reconnu comme « lung development ») et qui complètent le traitement fait par MeatAnnot à savoir la lemmatisation (récupérer la racine des mots).
Phase 3 : Génération de l'annotation Dans cette phase, MeatAnnot utilise le module RASP, qui affecte à chaque mot son rôle linguistique dans la phrase (sujet, objet…), ce qui permet d'identifier les concepts liés par la relation : Pour chaque relation détectée, MeatAnnot vérifie si les sujets et les objets sont des instances de concepts de UMLS et génère une annotation décrivant l'instance de la relation.
L'exemple ci-dessous résume les différentes étapes. Considérons la phrase suivante : "In vitro assays demonstrated that only p38alpha and p38beta are inhibited by csaids."
Étape 1: En appliquant les grammaires d'extraction de relations sur cette phrase, MeatAnnot détecte la présence de la relation « inhibits » (appartenant à l'ontologie UMLS). 
FIG. 3 -Résultat de RASP
-180 -RNTI-E-6 p38alpha et p38beta sont détectés comme étant les objets de la relation inhibits. csaids est détecté comme étant le sujet.
MeatAnnot génère ensuite une annotation RDF 5 pour chacune des instances.
<m: Pharmacologic_Substance rdf:about='csaids#'> <m:inhibits> <m:Gene_or_Genome rdf:about='p38alpha#'/> </m:inhibits > <m:inhibits > <m: Gene_or_Genome rdf:about='p38beta#'/> </m:inhibits > </m:Pharmacologic_Substance>
FIG. 4 -Exemple d'annotation RDF générée par MeatAnnot
Toutes les annotations décrivant les interactions dans un article sont stockées dans un ré-pertoire contenant les articles fournis par le biologiste. Ces annotations sont ensuite utilisées, soit pour faire de la recherche documentaire (retrouver un article parlant d'un gène particulier ou d'un phénomène biologique), soit dans un scénario plus complexe de recherche d'informations, comme la recherche de relations entre gènes ou autres entités biomédicales.
Validation des annotations
Afin de valider nos annotations, nous avons adopté une approche centrée utilisateur : nous avons choisi au hasard un corpus de test (2540 phrases) parmi les documents fournis par les biologistes et nous avons présenté les suggestions proposées par MeatAnnot aux biologistes à travers une interface de validation pour qu'ils évaluent leur qualité. Cette interface a été conçue de manière à présenter les annotations dans un format compréhensible (textuel) pour les biologistes, qui ne sont pas spécialistes de RDF.
Etant dans un contexte de recherche d'informations (RI), nous nous sommes basés sur des mesures classiques de RI et nous les avons adaptés à notre cas.
Au cours de cette phase, nous avons remarqué aussi que quelques suggestions de MeatAnnot sont considérées correctes mais inutiles pour les biologistes car elles décrivent soit des connaissances de base soit des connaissances vagues. Nous avons donc introduit une nouvelle mesure de qualité nommée utilité pour mesurer le taux des suggestions utiles. La troisième colonne décris le nombre de relations existant dans le texte mais que MeatAnnot n'a pas pu extraire. Ce silence est dû dans certains cas aux erreurs générées par les outils de TALN mais principalement aux relations déduites par les biologistes en lisant la phrase mais qui ne peuvent pas être générées automatiquement.
Mesures
Example de phrase: "Upon interferon-gamma induction, after viral infection for example, a regulator of the proteasome, PA28 plays a role in antigen processing."
Dans cet exemple, MeatAnnot extrait automatiquement la relation "PA28 plays_role antigen processing", mais le biologiste en lisant la phrase peut déduire, en utilisant ses connaissances implicites, une autre relation qui est "interferon-gamma have_effect PA28".
Enfin, MeatAnnot a une bonne utilité puisque 96% des suggestions correctes sont considérées utiles par les biologistes. Ces résultats montrent bien que MeatAnnot génère des annotations de bonne qualité, condition essentielle dans un contexte de recherche d'informations.
L'utilisation des annotations : MeatSearch
Le but étant d'utiliser les annotations générées par MeatAnnot ainsi que celles éditées par MeatEditor afin de faciliter la validation/l'interprétation des résultats des expériences, nous avons développé le système MeatSearch basé sur le moteur CORESE (Corby et al, 2004)  MeatSearch traduit les résultats de CORESE en présentation graphique et/ou textuelle qui est plus compréhensible par les biologistes. Il fournit aussi des informations complémen-taires, telles que le document ou la phrase à partir desquels elle est extraite, les auteurs et les personnes qui ont validé l'annotation ou fourni les articles. Cette richesse en information et cette traçabilité des annotations offrent de très intéressants scénarios d'utilisation.
L'utilisation de CORESE
Pour la formalisation de nos ontologies ainsi que nos annotations, nous avons choisi les langages RDFS et RDF, qui sont deux recommandations du W3C, respectivement pour la représentation des ontologies légères et pour la description des ressources du web en utilisant les annotations basées sur les ontologies.
Ce choix nous a permis d'utiliser CORESE afin de permettre de: ? Naviguer dans la base d'annotations en tenant compte de la structure des ontologies.
? Ajouter des règles qui complètent la base d'annotations. ? Raisonner sur des d'annotations construites à partir de sources différentes et hétérogè-nes afin de déduire des connaissances à la fois implicites et explicites sur un gène. ? Utiliser différents niveaux d'accès (admin, public, groupe…) à la base d'annotations.
Exemples d'utilisation
CORESE fournit un langage de requêtes pour les données RDF qui est proche du langage SPARQL 6 en cours d'élaboration au W3C; Ce langage de requêtes permet d'écrire des requêtes composées de combinaisons booléennes de triplets RDF.
Comme exemple, la requête suivante permet de retrouver toutes les relations entre le gène « cav3. Notre approche propose des solutions à quelques problèmes posés dans la discussion finale du groupe de travail du W3C dans le domaine des sciences de la vie 7 :
? 
Travaux connexes
La méthode sur laquelle MeatAnnot repose peut être comparée avec (a) les travaux exploitant l'extraction d'informations dans le domaine biomédical (Alamarguy et al, 2005) (Staab, 2002) (b) ceux sur la génération d'annotations sémantiques pour le web sémantique (Handschuh et al, 2003). Reposant sur des techniques linguistiques, notre approche diffère des méthodes basées sur les techniques d'apprentissage proposées par (Nédellec, 2002). Contrairement à (Golebiowska et al, 2001) notre approche permet de créer des annotations consistantes, non seulement, en des instances de concepts mais en des instances de relations, et le tout en reposant sur une ontologie déjà existante. Ces instances de relations peuvent relier les différents concepts de l'ontologie et pas seulement les gènes ou les protéines comme décrit dans (Proux, 2000).
Par rapport aux approches linguistiques pour extraire des relations sémantiques (Séguéla, 1999), nous ne visons pas l'aide à la création ou enrichissement d'une ontologie mais plutôt l'extraction d'informations à partir de textes pour générer des annotations sémantiques sur lesquelles raisonner pour la recherche d'information.
Le couple MeatAnnot/MeatSearch qui permet de générer des annotations sémantiques basées sur une ontologie et extraites à partir des textes et qui offre un système de recherche sémantique sur ces annotations, a plusieurs points communs avec le système proposé par (Muller et al, 2004) qui repose sur une ontologie mais ne l'utilise pas pour faire la recherche.
Perspectives
Comme perspective pour ces travaux, nous allons étudier l'extraction d'information à partir des graphiques et les tableaux, compte tenu de leur importance pour les biologistes, de manière à intégrer un nouveau module à MeatAnnot pour les prendre en compte. Nous approfondirons aussi les problèmes de la gestion de l'évolution de UMLS, ce qui impliquera d'approfondir l'évolution des annotations. Enfin, nous sommes en train d'étudier avec les biologistes plusieurs scénarios d'utilisation avec des requêtes typiques afin de leur faciliter l'utilisation de notre système et la navigation contextuelle dans la base d'annotations. 
Références

Introduction
L'analyse du comportement des utilisateurs d'un site Web, également connue sous le nom de Web Usage Mining, est un domaine de recherche qui consiste à adapter des techniques de fouille de données sur les enregistrements contenus dans les fichiers logs d'accès Web (ou fichiers "access log") afin d'en extraire des relations entre les différentes données stockées Cooley et al. (1999), Masseglia et al. (2003), Mobasher et al. (2002), Spiliopoulou et al. (1999). Ces derniers regroupent des informations sur l'adresse IP de la machine, l'URL demandée, la date, et d'autres renseignements concernant la navigation de l'utilisateur. Parmi les méthodes développées, celles qui consistent à extraire des motifs séquentiels Agrawal et Srikant (1995) s'adaptent particulièrement bien au cas des logs mais dépendent du découpage qui est fait des données. Ce découpage provient soit d'une décision arbitraire de produire un log tous les x jours (e.g. un log par mois), soit d'un désir de trouver des comportements particuliers (e.g. les comportements des internautes du 15 novembre au 23 décembre lors des achats de Noël). Pour comprendre l'enjeu de ces travaux, prenons l'exemple d'étudiants connectés lors d'une séance de TP. Imaginons que ces étudiants soient répartis en 2 groupes. Le groupe 1 était en TP le lundi 31 janvier. Le groupe 2 en revanche était en TP le mardi 1 er février. Chacun de ces
Motifs séquentiels et Web Usage Mining
Ce paragraphe expose et illustre la problématique liée à l'extraction de motifs séquentiels dans de grandes bases de données. Il reprend les différentes définitions proposées dans Agrawal et Srikant (1995), Masseglia et al. (1998 Les principes généraux de l'utilisation des motifs séquentiels dans le cas des logs d'accès Web sont similaires à ceux du processus d'extraction de connaissances exposé dans Fayad et al. (1996). Les données brutes sont collectées dans des fichiers access log des serveurs Web. La démarche se décompose en trois phases principales (prétraitement, fouille de données et interprétation par l'utilisateur des résultats obtenus).
Dans les grandes lignes, notre objectif est d'énumérer l'ensemble des périodes issues du log à analyser afin de déterminer quelles sont celles qui contiennent des motifs séquentiels fréquents. Soit C l'ensemble des clients du log et D l'ensemble des dates enregistrées.
Définition 3 L'ensemble P des périodes possibles sur le log est défini de la manière suivante :
Dans la définition suivante, nous considérons que d min (c) et d max (c) sont les dates d'entrée et de sortie de c dans le log (première et dernière action enregistrées pour c). 
t).}
Dans la définition 5, la condition 1 exprime le fait qu'il n'existe pas de période plus grande qui soit en "contact" avec P stab et qui concerne les mêmes clients. La condition 2 exprime le fait que toutes les périodes de P stab concernent les mêmes clients.
Définition 6
Une période stable p est dite dense si C p contient au moins un motif séquentiel fréquent respectant le support minimum spécifié par l'utilisateur proportionnellement à |C p |.
Extraire les motifs séquentiels fréquents sur chacune de ces périodes avec une méthode classique n'est pas une solution envisageable en raison du nombre de périodes stables (environ 2 millions pour 14 mois de log dans nos expérimentations). Dans la mesure où notre proposition est basée sur une heuristique, notre but est de fournir un résultat répondant aux critères suivants : Pour chaque période p appartenant à l'historique du log, soit resultatReel le résul-tat à obtenir (le résultat qu'obtiendrait un algorithme de fouille de données qui explore tout l'ensemble des solutions après analyse des clients de C p ). resultatReel est alors l'ensemble des motifs séquentiels à trouver. Soit resultatP er les résultats obtenus par la méthode proposée dans cet article. Nous voulons minimiser taille(resultatP er)
En d'autres termes, nous voulons trouver toutes les séquences apparaissant dans resultatReel tout en évitant que le résultat soit plus grand qu'il ne le devrait (sinon l'ensemble de toutes les séquences, de toutes les navigations, pourrait constituer un résultat, car il englobe le résultat réel). Cette heuristique emprunte aux algorithmes génétiques leur conception du voisinage, en y intégrant les propriétés des motifs fréquents pour optimiser les candidats proposés. La principale idée, sur laquelle PERIO se base, consiste à parcourir l'ensemble P stab des périodes stables et, pour chaque période p de P stab , à générer des populations de candidats grâce aux items fréquents et aux opérateurs de voisinage. Ensuite ces candidats sont comparés avec les séquences de C p afin d'évaluer leur pertinence (ou tout au moins leur distance par rapport à une séquence fréquente). L'heuristique PERIO est décrite par l'algorithme suivant : ? taille(c) ; End algorithm Noter Pour chaque période de P stab , PERIO génère les nouveaux candidats et compare ensuite chacun des candidats aux séquences de C p . La comparaison consiste à obtenir un pourcentage qui représente la distance entre la séquence s du candidat et chaque séquence c de C p . Si s est incluse dans c, le pourcentage sera de 100% et ce taux va décroitre avec l'apparition de "parasites" (différences entre le candidat et la séquence de navigation). Pour évaluer cette distance, le pourcentage est obtenu en divisant la taille de la plus longue séquence commune (PLSC) entre s et c par la taille de s. Par exemple, si s, de taille 4, contient une sous-séquence de taille 3 en commun avec c, alors la note de s pour c sera de 3/4. De plus, dans le but d'obtenir des séquences les plus longues possible, nous utilisons un algorithme qui favorise les séquences les plus grandes, si elles sont incluses dans c. D'un autre côté, l'algorithme sanctionne les séquences trop longues si elles ne sont pas incluses (plus la séquences est longue, plus sa distance à la séquence de navigation sera pénalisante). Pour prendre en compte tous ces paramètres, le calcul effectué pour comparer les candidats à chaque séquence de C p est décrit par l'algorithme NOTER. Enfin, les candidats évalués par l'algorithme NOTER seront insérés dans SP si leur support est supérieur au support minimum ou s'ils correspondent à un "critère de sélection naturelle". Ce critère, spécifié par l'utilisateur prend la forme d'un pourcentage qui définit la distance entre le support du candidat et le support minimum. Dans un nombre de cas (choisis de façon aléatoire), les candidats dont le support correspond au critère peuvent être insérés dans SP afin d'éviter à l'heuristique PERIOD de converger vers un optimum local.
Algorithm
-406 -RNTI-E-6
Expérimentations
FIG. 1 -Pics de fréquence pour un comportement sur une période longue
Les programmes d'extraction sont réalisés en C++ sur des machines de type PC équipés de processus pentium 2,1 Ghz et exploités par un système RedHat. Nous avons effectué nos expérimentations sur les logs de l'Inria Sophia Antipolis. Ces logs sont découpés à raison de un log par jour. A la fin du mois, les logs journaliers sont regroupés sous forme d'un log mensuel. Nous avons donc travaillé sur les 14 logs mensuels disponibles, que nous avons considérés comme un seul log global recouvrant 14 mois d'enregistrements (de janvier 2004 à mars 2005). Ce log de 14 mois représente environ 14 Go de données. Il contient 3,5 millions de séquences (clients), la longueur moyenne de ces séquences est de 2, 68 et la longueur maximale est de 174 requêtes. Le log contient environ 2 millions de périodes et 300000 items. Le temps d'exé-cution de PERIOD sur ce log est d'environ 6 heures avec un support minimum de 2% (nous avons trouvé environ 400 groupes de comportements différents). Voici quelques exemples de comportements extraits :
C1 =<(semir/restaurant) (semir/restaurant/consult.php) (semir/restaurant/index.php) (semir/restaurant/index.php)> C2 =<(eg06) (eg06/dureve_040702.pdf) (eg06/fer_040701.pdf) (eg06)> C3 =<(requete.php3) (requete.php3) (requete.php3)> C4 =<(Hello.java) (HelloClient.java) (HelloServer.java)> C5 =<(mimosa/fp/Skribe) (mimosa/fp/Skribe/skribehp.css) (mimosa/fp/Skribe/index-5.html)> C6 =<(sgp2004) (navbar.css) (submission.html)> C7 =<(css/inria_sophia.css) (commun/votre_profil_en.shtml) (presentation/chiffres_en.shtml) (actu/actu_scient_colloque_ encours_fr.shtml)> Le comportement C1 correspond à une navigation typiquement périodique. En effet, la cantine de l'Inria Sophia Antipolis a été en travaux pendant cette période et les membre de l'unité pouvaient, via les pages web "semir/restaurant", commander leurs repas froids de la semaine. Le comportement C2 correspond à une navigation sur des pages relatives aux "états généraux" de la recherche. C3 se trouve sur le site du projet "mascotte" et C4 sur celui de "oasis". Ces deux comportements correspondent à la consultation de pages de TD/TP sur les sites des chercheurs qui les proposent. Le comportement C5 est interprété par l'auteur des pages comme une conséquence de nombreux échanges en mars 2004 sur la mailing-list de Skribe. Avec le préfixe "geometrica/events/" pour C6. Le comportement C6 connaît deux pics (début avril et mi-avril). Il s'agit d'un comportement relatif à la soumission d'articles pour le symposium sgp2004, dont la date limite de soumission était le 7 avril pour les résumés et le 14 avril pour les articles.

